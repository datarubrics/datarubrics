{"id": "shi23g_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. References\\n\\n[1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d JSTSP, 2022.\\n\\n[2] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal Performance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u20131198.\\n\\n[3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Proc. NeurIPS, vol. 33, pp. 12 449\u201312 460, 2020.\\n\\n[4] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech representation learning by masked prediction of hidden units,\u201d TASLP, vol. 29, pp. 3451\u20133460, 2021.\\n\\n[5] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492.\\n\\n[6] A. Babu et al., \u201cXLS-R: Self-supervised cross-lingual speech representation learning at scale,\u201d arXiv preprint arXiv:2111.09296, 2021.\\n\\n[7] A. Conneau et al., \u201cUnsupervised cross-lingual representation learning for speech recognition,\u201d arXiv preprint arXiv:2006.13979, 2020.\\n\\n[8] P.-A. Duquenne et al., \u201cSpeechmatrix: A large-scale mined corpus of multilingual speech-to-speech translations,\u201d arXiv preprint arXiv:2211.04508, 2022.\\n\\n[9] J. Zhao and W.-Q. Zhang, \u201cImproving automatic speech recognition performance for low-resource languages with self-supervised models,\u201d JSTSP, vol. 16, no. 6, pp. 1227\u20131241, 2022.\\n\\n[10] D. Berrebbi, J. Shi, B. Yan, et al., \u201cCombining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537.\\n\\n[11] A. Wu et al., \u201cSelf-supervised representations improve end-to-end speech translation,\u201d Proc. Interspeech 2020, pp. 1491\u20131495, 2020.\\n\\n[12] X. Li et al., \u201cASR2K: Speech Recognition for Around 2000 Languages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u20134889.\\n\\n[13] S. Evain et al., \u201cLeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech,\u201d in Proc. Interspeech, 2021, pp. 1439\u20131443.\\n\\n[14] T. Javed et al., \u201cIndicsuperb: A speech processing universal performance benchmark for indian languages,\u201d arXiv preprint arXiv:2208.11761, 2022.\\n\\n[15] A. Conneau et al., \u201cXTREME-S: Evaluating Cross-lingual Speech Representations,\u201d in Proc. Interspeech, 2022, pp. 3248\u20133252.\\n\\n[16] V. Pratap et al., \u201cMLS: A large-scale multilingual dataset for speech research,\u201d Proc. Interspeech 2020, pp. 2757\u20132761, 2020.\\n\\n[17] R. Ardila et al., \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proc. LREC, 2020, pp. 4218\u20134222.\\n\\n[18] K. MacLean, \u201cVoxforge,\u201d Ken MacLean. [Online]. Available: http://www.voxforge.org/home. [Accessed by 2022], 2018.\\n\\n[19] C. Wang et al., \u201cVoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d in Proc. ACL, 2021, pp. 993\u20131003.\\n\\n[20] K. Sodimana et al., \u201cA step-by-step process for building tts voices using open source data and framework for bangla, javanese, khmer, nepali, sinhala, and sundanese,\u201d in Proc. SLTU, 2018, pp. 66\u201370.\\n\\n[21] O. Kjartansson et al., \u201cOpen-source high quality speech datasets for basque, catalan and galician,\u201d in Proc. SLTU, 2020, pp. 21\u201327.\\n\\n[22] F. He et al., \u201cOpen-source multi-speaker speech corpora for building gujarati, kannada, malayalam, marathi, tamil and telugu speech synthesis systems,\u201d in Proc. LREC, 2020, pp. 6494\u20136503.\\n\\n[23] G. Rehm and H. Uszkoreit, \u201cLanguage technology support for norwegian,\u201d in The Norwegian Language in the Digital Age: Bokmalsversion, 2012, pp. 52\u201370.\\n\\n[24] A. Conneau et al., \u201cFleurs: Few-shot learning evaluation of universal representations of speech,\u201d in Proc. SLT, 2023, pp. 798\u2013805.\\n\\n[25] E. Barnard et al., \u201cThe nchlt speech corpus of the south african languages,\u201d 2014.\\n\\n[26] T. Baumann, A. K\u00f6hn, and F. Hennig, \u201cThe spoken wikipedia corpus collection: Harvesting, alignment and an application to hyperlistening,\u201d LREC, vol. 53, pp. 303\u2013329, 2019.\\n\\n[27] J. Shi et al., \u201cLeveraging end-to-end asr for endangered language documentation: An empirical study on yoloxochitl mixtec,\u201d in Proc. ACL, 2021, pp. 1134\u20131145.\\n\\n[28] J. Shi et al., \u201cHighland puebla nahuatl speech translation corpus for endangered language documentation,\u201d in Proc. AmericaNLP, 2021, pp. 53\u201363.\\n\\n[29] I. Solak, \u201cMailab speech dataset,\u201d Imdat Solak. [Online]. Available: https://www.caito.de/2019/01/03/the-mailabs-speech-dataset/. [Accessed by 2022], 2018.\\n\\n[30] D. A. Braude et al., \u201cAll together now: The living audio dataset.,\u201d in INTERSPEECH, 2019, pp. 1521\u20131525.\\n\\n[31] N. J. De Vries et al., \u201cA smartphone-based asr data collection tool for under-resourced languages,\u201d Speech communication, vol. 56, pp. 119\u2013131, 2014.\\n\\n[32] S. Watanabe, T. Hori, and J. R. Hershey, \u201cLanguage independent end-to-end architecture for joint language identification and speech recognition,\u201d in Proc. ASRU, 2017, pp. 265\u2013271.\\n\\n[33] W. Hou et al., \u201cLarge-Scale End-to-End Multilingual Speech Recognition and Language Identification with Multi-Task Learning,\u201d in Proc. Interspeech, 2020, pp. 1037\u20131041.\\n\\n[34] C. Zhang et al., \u201cStreaming End-to-End Multilingual Speech Recognition with Joint Language Identification,\u201d in Proc. Interspeech, 2022, pp. 3223\u20133227.\\n\\n[35] W. Chen et al., \u201cImproving massively multilingual ASR with auxiliary CTC objectives,\u201d Proc. ICASSP 2023, 2023.\\n\\n[36] T. Wolf et al., \u201cTransformers: State-of-the-art natural language processing,\u201d in Proc. EMNLP, 2020, pp. 38\u201345.\\n\\n[37] S. Watanabe et al., \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proc. Interspeech, 2018, pp. 2207\u20132211.\\n\\n[38] T.-h. Feng et al., \u201cSUPERB@ SLT 2022: Challenge on generalization and efficiency of self-supervised speech representation learning,\u201d in Proc. SLT, 2023, pp. 1096\u20131103.\\n\\n[39] X. Chang et al., \u201cAn exploration of self-supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021, pp. 228\u2013235.\\n\\n[40] S. Chen et al., \u201cWavLM: Large-scale self-supervised pre-training for full stack speech processing,\u201d JSTSP, vol. 16, no. 6, pp. 1505\u20131518, 2022.\\n\\n[41] W.-N. Hsu et al., \u201cRobust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training,\u201d in Proc. Interspeech, 2021, pp. 721\u2013725.\\n\\n[42] S. Liu and P. Guo. \u201cChinese speech pretraining.\u201d (2023), [Online]. Available: https://github.com/TencentGameMate/chinese_speech_pretrain (visited on 06/30/2022).\\n\\n[43] A. Lee et al., \u201cTextless speech-to-speech translation on real data,\u201d in Proc. NAACL, 2022, pp. 860\u2013872.\\n\\n[44] A. Pasad, B. Shi, and K. Livescu, \u201cComparative layer-wise analysis of self-supervised speech models,\u201d Proc. ICASSP 2023, 2022.\\n\\n[45] D. Berrebbi, B. Yan, and S. Watanabe, \u201cAvoid overthinking in self-supervised models for speech recognition,\u201d Proc. ICASSP 2023, 2022.\"}"}
{"id": "shi23g_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ML-SUPERB: Multilingual Speech Universal Performance Benchmark\\n\\nJiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Ping Huang, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe\\n\\n1 Carnegie Mellon University\\n2 National Taiwan University\\n3 Meta AI\\n4 Rembrand\\n\\n{jiatongs, dberrebbi, wc4, swatanab}@cs.cmu.edu, shangwenl@meta.com, abdo@rembrand.com, hungyilee@ntu.edu.tw\\n\\nAbstract\\n\\nSpeech processing Universal Performance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.\\n\\nIndex Terms: speech self-supervised learning, multilingual speech recognition, language identification\\n\\n1. Introduction\\n\\nSelf-supervised learning (SSL) has been a popular method in the speech community. SSL models have shown promising results by capturing important speech features, such as phonemes and other acoustic units, through training on large amounts of unlabeled speech data [1]. These models have led to significant improvements in downstream tasks, such as speech recognition, speaker identification, and emotion recognition [2]. Over the past few years, researchers have proposed a variety of SSL models with different training objectives, operating under various data conditions, model architectures, and modalities [3, 4].\\n\\nA major challenge in evaluating SSL models for speech is the difficulty of comparison since most models have been evaluated using different experimental setups. To address this issue, Yang et al. introduced the Speech processing Universal Performance Benchmark (SUPERB) [2]. Recently, an extension of SUPERB called SUPERB-SG [5] has been introduced. SUPERB provides a comprehensive speech SSL benchmark including tasks such as recognition, detection, semantics, speaker identification, paralinguistics, and generation. With SUPERB, researchers can more easily compare the performance of different SSL models on various speech-related tasks, universally.\\n\\nWhile SUPERB covers a wide range of speech tasks, it was designed primarily for English speech. However, there has been growing interest in applying SSL models to multilingual scenarios, such as training multilingual SSL models [6\u20138] or using SSL models in a cross-lingual manner [9\u201312]. To support future research in these areas, we propose a new benchmark called multilingual SUPERB (ML-SUPERB).\\n\\nML-SUPERB is designed to cover a wide range of languages, including both high-resource languages like English and endangered languages such as Totonac. The benchmark primarily focuses on evaluating SSL models for automatic speech recognition (ASR) and language identification (LID). To accommodate different use cases for SSL models, ML-SUPERB includes two tracks with four different tasks: the monolingual track (monolingual ASR), and the multilingual track (multilingual ASR, LID, joint multilingual ASR/LID). Similar to SUPERB, ML-SUPERB employs frozen SSL models as feature extractors and a lightweight downstream model that can be fine-tuned for different tracks to achieve high training efficiency.\\n\\nSeveral existing benchmarks also include multilingual SSL models [13\u201315]. Lebenchmark primarily evaluates speech tasks in French [13]; IndicSUPERB focuses mostly on Indian languages [14]. XTREME-S focuses on multilingual speech representation benchmarks, including ASR, speech translation, speech classification, and speech retrieval [15]. There are three main differences between XTREME-S and ML-SUPERB. Firstly, ML-SUPERB covers a wider range of languages, with 143 languages compared to XTREME-S's 102. Secondly, ML-SUPERB focuses on ASR and LID, while XTREME-S covers four different tasks. However, ML-SUPERB expands the tasks by evaluating them in four common multilingual research scenarios, while XTREME-S considers multilingual training only. Finally, ML-SUPERB is designed for efficiency, using smaller benchmark datasets and downstream models, and does not include fine-tuning. This lightweight setup allows us to conduct experiments for a dozen of popular speech SSL models, trained with various sizes and pre-training sets, and compare their performances across the proposed tracks. We expect ML-SUPERB would be a valuable complement to existing benchmarks.\\n\\n2. Benchmark Details\\n\\n2.1. Data Collection\\n\\nML-SUPERB gathers data from a wide range of multilingual speech corpora, including Multilingual Librispeech [16], Commonvoice [17], Voxforge [18], Voxpopuli [19], Googlei18n open-source project [20\u201322], Nordic Language Technology ASR corpora [23], Fleurs [24], NCHLT Speech [25], Spoken Wikipedia corpus [26], Mexican endangered languages [10, 27, 28], M-AILab multilingual corpora [29], Living Audio dataset [30], ALFFA corpus [31]. All corpora are with either Creative Commons, MIT, GNU, or Free-BSD licenses, which are available for both industrial and academic research, permissively.\\n\\nFor each language-corpus pair denoted as (lang, data), ML-SUPERB: Multilingual Speech Universal Performance Benchmark\\n\\nJiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Ping Huang, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe\\n\\n1 Carnegie Mellon University\\n2 National Taiwan University\\n3 Meta AI\\n4 Rembrand\\n\\n{jiatongs, dberrebbi, wc4, swatanab}@cs.cmu.edu, shangwenl@meta.com, abdo@rembrand.com, hungyilee@ntu.edu.tw\\n\\nAbstract\\n\\nSpeech processing Universal Performance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.\\n\\nIndex Terms: speech self-supervised learning, multilingual speech recognition, language identification\\n\\n1. Introduction\\n\\nSelf-supervised learning (SSL) has been a popular method in the speech community. SSL models have shown promising results by capturing important speech features, such as phonemes and other acoustic units, through training on large amounts of unlabeled speech data [1]. These models have led to significant improvements in downstream tasks, such as speech recognition, speaker identification, and emotion recognition [2]. Over the past few years, researchers have proposed a variety of SSL models with different training objectives, operating under various data conditions, model architectures, and modalities [3, 4].\\n\\nA major challenge in evaluating SSL models for speech is the difficulty of comparison since most models have been evaluated using different experimental setups. To address this issue, Yang et al. introduced the Speech processing Universal Performance Benchmark (SUPERB) [2]. Recently, an extension of SUPERB called SUPERB-SG [5] has been introduced. SUPERB provides a comprehensive speech SSL benchmark including tasks such as recognition, detection, semantics, speaker identification, paralinguistics, and generation. With SUPERB, researchers can more easily compare the performance of different SSL models on various speech-related tasks, universally.\\n\\nWhile SUPERB covers a wide range of speech tasks, it was designed primarily for English speech. However, there has been growing interest in applying SSL models to multilingual scenarios, such as training multilingual SSL models [6\u20138] or using SSL models in a cross-lingual manner [9\u201312]. To support future research in these areas, we propose a new benchmark called multilingual SUPERB (ML-SUPERB).\\n\\nML-SUPERB is designed to cover a wide range of languages, including both high-resource languages like English and endangered languages such as Totonac. The benchmark primarily focuses on evaluating SSL models for automatic speech recognition (ASR) and language identification (LID). To accommodate different use cases for SSL models, ML-SUPERB includes two tracks with four different tasks: the monolingual track (monolingual ASR), and the multilingual track (multilingual ASR, LID, joint multilingual ASR/LID). Similar to SUPERB, ML-SUPERB employs frozen SSL models as feature extractors and a lightweight downstream model that can be fine-tuned for different tracks to achieve high training efficiency.\\n\\nSeveral existing benchmarks also include multilingual SSL models [13\u201315]. Lebenchmark primarily evaluates speech tasks in French [13]; IndicSUPERB focuses mostly on Indian languages [14]. XTREME-S focuses on multilingual speech representation benchmarks, including ASR, speech translation, speech classification, and speech retrieval [15]. There are three main differences between XTREME-S and ML-SUPERB. Firstly, ML-SUPERB covers a wider range of languages, with 143 languages compared to XTREME-S's 102. Secondly, ML-SUPERB focuses on ASR and LID, while XTREME-S covers four different tasks. However, ML-SUPERB expands the tasks by evaluating them in four common multilingual research scenarios, while XTREME-S considers multilingual training only. Finally, ML-SUPERB is designed for efficiency, using smaller benchmark datasets and downstream models, and does not include fine-tuning. This lightweight setup allows us to conduct experiments for a dozen of popular speech SSL models, trained with various sizes and pre-training sets, and compare their performances across the proposed tracks. We expect ML-SUPERB would be a valuable complement to existing benchmarks.\\n\\n2. Benchmark Details\\n\\n2.1. Data Collection\\n\\nML-SUPERB gathers data from a wide range of multilingual speech corpora, including Multilingual Librispeech [16], Commonvoice [17], Voxforge [18], Voxpopuli [19], Googlei18n open-source project [20\u201322], Nordic Language Technology ASR corpora [23], Fleurs [24], NCHLT Speech [25], Spoken Wikipedia corpus [26], Mexican endangered languages [10, 27, 28], M-AILab multilingual corpora [29], Living Audio dataset [30], ALFFA corpus [31]. All corpora are with either Creative Commons, MIT, GNU, or Free-BSD licenses, which are available for both industrial and academic research, permissively.\\n\\nFor each language-corpus pair denoted as (lang, data),\\n\\nML-SUPERB: Multilingual Speech Universal Performance Benchmark\\n\\nJiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Ping Huang, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe\\n\\n1 Carnegie Mellon University\\n2 National Taiwan University\\n3 Meta AI\\n4 Rembrand\\n\\n{jiatongs, dberrebbi, wc4, swatanab}@cs.cmu.edu, shangwenl@meta.com, abdo@rembrand.com, hungyilee@ntu.edu.tw\\n\\nAbstract\\n\\nSpeech processing Universal Performance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.\\n\\nIndex Terms: speech self-supervised learning, multilingual speech recognition, language identification\\n\\n1. Introduction\\n\\nSelf-supervised learning (SSL) has been a popular method in the speech community. SSL models have shown promising results by capturing important speech features, such as phonemes and other acoustic units, through training on large amounts of unlabeled speech data [1]. These models have led to significant improvements in downstream tasks, such as speech recognition, speaker identification, and emotion recognition [2]. Over the past few years, researchers have proposed a variety of SSL models with different training objectives, operating under various data conditions, model architectures, and modalities [3, 4].\\n\\nA major challenge in evaluating SSL models for speech is the difficulty of comparison since most models have been evaluated using different experimental setups. To address this issue, Yang et al. introduced the Speech processing Universal Performance Benchmark (SUPERB) [2]. Recently, an extension of SUPERB called SUPERB-SG [5] has been introduced. SUPERB provides a comprehensive speech SSL benchmark including tasks such as recognition, detection, semantics, speaker identification, paralinguistics, and generation. With SUPERB, researchers can more easily compare the performance of different SSL models on various speech-related tasks, universally.\\n\\nWhile SUPERB covers a wide range of speech tasks, it was designed primarily for English speech. However, there has been growing interest in applying SSL models to multilingual scenarios, such as training multilingual SSL models [6\u20138] or using SSL models in a cross-lingual manner [9\u201312]. To support future research in these areas, we propose a new benchmark called multilingual SUPERB (ML-SUPERB).\\n\\nML-SUPERB is designed to cover a wide range of languages, including both high-resource languages like English and endangered languages such as Totonac. The benchmark primarily focuses on evaluating SSL models for automatic speech recognition (ASR) and language identification (LID). To accommodate different use cases for SSL models, ML-SUPERB includes two tracks with four different tasks: the monolingual track (monolingual ASR), and the multilingual track (multilingual ASR, LID, joint multilingual ASR/LID). Similar to SUPERB, ML-SUPERB employs frozen SSL models as feature extractors and a lightweight downstream model that can be fine-tuned for different tracks to achieve high training efficiency.\\n\\nSeveral existing benchmarks also include multilingual SSL models [13\u201315]. Lebenchmark primarily evaluates speech tasks in French [13]; IndicSUPERB focuses mostly on Indian languages [14]. XTREME-S focuses on multilingual speech representation benchmarks, including ASR, speech translation, speech classification, and speech retrieval [15]. There are three main differences between XTREME-S and ML-SUPERB. Firstly, ML-SUPERB covers a wider range of languages, with 143 languages compared to XTREME-S's 102. Secondly, ML-SUPERB focuses on ASR and LID, while XTREME-S covers four different tasks. However, ML-SUPERB expands the tasks by evaluating them in four common multilingual research scenarios, while XTREME-S considers multilingual training only. Finally, ML-SUPERB is designed for efficiency, using smaller benchmark datasets and downstream models, and does not include fine-tuning. This lightweight setup allows us to conduct experiments for a dozen of popular speech SSL models, trained with various sizes and pre-training sets, and compare their performances across the proposed tracks. We expect ML-SUPERB would be a valuable complement to existing benchmarks.\\n\\n2. Benchmark Details\\n\\n2.1. Data Collection\\n\\nML-SUPERB gathers data from a wide range of multilingual speech corpora, including Multilingual Librispeech [16], Commonvoice [17], Voxforge [18], Voxpopuli [19], Googlei18n open-source project [20\u201322], Nordic Language Technology ASR corpora [23], Fleurs [24], NCHLT Speech [25], Spoken Wikipedia corpus [26], Mexican endangered languages [10, 27, 28], M-AILab multilingual corpora [29], Living Audio dataset [30], ALFFA corpus [31]. All corpora are with either Creative Commons, MIT, GNU, or Free-BSD licenses, which are available for both industrial and academic research, permissively.\\n\\nFor each language-corpus pair denoted as (lang, data),\\n\\nML-SUPERB: Multilingual Speech Universal Performance Benchmark\\n\\nJiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Ping Huang, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe\\n\\n1 Carnegie Mellon University\\n2 National Taiwan University\\n3 Meta AI\\n4 Rembrand\\n\\n{jiatongs, dberrebbi, wc4, swatanab}@cs.cmu.edu, shangwenl@meta.com, abdo@rembrand.com, hungyilee@ntu.edu.tw\\n\\nAbstract\\n\\nSpeech processing Universal Performance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the"}
{"id": "shi23g_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics of the data used for training, development, and testing in ML-SUPERB. Detailed discussed in Sec. 2.1.\\n\\n| Dataset | Hours | Normal Langs (123) Few-shot Langs (20) |\\n|---------|-------|----------------------------------------|\\n| 10-minute | 37.43 | \u223c10min \u00d7 240 (lang, data) 5 utt. \u00d7 20 lang |\\n| 1-hour | 222.46 | \u223c1h \u00d7 240 (lang, data) |\\n| Dev. | 41.82 | \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) |\\n| Test | 44.97 | \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) |\\n\\nThree 10-minute subsets are randomly extracted for training, development, and testing, along with an additional 1-hour training set that includes the 10-minute training set. The reasons for using a small 10-minute/1-hour training set are: (1) Challenging design: using a large training data size could lead to high performance easily and may result in a saturated benchmark in evaluation metrics [3, 4]. Therefore, using a smaller training set size presents a more challenging design for the SSL models, which can help evaluate their robustness and generalization capability. (2) Reasonable performance: previous speech SSL works have frequently adopted 10-minute and 1-hour training sizes. Even in such extreme cases, the performances with SSL are generally reasonable [3, 4], indicating that this setting could be a feasible solution to the benchmark as well. (3) Training efficiency: with 143 languages coverage, limiting the training size is important to keep the experiments within reasonable computational efforts. Using a smaller training set size can help reduce the computational cost and make the training process more efficient. A full evaluation cycle of ML-SUPERB can take up to 3 days using 4 2080Ti GPUs.\\n\\nAdditionally, the benchmark includes few-shot cases with 20 languages and uses only 5 utterances in training for each language. These reserved few-shot training sets are not used in the monolingual ASR track. A detailed summary of the dataset is shown in Table 1.\\n\\n2.2. Monolingual Track\\n\\nThe literature suggests that speech SSL models are commonly fine-tuned on monolingual corpora [9\u201311]. In ML-SUPERB, we introduce a dedicated track for monolingual ASR to facilitate this approach. We select nine languages based on geographical and linguistic considerations to balance language and domain coverage with manageable experimental mass.\\n\\nIn total, we introduce 14 monolingual experiments. For a monolingual experiment in language lang we select one dataset of this language and use it for training the model and for validation. For evaluation of a monolingual experiment, we use all the datasets of lang to test the trained model on various accent or domain conditions. We select one pair (lang, data) for training for lang \u2208 {rus, swa, swe, jpn, cmn, xty}. For lang \u2208 {eng, fra, deu} we select respectively 3, 2 and 2 pairs (lang, data) in order to evaluate the impact of the training domain on the models' performances. For instance, for eng we have 3 monolingual experiments, with (eng, MLS), (eng, NCHLT) and (eng, VoxPopuli).\\n\\n2.3. Multilingual Track\\n\\nMultilingual ASR task: in the multilingual ASR task, we use the training set where combining text transcriptions from all 143 languages. The multilingual ASR task has two sub-tasks on the 10-minute train set and the 1-hour train set. For both training sets, we reserve 20 languages for few-shot learning scenarios as discussed in Sec. 2.1. In this track, the model is expected to directly predict the correct orthography in the target language.\\n\\nLID task: LID track focuses on language identification with the same training set of 143 languages in 10 minutes and 1 hour. However, we do not consider evaluation for languages with few-shot settings, given that the identification of those languages is very challenging due to the label biasing.\\n\\nJoint Multilingual ASR/LID task: A widely used technique in previous literature involves adding the language ID to the start of the speech transcript to facilitate joint training of multilingual ASR and LID models [32\u201335]. Joint training can improve performance in certain scenarios, and it can also enhance model interpretability by separating language identification errors. Therefore, we have included this task in our multilingual track. The task's design is the same as the multilingual ASR task for ASR and the LID task for language identification.\\n\\n2.4. Framework and Benchmark Settings\\n\\nToolkits: We utilize the S3PRL toolkit [2] for upstream models, which offers a wide range of speech SSL model architectures and APIs that support customized SSL models from Huggingface [36] and user-defined models. For task-specific downstream training, we use ESPnet [37]. We plan to publish ML-SUPERB as an all-in-one recipe in ESPnet's egstraining collection, encompassing data preprocessing, training, inference, and evaluation.\\n\\nDownstream model and training details: Our downstream model design is based on the SUPERB concept. First, we compute a weighted summation of frozen speech SSL representations using learnable weights. Next, we apply a convolutional downsample layer that reduces the sequence of speech SSL features by half, passing the resulting hidden states to a transformer model consisting of two layers with an attention dimension of 256, a feedforward layer dimension of 1024, and 8 attention heads. A dropout rate of 0.1 is employed, and the model is trained using the connectionist temporal classification loss. We use the Adam optimizer with a learning rate of 0.0001 and 1e-6 weight decay. Specaugment is applied to the representation (i.e., the weighted sum of speech SSL representation) following the SUPERB benchmark. The batch size is set to 8 with the gradient accumulation as 4. The same configuration is used for all tasks in both the monolingual and multilingual tracks. The number of iterations in training is the only difference across tasks. In the monolingual track, due to the small training size, we set it to 15,000. In the multilingual track, we use 300,000 iterations for the 10-minute train set and 600,000 for the 1-hour train set.\\n\\nEvaluation metric: In the monolingual track, the phoneme error rate is used for jpn and cmn, while Character Error Rate (CER) is used for the remaining languages. In the multilingual track, we use CER for ASR evaluation and accuracy rate.\"}"}
{"id": "shi23g_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Description of the candidate models.\\n\\n| Model                          | Params (M) | Pre-Training |\\n|-------------------------------|------------|-------------|\\n| wav2vec2-base [3]             | 95         | 1k          |\\n| wav2vec2-large [3]            | 317        | 60k         |\\n| robust-wav2vec2-large [41]    | 317        | 65k         |\\n| wav2vec2-base-23 [19]         | 95         | 100k        |\\n| wav2vec2-large-23 [19]        | 317        | 100k        |\\n| XLSR-53 [7]                   | 317        | 56k         |\\n| XLSR-128 [6]                  | 317        | 400k        |\\n| HuBERT-base [4]               | 95         | 1k          |\\n| HuBERT-large [4]              | 317        | 60k         |\\n| HuBERT-base-cmn [42]          | 95         | 10k         |\\n| HuBERT-large-cmn [42]         | 317        | 10k         |\\n| mHuBERT-base [43]             | 95         | 14k         |\\n| mHuBERT-large [43]            | 317        | 10k         |\\n\\nFor LID evaluation, reporting results separately for the normal training set and the few-shot training set.\\n\\nFor overall performance, we use the SUPERB metric from the SUPERB benchmark [38]. We denote $s_{t,i}(u)$ as the $i$th metrics for task $t$ and SSL model $u$. $T$ is the set of four tasks and $I_t$ is the set of metrics for the task $t$. SUPERB aggregates all task-specific scores $s_{t,i}(u)$ with respect to baseline (i.e., FBANK) and state-of-the-art (SOTA) model on the task $t$. The SUPERB is defined as:\\n\\n$$\\n\\\\text{SUPERB} = \\\\frac{1000}{|T|} \\\\sum_{T} s_{t}(u) - s_{t}(\\\\text{FBANK}) - s_{t}(\\\\text{SOTA})\\n$$\\n\\nWe expect SUPERB can provide a comprehensive view of the model performance on the benchmark and take the difficulty of tasks into consideration.\\n\\nAnalysis support: To facilitate a more comprehensive analysis of the benchmark, we provide various analysis tools. For the multilingual ASR evaluation, we present the character error rate (CER) for each language as well as aggregated scores for different language groups, in addition to the average CER for both normal and few-shot cases. In line with previous studies [39, 40], we also offer visualizations of the learnable layer weights and their learning curve during training.\\n\\n3. Experiments\\n\\n3.1. Candidate models\\n\\nML-SUPERB welcomes all speech SSL models trained on either monolingual or multilingual data. We believe the analysis of multilingual scenarios for monolingual speech SSLs is also valuable according to previous works [9\u201311]. In this paper, we show the experimental results of some example model candidates as shown in Table 2.\\n\\n**wav2vec2**: wav2vec2 is a popular speech SSL model for speech recognition [3]. Its pre-training uses a contrastive learning approach that prioritizes identifying true quantized latent speech representations over masked time steps from distractors. The wav2vec2 model has also been extended to many other versions for specialized use cases. For example, robust-wav2vec2-large [41] considers the diversity of speech types, such as read speech, conversational speech, and noisy speech, by including additional corpora in the pre-training stage. Wav2vec2-base-23 and wav2vec2-large-23 are pre-trained on Voxpopuli [19], with a focus on European languages. Additionally, XLSR scales up the multilingual training in wav2vec2 by incorporating more languages and data [6, 7].\\n\\nThe SOTA models for each setting are discussed in Sec. 3.2.\\n\\n**HuBERT**: HuBERT uses an iterative offline clustering step to generate pseudo labels for each frame. During training, it predicts the pseudo labels of the masked frame, which helps to improve the quality of the learned features. Similar to wav2vec2, HuBERT also has different versions, such as a multilingual HuBERT [43] trained in three European languages (fra, spa, eng) and HuBERT trained on Mandarin [42].\\n\\n3.2. Experimental Results\\n\\nThe experimental results are shown in Table 3 for 10-minute set and Table 4 for 1-hour set.\\n\\n**Monolingual ASR**: In the monolingual ASR task, all speech SSL models outperform the FBANK baseline. XLSR-128 achieves the best performance in the 1-hour set, while HuBERT-large obtains the best performance in the 10-minute set. Several findings are noteworthy: (1) HuBERT-based models outperform wav2vec2-based models when the training data and model size are similar. (2) Large models usually obtain better results than their base versions. (3) While the XLSR series of models deliver impressive performances in the 1-hour set, we have observed their instability in the 10-minute set, particularly on Asian languages such as cmn.\\n\\n**Multilingual ASR**: In the multilingual ASR task, all models trained using self-supervised learning (SSL) techniques have shown superior performance compared to the baseline model using FBANK features. Among the SSL models, XLSR-128 achieves the best results across all conditions. Our experiments also reveal some interesting findings: (1) Models trained with more languages generally outperform those trained on monolingual datasets, although this may not always be the case. For example, mHuBERT-base performs worse than HuBERT-based models trained on English only. (2) Large models trained on monolingual data do not necessarily have better representations for multilingual scenarios. For instance, HuBERT-large performs worse than HuBERT-base, and wav2vec2-large is less effective than wav2vec2-base. One possible explanation for the lack of performance improvement with larger models is their limited ability to generalize, despite having similar training losses as base models. (3) The robust-wav2vec2-large model achieves decent scores on multilingual ASR, suggesting that our benchmark corpus may need to consider different acoustic environments, as it includes multiple source datasets.\\n\\n**LID**: In the LID task, we notice similarities with multilingual ASR, but there are also notable differences. (1) XLSR-128 has been the dominant model for both 10-minute and 1-hour datasets. (2) While most SSL models have improvements over FBANK, some do not, particularly those based on wav2vec2 (e.g., wav2vec2-large-23 for the 10-minute set and wav2vec2-large for the 1-hour set). (3) Larger models with more parameters and pre-trained data do not necessarily lead to better performance compared to base models.\\n\\n**Joint Multilingual ASR + LID**: In the joint multilingual ASR+LID task, the results generally align with the other two tasks in the multilingual track. (1) SSL models outperform FBANK on ASR, but some models perform worse on LID. (2) Base models exhibit better generalization ability and often perform better on test sets. (3) There is no single best model that dominates the task, particularly in few-shot cases and LID tasks.\\n\\n**Overall**: In terms of overall performance as measured by SUPERB in Sec. 2.4, XLSR-128 is the best model for both the 10-minute and 1-hour sets. Major findings include: (1) multilingual training with a broad coverage of languages, as seen in XLSR models that include more than 50 languages, has proven\"}"}
{"id": "shi23g_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 3: 10-minute set ML-SUPERB benchmark.\\n\\n| SSL        | Monolingual ASR | Multilingual ASR | LID | Multilingual ASR + LID |\\n|------------|-----------------|------------------|-----|------------------------|\\n|            | Normal Few-shot | Normal            |     |                        |\\n|            | CER             | PER              | ACC | CER                    |\\n| FBANK      | 72.1            | 62.4             | 58.3| 11.11                  |\\n| wav2vec2-base [3] | 44.2          | 43.0             | 45.7| 54.4                   |\\n| wav2vec2-large [3] | 42.0          | 42.6             | 45.8| 30.9                   |\\n| robust-wav2vec2-large [41] | 44.4      | 40.1             | 45.4| 50.8                   |\\n| wav2vec2-base-23 [19] | 49.2         | 37.7             | 43.4| 58.7                   |\\n| wav2vec2-large-23 [19] | 42.0         | 42.1             | 44.3| 1.1                    |\\n| XLSR-53 [7]   | 49.5            | 33.9             | 43.6| 6.6                    |\\n| XLSR-128 [6]  | 39.7            | 29.2             | 40.9| 66.9                   |\\n| HuBERT-base [4] | 42.8          | 39.8             | 44.5| 61.2                   |\\n| HuBERT-large [4] | 38.2          | 44.4             | 48.2| 46.5                   |\\n| HuBERT-base-cmn [42] | 43.1       | 40.8             | 45.4| 49.3                   |\\n| HuBERT-large-cmn [42] | 39.4       | 42.6             | 45.8| 39.5                   |\\n| mHuBERT-base [43] | 41.0         | 40.5             | 45.6| 52.4                   |\\n\\n### Table 4: 1-hour set ML-SUPERB benchmark.\\n\\n| SSL        | Monolingual ASR | Multilingual ASR | LID | Multilingual ASR + LID |\\n|------------|-----------------|------------------|-----|------------------------|\\n|            | Normal Few-shot | Normal            |     |                        |\\n|            | CER             | PER              | ACC | CER                    |\\n| FBANK      | 63.7            | 59.3             | 57.4| 9.3                    |\\n| wav2vec2-base [3] | 35.9          | 35.5             | 44.3| 80.8                   |\\n| wav2vec2-large [3] | 35.4          | 35.7             | 43.9| 8.0                    |\\n| robust-wav2vec2-large [41] | 35.7      | 31.1             | 42.2| 72.1                   |\\n| wav2vec2-base-23 [19] | 35.1         | 32.0             | 42.2| 71.9                   |\\n| wav2vec2-large-23 [19] | 34.2         | 35.3             | 42.4| 64.2                   |\\n| XLSR-53 [7]   | 34.9            | 26.9             | 40.6| 87.1                   |\\n| XLSR-128 [6]  | 30.6            | 22.0             | 39.3| 87.9                   |\\n| HuBERT-base [4] | 35.3            | 31.4             | 42.7| 86.1                   |\\n| HuBERT-large [4] | 32.2            | 37.7             | 43.5| 64.1                   |\\n| HuBERT-base-cmn [42] | 35.6       | 43.2             | 46.6| 85.3                   |\\n| HuBERT-large-cmn [42] | 33.7       | 39.6             | 45.1| 57.3                   |\\n| mHuBERT-base [43] | 33.0            | 33.4             | 43.6| 72.5                   |\\n\\n---\\n\\n**3.3. Layerwise analysis**\\n\\nOur benchmark offers tools to guide users in the use of SSL representations according to their needs, including an analysis of the learned weights for layer importance. The results for the XLSR-128 model in monolingual ASR tasks (shown in Fig 1) confirm the conclusions reached by [44] and [45]: the most relevant layers for ASR are not the last few layers. We also observed that English3, French2, and German2 have very similar behavior. These tasks use VoxPopuli data for training, which is the only dataset with lecture speech in our collection. Additionally, Mixtec is the only conversational speech data among our sets, and we can see a distinct behavior in Fig 1. Therefore, the relevance of SSL model layers may be related to the speech domain (in addition to the speech task) rather than the language.\\n\\n---\\n\\n**4. Conclusion**\\n\\nThis paper introduces ML-SUPERB, a benchmark that extends SUPERB to multilingual tasks. We present the design of the open-source framework and discuss experimental results for some example models. More detailed policies can be found at https://multilingual.superbbenchmark.org/. We invite the community to participate in this challenge.\"}"}
