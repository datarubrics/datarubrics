{"id": "anwar23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We introduce MuA ViC, a multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation providing 1200 hours of audio-visual speech in 9 languages. It is fully transcribed and covers 6 English-to-X translation as well as 6 X-to-English translation directions. To the best of our knowledge, this is the first open benchmark for audio-visual speech-to-text translation and the largest open benchmark for multilingual audio-visual speech recognition. Our baseline results show that MuA ViC is effective for building noise-robust speech recognition and translation models. We make the corpus available at https://github.com/facebookresearch/muavic.\\n\\nIndex Terms: corpus, audio-visual, multimodal, multilingual, speech recognition, speech-to-text translation\\n\\n1. Introduction\\nVisual perception of lip movement plays a crucial role in supplementing audio signals for a better understanding of spoken languages [1]. Its immunity to acoustic noise makes it a valuable asset in building noise-robust speech processing systems. Audio-visual speech recognition (AVSR), which transcribes spoken utterances using both audio and visual inputs, has been shown to be effective in improving the robustness of speech recognition [2, 3]. The application of deep learning has greatly improved the performance of AVSR systems, as evidenced by the state-of-the-art (SOTA) models in the widely used LRS3-TED public benchmark. Typically, SOTA systems [3] are able to lower word error rate (WER) by 3 times compared to its audio-only counterpart under challenging conditions with 0 dB babble noise.\\n\\nIn spite of the encouraging progress mentioned earlier, modern AVSR models are heavily benchmarked under a monolingual setup, with the majority being based on English. Non-English audio-visual datasets are orders of magnitude smaller, with a common dataset, CMU-MOSEAS [4] providing less than 20 hours of audio-visual speech for each of the 4 non-English languages it covers, which is at least 20 times smaller than the English-based LRS3-TED corpus. The lack of large-scale audio-visual datasets also impedes the development of audio-visual speech-to-text translation (AVST), which is only simulated using synthetic data in prior works, but is expected to bring a noise-robust effect similar to AVSR, as will be demonstrated in this paper. With the increasing popularity of self-supervised audio-visual pre-training approaches [5], proper benchmarking of multilingual AVSR and AVST models will enable multilingual pre-training, which allows more effective use of large-scale multilingual audio-visual data such as AV-Speech [6].\\n\\nIn this paper, we present MuA ViC, a multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation. This corpus is sourced from TED and TEDx talks including total 1200 hours of transcribed audio-visual speech from over 8000 speakers in 9 languages: English (En), Arabic (Ar), German (De), Greek (El), Spanish (Es), French (Fr), Italian (It), Portuguese (Pt) and Russian (Ru). This makes MuA ViC the largest open benchmark so far for multilingual audio-visual speech recognition (AVSR) and lipreading. In addition, we provide text translations and establish baselines for 6 English-to-X translation as well as 6 X-to-English translation directions. To the best of our knowledge, MuA ViC is the first publicly available corpus for audio-visual speech-to-text translation.\\n\\n2. Related Works\\nAudio-visual speech recognition\\nAudio-visual speech recognition (AVSR) has been a popular research topic for the speech recognition community due to its potential to improve robustness in noisy environments. Early approaches relied on hand-crafted features such as optical flow and lip movement detection, but with the advent of deep learning, more data-efficient methods have emerged. One such approach is to use convolutional neural networks (CNNs) to process both audio and visual inputs in parallel, and then combine them at the fusion stage. A popular example of this is the multi-view audio-visual network (MAVNet) introduced by [2]. Another approach is to use recurrent neural networks (RNNs) to process the audio-visual inputs sequentially, such as the audio-visual integrated network (AVIN) proposed by [3]. These models have achieved state-of-the-art performance on public benchmarks such as the LibriSpeech ASR corpus [8].\\n\\nTable 1: Duration and speaker statistics for MuAViC.\\n\\n| Language | Duration (Hours) | Speakers | Train (H+P) | Dev | Test |\\n|----------|-----------------|---------|-------------|-----|------|\\n| Train    |                 |         |             |     |      |\\n| En       | 436             | 1.1     | 0.8         | 4.7K| 0.9K |\\n| Ar       | 16              | 1.5     | 1.2         | 95  | 7    |\\n| De       | 10              | 1.6     | 1.5         | 53  | 9    |\\n| El       | 25              | 2.3     | 2.0         | 113 | 10   |\\n| Es       | 178             | 1.6     | 1.7         | 987 | 16   |\\n| Fr       | 176             | 1.8     | 1.5         | 948 | 12   |\\n| It       | 101             | 1.9     | 1.9         | 487 | 8    |\\n| Pt       | 153             | 1.5     | 1.8         | 810 | 9    |\\n| Ru       | 49              | 1.7     | 1.8         | 238 | 7    |\\n\\nIn summary, MuA ViC offers a comprehensive dataset for both AVSR and AVST, enabling researchers to develop models that are robust to noise and capable of handling multilingual speech.\"}"}
{"id": "anwar23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Results for English speech recognition and En-X speech translation in a clean environment (A: audio, AV: audio+video).\\n\\n| ID  | Model                  | Mode | Target | Avg  |\\n|-----|------------------------|------|--------|------|\\n|     |                        |      |        | En   | El  | Es   | Fr   | It   | Pt   | Ru   |\\n| A1  | Monolingual A V-HuBERT | A    | 2.5    | -    | -   | -    | -    | -    | -    | -    |\\n|     |                        | A V  | 2.3    | -    | -   | -    | -    | -    | -    | -    |\\n| A2  | Bilingual Transformer  | base |       | -    | -   | 25.8 | 29.5 | 27.0 | 22.6 | 23.9 |\\n| A3  | M2M-100                | 418M |       | -    | -   | 24.5 | 28.7 | 25.6 | 21.8 | 22.2 |\\n\\nCommunity for a long time. Most of the early A VSR datasets, such as GRID [9], were collected in a controlled environment where subjects were required to recite pre-defined phrases in a laboratory-like setting (for example, with uniform illumination conditions). These datasets also have limitations in terms of visual perspectives and often consist of small-scale data with limited vocabulary size and number of speakers. The first large-scale \u201cin-the-wild\u201d A VSR dataset was LRW [10], which contains 173 hours of speaking video clips of 500 isolated words from BBC broadcasts. In recent years, there has been a growing trend towards collecting sentence-level audio-visual speech data, such as LRS3-TED [8, 2], which is more natural and widely accessible. LRS3-TED is the largest publicly available A VSR dataset and contains approximately 437 hours of English speaking videos from over 5000 speakers.\\n\\nThe majority of the widely-used A VSR datasets are in English, while non-English A VSR datasets [4, 11, 12] are significantly smaller in size. For example, the largest A VSR dataset in Mandarin Chinese, CMLR [11], comprises only 86 hours of utterances from 11 speakers. The current largest multilingual A VSR dataset, CMU-MOSEAS [4], contains total 68 hours of speech in Spanish, Portuguese, German, and French. Furthermore, it lacks cross-lingual translations, hindering its use for audio-visual speech-to-text translation (A VST).\\n\\nIt is noteworthy that there exist large-scale, unlabeled multilingual audio-visual speech datasets, such as VoxCeleb2 [13] (2442 hours) and AV-Speech [6] (around 4700 hours). They have been widely utilized for various non-transcription tasks, such as speaker verification [13, 14] and source separation [6], or as a source of pre-training data for self-supervised audio-visual representation learning [5, 15, 16]. However, they are not directly applicable to A VSR and A VST due to the lack of transcriptions.\\n\\nSpeech-to-text translation\\n\\nSpeech-to-text translation (ST) or spoken language translation (SLT) has become an increasingly popular topic of speech research with the recent emergence of open ST benchmarks. Almost all the ST corpora were created by adding translations of transcriptions to pre-existing speech recognition corpora, where speech data was collected from various sources, such as phone call recordings [17], read speech recordings [18, 19, 20] and public speech recordings [21, 22, 23, 24]. Despite the occasional availability of video tracks in the original data sources of these corpora, their creators have focused on only the audio modality and included only audio data in the corpora. Due to the lack of labeled audio-visual corpora, audio-visual speech-to-text translation remains a largely uncharted research area.\\n\\n3. Corpus Creation\\n\\nMuA ViC sources data from TED and TEDx talk recordings, where native or non-native speakers (only one speaker most of the time) deliver public speech on stage and cameras capture stage scenes switching among different viewpoints. We collect both audio and video tracks from the recordings, and align them with human transcriptions as well as text translations. For English talks, we reuse the audio-visual data from LRS3-TED [8] and follow the original data split. We find human translations for these talks from a machine translation corpus, TED2020 [26] by matching transcriptions in LRS3-TED and source sentences in TED2020. Matched LRS3-TED examples are then paired with the corresponding target sentences in TED2020 for translation labels. We apply exact text matching for development set and test set examples to ensure the best accuracy. To improve matching recall on the train set, we develop a fuzzy text matching strategy: we first segment TED2020 source and target sentences by punctuation if both sides of the sentence pair contain the same amount of segments. Then we normalize TED2020 and LRS3-TED texts by punctuation removal and lowercasing. Finally, we conduct exact text matching between the two corpora. For LRS3-TED train set examples without a match from TED2020, we acquire pseudo-translation labels from a machine translation model, M2M-100 418M [7] with default decoding hyper-parameters.\\n\\nFor non-English talks, we reuse the audio-only data, transcriptions and text translations collected by mTEDx [24]. Our data split also follows mTEDx. We acquire video tracks of the original recordings, and align processed video data with the audio one to form audio-visual data, similar to LRS3-TED [8]. Although all the audio data in mTEDx is transcribed, only a subset of it is translated. We acquire pseudo-translation labels from M2M-100 418M [7] for the untranslated train set examples with default decoding hyper-parameters.\"}"}
{"id": "anwar23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| ID | Model               | Mode | Source            | Avg Source | Ar De El Es Fr It Pt Ru |\\n|----|---------------------|------|-------------------|------------|-------------------------|\\n|    |                     |      |                   | Speech Recognition, Test WER | # |\\n|    | Transformer, monolingual [24] | A   | 104.4 111.1 109.5 46.4 45.6 48.0 54.8 74.7 74.3 |\\n|    | Hybrid LF-MMI, monolingual [24] | A   | 80.8 42.3 25.0 16.2 19.4 16.4 20.2 28.4 31.1 |\\n|    | Whisper, multilingual, unconstrained data [25] | A | 91.5 24.8 25.4 12.0 12.7 13.0 15.5 31.1 28.2 |\\n|    | Monolingual A V-HuBERT | A V | 99.3 61.1 35.1 16.5 24.4 19.3 23.0 33.3 39.0 |\\n|    | Multilingual A V-HuBERT | A V | 67.7 46.5 40.4 30.6 27.0 19.3 19.8 37.6 36.1 |\\n|    | Bilingual Transformer | base | - - - 13.9 28.2 33.8 27.2 31.9 14.2 27.0 |\\n|    | M2M-100 | 418M | - - - 25.9 29.5 34.9 30.7 33.6 19.1 29.0 |\\n|    | Whisper, multilingual, unconstrained data [25] | A | - - 24.2 28.9 34.5 29.2 32.6 16.1 29.9 |\\n|    | Bilingual A V-HuBERT | A V | 7.3 20.4 26.4 19.5 24.2 9.2 17.8 |\\n|    | Multilingual A V-HuBERT | A V | 9.3 21.0 26.3 21.2 24.3 9.3 18.6 |\\n\\nWe observe that our monolingual A VSR models in the audio-only mode achieve a low test WER of 2.5 and 2.3 respectively for audio-only and audio-visual modes. For non-English A VSR, we fine-tune the pre-trained English A V-HuBERT model either separately on each language (8 monolingual models) or jointly on all the 8 non-English languages (a multilingual model), whose test WER can be found in Table 3.\"}"}
{"id": "anwar23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model Mode          | Source/Target | Avg | En | Ar | De | El | Es | Fr | It | Pt | Ru |\\n|--------------------|---------------|-----|----|----|----|----|----|----|----|----|----|\\n| Whisper, multilingual, unconstrained data [25] | A | 202.4 | 197.9 | 244.4 | 113.3 | 116.3 | 172.3 | 172.4 | 223.6 | 126.2 | 174.3 |\\n| Monolingual        | A V-HuBERT    | 25.9 | 102.3 | 83.5 | 74.6 | 65.1 | 63.6 | 71.0 | 77.3 | 68.1 | 70.2 |\\n|                    | A V           | 5.7 | 101.0 | 74.6 | 53.1 | 45.1 | 48.1 | 51.6 | 47.2 | 51.9 | 53.2 |\\n| Multilingual       | A V-HuBERT    | -   | 86.1 | 76.0 | 73.7 | 59.7 | 54.7 | 61.0 | 61.9 | 66.9 |\\n|                    | A V           | -   | 82.2 | 66.9 | 62.2 | 40.7 | 39.0 | 44.3 | 43.1 | 43.1 | 52.7 |\\n| En-X Speech-to-Text Translation, Test BLEU       | Bilingual A V-HuBERT | -   | -   | -   | 15.9 | 19.2 | 17.1 | 12.9 | 14.4 | 10.3 | 15.0 |\\n|                    | Bilingual A V | -   | -   | -   | 3.9 | 8.6 | 13.1 | 7.7 | 9.5 | 5.7 | 8.1 |\\n|                    | Multilingual A V-HuBERT | -   | -   | -   | 2.9 | 8.4 | 12.4 | 8.1 | 8.6 | 0.9 | 6.9 |\\n|                    | Multilingual A V | -   | 4.2 | 12.8 | 15.0 | 12.5 | 14.8 | 4.6 | 10.7 |\\n\\nVisual mode outperform a comparable ASR baseline (Transformer, monolingual) by average 52% WER reduction. They underperform the hybrid LF-MMI monolingual ASR baselines by average 14% WER increase, which leverage 4-gram language models in decoding while our models do not. Our multilingual A VSR model on average slightly outperforms monolingual A VSR models, with gains observed on some low-resource languages (Ar and De) and degradation observed on the others (El and Ru). We also observe that our multilingual A VSR model falls behind Whisper by average 20% WER due to the large gap in the amount of training data (0.7K hours compared to 680K hours).\\n\\n4.2.2. Noisy Setup\\n\\nThe first section of Table 4 shows the test WER of our A VSR models in a noisy setup, where we simulate noisy environments by adding multilingual babble noises to clean speech inputs with a SNR (signal-to-noise ratio) of 0.2. We observe that Whisper, a SOTA multilingual ASR model, performs catastrophically in this challenging setup, with a high average WER of 174.3 over the 9 languages. In contrast, our monolingual A VSR models in the audio-only mode have an average WER of 70.2 and 66.7 respectively. In the audio-visual mode, the average WER of our models drop significantly by 32%, suggesting their efficient use of visual information to alleviate the distraction of noisy environments. Our multilingual A VSR model outperforms the monolingual counterparts on every non-English language (except El) in both audio-only and audio-visual modes.\\n\\n4.3. Audio-Visual Speech-to-Text Translation (A VST)\\n\\n4.3.1. Clean Setup\\n\\nWe report test BLEU for En-X A VST and X-En A VST models in Table 2 and Table 3, respectively. Besides end-to-end A V-HuBERT A VST models, we also set up bilingual and multilingual MT baselines, and build cascaded A VST baselines by pipelining A V-HuBERT A VSR models with MT models. We see that our end-to-end A VST models are on par with the cascaded counterparts in the constrained data setup (\\\"A1 + A3\\\" and \\\"B1 + B3\\\") for both En-X and X-En directions. Similar to the case in non-English A VSR, our 6-language multilingual X-En A VST model on average perform slightly better than the corresponding bilingual A VST models.\\n\\n4.3.2. Noisy Setup\\n\\nWe evaluate our En-X A VST and X-En A VST models in a noisy setup, whose test BLEU are shown in the second and third section of Table 4, respectively. We simulate noisy environments in the same approach as that for A VSR models, where multilingual babble noises are added to clean speech inputs with a SNR of 0. We observe that Whisper, a SOTA multilingual X-En speech-to-text translation model, has a catastrophic performance under this setup, with only 0.3 average BLEU over the 6 directions. Our bilingual and multilingual A VST models in the audio-only mode outperform it largely with 7.8 and 6.6 average BLEU improvement, respectively. A VST models in the audio-visual mode consistently outperform those in the audio-only mode, with an improvement of 3.2 and 3.3 on average BLEU for the bilingual and multilingual settings, respectively.\\n\\n5. Conclusion\\n\\nWe introduce a multilingual audio-visual corpus, MuA ViC, for 9 languages totaling 1200 hours of speech. It is the first open benchmark for audio-visual speech-to-text translation and the largest open benchmark for multilingual audio-visual speech recognition. The corpus is available at https://github.com/facebookresearch/muavic.\\n\\n6. References\\n\\n[1] W. H. Sumby and I. Pollack, \\\"Visual contribution to speech intelligibility in noise,\\\" Journal of the Acoustical Society of America, vol. 26, pp. 212\u2013215, 1954.\\n[2] T. Afouras, J. S. Chung, A. W. Senior, O. Vinyals, and A. Zisser\"}"}
{"id": "anwar23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"man, \u201cDeep audio-visual speech recognition,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, pp. 8717\u20138727, 2018.\\n\\n[3] B. Shi, W.-N. Hsu, and A. Mohamed, \u201cRobust Self-Supervised Audio-Visual Speech Recognition,\u201d in Proc. Interspeech 2022, 2022, pp. 2118\u20132122.\\n\\n[4] A. Bagher Zadeh, Y. Cao, S. Hessner, P. P. Liang, S. Poria, and L.-P. Morency, \u201cCMU-MOSEAS: A multimodal language dataset for Spanish, Portuguese, German and French,\u201d in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Nov. 2020, pp. 1801\u20131812.\\n\\n[5] B. Shi, W.-N. Hsu, K. Lakhotia, and A. Mohamed, \u201cLearning audio-visual speech representation by masked multimodal cluster prediction,\u201d in International Conference on Learning Representations, 2021.\\n\\n[6] A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. W. Wilson, A. Hasidim, W. T. Freeman, and M. Rubinstein, \u201cLooking to listen at the cocktail party,\u201d ACM Transactions on Graphics (TOG), vol. 37, pp. 1\u201311, 2018.\\n\\n[7] A. Fan, S. Bhosale, H. Schwenk, Z. Ma, A. El-Kishky, S. Goyal, M. Baines, O. Celebi, G. Wenzek, V. Chaudhary et al., \u201cBeyond english-centric multilingual machine translation,\u201d Journal of Machine Learning Research, vol. 22, pp. 1\u201348, 2021.\\n\\n[8] T. Afouras, J. S. Chung, and A. Zisserman, \u201cLrs3-ted: a large-scale dataset for visual speech recognition,\u201d arXiv preprint arXiv:1809.00496, 2018.\\n\\n[9] M. Cooke, J. Barker, S. P. Cunningham, and X. Shao, \u201cAn audio-visual corpus for speech perception and automatic speech recognition.\u201d The Journal of the Acoustical Society of America, vol. 120 5 Pt 1, pp. 2421\u20134, 2006.\\n\\n[10] J. S. Chung and A. Zisserman, \u201cLip reading in the wild,\u201d in Asian Conference on Computer Vision, 2016.\\n\\n[11] Y. Zhao, R. Xu, and M. Song, \u201cA cascade sequence-to-sequence model for chinese mandarin lip reading,\u201d Proceedings of the ACM Multimedia Asia, 2019.\\n\\n[12] D. Ivanko, A. Axyonov, D. Ryumin, A. Kashevnik, and A. Karpov, \u201cRUSA VIC corpus: Russian audio-visual speech in cars,\u201d in Proceedings of the Thirteenth Language Resources and Evaluation Conference, Jun. 2022, pp. 1555\u20131559.\\n\\n[13] J. S. Chung, A. Nagrani, and A. Zisserman, \u201cVoxceleb2: Deep speaker recognition,\u201d in Interspeech, 2018.\\n\\n[14] J. S. Chung, J. Huh, S. Mun, M. Lee, H.-S. Heo, S. Choe, C. Ham, S.-Y. Jung, B.-J. Lee, and I. Han, \u201cIn defence of metric learning for speaker recognition,\u201d ArXiv, vol. abs/2003.11982, 2020.\\n\\n[15] W.-N. Hsu and B. Shi, \u201cu-hubert: Unified mixed-modal speech pretraining and zero-shot transfer to unlabeled modality,\u201d in NeurIPS, 2022.\\n\\n[16] T. Afouras, J. S. Chung, and A. Zisserman, \u201cAsr is all you need: Cross-modal distillation for lip reading,\u201d ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2143\u20132147, 2019.\\n\\n[17] M. Post, G. Kumar, A. Lopez, D. Karakos, C. Callison-Burch, and S. Khudanpur, \u201cImproved speech-to-text translation with the fisher and callhome Spanish-English speech translation corpus,\u201d in Proceedings of the 10th International Workshop on Spoken Language Translation: Papers, Dec. 5-6 2013.\\n\\n[18] A. C. Kocabiyikoglu, L. Besacier, and O. Kraif, \u201cAugmenting librispeech with French translations: A multimodal corpus for direct speech translation evaluation,\u201d in Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), May 2018.\\n\\n[19] C. Wang, J. Pino, A. Wu, and J. Gu, \u201cCovost: A diverse multilingual speech-to-text translation corpus,\u201d in Proceedings of the Twelfth Language Resources and Evaluation Conference, May 2020, pp. 4197\u20134203.\\n\\n[20] C. Wang, A. Wu, J. Gu, and J. Pino, \u201cCovost 2 and Massively Multilingual Speech Translation,\u201d in Proc. Interspeech 2021, 2021, pp. 2247\u20132251.\\n\\n[21] M. A. Di Gangi, R. Cattoni, L. Bentivogli, M. Negri, and M. Turchi, \u201cMust-c: a multilingual speech translation corpus,\u201d in 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019, pp. 2012\u20132017.\\n\\n[22] J. Iranzo-Sanchez, J. A. Silvestre-Cerda, J. Jorge, N. Rosell\u00f3, A. Gim\u00e9nez, A. Sanchis, J. Civera, and A. Juan, \u201cEuroparl-st: A multilingual corpus for speech translation of parliamentary debates,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 8229\u20138233.\\n\\n[23] R. Cattoni, M. A. Di Gangi, L. Bentivogli, M. Negri, and M. Turchi, \u201cMust-c: A multilingual corpus for end-to-end speech translation,\u201d Computer Speech & Language, vol. 66, p. 101155, 2021.\\n\\n[24] E. Salesky, M. Wiesner, J. Bremerman, R. Cattoni, M. Negri, M. Turchi, D. W. Oard, and M. Post, \u201cMultilingual tedx corpus for speech recognition and translation,\u201d in Proceedings of Interspeech, 2021.\\n\\n[25] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d arXiv preprint arXiv:2212.04356, 2022.\\n\\n[26] N. Reimers and I. Gurevych, \u201cMaking monolingual sentence embeddings multilingual using knowledge distillation,\u201d in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, 11 2020.\\n\\n[27] J. S. Chung, A. Nagrani, and A. Zisserman, \u201cVoxceleb2: Deep speaker recognition,\u201d Proc. Interspeech 2018, pp. 1086\u20131090, 2018.\\n\\n[28] D. Snyder, G. Chen, and D. Povey, \u201cMusan: A music, speech, and noise corpus,\u201d arXiv preprint arXiv:1510.08484, 2015.\\n\\n[29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in neural information processing systems, vol. 30, 2017.\\n\\n[30] C. Wang, Y. Tang, X. Ma, A. Wu, D. Okhonko, and J. Pino, \u201cFairseq S2T: Fast speech-to-text modeling with fairseq,\u201d in Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations, Dec. 2020, pp. 33\u201339.\\n\\n[31] M. Post, \u201cA call for clarity in reporting BLEU scores,\u201d in Proceedings of the Third Conference on Machine Translation: Research Papers, Oct. 2018, pp. 186\u2013191.\\n\\n[32] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \u201cBleu: a method for automatic evaluation of machine translation,\u201d in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311\u2013318.\"}"}
