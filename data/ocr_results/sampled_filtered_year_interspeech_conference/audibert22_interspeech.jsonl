{"id": "audibert22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Intra-speaker phonetic variation in read speech: comparison with inter-speaker variability in a controlled population\\n\\nNicolas Audibert, C\u00e9cile Fougeron\\n\\nLaboratoire de Phon\u00e9tique et Phonologie, UMR7018 CNRS/University Sorbonne-Nouvelle, Paris, France\\n\\n{nicoles.audibert; cecile.fougeron}@sorbonne-nouvelle.fr\\n\\nAbstract\\n\\nOur knowledge of speech is historically built on data averaged across speakers or comparing different speakers. We therefore know little about the variability of speech produced by the same speaker: to what extent does it vary from one repetition to another and in what dimensions? In this study, we document the stability of speech and voice characteristics in 9 French speakers, on the reading of two texts recorded over ten sessions on different days over a two-month period. 21 features related to temporal, spectral, f0 and harmonicity aspects as well as their modulation between consecutive chunks are studied. The stability of these features between sessions is evaluated in comparison with their variability between speakers. Results show that short-term variability of energy in the 0-1kHz band, mean F0 and the slope of the LTAS vary the most between sessions for a given speaker, and are also among the speech and voice features that vary the most between speakers in this small cohort, while modulation features between consecutive chunks remain more stable across sessions.\\n\\nIndex Terms: intra-speaker variability, French, voice and speech dimensions, cross-session stability\\n\\n1. Introduction\\n\\nOur knowledge of speech has historically been built on data comparing different speakers or on data averaged across speakers. Consequently, little is known about the variability of speech within an individual. Yet we know that speakers adapt to linguistic and speech contexts, and modify their speech according to their emotional or biological state, for example. However, it is not clear to what extent speakers vary from one recording session to another, on which speech features, and to what extent recordings collected days, months, or years apart are \u201cnaturally\u201d variable.\\n\\nWhile variability in speech is ubiquitous, it is also, at least in part, structured and governed by rules. Therefore, documenting variation in speech and explaining its origins is at the heart of phonetics research. Two types of variability can be distinguished according to their origins and characteristics: inter-speaker variability and intra-speaker variability [1].\\n\\nOn the one hand, inter-speaker variability is rooted in speaker-specific physiological or anatomical traits, as well as demographic, regional or social factors [2]. This inter-speaker variability carries indexical information about the speaker: this indexicality is disseminated over a cluster of speech features and it is usually this cluster that discriminates one speaker (or group of speakers) from another. Moreover, while speech conditions have been considered for a long time by researchers in voice identification and forensic phonetics as a factor of intra-speaker variability that may impair the ability to recognize a speaker's voice (see e.g. [3], or Brunner [4] for a review), the influence of speech conditions on inter-speaker variability remains understudied.\\n\\nOn the other hand, intra-speaker variability comes from a variety of sources (see e.g., [5] for a review). Among these, changes in phonetic realization of phonemes, due to phonetic or prosodic contexts, have been the most documented and this intra-speaker variability is easier to model. This is not the case for other sources of intra-speaker variability which are due to the fact that individuals adapt their speech to the speech context, to its formality, to the social relationship they have with their interlocutors, to the specific needs of the listeners (in a noisy context for example, or if they share common knowledge on the subject or not), etc. These different factors, often grouped under the notion of \u201cstyle effects\u201d, mainly influence the degree of articulatory precision (or hyper/hypo articulation) and the speech rate adopted by the speaker. A third type of factor affecting the way a speaker speaks is related to the speaker's affective and cognitive state, related for instance to emotion, attention or fatigue (see among others [6], [7], [8]).\\n\\nAlthough recognized and documented in some way, changes in speech resulting from speech context or speaker state are difficult to delineate and manipulate experimentally (see for example the work on \u201cclear speech\u201d by Scarborough & Zellou [9]). Moreover, our knowledge of speaker-internal variation is further complicated by the fact that all of these effects are not static but change dynamically over the course of a utterance [10]. Indeed, environmental conditions, speech content, prosodic properties affecting pronunciation, as well as fatigue, emotion, arousal, or attention vary from moment to moment.\\n\\nTherefore, a common and repeated belief in language science is that a unit of speech will never be pronounced the same way twice by the same individual. However, little is known about how much speech diverges from one iteration to the next, in what aspects, how much granularity is needed to measure these variations, or what factors contribute to the variability from one iteration to the next. Heald & Nusbaum [11], for example, explored variability in isolated vowel production for eight speakers across 9 repetitions collected either on the same day at three different times or on different days. They found greater variability in vowel acoustics between different sessions on the same day than between days, and proposed a broad set of possible causes for this phenomenon.\\n\\nIn forensics, where an individual's recordings must be compared, these issues seem crucial. Indeed, as unlined by Rose [12], speech samples compared in forensic applications are never contemporaneous. Results obtained by Rhodes [13] show that performance of voice identification by an automatic system\"}"}
{"id": "audibert22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"decreases with longer delays between recordings of a same speaker on controlled material (7 to 28 years in his dataset), in line with changes in formant frequencies, though with variations between speakers. However, to the best of our knowledge, we have no scientific criteria concerning the validity of a comparison between recordings spaced only days, weeks, or months apart and on the conditions allowing such comparisons. As part of a larger research project on intra- and inter-individual variability in databases collected at the LPP on the same speakers with different inter-session delays (PATATRA and PATAFreq databases), the study presented here focuses on the stability of speech features read over sessions spaced closely together in time. In order to make sense of this inter-session variability, it is compared to the variability of these speech features between the different speakers.\\n\\n2. Methods\\n\\n2.1. Speech material\\n\\nThe speech data used for this study come from the PATAFreq recording database which was collected during the first lockdown due to the Covid-19 pandemic in 2020 by 9 speakers, including 6 women, all native French speakers, close to the speech research community, and participating in the PATATRA longitudinal recording database coordinated by the LPP lab. These 9 speakers recorded themselves regularly, with a goal of about 10 recordings over about a month and a half. Five speakers reached 10 sessions, 3 speakers reached 8 sessions, and one speaker who encountered technical problems could only complete 6 sessions. Speakers were asked to record regularly, with a minimum of 24 hours between sessions and over a maximum period of two months, and were encouraged to change the time of day, but family constraints during lockdown did not always allow this. All speakers were equipped with sound cards and head-mounted professional microphones and were required to record in a quiet environment, always in the same room if possible, and with the same settings (incl. distance mouth-microphone). Any unexpected changes or environmental noise had to be reported in a questionnaire to be filled in after each recording, with a self-assessment on a 4-level scale of fatigue, emotional state, vocal fatigue and frequency of voice use on the day of the recording. Seven of the nine speakers drove their recordings using a dedicated application developed in Python (a modified version of MonPaGe, [14]) that managed the presentation of instructions, recording, and organization of sound files. Two speakers were unable to use this application and made their recordings with Praat [22], relying on a version of the instructions displayed as a slideshow.\\n\\nAmong the 7 tasks of the PATAFreq protocol, we study here only the task of reading two texts (text 1 and 2) which were manually segmented into chunks, so as to obtain about ten syllables per chunk. In addition to the segmentation into chunks, all the perceived silent pauses were delimited, the pauses between two chunks being by convention attached to the first of the two.\\n\\n2.2. Acoustic features\\n\\nThe 11 acoustic measures summarized in Table 1 were extracted with Praat [22] on each of the previously segmented chunks, using a custom script. With the exception of the measure of intra-chunk variability of the harmonic-to-noise ratio (\\\\(sd(HNR)\\\\)), these measures were also derived to obtain a measure of modulation between consecutive chunks, denoted as \u201cd(X)\u201d (for instance the variable d(articRate) corresponds to the chunk-to-chunk modulation of the articulatory rate), and normalized as follows:\\n\\n\\\\[\\n\\\\frac{c_{h_{\\\\text{unk}}}(i) - c_{h_{\\\\text{unk}}}(i-1)}{c_{h_{\\\\text{unk}}}(i) + c_{h_{\\\\text{unk}}}(i-1)} \\\\times \\\\frac{2}{2}\\n\\\\]\\n\\nAs a result, we get a total of 21 descriptors on each of the chunks (except for the last chunk of the texts). Table 1: Definition of the 11 features computed on each chunk. This set is complemented by normalized chunk-to-chunk modulation measures d(X) for all features except meanHNR to get the final set of 21 features.\\n\\n| Dimension | Nom | Description |\\n|-----------|-----|-------------|\\n| Time      | articRate | Speech rate, pauses excluded (phones/sec). |\\n|          | pauseTime | Cumulated time of silent pauses (seconds) |\\n| Pitch     | meanF0 | Mean F0 level (Hz) |\\n|          | F0 range | F0 spread in the 10%-90% percentile range (semitones) |\\n| Harmonicity | meanHNR | Mean harmonics to noise ratio (dB) |\\n|          | sd(HNR) | Intra-chunk variation of HNR (dB) |\\n| Spectrum/ | LTASslope | Slope between 0-1kHz and 1-4kHz frequency bands of the long-term average spectrum on voiced parts (dB) |\\n| Voice     | sd(energy0-1kHz) | Variability of intensity (dB) on 0-1kHz frequency band (articulation of segments) |\\n|          | sd(energy1-2.5kHz) | |\\n|          | sd(energy2.5-4kHz) | |\\n|          | sd(energy4-8kHz) | |\\n\\n2.3. Analyses\\n\\nThe relative importance of the 21 measures selected for distinguishing between sessions produced by the same speaker was evaluated using a variable selection procedure based on random forest classification and implemented in the R package Boruta [15]. In this procedure, the evaluation of the importance of each variable is based on the estimation of the loss of precision in the classification task induced by the random permutation of values between elements to be classified. Each variable is attributed an importance score allowing to prioritize them and eventually to discard the variables not resulting in a better classification than the random reference. In addition to classical random forest methods, the general principle of the Boruta algorithm is to use shadow variables obtained by applying random permutations to each predictor as baseline, the shadow variable with the highest importance score being retained as a reference. Importance scores are determined over multiple runs, resulting in a distribution of importance score values for each variable. The efficiency of the Boruta algorithm was assessed by [16], and this method has already been applied to phonetic data by [17].\"}"}
{"id": "audibert22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In our case, this procedure was applied separately to the data of each speaker for the classification in sessions, and on the whole dataset for the classification in speakers, over 100 runs. We do not consider here the classification in texts. In order to allow a direct comparison between classification tasks involving a non-homogeneous number of classes and observations, we use as a metric the rank of each variable after sorting them by importance score.\\n\\n3. Results\\n\\nFigure 1 presents the descriptors ordered by rank according to their importance for the separation into sessions for each speaker, and for the separation into speakers. The distributions of the descriptors discussed below are illustrated in Figure 2, for each consecutive session and each speaker.\\n\\nAmong the descriptors that stand out among the 5 variables estimated to be most important by the Boruta classification for separation into sessions (at the top of Figure 1 and Figure 2) we find: meanF0 for all speakers except F05 (for whom it appears at rank 7), sd(energy(0-1kHz) for all except F02 (at rank 7), LTASslope f for all except F04 and H02 (at rank 6 and 7 respectively) and meanHNR for all except F05 (at rank 7) and H01 (at rank 12). All these descriptors which characterize low frequency aspects, supposedly related to the voice, are therefore quite unstable from one session to another and/or within the same session (i.e. between chunks). The comparison of the weights of these descriptors with those estimated to be the most important for the classification into speakers (Figure 1) reveals the variables that were found to be among the most important for the separation into sessions are also those that differentiate our 9 speakers the most, with a rather high Spearman correlation between the rank for the separation in speakers and the mean rank for the separation in sessions for each speaker (\u03c1 = .81).\\n\\nIndeed, meanF0, LTASslope and sd(energy(0-1kHz)) are among the top-5 descriptors useful for speaker separation. These descriptors distinguish between sessions of the same speaker but also between speakers. For instance, we can see on Figure 2 that according to the session, the average F0 of speaker F02 is either closer to that of speaker F05 (sessions 3-6-9-10), or closer to that of F03 on other sessions.\\n\\nOn the selection of descriptors presented in Figure 2, it is also clear that some speakers are quite variable from one session to another while others are remarkably stable, like F03 and H01. As also illustrated in Figure 2, the stability/variability of the articulatory rate between sessions depends on the speaker: this descriptor ranks in the top-10 variables for session separation for F01, F03, F04, F05 and H01, while it ranks much lower for F02, F07, H02 and H03. Among these 4 speakers, the patterns are different: for F02 and H02 the articulatory rate is particularly stable from one session to another and also shows a restricted range of variation within the same session (between chunks). F07 and H03, on the other hand, show much more variation in their productions within the same session and between sessions. These individual characteristics of articulatory rate explain why this descriptor reaches the top 5 of speaker separation on the cohort.\\n\\nFrequency fluctuations in the 4-8kHz band (sd(energy(4-8kHz)), also shown in Figure 2, are as expected among the top 5 descriptors for speaker separation. However, we also notice that they are also variable between sessions and present a significant range of variation within sessions: for all speakers except F03, this descriptor thus appears in the top-10 of the separation into sessions. Its variation between chunks within the same session suggests that in this frequency band, we capture more variations related to the segmental content than expected.\\n\\nChunk-to-chunk modulation of features along the text reading (d(X)) have little weight in the separation between sessions of the same speaker (variables listed at the bottom in Figure 1 and Figure 2). Among these, the chunk-to-chunk modulation of the energy fluctuation in the mid-frequencies (d(sd(energy(1-2.5kHz)) or d(sd(energy(2.5-4kHz)) depending on the speakers) are found among the 5 most stable measures between sessions for all speakers. If we consider that on these frequency bands, we capture spectral variations related to the dynamics of the articulation of sounds, we can interpret this result as reflecting the stability of this articulatory chunk-to-chunk dynamics. The chunk-to-chunk modulation in high frequencies (4-8kHz) also appears among the most stable descriptors for 7 speakers out of 9. The HNR modulation (d(meanHNR), not shown here) is also stable for all speakers except F05 and F07, and could reflect a higher variability of voice quality along the reading, possibly linked to vocal fatigue.\\n\\nHowever, this interpretation remains to be evaluated through measures of the evolution over time in the same session.\"}"}
{"id": "audibert22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1 speaker, 300 repetitions of monosyllabic words; Tielsen.\\n\\nWe focused on the observation of intra-speaker variability in a restricted cohort of 9 speakers over a relatively large number of sessions, and on a read speech content allowing the characterization of productions on several dimensions using 21 acoustic descriptors. To make sense of this variability, it was compared to the variability that can be observed between two readings of different linguistic material or between speakers of the cohort.\\n\\nThe similarity observed between the descriptors showing the most variation between sessions and those varying the most between speakers raises serious questions about the influence of the recording session chosen in speaker separation tasks, even when the time-lapse between consecutive sessions is limited to days or weeks.\\n\\nAnother avenue for future research is related to the behavior of descriptors capturing articulatory dynamics in frequency bands (sd(energy) measures and their modulation). Their stability from one session to the next and within the same reading suggests the interest of further investigating these characteristics of the \u201cway\u201d of articulating and co-articulating which seems to be specific to the speaker. Our next studies on this corpus will therefore investigate the variability of temporal coordination measures between sounds produced successively through measures of coarticulation and undershoot of acoustic targets, in read speech but also on the semi-spontaneous recordings that we have also recorded for the same speakers.\\n\\n5. Acknowledgements\\n\\nThis work has been supported by the \u201cInvestissements d'Avenir\u201d program ANR-10-LABX-0083 (Labex EFL) and the VoxCrim project (ANR-17-CE39-0016).\"}"}
{"id": "audibert22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] R. Wright. \\\"Intra-speaker variation and units in human speech perception and ASR.\\\" Speech Recognition and Intrinsic Variation Workshop, 2006.\\n\\n[2] P. Foulkes, and G. Docherty. \\\"The social life of phonetics and phonology.\\\" Journal of phonetics, vol. 34, no. 4, pp. 409-438, 2006.\\n\\n[3] R. H. Bahr, and K. J. Pass. \\\"The influence of style-shifting on voice identification.\\\" International Journal of Speech, Language and the Law, vol. 3, no. 1, pp. 25-38, 1996.\\n\\n[4] E. G. Brunner. \\\"The study of variation from two perspectives.\\\" Language and Linguistics Compass, vol. 3, no. 3, pp. 734-750, 2009.\\n\\n[5] A. B\u00fcrki. \\\"Variation in the speech signal as a window into the cognitive architecture of language production.\\\" Psychonomic Bulletin & Review, vol. 25, pp. 1973\u20132004, 2018.\\n\\n[6] M. P. Gelfer. \\\"Effects of prolonged loud reading on selected measures of vocal function in trained and untrained singers.\\\" Journal of Voice, vol. 5, pp. 15\u2013167, 1991.\\n\\n[7] J. Barrett, and T. Paus. \\\"Affect-induced changes in speech production.\\\" Experimental Brain Research, vol. 146, no. 4, pp. 531\u2013537, 2002.\\n\\n[8] T. Johnstone, and K. R. Scherer. \\\"Vocal communication of emotion.\\\" Handbook of emotions, vol. 2, pp. 220-235, 2000.\\n\\n[9] R. Scarborough, and G. Zellou. \\\"Clarity in communication: 'Clear' speech authenticity and lexical neighborhood density effects in speech production and perception.\\\" Journal of the Acoustical Society of America, vol. 134, no. 5, pp. 3793\u20133807, 2013.\\n\\n[10] R. A. Bates. Speaker dynamics as a source of pronunciation variability. University of Washington, 2004.\\n\\n[11] S. L. M. Heald, and H. C. Nusbaum. \\\"Variability in Vowel Production within and between Days.\\\" PLoS ONE, vol. 10, no. 9, 2015.\\n\\n[12] P. Rose. Forensic speaker identification. CRC Press, 2002.\\n\\n[13] R. Rhodes. \\\"Aging effects on voice features used in forensic speaker comparison,\\\" International Journal of Speech, Language & the Law, vol. 24, no. 2, pp. 177\u2013199, 2017.\\n\\n[14] R. Trouville, V. Delvaux, C. Fougeron, and M. Laganaro. Logiciel d'\u00e9valuation de la parole (version screening) MonPaGe-2.0.s [Computer program] Retrieved on 05/01/2022 from https://lpp.in2p3.fr/monpage/.\\n\\n[15] M. B. Kursa, W. R. Rudnicki. \\\"Feature selection with the Boruta package.\\\" Journal of Statistical Software, vol. 36, no. 11, pp. 1-13, 2010.\\n\\n[16] F. Degenhardt, S. Seifert, and S. Szymczak. \\\"Evaluation of variable selection methods for random forests and omics data sets.\\\" Briefings in bioinformatics, vol. 20, no. 2, pp. 492\u2013503, 2019.\\n\\n[17] E. Klein, J. Brunner, and P. Hoole. \\\"The relevance of auditory feedback for consonant production: The case of fricatives.\\\" Journal of Phonetics, vol. 77, pp. 100931, 2019.\\n\\n[18] P. A. Keating, J. Kreiman, and A. Alwan. \\\"A new speech database for within- and between-speaker variability,\\\" Proceedings of the ICPhS XIX, 2019.\\n\\n[19] Y. Lee, Keating, P., Kreiman J. \\\"Acoustic voice variation within and between speakers,\\\" Journal of the Acoustical Society of America, vol. 146, no. 3, pp. 1568\u20131579, 2019.\\n\\n[20] D. H. Whalen, W. R. Chen, M. K. Tiede, and H. Nam. \\\"Variability of articulator positions and formants across nine English vowels.\\\" Journal of Phonetics, vol. 68, pp. 1\u201314, 2018.\\n\\n[21] S. Tielsen. \\\"Exertive modulation of speech and articulatory phasing.\\\" Journal of Phonetics, vol. 64, pp. 34\u201350, 2017.\\n\\n[22] Boersma, Paul, and David Weenink. \\\"Praat: Doing phonetics by computer (Version 6.0.37),\\\" Computer Program. Available online: http://www.fon.hum.uva.nl/praat, 2018.\"}"}
