{"id": "borsdorf22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Blind Language Separation: Disentangling Multilingual Cocktail Party Voices by Language\\n\\nMarvin Borsdorf 1, Kevin Scheck 2, Haizhou Li 3, Tanja Schultz 2\\n\\n1 Machine Listening Lab (MLL), University of Bremen, Germany\\n2 Cognitive Systems Lab (CSL), University of Bremen, Germany\\n3 The Chinese University of Hong Kong, Shenzhen, China\\n\\nmarvin.borsdorf@uni-bremen.de, scheck@uni-bremen.de\\n\\nAbstract\\n\\nWe introduce blind language separation (BLS) as novel research task, in which we seek to disentangle overlapping voices of multiple languages by language. BLS is expected to separate seen as well as unseen languages, which is different from the target language extraction task that works for one seen target language at a time. To develop a BLS model, we simulate a multilingual cocktail party database, of which each scene consists of two randomly selected languages, each represented by two randomly selected speakers. The database follows the recently proposed GlobalPhoneMCP database design concept that uses the audio data of the GlobalPhone 2000 Speaker Package. We show that a BLS model is able to learn the language characteristics so as to disentangle overlapping voices by language. We achieve a mean SI-SDR improvement of 12.63 dB over 231 test sets. The performance on the individual test sets varies depending on the language combination. Finally, we show that BLS can generalize well to unseen speakers and languages in the mixture.\\n\\nIndex Terms: Blind language separation, selective auditory attention, multilingual, GlobalPhone, cocktail party problem\\n\\n1. Introduction\\n\\nSelective auditory attention, also referred to as the cocktail party problem [1], describes the human ability to disentangle overlapping voices or sounds into the individual sources for arbitrary soundscapes. Speech separation represents the research field that attempts to equip machines with such an ability. The study of speech separation seeks to separate the individual voices in a speech mixture, that is called blind source separation [2\u201311] or target speaker extraction [12\u201320].\\n\\nIn blind source separation, a speech separation model disentangles all voices in a given overlapping speech mixture in one step. This usually requires the number of individual voices to be known in advance, because such systems are tied to a fixed number of voices or, more precisely, output channels. In real-world everyday conversational situations, this information is not always accessible and also may vary. In order to address this constraint, the research in this field proposed to estimate a maximum number of sources [5, 21, 22], to convert the reconstructed sources into a sequential output stream [23], or to apply recursive or iterative approaches [24, 25]. Blind source separation faces the permutation problem which is caused by the ambiguous assignment of sources to the output channels. The permutation invariant training (PIT) [4, 5] has been introduced as a method to solve this problem.\\n\\nIn target speaker extraction, a single speaker's voice is isolated from the remaining sources of a given cocktail party mixture. To achieve this, the extraction model utilizes a reference signal of the desired speaker, that is typically given as additional speech signal. Therefore, target speaker extraction is not restricted by the outlined limitations.\\n\\nThe required databases to develop such models usually provide data tuples consisting of an input mixture of overlapped speech, the corresponding isolated clean sources, and, for target speaker extraction, an additional reference signal. While speech separation has been widely studied on monolingual databases in English language, e.g., on WSJ0-2Mix [2], WSJ0-3Mix [2], LibriCSS [26], WHAM! [27], WHAMR! [28], and LibriMix [29], recently multilingual studies have been performed, as in Applants et al. [30] and on GlobalPhoneMS2 [31].\\n\\nRecently, target language extraction (TLE) [32] has been introduced as novel task that considers the cocktail party problem as multilingual scene. In TLE, a model isolates the voices of all speakers who talk in the target language from the remaining voices at once, independent of the interfering language and the number of speakers in the mixture. For this purpose, GlobalPhoneMCP [32] has been introduced as a new corpus design concept to simulate multilingual cocktail party mixtures of two and four speakers. While previous techniques on blind source separation and target speaker extraction, except for X-TaSNet [17], treated the output and ground truth data as single sources, TLE extracts mixture signals comprising one or two speakers. Since a TLE model is tied to a fixed target language by design, multiple TLE models are required to extract voices of different languages. To remove this limitation, multi-target language extraction (Multi-TLE) [33] models have been developed which can extract the voices for different target languages, based on an additional conditioning input that serves as language switch. However, both TLE and Multi-TLE models can only extract voices for a single target language at a time.\\n\\nIn this work, we introduce a novel task called blind language separation (BLS) that builds the bridge between blind source separation and TLE. BLS seeks to disentangle multilingual cocktail party voices into speaker groups by language in a single step. In comparison to TLE, a BLS model is not conditioned on the to-be-extracted target language, but performs the separation task using the languages' characteristics that are given in the mixture. Consequently, a BLS model is also able to disentangle voices by language, even if the languages were unseen during training. Furthermore, a single BLS model can replace several applications of TLE or Multi-TLE on the same multilingual speech mixture in order to output different monolingual mixtures. This allows to reduce the execution time when compared to subsequent TLE applications or the memory footprint in comparison to a parallel TLE processing. We establish baselines for the novel BLS task by training seven different models that are based on the Conv-TasNet [8] and...\"}"}
{"id": "borsdorf22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SepFormer architectures. We investigate different training language combinations and evaluate all models on 231 different test sets in total. For training the models, we adapt the recent dynamic language mixing algorithm [33] and create training data consisting of multilingual speech mixtures and language-specific speech mixtures on-the-fly.\\n\\nThe rest of the paper is organized as follows. In Section 2, we introduce the BLS task and provide details on the design. In Section 3, we describe the experimental setup, followed by Section 4, in which we show and discuss the results. In Section 5, we conclude our study and talk about our future work. We will make all information and scripts publicly available.\\n\\n2. Blind Language Separation\\n\\nThe recent results on TLE and Multi-TLE show that it is possible to equip neural network based models with the capability to extract voices of a specific target language from a multilingual cocktail party mixture [32, 33]. More precisely, the mechanism uses language characteristics to extract all target voices, independent of the interfering language and the number of speakers in the mixture, at once (see Figure 1 - bottom). This represents a novel approach to work on the solution of the cocktail party problem. However, TLE and Multi-TLE can only extract the voices for one target language at a time. In blind source separation (BSS), speaker characteristics are used to disentangle all overlapping speakers of a mixture into the individual sound sources in one step (see Figure 1 - top). With the introduction of blind language separation (BLS), we build the bridge between both aforementioned mechanisms to disentangle the overlapping voices of a multilingual cocktail party into the contributing speaker groups by language in a single step. The groups are given as monolingual voice mixtures (see Figure 1 - middle).\\n\\nIt has been shown that TLE and Multi-TLE models can be developed by applying a supervised training scheme with appropriate data tuples of a well constructed database. We extend this method with BSS-specific training mechanisms to realize a BLS development setup. In this setup, a BLS model processes a multilingual cocktail party input mixture and estimates a mask for each contributing, intra-language speaker group. Those masks are used to filter out the respective speaker groups' voices which are given in the corresponding language. Similar to BSS, the model outputs the target signals on different output channels in parallel and thus requires the number of languages in the mixture to be known in advance. As the design concepts of BSS and, consequently, BLS face the permutation problem, we apply PIT that allows to find the best permutation of separated sources (for BSS) and mixture signals (for BLS).\\n\\nIt is worth noting that if each language in the mixture would be represented by only one speaker, the BLS task would be equal to a BSS task as each output channel would comprise a single speaker's voice. Since this would be contrary to our training concept, we constrain the BLS task to mixtures that contain at least one language that is represented by at least two speakers.\\n\\n3. Experimental Setup\\n\\nWe perform experiments based on two different model architectures to study BLS. We evaluate the models under conditions that comprise unseen speakers as well as seen and unseen languages. Additionally, we investigate mixed conditions, i.e. the mixture comprises seen and unseen languages.\\n\\n3.1. Data setup\\n\\nWe describe the data source, the language mixing strategy, and the data design for our experiments.\\n\\nCorpus\\n\\nThe data in our study is based on the multilingual GlobalPhone corpus that has been proposed by Schultz [34] and Schultz et al. [35]. More precisely, we utilize the speech data of a subset, called GlobalPhone 2000 Speaker Package, which is distributed by the European Language Resources Association (ELRA) [36] under research and commercial licenses. The subset covers 22 languages with 2,000 native speakers, each one represented with roughly 40 seconds of speech. The included languages are: Arabic (AR), Bulgarian (BG), Chinese Mandarin (CH), Chinese Shanghai (WU), Croatian (CR), Czech (CZ), French (FR), German (GE), Hausa (HA), Japanese (JA), Korean (KO), Polish (PL), Portuguese (PO), Russian (RU), Spanish (SP), Swahili (SA), Swedish (SW), Tamil (TA), Thai (TH), Turkish (TU), Ukrainian (UA), and Vietnamese (VN). To split the data into training, validation, and test sets, and to create the multilingual mixtures, we apply the GlobalPhoneMCP corpus design concept [32]. The training and validation sets share the same speakers, but with different utterances. The test set speakers are disjoint from those sets to allow an evaluation under unseen speaker conditions. We reduce the computational costs by downsampling the audio data to 8 kHz.\\n\\nDynamic language mixing procedure\\n\\nTo simulate the multilingual cocktail party audio mixtures, we adapt the dynamic language mixing procedure [33] as follows (see Figure 2). First, we sample \\\\( L \\\\) languages from a fixed set of languages. Second, for each language, we sample a random number of speakers. Third, for each speaker, we select a random utterance. Fourth, we sample signal-to-noise ratios (SNRs) with zero mean and, under consideration of the respective active speech levels, we create intra-language mixtures for each language. It is worth noting that we do not normalize the signals based on the maximum amplitude in this step. The intra-language mixtures represent the ground truth data for PIT.\\n\\nFifth, we sample SNR values to simulate the overall mixture that is given as a weighted sum of all intra-language mixtures. The final overall mixture represents the multilingual cocktail party mixture, which is also used as the input to the model. The intra-language mixtures and the overall mixture are normalized based on the maximum amplitude. When creating the intra-language and overall mixtures, we cut the longer signal to the length of the shorter one, which is commonly referred to as min version. Before performing the adapted dynamic language mixing procedure, we apply PIT that allows to find the best permutation of separated sources (for BSS) and mixture signals (for BLS).\\n\\nIt is worth noting that if each language in the mixture would be represented by only one speaker, the BLS task would be equal to a BSS task as each output channel would comprise a single speaker's voice. Since this would be contrary to our training concept, we constrain the BLS task to mixtures that contain at least one language that is represented by at least two speakers.\"}"}
{"id": "borsdorf22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Illustration of the adapted dynamic language mixing procedure. For each of the \\\\( L \\\\) sampled languages, utterances of different speakers are sampled to mix the intra-language mixtures and, subsequently, the overall mixture. The adapted dynamic language mixing for BLS can be used to create mixtures with an arbitrary number of languages and speakers. However, as this work represents the first attempt at BLS, we study conditions with a fixed number of languages and speakers. We set the number of languages and the number of speakers per language to two, leading to a total of four speakers in the mixture. With these settings, we fulfil the requirement of having at least one language that comprises more than one speaker (see Section 2). During training and validation, we apply the adapted dynamic language mixing method to create data tuples of input mixture and ground truth data on-the-fly. The SNRs to simulate the intra-language and overall mixtures are sampled from a uniform distribution in a range of \\\\[0, 5\\\\] dB.\\n\\nTest data\\nFor creating the test data, we use a static version of the adapted dynamic language mixing routine, which guarantees comparability between different studies. To simulate the data, we perform the following steps. First, we create 360 intra-language speaker pairs for each language (following the test set size in [32]) and balance the repetitions of each utterance as far as possible. Second, we randomly mix the intra-language speaker pairs of two languages to create overall mixtures. We repeat this for all unique language combinations of the 22 languages in the GlobalPhone 2000 Speaker Package, without same-language pairs. This leads to 231 test sets with 83,160 utterances in total. We make sure to avoid duplicates in combinations for intra-language and overall mixtures. We apply the same steps to sample SNRs and normalize the data as for training and validation. Since each language is represented with a different number of utterances in the GlobalPhone 2000 Speaker Package, this creates an imbalance in the contribution of each individual utterance in the intra-language and overall mixtures. For example, single utterances of WU (16 speakers and 80 utterances) may influence the results more than utterances of HA (23 speakers and 179 utterances). However, we favor this approach to the alternative of using a fixed number of utterances per language, such as using only 80 utterances, because this would discard a substantial amount of test data.\\n\\n3.2. Network architectures\\nIn our study, we train BLS models based on the Conv-TasNet [8] and SepFormer [11] time-domain architectures. The architectures estimate a mask for each output channel, in which every channel corresponds to an intra-language mixture in the overall input mixture. For the Conv-TasNet model, we set the hyper-parameters according to the best-performing model in Luo and Mesgarani [8], except that we change the mask activation to rectified linear unit (ReLU). The implementation is done with help of the Asteroid [38] toolkit. In total, the model comprises 5.05 million trainable parameters. For the SepFormer model, we adopt the best-performing settings in Subakan et al. [11], resulting in 25.68 million trainable parameters, and use the SpeechBrain [39] toolkit implementation.\\n\\n3.3. Training and evaluation\\nIn our experiments, we use five training sets, each defined by a different composition of languages. The sets comprise two, three, five, ten, and eighteen randomly chosen languages, denoted as L-2, L-3, L-5, L-10, and L-18 respectively. The sets are subsets of each other, i.e. larger sets include the languages of smaller sets. The languages CH, CR, JA, and PL are only part of the test data, such that we can evaluate the models under seen, unseen, and mixed conditions. The speakers in the test data are always unseen.\\n\\nWe train five Conv-TasNet models, each on one training set, to establish first results. We further train two instances of SepFormer on ten and eighteen languages respectively, to investigate whether BLS models with a larger number of parameters can benefit from more languages in the training set. For training, we provide data tuples comprising overall input mixture and intra-language mixtures and apply a supervised training scheme using PIT. We use Adam [40] as optimizer with initial learning rates of \\\\(1 \\\\times 10^{-3}\\\\) and \\\\(1.5 \\\\times 10^{-4}\\\\) for Conv-TasNet and SepFormer respectively. The gradient clipping is set to five and the weight decay to zero for both architectures. We train the Conv-TasNet on batches with a size of forty while the SepFormer is trained with a batch size of two.\\n\\nWe use the negative scale-invariant signal-to-distortion ratio (SI-SDR) [41] as training objective and validation loss. The positive SI-SDR improvement (dB) is used to evaluate the model's performance during testing. To determine the latter, we calculate the difference between the SI-SDR of the separated signals (using the permutation that yields the maximum mean value) and the SI-SDR of the input mixture, both with respect to the ground truth signals. In order to monitor and control the training of our models, we utilize (a) a learning rate scheduler that halves the current learning rate if the validation loss does not decrease within three epochs and (b) an early stopping algorithm which stops the training if there is no improvement in the validation loss within six subsequent epochs. Each model is trained from scratch on the respective training set and the maximum number of training epochs is 200.\\n\\nDuring training and validation, we use the adapted dynamic language mixing method to continuously create 20,000 and 10,000 random data tuples per epoch. We split the data into chunks of four seconds. While samples with a duration between two and four seconds are padded with zeros, samples with a duration of less than two seconds are neglected. The evaluation is performed on the 231 pre-simulated test sets and the samples are processed on their entire length at once.\\n\\n4. Results and Discussion\\nWe report the blind language separation results for seven models in terms of average SI-SDR improvement (dB) over test samples of different language combinations. We illustrate the mean results over all 231 test sets (Mean 231), as well as some individual results. For the mixed conditions, we report the mean performances over GE-\\\\{CH, CR, JA, PL\\\\} and PO-\\\\{CH, CR, JA, PL\\\\} as GE mix and PO mix respectively (see Table 1).\"}"}
{"id": "borsdorf22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Blind language separation results for models trained on different language sets reported in terms of average SI-SDR improvement (dB) over all test samples of specific language combinations. With \\\\( \\\\text{Mean}_{231} \\\\) we report the mean value over all 231 test sets. While \\\\( \\\\text{seen} \\\\) provides the results for the language combination GE-PO that is part of all training sets, \\\\( \\\\text{unseen} \\\\) shows the results for mixtures in which both languages have not been part of the training. Mixed (mean) denotes the mean performance under mixed conditions, i.e. only one of the languages in the mixture is seen during training. More precisely, we report the mean performances over GE-\\\\{CH, CR, JA, PL\\\\} and PO-\\\\{CH, CR, JA, PL\\\\} as GE\\\\text{mix} and PO\\\\text{mix} respectively. The test set speakers are always unseen.\\n\\n| Training set | BLS model | GE-PO | Mixed (mean) | Unseen |\\n|--------------|------------|-------|--------------|--------|\\n|              | L-2 Conv-TasNet | 1.28  | 12.69        | 4.79   | 2.43   | -3.42 | 10.87 | 5.56 | 10.61 | 5.67 | -1.71 |\\n|              | L-3 Conv-TasNet | 2.11  | 12.00        | 4.30   | 4.10   | 0.02  | 10.38 | 6.85 | 9.91  | 6.58 | -2.19 |\\n|              | L-5 Conv-TasNet | 4.99  | 11.46        | 5.91   | 4.71   | -0.04 | 10.29 | 6.49 | 10.00 | 8.01 | 2.94  |\\n|              | L-10 Conv-TasNet | 6.85 | 11.28        | 6.54   | 5.77   | 4.61  | 10.64 | 6.16 | 10.65 | 7.59 | 3.26  |\\n|              | L-10 SepFormer  | 11.00 | 15.16        | 8.64   | 9.69   | 6.30  | 14.53 | 12.66 | 14.42 | 13.22 | 3.78  |\\n|              | L-18 Conv-TasNet | 7.57  | 10.77        | 6.37   | 5.68   | 5.15  | 9.93  | 5.88 | 10.19 | 7.31 | 4.14  |\\n|              | L-18 SepFormer  | 12.63 | 14.74        | 9.46   | 8.96   | 9.66  | 14.65 | 10.98 | 14.49 | 11.59 | 8.02  |\\n\\n**Training on two languages** performs well under the **seen** languages condition. While the performance under the **mixed** condition GE\\\\text{mix} is moderate, it is worse under PO\\\\text{mix}. We assume that the model is slightly biased towards GE and can better estimate GE-specific masks for separation. Recent studies on Multi-TLE show similar findings in the comparison of GE and PO [33]. The individual results under the **unseen** conditions vary strongly, showing that the model only works for some of those language combinations. The negative or rather corrupting performances are similar to recent findings in Borsdorf et al. [31]. Listening to the reconstructed audio of the test sets with negative results reveals that occasionally three speakers are assigned to one output channel while the remaining speaker and artifacts/noise are assigned to the other output channel. We cannot find any language similarities w.r.t. phonetics (mono-phone level) that could help to interpret the results. We think that different recording conditions, even if the database was carefully recorded, may influence the performance.\\n\\nTraining on three languages shows slightly degraded results under the conditions which have shown good results when trained on two languages. Under almost all other conditions, the performance increases, except for JA-PL. The performances for GE\\\\text{mix} and PO\\\\text{mix} are now similar. Training on five languages further raises the Mean\\\\(_{231}\\\\) performance and improves the performance under mixed and **unseen** (CR-JA, CR-PL and JA-PL) conditions. We attribute this to an increase in the number of training languages and, consequently, in the number of speakers, utterances, and recording conditions.\\n\\nTraining on ten and eighteen languages improves the performance under most conditions, especially under the CH-CR condition. The largest overall drop is given under the GE-PO condition. A comparison between the L-18 Conv-TasNet model and the L-2 model, which could be seen as an expert for this condition, shows a degradation of about 1.92 dB. We clearly see that an overall improvement (indicated by 7.57 dB for Mean\\\\(_{231}\\\\)) comes at the cost of some individual performances.\\n\\nTraining SepFormer models on L-10 and L-18 sets outperforms the respective Conv-TasNet models. This is in line with recent results for different TLE architectures [32]. With a Mean\\\\(_{231}\\\\) result of 12.63 dB, the L-18 SepFormer model yields the best overall performance. However, for some individual language combinations, the L-10 SepFormer model outperforms the L-18 model. We also indicate that the gain in Mean\\\\(_{231}\\\\) performance between the L-10 and L-18 SepFormer models is larger compared to their L-10 and L-18 Conv-TasNet counterparts. The stronger separation performance in general as well as the larger performance gain may be based on the significantly larger amount of model parameters (approx. 5 million vs. 26 million parameters). In addition, we assume that the context information processing of the SepFormer could help to better identify language-specific characteristics in the input mixture, which may contribute to the superior language-based separation performance of the model.\\n\\n**5. Conclusion and Future Work**\\n\\nIn this paper, we have introduced the novel blind language separation (BLS) task, which disentangles multilingual cocktail party voices into speaker groups by language. We established baseline BLS models which can separate the voices based on the language without any language conditioning. Models trained on more languages show a better performance on average and generalize well to unseen speakers and languages. However, the models show low performances for some language combinations. The current BLS models assume a fixed number of languages in the mixture and thus cannot disentangle multilingual mixtures if the number of languages is not known in advance. To counteract this problem, we will investigate recent blind source separation methods that have been proposed to address this limitation [5, 21\u201325] for BLS. We will combine this objective with a larger-scale study on language separation by including more languages. We also intend to find methods to analyze the imbalance of the separation performance.\\n\\n**6. Acknowledgements**\\n\\nThis research is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany\u2019s Excellence Strategy (University Allowance, EXC 2077, University of Bremen).\"}"}
{"id": "borsdorf22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] E. C. Cherry, \u201cSome Experiments on the Recognition of Speech, with One and with Two Ears,\u201d The Journal of the Acoustical Society of America, vol. 25, no. 5, 1953.\\n\\n[2] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, \u201cDeep Clustering: Discriminative Embeddings for Segmentation and Separation,\u201d in ICASSP, 2016.\\n\\n[3] Y. Isik, J. Le Roux, Z. Chen, S. Watanabe, and J. R. Hershey, \u201cSingle-Channel Multi-Speaker Separation using Deep Clustering,\u201d in INTERSPEECH, 2016.\\n\\n[4] D. Yu, M. Kolb\u00e6k, Z. Tan, and J. Jensen, \u201cPermutation Invariant Training of Deep Models for Speaker-Independent Multi-talker Speech Separation,\u201d in ICASSP, 2017.\\n\\n[5] M. Kolb\u00e6k, D. Yu, Z.-H. Tan, and J. Jensen, \u201cMulti-talker Speech Separation with Utterance-level Permutation Invariant Training of Deep Recurrent Neural Networks,\u201d IEEE/ACM TASLP, vol. 25, no. 10, 2017.\\n\\n[6] Z. Chen, Y. Luo, and N. Mesgarani, \u201cDeep Attractor Network for Single-microphone Speaker Separation,\u201d in ICASSP, 2017.\\n\\n[7] Y. Luo, Z. Chen, and N. Mesgarani, \u201cSpeaker-independent Speech Separation with Deep Attractor Network,\u201d IEEE/ACM TASLP, vol. 26, no. 4, 2018.\\n\\n[8] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing Ideal Time\u2013Frequency Magnitude Masking for Speech Separation,\u201d IEEE/ACM TASLP, vol. 27, no. 8, 2019.\\n\\n[9] Y. Luo, Z. Chen, and T. Yoshioka, \u201cDual-Path RNN: Efficient Long Sequence Modeling for Time-Domain Single-Channel Speech Separation,\u201d in ICASSP, 2020.\\n\\n[10] N. Zeghidour and D. Grangier, \u201cWavesplit: End-to-End Speech Separation by Speaker Clustering,\u201d arXiv preprint arXiv:2002.08933v2, 2020.\\n\\n[11] C. Subakan, M. Ravanelli, S. Cornell, M. Bronzi, and J. Zhong, \u201cAttention Is All You Need In Speech Separation,\u201d in ICASSP, 2021.\\n\\n[12] M. Delcroix, K. Zmolikova, T. Ochiai, K. Kinoshita, S. Araki, and T. Nakatani, \u201cCompact Network for SpeakerBeam Target Speaker Extraction,\u201d in ICASSP, 2019.\\n\\n[13] K. \u02c7Zmol\u00b4\u0131kov\u00b4a, M. Delcroix, K. Kinoshita, T. Ochiai, T. Nakatani, L. Burget, and J. \u02c7Cernock\u00b4y, \u201cSpeakerBeam: Speaker Aware Neural Network for Target Speaker Extraction in Speech Mixtures,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 13, no. 4, pp. 800\u2013814, 2019.\\n\\n[14] Q. Wang, H. Muckenhirn, K. Wilson, P. Sridhar, Z. Wu, J. R. Hershey, R. A. Saurous, R. J. Weiss, Y. Jia, and I. L. Moreno, \u201cVoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking,\u201d in INTERSPEECH, 2019.\\n\\n[15] Q. Wang, I. L. Moreno, M. Saglam, K. Wilson, A. Chiao, R. Liu, Y. He, W. Li, J. Pelecanos, M. Nika, and A. Gruenstein, \u201cVoiceFilter-Lite: Streaming Targeted Voice Separation for On-Device Speech Recognition,\u201d in INTERSPEECH, 2020.\\n\\n[16] M. Ge, C. Xu, L. Wang, E. S. Chng, J. Dang, and H. Li, \u201cSpEx+: A Complete Time Domain Speaker Extraction Network,\u201d in INTERSPEECH, 2020.\\n\\n[17] Z. Zhang, B. He, and Z. Zhang, \u201cX-TaSNet: Robust and Accurate Time-Domain Speaker Extraction Network,\u201d in INTERSPEECH, 2020.\\n\\n[18] M. Delcroix, T. Ochiai, K. Zmolikova, K. Kinoshita, N. Tawara, T. Nakatani, and S. Araki, \u201cImproving Speaker Discrimination of Target Speech Extraction With Time-Domain SpeakerBeam,\u201d in ICASSP, 2020.\\n\\n[19] T. Ochiai, M. Delcroix, Y. Koizumi, H. Ito, K. Kinoshita, and S. Araki, \u201cListen to What You Want: Neural Network-based Universal Sound Selector,\u201d in INTERSPEECH, 2020.\\n\\n[20] J. Zhang, C. Zoril \u02d8a, R. Doddipatla, and J. Barker, \u201cTime-Domain Speech Extraction with Spatial Information and Multi Speaker Conditioning Mechanism,\u201d in ICASSP, 2021.\\n\\n[21] Y. Luo and N. Mesgarani, \u201cSeparating Varying Numbers of Sources with Auxiliary Autoencoding Loss,\u201d in INTERSPEECH, 2020.\\n\\n[22] S. Wisdom, H. Erdogan, D. P. W. Ellis, R. Serizel, N. Turchault, E. Fonseca, J. Salamon, P. Seetharaman, and J. R. Hershey, \u201cWhat\u2019s All the FUSS About Free Universal Sound Separation Data?\u201d in ICASSP, 2021.\\n\\n[23] N. Kanda, Y. Gaur, X. Wang, Z. Meng, and T. Yoshioka, \u201cSerialized Output Training for End-to-End Overlapped Speech Recognition,\u201d in INTERSPEECH, 2020.\\n\\n[24] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, \u201cListening to Each Speaker One by One with Recurrent Selective Hearing Networks,\u201d in ICASSP, 2018.\\n\\n[25] T. von Neumann, C. Boeddeker, L. Drude, K. Kinoshita, M. Delcroix, T. Nakatani, and R. Haeb-Umbach, \u201cMulti-talker ASR for an Unknown Number of Sources: Joint Training of Source Counting, Separation and ASR,\u201d in INTERSPEECH, 2020.\\n\\n[26] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, and J. Li, \u201cContinuous Speech Separation: Dataset and Analysis,\u201d in ICASSP, 2020.\\n\\n[27] G. Wichern, J. Antognini, M. Flynn, L. R. Zhu, E. McQuinn, D. Crow, E. Manilow, and J. Le Roux, \u201cWHAM!: Extending Speech Separation to Noisy Environments,\u201d in INTERSPEECH, 2019.\\n\\n[28] M. Maciejewski, G. Wichern, and J. Le Roux, \u201cWHAMR!: Noisy and Reverberant Single-Channel Speech Separation,\u201d in ICASSP, 2020.\\n\\n[29] J. Cosentino, M. Pariente, S. Cornell, A. Deleforge, and E. Vincent, \u201cLibriMix: An Open-Source Dataset for Generalizable Speech Separation,\u201d arXiv preprint arXiv:2005.11262v1, 2020.\\n\\n[30] P. Appeltans, J. Zegers, and Hugo van Hamme, \u201cPractical applicability of Deep Neural Networks for Overlapping Speaker Separation,\u201d in INTERSPEECH, 2019.\\n\\n[31] M. Borsdorf, C. Xu, H. Li, and T. Schultz, \u201cGlobalPhone Mix-to-Separate out of 2: A Multilingual 2000 Speakers Mixtures Database for Speech Separation,\u201d in INTERSPEECH, 2021.\\n\\n[32] M. Borsdorf, H. Li, and T. Schultz, \u201cTarget Language Extraction at Multilingual Cocktail Parties,\u201d in ASRU, 2021.\\n\\n[33] M. Borsdorf, K. Scheck, H. Li, and T. Schultz, \u201cExperts Versus All-Rounders: Target Language Extraction For Multiple Target Languages,\u201d accepted to ICASSP, 2022.\\n\\n[34] T. Schultz, \u201cGlobalPhone: A Multilingual Speech and Text Database Developed at Karlsruhe University,\u201d in ICSLP, 2002.\\n\\n[35] T. Schultz, N. T. Vu, and T. Schlippe, \u201cGlobalPhone: A Multilingual Text & Speech Database in 20 Languages,\u201d in ICASSP, 2013.\\n\\n[36] ELRA, \u201cEuropean Language Resources Association (ELRA),\u201d eLRA Catalog, retrieved October 15, 2020, from http://catalog.elra.info, 2020, [Online].\\n\\n[37] Y. Isik, J. L. Roux, S. W. Z. Chen, and J. R. Hershey, \u201cScripts to Create wsj0-2 Speaker Mixtures,\u201d MERL Research, retrieved June 2, 2020, from https://www.merl.com/demos/deep-clustering/create-speaker-mixtures.zip, [Online].\\n\\n[38] M. Pariente, S. Cornell, J. Cosentino, S. Sivasankaran, E. Tzinis, J. Heitkaemper, M. Olvera, F.-R. St \u00a8oter, M. Hu, J. M. Mart \u00b4\u0131n-Do\u02dcnas, D. Ditter, A. Frank, A. Deleforge, and E. Vincent, \u201cAsteroid: The PyTorch-based Audio Source Separation Toolkit for Researchers,\u201d in INTERSPEECH, 2020.\\n\\n[39] M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell, L. Lugosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong, J.-C. Chou, S.-L. Yeh, S.-W. Fu, C.-F. Liao, E. Ras-\"}"}
