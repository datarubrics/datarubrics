{"id": "huang22l_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"QbyE-MLPMixer: Query-by-Example Open-Vocabulary Keyword Spotting using MLPMixer\\nJinmiao Huang1\u2217, Waseem Gharbieh1\u2217, Qianhui Wan1\u2217, Han Suk Shim2, Chul Lee2\\n1LG Electronics Toronto AI Lab\\n2LG Electronics Artificial Intelligence Lab\\njinmiao.huang@lge.com, waseem.gharbieh@lge.com, qianhui.wan@lge.com, hansuk.shim@lge.com, clee.lee@lge.com\\n\\nAbstract\\nCurrent keyword spotting systems are typically trained with a large amount of pre-defined keywords. Recognizing keywords in an open-vocabulary setting is essential for personalizing smart device interaction. Towards this goal, we propose a pure MLP-based neural network that is based on MLPMixer - an MLP model architecture that effectively replaces the attention mechanism in Vision Transformers. We investigate different ways of adapting the MLPMixer architecture to the QbyE open-vocabulary keyword spotting task. Comparisons with the state-of-the-art RNN and CNN models show that our method achieves better performance in challenging situations (10dB and 6dB environments) on both the publicly available Hey-Snips dataset and a larger scale internal dataset with 400 speakers. Our proposed model also has a smaller number of parameters and MACs compared to the baseline models.\\n\\nIndex Terms: open-vocabulary, keyword spotting, MLPMixer, user-defined keyword spotting, Query-by-Example\\n\\n1. Introduction\\nTraditional keyword spotting relies on fixed wake words such as \\\"Hey Siri\\\", \\\"Alexa\\\", or \\\"Ok Google\\\". By allowing a user to set their own custom wake word, we enable greater degree of flexibility and personalization. This is the essence of user-defined keyword spotting but it comes with many challenges including low latency, small memory footprint, and utterances that are out of the training distribution.\\n\\nOver the years, different neural networks have been proposed for the fixed keyword spotting (KWS) task: for example, using Deep Neural Networks (DNNs) [1, 2], Time Delay Neural Networks [3], Convolutional Neural Networks (CNNs) [4], Recurrent Neural Networks (RNNs) [5, 6] or transformers [7, 8]. However, a large amount of target keyword data is required to effectively train those models. On the other hand, for open-vocabulary keyword spotting, early work relied on the output of an Automated Speech Recognition (ASR) system. For example, by looking for transcript matching on the word [9] or phoneme [10] level. These systems are computationally expensive and usually show performance degradation when the keywords are out of the vocabulary. Recent work in open-vocabulary keyword spotting focuses on mapping variable-duration audio signals to a fixed-length embedding in vector space. This approach is known as Query By Example (QbyE). In a QbyE system, RNNs are generally used as an encoder to extract the keyword embedding, for example, [11] is the first to use Long Short Term Memory (LSTM) networks [12] for QbyE open-vocabulary keyword spotting problem. [13, 14, 15] use RNNs with Connectionist Temporal Classification (CTC) loss to predict keyword sequences. [16] uses Gated Recurrent Unit (GRU) with a multi-head self-attention mechanism and softtriple loss. Non QbyE efforts on this task include the use of a Siamese network with triplet hinge loss [17], Recurrent Neural Network Transducer (RNN-T) model to predict phoneme or grapheme based subword units [18], and a metric-based meta-learning algorithm called Prototypical Networks with Max-Mahalanobis Center loss to tackle the scenario where users can define various number of spoken terms [19].\\n\\nThe MLPMixer [20] is a recently proposed alternative to the Vision Transformer (ViT) [21]. It uses Multi Layer Perceptrons (MLPs) exclusively to replace the attention mechanism in ViT. Other similar MLP-based models also achieve competitive performance on vision [22, 23, 24] and language [22] tasks. That being said, we find the MLPMixer's simple and intuitive design to be appealing so we decided to adapt it to the QbyE open-vocabulary KWS task.\\n\\nWe compare the MLPMixer's performance to an RNN baseline model and some popular CNN models. Our results show that the MLPMixer model is able to outperform those models using a smaller number of parameters and multiply-accumulates (MACs), which indicates that the MLPMixer can also serve as a strong alternative model for audio applications.\\n\\nOur contributions are the following: (1) We propose a simple yet effective adaptation for the MLPMixer for the QbyE KWS problem. (2) Our adaptation to audio provides a significant performance boost compared to the original MLPMixer model. (3) We show that the MLPMixer outperforms other state-of-the-art CNN and RNN based models. (4) We used the publicly available Hey-Snips dataset to conduct experiments on both non far-field and far-field environments, which can serve as a foundation for future work on open-vocabulary keyword spotting.\\n\\n2. Methods\\nThe encoder-decoder structure can be naturally applied to a QbyE system. The encoder compress data from high dimension into low dimensional embeddings. The decoder is used to tie the encoder to loss functions so embeddings belonging to the same class will be closer to each other while embeddings belonging to different classes will be further apart. During inference, the decoder is completely dropped and the triggering decision is made by comparing the distances between the enrolled and query embeddings. Figure 1 shows the system architecture.\\n\\n2.1. Input Representation\\nWe used Mel-frequency Cepstral Coefficients (MFCCs) as the input features. We extract 81-dimensional MFCC features from...\"}"}
{"id": "huang22l_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1s long audio computed every 12.5 ms with a window of 25 ms. This results in a matrix with 81 dimensions in both feature and temporal space. CMVN is then applied on the MFCC's temporal dimension before being input to the encoder. Since the ViT [21], many models including the MLPMixer split the input into non-overlapping patches and compute the embedding of each patch before feeding it to the model. In this paper, we keep the feature and timestep dimensions intact and use the 81 features across 81 timesteps directly as input to our model. There are two reasons behind this: firstly, we wanted to study the impact of input representation on the model's ability to extract information from the entire feature space. Secondly, providing the input directly reduces the model's computational and memory footprint. Our experiments show that this approach is more effective than patching (See details in 4.3.1).\\n\\n2.2. MLPMixer\\n\\nUnlike the original MLPMixer which first operates on flattened image embedding patches (token-mixing) and then on the channel dimension (channel-mixing), we apply the mixing on the feature and temporal dimensions. Specifically, we directly apply a linear layer to the normalized features to project the feature dimension $f$ to a hidden dimension $h$, followed by another linear layer to project the hidden dimension $h$ back to $f$. The two linear layers form the major components of an MLP block. We call the first MLP block feature-mixing. Similarly, we apply two linear layers on the temporal space to project the time dimension $t$ to a hidden dimension $g$ and then project $g$ back to $t$, we call this second MLP block time-mixing. For a given input $X \\\\in \\\\mathbb{R}^{f \\\\times t}$ (since there is only one channel, the channel dimension is omitted to simplify the notation), the feature mixing MLP block is implemented as follows:\\n\\n$$U = X + W_2 \\\\sigma(W_1 \\\\text{LayerNorm}(X))$$\\n\\nwhere $X \\\\in \\\\mathbb{R}^{f \\\\times t}$ denotes the extracted features, $W_1 \\\\in \\\\mathbb{R}^{h \\\\times f}$ and $W_2 \\\\in \\\\mathbb{R}^{f \\\\times h}$ are the weights in the first and second linear layer in an MLP block respectively. We also added LayerNorm and activation function $\\\\sigma$ in each MLP block. We use Hardswish [25] as the activation function in this paper as it shows better performance than other activation functions (see detailed comparisons in section 4.3.2). A residual connection is added to link the projection from MLP block with the original input $X$ to form the final projection $U \\\\in \\\\mathbb{R}^{f \\\\times t}$ on the feature dimension. We then use $U$ as the input to the time mixing MLP block. Similar operations were added to $U^T$ to extract information on the temporal dimension:\\n\\n$$Y = U + (W_4 \\\\sigma(W_3 \\\\text{LayerNorm}(U^T)))^T$$\\n\\nwhere $W_3 \\\\in \\\\mathbb{R}^{g \\\\times t}$ and $W_4 \\\\in \\\\mathbb{R}^{t \\\\times g}$ are the weights of the two fully-connected layers in the time-mixing MLP block. $Y \\\\in \\\\mathbb{R}^{f \\\\times t}$ is the projection on the temporal dimension. Each MLP block generates the same output size as its input. For example, regardless of how we setup the dimension of the hidden space $h$ and $g$ in each MLP block, the output from MLP block is always $f \\\\times t$. Due to this design, MLP blocks can be easily stacked together and we only need to tune parameters $h$, $g$, number of stacked blocks $n$, and the activation function $\\\\sigma$, which makes hyperparameter optimization straightforward.\\n\\nThe output of the MLPMixer is passed to an average pooling layer to aggregate the information on the temporal dimension. Formally, for an output $O \\\\in \\\\mathbb{R}^{f \\\\times t}$ from the last MLP block, we apply the average pooling operation on the temporal dimension to generate the embedding $z \\\\in \\\\mathbb{R}^{f}$:\\n\\n$$z = \\\\frac{1}{t} \\\\sum_{i=1}^{t} O$$\\n\\n2.3. Inference\\n\\nDuring inference, the linear layer from the decoder is dropped and the embedding output from the average pooling layer is used. Since the model is trained on 1s long audio samples, for any input audio, we take a 1s moving window with a stride of 100 ms and feed it to the network. The cosine distance is then used to compare the similarity between the embedding vectors generated from query audio and each of the $n$ embedding vectors saved from enrollment. When calculating the cosine distance, if the enrollment embedding is shorter than the query embedding, we convolve the enrollment embedding with the query embedding and take the minimum distance. Otherwise, if the enrollment embedding is longer than the query embedding, we zero pad the left side of the query embedding vector to match the size of the enrollment embedding. Note that in a streaming setting, padding is not necessary, since we can create a buffer with the same size as the longest enrollment so the query always matches the length of the longest enrollment. After computing the cosine distance for all enrollments, the minimum value is taken and compared to a threshold to make a triggering decision. If the value is smaller than the set threshold, the system triggers a positive response. For our system, the embedding size is the same as the MFCC feature dimensions (81), which is considerably small compared to other baseline models which have an embedding size on the order of 1000 (e.g. MobilenetV3: 960). The small embedding size also shows that the MLPMixer is able to effectively project the useful information to a small hidden space.\"}"}
{"id": "huang22l_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Experiments\\n\\n3.1. Training\\n\\nWe used the same protocol as [16] to train our model. We used the Librispeech [26] dataset which is comprised of 1000 hours of read English audiobooks sampled at 16 kHz along with annotated text. Since we want to train our model to classify the word given the audio, we use the Montreal Forced Aligner [27] tool to generate word level annotated segmentations. After segmenting the audio, we standardize it to be 1s long by random clipping or zero padding on both sides of the audio depending on whether the audio is longer or shorter than 1s.\\n\\nWe augmented the training data with random 4 to 12 dB background noise from the \u201cnoise-train\u201d folder in the Microsoft Scalable Noisy Speech Dataset (MS-SNSD) [28]. In addition, we simulated effects of far-field conditions following [29] with 50% probability of adding 4dB to 15dB point source noise on top of the far-field effect. Cross entropy loss was used as the objective function.\\n\\n3.2. Evaluation\\n\\nFor the positive queries, we used the test portion of the Hey-Snips dataset and an internal dataset to test our model. The test portion of the Hey-Snips dataset contains 2,588 positive utterances from 520 speakers with a maximum of 10 utterances per speaker. Since we need multiple keyword recordings from the same speaker to perform the evaluation, we selected all the speakers with 10 keyword utterances and used three random utterances for enrollment and the remaining seven utterances for query. This reduced the number of speakers down to 40. The internal dataset contains 8 keywords named after actual home appliances: \u201cLG Styler\u201d, \u201cLG Washer\u201d, \u201cLG Dryer\u201d, \u201cHey LG\u201d, \u201cLG Fridge\u201d, \u201cLG Puricare\u201d, \u201cLG Oven\u201d, and \u201cHey Cloi\u201d. Each keyword was uttered 10 times by 50 speakers. Similar to Hey-Snips, we used three random utterances for enrollment and the remaining seven for query. Both datasets were recorded in a clean environment. We synthetically distorted both datasets in order to simulate real-world environment by adding noise and far-field effects to them. Specifically, we used the \u201cnoise-test\u201d folder from the MS-SNSD dataset to add 10dB and 6dB noises. We also added far-field effects to the clean dataset before adding 10dB and 6dB noise to it. This resulted in six scenarios in total.\\n\\nFor the negative queries, we used the negative samples from Hey-Snips test set. It contains about 20K utterances of general sentences from 1,469 speakers with a maximum of 30 utterances per speaker. We split the negative set into 2 halves and randomly add 10dB and 6dB noise to each half. The \u201cnoise-test\u201d folder in the MS-SNSD dataset was used for adding noise. In the far-field case, we added far-field effects to the negative set before adding noise. Each set was evaluated alongside its corresponding positive queries.\\n\\n4. Results\\n\\n4.1. Baseline Models\\n\\nFor baseline models, we first choose a state-of-the-art RNN model with GRU and self-attention (GRU-ATTN) [16] which previously reported the open-vocabulary keyword spotting results on the Hey-Snips dataset.\\n\\nIn addition, since computer vision models show good performance on many audio tasks (for example, MobileNetV2 [30] were used for audio tagging [31], pre-defined keyword spotting [32] and personalized keyword spotting [33]), we use them as baseline models to compare our model against. Specifically, we experimented with the edge friendly MobileNetV2 [30], MobileNetV3 [25], and EfficientNetB0 [34]. We also included ViT in our baseline, as it shows competitive performance compared to state-of-the-art CNN models in vision tasks.\\n\\nThe inputs to vision models are usually images with three color channels, whereas our input has only one channel. In order to adapt the vision models to our task, we adopt the same preprocessing technique from [32] to map one-channel audio into three channels. We found that models with ImageNet [35] pretrained weights show significantly better performance than the ones without, even though there is no clear relationship between MFCC features and ImageNet samples. Similar observations on the effectiveness of transfer learning from ImageNet pretrained models on audio tasks is also reported in [36].\\n\\n4.2. Results and Size Comparison\\n\\nWhen designing our MLPMixer model, our objective was to answer the following question: Can we find a model architecture that has competitive performance while keeping the number of parameters and MACs inline with the baseline models?\\n\\nTable 1 shows that among the baseline models, GRU-ATTN [16] has the smallest number of parameters (550K) while MobileNetV3 [25] has the smallest number of MACs (22.24M). Under this criterion, we ran hyperparameter optimization trials using Asynchronous Hyperband Search (ASHA) with the intention of finding a model that has a smaller footprint than the baseline models. Our hyperparameter search with hundreds of sampled experiments yielded a model with 250K parameters and 20M MACs. This model is constructed with 12 mixing blocks containing 64 hidden layers for both the time and frequency mixing blocks and no dropout. Our implementation of the MLPMixer is able to outperform MobileNetV3 in most cases, especially under challenging conditions.\\n\\nTable 1:\\n\\n| Model          | Params (M) | MACs (M) |\\n|----------------|------------|----------|\\n| GRU-ATTN [16]  | 0.55       | 41.23    |\\n| MobileNetV2 [30] | 2.22       | 29.22    |\\n| MobileNetV3 [25] | 2.97       | 22.24    |\\n| EfficientnetB0 [34] | 4.01       | 39.06    |\\n| ViT [21]       | 0.96       | 77.06    |\\n| QbyE-MLPMixer   | 0.25       | 20.16    |\\n\\nTables 2 and 3 show the False Rejection Rate (FRR) at 0.3 False Acceptance (FA) per hour for Hey-Snips and the internal dataset respectively. In general, our model gives lower FRRs compared to the baseline models under more challenging conditions. More specifically, compared to the best performance among baseline models on Hey-Snips dataset, our model shows 2.15% and 4.29% decrease in FRR under on non far-field 10dB and 6dB condition, and 6.43% and 8.57% decrease under far-field 10dB and 6dB condition. For the internal dataset, MobileNetV3 shows the best performance when there is no additional noise added, whereas our model gives better FRRs under more challenging conditions.\"}"}
{"id": "huang22l_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: FRR (%) at 0.3 FAs per hour for clean, 10dB and 6dB on Hey-Snips dataset\\n\\n| Model          | Non Far-Field clean | Non Far-Field 10dB | Non Far-Field 6dB | Far-Field clean | Far-Field 10dB | Far-Field 6dB |\\n|----------------|---------------------|--------------------|-------------------|----------------|----------------|----------------|\\n| GRU-ATTN [16]  | 7.86                | 11.79              | 15.71             | 20.00          | 20.64          | 32.50          |\\n| MobileNetV2 [30] | 5.36               | 12.50              | 20.71             | 14.64          | 15.00          | 35.36          |\\n| MobileNetV3 [25] | 5.71               | 13.93              | 21.79             | 12.86          | 18.21          | 38.93          |\\n| EfficientnetB0 [34] | 5.71               | 12.86              | 18.21             | 16.43          | 13.93          | 36.79          |\\n| ViT [21]      | 8.21                | 10.36              | 14.29             | 12.14          | 15.36          | 28.93          |\\n| QbyE-MLPMixer  | 5.36                | 8.21               | 10.00             | 8.93           | 7.50           | 20.36          |\\n\\nTable 3: FRR (%) at 0.3 FAs per hour for clean, 10dB and 6dB on internal dataset\\n\\n| Model          | Non Far-Field clean | Non Far-Field 10dB | Non Far-Field 6dB | Far-Field clean | Far-Field 10dB | Far-Field 6dB |\\n|----------------|---------------------|--------------------|-------------------|----------------|----------------|----------------|\\n| GRU-ATTN [16]  | 0.81                | 4.43               | 7.06              | 8.01           | 18.63          | 26.78          |\\n| MobileNetV2 [30] | 3.54               | 5.38               | 6.84              | 11.25          | 15.44          | 20.09          |\\n| MobileNetV3 [25] | 0.62               | 3.69               | 5.82              | 3.85           | 11.35          | 16.82          |\\n| EfficientnetB0 [34] | 2.12               | 4.60               | 6.43              | 5.71           | 11.76          | 16.13          |\\n| ViT [21]      | 4.59                | 5.06               | 6.10              | 9.03           | 13.75          | 16.13          |\\n| QbyE-MLPMixer  | 2.68                | 3.71               | 4.32              | 5.29           | 9.25           | 11.34          |\\n\\n4.3. Ablation Study\\n\\n4.3.1. Alternative Input Representations\\n\\nThe original MLPMixer model divides the input image into non-overlapping patches and then computes a linear embedding for each patch. Although this procedure is effective for images, we observed that it is suboptimal for audio. Instead, we show that feeding the MFCC features directly to the model to be much more effective. To mimic the original MLPMixer, we divide the MFCC features into $9 \\\\times 9$ non-overlapping patches and compute an 81 dimensional linear embedding for each patch (\u201cwith PE\u201d in Table 4). This way the input shape is identical to the input to our proposed model. The patch embedding layer increases the model\u2019s number of parameters by 7K and MACs by 500K. For completeness, we also experimented with dividing the MFCC features into $9 \\\\times 9$ non-overlapping patches and reshaping them (without computing linear embeddings) to match the input shape of our proposed model (\u201cw/o PE\u201d in Table 4). Table 4 shows the results of our experiments. We ran the models on all six scenarios (clean, 10dB, and 6dB noise on the non far-field and far-field conditions) on the Hey-Snips and Internal datasets and report the mean FRR for the non far-field and far-field conditions separately.\\n\\nThe results show that the way in which the input is presented to the model matters. The difference in performance is substantial, our representation is able to reduce the error rate by 40% under the non far-field condition as well as Hey-Snips far-field and 34% under the internal far-field condition compared to the patch embedding approach. The table also shows that not using patch embeddings results in worse performance on the Hey-Snips dataset but slightly better results on the internal dataset compared to using patch embeddings.\\n\\n4.3.2. Activation Function\\n\\nOur hyperparameter search showed that Hardswish [25] activation is more effective than GELU [37] which is originally used in the MLPMixer model. Table 5 compares the mean FRR between using Hardswish and other popular activation functions on the Hey-Snips and the internal datasets using the same procedure as section 4.3.1. Hardswish performs the best overall while being simpler to implement on hardware than GELU or SiLU [37].\\n\\nTable 5: Mean FRR (%) at 0.3 FAs per hour for various activation functions on the Hey-Snips and Internal datasets.\\n\\n| Activation      | Non Far-Field Hey-Snips | Non Far-Field Internal | Far-Field Hey-Snips | Far-Field Internal |\\n|-----------------|-------------------------|------------------------|--------------------|-------------------|\\n| GELU            | 8.69                    | 3.95                   | 13.13              | 10.02             |\\n| ReLU            | 9.76                    | 4.03                   | 14.90              | 9.29              |\\n| SiLU            | 8.33                    | 4.29                   | 11.90              | 9.91              |\\n| Hardswish       | 7.86                    | 3.57                   | 12.26              | 8.63              |\\n\\n5. Conclusion and Future Work\\n\\nIn this paper, we developed an effective small footprint QbyE-MLPMixer for open-vocabulary KWS by adapting the MLP-Mixer. We compared the performance of our model with other state-of-the-art RNN and CNN models to demonstrate the effectiveness of our approach. Experiments were conducted on both the publicly available Hey-Snips dataset and an internal dataset with eight keywords and 400 speakers. The results show that the MLPMixer model performs the best overall especially under noisy conditions. We also show that feeding the MFCC features directly to the model results in better performance compared to flattened patch embeddings. Finally, we show that Hardswish activation performs the best overall compared to other activation functions. To the best of our knowledge, this is the first investigation of an MLPMixer model on the QbyE open-vocabulary KWS task. The model\u2019s ability to extract information to a small embedding space can positively impact other QbyE related applications such as speaker verification and novelty detection. Future research on the rethinking of MLPs and the mixing idea could be beneficial for understanding the roles that these fundamental layers play in achieving competitive performance.\"}"}
{"id": "huang22l_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] G. Chen, C. Parada, and G. Heigold, \u201cSmall-footprint keyword spotting using deep neural networks,\u201d in 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 4087\u20134091.\\n\\n[2] R. Prabhavalkar, R. Alvarez, C. Parada, P. Nakkiran, and T. N. Sainath, \u201cAutomatic gain control and multi-style training for robust small-footprint keyword spotting with deep neural networks,\u201d in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4704\u20134708.\\n\\n[3] Y. Bai, J. Yi, J. Tao, Z. Wen, Z. Tian, C. Zhao, and C. Fan, \u201cA time delay neural network with shared weight self-attention for small-footprint keyword spotting.\u201d in INTERSPEECH, 2019, pp. 2190\u20132194.\\n\\n[4] T. N. Sainath and C. Parada, \u201cConvolutional neural networks for small-footprint keyword spotting,\u201d in INTERSPEECH, 2015.\\n\\n[5] S. Fern\u00e1ndez, A. Graves, and J. Schmidhuber, \u201cAn application of recurrent neural networks to discriminative keyword spotting,\u201d in International Conference on Artificial Neural Networks. Springer, 2007, pp. 220\u2013229.\\n\\n[6] T. Yamamoto, R. Nishimura, M. Misaki, and N. Kitaoka, \u201cSmall-footprint magic word detection method using convolutional lstm neural network.\u201d in INTERSPEECH, 2019, pp. 2035\u20132039.\\n\\n[7] S. Adya, V. Garg, S. Sigtia, P. Simha, and C. Dhir, \u201cHybrid transformer/ctc networks for hardware efficient voice triggering,\u201d INTERSPEECH, 2020.\\n\\n[8] Y. Wang, H. Lv, D. Povey, L. Xie, and S. Khudanpur, \u201cWake word detection with streaming transformers,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5864\u20135868.\\n\\n[9] D. R. Miller, M. Kleber, C.-L. Kao, O. Kimball, T. Colthurst, S. A. Lowe, R. M. Schwartz, and H. Gish, \u201cRapid and accurate spoken term detection,\u201d in Eighth Annual Conference of the international speech communication association, 2007.\\n\\n[10] M. G. Brown, J. T. Foote, G. J. Jones, K. S. Jones, and S. J. Young, \u201cOpen-vocabulary speech indexing for voice and video mail retrieval,\u201d in Proceedings of the Fourth ACM international Conference on Multimedia, 1997, pp. 307\u2013316.\\n\\n[11] G. Chen, C. Parada, and T. N. Sainath, \u201cQuery-by-example keyword spotting using long short-term memory networks,\u201d in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 5236\u20135240.\\n\\n[12] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\\n\\n[13] L. Lugosch, S. Myer, and V. S. Tomar, \u201cDonut: Ctc-based query-by-example keyword spotting,\u201d arXiv preprint arXiv:1811.10736, 2018.\\n\\n[14] T. Bluche, M. Primet, and T. Gisselbrecht, \u201cSmall-footprint open-vocabulary keyword spotting with quantized lstm networks,\u201d arXiv preprint arXiv:2002.10851, 2020.\\n\\n[15] Y. Zhuang, X. Chang, Y. Qian, and K. Yu, \u201cUnrestricted vocabulary keyword spotting using lstm-ctc.\u201d in Interspeech, 2016, pp. 938\u2013942.\\n\\n[16] J. Huang, W. Gharbieh, H. S. Shim, and E. Kim, \u201cQuery-by-example keyword spotting system using multi-head attention and soft-triple loss,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6858\u20136862.\\n\\n[17] S. Settle and K. Livescu, \u201cDiscriminative acoustic word embeddings: Tecurrent neural network-based approaches,\u201d in 2016 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2016, pp. 503\u2013510.\\n\\n[18] Y. He, R. Prabhavalkar, K. Rao, W. Li, A. Bakhtin, and I. McGraw, \u201cStreaming small-footprint keyword spotting using sequence-to-sequence models,\u201d in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 474\u2013481.\\n\\n[19] Y. Chen, T. Ko, and J. Wang, \u201cA Meta-Learning Approach for User-Defined Spoken Term Classification with Varying Classes and Examples,\u201d in Proc. Interspeech 2021, 2021, pp. 4224\u20134228.\\n\\n[20] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit et al., \u201cMlp-mixer: An all-mlp architecture for vision,\u201d Advances in Neural Information Processing Systems, vol. 34, 2021.\\n\\n[21] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d arXiv preprint arXiv:2010.11929, 2020.\\n\\n[22] H. Liu, Z. Dai, D. So, and Q. Le, \u201cPay attention to mlps,\u201d Advances in Neural Information Processing Systems, vol. 34, 2021.\\n\\n[23] H. Touvron, P. Bojanowski, M. Caron, M. Cord, A. El-Nouby, E. Grave, G. Izacard, A. Joulin, G. Synnaeve, J. Verbeek et al., \u201cResmlp: Feedforward networks for image classification with data-efficient training,\u201d arXiv preprint arXiv:2105.03404, 2021.\\n\\n[24] X. Ding, C. Xia, X. Zhang, X. Chu, J. Han, and G. Ding, \u201cRepmlp: Re-parameterizing convolutions into fully-connected layers for image recognition,\u201d arXiv preprint arXiv:2105.01883, 2021.\\n\\n[25] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan et al., \u201cSearching for mobilenetv3,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 1314\u20131324.\\n\\n[26] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an asr corpus based on public domain audio books,\u201d in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 5206\u20135210.\\n\\n[27] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sunderegger, \u201cMontreal forced aligner: Trainable text-speech alignment using kaldi.\u201d in Interspeech, vol. 2017, 2017, pp. 498\u2013502.\\n\\n[28] C. K. Reddy, E. Beyrami, J. Pool, R. Cutler, S. Srinivasan, and J. Gehrke, \u201cA scalable noisy speech dataset and online subjective test framework,\u201d Proc. Interspeech 2019, pp. 1816\u20131820, 2019.\\n\\n[29] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur, \u201cA study on data augmentation of reverberant speech for robust speech recognition,\u201d in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 5220\u20135224.\\n\\n[30] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, \u201cMobilenetv2: Inverted residuals and linear bottlenecks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 4510\u20134520.\\n\\n[31] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, \u201cPanns: Large-scale pretrained audio neural networks for audio pattern recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2880\u20132894, 2020.\\n\\n[32] R. Tang, J. Lee, A. Razi, J. Cambre, I. Bicking, J. Kaye, and J. Lin, \u201cHowl: a deployed, open-source wake word detection system,\u201d arXiv preprint arXiv:2008.09606, 2020.\\n\\n[33] Y. Jia, X. Wang, X. Qin, Y. Zhang, X. Wang, J. Wang, D. Zhang, and M. Li, \u201cThe 2020 personalized voice trigger challenge: Open datasets, evaluation metrics, baseline system and results,\u201d in Proc. Annu. Conf. Int. Speech Commun. Assoc, 2021, pp. 4239\u20134243.\\n\\n[34] M. Tan and Q. Le, \u201cEfficientnet: Rethinking model scaling for convolutional neural networks,\u201d in International Conference on Machine Learning. PMLR, 2019, pp. 6105\u20136114.\\n\\n[35] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009, pp. 248\u2013255.\\n\\n[36] K. Palanisamy, D. Singhania, and A. Yao, \u201cRethinking cnn models for audio classification,\u201d arXiv preprint arXiv:2007.11154, 2020.\\n\\n[37] D. Hendrycks and K. Gimpel, \u201cGaussian error linear units (gelus),\u201d arXiv preprint arXiv:1606.08415, 2016.\"}"}
