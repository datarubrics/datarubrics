{"id": "ogun24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. Acknowledgements\\n\\nWe appreciate the over 700 African contributors whose voices made this work possible. We appreciate the invaluable support from Intron Health for contributing the datasets for this work, the pan-African platform for data collection, developing custom UIs for human evaluation (MOS), quality reviews, multi-currency contributor payments, and compute for experiments. Experiments presented in this paper were partly carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RE-NATER and several Universities as well as other organizations (see https://www.grid5000.fr).\\n\\nTejumade Afonja is partially supported by ELSA \u2013 European Lighthouse on Secure and Safe AI funded by the European Union under grant agreement No. 101070617. We appreciate the support provided by the BioRAMP researchers, whose collaboration and insights have been fundamental to our research.\\n\\n9. References\\n\\n[1] J. Kim, J. Kong, and J. Son, \u201cConditional variational autoencoder with adversarial learning for end-to-end text-to-speech,\u201d in International Conference on Machine Learning. PMLR, 2021, pp. 5530\u20135540.\\n\\n[2] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv preprint arXiv:2301.02111, 2023.\\n\\n[3] E. Casanova, J. Weber, C. D. Shulby, A. C. Junior, E. G\u00f6lge, and M. A. Ponti, \u201cYourTTS: Towards zero-shot multi-speaker TTS and zero-shot voice conversion for everyone,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 2709\u20132720.\\n\\n[4] V. Pratap, A. Tjandra, B. Shi, P. Tomasello, A. Babu, S. Kundu, A. Elkahky, Z. Ni, A. Vyas, M. Fazel-Zarandi et al., \u201cScaling speech technology to 1,000+ languages,\u201d Journal of Machine Learning Research, vol. 25, no. 97, pp. 1\u201352, 2024.\\n\\n[5] Wikipedia contributors, \u201cList of countries by English-speaking population \u2014 Wikipedia, the free encyclopedia,\u201d https://en.wikipedia.org/w/index.php?title=List of countries by English-speaking population&oldid=1211690147, 2024, [Online; accessed 9-March-2024].\\n\\n[6] K. Ito and L. Johnson, \u201cThe LJ speech dataset,\u201d https://keithito.com/LJ-Speech-Dataset/, 2017.\\n\\n[7] J. Yamagishi, C. Veaux, K. MacDonald et al., \u201cCSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92),\u201d University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2019.\\n\\n[8] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A corpus derived from LibriSpeech for text-to-speech,\u201d Interspeech 2019, 2019.\\n\\n[9] R. Badlani, R. Valle, K. J. Shih, J. F. Santos, S. Gururani, and B. Catanzaro, \u201cMultilingual multiaccented multispeaker TTS with RADTTS,\u201d in Proc. ICASSP. IEEE, 2023.\\n\\n[10] A. W. Black, \u201cCMU wilderness multilingual speech dataset,\u201d in Proc. ICASSP. IEEE, 2019, pp. 5971\u20135975.\\n\\n[11] P. Ogayo, G. Neubig, and A. W Black, \u201cBuilding African Voices,\u201d in Proc. Interspeech 2022, 2022, pp. 1263\u20131267.\\n\\n[12] J. Meyer, D. I. Adelani, E. Casanova, A. \u00d6ktem, D. W. J. Weber, S. Kabongo, E. Salesky, I. Orife, C. Leong, P. Ogayo et al., \u201cBibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus,\u201d in Proc. Interspeech, 2022, pp. 2383\u20132387.\\n\\n[13] A. Gutkin, I. Demirsahin, O. Kjartansson, C. E. Rivera, and K. Tubosun, \u201cDeveloping an open-source corpus of yoruba speech,\u201d in Proc. Interspeech, October 25\u201329, Shanghai, China, 2020., 2020, pp. 404\u2013408. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2020-1096.\\n\\n[14] D. van Niekerk, E. Barnard, O. Giwa, and A. Sosimi, \u201cLagos-NWU Yoruba speech corpus,\u201d 2015.\\n\\n[15] D. Van Niekerk and E. Barnard, \u201cTone realisation in a Yoruba speech recognition corpus,\u201d 2012.\\n\\n[16] A. \u00d6ktem, M. A. Jaam, E. DeLuca, and G. Tang, \u201cGamayun-language technology for humanitarian response,\u201d in IEEE Global Humanitarian Technology Conference (GHTC). IEEE, 2020, pp. 1\u20134.\\n\\n[17] D. van Niekerk, C. van Heerden, M. Davel, N. Kleynhans, O. Kjartansson, M. Jansche, and L. Ha, \u201cRapid development of TTS corpora for four south african languages,\u201d 2017.\\n\\n[18] T. K. Dagba and C. Boco, \u201cA text to speech system for Fon language using multisyn algorithm,\u201d Procedia Computer Science, vol. 35, pp. 447\u2013455, 2014.\\n\\n[19] T. Olatunji, T. Afonja, A. Yadavalli, C. C. Emezue, S. Singh, B. F. Dossou, J. Osuchukwu, S. Osei, A. L. Tonja, N. Etori et al., \u201cAfriSpeech-200: Pan-African accented speech dataset for clinical and general domain ASR,\u201d Transactions of the Association for Computational Linguistics, vol. 11, pp. 1669\u20131685, 2023.\\n\\n[20] A. Defossez, G. Synnaeve, and Y. Adi, \u201cReal Time Speech Enhancement in the Waveform Domain,\u201d in Proc. Interspeech 2020, 2020, pp. 3291\u20133295.\\n\\n[21] H. Liu, Q. Kong, Q. Tian, Y. Zhao, D. Wang, C. Huang, and Y. Wang, \u201cVoiceFixer: Toward general speech restoration with neural vocoder,\u201d 2021.\\n\\n[22] P. Andreev, A. Alanov, O. Ivanov, and D. Vetrov, \u201cHiFi++: a unified framework for bandwidth extension and speech enhancement,\u201d in Proc. ICASSP. IEEE, 2023, pp. 1\u20135.\\n\\n[23] S. Ogun, V. Colotte, and E. Vincent, \u201cCan we use common voice to train a multi-speaker TTS system?\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 900\u2013905.\\n\\n[24] J. Wiseman, \u201cpy-webrtcvad,\u201d https://github.com/wiseman/py-webrtcvad, 2016.\\n\\n[25] Y. Zhang, E. Bakhturina, and B. Ginsburg, \u201cNeMo (Inverse) Text Normalization: From Development to Production,\u201d in Proc. Interspeech, 2021, pp. 4857\u20134859.\\n\\n[26] Eren G\u00f6lge, \u201cXtts v1 \u2014 technical notes,\u201d https://medium.com/@erogol/xtts-v1-technical-notes-eb83ff05bdc, 2023, [Online; accessed 1-March-2024].\\n\\n[27] C. Jemine, \u201cResemblyzer,\u201d https://github.com/resemble-ai/Resemblyzer, 2019.\\n\\n[28] T. Yoshimura, \u201cSpeaker interpolation in HMM-based speech synthesis system,\u201d in Proc. of Eurospeech, 1997, pp. 2523\u20132526.\\n\\n[29] G. Mittag and S. M\u00f6ller, \u201cDeep learning based assessment of synthetic speech naturalness,\u201d in Proc. Interspeech, 2020.\\n\\n[30] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 28 492\u201328 518.\\n\\n[31] S. Vaibhav, F. Cl\u00e9mence et al., \u201cTTS Arena: Benchmarking text-to-speech models in the wild,\u201d https://huggingface.co/blog/arena-tts, 2024, [Online; accessed 1-March-2024].\\n\\n[32] A. Owodunni, A. Yadavalli, C. Emezue, T. Olatunji, and C. Mbataku, \u201cAccentFold: A journey through African accents for zero-shot ASR adaptation to target accents,\u201d in Findings of the Association for Computational Linguistics: EACL 2024, Y. Graham and M. Purver, Eds. St. Julian\u2019s, Malta: Association for Computational Linguistics, Mar. 2024, pp. 2146\u20132161. [Online]. Available: https://aclanthology.org/2024.findings-eacl.142.\"}"}
{"id": "ogun24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nRecent advances in speech synthesis have enabled many useful applications like audio directions in Google Maps, screen readers, and automated content generation on platforms like TikTok. However, these systems are mostly dominated by voices sourced from data-rich geographies with personas representative of their source data. Although 3000 of the world's languages are domiciled in Africa, African voices and personas are under-represented in these systems. As speech synthesis becomes increasingly democratized, it is desirable to increase the representation of African English accents. We present AfroTTS, the first pan-African accented English speech synthesis system able to generate speech in 86 African accents, with 1000 personas representing the rich phonological diversity across the continent for downstream application in Education, Public Health, and Automated Content Creation. Speaker interpolation retains naturalness and accentedness, enabling the creation of new voices.\\n\\nIndex Terms: text-to-speech, African-accented TTS, accented speech, multi-accent TTS, multi-speaker TTS\\n\\n1. Introduction\\nSynthetic voices are used in everyday applications for text reading, content generation, voice-over, etc., and provide audio feedback in applications such as maps, language learning tools, smart speakers, and voice assistants. Synthetic voices have also found wider adoption as a plugin with the proliferation of large language models. With the high naturalness and high quality of synthetic voices [1, 2], they have become more widely used on online platforms and social media.\\n\\nSynthetic voices are usually generated by speech synthesis systems, and there are several open-source systems available for use, e.g., [3]. Several of these systems are provided in English, and several efforts have been made to cover over 1000 languages of the world, including several African languages [4].\\n\\nOver 1.4 billion people reside in Africa, and more than 237 million people speak the English language at a bilingual or native level [5], including Nigeria, Uganda, South Africa, etc., and several other African countries speak it as a second or third language. Considering this large demography of speakers, the current representation of personas in synthetic voices does not show this diversity in terms of the typical African accent.\\n\\nGenerating synthetic speech with personas similar to one's demography is paramount for the widespread adoption and acceptability of the technology. For example, a Nigerian content creator would prefer to generate speech in an accent (or language) that is familiar and natural to their audience. Voice cloning methods have been created to generate speech in the voice of a reference speaker, however, these systems still fail to generalize to the diverse African accents from our analysis. This is because speaker representation in speech synthesis systems favors more general accents like the American, British, or Australian accents, etc. Even the widely used English text-to-speech (TTS) datasets are not representative of diverse demographies [6, 7, 8].\\n\\nTherefore, in this work, we focus on expanding the diversity of synthetic personas in TTS systems to the typical African accent. Our work covers 747 speakers and 86 accents from 9 countries. We focus on improving several TTS systems in generating voices with African-like personas, enabling a larger representation of African voices for use in applications like podcasts and content creation. Finally, we explore a speaker averaging method to create new personas different from those used to train the TTS systems. This was done to ensure that users can generate synthetic speech of diverse accents, without the fallout of overuse of a specific speaker or accent in our dataset.\\n\\nSection 2 reviews some literature particular to this research. Sections 3 and 4 describe the dataset, data preprocessing methods, TTS models, evaluation protocols, and experiments. We discuss the results in Section 5 and conclude with limitations and the summary of our work in Sections 6 and 7 respectively.\\n\\n2. Literature Review\\nThere have been works extending TTS to other accents. These include works on multilingual, multi-speaker TTS synthesis, e.g., [3], where the accents of different speakers were learned jointly with the language, enabling the model to produce English speech of different accents. Similarly, [9] disentangled the speaker from the accent so that the accent can be assigned to any speaker in the TTS model. Some curated datasets also covered African languages, e.g., CMU Wilderness [10] and MMS [4] datasets, but majorly have single speakers for each language.\\n\\nIn the African context, the authors of [11] curated datasets and built speech synthesis systems for 12 low-resourced African languages. Similarly, BibleTTS [12] contained 86 hours of recordings involving 10 African languages, including Asante Twi, Hausa, and Yoruba. The authors in [13] introduced an open-source corpus of Yoruba, a language spoken by over 22 million people. Other efforts include Lagos-NWU Yoruba corpus [14] involving 16 female and 17 male speakers, and the work by [15] involving 33 speakers. Gamayun [16] introduced multi-speaker TTS for some marginalized languages and code-switched data for four South African languages by [17]. Similarly, a TTS model was implemented in Festival for the Fon language by [18]. Although these works have explored creating TTS datasets and TTS systems for African languages, our work focuses on increasing the representation of African accents in English-based TTS systems.\"}"}
{"id": "ogun24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Afro-TTS dataset statistics. The country column indicates the ISO 3166-1 country code.\\n\\n| Country | # samples | # speakers | # accents | Duration (h) |\\n|---------|-----------|------------|-----------|--------------|\\n| NG      | 25,564    | 549        | 48        | 99.85        |\\n| KE      | 5,307     | 58         | 8         | 16.45        |\\n| ZA      | 4,279     | 125        | 20        | 16.41        |\\n| GH      | 727       | 4          | 3         | 2.28         |\\n| ZW      | 47        | 6          | 3         | 0.20         |\\n| RW      | 40        | 1          | 1         | 0.15         |\\n| SL      | 38        | 1          | 1         | 0.14         |\\n| UG      | 26        | 2          | 1         | 0.08         |\\n| ZM      | 8         | 1          | 1         | 0.04         |\\n\\n3. Methodology\\n\\n3.1. Dataset: Afro-TTS\\n\\nCurating a dataset of diverse African-accented English speakers is crucial for this work. Therefore, we initiated our research by online crowd-sourcing of data, following the methodology used in a previous work [19]. Volunteers were asked to read and record a sequence of English texts. The texts contained general domain words and were specifically enriched with African-named entities to accurately represent the diversity of African names and organizations. Table 1 shows the dataset statistics.\\n\\nThe data collection process involved 747 paid contributors from 9 countries representing 86 accents. Contributors consented to have their audio used for TTS. The 136-hour, 16-bit, 48 kHz audio files, along with their metadata containing anonymized speaker identity, country, accent, age group, gender, etc., have been open-sourced to facilitate African speech research.\\n\\n3.1.1. Dataset pre-processing\\n\\nAs the dataset was recorded remotely on different devices, it needed to be processed to be suitable for TTS training. Firstly, the speech samples were denoised using a speech enhancement model [20], which removes various background noise, including stationary and non-stationary noises, and room reverberation. The denoised samples were then passed through a bandwidth-extension model, VoiceFixer [21], to improve the quality of some of the highly degraded utterances, i.e., utterances recorded with low-resolution and clipping distortion. VoiceFixer has three audio processing modes which are suitable for different levels of degradation. To keep the audio file with the best quality per sample, we evaluated the quality of the denoised sample and the three enhanced samples using a quality estimator, WVMOS [22, 23], then we selected the sample with the highest predicted MOS score among the samples per utterance. All samples were processed at a sampling rate of 16 kHz. The speech samples were then normalized to a volume level of -27 dB using the RMS-based normalization method in ffmpeg [3] and a voice activity detection tool [24] was used to eliminate long pauses from the speech recordings.\\n\\nWe selected 736 samples covering speakers and accents with more than 20 minutes of data for testing. Finally, the remaining samples were randomly split into training (35,042) and development (200) sets. We excluded recordings with a duration of over 50 seconds or samples with character counts exceeding 400 for faster training.\\n\\nThe models and dataset can be accessed via https://huggingface.co/intronhealth/afro-tts\\n\\nAbbreviations and name titles were expanded, e.g., \u201cAlh\u201d \u2192 \u201cAlhaji\u201d, \u201cMaj\u201d \u2192 \u201cMajor\u201d and numbers were converted to their word forms using the NeMo text normalization toolkit [25]. In addition, some punctuation marks (open bracket, closed bracket, colon, and semicolon) were also expanded to their word form, as they were read out in the dataset according to annotation instructions.\\n\\n3.2. TTS models\\n\\nTwo state-of-the-art, open-source, end-to-end TTS models, VITS [1] and XTTS [26], were used in our experiments. VITS is an end-to-end model that adopts variational inference augmented with normalizing flows and an adversarial training process. XTTS is a recent multilingual TTS system with cross-language voice cloning capabilities comprising three modules; a VQ-VAE module, a GPT module, and an audio decoder. VITS (86.6 M parameters) was trained on the VCTK dataset (44 h of 109 native English speakers) for 500k iterations while XTTS (version 2, 750 M parameters) had been trained on a dataset of over 16k hours comprising 16 languages, including English. The models were fine-tuned on the training set as VITS-FT and XTTS-FT respectively. We also trained a randomly initialized VITS model from scratch as VITS-O on the training set to validate the quality of the data for TTS experiments. Lastly, we modified the VITS model as VITS-EXT to take an external 256-dimensional l2-normalized speaker embedding vector as speaker conditioning. The speaker embeddings were extracted from Resemblyzer [27], a speaker embedding extractor. The weights of incompatible layers were re-initialized during fine-tuning, e.g., the speaker embedding module.\\n\\n3.3. Speaker interpolation\\n\\nSpeaker interpolation has been typically used in Hidden Markov Model (HMM)-based TTS systems for changing speaker characteristics [28]. Interpolation can be done in several ways including interpolating between different Gaussian distributions of speakers or between speaker representations. Given speakers S1 and S2, for example, a new speaker S3 can be generated using a linear interpolation of their speaker representations:\\n\\n$$S_3 = \\\\alpha \\\\ast S_1 + (1 - \\\\alpha) \\\\ast S_2$$\\n\\n(1)\\n\\nThe interpolation ratio \\\\(\\\\alpha\\\\) enables the speaker characteristics to be changed from one speaker to another along a spectrum. Therefore, to generate more personas with African accents, we interpolated speakers with the same gender, country, and accent, using their speaker embeddings, creating over 200 additional speakers. We filtered by country because there are regional differences in the accent from different countries even for the same language, e.g., Swahili and Hausa language speakers in different countries.\\n\\n3.4. Evaluation protocol\\n\\nSeveral subjective and objective metrics were used to compare the performance of the TTS systems. For objective evaluation, we computed the overall quality (WV-MOS) and the naturalness (NISQA) [29] of the utterances using model-based quality estimators. The cosine similarity (cos-sim) of the target speaker 4https://docs.coqui.ai/en/latest/models/xtts.html Only the GPT module was fine-tuned in our experiments.\"}"}
{"id": "ogun24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"## Experiments\\n\\n### Training and finetuning hyper-parameters\\n\\nAll the VITS-like models, VITS-O, VITS-FT, VITS-EXT, were trained with mixed-precision training on 4 GPUs for 350k iterations. XTTS was fine-tuned on 1 GPU using the Coqui library.\\n\\nThe XTTS-FT model was fine-tuned at 24 kHz by upsampling the 16 kHz processed training data. In addition, the XTTS-FT model was fine-tuned at 24 kHz by using a batch size of 2 and gradient accumulation of 128. The VITS-like modifications with default finetuning hyper-parameters, using a batch size of 64. We used the VITS implementation from the authors' repository for training and inference, with the duration noise scale set to 0.8 following the original work. For speaker and accent representation, we either used the learned speaker embedding, the speaker embedding extracted from the speaker extractor model (for VITS-E), or the test utterance (for XTTS models). XTTS-based models were then trained using the speaker embedding to the reference speaker embedding was also computed.\\n\\n### Results and Discussion\\n\\nThe best model or best models with similar scores among the Trained/fine-tuned models were selected for objective and subjective evaluation. We generated 735 utterances per model, along with 162 speaker-interpolated utterances from VITS-based models. Also, 427 unique participants (64.6 % female, 60 accents, 162 speaker-interpolated utterances from VITS-based models). From 9 countries) provided 26,974 human ratings representing 102 unique speaker combinations. Also, 427 unique participants (64.6 % female, 60 accents, 162 speaker-interpolated utterances from VITS-based models) were provided across models and 858 ratings were provided for speaker-interpolated utterances. Here, we only show results for three counties where the best fine-tuned model was XTTS-FT.\\n\\n### Table 2: Objective and subjective evaluation results (with 95 % confidence interval) for pre-trained and fine-tuned TTS models\\n\\n| Model | Nat-MOS | A-MOS | Country-M | Accent-M | Gender-M | Preference |\\n|-------|---------|-------|-----------|----------|----------|------------|\\n| XTTS  | 92 \u00b1 03 | 31 \u00b1 02 | 189 \u00b1 07 | 46 \u00b1 04 | 46 \u00b1 04 | 10 \u00b1 02 |\\n| VITS-O| 92 \u00b1 03 | 31 \u00b1 02 | 189 \u00b1 07 | 46 \u00b1 04 | 46 \u00b1 04 | 10 \u00b1 02 |\\n| VITS-FT| 92 \u00b1 03 | 31 \u00b1 02 | 189 \u00b1 07 | 46 \u00b1 04 | 46 \u00b1 04 | 10 \u00b1 02 |\\n| VITS-EXT| 92 \u00b1 03 | 31 \u00b1 02 | 189 \u00b1 07 | 46 \u00b1 04 | 46 \u00b1 04 | 10 \u00b1 02 |\\n| GT denoised | 92 \u00b1 03 | 31 \u00b1 02 | 189 \u00b1 07 | 46 \u00b1 04 | 46 \u00b1 04 | 10 \u00b1 02 |\\n\\nGT denoised are the Ground truth (GT) reference test samples. The best model or best models with similar scores among the Trained/fine-tuned models were selected for objective and subjective evaluation. We generated 735 utterances per model, along with 162 speaker-interpolated utterances from VITS-based models. The XTTS-FT model was fine-tuned at 24 kHz by upsampling the 16 kHz processed training data. In addition, the XTTS-FT model was fine-tuned at 24 kHz by using a batch size of 2 and gradient accumulation of 128. The VITS-like modifications with default finetuning hyper-parameters, using a batch size of 64. We used the VITS implementation from the authors' repository for training and inference, with the duration noise scale set to 0.8 following the original work. For speaker and accent representation, we either used the learned speaker embedding, the speaker embedding extracted from the speaker extractor model (for VITS-E), or the test utterance (for XTTS models). XTTS-based models were then trained using the speaker embedding to the reference speaker embedding was also computed.\\n\\n### Table 3: Country-level MOS results (with 95 % confidence interval)\\n\\n| Country | MOS | Nat-MOS | A-MOS | Country-M | Accent-M |\\n|---------|-----|---------|-------|-----------|----------|\\n| Country-A | 92 \u00b1 03 | 31 \u00b1 02 | 189 \u00b1 07 | 46 \u00b1 04 | 46 \u00b1 04 |\\n| Country-B | 92 \u00b1 03 | 31 \u00b1 02 | 189 \u00b1 07 | 46 \u00b1 04 | 46 \u00b1 04 |\\n| Country-C | 92 \u00b1 03 | 31 \u00b1 02 | 189 \u00b1 07 | 46 \u00b1 04 | 46 \u00b1 04 |\\n| Country-D | 92 \u00b1 03 | 31 \u00b1 02 | 189 \u00b1 07 | 46 \u00b1 04 | 46 \u00b1 04 |\\n| Country-E | 92 \u00b1 03 | 31 \u00b1 02 | 189 \u00b1 07 | 46 \u00b1 04 | 46 \u00b1 04 |\\n| Country-F | 92 \u00b1 03 | 31 \u00b1 02 | 189 \u00b1 07 | 46 \u00b1 04 | 46 \u00b1 04 |\\n| Country-G | 92 \u00b1 03 | 31 \u00b1 02 | 189 \u00b1 07 | 46 \u00b1 04 | 46 \u00b1 04 |\\n| Country-H | 92 \u00b1 03 | 31 \u00b1 02 | 189 \u00b1 07 | 46 \u00b1 04 | 46 \u00b1 04 |\\n| Country-I | 92 \u00b1 03 | 31 \u00b1 02 | 189 \u00b1 07 | 46 \u00b1 04 | 46 \u00b1 04 |\\n\\nFor subjective evaluation, we performed listening tests on the generated utterances. We asked participants to evaluate the overall quality of the generated utterances, we computed the word error rate (WER) and the equal error rate (EER). Also, for intelligibility, we measured the naturalness MOS (Nat-MOS), accentedness (A-MOS), country-match (Country-M), and accent-match (Accent-M) of the generated utterances. We also computed the equal error rate (EER) of the generated utterances. For subjective evaluation, we performed listening tests on the generated utterances. Participants were shown a pair of utterances generated by two TTS models and asked to select the more natural utterance between the pair. Participants were asked to select the more natural utterance between the pair. Participants were asked to select the more natural utterance between the pair.\"}"}
{"id": "ogun24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Accent-level: Best model (XTTS-FT) results for ratings where the utterance accent is matched to the rater's accent.\\n\\n| Accent  | MOS \u00b1 Standard Deviation | NAT-MOS \u00b1 Standard Deviation | A-MOS \u00b1 Standard Deviation | %EER |\\n|---------|--------------------------|------------------------------|---------------------------|------|\\n| Afrikaans | 4.80 \u00b1 0.56 | 2.67 \u00b1 5.17 | 3.12 \u00b1 5.17 | 20.42 |\\n| Hausa | 4.23 \u00b1 0.22 | 3.93 \u00b1 0.25 | 3.93 \u00b1 0.25 | 16.20 |\\n| Igbo | 4.12 \u00b1 1.37 | 2.25 \u00b1 1.24 | 2.25 \u00b1 1.24 | 16.20 |\\n| Swahili | 3.89 \u00b1 0.56 | 3.77 \u00b1 0.57 | 3.77 \u00b1 0.57 | 16.20 |\\n| Tswana | 3.75 \u00b1 3.01 | 3.50 \u00b1 1.59 | 3.50 \u00b1 1.59 | 16.20 |\\n| Yoruba | 4.25 \u00b1 0.14 | 3.30 \u00b1 0.20 | 3.30 \u00b1 0.20 | 16.20 |\\n| Zulu | 3.09 \u00b1 0.59 | 2.88 \u00b1 0.62 | 2.88 \u00b1 0.62 | 16.20 |\\n\\n5.1. Naturalness and overall quality results\\n\\nTable 2 shows that although participants rated the pre-trained models higher in overall quality (MOS), the best fine-tuned model (XTTS-FT) was rated 1.08 MOS points higher than its pre-trained version, and 1.55 MOS points higher than the VITS baseline in terms of naturalness, probably due to the better pronunciation of African named entities in the reference text. Our results demonstrate that we are indeed able to generate speech that is more relatable to an African audience. However, model-based quality metrics (WV-MOS and NISQA) show inverse results (pre-trained models are better) possibly because underlying models lack exposure to African-sounding speech.\\n\\n5.2. Accentedness and speaker similarity results\\n\\nMost significantly in Table 2, participants rated XTTS-FT only 0.14 MOS points lower in Accentedness (Accent-MOS) than the GT in contrast to the XTTS baseline which was rated 2.18 MOS points below the GT. This validates that our approach generates natural-sounding accented speech, bridging the current gap in the representation of African voices in speech synthesis. Speaker similarity results (cos-sim) also showed that generated utterances from fine-tuned models are closer to reference utterances than generated utterances from pre-trained models.\\n\\n5.3. Preference scores\\n\\nPreference test scores in Table 2 show that raters prefer utterances generated by XTTS-FT. Preference test scores aligned well with MOS metrics where the XTTS-FT model outperformed the VITS-based models. Also, the pre-trained XTTS model had a higher MOS score on average than the pre-trained VITS model likely because of its multilingual pretraining.\\n\\n5.4. Intelligibility\\n\\nUtterances by XTTS models had lower WER than VITS-based models perhaps as a result of the greater diversity and quantity of its pretraining data. A higher WER after fine-tuning of XTTS was also due to noise artifacts in the Afro-TTS dataset.\\n\\n5.5. Regional diversity considerations\\n\\nTable 3 reveals that although the naturalness and accentedness of generated utterances from our best model are close to the GT, regional differences surface. South Africans (ZA) rated South African-generated utterances lower in Accent-Match than West Africans (NG) rated the generated utterances with West African accents. Although most participants agreed that the generated utterances represent the reference country from Table 4, the generated accents do not always match the reference accent, e.g., generated speech in Afrikaans accent may sound like Zulu, and Igbo generated accent may sound like Yoruba.\\n\\nTable 5: Speaker interpolation results showing MOS, naturalness (Nat-MOS), and accentedness (A-MOS) of utterances generated using novel speakers from speaker interpolation.\\n\\n| Model        | MOS \u00b1 Standard Deviation | Nat-MOS \u00b1 Standard Deviation | A-MOS \u00b1 Standard Deviation | %EER |\\n|--------------|--------------------------|------------------------------|---------------------------|------|\\n| VITS-EXT     | 3.17 \u00b1 0.16              | 4.05 \u00b1 0.15                 | 4.10 \u00b1 0.14              | 20.42 |\\n| VITS-FT      | 3.47 \u00b1 0.15              | 4.22 \u00b1 0.14                 | 4.37 \u00b1 0.13              | 16.20 |\\n| VITS-O       | 3.18 \u00b1 0.17              | 4.08 \u00b1 0.16                 | 4.20 \u00b1 0.15              | 16.20 |\\n\\nTable 6: Overall speaker interpolation MOS results showing how much synthetic utterances from interpolated speakers match the expected accent, country, and gender of the source speakers. Accent-match (Accent-M), gender-match (Gender-M), and country-match (Country-M) are provided.\\n\\n| Country | Accent-M \u00b1 Standard Deviation | Gender-M \u00b1 Standard Deviation | Country-M \u00b1 Standard Deviation | %EER |\\n|---------|-------------------------------|-------------------------------|-------------------------------|------|\\n| KE      | 4.17 \u00b1 0.21                   | 4.79 \u00b1 0.13                   | 4.20 \u00b1 0.13                   | 20.42 |\\n| NG      | 3.65 \u00b1 0.13                   | 4.62 \u00b1 0.08                   | 3.91 \u00b1 0.13                   | 16.20 |\\n| ZA      | 3.47 \u00b1 0.19                   | 4.65 \u00b1 0.09                   | 3.64 \u00b1 0.18                   | 16.20 |\\n\\nIndeed, in multilingual countries like NG and ZA, speaker accents are difficult to classify into binary accent classes [32], as many speakers have dual accents. Notably, although East African speakers have lower representation in the dataset compared to Southern Africans, East Africans (e.g., Swahili) generally rated Accent-Match higher than South Africans (e.g., Zulu, Afrikaans). These inconsistencies may reflect the accent imbalance in the Afro-TTS dataset and require further investigation.\\n\\n5.6. Effects of speaker interpolation\\n\\nTable 5 shows MOS results on speaker-interpolated utterances from fine-tuned VITS models. Although %EER shows interpolated speakers have a high correlation with source speakers, our results show that speaker interpolation is indeed a viable approach for creating novel synthetic speakers that sound African (i.e., natural and accented). Furthermore, Table 6 shows that the generated utterances' gender, accent, and country match that of the reference interpolated speakers. In the future, speaker interpolation outside of the same accents could facilitate the exploration of novel or multilingual accents.\\n\\n6. Limitations\\n\\nAlthough we included 86 distinct African accents, this is a small fraction of more than 3000 languages and accents across the continent. Additionally, imbalanced accent representation in our dataset may yield biased performance favoring majority accents. Lastly, we acknowledge the privacy risk of releasing multi-speaker TTS systems that mimic voices in the source data increasing the risk of voice cloning or voice theft. We mitigate this by removing any speaker identifiers, making it more challenging to identify individuals. Finally, disentangling of speaker and accent characteristics is left for future work.\\n\\n7. Conclusion\\n\\nWe developed an African-accented TTS system that achieves near-GT MOS for naturalness and accentedness using the pan-African TTS dataset, a 136-hour dataset containing 747 speakers with 86 African accents from 9 countries. Although open questions remain, our work greatly improves the representation of African voices in speech synthesis.\"}"}
