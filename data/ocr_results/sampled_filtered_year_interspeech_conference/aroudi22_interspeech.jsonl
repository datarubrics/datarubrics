{"id": "aroudi22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"TRUNet: Transformer-Recurrent-U Network for End-to-end Multi-channel Reverberant Sound Source Separation\\n\\nAli Aroudi, Stefan Uhlich, Marc Ferras Font\\nSony Europe B.V., Stuttgart, Germany\\n\\nAbstract\\nIn recent years, many deep learning techniques for single-channel sound source separation have been proposed using recurrent, convolutional and transformer networks. When multiple microphones are available, spatial diversity between speakers and background noise in addition to spectro-temporal diversity can be exploited by using multi-channel filters for sound source separation. Aiming at end-to-end multi-channel source separation, in this paper we propose a transformer-recurrent-U network (TRUNet), which directly estimates multi-channel filters from multi-channel input spectra. TRUNet consists of a spatial processing network with an attention mechanism across microphone channels aiming at capturing the spatial diversity, and a spectro-temporal processing network aiming at capturing spectral and temporal diversities. In addition to multi-channel filters, we also consider estimating single-channel filters from multi-channel input spectra using TRUNet. We train the network on a large reverberant dataset using a proposed combined compressed mean-squared error loss function, which further improves the sound separation performance. We evaluate the network on a realistic and challenging reverberant dataset, generated from measured room impulse responses of an actual microphone array. The experimental results on realistic reverberant sound source separation show that the proposed TRUNet outperforms state-of-the-art single-channel and multi-channel source separation methods.\\n\\nIndex Terms: sound source separation, deep learning, transformers, multi-channel filtering, spatial filtering\\n\\n1. Introduction\\nSpeech signals captured by microphones placed at a distance from speakers are often corrupted with various undesired acoustic sources, such as interfering speakers, reverberation and ambient noise, which lead to a decreased speech quality and intelligibility. In recent years, aiming at separating out the speakers from the microphone signals and reduce background noise, sound source separation techniques based on deep learning have been proposed. Sound source separation techniques can be broadly categorized into single-channel and multi-channel methods, based on the number of microphones which are used. Single-channel source separation methods typically exploit spectro-temporal diversity between the speech and the noise signals [1 \u20138]. These methods typically perform source separation by estimating masks corresponding to each sound source using convolutional, recurrent or transformer networks. To improve the source separation performance, these methods also aim to learn short-term and long-term temporal dependencies of speech signals by neural structures, which have large receptive fields [2, 3], deep and wide recurrent layers [1, 5], or dual path architectures using recurrent or transformer layers [6\u20138].\\n\\nWhen multiple microphones are available, multi-channel filters allow to exploit the spatial diversity between the speakers and the background noise in addition to the spectro-temporal diversity [9\u201315]. Multi-channel filters, also often referred to as spatial filters and beamformers [9, 16], perform source separation by linearly filtering and summing the microphone signals. Conventional multi-channel filters are typically estimated based on a linear optimization problem and require estimates of certain parameters, e.g., covariance matrices, direction-of-arrivals (DOAs) or steering vectors of sound sources [9,16]. These parameters can be estimated based on masks obtained by, e.g., single-channel neural-network-based source separation techniques [10, 12, 15], or can be estimated directly from microphone signals using neural networks in an end-to-end fashion, as proposed in a DOA-driven beamforming network (DBNet) [11].\\n\\nInstead of formulating the multi-channel filter as a linear optimization problem, it has been recently proposed to directly estimate multi-channel filters by a generalized recurrent beamformer (GRNN-BF) network [15], learning a non-linear optimization solution. GRNN-BF network is able to estimate multi-channel filters from microphone signals, but also relies on a camera input, which may not be available in many applications. Aiming at end-to-end source separation by directly estimating multi-channel filters from only microphone signals, we propose a transformer-recurrent-U network (TRUNet) in this paper. To draw valid conclusions on reverberant sound source separation, we evaluate the proposed network on a challenging and realistic reverberant dataset, generated from measured room impulse responses of an actual microphone array.\\n\\nThe proposed TRUNet is depicted in Fig. 1. TRUNet consists of a spatial processing unit using a transformer network (TNet), a spectro-temporal processing unit using a recurrent-U network recurrent convolutional network with U structure (RUNet), and an inverse short-time Fourier transform (iSTFT) layer. First, it accepts spectra of the multi-channel signals as input. Then, the multi-channel filters are estimated by the spatial processing unit and the spectro-temporal processing unit in an end-to-end fashion. For the spatial processing unit, since capturing the spatial diversity is not straightforward, we propose three transformer network architectures operating across microphone channels. For the spectro-temporal processing unit, we adopt a multi-channel RUNetto efficiently capture spectral and temporal dependencies corresponding to each speaker. To separate out speakers the multi-channel filters, which are complex-valued and time-varying, are applied to the multi-channel input spectra. In addition to multi-channel filters, we also consider estimating single-channel filters that can still benefit from a multi-channel spectro-temporal designed filter. Finally, the separated sources are transformed to the time domain using theiSTFT layer, which enforces short-time Fourier transform (STFT) consistency in the network [4,17].\\n\\nWe train the proposed network to separate out speakers from noisy and reverberant speech mixtures. The reverberant speech signals in the mixtures can be thought as containing early reflections, which are beneficial for speech intelligibility and naturalness, and a late reverbera-\"}"}
{"id": "aroudi22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A function, which is known to have a detrimental effect on speech quality and intelligibility [18]. Therefore, we aim at separating out aation component, which is known to have a detrimental effect on speech quality and intelligibility [18]. Therefore, we aim at separating out a\\n\\nWe consider an acoustic scenario comprising two competing speakers where\\n\\nwhere\\n\\nand\\n\\nwork. The multi-channel filtering on\\n\\nThe spatial transformer blocks accept spectra as the input and have a\\n\\nThe proposed spatial processing unit is a TNet, which consists of\\n\\nvalues\\n\\nz\\n\\nin (3) could be seen as a similar way as covariance matrices in\\n\\nfollowed by a softmax function, as\\n\\ncomputed by (real-valued) dot products of the queries and the keys,\\n\\nperformed by weighting the sum of the values, where the weights are\\n\\non the sub-spaces in parallel with a head embedding dimension\\n\\n heads [20], and\\n\\ninter-channel information. For spatial transformer blocks we adopt\\n\\nmagnitude and the phase of the input spectra. To direct the attention\\n\\ntransformers proposed for language translation tasks in [20]. The\\n\\ninter-channel information. For spatial transformer blocks we adopt\\n\\nspatial attention function with an output, a representation incorporating\\n\\nspatial transformer blocks and operates across microphone channels.\\n\\n2.1. Spatial processing unit using TNet\\n\\nThe spatial processing network and the spectro-temporal processing net-\\n\\nthe speakers, the network estimates two complex-valued filters, using\\n\\nspectra of the multi-channel signals as input. Aiming at separating out\\n\\nand background noise in a reverberant environment. TRUNet accepts\\n\\nmulti-channel audio such as a large number of speakers, various noise\\n\\nnetwork on a large dataset accounting for the crucial aspects of realistic\\n\\nseparation [11]. We also explore the impact of the exponent factor on\\n\\nerrors [17,19], which was found to be superior to other losses for source\\n\\nan exponent factor to balance the optimization of small and large MSE\\n\\nthe complex mean-squared error (MSE) loss function compressed with\\n\\nlow-reverberant speech signal preserving the early reflections. We use\\n\\nquality and intelligibility [18]. Therefore, we aim at separating out a\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is known to have a detrimental effect on speech\\n\\ntion component, which is"}
{"id": "aroudi22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Motivated by the results in [17] for speech enhancement, the residual addition, a residual connection linking the BLSTM layers was used. Aiming at a better network training while avoiding vanishing gradients, residual connections are used which link flattened along the channels. Aiming at a better network training while decreasing mirrored in the decoder. The encoder and decoder layers with kernels of size(6,6), aiming to deal with reverberation, and a stride of(1,2) in time and frequency dimensions. All convolutional layers across layers $l \\\\in \\\\{1, \\\\ldots, 5\\\\}$ are followed by leaky ReLU activations. The encoder and decoder connections are implemented as convolutions with $5$ channels and a tanh activation to estimate multi-channel filters of both sources. In order to estimate single-channel and multi-channel filters, we consider to estimate single-channel filters using a similar network, however, with an extra convolutional layer after the last decoder layer with the output channel $1$.\\n\\nWe also considered $N = 6$ and $8$, but no significant performance improvement was observed.\\n\\nWe consider a number of end-to-end methods in our experiments: 1) $\\\\text{DBNet}$, consisting of a network-based DOA estimator and conventional spatial filters. We observe that TNets result in larger SDR and SIR improvements compared to $\\\\text{DBnet}$, indicating TNets are better able to spatial filter. The largest SIR improvement, $\\\\Delta \\\\text{SIR}$ of $29$ dB, is obtained using $\\\\text{TNet\u2013Cat}$ compared with $\\\\text{DBnet}$, which consists of a network-based DOA estimation and conventional spatial filtering [11]. 2) $\\\\text{eDBNet}$, similar to the proposed TRUNet, but without a TNet spatial processing unit. 3) $\\\\text{maskDaulPathTNet}$ followed by $\\\\text{maskRUNet}$ as proposed in RUNet. In Table 1 the source separation performance of the networks which incorporate only a TNet spatial processing unit and a RUNet spectro-temporal processing unit, and benchmark it against multi-channel and single-channel baseline methods.\\n\\nIn Section 4.1, we investigate the source separation performance of the proposed networks in terms of the scale-invariant signal-to-distortion ratio (SIR) and the signal-to-noise ratio (SIR) of BSSEval [26]. In Section 4.2, we investigate the performance of the proposed TRUNet which incorporates both TNet spatial processing unit and RUNet spectro-temporal processing unit. In Table 1, $\\\\Delta \\\\text{SIR}$ indicates how well the speakers are separated, is obtained by\\n\\n$$\\\\Delta \\\\text{SIR} = \\\\frac{\\\\text{SIR}_{\\\\text{target}} - \\\\text{SIR}_{\\\\text{noisy}}}{\\\\text{SIR}_{\\\\text{noisy}}} \\\\times 100\\\\%$$\\n\\nwhere $\\\\text{SIR}_{\\\\text{target}}$ and $\\\\text{SIR}_{\\\\text{noisy}}$ denote the SIR of the target and separated speech signals, respectively. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we consider compressed MSE loss function $\\\\text{cMSE}$ with an exponent factor $2.3$. For training the networks we"}
{"id": "aroudi22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Comparison of TRUNet with multi-channel baseline methods.\\n\\n* indicates methods accepting multi-channel inputs, but using single-channel complex-valued filtering.\\n\\n| Method            | \u25b3SDR  | \u25b3SIR  | \u25b3PESQ | Model Size [M] |\\n|-------------------|-------|-------|-------|----------------|\\n| Multi-channel Baseline |       |       |       |                |\\n| DBNet [11]        | 5.65  | 0.05  | 0.01  | 14             |\\n| eDBNet [11]       | 5.50  | 0.31  | 0.20  | 67             |\\n| RUNet             | 4.75  | 0.82  | 0.14  | 54             |\\n| RUNet*            | 4.44  | 0.35  | 0.21  | 54             |\\n| TNets             |       |       |       |                |\\n| TNet\u2013Cat          | 4.22  | 1.14  | 0.02  | 16             |\\n| TNet\u2013RealImag     | 4.24  | 0.83  | 0.02  | 16             |\\n| TNet\u2013MagPhase     | 4.18  | 0.55  | 0.02  | 16             |\\n| TRUNets           |       |       |       |                |\\n| TRUNet\u2013Cat        | 4.87  | 0.98  | 0.13  | 31             |\\n| TRUNet\u2013Cat*       | 4.05  | 1.51  | 0.22  | 31             |\\n| TRUNet\u2013RealImag*  | 4.12  | 1.08  | 0.15  | 29             |\\n| TRUNet\u2013MagPhase*  | 4.38  | 1.27  | 0.22  | 29             |\\n\\nTable 3: Comparison of TRUNet\u2013MagPhase* with single-channel baseline methods.\\n\\n| Method                        | \u25b3SDR  | \u25b3SIR  | \u25b3PESQ | Model Size [M] |\\n|-------------------------------|-------|-------|-------|----------------|\\n| maskRUNet [11]                | 9.57  | 0.05  | 0.07  | 52             |\\n| maskDualPathTNet [8]          | 9.36  | 1.36  | 0.10  | 11             |\\n| maskDualPathTRUNet            | 9.81  | 1.16  | 0.10  | 18             |\\n| TRUNet\u2013MagPhase*              | 9.38  | 1.27  | 0.22  | 29             |\\n\\nand keys of TNet\u2013MagPhase are computed from the complex-valued spectra, from which spatial information are straightforward to extract, and values of TNet\u2013MagPhase are computed from the magnitude and the phase, which directly has the spatial information between channels.\\n\\nNevertheless, the improvement of TNets is limited particularly for the SIR of about 2.55 dB, which can be mainly attributed to the limited capability of TNets to efficiently exploit spectral and temporal diversities in addition to the spatial diversity. We also observe that a lower number of attention heads consistently results in a larger SIR. A lower number of attention heads results in more global attention across spectrum sub-spaces as well as larger embedding size. We focus from now on systems using with 16 heads, as they outperform the other settings.\\n\\n4.2. TRUNet source separation performance\\n\\nTable 2 shows the source separation performance of TRUNets, incorporating a TNet spatial processing unit and a RUNet spectro-temporal processing unit, versus the multi-channel baseline methods and the networks incorporating only TNets. Please note that TRUNets with * indicate the methods accepting multi-channel inputs, but using single-channel complex-valued filtering. We observe that TRUNets result in larger performance measures, particularly for the SIR and the PESQ improvements, compared to TNets, indicating the importance of both spatial processing and spectro-temporal processing units for source separation performance. We also observe that only some TRUNets (TRUNet\u2013Cat*, TRUNet\u2013RealImag*, TRUNet\u2013MagPhase*) result in a larger SDR improvement (about 9.0 \u2212 9.3 dB) and a larger SIR improvement (about 11.08 \u2212 12.8 dB) compared to the multi-channel baseline methods. Therefore, we investigate the main factors contributing to the performance measure improvement of TRUNets in the remainder.\\n\\nWe observe that TRUNet\u2013Cat*, which uses single-channel filtering, compared to TRUNet-Cat, which uses multi-channel filtering, with a gain of 5.5 dB for the SIR improvement and a gain of 0.09 for the PESQ improvement. This may imply that, for the considered networks, single-channel filtering is sufficient and summing all microphone spectra after filtering may be unnecessary. Among the TRUNets with single-channel filtering, the network using the spectral phase and the spectral magnitude (TRUNet\u2013MagPhase*) obtains the best SDR improvement of 9.38 and SIR improvement of 12.87.\\n\\nFigure 3: SDR and SIR improvement for the compressed loss function $L_{cMSE}(c)$ and $L_{comb}(c, \\\\alpha)$ when using TRUNet\u2013MagPhase*. The blue data points represent the SDR improvement and the black data points represent the SIR improvement.\\n\\nIn addition, TRUNet\u2013MagPhase* yields the largest performance measures even compared to all other multi-channel methods. We further compare the source separation performance of TRUNet\u2013MagPhase* with the single-channel baseline methods (see Table 3). We observe that although all considered methods result in a similar SDR improvement of about 9.60 \u2212 9.81 dB, the proposed TRUNet\u2013MagPhase* stands out with a larger SIR improvement of 12.87 and a PESQ improvement of 0.22.\\n\\nThe better SIR improvement of TRUNet-MagPhase method indicates that the proposed method is able to better separate speakers while the similar SDR improvement might imply that the method may not be competitive in terms of ISR and SAR, which have not been investigated in this paper.\\n\\nFinally, we explore the impact of the exponent factor on the source separation performance of TRUNet\u2013MagPhase*. Figure 3 depicts SDR and SIR improvements, when the compressed loss function $L_{cMSE}(c)$ is used. Smaller exponent factors are shown to obtain a larger SIR improvement (about 5.6 to 13.3 dB) and a smaller SDR improvement (about 10.5 to 8.3 dB), compared to large factors. In order to achieve large improvements for both SDR and SIR, we finally experiment with linearly combining two $cMSE$ losses with complementary exponent factors, i.e. $L_{comb}(c, \\\\alpha) = \\\\alpha L_{cMSE}(c) + (1 - \\\\alpha) L_{cMSE}(1 - c)$.\\n\\nWhen combining the compressed loss functions using $c = 0.3$ and the combination factor $\\\\alpha = 0.7$, we obtain additional improvements resulting in final SDR and SIR of 9.70 and 13.40, respectively.\\n\\n5. Conclusion\\n\\nWe proposed an end-to-end multi-channel source separation network that directly estimates multi-channel filters from multi-channel input spectra. The network consists of a spatial processing unit using transformers and a spectro-temporal processing unit using a recurrent U-structured convolutional network. In addition to multi-channel filters, we also consider estimating single-channel filters from multi-channel input spectra using TRUNet. We trained the network using a proposed combined $cMSE$ loss function on a large reverberant dataset, and tested on realistic data using measured RIRs from an actual microphone array. The experimental results show that both proposed spatial and spectro-temporal processing units are crucial to obtain competitive performance. In particular, the results show that the proposed transformer-based spatial processing unit is better able to perform spatial filtering compared to networks using conventional spatial filtering. Moreover, the results show that our proposed architecture TRUNet achieves larger separation performance with single-channel filtering than multi-channel filtering, even larger than the performance obtained by the state-of-the-art source separation methods.\\n\\n$L_{cMSE}(c)$ with $c = 0.1$ results in an unstable network training.\"}"}
{"id": "aroudi22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] M. Kolb\u00e6k, D. Yu, Z. Tan, and J. Jensen, \u201cMulti-talker speech separation with utterance-level permutation invariant training of deep recurrent neural networks,\u201d IEEE/ACM Trans. Audio, Speech, Language Process., vol. 25, no. 10, pp. 1901\u20131913, Oct. 2017.\\n\\n[2] Y. Luo and N. Mesgarani, \u201cConv-tasnet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\u201d IEEE/ACM Trans. Audio, Speech, Language Process., vol. 27, no. 8, pp. 1256\u20131266, Aug. 2019.\\n\\n[3] Z. Shi, H. Lin, L. Liu, R. Liu, S. Hayakawa, S. Harada, and J. Han, \u201cFurcaNet: An end-to-end deep gated convolutional, long short-term memory, deep neural networks for single channel speech separation,\u201d arXiv preprint, 2019.\\n\\n[4] S. Wisdom, J. R. Hershey, K. Wilson, J. Thorpe, M. Chinen, B. Patton, and R. A. Saurous, \u201cDifferentiable consistency constraints for improved deep speech enhancement,\u201d in ICASSP, May 2019, pp. 900\u2013904.\\n\\n[5] M. Maciejewski, G. Wichern, E. McQuinn, and J. L. Roux, \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in ICASSP, 2020, pp. 696\u2013700.\\n\\n[6] Y. Luo, Z. Chen, and T. Yoshioka, \u201cDual-path RNN: Efficient long sequence modeling for time-domain single-channel speech separation,\u201d in ICASSP, May 2020, pp. 46\u201350.\\n\\n[7] K. Wang, B. He, and W.-P. Zhu, \u201cTstnn: Two-stage transformer based neural network for speech enhancement in the time domain,\u201d in ICASSP, June 2021, pp. 7098\u20137102.\\n\\n[8] C. Subakan, M. Ravanelli, S. Cornell, M. Bronzi, and J. Zhong, \u201cAttention is all you need in speech separation,\u201d in ICASSP, Jun. 2021, pp. 21\u201325.\\n\\n[9] S. Gannot, E. Vincent, S. Markovich-Golan, and A. Ozerov, \u201cA consolidated perspective on multimicrophone speech enhancement and source separation,\u201d IEEE/ACM Trans. Audio, Speech, Language Process., vol. 25, no. 4, pp. 692\u2013730, 2017.\\n\\n[10] T. Ochiai, M. Delcroix, R. Ikeshita, K. Kinoshita, T. Nakatani, and S. Araki, \u201cBeam-TasNet: Time-domain audio separation network meets frequency-domain beamformer,\u201d in ICASSP, 2020, pp. 6384\u20136388.\\n\\n[11] A. Aroudi and S. Braun, \u201cDBNet: DOA-driven beamforming network for end-to-end reverberant sound source separation,\u201d in ICASSP, June 2021, pp. 211\u2013215.\\n\\n[12] T. Nakatani, R. Ikeshita, K. Kinoshita, H. Sawada, and S. Araki, \u201cBlind and neural network-guided convolutional beamformer for joint denoising, dereverberation, and source separation,\u201d in ICASSP, Jun. 2021, pp. 6129\u20136133.\\n\\n[13] Z. Zhang, Y. Xu, M. Yu, S.-X. Zhang, L. Chen, D. S. Williamson, and D. Yu, \u201cMulti-channel multi-frame ADL-MVDR for target speech separation,\u201d IEEE/ACM Trans. Audio, Speech, Language Process., vol. 29, pp. 3526\u20133540, Nov. 2021.\\n\\n[14] Z.-Q. Wang, P. Wang, and D. Wang, \u201cMulti-microphone complex spectral mapping for utterance-wise and continuous speech separation,\u201d IEEE/ACM Trans. Audio, Speech, Language Process., vol. 29, pp. 2001\u20132014, May 2021.\\n\\n[15] Y. Xu, Z. Zhang, and D. Yu, Shi-Xiong Zhang, \u201cGeneralized spatio-temporal rnn beamformer for target speech separation,\u201d in INTERSPEECH, Brno, Czechia, Sep. 2021, pp. 3076\u20133080.\\n\\n[16] S. Doclo, W. Kellermann, S. Makino, and S. E. Nordholm, \u201cMultichannel signal enhancement algorithms for assisted listening devices,\u201d IEEE Signal Process. Magazine, vol. 32, no. 2, pp. 18\u201330, Mar. 2015.\\n\\n[17] S. Braun, H. Gamper, C. K. Reddy, and I. Tashev, \u201cTowards efficient models for real-time deep noise suppression,\u201d in ICASSP, June 2021, pp. 656\u2013660.\\n\\n[18] A. Warzybok, J. Rennies, T. Brand, S. Doclo, and B. Kollmeier, \u201cEffects of spatial and temporal integration of a single early reflection on speech intelligibility,\u201d The Journal of the Acoustical Society of America, vol. 133, no. 1, pp. 269\u2013282, Jan 2013.\\n\\n[19] A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W. T. Freeman, and M. Rubinstein, \u201cLooking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation,\u201d ACM Trans. Graph., vol. 37, no. 4, Jul. 2018.\\n\\n[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in NIPS. Red Hook, NY, USA: Curran Associates Inc., 2017, p. 6000\u20136010.\\n\\n[21] K. Tan, X. Zhang, and D. Wang, \u201cReal-time speech enhancement using an efficient convolutional recurrent network for dual-microphone mobile phones in close-talk scenarios,\u201d in ICASSP, 2019, pp. 5751\u20135755.\\n\\n[22] K. Wilson, M. Chinen, J. Thorpe, B. Patton, J. Hershey, R. A. Saurous, J. Skoglund, and R. F. Lyon, \u201cExploring tradeoffs in models for low-latency speech enhancement,\u201d in IWAENC, 2018, pp. 366\u2013370.\\n\\n[23] \u201cIEEE ICASSP 2021 Deep Noise Suppression (DNS) Challenge,\u201d https://github.com/microsoft/DNS-Challenge.\\n\\n[24] J. Yamagishi, C. Veaux, and K. MacDonald, \u201cCSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92), [sound],\u201d in University of Edinburgh. The Centre for Speech Technology Research, 2019.\\n\\n[25] \u201cDevice and produced speech (DAPS) dataset,\u201d https://ccrma.stanford.edu/~gautham/Site/daps.html.\\n\\n[26] G. Wichern, J. Antognini, M. Flynn, L. R. Zhu, E. McQuinn, D. Crow, J. Manilow, and J. Le Roux, \u201cWHAM!: Extending speech separation to noisy environments,\u201d in INTERSPEECH, Sep. 2019.\\n\\n[27] D. Dean, S. Sridharan, R. Vogt, and M. Mason, \u201cThe qut-noise-timit corpus for evaluation of voice activity detection algorithms,\u201d in Proceedings of the Annual Conference of the International Speech Communication Association, 2010, pp. 3110\u20133113.\\n\\n[28] M. Ferras, S. R. Madikeri, P. Motl\u00b4\u0131cek, S. Dey, and H. Bourlard, \u201cA large-scale open-source acoustic simulator for speaker recognition,\u201d IEEE Signal Process. Lett., vol. 23, no. 4, pp. 527\u2013531, 2016.\\n\\n[29] J. B. Allen and D. A. Berkley, \u201cImage method for efficiently simulating small-room acoustics,\u201d The Journal of the Acoustical Society of America, vol. 65, no. 4, pp. 943\u2013950, Apr. 1979.\\n\\n[30] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint, 2014.\"}"}
