{"id": "yang22e_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] D. Yang, H. Wang, Y. Zou, and C. Weng, \\\"Detect what you want: Target sound detection,\\\" arXiv preprint arXiv:2112.10153, 2021.\\n\\n[2] J. P. Bello, C. Silva, O. Nov, R. DuBois, A. Arora, J. Salamon, C. Mydlarz, and H. Doraiswamy, \\\"Sonyc: A system for the monitoring, analysis and mitigation of urban noise pollution,\\\" arXiv preprint arXiv:1805.00889, 2018.\\n\\n[3] S. Hershey, S. Chaudhuri, D. Ellis, J. Gemmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold et al., \\\"CNN architectures for large-scale audio classification,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 131\u2013135.\\n\\n[4] H. Dinkel, M. Wu, and K. Yu, \\\"Towards duration robust weakly supervised sound event detection,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 887\u2013900, 2021.\\n\\n[5] L. Lin, X. Wang, H. Liu, and Y. Qian, \\\"Specialized decision surface and disentangled feature for weakly-supervised polyphonic sound event detection,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 1466\u20131478, 2020.\\n\\n[6] Q. Kong, Y. Xu, W. Wang, and M. D. Plumbley, \\\"Sound event detection of weakly labelled data with cnn-transformer and automatic threshold optimization,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2450\u20132460, 2020.\\n\\n[7] A. Mesaros, T. Heittola, T. Virtanen, and M. D. Plumbley, \\\"Sound event detection: A tutorial,\\\" IEEE Signal Processing Magazine, vol. 38, no. 5, pp. 67\u201383, 2021.\\n\\n[8] Y. Wang, J. Li, and F. Metze, \\\"A comparison of five multiple instance learning pooling functions for sound event detection with weak labeling,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 31\u201335.\\n\\n[9] I. Mart\u00edn-Morat\u00f3, A. Mesaros, T. Heittola, T. Virtanen, M. Cobos, and F. Ferri, \\\"Sound event envelope estimation in polyphonic mixtures,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 935\u2013939.\\n\\n[10] Q. Wang, H. Muckenhirn, K. Wilson, P. Sridhar, Z. Wu, J. R. Hershey, R. A. Saurous, R. J. Weiss, Y. Jia, and I. L. Moreno, \\\"Voice-filter: Targeted voice separation by speaker-conditioned spectrogram masking,\\\" Proc. Interspeech, pp. 2728\u20132732, 2019.\\n\\n[11] M. Ge, C. Xu, L. Wang, E. S. Chng, J. Dang, and H. Li, \\\"Multi-stage speaker extraction with utterance and frame-level reference signals,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6109\u20136113.\\n\\n[12] K. \u017dmol\u00edkov\u00e1, M. Delcroix, K. Kinoshita, T. Ochiai, T. Nakatani, L. Burget, and J. \u010cernock\u00fd, \\\"Speakerbeam: Speaker aware neural network for target speaker extraction in speech mixtures,\\\" IEEE Journal of Selected Topics in Signal Processing, vol. 13, no. 4, pp. 800\u2013814, 2019.\\n\\n[13] M. Borsdorf, C. Xu, H. Li, and T. Schultz, \\\"Universal speaker extraction in the presence and absence of target speakers for speech of one and two talkers,\\\" in Proc. Interspeech, 2021, pp. 1469\u20131473.\\n\\n[14] Z. Pan, M. Ge, and H. Li, \\\"Usev: Universal speaker extraction with visual cue,\\\" arXiv preprint arXiv:2109.14831, 2021.\\n\\n[15] T. Ochiai, M. Delcroix, Y. Koizumi, H. Ito, K. Kinoshita, and S. Araki, \\\"Listen to what you want: Neural network-based universal sound selector,\\\" Proc. Interspeech, pp. 1441\u20131445, 2020.\\n\\n[16] M. Delcroix, J. B. V\u00e1zquez, T. Ochiai, K. Kinoshita, and S. Araki, \\\"Few-shot learning of new sound classes for target sound extraction,\\\" arXiv preprint arXiv:2106.07144, 2021.\\n\\n[17] Y. Okamoto, S. Horiguchi, M. Yamamoto, K. Imoto, and Y. Kawaguchi, \\\"Environmental sound extraction using onomatopoeia,\\\" arXiv preprint arXiv:2112.00209, 2021.\\n\\n[18] J. Salamon, C. Jacoby, and J. P. Bello, \\\"A dataset and taxonomy for urban sound research,\\\" in Proceedings of the 22nd ACM International Conference on Multimedia, 2014, pp. 1041\u20131044.\\n\\n[19] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. Plumbley, \\\"Panns: Large-scale pretrained audio neural networks for audio pattern recognition,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2880\u20132894, 2020.\\n\\n[20] H. Wang, Y. Zou, D. Chong, and W. Wang, \\\"Environmental sound classification with parallel temporal-spectral attention,\\\" in Proc. Interspeech, 2020, pp. 821\u2013825.\\n\\n[21] Q. Kong, C. Yu, Y. Xu, T. Iqbal, W. Wang, and M. D. Plumbley, \\\"Weakly labelled audioset tagging with attention neural networks,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 11, pp. 1791\u20131802, 2019.\\n\\n[22] Y. Wang, J. Salamon, N. J. Bryan, and J. P. Bello, \\\"Few-shot sound event detection,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 81\u201385.\\n\\n[23] D. Yang, H. Wang, Y. Zou, Z. Ye, and W. Wang, \\\"A mutual learning framework for few-shot sound event detection,\\\" arXiv preprint arXiv:2110.04474, 2021.\\n\\n[24] Y. Xian, Y. Sun, W. Wang, and S. M. Naqvi, \\\"Multi-scale residual convolutional encoder decoder with bidirectional long short-term memory for single channel speech enhancement,\\\" in IEEE European Signal Processing Conference (EUSIPCO). IEEE, 2021, pp. 431\u2013435.\\n\\n[25] Y. Xu, Q. Kong, W. Wang, and M. D. Plumbley, \\\"Large-scale weakly supervised audio classification using gated convolutional neural network,\\\" in International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 121\u2013125.\\n\\n[26] V. Nair and G. Hinton, \\\"Rectified linear units improve restricted boltzmann machines,\\\" in ICML, 2010.\\n\\n[27] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00e1r, \\\"Focal loss for dense object detection,\\\" in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 2980\u20132988.\\n\\n[28] J. Gemmeke, D. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, \\\"Audio set: An ontology and human-labeled dataset for audio events,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 776\u2013780.\\n\\n[29] S. Hershey, D. P. Ellis, E. Fonseca, A. Jansen, C. Liu, R. C. Moore, and M. Plakal, \\\"The benefit of temporally-strong labels in audio event classification,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 366\u2013370.\\n\\n[30] A. Mesaros, T. Heittola, and T. Virtanen, \\\"Metrics for polyphonic sound event detection,\\\" Applied Sciences, vol. 6, no. 6, p. 162, 2016.\\n\\n[31] D. P. Kingma and J. Ba, \\\"Adam: A method for stochastic optimization,\\\" in International Conference for Learning Representations (ICML), 2015.\"}"}
{"id": "yang22e_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"RaDur: A Reference-aware and Duration-robust Network for Target Sound Detection\\n\\nDongchao Yang\u2020, Helin Wang\u2020, Zhongjie Ye, Yuexian Zou\u2217, Wenwu Wang\\n\\n1ADSPLAB, School of ECE, Peking University, Shenzhen, China\\n2Center for Vision, Speech and Signal Processing, University of Surrey, UK\\n\\n{dongchao98,zhongjieye}@stu.pku.edu.cn, {wanghl15,zouyx}@pku.edu.cn, w.wang@surrey.ac.uk\\n\\nAbstract\\nTarget sound detection (TSD) aims to detect the target sound from a mixture audio given the reference information. Previous methods use a conditional network to extract a sound-discriminative embedding from the reference audio, and then use it to detect the target sound from the mixture audio. However, the network performs much differently when using different reference audios (e.g. performs poorly for noisy and short-duration reference audios), and tends to make wrong decisions for transient events (i.e. shorter than 1 second). To overcome these problems, in this paper, we present a reference-aware and duration-robust network (RaDur) for TSD. More specifically, in order to make the network more aware of the reference information, we propose an embedding enhancement module to take into account the mixture audio while generating the embedding, and apply the attention pooling to enhance the features of target sound-related frames and weaken the features of noisy frames. In addition, a duration-robust focal loss is proposed to help model different-duration events. To evaluate our method, we build two TSD datasets based on UrbanSound and Audioset. Extensive experiments show the effectiveness of our methods.\\n\\nIndex Terms: target sound detection, embedding enhancement, reference-aware, duration-robust focal loss\\n\\n1. Introduction\\nHuman beings have the ability to focus their auditory attention on a particular sound in a multi-source environment, which attracts the related studies in machine hearing. In this paper, we focus on the target sound detection (TSD) task [1], which aims to recognize and localize target sound source within a mixture audio given a reference audio or/and a sound label, e.g. detecting the talking sound within a noisy cafe environment. TSD has many potential applications, such as noise monitoring for smart cities [2] and large-scale multimedia indexing [3]. TSD is similar to sound event detection (SED), however, the difference is that SED aims to classify and localize all pre-defined sound events (e.g., train horn, car alarm) within an audio clip, which has been widely studied [4, 5, 6, 7, 8, 9]. Other related tasks include speaker extraction [10, 11, 12, 13, 14] where the target speech is extracted from a mixture speech given a reference utterance of the target speaker, and acoustic events sound selection (or removal) problems [15, 16, 17]. Different from them, TSD focuses on the detection task, as seen in multimedia retrieval applications where the training data can be more easily obtained.\\n\\nIn a recent work [1], a target sound detection network (TSDNet) is presented, which is composed of a conditional network and a detection network. In TSDNet, a sound-discriminative embedding generated by the conditional network is used as the reference information to guide the detection network for detecting the target sound from the mixture audio. TSDNet provides a good detection performance on a small-scale training dataset (i.e. UrbanSound [18]). However, we observe that TSDNet tends to make wrong detection on short-duration events (such as chop and bouncing), as Figure 2(a) shows. In addition, the performance of detection highly relies on the quality of the reference information, which may be severely degraded when the reference audio is noisy or quite short.\\n\\nTo address these issues, in this paper, we propose a reference-aware and duration-robust network (called RaDur) for target sound detection task. More specifically, we design an embedding enhancement module in the conditional network, which utilizes the frames of mixture audio related to the reference information to enhance the embedding. We employ the attention pooling function to guide the conditional network to attend to target-related frames and ignore noisy frames or interference. In addition, we apply a multi-scale feature extractor to extract characteristics of events with different duration and we propose a duration-aware focal loss to solve the problems induced by short-duration events. To evaluate our method, we use URBAN-TSD dataset [1] and establish a new large-scale dataset (Audioset-TSD) based on Audioset [3]. The experiments show that our proposed method provides 6.6% and 16.7% improvement for the segment-based and event-based F scores on the URBAN-TSD dataset, and 23.8% and 8.0% improvement for the segment-based and event-based F scores on the Audioset-TSD dataset.\\n\\n2. Proposed Method\\n\\nThe architecture of our proposed network (RaDur) is shown in Fig. 1, which is composed of two parts: a conditional network and a detection network. In the conditional network, we propose an embedding enhancement module and use the attention pooling to enhance the embedding and apply the attention pooling function to guide the conditional network to attend to target-related frames and ignore noisy frames or interference. In addition, we apply a multi-scale feature extractor to extract characteristics of events with different duration and we propose a duration-aware focal loss to solve the problems induced by short-duration events. To evaluate our method, we use URBAN-TSD dataset [1] and establish a new large-scale dataset (Audioset-TSD) based on Audioset [3]. The experiments show that our proposed method provides 6.6% and 16.7% improvement for the segment-based and event-based F scores on the URBAN-TSD dataset, and 23.8% and 8.0% improvement for the segment-based and event-based F scores on the Audioset-TSD dataset.\"}"}
{"id": "yang22e_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the detection network, we apply multi-scale feature extraction to obtain more sound-discriminative embeddings. We then use these embeddings to calculate attention weights of all the frames. After that, we use the global feature of the encoder to get the global feature of the mixture audio. Then we use the global average pooling operation to get the feature vector of each frame and calculate attention weights of all the frames. Finally, we use these weights to get the final embedding.\\n\\nThe conditional network aims to extract a sound-discriminative embedding vector from the reference audio. Similar to the previous work [1], we adopt a VGG-like convolutional neural network (CNN) model [19] for the conditional network, which uses the log-mel spectrogram as input and consists of 64 blocks with 5 convolutional layers each. The encoder of the conditional network is a convolutional neural network (CNN) with the same architecture as the encoder of the detection network. We present an embedding enhancement (EE) module which works to use the mixture audio to enhance the embedding and make the embedding attend to the mixture audio, which will be degraded. To make the embedding more discriminative and robust to noise and other interference, we propose an attention pooling function and an embedding enhancement (EE) module to enhance the embedding. Inspired by the temporal attention pooling function which enables the network to attend to the frames containing the target reference information, we place the global pooling layer in [1] with an attention pooling function similarly to [2].\\n\\nWe observe that the quality of the embedding is crucial to distinguishing the reference audio from the mixture audio. If the reference audio contains noise or if the duration of the event within the reference audio is shorter than 1 second, the quality of the embedding will be degraded. To make the embedding more discriminative and robust to noise and other interference, we propose an attention pooling function and an embedding enhancement (EE) module to enhance the embedding. The EE module utilizes the information of the target sound, instead, these frames may be the result of noise and other interference. As a result, we set the EE module to control the EE module. Here, \u03c4 is a threshold which is used to filter the detection scores \u02c6y\u2032. Finally, we use the global average pooling operation in the audio classification tasks [20, 8, 21], we re Initializes the parameters of the detection network, and and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of the detection network, and the parameters of"}
{"id": "yang22e_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we use a fusion layer (1D convolutional layer) to integrate the original embedding and the enhanced embedding.\\n\\n\\\\[ \\\\hat{y}'_i = \\\\begin{cases} \\n0, & \\\\text{if } \\\\hat{y}'_i < \\\\tau \\\\\\\\\\n\\\\hat{y}'_i, & \\\\text{if } \\\\hat{y}'_i \\\\geq \\\\tau \\n\\\\end{cases} \\\\]\\n\\n\\\\[ a'' = a' \\\\otimes \\\\hat{y}' \\\\] (11)\\n\\n\\\\[ E'_s = E'_m \\\\otimes a'' \\\\]\\n\\n\\\\[ e''_f = k \\\\sum_{i=1}^{k} E'_i s \\\\] (12)\\n\\n\\\\[ e \\\\star f = \\\\text{Conv1d}(e_f) \\\\otimes \\\\text{Conv1d}(e'_{\\\\star f}) \\\\] (13)\\n\\nwhere \\\\( e \\\\star f \\\\in \\\\mathbb{R}^{C_r} \\\\) is the modified enhanced embedding, \\\\( a'' \\\\in \\\\mathbb{R}^k \\\\) is the modified attention weights of selected frames, and \\\\( E'_s = \\\\{ E'_1 s, E'_2 s, \\\\ldots, E'_k f \\\\} \\\\in \\\\mathbb{R}^{k \\\\times C_r} \\\\) denotes the modified weighted selected feature. The whole process for generating the enhanced embedding is summarized in Algorithm 1.\\n\\n**Algorithm 1**\\n\\n**Embedding Generation.**\\n\\n**Input:** The input representation of the reference audio \\\\( x_r \\\\); The input representation of the mixture audio \\\\( x_m \\\\);\\n\\n**Output:** The enhanced embedding \\\\( e \\\\star f \\\\);\\n\\n1: Get the original embedding \\\\( e_f \\\\) using equation (1)-(5);\\n2: Get the feature of the mixture audio \\\\( E'_m \\\\) using equation (7);\\n3: Get the detection results of the previous stage \\\\( \\\\hat{y} \\\\) using equation (6);\\n4: Select top-\\\\( k \\\\) frames from \\\\( E'_m \\\\), and get the selected feature \\\\( E'_m \\\\) and the corresponding detection scores \\\\( \\\\hat{y} \\\\);\\n5: Calculate the attention weights \\\\( a' \\\\) using equation (8)-(9);\\n6: Modify the attention weights and get \\\\( a'' \\\\) using equation (11);\\n7: Calculate \\\\( e \\\\star f \\\\) according to equation (12)-(13);\\n8: return \\\\( e \\\\star f \\\\);\"}"}
{"id": "yang22e_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: The comparison between TSDNet and Radur on the URBAN-TSD dataset. S-F and E-F denote segment- and event-based F score. We did the experiments three times and report the mean value.\\n\\n| Model        | Loss  | AP    | EE    | S-F   | E-F   |\\n|--------------|-------|-------|-------|-------|-------|\\n| TSDNet [1]   | BCE   | 69.3  | 30.6  |       |       |\\n| Radur        | BCE   | 71.5  | 33.3  |       |       |\\n| Radur        | BCE   |       | \u2713     | 71.7  | 33.5  |\\n| Radur        | BCE   | \u2713     | \u2713     | 73.5  | 35.2  |\\n| Radur        | Focal | \u2713     | \u2713     | 73.9  | 35.7  |\\n| Radur        | Du-Focal | \u2713   | \u2713     | 73.8  | 35.7  |\\n\\nTable 2: The comparison between TSDNet and Radur on the Audioset-TSD dataset.\\n\\n| Model        | Loss  | AP    | EE    | S-F   | E-F   |\\n|--------------|-------|-------|-------|-------|-------|\\n| TSDNet [1]   | BCE   | 49.5  | 48.6  |       |       |\\n| Radur        | BCE   | 51.3  | 50.0  |       |       |\\n| Radur        | BCE   | \u2713     | \u2713     | 52.5  | 51.4  |\\n| Radur        | BCE   | \u2713     | \u2713     | 54.4  | 52.2  |\\n| Radur        | Focal | \u2713     | \u2713     | 58.3  | 51.1  |\\n| Radur        | Du-Focal | \u2713   | \u2713     | 61.3  | 52.5  |\\n\\nFollowing [1], the dimension of the embedding vector is 128 and we use the multiplication fusion manner. We choose top-2 frames for the EE module. Unless specifically stated, the hyper-parameters $\\\\alpha$, $\\\\beta$, $\\\\gamma$ and $\\\\tau$ are set to 1, 0.65, 2 and 0.7 respectively, empirically based on the validation set.\\n\\n4.2. Experimental results\\n\\nWe evaluate our proposed Radur and the state-of-the-art method [1] on URBAN-TSD and Audioset-TSD datasets, and report the experimental results in Tables 1 and 2. By comparing row 1 and row 2 in Tables 1 and 2, we can see that RaDur has better performance thanks to the multi-scale convolutional scheme and deeper network structure. By comparing row 2 and row 3 in Table 2, we can see that the AP module leads to about 1.4% improvement on the event-based F1 score, which means that the AP module can improve the quality of reference embedding. In addition, the comparison between row 3 and 4 shows the effectiveness of the EE module. We can see that the focal loss performs better than the BCE loss especially on the Audioset-TSD dataset, and our proposed Du-Focal loss further improves the performance on the Audioset-TSD dataset. Instead, the Du-Focal loss does not bring improvement on the URBAN-TSD dataset, for the reason that all of the events in this dataset have similar duration (1-2 seconds).\\n\\nTo further analyze the influence of event duration on the performance, we divide the 192 events into 5 groups according to the average duration in the test set. As Figure 2 (a) shows, the duration of the first group is from 0 seconds to 1 seconds, which represents the transient events. The duration of the last group is from 7 seconds to 10 seconds, which represents the long events. We can find that detecting transient events is the most difficult task. Because of the multi-scale feature extractor and duration-aware focal loss, RaDur provides significant improvement on transient events. However, the detection results for transient and long events still have a large gap, which deserves further study in our future work.\\n\\nTable 3: Ablation studies on hyper-parameter $\\\\tau$ on the URBAN-TSD dataset.\\n\\n| Model | $\\\\tau$ | Segment-based F-score | Event-based F-score |\\n|-------|--------|-----------------------|---------------------|\\n| Radur | 0.5    | 71.4                  | 32.9                |\\n| Radur | 0.6    | 72.6                  | 34.1                |\\n| Radur | 0.7    | 73.5                  | 35.2                |\\n| Radur | 0.8    | 73.1                  | 34.9                |\\n| Radur | 0.9    | 71.6                  | 33.8                |\\n\\n4.3. Ablation Studies\\n\\nWe first conduct ablation studies to investigate the effect of hyper-parameter $\\\\tau$ on the EE module and report experimental results in Table 3. We can see that the performance gradually improves with $\\\\tau$ increasing from 0.5 to 0.7. After that, the performance begins to decrease. The experimental phenomenon meets our hypothesis: If $\\\\tau$ is smaller than 0.6, we may select non-target feature frame, which may influence the original embedding. On the contrary, if $\\\\tau$ is larger that 0.8, most of the frames will be filtered, then the EE module may not work.\\n\\nWe also conduct ablation studies to investigate the influence of two of the hyper-parameters ($\\\\alpha$ and $\\\\beta$) in the Du-Focal loss. As shown in Figure 2 (b), the value of $\\\\beta$ is very important, and $\\\\beta$ should be larger than 0.5 because there are more negative sample frames than positive ones in the Audioset-TSD dataset. Furthermore, as the value of $\\\\alpha$ increases, the F-score is boosted. However, we can see that if $\\\\beta > 0.65$ and $\\\\alpha > 1.2$, the performance will drop. We argue that if $\\\\beta$ and $\\\\alpha$ are both set too large, the ratios of negative samples will be too small, as a result, the negative samples may be easily ignored in the training process.\\n\\n5. Conclusions\\n\\nWe have presented an improved TSDNet (RaDur) by modelling short-duration events and enhancing the discriminating ability of the embedding vectors. In the future work, we will explore the extension of novel classes. The source code and dataset of this work have been released.\\n\\n6. Acknowledgements\\n\\nThis paper was partially supported by Shenzhen Science & Technology Research Program (No: GXWD20201231165807007-20200814115301001; No: JSGG20191129105421211) and NSFC (No: 62176008).\\n\\n1https://github.com/yangdongchao/RaDur\"}"}
