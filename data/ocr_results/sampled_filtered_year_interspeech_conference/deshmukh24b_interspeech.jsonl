{"id": "deshmukh24b_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nAudio quality is a key performance metric for various audio processing tasks, including generative modeling, however its objective measurement remains a challenge. Audio-Language Models (ALM) are pre-trained on millions of audio-text pairs that may contain information about audio quality, the presence of artifacts or noise. Given an audio input and a text prompt about quality, an ALM can calculate a similarity score between the two. We exploit this capability and introduce PAM, a truly reference-free metric for assessing audio quality for different audio processing tasks. Contrary to other \u201creference-free\u201d metrics, PAM does not require computing embeddings on a reference dataset nor training a task-specific model on a costly set of human listening scores. We extensively evaluate PAM against established metrics and newly collected human listening scores on four tasks: text-to-audio (TTA), text-to-music generation (TTM), text-to-speech (TTS), and deep noise suppression (DNS). We perform multiple ablation studies with controlled audio distortions, in-the-wild setups, and prompt choices. Our evaluation shows that overall, PAM correlates strongly with human listening scores and performs better than existing metrics. These results demonstrate the potential of ALM for computing a general-purpose audio quality metric. Code and human listening scores will be released.\\n\\nIndex Terms: speech recognition, audio quality assessment, audio classification\\n\\n1. Introduction\\nAudio Quality Assessment (AQA) refers to the subjective assessment of the perceived overall quality of a signal [1]. The gold standard of AQA consists of assessment by humans, which is a challenging task that requires many listening tests in controlled setups. These experiments are time-intensive and costly and cannot be carried out multiple times for every setup or result. Hence, measurements that can closely estimate human assessment of audio quality are essential for the development and evaluation of models that perform audio generation tasks.\\n\\nAudio generation tasks entail sounds, music, and speech. All tasks employed different audio quality metrics, including some that aim to resemble human assessments. TTA uses metrics like Fr\u00e9chet Audio Distance (FAD) [2], IS, KL, and subjective metrics like Overall Quality (OVL) and Relation of audio to text caption (REL) [3]. TTM uses FAD and subjective metrics like MCC [4]. TTS uses metrics like WER, SpeechLMScore [5], and perceptual metrics like MOSNet [6], FSD [7], and MCC. However, several aspects of audio quality are shared across tasks, such as the presence of artifacts. Ideally, one metric should measure quality regardless of the task hence, addressing the challenges of task-specific metrics.\\n\\nCurrent metrics provide a reliable evaluation but pose different challenges. Reference-based metrics require ground truth for computation. To assess the quality of a recording, the generated audio is compared against a desired recording to measure how much the quality degraded. Reference-free metrics do not require a desired recording, but usually require a pretrained model to compute embeddings on a reference dataset. The selection of the model and the dataset would highly affect the score [8]. Other metrics like DPAM [9], MOSNet [6], and DNSMOS [10] train a model using human evaluation and at inference use the model predictions as a proxy for human evaluation. This requires the curation of human scores and training for each audio task.\\n\\nInstead, we propose a no-reference metric that leverages learning perceptual audio quality from human assessments in text descriptions. ALM have learned from millions of audio-text pairs sourced from the Internet. Some of the audio has a corresponding natural language description of quality. For example, audio-text models [11, 12, 13] trained on FreeSound data, have seen text descriptions like \u201cPad sound, with a lo-fi, high compression type feel to it. The noise floor, with a low pass filter set around 50Hz and several octaves of pitch bend\u201d. Although the ALM is not explicitly trained for audio quality assessment, it has ingested hundreds of human annotations describing their perception of the audio. Because ALM can be used out of the box in a Zero-Shot fashion, they can compare text prompts about quality against audio without requiring a reference dataset.\\n\\nWe introduce PAM, a novel, truly reference-free method for assessing audio quality, which provides an advancement in audio quality evaluation. Our contribution includes (1) The first ALM-based metric for general-purpose audio-quality assessment which is truly reference-free (2) A two-prompt antonym strategy that makes using ALM feasible and improves correlation with human perception (3) Extensively testing PAM on four audio tasks: TTA Generation, TTM Generation, TTS, and DNS. (4) For each task, we collected human listening scores to be released.\\n\\nWe hope our work and data release push the development of general-purpose audio quality assessment.\"}"}
{"id": "deshmukh24b_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Sensitivity of PAM to (a) Additive White Gaussian Noise, (b) Tanh distortion, (c) Mu-Law compression, (d) Reverb. The samples are professionally recorded sounds. PAM score degrades as the noise is added except for damping (reverb).\\n\\n2. PAM\\n\\nOur proposed metric PAM can perform audio quality assessment by exploiting the joint multimodal space learned by an ALM. The learned space can be used to quantify the correspondence between quality-related text prompts and audio recordings.\\n\\n2.1. Audio Quality Assessment\\n\\nThe term implies a variety of properties in various contexts. For this work, we consider audio to be high quality when the presence of artifacts and noise is imperceptible. For example, white noise, clipping, and other distortions. We did not consider non-speech audio as noise, such as sound events, music, reverb, echo, and in general all naturally occurring sounds.\\n\\n2.2. Prompting setup\\n\\nThe user can provide an audio file and text class of \\\"the sound is clear and clean\\\" and determine the audio-text similarly using the model. The similarity can be squashed between 0 and 1 and used as a score. Higher is better quality. Though this method is valid and used for multiple tasks [14, 3], we see prompting with just one class of \\\"the sound is clear and clean\\\" leads to a poor correlation with human perception and distortions across various tasks and distributions. One of the reasons this strategy does not work is due to linguistic ambiguity. Particularly, if the prompt is \\\"the sound is clear and clean\\\", then depending on the context, the model can infer: (1) The sound is easy to understand, see, or hear, without any distortion, noise, or interference. (2) The sound is pure, crisp, and pleasant, without any harshness, dullness, or muddiness. This meaning is based on the definition of \\\"clean\\\" as \\\"having a pure, fresh, or smooth quality\\\", or (3) The sound is honest, accurate, and truthful, without any deception, manipulation, or bias. This meaning is based on the definition of \\\"clean\\\" as \\\"free from dishonesty or corruption\\\".\\n\\nTo address this problem, we proposed a prompting strategy that will minimize ambiguity in the latent space. This is achieved by using two prompts that force the model to make similarity calculations along the latent subspace of audio quality (e.g., \\\"the sound is clear and clean\\\" and \\\"the sound is noisy and with artifacts\\\"). The word choice was determined from the ALM CLAP's training data. A frequency count per word is obtained (unigram) from the descriptions used in training data. We removed stop-words. Then, we computed BERT embeddings for each word and took cosine similarity between filtered words and the word \\\"noise\\\". We picked words from the top 10 words. This first work of using ALM as a metric should be about verifying feasibility, thus further prompt exploration is future work.\\n\\nPAM computation is shown in Fig 1, right section. The user provides an audio file which is converted into a mel-spectrogram ($x \\\\in \\\\mathbb{R}^{T \\\\times F}$) and passed to CLAP's audio encoder to produce an audio embedding $v \\\\in \\\\mathbb{R}^{1 \\\\times d}$. In parallel, the two \\\"opposing\\\" prompts about quality (\\\"the sound is clear and clean\\\" and \\\"the sound is noisy and with artifacts\\\") are tokenized and embedded using the text encoder to produce text embeddings $u \\\\in \\\\mathbb{R}^{d \\\\times N}$.\\n\\nAfter projection into the multimodal space, the dot product is computed between the two embeddings, followed by softmax:\\n\\n$$p_h = \\\\frac{e^{z_h}}{\\\\sum_{j=1}^{2} e^{z_j}},$$\\n\\nwhere $h$ is the index of the prompt related to high quality, $z_j = u_j \\\\cdot v$, ($\\\\cdot$) denotes the dot product, and $p \\\\in \\\\mathbb{R}^{1 \\\\times 2}$. The value of $p_h \\\\in [0, 1]$ is the PAM score and informs about the quality of the audio.\\n\\n3. Experiments\\n\\nThe experimental setup is designed to provide a comprehensive evaluation of PAM across different distortions, prompting strategies, and datasets from different audio generation tasks. All experiments are run using a single 16GB V100 GPU.\\n\\nDistortions. We systematically add various types of distortions: Gaussian Noise, Gaussian Noise with Signal-to-Noise Ratio (SNR), Tanh distortion, Mu-Law compression, and Reverb across various source distributions and check its effect on the PAM. The results are in Section 4.1.\\n\\nPrompting strategy. PAM uses a two-prompt antonym strategy with the text of \\\"the sound is clear and clean.\\\" and \\\"the sound is noisy and with artifacts.\\\". In section 4.2, we compare it against the naive single-prompt strategy. We also compare it against human evaluation.\\n\\nAudio tasks. In Section 5, we consider different generation tasks like Text-to-Audio, Text-to-Music and Text-to-Speech. For each task, we use multiple models, perform human listening tests, and compare PAM against established metrics.\\n\\n4. Results\\n\\n4.1. Effect of distortions\\n\\nAn audio quality metric should degrade with the presence of distortions and artifacts in the audio. To verify this, we add common simulated distortions to audio sourced from a professionally recorded sound effect pack. The four types of distortions used are (1) Gaussian Noise addition with particular SNR (3) Tanh distortion (4) Mu Law compression. Lastly, we add Reverb, which by the definition in Section 2.1 is not considered an artifact or distortion. Figure 2 shows the effect of distortions on PAM score when tested on a professionally recorded sound effect pack. The PAM score degrades as the noise is added except for Reverb. For Reverb, the PAM score is fairly constant, i.e., changes from 0.76 to 0.81. While for others we see considerable degradation in PAM score. To check robustness across source distribution, we change the dataset from professionally recorded to AudioCaps (audio from YouTube videos containing sound events), MusiCaps (music tracks from YouTube), and LibriTTS (speech, audiobooks). We see similar trends of PAM score degrading with the addition of distortions and consistent scores across Reverb.\\n\\n4.2. Prompting strategy\\n\\nFigure 1 shows two different prompting strategies that can be used to get a quality-related score. The figure on the left shows naive single prompting and the figure on the right shows the two-prompt antonym strategy of PAM. The advantages of the two-prompt setup and the limitations of the naive prompt are explained in Section 2.2. In this section, we perform experiments to compare the two-prompt setup with human listening scores. We use the NISQA (Non-Intrusive Speech Quality and TTS Naturalness Assessment) [15] dataset to check the correlation between PAM, the single prompt strategy, and human perceptual evaluation. The NISQA Corpus includes more than 14,000 speech samples with simulated (e.g. codecs, packet-loss, background noise) and live (e.g. mobile phone, Zoom, Skype, What-\"}"}
{"id": "deshmukh24b_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Each file is labeled with subjective ratings of the overall quality. We use simulated and live talk corpus from NISQA. The simulated corpus contains simulated distortions with speech samples from four different datasets and the live talk corpus contains recordings of real phone and VoIP calls. Unlike PAM, NISQA considers sounds events as noise, so human raters labelled the recordings as low quality. Therefore, we created a filtered NISQA set and applied four distortions: (1) white noise addition with a particular SNR (2) live talk on a laptop or smartphone (3) low bandpass filter (4) high bandpass filter.\\n\\nWe check the correlation of the single prompt strategy and the opposite prompt strategy against the Mean Opinion Score (MOS) from human listeners. The Pearson Correlation Coefficient (PCC) measures linear correlation between two sets of data [16] and it is shown in Table 1. PCC ranges from -1 to 1, where -1 indicates a perfect negative correlation, 0 indicates no correlation, and 1 indicates a perfect positive correlation. We see that the single-prompt strategy does not correlate with MOS, the human perceptual evaluation. While PAM not only correlates, but achieves a PCC greater than 0.7 on (1) white noise distortion and (2) real-world talk recorded from laptops and smartphones.\\n\\n| Distortion       | Single prompt | PAM (Two prompts) |\\n|------------------|---------------|-------------------|\\n| White noise      | 0.0738        | 0.7667            |\\n| Live talk        | 0.0121        | 0.5455            |\\n| Low pass filter  | 0.1278        | 0.6520            |\\n| High pass filter | 0.0939        | 0.7589            |\\n\\nTable 1: (PCC) Correlation between MOS (human subjective evaluations) and prompting strategies for four distortions applied to the NISQA dataset. Using one prompt for AQA does not correlate with MOS, but using two prompts (PAM) does.\\n\\n5. PAM for audio tasks\\n\\nIn this section, we use PAM to evaluate models for generation tasks. We compare PAM against task-specific metrics and human scores to evaluate its reliability as an AQA metric. In all experiments, we use two-prompt antonym strategy with the text of \u201cthe sound is clear and clean.\u201d and \u201cthe sound is noisy and with artifacts.\u201d\\n\\n5.1. Text-to-Audio generation\\n\\nTTA generation models synthesize non-speech non-music audio (sounds) from text descriptions. We use publicly available variants of TTA models like AudioLDM [14], AudioLDM2 [17], AudioGen [3], and MelDiffusion. The captions sourced from the AudioCaps test set (747 captions) are used to generate audio from the above 4 models and their variants.\\n\\nWe carry out a human listening experiment to compute the correlation between metrics and human perception. We randomly picked 100 captions and their corresponding generated audio from the test set. During the experiment, each participant was asked to rate each audio in terms of Overall Quality on a five-point Likert scale. The order of audios was fully randomized and each audio was rated by 10 participants. Raters were recruited using the Amazon Mechanical Turk platform. To ensure quality in the annotations, participants who consistently provided identical scores in every HIIT (e.g., all 1s) or who completed the task in less than 10 seconds were excluded.\\n\\nFigure 3 summarizes the PCC between per-model metrics and OVL. PAM correlates significantly better with human perception of quality (OVL) than the task-specific metrics of KL softmax (softmax over logits) and KL sigmoid (sigmoid over logits). The KL metric uses the CNN14 [18] model to extract audio embeddings for the generated and reference set. The CNN14 model is trained to classify audio into different sound events and hence does well at recognizing the presence of sound events rather than overall quality. Also, a recent work [17] observed that reference-free metrics like KL provide high scores when the generation model is trained on the same distribution data as the KL reference set. PAM is a no-reference metric so it does not have these drawbacks.\\n\\n5.2. Text-to-Music generation\\n\\nSubjective performance can be described in terms of Acoustic Quality (AQ), which measures whether the generated sound is free of noise and artifacts, and Musical Quality (MQ), which measures the quality of the musical composition and performance. A commonly used reference set for evaluating TTM models is MusicCaps [19], a music subset of AudioSet [20] that contains rich text captions provided by musical experts.\\n\\nFor a direct comparison with human perception, we calculate PAM on a set of real and generated music recordings. Subjective AQ and MQ labels were collected by authors in [8] as MOS scores from several human judges. The real samples were taken from the Free Music Archive (FMA) and MusicCaps. For TTM generation, publicly available variants of MusicLM [21] and MusicGen [4], as well as Mubert [22] were used.\\n\\n| Acoustic Quality | PCC | MusicCaps | MusicGen | MusicLM | Mubert |\\n|-----------------|-----|-----------|----------|---------|--------|\\n| FMA             | 0   | 0.5       | 1        | 0.5     | 1      |\\n| MusicCaps       | 0.5 | 1         | 0.5      | 1       | 0.5    |\\n| MusicGen        | 1   | 0.5       | 1        | 0.5     | 1      |\\n| MusicLM         | 0.5 | 1         | 0.5      | 1       | 0.5    |\\n| Mubert          | 1   | 0.5       | 1        | 0.5     | 1      |\\n\\nFigure 4 shows PCC between the MOS ratings and PAM. We adopt the method by Gui et al. [8] and calculate per-sample FADs. For comparison, the (absolute) PCC of the Fr\u00e9chet Audio Distance (FAD) is shown for two pretrained models. Recall that FAD requires a pretrained model to compute audio embeddings on a reference dataset. We used MusCC, a dataset of studio quality music [8] as a reference for FAD. PAM performs competitively, outperforming the established FAD-VGGish metric.\\n\\nSimilar to TTA, we perform a human listening experiment where 100 randomly picked captions from the MusicCaps test set are used to generate music. The listening test setup in terms of subjective score, participant recruiting and quality control is the same as Section 5.1. Table 2 shows the correlation of...\"}"}
{"id": "deshmukh24b_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"TTM metrics with MOS. PAM correlates with human scores for Overall Quality (OVL) better than REL. This is expected, as PAM does not include text caption in score computation.\\n\\n| Model           | Subjective KL \u2193 | Subjective Sig \u2193 | PAM \u2191 |\\n|-----------------|-----------------|------------------|-------|\\n| AudioLDM2       | 0.1241          | 0.0929           | 0.3295|\\n| MusicLDM        | 0.1291          | 0.1617           | 0.4235|\\n| MusicGen-l      | 0.3353          | 0.3275           | 0.6018|\\n| MusicGen-mel    | 0.0993          | 0.1423           | 0.6908|\\n| MusicCaps       | 0.2314          | 0.2319           | 0.5549|\\n\\nTable 2: PCC between human evaluation MOS and different metrics for the models in Table. The subjective metric (Subj.) indicates the metric used for PCC computation.\\n\\n5.3. Speech Synthesis\\nSpeech Synthesis involves creating artificial speech, either by converting text to speech (TTS) or altering existing speech to sound like a different speaker or style, known as voice conversion.\\n\\nWe examine the effectiveness of PAM in the above two tasks. For TTS, a recent work [23] conducted human evaluation studies for different TTS systems. The study used three TTS models [24, 25, 26] to generate speech for 100 sentences from the LibriTTS dataset [27]. Each generated sample was rated by 10 raters. We use this dataset and compare PAM with existing metrics. The PCC correlation with human evaluation is shown in Figure 5. On average, PAM correlates better with human perception of speech quality than existing metrics.\\n\\nFigure 5: Absolute PCC between the human evaluation and metrics for TTS models. The transcripts are sourced from the LibriTTS dataset. On average, PAM correlates better with human perception of speech quality than existing metrics.\\n\\nThe second speech synthesis task we consider is Voice Conversion (VC), where the aim is to convert audio containing the original speech to audio containing the target speaker's voice. For this, we use the VoiceMOS 2022 challenge dataset [28], specifically the VCC subset. The VCC subset includes 3,002 utterances from 79 systems. We test PAM on this dataset and compare it with existing metrics of MOSNet [6], MOS-SSL [29], and SpeechLMScore [5]. PAM performs worse than other speech-based finetuned metrics.\\n\\n| Source Model | Utterance-level | System-level |\\n|--------------|-----------------|--------------|\\n|              | PCC SRCC        | PCC SRCC     |\\n| VCC MOSNet   | 0.654 0.639     | 0.817 0.796  |\\n| VCC MOS-SSL  | 0.891 0.883     | 0.983 0.964  |\\n| VCC SLMS.    | 0.505 0.501     | 0.863 0.829  |\\n| VCC PAM      | 0.389 0.411     | 0.563 0.593  |\\n| OOD MOSNet   | 0.259 0.153     | 0.537 0.430  |\\n| OOD MOS-SSL  | 0.467 0.459     | 0.357 0.437  |\\n| OOD SLMS.    | 0.138 0.224     | 0.049 0.199  |\\n| OOD PAM      | 0.582 0.585     | 0.634 0.703  |\\n\\nTable 3: Utterance-level and system-level correlation of different metrics with MOS scores. PAM correlates better with MOS than other metrics in the OOD setup, suggesting better generalization.\\n\\nOn both the setup of TTS and VC, the literature metrics, MOSNet and MOS-SSL, were trained on the train split. Thus, all the evaluation setup is in-distribution for the metrics. To check out-of-distribution performance, we consider an out-of-domain (OOD) subset of the VoiceMOS challenge. The OOD subset is sourced from the 2019 Blizzard Challenge [30] and contains 136 Chinese TTS samples. The PCC results of metrics are shown in Table 3. In the OOD setup, PAM correlates better than existing metrics that are not trained on the OOD data. This showcases the ability of PAM to be a Zero-Shot audio quality metric.\\n\\nOverall, PAM can detect audio quality and distortions in generated speech. For speech tasks, it falls short of task-specific metric, where the generated speech is rated based on intelligibility or speaker characteristics.\\n\\n5.4. Noise suppression\\nNoise and artifacts negatively impact perceived speech quality, e.g., in voice communication systems [31]. Deep Noise Suppression (DNS) aims at enhancing speech quality by suppressing noise. MOS derived from listeners judging the output of a DNS model provides a subjective performance metric to develop or tune the model. Machine-Learning based blind MOS estimators such as DNS-MOS have shown to outperform existing objective metrics for estimating the speech quality of DNS models [32, 10].\\n\\nWe compute PAM on the output of models participating in the ICASSP 2021 DNS challenge [31] and compare it against a state-of-the-art DNS-MOS estimator [10]. Unlike generative models, DNS involves removing unwanted signals, hence its perceptual quality is impacted both by the quality of the (desired) speech as well as the quality and suppression of noise. We hypothesise that estimating such multifaceted quality may benefit from more comprehensive prompts. As a proof of concept, we calculate PAM with two prompt averaging strategies. Table 4 summarizes the results in terms of Spearman's Rank Correlation Coefficient (SRCC) between the average human-labeled MOS of each tested DNS model and the average DNS-MOS or PAM. The SRCC indicates how well the ranking of the tested DNS models in terms of their subjective quality is preserved [10]. PAM performs competitively compared to the state-of-the-art MOS estimator trained specifically for this task.\\n\\n| Model          | SRCC          | SRCC          |\\n|----------------|---------------|---------------|\\n| DNS-MOS        | 0.975 0.879  | 0.896 0.929  |\\n\\nTable 4: SRCC of DNS models participating in the ICASSP 2021 DNS challenge for SoTA DNS MOS estimation model and PAM. PAM avgsim and PAM avg use alternative prompting strategies (see appendix).\\n\\nThe evaluation setup is in-distribution for the metrics. To check out-of-distribution performance, we consider an out-of-domain (OOD) subset of the VoiceMOS challenge. The OOD subset is sourced from the 2019 Blizzard Challenge [30] and contains 136 Chinese TTS samples. The PCC results of metrics are shown in Table 3. In the OOD setup, PAM correlates better than existing metrics that are not trained on the OOD data. This showcases the ability of PAM to be a Zero-Shot audio quality metric.\\n\\nOverall, PAM can detect audio quality and distortions in generated speech. For speech tasks, it falls short of task-specific metric, where the generated speech is rated based on intelligibility or speaker characteristics.\\n\\n6. Limitations\\nPAM has two main limitations. First, PAM has better PCC with human perception for music and non-speech audio generation tasks. However, for speech generation tasks, the PCC is comparable and sometimes lower than existing objective perceptual metrics trained for the specific task. Second, this work focuses on designing an audio metric with a task-agnostic prompt. As a first work on using ALM as a metric we focused on verifying feasibility and further prompt exploration is future work. Furthermore, adapting the prompt with task-specific information might lead to better performance for the target task.\\n\\n7. Conclusion\\nWe introduced PAM, a truly reference-free metric for assessing audio quality for generated audio. The metric is Zero-Shot and does not require a reference dataset or task-specific fine tuning. PAM correlates strongly with human perception of audio quality across various tasks\u2014Text-to-Audio, Text-to-Music, Speech Synthesis and Noise suppression. PAM is especially useful in OOD scenarios where existing objective metrics fall short.\"}"}
{"id": "deshmukh24b_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. References\\n\\n[1] M. Torcoli, T. Kastner, and J. Herre, \u201cObjective measures of perceptual audio quality reviewed: An evaluation of their application domain dependence,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 1530\u20131541, 2021.\\n\\n[2] K. Kilgour, M. Zuluaga, D. Roblek, and M. Sharifi, \u201cFr\u00e9chet audio distance: A metric for evaluating music enhancement algorithms,\u201d arXiv preprint arXiv:1812.08466, 2018.\\n\\n[3] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. D\u00e9fossez, J. Copet, D. Parikh, Y. Taigman, and Y. Adi, \u201cAudiogen: Textually guided audio generation,\u201d in The Eleventh International Conference on Learning Representations, 2022.\\n\\n[4] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. D\u00e9fossez, \u201cSimple and controllable music generation,\u201d arXiv preprint arXiv:2306.05284, 2023.\\n\\n[5] S. Maiti, Y. Peng, T. Saeki, and S. Watanabe, \u201cSpeechlmscore: Evaluating speech generation using speech language model,\u201d in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1\u20135.\\n\\n[6] C.-C. Lo, S.-W. Fu, W.-C. Huang, X. Wang, J. Yamagishi, Y. Tsao, and H.-M. Wang, \u201cMosnet: Deep learning-based objective assessment for voice conversion,\u201d Interspeech 2019, 2019.\\n\\n[7] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi et al., \u201cVoicebox: Text-guided multilingual universal speech generation at scale,\u201d in Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\n\\n[8] A. Gui, H. Gamper, S. Braun, and D. Emmanouilidou, \u201cAdapting Fr\u00e9chet audio distance for generative music evaluation,\u201d arXiv preprint arXiv:2311.01616, 2023.\\n\\n[9] P. Manocha, A. Finkelstein, R. Zhang, N. J. Bryan, G. J. Mysore, and Z. Jin, \u201cA differentiable perceptual audio metric learned from just noticeable differences,\u201d arXiv preprint arXiv:2001.04460, 2020.\\n\\n[10] C. K. Reddy, V. Gopal, and R. Cutler, \u201cDnsmos: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6493\u20136497.\\n\\n[11] B. Elizalde, S. Deshmukh, and H. Wang, \u201cNatural language supervision for general-purpose audio representations,\u201d arXiv preprint arXiv:2309.05767, 2023.\\n\\n[12] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov, \u201cLarge-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation,\u201d arXiv preprint arXiv:2211.06687, 2022.\\n\\n[13] S. Deshmukh, B. Elizalde, R. Singh, and H. Wang, \u201cPengi: An audio language model for audio tasks,\u201d arXiv preprint arXiv:2305.11834, 2023.\\n\\n[14] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and M. D. Plumbley, \u201cAudioldm: Text-to-audio generation with latent diffusion models,\u201d arXiv preprint arXiv:2301.12503, 2023.\\n\\n[15] G. Mittag, B. Naderi, A. Chehadi, and S. M\u00f6ller, \u201cNISQA: A Deep CNN-Self-Attention Model for Multidimensional Speech Quality Prediction with Crowdsourced Datasets,\u201d in Proc. Interspeech 2021, 2021, pp. 2127\u20132131.\\n\\n[16] K. Pearson, \u201cNotes on the history of correlation,\u201d Biometrika, vol. 13, no. 1, pp. 25\u201345, 1920.\\n\\n[17] H. Liu, Q. Tian, Y. Yuan, X. Liu, X. Mei, Q. Kong, Y. Wang, W. Wang, Y. Wang, and M. D. Plumbley, \u201cAudioldm 2: Learning holistic audio generation with self-supervised pretraining,\u201d arXiv preprint arXiv:2308.05734, 2023.\\n\\n[18] Q. Kong, Y. Cao, T. Iqbal, Y. Wang et al., \u201cPanns: Large-scale pretrained audio neural networks for audio pattern recognition,\u201d IEEE/ACM Trans. Audio, Speech and Lang. Proc., 2020. [Online]. Available: https://doi.org/10.1109/TASLP.2020.3030497\\n\\n[19] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi et al., \u201cMusiclm: Generating music from text,\u201d arXiv preprint arXiv:2301.11325, 2023.\\n\\n[20] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, \u201cAudio set: An ontology and human-labeled dataset for audio events,\u201d in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017, pp. 776\u2013780.\\n\\n[21] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi et al., \u201cMusiclm: Generating music from text,\u201d arXiv preprint arXiv:2301.11325, 2023.\\n\\n[22] Mubert-Inc, \u201cMubert,\u201d 2023, available at: https://mubert.com/.\\n\\n[23] D. Alharthi, R. Sharma, H. Dhamyal, S. Maiti, B. Raj, and R. Singh, \u201cEvaluating speech synthesis by training recognizers on synthetic speech,\u201d arXiv preprint arXiv:2310.00706, 2023.\\n\\n[24] Y. A. Li, C. Han, and N. Mesgarani, \u201cStyletts: A style-based generative model for natural and diverse text-to-speech synthesis,\u201d arXiv preprint arXiv:2205.15439, 2022.\\n\\n[25] L.-W. Chen, S. Watanabe, and A. Rudnicky, \u201cA vector quantized approach for text to speech synthesis on real-world spontaneous speech,\u201d arXiv preprint arXiv:2302.04215, 2023.\\n\\n[26] E. Casanova, J. Weber, C. D. Shulby, A. C. Junior, E. G\u00f6lge, and M. A. Ponti, \u201cYourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 2709\u20132720.\\n\\n[27] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibritts: A corpus derived from librispeech for text-to-speech,\u201d arXiv preprint arXiv:1904.02882, 2019.\\n\\n[28] W.-C. Huang, E. Cooper, Y. Tsao, H.-M. Wang, T. Toda, and J. Yamagishi, \u201cThe voicemos challenge 2022,\u201d arXiv preprint arXiv:2203.11389, 2022.\\n\\n[29] E. Cooper, W.-C. Huang, T. Toda, and J. Yamagishi, \u201cGeneralization ability of mos prediction networks,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8442\u20138446.\\n\\n[30] Z. Wu, Z. Xie, and S. King, \u201cThe blizzard challenge 2019,\u201d in Proc. Blizzard Challenge Workshop, vol. 2019, 2019.\\n\\n[31] C. K. A. Reddy, H. Dubey, V. Gopal, R. Cutler, S. Braun, H. Gamper, R. Aichner, and S. Srinivasan, \u201cIcassp 2021 deep noise suppression challenge,\u201d in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 6623\u20136627.\\n\\n[32] A. R. Avila, H. Gamper, C. Reddy, R. Cutler, I. Tashev, and J. Gehrke, \u201cNon-intrusive speech quality assessment using neural networks,\u201d in ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 631\u2013635.\"}"}
