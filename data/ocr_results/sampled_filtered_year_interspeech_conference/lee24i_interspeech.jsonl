{"id": "lee24i_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. References\\n\\n[1] R. Ardila, M. Branson, K. Davis, M. Kohler, J. Meyer, M. Henry, R. Morais, L. Saunders, F. Tyers, and G. Weber, \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proceedings of the Twelfth Language Resources and Evaluation Conference, Marseille, France, May 2020, pp. 4218\u20134222.\\n\\n[2] A. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, S. Dalmia, J. Riesa, C. Rivera, and A. Bapna, \u201cFleurs: Few-shot learning evaluation of universal representations of speech,\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT), 2023, pp. 798\u2013805.\\n\\n[3] M. Zanon Boito, W. Havard, M. Garnerin, \u00c9. Le Ferrand, and L. Besacier, \u201cMaSS: A large and clean multilingual corpus of sentence-aligned spoken utterances extracted from the Bible,\u201d in Proceedings of the Twelfth Language Resources and Evaluation Conference. Marseille, France: European Language Resources Association, May 2020, pp. 6486\u20136493.\\n\\n[4] V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, \u201cMls: A large-scale multilingual dataset for speech research,\u201d in Interspeech 2020. ISCA, Oct. 2020. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2020-2826\\n\\n[5] M. A. Di Gangi, R. Cattoni, L. Bentivogli, M. Negri, and M. Turchi, \u201cMuST-C: a Multilingual Speech Translation Corpus,\u201d in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 2012\u20132017.\\n\\n[6] E. Salesky, M. Wiesner, J. Bremerman, R. Cattoni, M. Negri, M. Turchi, D. W. Oard, and M. Post, \u201cThe Multilingual TEDx Corpus for Speech Recognition and Translation,\u201d in Proc. Interspeech 2021, 2021, pp. 3655\u20133659.\\n\\n[7] J. Iranzo-S\u00e1nchez, J. A. Silvestre-Cerd\u00e0, J. Jorge, N. Rosell\u00f3, A. Gim\u00e9nez, A. Sanchis, J. Civera, and A. Juan, \u201cEuroparl-st: A multilingual corpus for speech translation of parliamentary debates,\u201d in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 8229\u20138233.\\n\\n[8] C. Wang, A. Wu, and J. M. Pino, \u201cCovost 2: A massively multilingual speech-to-text translation corpus,\u201d CoRR, vol. abs/2007.10310, 2020. [Online]. Available: https://arxiv.org/abs/2007.10310\\n\\n[9] P. Lewis, B. Oguz, R. Rinott, S. Riedel, and H. Schwenk, \u201cMLQA: Evaluating cross-lingual extractive question answering,\u201d in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Online: Association for Computational Linguistics, Jul. 2020, pp. 7315\u20137330.\\n\\n[10] N. Moghe, E. Razumovskaia, L. Guillou, I. Vuli\u0107, A. Korhonen, and A. Birch, \u201cMulti3NLU++: A multilingual, multi-intent, multi-domain dataset for natural language understanding in task-oriented dialogue,\u201d in Findings of the Association for Computational Linguistics: ACL 2023. Toronto, Canada: Association for Computational Linguistics, Jul. 2023, pp. 3732\u20133755.\\n\\n[11] W. Xu, B. Haider, and S. Mansour, \u201cEnd-to-end slot alignment and recognition for cross-lingual NLU,\u201d in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Online: Association for Computational Linguistics, Nov. 2020, pp. 5052\u20135063.\\n\\n[12] J. FitzGerald, C. Hench, C. Peris, S. Mackie, K. Rottmann, A. Sanchez, A. Nash, L. Urbach, V. Kakarala, R. Singh, S. Ranganath, L. Crist, M. Britan, W. Leeuwis, G. Tur, and P. Nataraajan, \u201cMASSIVE: A 1M-example multilingual natural language understanding dataset with 51 typologically-diverse languages,\u201d in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Toronto, Canada: Association for Computational Linguistics, Jul. 2023, pp. 4277\u20134302.\\n\\n[13] E. Bastianelli, A. Vanzo, P. Swietojanski, and V. Rieser, \u201cSLURP: A spoken language understanding resource package,\u201d in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Online: Association for Computational Linguistics, Nov. 2020, pp. 7252\u20137262.\\n\\n[14] A. Coucke, A. Saade, A. Ball, T. Bluche, A. Caulier, D. Leroy, C. Doumouro, T. Gisselbrecht, F. Caltagirone, T. Lavril, M. Primet, and J. Dureau, \u201cSnips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces,\u201d CoRR, vol. abs/1805.10190, 2018. [Online]. Available: http://arxiv.org/abs/1805.10190\\n\\n[15] F. Lef\u00e8vre, D. Mostefa, L. Besacier, Y. Est\u00e8ve, M. Quignard, N. Camelin, B. Favre, B. Jabaian, and L. M. Rojas-Barahona, \u201cLeveraging study of robustness and portability of spoken language understanding systems across languages and domains: the PORTMEDIA corpora,\u201d in Proceedings of the Eighth International Conference on Language Resources and Evaluation, LREC 2012, Istanbul, Turkey, May 23-25, 2012. European Language Resources Association (ELRA), 2012, pp. 1436\u20131442.\\n\\n[16] A. Koudounas, M. La Quatra, L. Vaiani, L. Colomba, G. Attanasio, E. Pastor, L. Cagliero, and E. Baralis, \u201cITALIC: An Italian Inent Classification Dataset,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 2153\u20132157.\\n\\n[17] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d in Proceedings of the 40th International Conference on Machine Learning, ser. ICML\u201923. JMLR.org, 2023.\\n\\n[18] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel, \u201cmT5: A massively multilingual pre-trained text-to-text transformer,\u201d in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, and Y. Zhou, Eds. Online: Association for Computational Linguistics, Jun. 2021, pp. 483\u2013498. [Online]. Available: https://aclanthology.org/2021.naacl-main.41\\n\\n[19] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzm\u00e1n, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov, \u201cUnsupervised cross-lingual representation learning at scale,\u201d in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, Eds. Online: Association for Computational Linguistics, Jul. 2020, pp. 8440\u20138451. [Online]. Available: https://aclanthology.org/2020.acl-main.747\\n\\n[20] M. Wang, Y. Li, J. Guo, X. Qiao, Z. Li, H. Shang, D. Wei, S. Tao, M. Zhang, and H. Yang, \u201cWhislu: End-to-end spoken language understanding with whisper,\u201d in Proc. Interspeech, vol. 2023, 2023, pp. 770\u2013774.\\n\\n[21] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020. [Online]. Available: https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html\\n\\n[22] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021.\\n\\n[23] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \u201cBleu: a method for automatic evaluation of machine translation,\u201d in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311\u2013318.\"}"}
{"id": "lee24i_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Speech-MASSIVE: A Multilingual Speech Dataset for SLU and Beyond\\n\\nBeomseok Lee1,2,3, Ioan Calapodescu2, Marco Gaido3, Matteo Negri3, Laurent Besacier2\\n\\n1University of Trento, Italy\\n2NA VER LABS Europe, France\\n3Fondazione Bruno Kessler, Italy\\n\\nbeomseok.lee@unitn.it, {ioan.calapodescu,laurent.besacier}@naverlabs.com, {mgaido,negri}@fbk.eu\\n\\nAbstract\\n\\nWe present Speech-MASSIVE, a multilingual Spoken Language Understanding (SLU) dataset comprising the speech counterpart for a portion of the MASSIVE textual corpus. Speech-MASSIVE covers 12 languages from different families and inherits from MASSIVE the annotations for the intent prediction and slot-filling tasks. Our extension is prompted by the scarcity of massively multilingual SLU datasets and the growing need for versatile speech datasets to assess foundation models (LLMs, speech encoders) across languages and tasks. We provide a multimodal, multitask, multilingual dataset and report SLU baselines using both cascaded and end-to-end architectures in various training scenarios (zero-shot, few-shot, and full fine-tune). Furthermore, we demonstrate the suitability of Speech-MASSIVE for benchmarking other tasks such as speech transcription, language identification, and speech translation. The dataset, models, and code are publicly available at: https://github.com/hlt-mt/Speech-MASSIVE\\n\\nIndex Terms: spoken language understanding, speech recognition, speech resources, multi-task model\\n\\n1. Introduction\\n\\nMultilingual speech corpora have limited coverage of speech-related tasks, primarily focusing on automatic speech recognition (ASR) [1, 2, 3, 4] and speech translation (ST) [5, 6, 7, 8], while neglecting spoken language understanding (SLU \u2013 the task of extracting semantic information from spoken utterances, which typically involves subtasks like intent detection and slot filling). Unlike text processing, where extensive efforts in natural language understanding (NLU) have led to resources covering a wide range of languages [9, 10, 11, 12], SLU datasets are mainly English-centric [13], with few exceptions [14, 15, 16].\\n\\nOur goal is to bridge the gap in multilingual SLU drawing inspiration from [16] and collecting speech recordings in multiple languages. We start with the MASSIVE NLU (i.e. textual) dataset [12], an ideal foundation due to its size, domain diversity, and broad coverage of languages, intent, and slot types. Developed by commissioning professional translators to localize the English SLURP dataset [13] into 51 languages, MASSIVE comprises 1M labeled utterances spanning 18 domains, with 60 intents and 55 slots. Our contribution, Speech-MASSIVE, spans 12 languages from diverse families: Arabic, German, Spanish, French, Hungarian, Korean, Dutch, Polish, European Portuguese, Russian, Turkish, and Vietnamese. It also facilitates evaluation across various speech tasks beyond SLU, including ASR, ST, and language identification (LID). We release Speech-MASSIVE publicly under CC-BY-SA license.\\n\\n2. Speech-MASSIVE\\n\\n2.1. Speech data collection and validation process\\n\\nWe created the speech counterpart of textual MASSIVE data by recruiting native speakers through the Prolific crowdsourcing platform.\\n\\nA first group of workers was instructed to record the spoken version of MASSIVE sentences with guidelines emphasizing the importance of accurate and natural reading, as well as proper recording conditions and strict adherence to the corresponding text. To ensure high final data quality, a second group of native speakers validated the recorded utterances. During validation, participants were directed to read the original text, listen to the recording, and label it as valid or invalid. Those marked as invalid underwent a second iteration of this two-step (recording and validation) process. After the second iteration, the process concluded, irrespective of the outcome of the second validation phase, to avoid potentially endless cycles. This decision was also informed by the observation that, upon inspecting the invalid recordings, we found some were marked as such not due to a lack of adherence of the speech to the text but because of grammatical errors in the original MASSIVE dataset text. Correcting these errors was beyond the scope of our work.\\n\\nTo further enhance the reliability of the collected dataset, we implemented two additional precautions. During the recording phase, we instructed participants to review their own recordings before proceeding to the next sample, allowing them to re-record if the audio was not properly acquired. Additionally, in the validation step, four speech utterances were chosen from Common Voice [1] and inserted among the samples for validation. Out of these four quality control samples, two intentionally featured audio-transcript mismatches to be marked as invalid. The other two cases had perfect audio-transcript alignment to be marked as valid. Care was taken to select quality control samples with clear and intelligible audio. Validation results from a Prolific user were retained only if they accurately assessed all four quality control samples. Any mistakes led to the disregarding of their validations, requiring the entire set of samples from that user to be re-validated by other participants.\\n\\nhttps://www.prolific.com, Compensated \u00a39 per hour.\"}"}
{"id": "lee24i_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1: Speech-MASSIVE's overall statistics.\\n\\n| lang | split  | # sample  | # valid | # hrs  | # spk (M/F/U) | WER | CER |\\n|------|--------|-----------|---------|--------|---------------|-----|-----|\\n| ar   | dev    | 2033      | 2027    | 2.12   | 36 (22/14/0)  | 31.75 | 14.43 |\\n|      | test   | 2974      | 2962    | 3.23   | 37 (15/17/5)  | 34.19 | 15.85 |\\n| de   | dev-full| 11514    | 11201   | 12.61  | 117 (50/63/4) | -   | -   |\\n|      | dev    | 2033      | 2032    | 2.33   | 68 (35/32/1)  | 11.24 | 3.96 |\\n|      | test   | 2974      | 2969    | 3.41   | 82 (36/36/10)| 11.84 | 4.16 |\\n| es   | dev    | 2033      | 2024    | 2.53   | 109 (51/53/5)| 7.61  | 3.00 |\\n|      | test   | 2974      | 2948    | 3.61   | 85 (37/33/15)| 8.95  | 3.76 |\\n| fr   | train-full| 11514 | 11481   | 12.42  | 103 (50/52/1)| -    | -   |\\n|      | dev    | 2033      | 2031    | 2.20   | 55 (26/26/3)  | 10.20 | 4.42 |\\n|      | test   | 2974      | 2972    | 2.65   | 75 (31/35/9)  | 11.09 | 4.71 |\\n| hu   | dev    | 2033      | 2019    | 2.27   | 69 (33/33/3)  | 25.96 | 10.93 |\\n|      | test   | 2974      | 2932    | 3.30   | 55 (25/24/6)  | 20.98 | 6.01 |\\n| ko   | dev    | 2033      | 2032    | 2.12   | 21 (8/13/0)   | 25.29 | 7.13 |\\n|      | test   | 2974      | 2970    | 2.66   | 31 (10/18/3)  | 26.42 | 8.04 |\\n| nl   | dev    | 2033      | 2032    | 2.14   | 37 (17/19/1)  | 11.03 | 3.98 |\\n|      | test   | 2974      | 2959    | 3.30   | 100 (48/49/3)| 10.52 | 3.82 |\\n| pl   | dev    | 2033      | 2024    | 2.24   | 105 (50/52/3)| 9.94  | 4.88 |\\n|      | test   | 2974      | 2933    | 3.21   | 151 (73/71/7)| 12.58 | 6.22 |\\n| pt   | dev    | 2033      | 2031    | 2.20   | 107 (51/53/3)| 11.73 | 5.10 |\\n|      | test   | 2974      | 2967    | 3.25   | 102 (48/50/4)| 12.11 | 5.13 |\\n| ru   | dev    | 2033      | 2032    | 2.25   | 40 (7/31/2)   | 8.55  | 4.06 |\\n|      | test   | 2974      | 2969    | 3.44   | 51 (25/23/3)  | 8.99  | 4.57 |\\n| tr   | dev    | 2033      | 2030    | 2.17   | 71 (36/34/1)  | 16.65 | 4.56 |\\n|      | test   | 2974      | 2950    | 3.00   | 42 (17/18/7)  | 18.06 | 5.05 |\\n| vi   | dev    | 2033      | 1978    | 2.10   | 28 (13/14/1)  | 16.65 | 10.5  |\\n|      | test   | 2974      | 2954    | 3.23   | 30 (11/14/5)  | 14.94 | 9.77 |\\n\\n**2.2. Overall statistics**\\n\\nWe chose 12 languages based on various criteria. Initially, we considered the number of registered users on Prolific, sorting the 51 languages covered in MASSIVE. Languages with fewer than 200 users were excluded to ensure sufficient worker participation to complete the entire acquisition and validation process in reasonable time. Italian was also excluded due to the availability of the full dataset elsewhere. Finally, with an eye at the balance between budget considerations and linguistic diversity, from the remaining 18 languages we selected Arabic, German, Spanish, French, Hungarian, Korean, Dutch, Polish, European Portuguese, Russian, Turkish, and Vietnamese.\\n\\nWe collected speech recordings for MASSIVE's development and test splits. Acquiring the full training dataset (11,514 utterances for each of the 12 languages) exceeded our budget. In a concession, our emphasis was placed on acquiring comprehensive training data for French and German, while we obtained limited few-shot training data consisting of 115 utterances from the training set for the remaining 10 languages.\\n\\nColumns 1-6 of Table 1 provide statistics for the collected dataset, including, for each language, the available data splits, the number of recordings, hours of speech, and speakers (total, male, female and unknown). The \"# valid\" column indicates the count of human-validated utterances for each data split after the two iterations. As a few speech recordings remained invalid after our two recording-validation cycles, we retained for each utterance the candidate with the lowest Word Error Rate (WER) as transcribed using Whisper. This ensures speech availability for all MASSIVE utterances, even if some may not perfectly align with the reference transcript. Additional information regarding this is included in the corpus metadata.\\n\\n**2.3. ASR assessment**\\n\\nTo assess Speech-MASSIVE in multilingual ASR, we used Whisper, since it is one of the recent state-of-the-art multilingual speech recognition models. We selected Whisper-large-v3, utilizing it without additional fine-tuning for our ASR evaluation. Table 1 shows WER and character error rate (CER) across languages and data splits. We compared ASR error rates to those obtained on the FLEURS dataset. FLEURS generally yields lower WERs/CERs compared to Speech-MASSIVE. The same observation was made for Italian in [16], which followed a recording methodology similar to ours. This suggests that the higher WERs are likely due to the inherent difficulty of MASSIVE utterances compared to those in FLEURS. Furthermore, there are still discrepancies between our Whisper model's hypotheses and the references in the MASSIVE dataset (e.g., numbers reported in letters in MASSIVE references), which we did not address as optimizing ASR WER was not our main goal. Finally, we calculated the correlation coefficient between WERs (CER for Korean) on Speech-MASSIVE and FLEURS, resulting in a value of 0.96. This shows that Whisper consistently performs across both datasets, despite Speech-MASSIVE being more challenging than FLEURS for ASR.\\n\\n### 3. SLU Baselines and Beyond\\n\\nIn this section, we establish several SLU baselines, evaluating them with different training conditions and metrics described in \u00a73.1. Firstly (\u00a73.2), we build a NLU model, serving as an upper bound free from ASR errors. Secondly, we build a cascaded SLU system (\u00a73.3), in which an ASR component transcribes input audio and the NLU model utilizes ASR output for inference. Thirdly, to complete the inventory of SLU baselines, we introduce an end-to-end (E2E) model (\u00a73.4). We conclude by showcasing the versatility of Speech-MASSIVE beyond SLU, computing additional baselines for tasks such as speech translation and language identification (\u00a73.5).\\n\\n### 3.1. NLU/SLU training conditions and metrics\\n\\nTo simulate different training resource scenarios, we report performance in three different settings:\\n\\n(a) **Zero-shot**: we train the model only with one language data from the train split (11,514 utterances) and evaluate in all other different languages;\\n\\n(b) **Few-shot**: we employ subsets (115 examples) for each of the 12 non-English languages, aligning with our train-115 split. Additionally, we integrate the full zero-shot training split to enrich the multilingual training dataset, totaling 12.8k samples for training;\\n\\n(c) **Full fine-tune (NLU only)**: 11,514 training examples of all 12 languages are pooled (138k samples for training).\\n\\nWe assess intent prediction in a given text or speech with intent accuracy. We report the average result (and standard deviation) of three runs with different seeds. All experiments were executed on 1 A100 80GB GPU.\\n\\n---\\n\\n3 https://hf.co/openai/whisper-large-v3\\n4 Accessible for our 12 languages except Arabic at https://github.com/openai/whisper/discussions/1762\\n5 train-115 covers all 18 domains, 60 intents, and 55 slots (including empty slots).\\n6 Due to space limitations, we report only intent accuracy scores. However, additional SLU metrics (e.g., micro-averaged slot F1, exact match accuracy, slot-type F1, slot-value CER) exhibit a similar trend and are available in the GitHub repository.\"}"}
{"id": "lee24i_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. NLU model\\n\\nOur NLU system uses the mT5 encoder-decoder architecture [18], selected for its superior performance as demonstrated in [12], where the mT5 text-to-text model outperformed both the mT5 encoder-only model and the XLM-R model [19]. We use a pre-trained mT5-base model, and fine-tune both the encoder and decoder in a sequence-to-sequence manner. We supply source and target texts as described in [12] and shown in Figure 2. For instance, the French sentence (Fr) \u201co`u puis-je aller ce soir\u201d is annotated in slots (Fr-Slots) as \u201cOther Other Other timeofday timeofday\u201d and intent (Intent) as \u201crecommendation events\u201d in MASSIVE. We adapt those annotations to create source and target texts to be used in training: for the source text (Fr-Src in [NLU]), we prepend \u201cAnnotate:\u201d to the French sentence (Fr); for the target text (Fr-Tgt in [NLU]), we concatenate slots (Fr-Slots) and intent (Intent).\\n\\nFigure 1 displays the intent accuracy results of our NLU system across all languages and modes (zero-shot, few-shot, full fine-tune), along with those of the cascaded SLU models discussed in \u00a73.3. Unsurprisingly, NLU performance increases when moving from zero-shot to full fine-tune regimes. Also, as expected, higher scores are observed for languages (Nl, Fr, De, Pt, Ru, Es and Pl) that are better represented in the mC4 multilingual dataset used to train mT5 model [18]. Finally, the highest results align with those reported in the MASSIVE paper [12], serving as a suitable reference upper bound for comparisons with the SLU models discussed in the following section.\\n\\n3.3. Cascaded SLU model\\n\\nWe develop a cascaded SLU system in which an ASR model based on Whisper-large-v3 transcribes the speech, and the same NLU models of \u00a73.2 (zero-shot, few-shot, full fine-tune) predict slots and intent from the transcribed texts.\\n\\nThe SLU intent accuracy scores in Figure 1 reveal that processing automatically transcribed utterances introduces performance drops of varying magnitude across the different languages and training modes. This is especially notable for languages with lower ASR quality (i.e., higher WER), such as Ar, Hu, Ko, Tr, and Vn. This supports our expectations about the difficulty for the downstream textual NLU component of the SLU cascade to handle unrecoverable transcription errors. As a matter of fact, in zero-shot mode, the distance with the text-only upper-bound NLU system is considerably smaller for languages featuring higher ASR quality. Similar to what we observe, the gap with the text-only upper-bound NLU system is considerably smaller for languages featuring higher ASR quality.\\n\\n3.4. E2E SLU model\\n\\nTo complete the inventory of SLU baselines for comparison, we introduce an end-to-end (E2E) SLU model: a direct solution that bypasses intermediate text representations (ASR transcriptions). We utilize Whisper, following the approach proposed in [20], which showed superior performance compared to cascaded systems and other speech encoders like wav2vec2.0 [21] and HuBERT [22]. Model training follows a sequence-to-sequence approach, with predictions extended to include transcript, slots, and intent. This allows us to leverage both speech and text information in the model\u2019s predictions. We introduce an end-to-end (E2E) SLU model: a direct solution that bypasses intermediate text representations (ASR transcriptions). We utilize Whisper, following the approach proposed in [20], which showed superior performance compared to cascaded systems and other speech encoders like wav2vec2.0 [21] and HuBERT [22]. Model training follows a sequence-to-sequence approach, with predictions extended to include transcript, slots, and intent. This allows us to leverage both speech and text information in the model\u2019s predictions.\"}"}
{"id": "lee24i_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: LID accuracy and ST BLEU results with Whisper-large-v3 on Speech-MASSIVE.\\n\\n| lang | split | LID accuracy | ST BLEU |\\n|------|-------|--------------|---------|\\n| ar   | dev   | 90.9         | 17.2    |\\n| de   | test  | 89.5         | 16.6    |\\n| es   | dev   | 88.9         | 36.7    |\\n| fr   | test  | 98.4         | 38.2    |\\n| hu   | dev   | 94.7         | 19.4    |\\n| ko   | test  | 95.8         | 20.6    |\\n| nl   | dev   | 99.1         | 19.7    |\\n| pl   | test  | 94.9         | 40.0    |\\n| pt   | dev   | 95.3         | 29.9    |\\n| ru   | test  | 95.9         | 32.4    |\\n| tr   | dev   | 99.1         | 28.4    |\\n| vi   | test  | 96.1         | 26.7    |\\n\\n| lang | split | LID accuracy | ST BLEU |\\n|------|-------|--------------|---------|\\n| ar   | test  | 90.7         | 93.2    |\\n| de   | dev   | 93.0         | 94.7    |\\n| es   | test  | 99.0         | 95.3    |\\n| fr   | dev   | 98.8         | 94.6    |\\n| hu   | test  | 94.8         | 95.9    |\\n| ko   | dev   | 98.7         | 96.1    |\\n| nl   | test  | 99.1         | 96.0    |\\n| pl   | dev   | 94.6         | 90.7    |\\n| pt   | test  | 95.8         | 90.7    |\\n| ru   | dev   | 95.8         | 90.7    |\\n| tr   | test  | 96.1         | 90.7    |\\n| vi   | dev   | 97.0         | 90.7    |\\n\\nduce an additional separator ''|'' between the tasks, allowing Whisper's tokenizer to tokenize the target text as is, without the need to add slots or intents to the original vocabulary. Two specific tokens, ''|'' and '', are removed from Whisper's suppressed token list, as they are required for predicting SLU outputs as task separators and in certain intent values. In zero-shot mode, we fine-tune Whisper-large-v3 with either a) the English train set of [13], or b) the French train set of Speech-MASSIVE. These two conditions (En vs Fr) allow us to investigate the impact of the training language on zero-shot E2E SLU across all other languages. Additionally, in few-shot mode, we fine-tune Whisper-large-v3 with the English or French train sets, along with train-115 splits from other languages. We do not provide a full fine-tune E2E SLU mode since only two languages in Speech-MASSIVE are supported by full train splits.\\n\\nTable 3 compares cascaded and E2E SLU performance in both zero-shot and few-shot modes. It is worth noting that the comparison between the two approaches is fair only when using the English train set (En), since they utilize the same training utterances albeit in different modalities (written form for cascade and spoken form for E2E). In this condition (En), for zero-shot mode, cascaded SLU outperforms E2E SLU for all languages. In few-shot mode, we note a different trend, with cascaded and E2E models exhibiting similar average performance. Employing the French training set from Speech-MASSIVE (Fr), E2E SLU surpasses models trained on the English dataset from [13] (En) in both zero-shot and few-shot modes. In zero-shot mode, we observe improvements of more than 5 points for 9 out of 11 languages. In few-shot mode, although the influence of the training language (En vs Fr) diminishes due to multilingual training, using French as the majority language still yields better performance than using English. These results highlight the significant influence of the 'training language' on the performance of E2E SLU models in zero/few-shot settings. Speech-MASSIVE provides a unique opportunity to explore this intriguing observation further. Finally, examining French (Fr) results representing the full fine-tune mode for this language, E2E SLU achieves intent accuracy of 85.87%, compared to 84.73% for cascaded SLU and 87.43% for NLU given in Fig.1.\\n\\n3.5. Other baselines\\n\\nWe conclude our experiments using Whisper-large-v3 without any finetuning to compute other baselines and demonstrate the versatility of Speech-MASSIVE. We perform Language Identification (LID) and Speech Translation (ST) across language directions. Different types of tokens are fed to Whisper's decoder depending on the tasks as shown in Figure 3. Table 2 reports Whisper-large-v3 model's LID accuracy and ST BLEU results on Speech-MASSIVE. LID is calculated over all the samples in dev and test splits. For ST, instead, BLEU is computed on subsets of dev and test splits identified using meta information from MASSIVE to exclude samples with localized translation. This filtering is necessary to ensure an accurate assessment of translation quality, as localized references may introduce discrepancies in word choice (see \u00a71). Besides indicating the versatility of Speech-MASSIVE for evaluation purposes, our additional baselines on speech-related tasks offer valuable reference scores for cross-task comparisons and for exploring collaborative solutions to leverage potential mutual benefits.\\n\\nTable 3: Intent accuracy of cascaded and E2E SLU. Both E2E SLU zero-shot and few-shot models are trained either with initial English train set of [13] (En) or with French train set of Speech-MASSIVE (Fr). We exclude French (*) from the average as fr-FR scores are no longer zero/few-shot when French is used as the training language.\\n\\n| lang | Casc. (En) zero-shot | E2E (En) zero-shot | E2E (Fr) zero-shot | Casc. (En) few-shot | E2E (En) few-shot | E2E (Fr) few-shot |\\n|------|---------------------|--------------------|-------------------|-------------------|------------------|------------------|\\n| ar   | 49.27 \u00b1 0.90        | 33.04 \u00b1 4.74       | 40.00 \u00b1 2.44      | 54.56 \u00b1 0.73      | 57.71 \u00b1 1.46     | 61.22 \u00b1 1.74     |\\n| de   | 76.29 \u00b1 0.14        | 70.68 \u00b1 1.37       | 73.91 \u00b1 0.73      | 78.08 \u00b1 0.50      | 78.64 \u00b1 0.65     | 78.45 \u00b1 0.64     |\\n| es   | 75.70 \u00b1 0.19        | 73.12 \u00b1 0.75       | 78.62 \u00b1 0.41      | 78.05 \u00b1 0.33      | 79.79 \u00b1 0.66     | 80.59 \u00b1 0.31     |\\n| fr   | 75.61 \u00b1 0.48        | 68.43 \u00b1 2.30       | 85.87 \u00b1 0.26*     | 77.56 \u00b1 0.13      | 77.11 \u00b1 0.77     | 85.93 \u00b1 0.35*    |\\n| hu   | 63.43 \u00b1 0.92        | 36.62 \u00b1 1.49       | 42.28 \u00b1 2.20      | 68.70 \u00b1 0.80      | 60.75 \u00b1 2.40     | 63.93 \u00b1 0.19     |\\n| ko   | 60.93 \u00b1 0.84        | 57.96 \u00b1 2.26       | 66.09 \u00b1 1.86      | 68.11 \u00b1 0.04      | 72.82 \u00b1 0.23     | 74.09 \u00b1 0.73     |\\n| nl   | 78.82 \u00b1 0.45        | 65.17 \u00b1 0.57       | 67.24 \u00b1 1.44      | 78.93 \u00b1 0.34      | 77.49 \u00b1 0.77     | 77.37 \u00b1 0.47     |\\n| pl   | 74.57 \u00b1 0.37        | 64.82 \u00b1 1.51       | 64.38 \u00b1 1.29      | 76.11 \u00b1 0.39      | 74.85 \u00b1 0.58     | 76.88 \u00b1 1.37     |\\n| pt   | 73.12 \u00b1 0.49        | 62.91 \u00b1 1.97       | 72.60 \u00b1 1.01      | 77.21 \u00b1 0.65      | 78.15 \u00b1 1.16     | 80.02 \u00b1 0.29     |\\n| ru   | 75.96 \u00b1 0.19        | 69.06 \u00b1 1.71       | 74.75 \u00b1 0.28      | 76.96 \u00b1 0.08      | 79.22 \u00b1 0.67     | 79.51 \u00b1 0.26     |\\n| tr   | 65.32 \u00b1 0.61        | 47.60 \u00b1 3.08       | 55.08 \u00b1 1.09      | 70.32 \u00b1 0.48      | 69.44 \u00b1 1.62     | 71.14 \u00b1 1.15     |\\n| vi   | 60.19 \u00b1 0.39        | 35.44 \u00b1 1.48       | 49.67 \u00b1 2.30      | 64.77 \u00b1 0.98      | 63.36 \u00b1 1.69     | 68.71 \u00b1 0.33     |\\n\\navg. 69.10 \u00b1 0.19        | 57.07 \u00b1 1.82       | 62.24 \u00b1 0.92       | 72.45 \u00b1 0.32      | 72.45 \u00b1 0.53     | 73.81 \u00b1 0.58     |\\n\\n4. Conclusion\\n\\nWe introduced Speech-MASSIVE, a multilingual SLU dataset spanning 12 languages for intent prediction and slot-filling tasks. Alongside dataset creation, we established baselines for SLU across various resource and architecture configurations. Additionally, we showcased Speech-MASSIVE's versatility beyond SLU, extending to tasks such as ASR, LID, and ST. With its diverse array of native speakers and recording environments, Speech-MASSIVE holds promise as a benchmark for multilingual, multimodal, and multi-task speech research. Future research opportunities include exploring further the influence of training languages on zero/few-shot SLU performance, thoroughly comparing cascade and E2E SLU solutions, assess the effect of including multi-task and multilingual corpora in the training of speech foundation models, and pushing the boundaries of E2E multi-task speech systems beyond our baselines.\\n\\nAcknowledgements.\\n\\nThe speech collection was funded by EU Horizon Europe (HE) Research and Innovation programme grant No 101070631. We also acknowledge the support of the PNRR project FAIR - Future AI Research (PE00000013), under the NRRP MUR program funded by the NextGenerationEU.\"}"}
