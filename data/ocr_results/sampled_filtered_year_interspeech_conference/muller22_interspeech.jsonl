{"id": "muller22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] \\\"Audio deep fake: Demonstrator entwickelt am fraunhofer aise -\\nyoutube,\\\" https://www.youtube.com/watch?v=MZTF0eAALmE, (Accessed on 04/01/2021).\\n\\n[2] \\\"Deepfake video of volodymyr zelensky surrendering surfaces:\\nyoutube,\\\" https://www.youtube.com/watch?v=X17yrEV5sl4, (Accessed on 03/23/2022).\\n\\n[3] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \\\"Wavenet: A generative model for raw audio,\\\" arXiv preprint arXiv:1609.03499, 2016.\\n\\n[4] Y. Wang, R. J. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. V. Le, Y. Agiomyrgiannakis, R. Clark, and R. A. Saurous, \\\"Tacotron: A fully end-to-end text-to-speech synthesis model,\\\" CoRR, vol. abs/1703.10135, 2017. [Online]. Available: http://arxiv.org/abs/1703.10135\\n\\n[5] M. Todisco, X. Wang, V. Vestman, M. Sahidullah, H. Delgado, A. Nautsch, J. Yamagishi, N. Evans, T. Kinnunen, and K. A. Lee, \\\"Asvspoof 2019: Future horizons in spoofed and fake audio de-\\ndetection,\\\" arXiv preprint arXiv:1904.05441, 2019.\\n\\n[6] A. Nautsch, X. Wang, N. Evans, T. H. Kinnunen, V. Vestman, M. Todisco, H. Delgado, M. Sahidullah, J. Yamagishi, and K. A. Lee, \\\"ASVspoof 2019: Spoofing Countermeasures for the Detection of Synthesized, Converted and Replayed Speech,\\\" vol. 3, no. 2, pp. 252\u2013265.\\n\\n[7] J. Yamagishi, C. Veaux, and K. MacDonald, \\\"CSTR VCTK Corpus: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92),\\\" 2019.\\n\\n[8] A. Gomez-Alanis, A. M. Peinado, J. A. Gonzalez, and A. M. Gomez, \\\"A Gated Recurrent Convolutional Neural Network for Robust Spoofing Detection,\\\" vol. 27, no. 12, pp. 1985\u20131999.\\n\\n[9] A. Chintha, B. Thai, S. J. Sohrawardi, K. M. Bhatt, A. Hickerson, M. Wright, and R. Ptucha, \\\"Recurrent Convolutional Structures for Audio Spoof and Video Deepfake Detection,\\\" pp. 1\u20131.\\n\\n[10] L. Zhang, X. Wang, E. Cooper, J. Yamagishi, J. Patino, and N. Evans, \\\"An initial investigation for detecting partially spoofed audio,\\\" arXiv preprint arXiv:2104.02518, 2021.\\n\\n[11] S. Tambe, A. Pawar, and S. Yadav, \\\"Deep fake videos identification using ann and lstm,\\\" Journal of Discrete Mathematical Sciences and Cryptography, vol. 24, no. 8, pp. 2353\u20132364, 2021.\\n\\n[12] X. Wang and J. Yamagishi. A Comparative Study on Recent Neural Spoofing Countermeasures for Synthetic Speech Detection. [Online]. Available: http://arxiv.org/abs/2103.11326\\n\\n[13] G. Lavrentyeva, S. Novoselov, E. Malykh, A. Kozlov, O. Kudashchev, and V. Shchemelinin, \\\"Audio replay attack detection with deep learning frameworks,\\\" in Interspeech 2017. ISCA, pp. 82\u201386. [Online]. Available: http://www.isca-speech.org/archive/Interspeech2017/abstracts/0360.html\\n\\n[14] G. Lavrentyeva, S. Novoselov, A. Tseren, M. Volkova, A. Gorlanov, and A. Kozlov, \\\"STC antispoofing systems for the ASVspoof2019 challenge,\\\" in Interspeech 2019. ISCA, pp. 1033\u20131037. [Online]. Available: http://www.isca-speech.org/archive/Interspeech2019/abstracts/1768.html\\n\\n[15] D. Afchar, V. Nozick, J. Yamagishi, and I. Echizen, \\\"MesoNet: A Compact Facial Video Forgery Detection Network,\\\" in 2018 IEEE International Workshop on Information Forensics and Security (WIFS), pp. 1\u20137.\\n\\n[16] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \\\"Going deeper with convolutions,\\\" in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1\u20139.\\n\\n[17] M. Alzantot, Z. Wang, and M. B. Srivastava, \\\"Deep Residual Neural Networks for Audio Spoofing Detection,\\\" in Interspeech 2019. ISCA, pp. 1078\u20131082. [Online]. Available: http://www.isca-speech.org/archive/Interspeech2019/abstracts/3174.html\\n\\n[18] Y. Zhang, F. Jiang, and Z. Duan, \\\"One-class learning towards synthetic voice spoofing detection,\\\" IEEE Signal Processing Letters, vol. 28, pp. 937\u2013941, 2021.\\n\\n[19] J. Monteiro, J. Alam, and T. H. Falk, \\\"Generalized end-to-end detection of spoofing attacks to automatic speaker recognizers,\\\" Computer Speech & Language, vol. 63, p. 101096, 2020.\\n\\n[20] K. He, X. Zhang, S. Ren, and J. Sun, \\\"Deep residual learning for image recognition,\\\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770\u2013778.\\n\\n[21] Z. Zhang, X. Yi, and X. Zhao, \\\"Fake speech detection using residual network with transformer encoder,\\\" in Proceedings of the 2021 ACM Workshop on Information Hiding and Multimedia Security, 2021, pp. 13\u201322.\\n\\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \\\"Attention is all you need,\\\" Advances in neural information processing systems, vol. 30, 2017.\\n\\n[23] H. Tak, J. Patino, M. Todisco, A. Nautsch, N. Evans, and A. Larcher, \\\"End-to-End anti-spoofing with RawNet2,\\\" in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6369\u20136373.\\n\\n[24] M. Ravanelli and Y. Bengio, \\\"Speaker recognition from raw waveform with sincnet,\\\" in 2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2018, pp. 1021\u20131028.\\n\\n[25] W. Ge, J. Patino, M. Todisco, and N. Evans, \\\"Raw differentiable architecture search for speech deepfake and spoofing detection,\\\" arXiv preprint arXiv:2107.12212, 2021.\\n\\n[26] H. Tak, J.-w. Jung, J. Patino, M. Kamble, M. Todisco, and N. Evans, \\\"End-to-end spectro-temporal graph attention networks for speaker verification anti-spoofing and speech deepfake detection,\\\" arXiv preprint arXiv:2107.12710, 2021.\\n\\n[27] D. P. Kingma and J. Ba, \\\"Adam: A method for stochastic optimization,\\\" in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2015. [Online]. Available: http://arxiv.org/abs/1412.6980\\n\\n[28] N. M. M \u00fcller, F. Dieckmann, P. Czempin, R. Canals, J. Williams, and K. B \u00f6ttinger. Speech is Silver, Silence is Golden: What do ASVspoof-trained Models Really Learn? [Online]. Available: http://arxiv.org/abs/2106.12914\\n\\n[29] T. Kinnunen, K. A. Lee, H. Delgado, N. Evans, M. Todisco, M. Sahidullah, J. Yamagishi, and D. A. Reynolds, \\\"t-DCF: a detection cost function for the tandem assessment of spoofing countermeasures and automatic speaker verification,\\\" in Odyssey 2018 The Speaker and Language Recognition Workshop. ISCA, pp. 312\u2013319.\\n\\n[30] \\\"tdcf official implementation,\\\" https://www.asvspoof.org/asvspoof2019/tDCFpythonv1.zip, (Accessed on 03/03/2022).\\n\\n[31] J. C. Brown, \\\"Calculation of a constant q spectral transform,\\\" The Journal of the Acoustical Society of America, vol. 89, no. 1, pp. 425\u2013434, 1991.\\n\\n[32] S. S. Stevens, J. Volkmann, and E. B. Newman, \\\"A scale for the measurement of the psychological magnitude pitch,\\\" The journal of the acoustical society of america, vol. 8, no. 3, pp. 185\u2013190, 1937.\\n\\n[33] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar, E. Battenberg, and O. Nieto, \\\"librosa: Audio and music signal analysis in python,\\\" in Proceedings of the 14th python in science conference, vol. 8. Citeseer, 2015, pp. 18\u201325.\\n\\n[34] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright et al. \\\"Scipy 1.0: fundamental algorithms for scientific computing in python,\\\" Nature methods, vol. 17, no. 3, pp. 261\u2013272, 2020.\"}"}
{"id": "muller22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Does Audio Deepfake Detection Generalize?\\n\\nNicolas M. M\u00fcller1, Pavel Czempin2, Franziska Dieckmann2, Adam Froghyar3, Konstantin B\u00f6ttiger1\\n\\n1 Fraunhofer AISEC\\n2 Technical University Munich\\n3 why do birds GmbH\\n\\nnicolas.mueller@aisec.fraunhofer.de\\n\\nAbstract\\n\\nCurrent text-to-speech algorithms produce realistic fakes of human voices, making deepfake detection a much-needed area of research. While researchers have presented various deep learning models for audio spoofing detection, it is often unclear exactly why these architectures are successful: Preprocessing steps, hyperparameter settings, and the degree of fine-tuning are not consistent across related work. Which factors contribute to success, and which are accidental?\\n\\nIn this work, we address this problem: We systematize audio spoofing detection by re-implementing and uniformly evaluating twelve architectures from related work. We identify overarching features for successful audio deepfake detection, such as using \\\\textit{cqtspec} or \\\\textit{logspec} features instead of \\\\textit{melspec} features, which improves performance by 37\\\\% EER on average, all other factors constant.\\n\\nAdditionally, we evaluate generalization capabilities: We collect and publish a new dataset consisting of 37.9 hours of found audio recordings of celebrities and politicians, of which 17.2 hours are deepfakes. We find that related work performs poorly on such real-world data (performance degradation of up to one thousand percent). This could suggest that the community has tailored its solutions too closely to the prevailing ASVspoof benchmark and that deepfakes are much harder to detect outside the lab than previously thought.\\n\\n1. Introduction\\n\\nModern text-to-speech synthesis (TTS) is capable of realistic fakes of human voices, also known as audio deepfakes or spoofs. While there are many ethical applications of this technology, there is also a serious risk of malicious use. For example, TTS technology enables the cloning of politicians' voices [1, 2], which poses a variety of risks to society, including the spread of misinformation.\\n\\nReliable detection of speech spoofing can help mitigate such risks and is therefore an active area of research. However, since the technology to create audio deepfakes has only been available for a few years (see Wavenet [3] and Tacotron [4], published in 2016/17), audio spoof detection is still in its infancy. While many approaches have been proposed (cf. Section 2), it is still difficult to understand why some of the models work well: Each work uses different feature extraction techniques, preprocessing steps, hyperparameter settings, and fine-tuning. Which are the main factors and drivers for models to perform well? What can be learned in principle for the development of such systems?\\n\\nFurthermore, the evaluation of spoof detection models has so far been performed exclusively on the ASVspoof dataset [5, 6], which means that the reported performance of these models is based on a limited set of TTS synthesis algorithms. ASVspoof is based on the VCTK dataset [7], which exclusively features professional speakers and has been recorded in a studio environment, using a semi-anechoic chamber. What can we expect from audio spoof detection trained on this dataset? Is it capable of detecting realistic, unseen, 'in-the-wild' audio spoofs like those encountered on social media?\\n\\nTo answer these questions, this paper presents the following contributions:\\n\\n\u2022 We reimplement twelve of the most popular architectures from related work and evaluate them according to a common standard. We systematically exchange components to attribute performance reported in related work to either model architecture, feature extraction, or data preprocessing techniques. In this way, we identify fundamental properties for well-performing audio deepfake detection.\\n\\n\u2022 To investigate the applicability of related work in the real world, we introduce a new audio deepfake dataset. We collect 17.2 hours of high-quality audio deepfakes and 20.7 hours of authentic material from 58 politicians and celebrities.\\n\\n\u2022 We show that established models generally perform poorly on such real-world data. This discrepancy between reported and actual generalization ability suggests that the detection of audio fakes is a far more difficult challenge than previously thought.\\n\\n2. Related Work\\n\\n2.1. Model Architectures\\n\\nThere is a significant body of work on audio spoof detection, driven largely by the ASVspoof challenges and datasets [5, 6]. In this section, we briefly present the architectures and models used in our evaluation in Section 5.\\n\\nLSTM-based models. Recurrent architectures are a natural choice in the area of language processing, with numerous related work utilizing such models [8, 9, 10, 11]. As a baseline for evaluating this approach, we implement a simple LSTM model: it consists of three LSTM layers followed by a single linear layer. The output is averaged over the time dimension to obtain a single embedding vector.\\n\\nLCNN. Another common architecture for audio spoof detection are LCNN-based learning models such as LCNN, LCNN-Attention, and LCNN-LSTM [12, 13, 14]. LCNNs combine convolutional layers with Max-Feature-Map activations to create \u2018light\u2019 convolutional neural networks. LCNN-Attention has an added single-head-attention pooling layer, while LCNN-LSTM uses a Bi-LSTM layer and a skip connection.\\n\\nMesoNet. MesoNet is based on the Meso-4 architecture, which was originally used for detecting facial video.\\n\\nhttps://deepfake-demo.aisec.fraunhofer.de/in the wild\"}"}
{"id": "muller22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"deepfakes. It uses 4 convolutional layers in addition to Batch Normalization, Max Pooling, and a fully connected classifier. Based on the facial deepfake detector Meso-Inception-4 [15], MesoInception extends the Meso-4 architecture with Inception blocks [16]. ResNet18. Residual Networks were first used for audio deepfake detection by [17], and continue to be employed [18, 19]. This architecture, first introduced in the computer vision domain [20], uses convolutional layers and shortcut connections, which avoids the vanishing gradient problem and allows to design especially deep networks (18 layers for ResNet18).\\n\\nTransformer. The Transformer architecture has also found its way into the field of audio spoof detection [21]. We use four self-attention layers with 256 hidden dimensions and skip-connections, and encode time with positional encodings [22]. CRNNSpoof. This end-to-end architecture combines 1D convolutions with recurrent layers to learn features directly from raw audio samples [9]. RawNet2 [23] is another end-to-end model. It employs Sinc-Layers [24], which correspond to rectangular band-pass filters, to extract information directly from raw waveforms. RawPC is an end-to-end model which also uses Sinc-layers to operate directly on raw waveforms. The architecture is found via differentiable architecture search [25]. RawGAT-ST, a spectro-temporal graph attention network (GAT), trained in an end-to-end fashion. It introduces spectral and temporal sub-graphs and a graph pooling strategy, and reports state-of-the-art spoof detection capabilities [26], which we can verify experimentally, c.f. Table 1.\\n\\n3. Datasets\\nTo train and evaluate our models, we use the ASVspoof 2019 dataset [5], in particular its Logical Access (LA) part. It consists of audio files that are either real (i.e., authentic recordings of human speech) or fake (i.e., synthesized or faked audio). The spoofed audio files are from 19 different TTS synthesis algorithms. From a spoofing detection point of view, ASVspoof considers synthetic utterances as a threat to the authenticity of the human voice, and therefore labels them as 'attacks'. In total, there are 19 different attackers in the ASVspoof 2019 dataset, labeled A1 - A19. For each attacker, there are 4914 synthetic audio recordings and 7355 real samples. This dataset is arguably the best known audio deefake dataset used by almost all related work.\\n\\nIn order to evaluate our models on realistic unseen data in-the-wild, we additionally create and publish a new audio deefake dataset, c.f. Figure 1. It consists of 37.9 hours of audio clips that are either fake (17.2 hours) or real (20.7 hours). We feature English-speaking celebrities and politicians, both from present and past. The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes. Since the speakers talk absurdly and out-of-character ('Donald Trump reads Star Wars'), it is easy to verify that the audio files are really spoofed. We then manually collect corresponding genuine instances from the same speakers using publicly available material such as podcasts, speeches, etc. We take care to include clips where the type of speaker, style, emotions, etc. are similar to the fake (e.g., for a fake speech by Barack Obama, we include an authentic speech and try to find similar values for background noise, duration, etc.). The clips have an average length of 4.3 seconds and are available at deepfake-demo.aisec.fraunhofer.de/inthe-wild. We feature English-speaking celebrities and politicians, both from present and past. The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes. Since the speakers talk absurdly and out-of-character ('Donald Trump reads Star Wars'), it is easy to verify that the audio files are really spoofed. We then manually collect corresponding genuine instances from the same speakers using publicly available material such as podcasts, speeches, etc. We take care to include clips where the type of speaker, style, emotions, etc. are similar to the fake (e.g., for a fake speech by Barack Obama, we include an authentic speech and try to find similar values for background noise, duration, etc.). The clips have an average length of 4.3 seconds and are available at deepfake-demo.aisec.fraunhofer.de/inthe-wild.\\n\\nIn order to evaluate our models on realistic unseen data in-the-wild, we additionally create and publish a new audio deefake dataset, c.f. Figure 1. It consists of 37.9 hours of audio clips that are either fake (17.2 hours) or real (20.7 hours). We feature English-speaking celebrities and politicians, both from present and past. The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes. Since the speakers talk absurdly and out-of-character ('Donald Trump reads Star Wars'), it is easy to verify that the audio files are really spoofed. We then manually collect corresponding genuine instances from the same speakers using publicly available material such as podcasts, speeches, etc. We take care to include clips where the type of speaker, style, emotions, etc. are similar to the fake (e.g., for a fake speech by Barack Obama, we include an authentic speech and try to find similar values for background noise, duration, etc.). The clips have an average length of 4.3 seconds and are available at deepfake-demo.aisec.fraunhofer.de/inthe-wild.\\n\\nIn order to evaluate our models on realistic unseen data in-the-wild, we additionally create and publish a new audio deefake dataset, c.f. Figure 1. It consists of 37.9 hours of audio clips that are either fake (17.2 hours) or real (20.7 hours). We feature English-speaking celebrities and politicians, both from present and past. The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes. Since the speakers talk absurdly and out-of-character ('Donald Trump reads Star Wars'), it is easy to verify that the audio files are really spoofed. We then manually collect corresponding genuine instances from the same speakers using publicly available material such as podcasts, speeches, etc. We take care to include clips where the type of speaker, style, emotions, etc. are similar to the fake (e.g., for a fake speech by Barack Obama, we include an authentic speech and try to find similar values for background noise, duration, etc.). The clips have an average length of 4.3 seconds and are available at deepfake-demo.aisec.fraunhofer.de/inthe-wild.\\n\\nIn order to evaluate our models on realistic unseen data in-the-wild, we additionally create and publish a new audio deefake dataset, c.f. Figure 1. It consists of 37.9 hours of audio clips that are either fake (17.2 hours) or real (20.7 hours). We feature English-speaking celebrities and politicians, both from present and past. The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes. Since the speakers talk absurdly and out-of-character ('Donald Trump reads Star Wars'), it is easy to verify that the audio files are really spoofed. We then manually collect corresponding genuine instances from the same speakers using publicly available material such as podcasts, speeches, etc. We take care to include clips where the type of speaker, style, emotions, etc. are similar to the fake (e.g., for a fake speech by Barack Obama, we include an authentic speech and try to find similar values for background noise, duration, etc.). The clips have an average length of 4.3 seconds and are available at deepfake-demo.aisec.fraunhofer.de/inthe-wild.\\n\\nIn order to evaluate our models on realistic unseen data in-the-wild, we additionally create and publish a new audio deefake dataset, c.f. Figure 1. It consists of 37.9 hours of audio clips that are either fake (17.2 hours) or real (20.7 hours). We feature English-speaking celebrities and politicians, both from present and past. The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes. Since the speakers talk absurdly and out-of-character ('Donald Trump reads Star Wars'), it is easy to verify that the audio files are really spoofed. We then manually collect corresponding genuine instances from the same speakers using publicly available material such as podcasts, speeches, etc. We take care to include clips where the type of speaker, style, emotions, etc. are similar to the fake (e.g., for a fake speech by Barack Obama, we include an authentic speech and try to find similar values for background noise, duration, etc.). The clips have an average length of 4.3 seconds and are available at deepfake-demo.aisec.fraunhofer.de/inthe-wild.\\n\\nIn order to evaluate our models on realistic unseen data in-the-wild, we additionally create and publish a new audio deefake dataset, c.f. Figure 1. It consists of 37.9 hours of audio clips that are either fake (17.2 hours) or real (20.7 hours). We feature English-speaking celebrities and politicians, both from present and past. The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes. Since the speakers talk absurdly and out-of-character ('Donald Trump reads Star Wars'), it is easy to verify that the audio files are really spoofed. We then manually collect corresponding genuine instances from the same speakers using publicly available material such as podcasts, speeches, etc. We take care to include clips where the type of speaker, style, emotions, etc. are similar to the fake (e.g., for a fake speech by Barack Obama, we include an authentic speech and try to find similar values for background noise, duration, etc.). The clips have an average length of 4.3 seconds and are available at deepfake-demo.aisec.fraunhofer.de/inthe-wild.\\n\\nIn order to evaluate our models on realistic unseen data in-the-wild, we additionally create and publish a new audio deefake dataset, c.f. Figure 1. It consists of 37.9 hours of audio clips that are either fake (17.2 hours) or real (20.7 hours). We feature English-speaking celebrities and politicians, both from present and past. The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes. Since the speakers talk absurdly and out-of-character ('Donald Trump reads Star Wars'), it is easy to verify that the audio files are really spoofed. We then manually collect corresponding genuine instances from the same speakers using publicly available material such as podcasts, speeches, etc. We take care to include clips where the type of speaker, style, emotions, etc. are similar to the fake (e.g., for a fake speech by Barack Obama, we include an authentic speech and try to find similar values for background noise, duration, etc.). The clips have an average length of 4.3 seconds and are available at deepfake-demo.aisec.fraunhofer.de/inthe-wild.\\n\\nIn order to evaluate our models on realistic unseen data in-the-wild, we additionally create and publish a new audio deefake dataset, c.f. Figure 1. It consists of 37.9 hours of audio clips that are either fake (17.2 hours) or real (20.7 hours). We feature English-speaking celebrities and politicians, both from present and past. The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes. Since the speakers talk absurdly and out-of-character ('Donald Trump reads Star Wars'), it is easy to verify that the audio files are really spoofed. We then manually collect corresponding genuine instances from the same speakers using publicly available material such as podcasts, speeches, etc. We take care to include clips where the type of speaker, style, emotions, etc. are similar to the fake (e.g., for a fake speech by Barack Obama, we include an authentic speech and try to find similar values for background noise, duration, etc.). The clips have an average length of 4.3 seconds and are available at deepfake-demo.aisec.fraunhofer.de/inthe-wild.\\n\\nIn order to evaluate our models on realistic unseen data in-the-wild, we additionally create and publish a new audio deefake dataset, c.f. Figure 1. It consists of 37.9 hours of audio clips that are either fake (17.2 hours) or real (20.7 hours). We feature English-speaking celebrities and politicians, both from present and past. The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes. Since the speakers talk absurdly and out-of-character ('Donald Trump reads Star Wars'), it is easy to verify that the audio files are really spoofed. We then manually collect corresponding genuine instances from the same speakers using publicly available material such as podcasts, speeches, etc. We take care to include clips where the type of speaker, style, emotions, etc. are similar to the fake (e.g., for a fake speech by Barack Obama, we include an authentic speech and try to find similar values for background noise, duration, etc.). The clips have an average length of 4.3 seconds and are available at deepfake-demo.aisec.fraunhofer.de/inthe-wild.\\n\\nIn order to evaluate our models on realistic unseen data in-the-wild, we additionally create and publish a new audio deefake dataset, c.f. Figure 1. It consists of 37.9 hours of audio clips that are either fake (17.2 hours) or real (20.7 hours). We feature English-speaking celebrities and politicians, both from present and past. The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes. Since the speakers talk absurdly and out-of-character ('Donald Trump reads Star Wars'), it is easy to verify that the audio files are really spoofed. We then manually collect corresponding genuine instances from the same speakers using publicly available material such as podcasts, speeches, etc. We take care to include clips where the type of speaker, style, emotions, etc. are similar to the fake (e.g., for a fake speech by Barack Obama, we include an authentic speech and try to find similar values for background noise, duration, etc.). The clips have an average length of 4.3 seconds and are available at deepfake-demo.aisec.fraunhofer.de/inthe-wild.\\n\\nIn order to evaluate our models on realistic unseen data in-the-wild, we additionally create and publish a new audio deefake dataset, c.f. Figure 1. It consists of 37.9 hours of audio clips that are either fake (17.2 hours) or real (20.7 hours). We feature English-speaking celebrities and politicians, both from present and past. The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes. Since the speakers talk absurdly and out-of-character ('Donald Trump reads Star Wars'), it is easy to verify that the audio files are really spoofed. We then manually collect corresponding genuine instances from the same speakers using publicly available material such as podcasts, speeches, etc. We take care to include clips where the type of speaker, style, emotions, etc. are similar to the fake (e.g., for a fake speech by Barack Obama, we include an authentic speech and try to find similar values for background noise, duration, etc.). The clips have an average length of 4.3 seconds and are available at deepfake-demo.aisec.fraunhofer.de/inthe-wild.\\n\\nIn order to evaluate our models on realistic unseen data in-the-wild, we additionally create and publish a new audio deefake dataset, c.f. Figure 1. It consists of 37.9 hours of audio clips that are either fake (17.2 hours) or real (20.7 hours). We feature English-speaking celebrities and politicians, both from present and past. The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes. Since the speakers talk absurdly and out-of-character ('Donald Trump reads Star Wars'), it is easy to verify that the audio files are really spoofed. We then manually collect corresponding genuine instances from the same speakers using publicly available material such as podcasts, speeches, etc. We take care to include clips where the type of speaker, style, emotions, etc. are similar to the fake (e.g., for a fake speech by Barack Obama, we include an authentic speech and try to find similar values for background noise, duration, etc.). The clips have an average length of 4.3 seconds and are available at deepfake-demo.aisec.fraunhofer.de/inthe-wild.\\n\\nIn order to evaluate our models on realistic unseen data in-the-wild, we additionally create and publish a new audio deefake dataset, c.f. Figure 1. It consists of 37.9 hours of audio clips that are either fake (17.2 hours) or real (20.7 hours). We feature English-speaking celebrities and politicians, both from present and past. The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes. Since the speakers talk absurdly and out-of-character ('Donald Trump reads Star Wars'), it is easy to verify that the audio files are really spoofed. We then manually collect corresponding genuine instances from the same speakers using publicly available material such as podcasts, speeches, etc. We take care to include clips where the type of speaker, style, emotions, etc. are similar to the fake (e.g., for a fake speech by Barack Obama, we include an authentic speech and try to find similar values for background noise, duration, etc.). The clips have an average length of 4.3 seconds and are available at deepfake-demo.aisec.fraunhofer.de/inthe-wild.\\n\\nIn order to evaluate our models on realistic unseen data in-the-wild, we additionally create and publish a new audio deefake dataset, c.f. Figure 1. It consists of 37.9 hours of audio clips that are either fake ("}
{"id": "muller22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model Name       | Feature Type | Input Length | EER% (\u00b1std) | t-DCF (\u00b1std) | EER% (\u00b1std) |\\n|------------------|--------------|--------------|-------------|-------------|-------------|\\n| LCNN             | cqtspec      | Full         | 6.354\u00b10.39  | 0.174\u00b10.03  | 65.559\u00b111.14|\\n|                 |              | 4s           | 25.534\u00b10.10 | 0.512\u00b10.00  | 70.015\u00b14.74 |\\n| LCNN             | logspec      | Full         | 7.537\u00b10.42  | 0.141\u00b10.02  | 72.515\u00b12.15 |\\n|                 |              | 4s           | 22.271\u00b12.36 | 0.377\u00b10.01  | 91.110\u00b12.17 |\\n| LCNN             | melspec      | Full         | 15.093\u00b12.73 | 0.428\u00b10.05  | 70.311\u00b12.15 |\\n|                 |              | 4s           | 30.258\u00b13.38 | 0.503\u00b10.04  | 81.942\u00b13.50 |\\n| LCNN-Attention   | cqtspec      | Full         | 6.762\u00b10.27  | 0.178\u00b10.01  | 66.684\u00b11.08 |\\n|                 |              | 4s           | 23.228\u00b13.98 | 0.468\u00b10.06  | 75.317\u00b18.25 |\\n| LCNN-Attention   | logspec      | Full         | 7.888\u00b10.57  | 0.180\u00b10.05  | 77.122\u00b14.91 |\\n|                 |              | 4s           | 14.958\u00b12.37 | 0.354\u00b10.03  | 80.651\u00b16.14 |\\n| LCNN-Attention   | melspec      | Full         | 13.487\u00b15.59 | 0.374\u00b10.14  | 70.986\u00b19.73 |\\n|                 |              | 4s           | 19.534\u00b12.57 | 0.449\u00b10.02  | 85.118\u00b11.01 |\\n| LCNN-LSTM        | cqtspec      | Full         | 6.228\u00b10.50  | 0.113\u00b10.01  | 61.500\u00b11.37 |\\n|                 |              | 4s           | 20.857\u00b10.14 | 0.478\u00b10.01  | 72.251\u00b12.97 |\\n| LCNN-LSTM        | logspec      | Full         | 9.936\u00b11.74  | 0.158\u00b10.01  | 79.109\u00b10.84 |\\n|                 |              | 4s           | 13.018\u00b13.08 | 0.330\u00b10.05  | 79.706\u00b115.80|\\n| LCNN-LSTM        | melspec      | Full         | 9.260\u00b11.33  | 0.240\u00b10.04  | 62.304\u00b10.17 |\\n|                 |              | 4s           | 27.948\u00b14.64 | 0.483\u00b10.03  | 82.857\u00b13.49 |\\n| LSTM             | cqtspec      | Full         | 7.162\u00b10.27  | 0.127\u00b10.00  | 53.711\u00b111.68|\\n|                 |              | 4s           | 14.409\u00b12.19 | 0.382\u00b10.05  | 55.880\u00b10.88 |\\n| LSTM             | logspec      | Full         | 10.314\u00b10.81 | 0.160\u00b10.00  | 73.111\u00b12.52 |\\n|                 |              | 4s           | 23.232\u00b10.32 | 0.512\u00b10.00  | 78.071\u00b10.49 |\\n| LSTM             | melspec      | Full         | 16.216\u00b12.92 | 0.358\u00b10.00  | 65.957\u00b17.70 |\\n|                 |              | 4s           | 37.463\u00b10.46 | 0.553\u00b10.01  | 64.297\u00b12.23 |\\n| MesoInception    | cqtspec      | Full         | 11.353\u00b11.00 | 0.326\u00b10.03  | 50.007\u00b114.69|\\n|                 |              | 4s           | 21.973\u00b14.96 | 0.453\u00b10.09  | 68.192\u00b112.47|\\n| MesoInception    | logspec      | Full         | 10.019\u00b10.18 | 0.238\u00b10.02  | 37.414\u00b19.16 |\\n|                 |              | 4s           | 16.377\u00b13.72 | 0.375\u00b10.09  | 72.753\u00b16.62 |\\n| MesoInception    | melspec      | Full         | 14.058\u00b15.67 | 0.331\u00b10.11  | 61.996\u00b112.65|\\n|                 |              | 4s           | 21.484\u00b13.51 | 0.408\u00b10.03  | 51.980\u00b115.32|\\n| MesoNet          | cqtspec      | Full         | 7.422\u00b11.61  | 0.219\u00b10.07  | 54.544\u00b111.50|\\n|                 |              | 4s           | 20.395\u00b12.03 | 0.426\u00b10.06  | 65.928\u00b12.57 |\\n| MesoNet          | logspec      | Full         | 8.369\u00b11.06  | 0.170\u00b10.05  | 46.939\u00b15.81 |\\n|                 |              | 4s           | 11.124\u00b10.79 | 0.263\u00b10.03  | 80.707\u00b112.03|\\n| MesoNet          | melspec      | Full         | 11.305\u00b11.80 | 0.321\u00b10.06  | 58.405\u00b111.28|\\n|                 |              | 4s           | 21.761\u00b10.26 | 0.467\u00b10.00  | 64.415\u00b115.68|\\n| ResNet18         | cqtspec      | Full         | 6.552\u00b10.49  | 0.140\u00b10.01  | 49.759\u00b10.17 |\\n|                 |              | 4s           | 18.378\u00b11.76 | 0.432\u00b10.07  | 61.827\u00b17.46 |\\n| ResNet18         | logspec      | Full         | 7.386\u00b10.42  | 0.139\u00b10.02  | 80.212\u00b10.23 |\\n|                 |              | 4s           | 15.521\u00b11.83 | 0.387\u00b10.02  | 88.729\u00b12.88 |\\n| ResNet18         | melspec      | Full         | 21.658\u00b12.56 | 0.551\u00b10.04  | 77.614\u00b11.47 |\\n|                 |              | 4s           | 28.178\u00b10.33 | 0.489\u00b10.01  | 83.006\u00b17.17 |\\n| Transformer      | cqtspec      | Full         | 7.498\u00b10.34  | 0.129\u00b10.01  | 43.775\u00b12.85 |\\n|                 |              | 4s           | 11.256\u00b10.07 | 0.329\u00b10.00  | 48.208\u00b11.49 |\\n| Transformer      | logspec      | Full         | 9.949\u00b11.77  | 0.210\u00b10.06  | 64.789\u00b10.88 |\\n|                 |              | 4s           | 13.935\u00b11.70 | 0.320\u00b10.03  | 44.406\u00b12.17 |\\n| Transformer      | melspec      | Full         | 20.813\u00b16.44 | 0.394\u00b10.10  | 73.307\u00b12.81 |\\n|                 |              | 4s           | 26.495\u00b11.76 | 0.495\u00b10.00  | 68.407\u00b15.53 |\\n| CRNNSpoof        | raw          | Full         | 15.658\u00b10.35 | 0.312\u00b10.01  | 44.500\u00b18.13 |\\n|                 |              | 4s           | 19.640\u00b11.62 | 0.360\u00b10.04  | 41.710\u00b14.86 |\\n| RawNet2          | raw          | Full         | 3.154\u00b10.87  | 0.078\u00b10.02  | 37.819\u00b12.23 |\\n|                 |              | 4s           | 4.351\u00b10.29  | 0.132\u00b10.01  | 33.943\u00b12.59 |\\n| RawPC            | raw          | Full         | 3.092\u00b10.36  | 0.071\u00b10.00  | 45.715\u00b112.20|\\n|                 |              | 4s           | 3.067\u00b10.91  | 0.097\u00b10.03  | 52.884\u00b16.08 |\\n| RawGAT-ST        | raw          | Full         | 1.229\u00b10.43  | 0.036\u00b10.01  | 37.154\u00b11.95 |\\n|                 |              | 4s           | 2.297\u00b10.98  | 0.074\u00b10.03  | 38.767\u00b11.28 |\\n\\nTable 1: Full results of evaluation on the ASVspoof 2019 LA 'eval' data. We compare different model architectures against different feature types and audio input lengths (4s, fixed-sized inputs vs. variable-length inputs). Results are averaged over three independent trials with random initialization, and the standard deviation is reported. Best-performing configurations are highlighted in boldface.\\n\\nWhen evaluating the models on our proposed 'in-the-wild' dataset, we see an increase in EER by up to 1000% compared to ASVspoof 2019 (rightmost column).\"}"}
{"id": "muller22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Model performance averaged by input preprocessing. Fixed-length, 4s inputs perform significantly worse on the ASVspoof data and on the 'in-the-wild' dataset than variable-length inputs. This suggests that related work using fixed-length inputs may (unnecessarily) sacrifice performance.\\n\\n4.3. Audio Input Length\\nAudio samples usually vary in length, which is also the case for the data in ASVspoof 2019 and our proposed 'in-the-wild' dataset. While some models can accommodate variable-length input (and thus also fixed-length input), many can not. We extend these by introducing a global averaging layer, which adds such capability.\\n\\nIn our evaluation of fixed-length input, we chose a length of four seconds, following [23]. If an input sample is longer, a random four-second subset of the sample is used. If it is shorter, the sample is repeated. To keep the evaluation fair, these shorter samples are also repeated during the full-length evaluation. This ensures that full-length input is never shorter than truncated input, but always at least 4s.\\n\\n5. Results\\nTable 1 shows the results of our experiments, where we evaluate all models against all configurations of data preprocessing: we train twelve different models, using one of four different feature types, with two different ways of handling variable-length audio. Each experiment is performed three times, using random initialization. We report averaged EER and t-DCF, as well as standard deviation. We observe that on ASVspoof, our implementations perform comparable to related work, with a margin of approximately 2\u22124% EER and 0.1 t-DCF. This is likely because we do not fine-tune our models' hyper-parameters.\\n\\n5.1. Fixed vs. Variable Input Length\\nWe analyze the effects of truncating the input signal to a fixed length compared to using the full, unabridged audio. For all models, performance decreases when the input is trimmed to 4s. Table 2 averages all results based on input length. We see that average EER on ASVspoof drops from 19.89% to 9.85% when the full-length input is used. These results show that a four-second clip is insufficient for the model to extract useful information compared to using the full audio file as input. Therefore, we propose not to use fixed-length truncated inputs, but to provide the full audio file to the model. This may seem obvious, but the numerous works that use fixed-length inputs [23, 25, 26] suggest otherwise.\\n\\n5.2. Effects of Feature Extraction Techniques\\nWe discuss the effects of different feature preprocessing techniques, c.f. 1: The 'raw' models outperform the feature-based models, obtaining up to 1.2% EER on ASVspoof and 33.9% EER on the 'in-the-wild' dataset (RawGAT-ST and RawNet2). The spectrogram-based models perform slightly worse, achieving up to 6.3% EER on ASVspoof and 37.4% on the 'in-the-wild' dataset (LCNN and MesoNet). The superiority of the 'raw' models is assumed to be due to finer feature-extraction resolution than the spectogram-based models [26]. This has lead recent research to focus largely on such raw-feature, end-to-end models [25, 26].\\n\\nConcerning the spectogram-based models, we observe that melspec features are always outperformed by either cqtspec or logspec. Simply replacing melspec with cqtspec increases the average performance by 37%, all other factors constant.\\n\\n5.3. Evaluation on 'in-the-wild' data\\nEspecially interesting is the performance of the models on real-world deepfake data. Table 1 shows the performance of our models on the 'in-the-wild' dataset. We see that there is a large performance gap between the ASVSpoof 2019 evaluation data and our proposed 'in-the-wild' dataset. In general, the EER values of the models deteriorate by about 200 to 1000 percent. Often, the models do not perform better than random guessing. To investigate this further, we train our best 'in-the-wild' model from Table 1, RawNet2 with 4s input length, on all from ASVspoof 2019, i.e., the 'train', 'dev', and 'eval' splits. We then re-evaluate on the 'in-the-wild' dataset to investigate whether adding more ASVspoof training data improves out-of-domain performance. We achieve 33.1\u00b10.2% EER, i.e., no improvement over training with only the 'train' and 'dev' data. The inclusion of the 'eval' split does not seem to add much information that could be used for real-world generalization. This is plausible in that all splits of ASVspoof are fundamentally based on the same dataset, VCTK, although the synthesis algorithms and speakers differ between splits [5].\\n\\n6. Conclusion\\nIn this paper, we systematically evaluate audio spoof detection models from related work according to common standards. In addition, we present a new audio deefake dataset of 'in-the-wild' audio spoofs that we use to evaluate the generalization capabilities of related work in a real-world scenario. We find that regardless of the model architecture, some preprocessing steps are more successful than others. It turns out that the use of cqtspec or logspec features consistently outperforms the use of melspec features in our comprehensive analysis. Furthermore, we find that for most models, four seconds of input audio does not saturate performance compared to longer examples. Therefore, we argue that one should consider using cqtspec features and unabridged input audio when designing audio deepfake detection architectures.\\n\\nMost importantly, however, we find that the 'in-the-wild' generalization capabilities of many models may have been overestimated. We demonstrate this by collecting our own audio deepfake dataset and evaluating twelve different model architectures on it. Performance drops sharply, and some models degenerate to random guessing. It may be possible that the community has tailored its detection models too closely to the prevailing benchmark, ASVSpoof, and that deepfakes are much harder to detect outside the lab than previously thought.\"}"}
