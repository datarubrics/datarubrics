{"id": "fu23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] A. Housen and F. Kuiken, \u201cComplexity, accuracy, and fluency in second language acquisition,\u201d Applied linguistics, vol. 30, no. 4, pp. 461\u2013473, 2009.\\n\\n[2] P. Lennon, \u201cInvestigating fluency in EFL: A quantitative approach,\u201d Language learning, vol. 40, no. 3, pp. 387\u2013417, 1990.\\n\\n[3] R. Ellis and R. R. Ellis, The study of second language acquisition. Oxford University, 1994.\\n\\n[4] S. G\u00f6tz, \u201cFluency in native and nonnative English speech,\u201d Fluency in Native and Nonnative English Speech, pp. 1\u2013262, 2013.\\n\\n[5] E. Guz, \u201cEstablishing the fluency gap between native and non-native-speech,\u201d Research in Language, vol. 13, no. 3, pp. 230\u2013247, 2015.\\n\\n[6] S. Mao, Z. Wu, J. Jiang, P. Liu, and F. K. Soong, \u201cNN-based ordinal regression for assessing fluency of ESL speech,\u201d in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 7420\u20137424.\\n\\n[7] H. Zhang, K. Shi, and N. F. Chen, \u201cMultilingual speech evaluation: case studies on English, Malay and Tamil,\u201d in Proc. Interspeech 2021, 2021, pp. 4443\u20134447.\\n\\n[8] Y. Gong, Z. Chen, I.-H. Chu, P. Chang, and J. Glass, \u201cTransformer-based multi-aspect multi-granularity non-native English speaker pronunciation assessment,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7262\u20137266.\\n\\n[9] F.-A. Chao, T.-H. Lo, T.-I. Wu, Y.-T. Sung, and B. Chen, \u201c3M: An Effective Multi-view, Multi-granularity, and Multi-aspect Modeling Approach to English Pronunciation Assessment,\u201d in 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). IEEE, 2022, pp. 575\u2013582.\\n\\n[10] K. Fu, S. Gao, X. Tian, W. Li, Z. Ma, and A. Bytedance, \u201cUsing Fluency Representation Learned from Sequential Raw Features for Improving Non-native Fluency Scoring,\u201d in Proc. Interspeech 2022, pp. 4337\u20134341, 2022.\\n\\n[11] E. Kim, J.-J. Jeon, H. Seo, and H. Kim, \u201cAutomatic Pronunciation Assessment using Self-Supervised Speech Representation Learning,\u201d in Proc. Interspeech 2022, 2022, pp. 1411\u20131415.\\n\\n[12] O. D. Deshmukh, K. Kandhway, A. Verma, and K. Audhkhasi, \u201cAutomatic evaluation of spoken English fluency,\u201d in 2009 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2009, pp. 4829\u20134832.\\n\\n[13] K. Zechner, D. Higgins, X. Xi, and D. M. Williamson, \u201cAutomatic scoring of non-native spontaneous speech in tests of spoken English,\u201d Speech Communication, vol. 51, no. 10, pp. 883\u2013895, 2009.\\n\\n[14] H. Deng, Y. Lin, T. Utsuro, A. Kobayashi, H. Nishizaki, and J. Hoshino, \u201cAutomatic fluency evaluation of spontaneous speech using disfluency-based features,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 9239\u20139243.\\n\\n[15] W. Liu, K. Fu, X. Tian, S. Shi, W. Li, Z. Ma, and T. Lee, \u201cAn ASR-free Fluency Scoring Approach with Self-Supervised Learning,\u201d arXiv preprint arXiv:2302.09928, 2023.\\n\\n[16] A. Lee et al., \u201cLanguage-independent methods for computer-assisted pronunciation training,\u201d Ph.D. dissertation, Massachusetts Institute of Technology, 2016.\\n\\n[17] J. Zhang, Z. Zhang, Y. Wang, Z. Yan, Q. Song, Y. Huang, K. Li, D. Povey, and Y. Wang, \u201cSpeechocean762: An Open-Source Non-native English Speech Corpus for Pronunciation Assessment,\u201d in Proc. Interspeech 2021, 2021, pp. 3710\u20133714.\\n\\n[18] N. F. Chen, D. Wee, R. Tong, B. Ma, and H. Li, \u201cLarge-scale characterization of non-native Mandarin Chinese spoken by speakers of European origin: Analysis on iCALL,\u201d Speech Communication, vol. 84, pp. 46\u201356, 2016.\\n\\n[19] L. Yang, K. Fu, J. Zhang, and T. Shinozaki, \u201cPronunciation Error Tendency Detection with Language Adversarial Representation Learning,\u201d in Proc. Interspeech 2020, 2020, pp. 3042\u20133046.\\n\\n[20] L. Peng, K. Fu, B. Lin, D. Ke, and J. Zhan, \u201cA Study on Fine-Tuning wav2vec2.0 Model for the Task of Mispronunciation Detection and Diagnosis,\u201d in Proc. Interspeech 2021, 2021, pp. 4448\u20134452.\\n\\n[21] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\u201d in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 4171\u20134186. [Online]. Available: https://aclanthology.org/N19-1423\\n\\n[22] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \u201cRoberta: A robustly optimized bert pretraining approach,\u201d arXiv preprint arXiv:1907.11692, 2019.\\n\\n[23] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, \u201cERNIE: Enhanced language representation with informative entities,\u201d in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy: Association for Computational Linguistics, Jul. 2019, pp. 1441\u20131451.\\n\\n[24] E. Kharitonov, A. Lee, A. Polyak, Y. Adi, J. Copet, K. Lakhotia, T. A. Nguyen, M. Riviere, A. Mohamed, E. Dupoux, and W.-N. Hsu, \u201cText-free prosody-aware generative spoken language modeling,\u201d in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 8666\u20138681. [Online]. Available: https://aclanthology.org/2022.acl-long.593\\n\\n[25] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020.\\n\\n[26] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maal\u00f8e, T. N. Sainath, and S. Watanabe, \u201cSelf-supervised speech representation learning: A review,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1179\u20131210, 2022.\\n\\n[27] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021.\\n\\n[28] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022.\\n\\n[29] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2015, pp. 5206\u20135210.\\n\\n[30] S. Zhang, M. Lei, Z. Yan, and L. Dai, \u201cDeep-FSMN for large vocabulary continuous speech recognition,\u201d in ICASSP 2018-2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5869\u20135873.\\n\\n[31] W. Hu, Y. Qian, F. K. Soong, and Y. Wang, \u201cImproved mispronunciation detection with deep neural network trained acoustic models and transfer learning based logistic regression classifiers,\u201d Speech Communication, vol. 67, pp. 154\u2013166, 2015.\"}"}
{"id": "fu23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring\\n\\nKaiqi Fu, Shaojun Gao, Shuju Shi, Xiaohai Tian, Wei Li, Zejun Ma\\n\\nByteDance\\nkaiq.fu@gmail.com, gaoshaojun123@163.com, {shuju.shi,xiaohai.tian,liwei.speech,mazejun}@bytedance.com\\n\\nAbstract\\nSpeech fluency/disfluency can be evaluated by analyzing a range of phonetic and prosodic features. Deep neural networks are commonly trained to map fluency-related features into the human scores. However, the effectiveness of deep learning-based models is constrained by the limited amount of labeled training samples. To address this, we introduce a self-supervised learning (SSL) approach that takes into account phonetic and prosody awareness for fluency scoring. Specifically, we first pre-train the model using a reconstruction loss function, by masking phones and their durations jointly on a large amount of unlabeled speech and text prompts. We then fine-tune the pre-trained model using human-annotated scoring data. Our experimental results, conducted on datasets such as Speechocean762 and our non-native datasets, show that our proposed method outperforms the baseline systems in terms of Pearson correlation coefficients (PCC). Moreover, we also conduct an ablation study to better understand the contribution of phonetic and prosody factors during the pre-training stage.\\n\\nIndex Terms\\nComputer Assisted Pronunciation Training (CAPT), Non-native Fluency Scoring, Phonetic and Prosody-aware, Self-supervised Learning\\n\\n1. Introduction\\nThe ability to speak fluently is a significant aspect when evaluating a learner's language proficiency [1]. It is characterized by the seamless and effortless production of speech with minimal pauses, hesitation, or corrections [2\u20135]. L2 learners typically exhibit slower speech and more frequent unnecessary pauses compared to native speakers. Automatic scoring of fluency serves as an essential module in computer-aided language learning (CALL) systems. It has been extensively studied in both \u201cread aloud\u201d [6\u201311] and \u201copen response\u201d [12\u201315] scenarios. In the read aloud\u201d scenario, L2 learners are required to read a provided prompt text, whereas the \u2018open response\u201d requires them to express their opinions freely based on a given question.\\n\\nIn this paper, we focus on \u201cread aloud\u201d scenario, where forced-alignment model is first applied to a pair of non-native speech and prompt text to generate time stamps of speech segments, such as phonemes, words and etc. Fluency related features are then extracted and fed into subsequent fluency scorers. Although recent end-to-end neural network based fluency scorers have achieved satisfactory results [7\u201311, 15], their performances heavily rely on the size of labeled scoring samples. In fact, the non-native data labeling process is costly and has scalability issues [16]. Take the recently released public free dataset Speechocean762 [17] for example, only 5,000 sentences have been assigned with human fluency scores. The comparison in [18] shows that the largest non-native corpus only contains 90,841 utterances, but it is not publicly available.\\n\\nTo overcome the challenge of limited labeled data, many researchers are using pre-training and fine-tuning paradigms to leverage large amounts of unlabeled data [19,20]. In the field of natural language processing (NLP), masked language modeling (MLM) has become a popular method for pre-training models such as BERT [21], RoBERTa [22], and ERNIE [23]. MLM involves masking a subset of tokens in a sequence and training the model to predict these masked tokens, which enables the model to learn high-level contextual representations that can be beneficial for downstream tasks.\\n\\nRecently, a new multi-stream transformer language model (MS-TLM) was proposed to jointly model phonetic content and prosody [24], which demonstrated the effectiveness of prosody prediction. In this paper, we propose a self-supervised learning approach that incorporates phonetic and prosodic information to improve non-native fluency scoring. The pre-trained model is used to predict masked phones and durations, which enhances the model's ability to represent long-range phonetic and prosodic information. Specifically, we use an automatic speech recognition (ASR) system to generate phone-level raw sequential features, e.g. acoustic features, phone sequences, and duration, for pairs of non-native speech and prompt text. We then randomly mask 15% of these phone-level features and train our fluency scorer to predict the masked phone and duration. Finally, the pre-trained model is fine-tuned using limited human-annotated fluency scores. Experimental results show that the proposed approach can significantly improve fluency scoring in various configurations. An ablation study is also conducted to analyze the effect of different loss functions used in our pre-training stage on fluency scoring.\\n\\n2. Related work\\nOver the past few decades, extensive research has been conducted on spoken fluency scoring. Traditionally, handcrafted features such as the statistics of speech break [6], speech rate [6, 7, 12\u201314], filled pause, and goodness of pronunciation (GOP) [7\u20139] were collected based on phone boundaries and fed into various fluency scorers such as SVM [12, 14], and multiple linear [6]. Recent works have employed sequence models to directly learn utterance-level fluency scores from phone-level raw features, including phonetic features (e.g., phone sequence [7\u201310]), prosodic features (e.g., energy [9], pitch [7] and phone duration [10]. Bi-directional Long Short Term Memory (BLSTM) [7, 10, 11, 15] and Transformer models [8, 9] have been used to capture the dynamic changes of phone-level pronunciation-related features for better modeling the evolution of local fluency over time.\\n\\nMore recently, self-supervised learning (SSL)-based speech...\"}"}
{"id": "fu23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"models such as wav2vec2 [25] have been shown to be effective in learning meaningful representations from raw speech signals in various downstream tasks [26]. Inspired by this success, researchers used pre-trained SSL models like wav2vec2 [25], HuBERT [27], and WavLM [28] to extract features directly and feed them into fluency scorers [9, 11, 15]. Due to the promising performance, we consider the two SSL-based models [9, 15] as strong baselines of this work.\\n\\n3. Method\\n\\nThis section details our approach for fluency scoring. Initially, we outline our phonetic and prosody-aware pre-training technique that employs self-supervised learning to reconstruct the masked phone and duration in each pre-training sample. The process flow for pre-training is illustrated in Figure 1. Subsequently, we elaborate on how we employ the pre-trained model for fluency scoring in downstream tasks.\\n\\n3.1. Phonetic and prosody-aware pre-training\\n\\n3.1.1. Phone-level fluency feature extraction\\n\\nIn [10], phone-level raw features were shown to be effective for assessing learners' speech fluency. Followed by the previous study, three segmental features (deep features, phone, and its duration) were extracted in this study. Initially, forced alignment is carried out to obtain time boundaries at the phone level, such as the beginning and end time of each phone and pause. The boundary information is then used to derive the phone sequence and its duration, which are denoted as \\\\( e \\\\in \\\\mathbb{R}^{1 \\\\times N} \\\\) and \\\\( t \\\\in \\\\mathbb{R}^{1 \\\\times N} \\\\), respectively. \\\\( t \\\\) refers to the number of frames within the phone. Following that, frame-level deep features (also known as bottleneck features) extracted from the acoustic model are averaged by the duration of each phone to obtain phone-level deep features, represented as \\\\( X \\\\in \\\\mathbb{R}^{N \\\\times D} \\\\), where \\\\( D \\\\) and \\\\( N \\\\) represent the acoustic feature dimension and the number of phones, respectively. Finally, the phone-level raw features are inputted into the model for pre-training and fine-tuning.\\n\\n3.1.2. Masking strategy\\n\\nDuring the pre-train stage, we utilize random masking to randomly replace 15% of phone-level raw features in each sample with a special mask token. Among the selected positions, 90% will be replaced with the special mask token and the remaining 10% will be kept unchanged. The model takes three input features, which correspond to three different mask marker methods: 1) the selected phonemes are replaced with the mask token, 2) the selected phone duration is set to zero, and 3) the selected deep features are replaced with zero vectors. To provide the model with duration ground-truth, we set the duration label to a range of 1-100. If the phone duration exceeds 100 frames, we cap the duration label at 100.\\n\\n3.1.3. Multitask based reconstruction loss\\n\\nThe pre-processing steps takes phone-level deep features \\\\( X \\\\) as input, which are initially transformed into a condensed feature space \\\\( X' \\\\) using a fully connected layer. Next, the phone sequence \\\\( e \\\\) is converted into phone embeddings \\\\( E \\\\). The sum of \\\\( E \\\\) and \\\\( X' \\\\) output is then concatenated with phone duration \\\\( t \\\\) and utilized as a sequence of input features for the SSL encoder, which generates the phone-level hidden representations \\\\( H \\\\).\\n\\n\\\\[\\nH = E(\\\\lfloor X' + E; t \\\\rfloor),\\n\\\\]\\n\\nwhere \\\\( E \\\\) is presented the SSL encoder.\\n\\nThe phone-level hidden representations \\\\( H \\\\) are then passed through two classifiers for phoneme and duration prediction, respectively. The pre-training model is optimized jointly by utilizing a multitask approach that minimizes the cross-entropy loss between the predicted and ground truth phonemes and durations. The loss function of \\\\( i \\\\)-th masked token is described as follows:\\n\\n\\\\[\\nL_i = L_{ce}(y_p, P_p(h_i)) + L_{ce}(y_d, P_d(h_i)),\\n\\\\]\\n\\nwhere \\\\( y_p \\\\) and \\\\( y_d \\\\) are presented the ground truth phoneme and duration, respectively. \\\\( h_i \\\\) is the phone-level hidden representations in \\\\( i \\\\)-th masked position. The phoneme and duration classifiers are denoted as \\\\( P_p(\\\\cdot) \\\\) and \\\\( P_d(\\\\cdot) \\\\), respectively.\"}"}
{"id": "fu23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Data splitting for fluency scorer\\n\\n|         | Train | Dev  | Test |\\n|---------|-------|------|------|\\n| Pre-train | Unlabeled data | 203,206 | 2,000 | - |\\n| Fine-tune | ByteRead | 10,000 | 2,000 | 2,000 |\\n|          | Speechocean762 | 2,500 | - | 2,500 |\\n\\nshould be noted that the loss function is calculated exclusively based on the masked phonemes and duration. The total loss is calculated by summing the loss values of all the masked tokens across sentences.\\n\\n3.2. Fine-tuning for fluency scoring\\n\\nIn this phase, our objective is to fine-tune the pre-trained model for fluency scoring. The fluency scoring model comprises an encoder and a scorer. Initially, we utilize the pre-trained weights to initialize the encoder of the scoring model. Subsequently, the scorer performs average pooling on a sequence of encoder outputs $H$ (as illustrated in Eq. (1)), resulting in an utterance-level fluency representation. This representation is then fed into a linear layer to generate machine score. Mean square error (MSE) calculated between predicted and human-annotated fluency scores are used as the objective for entire network fine-tuning.\\n\\n4. Experimental setup\\n\\n4.1. Speech corpora\\n\\nThe acoustic model was trained on a total of 5,000 hours of English speech data, including 960 hours of native speech from the LibriSpeech [29] and 4,000 hours of non-native private recordings from Bytedance. Additionally, we collected approximately 436 hours (about 200,000 utterances) of reading speech by Chinese L2 adult learners and prompt text for MLM pre-training.\\n\\nTo evaluate fluency scoring, we performed experiments on two additional datasets: ByteRead, an internal dataset of 14,000 English utterances collected from Bytedance\u2019s education product (described in detail in [10]), and Speechocean762, an open-source speech assessment corpus consisting of 5,000 utterances collected from 250 speakers [17]. The data statistics were detailed in Table 1.\\n\\n4.2. Feature extraction\\n\\nRaw fluency features were extracted using the deep feedforward sequential memory network-hidden Markov models (DFSMN-HMM) acoustic model, as described in [30]. The model architecture includes 2 convolutional layers, 24 FSMN layers, a bottleneck layer, and a feedforward layer. The input features were 39-dimensional Mel-frequency cepstral coefficients (MFCCs). The bottleneck layer extracts frame-level deep features with a dimensionality of 512. A HMM-based force-aligner is employed to obtain the phone sequence along with the corresponding start and end time boundaries for each phone.\\n\\n4.3. Setup of proposed and baseline systems\\n\\n4.3.1. Proposed systems setup\\n\\nGiven the L2 learner\u2019s speech and prompt text, phone-level raw features (deep features, phone sequence, and duration) can be first obtained in the fluency feature extraction module. The phone sequence is projected into a 32-dim phone embedding, while the 512-dim deep features are transformed into 32-dim features and added to the phone embedding to obtain the compact features. These compact features are then concatenated with a 1-dimensional duration feature, resulting in a 33-dimensional output of the pre-processing. This output serves as input to the pretrain model.\\n\\n- Transformer-pre: The proposed Transformer-based pretrain model. A trainable [CLS] token was appended to the processed feature sequence. And a trainable position embedding and the 33-dimensional processed features were summed together. The pre-trained encoder consists of two transformer layers, with the first layer removing the residual connection to increase the input feature dimension to 128. The multi-head attention block employs 4 heads. The output of the transformer encoder for the [CLS] token, with a dimensionality of 128, was used as the corresponding utterance-level representation for predicting the fluency score.\\n\\n- BLSTM-pre: The proposed BLSTM-based pretrain model. The 64-dim phone-level contextual representations output of the BLSTM encoder will be fed into a mean pooling layer and a linear layer to get the final fluency score. According to our empirical study, an 8-layer BLSTM architecture lead to the best results.\\n\\nThe adam optimization algorithm was employed for updating the pre-training and scoring models in all proposed systems. During pre-training, the batch size was set to 256 and the learning rate was 0.001. For fine-tuning, the batch size was set to 32 and the learning rate was 0.002.\\n\\n4.3.2. Baseline systems setup\\n\\n- BLSTM [10]: The baseline system without pretraining, where the scorer consists of pre-processing, a 2-layer BLSTM encoder, and a fully connected layer. The input feature and the pre-processing steps were the same as our proposed system as described in 4.3.1.\\n\\n- 3M-Transformer [9]: The model input comprises multi-view phone-level features, which consist of prosodic features (duration, energy), SSL features (wav2vec2 [25], HuBERT [27], WavLM [28]) , and GOP feature [31]. These features are simply concatenated and subsequently fed into the 3-layer transformer-based scorer to get the fluency score. Multi-granularity pronunciation score labels are used to model the association between different scoring tasks.\\n\\n- SSL-IDX-BLSTM [15]: The system takes the frame-level SSL representations extracted from wav2vec2 Large as input. The k-means clustering algorithm is used to generate the clustered index, which is seen as pseudo phonetic information. A linear layer project the SSL feature into a compact feature, which is concatenated with the index embedding through an embedding layer and fed into 2-layer BLSTM to get the fluency score.\\n\\n5. Results and analyses\\n\\nIn our experiments, the system performance was evaluated using the Pearson correlation coefficient (PCC) between the machine-predicted scores and the human scores. A higher PCC value indicates a better system performance.\"}"}
{"id": "fu23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: The PCC performance of different systems on ByteRead and Speechocean762 data sets.\\n\\n| Model               | #Param | Pre-train | Speechocean762 | ByteRead |\\n|---------------------|--------|-----------|----------------|----------|\\n| BLSTM [10]          | 278K   | -         | -              | 0.817    |\\n| 3M-Transformer [9]  | -      | -         | 0.828          | -        |\\n| SSL+IDX+BLSTM [15]  | -      | -         | 0.795          | 0.828    |\\n| Transformer-pre 795K| -      | \u2713         | 0.784          | 0.783    |\\n| BLSTM-pre 871K      | -      | \u2713         | 0.797          | 0.804    |\\n\\nTable 3: The PCC performance of BLSTM-based systems on different scales of the scoring training sets. ByteRead(1000) means 1,000 utterances with prompt text was randomly selected to fine-tune the pre-trained model in the ByteRead training set. Phn and dur loss represent phonetic and prosodic loss, respectively.\\n\\n| Model       | Pre-train | Loss    | ByteRead(1000) | ByteRead(2500) | ByteRead(5000) | ByteRead | Speechocean762 |\\n|-------------|-----------|---------|----------------|----------------|----------------|----------|----------------|\\n| BLSTM-pre   | \u2713         | -       | 0.669          | 0.773          | 0.787          | 0.804    | 0.797          |\\n| phn+dur     | \u2713         | \u2713       | 0.787          | 0.807          | 0.820          | 0.833    | 0.835          |\\n| dur         | \u2713         | \u2713       | 0.780          | 0.800          | 0.818          | 0.826    | 0.838          |\\n| phn         | \u2713         | \u2713       | 0.734          | 0.784          | 0.813          | 0.820    | 0.822          |\\n\\n5.1. Main results\\n\\nThis subsection presents a comparison of the proposed method\u2019s performance on various encoder structures using the Speechocean762 and ByteRead datasets. Additionally, we assessed the proposed method\u2019s effectiveness by comparing its results with baselines. The results of the different systems are presented in Table 2.\\n\\nFirst, we evaluated the effectiveness of phonetic and prosody-aware pretrain models using both Transformer and BLSTM architectures, as shown in rows (d) and (e) of Table 2. The results demonstrate that the proposed pretrain model methods consistently outperform their counterparts, which were trained from scratch with labeled data. This suggests that phonetic and prosody-aware pretraining can be beneficial for fluency scoring. Furthermore, we observed that the BLSTM pretrain model outperforms its Transformer counterpart. Hence, BLSTM pretrain model is used in the rest of the experiments.\\n\\nApart from the self implemented systems, we also conducted performance comparisons between the proposed BLSTM-pre system and the baseline systems presented in Table 2 (a), (b), and (c). We first compared the performance between proposed BLSTM-pre and the BLSTM system reported in [10]. The results show that our BLSTM-pre outperformed the BLSTM baseline, with an improvement in PCC on the ByteRead database from 0.817 to 0.833. This confirms that the pretrain model is effective for fluency scoring. We then conducted comparisons between our proposed BLSTM-pre system and two SSL feature-based approaches, namely 3M-Transformer and SSL-IDX-BLSTM. Our proposed BLSTM-pre system showed better performance than the 3M-Transformer baseline on the Speechocean762 database, resulting in an increase in PCC from 0.828 to 0.835. Similar results were observed in comparison with SSL-IDX-BLSTM, where our proposed BLSTM-pre system consistently achieved better performance on both Speechocean762 and ByteRead datasets. These findings suggest that our proposed system outperforms state-of-the-art systems for fluency scoring on both Speechocean762 and ByteRead datasets.\\n\\n5.2. Ablation studies\\n\\nIn this section, we conducted a series of experiments to determine the relative importance of the phonetic and prosodic components in the proposed method for scoring fluency. We performed ablation studies by testing different loss function configurations and analyzing the performance of each component. The results are presented in Table 3.\\n\\nSpecifically, we first evaluated the pre-training model's performance by using only the phonetic aspect as the pre-training loss, which involved predicting the masked phone. Our findings revealed that this approach yielded better results than the no pre-training system. Moreover, we discovered that the prosodic aspect's contribution to the improvement was more substantial than that of the phonetic aspect. This could be attributed to the duration factor's significant role in assessing speech fluency. Finally, we combined both phonetics and prosody to optimize the pre-training model, resulting in a more significant improvement, highlighting the effectiveness of the proposed SSL method in fluency scoring.\\n\\n6. Conclusion\\n\\nThis article introduces a self-supervised learning technique that is phonetic and prosody-aware for assessing the fluency of L2 learners\u2019 speech. The method involves masking the phone and duration of input features and then reconstructing them by utilizing a vast amount of unlabeled non-native data during the pre-training phase. To predict the fluency score, a small amount of scoring data was utilized to fine-tune the pre-trained model. Results based on the Speechocean762 datasets and our non-native dataset indicate that the proposed approach outperforms the baseline systems. Our future research aims to explore the benefits of our approach for scoring at various levels (such as phone, and word) and granularities (such as accuracy, and proficiency). Additionally, we plan to explore the impact of using the L1 dataset when pre-training.\"}"}
