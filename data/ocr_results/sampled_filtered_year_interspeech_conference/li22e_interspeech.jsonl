{"id": "li22e_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[11] C. Valentini-Botinhao, X. Wang, S. Takaki, and J. Yamagishi, \u201cInvestigating RNN-based speech enhancement methods for noise-robust Text-to-Speech,\u201d in SSW, 2016, pp. 146\u2013152.\\n\\n[12] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun et al., \u201cThe Interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d arXiv preprint arXiv:2005.13981, 2020.\\n\\n[13] G. J. Mysore, \u201cCan we automatically transform speech recorded on common consumer devices in real-world environments into professional production quality speech?\u2014a dataset, insights, and challenges,\u201d IEEE Signal Processing Letters, vol. 22, no. 8, pp. 1006\u20131010, 2014.\\n\\n[14] J. Yamagishi, C. Veaux, and K. MacDonald, \u201cCSTR VCTK Corpus: English multi-speaker corpus for CSTR V oice Cloning Toolkit (version 0.92),\u201d 2019.\\n\\n[15] S. Sun, B. Zhang, L. Xie, and Y. Zhang, \u201cAn unsupervised deep domain adaptation approach for robust speech recognition,\u201d Neurocomputing, vol. 257, pp. 79\u201387, 2017.\\n\\n[16] S. Yang, Y. Wang, and L. Xie, \u201cAdversarial feature learning and unsupervised clustering based speech synthesis for found data with acoustic and textual noise,\u201d IEEE Signal Processing Letters, vol. 27, pp. 1730\u20131734, 2020.\\n\\n[17] M. Todisco, X. Wang, V. Vestman, M. Sahidullah, H. Delgado, A. Nautsch, J. Yamagishi, N. Evans, T. Kinnunen, and K. A. Lee, \u201cASVspoof 2019: Future horizons in spoofed and fake audio detection,\u201d arXiv preprint arXiv:1904.05441, 2019.\\n\\n[18] A. Mathur, F. Kawsar, N. Berthouze, and N. D. Lane, \u201cLibriAdapt: a new speech dataset for unsupervised domain adaptation,\u201d in 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7439\u20137443.\\n\\n[19] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an ASR corpus based on public domain audio books,\u201d in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 5206\u20135210.\\n\\n[20] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra, \u201cPerceptual evaluation of speech quality (PESQ)\u2014a new method for speech quality assessment of telephone networks and codecs,\u201d in 2001 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), vol. 2. IEEE, 2001, pp. 749\u2013752.\\n\\n[21] J. Jensen and C. H. Taal, \u201cAn algorithm for predicting the intelligibility of speech masked by modulated noise maskers,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 11, pp. 2009\u20132022, 2016.\\n\\n[22] Y. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu, B. Zhang, and L. Xie, \u201cDCCRN: Deep complex convolutional recurrent network for phase-aware speech enhancement,\u201d arXiv preprint arXiv:2008.00264, 2020.\\n\\n[23] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, \u201cSDR\u2013half-baked or well done?\u201d in 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 626\u2013630.\\n\\n[24] J. Su, A. Finkelstein, and Z. Jin, \u201cPerceptually-motivated environment-specific speech enhancement,\u201d in 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 7015\u20137019.\\n\\n[25] J. Su, Z. Jin, and A. Finkelstein, \u201cHiFi-GAN: High-fidelity denoising and dereverberation based on speech deep features in adversarial networks,\u201d arXiv preprint arXiv:2006.05694, 2020.\\n\\n[26] \u2014\u2014, \u201cHiFi-GAN-2: Studio-quality speech enhancement via generative adversarial networks conditioned on acoustic features,\u201d in WASPAA 2021, 2021.\\n\\n[27] H. Li, Y. Ai, and J. Yamagishi, \u201cEnhancing low-quality voice recordings using disentangled channel factor and neural waveform model,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2021, pp. 734\u2013741.\\n\\n[28] N. Kalchbrenner, E. Elsen, K. Simonyan, S. Noury, N. Casagrande, E. Lockhart, F. Stimberg, A. Oord, S. Dieleman, and K. Kavukcuoglu, \u201cEfficient neural audio synthesis,\u201d in International Conference on Machine Learning. PMLR, 2018, pp. 2410\u20132419.\"}"}
{"id": "li22e_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A large and growing amount of speech content in real-life scenarios is being recorded on consumer-grade devices in uncontrolled environments, resulting in degraded speech quality. Transforming such low-quality device-degraded speech into high-quality speech is a goal of speech enhancement (SE). This paper introduces a new speech dataset, DDS, to facilitate the research on SE. DDS provides aligned parallel recordings of high-quality speech (recorded in professional studios) and a number of versions of low-quality speech, producing approximately 2,000 hours speech data. The DDS dataset covers 27 realistic recording conditions by combining diverse acoustic environments and microphone devices, and each version of a condition consists of multiple recordings from six microphone positions to simulate different noise and reverberation levels. We also test several SE baseline systems on the DDS dataset and show the impact of recording diversity on performance.\\n\\nIndex Terms: speech dataset, speech enhancement, device recording, acoustic environment\\n\\n1. Introduction\\n\\nHigh-quality speech is desired not only in speech communication systems such as mobile telephony but also for speech-generation tasks such as text-to-speech (TTS) [1] and voice conversion (VC) [2]. However, much of the speech content in real-life scenarios is recorded using non-professional devices (e.g., smartphones and laptops) in non-acoustically treated environments (e.g., homes and offices), where environmental noise, reverberation, and distortion of microphone frequency response degrade the quality of the speech. In this paper, we refer to speech that has been collected under such uncontrolled recording conditions as device-degraded speech.\\n\\nVarious speech enhancement (SE) techniques for raising the quality of device-degraded speech have been attracting attention. Recently, deep neural network (DNN)-based SE methods [3, 4, 5] have become mainstream and shown significant performance improvement over traditional methods [6, 7]. Training a data-driven DNN model usually requires large datasets, but the existing datasets are relatively smaller compared to those used in other domains such as image classification [8]. Also, most datasets consist of only synthetic noisy speech rather than real noisy recordings. Although noisy speech can be obtained easily enough by adding clean speech with random noise segments [9, 10] or convolving with room impulse responses [11], Reddy et al. [12] pointed out that models trained on synthetic datasets often degrade significantly on real recordings. This is mostly because the realistic device degradation cannot be perfectly simulated by synthetic datasets. For example, the measured transfer functions cannot capture the nonlinear reverberation and nonlinear distortion of microphone occurred in real-world recording.\\n\\nIn this work, we present a large-scale public dataset consisting of realistic device-degraded speech, named DDS, to better facilitate the study of SE. DDS contains real recordings that are collected in diverse realistic environments using various microphone devices. More specifically, DDS is built on top of two existing datasets: DAPS [13] and VCTK [14]. We play clean speech recordings (four hours from DAPS and eight hours from VCTK) and re-record waveforms in nine environments (two offices, two conference rooms, three working studios, one living room, and one waiting room) on three different devices (one MEMS and two condenser microphones), producing 27 different recording conditions. For each condition, recordings are conducted with six microphone positions to simulate different noise and reverberation levels. In total, DDS contains 1,944 hours (3 devices \\\\(\\\\times\\\\) 9 environments \\\\(\\\\times\\\\) 6 positions \\\\(\\\\times\\\\) 12 hours) of realistic recordings. As far as we are concerned, this is the largest public dataset comprehensively covering various recording factors (i.e., environment, device, and position). In addition to the study of SE, it can be used in research domains such as domain adaptation in automatic speech recognition (ASR) [15], TTS/VC from found voice data [16], and replay spoof detection in automatic speaker verification (ASV) [17]. The dataset is publicly available online: https://doi.org/10.5281/zenodo.5464104\\n\\n2. Related Work\\n\\nMany speech datasets have been released [9, 11, 13] for the purpose of SE research. However, as mentioned in Section 1, most of them contain only synthetic noisy speech (e.g., MS-SNSD [9] and Valentini's dataset [11]) and disregard microphone variability. Recently, Mathur et al. released Libri-Adapt [18], which contains real recordings on six different microphones. However, as Libri-Adap is primarily developed for ASR research, the quality of its clean speech set (Librispeech-clean100 dataset [19]) is not sufficient for tasks such as TTS. Also, the variability in acoustic environments is simply simulated by artificially adding different types of noises instead of recording in actual rooms.\\n\\nOur work is most closely related to the work on the DAPS dataset [13], in which speech data are collected on different devices in realistic environments. Compared to their work, our dataset has a much larger size (DDS: 1,944 hours; DAPS: 50 hours) and contains more diverse recording conditions (DDS: 1 specifically: a photo studio, a capture studio, and a voice studio. 2 We only released a down-sampled (16 kHz) version of DDS due to limited drive storage capacity. If the reader is interested in acquiring the full version, please contact haoyuli@nii.ac.jp and jyamagis@nii.ac.jp.)\"}"}
{"id": "li22e_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Recording setup. Under each environment, studio-quality speech is played through a monitor loudspeaker and re-recorded on three devices (iPad Air, Uber Mic, and MPM-1000) at six (A\u2013E) positions.\\n\\n3. Dataset Overview\\n\\nIn this section, we explain how we collected the DDS dataset and conduct an initial analysis. Table 1 gives an overview of the dataset settings.\\n\\n3.1. Speech materials\\n\\nClean speech materials are selected from the DAPS [13] and VCTK [14] datasets, which both contain professional voice recordings. Specifically, the DAPS portion has four hours of speech data consisting of 20 speakers (ten female and ten male), and the VCTK portion has eight hours of speech data consisting of 28 speakers (14 female and 14 male). As shown in Fig. 1, we played and recorded speech using devices at a sampling rate of 48 kHz. To avoid the probable bias caused by the loudspeaker characteristics, we used a high-quality coaxial monitor speaker (Presonus Sceptre S6) with very nice flat frequency response. For the DAPS portion, we re-sampled speech files into 44.1 kHz to match the original sampling rate of the DAPS clean set. Finally, we applied a cross-correlation algorithm to align the recorded speech with the original clean speech.\\n\\n3.2. Environments\\n\\nAll recordings were conducted in realistic rooms. We selected a total of nine rooms with different layouts and sizes: two conference rooms, two offices, three studios, one living room, and one waiting room. Each room had a certain level of environmental noise and reverberation. It is worth noting that there is no constraint on the room noise. For example, the noise collected during recording may contain the sound of air conditioner, computer fans, or outdoor noise. Such background noise is close to that occurred in real-world recording, e.g., in home and office.\\n\\n3.3. Devices and recording positions\\n\\nTable 1 lists the three microphone devices used during recording. These were a micro-electromechanical system (MEMS)-processed microphone, which is of small size and commonly embedded in smart devices, and two condenser microphones, which can offer a better sound quality than the MEMS microphones. In addition to recording device, we conducted multiple recordings at six different positions for each device in each environment. The closest position was set to 50 cm directly in front of the speaker, while the farthest was set to 200 cm and at 75\u00b0 angle from the speaker. In this manner, we collected replayed speech with various noise and reverberation levels for each recording condition.\\n\\n3.4. Summary of DDS dataset\\n\\nIn total, the DDS dataset consists of 9 environment settings and 3 device settings, resulting in a total of 27 recording conditions. Each condition consists of 83,058 speech files (13,843 files \u00d7 6).\"}"}
{"id": "li22e_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Average PESQ and ESTOI scores in different environments.\\n\\n| Environment    | DAPS portion | VCTK portion |\\n|----------------|--------------|--------------|\\n|                | PESQ | ESTOI | PESQ | ESTOI |\\n| confroom1      | 2.34 | 0.715 | 2.58 | 0.630 |\\n| confroom2      | 1.98 | 0.617 | 2.27 | 0.527 |\\n| office1        | 2.60 | 0.758 | 2.80 | 0.660 |\\n| office2        | 2.31 | 0.724 | 2.54 | 0.627 |\\n| studio1        | 2.37 | 0.725 | 2.59 | 0.602 |\\n| studio2        | 3.01 | 0.815 | 3.10 | 0.735 |\\n| studio3        | 3.10 | 0.811 | 3.16 | 0.735 |\\n| waitingroom1   | 3.02 | 0.796 | 3.13 | 0.722 |\\n| livingroom1    | 2.34 | 0.723 | 2.61 | 0.647 |\\n\\nTable 3: Average PESQ and ESTOI scores for different devices.\\n\\n| Device     | DAPS portion | VCTK portion |\\n|------------|--------------|--------------|\\n|            | PESQ | ESTOI | PESQ | ESTOI |\\n| iPad       | 2.35 | 0.688 | 2.56 | 0.585 |\\n| Uber Mic   | 2.66 | 0.767 | 2.85 | 0.684 |\\n| MPM-1000   | 2.68 | 0.773 | 2.86 | 0.693 |\\n\\npositions) at sampling rates of 44.1 kHz (for the DAPS portion) and 48 kHz (for the VCTK portion).\\n\\n3.5. Initial analysis of DDS\\n\\nWe conducted an analysis to investigate the effects of the various environments and devices on recording quality. We used PESQ [20] and ESTOI [21] measures to evaluate objective speech quality and intelligibility, respectively. Tables 2, 3, and 4 list the average scores under different conditions of environment, device, and position, respectively. We can clearly see that all recording factors dramatically affect speech quality. For example, as shown in Table 2, recording quality is directly related to room environment. Table 3 shows that the condenser microphones (Uber Mic and MPM-1000) can offer a better sound quality than the MEMS one (iPad). Table 4 shows that speech recorded at a closer position has a better quality. In summary, these results demonstrate that DDS provides a sufficiently large variation of speech data to comprehensively cover common recording factors.\\n\\n4. Baseline Experiments\\n\\nIn this section, we tested three baseline systems on the DDS dataset. We introduce and notate each system as follows.\\n\\n\u2022 DCCRN: Deep complex convolution recurrent network model [22] that performs speech denoising on complex-valued spectrogram instead of real-valued magnitude. This model showed good results in the Deep Noise Suppression (DNS) challenge 2020 [12]. We reimplemented it using the official released codes with the same loss function, i.e., scale-invariant signal-to-noise ratio (SI-SNR) [23]. The number of model parameters is 3.7M.\\n\\n6 https://github.com/huyanxin/DeepComplexCRN\\n\\n\u2022 WaveNet: An end-to-end waveform prediction model based on WaveNet [24], which is the backbone network architecture for many other waveform-domain enhancement models [25, 26]. We reimplemented it with the same model architecture and training objective (L1 loss on log spectrogram magnitude). The number of parameters is 8.4M.\\n\\n\u2022 MelMapping: Our previously proposed system [27] that predicts the clean Mel spectrogram and then reconstructs the speech signal using a universal WaveRNN vocoder [28]. The number of parameters is 11.7M.\\n\\nWe selected 500 utterances as the test set, and all these 500 utterances were recorded at F position (200 cm, 75\u00b0) in the livingroom1 by Uber Mic. To investigate the effect of recording diversity on performance, we selected and built five different training sub-sets from the original DDS dataset:\\n\\n\u2022 D1: Training utterances selected from {Uber} device at A\u2013E positions in {livingroom1} environment.\\n\u2022 D2: Training utterances selected from {iPad} device at A\u2013E positions in {confroom1} environment.\\n\u2022 D3: Training utterances selected from {iPad, MPM-1000} devices at A\u2013E positions in {confroom1} environment.\\n\u2022 D4: Training utterances selected from {iPad, MPM-1000} devices at A\u2013E positions in {confroom1, confroom2, office1, office2} environments.\\n\u2022 D5: Training utterances selected from {iPad, MPM-1000} devices at A\u2013E positions in {confroom1, confroom2, office1, office2, studio1, studio2, studio3, waitingroom1} environments.\\n\\nEach training sub-set contains 40,000 (# sentences \u00d7 # devices \u00d7 # environments \u00d7 # positions) training utterances of the same 32-hour duration. D1 shares the same recording condition (i.e., in living room by Uber Mic) as the test set, so we regard it as a closed-set task. For the remaining four training sets, they are unseen to the test set in terms of recording device and environment and regarded as open-set tasks. Also, they were intentionally designed with increasing recording diversity. For example, D2 comprises only a single condition with one device and one environment, whereas D5 comprises various conditions including two devices and eight environments. We used PESQ and ESTOI scores to evaluate the speech quality and intelligibility, respectively.\\n\\nThe results are plotted in Fig. 2. Among the compared systems, MelMapping performed best in terms of PESQ and\"}"}
{"id": "li22e_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Averaged PESQ and ESTOI scores for different speech enhancement systems on different training sets.\\n\\nESTOI. Although DCCRN and WaveNet suppressed the additive noise well, they were less good at addressing the reverberation and device distortion, resulting in relatively lower scores. Besides, as mentioned in Section 3.1, the alignment between device-degraded speech and its clean counterpart was done using cross-correlation algorithm, which inevitably results in a small time shift. When using time-domain loss functions, even a small time shift can degrade the performance. This explains why the state-of-the-art DCCRN, which was trained on time-domain SI-SNR loss, performed not good in our experiments.\\n\\nThe performance of each system also varied with the training set types. For all three systems, the performance on closed-set D1 was significantly better than that on open-set D2, which indicates that the domain mismatch between training and test affects the enhancement performance significantly. By increasing the recording diversity (from D2 to D5), however, the performance can clearly be improved. It is worth noting that all three systems achieved better or comparable results on D5 (open-set but with the most diversity in recording devices and environments) than on closed-set D1. This further indicates that incorporating various recording conditions into training can help SE models generalize to the noisy recordings encountered in real-world scenarios.\\n\\n5. Conclusion\\nIn this paper, we introduced a large device-degraded speech dataset called DDS to facilitate the research on speech enhancement, especially the enhancement of real-world consumer-grade recordings. This dataset contains studio-quality clean speech and corresponding low-quality versions, with 1,944 hours of real recordings collected under 27 realistic conditions spanning three microphones and nine acoustic environments. We reported several baseline results on this dataset, and showed the beneficial effect of recording diversity of training data on model performance. The DDS dataset is publicly available at https://zenodo.org/record/5464104.\\n\\n6. Acknowledgements\\nThis work was supported in part by JST CREST VoicePersonae project under Grant JPMJCR18A6, Japan, in part by MEXT KAKENHI Grants, 18H04112 and 21H04906, Japan, and in part by SOKENDAI (The Graduate University for Advanced Studies), Japan.\\n\\n7. References\\n[1] H. Zen, A. Senior, and M. Schuster, \u201cStatistical parametric speech synthesis using deep neural networks,\u201d in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 7962\u20137966.\\n[2] L.-H. Chen, Z.-H. Ling, L.-J. Liu, and L.-R. Dai, \u201cVoice conversion using deep neural networks with layer-wise generative training,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 12, pp. 1859\u20131872, 2014.\\n[3] X. Lu, Y. Tsao, S. Matsuda, and C. Hori, \u201cSpeech enhancement based on deep denoising autoencoder.\u201d in Interspeech, vol. 2013, 2013, pp. 436\u2013440.\\n[4] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2014.\\n[5] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. Le Roux, J. R. Hershey, and B. Schuller, \u201cSpeech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR,\u201d in International Conference on Latent Variable Analysis and Signal Separation. Springer, 2015, pp. 91\u201399.\\n[6] S. Boll, \u201cSuppression of acoustic noise in speech using spectral subtraction,\u201d IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 27, no. 2, pp. 113\u2013120, 1979.\\n[7] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, and B.-H. Juang, \u201cSpeech dereverberation based on variance-normalized delayed linear prediction,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 7, pp. 1717\u20131731, 2010.\\n[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImageNet: A large-scale hierarchical image database,\u201d in 2009 IEEE conference on computer vision and pattern recognition. IEEE, 2009, pp. 248\u2013255.\\n[9] C. K. Reddy, E. Beyrami, J. Pool, R. Cutler, S. Srinivasan, and J. Gehrke, \u201cA scalable noisy speech dataset and online subjective test framework,\u201d arXiv preprint arXiv:1909.08050, 2019.\\n[10] Y. Hu, \u201cSubjective evaluation and comparison of speech enhancement algorithms,\u201d Speech Communication, vol. 49, pp. 588\u2013601, 2007.\"}"}
