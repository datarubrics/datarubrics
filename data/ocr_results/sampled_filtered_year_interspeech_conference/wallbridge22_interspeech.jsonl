{"id": "wallbridge22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating perception of spoken dialogue acceptability through surprisal\\n\\nSarenne Wallbridge, Peter Bell, Catherine Lai\\nCentre for Speech Technology Research, University of Edinburgh, United Kingdom\\n{s1301730, peter.bell, c.lai}@ed.ac.uk\\n\\nAbstract\\nSurprisal is used throughout computational psycholinguistics to model a range of language processing behaviour. There is growing evidence that language model (LM) estimates of surprisal correlate with human performance on a range of written language comprehension tasks. Although communicative interaction is arguably the primary form of language use, most studies of surprisal are based on monological, written data. Towards the goal of understanding perception in spontaneous, natural language, we present an exploratory investigation into whether the relationship between human comprehension behaviour and LM-estimated surprisal holds when applied to dialogue, considering both written dialogue, and the lexical component of spoken dialogue. We use a novel judgement task of dialogue utterance acceptability to ask two questions: \u201cHow well can people make predictions about written dialogue and transcripts of spoken dialogue?\u201d and \u201cDoes surprisal correlate with these acceptability judgements?\u201d. We demonstrate that people can make accurate predictions about upcoming dialogue and that their ability differs between spoken transcripts and written conversation. We investigate the relationship between global and local operationalisations of surprisal and human acceptability judgements, finding a combination of both to provide the most predictive power.\\n\\nIndex Terms: psycholinguistics, spoken dialogue, speech perception, discourse structure\\n\\n1. Introduction\\nRecent developments in automatic language modelling have made it possible to test (and extend) psycholinguistic models of human language comprehension. In particular, autoregressive Language Model (LM) estimates of surprisal have been found to correlate with aspects of language perception, including reading times and grammatical acceptability judgements of written text. This is thought to be caused, at least in part, by the shared use of predictive processing that both people and such LMs rely on [1, 2]. In this work, we explore whether improved language modelling capabilities allow us to explore perception of more natural forms of language: interactive communication.\\n\\nAlthough monological texts have been the primary testing ground for psycholinguistic theories of language comprehension, the cognitive mechanisms for comprehension are tuned to natural, spontaneous language [3]. To better understand comprehension, we must study perception of more realistic language. Controlled linguistic stimuli differ from casual language in numerous ways. In particular, they are often isolated [4]. Accounting for context above sentence-level has been a major hurdle in modelling realistic perception, but modern LMs allow integration of much longer contexts than previously possible [5, 6].\\n\\nSpeech is perhaps the most intrinsic modality for communication, but progress in developing models of perceptual salience for the speech signal has been markedly slower than for its textual counterpart, with good reason. Although we learn to use and understand spoken language long before learning to read or write, it differs from text in a number of ways that make modelling more complex. One feature of particular interest here is that speech signals are generated using multiple channels of information transmission: the lexical channel of written language (which words are used), and an additional non-lexical channel (how those words are said) [7]. Channel access changes the way in which we design communicative signals. In this paper, we ask whether surprisal still aligns with perception in spoken transcripts.\\n\\nAs a step towards modelling language perception in more realistic communicative settings, we examine perception through a novel acceptability rating task over dialogue turns. The task is designed to be applied to both written dialogues and transcripts of spoken dialogues, and investigates differences in how lexical information is distributed between these modalities during communicative interaction. Following works that have demonstrated a relationship between LM surprisal estimates and human comprehension behaviour on written, monological data, we explore the relationship between different definitions of turn-level surprisal and human judgements of dialogue.\\n\\n2. Background\\n\\n2.1. Surprisal and Language Comprehension\\nHuman language comprehension is often formalised, at least in part, as a predictive process. Surprisal Theory, one of the most widely-adopted theories of human language comprehension, suggests that the cognitive cost of processing a linguistic segment is determined by how predictable the segment is in its preceding context [8, 9]. It draws on the information-theoretic formalisation of surprisal, which quantifies the amount of information conveyed by a unit as the uncertainty associated with its occurrence [10]. The standard definition of conditional surprisal is the negative log-probability of a unit in \\\\([u_1,\\\\ldots,u_N]\\\\) conditioned on its prior context:\\n\\n\\\\[\\nS(u_n) = -\\\\log_2 p(u_n|u_{<n})\\n\\\\]\\n\\nSurprisal theory has been used to model a range of human language comprehension behaviour including processing of syntactic and pronoun ambiguity, sentence interpretation, and word predictability effects as measured by self-paced reading times and eye-tracking studies [11, 12, 13, 14]. As such, we use surprisal theory as a basis for investigating perception of both written dialogue, and transcripts of spoken dialogue.\\n\\n2.2. Surprisal and Language models\\nLanguage models have been inextricably linked to surprisal since their inception. LMs estimate the probability of a word\u2014in context\u2014its predictability. Recent advances in the capability\"}"}
{"id": "wallbridge22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of LMs to capture longer contexts has prompted the use of LMs to study language and comprehension behaviour. The primary focus of such prior works has been sentence comprehension: investigating the relationship between LM surprisal estimates and self-paced reading times, gaze duration [13, 14, 15], acceptability judgements [16, 17, 18], and brain response data [11, 1]. Though there is general consensus that a relationship between LM surprisal estimates and human perception exists, there are still important aspects that require investigation. Architectural differences in LMs have been found to influence aspects of psychometric predictive power differently [1]. For example, [15] find that surprisal estimates from BERT are highly predictive of acceptability judgments, yet remarkably poor for reading time estimates, and [13] demonstrates that once perplexity is controlled for, syntactic generalization is largely determined by model architecture. The relationship between LM quality and psychometric predictive power doesn't necessarily generalise to typologically different languages [19]. Sentence processing behaviour is also affected by other linguistic features independently of surprisal, including local statistics such as word n-gram frequency [20, 21].\\n\\nThe predictive power of surprisal estimates has been explored in other aspects of perception, e.g., essay quality [22], but there is far less evidence for the relationship between surprisal and perception beyond the sentence processing task.\\n\\n2.3. Extending surprisal to (spoken) dialogues\\n\\nThe vast majority of computational psycholinguistic theories have been developed using monologues. However, the most fundamental forms of language use are interactive [3]. Theories of communication are often centered around interaction and collaboration, e.g., as a joint process where interlocutors collaborate to build common ground [23]. The interactive nature of dialogue likely requires expectations to be conditioned on additional pragmatic features and wider discourse context [24, 25].\\n\\nAutomatic language modelling has been similarly focused on monologue data. Recent work has begun to explore learning latent spaces that are more suited to dialogue by augmenting training objectives to encode dialogue-specific structure and amplify the importance of temporal dependencies between utterances [26, 27, 28, 29, 30]. However, it is unclear whether these strategies encourage encoding of the interactive, joint nature of communication emphasized by psycholinguistic theories.\\n\\nAlthough closely related, spoken and written communication are generated in fundamentally different conditions. The additional non-lexical channel of speech conveys novel information in its own right [31, 32, 33] and interact with the lexical channel to mark novel content, disambiguate lexical information [34, 35], and moderate the distribution of information during communication [7]. Incrementality also asserts much stronger pressure in the spoken domain where utterance design in dialog is often modelled as a parallel and predictive process [36]. As such, lexical information is likely to be distributed differently across written and spoken signals.\\n\\n3. Experimental Design\\n\\n3.1. The Human Judgement Task\\n\\nTo test whether different definitions of surprisal reflect perception of dialogue in the lexical channel of written and spoken conversations, we present a novel dialogue continuation acceptability judgement task. We present participants with a segment of a dialogue, followed by a potential upcoming turn. Following evidence that acceptability judgements are intrinsically gradient [16, 37], participants rate how plausible the turn is in the context of the dialogue on a scale of 1-5 (\u201cVery Unlikely\u201d \u2013 \u201cVery Likely\u201d). This task is similar to sentence acceptability judgements which have been widely studied in the context of surprisal. However, using dialogue turns as a base unit allows us to explore whether surprisal is predictive of acceptability perception in both written and spoken dialogues.\\n\\n3.2. Data\\n\\nExperiments on spoken dialogue were carried out using the Switchboard Telephone Corpus [38] which consists of over 2,400 chit-chat style conversations between 542 participants covering 70 topics. The corpus includes manual transcriptions and turn segmentations. These telephone conversations are an ideal data source for this task as speech is spontaneous and compared to other dialogue domains such as interviews, turns are relatively short, providing a diverse set of upcoming turns from which to sample. We carry out written-dialogue experiments on the DailyDialog corpus. This corpus includes 13,100 written conversations intended to resemble conversations from \u201cdaily life\u201d [39] and thus provides a good match for Switchboard. Dialogues were extracted from web pages for English learners and, similar to Switchboard, span a broad range of topics.\\n\\n3.3. Language model surprisal\\n\\nWe obtained surprisal estimates using the TurnGPT architecture [27], a variant of the GPT-2 [40]. Previous works which investigate the relationship between different language models and human comprehension behaviour consistently demonstrated that GPT-2 outperforms other comparable language model families [13, 15]. TurnGPT is trained with cross-entropy loss and uses an augmented input of three embeddings: token, position and speaker id, with the latter providing important cues for dialogue turn structure.\\n\\nWe took several steps to ensure comparability between spoken and written dialogue surprisal estimates. We used the GPT-2 BPE subword vocabulary, avoiding domain differences from out-of-vocabulary tokens (50259 tokens). We also removed punctuation except for turn-segmentation from DailyDialog. Our TurnGPT model was trained from scratch on equal amounts of data from DailyDialog and Switchboard (\u223c4M tokens total). We used a slightly smaller architecture compared to the originally published model, with 8 layers, 8 attention heads, and an embedding size of 256. To verify that our model does not overfit, we checked that a 4 layer/4 head model didn't obtain lower perplexity on the validation set. The model was trained to achieve the lowest cross-entropy on a validation set containing equal proportions of data from Switchboard and DailyDialog. The model achieves modality-specific perplexities of 60.68 31 on DailyDialog and Switchboard, respectively. Surprisal estimates are scaled by this modality ratio to adjust for inherent differences in predictability across corpora.\\n\\n3.4. Behavioural study and Participants\\n\\nTo study a wide range of realistic instances of communication, stimuli were generated from each corpus by first obtaining contexts with a comparable quantity of information, operationalised as the cumulative per-token surprisal of a set of turns. This context surprisal measure was normalised by the corpus perplexity ratio (see Section 3.3) to enable comparison.\"}"}
{"id": "wallbridge22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Surprisal measure definitions: \\\\( r \\\\) and \\\\( c \\\\) are response and context word sequences resp.\\n\\n\\\\[\\nS_{\\\\text{total}}(r|c) = \\\\sum_{n=1}^{N} \\\\left[ S(r_n|r_{<n},c) \\\\right]\\n\\\\]\\n\\n\\\\[\\nS_{\\\\text{mean}}(r|c) = \\\\frac{1}{N} \\\\sum_{n=1}^{N} \\\\left[ S(r_n|r_{<n},c) \\\\right]\\n\\\\]\\n\\n\\\\[\\nS_{\\\\text{relative}}(r|c) = S_{\\\\text{mean}}(r|c) - S_{\\\\text{mean}}(r)\\n\\\\]\\n\\n\\\\[\\nS_{\\\\text{max}}(r|c) = \\\\max \\\\left[ S(r_n|r_{<n},c) \\\\right]\\n\\\\]\\n\\n\\\\[\\nS_{\\\\text{var}}(r|c) = \\\\frac{1}{N-1} \\\\sum_{n=2}^{N} \\\\left[ S(r_n|r_{<n},c) - S(r_{n-1}|r_{<n-1},c) \\\\right]^2\\n\\\\]\\n\\nAmong the written and spoken stimuli. 10 contexts from each modality were sampled. Each was used to create 10 \\\\((d, c, r)\\\\) stimuli, 1 with the true upcoming turn, and 9 with negative upcoming turns. Negative turns were sampled to span the range of conditional surprisals expected from true \\\\((d, c, r)\\\\) pairs. In total, 100 stimuli were generated for each modality.\\n\\n52 participants were recruited from Prolific Academic, all were native English speakers based in North America. Each participant was presented with 25 stimuli through a Qualtrics survey, taking \\\\(9 \\\\pm 2\\\\) minutes to complete. Attention check questions that were manually selected as extremely likely (including noun overlap) and unlikely were interspersed throughout each survey. 20 participants obtained less than 75% overall accuracy on the check questions; their results were excluded.\\n\\n3.5. Surprisal on a turn level\\n\\nToken-level surprisal estimates were obtained from TurnGPT as the cross entropy loss between predicted and true tokens. From token-level surprisals, we compare a number of 'global' and 'local' operationalisations of turn surprisal suggested in previous work, summarized in Table 1 and described below.\\n\\nGlobal metrics. Cumulative surprisal, \\\\( S_{\\\\text{total}}(r|c) \\\\), is often used to model processing effort of an utterance [15]. To eliminate the influence of sentence length, we consider average surprisal per token, \\\\( S_{\\\\text{mean}}(r|c) \\\\) [16]. We also consider the difference between the conditional and isolated mean utterance surprisal, \\\\( S_{\\\\text{relative}}(r|c) \\\\), to control for the inherent surprisal of an utterance. Psycholinguistic theories diverge on whether or not discourse comprehension involves a context-independent analysis before integrating wider discourse context [41]. The isolated utterance surprisal, \\\\( S_{\\\\text{mean}}(r) \\\\), is computed as the average surprisal of response turn \\\\( r \\\\) conditioned on 100 randomly sampled contexts within the range of acceptable cumulative surprisal.\\n\\nLocal metrics. We are particularly interested in differences in information distribution between written and spoken language (cf [42, 7]) which may require the additional detail of local metrics. Thus, we consider maximum per-unit surprisal, \\\\( S_{\\\\text{max}}(r|c) \\\\), as this has been used to capture points of extreme cognitive load [16]. We also quantify information distribution as surprisal variance between words, \\\\( S_{\\\\text{var}}(r|c) \\\\) [15].\\n\\n4. Results\\n\\nSimilar to previous works in sentence processing, we examine the relationship between these definitions of surprisal and judgement scores using \\\\( \\\\rho \\\\) correlation [16, 15], as well as ordinal regression models.\\n\\n4.1. Perceptual task\\n\\nFigure 1 demonstrates that participants were able to distinguish true turns from negatives samples; 95% and 90% of scores for true turns were either \\\\([4, 5]\\\\) for DailyDialog and Switchboard, respectively. Using the highest mean score per stimuli as a proxy for turn selection, participants obtained respective accuracies of 90% and 70%.\\n\\nFigure 1 also highlights differences between the spoken and written corpora, particularly that participants make less certain judgements for Switchboard stimuli. Although score distributions for true stimuli are similar across corpora, negative stimuli from Switchboard receive a wider range of scores \u2013 twice as many were rated as likely \\\\([3, 4]\\\\) in Switchboard, and participants were more likely to rate turns as \\\"Very Unlikely\\\" \\\\([1]\\\\) in DailyDialog stimuli. Because the informativeness of our stimuli context was controlled for, differences in score distributions are likely the result of turn characteristics. This suggests differences in the informative nature of basic turn units between the lexical content of spoken and written dialogue, which need to be explored in further.\\n\\n4.2. Quantifying predictive power of surprisal\\n\\nGiven that people could leverage the stimuli context to accurately discriminate true upcoming turns from false ones, we used the plausibility scores to explore the relationship between human judgements and LM-estimated surprisal characteristics. If surprisal correlates with perception, we should expect responses that are surprising in context to obtain lower scores, i.e., a negative relationship between surprisal and score.\\n\\nResults in Table 2 demonstrate weak but statistically significant negative correlation of median score with our operationalisations of surprisal.\"}"}
{"id": "wallbridge22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 3: Ablation from full model: Significant differences (>2SE) are in bold.\\n\\n| Removed feature | ELPD diff | SE |\\n|-----------------|-----------|----|\\n| $S_{total}$     | +1.9      | 0.4|\\n| $S_{relative}$  | +0.7      | 0.9|\\n| $S_{mean}$      | +0.3      | 1.2|\\n| $S_{max}$       | -9.6      | 4.3|\\n| $S_{var}$       | -2.1      | 1.9|\\n| Corpus          | +0.3      | 1.2|\\n| All surprisal   | -39.0     | 10.5|\\n\\nIncorporating surprisal estimates is of interest for integrating perceptual and linguistic processing. Slightly stronger correlation coefficients have been reported for related judgements of grammatical acceptability [15, 16], which likely reflect differences between tasks. The reasoning required to make predictions about dialogue (e.g., pragmatic/interaction features) may differ from factors involved in making isolated syntactic judgements.\\n\\nInterestingly, previous works using surprisal estimates from similar language models find global operationalisations to offer more explanatory power than local ones [15]. Our results show the maximum conditional surprisal per token to have the strongest correlation with judgement scores. Again, this variation may be explained by differences in the reasoning required for these behavioural tasks.\\n\\nBecause the individual surprisal measures had significant but relatively weak correlations with median scores, we examine whether they could provide more information in combination. Since our perception experiment used a categorical rating scale (our predictee), we fit multilevel cumulative ordinal regression models (logit link function, uninformative flat priors), using the R package `brms` [43, 44]. Our predictors include the 5 surprisal metrics and a corpus indicator. We also include the context surprisal, and unigram and bigram overlap between the context and response (weighted by corpus frequency) to control for these potential sources of variation. Similarly, we include group level effects (i.e., random intercepts) to control for participant and context identity. For brevity, we don't report group level effects here except to note that we consistently see non-zero variance associated with members of those groups.\\n\\nWe evaluate models using leave-one-out cross-validation, estimating Expected Log Predictive Density (ELPD) [45]. We perform ablation of the individual surprisal measures with respect to the model using all predictors to investigate their contributions to model fit. Table 3 shows the difference in ELPD with respect to the full model (ELPD diff), as well as the associated standard error of the difference (SE). We take a model to have a significantly better fit when the ELPD difference is more than twice the SE. Removing all surprisal measures (leaving only n-gram and group predictors) significantly decreases the fit, indicating that participants were not making decisions based only on direct lexical matching between context and turn. In general, removing local measures (particularly $S_{max}$) reduces the fit, while removing global measures improves it, though not all differences were significant. Removing the corpus indicator also potentially reduces model fit, though the change is within the error margin.\\n\\n### Table 4: Differences between models with full, local, and global features. *Corpus adds an interaction between the corpus indicator and the surprisal measures for that model.\\n\\n| Included Features                      | ELPD diff | SE  |\\n|---------------------------------------|-----------|-----|\\n| global                                | -8.4      | 4.4 |\\n| local                                 | +0.6      | 2.2 |\\n| local+relative                        | +1.3      | 1.2 |\\n| local+mean                            | +2.2      | 1.0 |\\n| (local+mean)*corpus                   | +0.4      | 2.1 |\\n\\n5. Discussion and Conclusions\\n\\nThese results have demonstrated that people can make accurate judgements about upcoming utterances in both written dialogue and transcripts of spoken dialog based on a fixed amount of context, but do so less effectively in the later. They confirm the utility of our task for studying perception of dialogue and indicate that lexical information is distributed differently between written and spoken dialogue (potentially an effect of the different channels available in these modalities). The results also probe the perceptual validity of the response selection paradigm used throughout conversational language modelling [26, 29]. Although people can accurately discriminate between true and false upcoming turns, there is often more than one plausible response for a given context [46].\\n\\nWe then explored the predictive power of global and local operationalisations of surprisal for this communication-based task. In some ways, our findings are complementary to previous works \u2013 all operationalisations displayed weak but significant correlation with human judgements across written and spoken dialogue. However, we found differences in the respective utility of local and global surprisal compared to previously reported results on monologue-based tasks [16, 15, 13]. Combinations of global and local operationalisations provided the highest predictive power for our task. Surprisingly, once other sources of variation had been accounted for, $S_{var}$ had a positive effect on the fit of the logistic regression model such that more variance between turn tokens produces higher rating. Further investigation of the utility of different surprisal metrics and their interactions is required.\\n\\nStimuli used in this work were sampled within a fixed range of cumulative context surprisal. People have been shown to make effective predictions in dialogue using very little context [47], but testing different amounts of context could provide insight into how much information people use to make judgements. Given that LM architecture is known to affect the explanatory power of surprisal estimates, future work could also compare estimates from different architectures [13, 15, 16].\"}"}
{"id": "wallbridge22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] M. Schrimpf, I. A. Blank, G. Tuckute, C. Kauf, E. A. Hosseini, N. G. Kanwisher, J. B. Tenenbaum, ... said it: discriminative perception of speech as a multichannel communication system,\" Proceedings of Interspeech, 2021.\\n\\n[2] V. M. Silva and M. Franke, \\\"Pragmatic prediction in the process-...\\\"\\n\\n[3] M. J. Pickering and S. Garrod, \\\"Toward a mechanistic psychology of cognition: Language and speech processing and the mental lexicon,\\\" in Trends in Cognitive Science, vol. 27, pp. 169 \u2013 180, 2021.\\n\\n[4] B. V. Tucker and M. Ernestus, \\\"Chapter 4. Why we need to investigate casual speech to truly understand language production, a constraint-based approach,\\\" in Behavioral and Brain Sciences, vol. 39, 2016.\\n\\n[5] K. Ethayarajh, \\\"How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings,\\\" in ArXiv, vol. abs/2003.07278, 2020.\\n\\n[6] Q. Liu, M. J. Kusner, and P. Blunsom, \\\"A survey on contextual embeddings,\\\" in ArXiv, vol. abs/1911.03688, 2020.\\n\\n[7] M. P. Aylett and A. Turk, \\\"The smooth signal redundancy hypothesis: A constraint-based approach,\\\" in Cognitive Science, vol. 41, no. 5, pp. 1202\u20131241, 2017.\\n\\n[8] J. Hale, \\\"A probabilistic earley parser as a psycholinguistic model,\\\" in Proceedings of EMNLP, 2001.\\n\\n[9] R. Levy, \\\"Expectation-based syntactic comprehension,\\\" in Language and Speech, vol. 47, pp. 31 \u2013 56, 2004.\\n\\n[10] C. E. Shannon, \\\"A mathematical theory of communication,\\\" in The Bell system technical journal, vol. 27, no. 3, pp. 379\u2013423, 1948.\\n\\n[11] S. L. Frank, L. J. Otten, G. Galli, and G. Vigliocco, \\\"The ERP evidence for the power of discourse,\\\" in Perspectives on socially shared cognition. in Behavioral and brain sciences, vol. 27, pp. 667\u2013710, 2015.\\n\\n[12] R. Levy, \\\"Integrating surprisal and uncertain-input models in on-the-fly repetition,\\\" in Cognitive Science, vol. 44, 2020.\\n\\n[13] E. G. Wilcox, J. Gauthier, J. Hu, P. Qian, and R. P. Levy, \\\"On the hypothesis: A functional explanation for relationships between repetition and interlocutor-track information for language modeling,\\\" in Findings of ACL: EMNLP, 2019, pp. 2396\u20132406.\\n\\n[14] A. Goodkind and K. Bicknell, \\\"Predictive power of word surprisal in factive and manner-of-speaking islands,\\\" in Cognitive Science, vol. 41, no. 1, pp. EL49\u2013EL55, 2017.\\n\\n[15] C. Meister, T. Pimentel, P. Haller, L. J. \\\"A fundamental constraint on language,\\\" in Behavioral and brain sciences, vol. 39 4, pp. 127\u2013149, 2004, pp. 127\u2013149.\\n\\n[16] J. H. Lau, A. Clark, and S. Lappin, \\\"Grammaticality, Acceptability Judgments,\\\" in Transactions of ACL, 2019.\\n\\n[17] S. Richter and R. Chaves, \\\"Investigating the role of verb frequency and interlocutor-track information for language modeling,\\\" in Findings of ACL: EMNLP, 2019, pp. 2396\u20132406.\\n\\n[18] A. Warstadt, A. Singh, and S. R. Bowman, \\\"Neural Network Architecture of Language: Integrative Modeling Converges on Neural Architecture of Language,\\\" in Advances in Methods and Practices in Psychology: A Tutorial,\\\" in Statistics and computing, vol. 27, no. 5, pp. 1413\u20131432, 2017.\\n\\n[19] M. J. Sjerps, C. Decuyper, and A. S. Meyer, \\\"Initiation of utterance planning in response to pre-recorded and \\\"live\\\" utterances,\\\" in NIPS, 2006.\\n\\n[20] A. Goodkind and K. Bicknell, \\\"Local word statistics affect reading times independently of surprisal,\\\" in Proceedings of PNAS, vol. 118, 2021.\\n\\n[21] R. Futrell, E. Gibson, and R. P. Levy, \\\"Lossy-context surprisal: An information-theoretic model of memory effects in sentence comprehension behavior,\\\" in Proceedings of CMCL, 2018, pp. 10\u201318.\\n\\n[22] G. Kharkwal and S. Muresan, \\\"Surprisal as a predictor of essay quality,\\\" in Proceedings of PNAS, vol. 115, no. 20, pp. 5177\u20135182, 2018.\\n\\n[23] H. H. Clark and D. Wilkes-Gibbs, \\\"Referring as a collaborative process,\\\" in Cognition, vol. 22, pp. 1\u201339, 1986.\\n\\n[24] H. H. Clark and S. E. Brennan, \\\"Grounding in communication.\\\" in Advances in Methods and Practices in Psychology: A Tutorial,\\\" in Statistics and computing, vol. 27, no. 5, pp. 1413\u20131432, 2017.\\n\\n[25] J. Degen and M. K. Tanenhaus, \\\"Processing scalar implicature: A constraint-based approach,\\\" in Cognitive Science, vol. 41, no. 5, pp. 1202\u20131241, 2017.\\n\\n[26] M. Henderson, I. Casanueva, N. Mrkv\u0161i'c, P. hao Su, Tsung-Yi Lin, and Tsung-Yi Lin, \\\"Transfo: A transfer learning approach for neural network based conversational agents,\\\" in Cognitive science, vol. 42, no. 5, pp. 1202\u20131241, 2017.\\n\\n[27] E. Ekstedt and G. Skantze, \\\"TurnGPT: a Transformer-based Language Model for Predicting Turn-taking in Spoken Dialog,\\\" in Proceedings of ICASSP 1992, 1992, pp. 517\u2013520.\\n\\n[28] T. Wolf, V. Sanh, J. Chaumond, and C. Delangue, \\\"Transfer learning from dialogue acts and entities,\\\" in SIGDIAL, 2020.\\n\\n[29] J. Han, T. Hong, B. Kim, Y. Ko, and J. Seo, \\\"Fine-grained findings of ACL: EMNLP, 2019, pp. 986\u2013995.\\n\\n[30] C. Liu, R. Wang, J. Liu, J. Sun, F. Huang, and L. Si, \\\"DialogueCSE: Dialogue-based contrastive learning of sentence embeddings,\\\" in Proceedings of EMNLP, 2019, pp. 2396\u20132406.\\n\\n[31] N. G. Ward and B. H. Walker, \\\"Estimating the potential of signal redundancy, prosodic prominence, and duration in spontaneous telephone speech,\\\" in Cognition, vol. 106, no. 3, pp. 1126\u20131177, 2008.\\n\\n[32] S. K. Kim and M. Sumner, \\\"Beyond lexical meaning: The effect of discourse and interlocutor-track information for language modeling,\\\" in Proceedings of EMNLP, 2021.\\n\\n[33] B. M. Ben-David, N. Multani, V. Shakuf, F. Rudzicz, and P. H. van Beek, \\\"Estimating the potential of signal redundancy, prosodic prominence, and duration in spontaneous telephone speech,\\\" in Cognition, vol. 106, no. 3, pp. 1126\u20131177, 2008.\\n\\n[34] J. Hirschberg and J. Pierrehumbert, \\\"The intonational structuring of discourse,\\\" in Perspectives on socially shared cognition. in Behavioral and brain sciences, vol. 39, 2016.\\n\\n[35] H. H. Clark and S. E. Brennan, \\\"Grounding in communication.\\\" in Advances in Methods and Practices in Psychology: A Tutorial,\\\" in Statistics and computing, vol. 27, no. 5, pp. 1413\u20131432, 2017.\\n\\n[36] M. J. Sjerps, C. Decuyper, and A. S. Meyer, \\\"Initiation of utterance planning in response to pre-recorded and \\\"live\\\" utterances,\\\" in NIPS, 2006.\\n\\n[37] N. Chater, J. B. Tenenbaum, and A. L. Yuille, \\\"Probabilistic modelling of cognition: Conceptual foundations,\\\" in Trends in Cognitive Sciences, vol. 4, pp. 127\u2013149, 2004.\\n\\n[38] J. J. Godfrey, E. C. Holliman, and J. McDaniel, \\\"Switchboard: Telephone speech corpus for research and development,\\\" in Proceedings of PNAS, vol. 110, pp. 8329\u20138334, 2013.\\n\\n[39] Y. Li, H. Su, X. Shen, W. Li, Z. Cao, and S. Niu, \\\"DailyDialog: A manually labelled multi-turn dialogue dataset,\\\" in Proceedings of ICASSP 1992, 1992, pp. 517\u2013520.\\n\\n[40] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, and D. Courville, \\\"Learning Transferable Adversarial Examples and Representation Decoders,\\\" in OpenAI blog, 2019.\\n\\n[41] M. S. Nieuwland and J. J. A. V. Berkum, \\\"When peanuts fall in love: N400 evidence for the power of discourse,\\\" in Journal of Cognition, vol. 374, 2019.\\n\\n[42] R. Levy and T. F. Jaeger, \\\"Speakers optimize information density in factive and manner-of-speaking islands,\\\" in Cognitive Science, vol. 41, no. 5, pp. 1202\u20131241, 2017.\\n\\n[43] P.-C. B\u00fcrkner and M. Vuorre, \\\"Ordinal regression models in psychological science,\\\" in Statistics and computing, vol. 27, pp. 1413\u20131432, 2017.\\n\\n[44] P.-C. B\u00fcrkner, \\\"brms: An R package for Bayesian multilevel models using Stan,\\\" in Journal of Statistical Software, vol. 80, no. 1, pp. 1\u201328, 2017.\\n\\n[45] A. Vehtari, A. Gelman, and J. Gabry, \\\"Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC,\\\" in Journal of Statistical Software, vol. 80, no. 1, pp. 1\u201328, 2017.\\n\\n[46] A. Cervone and G. Riccardi, \\\"Is this dialogue coherent? learning from dialogue acts and entities,\\\" in SIGDIAL, 2020.\\n\\n[47] S. Wallbridge, P. Bell, and C. Lai, \\\"It's not what you said, it's how you said it: A constraint-based approach,\\\" in Linguistics and Interdisciplinary Approaches to Language Processing, 2017.\\n\\n[48] P. Rahtz and J. D. A. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D"}
