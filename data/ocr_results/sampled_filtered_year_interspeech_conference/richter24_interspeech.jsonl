{"id": "richter24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"EARS: An Anechoic Fullband Speech Dataset Benchmarked for Speech Enhancement and Dereverberation\\n\\nJulius Richter\u00b9, Yi-Chiao Wu\u00b2, Steven Krenn\u00b2, Simon Welker\u00b9, Bunlong Lay\u00b9, Shinji Watanabe\u00b3, Alexander Richard\u00b2, Timo Gerkmann\u00b9\\n\\n\u00b9Signal Processing (SP), Universit\u00e4t Hamburg, Germany\\n\u00b2Codec Avatars Lab, Meta, Pittsburgh, PA, USA\\n\u00b3Language Technology Institute, Carnegie Mellon University, Pittsburgh, PA, USA\\n\\njulius.richter@uni-hamburg.de, timo.gerkmann@uni-hamburg.de\\n\\nAbstract\\n\\nWe release the EARS (Expressive Anechoic Recordings of Speech) dataset, a high-quality speech dataset comprising 107 speakers from diverse backgrounds, totaling in 100 hours of clean, anechoic speech data. The dataset covers a large range of different speaking styles, including emotional speech, different reading styles, non-verbal sounds, and conversational freeform speech. We benchmark various methods for speech enhancement and dereverberation on the dataset and evaluate their performance through a set of instrumental metrics. In addition, we conduct a listening test with 20 participants for the speech enhancement task, where a generative method is preferred. We introduce a blind test set that allows for automatic online evaluation of uploaded data. Dataset download links and automatic evaluation server can be found online.\\n\\nIndex Terms: speech dataset, speech enhancement, dereverberation, benchmark\\n\\n1. Introduction\\n\\nLearning-based speech processing has seen huge leaps forward in recent years, with the impact of deep learning spanning essentially all areas from speech representation learning [1] over text-to-speech [2] to speech enhancement [3]. Publicly available datasets such as LibriSpeech [4] or VCTK [5] have undoubtedly been a key driver of open and reproducible research in our field and have enabled steady progress. However, these datasets typically come with multiple shortcomings and are either too small, of low recording quality or do not span a large enough variety of different speakers and speaking styles.\\n\\nTo overcome these shortcomings, we release the EARS dataset. EARS contains 100 h of anechoic speech recordings at 48 kHz from over 100 English speakers with high demographic diversity. The dataset spans the full range of human speech, including reading tasks in seven different reading styles, emotional reading and freeform speech in 22 different emotions, conversational speech, and non-verbal sounds like laughter or coughing.\\n\\nIn addition, we set up a speech enhancement and speech dereverberation benchmark on EARS, comparing several predictive [6, 7] and generative [8, 9] speech enhancement methods. The benchmarks are intended to provide valuable insights into models' strengths, limitations, and comparability, thus promoting the development of robust and efficient speech enhancement systems.\\n\\n1. https://sp-uhh.github.io/ears_dataset/\\n\\n2. EARS dataset\\n\\nA good speech dataset is characterized by its scale, diversity, and high recording quality. However, most existing datasets fall short in one or more of these characteristics; see Table 1. Most notably, a dataset that is of high recording quality (clean 48 kHz audio), has a sufficient scale and covers the full range of human speech as opposed to only reading or neutral speech does not exist to the best of our knowledge. Yet, such a dataset is strongly required to advance research ranging from speech synthesis over voice and style conversion to speech enhancement.\\n\\nWe overcome these limitations with the EARS dataset, which provides high speaker and speech diversity paired with the highest recording quality.\\n\\nHigh Recording Quality.\\n\\nAll speech is recorded in an anechoic chamber as 32-bit audio at 48 kHz. We simultaneously record with a low-noise GRAS 40HH and a GRAS 48BL microphone, which are both mounted about 1 m in front of the participant. The first microphone has low self-noise and high sensitivity to capture subtle and nuanced speech signals, while the second has lower sensitivity to capture high-energy speech like yelling without clipping, allowing us to capture the full dynamic range of human speech, see Figure 1. We use the high-sensitivity recording for our dataset whenever possible. In the few (5% of the dataset) cases, like yelling, where the high-sensitivity microphone clips, we replace it with the lower-sensitivity microphone. To maintain the same audio characteristics between both microphones, we measure the transfer function between them using a sine-sweep and deconvolution and equalize the low-sensitivity microphone accordingly. See the project page for examples.\\n\\nHigh Speaker Diversity.\\n\\nWe recorded 107 speakers from diverse demographic backgrounds, each for close to one hour, resulting in a dataset with 100 h of clean speech. Our speakers...\"}"}
{"id": "richter24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Speech Datasets | # Files | # Rooms | Rate | Quality |\\n|----------------|--------|---------|------|---------|\\n| DNS (LibriVox) | 10     | 556     | 48 kHz | \u2717    |\\n| MSP-Conversation (v1.0) | 14   | 1948   | 16 kHz | \u2717    |\\n| MSP-Podcast (v1.10) | 166   | 1458   | 16 kHz | \u2717    |\\n| LibriSpeech | 982    | 2484    | 16 kHz | \u2717    |\\n| LJSpeech | 24     | 1      | 22.05 kHz | \u2717    |\\n| TIMIT | 5      | 632     | 16 kHz | \u2717    |\\n| VCTK | 44     | 110     | 48 kHz | \u2717    |\\n| WSJ0 | 29     | 119     | 16 kHz | \u2717    |\\n| EARS (ours) | 100   | 107     | 48 kHz | \u2713    |\\n\\nTable 1: Speech datasets.\\n\\nIn contrast to existing datasets, the EARS dataset is of higher recording quality, large, and more diverse. Reading tasks feature seven styles (regular, loud, whisper, fast, slow, high pitch, and low pitch). Additionally, the dataset features unconstrained freeform speech and speech in 22 different emotional styles. We provide transcriptions of the reading portion and metadata of the speakers (gender, age, race, first language).\\n\\n| RIR Datasets | # Files | Rooms |\\n|--------------|--------|-------|\\n| ACE-Challenge | 84     | building lobby, lecture room, meeting room, office |\\n| AIR          | 344    | auditorium, corridor, lecture room, meeting room, stairway |\\n| ARNI         | 1000   | variable acoustics laboratory |\\n| BRUDEX       | 144    | variable acoustics laboratory |\\n| dEchorate    | 648    | variable acoustics laboratory |\\n| DetmoldSRIR  | 49     | concert hall, music chamber, theater |\\n| Palimpsest   | 44     | air raid shelter, dockyard, submarine |\\n\\nTable 2: RIR datasets. To construct EARS-Reverb, we use 2313 RIR files with different room characteristics.\\n\\n3.1. EARS-WHAM\\nFor the speech enhancement task, we construct the EARS-WHAM dataset, which mixes speech from the EARS dataset with real noise recordings from the WHAM! dataset (CC BY-NC 4.0 license). We mix speech and noise files at signal-to-noise ratios (SNRs) randomly sampled in a range of $[-2.5, 17.5]$ dB, where we compute the SNR using loudness K-weighted relative to full scale (LKFS) standardized in ITU-R BS.1770 to obtain a more perceptually meaningful scaling and also to remove silent regions from the SNR computation. We additionally create a blind test set for which we only publish the noisy audio files but not the clean ground truth. It contains 743 files (2 h) from six speakers (3 male, 3 female) that are not part of the EARS dataset and noise especially recorded for this test set. We set up an evaluation server for blind evaluation on this test set, which can be found online.\\n\\n3.2. EARS-Reverb\\nFor the task of dereverberation, we use real recorded room impulse responses (RIRs) from multiple public datasets (CC BY 4.0, MIT license). Table 2 shows statistics on the RIR datasets used. All RIRs are fullband, and we use a randomly selected channel for multi-channel recordings. We generate reverberant speech by convolving the clean speech with the RIR. To avoid a time delay between the reverberant and clean speech signal caused by the direct path of the RIR, we cut off the beginning of the RIR up to the index with the highest amplitude. We only use RIRs with an RT60 reverberation time of less than 1.0 s.\\n\\n### Baseline Methods\\n\\n| Method                  | Date       | # Params | GMACs | Proc/s [s] |\\n|-------------------------|------------|----------|-------|------------|\\n| Conv-TasNet             | May 2019   | 8.7 M    | 28    | 0.015      |\\n| CDiffuSE                | May 2022   | 18.1 M   | 18,382| 42.268     |\\n| Demucs                  | June 2023  | 83.6 M   | 60    | 0.027      |\\n| SGMSE+                  | June 2023  | 64.8 M   | 47,984| 2.575      |\\n\\nTable 3: Baseline methods. Date of publication, number of parameters, MACs for an input of four seconds, and average processing time per one-second input length.\\n\\n3.1. EARS-WHAM\\nFor the speech enhancement task, we construct the EARS-WHAM dataset, which mixes speech from the EARS dataset with real noise recordings from the WHAM! dataset (CC BY-NC 4.0 license). We mix speech and noise files at signal-to-noise ratios (SNRs) randomly sampled in a range of $[-2.5, 17.5]$ dB, where we compute the SNR using loudness K-weighted relative to full scale (LKFS) standardized in ITU-R BS.1770 to obtain a more perceptually meaningful scaling and also to remove silent regions from the SNR computation. We additionally create a blind test set for which we only publish the noisy audio files but not the clean ground truth. It contains 743 files (2 h) from six speakers (3 male, 3 female) that are not part of the EARS dataset and noise especially recorded for this test set. We set up an evaluation server for blind evaluation on this test set, which can be found online.\\n\\n3.2. EARS-Reverb\\nFor the task of dereverberation, we use real recorded room impulse responses (RIRs) from multiple public datasets (CC BY 4.0, MIT license). Table 2 shows statistics on the RIR datasets used. All RIRs are fullband, and we use a randomly selected channel for multi-channel recordings. We generate reverberant speech by convolving the clean speech with the RIR. To avoid a time delay between the reverberant and clean speech signal caused by the direct path of the RIR, we cut off the beginning of the RIR up to the index with the highest amplitude. We only use RIRs with an RT60 reverberation time of less than 1.0 s.\"}"}
{"id": "richter24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Listening Test.\\n\\nWe conduct a MUSHRA-like (Stimuli with Hidden Reference and Anchor) multiple choice rating (MCR) listening test on the EARS-WHAM test set, respectively. Among the methods, the generative speech separation model Demucs v4 [7] scores the highest across most metrics, particularly high scores in POLQA and SIGMOS. For instance, Demucs achieves an SI-SDR of 73 \u00b1 81 dB, 63 \u00b1 0 dB, and 57 \u00b1 0 dB, respectively, which includes the best results across most metrics, and the reference of Table 5 demonstrates the overall performance. In this section, we present the overall performance of the methods. In this section, we present the overall performance of the methods. We provide an empirical evaluation of the speech enhancement results on the EARS-WHAM test set and the blind test set, respectively. Among the methods, the generative speech separation model Demucs v4 [7], as a representative of the predictive methods, performs well, particularly in the POLQA and SIGMOS metrics. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly in the POLQA and SIGMOS metrics. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly high scores in POLQA and SIGMOS. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly high scores in POLQA and SIGMOS. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly high scores in POLQA and SIGMOS. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly high scores in POLQA and SIGMOS. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly high scores in POLQA and SIGMOS. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly high scores in POLQA and SIGMOS. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly high scores in POLQA and SIGMOS. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly high scores in POLQA and SIGMOS. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly high scores in POLQA and SIGMOS. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly high scores in POLQA and SIGMOS. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly high scores in POLQA and SIGMOS. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly high scores in POLQA and SIGMOS. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly high scores in POLQA and SIGMOS. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly high scores in POLQA and SIGMOS. Demucs [7] achieves the best performance across most metrics, particularly high scores in POLQA and SIGMOS. Demucs [7], as a representative of the predictive methods, performs well, particularly high scores in POLQA and SIGMOS.\"}"}
{"id": "richter24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"subjective scores based on each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the qualitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the qualitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u2013100. The trends support the quantitative \\\"rate the overall quality considering artifacts and residual noise\\\" note that range [TOI, and WER scores segmented by input SNR, where 0 dB de-\\nlar of each on a scale of 0\u201310"}
{"id": "richter24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. Acknowledgements\\nThis work has been funded by the German Research Foundation (DFG) in the transregio project Crossmodal Learning (TRR 169) and DASHH (Data Science in Hamburg \u2013 Helmholtz Graduate School for the Structure of Matter) with Grant-No. HIDSS-0002. We would like to thank J. Berger and Rohde & Schwarz SwissQual AG for their support with POLQA.\\n\\n7. References\\n\\n[1] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maal\u00f8e et al., \\\"Self-supervised speech representation learning: A review,\\\" IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1179\u20131210, 2022.\\n\\n[2] X. Tan, T. Qin, F. Soong, and T.-Y. Liu, \\\"A survey on neural speech synthesis,\\\" arXiv preprint arXiv:2106.15561, 2021.\\n\\n[3] D. Wang and J. Chen, \\\"Supervised speech separation based on deep learning: An overview,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 10, pp. 1702\u20131726, 2018.\\n\\n[4] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \\\"Librispeech: An ASR corpus based on public domain audio books,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2015, pp. 5206\u20135210.\\n\\n[5] J. Yamagishi, C. Veaux, and K. MacDonald, \\\"CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92),\\\" 2019. [Online]. Available: https://datashare.ed.ac.uk/handle/10283/3443\\n\\n[6] Y. Luo and N. Mesgarani, \\\"Conv-TasNet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 8, pp. 1256\u20131266, 2019.\\n\\n[7] S. Rouard, F. Massa, and A. D. D\u00e9fossez, \\\"Hybrid transformers for music source separation,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2023.\\n\\n[8] Y.-J. Lu, Z.-Q. Wang, S. Watanabe, A. Richard, C. Yu, and Y. Tsao, \\\"Conditional diffusion probabilistic model for speech enhancement,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2022, pp. 7402\u20137406.\\n\\n[9] J. Richter, S. Welker, J.-M. Lemercier, B. Lay, and T. Gerkmann, \\\"Speech enhancement and dereverberation with diffusion-based generative models,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 31, pp. 2351\u20132364, 2023.\\n\\n[10] H. Dubey, A. Aazami, V. Gopal, B. Naderi, S. Braun, R. Cutler, H. Gamper, M. Golestaneh, and R. Aichner, \\\"ICASSP 2023 deep noise suppression challenge,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2023.\\n\\n[11] L. Martinez-Lucas, M. Abdelwahab, and C. Busso, \\\"The MSP-conversation corpus,\\\" in ISCA Interspeech, 2020, pp. 1823\u20131827.\\n\\n[12] R. Lotfian and C. Busso, \\\"Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings,\\\" IEEE Transactions on Affective Computing, vol. 10, no. 4, pp. 471\u2013483, 2019.\\n\\n[13] K. Ito and L. Johnson, \\\"The LJ Speech Dataset,\\\" 2017. [Online]. Available: https://keithito.com/LJ-Speech-Dataset/\\n\\n[14] J. S. Garofolo, \\\"TIMIT acoustic phonetic continuous speech corpus,\\\" Linguistic Data Consortium, 1993.\\n\\n[15] J. S. Garofolo, D. Graff, D. Paul, and D. Pallett, \\\"CSR-I (WSJ0) Complete - Linguistic Data Consortium,\\\" 1993. [Online]. Available: https://catalog.ldc.upenn.edu/LDC93s6a\\n\\n[16] J. Eaton, N. D. Gaubitch, A. H. Moore, and P. A. Naylor, \\\"Estimation of room acoustic parameters: The ACE challenge,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 10, pp. 1681\u20131693, 2016.\\n\\n[17] M. Jeub, M. Schafer, and P. Vary, \\\"A binaural room impulse response database for the evaluation of dereverberation algorithms,\\\" in IEEE Int. Conference on Digital Signal Processing, 2009.\\n\\n[18] K. Prawda, S. J. Schlecht, and V. Valim\u00e4ki, \\\"Robust selection of clean swept-sine measurements in non-stationary noise,\\\" The Journal of the Acoustical Society of America, vol. 151, no. 3, pp. 2117\u20132126, 2022.\\n\\n[19] D. Fejgin, W. Middelberg, and S. Doclo, \\\"BRUDEX database: Binaural room impulse responses with uniformly distributed external microphones,\\\" in Proc. ITG Conference on Speech Communication, 2023, pp. 126\u2013130.\\n\\n[20] D. D. Carlo, P. Tandeitnik, C. Foy, N. Bertin, A. Deleforge, and S. Gannot, \\\"dEchorate: a calibrated room impulse response dataset for echo-aware signal processing,\\\" EURASIP Journal on Audio, Speech, and Music Processing, 2021.\\n\\n[21] S. V. Amengual Gari, B. Sahin, D. Eddy, and M. Kob, \\\"Open database of spatial room impulse responses at Detmold university of music,\\\" in Audio Engineering Society Convention 149, 2020.\\n\\n[22] \\\"A sonic Palimpsest: Revisiting Chatham historic dockyards.\\\" [Online]. Available: https://research.kent.ac.uk/sonic-palimpsest/impulse-responses/\\n\\n[23] G. Wichern, J. Antognini, M. Flynn, L. R. Zhu, E. McQuinn, D. Crow, E. Manilow, and J. L. Roux, \\\"WHAM!: Extending speech separation to noisy environments,\\\" in ISCA Interspeech, 2019, pp. 1368\u20131372.\\n\\n[24] Recommendation ITU-R BS.1770-5, \\\"Algorithms to measure audio programme loudness and true-peak audio level,\\\" International Telecommunication Union (ITU), 2023. [Online]. Available: https://www.itu.int/rec/R-REC-BS.1770-5-202311-I/en\\n\\n[25] C. J. Steinmetz and J. D. Reiss, \\\"pyloudnorm: A simple yet flexible loudness meter in python,\\\" in 150th AES Convention, 2021.\\n\\n[26] ITU-T Rec. P.863, \\\"Perceptual objective listening quality prediction,\\\" Int. Telecom. Union (ITU), 2018. [Online]. Available: https://www.itu.int/rec/T-REC-P.863-201803-I/en\\n\\n[27] A. Rix, J. Beerends, M. Hollier, and A. Hekstra, \\\"Perceptual evaluation of speech quality (PESQ) - a new method for speech quality assessment of telephone networks and codecs,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2001, pp. 749\u2013752.\\n\\n[28] J. Jensen and C. H. Taal, \\\"An algorithm for predicting the intelligibility of speech masked by modulated noise maskers,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 11, pp. 2009\u20132022, 2016.\\n\\n[29] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, \\\"SDR\u2013half-baked or well done?\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2019, pp. 626\u2013630.\\n\\n[30] N. C. Ristea, A. Saabas, R. Cutler, B. Naderi, S. Braun, and S. Branets, \\\"ICASSP 2024 speech signal improvement challenge,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2024.\\n\\n[31] B. Naderi, R. Cutler, and N.-C. Ristea, \\\"Multi-dimensional speech quality assessment in crowdsourcing,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2024.\\n\\n[32] C. K. Reddy, V. Gopal, and R. Cutler, \\\"DNSMOS: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2021, pp. 6493\u20136497.\\n\\n[33] ITU-T Rec. P.808, \\\"Subjective evaluation of speech quality with a crowdsourcing approach,\\\" International Telecommunication Union, 2021. [Online]. Available: https://www.itu.int/rec/T-REC-P.808-202106-I/en\\n\\n[34] O. Kuchaiev, J. Li, H. Nguyen, O. Hrinchuk, R. Leary, B. Ginsburg, S. Kriman, S. Beliaev, V. Lavrukhin, J. Cook et al., \\\"NeMo: a toolkit for building AI applications using neural modules,\\\" arXiv preprint arXiv:1909.09577, 2019.\\n\\n[35] J.-M. Lemercier, J. Richter, S. Welker, and T. Gerkmann, \\\"Analysing diffusion-based generative approaches versus discriminative approaches for speech restoration,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2023.\"}"}
