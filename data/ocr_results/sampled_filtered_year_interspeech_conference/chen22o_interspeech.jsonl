{"id": "chen22o_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] H. Erdogan, J. R. Hershey, S. Watanabe et al., \u201cImproved MVDR Beamforming Using Single-Channel Mask Prediction Networks,\u201d in Proc. Interspeech 2016, 2016, pp. 1981\u20131985.\\n\\n[2] L. Chai, J. Du, Q.-F. Liu et al., \u201cA cross-entropy-guided measure (cegm) for assessing speech recognition performance and optimizing dnn-based speech enhancement,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 106\u2013117, 2021.\\n\\n[3] S. Watanabe, M. Delcroix, F. Metze et al., New Era for Robust Speech Recognition - Exploiting Deep Learning. Springer, 2017.\\n\\n[4] S. Renals, T. Hain, and H. Bourlard, \u201cRecognition and understanding of meetings the ami and amida projects,\u201d in Proc. ASRU 2007, 2007, pp. 238\u2013247.\\n\\n[5] A. Janin, D. Baron, J. Edwards et al., \u201cThe icsi meeting corpus,\u201d in Proc. ICASSP 2003, 2003.\\n\\n[6] W. Rao, Y.-H. Fu, Y.-X. Hu et al., \u201cConferencingspeech challenge: Towards far-field multi-channel speech enhancement for video conferencing,\u201d in Proc. ASRU 2021, 2021.\\n\\n[7] S. Watanabe, M. Mandel, J. Barker et al., \u201cCHiME-6 Challenge: Tackling Multispeaker Speech Recognition for Unsegmented Recordings,\u201d in Proc. CHiME 2020, 2020, pp. 1\u20137.\\n\\n[8] H. McGurk and J. MacDonald, \u201cHearing lips and seeing voices,\u201d Nature, pp. 746\u2013748, 1976.\\n\\n[9] L. D. Rosenblum, \u201cSpeech perception as a multimodal phenomenon,\u201d Current Directions in Psychological Science, pp. 405\u2013409, 2008.\\n\\n[10] D. W. Massaro and J. A. Simpson, Speech perception by ear and eye: A paradigm for psychological inquiry. Psychology Press, 2014.\\n\\n[11] J. Gowdy, A. Subramanya, C. Bartels et al., \u201cDbn based multi-stream models for audio-visual speech recognition,\u201d in Proc. ICASSP 2004, 2004, pp. I\u2013993.\\n\\n[12] G. Papandreou, A. Katsamanis, V. Pitsikalis et al., \u201cAdaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition,\u201d IEEE Transactions on Audio, Speech, and Language Processing, pp. 423\u2013435, 2009.\\n\\n[13] G. Potamianos, C. Neti, G. Gravier et al., \u201cRecent advances in the automatic recognition of audiovisual speech,\u201d Proceedings of the IEEE, pp. 1306\u20131326, 2003.\\n\\n[14] J. S. Chung, A. Senior, O. Vinyals et al., \u201cLip reading sentences in the wild,\u201d in Proc. CVPR 2017, 2017, pp. 3444\u20133453.\\n\\n[15] T. Afouras, J. S. Chung, A. Senior et al., \u201cDeep audio-visual speech recognition,\u201d IEEE transactions on pattern analysis and machine intelligence, 2018.\\n\\n[16] S. Petridis, T. Stafylakis, P. Ma, G. Tzimiropoulos, and M. Pantic, \u201cAudio-visual speech recognition with a hybrid ctc/attention architecture,\u201d in Proc. SLT 2018, 2018, pp. 513\u2013520.\\n\\n[17] A. Vaswani, N. Shazeer, N. Parmar et al., \u201cAttention is all you need,\u201d Advances in neural information processing systems, 2017.\\n\\n[18] G. Sterpu, C. Saam, and N. Harte, \u201cHow to teach dnns to pay attention to the visual modality in speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1052\u20131064, 2020.\\n\\n[19] B. Xu, C. Lu, Y. Guo et al., \u201cDiscriminative multi-modality speech recognition,\u201d in Proc. CVPR 2020, 2020, pp. 14 433\u201314 442.\\n\\n[20] P. Ma, S. Petridis, and M. Pantic, \u201cEnd-to-end audio-visual speech recognition with conformers,\u201d in Proc. ICASSP 2021, 2021, pp. 7613\u20137617.\\n\\n[21] A. Gulati, J. Qin, C.-C. Chiu et al., \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech 2020, 2020, pp. 5036\u20135040.\\n\\n[22] T. Makino, H. Liao, Y. Assael et al., \u201cRecurrent neural network transducer for audio-visual speech recognition,\u201d in Proc. ASRU 2019, 2019, pp. 905\u2013912.\\n\\n[23] O. Braga, T. Makino, O. Siohan et al., \u201cEnd-to-end multi-person audio/visual automatic speech recognition,\u201d in Proc. ICASSP 2020, 2020, pp. 6994\u20136998.\\n\\n[24] F. Tao and C. Busso, \u201cGating neural network for large vocabulary audiovisual speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1290\u20131302, 2018.\\n\\n[25] M. Cooke, J. Barker, S. Cunningham et al., \u201cAn audio-visual corpus for speech perception and automatic speech recognition,\u201d The Journal of the Acoustical Society of America, pp. 2421\u20132424, 2006.\\n\\n[26] G. Zhao, M. Barnard, and M. Pietikainen, \u201cLipreading with local spatiotemporal descriptors,\u201d IEEE Transactions on Multimedia, pp. 1254\u20131265, 2009.\\n\\n[27] I. Anina, Z. Zhou, G. Zhao et al., \u201cOuluvs2: A multi-view audio-visual database for non-rigid mouth motion analysis,\u201d in Proc. FG 2015, 2015, pp. 1\u20135.\\n\\n[28] N. Harte and E. Gillen, \u201cTcd-timit: An audio-visual corpus of continuous speech,\u201d IEEE Transactions on Multimedia, pp. 603\u2013615, 2015.\\n\\n[29] J. S. Chung and A. Zisserman, \u201cLip reading in the wild,\u201d in Proc. ACCV 2016, 2016.\\n\\n[30] T. Afouras, J. S. Chung, and A. Zisserman, \u201cLrs3-ted: a large-scale dataset for visual speech recognition,\u201d arXiv preprint arXiv:1809.00496, 2018.\\n\\n[31] A. Ephrat, I. Mosseri, O. Lang et al., \u201cLooking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation.\u201d ACM Trans. Graph., 2018.\\n\\n[32] H. Liu, Z. Chen, and W. Shi, \u201cRobust audio-visual mandarin speech recognition based on adaptive decision fusion and tone features,\u201d in Proc. ICIP 2020, 2020.\\n\\n[33] J. Yu, R. Su, L. Wang et al., \u201cA multi-channel/multi-speaker inter-active 3d audio-visual speech corpus in mandarin,\u201d in Proc. ISC-SP 2016, 2016, pp. 1\u20135.\\n\\n[34] Y. Zhao, R. Xu, and M. Song, \u201cA cascade sequence-to-sequence model for chinese mandarin lip reading,\u201d in Proceedings of the ACM Multimedia Asia, 2019.\\n\\n[35] H. Chen, H. Zhou, J. Du et al., \u201cThe first multimodal information based speech processing (misp) challenge: Data, tasks, baselines and results,\u201d in Proc. ICASSP 2022, 2022.\\n\\n[36] L. Drude, J. Heymann, C. Boeddeker et al., \u201cNara-wpe: A python package for weighted prediction error dereverberation in numpy and tensorflow for online and offline processing,\u201d in Speech Communication; 13th ITG-Symposium, 2018, pp. 1\u20135.\\n\\n[37] X. Anguera, C. Wooters, and J. Hernando, \u201cAcoustic beamforming for speaker diarization of meetings,\u201d IEEE Transactions on Audio, Speech, and Language Processing, pp. 2011\u20132022, 2007.\\n\\n[38] B. Martinez, P.-C. Ma, S. Petridis, and M. Pantic, \u201cLipreading using temporal convolutional networks,\u201d in Proc. ICASSP 2020, 2020, pp. 6319\u20136323.\\n\\n[39] H. Chen, J. Du, Y. Hu et al., \u201cCorrelating subword articulation with lip shapes for embedding aware audio-visual speech enhancement,\u201d Neural Network, p. 171\u2013182, 2021.\\n\\n[40] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Proc. ICLR 2015, 2015.\\n\\n[41] I. Loshchilov and F. Hutter, \u201cSgdr: Stochastic gradient descent with warm restarts,\u201d in Proc. ICLR 2017, 2017.\\n\\n[42] C. Boeddecker, J. Heitkaemper, J. Schmalenstroeer et al., \u201cFront-end processing for the CHiME-5 dinner party scenario,\u201d in Proc. CHiME 2018, 2018, pp. 35\u201340.\\n\\n[43] D. S. Park, W. Chan, Y. Zhang et al., \u201cSpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,\u201d in Proc. Interspeech 2019, 2019, pp. 2613\u20132617.\\n\\n[44] E. Kharitonov, M. Rivi`ere, G. Synnaeve et al., \u201cData augmenting contrastive learning of speech representations in the time domain,\u201d in Proc. SLT 2021, 2021, pp. 215\u2013222.\"}"}
{"id": "chen22o_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Audio-Visual Speech Recognition in MISP2021 Challenge: Dataset Release and Deep Analysis\\n\\nHang Chen1, Jun Du1,*, Yusheng Dai1, Chin-Hui Lee2, Sabato Marco Siniscalchi2,4, Shinji Watanabe3, Odette Scharenborg6, Jingdong Chen7, Bao-Cai Yin5, Jia Pan5\\n\\n1 University of Science and Technology of China, China\\n2 Georgia Institute of Technology, USA\\n3 Carnegie Mellon University, USA\\n4 Kore University of Enna, Italy\\n5 iFlytek, China\\n6 Delft University of Technology, The Netherlands\\n7 Northwestern Polytechnical University, China\\n\\n1 Corresponding author\\n\\nAbstract\\n\\nIn this paper, we present the updated Audio-Visual Speech Recognition (AVSR) corpus of MISP2021 challenge, a large-scale audio-visual Chinese conversational corpus consisting of 141h audio and video data collected by far/middle/near microphones and far/middle cameras in 34 real-home TV rooms. To our best knowledge, our corpus is the first distant multi-microphone conversational Chinese audio-visual corpus and the first large vocabulary continuous Chinese lip-reading dataset in the adverse home-tv scenario. Moreover, we make a deep analysis of the corpus and conduct a comprehensive ablation study of all audio and video data in the audio-only/video-only/audio-visual systems. Error analysis shows video modality supplements acoustic information degraded by noise to reduce deletion errors and provides discriminative information in overlapping speech to reduce substitution errors. Finally, we also design a set of experiments such as frontend, data augmentation and end-to-end models for providing the direction of future work. The corpus1 and the code2 are released to promote the research not only in speech area but also for the computer vision area and cross-disciplinary research.\\n\\nIndex Terms: Audio-visual, speech recognition, speech enhancement, data augmentation\\n\\n1. Introduction\\n\\nModern automatic speech recognition (ASR) systems still suffer from performance degradations in real-world application scenarios (e.g., home and meeting). Even with advances in technology [1, 2, 3] and the publicity of large-scale corpora recorded in real environments [4, 5, 6], state-of-the-art robust ASR systems still face performance plateaus, e.g., the CHiME-6 [7] dinner party scenario reaches a word error rate of about 40%, which falls short of the deployability of the application. Many researches [8, 9, 10] have shown visual cues can help speech perception, especially in noisy environments. Inspired by these discoveries, Audio-Visual Speech Recognition (AVSR) systems have been developed.\\n\\nEarly works on AVSR relied on handcrafted audio-visual feature extraction pipelines and statistical models [11, 12, 13]. Recently, deep neural network (DNN) brought rapid progress to AVSR. [14] proposed a WLAS model. [15, 16] proposed an attention-based [17] AVSR model. [18] proposed an AV Align framework. [19] adopted Element-wise-Attention* Gated Recurrent Unit (EleAtt-GRU) in AVSR. [20] developed a Conformer [21]-based AVSR model. [22, 23] adopted Recurrent Neural Network Transducer in AVSR. The above AVSR systems are all end-to-end AVSR systems, a few works [24] used Deep Neural Network-Hidden Markov Model (DNN-HMM) hybrid AVSR systems.\\n\\nMeanwhile, various audio-visual corpora were released for training. Conventional audio-visual corpora were collected in the controlled environment, such as GRID [25], OuluVS [26], OuluVS2 [27] and TCD-TIMIT [28], etc. These corpora are limited in duration, number of speakers and vocabulary size. More recently, an effort of the research community has been put to gather data from different sources in the wild, e.g., public media websites or TV broadcasts. One of the first audio-visual in-the-wild corpora is LRW [29]. The trend of collecting larger datasets has continued in subsequent collections which consist of materials from British television programs [15, 14], TED talks [30], [31], etc. These large-scale datasets have a vast variety of speakers, sentences, languages and visual/auditory environments, but lack sample-level information. All these corpora are in English, a few corpora [32, 33, 34] have been released to AVSR in Chinese, which is the most widely used language worldwide. Nevertheless, there is still a lack of a large-scale public audio-visual speech corpus recorded in real-world application scenarios, especially for Chinese.\\n\\nMISP2021 challenge [35] presented a distant multi-microphones conversational audio-visual corpus recorded in the home TV scenario, where several people are chatting in Chinese while watching TV and interacting with a smart speaker/TV in a living room. During the challenge, the corpus was only released to the registered participants and lacked a detailed corpus analysis. In this work, we have resolved authorization and storage issues to fully release the updated AVSR corpus of MISP2021 Challenge to all researchers and make a dataset update including correcting the asynchronous sample in the training/development set and adding more data to increase the data diversity of the evaluation set. Moreover, we provide extensive experiments for various baselines and deep analysis of the corpus. Specifically, we design a set of experiments of the audio-only/video-only/audio-visual speech recognition systems with all far/middle/near audio data and far/middle video data. Error analysis is conducted to explain that video modality supplements acoustic information to reduce deletion errors in noise conditions and provides discriminative information in speech overlap. Finally, we also conduct a set of experiments such as frontend, data augmentation and end-to-end models to provide the direction of future work.\\n\\nIn the following, Section 2 introduces the updated corpus.\"}"}
{"id": "chen22o_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Illustration of an overall framework for hybrid ASR/VSR/AVSR systems with far/middle/near audio and far/middle video.\\n\\nTable 1: Overview of the updated MISP2021-AVSR corpus.\\n\\n| Dataset | Training | Dev | Eval | Total |\\n|---------|----------|-----|------|-------|\\n| Duration (h) | 106.09 | 13.32 | 21.83 | 141.24 |\\n| Overlapping Duration (h) | 44.57 | 6.47 | 10.70 | 61.74 |\\n| Room | 21 | 5 | 8 | 34 |\\n| Participant | 200 | 21 | 42 | 263 |\\n| Male | 79 | 9 | 10 | 98 |\\n| Female | 121 | 12 | 32 | 165 |\\n\\nTable 2: Scene configurations.\\n\\n| ConfigID | C1 | C2 | C3 | C4 |\\n|----------|----|----|----|----|\\n| Group | 1 | 2 | 2 | 2 |\\n| TV | Off | On | Off | On |\\n| Time | Day | Night | Day | Night |\\n| Light | on | off | on | on |\\n\\nTable 2: Scene configurations.\\n\\n2. Dataset\\n\\nAs shown in Table 1, the updated MISP2021-AVSR corpus contains 141.24 hours of audio and video data from 253 native Chinese (98 males and 165 females) speaking Mandarin without strong accents which were collected in 34 real-home TV rooms. There is no overlap in speakers and recording rooms among the data in each subset. There are multiple microphone arrays and cameras used to collect far/middle/near audio and far/middle video data, respectively. [35] shows more recording details.\\n\\nThere are some variables that can be controlled during recording, for example, the TV/light can be turned on/off, etc. Moreover, speakers could be divided into several groups to discuss different topics, which results in higher overlap ratios. As shown in Table 2, we divide all combinations of variables into 4 categories based on noise and overlap.\\n\\nIn contrast to the challenge corpus, the updated corpus has the following features:\\n\\n- During challenge, only far audio/video data in the evaluation set was released. In the updated corpus, all evaluation data has been released to support various research, such as middle video could be used for lipreading task, etc.\\n- The time synchronizing problem for training and development sets was reported. We have double-checked all training/development data and corrected asynchronous samples.\\n- 10-hour new data has been added to the evaluation set for increasing the data diversity.\\n\\n3. Framework\\n\\nAs shown in Eq.1, the recognition process can be formulated as the Bayesian decision problem, where $w = [w_0, ..., w_{n-1}]$, $X = [x_0, ..., x_{T-1}]$ and $H$ denotes words, input features, and hypotheses, respectively. The Language Model (LM) $p(w)$ is the probability of an $n$-characters sequence $w$ and can be decomposed as Eq.2. With HMM, the Acoustic Model (AM) $p(X|w)$ can be decomposed in the frame level as Eq.3 shown:\\n\\n$$y = [y_0, ..., y_{T-1}]$$ is a clustered hidden state sequence. Each HMM with a set of states represents one tri-phone class. $\\\\pi(y_0)$ is the initial state probability, $a_{y_{t-1}y_t}$ is the state transition probability from $t-1$ to $t$, $p(x_t|y_t)$ is the output probability, $p(y_t)$ is the prior probability estimated from the training set, $p(y_t|x_t)$ is the posterior probability and $p(x_t)$ is independent of the word sequence. Gaussian Mixture Model (GMM) can be used to calculate $p(x_t|y_t)$ for the GMM-HMM system while DNN can be adopted to compute $p(y_t|x_t)$ for the DNN-HMM hybrid system.\\n\\n$$w^* = \\\\arg \\\\max_{w \\\\in H} p(w | X) = \\\\arg \\\\max_{w \\\\in H} p(X | w)p(w) \\\\quad (1)$$\\n\\n$$p(w) = \\\\prod_{i=0}^{n-1} p(w_i | w_{i-1} ... w_0) \\\\quad (2)$$\\n\\n$$p(X | w) = \\\\sum_y \\\\left[ \\\\pi(y_0) \\\\prod_{t=1}^{T-1} a_{y_{t-1}y_t} \\\\prod_{t=0}^{T-1} p(x_t | y_t) \\\\right]$$\\n\\n$$= \\\\sum_y \\\\left[ \\\\pi(y_0) \\\\prod_{t=1}^{T-1} a_{y_{t-1}y_t} \\\\prod_{t=0}^{T-1} p(y_t | x_t)p(x_t) / p(y_t) \\\\right] \\\\quad (3)$$\\n\\nAs shown in Fig.1, all systems are DNN-HMM systems as described above. The main difference lies in $X$ and $p(X|w)$, which are described in Section 3.1 and Section 3.2 respectively. Section 3.3 introduces $p(w)$, the lexicon and the metric.\\n\\n3.1. Data Processing\\n\\nMel Filter Bank (FBANK) features are adopted as audio feature $X_A$. A frontend consisting of weighted prediction error (WPE) dereverberation [36] and weighted delay-and-sum beamforming (BeamformIt) [37] is applied to the far-field 6-channel speech and the middle-field 2-channel speech before the FBANK feature extraction. For the near-field monoaural speech, FBANK feature can be extracted directly from the raw audio waveform.\\n\\nLip Regions of Interest (RoIs) of the target speaker are cropped as visual feature $X_V$. Because each frame contains only the target speaker in the middle-field video, an off-the-shelf face and lip detector is used to find face and lip, then a lip-centered window is cropped. All the speakers appear in each far-field video frame, we first detect all the faces and lips, then the face that best matches the mid-field video of the target speaker is selected.\"}"}
{"id": "chen22o_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: A overall comparison of CER among the DNN-HMM hybrid ASR/VSR/AVSR systems with far/middle/near audio and far/middle video over 4 recording configurations in Table 2.\\n\\n| System | Audio       | Video       | CER (in %) |\\n|--------|-------------|-------------|------------|\\n|        | C1          | C2          | C3          | C4          |\\n| A1     | far         | far         | 55.47      | 50.51       | 25.71       | 95.69      |\\n| A2     | middle      | middle      | 74.70      | 61.79       | 23.45       | 95.62      |\\n| A3     | near        | near        | 82.19      | 75.05       | 22.59       | 95.98      |\\n| V1     | far         | far         | 84.21      | 75.83       | 20.89       | 95.69      |\\n| V2     | middle      | middle      | 50.99      | 45.57       | 22.64       | 95.69      |\\n| A V1   | near        | near        | 66.21      | 58.59       | 20.44       | 95.69      |\\n| A V2   | far         | middle      | 71.55      | 63.16       | 19.97       | 95.69      |\\n| A V3   | near        | middle      | 73.73      | 64.61       | 18.58       | 95.69      |\\n\\nTable 4: A comparison of $S$, $D$ and $I$ among A1 and AV2 systems.\\n\\n| System | S (in %) | D (in %) | I (in %) |\\n|--------|----------|----------|----------|\\n|        | Off      | On       | Off      | On       |\\n| A1     | 48.16    | 48.87    | 17.71    | 30.96    |\\n| A V2   | 35.05    | 36.88    | 17.69    | 23.44    |\\n\\n3.2. DNN-HMM Hybrid Acoustic Model\\n\\nThe acoustic model consists of an embedding extractor and a followed sequence modeling model. The embedding extractor $f \\\\in \\\\{ f^A, f^V, f^{AV} \\\\}$ takes the audio feature $X^A$ or/and the visual feature $X^V$ as input to produce an embedding $E \\\\in \\\\{ E^A, E^V, E^{AV} \\\\}$. The sequence module is next employed to model the temporal dynamics. Finally, the posterior probability $p(y|X)$ is predicted by the ensuing full connection and SoftMax layers. Multi-Scale Temporal Convolution Network (MS-TCN) \\\\[38\\\\] is adopted as the sequential modeling module in our systems. It can be formulated as:\\n\\n$$p(y|X) = \\\\text{SoftMax}(\\\\text{FC}(\\\\text{MS-TCN}(E))) \\\\quad (4)$$\\n\\n$$E^V = f^V(X^V) \\\\quad (5)$$\\n\\n$$E^A = f^A(X^A) \\\\quad (6)$$\\n\\n$$E^{AV} = \\\\text{Concat}(f^{AV}(X^A), f^{AV}(X^V)) \\\\quad (7)$$\\n\\nwhere the visual embedding extractor $f^V$ is the same as \\\\[39\\\\] which consists of a spatiotemporal convolution followed by an 18-layer ResNet. A spatiotemporal convolution consists of a convolution layer with 64 3D-kernels, a batch normalization, a ReLU activation and a spatiotemporal max-pooling layer. The audio embedding extractor $f^A$ has the similar structure as $f^V$. The 3D-kernels in spatiotemporal convolution and the 2D-kernels in ResNet-18 are replaced by 1D-kernels meanwhile the 3D-MaxPooling layer is dropped. The audio-visual embedding extractor $f^{AV}$ consists of a visual module $f^{AV}$ and an audio module $f^{AV}$ which have the same structure as $f^V$ and $f^A$, respectively. The concatenation is over the channel dimension and the frame mismatch between the audio and video is solved by repeating a video frame for several audio frames.\\n\\nThe Cross Entropy criterion $L_{CE}$ between $p(y|X)$ and the true distribution of state $p_{GT}$ is calculated as:\\n\\n$$L_{CE} = -\\\\frac{1}{N_B} \\\\sum_{t=0}^{N_B-1} p_{GT}(t) \\\\log p(y(t)|X) \\\\quad (8)$$\\n\\nwhere $N_B$ is the minibatch size. $L_{CE}$ is minimized by using Adam optimizer \\\\[40\\\\] for 100 epochs with an initial learning rate of 0.0003 and cosine scheduler \\\\[41\\\\]. The best model is selected by the highest classification accuracy on the development set. $p_{GT}$ is generated with the GMM-HMM system. In ASR experiments, three GMM-HMM systems with far/middle/near audio are built. Far-field and near-field GMM-HMM systems are adopted for far-field and middle-field VSR, respectively. In A VSR experiments, the GMM-HMM system with the corresponding audio is adopted.\\n\\n3.3. Decoding and Scoring\\n\\nWe use the DaCiDian dictionary as the pronunciation dictionary. A 3-gram language model is trained by the maximum entropy modeling method implemented in the SRILM toolkit. We adopt Character Error Rate (CER) as the metric. It is represented with Eq. 9:\\n\\n$$\\\\text{CER} = S + D + I \\\\quad (9)$$\\n\\nwhere $S$, $D$, $I$ are substitution, deletion, insertion error rates, respectively. The lower the CER value (with 0 being a perfect score), the better the recognition performance. For speech overlap segments, we calculate all the substitution, deletion and insertion errors based on the recognition results and the ground truth for each speaker based on the oracle speaker diarization.\\n\\n4. Experiments\\n\\nThree ASR systems with far/middle/near audio and two VSR systems with far/middle video have been built as baselines for various research. For A VSR system, we considered three combinations: the most challenging combination of far audio and far video, the most ideal combination of near audio and middle video and the compromise of far audio and middle video. Far video to middle video could be achieved with the help of a zoom lens in real-world application. Table 3 shows a comparison of CER among all systems over 4 recording configurations in Table 2. Three ASR results show the performance degradation of ASR results from far-field channel distortion, ambient noises and speech overlap. Two VSR results show the performance of VSR is robust to noise and speech overlap but still very limited. The challenges mainly come from two aspects. Unfavorable lighting and variable head posture make it difficult for the far camera to capture lip changes, which is improved in the middle video. Chinese is a tone language and the tone is the use of pitch in language to distinguish lexical or grammatical meaning, which leads more words to look the same on the lip when pronounce.\\n\\n4.1. Analysis of errors in noise condition\\n\\nBased on the presence of noise, we divide the whole evaluation set into two categories and list the corresponding $S$/D/I in Table 4. Table 4 reflects two trends. With the presence of TV noise, the $D$ of the A1 system rose sharply from 17.71 to 30.96, while neither $S$ nor I rose much. A V2 system shows improvements over A1 across all evaluation metrics, specifically, there is a stable gain in $S$ whether noise's presence or absence and larger improvement of $D$ in the noise condition. Due to the segmentation according to the oracle speaker diarization information during preprocessing, $I$ is very rare. Accordingly, we propose that video modality supplement acoustic information degraded by noise to reduce deletion errors.\\n\\nIn Fig.2, we show a noisy example selected from the evaluation set randomly and the comparison between the outputs of A1, V2, A V2 systems. In the second half of the far spectrum, the noise component almost completely drowns out the target speech component, which causes four deletion errors in the results of the A1 system. As mentioned above, the challenges...\"}"}
{"id": "chen22o_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: A noisy sample and comparison between the outputs of A1, V2, AV2 systems. GT means Ground Truth.\\n\\nFigure 3: A comparison of the substitution error among A1 and AV2 systems on different categories with the same number of speakers participating in the conversation.\\n\\nFrom the environment and the tone language make the performance of V2 very limited. But the AV2 system utilizes the acoustic information in the change of lip shape to predict the last four characters correctly.\\n\\n4.2. Analysis of errors in overlap condition\\n\\nWe think substitution errors are more related to speech overlap. So based on the number of speakers participating in the conversation, we divide the whole evaluation set into six categories and plot the corresponding $S$ in Fig. 3. More speakers participating in the conversation means more overlapping speech. A sharp rise of $S$ follows to the number of speakers participating in the conversation is shown in Fig. 3. AV2 system shows improvements over A1 across all six categories and larger improvement is observed in the category with more speakers.\\n\\nIn Fig. 4, we show an overlapping example selected from the evaluation set randomly and the comparison between the outputs of A1, V2, AV2 systems. The target speech overlaps with the interfering speech almost completely, consequently, the output of A1 contained four substitution errors and a deletion error. V2 excels on the last character due to unambiguous pronunciation actions. AV2 corrected these five errors by utilizing the discriminative acoustic information in the change of lip shape.\\n\\n4.3. Analysis on advanced techniques\\n\\nLastly, we explored some advanced techniques for the most challenging combinations of far-field audio and far-field video. Guided source separation (GSS) [42] is adopted as frontend and SpecAug [43] is applied during the acoustic model training, denoted as AV4 in 5. Further, we build three end-to-end AVSR systems which consist of a hybrid CTC/Attention based acoustic model [20] and a transformer-based language model, denoted as E2E1, E2E2 and E2E3 respectively. E2E1 and E2E2 are developed on a three-layer MS-TCN and a six-layer conformer based encoder backbone respectively using FBANK as audio input. E2E3 has a more flexible conformer-based encoder backbone with skip-connection and multiple fusion. The audio input is raw waveform and WavAug [44] is adopted during training. More details can be found in our source code.\\n\\nAs shown in Table 5, GSS and SpecAug yield significant performance improvements in the DNN-HMM hybrid AVSR system. The end-to-end AVSR system can achieve a comparable performance by simply changing. Besides, it is worth mentioning that many native end-to-end AVSR systems have been proposed on AVSR task in the MISP2021 challenge [35].\\n\\nTable 5: A comparison of CER between DNN-HMM hybrid AVSR systems and end-to-end AVSR systems with different frontends, data augmentation schemes and acoustic models. Embedding extractors in AMs are omitted. DA: data augmentation.\\n\\n| System | Frontend | DA   | AM         | CER (in %) |\\n|--------|----------|------|------------|------------|\\n| AV1    | Beamform |      | MS-TCN     | 64.37      |\\n| AV4    | GSS      | SpecAug | MS-TCN     | 48.78      |\\n| E2E1   | GSS      | SpecAug | MS-TCN     | 76.46      |\\n| E2E2   | GSS      | SpecAug | Conformer  | 60.15      |\\n| E2E3   | GSS      | WavAug * | Conformer  | 51.04      |\\n\\nAcoustic model [20] and a transformer-based language model, denoted as E2E1, E2E2 and E2E3 respectively. E2E1 and E2E2 are developed on a three-layer MS-TCN and a six-layer conformer based encoder backbone respectively using FBANK as audio input. E2E3 has a more flexible conformer-based encoder backbone with skip-connection and multiple fusion. The audio input is raw waveform and WavAug [44] is adopted during training. More details can be found in our source code.\\n\\nAs shown in Table 5, GSS and SpecAug yield significant performance improvements in the DNN-HMM hybrid AVSR system. The end-to-end AVSR system can achieve a comparable performance by simply changing. Besides, it is worth mentioning that many native end-to-end AVSR systems have been proposed on AVSR task in the MISP2021 challenge [35].\\n\\n5. Conclusions\\n\\nThe updated AVSR corpus of MISP2021 Challenge is fully released. The corpus is the first distant multi-microphone conversational Chinese audio-visual corpus and the first large vocabulary continuous lip-reading Chinese corpus in the adverse home-tv scenario which promotes the research in the speech area, the computer vision area and cross-disciplinary area. Extensive experiments and error analysis show video supplement acoustic information to reduce deletion errors in noise condition and provide discriminative information in overlapping speech to reduce substitution errors. Finally, a set of experiments with advanced techniques provide the direction of the future work.\\n\\n6. Acknowledgements\\n\\nThis work was supported by the National Natural Science Foundation of China under Grant No. 62171427 and the Strategic Priority Research Program of Chinese Academy of Sciences under Grant No. XDC08050200.\"}"}
