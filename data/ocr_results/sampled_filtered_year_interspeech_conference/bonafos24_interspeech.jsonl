{"id": "bonafos24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] S. M. ter Haar, A. A. Fernandez, M. Gratier, M. Kn\u00f6rschild, C. Levelt, R. K. Moore, M. Vellema, X. Wang, and D. K. Oller, \u201cCross-species parallels in babbling: Animals and algorithms,\u201d Philosophical Transactions of the Royal Society B: Biological Sciences, vol. 376, no. 1836, p. 20200239, Oct. 2021.\\n\\n[2] P. K. Kuhl, \u201cEarly language acquisition: Cracking the speech code,\u201d Nature Reviews Neuroscience, vol. 5, no. 11, pp. 831\u2013843, Nov. 2004.\\n\\n[3] D. K. Oller, R. E. Eilers, A. R. Neal, and H. K. Schwartz, \u201cPrecursors to speech in infancy: The prediction of speech and language disorders,\u201d Journal of Communication Disorders, vol. 32, no. 4, pp. 223\u2013245, Jul. 1999.\\n\\n[4] K. D. Bartl-Pokorny, F. B. Pokorny, D. Garrido, B. W. Schuller, D. Zhang, and P. B. Marschik, \u201cVocalisation Repertoire at the End of the First Year of Life: An Exploratory Comparison of Rett Syndrome and Typical Development,\u201d Journal of Developmental and Physical Disabilities, Mar. 2022.\\n\\n[5] D. K. Oller, P. Niyogi, S. Gray, J. A. Richards, J. Gilkerson, D. Xu, U. Yapanel, and S. F. Warren, \u201cAutomated vocal analysis of naturalistic recordings from children with autism, language delay, and typical development,\u201d Proceedings of the National Academy of Sciences of the United States of America, vol. 107, no. 30, pp. 13 354\u201313 359, Jul. 2010.\\n\\n[6] M. Milling, F. B. Pokorny, K. D. Bartl-Pokorny, and B. W. Schuller, \u201cIs Speech the New Blood? Recent Progress in AI-Based Disease Detection From Audio in a Nutshell,\u201d Frontiers in Digital Health, vol. 4, p. 886615, May 2022.\\n\\n[7] A. Salch, A. Regalski, H. Abdallah, R. Suryadevara, M. J. Catanziaro, and V. A. Diwadkar, \u201cFrom mathematics to medicine: A practical primer on topological data analysis (TDA) and the development of related analytic tools for the functional discovery of latent structure in fMRI data,\u201d PLOS ONE, vol. 16, no. 8, p. e0255859, Aug. 2021.\\n\\n[8] Y. Cao, S. Zhang, F. Yan, W. Li, F. Sun, and H. Sun, \u201cUnsupervised Environmental Sound Classification Based On Topological Persistence,\u201d in 2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP), Dec. 2019, pp. 1\u20135.\\n\\n[9] F. Hensel, M. Moor, and B. Rieck, \u201cA Survey of Topological Machine Learning Methods,\u201d Frontiers in Artificial Intelligence, vol. 4, p. 681108, May 2021.\\n\\n[10] D. Cohen-Steiner, H. Edelsbrunner, and J. Harer, \u201cStability of Persistence Diagrams,\u201d Discrete & Computational Geometry, vol. 37, no. 1, pp. 103\u2013120, Jan. 2007.\\n\\n[11] G. Bonafos, P. Pudlo, J.-M. Freyermuth, T. Legou, J. Fagot, S. Tron\u00e7on, and A. Rey, \u201cDetecting human and non-human vocal productions in large scale audio recordings,\u201d Feb. 2023.\\n\\n[12] F. Chazal and B. Michel, \u201cAn Introduction to Topological Data Analysis: Fundamental and Practical Aspects for Data Scientists,\u201d Frontiers in Artificial Intelligence, vol. 4, 2021.\\n\\n[13] G. Carlsson, \u201cTopology and data,\u201d Bulletin of the American Mathematical Society, vol. 46, no. 2, pp. 255\u2013308, 2009.\\n\\n[14] A. Zomorodian and G. Carlsson, \u201cComputing Persistent Homology,\u201d Discrete & Computational Geometry, vol. 33, no. 2, pp. 249\u2013274, Feb. 2005.\\n\\n[15] F. Takens, \u201cDetecting strange attractors in turbulence,\u201d in Dynamical Systems and Turbulence, Warwick 1980, D. Rand and L.-S. Young, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 1981, vol. 898, pp. 366\u2013381.\\n\\n[16] L. Cao, \u201cPractical method for determining the minimum embedding dimension of a scalar time series,\u201d Physica D: Nonlinear Phenomena, vol. 110, no. 1, pp. 43\u201350, Dec. 1997.\\n\\n[17] L. McInnes, J. Healy, and J. Melville, \u201cUMAP: Uniform Manifold Approximation and Projection for Dimension Reduction,\u201d Sep. 2020.\\n\\n[18] L. Wasserman, \u201cTopological Data Analysis,\u201d Annual Review of Statistics and Its Application, vol. 5, no. 1, pp. 501\u2013532, 2018.\\n\\n[19] N. Atienza, R. Gonzalez-Diaz, and M. Rucco, \u201cPersistent entropy for separating topological features from noise in vietoris-rips complexes,\u201d Journal of Intelligent Information Systems, vol. 52, no. 3, pp. 637\u2013655, Jun. 2019.\\n\\n[20] D. Cohen-Steiner, H. Edelsbrunner, J. Harer, and Y. Mileyko, \u201cLipschitz Functions Have L_p-Stable Persistence,\u201d Foundations of Computational Mathematics, vol. 10, no. 2, pp. 127\u2013139, Apr. 2010.\\n\\n[21] H. Edelsbrunner and J. Harer, Computational Topology: An Introduction. AMS Press, 2009.\\n\\n[22] T. Fireaizen, S. Ron, and O. Bobrowski, \u201cAlarm Sound Detection Using Topological Signal Processing,\u201d in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Singapore, Singapore: IEEE, May 2022, pp. 211\u2013215.\\n\\n[23] C. M. Pereira and R. F. de Mello, \u201cPersistent homology for time series and spatial data clustering,\u201d Expert Systems with Applications, vol. 42, no. 15-16, pp. 6026\u20136038, Sep. 2015.\\n\\n[24] J. Sueur, Sound Analysis and Synthesis with R, 1st ed. New York, NY: Springer Berlin Heidelberg, 2018.\\n\\n[25] Y. W. Teh, \u201cDirichlet processes,\u201d in Encyclopedia of Machine Learning. Springer, 2010.\\n\\n[26] R. M. Dorazio, \u201cOn selecting a prior for the precision parameter of Dirichlet process mixture models,\u201d Journal of Statistical Planning and Inference, vol. 139, no. 9, pp. 3384\u20133390, Sep. 2009.\\n\\n[27] M. Cychosz, A. Seidl, E. Bergelson, M. Casillas, G. Baudet, A. S. Warlaumont, C. Scaff, L. Yankowitz, and A. Cristia, \u201cBabbleCor: A Crosslinguistic Corpus of Babble Development in Five Languages,\u201d Oct. 2019.\\n\\n[28] R. M. Neal, \u201cMarkov Chain Sampling Methods for Dirichlet Process Mixture Models,\u201d Journal of Computational and Graphical Statistics, vol. 9, no. 2, pp. 249\u2013265, 2000.\\n\\n[29] S. Wade and Z. Ghahramani, \u201cBayesian Cluster Analysis: Point Estimation and Credible Balls (with Discussion),\u201d Bayesian Analysis, vol. 13, no. 2, pp. 559\u2013626, Jun. 2018.\\n\\n[30] D. K. Oller, E. H. Buder, H. L. Ramsdell, A. S. Warlaumont, L. Chorna, and R. Bakeman, \u201cFunctional flexibility of infant vocalization and the emergence of language,\u201d Proceedings of the National Academy of Sciences, vol. 110, no. 16, pp. 6318\u20136323, Apr. 2013.\\n\\n[31] M. Cychosz, A. Cristia, E. Bergelson, M. Casillas, G. Baudet, A. S. Warlaumont, C. Scaff, L. Yankowitz, and A. Seidl, \u201cVocal development in a large-scale crosslinguistic corpus,\u201d Developmental Science, vol. 24, no. 5, Sep. 2021.\\n\\n[32] A. Koutseff, D. Reby, O. Martin, F. Levrero, H. Patural, and N. Mathevon, \u201cThe acoustic space of pain: cries as indicators of distress recovering dynamics in pre-verbal infants,\u201d Bioacoustics, vol. 27, no. 4, pp. 313\u2013325, Oct. 2018. [Online]. Available: https://www.tandfonline.com/doi/full/10.1080/09524622.2017.1344931\\n\\n[33] M. Lockhart-Bouron, A. Anikin, K. Pisanski, S. Corvin, C. Cornec, L. Papet, F. Levr\u00e9ro, C. Fauchon, H. Patural, D. Reby, and N. Mathevon, \u201cInfant cries convey both stable and dynamic information about age and identity,\u201d Communications Psychology, vol. 1, no. 1, pp. 1\u201315, Oct. 2023.\\n\\n[34] F. A. Quintana, P. M\u00fcller, A. Jara, and S. N. MacEachern, \u201cThe Dependent Dirichlet Process and Related Models,\u201d Statistical Science, vol. 37, no. 1, pp. 24\u201341, Feb. 2022.\\n\\n[35] M. Moor, M. Horn, B. Rieck, and K. Borgwardt, \u201cTopological Autoencoders,\u201d in arXiv:1906.00722 [Cs, Math, Stat], vol. PMLR 119, 2020, pp. 7045\u20137054.\\n\\n[36] I. Trofimov, D. Cherniavskii, E. Tulchinskii, N. Balabin, E. Burinaev, and S. Barannikov, \u201cLearning topology-preserving data representations,\u201d in The Eleventh International Conference on Learning Representations, Feb. 2023.\\n\\n[37] Y. Jhang, \u201cEmergence of Functional Flexibility in Infant Vocalizations of the First 3 Months,\u201d Frontiers in Psychology, vol. 8, p. 11, 2017.\"}"}
{"id": "bonafos24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dirichlet process mixture model based on topologically augmented signal representation for clustering infant vocalizations\\n\\nGuillem Bonafos\u00b9, Clara Bourot\u00b2, Pierre Pudlo\u00b2, Jean-Marc Freyermuth\u00b2, Laurence Reboul\u00b2, Samuel Tronc\u00b3, Arnaud Rey\u00b2\\n\\n\u00b9Universit\u00e9 de Saint-\u00c9tienne, \u00b2Universit\u00e9 d'Aix-Marseille, \u00b3R\u00e9surgences R&D\\n\\nAbstract\\n\\nBased on audio recordings made once a month during the first 12 months of a child's life, we propose a new method for clustering this set of vocalizations. We use a topologically augmented representation of the vocalizations, employing two persistence diagrams for each vocalization: one computed on the surface of its spectrogram and one on the Takens' embeddings of the vocalization. A synthetic persistent variable is derived for each diagram and added to the MFCCs (Mel-frequency cepstral coefficients). Using this representation, we fit a non-parametric Bayesian mixture model with a Dirichlet process prior to model the number of components. This procedure leads to a novel data-driven categorization of vocal productions. Our findings reveal the presence of 8 clusters of vocalizations, allowing us to compare their temporal distribution and acoustic profiles in the first 12 months of life.\\n\\nIndex Terms: clustering, Bayesian non-parametric, Dirichlet process, mixture model, topologically-augmented machine learning, TDA, babbling, language development, vocalizations\\n\\n1. Introduction\\n\\nDuring the first year of life, the vocal productions of human infants undergo a developmental trajectory, actively exploring their acoustic environment through behaviors such as crying, cooing, and babbling. Infants adapt the evolution and diversification of their vocalizations to a target language [1] and typically produce their first word by the end of the first year [2]. Monitoring these pre-language vocalizations is of great importance, not only for gaining a deeper comprehension of the distinct phases of language development but also for predictive insights into various disorders [3, 4]. The use of advanced storage and recording tools allows for the creation of extensive new databases. When combined with innovative statistical analysis techniques, these tools contribute to a deeper exploration of the early stages of language development [5, 6].\\n\\nIn this study, we worked with a database that includes vocalizations automatically extracted from long-form audio recordings of a child at home. Recordings were done over her first year, spanning from 0 to 12 months, with three days of recordings per month. The outcome is a longitudinal vocalization database, capturing vocalizations in a real-life setting and diverse contexts.\\n\\nOur objective is to propose a novel method to categorize vocal productions, without predefining the number of categories, but rather estimating them from the data. To achieve this, we employ a non-parametric Bayesian model, specifically a Dirichlet process mixture model. The clustering process is grounded in a topologically augmented representation of the signal, allowing the incorporation of additional information pertaining to the topology of the vocalizations.\\n\\nFor clustering, we need to represent vocalizations in a low-dimensional space. Topological Data Analysis (TDA), which has demonstrated its efficacy across various domains [7, 8, 9], is a promising candidate for enhancing the current representation of infant vocalizations. Its stability-theoretic properties make it particularly valuable for the examination of natural signals [10]. The integration of topological information can provide valuable additional information for a more nuanced description of these vocal productions.\\n\\nIn the subsequent sections, we provide an overview of the database in Section 2. The computation of the augmented topological representation and the clustering model is detailed in Section 3. Section 4 outlines the clustering results, followed by a comprehensive discussion in Section 5. Finally, we draw conclusions in Section 6.\\n\\n2. Data\\n\\nThe dataset comprises vocalizations of a child, spanning from birth to the child's first birthday. Each vocalization is represented by a stereo-channel audio signal sampled at 44.1 kHz in PCM format. Extracted from longer audio files, the signals are converted to mono by averaging both channels and then rescaled the pulse modulation signal to a range of $-1$ to $1$.\\n\\nThese vocalizations originate from long-form audio recordings made by the parents of a female French child at home, at regular intervals, over a one-year period. Ethical approval was obtained, along with a declaration of conformity for experimental research involving humans, allowing for the recording of human baby vocalizations. Parents, equipped with a portable microphone located near the child, recorded audio samples three days a month, capturing various moments throughout the day and night. Following the methodology outlined in [11], we automatically extracted all the segments labeled as baby vocalizations from these continuous recordings, resulting in a dataset of 1924 vocalizations. Unfortunately, due to legal constraints, we are unable to publicly share the data. Vocalizations lasting more than 10 s were excluded, yielding a final set of 1851 vocalizations with an average duration of 2.51 s.\\n\\nTable 1 provides the distribution of vocalizations detected over the first year. It's noteworthy that vocalizations are not available for every month; specifically, we lack data for the first, fourth, fifth, and tenth months. The absence of vocalizations for these months stems from the inability to conduct recordings during these periods or the absence of detected vocalizations in the recordings.\"}"}
{"id": "bonafos24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Number of vocalizations per month in the long-form audio recordings, as well as the mean and the standard deviation.\\n\\n| Month | Count | Mean duration | Standard deviation |\\n|-------|-------|---------------|--------------------|\\n| 05    | 11    | 75 s         | 2 s                |\\n| 06    | 12    | 81 s         | 1 s                |\\n| 07    | 15     | 74 s      | 2 s               |\\n| 08    | 12    | 71 s        | 2 s               |\\n| 09    | 13    | 70 s        | 2 s               |\\n| 10    | 21     | 57 s     | 09 s             |\\n| 11    | 212   | 2 s         | 1 s               |\\n| 12    | 98    | 2 s         | 1 s               |\\n\\nThe resulting topologically augmented representation of the spectrogram is used to compute a synthetic persistent variable using principal component analysis (PCA). The first principal component of the PCA is retained, explaining 65% of the variance of the set of variables from the persistence diagram of Takens' embeddings. We apply an Alpha filtration to compute the persistent homology of the embeddings, ensuring uniformity across all embeddings. This yields the vocalization representation as a point cloud.\\n\\nWe estimate the embedding dimension $D$ has two coordinates, the value of persistence homology of the object, where a point in a diagram representation as a point cloud. We choose the concentration parameter of the Gaussian window of $6$ ms.\\n\\nWe utilize the collapsed Gibbs sampler of [28], based on the base measure $\\\\nu$. We use non-informative priors for the degree of freedom of the inverse Wishart, $\\\\nu$, and the concentration parameter $\\\\alpha$ of its birth and the value $\\\\tau$ of its death. Persistent homology then yields a multiscale topological description of the object.\\n\\nWe apply a sublevel set filtration to compute the persistent homology of the spectrogram and its Takens' embeddings. Depending on the object, we give here the technical details for reproducibility, we refer the reader to [12] for more details. TDA assumes that data has a consistent number of MFCC for all vocalizations.\\n\\nFor each audio recording, we first compute the spectrogram $S(t, \\\\omega)$ of the vector collecting the lifetime of the points of the diagram $[20]$, persistent Betti number $[21]$, and descriptors computing a set of variables: persistent entropy $[19]$, for invariance to a nested family of Alpha complex $\\\\{x \\\\in \\\\mathbb{R}^D : \\\\text{dist}(x, \\\\mathbb{R}^D) \\\\geq \\\\epsilon\\\\}$, for increasing value of $\\\\epsilon$, where $\\\\text{dist}(x, \\\\mathbb{R}^D)$ is the intersection of each Euclidean ball with its corresponding V oronoi cell, for increasing value of $r$.\\n\\nSecond, we compute the Takens' embeddings, which embedding a time series into a $D$-dimensional Euclidean space using a Gaussian window of $6$ ms. We estimate the time delay parameter $\\\\tau$ using two different objects: the surface of its spectrogram, for logical spaces. We compute the homology at all scales, i.e. $\\\\text{dim}_p(S)$ for all embeddings using UMAP $[17]$, ensuring uniformity across all embeddings. We resume in a persistence diagram the description of the object $[18]$.\\n\\nPersisence diagrams cannot be used directly for statistical analysis. We therefore extract information from the diagrams by modeling them. We aim to determine the number of clusters in the dataset, and another summarizing the persistence diagram computed on the surface of the spectrogram. We compute Mel Frequency Cepstral Coefficients (MFCC), classical frequency descriptors of human speech analysis $[24]$. We compute twelve coefficients, with a window length of $25$ ms.\\n\\nIn addition to the topological features, we compute Mel Frequency Cepstral Coefficients (MFCC), classical frequency descriptors of human speech analysis $[24]$. We compute twelve coefficients, with a window length of $25$ ms.\\n\\nFor both objects, we have an increasing sequence of topological spaces. We recover this shape through a filtration, a nested sequence of simplicial complexes $[14]$. We then derive from the shape $[13]$. We compute the homology at all scales, i.e. $\\\\text{dim}_p(S)$ for all embeddings using UMAP $[17]$, ensuring uniformity across all embeddings. We resume in a persistence diagram the description of the object $[18]$.\\n\\nWe apply an Alpha filtration to compute the persistent homology of the embeddings, ensuring uniformity across all embeddings. This yields the vocalization representation as a point cloud.\\n\\nWe estimate the embedding dimension $D$ has two coordinates, the value of persistence homology of the object, where a point in a diagram representation as a point cloud. We choose the concentration parameter of the Gaussian window of $6$ ms.\\n\\nWe utilize the collapsed Gibbs sampler of [28], based on the base measure $\\\\nu$. We use non-informative priors for the degree of freedom of the inverse Wishart, $\\\\nu$, and the concentration parameter $\\\\alpha$ of its birth and the value $\\\\tau$ of its death. Persistent homology then yields a multiscale topological description of the object.\\n\\nWe apply a sublevel set filtration to compute the persistent homology of the spectrogram and its Takens' embeddings. Depending on the object, we give here the technical details for reproducibility, we refer the reader to [12] for more details. TDA assumes that data has a consistent number of MFCC for all vocalizations.\\n\\nFor each audio recording, we first compute the spectrogram $S(t, \\\\omega)$ of the vector collecting the lifetime of the points of the diagram $[20]$, persistent Betti number $[21]$, and descriptors computing a set of variables: persistent entropy $[19]$, for invariance to a nested family of Alpha complex $\\\\{x \\\\in \\\\mathbb{R}^D : \\\\text{dist}(x, \\\\mathbb{R}^D) \\\\geq \\\\epsilon\\\\}$, for increasing value of $\\\\epsilon$, where $\\\\text{dist}(x, \\\\mathbb{R}^D)$ is the intersection of each Euclidean ball with its corresponding V oronoi cell, for increasing value of $r$.\\n\\nSecond, we compute the Takens' embeddings, which embedding a time series into a $D$-dimensional Euclidean space using a Gaussian window of $6$ ms. We estimate the time delay parameter $\\\\tau$ using two different objects: the surface of its spectrogram, for logical spaces. We compute the homology at all scales, i.e. $\\\\text{dim}_p(S)$ for all embeddings using UMAP $[17]$, ensuring uniformity across all embeddings. We resume in a persistence diagram the description of the object $[18]$.\\n\\nPersisence diagrams cannot be used directly for statistical analysis. We therefore extract information from the diagrams by modeling them. We aim to determine the number of clusters in the dataset, and another summarizing the persistence diagram computed on the surface of the spectrogram. We compute Mel Frequency Cepstral Coefficients (MFCC), classical frequency descriptors of human speech analysis $[24]$. We compute twelve coefficients, with a window length of $25$ ms.\\n\\nIn addition to the topological features, we compute Mel Frequency Cepstral Coefficients (MFCC), classical frequency descriptors of human speech analysis $[24]$. We compute twelve coefficients, with a window length of $25$ ms.\\n\\nFor both objects, we have an increasing sequence of topological spaces. We recover this shape through a filtration, a nested sequence of simplicial complexes $[14]$. We then derive from the shape $[13]$. We compute the homology at all scales, i.e. $\\\\text{dim}_p(S)$ for all embeddings using UMAP $[17]$, ensuring uniformity across all embeddings. We resume in a persistence diagram the description of the object $[18]$.\\n\\nWe apply an Alpha filtration to compute the persistent homology of the embeddings, ensuring uniformity across all embeddings. This yields the vocalization representation as a point cloud.\\n\\nWe estimate the embedding dimension $D$ has two coordinates, the value of persistence homology of the object, where a point in a diagram representation as a point cloud. We choose the concentration parameter of the Gaussian window of $6$ ms.\\n\\nWe utilize the collapsed Gibbs sampler of [28], based on the base measure $\\\\nu$. We use non-informative priors for the degree of freedom of the inverse Wishart, $\\\\nu$, and the concentration parameter $\\\\alpha$ of its birth and the value $\\\\tau$ of its death. Persistent homology then yields a multiscale topological description of the object.\\n\\nWe apply a sublevel set filtration to compute the persistent homology of the spectrogram and its Takens' embeddings. Depending on the object, we give here the technical details for reproducibility, we refer the reader to [12] for more details. TDA assumes that data has a consistent number of MFCC for all vocalizations.\\n\\nFor each audio recording, we first compute the spectrogram $S(t, \\\\omega)$ of the vector collecting the lifetime of the points of the diagram $[20]$, persistent Betti number $[21]$, and descriptors computing a set of variables: persistent entropy $[19]$, for invariance to a nested family of Alpha complex $\\\\{x \\\\in \\\\mathbb{R}^D : \\\\text{dist}(x, \\\\mathbb{R}^D) \\\\geq \\\\epsilon\\\\}$, for increasing value of $\\\\epsilon$, where $\\\\text{dist}(x, \\\\mathbb{R}^D)$ is the intersection of each Euclidean ball with its corresponding V oronoi cell, for increasing value of $r$.\\n\\nSecond, we compute the Takens' embeddings, which embedding a time series into a $D$-dimensional Euclidean space using a Gaussian window of $6$ ms. We estimate the time delay parameter $\\\\tau$ using two different objects: the surface of its spectrogram, for logical spaces. We compute the homology at all scales, i.e. $\\\\text{dim}_p(S)$ for all embeddings using UMAP $[17]$, ensuring uniformity across all embeddings. We resume in a persistence diagram the description of the object $[18]$.\\n\\nPersisence diagrams cannot be used directly for statistical analysis. We therefore extract information from the diagrams by modeling them. We aim to determine the number of clusters in the dataset, and another summarizing the persistence diagram computed on the surface of the spectrogram. We compute Mel Frequency Cepstral Coefficients (MFCC), classical frequency descriptors of human speech analysis $[24]$. We compute twelve coefficients, with a window length of $25$ ms.\\n\\nIn addition to the topological features, we compute Mel Frequency Cepstral Coefficients (MFCC), classical frequency descriptors of human speech analysis $[24]$. We compute twelve coefficients, with a window length of $25$ ms.\\n\\nFor both objects, we have an increasing sequence of topological spaces. We recover this shape through a filtration, a nested sequence of simplicial complexes $[14]$. We then derive from the shape $[13]$. We compute the homology at all scales, i.e. $\\\\text{dim}_p(S)$ for all embeddings using UMAP $[17]$, ensuring uniformity across all embeddings. We resume in a persistence diagram the description of the object $[18]$.\\n\\nWe apply an Alpha filtration to compute the persistent homology of the embeddings, ensuring uniformity across all embeddings. This yields the vocalization representation as a point cloud.\\n\\nWe estimate the embedding dimension $D$ has two coordinates, the value of persistence homology of the object, where a point in a diagram representation as a point cloud. We choose the concentration parameter of the Gaussian window of $6$ ms.\\n\\nWe utilize the collapsed Gibbs sampler of [28], based on the base measure $\\\\nu$. We use non-informative priors for the degree of freedom of the inverse Wishart, $\\\\nu$, and the concentration parameter $\\\\alpha$ of its birth and the value $\\\\tau$ of its death. Persistent homology then yields a multiscale topological description of the object.\\n\\nWe apply a sublevel set filtration to compute the persistent homology of the spectrogram and its Takens' embeddings. Depending on the object, we give here the technical details for reproducibility, we refer the reader to [12] for more details. TDA assumes that data has a consistent number of MFCC for all vocalizations.\\n\\nFor each audio recording, we first compute the spectrogram $S(t, \\\\omega)$ of the vector collecting the lifetime of the points of the diagram $[20]$, persistent Betti number $[21]$, and descriptors computing a set of variables: persistent entropy $[19]$, for invariance to a nested family of Alpha complex $\\\\{x \\\\in \\\\mathbb{R}^D : \\\\text{dist}(x, \\\\mathbb{R}^D) \\\\geq \\\\epsilon\\\\}$, for increasing value of $\\\\epsilon$, where $\\\\text{dist}(x, \\\\mathbb{R}^D)$ is the intersection of each Euclidean ball with its corresponding V oronoi cell, for increasing value of $r$.\\n\\nSecond, we compute the Takens' embeddings, which embedding a time series into a $D$-dimensional Euclidean space using a Gaussian window of $6$ ms. We estimate the time delay parameter $\\\\tau$ using two different objects: the surface of its spectrogram, for logical spaces. We compute the homology at all scales, i.e. $\\\\text{dim}_p(S)$ for all embeddings using UMAP $[17]$, ensuring uniformity across all embeddings. We resume in a persistence diagram the description of the object $[18]$.\\n\\nPersisence diagrams cannot be used directly for statistical analysis. We therefore extract information from the diagrams by modeling them. We aim to determine the number of clusters in the dataset, and another summarizing the persistence diagram computed on the surface of the spectrogram. We compute Mel Frequency Cepstral Coefficients (MFCC), classical frequency descriptors of human speech analysis $[24]$. We compute twelve coefficients, with a window length of $25$ ms.\\n\\nIn addition to the topological features, we compute Mel Frequency Cepstral Coefficients (MFCC), classical frequency descriptors of human speech analysis $[24]$. We compute twelve coefficients, with a window length of $25$ ms.\\n\\nFor both objects, we have an increasing sequence of topological spaces. We recover this shape through a filtration, a nested sequence of simplicial complexes $[14]$. We then derive from the shape $[13]$. We compute the homology at all scales, i.e. $\\\\text{dim}_p(S)$ for all embeddings using UMAP $[17]$, ensuring uniformity across all embeddings. We resume in a persistence diagram the description of the object $[18]$.\"}"}
{"id": "bonafos24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3. Acoustics differences between clusters\\n\\nAfter obtaining the partition, we proceed to compare the different clusters by computing various acoustic descriptors. Subsequently, we utilize these descriptors as input for a multinomial logit model, where the cluster serves as the response variable. We estimate one model per cluster, treating each cluster as the referential group. For each of the eight models, we assess the statistical differences in each acoustic descriptor between the referential group and the other clusters. This analysis provides insights into the distinctive acoustic characteristics associated with each cluster, helping to characterize and differentiate them based on the selected features.\\n\\n4. Data analysis\\n\\n4.1. Partition\\n\\nOur model identifies 8 distinct clusters. Initially, we detected 9 clusters, but one of them comprised only 5 records, and none of these records included baby vocalizations. This cluster essentially served as a \\\"garbage cluster,\\\" grouping false positives that remained in the dataset. The Dirichlet process mixture model, leveraging our topologically augmented representation, effectively groups together recordings that differ from the rest of the dataset. It automatically recognizes and segregates a \\\"garbage\\\" class, helping eliminate false positives.\\n\\nTable 2 provides a summary of the cluster distribution by month of the year, indicating the proportion of production for each cluster during each month. This breakdown offers insights into how vocalization patterns vary across different clusters and months.\\n\\nTable 2: Proportion (percentage) of production for each cluster during the year\\n\\n| Month | Cluster 1 | Cluster 2 | Cluster 3 | Cluster 4 | Cluster 5 | Cluster 6 | Cluster 7 | Cluster 8 |\\n|-------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\\n| 1     | 15%       | 16%       | 2%        | 9%        | 11%       | 22%       | 19%       | 6%        |\\n| 2     | 11%       | 4%        | 1%        | 7%        | 16%       | 33%       | 20%       | 8%        |\\n| 3     | 65%       | 2%        | 2%        | 2%        | 7%        | 15%       | 4%        | 3%        |\\n| 4     | 31%       | 23%       | 0%        | 8%        | 0%        | 31%       | 8%        | 0%        |\\n| 5     | 83%       | 1%        | 4%        | 2%        | 3%        | 1%        | 6%        | 1%        |\\n| 6     | 24%       | 11%       | 4%        | 7%        | 16%       | 4%        | 24%       | 9%        |\\n| 7     | 22%       | 10%       | 20%       | 16%       | 8%        | 6%        | 10%       | 8%        |\\n| 8     | 0%        | 2%        | 3%        | 15%       | 2%        | 73%       | 3%        | 3%        |\\n\\n4.2. Comparison of clusters\\n\\nThe insights from Table 2 reveal distinctive temporal patterns among the clusters. First, Cluster 2 is characterized by late vocalizations in the first year, with a substantial portion (a third) produced in the ninth month and a notable increase in production during the eighth month. About 20% of its production occurs during the eleventh month, indicating that more than half of the cluster's production takes place after the ninth month. Similar to Cluster 2, Cluster 8 represents late vocalizations, primarily produced from the ninth month onwards. In contrast to Clusters 2 and 8, Cluster 5 comprises vocalizations produced predominantly in the first months of life, with over 80% occurring during this period. This cluster encapsulates early vocalizations, which decrease as the child learns to produce other types of vocalizations. Like Cluster 5, Cluster 3 also contains vocalizations primarily produced in the first two months, constituting 65% of its production. Clusters 4 and 7 exhibit a skewed distribution towards the first few months of life, with the majority of vocalizations produced in the first 6 months, and even just the first three months for Cluster 4. However, there is comparatively more vocal production from these clusters over the rest of the year than Clusters 3 and 5. Clusters 1 and 6 stand out for being produced throughout the entire year, indicating a more consistent vocalization pattern across the different months.\\n\\nThese temporal variations in vocalization patterns highlight the diversity of the clusters and the developmental changes in vocal behavior over the course of the first year of life, that we illustrate in Figure 1. Whereas we have the proportion of vocalization of each cluster per month (i.e., it sums to one per cluster) in Table 2, we plot in Figure 1 the proportion of vocalization of each cluster at each month (i.e., it sums to one per month).\\n\\nWe present a summary of median acoustic descriptors per cluster in Table 3. Utilizing these descriptors, we employed multinomial logit models to estimate acoustic differences among the identified clusters. To enhance clarity, we provide a concise overview of the main results and distinctions between clusters. Specifically, we highlight instances where the parameter associated with a descriptor is statistically different from zero for the majority of other levels (i.e., from other clusters).\\n\\nClusters 2 and 8, characterized as late vocalizations, exhibit differences from other clusters. Cluster 2 differs in its proportion of voiced frames, entropy level, and $F_3$. Cluster 8, on the other hand, varies in spectral centroid level, entropy level, $F_2$, and $F_3$.\\n\\nClusters 3 and 5, representing earlier vocalizations, also show distinctions from other clusters. Cluster 3 differs in its proportion of voiced frames, spectral centroid level, loudness, and $F_3$. Cluster 5 exhibits differences in its proportion of voiced frames, spectral centroid level, entropy level, Harmonics-to-Noise Ratio, loudness, Frequency Modulation, and $F_3$.\\n\\nCluster 4 stands out from others due to differences in its proportion of voiced frames and entropy level. Cluster 7 differs in the proportion of voiced frames, spectral centroid level, loudness, $F_2$, and $F_3$. Cluster 1 exhibits distinctions in the proportion of voiced frames, spectral centroid level, Frequency Modulation, loudness, roughness, $F_1$, and $F_3$. Lastly, Cluster 6 varies in its proportion of voiced frames, spectral centroid level, and $F_3$. \\n\"}"}
{"id": "bonafos24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Acoustics descriptors of each cluster. We report the median for each cluster. Unit of Pitch, Formants, Spectral Centroid and FM is Hz, Loudness is in sone, HNR is in dB, Duration is in sec, Voiced and Roughness are proportions.\\n\\n| Cluster | Duration | Pitch | F1 | F2 | F3 | Voiced | SC | Entropy | HNR | FM | Loudness | Roughness |\\n|---------|----------|-------|----|----|----|--------|----|---------|-----|----|----------|-----------|\\n| 1       | 1.4      | 301   | 725 | 2374 | 4640 | 46  | 3084 | 0.33 | 9.39 | 6.23 | 12.46 | 22.75     |\\n| 2       | 2.6      | 334   | 832 | 2041 | 3757 | 48  | 2596 | 0.24 | 12.54 | 5.46 | 46.46 | 13.46     |\\n| 3       | 2.6      | 338   | 1534 | 3571 | 6000 | 17  | 2268 | 0.27 | 5.75 | 7.54 | 46.46 | 7.15      |\\n| 4       | 2.6      | 342   | 878 | 2610 | 4804 | 16  | 1870 | 0.35 | 9.48 | 5.54 | 10.00 | 11.35     |\\n| 5       | 2.6      | 316   | 913 | 3056 | 5685 | 30  | 3769 | 0.41 | 8.41 | 6.54 | 10.35 | 8.23      |\\n| 6       | 2.6      | 382   | 925 | 2546 | 4244 | 43  | 3549 | 0.36 | 8.23 | 5.46 | 13.46 | 7.23      |\\n| 7       | 2.6      | 346   | 856 | 2117 | 3724 | 28  | 3252 | 0.39 | 9.39 | 5.46 | 12.46 | 23.15     |\\n| 8       | 2.6      | 381   | 1034 | 2384 | 4065 | 23  | 8528 | 0.48 | 8.23 | 5.54 | 12.46 | 8.23      |\\n\\n5. Discussion\\n\\nWe conducted an analysis on a unique dataset comprising 1851 vocalizations from a baby, spanning her birth to her first birth-day. These vocalizations were extracted from longitudinal recordings captured at home, devoid of external interactions. Employing an innovative topologically augmented signal representation, we adapted an unsupervised strategy to cluster the vocalizations based on this representation.\\n\\nRemarkably, certain clusters of vocalizations only emerge after a specific period, while others exhibit a decreasing production trend. Clusters 3 and 5, prominently generated post-birth, experience minimal production throughout the remainder of the year. It is plausible to hypothesize that these clusters represent the initial vocalization classes, serving as a foundation for subsequent vocalization categories.\\n\\nIn contrast, clusters 2 and 8 materialize towards the end of the year, predominantly around the ninth month, with cluster 8 showing distinctive formant characteristics. These late-emerging clusters coincide with the child's increased diversification of vocal productions and heightened babbling, a phenomenon documented in the literature, peaking between the ninth and tenth months [30, 31].\\n\\nFrom a language development perspective, aligning with the concept of calibration [1], the child undergoes a learning process during the initial months, gradually mastering her vocal apparatus to produce sounds resembling the phonemes of her native language. Notably, an early vocalization cluster such as cluster 5 also exhibits nonlinear phenomena, indicative of strong vocal tension [32].\\n\\nCluster 5, primarily produced in the initial two months, stands out with a notably high entropy level. On the contrary, Cluster 2, produced later in the year, distinguishes itself from other clusters with lower entropy. This suggests an improvement in the child's motor control of the buco-phonatory apparatus over the course of the year, resulting in vocalizations with lower entropy compared to earlier productions. The identified clusters, with diverse temporal distributions, highlight variations in precocity among vocal productions.\\n\\nThe incorporation of topological information in signal representation proves effective in clustering vocalizations based on various acoustic parameters. Notably, we observe no differences in pitch between clusters. Given that we have only one child in this database and that pitch is a good individual marker [33], our model does not rely on this feature for clustering. However, our approach exhibits limitations. First, our model treats all vocalizations as exchangeable, neglecting time dependence. To do this, we need to consider the temporal aspect by employing a non-parametric regression [34], enabling the incorporation of covariates.\\n\\nMoreover, while the current topological representation aids in identifying clusters with distinct acoustic profiles, refinement is needed. Synthetic persistent variables are constructed to mitigate the curse of dimensionality, resulting in information loss, particularly for the persistent homology of the spectrogram. Exploring methods for constructing a lower-dimensional signal representation that incorporates topological information, such as [35, 36] is a worthwhile avenue.\\n\\nIn terms of modeling, incorporating expert knowledge and refining priors, especially the choice of $\\\\alpha$, could enhance the clustering process. Adjusting $\\\\alpha$ might impact the final clustering outcome, and its initial computation based on expecting five clusters could be refined based on the diversity of vocalizations observed in literature during a baby's first year [30, 37].\\n\\nFinally, the current analysis, although valuable for its longitudinal and ecological nature, pertains to a single child. The results, while insightful, cannot propose a new categorization of vocal productions. Future research should deepen the analysis by including more children. Introducing hierarchy in subsequent analyses could facilitate comparisons of vocal productions and their evolution over the first year of life, considering integrated covariates.\\n\\n6. Conclusion\\n\\nIn conclusion, we investigated a novel database comprising vocalizations extracted from long-form audio recordings of a child from birth to her first birthday. This dataset offers a unique perspective, capturing vocalizations in an uncontrolled, longitudinal setting without interaction, allowing for the exploration of new inquiries.\\n\\nWe introduced an innovative approach to analyze this database, aiming to identify distinct clusters of vocalizations produced by the child. Employing an unsupervised methodology, we utilized a Dirichlet process mixture model without specifying the number of classes beforehand. By incorporating topological information into the signal representation, we successfully identified 8 vocalization classes throughout the year. Acknowledging the outlined limitations, the detected clusters exhibited varying production proportions over time. Furthermore, our topologically augmented representation facilitated the identification of clusters with diverse acoustic profiles, illustrating the child's evolving motor control of her buco-phonatory apparatus.\"}"}
