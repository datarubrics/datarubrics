{"id": "mujtaba24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Inclusive ASR for Disfluent Speech: Cascaded Large-Scale Self-Supervised Learning with Targeted Fine-Tuning and Data Augmentation\\n\\nDena Mujtaba, Nihar R. Mahapatra, Megan Arney, J. Scott Yaruss, Caryn Herring, Jia Bin\\n\\n1 Michigan State University\\n2 FRIENDS: The National Association of Young People Who Stutter\\n\\n{mujtabad,nrm,arneymeg,jsy,binjia}@msu.edu, caryn@friendswhostutter.org\\n\\nAbstract\\nAutomatic speech recognition (ASR) systems often falter while processing stuttering-related disfluencies\u2014such as involuntary blocks and word repetitions\u2014yielding inaccurate transcripts. A critical barrier to progress is the scarcity of large, annotated disfluent speech datasets. Therefore, we present an inclusive ASR design approach, leveraging large-scale self-supervised learning on standard speech followed by targeted fine-tuning and data augmentation on a smaller, curated dataset of disfluent speech. Our data augmentation technique enriches training datasets with various disfluencies, enhancing ASR processing of these speech patterns. Results show that fine-tuning wav2vec 2.0 with even a relatively small, labeled dataset, alongside data augmentation, can significantly reduce word error rates for disfluent speech. Our approach not only advances ASR inclusivity for people who stutter, but also paves the way for ASRs that can accommodate wider speech variations.\\n\\nIndex Terms: accessibility, automatic speech recognition, bias, data augmentation, fine-tuning, stuttering\\n\\n1. Introduction\\nStuttering is a complex neurodevelopmental condition characterized by moments of involuntary disruptions in the flow of speech when people who stutter experience a loss of control [1]. These disruptions manifest as disfluencies like word repetitions (e.g., \u201cmy my my name is\u201d), prolongations (e.g., \u201cmmmy name is\u201d), and blocks or pauses (e.g., \u201cmy n\u2014ame is\u201d) [2, 3]. Although these and other types of disfluencies occur in everyday speech, they are markedly more frequent in individuals who stutter, a condition that affects over 80 million people globally [2, 4]. The prevalence and severity of these disfluencies can vary significantly across individuals, influenced by environmental and situational factors [5].\\n\\nThe widespread integration of automatic speech recognition (ASR) into voice-activated artificial intelligence (voice AI) applications across sectors such as education, employment, home automation, and transportation presents a significant challenge for people who stutter. Although ASRs can achieve up to 95% word accuracy on standard speech [6], their performance markedly declines when processing disfluent speech, often failing to accurately decode the variations inherent in stuttering [2, 7]. This discrepancy not only impacts people who stutter in their daily and professional lives but also amplifies societal biases. It leads to discrimination, restricts equal opportunities in automated services and employment, and contributes to difficulties in utilizing voice-activated services like automated phone systems, voice assistants (e.g., Apple\u2019s Siri, Amazon\u2019s Alexa), and biases in automated job interview scoring systems [8]. The effect of these biases is profound, often resulting in decreased job satisfaction and increased marginalization, as fluent speech is frequently preferred in employment settings [9, 10]. Consequently, the disparity in ASR accuracy will exacerbate the marginalization of people who stutter, impacting all downstream applications.\\n\\n1.1. Related Work\\nAddressing the need for inclusive ASR models has prompted diverse strategies. One such approach involves providing alternative communication options, like typing commands for voice assistants [11]. While helpful, this method does not directly tackle the intrinsic bias within ASRs. In a more direct effort, initiatives such as Google\u2019s Project Euphonia and Relate have sought to refine the core ASR model, collecting diverse speech data to better accommodate conditions like dysarthria [6]. Nonetheless, these efforts have not specifically addressed the challenges presented by stuttering-related disfluent speech, characterized notably by its variability [5].\\n\\nIn response to the distinctive needs of people who stutter, research has increasingly focused on creating ASR systems specifically designed for stuttering. The majority of such work has focused on disfluency detection, wherein disfluency events are detected and classified and subsequently adjusted, ignored, highlighted, or removed from speech prior to ASR processing, thereby enhancing accuracy [12]. Although datasets like SEPl28k and LibriStutter have emerged to support disfluency detection [13, 14], there is a scarcity of datasets with paired transcripts and audio to train ASR models. Very few approaches have been proposed to tailor ASR models directly for stuttering. Lea et al., for instance, tuned Apple Speech framework model parameters using data on stuttering characteristics (e.g., distribution of duration of blocks) obtained from individuals who stutter [7]. Similarly, Mitra et al. focused on tuning decoding parameters of ASRs [15]. Other approaches have also been investigated to enhance ASR models, although they focus on other types of speech variations like dysarthria, dysphonia, or aphasia [6, 16, 17, 18, 19].\\n\\n1.2. Our Contributions\\nThis paper aims to address the aforementioned challenges and develop an accessible ASR for people who stutter using a combination of fine-tuning and data augmentation. A primary limitation in creating an ASR for stuttered speech is the absence of a comprehensive dataset covering various types of disfluencies and their variations among individuals who stutter. To overcome this limitation, we introduce a novel speech data augmentation method specifically designed for stuttered speech that also allows for the manipulation and control of disfluency types during the fine-tuning process.\"}"}
{"id": "mujtaba24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Schematic of our method integrating data augmentation and fine-tuning of wav2vec 2.0 for stuttered speech, exemplified by augmentations from a FluencyBank speech sample [20]. In wav2vec 2.0, $L$ denotes the loss function, $C$ represents context representations, $Q$ indicates quantized representations, and $Z$ corresponds to latent speech representations.\\n\\nThe core contributions of this paper, distinguishing it from prior efforts, are fourfold: (1) ASR fine-tuning for accessibility with a stuttering-specific focus: We investigate the impact of fine-tuning a pre-trained wav2vec 2.0 model with stuttered speech datasets of various sizes on ASR word error rate (WER) and semantic similarity, aiming to enhance model performance for disfluent speech. We specifically target disfluencies in stuttering, which vary greatly across people and environments, necessitating specialized solutions. (2) Disfluent speech data augmentation: An innovative data augmentation method designed specifically for stuttered speech is introduced, overcoming the challenges of limited data. This method provides precise control over the types, frequency, and placement of disfluencies within speech samples, enabling the enrichment of our dataset and more effective training of robust ASRs. (3) Accuracy bias analysis: We evaluate the effectiveness of our method in mitigating accuracy bias\u2014the discrepancy in ASR performance when processing speech with and without stuttering. By comparing ASR performance against a baseline of non-stuttered speech, we assess the effectiveness of our strategies in promoting equitable ASR accuracy. (4) Diverse and realistic evaluation settings: We evaluate our fine-tuned ASR using speech from diverse contexts, including interview and reading videos, and assess its performance across various demographics of people who stutter. With these contributions, we anticipate our approach will advance the development of fair and accessible ASRs for individuals who stutter, with potential extensions to accommodate other speech differences featuring disfluencies.\\n\\n2. Methodology\\n\\n2.1. wav2vec 2.0\\n\\nOur study leverages wav2vec 2.0, a large-scale, self-supervised model designed for speech processing. Figure 1 provides a visual overview of the model's architecture. Pre-trained on an extensive corpus of unlabeled speech, it can be subsequently fine-tuned on a smaller, targeted dataset. This model excels in constructing a contextually rich representation of speech through a series of lightweight components. Compared to larger end-to-end models such as Whisper [21], wav2vec 2.0, although not a state-of-the-art model, offers a more computationally-efficient approach for our experiments, providing a balance of performance and feasibility. Additionally, it offers precise control over the training dataset, avoiding the potential for data leakage into test sets that could occur with Whisper due to its opaque training data. For fine-tuning, we utilize the 'wav2vec2-base-960h' model variant from HuggingFace [22], pre-trained on 960 hours of the LibriSpeech dataset [23]. This selection aids in precisely attributing performance enhancements to our methodological innovations, despite the model's potential suboptimal performance on conversational speech.\\n\\n2.2. FluencyBank\\n\\nFor assessing ASR performance on authentic stuttered speech, we utilize the FluencyBank dataset [20], which comprises video recordings of people who stutter in two distinct settings: reading passages and participating in interviews. This collection features 7 readings and 12 interview videos from 12 participants, each paired with a transcript adhering to the Codes for the Human Analysis of Transcripts (CHAT) standard [24]. CHAT transcripts segment speech into individual utterances\u2014sentences or fragments thereof, uttered by a single speaker\u2014annotated with disfluency event labels. Faced with temporal misalignment and inaccuracies in FluencyBank's CHAT transcripts\u2014a problem also reported by others [12, 13]\u2014we had this dataset re-annotated in CHAT format by trained speech-language pathologists with expertise in stuttering, while ensuring inter-annotator agreement. In total, the final dataset consists of 1373 utterances and 2.21 hours of audio duration.\\n\\n2.2.1. Non-Disfluent Speech Datasets\\n\\nTo evaluate our fine-tuned ASR's efficacy on non-disfluent speech, we generated a modified version of the FluencyBank dataset, termed FluencyBank-N, from which all apparent disfluencies have been removed. This variant was synthesized using the SpeechT5 text-to-speech (TTS) model [25] in conjunction with a selection of pre-trained speaker vectors [26], aiming to produce speech that mirrors the original transcripts but without disfluencies. By carefully selecting and employing 10 distinct pre-trained speaker vectors, we ensured a varied vocal representation. This approach allows us to precisely assess how our ASR model performs on the FluencyBank dataset under the hypothetical scenario where the speakers exhibit no disfluencies. We note that although synthetic speech is not identical to real speech, the performance of wav2vec 2.0 on this synthetic dataset is comparable to, albeit slightly worse than, its performance on the test set. This similarity, detailed in our results (Sec. 3), indicates that the synthetic dataset is a suitable metric for studying accuracy bias.\\n\\n2.3. Data Augmentation for Disfluent Speech\\n\\nTo overcome the diversity shortfall in small stuttered speech datasets, our data augmentation strategy introduces a broader spectrum of disfluency events, more closely mirroring natural speech patterns observed in individuals who stutter. While the LibriStutter dataset [14] represents a prior attempt at simulating stuttering through audio processing, it falls short in capturing the full complexity of the stuttering phenomena, notably in variability. In contrast, our method generates a richer assortment of disfluency events, including word repetitions (e.g., \\\"my my my name is\\\"), phrase repetitions (e.g., \\\"my name my name my name is\\\"), and interjections (e.g., \\\"um um my name uh is\\\"), inserted at randomized locations and frequencies during ASR training, thereby producing a simulation that more accurately reflects the inherent variability in disfluent speech of real individuals. Although our approach does not cover every type of stuttered disfluency, the selected event types are crucial. They...\"}"}
{"id": "mujtaba24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"enhance the realism of the training data and help study the effectiveness of our augmented dataset, ultimately improving ASR accuracy for all types of stuttered disfluencies.\\n\\nDuring training, we augment FluencyBank utterances to assess the impact of incorporating \\\\( N \\\\) augmented samples, employing random sampling from the original dataset with replacement. Starting with disfluency-free ground truth transcripts, we introduce specific disfluency events in text form. These texts are processed by the OpenAI TTS API [27] to synthesize speech with the inserted disfluencies, using voices randomly selected from the OpenAI library. This approach enables us to simulate a wide range of voices and disfluency events, along with their occurrence frequency in each utterance, facilitating a comprehensive evaluation of ASR performance.\\n\\nWe randomize disfluency frequencies and placements within set ranges: (1) for word repetition, we select between 1 to 3 words in each utterance to repeat 1 to 4 additional times; (2) for phrase repetition, we choose a phrase of 2 to 4 words and repeat it 1 to 3 times; (3) for interjections, we randomly insert either \u201cuh\u201d or \u201cum\u201d in 1 to 4 locations and repeat them 1 to 4 times. Initially, we explore the effectiveness of adding \\\\( N \\\\) samples during training, for \\\\( N = 500, 1000, 2000, 3000 \\\\), represented as \\\\( p \\\\) percent of the original dataset size. Subsequently, we change the number of repeats of disfluencies in our generation approach, and produce up to \\\\( N = 6000 \\\\) augmented samples to assess the impact of increased disfluency variability on ASR performance. The new ranges used are the following: (1) for word repetition, each selected word is repeated 1 to 6 additional times; (2) for phrase repetition, each selected phrase is repeated 1 to 5 additional times; (3) for interjections, each interjection is repeated 1 to 7 times. The upper bound for all modification ranges is adjusted to fit the shortest length of the utterance (e.g., if there are only two words in an utterance, no more than two disfluency insertions can occur).\\n\\n2.4. Evaluation Metrics\\n\\nto assess the performance of our ASR models and examine accuracy bias, we employ two metrics: the word error rate (WER) and BERTScore.\\n\\nWER is a standard metric in ASR evaluations, defined as:\\n\\n\\\\[\\n\\\\text{WER} = \\\\frac{S + D + I}{S + D + C},\\n\\\\]\\n\\nwhere \\\\( S, D, I, \\\\) and \\\\( C \\\\) denote the number of word substitutions, deletions, insertions, and correct words, respectively, compared to the reference transcript. A lower WER, approaching zero, is desirable, indicating higher word transcription accuracy.\\n\\nBERTScore, on the other hand, measures semantic similarity between the predicted and reference sentences, crucial for identifying significant changes in meaning caused by minor transcription errors. Given an ASR-produced transcript \\\\( x = \\\\langle x_1, ..., x_k \\\\rangle \\\\) and a reference transcript \\\\( \\\\hat{x} = \\\\langle \\\\hat{x}_1, ..., \\\\hat{x}_k \\\\rangle \\\\), where \\\\( x_i \\\\) and \\\\( \\\\hat{x}_i \\\\) are \\\\( i \\\\)th token embeddings (e.g., from BERT), BERTScore is calculated as follows [28, 29]:\\n\\n\\\\[\\nF_{\\\\text{BERT}} = \\\\frac{2 \\\\times P_{\\\\text{BERT}} \\\\times R_{\\\\text{BERT}}}{P_{\\\\text{BERT}} + R_{\\\\text{BERT}}},\\n\\\\]\\n\\nwhere\\n\\n\\\\[\\nR_{\\\\text{BERT}} = \\\\frac{1}{|x|} \\\\sum_{x_i \\\\in x} \\\\max_{\\\\hat{x}_j \\\\in \\\\hat{x}} (x^T_i \\\\cdot \\\\hat{x}_j),\\n\\\\]\\n\\nand\\n\\n\\\\[\\nP_{\\\\text{BERT}} = \\\\frac{1}{|\\\\hat{x}|} \\\\sum_{\\\\hat{x}_i \\\\in \\\\hat{x}} \\\\max_{x_j \\\\in x} (x^T_i \\\\cdot \\\\hat{x}_j).\\n\\\\]\\n\\nHere, \\\\( F_{\\\\text{BERT}} \\\\) represents the F1 score, and we utilize the re-scaled version of BERTScore to obtain a range of \\\\([-1, 1]\\\\), where 1 would be semantically identical transcripts. We also utilize the \u201cmicrosoft/deberta-large-mnli\u201d model embeddings [30], which offers more nuanced contextual representations than BERT.\\n\\nTable 1: Comparative analysis of ASR performance metrics (WER and \\\\( F_{\\\\text{BERT}} \\\\)) on FluencyBank (FB) and FluencyBank-N (FBN). \u201cBase\u201d denotes wav2vec 2.0 without any fine-tuning. Subsequent ASRs shown are trained with a \\\\( p \\\\) percentage increase in the training dataset size through augmentation.\\n\\n|                | FB     | FBN   | FB     | FBN   |\\n|----------------|--------|-------|--------|-------|\\n| WER            | 0.4223 | 0.4700| 0.1029 | 0.8561|\\n| \\\\( p = 0 \\\\)    | 0.2721 | 0.6357| 0.0952 | 0.8884|\\n| \\\\( p = 36 \\\\)   | 0.2705 | 0.6291| 0.1306 | 0.8063|\\n| \\\\( p = 73 \\\\)   | 0.2668 | 0.6332| 0.1389 | 0.7736|\\n| \\\\( p = 146 \\\\)  | 0.2631 | 0.6349| 0.1436 | 0.7590|\\n| \\\\( p = 218 \\\\)  | 0.2624 | 0.6367| 0.1399 | 0.7791|\\n| \\\\( p = 291 \\\\)  | 0.2581 | 0.6402| 0.1318 | 0.7885|\\n| \\\\( p = 364 \\\\)  | 0.2569 | 0.6390| 0.1336 | 0.7833|\\n| \\\\( p = 437 \\\\)  | 0.2522 | 0.6566| 0.1248 | 0.8128|\\n\\n2.5. Implementation\\n\\nFor our study, we fine-tuned wav2vec 2.0 models on the FluencyBank dataset, augmented with \\\\( N \\\\) randomly selected samples to address the limited dataset size. We implemented a 6-fold cross-validation strategy, ensuring each fold contained utterances from two distinct speakers, with no speaker overlap across folds. This segmentation of FluencyBank into six parts based on speaker identity guarantees that any two folds are free from speaker overlap, whether across videos, utterances, or generated utterances. Such a methodological approach allows for a focused evaluation of ASR performance on voices not previously encountered during training, with augmentation applied only to utterances from speakers allocated to each specific fold.\\n\\nThe fine-tuning process leveraged the Connectionist Temporal Classification Loss (CTCLoss), a conventional choice for optimizing wav2vec 2.0 [31]. Training was conducted on an NVIDIA V100 GPU for about 55 epochs, or until no further reduction in loss was observed, employing early stopping to identify the most effective model configuration per fold. In addition, through experimentation with several hyperparameters, we set the learning rate to 6.25e-6, weight decay to 0.01, and use a batch size of 4 with gradient accumulation applied at each step. Additionally, to standardize input data, normalization procedures were applied to adjust casing and punctuation using the BasicTextNormalizer function from Whisper, available through the HuggingFace library [32].\\n\\n3. Results & Discussion\\n\\nWe present the performance outcomes of the wav2vec 2.0 models on the FluencyBank dataset, both with disfluencies (FluencyBank) and without disfluencies (FluencyBank-N), in Table 1. The results reveal a consistent trend across all evaluated wav2vec 2.0 models: they achieve lower WER and higher \\\\( F_{\\\\text{BERT}} \\\\) scores when tested on the FluencyBank-N dataset. This pattern suggests the presence of systemic bias in the models against stuttered speech.\\n\\nFine-tuning wav2vec 2.0 on datasets containing stuttering significantly improves both WER and \\\\( F_{\\\\text{BERT}} \\\\) metrics. Specifically, we observed a substantial 15% reduction in WER for stuttered speech, even in the absence of augmented samples. Additionally, there is a 25% enhancement in \\\\( F_{\\\\text{BERT}} \\\\), reflecting more accurate transcriptions that better capture the intended meaning. This fine-tuning not only benefits stuttered speech but...\"}"}
{"id": "mujtaba24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Comparative analysis of ASR WER on FluencyBank (FB) and FluencyBank-N (FBN) per video setting (i.e., reading or interview). \u201cBase\u201d refers to wav2vec 2.0 without any fine-tuning. Subsequent ASRs shown are trained with a percentage increase in the training dataset size through augmentation.\\n\\n|          | WER Reading | WER Interview |\\n|----------|-------------|---------------|\\n| FB       | 0.3775      | 0.4279        |\\n| FBN      | 0.1045      | 0.1027        |\\n| FB       | 0.1912      | 0.1947        |\\n| FBN      | 0.0795      | 0.0938        |\\n| FB       | 0.1947      | 0.1988        |\\n| FBN      | 0.0938      | 0.0938        |\\n| FB       | 0.1888      | 0.2800        |\\n| FBN      | 0.1445      | 0.1352        |\\n| FB       | 0.1834      | 0.2731        |\\n| FBN      | 0.1487      | 0.1415        |\\n| FB       | 0.1780      | 0.2694        |\\n| FBN      | 0.1385      | 0.1306        |\\n| FB       | 0.1673      | 0.2691        |\\n| FBN      | 0.1625      | 0.0783        |\\n\\nFigure 2: Distribution of WER for four of our models per disfluency type: wav2vec 2.0 (W2V2) tested on FluencyBank (FB), W2V2 tested on FluencyBank-N (FBN), W2V2 fine-tuned (FT) on FB and tested on FB, and W2V2 fine-tuned with N = 6000 additional samples (i.e., p = 437) tested on FB.\\n\\nAlso slightly enhances the performance on non-disfluent speech, as demonstrated by the FluencyBank-N dataset outcomes. The results, detailed in Table 1, indicate a positive trend: incorporating augmented disfluent speech samples during training leads to further improvements in WER and BERT. However, we note a minor increase in WER for FluencyBank-N, suggesting a potential overcorrection towards stuttered speech characteristics. Despite this, as the training incorporates a larger pool of samples, the WER begins to stabilize and shows a gradual decline, underlining the benefits of extensive data augmentation.\\n\\nOur analysis across different ASR configurations and FluencyBank video types\u2014interviews and reading passages\u2014revealed variable WER performance. Interviews, involving personal experience discussions, showed less ground truth word overlap than reading passages, yet occasionally had content overlap due to discussions on life with stuttering. Detailed in Table 2, this analysis mirrors our overall results (Table 1), with reading passages experiencing greater WER improvements than interviews, though both video types saw notable enhancements.\\n\\nWe also analyzed the WER across different speakers and disfluency types. The speaker-specific WERs for three distinct model configurations are shown in Figure 3, and the WER distributions related to specific disfluencies for four configurations are presented in Figure 2. Our focus was on repetitions and interjections, identified through analysis of CHAT codes in FluencyBank transcripts. We found that 31% of utterances in the FluencyBank dataset contained at least one word and/or phrase repetition, and 59% included interjections, though other disfluency types were also noted. Our primary analysis remained focused on these disfluencies, which were also the key targets in our data augmentation process. Post fine-tuning and data augmentation, we observed a reduction in WER across all speakers and various age and gender demographics. Additionally, there was a decrease in disfluency-specific WER for each model configuration, demonstrating the efficacy of our approach.\\n\\n4. Conclusion\\n\\nIn this paper, we introduced a new approach to improving ASR accuracy for individuals who stutter, overcoming previous limitations in dataset size and diversity with a novel data augmentation strategy that infuses new training samples with synthetic disfluencies. Our findings reveal that even minimal fine-tuning with disfluent speech significantly enhances ASR performance, as evidenced by reduced WER, with further improvements stemming from our data augmentation techniques. Looking ahead, we plan to broaden our research to include a wider array of disfluency types, aiming for a more comprehensive representation of stuttered speech. We also seek to fine-tune our methodology to better align ASR-generated transcripts with the preferences of people who stutter, acknowledging that some may wish to see their disfluencies reflected in transcripts to accurately convey their speech patterns. This aspiration underscores the importance of understanding the diverse needs and preferences of those who stutter. Ultimately, our research contributes to making ASR technology more accessible and adaptable, laying the groundwork for future innovations that accommodate a range of speech variations.\"}"}
{"id": "mujtaba24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This material is based upon work supported by the U.S. National Science Foundation under Grant Nos. 2235916 and 2345086.\\n\\n6. References\\n\\n[1] S. E. Tichenor and J. S. Yaruss, \u201cStuttering as defined by adults who stutter,\u201d *Journal of Speech, Language, and Hearing Research*, vol. 62, no. 12, pp. 4356\u20134369, 2019.\\n\\n[2] S. Wu, \u201cThe World is Designed for Fluent People\u201d: Benefits and challenges of videoconferencing technologies for people who stutter,\u201d in *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*, 2023, pp. 1\u201317.\\n\\n[3] W. Johnson, *The onset of stuttering: Research findings and implications*. U of Minnesota Press, 1959.\\n\\n[4] E. Yairi and N. Ambrose, \u201cEpidemiology of stuttering: 21st century advances,\u201d *Journal of fluency disorders*, vol. 38, no. 2, pp. 66\u201387, 2013.\\n\\n[5] S. E. Tichenor and J. S. Yaruss, \u201cVariability of stuttering: Behavior and impact,\u201d *American Journal of Speech-Language Pathology*, vol. 30, no. 1, pp. 75\u201388, 2021.\\n\\n[6] J. Tobin and K. Tomanek, \u201cPersonalized automatic speech recognition trained on small disordered speech datasets,\u201d in *ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, 2022, pp. 6637\u20136641.\\n\\n[7] C. Lea, Z. Huang, J. Narain, L. Tooley, D. Yee, D. T. Tran, P. Georgiou, J. P. Bigham, and L. Findlater, \u201cFrom user perceptions to technical improvement: Enabling people who stutter to better use speech recognition,\u201d in *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*, 2023, pp. 1\u201316.\\n\\n[8] D. F. Mujtaba and N. R. Mahapatra, \u201cEthical considerations in AI-based recruitment,\u201d in *2019 IEEE International Symposium on Technology and Society (ISTAS)*. IEEE, 2019, pp. 1\u20137.\\n\\n[9] H. Gerlach, E. Totty, A. Subramanian, and P. Zebrowski, \u201cStuttering and labor market outcomes in the United States,\u201d *Journal of Speech, Language, and Hearing Research*, vol. 61, no. 7, pp. 1649\u20131663, 2018.\\n\\n[10] L. W. Plexico, M.-B. Hamilton, H. Hawkins, and S. Erath, \u201cThe influence of workplace discrimination and vigilance on job satisfaction with people who stutter,\u201d *Journal of Fluency Disorders*, vol. 62, p. 105725, 2019.\\n\\n[11] K. Wheeler, \u201cFor people who stutter, the convenience of voice assistant technology remains out of reach,\u201d https://techxplore.com/news/2020-01-people-stutter-convenience-voice-technology.html.\\n\\n[12] S. A. Sheikh, M. Sahidullah, F. Hirsch, and S. Ouni, \u201cMachine learning for stuttering identification: Review, challenges and future directions,\u201d *Neurocomputing*, 2022.\\n\\n[13] C. Lea, V. Mitra, A. Joshi, S. Kajarekar, and J. P. Bigham, \u201cSEP-28k: A dataset for stuttering event detection from podcasts with people who stutter,\u201d in *ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, 2021, pp. 6798\u20136802.\\n\\n[14] T. Kourkounakis, A. Hajavi, and A. Etemad, \u201cFluentNet: end-to-end detection of speech disfluency with deep learning,\u201d *arXiv preprint arXiv:2009.11394*, 2020.\\n\\n[15] V. Mitra, Z. Huang, C. Lea, L. Tooley, S. Wu, D. Botten, A. Palekar, S. Thelapurath, P. Georgiou, S. Kajarekar et al., \u201cAnalysis and tuning of a voice assistant system for dysfluent speech,\u201d *arXiv preprint arXiv:2106.11759*, 2021.\\n\\n[16] M. Moore, P. Papreja, M. Saxon, V. Berisha, and S. Panchanathan, \u201cUncommonVoice: A crowdsourced dataset of dysphonic speech.\u201d in *Interspeech*, 2020, pp. 2532\u20132536.\\n\\n[17] E. Hermann and M. M. Doss, \u201cFew-shot dysarthric speech recognition with text-to-speech data augmentation.\u201d\\n\\n[18] M. K. Baskar, T. Herzig, D. Nguyen, M. Diez, T. Polzehl, L. Burget, and J. \u010cernock\u00fd, \u201cSpeaker adaptation for wav2vec2 based dysarthric ASR,\u201d *arXiv preprint arXiv:2204.00770*, 2022.\\n\\n[19] B. Macwhinney and D. Fromm, \u201cTalkBank methods for studying spoken discourse,\u201d in *Spoken Discourse Impairments in the Neurogenic Populations: A State-of-the-Art, Contemporary Approach*. Springer, 2024, pp. 97\u2013109.\\n\\n[20] N. B. Ratner and B. MacWhinney, \u201cFluency Bank: A new resource for fluency research and practice,\u201d *Journal of fluency disorders*, vol. 56, pp. 69\u201380, 2018.\\n\\n[21] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d in *International Conference on Machine Learning*. PMLR, 2023, pp. 28 492\u201328 518.\\n\\n[22] Facebook, \u201cwav2vec2-base-960h,\u201d https://huggingface.co/facebook/wav2vec2-base-960h.\\n\\n[23] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibriSpeech: An ASR corpus based on public domain audio books,\u201d in *Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on*. IEEE, 2015, pp. 5206\u20135210.\\n\\n[24] B. MacWhinney, \u201cTools for analyzing talk part 1: The CHAT transcription format,\u201d *Carnegie.*[Google Scholar]*, vol. 16, 2017.\\n\\n[25] J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, Y. Wu, S. Liu, T. Ko, Q. Li, Y. Zhang, Z. Wei, Y. Qian, J. Li, and F. Wei, \u201cSpeechT5: Unified-modal encoder-decoder pre-training for spoken language processing,\u201d in *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, May 2022, pp. 5723\u20135738.\\n\\n[26] M. Hollemans, \u201cMattijs/cmu-arctic-xvectors,\u201d https://huggingface.co/datasets/Matthijs/cmu-arctic-xvectors.\\n\\n[27] \u201cOpenAI Text-to-Speech Service,\u201d https://platform.openai.com/docs/models/tts, accessed: February 2024.\\n\\n[28] J. Tobin, Q. Li, S. Venugopalan, K. Seaver, R. Cave, and K. Tomanek, \u201cAssessing ASR model quality on disordered speech using BERTScore,\u201d *arXiv preprint arXiv:2209.10591*, 2022.\\n\\n[29] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, \u201cBERTScore: Evaluating text generation with BERT,\u201d *arXiv preprint arXiv:1904.09675*, 2019.\\n\\n[30] P. He, X. Liu, J. Gao, and W. Chen, \u201cDeBERTa: Decoding-enhanced BERT with disentangled attention,\u201d in *International Conference on Learning Representations*, 2021. [Online]. Available: https://openreview.net/forum?id=XPZIaotutsD\\n\\n[31] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d in *Advances in neural information processing systems*, vol. 33, pp. 12 449\u201312 460, 2020.\\n\\n[32] HuggingFace, \u201cNormalization and pre-tokenization,\u201d https://huggingface.co/learn/nlp-course/chapter6/4.\"}"}
