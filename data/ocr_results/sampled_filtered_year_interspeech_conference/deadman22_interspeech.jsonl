{"id": "deadman22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Modelling Turn-taking in Multispeaker Parties for Realistic Data Simulation\\n\\nJack Deadman, Jon Barker\\nDepartment of Computer Science, University of Sheffield, UK\\njdeadman1@sheffield.ac.uk, j.p.barker@sheffield.ac.uk\\n\\nAbstract\\nSimulation plays a crucial role in developing components of automatic speech recognition systems such as enhancement and diarization. In source separation and target-speaker extraction, datasets with high degrees of temporal overlap are used both in training and evaluation. However, this contrasts with the fact that people tend to avoid such overlap in real conversations. It is well known that artifacts introduced from pre-processing with no overlapping speech can be detrimental to recognition performance. This work proposes a finite-state based generative method trained on timing information in speech corpora, which leads to two main contributions. First, a method for generating arbitrary large datasets which follow desired statistics of real parties. Second, features extracted from the models are shown to have a correlation with speaker extraction performance. This leads to the contribution of quantifying how much difficulty in a mixture is due to turn-taking, factoring out other complexities in the signal. Models which treat speakers as independent produce poor generation and representation results. We improve upon this by proposing models which have states conditioned on whether another person is speaking.\\n\\nIndex Terms: turn-taking, representations, target speaker extraction\\n\\n1. Introduction\\nAutomatic speech recognition (ASR) is a challenging task with numerous factors contributing to its difficulty, with recognising multi-party conversations in real, noisy environments being one of the most difficult scenarios. Recent ASR challenges have shown that much work is left to be done [1, 2]. It is therefore vital to break down the factors that are contributing to the difficulty. In recent work, we explored the impact of the angular separation between speakers [3] and the microphone distance [4] through analysing videos in the CHiME-5 dataset [1]. In this work, we extend this analysis by looking at speakers' temporal behaviour, i.e., their turn-taking behaviour.\\n\\nGenerating realistically overlapped speech data is crucial for the development of better conversational ASR systems. There has been much recent work in speech separation (a crucial ASR component) but it has mainly focused on highly-overlapped mixtures [5, 6, 7]. This data poorly models the real challenges. For example, applying separation techniques can be detrimental to ASR if the models attempt to extract more sources than those that are present [8], hence good estimates of the number of active speakers are needed. However, this is trivial in fully-overlapped cases. More recently, attention has turned to sparse versions of commonly used datasets [9] that use a parameter to govern the amount of overlap when creating mixtures. This is a step towards creating more realistic simulations but still fails to model the complexity of real conversations.\\n\\nThe simulated dataset most closely capturing real conversation dynamics is LibriCSS [10], which creates long-form parties containing many utterances from several speakers. It can be processed into segmented mixtures, directly used for tasks such as diarization, or used in experiments with enhancement systems requiring long context windows [11]. However, LibriCSS contains just 10 hours of data, which are recordings of audio played back in a room. Therefore, the dataset is only appropriate for evaluation rather than training. The current work greatly extends this, through analysis of the 50 hours of conversations recorded from 20 parties in the CHiME-5 dataset [1].\\n\\nThe uses of the generative overlapped speech simulation techniques presented in this paper extend beyond ASR to the task of diarization. The recent trends in diarization has been towards jointly optimising voice activity detection and segmentation [12], and towards end-to-end systems [13]. If such systems are to be trained using simulated datasets, realistic turn-taking behaviour is essential.\\n\\nIn this work, we establish a framework for analysing the turn-taking behaviour of people in real life recordings and establish a method for extracting how much of the difficulty of a recording can be attributed to the turn-taking behaviour. In Section 2, we discuss the complexity of human turn-taking. In Section 3, we introduce a framework for modelling the turn-taking behaviour of multi-person \u201cparties\u201d using a simple finite-state representation. Section 4 shows that representations can be created that can be used to characterise recordings in real datasets with interpretable meanings. These representations are then evaluated in Section 5 in the context of target-speaker extraction, where the representations can predict the difficulty of a mixture purely based on turn-taking behaviour. Finally, Section 6 concludes our findings.\\n\\n2. Background\\nModelling the turn-taking behaviour of humans is a well-established research field [14, 15]. The behaviour of people changes depending on numerous factors such as their environment, who they are talking to and whether the conversation is physical or virtual. Modelling turn-taking is also a multi-modal activity, where cues are not always verbal. Gazes are often used to select the next speaker, or head nods are used to indicate confirmation and encourage the speaker to continue talking. Much work has been done towards predicting how the turns will develop, allowing for conversational agents to naturally take their turn and add backchannels without the perception of interruption [16].\\n\\nInstead of predicting this behaviour, we aim to observe real turn-taking and generate more data following the observed distribution. Whilst prediction will require observations of the speaker (e.g., a video recording), a generator will not require a recording. The generator can model characteristics of this behaviour by looking at the resulting turns that were taken even if that turn occurs due to non-verbal behaviour.\\n\\nWhen generating parties for larger mixtures of speech, simplistic approaches are used, such as in [13], where speakers are\"}"}
{"id": "deadman22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Depiction of the three model types. The states represent which speakers are actively talking when in that state. The speakers are given the labels 'a' and 'b' respectively. In the competing speaker model, \\\\( \\\\xi \\\\) in the state set represents any other person is speaking. Only transitions between states which yield a single onset or offset change are shown.\\n\\nThis completely neglects how speakers interact with each other. Agent-based models for generating has been explored [17], where participants are parameterised by engineered features such as talkativeness (desire to talk), confidence (persistence to talk when others are talking), verbosity (desire to continue talking). Motivated by this, we aim to build a structure that can learn these parameters from the data.\\n\\n3. Framework for Modelling Turns\\n\\nIn this section, we introduce a framework for modelling turn-taking solely based on utterance timings, i.e., looking at the turn-taking behaviour of speakers while ignoring any linguistic and acoustic cues that will come with these signals. This simple approach allows for easily computed models that can be used across a wide array of datasets, i.e., not all datasets provide fully transcribed text but most provide end-pointing.\\n\\nWith a transcript providing start and endpoints of utterances in a party, a discrete representation of \\\\( L \\\\) observations can be created through sampling at a predefined frame rate \\\\( f_s \\\\).\\n\\nGiven a party with a set of speakers \\\\( P = \\\\{a,b,c,...\\\\} \\\\). The state of the speaker activity of the party is defined as matrix \\\\( Y = [y_1, ..., y_L]^T \\\\) where \\\\( y_k^l = 1 \\\\) if speaker \\\\( k \\\\) is speaking at time frame \\\\( l \\\\) and \\\\( y_k^l = 0 \\\\) otherwise.\\n\\n3.1. Finite-state Model Formulation\\n\\nWe want to build a vector representation \\\\( \\\\phi \\\\) to summarise the speaker activity matrix, \\\\( Y \\\\). We propose to achieve this through training a generative model \\\\( M_\\\\theta \\\\) and computing features from the learnt parameters i.e., \\\\( f(\\\\theta) = \\\\phi \\\\). Finite-state models have been ubiquitous in speech recognition history [18] with their relatively simple design and well-defined mathematical properties. They have also been shown to be useful for modelling and predicting turn-taking [19]. Using the parameters of hidden-Markov models has been well established in many tasks requiring speaker representations e.g., speaker recognition [20], speaker verification [21] and adaptation for robust speech recognition [22, 23].\\n\\nWe propose to model the behaviour of speakers as a series of observations \\\\( y_t \\\\) being generated by a model depending only on \\\\( y_{t-1} \\\\). This can be achieved through Markov models where states indicate the activate speakers at that point in time. First, a fully-connected Markov model, where every possible combination of speakers has a state, \\\\( S_{\\\\text{full}} = \\\\mathcal{P}(\\\\mathcal{P}) \\\\) and an \\\\( |S_{\\\\text{full}}| \\\\times |S_{\\\\text{full}}| \\\\) transition matrix \\\\( T_{\\\\text{full}} \\\\) where \\\\( T_{ij} \\\\) is the probability of transitioning from state \\\\( i \\\\) to state \\\\( j \\\\). The full model requires many parameters (\\\\( O(2^K) \\\\)). We therefore also describe two further models which require fewer parameters (\\\\( O(K) \\\\)) by treating speakers as independent generators with their own Markov models. First, an independent model with a set of independent models \\\\( S_{\\\\text{ind}} = \\\\{s_{\\\\text{ind}1}, ..., s_{\\\\text{ind}K}\\\\} \\\\) where \\\\( s_{\\\\text{ind}k} = \\\\{\\\\{\\\\mathcal{P}k\\\\}, \\\\emptyset\\\\} \\\\), with the corresponding transition matrices for each of the sub-models \\\\( T_{\\\\text{ind}} = \\\\{T_{\\\\text{ind}1}, ..., T_{\\\\text{ind}K}\\\\} \\\\). Next, a competing speaker model where the sub-models have dependencies on other speakers, \\\\( S_{\\\\text{comp}} = \\\\mathcal{P}(\\\\{\\\\mathcal{P}k, \\\\xi\\\\}) \\\\) where \\\\( \\\\xi \\\\) symbolises some other person speaking, the model also has independent transition matrices \\\\( T_{\\\\text{comp}} = \\\\{T_{\\\\text{comp}1}, ..., T_{\\\\text{comp}K}\\\\} \\\\).\\n\\nThe modelling power of all models can be further extended through fitting a time distribution \\\\( P_s(D = d; \\\\Theta_s) \\\\) on each of the states \\\\( s \\\\) for a time \\\\( d \\\\), making the model semi-Markovian [24]. The time spent in the state is drawn from this distribution instead of being purely based on the previous state. A comparison of the different models is depicted in Figure 1 showing how the topology changes for modelling parties where \\\\( K = 2 \\\\).\\n\\n3.2. Training Models\\n\\nThe models can be trained by observing real parties and counting the transitions. The transitions weights are estimated by,\\n\\n\\\\[\\nT_{ij} = \\\\frac{\\\\eta_{ij}}{\\\\sum_k \\\\eta_{ik}}\\n\\\\]\\n\\nwhere \\\\( \\\\eta_{ij} \\\\) is the count of the transitions from \\\\( s_i \\\\) to \\\\( s_j \\\\). This is the same process for all the models. The state duration distributions \\\\( P_s(D = d; \\\\Theta_s) \\\\) are fit through counting the number of consecutive samples that have the same state, i.e., computing a duration. These data are then used to fit the duration models inside each state, e.g., using Maximum Likelihood Estimation.\\n\\n3.3. Sampling Models\\n\\nTo generate an activity matrix \\\\( Y \\\\), the model is initialised at a starting state (e.g., silent state), sampling the duration from the state distribution \\\\( P_s(D = d; \\\\Theta_s) \\\\) and then sampling the next state based on the transition weight. The model then transitions to this state, and a new duration is sampled. The process is repeated until a desired number of samples have been generated.\\n\\nFor the independent model, this process can be done in parallel across the sub-models.\"}"}
{"id": "deadman22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of the fully-connected model with the independent model. Computed using transcript, no voice activity detection. Monte Carlo estimation using 500 samples to estimate mean overlap for the models. \\n\\n| # People Speaking | CHiME-5 | Fully-connected | Independent |\\n|-------------------|---------|----------------|-------------|\\n|                   | 0%      | 19%            | 6%          |\\n|                   | 1%      | 42%            | 25%         |\\n|                   | 2%      | 20%            | 37%         |\\n|                   | 3%      | 5%             | 25%         |\\n|                   | 4%      | 2%             | 6%          |\\n\\n3.4. Model Comparison\\n\\nWe will briefly compare the independent speaker model and the fully-connected model with respect to generation power. Generating from the competing speaker model is not as straightforward. This is due to the competing model using sub-models which are sampled independently, but the data it generates does not have an independent meaning. The results in Table 1 show the distribution of the CHiME-5 overlap according to the transcript. For each session in the dataset, a $Y$ is computed using $f_s = 100$ and a Wald distribution [25] is used for the time distribution ($P_s$) in the states. A model is fit on the session, and then data is generated matching the original session length. The overlap statistics are then computed. The process is repeated 500 times in order to estimate the mean statistics of the overlap. The numbers reported in the table are the average of this across all the parties. From the table, we can see that the independent model produces a larger expected number of speakers than the original dataset. It is also mismatched with respect to each of the number of people speaking at one time. The fully-connected model still produces more overlapped data, but it is far closer to the training data. Although the model is not directly trained on the overlap distribution, it successfully approximates the degree of overlap observed in CHiME-5.\\n\\n4. Party representations\\n\\nUsing the Markov-models we can compute features from the parameters learnt from the data. For the independent generators, features are computed for each model and then concatenated together, creating a larger representation.\\n\\n4.1. Extracting Features from Models\\n\\nGiven a transition matrix for a Markov-model, a steady-state distribution can be computed to give the probability of being in each of the states [26], $\\\\phi_{\\\\text{state}} = \\\\left(\\\\left(Q^T Q\\\\right)^{-1} \\\\mathbf{1}\\\\right)^T$ (2)\\n\\nwhere $Q = \\\\left[T - I, \\\\mathbf{1}\\\\right]$ and $\\\\mathbf{1}$ is a column vector of ones with length $|S|$. From the steady-state, we can compute a distribution over the transitions to give the probability of a transition occurring, $\\\\Phi_{\\\\text{trans}}_{ij} = \\\\phi_{\\\\text{state}}_i T_{ij}$ (3)\\n\\nwhich is flattened into a vector $\\\\phi_{\\\\text{trans}} \\\\in \\\\mathbb{R}^{|S||S|}$. Finally, using the state time distributions, an expected duration in the state can be computed, $\\\\phi_{\\\\text{dur}} = \\\\phi_{\\\\text{state}} \\\\odot \\\\left[E\\\\[P_{S_1}\\\\],...,E\\\\[P_{S_{|S|}}\\\\]\\\\right]^T$ (4)\\n\\nwhere $\\\\odot$ is the Hadamard product.\\n\\n4.2. Visualisation\\n\\nTo illustrate the embeddings, we will train models on a real-world dataset, CHiME-5. For visualisation, we train the fully-connected model on 40-minute chunks with an overlap of 5 minutes using a sample rate $f_s = 100$ and Wald for state distribution $P_s$. The speaker IDs are assigned based on activity, i.e., $P_1$ is always the most active, and $P_K$ is the least. This is done to mitigate the effect of speaker ordering being arbitrary but affecting the feature order. The plot in Figure 2 shows a t-SNE representation of $[\\\\phi_{\\\\text{state}}; \\\\phi_{\\\\text{trans}}; \\\\phi_{\\\\text{dur}}]^T$. The plot illustrates how sessions in CHiME-5 form clusters, i.e., the statistics of the parties remain fairly consistent. The size of the points in the plot are derived from their position in time in the party, i.e., showcasing the movements of the points around the space over time. The plot illustrates some interesting behaviour amongst parties. $S07$ and $S21$ exhibit homogeneous behaviour throughout the parties, whilst other parties show a more varied distribution.\\n\\n5. Evaluation\\n\\nNow that a representation has been described, we will evaluate the representation with respect to the task of speaker extraction. That is, we aim to investigate how SI-SDR [27] changes with respect to the location of parties in the embedding space.\\n\\nSpeaker extraction involves extracting the speech signal from the desired speaker in a mixture of zero or more other speakers and noise. The target speaker is indicated to a model through an enrollment. For this work, an utterance from a speaker is used that is not present in the mixture.\\n\\n5.1. Target-speaker Extraction Model\\n\\nFor evaluation, time-domain Speakerbeam model [28] will be used. The model is based on Conv-Tasnet [29] with an additional component to learn a speaker embedding encoder jointly. The model is trained on Libri2Mix with WHAM noise [30]. The advantage of target-speaker extraction as an evaluation task is that the model works with mixtures with more than two speakers, which will be the case with the mixtures generated from the models.\"}"}
{"id": "deadman22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.2. Data Generation\\n\\nIn total, 506 chunks are created using CHiME-5, and AMI [31] transcript data. Chunk lengths are again 40-minutes with a 5-minute overlap. The AMI data is reduced to only sessions containing four speakers and at least 40 minutes in duration. The fully-connected model is then fit on the chunks to generate the data, again a Wald distribution is used for states ($P_s$). To generate the mixture data, first, the activity matrix $Y$ is generated from the fully-connected model, starting the model at the silent state ($\\\\emptyset$). For this evaluation, $Y$ contains 2.5 hours of activity for each chunk being evaluated.\\n\\nThe large $Y$ matrix is broken into chunks 1 minute in length. Utterances from LibriSpeech (dev-clean) are then placed onto these segments such that they minimise the difference between LibriSpeech utterance duration and turn-generated duration. This process is repeated for all the chunks, and care is taken such that mixtures do not contain speakers overlapping themselves. If the error between the segments and LibriSpeech utterances is too large, it is discarded.\\n\\nThe 1-minute chunks are then broken into segmented mixtures. Each of the utterances in the chunk becomes a mixture where the duration is the length of the target signal. This allows for the steady-state of this learnt turn-taking behaviour to be evaluated, i.e., the fully-connected model has been shown to accurately produce turn-taking behaviour with approximate statistics to the learnt data. Therefore generating from this model allows for statistically stationary turn-taking data to be generated.\\n\\n5.3. Results\\n\\nEach of the models is evaluated using the 2.5 hours of turn-taking generated from the models. A visualisation of the results is shown in Figure 3. The plot illustrates the average performance of the target-speaker extraction for data generated by each of the models. The t-SNE representation shows a clear trend where the parties in the top left of the space start with a low SI-SDR and gradually go into higher SI-SDR. We can then compare this with the performance of the speaker extraction scores, where models in the middle of the space provide the largest gains. Models in the top left are too noisy for the extraction to perform well, and models in the bottom right are already too clean, and any enhancement attempt has resulted in signal degradation. Such visualisation gives an overall picture of the performance of an extraction system across a range of parties.\\n\\nNext, to evaluate the efficacy of the features, we use the application of predicting the SI-SDR scores. A Support Vector Regressor [32] is trained with an radial-basis function kernel using combinations of features presented in Section 4. The 506 samples are evaluated using a bootstrap cross-fold validation where 80% sample of sessions is used as training data, and the other 20% is used for the test. Where in each fold all the chunks in a session are evaluated after being trained on all other chunks, we then report Pearson correlation and root mean squared statistics in Table 2; this is the average across 100 repeats of this sampling procedure. The results show all the features provide information in predicting SI-SDR with a fairly strong correlation when using all the features together. The result indicates that the embeddings can provide a method of extracting how difficult a mixture can be when the generator is known. In addition to this, the table also shows the performance of the independent generator models at predicting the SI-SDR. The features of the fully-connected model are able to predict the performance of SI-SDR well. The competing model is able to surpass the performance of the fully-connecting model. This may be attributed to the fact that the full model is over parameterised for the embedding task but well suited for the generation task. The independent model lacks representation power. Each of the Markov-models inside of the independent models contain transitions which all have a weight $T_{ij}=1$, therefore $\\\\phi_{state}$ and $\\\\phi_{trans}$ contain no information.\\n\\n6. Conclusions\\n\\nIn this work, we have presented a simple Markov-model approach for generating arbitrary large datasets with statistical behaviour approximating the behaviour of people in real parties in terms of overlap. From these models, we showed features that can be computed to give a vector representation of a party. Using the CHiME-5 dataset, we illustrated that sessions within the dataset provided homogeneous behaviour and clustering. These embeddings were then evaluated within the task of target-speaker extraction, where they were shown to have an interpretable meaning with respect to the performance of SI-SDR improvement. This was evaluated with respect to a prediction task based solely on the embeddings. We have shown that treating people as independent generators does not provide a realistic way of creating turn-taking behaviour, and it does not provide an adequate way of building a representation to predict difficulty. The fully-connected model was shown to be the best approach for generating turn-taking. The competing model was shown to surpass the performance of the fully-connected model as a representation.\"}"}
{"id": "deadman22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] J. Barker, S. Watanabe, E. Vincent, and J. Trmal, \u201cThe fifth CHiME Speech separation and recognition challenge: Dataset, task and baselines,\u201d in Interspeech 2018-19th Annual Conference of the International Speech Communication Association, 2018.\\n\\n[2] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang, S. Khudanpur, V. Manohar, D. Povey, D. Raj et al., \u201cCHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,\u201d arXiv preprint arXiv:2004.09249, 2020.\\n\\n[3] J. Deadman and J. Barker, \u201cSimulating realistically-spatialised simultaneous speech using video-driven speaker detection and the CHiME-5 dataset,\u201d in Interspeech 2020, 2020, pp. 349\u2013353.\\n\\n[4] \u2014\u2014, \u201cImproved simulation of realistically-spatialised simultaneous speech using multi-camera analysis in the CHiME-5 dataset,\u201d in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, May 2022.\\n\\n[5] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, \u201cDeep clustering: Discriminative embeddings for segmentation and separation,\u201d in ICASSP 2016 - 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 31\u201335.\\n\\n[6] L. Drude, J. Heitkaemper, C. Boeddeker, and R. Haeb-Umbach, \u201cSMS-WSJ: Database, performance measures, and baseline recipe for multi-channel source separation and recognition,\u201d arXiv preprint arXiv:1910.13934, 2019.\\n\\n[7] J. Cosentino, M. Pariente, S. Cornell, A. Deleforge, and E. Vincent, \u201cLibrimix: An open-source dataset for generalizable speech separation,\u201d arXiv preprint arXiv:2005.11262, 2020.\\n\\n[8] H. Sato, T. Ochiai, M. Delcroix, K. Kinoshita, T. Moriya, and N. Kamo, \u201cShould we always separate?: Switching between enhanced and observed signals for overlapping speech recognition,\u201d arXiv preprint arXiv:2106.00949, 2021.\\n\\n[9] T. Menne, I. Sklyar, R. Schl\u00fcter, and H. Ney, \u201cAnalysis of deep clustering as preprocessing for automatic speech recognition of sparsely overlapping speech,\u201d arXiv preprint arXiv:1905.03500, 2019.\\n\\n[10] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, and J. Li, \u201cContinuous speech separation: Dataset and analysis,\u201d in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7284\u20137288.\\n\\n[11] T. Nakatani, C. Boeddeker, K. Kinoshita, R. Ikeshita, M. Delcroix, and R. Haeb-Umbach, \u201cJointly optimal denoising, dereverberation, and source separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020.\\n\\n[12] H. Bredin and A. Laurent, \u201cEnd-to-end speaker segmentation for overlap-aware resegmentation,\u201d arXiv preprint arXiv:2104.04045, 2021.\\n\\n[13] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and S. Watanabe, \u201cEnd-to-end neural speaker diarization with self-attention,\u201d in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 296\u2013303.\\n\\n[14] E. A. Schegloff, \u201cOverlapping talk and the organization of turn-taking for conversation,\u201d Language in society, vol. 29, no. 1, pp. 1\u201363, 2000.\\n\\n[15] G. Skantze, \u201cTurn-taking in conversational systems and human-robot interaction: A review,\u201d Computer Speech & Language, vol. 67, p. 101178, 2021.\\n\\n[16] E. Ekstedt and G. Skantze, \u201cTurnGPT: A transformer-based language model for predicting turn-taking in spoken dialog,\u201d arXiv preprint arXiv:2010.10874, 2020.\\n\\n[17] E. G. Padilha, \u201cModelling turn-taking in a simulation of small group discussion,\u201d Ph.D. dissertation, University of Edinburgh, 2006.\\n\\n[18] E. Trentin and M. Gori, \u201cA survey of hybrid ANN/HMM models for automatic speech recognition,\u201d Neurocomputing, vol. 37, no. 1-4, pp. 91\u2013126, 2001.\\n\\n[19] A. Raux and M. Eskenazi, \u201cA finite-state turn-taking model for spoken dialog systems,\u201d in Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics on - NAACL '09, 2009, pp. 629\u2013637.\\n\\n[20] D. Garcia-Romero and C. Y. Espy-Wilson, \u201cAnalysis of i-vector length normalization in speaker recognition systems,\u201d in Interspeech 2011, 2011.\\n\\n[21] W. Campbell, D. Sturim, and D. Reynolds, \u201cSupport vector machines using GMM supervectors for speaker verification,\u201d IEEE Signal Process Lett., vol. 13, no. 5, pp. 308\u2013311, 2006.\\n\\n[22] R. Kuhn, J.-C. Junqua, P. Nguyen, and N. Niedzielski, \u201cRapid speaker adaptation in eigenvoice space,\u201d IEEE Transactions on Speech and Audio Processing, vol. 8, no. 6, pp. 695\u2013707, 2000.\\n\\n[23] A. Senior and I. Lopez-Moreno, \u201cImproving DNN speaker independence with i-vector inputs,\u201d in ICASSP 2014 - 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 225\u2013229.\\n\\n[24] J. Janssen and N. Limnios, Semi-Markov Models and Applications. Springer US, 1999.\\n\\n[25] V. Seshadri, The Inverse Gaussian Distribution. Springer New York, 1999, vol. 95.\\n\\n[26] P. A. Gagniuc, Markov chains: from theory to implementation and experimentation. John Wiley & Sons, 2017.\\n\\n[27] J. L. Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, \u201cSDR \u2013 half-baked or well done?\u201d in ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 626\u2013630.\\n\\n[28] M. Delcroix, T. Ochiai, K. Zmolikova, K. Kinoshita, N. Tawara, T. Nakatani, and S. Araki, \u201cImproving speaker discrimination of target speech extraction with time-domain speakerbeam,\u201d in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 691\u2013695.\\n\\n[29] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing ideal Time\u2013Frequency magnitude masking for speech separation,\u201d IEEE/ACM transactions on audio, speech, and language processing, vol. 27, no. 8, pp. 1256\u20131266, 2019.\\n\\n[30] G. Wichern, J. Antognini, M. Flynn, L. R. Zhu, E. McQuinn, D. Crow, E. Manilow, and J. L. Roux, \u201cWHAM!: Extending speech separation to noisy environments,\u201d arXiv preprint arXiv:1907.01160, 2019.\\n\\n[31] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal et al., \u201cThe AMI meeting corpus: A pre-announcement,\u201d in International workshop on machine learning for multimodal interaction. Springer, 2005, pp. 28\u201339.\\n\\n[32] M. Awad and R. Khanna, \u201cSupport vector regression,\u201d in Efficient learning machines. Springer, 2015, pp. 67\u201380.\"}"}
