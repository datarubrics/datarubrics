{"id": "8aHzds2uUyB", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Evaluation of GPT2 with different algorithms on IMDB sentiment text continuation task, NLPO hyperparameter ablations: Mean and standard deviations over 5 random seeds is reported for sentiment scores along with fluency and diversity metrics. This table shows results of NLPO's stability to the unique hyperparameters introduced in the algorithm\u2014all other parameters held constant from the best PPO model. The number of iterations after which the masking model syncs with the policy and the top-p nucleus percentage for the mask model itself. We see that in general, the higher the top-p mask percentage, the better the performance. For target update iterations, performance is low if the mask model is not updated often enough or if it updated too often.\\n\\n| Algorithm       | Unique N | Coherence Value | SENTIMENT VALUE | Alpha | Skew |\\n|-----------------|----------|-----------------|-----------------|-------|------|\\n| NLPO with KL    | 27       | 3.49            | 0.196           | 3.497 | 3.61 |\\n| NLPO without KL | 29       | 3.16            | 0.21            | 3.158 | 4.41 |\\n| PPO without KL  | 27       | 3.16            | 0.17            | 3.163 | 4.36 |\\n| PPO with KL     | 29       | 3.46            | 0.124           | 3.462 | 3.58 |\\n| Zero Shot       | 28       | 3.6             | 0.162           | 3.591 | 3.1  |\\n| Supervised      | 29       | 3.51            | 0.192           | 3.512 | 3.43 |\\n| Human           | 27       | 4.13            | 0.159           | 4.128 | 3.01 |\\n| Supervised+PPO  | 22       | 3.45            | 0.211           | 3.147 | 3.64 |\\n| Supervised+NLPO | 22       | 3.48            | 0.181           | 3.226 | 3.73 |\\n\\nTable 9: Results of the human subject study showing the number of participants N, average Likert scale value for coherence and sentiment, Krippendorf's alpha showing inter-annotator agreement, and Skew. For each model a total of 100 samples were drawn randomly from the test set and rated by 3 annotators each, resulting in 300 data points per algorithm.\"}"}
{"id": "8aHzds2uUyB", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Instructions, example, and interface for the IMDB sentiment completion task.\"}"}
{"id": "8aHzds2uUyB", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm       | Coherence | Sentiment | Diff (G2-G1) | p-values | Diff (G2-G1) | p-values |\\n|----------------|-----------|-----------|--------------|----------|--------------|----------|\\n| PPO with KL    | -0.30     | 0.035     | 0.783        | 0.001    | -0.035       | 0.001    |\\n| NLPO with KL   | 0.03      | 0.9       | -0.9        | 0.027    | 0.04         | 0.9      |\\n| PPO with KL    | -0.30     | 0.035     | 0.827        | 0.001    | -0.035       | 0.001    |\\n| Supervised     | 0.05      | 0.9       | -0.15        | 0.591    | -0.05        | 0.9      |\\n| Human          | 0.67      | 0.001     | -0.567       | 0.001    | 0.67         | 0.001    |\\n| Zero Shot      | 0.14      | 0.776     | -0.483       | 0.001    | -0.14        | 0.776    |\\n| PPO without KL | 0.33      | 0.013     | 0.8          | 0.001    | 0.33         | 0.013    |\\n| NLPO without KL| 0.001     | 0.9       | 0.043        | 0.9      | 0.001        | 0.9      |\\n| Supervised     | 0.35      | 0.006     | -0.94        | 0.001    | 0.35         | 0.006    |\\n| Human          | 0.97      | 0.009     | -1.393       | 0.001    | 0.97         | 0.009    |\\n| Zero Shot      | 0.44      | 0.001     | -1.31        | 0.001    | 0.44         | 0.001    |\\n\\nTable 10: Results of an post-hoc Tukey HSD Test for difference in means between pairs of algorithms (Group 2 - Group 1) and corresponding p-values. Individually statistically significant results are bolded and are used to discuss results in the analysis. Overall p-values showing that there is a significant difference in means between the models via a one-way ANOVA test are significant with $p \\\\ll 0.05$ for both coherence and sentiment.\"}"}
{"id": "8aHzds2uUyB", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We show sample generations from each of the algorithms for three randomly picked prompts below.\\n\\n**Sample 1**\\nPrompt: A man and his wife are not getting along because of the wife's jealousy and drinking problems. When the wife disappears, her sister begins a vigorous search involving the husband's sister, which leads several men to a remote cabin and then a boat. The wife and her husband then start to see the same things. Finally, a strange woman is driving out to her cabin and discovers the husband's disappearance but references: local police, state government, and a television crime show. What she finds out stuns the entire community. Good drama with lots of fine performances; based on a true story.\\n\\n**Sample 2**\\nPrompt: A very credible and unsettling movie portraying the effects of war on the general public, Shame gives us a chilling understanding of Ingmar Bergman's beliefs. The absence of color and sound (as in a soundtrack) helps to give the film a more realistic feeling. There is no soft or hard lighting or a variety of camera angles to give the story a charming or dramatic effect. Everything is very simply portrayed as it would be in reality. At times the storyline was confusing, but perhaps in order to give a sense of the characters' frame of mind how they too do now know what is happening or why. The simplicity of the camera work is at once apparent and fascinating in the manner in which it affects the way things are portrayed. In other films these two have been referred to from time to time in a form and manner more suited to the screen. The film shows film made it somewhat boring, but I could understand how the director is trying not to glamorize the story and points. He depicted the harsh and unsettling events and effects of war. In the beginning the characters were introduced and shown as content with the lighter settings characters relationships is so engrossing and the great and unique story presented by each pair of brothers and sisters creates a truly gripping and enjoyable experience. To put it simply, I strongly recommend to everyone who plays the role to buy this DVD!!!\\n\\n**Sample 3**\\nPrompt: Shakespearean language is wild and rambling, saturated in multiple meanings, word play and metaphor. To be understood it must be wrangled and tamed by an actor with the strength and knowledge to do so. When an actor fails, the words pour forth in a torrent of incomprehensible words, but when he succeeds, the English language springs to life with an immediacy and vibrancy that takes your breath away. Al Pacino is one such actor, and here displays an incredible level of clarity and control that, were there any justice, might still be seen in the world today. This movie, like \\\"Vacation\\\", is an effort at telling the tale of two families from different races. One is African but never grew up the other. One of them, a man would sweep every award in the offering. He meets the challenge of presenting Shylock head on, and delivers an extraordinarily subtle and nuanced performance. It would be a crime if we never got the opportunity to see what he does with King Lear.<br />\\n\\nThe supporting cast would have made him an instant-classic. This movie captures the spirit of childhood and has everyone in the world watching it. It's a love story and it's really heartbreaking. And at the same time, it's a classic. It...\"}"}
{"id": "8aHzds2uUyB", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP?\\n\\nTo help answer this, we first introduce an open-source modular library, RL4LMs, for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al., 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference. GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization) that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al., 2017)), based on both automatic and human evaluations.\\n\\nINTRODUCTION\\n\\nThe ultimate aim of language technology is to interact with humans. However, most language models are trained without direct signals of human preference, with supervised target strings serving as (a sometimes crude) proxy. One option to incorporate user feedback is via human-in-the-loop, i.e., a user would be expected to provide feedback for each sample online as the model trains, but this degree of dense supervision is often prohibitive and inefficient. Automated metrics offer a promising compromise: models of human preference like pairwise learned preference models (Ouyang et al., 2022), BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020) have significantly improved correlation with human judgment compared to earlier metrics (BLEU, METEOR, etc.), and are cheap to evaluate. But \u2014 these functions are usually not per-token differentiable: like humans, metrics...\"}"}
{"id": "8aHzds2uUyB", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Natural Language Policy Optimization (NLPO) in the case of sentiment-guided continuation. Here, the LM (i.e., the policy) needs to produce a positive sentiment continuation given a review prompt (we cover other models of human preference in Sec. 3.2). Two objectives are balanced:\\n1) an automated proxy of human preference that serves as a reward (here: a sentiment classifier); and 2) \u201cnaturalness\u201d as measured by a KL divergence from an LM not trained with explicit human feedback. The plots show validation learning curves comparing our NLPO to the popular policy gradient method PPO. (Top plot:) RL methods can easily achieve high reward if the KL penalty is removed, (Bottom:) but at the cost of higher perplexity. NLPO+KL, our proposed approach, succeeds in balancing reward and naturalness more effectively than prior work.\\n\\nTo facilitate research in building RL algorithms to better align LMs, we release a library, a benchmark, and an algorithm. First, we release the RL4LMs library, which enables generative HuggingFace models (e.g., GPT-2 or T5) to be trained using a variety of existing RL methods like PPO/A2C/etc. Next, we apply models trained using RL4LMs to the new GRUE (General Reinforced-language Understanding Evaluation) benchmark: GRUE is a collection of 7 contemporary NLP tasks (see Table 1 for details); in contrast to other benchmarks, instead of supervised training, we pair each task with reward function(s). GRUE challenges models to optimize these reward functions while remaining fluent language generators. We train language models via RL\u2014both with and without task specific supervised pre-training\u2014to optimize rewards. Finally, beyond existing RL methods, we introduce a novel on-policy RL algorithm called NLPO (Natural Language Policy Optimization), that dynamically learns task-specific constraints over the distribution of language at a token level. Experiments on GRUE and human evaluations show that NLPO better balances learning preference rewards while maintaining language fluency compared to alternatives, including PPO (Figure 1). We find that using RL to learn from scalar reward feedback can be more: (1) data efficient than using additional expert demonstrations via supervised learning (though a combination of both is best)\u2014a learned reward function enables greater performance when used as a signal for an RL method than a supervised method trained with 5 times more data, and (2) parameter efficient\u2014enabling a 220 million parameter model trained with a combination of supervision and NLPO to outperform a 3 billion supervised model. We hope that the benchmarks, baselines, and building blocks we release serve to drive forward research in aligning LMs to human preferences.\"}"}
{"id": "8aHzds2uUyB", "page_num": 51, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 28 presents our benchmarking results with 2 reward functions along with supervised baseline performances on the NarrativeQA test set. Similar to other methods, our main finding is that warm-started initial policies are crucial for learning to generate answers that successfully use the input context.\\n\\nB.7.3 Qualitative Results\\n\\nWe show sample generations from each of the algorithms for three randomly picked prompts below.\\n\\nSample 1\\nPrompt: who is mark hunter? mark hunter (slater), a high school student in a sleepy suburb of phoenix, arizona, starts an fm pirate radio station that broadcasts from the basement of his parents' house. mark is a loner, an outsider, whose only outlet for his teenage angst and aggression is his unauthorized radio station. his pirate station's theme song is \\\"everybody knows\\\" by leonard cohen and there are glimpses of cassettes by such alternative musicians as the jesus and mary chain, camper van beethoven, primal scream, soundgarden, ice-t, bad brains, concrete blonde, henry rollins, and the pixies. by day, mark is seen as a loner, hardly talking to anyone around him; by night, he expresses his outsider views about what is wrong with american society. when he speaks his mind about what is going on at his school and in the community, more and more of his fellow students tune in to hear his show.nobody knows the true identity of \\\"hard harry\\\" or \\\"happy harry hard-on,\\\" as mark refers to himself, until nora diniro (mathis), a fellow student, tracks him down and confronts him the day after a student named malcolm commits suicide after harry attempts to reason with him. the radio show becomes increasingly popular and influential after harry confronts the suicide head-on, exhorting his listeners to do something about their problems instead of surrendering to them through suicide at the crescendo of his yelled speech, an overachieving student named paige woodward (who has been a constant listener) jams her various medals and accolades into a microwave and turns it on. she then sits, watching the awards cook until the microwave explodes, injuring her. while this is happening, other students act out in cathartic release.eventually, the radio show causes so much trouble in the community that the fcc is called in to investigate. during the fracas, it is revealed that the school's principal (annie ross) has been expelling \\\"problem students,\\\" namely, students with below-average standardized test scores, in an effort to boost the district's test scores while still keeping their names on the rolls (a criminal offense) in order to retain government funding.realizing he has started something huge, mark decides it is up to him to end it. he dismantles his radio station and attaches it to his mother's old jeep, creating a mobile transmitter so his position can't be triangulated. pursued by the police and the fcc, nora drives the jeep around while mark broadcasts. the harmonizer he uses to disguise his voice breaks, and with no time left to fix it, mark decides to broadcast his final message as himself. they finally drive up to the crowd of protesting students, and mark tells them that the world belongs to them and that they should make their own future. the police step in and arrest mark and nora. as they are taken away, mark reminds the students to \\\"talk hard.\\\" as the film ends, the voices of other students (and even one of the teachers) speak as intros for their own independent stations, which can be heard broadcasting across the country.\\n\\nZero-shot: hunter (slater) is a high school student in phoenix, arizona. he starts an fm pirate radio station that broadcasts from the basement of his\\n\\nReferences: he is a high school student in phoenix 2) loner and outsider student with a radio station\\n\\nPPO: hunter (slater), a high school student in phoenix, arizona, starts an fm pirate radio station that broadcasts from the basement of his parents' house\\n\\nNLPO: hunter (slater), a high school student in phoenix, arizona, starts an fm pirate radio station that broadcasts from the basement of his parents' house\\n\\nSupervised: a high school student\\nSupervised + PPO: a high school student\\nSupervised + NLPO: a high school student\"}"}
{"id": "8aHzds2uUyB", "page_num": 52, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"accolades into a microwave and turns it on. She then sits, watching the awards cook until the microwave explodes, injuring her. While this is happening, other students act out in cathartic release. Eventually, the radio show causes so much trouble in the community that the FCC is called in to investigate. During the fracas, it is revealed that the school\u2019s principal (Annie Ross) has been expelling \u201cproblem students,\u201d namely, students with below-average standardized test scores, in an effort to boost the district\u2019s test scores while still keeping their names on the rolls (a criminal offense) in order to retain government funding. Realizing he has started something huge, Mark decides it is up to him to end it. He dismantles his radio station and attaches it to his mother\u2019s old jeep, creating a mobile transmitter so his position can\u2019t be triangulated. Pursued by the police and the FCC, Nora drives the jeep around while Mark broadcasts. The harmonizer he uses to disguise his voice breaks, and with no time left to fix it, Mark decides to broadcast his final message as himself. They finally drive up to the crowd of protesting students, and Mark tells them that the world belongs to them and that they should make their own future. The police step in and arrest Mark and Nora. As they are taken away, Mark reminds the students to \u201ctalk hard.\u201d As the film ends, the voices of other students (and even one of the teachers) speak as intros for their own independent stations, which can be heard broadcasting across the country.\\n\\nReferences:\\n1) expelling them. 2) expelling the students\"}"}
{"id": "8aHzds2uUyB", "page_num": 53, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We pick two languages, English and German, and frame this task similarly to other machine translation tasks\u2014requiring the models to translate from English to German. We train models on 4 rewards: SacreBLEU, chRF, TER, and BertScore.\\n\\n| Model Params value | supervised batch size: | 64 |\\n|--------------------|-------------------------|----|\\n|                    | epochs:                 | 5  |\\n|                    | learning rate:          | 0.00001 |\\n|                    | learning rate scheduler: | constant |\\n|                    | weight decay:           | 0.1 |\\n| ppo/nlpo steps per update: | 5120 |\\n| total number of steps: | 256000 |\\n| batch size:        | 64                      |\\n| epochs per update: | 5                       |\\n| learning rate:     | 0.0000005                |\\n| entropy coefficient: | 0.0                      |\\n| initial kl coeff:  | 0.001                    |\\n| target kl:         | 0.2                      |\\n| discount factor:    | 0.99                     |\\n| gae lambda:         | 0.95                     |\\n| clip ratio:         | 0.2                      |\\n| rollouts top k:     | 10                       |\\n| value function coeff: | 0.5                      |\\n| top mask ratio:     | 0.5                      |\\n| target update iterations: | 20 |\\n| supervised+ ppo (or nlpo) steps per update: | 2560 |\\n| total number of steps: | 256000 |\\n| batch size:        | 64                      |\\n| epochs per update: | 5                       |\\n| learning rate:     | 0.0000005                |\\n| entropy coefficient: | 0.0                      |\\n| initial kl coeff:  | 0.001                    |\\n| target kl:         | 0.2                      |\\n| discount factor:    | 0.99                     |\\n| gae lambda:         | 0.95                     |\\n| clip ratio:         | 0.2                      |\\n| rollouts top k:     | 10                       |\\n| value function coeff: | 0.5                      |\\n| top mask ratio:     | 0.5                      |\\n| target update iterations: | 20 |\\n| decoding num beams: | 4                       |\\n| length penalty:     | 0.6                      |\\n| max new tokens:     | 128                      |\\n| tokenizer padding side: | left                  |\\n| truncation side:    | right                    |\\n| max length:        | 128                      |\\n\\nTable 29: NMT Hyperparams: Table shows a list of all hyper-parameters and their settings.\\n\\nTables 30, 31 presents our benchmarking results with 4 reward functions along with supervised baseline performances on test set. Our main finding is that NLPO + Supervised performs better than PPO and supervised models.\"}"}
{"id": "8aHzds2uUyB", "page_num": 54, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Datasets\\n\\nLexical and Semantic Metrics\\n\\n| Algorithm | SacreBLEU | chRF | TER | BertScore |\\n|-----------|-----------|------|-----|-----------|\\n| Zero-Shot T5 | 0.635 | 0.414 | 0.591 | 0.591 |\\n| PPO T5 | 0.636 | 0.415 | 0.591 | 0.591 |\\n| Supervised T5 | 0.635 | 0.411 | 0.590 | 0.590 |\\n| Supervised + PPO T5 | 0.640 | 0.416 | 0.595 | 0.595 |\\n| Supervised + NLPO T5 | 0.642 | 0.419 | 0.596 | 0.596 |\\n\\n| Algorithm | SacreBLEU | chRF | TER | BertScore |\\n|-----------|-----------|------|-----|-----------|\\n| Zero-Shot T5 | 0.619 | 0.386 | 0.588 | 0.587 |\\n| PPO T5 | 0.621 | 0.383 | 0.587 | 0.587 |\\n| Supervised T5 | 0.638 | 0.400 | 0.610 | 0.609 |\\n| Supervised + PPO T5 | 0.640 | 0.407 | 0.610 | 0.610 |\\n| Supervised + NLPO T5 | 0.641 | 0.418 | 0.614 | 0.614 |\\n\\nTable 30: WMT-16 and IWSLT test evaluation - lexical and semantic metrics for RL algorithms with different reward functions bench-marked against supervised baseline models.\"}"}
{"id": "8aHzds2uUyB", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As a representative of the summarization task, we consider CNN/DM dataset consisting of long news articles and their highlights written by news authors. The dataset consists of 287k training, 13k validation and 11k test examples. We trained RL methods using 3 different automated metrics, namely Rouge-1, Rouge-avg and Meteor. We chose T5 as our base LM as it is pre-trained in a unified text-to-text framework and relishes Zero-Shot capabilities. For decoding, we use multinomial sampling with a temperature of 0.7 for all the models.\\n\\n**Table 16:** CNN/DM Hyperparams\\n\\n| Parameter | Value |\\n|-----------|-------|\\n| Supervised batch size | 16 |\\n| Epochs | 2 |\\n| Learning rate | 0.0001 |\\n| Learning rate scheduler | cosine |\\n| Weight decay | 0.1 |\\n| PPO/ NLPO steps per update | 5120 |\\n| Total number of steps | 512000 |\\n| Batch size | 64 |\\n| Epochs per update | 5 |\\n| Learning rate | 0.000002 |\\n| Entropy coefficient | 0.0 |\\n| Initial kl coeff | 0.001 |\\n| Target kl | 0.2 |\\n| Discount factor | 0.99 |\\n| GAE lambda | 0.95 |\\n| Clip ratio | 0.2 |\\n| Value function coeff | 0.5 |\\n| Rollouts top k | sweep of (50, 100) |\\n| Top mask ratio | 0.9 |\\n| Target update iterations | sweep of (10, 20, 30) |\\n| Supervised+PPO/ NLPO steps per update | 5120 |\\n| Total number of steps | 256000 |\\n| Batch size | 64 |\\n| Epochs per update | 5 |\\n| Learning rate | 0.000002 |\\n| Entropy coefficient | 0.0 |\\n| Initial kl coeff | 0.001 |\\n| Target kl | 0.2 |\\n| Discount factor | 0.99 |\\n| GAE lambda | 0.95 |\\n| Clip ratio | 0.2 |\\n| Value function coeff | 0.5 |\\n| Rollouts top k | sweep of (50, 100) |\\n| Top mask ratio | 0.9 |\\n| Target update iterations | sweep of (10, 20, 30) |\\n| Decoding sampling | True |\\n| Temperature | 0.7 |\\n| Min length | 50 |\\n| Max new tokens | 100 |\\n| Tokenizer padding side | left |\\n| Truncation side | right |\\n| Max length | 512 |\\n\\n**Table 17:** Benchmarking results on test set reporting a wide range of metrics: lexical, semantic, factual correctness and diversity metrics. As baselines, we report lead-3 which selects first three sentences as the summary, Zero-Shot and a supervised model. PPO and NLPO models are on par with supervised performance on several metrics including Rouge-2, Rouge-L, and Bleu. On fine-tuning on top of supervised model, performance improves consistently on all metrics indicating that RL fine-tuning is beneficial. Another interesting finding is that, RL fine-tuned models are factually consistent as measured by SummaCZS metric. For ablations on PPO params, NLPO params, we refer to Tables 18,19.\"}"}
{"id": "8aHzds2uUyB", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method                  | Rouge-1 | Rouge-2 | Rouge-L | Meteor | BLEU | BertScore | Mean Output Length |\\n|------------------------|---------|---------|---------|--------|------|-----------|--------------------|\\n| CNN/DM                 | 0.401   | 0.175   | 0.250   | 0.363  | 0.333| 0.099     | 0.874              |\\n| Lead-3                 |         |         |         |        |      |           | 0.993              |\\n| Zero-Shot T5           | 0.372   | 0.145   | 0.247   | 0.311  | 0.256| 0.077     | 0.864              |\\n| Supervised T5          | 0.411   | 0.177   | 0.276   | 0.343  | 0.309| 0.108     | 0.876              |\\n| Supervised + PPO       | 0.417   | 0.189   | 0.294   | 0.358  | 0.278| 0.101     | 0.882              |\\n| Supervised + NLPO      | 0.421   | 0.193   | 0.297   | 0.361  | 0.287| 0.108     | 0.882              |\\n\\nTable 17: CNN/Daily Mail test evaluation. Table presents a wide range of metrics: lexical, semantic, factual correctness and diversity metrics on test set. As baselines, we report lead-3 which selects first three sentences as the summary, Zero-Shot and a supervised model. PPO and NLPO models are on par with supervised performance on several metrics including Rouge-2, Rouge-L, and Bleu. On fine-tuning on top of supervised model, performance improves consistently on all metrics indicating that RL fine-tuning is beneficial. Another interesting finding is that, RL fine-tuned models are factually consistent as measured by SummaCZS metric.\"}"}
{"id": "8aHzds2uUyB", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Alg       | Reward Function | Top k (rollout) | Top p (Action mask) | target update n | Rouge-1 | Rouge-2 | Rouge-L | Rouge-LSum | Meteor | BLEU | BertScore |\\n|-----------|-----------------|-----------------|---------------------|-----------------|---------|---------|---------|-------------|--------|------|-----------|\\n| PPO       | 50              | 0.404           | 0.181               | 0.280           | 0.346   | 0.273   | 0.095   | 0.874       |        |      |           |\\n|           | 100             | 0.412           | 0.186               | 0.286           | 0.354   | 0.276   | 0.094   | 0.876       |        |      |           |\\n| Supervised+PPO | 50              | 0.414           | 0.190               | 0.293           | 0.358   | 0.272   | 0.097   | 0.881       |        |      |           |\\n|           | 100             | 0.420           | 0.193               | 0.295           | 0.362   | 0.277   | 0.100   | 0.881       |        |      |           |\\n\\nTable 18: PPO Ablation/Model Selection: Evaluation of PPO models on validation set with different reward functions and top k values for rollouts. For each alg-reward combo, best model (top k) is chosen.\\n\\n| Alg       | Reward Function | Top k (rollout) | Top p (Action mask) | target update n | Rouge-1 | Rouge-2 | Rouge-L | Rouge-LSum | Meteor | BLEU | BertScore |\\n|-----------|-----------------|-----------------|---------------------|-----------------|---------|---------|---------|-------------|--------|------|-----------|\\n| NLPO      | 50              | 0.400           | 0.178               | 0.275           | 0.343   | 0.269   | 0.094   | 0.872       | 0.396  | 0.173| 0.274     |\\n|           | 20              | 0.407           | 0.177               | 0.279           | 0.347   | 0.265   | 0.085   | 0.875       |        |      |           |\\n|           | 30              | 0.405           | 0.180               | 0.279           | 0.347   | 0.269   | 0.091   | 0.875       |        |      |           |\\n| Supervised+NLPO | 50              | 0.400           | 0.178               | 0.279           | 0.344   | 0.266   | 0.087   | 0.874       |        |      |           |\\n|           | 20              | 0.406           | 0.179               | 0.280           | 0.347   | 0.272   | 0.092   | 0.875       |        |      |           |\\n|           | 30              | 0.409           | 0.184               | 0.280           | 0.348   | 0.291   | 0.108   | 0.873       |        |      |           |\\n\\nTable 19: NLPO Ablation/Model Selection: Evaluation of NLPO models on validation set with different reward functions, top k values for rollouts and target update iterations. For each alg-reward combo, best model is chosen.\"}"}
{"id": "8aHzds2uUyB", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 20: Results of the human subject study showing the number of participants $N$, average Likert scale value for coherence and sentiment, Krippendorf\u2019s alpha showing inter-annotator agreement, and Skew. For each model a total of 50 samples were drawn randomly from the test set and rated by 3 annotators each, each resulting in 150 data points per algorithm.\\n\\n| Model                  | $N$ | Coherence Value | Alpha | Skew Value | Alpha | Skew |\\n|------------------------|-----|-----------------|-------|------------|-------|------|\\n| PPO+Supervised         | 22  | 4.21            | 0.198 | 4.224      | 0.256 | 3.98 |\\n| NLPO+Supervised        | 19  | 4.3             | 0.26  | 4.308      | 0.089 | 4    |\\n| Zero Shot              | 17  | 3.73            | 0.1   | 3.757      | 0.25  | 3.722|\\n| Supervised             | 19  | 4.25            | 0.116 | 4.241      | 0.2  | 3.986|\\n| NLPO                   | 17  | 4.03            | 0.13  | 4.042      | 0.191 | 3.832|\\n| PPO                    | 21  | 3.94            | 0.111 | 3.945      | 0.129 | 3.767|\\n| Human                  | 19  | 3.89            | 0.277 | 3.902      | 0.029 | 3.769|\\n\\nTable 21: Results of an post-hoc Tukey HSD Test for difference in means between pairs of algorithms (Group 2 - Group 1) and corresponding $p$-values. Individually statistically significant results are bolded and are used to discuss results in the analysis. Overall $p$-values showing that there is a significant difference in means between the models via a one-way ANOVA test are significant with $p \\\\ll 0.05$ for both coherence and sentiment.\"}"}
{"id": "8aHzds2uUyB", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Summarized results via automated metrics across all 7 GRUE tasks for each of the 5 algorithms we consider, and human participant studies for the 5 tasks suitable for human studies. Test results are averaged over all the respective metrics seen in Table 1.\\n\\nQuestions\\n\\nTasks\\n\\nIMDB\\nCommonGen\\nCNN/DM\\nToTTO\\nWMT16\\nNarrQA\\nDialog\\n\\nNeeds Warm Start\\n\\n\u2717\\n\u2713\\n\u2717\\n\u2713\\n\u2717\\n\u2713\\n\u2717\\n\\nEasily reward hackable?\\n\\n\u2713\\n\u2713\\n\u2717\\n\u2717\\n\u2717\\n\u2717\\n\u2717\\n\\nRL\\n\\n> Sup (auto)?\\n\\n\u2713\\n\u2717\\n\u2717\\n\u2717\\n\u2717\\n\u2717\\n\u2713\\n\\nRL\\n\\n> Sup (human)?\\n\\n\u2713\\n\u2717\\n\u2717\\n\u2717\\n-\\n-\\n\u2713\\n\\nSup+RL\\n\\n> Sup (auto)?\\n\\n\u2713\\n\u2713\\n\u2713\\n\u2713\\n\u2713\\n\u2713\\n\u2717\\n\\nSup+RL\\n\\n> Sup (human)?\\n\\n\u2713\\n\u2717\\n\u2713\\n\u2713\\n-\\n-\\n\u2717\\n\\nSup+NLPO\\n\\n> Sup+PPO (auto)?\\n\\n\u2713\\n\u2713\\n\u2713\\n\u2713\\n\u2713\\n\u2713\\n\u2713\\n\\nSup+NLPO\\n\\n> Sup+PPO (human)?\\n\\n\u2713\\n\u2713\\n\u2713\\n\u2713\\n-\\n-\\n\u2713\\n\\nTable 2: Key questions answered using GRUE + RL4LMs:\\n\\nThis table summarizes the results found in the ablations and Fig. 2 and provides an overview of the questions we ask in Section 5: which tasks require warm starts or are easily reward hackable; when to use RL over Supervised, when to use both; and when to use NLPO over PPO. All conclusions drawn are the result of statistical analysis as discussed in the experimental setup.\\n\\nAblation Sentiment Perplexity\\n\\nZero Shot 0.489 32.171\\n\\nSupervised 0.539 35.472\\n\\nPPO 0.602 33.816\\n\\nNLPO 0.611 33.832\\n\\nWarm Starting (Sec. 5.1)\\n\\nPPO+Supervised 0.626 35.049\\n\\nNLPO+Supervised 0.620 34.816\\n\\nData Budget (Reward trained on 10% of data, Sec. 5.3)\\n\\nPPO 0.598 35.929\\n\\nNLPO 0.599 33.536\\n\\nRemoving NLPO Top-k Constraints (Sec. 5.2) (k = 1 is equivalent to PPO, k = 0 is NLPO)\\n\\nNLPO\\n\\nk = 0.5\\n\\n0.579 32.451\\n\\nk = 0\\n\\n0.588 32.447\\n\\nRemoving KL Constraints (Sec. 5.2)\\n\\nPPO-no-KL 0.838 41.897\\n\\nNLPO-no-KL 0.858 41.429\\n\\nDiscount Ablations (\u03b3 = 1) (Sec. 5.4)\\n\\nPPO 0.651 41.035\\n\\nNLPO 0.624 43.720\\n\\nTable 3: IMDB Ablation Results.\\n\\nT5-base (220m parameter) LM currently outperforms all the models on the ToTTO leaderboard, many of which have \u22653b parameter supervised models\u2014suggesting that RL is parameter efficient as well. In these cases, it is critical that the initial policy already contain (some) signal for the task due to it being used as a KL constraint and masking constraint in NLPO. If the mask contains no initial priors about task specific language, it will be eliminating the wrong actions\u2014a better initial policy leads to better RL performance downstream.\"}"}
{"id": "8aHzds2uUyB", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Human agreement with automated metrics.\\n\\nAs human judgments can be noisy, we run additional statistical analysis such as measuring inter-annotator agreement, via Krippendorf\u2019s alpha score, and using a one-way ANOVA followed by a post-hoc Tukey HSD test to measure if differences in means of average scores between pairs of models are significant. We find that trends in our human evaluations generally match those seen in the automated metrics for both task and naturalness metrics (see Figures 2(c), 2(d) which summarize Appendix Tables 10,15,21,26, 35\u2014Supervised+NLPO \u2265 Supervised \u2265 Supervised+PPO > NLPO > PPO > Zero-shot\u2014with the exception of Supervised outperforming Supervised+PPO on 2 out of 5 tasks when automated metrics would indicate that Supervised+PPO outperforms Supervised on all of the tasks. We draw two conclusions from this: (1) if the generated text is above a certain threshold of naturalness, the automated metrics usually correlate with human judgments; (2) usually but not always as seen in the relative performance of Supervised and Supervised+PPO, potentially indicating reward hacking behaviors undetected by automated metrics but caught by human preference feedback.\\n\\n5.2 Reference Reward Learning, Selection, and Hacking\\n\\nWhile the GRUE benchmark\u2019s metric for each task is an average over several measures, the RL models we trained optimized only a single metric independently. Thus, we can empirically investigate which metric for which GRUE produces the best results. We observe that many possible single metric rewards provide task performance gains over supervised methods (results shown in Fig. 3(a), 2(c) are averaged across these reward functions) with the condition that the text is also coherent and natural.\\n\\nWhich constraints best prevent reward hacking?\\n\\nThe reward function in Equation 1 balances a task-specific reward with a KL constraint \u2014 models are penalized from straying too far from a base LM in their pursuit of high reward (Table 3 and Appendix Table 5) clearly show that if KL constraints are removed entirely, models reward hack). But which model works best as a base regularizing LM? When the initial policy (i.e., the raw, pretrained model) has low performance on the task, the KL penalty pushes the policy towards nonsense, e.g. on CommonGen and ToTTo the trained policy learns to simply repeat portions of the input (as seen in Tables B.4.5, B.6.4). This behavior is mitigated if the base regularizing LM is the supervised model\u2014the reward encourages the policy to balance the task-specific reward and a more reasonable regularization term. Deriving KL penalties from warm-started initial policies is critical for performance on such tasks.\\n\\nPPO vs. NLPO.\\n\\nFigure 2 shows that NLPO generally outperforms PPO and supervised, especially when applied after supervised training. We hypothesize that the primary reason for NLPO\u2019s improved performance and stability is because the masking policy provides an additional constraint for the current policy. This constraint is not based on the initial untuned policy like the KL penalty but of the policy from \\\\(\u00b5\\\\) iterations ago and likely contains more task-relevant information learned during RL training. Table 3 (and Appendix Table 8) shows how performance increases up to a point and then decreases as \\\\(p_{top}\\\\) in top-\\\\(p\\\\) sampling is increased for the masking policy, relaxing the constraint by eliminating less tokens at each step, implying that there is a balance to be found in how much the model should be constrained during RL training.\\n\\nHuman Preference Reward Learning.\\n\\nTo this point, our experiments have largely focused on optimizing evaluation metrics that correlate with human judgments, e.g., METEOR. Here: we additionally test how well preferences can be learned from direct human feedback. For this, we focus on CommonGen \u2014 a GRUE dataset well-suited for displaying differences due to human preferences. First, we randomly select prompts from the CommonGen train dataset and sample a single completion from both the Supervised and Supervised+NLPO models. We then present the prompt and the two completion candidates to 3 unique crowdworkers and ask them to select which one they prefer with respect to commonsense/fluency for 417 unique pairs (Krippendorf\u2019s\\\\(\\\\alpha = 28\\\\)). We use this data to train a reward model, T5-11B Raffel et al. (2020), on the balanced binary classification task of predicting which of the pair was preferred by a majority of 3 annotators, conditioned on the prompt and completion. The resulting model achieved 69.5 test ROC AUC suggesting it indeed captures average human preferences. Additional details on this process are found in Appendix B.4.4. We train Supervised+RL with a METEOR-only reward as a baseline, and compare it to a reward function that uses the fine-tuned T5-11B model. Finally, we rerun the same pairwise preference collection procedure\u2014this time sampling from CommonGen test\u2014with human participants to compare the generations from a preference optimized RL policy to the previously best Supervised+NLPO policy.\\n\\nComparing the METEOR-only to the preference model, the generations produced by the human\"}"}
{"id": "8aHzds2uUyB", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"feedback model are preferred in 682 cases, compared to the METEOR-only model which is preferred in 587 cases ($p < 0.01$). This implies that this pipeline of collecting preferences, training a reward, and further tuning the policy improves alignment to human preferences.\\n\\n5.3 DATA BUDGET: IMPROVE YOUR REWARD OR GATHER MORE DEMONSTRATION?\\n\\nGiven a fixed data collection budget, is it more efficient to gather feedback to improve a learned reward function or to gather more expert demonstrations? We use the IMDB text continuation task as a case study. In the IMDB task, a model is given a partial movie review as a prompt, and is asked to continue it as positively as possible (even if the prompt was negative). The original dataset consists of movie reviews and sentiment labels of positive, negative, or neutral. A DistilBERT (Sanh et al., 2019) classifier is trained on these labels and used to provide sentiment scores on how positive a given piece of text is, which serves as the task reward. The trade-off is between gathering more: 1) sentiment labels (improving the reward); or 2) positive sentiment reviews (improving supervised training). We train a classifier on varying amounts of training data and evaluate on the held out test dataset\u2014finding as expected that more training data improves test accuracy and so results in a higher quality reward. We then use each of these rewards of varying quality during RL training, and evaluate using the same metric as GRUE (i.e., a classifier trained with the entire training set). As seen in Table 3, we find that improving the reward quality improves LM performance as well. Further, we trained a supervised model with at least as many samples used to train each of these reward classifiers. We find that a learned reward function enables greater performance when used as a signal for an RL method than a supervised method trained with 5 times more data. This implies that improving reward models can be more data efficient than collection expert demonstrations for a task\u2014and that's not accounting for the fact that assigning sentiment labels is likely a simpler task than writing full demonstrations. Further details on this ablation are found in Appendix Table 7.\\n\\n5.4 PRACTICAL CONSIDERATIONS: WHICH IMPLEMENTATION MATTERS?\\n\\nGeneration as a token-level MDP, not a bandit environment. Most recent works that tune LMs using RL do so by calculating a reward for all the tokens in the sentence (Wu et al., 2021a; Ouyang et al., 2022; Lu et al., 2022). This setting is equivalent to a bandit feedback environment where the action space is the space of all possible generations for the task (Sutton & Barto, 2018). This type of environment can be simulated within our RL formulation by setting the discount factor $\\\\gamma = 1$. Table 3 (and Appendix Table 6) shows that this causes instability in training with respect to naturalness in both PPO and NLPO for IMDB. Our standard setting is $\\\\gamma = 0.95$ when calculating discounted rewards-to-go in the token-level MDP formulation, which reduces the magnitude of the reward that is applied to tokens selected at the beginning. The sentiment scores are approximately the same between both settings but the naturalness of language in the bandit setting is significantly less\u2014indicating that discounting rewards with $\\\\gamma < 1$ via a token-level MDP formulation is at least sometimes more effective for language generation.\\n\\nDropout and Sampling. We found two other implementation details to be critical for stability of RL training. The first is dropout, which in its standard form was found to cause instability in policy gradient methods in continuous control settings by Hausknecht & Wagener (2022). We find a similar effect when using dropout when RL training LMs as well, with training loss often diverging for dropout $> 0$ in training. The second important detail, particularly affecting the machine translation task, is sampling methods. We find that using the same sampling methods during exploration and inference is critical to translating training performance to test performance\u2013else the model exhibits high train rewards but low test metrics.\\n\\n6 CONCLUSIONS\\n\\nWe're hopeful that the GRUE benchmark and the RL4LMs library can push progress in aligning language models to human preferences via RL methods by providing the community with a standard means of comparing methods. Furthermore, we're optimistic that, as the stability and consistency of training improves, our methods provide a path towards iterative improvement of language technologies, with deployment, user feedback collection, and re-optimization enabling better user experiences when interacting with generative models.\"}"}
{"id": "8aHzds2uUyB", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGEMENTS\\n\\nWe'd like to acknowledge the support of DARPA MCS program through NIWC Pacific (N66001-19-2-4031), Google Cloud Compute, and the ReViz team at the Allen Institute for AI. KB is supported by NSF under grant No. 2127309 to the Computing Research Association for the CIFellows Project. Rajkumar is funded by the Federal Ministry of Education and Research of Germany and the state of North-Rhine Westphalia as part of the Lamarr-Institute for Machine Learning and Artificial Intelligence.\\n\\nREFERENCES\\n\\nAlekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, pp. 10\u20134, 2019.\\n\\nPrithviraj Ammanabrolu. Language Learning in Interactive Environments. PhD thesis, Georgia Institute of Technology, 2021.\\n\\nPrithviraj Ammanabrolu and Matthew Hausknecht. Graph constrained reinforcement learning for natural language action spaces. In International Conference on Learning Representations, 2020.\\n\\nURL https://openreview.net/forum?id=B1x6w0EtwH.\\n\\nPrithviraj Ammanabrolu, Liwei Jiang, Maarten Sap, Hannaneh Hajishirzi, and Yejin Choi. Aligning to social norms and values in interactive narratives. In NAACL, 2022.\\n\\nPeter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In European conference on computer vision, pp. 382\u2013398. Springer, 2016.\\n\\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In I. Guyon, U. von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf.\\n\\nKushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Cheung. Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 700\u2013710, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.58. URL https://aclanthology.org/2022.findings-acl.58.\\n\\nDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086, 2016.\\n\\nSatanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 65\u201372, 2005.\\n\\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. Advances in neural information processing systems, 28, 2015.\\n\\nOnd\u02c7rej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aur\u00e9lie N\u00e9v\u00e9ol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pp. 131\u2013198, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2301. URL https://aclanthology.org/W16-2301.\"}"}
{"id": "8aHzds2uUyB", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 14: Results of the human subject study showing the number of participants $N$, average Likert scale value for coherence and sentiment, Krippendorf's alpha showing inter-annotator agreement, and Skew. For each model a total of 100 samples were drawn randomly from the test set and rated by 3 annotators each, resulting in 300 data points per algorithm.\\n\\n| Model          | Group 1 | Group 2 | Coherence   | Commonsense | Diff (G2-G1) | p-values | Commonsense   | Diff (G2-G1) | p-values |\\n|----------------|---------|---------|-------------|-------------|-------------|----------|-------------|-------------|----------|\\n| NLPO + Supervised | 25      | 26      | 4.137       | 0.073       | 4.14       | 0.036    | 4.253       | 0.036       | 0.002    |\\n| PPO + Supervised | 24      | 23      | 2.849       | 0.16        | 2.84       | 0.16     | 4.387       | 0.159       | 0.058    |\\n| Supervised     |         |         |             |             |             |          |             |             |          |\\n| NLPO           | 24      | 23      | 2.154       | 0.391       | 2.15       | 0.363    | 2.003       | 0.335       | 0.018    |\\n| PPO           | 24      | 23      | 3.027       | 0.081       | 3.03       | 0.081    | 4.209       | 0.225       | 0.018    |\\n| NLPO          | 24      | 26      | 2.137       | 0.265       | 2.14       | 0.265    | 2.033       | 0.265       | 0.018    |\\n| PPO           | 24      | 26      | 2.097       | 0.265       | 2.10       | 0.265    | 2.033       | 0.265       | 0.018    |\\n| Zero Shot     |         |         |             |             |             |          |             |             |          |\\n\\nTable 15: Results of an post-hoc Tukey HSD Test for difference in means between pairs of algorithms (Group 2 - Group 1) and corresponding $p$-values. Individually statistically significant results are bolded and are used to discuss results in the analysis. Overall $p$-values showing that there is a significant difference in means between the models via a one-way ANOVA test are significant with $p \\\\ll 0.05$ for both coherence and sentiment.\"}"}
{"id": "8aHzds2uUyB", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Instructions, examples, and interface for the Commongen task.\\n\\nB.4.3 Human Participant Study\\n\\nFigure 6 shows the commongen instructions, examples, and interface used for the human evaluation experiments. Different from the other human evaluations, we didn't provide any prompt because knowing the set of words to be used isn't required for rating either of the axes. Tables 14, 15 show averaged results, annotator agreement, and the results of statistical significance tests to determine which models output better generations when rated by humans.\\n\\nB.4.4 Human Reference Learning Experiments\\n\\nFirst, we randomly select prompts from the Commongen train dataset and sample a single completion from both the Supervised and Supervised+NLPO models. Next, we filter to prompts where both models at least attempted to use all input concepts. This filtration step was conducted because if a model fails to use all concepts, it may generate a more natural/fluent sentence, but, a priori, it shouldn't be preferred by crowdworkers; instead of training crowdworkers to prefer sentences with all concepts, we perform this filter. Figure 7 shows the task presented to the crowdworkers. We then present the prompt and the two completion candidates to 3 unique crowdworkers and ask them to...\"}"}
{"id": "8aHzds2uUyB", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Instructions and interface for the pairwise Commongen HIT. We gathered 3 annotations on 417 pairs ($\\\\alpha = 0.28$), and split into 60/20/20 train/val/test split. We then trained a reward model, T5-11B Raffel et al. (2020), on the balanced binary classification task of predicting which of the pair was preferred by a majority of 3 annotators, conditioned on the prompt and completion. The resulting model achieved 69.5 test ROC AUC suggesting it indeed captures average human preferences. The model is then used as a reward function. We train Supervised+RL with a METEOR-only reward as a baseline, and compare it to a reward function that uses the fine-tuned T5-11B model. We design the reward function based on the preference model as $r = \\\\text{meteor} + \\\\frac{\\\\text{pref}}{1 + |\\\\text{miss}|}$ where $\\\\text{miss}$ is a set of concepts not covered in the generated text, in an attempt to mimic the data collection process that humans are instructed to follow. This reward function accounts for both the task of using all concepts and also human's preferences for how a sentence should look within the constraints stipulated by the task. Finally, we rerun the same pairwise preference collection procedure\u2014this time sampling from Commongen test\u2014with human participants to compare the generations from a preference optimized RL policy to the previously best Supervised+NLPO policy.\\n\\nComparing the METEOR-only to the preference model head-to-head, the generations produced by the human feedback model are preferred in 682 cases, compared to the METEOR-only model which is preferred in 587 cases ($p < 0.01$ the models are equally preferred).\\n\\nB.4.5 QUALITATIVE ANALYSIS\\n\\nThis section shows sample generations from different algorithms for three randomly picked prompts.\\n\\nSample 1\\n\\nPrompt: generate a sentence with: apron cut hat kitchen sausage\\n\\nZero-Shot: generate a sentence with: apron cut hat kitchen sausage.\\n\\nPPO: sausage in the kitchen on an apron.\\n\\nNLPO: sausage cut hat cut hat cut hat cut apron cut hat\\n\\nSupervised: A man is cutting sausage on an apron in a kitchen.\\n\\nSupervised + PPO: A man cutting sausage from a hat on an apron in the kitchen.\\n\\nSupervised + NLPO: A man cuts sausage in a hat on an apron in the kitchen.\"}"}
{"id": "8aHzds2uUyB", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prompt: generate a sentence with: backhand competition player reach shuttlecock\\n\\nZero-Shot: backhand competition player reach shuttlecock.\\nPPO: the ball in the backhand and the competition player in the shuttlecock.\\nNLPO: backhanded competition player reaches shuttlecock.\\nSupervised: tennis player reaches for a shuttlecock during a competition.\\nSupervised+PPO: football player reaches for a shuttlecock in backhand during the competition.\\nSupervised+NLPO: football player reaches for a shuttlecock after hitting a backhand during the competition.\\n\\nPrompt: generate a sentence with: bowl combine cucumber onion pasta\\n\\nZero-Shot: cucumber onion pasta.\\nPPO: the cucumber onion pasta in a small bowl.\\nNLPO: the in a bowl Combine cucumber onion pasta in a large bowl. Toss pasta\\nSupervised: A bowl containing pasta, tomatoes, cucumbers, and onions.\\nSupervised+PPO: A bowl containing pasta topped with cucumbers, onions, and peppers.\\nSupervised+NLPO: A bowl containing a mixture of pasta, cucumber, and onion.\"}"}
{"id": "8aHzds2uUyB", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "8aHzds2uUyB", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nCONTENTS\\n\\n1 Introduction\\n2 Related Work\\n3 RL4LMs: A Library for Training LMs with RL\\n   3.1 Environments: Generation as a Token-level MDP\\n   3.2 Reward Functions and Evaluation Metrics\\n   3.3 On-policy Actor-critic Algorithms\\n4 NLPO: Natural Language Policy Optimization\\n5 GRUE (General Reinforced-language Understanding Eval)\\n   5.1 Results on GRUE: Which Algorithm Should be Used to Learn Preferences?\\n   5.2 Preference Reward Learning, Selection, and Hacking\\n   5.3 Data Budget: Improve your Reward or Gather More Demonstration?\\n   5.4 Practical Considerations: Which Implementation Details Matter Most?\\n6 Conclusions\\n7 Acknowledgements\\nA On-policy Algorithm Implementation Details\\n   A.1 PPO Details\\n   A.2 NLPO Details\\nB Experimental Details\\n   B.1 Crowdworking Details\\n   B.2 GRUE Experiment Setup\\n   B.3 IMDB\\n      B.3.1 Setup\\n      B.3.2 Results and Discussion\\n      B.3.3 Human Participant Study\\n   B.4 CommonGen\\n      B.4.1 Setup\\n      B.4.2 Results and Discussion\\n      B.4.3 Human Participant Study\\n      B.4.4 Human Preference Learning Experiments\\n      B.4.5 Qualitative Analysis\\n   B.5 CNN Daily Mail\\n16\"}"}
{"id": "8aHzds2uUyB", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "8aHzds2uUyB", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 PPO Details\\n\\nGiven discussion and equations in Section 3.3, we further note that we follow (Ziegler et al., 2019) and dynamically adapt the KL coefficient $\\\\beta$ during training where,\\n\\n$$e_t = \\\\text{clip}(\\\\text{KL}(\\\\pi(a_t | s_t) || \\\\pi_0(a_t | s_t)) - \\\\text{KL}_{\\\\text{target}}, -0.2, 0.2)$$  \\\\hspace{1cm} (2)\\n\\n$$\\\\beta_{t+1} = \\\\beta_t (1 + K_\\\\beta e_t)$$  \\\\hspace{1cm} (3)\\n\\nwhere $\\\\text{KL}_{\\\\text{target}}$ is user-specified KL divergence between initial model $h$ and current policy $\\\\pi$ and $K_\\\\beta$ is rate of update which we generally set to $0.2$ in our experiments.\\n\\nTo increase stability during training, we further use Generalized Advantage Estimation (GAE) (Schulman et al., 2015b) and define the advantage estimator $\\\\hat{A}(s_n, a_n)$ based on the Temporal Difference residual as:\\n\\n$$\\\\delta_t = r(s_t, a_t) + V_{\\\\phi}(s_{t+1}) - V_{\\\\phi}(s_t).$$  \\\\hspace{1cm} (4)\\n\\n$$\\\\hat{A}(s_n, a_n) = \\\\sum_{t=0}^{\\\\infty} \\\\lambda_t \\\\delta_{n+t},$$  \\\\hspace{1cm} (5)\\n\\nwhere $\\\\lambda$ provides the trade-off between bias and variance.\\n\\nA.2 NLPO Details\\n\\nNLPO learns to mask irrelevant language by maintaining a masking policy $\\\\pi_\\\\psi$: the masking policy is a copy of the current policy ($\\\\pi_\\\\theta$), but is updated only every $\\\\mu$ steps. Given $Z(\\\\pi_\\\\theta) = \\\\sum_{a \\\\in V} \\\\pi_\\\\theta(a | s)$ the normalization value of the sum of probabilities of all action $a \\\\in A$ given a particular State $s \\\\in S$, let the parameterized top-$p$ vocabulary $V_p(\\\\pi_\\\\theta) \\\\subset V$ be the subset of the vocab, consisting of the top-$p$ highest probability vocabulary tokens with respect to $\\\\pi_\\\\theta$. Formally, let $Z_p$ be the normalization value for the parameterized top-$p$ vocabulary, can be defined as the subset of tokens that maximizes $Z_p(\\\\pi_\\\\theta) = \\\\sum_{a \\\\in V} \\\\pi_\\\\theta(a | s)$. Then optimizing a policy according to the parameterized top-$p$ vocabulary can be defined as:\\n\\n$$\\\\pi_\\\\psi(\\\\cdot | s, \\\\pi_\\\\theta) = \\\\frac{\\\\pi_\\\\theta(\\\\cdot | s)}{Z_p(\\\\pi_\\\\theta)}$$ if $a \\\\in V_p(\\\\pi_\\\\theta)$ and $Z(\\\\pi_\\\\theta) = 0$ otherwise.  \\\\hspace{1cm} (6)\"}"}
{"id": "8aHzds2uUyB", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Qualification round\\nWe ran a qualification round using the IMDB task. We opened the qualification round to users from {AU, CA, NZ, GB, US} with 5K prior approved HITs and a minimum acceptance rate of 97% on their previous HITs. We gathered judgments over 600 generations from 3 annotators per generation. One of the authors of this paper also completed 17 random HITs to serve as a proxy for \u201cground truth.\u201d After gathering these annotations, we selected workers who: 1) didn't significantly disagree with other annotators on the same instance more than 20% of the time; 2) who completed at least 5 HITs; 3) who didn't disagree with the author annotator on the 17 HITs by more than 1 point; and 4) (likely) spent a reasonable amount of time reading the instructions/examples provided. In the end, 56 annotators were qualified. Additional per-task details are provided in the per-task sections of the Appendix.\\n\\nCompensation details\\nAs per Amazon Mechanical Turk policy, annotators were compensated on a per-HIT basis. In addition, we used a timing script to estimate hourly wages to ensure our target of $15/hr was met. In cases where this minimum hourly rate was not met, we manually assigned bonuses.\\n\\nB.2 GRUE EXPERIMENT SETUP\\nWe benchmark 5 training algorithms on 6 tasks (see Table 1) using either an encoder model (e.g. GPT-2) or encoder-decoder model (e.g. T5). We train policies using PPO, NLPO with variations of whether supervised pre-training is applied before RL fine-tuning and compare against supervised policy. The choice of LM is based on the type of task. For IMDB text continuation, we use GPT-2 and T5 for rest of the tasks. We use two separate LM models as actor and critics networks (i.e. no shared layers) in which the critic network has an additional linear layer mapping last token's hidden representation to a scalar value. We use AdamW optimizer Loshchilov & Hutter (2017) with fixed learning rate and no scheduling.\\n\\n(a) Automated Task Metrics\\n\\n(b) Automated Naturalness Metrics\\n\\nFigure 3: Summarized results via automated metrics across all 7 GRUE tasks for each of the 5 algorithms we consider, and human participant studies for the 5 tasks suitable for human studies. We break up the metrics into task-specific, e.g. average positive sentiment for IMDB task, and naturalness metrics, such as perplexity and human perceived coherence for the human rated metrics. This plot differs from Figure 2 as this one averages over over multiple reward functions per each task.\\n\\n19\"}"}
{"id": "8aHzds2uUyB", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: IMDB Hyperparams\\n\\n| Parameter                  | Value       |\\n|----------------------------|-------------|\\n| Model Params               |             |\\n| supervised batch size      | 64          |\\n| epochs                     | 10          |\\n| learning rate              | 0.0000001   |\\n| ppo steps per update       | 1280        |\\n| total number of steps      | 64000       |\\n| batch size                 | 64          |\\n| epochs per update          | 5           |\\n| learning rate              | 0.0000001   |\\n| discount factor            | 0.99        |\\n| gae lambda                 | 0.95        |\\n| clip ratio                 | 0.2         |\\n| value function coeff       | 0.5         |\\n| nlpo steps per update      | 1280        |\\n| total number of steps      | 64000       |\\n| batch size                 | 64          |\\n| epochs per update          | 5           |\\n| learning rate              | 0.0000001   |\\n| discount factor            | 0.99        |\\n| gae lambda                 | 0.95        |\\n| clip ratio                 | 0.2         |\\n| top mask ratio             | 0.9         |\\n| target update iterations   | 5           |\\n| decoding sampling          | true        |\\n| top k                      | 50          |\\n| min length                 | 48          |\\n| max new tokens             | 48          |\\n| tokenizer padding side     | left        |\\n| truncation side            | left        |\\n| max length                 | 64          |\\n\\nB.3 IMDB\\n\\nB.3.1 SETUP\\n\\nWe consider IMDB dataset for the task of generating text with positive sentiment. The dataset consists of 25k training, 5k validation and 5k test examples of movie review text with sentiment labels of positive and negative. The input to the model is a partial movie review text (upto 64 tokens) that needs to be completed (generating 48 tokens) by the model with a positive sentiment while retaining fluency. For RL methods, we use a sentiment classifier Sanh et al. (2019) that is trained on pairs of text and labels as a reward model which provides sentiment scores indicating how positive a given piece of text is. For supervised Seq2Seq baselines, we consider only the examples with positive labels. We chose GPT-2 as LM for this task as it is more suited for text continuation than encoder-decoder LMs (eg. T5). We use top-k sampling with $K = 50$ as the decoding method and for fair comparison, we keep this setting for all methods. For PPO and NLPO models, we train for $64k$ steps in total and update policy and value networks every $1280$ steps with a mini-batch size of $64$ and epochs of $5$ per update. We apply adaptive KL controllers with different target KLs of $0.02$, $0.05$, $0.1$, $\\\\infty$ with an initial KL co-efficient of $\\\\beta = 0.1$. Table 4 provides an in-depth summary of all hyperparameters and other implementation details.\\n\\nB.3.2 RESULTS AND DISCUSSION\\n\\nTarget KL ablation\\n\\nFig 4 shows learning curves for PPO and NLPO in terms of episodic training reward, corpus level sentiment scores and perplexity scores on validation set averaged for 5 random seeds. It is seen that higher target KL of $0.1$ is desired to achieve higher rewards but results in drifting.\"}"}
{"id": "8aHzds2uUyB", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Learning Curves: Averaged learning curves over 5 different runs by varying target KL, shaded regions indicate one standard deviation. (a) shows the rollout episodic total reward during training. (b) shows evolution of sentiment scores on the validation split. (c) shows evolution of perplexity on the validation split. From (a) and (b), it is seen that higher target KL (0.1) is desired to achieve higher rewards. However, this setting drifts away from the original LM too much and loses fluency. Therefore, a lower target KL (0.02 or 0.05) is required to keep the model closer to the original LM. Similar trends hold for NLPO but when compared to PPO, it retains lower perplexities and is more stable even with higher KL targets.\\n\\nTable 5: Target KL Ablations: Mean and standard deviations over 5 random seeds is reported for sentiment scores along with fluency and diversity metrics on validation set. It is seen from perplexity scores that a lower target KL constraint is desired to keep the model closer to the original model. On the other hand, a higher target KL yields higher sentiment scores at the cost of fluency. inf KL penalty (target KL of inf), model simply learns to generate positive phrases (e.g., \u201cI highly recommend this movie to all!\u201d, \u201cworth watching\u201d) regardless of the context. NLPO achieves better sentiment and perplexity scores than PPO.\"}"}
{"id": "8aHzds2uUyB", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Training data size ablation\\n\\nWe vary the amount of data used to train the reward classifier and the supervised baseline model to understand whether it is more efficient to gather data to improve reward model or to gather expert demonstrations for supervised learning. As observed in Table 7, improving the quality of reward function increases the performance on the overall task better than training with more data for supervised training, indicating that improving reward models is efficient than collecting expert demonstrations for supervised training from a data efficiency perspective.\\n\\nDiscount factor ablation\\n\\nTo understand the effect of discounted vs undiscounted (bandit) environments, we report sentiment and perplexity scores for different values of discount factor (0.5, 0.95, and 1.0) in Table 6 and observe that using a bandit environment (discount factor of 1.0) results in performance loss in the case of NLPO and reward hacking in the case of PPO, indicating that the discounted setting (with 0.95) is desired.\\n\\nNLPO params\\n\\nTable 8 shows ablation on different hyperparameters in NLPO algorithm.\\n\\nTable 7: Evaluation of GPT2 with different algorithms on IMDB sentiment text continuation task, data budget ablations: Mean and standard deviations over 5 random seeds is reported for sentiment scores along with fluency and diversity metrics. This table measures performance differences as a function of the fraction of the dataset that has been used. In the case of the RL approaches, this measures how much data is used to train the reward classifier, and for the supervised method it directly measures fraction of positive reviews used for training. We note that using even a small fraction of data to train a reward classifier proves to be effective in terms of downstream task performance while this is not true for supervised approaches. This lends evidence to the hypothesis that adding expending data budget on a reward classifier is more effective than adding more gold label expert demonstrations.\"}"}
{"id": "8aHzds2uUyB", "page_num": 59, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 34: Results of the human subject study showing the number of participants N, average Likert scale value for coherence and sentiment, Krippendorf's alpha showing inter-annotator agreement, and Skew. For each model a total of 100 samples were drawn randomly from the test set and rated by 3 annotators each, each resulting in 300 data points per algorithm.\\n\\n| Model                  | N  | Coherence | Sentiment | Alpha | Skew |\\n|------------------------|----|-----------|-----------|-------|------|\\n| Zeroshot               | 31 | 3.84      | 0.225     | 4.181 | 3.2  |\\n| NLPO                   | 30 | 4.18      | 0.114     | 4.17  | 3.35 |\\n| PPO                    | 32 | 4.18      | 0.112     | 4.03  | 3.32 |\\n| Supervised+PPO         | 31 | 3.99      | 0.148     | 4.13  | 3.48 |\\n| Supervised+NLPO        | 31 | 4.13      | 0.186     | 3.95  | 3.58 |\\n| Supervised             | 31 | 3.96      | 0.249     | 3.83  | 3.59 |\\n\\n#### B.9.2 Results and Discussion\\n\\nTable 33 presents our benchmarking results of RL methods along with supervised baseline performances on test sets. Our main finding is that RL methods generally achieve better intent accuracy and automatic metric scores, in particular NLPO variants perform better than all other methods.\\n\\n#### B.9.3 Human Participant Study\\n\\nFigure 10 shows the Daily Dialogue instructions and interface used for the human evaluation experiments. Tables 34, 35 show averaged results, annotator agreement, and the results of statistical significance tests to determine which models output better generations when rated by humans.\\n\\n| Group 1 | Group 2 | Coherence | Quality | Diff (G2-G1) | p-values | Diff (G2-G1) | p-values |\\n|---------|---------|-----------|---------|--------------|-----------|--------------|-----------|\\n| NLPO    | PPO     | -0.003    | -0.030  | 0.900        | 0.900     | 0.900        | 0.900     |\\n| NLPO    | Supervised | -0.227   | 0.238   | 0.043        | 0.020     | 0.020        | 0.020     |\\n| NLPO    | Supervised+NLPO | -0.050   | 0.234   | 0.900        | 0.22     | 0.22         | 0.22      |\\n| NLPO    | Supervised+PPO | -0.194   | 0.127   | 0.013        | 0.803     | 0.803        | 0.803     |\\n| NLPO    | Zero Shot | -0.345   | -0.154  | 0.001        | 0.655     | 0.655        | 0.655     |\\n| PPO     | Supervised | -0.224   | 0.268   | 0.049        | 0.010     | 0.010        | 0.010     |\\n| PPO     | Supervised+NLPO | -0.047   | 0.264   | 0.900        | 0.11     | 0.11         | 0.11      |\\n| PPO     | Supervised+PPO | -0.191   | 0.157   | 0.144        | 0.636     | 0.636        | 0.636     |\\n| PPO     | Zero Shot | -0.341   | -0.124  | 0.001        | 0.822     | 0.822        | 0.822     |\\n| Supervised | Supervised+NLPO | 0.177   | -0.003  | 0.021        | 0.900     | 0.900        | 0.900     |\\n| Supervised | Supervised+PPO | 0.033   | -0.110  | 0.900        | 0.896     | 0.896        | 0.896     |\\n| Supervised | Zero Shot | -0.117  | -0.391  | 0.645        | 0.002     | 0.002        | 0.002     |\\n| Supervised+NLPO | Supervised+PPO | -0.144  | -0.107  | 0.444        | 0.009     | 0.009        | 0.009     |\\n| Supervised+NLPO | Zero Shot | -0.294  | -0.388  | 0.002        | 0.003     | 0.003        | 0.003     |\\n| Supervised+PPO | Zero Shot | -0.151  | -0.281  | 0.390        | 0.008     | 0.008        | 0.008     |\\n\\nTable 35: Results of an post-hoc Tukey HSD Test for difference in means between pairs of algorithms (Group 2 - Group 1) and corresponding p-values. Individually statistically significant results are bolded and are used to discuss results in the analysis. Overall p-values showing that there is a significant difference in means between the models via a one-way ANOVA test are significant with $p \\\\ll 0.05$ for both coherence and sentiment.\"}"}
{"id": "8aHzds2uUyB", "page_num": 60, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Instructions and interface for the Daily Dialogue task.\\n\\nWe show sample generations from each of the algorithms for three randomly picked prompts below.\\n\\nSample 1\\n\\nPrompt: Right. And China is the homeland of tea. <EOU> Yes, Chinese people love drinking tea so much. Some even claim they can't live without tea. <EOU> Do you know there are several categories of Chinese tea? <EOU> Yes, I believe there are green teas, black teas and scented teas. Any others? <EOU> Well, have you ever heard of Oulong tea and compressed tea? <EOU>\\n\\nReferences: Oh, yeah. Oulong tea is good for one's health, isn't it?\\n\\nZero-Shot: I know that Oulong tea can be enjoyed with very few meals, and there are very few\\n\\nPPO: I have, but it's not as strong as Oulong tea.\\n\\nNLPO: I haven't. I don't have tea.\\n\\nSupervised: Yes. I hear they do good. And I heard this Taijiquan Tea is very good\\n\\nSupervised + PPO: Actually, I have had Oulong tea, but I haven't had any for awhile now.\\n\\nSupervised + NLPO: No, I've never heard of that.\"}"}
{"id": "8aHzds2uUyB", "page_num": 61, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prompt: I think I might have a dahl. What's in a dahl? It's got chickpeas and vegetables in a spicy curry sauce with rice. That sounds nice. Do you want to share some kebabs as well? Ok. How about some lamb kebabs? That's my favourite. Do you want to have some wine or beer? Zero-shot: they're very salty. PPO: well you might want to take a peek. NLPO: well I like that. Supervised: ok, here they are. I'm so thirsty. I can't wait until the next bus to Supervised + PPO: I've tried that a few times, but they're too dry and I don't want any of Supervised + NLPO: oh, ok. That's a good choice. I'll take it. Sample 3 Prompt: Really? Have you had an auto-reverse break before? Yes, twice now. I think it's too high-tech, and so it's the first part that breaks. Hmm. You know, the problem might be dust. Did you clean the unit often? Well. No, I didn't clean it. But still, I only had the last one two months, and it broke. I don't trust auto-reverse. Alright. Well. We have many units without auto-reverse. Here is a good unit. Very reliable. It has an AM/FM band, built-in microphone, recording capability, and no auto-reverse. References: How much is it? Zero-shot: Oh. So you don't have the AM/FM band? PPO: You said AM/FM. Are you sure about that? NLPO: Well. Well. I had no idea when it was, and didn't care. Supervised: Good. Well, what else I need? Supervised + PPO: But I think it lacks the CD player or its CD player accessories. Supervised + NLPO: Cool! So what's the trouble with it?\"}"}
{"id": "8aHzds2uUyB", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "8aHzds2uUyB", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "8aHzds2uUyB", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "8aHzds2uUyB", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 2021.\\n\\nMarc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. In ICLR, 2016.\\n\\nSteven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In CVPR, 2017.\\n\\nSt\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627\u2013635. JMLR Workshop and Conference Proceedings, 2011.\\n\\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\\n\\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pp. 1889\u20131897. PMLR, 2015a.\\n\\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.\\n\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\n\\nThibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7881\u20137892, Online, July 2020. Association for Computational Linguistics. doi:10.18653/v1/2020.acl-main.704. URL https://www.aclweb.org/anthology/2020.acl-main.704.\\n\\nClaude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379\u2013423, 1948.\\n\\nCharlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural language generation with implicit language q learning. arXiv preprint arXiv:2206.11871, 2022.\\n\\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020.\\n\\nMarilyn Strathern. 'improving ratings': audit in the british university system. European review, 5(3):305\u2013321, 1997.\\n\\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\\n\\nGokul Swamy, Sanjiban Choudhury, J Andrew Bagnell, and Steven Wu. Of moments and matching: A game-theoretic framework for closing the imitation gap. In International Conference on Machine Learning, pp. 10022\u201310032. PMLR, 2021.\\n\\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4566\u20134575, 2015.\"}"}
{"id": "8aHzds2uUyB", "page_num": 43, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ToTTo (Parikh et al., 2020) is a controlled table-to-text generation task in which the goal is to produce one-sentence description of highlighted table cells. For training RL methods, we consider 5 different reward functions: BLEU, SacreBLEU, METEOR, PARENT and a combination of Meteor and PARENT. We chose T5 as our base LM here too, as they are more suitable for structure to text tasks. For decoding, we use beam search during inference and for generating rollouts, we use top k sampling. Other implementation details are captured in Table 22.\\n\\n| Model Params | value |\\n|--------------|-------|\\n| supervised batch size: | 8 |\\n| epochs: | 4 |\\n| learning rate: | 0.0001 |\\n| learning rate scheduler: | constant with warm up |\\n| weight decay: | 0.1 |\\n| ppo/nlpo steps per update: | 2560 |\\n| total number of steps: | 256000 |\\n| batch size: | 64 |\\n| epochs per update: | 5 |\\n| learning rate: | 0.000002 |\\n| entropy coefficient: | 0.0 |\\n| initial kl coeff: | 0.001 |\\n| target kl: | 2.0 |\\n| discount factor: | 0.99 |\\n| gae lambda: | 0.95 |\\n| clip ratio: | 0.2 |\\n| rollouts top k : | 50 |\\n| value function coeff: | 0.5 |\\n| top mask ratio: | 0.9 |\\n| target update iterations: | 20 |\\n| supervised+ ppo (or nlpo) steps per update: | 2560 |\\n| total number of steps: | 256000 |\\n| batch size: | 64 |\\n| epochs per update: | 5 |\\n| learning rate: | 0.0000005 |\\n| entropy coefficient: | 0.0 |\\n| initial kl coeff: | 0.01 |\\n| target kl: | 0.2 |\\n| discount factor: | 0.99 |\\n| gae lambda: | 0.95 |\\n| clip ratio: | 0.2 |\\n| rollouts top k : | 50 |\\n| value function coeff: | 0.5 |\\n| top mask ratio: | 0.9 |\\n| target update iterations: | 20 |\\n| decoding num beams: | 5 |\\n| min length: | 10 |\\n| max new tokens: | 50 |\\n| tokenizer padding side: | left |\\n| truncation side: | right |\\n| max length: | 512 |\\n\\nTable 22: ToTTo Hyperparams: Table shows a list of all hyper-parameters and their settings.\\n\\nB.6.2 RESULTS AND DISCUSSION\\n\\nTables 24, 23 presents our benchmarking results with 5 reward functions along with supervised baseline performances on dev and test sets respectively. Similar to other tasks, our main finding is that warm-started initial policies are crucial for learning to generate descriptions from highlighted cells. Without warm-start, policies suffer from reward hacking and resulting in sub-optimal solutions despite application of task-specific metrics such as PARENT etc. We find that Supervised+NLPO method outperforms all models on ToTTo leaderboard in terms of PARENT metric.\"}"}
{"id": "8aHzds2uUyB", "page_num": 44, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm        | Lexical Metric | Semantic Metric | Factual Metric |\\n|------------------|----------------|----------------|---------------|\\n|                  | BLEURT         | SACREBLEU       | PARENT        |\\n| ToTTo Zero-Shot  | 0.036 0.032    | -1.392 -1.397   | 0.116 0.112   |\\n| PPO T5 bleu      | 0.065 0.063    | -1.074 -1.098   | 0.246 0.244   |\\n|                  |                |                | 0.083 0.083   |\\n|                  |                |                | 0.090 -0.955  |\\n|                  |                |                | 0.090 0.083   |\\n|                  |                |                | -1.009 -1.079|\\n|                  |                |                | 0.128 0.128   |\\n|                  |                |                | 0.119 0.121   |\\n|                  |                |                | 0.119 0.121   |\\n|                  |                |                | 0.124 0.124   |\\n| NLPO T5 bleu     | 0.062 0.059    | -1.077 -1.097   | 0.235 0.233   |\\n|                  |                |                | 0.085 0.083   |\\n|                  |                |                | 0.085 0.083   |\\n|                  |                |                | -1.009 -1.079|\\n|                  |                |                | 0.329 0.330   |\\n|                  |                |                | 0.335 0.338   |\\n|                  |                |                | 0.332 0.332   |\\n| Supervised T5    | 0.457 0.377    | 0.204 0.199     | 0.583 0.534   |\\n| Supervised + PPO| 0.473 0.395    | 0.200 0.191     | 0.590 0.542   |\\n|                  |                |                | 0.474 0.389   |\\n|                  |                |                | 0.337 0.335   |\\n|                  |                |                | 0.322 0.322   |\\n|                  |                |                | 0.368 0.368   |\\n| Supervised + NLPO| 0.475 0.399  | 0.208 0.192     | 0.593 0.546   |\\n|                  |                |                | 0.475 0.392   |\\n|                  |                |                | 0.475 0.392   |\\n|                  |                |                | 0.330 0.330   |\\n|                  |                |                | 0.322 0.322   |\\n|                  |                |                | 0.368 0.368   |\\n\\nTable 23: ToTTo test evaluation. Table shows lexical, semantic and factual correctness metric scores of algorithms with different reward functions on hold-out test set. Without supervised pre-training, both PPO and NLPO results in sub-optimal solutions, with NLPO better than PPO. With supervised pre-training, PPO and NLPO achieve better scores across all metrics showing RL fine-tuning is beneficial. Most importantly, RL fine-tuned models produce more factually consistent text as seen in higher PARENT scores. Another observation, fine-tuning with a task-specific metric PARENT is better than training on task-agnostic lexical rewards.\"}"}
{"id": "8aHzds2uUyB", "page_num": 45, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Alg      | LM Reward function | Rouge-1 | Rouge-2 | Rouge-L | Rouge-LSum | Meteor | BertScore | SacreBleu | MSTTR Distinct | Unique Mean Output Length |\\n|----------|---------------------|---------|---------|---------|------------|--------|-----------|-----------|--------------------------|--------------------------|\\n|          |                     |         |         |         |            |        |           |           |                          |                          |\\n|          |                     | 0.131   | 0.055   | 0.127   | 0.127      | 0.057  | 0.805     | 0.038     | 0.042                     | 0.034                     |\\n| Zero-Shot T5 |                     | 0.274   | 0.138   | 0.249   | 0.249      | 0.139  | 0.844     | 0.068     | 0.071                     | 0.066                     |\\n| Supervised T5 |                     | 0.341   | 0.168   | 0.297   | 0.297      | 0.173  | 0.888     | 0.147     | 0.163                     | 0.133                     |\\n| PPO T5 bleu |                     | 0.410   | 0.283   | 0.388   | 0.388      | 0.222  | 0.954     | 0.477     | 0.549                     | 0.405                     |\\n| Supervised + PPO |                     | 0.408   | 0.283   | 0.388   | 0.388      | 0.222  | 0.954     | 0.477     | 0.549                     | 0.405                     |\\n| Supervised + NLPO |                     | 0.410   | 0.283   | 0.388   | 0.388      | 0.222  | 0.954     | 0.477     | 0.549                     | 0.405                     |\\n\\nTable 24: ToTTo dev evaluation. Table shows lexical, semantic and factual correctness metric scores of algorithms with different reward functions on dev set. Without supervised pre-training, both PPO and NLPO results in sub-optimal solutions, with NLPO better than PPO. With supervised pre-training, PPO and NLPO achieve better scores across all metrics showing RL fine-tuning is beneficial. Most importantly, RL fine-tuned models produce more factually correct text as seen in higher PARENT scores. Another observation, fine-tuning with a task-specific metric PARENT is better than training just on task-agnostic lexical metrics. \"}"}
{"id": "8aHzds2uUyB", "page_num": 46, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 25: Results of the human subject study showing the number of participants $N$, average Likert scale value for coherence and sentiment, Krippendorf's alpha showing inter-annotator agreement, and Skew. For each model a total of 50 samples were drawn randomly from the test set and rated by 3 annotators each, resulting in 150 data points per algorithm.\\n\\n| Group 1 | Group 2 | Coherence | Correctness |\\n|---------|---------|-----------|-------------|\\n| PPO     | NLPO    | -0.507    | -0.613      |\\n| PPO     | NLPO+Supervised | 1.827  | 1.340      |\\n| PPO     | Supervised | 1.833  | 1.313      |\\n| PPO     | PPO+Supervised | 1.813  | 1.253      |\\n| PPO     | Zero Shot | -1.120 | -1.293     |\\n| NLPO    | NLPO+Supervised | 2.333  | 1.953      |\\n| NLPO    | Supervised | 2.340  | 1.927      |\\n| NLPO    | PPO+Supervised | 2.320  | 1.867      |\\n| NLPO    | Zero Shot | -0.613 | -0.680     |\\n| NLPO+Supervised | Supervised | 0.007  | 0.9        |\\n| NLPO+Supervised | PPO+Supervised | -0.013 | 0.009      |\\n| NLPO+Supervised | Zero Shot | -2.947 | -2.633     |\\n| Supervised | PPO+Supervised | -0.020 | 0.009      |\\n| Supervised | Zero Shot | -2.953 | -2.607     |\\n| PPO+Supervised | Zero Shot | -2.933 | -2.547     |\\n\\nTable 26: Results of an post-hoc Tukey HSD Test for difference in means between pairs of algorithms (Group 2 - Group 1) and corresponding p-values. Individually statistically significant results are bolded and are used to discuss results in the analysis. Overall p-values showing that there is a significant difference in means between the models via a one-way ANOVA test are significant with $p \\\\ll 0.05$ for both coherence and sentiment.\"}"}
{"id": "8aHzds2uUyB", "page_num": 55, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm          | SacreBLEU | chRF  | TER      | Output Length | Execution Time | Mean Output Length |\\n|-------------------|-----------|-------|----------|---------------|----------------|--------------------|\\n| **WMT-16**        |           |       |          |               |                |                    |\\n| Zero-Shot T5      | 0.732     | 10.100| 7290     | 20.533        |                |                    |\\n| PPO T5            | 0.738     | 10.166| 7503     | 20.375        |                |                    |\\n| T5 chRF           | 0.738     | 10.175| 7376     | 20.337        |                |                    |\\n| T5 TER            | 0.736     | 10.132| 7447     | 20.356        |                |                    |\\n| T5 BertScore      | 0.736     | 10.129| 7272     | 20.035        |                |                    |\\n| **Supervised**    |           |       |          |               |                |                    |\\n| T5                | 0.729     | 10.048| 7205     | 20.622        |                |                    |\\n| **Supervised + PPO** T5 | 0.732     | 10.080| 7222     | 20.605        |                |                    |\\n| **Supervised + NLPO** T5 | 0.734     | 10.089| 7308     | 20.686        |                |                    |\\n\\n| Algorithm          | SacreBLEU | chRF  | TER      | Output Length | Execution Time | Mean Output Length |\\n|-------------------|-----------|-------|----------|---------------|----------------|--------------------|\\n| **IWSLT2017**     |           |       |          |               |                |                    |\\n| Zero-Shot T5      | 0.662     | 9.276 | 8312     | 18.739        |                |                    |\\n| PPO T5            | 0.657     | 9.230 | 8285     | 19.069        |                |                    |\\n| T5 chRF           | 0.660     | 9.253 | 8243     | 18.912        |                |                    |\\n| T5 TER            | 0.659     | 9.244 | 8129     | 18.268        |                |                    |\\n| T5 BertScore      | 0.673     | 9.288 | 6642     | 11.602        |                |                    |\\n| **Supervised**    |           |       |          |               |                |                    |\\n| T5                | 0.655     | 9.210 | 7970     | 18.440        |                |                    |\\n| **Supervised + PPO** T5 | 0.654     | 9.176 | 8061     | 18.803        |                |                    |\\n| **Supervised + NLPO** T5 | 0.655     | 9.182 | 8073     | 18.777        |                |                    |\\n\\nTable 31: WMT-16 and IWSLT test evaluation - diversity metrics\\n\\nThe table shows diversity metrics for RL algorithms with different reward functions benchmarked against supervised baseline models.\"}"}
{"id": "8aHzds2uUyB", "page_num": 56, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We show sample generations from each of the algorithms for three randomly picked prompts from IWSLT below.\\n\\nSample 1\\n\\nPrompt: translate English to German: Several years ago here at TED, Peter Skillman introduced a design challenge called the marshmallow challenge.\\n\\nZero-shot: Vor einigen Jahren hat Peter Skillman hier bei TED eine Design-Herausforderung namens Marshmallow Challenge vorgestellt.\\n\\nPPO: Vor einigen Jahren, hier bei TED, stellte Peter Skillman einen Design-Wettbewerb namens \\\"Die Marshmallow-Herausforderung\\\" vor.\\n\\nNLPO: Vor einigen Jahren hier bei TED f\u00fchrte Peter Skillman eine Design-Herausforderung namens Marshmallow Challenge ein.\\n\\nSupervised: Vor einigen Jahren hier bei TED f\u00fchrte Peter Skillman eine Design-Herausforderung namens Marshmallow-Herausforderung ein.\\n\\nSupervised + PPO: Vor einigen Jahren hier bei TED, f\u00fchrte Peter Skillman eine Design-Herausforderung namens Marshmallow-Herausforderung ein.\\n\\nSupervised + NLPO: Vor einigen Jahren hier bei TED f\u00fchrte Peter Skillman eine Design-Herausforderung ein, die Marshmallow-Herausforderung.\\n\\nSample 2\\n\\nPrompt: translate English to German: They spend the majority of their time assembling the sticks into ever-growing structures.\\n\\nZero-shot: Sie verbringen den gr\u00f6\u00dften Teil ihrer Zeit damit, die St\u00fccke in immer wachsende Strukturen zu montieren.\\n\\nReferences: Sie investieren einen Gro\u00dfteil ihrer Zeit in den Zusammenbau st\u00e4ndiger wachsender Strukturen.\\n\\nPPO: Sie verbringen den Gro\u00dfteil ihrer Zeit damit, die St\u00fccke in immer gr\u00fcber werdende Strukturen zu montieren.\\n\\nNLPO: Sie verbringen den Gro\u00dfteil ihrer Zeit, die St\u00fccke in immer wachsende Strukturen zu montieren.\\n\\nSupervised: Sie verbringen den Gro\u00dfteil ihrer Zeit damit, die St\u00fccke in immer wachsende Strukturen zu montieren.\\n\\nSupervised + PPO: Sie verbringen den Gro\u00dfteil ihrer Zeit damit, die St\u00fccke in immer wachsende Strukturen zu montieren.\\n\\nSupervised + NLPO: Sie verbringen den Gro\u00dfteil ihrer Zeit damit, die St\u00fccke in immer wachsende Strukturen zu montieren.\\n\\nSample 3\\n\\nPrompt: translate English to German: And of course there are teams that have a lot more \\\"ta-da\\\" structures, and among the best are recent graduates of kindergarten.\\n\\nZero-shot: Und nat\u00fcrlich gibt es Teams, die viel mehr \\\"ta-da\\\" Strukturen haben, und zu den besten geh\u00f6ren j\u00fcngste Absolventen des Kindergartens.\\n\\nReferences: Und nat\u00fcrlich gibt es auch Teams die viel mehr \\\"ta-da\\\"-Gebilde haben und unter den Besten sind frische Absolventen des Kindergartens.\\n\\nPPO: Und nat\u00fcrlich gibt es Teams, die viel mehr \\\"ta-da\\\" Strukturen, und unter den besten sind j\u00fcngste Absolventen des Kindergartens.\\n\\nNLPO: Und nat\u00fcrlich gibt es Teams, die viel mehr \\\"t-a-da\\\" Strukturen haben, und unter den besten sind j\u00fcngste Absolventen des Kindergartens.\\n\\nSupervised: Und nat\u00fcrlich gibt es Teams, die viel mehr \\\"ta-da\\\"-Strukturen haben, und unter den besten sind j\u00fcngste Absolventen des Kindergartens.\\n\\nSupervised + PPO: Und nat\u00fcrlich gibt es Teams, die viel mehr \\\"ta-da\\\"-Strukturen haben, und unter den besten sind j\u00fcngste Absolventen des Kindergartens.\\n\\nSupervised + NLPO: Und nat\u00fcrlich gibt es Teams, die viel mehr \\\"ta-da\\\"-Strukturen haben, und unter den besten sind j\u00fcngste Absolventen des Kindergartens.\"}"}
{"id": "8aHzds2uUyB", "page_num": 57, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We consider DailyDialog (Li et al., 2017) as the test bed for the dialogue generation task. The dataset includes conversations written by human on various topics. In addition, each utterance contains labels of intent and emotional information. For simplicity, we focus only on generating the next utterance, given the dialogue context. We chose a context window of size 5, which results in 35k training, 3k validation and 3k test utterances. The input to the model is dialogue history in which utterances are concatenated using a $<$EOU$>$ token. We picked GPT-2 as the LM as they are more suited for text continuation than encoder-decoder LMs. For a fair comparison, we use top-k sampling with $k=20$ as the decoding method for all methods. For RL methods, we use a linear combination of meteor score and intent match score (whether the generated text\u2019s intent matches with the reference\u2019s intent) as the reward function. The coefficients for meteor and intent are chosen based on both lexical scores and intent accuracy on the validation set. For this purpose, we trained an intent classifier (fine-tuned RoBERTa (Liu et al., 2019)) that classifies given text into intent categories such as inform, question, directive and commissive, etc. Table 32 provides a summary of hyperparameters and implementation details.\\n\\n| Model Params value       | value       |\\n|--------------------------|-------------|\\n| ppo/nlpo steps per update| 1280        |\\n| total number of steps    | 128000      |\\n| batch size               | 64          |\\n| epochs per update        | 5           |\\n| learning rate            | 0.000001    |\\n| entropy coefficient      | 0.0         |\\n| initial kl coeff         | 0.2         |\\n| target kl                | 0.5         |\\n| discount factor          | 0.99        |\\n| gae lambda               | 0.95        |\\n| clip ratio               | 0.2         |\\n| rollouts top k           | 20          |\\n| value function coeff     | 0.5         |\\n| meteor coeff             | 0.25        |\\n| intent coeff             | 0.75        |\\n| top mask ratio           | 0.9         |\\n| target update iterations | 20          |\\n| supervised+ ppo (or nlpo) steps per update | 1280 |\\n| total number of steps    | 64000       |\\n| batch size               | 64          |\\n| epochs per update        | 5           |\\n| learning rate            | 0.000001    |\\n| entropy coefficient      | 0.0         |\\n| initial kl coeff         | 0.2         |\\n| target kl                | 0.5         |\\n| discount factor          | 0.99        |\\n| gae lambda               | 0.95        |\\n| clip ratio               | 0.2         |\\n| rollouts top k           | 20          |\\n| value function coeff     | 0.5         |\\n| meteor coeff             | 0.50        |\\n| intent coeff             | 0.50        |\\n| top mask ratio           | 0.9         |\\n| target update iterations | 20          |\\n| decoding top k           | 20          |\\n| min length               | 2           |\\n| max new tokens           | 50          |\\n| tokenizer padding side   | left        |\\n| truncation side          | right       |\\n| max length               | 128         |\\n\\nTable 32: DailyDialog Hyperparams: Table shows a list of all hyper-parameters and their settings.\"}"}
{"id": "8aHzds2uUyB", "page_num": 58, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm                                      | Rouge-1 | Rouge-2 | Rouge-L | Rouge-LSum | Meteor | SacreBLEU | BertScore | Intent Accuracy | Distinct 1 | Distinct 2 | H1 | H2 | Unique 1 | Unique 2 | Mean Output Length |\\n|-----------------------------------------------|---------|---------|---------|------------|--------|-----------|----------|----------------|------------|------------|----|----|---------|---------|-------------------|\\n| Zero Shot GPT-2                               | 0.157   | 0.012   | 0.131   | 0.131      | 0.191  | 0.066     | 0.854    | 0.427         | 0.608      | 0.055      | 0.316 | 7.787 | 11.831  | 1574   | 12327             |\\n| Supervised GPT-2                              | 0.162   | 0.020   | 0.138   | 0.138      | 0.186  | 0.064     | 0.855    | 0.437         | 0.635      | 0.065      | 0.342 | 8.051 | 12.119  | 1925   | 13952             |\\n| PPO Meteor + Intent GPT-2                     | 0.168   | 0.012   | 0.142   | 0.142      | 0.221  | 0.085     | 0.861    | 0.474         | 0.581      | 0.058      | 0.310 | 7.653 | 11.437  | 1719   | 12156             |\\n| NLPO Meteor + Intent GPT-2                    | 0.169   | 0.013   | 0.142   | 0.142      | 0.221  | 0.087     | 0.860    | 0.490         | 0.568      | 0.059      | 0.309 | 7.630 | 11.351  | 1718   | 11946             |\\n| Supervised + PPO Meteor + Intent GPT-2        | 0.169   | 0.021   | 0.144   | 0.144      | 0.198  | 0.071     | 0.857    | 0.455         | 0.626      | 0.068      | 0.348 | 8.056 | 12.015  | 1983   | 14170             |\\n| Supervised + NLPO Meteor + Intent GPT-2       | 0.171   | 0.020   | 0.146   | 0.146      | 0.205  | 0.074     | 0.858    | 0.454         | 0.624      | 0.070      | 0.349 | 8.044 | 11.990  | 2051   | 14213             |\\n\\nTable 33: Evaluation of Daily Dialog\\n\\nTable shows lexical, semantic metrics for RL algorithms bench-marked against supervised baseline models.\"}"}
{"id": "8aHzds2uUyB", "page_num": 47, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9 shows the ToTTo instructions, example, and interface used for the human evaluation experiments. We made small modifications to the original code release\u2019s HTML renderer to make the tables display in our HITs. Tables 25, 26 show averaged results, annotator agreement, and the results of statistical significance tests to determine which models output better generations when rated by humans.\"}"}
{"id": "8aHzds2uUyB", "page_num": 48, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Sample 1\\n\\n**Prompt**: Under My Spell Tour\\n\\n**Tour dates**\\n\\n| Date     | North America | Australasia | Asia | North America |\\n|----------|---------------|-------------|------|-------------|\\n| August 1, 1992 | Allentown City | Allentown Fairgrounds Grandstand | | |\\n\\n- **Zero-Shot**\\n  - Under My Spell Tour\\n  - Tour dates\\n  - Allentown Fairgrounds Grandstand\\n\\n- **PPO**\\n  - Under My Spell Tour, August 1, 1992.\\n\\n- **NLPO**\\n  - Under My Spell Tour dates\\n  - Allentown Fairgrounds Grandstand\\n\\n- **Supervised**\\n  - The Under My Spell Tour began on August 1, 1992 at the Allentown Fairgrounds Grandstand in Allentown.\\n\\n- **Supervised + PPO**\\n  - On August 1, 1992, Under My Spell Tour was held at the Allentown Fairgrounds Grandstand in Allentown.\\n\\n- **Supervised + NLPO**\\n  - On August 1, 1992, Under My Spell Tour was held at the Allentown Fairgrounds Grandstand in Allentown.\\n\\n### Sample 2\\n\\n**Prompt**: List of French monarchs\\n\\n**House of Capet (987\u20131328)**\\n\\n| Name | Title |\\n|------|-------|\\n| Philip IV the Fair, the Iron King (Philippe) | King of France and of Navarre (Roi de France et de Navarre) |\\n\\n- **Zero-Shot**\\n  - Under My Spell Tour\\n  - Tour dates\\n  - Allentown Fairgrounds Grandstand\\n\\n- **PPO**\\n  - Philip IV the Fair, the Iron King (Philippe) 5 October 1285.\\n\\n- **NLPO**\\n  - King (Philippe) Philip IV the Fair, the Iron King (Philippe) 5 October 1285\\n\\n- **Supervised**\\n  - Philip IV the Fair, the Iron King (Philippe) was King of France and Navarre from 1285.\\n\\n- **Supervised + PPO**\\n  - Philip IV the Fair, the Iron King was King of France and Navarre from 1285.\\n\\n- **Supervised + NLPO**\\n  - Philip IV the Fair, the Iron King (Philippe) was King of France and Navarre (Roi de France et de Navarre) in 1285.\\n\\n### Sample 3\\n\\n**Prompt**: 87th United States Congress\\n\\n| Vacator | Reason for change | Successor | Date of successor's formal installation |\\n|---------|-------------------|-----------|----------------------------------------|\\n| Benjamin A. Smith II (D) | | Ted Kennedy (D) | November 7, 1962 |\\n\\n- **Zero-Shot**\\n  - Benjamin A. Smith II (D)\\n  - Successor elected November 6, 1962.\\n\\n- **PPO**\\n  - Ted Kennedy (D) succeeding November 6, 1962 as Vacator of the 87th United States Congress.\\n\\n- **NLPO**\\n  - Ted Kennedy (D) Successor elected November 6, 1962.\\n\\n- **Supervised**\\n  - Benjamin A. Smith II (D) served as senate until November 6, 1962 which was later served by Ted Kennedy (D) from November 7, 1962.\\n\\n- **Supervised + PPO**\\n  - Benjamin A. Smith II (D) served until November 6, 1962 and Ted Kennedy (D) succeeded him from November 7, 1962.\\n\\n- **Supervised + NLPO**\\n  - Benjamin A. Smith II (D) served until November 6, 1962 and Ted Kennedy (D) succeeded him from November 7, 1962.\"}"}
{"id": "8aHzds2uUyB", "page_num": 49, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NarrativeQA (Ko\u010disk\u00fd et al., 2018) deals with the task of generating answers to questions about a given story. For training RL methods, we consider two traditional lexical rewards namely Rouge Combined and Rouge-L-Max. We chose T5-base as the base LM since it has been shown to do well at question answering in prior work (Khashabi et al., 2020). We note that the supervised models we use are trained on the UnifiedQA dataset, which contains other QA datasets, and is shown by Khashabi et al. (2020) to outperform supervised fine-tuning only on NarrativeQA. Hyperparams for our models can be found in Table 27.\\n\\n| Model Params | Value |\\n|--------------|-------|\\n| ppo/nlpo steps per update | 5120 |\\n| total number of steps | 512000 |\\n| batch size | 64 |\\n| epochs per update | 5 |\\n| learning rate | 0.000002 |\\n| entropy coefficient | 0.0 |\\n| initial kl coeff | 0.001 |\\n| target kl | 1.0 |\\n| discount factor | 0.99 |\\n| gae lambda | 0.95 |\\n| clip ratio | 0.2 |\\n| rollouts top k | 50 |\\n| value function coeff | 0.5 |\\n| top mask ratio | 0.9 |\\n| target update iterations | 20 |\\n\\nSupervised + ppo (or nlpo) steps per update: 2560\\n\\nDecoding num beams: 4\\n\\nMax new tokens: 50\\n\\nTokenizer padding side: left\\n\\nTruncation side: right\\n\\nMax length: 512\\n\\nTable 27: NarrativeQA Hyperparams: Table shows a list of all hyper-parameters and their settings.\"}"}
{"id": "8aHzds2uUyB", "page_num": 50, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Alg Reward Function | LM | R | R | R | R | L | L | Mean Output Length |\\n|---------------------|----|---|---|---|---|---|---|-------------------|\\n|                     |    |   |   |   |   |   |   |                   |\\n| MSTTR Distinct      | 1  |   |   |   |   |   |   |                   |\\n|                     |    |   |   |   |   |   |   |                   |\\n| Unique Distinct     | 2  |   |   |   |   |   |   |                   |\\n|                     |    |   |   |   |   |   |   |                   |\\n| Mean Output Length  |    |   |   |   |   |   |   |                   |\\n|                     |    |   |   |   |   |   |   |                   |\\n| NarQA              |    | 0.095 | 0.022 | 0.084 | 0.084 | 0.117 | 0.095 | 0.009 | 0.835 | 0.415 | 0.026 | 0.097 | 9.641 | 13.468 | 1880 | 11495 | 31.688 |\\n|                     |    | 0.101 | 0.025 | 0.088 | 0.088 | 0.122 | 0.099 | 0.01  | 0.837 | 0.462 | 0.03  | 0.125 | 9.759 | 13.789 | 2522 | 17806 | 32.352 |\\n|                     |    | 0.097 | 0.023 | 0.085 | 0.085 | 0.118 | 0.098 | 0.009 | 0.836 | 0.418 | 0.025 | 0.096 | 9.652 | 13.528 | 1816 | 10980 | 32.117 |\\n|                     |    | 0.102 | 0.026 | 0.089 | 0.089 | 0.124 | 0.1  | 0.01  | 0.837 | 0.445 | 0.029 | 0.119 | 9.776 | 13.75 | 2181 | 14569 | 31.555 |\\n| Supervised          |    | 0.378 | 0.190 | 0.367 | 0.367 | 0.581 | 0.099 | 0.209 | 0.931 | 0.609 | 0.156 | 0.534 | 9.807 | 13.657 | 3250 | 14995 | 4.923 |\\n| Supervised + PPO    |    | 0.38 | 0.177 | 0.371 | 0.371 | 0.585 | 0.09 | 0.229 | 0.931 | 0.64 | 0.174 | 0.559 | 10.132 | 13.547 | 3326 | 13785 | 4.353 |\\n|                     |    | 0.398 | 0.21 | 0.393 | 0.373 | 0.589 | 0.096 | 0.24  | 0.971 | 0.679 | 0.185 | 0.595 | 10.304 | 13.694 | 3371 | 15067 | 4.728 |\\n| Supervised + NLPO   |    | 0.368 | 0.18 | 0.36 | 0.36 | 0.585 | 0.083 | 0.239 | 0.931 | 0.641 | 0.187 | 0.576 | 10.201 | 13.452 | 3287 | 12436 | 3.913 |\\n\\nTable 28: Evaluation of NarrativeQA: Reference Metrics, supervised is based on UnifiedQA (Khashabi et al., 2020).\"}"}
{"id": "8aHzds2uUyB", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Imitation learning for NLP. Algorithms such as Schedule Sampling (SS) (Bengio et al., 2015), Parallel SS (Duckworth et al., 2019), SS for Transformers (Mihaylova & Martins, 2019), Diffential SS (Goyal et al., 2017), LOLS (Lampouras & Vlachos, 2016; Chang et al., 2015), TextGAIL (Wu et al., 2021b), and SEARNN (Leblond et al., 2017), have been inspired by DAGGER (Ross et al., 2011) and SEARN (Daum\u00e9 et al., 2009). However, these algorithms are known to suffer from exposure bias in generation (Chiang & Chen, 2021; Arora et al., 2022) and the cliff MDP problem (Husz\u00e1r, 2015; Agarwal et al., 2019; Swamy et al., 2021).\\n\\nRL for Large Action Spaces.\\n\\nMIXER (Ranzato et al., 2016) combined ideas from schedule sampling and REINFORCE (Williams, 1992). Bahdanau et al. (2016) proposed an actor-critic algorithm to address the variance/large action space problems when using REINFORCE for language generation; follow-up works such as KG-A2C (Ammanabrolu & Hausknecht, 2020), TrufLL (Martin et al., 2022), AE-DQN (Zahavy et al., 2018), and GALAD (Ammanabrolu et al., 2022) addressed similar issues by attempting to eliminate and reduce the action space during exploration.\\n\\nRL for NLP.\\n\\nRL, often in the form of bandit learning, has been used to improve models in machine translation (Wu et al., 2016; Nguyen et al., 2017; Kiegeland & Kreutzer, 2021), summarization (Stiennon et al., 2020; Paulus et al., 2017), dialogue (Li et al., 2016; Zhou et al., 2017; Jaques et al., 2020), image captioning (Rennie et al., 2017), question generation (Pang & He, 2021), text-games (Narasimhan et al., 2015; Hausknecht et al., 2020), and more (Ranzato et al., 2016; Snell et al., 2022).\\n\\nLu et al. (2022) adapt reward-conditioned transformers (Chen et al., 2021) for several language generation tasks. RL has been the focus of efforts to align LMs with human preferences (Stiennon et al., 2020; Wu et al., 2021a; Nakano et al., 2021; Ziegler et al., 2019), e.g., Ouyang et al. (2022) fine-tuned large language model with PPO (Schulman et al., 2017) to align with models of human preference, but their non-public dataset doesn't enable comparison. Though RL has been successful in some of the use cases described above, it has simultaneously been critiqued for being significantly less stable than supervised LM training (Choshen et al., 2020). As a result, there is relatively little consensus if RL is a worthwhile consideration for training LMs compared to, say, collecting additional supervised data.\\n\\n3.1 Environments: Generation as a Token-Level MDP\\n\\nEach environment is an NLP task: we are given a supervised dataset \\\\( D = \\\\{(x_i, y_i)\\\\}_{i=1}^N \\\\) of \\\\( N \\\\) examples, where \\\\( x \\\\in X \\\\) is a language input and \\\\( y \\\\in Y \\\\) is the target string. Generation can be viewed as a Markov Decision Process (MDP) \\\\( \\\\langle S, A, R, P, \\\\gamma, T \\\\rangle \\\\) using a finite vocabulary \\\\( V \\\\). Each episode in the MDP begins by sampling a datapoint \\\\((x, y)\\\\) from our dataset and ends when the current time step \\\\( t \\\\) exceeds the horizon \\\\( T \\\\) or an end of sentence (EOS) token is generated. The input \\\\( x = (x_0, \\\\ldots, x_m) \\\\) is a task-specific prompt that is used as our initial state \\\\( s_0 \\\\), where \\\\( s_0 \\\\in S \\\\) and \\\\( S \\\\) is the state space with \\\\( x_m \\\\in V \\\\). An action in the environment \\\\( a_t \\\\in A \\\\) consists of a token from our vocabulary \\\\( V \\\\). The transition function \\\\( P: S \\\\times A \\\\rightarrow \\\\Delta(S) \\\\) deterministically appends an action \\\\( a_t \\\\) to the end of the state \\\\( s_t-1 = (x_0, \\\\ldots, x_m, a_0, \\\\ldots, a_{t-1}) \\\\). This continues until the end of the horizon \\\\( t \\\\leq T \\\\) and we obtain a state \\\\( s_T = (x_0, \\\\ldots, x_m, a_0, \\\\ldots, a_T) \\\\). At the end of an episode a reward \\\\( R: S \\\\times A \\\\times Y \\\\rightarrow \\\\mathbb{R} \\\\) that depends on the \\\\((s_T, y)\\\\) (e.g., an automated metric like PARENT Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) style...\"}"}
{"id": "8aHzds2uUyB", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"API for an RL environment that simulates this LM-Based MDP formulation. This abstraction allows for new tasks to be added quickly with compatibility across all implemented algorithms.\\n\\n### 3.2 Reward Functions and Evaluation Metrics\\n\\nBecause RL4LMs provides a generic interface for per-token or per-sequence generation rewards, it is possible to quickly apply a wide array of RL algorithms to a similarly diverse range of textual metrics-as-rewards. Specifically, we provide interfaces to:\\n\\n1. **n-gram overlap metrics** such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), SacreBLEU (Post, 2018), METEOR (Banerjee & Lavie, 2005);\\n2. **model-based semantic metrics** such as BertScore (Zhang et al., 2019) and BLEURT (Sellam et al., 2020) which generally provide higher correlation with human judgment;\\n3. **task-specific metrics** such as CIDER (Vedantam et al., 2015), SPICE (Anderson et al., 2016) (for captioning/commonsense generation), PARENT (Dhingra et al., 2019) (for data-to-text) and SummaCZS (Laban et al., 2022) (for factuality of summarization);\\n4. **diversity/fluency/naturalness metrics** such as perplexity, Mean Segmented Type Token Ratio (MSSTR) (Johnson, 1944), Shannon entropy over unigrams and bigrams (Shannon, 1948), the ratio of distinct n-grams over the total number of n-grams (Distinct-1, Distinct-2) and count of n-grams that appear only once in the entire generated text (Li et al., 2015);\\n5. **task-specific, model-based human preference metrics** such as classifiers trained on human preference data collected in the methodology of Ouyang et al. (2022).\\n\\n### 3.3 On-Policy Actor-Critic Algorithms\\n\\nRL4LMs supports fine-tuning and training LMs from scratch via on-policy actor-critic algorithms on language environments. Formally, this class of algorithms allows us to train a parameterized control policy defined as $\\\\pi_\\\\theta: S \\\\rightarrow \\\\Delta(A)$, a function that attempts to select an action in a given state so as to maximize long term discounted rewards over a trajectory $E_{\\\\pi}[\\\\sum_{t=0}^{\\\\infty} \\\\gamma^t R(s_t, a_t)]$.\\n\\nOur benchmark experiments focus on fine-tuning a pre-trained LM denoted as $\\\\pi_0$ from which we initialize our agent's policy $\\\\pi_\\\\theta = \\\\pi_0$. Similarly, the value network $V_\\\\phi$ used to estimate the value function is also initialized from $\\\\pi_0$ except for the final layer which is randomly initialized to output a single scalar value. As with other deep RL actor-critic algorithms, we define our value and Q-value functions as $V_\\\\pi_t = E_{a_t \\\\sim \\\\pi}[\\\\sum_{\\\\tau=t}^{\\\\infty} \\\\gamma^\\\\tau R(s_\\\\tau, a_\\\\tau, y)]$, $Q_\\\\pi_t(s_t, a_t) = R(s_t, a_t, y) + \\\\gamma E_{s_{t+1} \\\\sim P}[V_\\\\pi_{t+1}(s_{t+1})]$, leading to a definition of our advantage function as $A_\\\\pi_t(s_t, a_t) = Q_\\\\pi_t(s_t, a_t) - V_\\\\pi_t(s_t)$. To increase training stability, advantage is approximated using Generalized Advantage Estimation (Schulman et al., 2015b).\\n\\nGiven an input-output pair $(x, y)$ and generation predictions from our agent; because the environment rewards are sequence-level and sparse, following Wu et al. (2021a) we regularize the reward function using a token-level KL penalty for all on-policy algorithms, to prevent the model from deviating too far from the initialized LM $\\\\pi_0$. Formally, the regularized reward function is:\\n\\n$$\\\\hat{R}(s_t, a_t, y) = R(s_t, a_t, y) - \\\\beta \\\\text{KL}(\\\\pi_\\\\theta(a_t|s_t)||\\\\pi_0(a_t|s_t))$$\\n\\nwhere $\\\\hat{R}$ is the regularized KL reward, $y$ is gold-truth predictions, $\\\\text{KL}(\\\\pi_\\\\theta(a_t|s_t)||\\\\pi_0(a_t|s_t)) = \\\\log \\\\pi_0(a_t|s_t) - \\\\log \\\\pi_\\\\theta(a_t|s_t)$ and the KL coefficient $\\\\beta$ is dynamically adapted (Ziegler et al., 2019).\\n\\nFurther details on actor-critic methods can be found in Appendix A.\\n\\n### 4 NLPO: Natural Language Policy Optimization\\n\\nLanguage generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2016; Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively. We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods. To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018; Huang & Onta\u00f1\u00f3n, 2020; Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-$p$ sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter $p$ (Holtzman et al., 2018).\\n\\nSpecifically, NLPO maintains a masking policy $\\\\pi_\\\\psi$: the masking policy is a copy of the current policy ($\\\\pi_\\\\theta$), but is updated only every $\\\\mu$ steps. A parameterized-invalid-mask is created from $\\\\pi_\\\\psi$ by first...\"}"}
{"id": "8aHzds2uUyB", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nselecting the top-$p$ tokens from the vocabulary, and then applying an invalid-mask to the remaining tokens\u2014i.e. setting their probabilities to zero when sampling actions from $\\\\pi_\\\\theta$ during training; this periodic updating policy $\\\\pi_\\\\psi$ is inspired by off-policy Q-learning algorithms (Andrychowicz et al., 2017), providing the policy $\\\\pi_\\\\theta$ with an additional constraint that balances between the benefits of containing more task relevant information than the KL penalty derived from $\\\\pi_\\\\theta$ and the risk of reward hacking. We provide pseudocode in Algorithm 1 (green portions highlight the differences with PPO).\\n\\nAlgorithm 1\\n\\nInput: Dataset $D = \\\\{(x_i, y_i)\\\\}_{i=1}^N$ of size $N$\\nInput: initial policy parameters $\\\\pi_\\\\theta$\\nInput: initial LM $\\\\pi_0$\\nInput: initial value function parameters $V_\\\\phi$\\nInput: initialize parameterized masked policy $\\\\pi_\\\\psi(\\\\cdot|\\\\cdot, \\\\pi_\\\\theta)$ with parameterized top-$p$ policy $\\\\pi_\\\\theta$\\nInput: policy update frequency $\\\\mu$\\n\\nrepeat\\nSample mini-batch $D_m = \\\\{(x_m, y_m)\\\\}_{m=1}^M$ from $D$\\nCollect trajectories $T_m = \\\\{\\\\tau_i\\\\}$ by running policy $\\\\pi_\\\\psi$ in for batch $D_m$ in env. \\\\hspace{1cm} \\\\text{\\\\textbf{\\\\small{\\\\textcopyright} Eq.6}}\\nCompute Preference and KL penalty rewards $\\\\hat{R}_t$ \\\\hspace{1cm} \\\\text{\\\\textbf{\\\\small{\\\\textcopyright} Eq. 1}}\\nCompute the advantage estimate $\\\\hat{A}_t$ \\\\hspace{1cm} \\\\text{Sec. 3.3}\\nUpdate the policy by maximizing the PPO-Clip objective:\\n$$\\\\pi_{\\\\theta}^{m+1} = \\\\arg\\\\max_{\\\\theta} \\\\frac{1}{|D_m|} \\\\sum_{\\\\tau \\\\in D_m} T \\\\sum_{X(\\\\tau)} \\\\min \\\\left( r_t(\\\\theta), 1 - \\\\epsilon, 1 + \\\\epsilon \\\\right) A_{\\\\pi_{\\\\theta}^m}$$\\nwhere $r_t(\\\\theta) = \\\\pi_{\\\\theta}(a_t|s_t)$.\\n\\nUpdate the value function:\\n$$V_{\\\\phi}^{m+1} = \\\\arg\\\\min_{\\\\phi} \\\\frac{1}{|D_m|} \\\\sum_{\\\\tau \\\\in D_m} T \\\\sum_{X(\\\\tau)} V(\\\\tau) - \\\\hat{R}_t$$\\n\\nUpdate the parameterized masked policy every $\\\\mu$ iterations:\\n$$\\\\pi_{\\\\psi}^{n+1}(\\\\cdot|\\\\cdot, \\\\pi_{\\\\theta}^{m+1})$$\\nuntil convergence and return $\\\\pi_{\\\\theta}$.\\n\\nGRUE (GENERALIZED UNDERSTANDING VAL)\\nGRUE is a collection of 7 generative NLP tasks. To combat reward hacking for any single metric, each task is evaluated at test time according to a task-specific mix of metrics, detailed in Table 1. The metrics span two categories. Task preference metrics capture how well the models produce generations that satisfy the desiderata of the specific generation task, e.g., for CommonGen, if the generations contain all the required words, or for IMDB, how positive the generated completions are. Naturalness metrics capture fluency, readability, etc. and provide perspective on factors beyond semantics. At training time, there are no special restrictions: models are free to use the supervised data, compute metrics on intermediate generations, etc. Train/val/test splits follow the original works. All results are averaged over multiple seeds, with exact counts being found in Appendix B.\\n\\nExperimental Setup. We use RL4LMs to test a large range of algorithms on the GRUE benchmark. Specifically: We compare 3 algorithms for direct fine-tuning \u2014 Supervised, PPO, and NLPO. In $\\\\pi_{\\\\psi}$ could be trained with alternate sampling techniques like top-$k$ or beam search (or even hard-coded via rules by domain experts), though we find top-$p$ sampling to be most effective in practice. We consider PPO representative of the present state-of-the-art \u2014 in particular, we do not consider the popular REINFORCE (Williams, 1988; Williams, 1992), as recent works have shown PPO to be strictly superior to REINFORCE in multiple domains (Schulman et al., 2017).\"}"}
{"id": "8aHzds2uUyB", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset             | Task                  | Input                         | Output                        | Task Preference | Metrics(s)                        |\\n|---------------------|-----------------------|-------------------------------|-------------------------------|-----------------|-----------------------------------|\\n| IMDB (Maas et al., 2011) | Text Continuation     | Partial Movie Review          | A positive completion of the movie review. | Learned Sentiment Classifier | Perplexity (GPT-2)               |\\n| CommonGEN (Lin et al., 2020) | Generative Commonsense Concept Set | A sentence coherently using all input concepts. | CIDER; ROUGE-2, L; BLEU-3, 4; METEOR; Coverage SPICE |\\n| CNN Daily Mail (Hermann et al., 2015) | Summarization | News Article                    | Summarized article. | SummCZS; ROUGE-1, 2, L; Sum; METEOR; BLEU | BertScore                 |\\n| ToTTo (Parikh et al., 2020) | Data to Text Highlighted WikiTable | Factually accurate text describing the information. | SacreBLEU; PARENT BLEURT |\\n| WMT-16 (en-de) (Bojar et al., 2016) | Machine Translation | Text (English)                 | Translated text (German). | TER; cHRF; ROUGE-1, 2, L, LSum, LMax; METEOR; SacreBLEU, BLEU | BertScore |\\n| NarrativeQA (Ko\u02c7cisk`y et al., 2018) | Question Answering | Question Context (a Story)     | Abstractive answer to the question. | ROUGE-1, 2, L, LSum, LMax; METEOR; BLEU | BertScore |\\n| DailyDialog (Li et al., 2017) | Chitchat Dialogue | Dialogue History               | A conversational response. | METEOR; Learned Intent Classifier | BertScore |\\n\\nTable 1: GRUE Benchmark using RL4LMS showing the various tasks, input and output types, and the metrics used. We note that we test RL algorithms on these tasks for a wider range of possible rewards than just the task specific ones shown here. Unless specified, datasets are in English.\\n\\nIn addition, we consider a hybrid approach of supervised learning and our RL methods by applying PPO and NLPO on checkpoints that have been fine-tuned in a supervised fashion\u2014we call these Supervised+PPO, Supervised+NLPO. As an additional baseline, we additionally run zero-shot evaluations where we design prompts which aim to elicit task-specific generations, but with no training data or parameter updates.\\n\\nFor each task, to isolate the effect of training method, we select a single pre-trained LM backbone. For IMDB text continuation we use GPT-2 (117m parameters), and for the rest of the tasks we use T5-base (220m parameters). For our RL models (PPO, NLPO, Supervised+PPO, Supervised+NLPO), for a thorough investigation of how reward-hacking might interplay with GRUE, we run a separate set of experiments optimizing multiple task rewards for each task independently, e.g., for CommonGEN which has 6 task rewards (CIDER, ROUGE-2, ROUGE-L, BLEU-3, BLEU-4, METEOR) we run 6 different experiments optimizing each metric independently and report all possible metrics seen in Table 1 regardless of which individual metric was being optimized for.\\n\\nHuman Participant Study. We gather human judgments for five of the tasks in GRUE. In doing so, our goals are 1) to validate that the automated metrics we selected for GRUE correlate with human judgments with respect to relative ranking between models; and 2) to provide additional empirical comparisons regarding NLPO vs. PPO, ablations to study the effects of the KL naturalness penalty, etc. We specifically consider IMDB, CommonGEN, ToTTO, DailyDialog, and CNN Daily Mail. For each individual sample in a task, we ask 3 unique human raters to provide Likert judgments of 1) quality, i.e., for the specific task, how correct/appropriate is the generation, given the context, and 2) fluency, i.e., how well-written is the generation. We used Amazon Mechanical Turk, and paid crowdworkers a minimum of $15/hr. More details, including qualification information, interface screenshots, instructions, etc. are given in the corresponding Appendicies.\\n\\n5.1 Results on GRUE: Which Algorithm Should Be Used to Earn References?\\n\\nFigures 2(a), 2(b) present the results on GRUE, split into task metrics and naturalness metrics, and Tables 2, 3 highlight key results via ablation studies. Full results are available in Appendix B. For text continuation and summarization, with non-trivial zero-shot performance, RL tends to perform better than supervised training, but for tasks like CommonGEN and ToTTO, which have very low zero-shot performance, supervised training performs best\u2014with both approaches outperforming zero-shot. However, using RL+Supervised learning in conjunction works best; NLPO+supervised and PPO+supervised usually always outperforms NLPO/PPO (or supervised in isolation) across both task metrics and naturalness metrics. Supervised warm-starting is particularly effective for CommonGEN and ToTTO, which our results suggest are more prone to reward hacking. The one exception to this trend is DailyDialog where the RL models outperform warm-started Supervised+RL models likely due to the low performance of the Supervised models. We note that Supervised+NLPO using a\"}"}
{"id": "8aHzds2uUyB", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8 shows the summarization instructions and interface used for the human evaluation experiments. Participants weren't required to read the entire article, but to encourage some reading, a minimum time on the window of 15s was enforced via hiding the sliders. Tables 20, 21 show averaged results, annotator agreement, and the results of statistical significance tests to determine which models output better generations when rated by humans.\\n\\nB.5.4 Qualitative Analysis\\n\\nWe show sample generations from each of the algorithms for three randomly picked prompts below.\\n\\nSample 1\\nPrompt: Manchester City are confident UEFA\u2019s punishment for breaching financial fairplay regulations will be lifted this summer which would allow them to bid for stellar names like Raheem Sterling, Gareth Bale, Kevin de Bruyne and Ross Barkley. City boss Manuel Pellegrini has been hampered over the past year by UEFA restricting them to a net transfer spend of 49 million in each window and keeping the club\u2019s overall wage bill to its current level of 205 million-a-year. UEFA\u2019s settlement with City published in May stated those penalties would remain in place until the end of the 2015/16 season but the club\u2019s latest financial figures showed drastically-reduced losses of 23 million which they feel proves they are now compliant with FPP regulations. Manuel Pellegrini is hoping that the financial restrictions imposed by UEFA for a breach of FFP rules will be lifted at the end of this season. Manchester City have been limited to a net spend of 49 million in the last two transfer windows - they spent 25 million bringing Wilfried Bony in from Swansea in January. Ahead of Monday night\u2019s trip to Crystal Palace, Pellegrini was certainly talking like a man excited at the prospect of signing \u2018crack\u2019 players this summer. \u2018I think that next season we don\u2019t have any restrictions so we will be in the same position that all the other English clubs have,\u2019 said Pellegrini. \u2018It\u2019s important. You have so many strong teams here in England and in Champions League, you can not allow them to keep the advantage every year; having less players to put in your squad or spending less money. We spend money, of course we always spend money, but they spent more.\u2019 Manchester United, Barcelona, Liverpool and Arsenal have all paid more in transfer fees in the past 12 months than City who were traditionally Europe\u2019s biggest...\"}"}
{"id": "8aHzds2uUyB", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"After the club was taken over by Abu Dhabi owners in 2008, Uefa also ordered City to play with a reduced squad from 25 players to 21 in the Champions League this season. While that restriction has now ended, any time reduction in the penalties on spending and wages is more controversial. Arsenal have paid more in transfer fees than City in the last 12 months, including 30m on Alexis Sanchez. The document published last May by UEFA\u2019s Club Financial Control Body investigative chamber explicitly said City\u2019s financial penalties would run for two seasons at least and there has been no official deviation from that decision.\\n\\nThe published statement said at the time: \u201cManchester City agrees to significantly limit spending in the transfer market for the seasons 2014/15 and 2015/16. It means City will have to argue their case with Uefa that as they have been financially compliant over the past year, they deserve to be free of restrictions moving forward. They have successfully argued their case with UEFA before. Last summer they persuaded the governing body to allow them to bypass the normal quota of eight homegrown players as their Champions League squad had been reduced. Eliaquim Mangala joined the champions from Porto for 32m last summer. The reigning Premier League champions have only paid money for Fernando, Willie Caballero, Eliaquim Mangala and Wilfried Bony in the last two transfer windows and that was part-paid by the sales of Javi Garcia and Jack Rodwell. Pellegrini admitted they weren\u2019t in a position to compete with neighbours United for established world stars like Angel di Maria because of the FFP penalties. It has cost City on the pitch. Even if they win their remaining eight Premier League fixtures their points tally will be lower than last season and they crashed out once again at the last-16 stage of the Champions League. Pellegrini and director of football Txiki Begiristain both accept the squad needs replenishing and they want to be in the market for the top available players. Manchester City have been heavily linked with a move for former Chelsea midfielder Kevin de Bruyne. The club also want to be involved in any race for Liverpool contract rebel Raheem Sterling. They have been strongly linked with former Chelsea winger Kevin de Bruyne who has been one of the best players in the Bundesliga with Wolfsburg. And if Liverpool do allow Sterling to leave, City want to be involved in the chase for one of England\u2019s most outstanding talents to help fit in with quotas for homegrown players.\\n\\n\u201cWe are always open to criticism because everyone expects a high level of performance,\u201d he said. \u201cThe major issue with our team always seems to be the money but this season I think we were the team that spent less than any other team. That is the weird thing. For this club trying to be a big team in so few years, maybe we are paying the cost for that.\u201d Since the transfer penalties were introduced, City have spent 91million on players in the last two windows (Net spend 68million). That compares to Manchester United 145.5million, Liverpool 113 million, Arsenal 92.5million and Chelsea 82.6million. Over the same time period Barcelona spent 118.3million on players and Real Madrid 81.2million though they also broke the world transfer record for Gareth Bale the previous summer.\"}"}
{"id": "8aHzds2uUyB", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"re not going to be able to breathe,\\\" Johnson recalled the flight attendant saying. The Federal Aviation Administration on Wednesday initially reported a pressurization problem with SkyWest Flight 5622, and said it would investigate. It later issued a statement that did not reference any pressurization issues. \\\"It was like being trapped and you couldn't do anything. You have no control over it. Your body just felt pressured,\\\" Johnson said. Johnson said his girlfriend was seeing spots and that \\\"when I was blinking, it was really bright. I could barely see.\\\" According to Marissa Snow, spokeswoman for SkyWest, three passengers reported a loss of consciousness while on the flight. Fifteen passengers were examined by medical personnel at the Buffalo airport, but no one was transported to the hospital, she said. The spokeswoman said that maintenance personnel found no indication of a pressurization problem with the aircraft, an Embraer E170, and that the airline continues to investigate the cause. An official with the National Transportation Safety Board told CNN that the agency is in communication with the FAA and SkyWest to gather information on the incident to better understand what took place.\\n\\nMary Cunningham was the nurse on board who attended to the first sick passenger. \\\"There was a woman, very lethargic, very nauseous, turning gray, just not looking good,\\\" Cunningham said. After administering oxygen to the woman, Cunningham, a nurse at Yale-New Haven Hospital in Connecticut, returned to her seat. Soon after, Cunningham said, she was called up again by the flight attendant after another women near the first started to lose consciousness. \\\"It was something that was affecting oxygen in that area,\\\" which was near the middle of the plane, she said. Cunningham said she had to return to her seat after feeling lightheaded. There were 84 passengers on board the plane, including nine crew members. Flight 5622 was originally scheduled to fly from Chicago to Hartford, Connecticut. The plane descended 28,000 feet in three minutes. \\\"It would feel like a roller coaster -- when you're coming over the top and you're going down,\\\" CNN aviation analyst Mary Schiavo said, describing how such a descent would feel. \\\"You know that these pilots knew they were in a very grave and very serious situation.\\\" Johnson, flying to visit his parents with their only grandchild, was glad that roller coaster ride ended safely. \\\"It was a shaky ride. It wasn't anything I would like to experience again,\\\" he said. CNN's Jean Casarez, Josh Gaynor, Dana Ford and Stephanie Gallman contributed to this report.\\n\\n\\\"you're going to feel dizzy and woozy and lightheaded,\\\" Johnson says. \\\"it was like being trapped and you couldn't do anything,\\\" Johnson says. \\\"the plane was trying to get back on the ground,\\\" passenger says of fear.\\n\\nReferences:\\n\\nThree passengers report a loss of consciousness on SkyWest flight. But officials say there is no evidence of a pressurization problem.\\n\\nSkyWest flight 5312 made emergency landing in Buffalo, New York. Passengers were told they were not going to be able to breathe, passenger recalls attendant. The FAA initially reported a pressurization problem with the flight 5622.\\n\\nSkyWest flight 5622 made emergency landing in Buffalo, New York. Larry Johnson says flight attendant announced there was a pressurization problem. Three passengers reported a loss of consciousness while on flight; no one transported to hospital.\\n\\nThe FAA says it is investigating the cause of the emergency landing. SkyWest Flight 5622 made an emergency landing in Buffalo, New York. The flight was carrying a family of three people. The flight attendant told the passengers to prepare for the landing.\\n\\n\\\"I thought we were done,\\\" the father of one passenger says. The flight attendant announced there was a pressurization problem. The flight made an emergency landing in Buffalo, New York. Three passengers reported a loss of consciousness while on the flight.\\n\\nSkyWest Airlines flight made an emergency landing in Buffalo, New York, on Thursday. \\\"There was no doubt in my mind that we weren't going to land,\\\" passenger says. \\\"You're going to feel dizzy and woozy and lightheaded...you're not going to be able to breathe,\\\" he says.\\\"\"}"}
{"id": "8aHzds2uUyB", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to losing on penalties. This was no time to repeat the mistake. \u2018I tried to emphasise to the players they would never have a better time to go on and beat Celtic, down to 10 men in the semi final of a cup. We needed to go for it,\u2019 Hughes said. \u2018Before Raven scored at the back post I was looking to change it.\\n\\nI was going to bring on another winger, Aaron Doran, and put him in the full-back position over on the right, but more advanced so he could take their left back on. Thankfully I didn\u2019t do that and David Raven came up with the goal. Virgil Van Dijk (centre) fired Celtic into an early lead with a superb free-kick in the 18th minute.\\n\\n\u2018I didn\u2019t realise this is the first time the club have been in the final of the Scottish Cup and that\u2019s a remarkable achievement given it was only formed 20 years ago. \u2018It is a great story isn\u2019t it? It\u2019s an absolutely fantastic story. It is 20 odd years since the amalgamation. We are a small provincial club up there in the Highlands.\\n\\nWe have lost a real inspirational skipper in Richie Foran right from the start of the season. He has never played. We have had to adjust to that. \u2018We had to sell Billy McKay, our top goalscorer, at Christmas. We have had to go again and adjust. I am a very humble guy and I am grateful and thankful that injuries have never caught up with us.\u2019\\n\\nThere is remarkable irony in the fact Falkirk will be the opponents for the final. A former Bairns captain, he was manager of the club in 2009 when they lost to Rangers at Hampden. Former Falkirk captain and manager John Hughes will take on his former club in the final.\\n\\n\u2018I had a lot of great times at Falkirk. So much so that it is possibly my favourite time in my playing career. I am still friendly with an awful lot of the characters who were in that dressing room. Neil Oliver is a good friend of mine from my Falkirk days. He comes along and sits on the bench and gives me a wee hand out.\\n\\nThat is the spirit that we had at that club. I have met some great guys. Brian Rice, my ex-coach, Davie Weir, Ian McCall, the list is endless. I was just talking the other day about that day at Falkirk. There are times even now when I see.\\n\\n\u2018I have a real special, special feel for the place. I am not kidded Falkirk is a right good side, trust me on that. No side goes to Tynecastle and beats Hearts without being a good side. No side beats Hibs the way they were playing without being a good side. You might say we are favourites I say it is even stevens and I am just looking forward to when the days comes.\u2019\\n\\nDavid Raven (second right) celebrates with his Inverness team-mates following his extra-time winner. An Inverness and Falkirk final would not have been the choice of sponsors William Hill, you suspect. Vast swathes of Hampden\u2019s west stand were empty, a 12.15 kick-off for the benefit of broadcasters proving unhelpful.\\n\\nThere will be no ticket rows this time. Not over a final unlikely to raise a crowd much higher than 35,000 in a stadium of 52,000. The contract with Hampden plc is signed, with no question of moving it. If the bitter amalgamation of 21 years ago produced a fine football club it also disenfranchised many of those who followed the old ones.\\n\\n\u2018I hope this encourages the next generation to follow Inverness,\u2019 added Hughes. \u2018In the peer groups it is still Celtic and Rangers they follow but I hope the next ones will want to support their local club. \u2018If we can add another few hundred or thousand to the support then we are playing our part.\\n\\nZero Shot: Inverness beat Celtic 2-0 in their opening game of the season. Josh Meekings was the subject of a handball after the break. the break could have meant a penalty and a red card for Craig Gordon.\\n\\nReferences: Josh Meekings handball should have been a penalty and a red card. If the penalty was awarded and converted Celtic would have gone to 2-0. Caley manager John Hughes admitted his side were fortunate in win. Virgil van Dijk scored Celtic\u2019s opener with a superb free-kick. Celtic keeper Craig Gordon was sent off early in the second half. Greg Tansey, Edward Ofere and finally Daven Raven scored for Caley.\\n\\nPPO: Josh Meekings handball could have meant sending off for Celtic in extra-time win. John Hughes says the break could have meant an penalty and red card. Caley boss also admits he would feel aggrieved had Celtic won the championship.\\n\\nNLPO: Caley beat Celtic 2-0 in their Scottish Cup clash. Josh Meekings handball could have led to a penalty and red card. John Hughes says the break was probably the turning point in the game. defender does not believe defensive striker can get away with being shown the yellow card - despite playing in same half we never scored.\\n\\nSupervised: Inverness beat Celtic 2-0 in their Scottish Cup clash. Josh Meekings appeared to get away with a handball on the line. Another player was sent off for a spot kick which ended in a red card. Caley manager John Hughes says the break could have meant a penalty. Hughes says he \u2018would feel aggrieved\u2019 after the break.\\n\\nSupervised + PPO: Caley beat Celtic 1-0 in their Scottish Cup encounter at the Etihad. Josh Meekings\u2019 handball on the line should have also seen a sending off. But Craig Gordon got away with a spot kick and sending off. John Hughes said it was a turning point in the game for Inverness.\\n\\nSupervised + NLPO: Josh Meekings appears to get away with a handball after Inverness\u2019s win. Inverness were down to 10 men when Craig Gordon was sent off. John Hughes says the break could have meant a penalty and red card. Hughes says Celtic will go on and win the Scottish Cup with win.\"}"}
{"id": "8aHzds2uUyB", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"PPO: will bring victory to the fight against hatred and extremism. Al Pacino is a very beautiful, well executed actor who brings an absolute intensity and sensibility to his portrayal of love and loyalty. A must see! Recommended! Also, an Excellent NLPO: I would watch and enjoy. 7/10, Oscar Wilde is always a pleasure to see. A must see. Highly recommend. Highly recommend! One of my top ten films of all time. A must see!!\\n\\nNLPO: the whole film would have fallen to the same fate, just as it did just a couple hundred years ago. Don\u2019t miss it. It\u2019s a real classic. Highly Recommended. * outta five stars for it!\\n\\nSupervised: his performance (so far) would seem mere shadow. He is truly in the middle of a movie, and this film is one of those films where he can be convincing in it (and his trademark acting, as you can see in the\"}"}
{"id": "8aHzds2uUyB", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"CommonGen (Lin et al., 2020) deals with the task of generating coherent sentences describing an input set of concepts (e.g., \\\"a man is throwing a frisbee\\\"). For training RL methods, we consider 3 traditional lexical rewards namely Rouge-1, Rouge-avg (which is an average of Rouge-1, 2 and L) and meteor. Additionally, we also train with task-specific rewards such as CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016) and SPiDer (Liu et al., 2017) which is a just a linear combination of both with equal weights. We chose T5-base as the base LM since it is well-suited for structure to text tasks. We additionally note that concept set inputs are prefixed with \\\"generate a sentence with:\\\" to encourage exploration.\\n\\nDuring our initial experiments when fine-tuning directly on an LM, we observed that policy learns to repeat the prompted concepts in order to maximize rewards resulting in a well-known problem of reward hacking. To mitigate this, we add a penalty score of $-1$ to the final task reward if the n-grams of prompt text overlaps with generated text. In contrast, when initialized with a supervised policy, this problem is not seen and hence penalty score is not applied. We use beam search as the decoding method during evaluation whereas for rollouts, we use top k sampling to favor exploration over exploitation. Table 11 provides an in-depth summary of setting of hyperparameter values along with other implementation details.\\n\\n| Model Params value | value |\\n|--------------------|-------|\\n| supervised batch size: | 8 |\\n| epochs: | 4 |\\n| learning rate: | 0.00001 |\\n| learning rate scheduler: | cosine |\\n| weight decay: | 0.01 |\\n| ppo/ nlpo steps per update: | 1280 |\\n| total number of steps: | 256000 |\\n| batch size: | 64 |\\n| epochs per update: | 5 |\\n| learning rate: | 0.000002 |\\n| entropy coefficient: | 0.01 |\\n| initial kl coeff: | 0.001 |\\n| target kl: | 2.0 |\\n| discount factor: | 0.99 |\\n| gae lambda: | 0.95 |\\n| clip ratio: | 0.2 |\\n| value function coeff: | 0.5 |\\n| top mask ratio: | 0.9 |\\n| target update iterations: | 20 |\\n| supervised+ ppo (or nlpo) steps per update: | 1280 |\\n| total number of steps: | 128000 |\\n| batch size: | 64 |\\n| epochs per update: | 5 |\\n| learning rate: | 0.000002 |\\n| entropy coefficient: | 0.01 |\\n| initial kl coeff: | 0.01 |\\n| target kl: | 1.0 |\\n| discount factor: | 0.99 |\\n| gae lambda: | 0.95 |\\n| clip ratio: | 0.2 |\\n| value function coeff: | 0.5 |\\n| top mask ratio: | 0.9 |\\n| target update iterations: | 20 |\\n| decoding num beams: | 5 |\\n| min length: | 5 |\\n| max new tokens: | 20 |\\n| tokenizer padding side: | left |\\n| max length: | 20 |\\n\\nTable 11: CommonGen Hyperparams - Table shows a list of all hyper-parameters and their settings.\"}"}
{"id": "8aHzds2uUyB", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Tables 13, 12 presents our benchmarking results with 6 reward functions along with supervised baseline performances on dev and test sets respectively. Our main finding is that warm-started initial policies are crucial for learning to generate coherent sentences with common sense. Without warm-start, policies suffer from reward hacking despite application of repetition penalty and task-specific metrics such as CIDer etc. Further, we find that RL fine-tuned models obtain very high concept coverage which is also seen in Table B.4.5. Supervised models often tend to miss few concepts in its generation compared to RL methods.\\n\\n| Tasks_ Lexical and Semantic Metrics | Alg LM | Reward function | Rouge-2 | Rouge-L | Bleu (n=3) | Bleu (n=4) | Meteor | CIDEr | SPICE | Coverage |\\n|-----------------------------------|-------|----------------|--------|--------|------------|------------|--------|-------|-------|----------|\\n| CommonGen                         | Zero-Shot T5 | 0.016 | 0.264 | 0.029 | 0.006 | 0.203 | 6.200 | 0.115 | 91.070 |\\n|                                   | PPO | Rouge-1 | 0.085 | \u00b10.008 | 0.354 | \u00b10.004 | 0.161 | \u00b10.011 | 0.087 | \u00b10.009 | 0.235 | \u00b10.002 | 8.673 | \u00b10.234 | 0.157 | \u00b10.001 | 88.544 | \u00b12.369 |\\n|                                   | T5 Rouge-Avg | 0.093 | \u00b10.005 | 0.351 | \u00b10.001 | 0.169 | \u00b10.032 | 0.097 | \u00b10.017 | 0.224 | \u00b10.012 | 8.212 | \u00b11.329 | 0.159 | \u00b10.011 | 82.584 | \u00b12.569 |\\n|                                   | T5 Meteor | 0.091 | \u00b10.008 | 0.308 | \u00b10.007 | 0.166 | \u00b10.016 | 0.088 | \u00b10.013 | 0.220 | \u00b10.006 | 7.251 | \u00b10.453 | 0.161 | \u00b10.007 | 79.718 | \u00b12.267 |\\n|                                   | T5 SPider | 0.065 | \u00b10.003 | 0.302 | \u00b10.002 | 0.115 | \u00b10.063 | 0.067 | \u00b10.041 | 0.193 | \u00b10.014 | 6.571 | \u00b11.312 | 0.175 | \u00b10.011 | 69.340 | \u00b13.617 |\\n|                                   | T5 CiDer | 0.066 | \u00b10.003 | 0.304 | \u00b10.002 | 0.132 | \u00b10.057 | 0.074 | \u00b10.036 | 0.211 | \u00b10.009 | 6.877 | \u00b11.218 | 0.143 | \u00b10.017 | 80.114 | \u00b14.852 |\\n|                                   | T5 SPider | 0.117 | \u00b10.005 | 0.352 | \u00b10.007 | 0.224 | \u00b10.014 | 0.137 | \u00b10.011 | 0.226 | \u00b10.01 | 9.162 | \u00b10.539 | 0.186 | \u00b10.006 | 73.374 | \u00b16.073 |\\n|                                   | NLPO T5 Rouge-1 | 0.087 | \u00b10.002 | 0.339 | \u00b10.009 | 0.127 | \u00b10.048 | 0.069 | \u00b10.035 | 0.213 | \u00b10.002 | 6.962 | \u00b10.883 | 0.145 | \u00b10.022 | 80.89 | \u00b19.544 |\\n|                                   | T5 Rouge-Avg | 0.095 | \u00b10.001 | 0.338 | \u00b10.002 | 0.159 | \u00b10.02 | 0.093 | \u00b10.013 | 0.216 | \u00b10.009 | 7.55 | \u00b10.688 | 0.153 | \u00b10.008 | 77.944 | \u00b12.770 |\\n|                                   | T5 Meteor | 0.110 | \u00b10.005 | 0.332 | \u00b10.003 | 0.214 | \u00b10.007 | 0.124 | \u00b10.007 | 0.235 | \u00b10.004 | 8.669 | \u00b10.164 | 0.173 | \u00b10.002 | 82.007 | \u00b11.012 |\\n|                                   | T5 SPider | 0.014 | \u00b10.006 | 0.242 | \u00b10.001 | 0.037 | \u00b10.011 | 0.018 | \u00b10.007 | 0.156 | \u00b10.007 | 4.685 | \u00b10.283 | 0.168 | \u00b10.008 | 56.998 | \u00b13.548 |\\n|                                   | T5 CiDer | 0.046 | \u00b10.001 | 0.241 | \u00b10.003 | 0.078 | \u00b10.028 | 0.043 | \u00b10.016 | 0.143 | \u00b10.018 | 3.964 | \u00b10.792 | 0.103 | \u00b10.012 | 49.606 | \u00b17.971 |\\n|                                   | T5 SPider | 0.060 | \u00b10.006 | 0.258 | \u00b10.001 | 0.090 | \u00b10.008 | 0.056 | \u00b10.005 | 0.151 | \u00b10.022 | 4.411 | \u00b10.837 | 0.123 | \u00b10.022 | 49.230 | \u00b110.468 |\\n\\nSupervised T5 | Rouge-1 | 0.215 | \u00b10.001 | 0.438 | \u00b10.001 | 0.444 | \u00b10.001 | 0.329 | \u00b10.001 | 0.321 | \u00b10.001 | 16.385 | \u00b10.046 | 0.299 | \u00b10.001 | 94.476 | \u00b10.172 |\\n\\nSupervised + PPO T5 | Rouge-1 | 0.232 | \u00b10.002 | 0.453 | \u00b10.002 | 0.454 | \u00b10.006 | 0.338 | \u00b10.006 | 0.320 | \u00b10.002 | 16.233 | \u00b10.159 | 0.288 | \u00b10.004 | 96.412 | \u00b10.424 |\\n\\nSupervised | Rouge-1 | 0.229 | \u00b10.002 | 0.450 | \u00b10.001 | 0.454 | \u00b10.005 | 0.338 | \u00b10.005 | 0.320 | \u00b10.001 | 16.208 | \u00b10.175 | 0.289 | \u00b10.002 | 96.342 | \u00b10.572 |\\n\\nTable 12: CommonGen test evaluation\\nTable shows official scores obtained from CommonGen hold-out evaluation. The most important result is that RL fine-tuning on a supervised model yields better performance across most metrics especially Coverage which indicates the ratio of concepts covered in generated texts.\"}"}
{"id": "8aHzds2uUyB", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm | Reward Function | Rouge-1 | Rouge-2 | Rouge-L | Rouge-LSum | Meteor | BLEU | BertScore | Cider | Spice | MSTTR | Distinct |\\n|-----------|-----------------|---------|---------|---------|------------|--------|------|-----------|-------|-------|-------|----------|\\n| Zero-Shot T5 | | 0.415 | 0.016 | 0.270 | 0.270 | 0.179 | 0.0 | 0.854 | 0.640 | 0.231 | 0.430 | 0.090 |\\n| | | \u00b10.004 | \u00b10.093 | \u00b10.012 | \u00b10.380 | \u00b10.006 | \u00b10.006 | \u00b10.235 | \u00b10.005 | \u00b10.950 | \u00b10.318 | \u00b10.526 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.001 | \u00b10.006 | \u00b10.015 | \u00b10.015 | \u00b10.016 | \u00b10.132 | \u00b110.572 | \u00b10.234 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.016 | \u00b10.020 | \u00b10.006 | \u00b10.020 | \u00b10.016 | \u00b10.006 | \u00b11.006 | \u00b10.234 | \u00b10.105 |\\n| | | \u00b10.006 | \u00b10.006 | \u00b10.0"}
