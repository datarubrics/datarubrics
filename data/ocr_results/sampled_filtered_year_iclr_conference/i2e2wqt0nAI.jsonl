{"id": "i2e2wqt0nAI", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This paper revisits datasets and evaluation criteria for Symbolic Regression, a task of recovering mathematical expressions from given data, specifically focused on its potential for scientific discovery. Focused on a set of formulas used in the existing datasets based on Feynman Lectures on Physics, we recreate 120 datasets to discuss the performance of symbolic regression for scientific discovery (SRSD). For each of the 120 SRSD datasets, we carefully review the properties of the formula and its variables to design reasonably realistic sampling ranges of values so that our new SRSD datasets can be used for evaluating the potential of SRSD such as whether or not an SR method can (re)discover physical laws from such datasets. As an evaluation metric, we also propose to use normalized edit distances between a predicted equation and the ground-truth equation trees. While existing metrics are either binary or errors between the target values and an SR model's predicted values for a given input, normalized edit distances evaluate a sort of similarity between the ground-truth and predicted equation trees. We have conducted experiments on our new SRSD datasets using five state-of-the-art SR methods in SRBench and a simple baseline based on a recent Transformer architecture. The results show that we provide a more realistic performance evaluation and open up a new machine learning-based approach for scientific discovery. We provide our datasets and code as part of the supplementary material.\\n\\nINTRODUCTION\\nRecent advances in machine learning (ML), especially deep learning (DL), have led to the proposal of many methods that can reproduce the given data and make appropriate inferences on new inputs. Such methods are, however, often black-box, which makes it difficult for humans to understand how they made predictions for given inputs. This property will be more critical especially when non-ML experts apply ML to problems in their research domains such as physics and chemistry.\\n\\nSymbolic regression (SR) is the task of producing a mathematical expression (symbolic expression) that fits a given dataset. SR has been studied in the genetic programming (GP) community (Hoai et al., 2002; Keijzer, 2003; Koza & Poli, 2005; Johnson, 2009; Uy et al., 2011; Orzechowski et al., 2018), and DL-based SR has been attracting more attention from the ML/DL community (Petersen et al., 2020; Landajuela et al., 2021; Biggio et al., 2021; Valipour et al., 2021; La Cava et al., 2021; Kamienny et al., 2022). Because of its interpretability, various scientific communities apply SR to advance research in their scientific fields e.g., Physics (Wu & Tegmark, 2019; Udrescu & Tegmark, 2020; Udrescu et al., 2020; Kim et al., 2020; Cranmer et al., 2020; Liu & Tegmark, 2021; Liu et al., 2021b), Applied Mechanics (Huang et al., 2021), Climatology (Abdellaoui & Mehrkanoon, 2021), Materials (Sun et al., 2019; Wang et al., 2019; Weng et al., 2020; Loftis et al., 2020), and Chemistry (Batra et al., 2020).\\n\\nGiven that SR has been studied in various communities, La Cava et al. (2021) propose SRBench, a unified benchmark framework for symbolic regression methods. In the benchmark study, they combine the Feynman Symbolic Regression Database (FSRD) (Udrescu & Tegmark, 2020) and the ODE-Strogatz repository (Strogatz, 2018) to compare a number of SR methods, using a large-scale heterogeneous computing cluster. They used hosts with 24-28 core Intel(R) Xeon(R) CPU E5-2690 v4 2.60GHz processors and 250GB RAM.\"}"}
{"id": "i2e2wqt0nAI", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table S13: An example (I.12.4) that Symbolic Transformer performed better than other baselines.\\n\\nTarget Skeleton\\n\\\\[ c_1 \\\\cdot x_1 / x_2 \\\\]\\n\\nNED\\nAI Feynman\\n\\\\[ \\\\tan(x_2 / p) \\\\]\\n\\\\[ 1.00 \\\\]\\nDSR\\n\\\\[ x_1 \\\\cdot (x_1 + c_1 \\\\cdot \\\\exp((c_2 \\\\cdot \\\\cos(x_2 + c_3) + c_4) / x_2)) \\\\cdot \\\\exp(-x_2) \\\\]\\n\\\\[ 1.00 \\\\]\\nST\\n\\\\[ c_1 \\\\cdot x_2 \\\\]\\n\\\\[ 0.167 \\\\]\\n\\nTable S14: An example (I.14.3) that AFP, AFP-FE, and AI Feynman outperformed ST.\\n\\nTarget Skeleton\\n\\\\[ c_1 \\\\cdot x_1 \\\\cdot x_2 \\\\]\\n\\nNED\\nAI Feynman\\n\\\\[ * \\\\]\\n\\\\[ 0.00 \\\\]\\nDSR\\n\\\\[ x_1 \\\\cdot x_2 \\\\cdot (c_1 - (c_2 \\\\cdot x_2 + c_3 \\\\cdot \\\\log(\\\\cos(x_2)))) \\\\cdot (\\\\frac{-x_1 + x_2 + c_4}{x_2}) \\\\]\\n\\\\[ 1.00 \\\\]\\nST\\n\\\\[ x_1 \\\\cdot x_2 \\\\]\\n\\\\[ 0.250 \\\\]\\n\\n\u2020AFT and AFP-FE produced exactly the same skeleton with AI Feynman, which resulted in NED = 0.\\n\\nTable S15: Normalized edit distances of baselines for noise-injected SRSD (Easy) datasets with different noise levels.\\n\\n| Noise Level (\\\\(\\\\gamma\\\\)) | Method | gplearn | AFP | AFP-FE | AI Feynman | DSR | ST |\\n|---------------------------|--------|---------|-----|--------|-------------|-----|----|\\n|                           | 0.876  | 0.703   | 0.712 | 0.646  | 0.551       | 0.435 |    |\\n| \\\\(10^{-3}\\\\)               | 0.928  | 0.799   | 0.814 | 0.797  | 0.820       | 0.435 |    |\\n| \\\\(10^{-2}\\\\)               | 0.940  | 0.824   | 0.880 | 0.870  | 0.793       | 0.435 |    |\\n| \\\\(10^{-1}\\\\)               | 0.948  | 0.823   | 0.960 | 0.882  | 0.841       | 0.461 |    |\\n\\nTable S16: Solution rates of common baselines for FSRD and SRSD (Easy, Medium, Hard) datasets.\\n\\n| Dataset | Method | gplearn | AFP | AFP-FE | AI Feynman | DSR |\\n|---------|--------|---------|-----|--------|-------------|-----|\\n| FSRD    | 15.5%  | 20.48%  | 26.23% | 52.65% | 19.71%      |     |\\n| SRSD    | 1.67%  | 5.83%   | 5.83%  | 9.17%  | 15.0%       |     |\\n\\nTable S15 shows normalized edit distances of our baselines for noise-injected SRSD (Easy), reusing the set of noise levels in SRBench (La Cava et al., 2021) i.e. \\\\(\\\\gamma \\\\in \\\\{0, 10^{-3}, 10^{-2}, 10^{-1}\\\\}\\\\). Interestingly, our Symbolic Transformer (ST) baseline seems less sensitive to noise injected to target variables than other baseline methods. Overall, the more the injected noise is, the more difficult it would be for the baseline models to (re-)discover the physics law in the data.\\n\\nTable S16 compares the solution rates of the five common baselines for the FSRD and our SRSD datasets. We can confirm that the overall solution rates for our SRSD are significantly degraded compared to those for the FSRD reported in SRBench (La Cava et al., 2021). The results indicate that our SRSD datasets are more challenging than the FSRD in terms of solution rate.\\n\\nTo investigate how aligned with human judges the existing SR and new SRSD evaluation metrics are, we recruited 23 people from industry and academia who either have doctoral degrees (scientists, professors, engineers) or are doctoral students, and performed a user study with approval from an ethics review board. The recruited people are in diverse research fields such as computer science, mathematics, physics, chemistry, material science, aerospace engineering, engineering, medical nutrition, and computational biology. Given a pair of true and estimate equations for an SRSD problem, the subjects were asked to assess an estimated equation on a discretized 1-to-5 scale, where...\"}"}
{"id": "i2e2wqt0nAI", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table S17: Pearson correlation coefficients (PCCs) between the human judges and SR/SRSD metrics.\\n\\n| Metrics      | PCC    | P-value       |\\n|--------------|--------|---------------|\\n| R\u00b2 score     | 4.66\u00d710\u207b\u00b3 | 0.913         |\\n| NED          | -0.316 | 1.85\u00d710\u207b\u00b2\u2074    |\\n\\n1 and 5 indicate \\\"1: Completely different from the true equation\\\" and \\\"5: Equivalent to the true equation\\\" respectively. We chose SRSD problems among the 120 SRSD datasets such that we can obtain from the experimental results in Section 5 at least two different equations estimated by different methods that are best in terms of $R\u00b2$ score and normalized edit distance. There were 24 resulting SRSD problems for the user study.\\n\\nTable S17 shows Pearson correlation coefficients (PCCs) between the human judges and SR/SRSD evaluation metrics. For normalized edit distance (NED), the Pearson correlation coefficient and p-value were $-0.316$ and $1.85\u00d710^{-24}$ respectively, which show a much stronger and statistically more significant correlation between NED and human judges than one for $R\u00b2$ scores. In other words, results of the user study indicate that normalized edit distance is more aligned with human judges than $R\u00b2$ score, and thus can be a better estimate about how close to the true equations the estimated equations are, in a more human-understandable way.\"}"}
{"id": "i2e2wqt0nAI", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this study, we recruited 23 human subjects and performed a user study (see Section G) with approval from an ethics review board. We performed the user study to discuss how meaningful our new SRSD evaluation metric is, compared to existing SR metrics. Subjects were asked to assess in scale of 1-to-5 how close to the true equation the estimated equation is, and in the user study we used only the human-based evaluations against equations. For this reason, we found no ethical concerns in the user study.\\n\\nFor reproducibility, we provide the following items in this paper and/or the supplementary material:\\n\\n1. our datasets and code repository with MIT License and instructions to run our scripts, including all the five existing and one new baselines (See the supplementary material),\\n2. annotation policy and details of the 120 proposed datasets (See Section 3.2.1 and Tables 1 - S11),\\n3. architecture design and hyperparameters of Symbolic Transformer, a new SRSD baseline we introduced (See Section C and the supplementary material),\\n4. computing resources and runtime constraints in this study (See Section 5.2 and Footnote 5), and\\n5. hyperparameters used for the six baseline methods (See Sections B and C.2).\\n\\nREFERENCES\\n\\nIsmail Alaoui Abdellaoui and Siamak Mehrkanoon. Symbolic regression for scientific discovery: an application to wind speed forecasting. In 2021 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 01\u201308. IEEE, 2021.\\n\\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A Next-generation Hyperparameter Optimization Framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 2623\u20132631, 2019.\\n\\nRohit Batra, Le Song, and Rampi Ramprasad. Emerging materials intelligence ecosystems propelled by machine learning. Nature Reviews Materials, pp. 1\u201324, 2020.\\n\\nLuca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Giambattista Parascandolo. Neural Symbolic Regression that Scales. In International Conference on Machine Learning, pp. 936\u2013945. PMLR, 2021.\\n\\nEdmund Burke, Steven Gustafson, Graham Kendall, and Natalio Krasnogor. Advanced Population Diversity Measures in Genetic Programming. In International Conference on Parallel Problem Solving from Nature, pp. 341\u2013350. Springer, 2002.\\n\\nR. Qi Charles, Hao Su, Mo Kaichun, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\nMiles Cranmer, Alvaro Sanchez Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, and Shirley Ho. Discovering Symbolic Models from Deep Learning with Inductive Biases. Advances in Neural Information Processing Systems, 33:17429\u201317442, 2020.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, 2019.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations, 2020.\"}"}
{"id": "i2e2wqt0nAI", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Richard P Feynman, Robert B Leighton, and Matthew Sands. The Feynman Lectures on Physics, Vol. I: The New Millennium Edition: Mainly Mechanics, Radiation, and Heat, volume 1. Basic books, 1963a.\\n\\nRichard P Feynman, Robert B Leighton, and Matthew Sands. The Feynman Lectures on Physics, Vol. II: The New Millennium Edition: Mainly Electromagnetism and Matter, volume 2. Basic books, 1963b.\\n\\nRichard P Feynman, Robert B Leighton, and Matthew Sands. The Feynman Lectures on Physics, Vol. III: The New Millennium Edition: Quantum Mechanics, volume 3. Basic books, 1963c.\\n\\nHerbert Goldstein, Charles Poole, and John Safko. Classical Mechanics. Addison Wesley, 2002.\\n\\nNguyen Xuan Hoai, Robert Ian McKay, D Essam, and R Chau. Solving the Symbolic Regression Problem with Tree Adjunct Grammar Guided Genetic Programming: The Comparative Results. In Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No. 02TH8600), volume 2, pp. 1326\u20131331. IEEE, 2002.\\n\\nZhanchao Huang, Chunjiang Li, Zhilong Huang, Yong Wang, and Hanqing Jiang. AI-Timoshenko: Automatedly Discovering Simplified Governing Equations for Applied Mechanics Problems From Simulated Data. Journal of Applied Mechanics, 88(10):101006, 2021.\\n\\nJohn David Jackson. Classical Electrodynamics. Wiley, 1999.\\n\\nColin G Johnson. Genetic Programming Crossover: Does it Cross Over? In European Conference on Genetic Programming, pp. 97\u2013108. Springer, 2009.\\n\\nPierre-Alexandre Kamienny, St\u00e9phane d\u2019Ascoli, Guillaume Lample, and Fran\u00e7ois Charton. End-to-end symbolic regression with transformers. arXiv preprint arXiv:2204.10532, 2022.\\n\\nMaarten Keijzer. Improving Symbolic Regression with Interval Arithmetic and Linear Scaling. In European Conference on Genetic Programming, pp. 70\u201382. Springer, 2003.\\n\\nSamuel Kim, Peter Y Lu, Srijon Mukherjee, Michael Gilbert, Li Jing, Vladimir \u010ceperi\u0107, and Marin Solja\u010di\u0107. Integration of Neural Network-Based Symbolic Regression in Deep Learning for Scientific Discovery. IEEE Transactions on Neural Networks and Learning Systems, 2020.\\n\\nMichael Kommenda, Bogdan Burlacu, Gabriel Kronberger, and Michael Affenzeller. Parameter identification for symbolic regression using nonlinear least squares. Genetic Programming and Evolvable Machines, 21(3):471\u2013501, 2020.\\n\\nJohn R Koza and Riccardo Poli. Genetic Programming. In Search methodologies, pp. 127\u2013164. Springer, 2005.\\n\\nWilliam La Cava, Kourosh Danai, and Lee Spector. Inference of compact nonlinear dynamic models by epigenetic local search. Engineering Applications of Artificial Intelligence, 55:292\u2013306, 2016.\\n\\nWilliam La Cava, Tilak Raj Singh, James Taggart, Srinivas Suri, and Jason H Moore. Learning concise representations for regression by evolving networks of trees. In International Conference on Learning Representations, 2018.\\n\\nWilliam La Cava, Thomas Helmuth, Lee Spector, and Jason H Moore. A Probabilistic and Multi-Objective Analysis of Lexicase Selection and \u03b5-Lexicase Selection. Evolutionary Computation, 27(3):377\u2013402, 2019.\\n\\nWilliam La Cava, Patryk Orzechowski, Bogdan Burlacu, Fabricio Olivetti de Franca, Marco Virgolin, Ying Jin, Michael Kommenda, and Jason H Moore. Contemporary Symbolic Regression Methods and their Relative Performance. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.\\n\\nMikel Landajuela, Brenden K Petersen, Sookyung Kim, Claudio P Santiago, Ruben Glatt, Nathan Mundhenk, Jacob F Pettit, and Daniel Faissol. Discovering symbolic policies with deep reinforcement learning. In International Conference on Machine Learning, pp. 5979\u20135989. PMLR, 2021.\"}"}
{"id": "i2e2wqt0nAI", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "i2e2wqt0nAI", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "i2e2wqt0nAI", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Distribution map of our proposed datasets based on three different subsets with respect to our complexity metrics. Data points at top right/bottom left indicate more/less complex problems.\\n\\nFigure 2: Example of preprocessing a true equation (III.7.38 in Table S1) in evaluation session. When converting to an equation tree, we replace constant values and variables with specific symbols e.g.\\n\\n\\\\[ 32647716907439 \\\\times 10^{-33} \\\\rightarrow C_1, \\\\mu \\\\rightarrow X_1, B \\\\rightarrow X_2. \\\\]\\n\\n4.1 METRICS\\n\\nIn general, it would be difficult to define \\\"accuracy\\\" of symbolic regression models since we will compare its estimated equation to the ground truth equation and need criteria to determine whether or not it is \\\"correct\\\". La Cava et al. (2021) suggested a reasonable definition of symbolic solution, which is designed to capture symbolic regression models that differ from the true model by a constant or scalar. They also used \\\\( R^2 \\\\) score (Eq. 2) and define as accuracy the percentage of symbolic regression problems that a model meets \\\\( R^2 > \\\\tau \\\\), where \\\\( \\\\tau \\\\) is a threshold e.g., \\\\( \\\\tau = 0.999 \\\\) in (La Cava et al., 2021).\\n\\n\\\\[\\nR^2 = \\\\frac{\\\\sum_{j=1}^{N} (f_{\\\\text{pred}}(X_j) - f_{\\\\text{true}}(X_j))^2}{\\\\sum_{k=1}^{N} (f_{\\\\text{true}}(X_k) - \\\\bar{y})^2},\\n\\\\]\\n\\nwhere \\\\( N \\\\) indicates the number of test samples (i.e., the number of rows in the test dataset), and \\\\( \\\\bar{y} \\\\) is a mean of target outputs produced by \\\\( f_{\\\\text{true}} \\\\). \\\\( f_{\\\\text{pred}} \\\\) and \\\\( f_{\\\\text{true}} \\\\) are a trained SR model and a true model, respectively. However, these two metrics are still binary (correct or not) or require a threshold and does not explain how structurally close to the true equation the estimated one is. While a key feature of symbolic regression is its interpretability, there are no single evaluation metrics to take into account both the interpretability and how close to the true expression the estimated expression is.\\n\\nTo offer more flexibility and assess estimated equations in such a way, we propose the use of edit distance between estimated and ground truth equations, processing equations as trees. Although edit distance has been employed in different domains such as machine translation (Przybocki et al., 2006) (text-based edit distance), its primary use has been to study the search process for genetic programming approaches (O'Reilly, 1997; Burke et al., 2002; Nakai et al., 2013). Different from prior work, we propose a use of tree-based edit distance as a new metric of solution quality for SRSD.\"}"}
{"id": "i2e2wqt0nAI", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For a pair of two trees, edit distance computes the minimum cost to transform one to another with a sequence of operations, each of which either 1) inserts, 2) deletes, or 3) renames a node. In this study, a node can be either a mathematical operation (e.g., add, exp as symbols), a variable symbol, or a constant symbol. For the detail of the algorithm, we refer readers to (Zhang & Shasha, 1989).\\n\\nAs illustrated in Fig. 2, we preprocess equations by 1) substituting constant values e.g., \\\\(\\\\pi\\\\) and Planck constant to the expression, and 2) converting the resulting expression to an equation tree that represents the preorder traversal of the equation with simplified symbols. It should be worth noting that before generating the equation tree, we simplify and convert equations to floating-point approximations by sympy (Meurer et al., 2017), a Python library for symbolic mathematics. It helps us consistently map a given equation to the unique equation tree and compute edit distance between the true and estimated equation trees since our evaluation interest is in simplified expressions of the estimated equations rather than how SR models produced the equations. For instance, \\\"\\\\(x + x + x\\\\)\\\", \\\"\\\\(4* x - x\\\\)\\\", and \\\"\\\\(x + 2 * x\\\\)\\\" will be simplified by sympy to \\\"\\\\(3 * x\\\\)\\\" and considered identical.\\n\\nFor edit distance, we use a method proposed by Zhang & Shasha (1989). Given that the range of edit distance values depends on complexity of equations, we normalize the distance in range of 0 to 1 as\\n\\n\\\\[\\nd(f_{\\\\text{pred}}, f_{\\\\text{true}}) = \\\\min \\\\left(1, \\\\frac{d(f_{\\\\text{pred}}, f_{\\\\text{true}})}{|f_{\\\\text{true}}|}\\\\right),\\n\\\\]\\n\\nwhere \\\\(f_{\\\\text{pred}}\\\\) and \\\\(f_{\\\\text{true}}\\\\) are estimated and true equation trees, respectively. \\\\(d(f_{\\\\text{pred}}, f_{\\\\text{true}})\\\\) is an edit distance between \\\\(f_{\\\\text{pred}}\\\\) and \\\\(f_{\\\\text{true}}\\\\). \\\\(|f_{\\\\text{true}}|\\\\) indicates the number of the tree nodes that compose an equation \\\\(f_{\\\\text{true}}\\\\). We note that this metric is designed to capture similarity between estimated and true equations, thus coefficient values themselves (e.g., value of \\\\(C_1\\\\) in Fig. 2) should not be important.\\n\\n### 4.2 Evaluation Framework\\n\\nFor real datasets (assuming observed datasets), only tabular data are available for training and validation. (In practice, a test dataset does not include the true equation.) For benchmark purposes, true equations are provided as test data besides test tabular data.\\n\\nFor each problem, we use the validation tabular dataset and choose the best trained SR model \\\\(f^*_{\\\\text{pred}}\\\\) from \\\\(F\\\\), a set of the trained models by a given method respect to Eq. (4)\\n\\n\\\\[\\nf^*_{\\\\text{pred}} = \\\\arg \\\\min_{f_{\\\\text{pred}} \\\\in F} \\\\frac{1}{n} \\\\sum_{i=1}^{n} (f_{\\\\text{pred}}(X_i) - f_{\\\\text{true}}(X_i))^2,\\n\\\\]\\n\\nwhere \\\\(X_i\\\\) indicates the \\\\(i\\\\)-th row of the validation tabular dataset \\\\(X\\\\).\\n\\nNotice that while we proposed in Section 4.1 a normalized edit distance between estimated and true equation trees, such true equations will not be available in practice, especially when using symbolic regression methods for scientific discovery. For this reason, we use the geometrical distance between predicted values against a validation tabular dataset to choose the best model obtained through hyperparameter tuning. Using the best model per method, we compute the normalized edit distance to assess the method.\\n\\n### 5 Experiments\\n\\n#### 5.1 Baseline Methods\\n\\nFor baselines, we use the five best symbolic regression methods in SRBench (La Cava et al., 2021). Specifically, we choose gplearn (Koza & Poli, 2005), AFP (Schmidt & Lipson, 2011), AFP-FE (Schmidt & Lipson, 2009), AI Feynman (Udrescu et al., 2020), and DSR (Petersen et al., 2020), referring to the rankings of solution rate for the FSRD datasets in their study. We note that La Cava et al. (2021) also benchmark symbolic regression methods for black-box problems, whose true symbolic expressions are unknown, and other symbolic regression methods e.g., Operon (Kommenda et al., 2020), SBP-GP (Virgolin et al., 2019), FEAT (La Cava et al., 2018), EPLEX (La Cava et al., 2019), and GP-GOMEA (Virgolin et al., 2021) outperform the five baseline methods we choose from their study, in terms of \\\\(R^2\\\\)-driven accuracy. However, we find solution rate more aligned with edit distance, thus we choose the five best symbolic regression methods in terms of solution rate.\"}"}
{"id": "i2e2wqt0nAI", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"empirically shown for the FSRD datasets in SRBench (La Cava et al., 2021). In addition to the five existing symbolic regression baselines, we introduce Symbolic Transformer, a new baseline model:\\n\\n1. **gplearn** (Koza & Poli, 2005): a genetic programming based symbolic regression method published as a Python package.\\n2. **AFP** (Schmidt & Lipson, 2011): Age-fitness pareto optimization.\\n3. **AFP-FE** (Schmidt & Lipson, 2009): AFP optimization with fitness estimates.\\n4. **AI Feynman** (Udrescu et al., 2020): an iterative approach to generate symbolic regression to seek to fit data to formulas that are Pareto-optimal.\\n5. **DSR** (Petersen et al., 2020): reinforcement learning based deep symbolic regression.\\n6. **Symbolic Transformer (ST)**: our Transformer-based symbolic regression baseline.\\n\\nFor the details of the baseline models, we refer readers to the corresponding papers (Koza & Poli, 2005; Schmidt & Lipson, 2011; 2009; Udrescu et al., 2020; Petersen et al., 2020). We provide the details of Symbolic Transformer in Section C, including our pretraining strategy. While Symbolic Transformer itself is a new model, we note that the main contribution of this work lies in the datasets and benchmark of symbolic regression for scientific discovery. Our Transformer-based baseline method is simply inspired by recent advances in deep learning, specifically transformer-based high-performance, modern, and flexible models such as (Vaswani et al., 2017; Devlin et al., 2019; Dosovitskiy et al., 2020). Thus, the new model is not necessarily designed to show improvements over the existing Transformer-based symbolic regression models (Valipour et al., 2021; Biggio et al., 2021) including contemporary work such as (Kamienny et al., 2022).\\n\\n5.2 **Constraints**\\n\\nThe implementations of the baseline methods in Section 5.1 except Symbolic Transformer do not use any GPUs. We run 600 high performance computing (HPC) jobs in total, using computing nodes in an HPC infrastructure, which have 5 - 20 assigned physical CPU cores, 30 - 120 GB RAM, and 720 GB local storage. Due to the properties of our HPC resource, we have some runtime constraints:\\n\\n1. Since each HPC job is designed to run for up to 24 hours due to the limited resource, we run a job with a pair of a target tabular dataset and a symbolic regression method.\\n2. Given a pair of a dataset and a method, each of our HPC jobs runs up to 100 separate training sessions with different hyperparameter values.\\n\\n5.3 **Results**\\n\\nIn this section, we discuss the experimental results of our baseline methods, using the proposed SRSD datasets. Tables 2 and 3 show the performance of the symbolic regression baseline methods in terms of $R^2$-driven accuracy ($R^2 > 0.999$) and solution rate respectively, and both the metrics are used in SRBench (La Cava et al., 2021). According to the metrics, DSR significantly outperforms all the other baselines we considered. The DSR results also indicate difficulty levels of the three categories of our SRSD datasets, which looks aligned with our complexity-aware dataset categorization (Section 3.2.2).\\n\\nNow we discuss the results using the normalized edit distance. Table 4 shows the results of the baseline methods in terms of the normalized edit distance. Interestingly, while the Symbolic Transformer performed the worst in Table 2 in terms of $R^2$-based accuracy, it achieved the best normalized edit distance for all the SRSD Easy, Medium, and Hard sets and significantly improved DSR. This trend also implies that the $R^2$-based accuracy does not always indicate how well the SR model can produce an equation that is structurally close to the true equation. We also confirmed that the difficulty level of each set of SRSD datasets is reflected to the overall trend in Tables 2 - 4.\\n\\nWe also performed a user study and how aligned with human judges the existing SR and new SRSD evaluation metrics are. The results show that the SRSD evaluation metric (NED) is more aligned with human judges than $R^2$ score. We refer readers to Section G for more details.\\n\\nWe used an NVIDIA GeForce RTX 3070 Ti for pretraining Symbolic Transformer.\"}"}
{"id": "i2e2wqt0nAI", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Baseline results: accuracy defined by La Cava et al. (2021).\\n\\n|        | SRSD | Datasets | Method | gplearn | AFP | AFP-FE | AI | Feynman | DSR | ST |\\n|--------|------|----------|--------|---------|-----|--------|----|---------|-----|----|\\n| Easy set (30 problems) |  | | 6.67% | 20.0% | 23.3% | 33.3% | 60.0% | 0.00% |\\n| Medium set (40 problems) |  | | 7.50% | 2.50% | 2.50% | 5.00% | 42.5% | 0.00% |\\n| Hard set (50 problems) |  | | 2.00% | 4.00% | 4.00% | 8.00% | 30.0% | 0.00% |\\n\\nTable 3: Baseline results: solution rate defined by La Cava et al. (2021).\\n\\n|        | SRSD | Datasets | Method | gplearn | AFP | AFP-FE | AI | Feynman | DSR | ST |\\n|--------|------|----------|--------|---------|-----|--------|----|---------|-----|----|\\n| Easy set (30 problems) |  | | 6.67% | 20.0% | 20.0% | 30.0% | 43.3% | 16.7% |\\n| Medium set (40 problems) |  | | 2.50% | 2.50% | 2.50% | 2.50% | 10.0% | 5.00% |\\n| Hard set (50 problems) |  | | 0.00% | 0.00% | 0.00% | 2.00% | 2.00% | 0.00% |\\n\\nTable 4: Baseline results: our proposed normalized edit distances (the smaller the better).\\n\\n|        | SRSD | Datasets | Method | gplearn | AFP | AFP-FE | AI | Feynman | DSR | ST |\\n|--------|------|----------|--------|---------|-----|--------|----|---------|-----|----|\\n| Easy set (30 problems) |  | | 0.876 | 0.703 | 0.712 | 0.646 | 0.551 | 0.435 |\\n| Medium set (40 problems) |  | | 0.939 | 0.873 | 0.897 | 0.936 | 0.789 | 0.556 |\\n| Hard set (50 problems) |  | | 0.978 | 0.960 | 0.956 | 0.930 | 0.833 | 0.704 |\\n\\n6 LIMITATIONS\\n\\n6.1 IMPLICIT FUNCTIONS\\nSymbolic regression generally has a limitation in inferring implicit functions, as the model infers a trivial constant function if there are no restrictions on variables. For example, \\\\( f(x, y) = 0 \\\\) is inferred as \\\\( 0 = 0 \\\\) for all \\\\( x, y \\\\). This problem can be solved by applying the constraint that an inferred function should depend on at least two variables e.g., inferring \\\\( f(x, y) = 0 \\\\) with \\\\( \\\\frac{\\\\partial f}{\\\\partial x} \\\\neq 0 \\\\) and \\\\( \\\\frac{\\\\partial f}{\\\\partial y} \\\\neq 0 \\\\), or by converting the function to an explicit form e.g., \\\\( y = g(x) \\\\). We converted some functions in the datasets into explicit forms and avoided the inverse trigonometric functions as described in Section 3.1.\\n\\n6.2 DUMMY VARIABLES AND NOISE INJECTION\\nWhen applying machine learning to real-world problems, it is often true that 1) not all the observed features (variables in symbolic regression) are necessary to solve the problems, and 2) the observed values contain some noise. While we follow La Cava et al. (2021) and show experimental results for our SRSD datasets with noise-injected target variables in Section E, these aspects are not thoroughly discussed in this study, such discussions can be a separate paper built on this work and further engage studies of symbolic regression for scientific discovery.\\n\\n7 CONCLUSION\\nIn this work, we pointed out issues of existing datasets and benchmarks of symbolic regression for scientific discovery (SRSD). To address the issues, we proposed 1) 120 new SRSD datasets based on a set of physics formulas in FSRD (Udrescu & Tegmark, 2020) and 2) a new evaluation metric for SRSD to discuss the structural similarity between the true and estimated symbolic expressions (equations). We note that this study argues that the normalized edit distance is a metric not to take the place of existing SR metrics but to incorporate such metrics (e.g., Tables 2 and 4). Besides the main contribution above, we proposed a Transformer-based symbolic regression baseline, named Symbolic Transformer that achieved the best normalized edit distance for the proposed SRSD datasets. To encourage the studies of SRSD, we provide our datasets and code with MIT License.\"}"}
{"id": "i2e2wqt0nAI", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we describe our Symbolic Transformer baseline and pretraining strategy, and discuss the degree of the test data leakage during the pretraining. We repeat that while the Symbolic Transformer itself is a new model, the main contribution of this work lies in the datasets and benchmark of symbolic regression for scientific discovery (SRSD). We also note that our Symbolic Transformer is not necessarily designed to show improvements over the existing Transformer-based symbolic regression models (Valipour et al., 2021; Biggio et al., 2021) including contemporary work such as Kamienny et al. (2022). Our Transformer-based baseline is simply inspired by recent advances in deep learning, specifically Transformer-based high-performance, modern, and flexible models such as Vaswani et al. (2017); Devlin et al. (2019); Dosovitskiy et al. (2020).\\n\\n### C.1 ARCHITECTURE DESIGN\\n\\nA Symbolic Transformer is an encoder-decoder network to predict a binary equation tree corresponding to input tabular data points. The encoder inputs sampled tabular data from an equation and outputs variable-wise features. The decoder inputs them and outputs a preorder sequence of tokens in an autoregressive manner to build a binary equation tree.\\n\\nFirst, we describe an encoder that inputs tabular data and outputs variable-wise features. The tabular data here have the following properties:\\n\\n- Each row is a data point sampled from the expected equation\\n- Each column corresponds to the input variable of the equation ($x_1, x_2, \\\\ldots$) or the output of the equation ($y$).\\n\\nFor such data, the relation between the input tabular data and the output equation is desired to have the following properties:\\n\\n- Permutation-invariant in each row; since there is no meaningful order between each sampled point, the output equation is expected not to change no matter how the rows are rearranged.\\n- Permutation-equivariant for each column and variable; when the columns of the input tabular data are rearranged, the order of variables in the output equation is expected to change corresponding to the input column. (e.g., assume the formula $y = x_1 - x_2$, if the $x_1$ column and the $x_2$ column of the input tabular data are swapped, the expected output is $y = x_2 - x_1$.)\\n\\nTo handle such permutation-invariant and permutation-equivariant data structures, the ideas of PointNet (Charles et al., 2017), which handles 3D point clouds, and Deep Sets (Zaheer et al., 2017), which handles set data, can be referred to. The approach proposed in these papers is to utilize point-wise transformation using Shared Multi Layer Perceptron (MLP) (permutation-equivariant operation) and aggregation using Pooling (permutation-invariant operation).\\n\\nWhen applied to tabular data, a sample point-wise feature can be obtained by combining Shared MLP and column-wise (variable-wise) pooling. Similarly, a variable-wise feature can be obtained by combining Shared MLP and row-wise (sample point-wise) Pooling. In both operations, by aggregating once and then combining the features with the features of each cell (this idea is used in semantic segmentation network in PointNet (Charles et al., 2017)), it can be expected that the whole information of aggregated direction will propagate to each cell. We call this operation feature splatting. Note that it is a permutation-equivariant operation.\\n\\nThe proposed network architecture is shown in Fig. S1. The proposed encoder consists of two encoding blocks, each block consisting of 1.) Shared MLP, 2.) row-wise (sample point-wise) pooling, 3.) row-wise feature splatting, 4.) Shared MLP, 5.) column-wise (variable-wise) pooling, and 6.) column-wise feature splatting. The encoder is permutation-equivariant, and information of each cell propagates to both the row-axis and column-axis. Finally, row-wise (sample point-wise) pooling can obtain the variable-wise feature.\\n\\nThe decoder of our Symbolic Transformer is a Transformer decoder (Vaswani et al., 2017) that takes a variable-wise feature and a preorder sequence of tokens (to express the equation as a binary equation tree) as input and outputs the next token. The next token is estimated with Multi-head Attention.\"}"}
{"id": "i2e2wqt0nAI", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure S1: Architecture of Symbolic Transformer.\\n\\nFigure S2: Illustration of Symbolic Transformer's inference process for given tabular data which consists of 1,000 samples. Each sample is a 3-dimensional vector whose first two and last values correspond to two variables and one target value, respectively. Yellow nodes indicate next tokens (nodes) to be predicted.\\n\\nOver the variable-wise feature from the input tokens. During training, we use the teacher forcing method (Williams & Zipser, 1989) and feed the first ($j - 1$) tokens in the ground-truth preorder sequence to the decoder for predicting the next ($j$-th) token and computing cross-entropy loss with a scheduled sampling strategy (Liu et al., 2021a). During inference, the decoder predicts the next token autoregressively, using a sequence of previously-predicted tokens as input tokens as illustrated in Fig. S2. From the resulting preorder sequence of tokens, the binary equation tree is generated as the output of the Symbolic Transformer method.\"}"}
{"id": "i2e2wqt0nAI", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For each training batch, we also mask next token candidates as the Symbolic Transformer model will know the potential maximum number of variables in the target equation from the shape of the training batch. Suppose that a training batch consists of tabular data whose shape is (1,000, 3), the decoder should not predict three or more unique variable tokens as part of the preorder sequence. It should be worth noting that when predicting the next token of a preorder sequence, we take advantage of properties of binary equation tree for efficient inference. Since we predict a preorder sequence to build a binary equation tree, we can terminate the inference of the next token when all the child nodes in the binary equation tree built from the resulting sequence expresses are tokens for either constant or variable symbols, i.e., the binary equation tree is complete and cannot be extended anymore, thus we can terminate the inference without predicting \\\"<EOS>\\\" , the end of sequence token used in standard seq2seq tasks (Sutskever et al., 2014).\\n\\nC.2 RETRAINING AND HYPERPARAMETERS\\n\\nHere, we describe our pretraining strategy for the Symbolic Transformer baseline. As described in Section C.1, the decoder of the Symbolic Transformer is trained to predict a next token of a preorder sequence to express the true binary equation tree, and thus requires both 1) a true sequence of the tokens and 2) tabular data (samples).\\n\\nThe 120 SRSD datasets proposed in this study, however, should not be directly used for pretraining the Symbolic Transformer baseline. Since the datasets are designed to assess the potential of symbolic regression methods towards scientific discovery, we cannot use the SRSD datasets for both pretraining and testing the Symbolic Transformer baseline. Otherwise, it will result in having the baseline method learn from the pairs of tabular data and true sequences during training session and test the method using tabular data from a similar distribution, which may be considered as test data leakage.\\n\\nMoreover, pretraining datasets are usually significantly larger than the target datasets (which are the SRSD datasets in this study) (Devlin et al., 2019; Liu et al., 2019; Dosovitskiy et al., 2020). To address the issues, we generated a large-scale set of synthesized datasets for pretraining the Symbolic Transformer baseline. Specifically, we built a bi-gram Na\u00efve Bayes language model to generate preorder sequences of tokens that express binary equation trees like those in the SRSD datasets. Having randomly generated a target equation for pretraining, we randomly chose \\\\(k\\\\) for each of the variables in the equation independently to define its sampling range \\\\(U_{\\\\log(10^k - 1, 10^k + 1)}\\\\). Given a generated equation, we created up to 10 sets of tabular datasets with different sampling ranges we randomly chose as described above.\\n\\nUsing the approach, we generated 24,232 synthesized datasets. In the datasets, there are 4,501 generated equations, and 1,841 of them are unique sequences. We pretrained the Symbolic Transformer baseline on the synthesized datasets for three epochs, using the SGD optimizer and a linear learning rate scheduler with warmup (1,000 steps) where the initial learning rate and weight decay are 0.1 and \\\\(10^{-4}\\\\), respectively. A training batch at each step consists of 1,000 random samples from one of the synthesized datasets, and one epoch in this pretraining consists of 24,232 training steps. The Symbolic Transformer is implemented with PyTorch (Paszke et al., 2019), and we implemented the pretraining and evaluation sessions, using torchdistill (Matsubara, 2021) for securing reproducibility of the experiments.\\n\\nC.3 ARE THE TEST DATASETS LEAKED WHILE PRETRAINING SYMBOLIC TRANSFORMER?\\n\\nIn terms of normalized edit distance we proposed, the Symbolic Transformer baseline performed the best among our baseline methods for the SRSD datasets as shown in Section 5.3. To achieve the results, the Symbolic Transformer baseline model took advantage of being pretrained on a large-scale set of synthesized datasets we generated (See Section C.2). From the performance gap between the Symbolic Transformer and the other baselines, it is reasonable to suspect that the test set of our SRSD datasets may be leaked during the pretraining step. Given a pair of a synthesized training dataset \\\\(X_R\\\\) and an SRSD test dataset \\\\(X_S\\\\), we assess the degree of the leakage, following the two steps:\\n\\n8 The last (rightmost) column in input tabular data indicates the target values, thus should not be a input variable for the equation to be predicted.\\n\\n9 https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup\"}"}
{"id": "i2e2wqt0nAI", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Step 1 Compute normalized edit distance (Eq. (3)) between the true equations in the two datasets.\\n\\nStep 2 If the resulting normalized edit distance (Step 1) is zero, then compute intersection of union (IoU) between sampling domains of the datasets by Eq. (S1) for each input variable:\\n\\n\\\\[\\n\\\\text{IoU}(X_i^{\\\\text{R}}, X_i^{\\\\text{S}}) = \\\\max_{\\\\min X_i^{\\\\text{R}}, \\\\min X_i^{\\\\text{S}}} - \\\\max_{\\\\max X_i^{\\\\text{R}}, \\\\max X_i^{\\\\text{S}}}, \\\\quad (S1)\\n\\\\]\\n\\nwhere \\\\(X_i^{\\\\text{R}}\\\\) and \\\\(X_i^{\\\\text{S}}\\\\) are sets of sampled values in the \\\\(i\\\\)-th variable for the given synthesized training dataset and the SRSD test dataset, respectively.\\n\\nFollowing the procedure above, we assess the degree of the leakage for all the combinations of 120 SRSD datasets versus 24,232 synthesized datasets for pretraining. We take equation-wise average of the IoU values and then compute the average IoU over the 120 equation-wise average IoU values.\\n\\nThe resulting mean IoU is 0.00215, and the IoU defined in Eq. (S1) is designed to be normalized in range of 0 to 1. From the result, we conclude that there is no significant leakage from our SRSD datasets while pretraining the Symbolic Transformer.\\n\\n**D QUANTITATIVE ANALYSIS**\\n\\nThis section discusses qualitative analysis for the experimental results above.\\n\\nHere we highlight some examples that the Symbolic Transformer baseline method perform better with respect to the normalized edit distance than other baselines and vice versa. Taking I.12.4 in Table 1 as an example, \\\\(E = q_1 \\\\frac{4}{\\\\pi \\\\epsilon r^2}\\\\) is simplified by sympy and converted to a skeleton equation as\\n\\n\\\\[f(x) = c_1 \\\\cdot x_1 \\\\cdot x_2^2,\\\\]\\n\\nwhere \\\\(c\\\\)'s are constant tokens and \\\\(x_1\\\\) and \\\\(x_2\\\\) are the first and second variables, respectively.\\n\\nTable S13 shows the predicted skeleton equations and normalized edit distance from the target skeleton. For the specific true symbolic expression, gplean, AFP, and AFP-FE could not finish the training to produce symbolic expressions for the corresponding tabular data within the session timeout, which is part of our runtime constraints. While the skeleton equation produced by the Symbolic Transformer (ST) is not a perfect solution, it is structurally closer to the target skeleton than those produced by AI Feynman and DSR. It seems that both AI Feynman and DSR attempted to fit the training data and minimize their regression errors, but they resulted in overcomplex symbolic expressions.\\n\\nTable S14 shows a different example (I.14.3 from Table 1) where some of the other baseline methods performed better than the Symbolic Transformer. While DSR and Symbolic Transformer produced overcomplex or simpler solutions, AFP, AFP-FE, and AI Feynman produced a perfect skeleton with respect to normalized edit distance from the true equation. Overall, our Symbolic Transformer baseline seems to prefer simpler symbolic expressions, that may avoid overcomplex symbolic expressions as a result.\\n\\n**E NJECTING NOISE TO TARGET VARIABLES**\\n\\nFollowing SRBench (La Cava et al., 2021), we introduce Gaussian noise with a parameter of noise level \\\\(\\\\gamma\\\\) to the target variables in our SRSD datasets. We inject Gaussian noise to each of the datasets separately, following Eq. (S2):\\n\\n\\\\[y_{\\\\text{noise}}^j = f^\\\\text{true}(X_j) + \\\\epsilon, \\\\quad \\\\epsilon \\\\sim N\\\\left(0, \\\\gamma^2 \\\\right)\\\\]\\n\\n\\\\[\\\\sum_{k=1}^N f^\\\\text{true}(X_k), \\\\quad (S2)\\\\]\\n\\nwhere \\\\(1 \\\\leq j \\\\leq N\\\\) and \\\\(N\\\\) indicates the number of samples in the dataset.\\n\\nWhen computing normalized edit distance, we treat all the constants (\\\\(c\\\\)'s) in a skeleton equation as duplicate nodes. i.e., As tree nodes, constant indices are not important since those will be estimated separately at the end, and in general the estimated constant values should not affect the edit distance.\"}"}
{"id": "i2e2wqt0nAI", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table S9: Properties of the proposed datasets\\n\\n| Property | Original | Ours | Original | Ours |\\n|----------|----------|------|----------|------|\\n| Size     | N/A      | N/A  | N/A      | N/A  |\\n| Volume   | N/A      | N/A  | N/A      | N/A  |\\n| Density  | N/A      | N/A  | N/A      | N/A  |\\n| Shape    | N/A      | N/A  | N/A      | N/A  |\\n| Material | N/A      | N/A  | N/A      | N/A  |\\n| Temperature | N/A | N/A  | N/A      | N/A  |\\n| Pressure | N/A      | N/A  | N/A      | N/A  |\\n| Viscosity| N/A      | N/A  | N/A      | N/A  |\\n| Electrical properties | N/A | N/A  | N/A      | N/A  |\\n| Magnetic properties | N/A | N/A  | N/A      | N/A  |\\n| Optical properties | N/A | N/A  | N/A      | N/A  |\\n\\nNote: N/A indicates not applicable or not available.\"}"}
{"id": "i2e2wqt0nAI", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hard set of our proposed datasets (part 4).\\n\\nUnder review as a conference paper at ICLR 2023.\"}"}
{"id": "i2e2wqt0nAI", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Hard set of our proposed datasets (part 5).\\n\\nUnder review as a conference paper at ICLR 2023\\n\\n\\\\[ E = 1 \\\\]\\n\\n\\\\[ \\\\omega = 0 \\\\]\\n\\n\\\\[ \\\\omega = 2 \\\\]\\n\\n**Table S11:**\\n\\n| Polarizability V, F | V, F | V, F, P |\\n|----------------------|------|---------|\\n| Mass V, F | V, F | V, F, P |\\n| Radius of dielectric sphere V, F | V, F | V, F, P |\\n| Speed of light V, F | C, F | P |\\n| Distance V, F | V, F | P |\\n| Deviation from the harmonic oscillator V, F | | |\\n| Variable V, F | V, F | P |\\n| Fine structure constant V, F | C, F | P |\\n| Wavelength of X-ray V, F | V, F | P |\\n| Frequency of electromagnetic waves V, F | V, F | P |\\n| Electron mass V, F | C, F | P |\\n| Position V, F | V, F | P |\\n| Distance between dipoles V, F | V, F | P |\\n| Number of phase difference V, F | I, P |\\n| Speed of light V, F | C, F | P |\\n| Distance V, F | V, F | P |\\n| Velocity V, F | V, F | P |\\n| Momentum V, F | V, F | P |\\n| Value V, F | V, F | P |\\n| Frequency of electromagnetic waves V, F | N/A | N/A |\\n| Charge V, F | V | P |\\n| Electric charge V, F | V, F | P |\\n| Voltage V, F | V, F | P |\\n| Potential V, F | V, F | P |\\n| Potential (out) V, F | N/A | N/A |\\n| Differential cross section V, F | N/A | N/A |\\n| Amplitude of wave V, F | V, F | N/A |\\n| Pressure V, F | V, F | N/A |\\n| Deviation from the harmonic oscillator V, F | N/A | P |\\n| Variable V, F | V, F | P |\\n| Distance V, F | V, F | P |\\n| Fine structure constant V, F | C, F | P |\\n| Frequency of electromagnetic waves V, F | N/A | N/A |\\n| Speed of light V, F | C, F | P |\\n| Vacuum permittivity V, F | C, F | P |\\n| Speed of light V, F | C, F | P |\\n| Distance V, F | V, F | P |\\n| Velocity V, F | V, F | P |\\n| Momentum V, F | V, F | P |\\n| Value V, F | V, F | P |\\n| Frequency of electromagnetic waves V, F | N/A | N/A |\"}"}
{"id": "i2e2wqt0nAI", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table S12 shows the hyperparameter space for the five existing symbolic regression baselines. The hyperparameters of gplearn (Koza & Poli, 2005), AFP (Schmidt & Lipson, 2011), and AFP-FE (Schmidt & Lipson, 2009) are optimized by Optuna (Akiba et al., 2019), a hyperparameter optimization framework.\\n\\n### Table S12: Hyperparameter sets for the five existing symbolic regression baselines.\\n\\n| Method   | Hyperparameter sets                                                                 |\\n|----------|-------------------------------------------------------------------------------------|\\n| gplearn  | 100 trials with random combinations of the following hyperparameter spaces:          |\\n|          | - population size: $U(10^2, 10^3)$                                                  |\\n|          | - generations: $U(10, 10^2)$                                                        |\\n|          | - stopping criteria: $U(10^{-10}, 10^{-2})$                                          |\\n|          | - warm start: $\\{True, False\\}$                                                      |\\n|          | - const range: $\\\\{None, (-1.0, 1.0), (-10, 10), (-10^2, 10^2), (-10^3, 10^3)\\\\}$     |\\n|          | - max samples: $U(0.9, 1.0)$                                                         |\\n|          | - parsimony coefficient: $U(10^{-3}, 10^{-2})$                                       |\\n| AFP      | 100 trials with random combinations of the following hyperparameter spaces:          |\\n|          | - popsize: $U(100, 1000)$                                                           |\\n|          | - $g$: $U(250, 2500)$                                                               |\\n|          | - stop threshold: $U(10^{-10}, 10^{-2})$                                             |\\n|          | - op list: $\\\\{n, v, +, -, *, /, exp, log, 2, 3, sqrt\\\\}$                            |\\n|          | - $op$ list: $\\\\{n, v, +, -, *, /, exp, log, 2, 3, sqrt, sin, cos\\\\}$                 |\\n| AFP-FE   | 100 trials with random combinations of the following hyperparameter spaces:          |\\n|          | - popsize: $U(100, 1000)$                                                           |\\n|          | - $g$: $U(250, 2500)$                                                               |\\n|          | - stop threshold: $U(10^{-10}, 10^{-2})$                                             |\\n|          | - op list: $\\\\{n, v, +, -, *, /, exp, log, 2, 3, sqrt\\\\}$                            |\\n|          | - $op$ list: $\\\\{n, v, +, -, *, /, exp, log, 2, 3, sqrt, sin, cos\\\\}$                 |\\n| AI Feynman | $\\\\{bftt: 60, epoch: 300, op: '7ops.txt', poly deg: 3\\\\}$                   |\\n|          | - $bftt$: 60, $epoch$: 300, $op$: '10ops.txt', $poly deg$: 3                     |\\n|          | - $bftt$: 60, $epoch$: 300, $op$: '14ops.txt', $poly deg$: 3                     |\\n|          | - $bftt$: 60, $epoch$: 300, $op$: '19ops.txt', $poly deg$: 3                     |\\n|          | - $bftt$: 120, $epoch$: 300, $op$: '14ops.txt', $poly deg$: 4                     |\\n|          | - $bftt$: 120, $epoch$: 300, $op$: '19ops.txt', $poly deg$: 4                     |\\n|          | - $bftt$: 60, $epoch$: 500, $op$: '7ops.txt', $poly deg$: 3                      |\\n|          | - $bftt$: 60, $epoch$: 500, $op$: '10ops.txt', $poly deg$: 3                      |\\n|          | - $bftt$: 60, $epoch$: 500, $op$: '14ops.txt', $poly deg$: 3                      |\\n|          | - $bftt$: 60, $epoch$: 500, $op$: '19ops.txt', $poly deg$: 3                      |\\n| DSR      | $\\\\{seed: 1, function set: \\\\{'add', 'sub', 'mul', 'div', 'sin', 'cos', 'exp', 'log'\\\\}\\\\}$ |\\n|          | - $seed$: 2, $function set$: \\\\{'add', 'sub', 'mul', 'div', 'sin', 'cos', 'exp', 'log'\\\\} |\\n|          | - $seed$: 3, $function set$: \\\\{'add', 'sub', 'mul', 'div', 'sin', 'cos', 'exp', 'log'\\\\} |\\n|          | - $seed$: 4, $function set$: \\\\{'add', 'sub', 'mul', 'div', 'sin', 'cos', 'exp', 'log'\\\\} |\\n|          | - $seed$: 5, $function set$: \\\\{'add', 'sub', 'mul', 'div', 'sin', 'cos', 'exp', 'log'\\\\} |\\n|          | - $seed$: 1, $function set$: \\\\{'add', 'sub', 'mul', 'div', 'sin', 'cos', 'exp', 'log', 'const'\\\\} |\\n|          | - $seed$: 2, $function set$: \\\\{'add', 'sub', 'mul', 'div', 'sin', 'cos', 'exp', 'log', 'const'\\\\} |\\n|          | - $seed$: 3, $function set$: \\\\{'add', 'sub', 'mul', 'div', 'sin', 'cos', 'exp', 'log', 'const'\\\\} |\\n|          | - $seed$: 4, $function set$: \\\\{'add', 'sub', 'mul', 'div', 'sin', 'cos', 'exp', 'log', 'const'\\\\} |\\n|          | - $seed$: 5, $function set$: \\\\{'add', 'sub', 'mul', 'div', 'sin', 'cos', 'exp', 'log', 'const'\\\\} |\"}"}
{"id": "i2e2wqt0nAI", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To discuss the potential of symbolic regression for scientific discovery (SRSD), there still remain some issues to be addressed: oversimplified datasets and lack of evaluation metric towards SRSD. For symbolic regression tasks, existing datasets consist of values sampled from limited domains such as in range of 1 to 5, and there are no large-scale datasets with reasonably realistic values that capture the properties of the formula and its variables. Thus, it is difficult to discuss the potential of symbolic regression for scientific discovery with such existing datasets. For instance, the FSRD consists of 120 formulas selected mostly from Feynman Lectures Series (Feynman et al., 1963a;b;c) and are core benchmark datasets used in SRBench (La Cava et al., 2021). While the formulas indicate physical laws, variables and constants used in each dataset have no physical meanings since the datasets are not designed to discover the physical laws from the observed data in the real world. (See Section 3.1.)\\n\\nMoreover, there is a lack of appropriate metrics to evaluate these methods for SRSD. An intuitive approach would be to measure the prediction error or correlation between the predicted values and the target values in the test data, as in standard regression problems. However, low prediction errors could be achieved even by complex models that differ from the original law. In addition, SRBench (La Cava et al., 2021) presents the percentage of agreement between the target and the estimated equations. But in such cases, both 1) equations that do not match at all and 2) that differ by only one term are equally treated as incorrect. As a result, it is considered as a coarse-resolution evaluation method for accuracy in SRSD, which still needs more discussion towards real-world applications. A key feature of SR is its interpretability, and some studies (Udrescu et al., 2020; La Cava et al., 2021) use complexity of the predicted expression as an evaluation metric (the simpler the better). However, it is based on a big assumption that a simpler expression may be more likely to be a hidden law in the data (scientific discovery such as physics law), which may not be true for SRSD. Therefore, there are no single evaluation metrics proposed to take into account both the interpretability and how close to the true expression the estimated expression is.\\n\\nTo address these issues, we propose new SRSD datasets, introduce a new evaluation method, and conduct benchmark experiments using representative SR methods and a new Transformer-based SR baseline. We carefully review and design annotation policies for the new datasets, considering the properties of the physics formulas. Besides, given that a formula can be represented as a tree structure, we introduce a normalized edit distance on the tree structure to allow quantitative evaluation of predicted formulas that do not perfectly match the true formulas. Using the proposed SRSD datasets and evaluation metric, we perform benchmark experiments with a set of SR baselines and find that there is still significant room for improvements in terms of the new evaluation metric.\\n\\n2 RELATED STUDIES\\n\\nIn this section, we briefly introduce related studies focused on 1) symbolic regression for scientific discovery and 2) symbolic regression dataset and evaluation.\\n\\n2.1 SRSD: SYMBOLIC REGRESSION FOR SCIENTIFIC DISCOVERY\\n\\nA pioneer study on symbolic regression for scientific discovery is conducted by Schmidt & Lipson (2009), who propose a data-driven scientific discovery method. They collect data from standard experimental systems like those used in undergrad physics education: an air-track oscillator and a double pendulum. Their proposed algorithm detects different types of laws from the data such as position manifolds, energy laws, and equations of motion and sum of forces laws.\\n\\nFollowing the study, data-driven scientific discovery has been attracting attention from research communities and been applied to various domains such as Physics (Wu & Tegmark, 2019; Udrescu & Tegmark, 2020; Udrescu et al., 2020; Kim et al., 2020; Cranmer et al., 2020; Liu & Tegmark, 2021; Liu et al., 2021b), Applied Mechanics (Huang et al., 2021), Climatology (Abdellaoui & Mehrkanoon, 2021), Materials (Sun et al., 2019; Wang et al., 2019; Weng et al., 2020; Loftis et al., 2020), and Chemistry (Batra et al., 2020).\\n\\n2 Udrescu & Tegmark (2020) extract 20 of the 120 equations as \u201cbonus\u201d from other seminal books (Goldstein et al., 2002; Jackson, 1999; Weinberg, 1972; Schwartz, 2014).\\n\\n3 If those differ by a constant or scalar, SRBench treats the estimated equation as correct for solution rate.\"}"}
{"id": "i2e2wqt0nAI", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"These studies leverage symbolic regression in different fields. While general symbolic regression tasks use synthetic datasets with limited sampling domains for benchmarks, many of the SRSD studies collect data from the real world and discuss how we could leverage symbolic regression toward scientific discovery.\\n\\nWhile SRSD tasks share the same input-output interface with general symbolic regression (SR) tasks (i.e., input: dataset, output: symbolic expression), we differentiate SRSD tasks in this study from general SR tasks by whether or not the datasets including true symbolic expressions are created with reasonably realistic assumptions for scientific discovery such as meaning of true symbolic expressions (whether or not they have physical meanings) and sampling domains for input variables.\\n\\n2.2 DATASET AND EVALUATION\\n\\nFor symbolic regression methods, there exist several benchmark datasets and empirical studies. The Feynman Symbolic Regression Database (Udrescu & Tegmark, 2020) is one of the largest symbolic regression datasets, which consists of 100 physics-inspired equations based on Feynman Lectures on Physics (Feynman et al., 1963a;b;c). By randomly sampling from small ranges of value, they generate the corresponding tabular datasets for the 100 equations. Inspired by Hoai et al. (2002); Keijzer (2003); Johnson (2009), Uy et al. (2011) suggest 10 different real-valued symbolic regression problems (functions) and create the corresponding dataset (a.k.a. Nguyen dataset). The suggested functions consist of either 1 or 2 variables e.g.,\\n\\n\\\\[ f(x) = x^6 + x^5 + x^4 + x^3 + x^2 + x \\\\]\\n\\nand\\n\\n\\\\[ f(x, y) = \\\\sin(x) + \\\\sin(y^2) \\\\].\\n\\nThey generate each dataset by randomly sampling 20 - 100 data points.\\n\\nLa Cava et al. (2021) design a symbolic regression benchmark, named SRBench, and conduct a comprehensive benchmark experiment, using existing symbolic regression datasets such as the Feynman Symbolic Regression Database (Udrescu & Tegmark, 2020) and ODE-Strogatz repository (La Cava et al., 2016). In SRBench, symbolic regression methods are assessed by 1) an error metric based on squared error between target and estimated values, and 2) solution rate that shows a percentage of the estimated symbolic regression models that match the true models (equations).\\n\\nHowever, these datasets and evaluations are not necessarily designed to discuss symbolic regression for scientific discovery. In Sections 3.1 and 4.1, we further describe potential issues in prior studies.\\n\\n3 DATASETS\\n\\nIn this section, we summarize issues we found in the existing symbolic regression datasets, and then propose new datasets to address them towards symbolic regression for scientific discovery (SRSD).\\n\\n3.1 ISSUES IN EXISTING DATASETS\\n\\nAs introduced in Section 2.2, there are many symbolic regression datasets. However, we consider that novel datasets are required to discuss SRSD for the following reasons:\\n\\n1. **No physical meaning:** Many of the existing symbolic regression datasets (Hoai et al., 2002; Keijzer, 2003; Johnson, 2009; Uy et al., 2011) are not necessarily physics-inspired, but instead randomly generated e.g.,\\n\\n\\\\[ f(x) = \\\\log(x) \\\\]\\n\\n\\\\[ f(x, y) = xy + \\\\sin((x - 1)(y - 1)) \\\\].\\n\\nTo discuss the potential of symbolic regression for scientific discovery, we need to further elaborate datasets and evaluation metrics, considering how we would leverage symbolic regression in practice.\\n\\n2. **Oversimplified sampling process:** While some of the datasets are physics-inspired such as the Feynman Symbolic Regression Database (FSRD) (Udrescu & Tegmark, 2020) and ODE-Strogatz repository (La Cava et al., 2016), their sampling strategies are very simplified. Specifically, the strategies do not distinguish between constants and variables e.g., speed of light \\\\(c\\\\) is treated as a variable and randomly sampled in range of 1 to 5. Besides, most of the sampling domains are far from values we could observe in the real world e.g., II.4.23 in Table S1 (the vacuum permittivity values are sampled from range of 1 to 5). When sampled ranges of the distributions are narrow, we cannot distinguish Lorentz transformation from Galilean transformation e.g., I.15.10 and I.16.6 in Table S3, I.48.2 in Table S5, I.15.3t, I.15.3x, and I.34.14 in Table S7, or the black body...\"}"}
{"id": "i2e2wqt0nAI", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"radiation can be misestimated to Stephan-Boltzmann law or the Wien displacement law e.g. I.41.16 in Table S8.\\n\\nDue to the two issues above, many of the equations in existing datasets turn out to be duplicate. e.g., as shown in Table 1, \\\\( F = \\\\mu N n \\\\) (I.12.1) and \\\\( F = q^2 E \\\\) (I.12.5) in the original Feynman Symbolic Regression Database are considered identical since both the equations are multiplicative and consist of two variables, and their sampling domains (Distributions in Table 1) are exactly the same. For instance, approximately 25% of the symbolic regression problems in the original FSRD have 1 - 5 duplicates in that regard.\\n\\nThe Feynman Symbolic Regression Database (Udrescu & Tegmark, 2020) treat every variables as float whereas they should be integer to be physically meaningful. For example, the number of phase difference in Bragg's law should be integer but sampled as real number (I.30.5 in Table S1). Furthermore, they don't even give special treatment of angle variables (I.18.12, I.18.16, and I.26.2 in Table 1). Physically some variables can be negative whereas the original Feynman Symbolic Regression Database (Udrescu & Tegmark, 2020) only samples positive values (e.g. I.8.14 and I.11.19 in Table S3). We also avoid using \\\\( \\\\text{arcsin} / \\\\text{arccos} \\\\) in the equations since the use of \\\\( \\\\text{arcsin} / \\\\text{arccos} \\\\) in the Feynman Symbolic Regression Database (Udrescu & Tegmark, 2020) just to obtain angle variable is not experimentally meaningful (I.26.2 in Table 1, I.30.5 in Table S1, and B10 in Table S11). Equations using \\\\( \\\\text{arcsin} \\\\) and \\\\( \\\\text{arccos} \\\\) in the original annotation are I.26.2 (Snell's law), I.30.5 (Bragg's law), and B10 (Relativistic aberration). These are all describing physical phenomena related to two angles, and it is an unnatural deformation to describe only one of them with an inverse function. Additionally, inverse function use implicitly limits the range of angles, but there is no such limitation in the actual physical phenomena.\\n\\nWe address the issues in existing datasets above by proposing new SRSD datasets based on the equations used in the FSRD (Udrescu & Tegmark, 2020). i.e., Section 3.1 summarizes the differences between the FSRD and our SRSD datasets. Our annotation policy is carefully designed to simulate typical physics experiments so that the SRSD datasets can engage studies on symbolic regression for scientific discovery in the research community.\\n\\nWe thoroughly revised the sampling range for each variable from the annotations in the FSRD (Udrescu & Tegmark, 2020). First, we reviewed the properties of each variable and treated physical constants (e.g., light speed, gravitational constant) as constants while such constants are treated as variables in the original FSRD datasets. Next, variable ranges were defined to correspond to each typical physics experiment to confirm the physical phenomenon for each equation. We also used (of Japan, 2022) as a reference. In cases where a specific experiment is difficult to be assumed, ranges were set within which the corresponding physical phenomenon can be seen. Generally, the ranges are set to be sampled on log scales within their orders as \\\\( 10^2 \\\\) in order to take both large and small changes in value as the order changes. Variables such as angles, for which a linear distribution is expected are set to be sampled uniformly. In addition, variables that take a specific sign were set to be sampled within that range. Tables 1 and S1 \u2013 S11 show the detailed comparisons between the original FSRD and our proposed SRSD datasets.\\n\\nWhile the proposed datasets consist of 120 different problems, there will be non-trivial training cost required to train a symbolic regression model for all the problems individually (La Cava et al., 2021) i.e., there will be 120 separate training sessions to assess the symbolic regression approach. To allow more flexibility in assessing symbolic regression models for scientific discovery, we define three clusters of the proposed datasets based on their complexity: Easy, Medium, and Hard sets, which consist of 30, 40, and 50 different problems respectively.\\n\\nWe define the complexity of problem, using the number of operations to represent the true equation tree and range of the sampling domains. The former measures how many mathematical operations compose the true equation such as add, mul, pow, exp, and log operations (see Fig. 2). The latter...\"}"}
{"id": "i2e2wqt0nAI", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nTable 1: Easy set of our proposed datasets (part 1). C: Constant, V: Variable, F: Float, I: Integer, P: Positive, N: Negative, NN: Non-Negative, U: Uniform distribution, U\\\\(\\\\log\\\\): Log-Uniform distribution.\\n\\n| Eq. ID | Formula | Symbols | Properties | Distributions |\\n|--------|---------|---------|------------|---------------|\\n| I.12.1 | \\\\[ F = \\\\mu \\\\mathcal{N} \\\\] | V, F | V, F, P | N/A, N/A |\\n|        | \\\\mu \\\\text{Coefficient of friction} V, F | V, F | P | U(1,5), U\\\\(\\\\log\\\\)(10^{-2}, 10^{0}) |\\n|        | \\\\mathcal{N} \\\\text{Normal force} V, F | V, F | P | U(1,5), U\\\\(\\\\log\\\\)(10^{-2}, 10^{0}) |\\n| I.12.4 | \\\\[ E = q_{1} \\\\frac{4\\\\pi \\\\epsilon r^{2}}{2} \\\\] | V, F | V, F | N/A, N/A |\\n|        | q_{1} \\\\text{Electric charge} V, F | V, F | | U(1,5), U\\\\(\\\\log\\\\)(10^{-3}, 10^{-1}) |\\n|        | \\\\epsilon \\\\text{Vacuum permittivity} V, F | C, F | P | \\\\(8.854 \\\\times 10^{-12}\\\\) |\\n| I.12.5 | \\\\[ F = q_{2} E \\\\] | V, F | V, F | N/A, N/A |\\n|        | q_{2} \\\\text{Electric charge} V, F | V, F | | U(1,5), U\\\\(\\\\log\\\\)(10^{-3}, 10^{-1}) |\\n|        | E \\\\text{Electric field} V, F | V, F | | U(1,5), U\\\\(\\\\log\\\\)(10^{1}, 10^{3}) |\\n| I.14.3 | \\\\[ U = mgz \\\\] | V, F | V, F, P | N/A, N/A |\\n|        | m \\\\text{Mass} V, F | V, F | P | U(1,5), U\\\\(\\\\log\\\\)(10^{-2}, 10^{0}) |\\n|        | g \\\\text{Gravitational acceleration} V, F | C, F | P | \\\\(9.807 \\\\times 10^{0}\\\\) |\\n|        | z \\\\text{Height} V, F | V, F | | U(1,5), U\\\\(\\\\log\\\\)(10^{-2}, 10^{0}) |\\n| I.14.4 | \\\\[ U = k_{\\\\text{spring}} x^{2} \\\\] | V, F | V, F, P | N/A, N/A |\\n|        | k_{\\\\text{spring}} \\\\text{Spring constant} V, F | V, F | P | U(1,5), U\\\\(\\\\log\\\\)(10^{2}, 10^{4}) |\\n|        | x \\\\text{Position} V, F | V, F | | U(1,5), U\\\\(\\\\log\\\\)(10^{-2}, 10^{0}) |\\n| I.18.1 | \\\\[ \\\\tau = rF \\\\sin \\\\theta \\\\] | V, F | V, F | N/A, N/A |\\n|        | r \\\\text{Distance} V, F | V, F | P | U(1,5), U\\\\(\\\\log\\\\)(10^{-1}, 10^{1}) |\\n|        | F \\\\text{Force} V, F | V, F | | U(1,5), U\\\\(\\\\log\\\\)(10^{-1}, 10^{1}) |\\n|        | \\\\theta \\\\text{Angle} V, F | V, F, NN | | U(0,5), U(0,2\\\\pi) |\\n| I.18.16 | \\\\[ L = mrv \\\\sin \\\\theta \\\\] | V, F | V, F | N/A, N/A |\\n|        | m \\\\text{Mass} V, F | V, F | P | U(1,5), U\\\\(\\\\log\\\\)(10^{-1}, 10^{1}) |\\n|        | r \\\\text{Distance} V, F | V, F | P | U(1,5), U\\\\(\\\\log\\\\)(10^{-1}, 10^{1}) |\\n|        | v \\\\text{Velocity} V, F | V, F | P | U(1,5), U\\\\(\\\\log\\\\)(10^{-1}, 10^{1}) |\\n|        | \\\\theta \\\\text{Angle} V, F | V, F, NN | | U(1,5), U(0,2\\\\pi) |\\n| I.25.13 | \\\\[ V = \\\\frac{q}{C} \\\\] | V, F | V, F | N/A, N/A |\\n|        | q \\\\text{Electric charge} V, F | V, F | | U(1,5), U\\\\(\\\\log\\\\)(10^{-5}, 10^{-3}) |\\n|        | C \\\\text{Electrostatic Capacitance} V, F | V, F | P | U(1,5), U\\\\(\\\\log\\\\)(10^{-5}, 10^{-3}) |\\n| I.26.2 | \\\\[ n = \\\\frac{\\\\sin \\\\theta_{1}}{\\\\sin \\\\theta_{2}} \\\\] | V, F | V, F, P | N/A |\\n|        | \\\\theta_{1} \\\\text{Refraction angle 1} V, F | V, F | | U(0,\\\\pi/2) |\\n|        | \\\\theta_{2} \\\\text{Refraction angle 2} V, F | V, F | | U(1,5), U(0,\\\\pi/2) |\\n| I.27.6 | \\\\[ f = \\\\frac{1}{d_{1} + n d_{2}} \\\\] | V, F | V, F | N/A, N/A |\\n|        | d_{1} \\\\text{Distance} V, F | V, F | P | U(1,5), U\\\\(\\\\log\\\\)(10^{-3}, 10^{-1}) |\\n|        | n \\\\text{Refractive index} V, F | V, F, P | | U(1,5), U\\\\(\\\\log\\\\)(10^{-1}, 10^{1}) |\\n|        | d_{2} \\\\text{Distance} V, F | V, F | P | U(1,5), U\\\\(\\\\log\\\\)(10^{-3}, 10^{-1}) |\\n\\nConsider magnitude of sampling distributions (Distributions column in Tables 1 and S1 \u2013 S11) and increases the complexity when sampling values from wide range of distributions. We define the domain range as follows:\\n\\n\\\\[\\n\\\\text{range}(S) = \\\\log_{10} \\\\max_{s \\\\in S} s - \\\\min_{s \\\\in S} s,\\n\\\\]\\n\\nwhere \\\\(S\\\\) indicates a set of sampling domains (distributions) for a given symbolic regression problem.\\n\\nAs we will show in Section 5.3, these clusters represent problem difficulties at high level. For instance, these subsets will help the research community to shortly tune and/or perform sanity-check new approaches on the Easy set (30 problems) instead of using the whole datasets (120 problems).\\n\\nFigure 1 shows the three different distribution maps of our proposed datasets. Easy, Medium, and Hard sets consist of 30, 40, and 50 individual symbolic regression problems, respectively.\\n\\n4 BENCHMARK\\n\\nBesides the conventional metrics, we propose a new metric to discuss the performance of symbolic regression for scientific discovery in Section 4.1. Following the set of metrics, we design an evaluation framework of symbolic regression for scientific discovery.\"}"}
{"id": "i2e2wqt0nAI", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"This section provides additional information regarding our SRSD datasets. We created the datasets to discuss the performance of symbolic regression for scientific discovery (SRSD). Each of SRSD datasets consists of 10,000 samples and has train, val, and test splits with ratio of 8:1:1. We refer readers to Section 3 for details of the datasets. Tables S1 \u2013 S11 comprehensively summarize the differences between FSRD and our SRSD datasets. Note that the table of Easy set (part 1) is provided as Table 1 in Section 3.1. As described in Section 3.2.2, we categorized each of the 120 SRSD datasets into one of Easy, Medium, and Hard sets. The datasets and the documentations are provided as part of the supplementary material.\\n\\nTable S1: Easy set of our proposed datasets (part 2). C: Constant, V: Variable, F: Float, I: Integer, P: Positive, N: Negative, NN: Non-Negative, I*: Integer treated as float due to the capacity of 32-bit integer, U: Uniform distribution, U\\\\log: Log-Uniform distribution.\\n\\n| Eq. ID | Formula | Symbols | Properties | Distributions |\\n|--------|---------|---------|------------|---------------|\\n| I.30.5 | $d = \\\\lambda n \\\\sin \\\\theta$ | V, F | V, F, P | U(2, 5) |\\n|       |         |         |            | U\\\\log(10^{-11}, 10^{-9}) |\\n|       |         |         |            | N/A |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{0}, 10^{2}) |\\n|       |         |         |            | N/A |\\n|       |         |         |            | U(\\\\pi, 2\\\\pi) |\\n| I.43.16| $v = \\\\mu q V$ | V, F | V, F | N/A | N/A |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-6}, 10^{-4}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-11}, 10^{-9}) |\\n|       |         |         |            | V(1, 5) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-1}, 10^{1}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-3}, 10^{-1}) |\\n| I.47.23| $c = q \\\\gamma P \\\\rho$ | V, F | V, F, P | N/A | N/A |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-1}, 10^{1}) |\\n|       |         |         |            | P(1, 5) |\\n|       |         |         |            | U(0.5 \\\\times 10^{-5}, 1.5 \\\\times 10^{-5}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-3}, 10^{-1}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-2}, 10^{0}) |\\n| II.2.42| $J = \\\\kappa (T_2 - T_1)$ | V, F | V, F | N/A | N/A |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-1}, 10^{1}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-3}, 10^{-1}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-2}, 10^{0}) |\\n| II.3.24| $h = \\\\frac{W}{4\\\\pi r^2}$ | V, F | V, F | N/A | N/A |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{0}, 10^{2}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-4}, 10^{-2}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-2}, 10^{0}) |\\n| II.4.23| $\\\\phi = \\\\frac{q}{4\\\\pi \\\\epsilon r}$ | V, F | V, F | N/A | N/A |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-3}, 10^{-1}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-2}, 10^{0}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-1}, 10^{1}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-2}, 10^{0}) |\\n| II.8.31| $u = \\\\frac{\\\\epsilon E^2}{2}$ | V, F | V, F | N/A | N/A |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-3}, 10^{-1}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-2}, 10^{0}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-1}, 10^{1}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-2}, 10^{0}) |\\n| II.10.9| $E = \\\\sigma_{\\\\text{free}} \\\\epsilon_1 \\\\left(1 + \\\\chi \\\\right)$ | V, F | V, F | N/A | N/A |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-3}, 10^{-1}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-2}, 10^{0}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-1}, 10^{1}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-2}, 10^{0}) |\\n| II.13.17| $B = \\\\frac{1}{4\\\\pi \\\\epsilon c^2} I r$ | V, F | V, F | N/A | N/A |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-3}, 10^{-1}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-2}, 10^{0}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-1}, 10^{1}) |\\n|       |         |         |            | U(1, 5) |\\n|       |         |         |            | U\\\\log(10^{-2}, 10^{0}) |\"}"}
{"id": "i2e2wqt0nAI", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| II.15.5 | \\\\( U = -pE \\\\cos \\\\theta \\\\) | Energy | V, F | V, F | N/A | N/A |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| Eq. ID | Formula | Symbols | Properties | Distributions |\\n| II.27.16 | \\\\( S = \\\\epsilon cE^2 \\\\) | Radiant intensity | V, F | V, F | N/A | N/A |\\n| | | Vacuum permittivity | V, F | C, F, P | \\\\( \\\\epsilon = 8.854 \\\\times 10^{-12} \\\\) | |\\n| | | Speed of light | V, F | C, F, P | \\\\( c = 2.998 \\\\times 10^8 \\\\) | |\\n| | | Magnitude of electric field | V, F | V, F, P | \\\\( E \\\\) | |\\n| | | Angle | V, F | V, F | \\\\( \\\\theta \\\\) | |\\n| II.27.18 | \\\\( u = \\\\epsilon E^2 \\\\) | Energy density | V, F | V, F, P | N/A | N/A |\\n| | | Vacuum permittivity | V, F | C, F, P | \\\\( \\\\epsilon = 8.854 \\\\times 10^{-12} \\\\) | |\\n| | | Magnitude of electric field | V, F | V, F, P | \\\\( E \\\\) | |\\n| | | Logarithmic | | \\\\( \\\\log_{10} \\\\) | |\\n| III.34.11 | \\\\( \\\\omega = gqB^2/m \\\\) | Angular frequency | V, F | V, F | N/A | N/A |\\n| | | G-factor | V, F | V, F | \\\\( g \\\\) | |\\n| | | Electric charge | V, F | V, F | \\\\( q \\\\) | |\\n| | | Magnetic field strength | V, F | V, F | \\\\( B \\\\) | |\\n| | | Mass | V, F | V, F, P | \\\\( m \\\\) | |\\n| | | Logarithmic | | \\\\( \\\\log_{10} \\\\) | |\\n| III.34.29b | \\\\( U = 2\\\\pi g\\\\mu B J_z \\\\) | Energy | V, F | V, F, P | N/A | N/A |\\n| | | G-factor | V, F | V, F | \\\\( g \\\\) | |\\n| | | Bohr magneton | V, F | C, F, P | \\\\( \\\\mu = 9.2740100783 \\\\times 10^{-24} \\\\) | |\\n| | | Magnetic field strength | V, F | V, F | \\\\( B \\\\) | |\\n| | | Element of angular momentum | V, F | V, F | \\\\( J_z \\\\) | |\\n| | | Planck constant | V, F | C, F, P | \\\\( h = 6.626 \\\\times 10^{-34} \\\\) | |\\n| III.38.3 | \\\\( F = Y A \\\\Delta l/l \\\\) | Force | V, F | V, F | N/A | N/A |\\n| | | Young's modulus | V, F | V, F, P | \\\\( Y \\\\) | |\\n| | | Area | V, F | V, F, P | \\\\( A \\\\) | |\\n| | | Displacement | V, F | V, F | \\\\( \\\\delta l \\\\) | |\\n| | | Length | V, F | V, F, P | \\\\( l \\\\) | |\\n| III.38.14 | \\\\( \\\\mu = Y^2 (1+\\\\sigma) \\\\) | Rigidity modulus | V, F | V, F, P | N/A | N/A |\\n| | | Young's modulus | V, F | V, F, P | \\\\( Y \\\\) | |\\n| | | Poisson coefficient | V, F | V, F, P | \\\\( \\\\sigma \\\\) | |\\n| | | Logarithmic | | \\\\( \\\\log_{10} \\\\) | |\\n| III.7.38 | \\\\( \\\\omega = 4\\\\pi\\\\mu B h \\\\) | Precession frequency | V, F | V, F | N/A | N/A |\\n| | | Magnetic moment | V, F | V, F | \\\\( \\\\mu \\\\) | |\\n| | | Magnetic flux density | V, F | V, F | \\\\( B \\\\) | |\\n| | | Planck constant | V, F | C, F, P | \\\\( h = 6.626 \\\\times 10^{-34} \\\\) | |\\n| III.12.43 | \\\\( J = m\\\\hbar^2/\\\\pi \\\\) | Variable | V, F | V, F | N/A | N/A |\\n| | | Spin state | V, F | V, I, NN | \\\\( m \\\\) | |\\n| | | Planck constant | V, F | C, F, P | \\\\( h = 6.626 \\\\times 10^{-34} \\\\) | |\\n| III.15.27 | \\\\( k = 2\\\\pi N \\\\sigma b \\\\) | Wavenumber | V, F | V, F | N/A | N/A |\\n| | | Parameter of state | V, F | V, I | \\\\( \\\\sigma \\\\) | |\\n| | | Number of atoms | V, F | V, I, P | \\\\( N \\\\) | |\\n| | | Lattice constant | V, F | V, F, P | \\\\( b \\\\) | |\"}"}
{"id": "i2e2wqt0nAI", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Eq. | ID | Formula | Symbols | Properties | Distributions\\n---|---|---|---|---|---\\n1 | $p_x$ | $\\\\mathbf{G}_m$ | $q_1$ | $\\\\mathbf{B}_v m_1 + \\\\mathbf{1} + \\\\mathbf{r}_1 \\\\mathbf{1} + \\\\mathbf{2}$ | $\\\\mathbf{v}_0 \\\\mathbf{c} + \\\\mathbf{2}$\\n2 | $\\\\mathbf{v}_1 \\\\mathbf{c} + \\\\mathbf{2} \\\\mathbf{w}_1 \\\\mathbf{c}$ | $\\\\mathbf{q}_2 \\\\mathbf{v}_1 \\\\mathbf{c} + \\\\mathbf{2} \\\\mathbf{w}_1 \\\\mathbf{c}$ | $\\\\mathbf{r}_1 \\\\mathbf{1} + \\\\mathbf{2} \\\\mathbf{w}_1 \\\\mathbf{c}$ | $\\\\mathbf{v}_0 \\\\mathbf{c} + \\\\mathbf{2}$\\n3 | $\\\\mathbf{v}_1 \\\\mathbf{c} + \\\\mathbf{2} \\\\mathbf{w}_1 \\\\mathbf{c}$ | $\\\\mathbf{q}_2 \\\\mathbf{v}_1 \\\\mathbf{c} + \\\\mathbf{2} \\\\mathbf{w}_1 \\\\mathbf{c}$ | $\\\\mathbf{r}_1 \\\\mathbf{1} + \\\\mathbf{2} \\\\mathbf{w}_1 \\\\mathbf{c}$ | $\\\\mathbf{v}_0 \\\\mathbf{c} + \\\\mathbf{2}$\\n\\nTable S3:\\n\\n| Angle $V$, $F$ | Vacuum permittivity $V$, $F$ | Distance $V$, $F$ | Element of velocity $V$, $F$ | Velocity $V$, $F$ |\\n|---|---|---|---|---|\\n| $\\\\mathbf{\\\\theta}$ | $\\\\mathbf{\\\\epsilon}_\\\\mathbf{F}$ | $\\\\mathbf{v}_0 \\\\mathbf{c}$ | $\\\\mathbf{\\\\mathbf{w}} \\\\mathbf{c}$ | $\\\\mathbf{v}_0 \\\\mathbf{c}$ |\\n\\n| Speed of light $V$, $F$ | Magnetic field strength $V$, $F$ | Mass $V$, $F$ | Center of gravity $V$, $F$ |\\n|---|---|---|---|\\n| $\\\\mathbf{\\\\mathbf{c}}$ | $\\\\mathbf{\\\\mathbf{B}}$ | $\\\\mathbf{\\\\mathbf{m}}_1$ | $\\\\mathbf{\\\\mathbf{r}}_1$ |\\n\\n| Kinetic energy $V$, $F$, $P$ | Gravitational constant $V$, $F$ | Potential energy $V$, $F$, $P$ | Electrostatic force $V$, $F$ |\\n|---|---|---|---|\\n| $\\\\mathbf{K}$ | $\\\\mathbf{G}$ | $\\\\mathbf{U}$ | $\\\\mathbf{F}$ |\\n\\n| Work $V$, $F$, $P$ | Force $V$, $F$ |\\n|---|---|\\n| $\\\\mathbf{W}$ | $\\\\mathbf{F}$ |\\n\\n| 16 | 854 | (10$^9$) | 674 | 998 | (10$^9$) |\\n| 10 | 998 | (10$^9$) | 674 | 998 | (10$^9$) |\\n| 3 | 6 | 10 | 0 | 1 | 10 |\\n| 2 | 12 | 10 | 0 | 1 | 10 |\"}"}
{"id": "i2e2wqt0nAI", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table S4: Medium set of our proposed datasets (part 2).\\n\\n| Eq. ID | Formula | Symbols | Properties | Distributions |\\n|--------|---------|---------|------------|---------------|\\n| I.24.6 | $E = 4m(\\\\omega^2 + \\\\omega_0^2)x^2$ | $E$, $m$ | Energy, Mass | V, F, V, F, P |\\n|        | $E$     |          | Energy     | N/A, N/A      |\\n| I.29.4 | $k = \\\\omega_c$ | $k$, $\\\\omega$ | Wavenumber, Frequency of electromagnetic waves | V, F, V, F, P |\\n|        | $k$     |          | Wavenumber  | N/A, N/A      |\\n|        | $\\\\omega$ |          | Angular velocity | V, F, V, F |\\n|        | $\\\\omega_0$ |          | Angular velocity | V, F, V, F |\\n|        | $x$     |          | Position    | V, F, V, F, P |\\n| I.32.5 | $P = q^2a^26\\\\pi\\\\epsilon_c^3$ | $P$, $q$, $a$ | Radiant energy, Electric charge, Magnitude of direction vector | V, F, V, F, P |\\n|        | $P$     |          | Radiant energy | N/A, N/A      |\\n|        | $q$     |          | Electric charge | V, F, V, F, P |\\n|        | $a$     |          | Magnitude of direction vector | V, F, V, F, P |\\n|        | $\\\\epsilon$ |        | Vacuum permittivity | V, F, C, F, P |\\n|        | $c$     |          | Speed of light | U(1, 10), 2.998 \u00d7 10^8 |\\n| I.34.8 | $\\\\omega = qvB/p$ | $\\\\omega$, $q$, $v$, $B$, $p$ | Angular velocity, Electric charge, Velocity, Magnetic field, Angular momentum | V, F, V, F |\\n|        | $\\\\omega$ |          | Angular velocity | V, F, V, F |\\n|        | $q$     |          | Electric charge | V, F, V, F |\\n|        | $v$     |          | Velocity     | U(1, 2), U(10, 5) |\\n|        | $B$     |          | Magnetic field | U(1, 5), U(10, 1) |\\n|        | $p$     |          | Angular momentum | U(1, 5), U(10, 9) |\\n| I.34.10| $\\\\omega = \\\\omega_0(1 - v/c)$ | $\\\\omega$, $\\\\omega_0$, $v$, $c$ | Frequency of electromagnetic waves, Frequency of electromagnetic waves, Velocity, Speed of light | V, F, V, F, P |\\n|        | $\\\\omega$ |          | Frequency of electromagnetic waves | V, F, V, F, P |\\n|        | $\\\\omega_0$ |          | Frequency of electromagnetic waves | V, F, V, F, P |\\n|        | $v$     |          | Velocity     | U(1, 2), U(10, 1) |\\n|        | $c$     |          | Speed of light | U(3, 10), 2.998 \u00d7 10^8 |\\n| I.34.27| $W = h^2\\\\pi\\\\omega$ | $W$, $h$, $\\\\omega$ | Energy, Planck constant, Frequency of electromagnetic waves | V, F, V, F, P |\\n|        | $W$     |          | Energy       | N/A, N/A      |\\n|        | $h$     |          | Planck constant | V, F, C, F, P |\\n|        | $\\\\omega$ |          | Frequency of electromagnetic waves | V, F, V, F, P |\\n| I.38.12| $r = 4\\\\pi\\\\epsilon\\\\left(h/(2\\\\pi)^2\\\\right)m^2q^2$ | $r$, $\\\\epsilon$, $h$, $m$, $q$ | Bohr radius | V, F, V, F, P |\\n|        | $r$     |          | Bohr radius  | N/A, N/A      |\\n|        | $\\\\epsilon$ |        | Vacuum permittivity | V, F, C, F, P |\\n|        | $h$     |          | Planck constant | U(1, 5), 6.626 \u00d7 10^-34 |\\n|        | $m$     |          | Mass         | U(1, 5), U(10^-28, 10^-26) |\\n|        | $q$     |          | Electric charge | U(1, 5), U(10^-11, 10^-9) |\\n| I.39.10| $U = \\\\frac{3}{2}PV$ | $U$, $P$, $V$ | Internal energy, Pressure, Volume | V, F, V, F, P |\\n|        | $U$     |          | Internal energy | N/A, N/A      |\\n|        | $P$     |          | Pressure     | U(1, 5), U(10^4, 10^6) |\\n|        | $V$     |          | Volume       | U(1, 5), U(10^-5, 10^-3) |\\n| I.39.11| $U = PV\\\\gamma - 1$ | $U$, $P$, $V$, $\\\\gamma$ | Energy, Pressure, Volume, Heat capacity ratio | V, F, V, F |\\n|        | $U$     |          | Energy       | N/A, N/A      |\\n|        | $P$     |          | Pressure     | U(1, 5), U(10^4, 10^6) |\\n|        | $V$     |          | Volume       | U(1, 5), U(10^-5, 10^-3) |\\n|        | $\\\\gamma$ |        | Heat capacity ratio | U(2, 5), U(10, 2) |\\n| I.43.31| $D = \\\\mu kT$ | $D$, $\\\\mu$, $k$, $T$ | Diffusion coefficient, Viscosity, Boltzmann constant, Temperature | V, F, V, F, P |\\n|        | $D$     |          | Diffusion coefficient | N/A, N/A      |\\n|        | $\\\\mu$   |          | Viscosity    | U(1, 5), U(10^13, 10^15) |\\n|        | $k$     |          | Boltzmann constant | U(1, 5), 1.381 \u00d7 10^-23 |\\n|        | $T$     |          | Temperature  | U(1, 5), U(10^-1, 10^3) |\"}"}
{"id": "i2e2wqt0nAI", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Eq. ID | Formula | Symbols | Properties | Distributions |\\n|--------|---------|---------|------------|---------------|\\n| I.43.43 | $\\\\kappa = 1 - \\\\gamma$ | $\\\\kappa$ | Thermal conductivity | V, F | N/A | N/A |\\n|         | $\\\\frac{k}{v} \\\\sigma_c$ | $\\\\gamma$ | Heat capacity ratio | V, F, P | $U(2, 5)$ | $U(1, 2)$ |\\n|         | $k_B$ | $k_B$ | Boltzmann constant | V, F, C, F, P | $U(1, 5)$ | 1.381 $\\\\times 10^{-23}$ |\\n|         | $v$ | $v$ | Velocity | V, F | $U(1, 2)$ | $U(\\\\log_10(2, 10^4))$ |\\n|         | $\\\\sigma_c$ | $\\\\sigma_c$ | Molecular collision cross section | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-21}, 10^{-19}))$ |\\n| I.48.2 | $E = mc^2 \\\\sqrt{1 - \\\\frac{v^2}{c^2}}$ | $E$ | Energy | V, F | N/A | N/A |\\n|         | $m$ | $m$ | Mass | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-29}, 10^{-27}))$ |\\n|         | $c$ | $c$ | Speed of light | V, F, C, F, P | $U(3, 10)$ | 2.998 $\\\\times 10^8$ |\\n|         | $v$ | $v$ | Velocity | V, F | $U(1, 2)$ | $U(\\\\log_10(10^5, 10^7))$ |\\n|         | $r$ | $r$ | Distance | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-10}, 10^{-8}))$ |\\n| II.6.11 | $\\\\phi = \\\\frac{1}{4 \\\\pi \\\\epsilon_0} \\\\rho \\\\cos \\\\theta r^2$ | $\\\\phi$ | Electric potential | V, F | N/A | N/A |\\n|         | $\\\\epsilon$ | $\\\\epsilon$ | Vacuum permittivity | V, F, C, F, P | $U(1, 3)$ | 8.854 $\\\\times 10^{-12}$ |\\n|         | $\\\\rho$ | $\\\\rho$ | Electric dipole moment | V, F | $U(1, 3)$ | $U(\\\\log_10(10^{-22}, 10^{-20}))$ |\\n|         | $\\\\theta$ | $\\\\theta$ | Angle | V, F | $U(1, 3)$ | $U(0, 2\\\\pi)$ |\\n|         | $r$ | $r$ | Distance | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-10}, 10^{-8}))$ |\\n| II.8.7 | $U = \\\\frac{3}{5} \\\\frac{Q^2}{4 \\\\pi \\\\epsilon_0 a^3}$ | $U$ | Energy | V, F | N/A | N/A |\\n|         | $Q$ | $Q$ | Electric charge | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-11}, 10^{-9}))$ |\\n|         | $\\\\epsilon$ | $\\\\epsilon$ | Vacuum permittivity | V, F, C, F, P | $U(1, 5)$ | 8.854 $\\\\times 10^{-12}$ |\\n|         | $a$ | $a$ | Radius | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-12}, 10^{-10}))$ |\\n| II.11.3 | $x = qE m (\\\\omega_0^2 - \\\\omega^2)$ | $x$ | Position | V, F | N/A | N/A |\\n|         | $q$ | $q$ | Electric charge | V, F | $U(1, 3)$ | $U(\\\\log_10(10^{-11}, 10^{-9}))$ |\\n|         | $E$ | $E$ | Magnitude of electric field | V, F, P | $U(1, 3)$ | $U(\\\\log_10(10^{-9}, 10^{-7}))$ |\\n|         | $m$ | $m$ | Mass | V, F | $U(1, 3)$ | $U(\\\\log_10(10^{-28}, 10^{-26}))$ |\\n|         | $\\\\omega_0$ | $\\\\omega_0$ | Angular velocity | V, F | $U(3, 5)$ | $U(\\\\log_10(10^9, 10^{11}))$ |\\n|         | $\\\\omega$ | $\\\\omega$ | Angular velocity | V, F | $U(1, 2)$ | $U(\\\\log_10(10^9, 10^{11}))$ |\\n| II.21.32 | $\\\\phi = \\\\frac{q}{4 \\\\pi \\\\epsilon_0 r} \\\\left(1 - \\\\frac{v}{c}\\\\right)$ | $\\\\phi$ | Electric potential | V, F | N/A | N/A |\\n|         | $q$ | $q$ | Electric charge | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-11}, 10^{-9}))$ |\\n|         | $\\\\epsilon$ | $\\\\epsilon$ | Vacuum permittivity | V, F, C, F, P | $U(1, 5)$ | 8.854 $\\\\times 10^{-12}$ |\\n|         | $r$ | $r$ | Distance | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-10}, 10^{-8}))$ |\\n|         | $v$ | $v$ | Velocity | V, F | $U(1, 2)$ | $U(\\\\log_10(10^5, 10^7))$ |\\n|         | $c$ | $c$ | Speed of light | V, F, C, F, P | $U(3, 10)$ | 2.998 $\\\\times 10^8$ |\\n| II.34.2 | $\\\\mu = qvr^2$ | $\\\\mu$ | Magnetic moment | V, F | N/A | N/A |\\n|         | $q$ | $q$ | Electric charge | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-11}, 10^{-9}))$ |\\n|         | $v$ | $v$ | Velocity | V, F | $U(1, 5)$ | $U(\\\\log_10(10^5, 10^7))$ |\\n|         | $r$ | $r$ | Radius | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-11}, 10^{-9}))$ |\\n| II.34.2a | $I = qvr$ | $I$ | Electric Current | V, F | N/A | N/A |\\n|         | $q$ | $q$ | Electric charge | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-11}, 10^{-9}))$ |\\n|         | $v$ | $v$ | Velocity | V, F | $U(1, 5)$ | $U(\\\\log_10(10^5, 10^7))$ |\\n|         | $r$ | $r$ | Radius | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-11}, 10^{-9}))$ |\\n| II.34.29a | $\\\\mu = qh \\\\frac{4}{\\\\pi m}$ | $\\\\mu$ | Bohr magneton | V, F | N/A | N/A |\\n|         | $q$ | $q$ | Electric charge | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-11}, 10^{-9}))$ |\\n|         | $h$ | $h$ | Planck constant | V, F, C, F, P | $U(1, 5)$ | 6.626 $\\\\times 10^{-34}$ |\\n|         | $m$ | $m$ | Mass | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-30}, 10^{-28}))$ |\\n| II.37.1 | $E = \\\\mu (1 + \\\\chi) B$ | $E$ | Energy of magnetic field | V, F | N/A | N/A |\\n|         | $\\\\mu$ | $\\\\mu$ | Magnetic moment | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-25}, 10^{-23}))$ |\\n|         | $\\\\chi$ | $\\\\chi$ | Volume magnetic susceptibility | V, F | $U(1, 5)$ | $U(\\\\log_10(10^4, 10^6))$ |\\n|         | $B$ | $B$ | Magnetic field strength | V, F | $U(1, 5)$ | $U(\\\\log_10(10^{-3}, 10^{-1}))$ |\"}"}
{"id": "i2e2wqt0nAI", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| III.4.32 | $n = \\\\exp(\\\\frac{h\\\\omega}{2\\\\pi kT}) - 1$ |\\n| --- | --- |\\n| III.8.54 | $|C|^{2} = \\\\sin(2\\\\pi At)$ |\\n| III.13.18 | $v = \\\\frac{4\\\\pi Ab}{h k}$ |\\n| III.14.14 | $I = I_{0}(\\\\exp(\\\\frac{q\\\\Delta V}{\\\\kappa T}) - 1)$ |\\n| III.15.12 | $E = 2A(1 - \\\\cos(\\\\kappa d))$ |\\n| III.15.14 | $m = \\\\frac{h^{2}}{8\\\\pi^{2}Ab^{2}}$ |\\n| III.17.37 | $f = \\\\beta(1 + \\\\alpha \\\\cos \\\\theta)$ |\\n| III.19.51 | $E = -\\\\frac{mq^{4}}{2(4\\\\pi\\\\epsilon)_{2}(\\\\frac{h}{2\\\\pi})^{2}n_{2}}$ |\"}"}
{"id": "i2e2wqt0nAI", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Eq. ID | Formula | Symbols | Properties | Distributions |\\n|--------|---------|---------|------------|---------------|\\n| F = | \u03b5 | c | F | P | I.32.17 | I.29.16 | I.15.3x | I.15.3t |\\n| P = | exp | 1 | x | 2 | 1 | 1 | 2 | 2 |\\n| I = | exp | 1 | x | 2 | 1 | 1 | 2 | 2 |\\n| x = | exp | 1 | x | 2 | 1 | 1 | 2 | 2 |\\n| f = | exp | 1 | x | 2 | 1 | 1 | 2 | 2 |\\n\\nTable S7: Original Ours Original Ours\"}"}
{"id": "i2e2wqt0nAI", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"II.11.20\\n\\nWe present the hard set of our proposed datasets (part 2).\\n\\nUnder review as a conference paper at ICLR 2023\\n\\n\\\\[ E = \\\\pi \\\\cos \\\\theta \\\\]\\n\\n\\\\[ I = \\\\frac{2}{\\\\pi} \\\\cos \\\\theta \\\\]\\n\\n\\\\[ P = \\\\frac{1}{2} \\\\pi k T \\\\]\\n\\n\\\\[ Q = \\\\frac{3}{2} \\\\pi k T \\\\]\\n\\n\\\\[ R = \\\\frac{3}{2} \\\\pi k T \\\\]\\n\\n\\\\[ S = \\\\frac{3}{2} \\\\pi k T \\\\]\\n\\n\\\\[ T = \\\\frac{3}{2} \\\\pi k T \\\\]\\n\\n\\\\[ U = \\\\frac{3}{2} \\\\pi k T \\\\]\\n\\n\\\\[ V = \\\\frac{3}{2} \\\\pi k T \\\\]\\n\\n\\\\[ W = \\\\frac{3}{2} \\\\pi k T \\\\]\\n\\n\\\\[ X = \\\\frac{3}{2} \\\\pi k T \\\\]\\n\\n\\\\[ Y = \\\\frac{3}{2} \\\\pi k T \\\\]\\n\\n\\\\[ Z = \\\\frac{3}{2} \\\\pi k T \\\\]\\n\\n\\\\[ \\\\omega = \\\\frac{2\\\\pi}{T} \\\\]\\n\\n\\\\[ \\\\omega t = \\\\frac{2\\\\pi}{T} t \\\\]\\n\\n\\\\[ \\\\omega x = \\\\frac{2\\\\pi}{T} x \\\\]\\n\\n\\\\[ \\\\omega y = \\\\frac{2\\\\pi}{T} y \\\\]\\n\\n\\\\[ \\\\omega z = \\\\frac{2\\\\pi}{T} z \\\\]\\n\\n\\\\[ \\\\omega \\\\tau = \\\\frac{2\\\\pi}{T} \\\\tau \\\\]\\n\\n\\\\[ \\\\omega \\\\phi = \\\\frac{2\\\\pi}{T} \\\\phi \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\delta = \\\\frac{2\\\\pi}{T} \\\\delta \\\\]\\n\\n\\\\[ \\\\omega \\\\gamma = \\\\frac{2\\\\pi}{T} \\\\gamma \\\\]\\n\\n\\\\[ \\\\omega \\\\beta = \\\\frac{2\\\\pi}{T} \\\\beta \\\\]\\n\\n\\\\[ \\\\omega \\\\alpha = \\\\frac{2\\\\pi}{T} \\\\alpha \\\\]\\n\\n\\\\[ \\\\omega \\\\mu = \\\\frac{2\\\\pi}{T} \\\\mu \\\\]\\n\\n\\\\[ \\\\omega \\\\nu = \\\\frac{2\\\\pi}{T} \\\\nu \\\\]\\n\\n\\\\[ \\\\omega \\\\lambda = \\\\frac{2\\\\pi}{T} \\\\lambda \\\\]\\n\\n\\\\[ \\\\omega \\\\kappa = \\\\frac{2\\\\pi}{T} \\\\kappa \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T} \\\\theta \\\\]\\n\\n\\\\[ \\\\omega \\\\varphi = \\\\frac{2\\\\pi}{T} \\\\varphi \\\\]\\n\\n\\\\[ \\\\omega \\\\psi = \\\\frac{2\\\\pi}{T} \\\\psi \\\\]\\n\\n\\\\[ \\\\omega \\\\xi = \\\\frac{2\\\\pi}{T} \\\\xi \\\\]\\n\\n\\\\[ \\\\omega \\\\zeta = \\\\frac{2\\\\pi}{T} \\\\zeta \\\\]\\n\\n\\\\[ \\\\omega \\\\eta = \\\\frac{2\\\\pi}{T} \\\\eta \\\\]\\n\\n\\\\[ \\\\omega \\\\theta = \\\\frac{2\\\\pi}{T"}
