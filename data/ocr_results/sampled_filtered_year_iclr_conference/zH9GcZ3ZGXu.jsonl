{"id": "zH9GcZ3ZGXu", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For $\\\\alpha \\\\geq 1 + \\\\theta^a, 1 - \\\\theta^b, 2$, the second term is greater and the derivative is positive. Similarly, for $\\\\alpha \\\\leq 1 + \\\\theta^a, 1 - \\\\theta^b, 2$, the first term is greater and the derivative is positive. Hence, the minima is obtained at $\\\\alpha = 1 + \\\\theta^a, 1 - \\\\theta^b, 2$. Assuming $0.5 \\\\leq \\\\theta^a, 1, \\\\theta^b, 2 \\\\leq 1$ or $0.5 \\\\leq \\\\theta^a, 1, \\\\theta^b, 2 \\\\leq 0.5$.\\n\\nNow, in order to compute $a, b$, we can look at the maximization problem again -\\n\\n$$a = \\\\arg \\\\max_i (1 - (\\\\theta^i, 1 + \\\\theta^b, 2))^2$$\\n\\n$$b = \\\\arg \\\\max_i (1 - (\\\\theta^a, 1 + \\\\theta^i, 2))^2$$\\n\\nSince $\\\\theta^i, 1, \\\\theta^i, 2$ are bounded between $[0, 1]$, the function to maximize is monotonically decreasing in $\\\\theta^i, 1, \\\\theta^i, 2$. Hence, $a = \\\\arg \\\\min_i \\\\theta^i, 1$ and $b = \\\\arg \\\\min_i \\\\theta^i, 2$. Conversely, $a = \\\\arg \\\\max_i \\\\theta^i, 2$, $b = \\\\arg \\\\max_i \\\\theta^i, 1$, and $\\\\alpha = 1 + (\\\\theta^b, 1 - \\\\theta^a, 2)^2$.\\n\\nHence, $\\\\langle w, \\\\Theta \\\\cdot 2 \\\\rangle \\\\langle w, \\\\Theta \\\\cdot 1 \\\\rangle = 1 - (\\\\theta^b, 1 - \\\\theta^a, 2)^2 + (\\\\theta^b, 1 - \\\\theta^a, 2)^2$.\\n\\nAssuming that the maximum correlation of both the features is close, FRR will lead to a solution which gives roughly equal weights to both the features.\\n\\nNote that for the case of feature replication, $\\\\theta^b, 1 = 1$ and $\\\\theta^a, 2 = 1$. Hence, $\\\\langle w, \\\\Theta \\\\cdot 2 \\\\rangle \\\\langle w, \\\\Theta \\\\cdot 1 \\\\rangle = 1$.\\n\\nB J USTIFICATION OF F EATURE R EPLICATION H YPOTHESIS (FRH)\\n\\nIn a practical scenario where features are not disentangled, our hypothesis translates to the following:\\n\\nConjecture: Simpler features of the input are represented more in the feature space of neural networks, while complex (hard-to-learn) features are sparse.\\n\\nAssumptions:\\n\\n\u2022 We consider simple features such as background to be spurious, and complex features such as shape to be robust.\\n\\n\u2022 We consider an overparameterized network that has the capacity to learn more features than what exist, resulting in feature repetition.\\n\\nJustification: We justify the conjecture by showing that all other possibilities discussed below cannot be true.\\n\\n1. Assumption: DNNs learn only Simple Features\\n\\nContradiction: Prior works (Rosenfeld et al., 2022; Kirichenko et al., 2022b) show that features learned by ERM are diverse, and last layer training on target domain is good enough to obtain robustness to spurious features. This cannot be possible if the network has learned only spurious features.\\n\\n2. Assumption: DNNs learn only Complex Features\\n\\nContradiction: The dominance of Simple features in the learning of DNNs is shown by Shah et al. (2020). Moreover, the existence of texture-bias (Geirhos et al., 2018) and background-bias (Xiao et al., 2020) have been demonstrated in prior works, which show the dominance of Simple features.\\n\\n3. Assumption: DNNs learn a uniform distribution of both Simple and Complex Features.\\n\\nContradiction: SGD converges to an SVM solution due to its implicit bias (Soudry et al., 2018). From Claim-3.1 (1), in the presence of balanced features that are correlated with the label, SVM solution gives equal weight to all features to maximize margin. This contradicts the existence of Simplicity Bias (Shah et al., 2020).\\n\\n4. Assumption: DNNs learn more Complex Features and less Simple Features.\\n\\nContradiction: Since Complex features are indeed more robust and are better correlated with the labels, the classifier would rely more on these features. This contradicts the existence of Simplicity Bias (Shah et al., 2020).\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Therefore, the only feasible option which supports the empirical observations in the prior works discussed above is that DNNs learn more Simple Features and Complex features are sparse, which justifies our conjecture.\\n\\n**C.1 Synthetic Datasets**\\n\\nWe present empirical validation to support the Feature Replication Hypothesis (FRH) on several semi-real datasets and describe them in detail below:\\n\\n1. **Coloured-MNIST-2** - In this dataset, we use images of digits superimposed on either of the two colours - red or green. The difference from Coloured-MNIST is that we consider only two colours for the background, rather than a range. We notice extreme simplicity bias in this case, with the network learning 32 colour features and 0 shape features.\\n\\n2. **Coloured-MNIST-MultiDigit** - This is similar to the Coloured-MNIST dataset described in Section-3.1, with the exception that each of the classes is now composed of two digits. More specifically, the digits \u20181\u2019 and \u20187\u2019 are mapped to Class 0 and digits \u20185\u2019 and \u20188\u2019 are mapped to Class 1. We note that \u20181\u2019 and \u20185\u2019 are chosen from the original Coloured-MNIST dataset, while the second digit (e.g. \u20187\u2019) in each class is selected to be one that is similar to the first digit (\u20181\u2019) in the same class. This dataset is constructed specifically to show that the issue of Simplicity Bias and FRH exists even when there is higher variation in the shape feature, and is reported as Coloured-MNIST-MultiDigit below. We see that while more shape features are learnt as compared to Coloured-MNIST, the network still relies more on colour to make its decisions.\\n\\n3. **Digit-Coloured-MNIST** - This is similar to the Coloured-MNIST dataset described in Section-3.1, with the exception that the digit is coloured rather than the background. This dataset is constructed specifically to show that the issue of SB and FRH exist even when the region that is coloured, which is the extent to which simple features exist in the image is much lesser, and is reported as Digit-Coloured-MNIST below. Although this dataset also demonstrates the presence of SB, we note that the average correlation of features with shape is higher when compared to the above datasets.\\n\\n**Table 4: Features learnt by an ERM trained model on synthetic datasets.**\\n\\n| Dataset                      | Number of Features | Average Correlation with Input | Correlation with Output | Algorithm           | Accuracy OOD |\\n|------------------------------|-------------------|--------------------------------|------------------------|---------------------|--------------|\\n| Coloured-MNIST               | 26                | 0.76                           | 0.26                   | 0.81                | 0.61         | 99.9       | 59.1      |\\n| Two Coloured-MNIST           | 32                | 0                              | -                      | -                   | 0.82         | 99.9       | 49.5      |\\n| Coloured-MNIST-MultiDigit    | 17                | 0.59                           | 0.32                   | 0.76                | 0.64         | 99.3       | 64.2      |\\n| Digit-Coloured-MNIST         | 26                | 0.76                           | 0.36                   | 0.79                | 0.45         | 99.9       | 62.1      |\\n\\n**C.2 Real-World Example**\\n\\nWe attempt to demonstrate feature replication in a model trained with ERM on the Real domain of OfficeHome. We train a ResNet-50 on this domain, and perform PCA on the features learnt by this network. The network learns 2048 features per example, and we compute the $2048 \\\\times 2048$ sized covariance matrix of the features over samples from a test domain (Clipart). We then compute the eigenvalues of this matrix, and find that 500 principal components can explain about 97.5% of the variance, i.e. the matrix is extremely low rank, as shown in Fig.3. This points to the fact that a lot of the learnt features are linearly dependent and highly correlated with each other. This trend is similar to what we observed on Coloured-MNIST, where a large number of features were highly correlated with the colour, and in turn with each other (Fig. 5 of the appendix).\\n\\nWe note that in all the additional datasets considered, simpler features are represented more in the network while complex (hard-to-learn) features are sparse. This empirically justifies our hypothesis in Section-3.\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Distribution of eigen values of covariance of learnt features: A small fraction of principal components can explain most of the variance in features, indicating that features are highly correlated with each other.\\n\\n**D.1** **COLOURED MNIST**\\n\\nIn order to empirically demonstrate feature replication, we use a binarized version of the coloured MNIST dataset (Gulrajani & Lopez-Paz, 2020). To construct this dataset, we firstly assign two digits of the MNIST dataset, namely \u201c1\u201d and \u201c5\u201d, to classes 0 and 1 respectively. For the in-domain training distribution, we associate colours in the range \\\\( R_0 = [(115, 0, 0) - (256, 141, 0)] \\\\) (i.e. red) to label 0 (i.e. the digit \u201c1\u201d) and the range \\\\( R_1 = [(0, 115, 0) - (141, 256, 0)] \\\\) (i.e. green) to the label 1 (i.e. the digit \u201c5\u201d), where colors are represented in the RGB space. To summarize, while training the network, we super-impose images of \u201c1\u201d onto colours of range \\\\( R_0 \\\\), and images of \u201c5\u201d onto colours of range \\\\( R_1 \\\\). It is to be noted that the choice of colour ranges as defined above introduces an overlapping range between \\\\([115, 115, 0] - [141, 141, 0] \\\\) where images are associated with labels 0 and 1 with equal probability. This overlap reduces the correlation of colour features with labels, while shape features have a correlation of 1 with the labels. In Figure 4, we show examples of images from the train and test distributions of this dataset. In Figure 5, we pictorially depict the correlations between the 32 features learnt by the network. We can see a block structure emerging, indicating that there is a high amount of feature replication.\\n\\n**D.2** **TOY DATASET**\\n\\nIn line with the theoretical formulation described in Section-3.3, we further empirically validate the brittleness of SVM models and the highlight the effectiveness of the proposed **Feature Reconstruction**\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We consider a linearly separable toy distribution consisting of two factors of variation as shown in Figure 1. We define the means of the two classes at (1,1) and (-1,-1) and construct 500 data points in each class by adding noise sampled from $\\\\text{Unif}[-0.5,0.5]$ independently along each dimension to the respective means. We sample an Out-of-Distribution (OOD) test set from a Uniform distribution with means centered at (1,0) and (-1,0) respectively, and similar noise along each dimension as the train set. Therefore, while the train distribution can be classified by considering either the features aligned with the X-coordinate or the Y-coordinate, the test set performance crucially depends on the variation along X-coordinate alone.\\n\\nWe consider feature replication along the y-axis, and hence construct this OOD dataset to verify the extent to which the other feature is considered for classification. To select the best hyperparameter for both SVM and FRR, we consider the presence of a validation set whose distribution is similar to the test distribution. As shown in Figure 1, we observe that the SVM model starts relying more on the replicated features alone in case of feature replication, compromising its performance on the OOD data. The proposed regularizer on the other hand, gives equal importance to both features even in the presence of feature replication, resulting in improved OOD generalization.\\n\\nAlgorithm\\n\\nOur training procedure is detailed in Alg 1.\\n\\nDetails on the OOD Generalization Setting Considered\\n\\nThe problem of improving robustness to distribution shifts has been studied in several settings, where, in addition to labeled source domain data, varying levels of access to the target domain data is assumed. Some of the well-researched settings include - Unsupervised Domain Adaptation (Pan & Yang, 2009; Ganin et al., 2016), and Domain Generalization, where typically data from several source distributions is assumed to be available, and the target domain in unseen during training (Blanchard et al., 2011; Li et al., 2018a; Gulrajani & Lopez-Paz, 2020). In the latter case, it is assumed that all training data samples are annotated with domain labels as well, so that training algorithms can explicitly impose invariance to attributes.\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FEATURE RECONSTRUCTION FROM OUTPUTS CAN MITIGATE SIMPLICITY BIAS IN NEURAL NETWORKS\\nSravanti Addepalli\u2020\u22c4\u2217 Anshul Nasery\u2020\u2217 Praneeth Netrapalli\u2020 Venkatesh Babu R.\u22c4 Prateek Jain\u2020\\n\u2020 Google Research India \u22c4 Indian Institute of Science, Bangalore\\n\\nABSTRACT\\nDeep Neural Networks are known to be brittle to even minor distribution shifts compared to the training distribution. While one line of work has demonstrated that Simplicity Bias (SB) of DNNs \u2013 bias towards learning only the simplest features \u2013 is a key reason for this brittleness, another recent line of work has surprisingly found that diverse/complex features are indeed learned by the backbone, and their brittleness is due to the linear classification head relying primarily on the simplest features. To bridge the gap between these two lines of work, we first hypothesize and verify that while SB may not altogether preclude learning complex features, it amplifies simpler features over complex ones. Namely, simple features are replicated several times in the learned representations while complex features might not be replicated. This phenomenon, we term Feature Replication Hypothesis, coupled with the Implicit Bias of SGD to converge to maximum margin solutions in the feature space, leads the models to rely mostly on the simple features for classification. To mitigate this bias, we propose Feature Reconstruction Regularizer (FRR) to ensure that the learned features can be reconstructed back from the logits. The use of FRR in linear layer training (FRR-L) encourages the use of more diverse features for classification. We further propose to finetune the full network by freezing the weights of the linear layer trained using FRR-L, to refine the learned features, making them more suitable for classification. Using this simple solution, we demonstrate up to 15% gains in OOD accuracy on the recently introduced semi-synthetic datasets with extreme distribution shifts. Moreover, we demonstrate noteworthy gains over existing SOTA methods on the standard OOD benchmark DomainBed as well.\\n\\n1 INTRODUCTION\\nDespite the remarkable success of Deep Neural Networks (DNNs) in various fields, they are known to be brittle against even minor shifts in the data distribution during inference, which are not uncommon in a real world setting (Quinonero-Candela et al., 2008; Torralba & Efros, 2011). For example, a self-driving car that works well in normal weather may perform poorly when it is snowing, leading to disastrous outcomes. The need for improving the robustness of such systems against distribution shifts has sparked interest in the area of Out-Of-Distribution or OOD generalization (Hendrycks & Dietterich, 2019; Gulrajani & Lopez-Paz, 2020).\\n\\nIn this work, we aim to tackle the problem of OOD generalization of Neural Networks in a covariate-shift (Shimodaira, 2000) based classification setting, by addressing the fundamental cause of their brittleness, rather than by explicitly enforcing invariances in the network using domain labels or data augmentations. More specifically, we aim to mitigate the issue of Simplicity Bias, which is the tendency of Stochastic Gradient Descent (SGD) based solutions to overly rely on simple features alone, rather than on a diverse set of features (Arpit et al., 2017; Valle-Perez et al., 2018). While this behavior was earlier used to explain the remarkable generalization of Deep Networks, recent works suggest that this is indeed a key reason behind their brittleness to domain shifts (Shah et al., 2020).\\n\\n\u2217 Equal Contribution. Correspondence to {sravantia, anshulnasery}@google.com\\n\\n1\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The extent of Simplicity Bias seen in models is a result of two important factors - diversity of features learned by the feature extractor, and the extent to which these diverse features are used for the task at hand, such as classification. Recent works suggest that generalization to distribution shifts can be improved by retraining the last layer alone, indicating that the features learned may already be good enough for the same (Rosenfeld et al., 2022; Kirichenko et al., 2022b). Does this imply that brittleness of models can be attributed to the learning of the classification head alone? If this is the case, why does SGD fail to utilize these diverse features despite its Implicit Bias to converge to a maximum margin solution in a linearly separable case (Soudry et al., 2018)?\\n\\nTo answer these questions, we firstly hypothesize and empirically verify that Simplicity Bias leads to the learning of simple features over and over again, as compared to other, more complex features. For example, among the 512 penultimate layer features of a ResNet, 462 of them might capture a simple feature such as color, while the remaining 50 might capture a more complex feature such as shape \u2013 we refer to this as (Simple) Feature Replication Hypothesis. Assuming feature replication hypothesis, we further show theoretically and empirically that a maximum margin classifier in the replicated feature space would give much higher importance to the replicated feature when compared to others, highlighting why the linear layer relies more on simpler features for classification.\\n\\nTo mitigate this, we propose a novel regularizer termed Feature Reconstruction Regularizer (FRR), to enforce that the features learned by the network can be reconstructed back from the logit or pre-softmax layer used for the classification task. As shown in Fig. 2, we firstly propose to train the linear classifier alone by freezing the weights of the feature extractor. This formulation enables the learning of an Invertible Mapping in the output layer, specifically for the domain of features seen during training. This further allows the logit layer to act as an information bottleneck, encouraging all the factors of variation in the features to be utilized for the classification task, thereby improving the diversity of features used. We theoretically show that adding this constraint while finetuning the linear layer can learn a max-margin classifier in the original input space, disregarding feature replication. Consequently, the learnt linear classifier also gives more importance to non-replicated complex features while making predictions. We further explore the possibility of improving the quality of features learned by the feature extractor, by using FRR for finetuning the backbone as well. In order to do this, we freeze the linear classification head, and further finetune the backbone with FRR. We find that this encourages the network to indeed learn better quality features that are more relevant for classification.\\n\\nWe list the key contributions of this work below:\\n\\n\u2022 Key Observation: We provide a crisp hypothesis of \u201cfeature replication\u201d to explain the brittleness of ERM trained neural networks to OOD data (Sec 3.1). Using this, we further provide theoretical and empirical evidence to justify the existence of Simplicity Bias in maximum margin classifiers.\\n\\n\u2022 Novel Algorithm based on the Observation: Based on this, we introduce a novel FRR regularizer to safeguard against the feature replication phenomenon (Sec 3.2). We also provide theoretical support for FRR in an intuitive data distribution setting. Furthermore, we introduce a simple FRR-L method to only regularize the linear head with FRR, and then introduce FRR-FLFT training regimen to train the feature extractor for improved OOD robustness (Sec 4).\\n\\n\u2022 Empirical validation of the hypothesis and the proposed algorithm: We demonstrate the effectiveness of FRR-FLFT and FRR-L by conducting extensive experiments on semi-real datasets (Table 2) constructed to study OOD brittleness, as well as on standard OOD generalization benchmarks, where FRR-FLFT can provide up to 3% gains over SOTA methods for OOD generalization (Table 3).\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nBiases in trained networks. Kirichenko et al. (2022a) show that reweighting train set examples and retraining the last layer of a pre-trained deep network can alleviate spurious correlations, provided one can access a balanced dataset. In contrast to these methods, our method can work simply on the training set data, and produce a single classifier which is debiased. Huang et al. (2020) propose to mute the features with highest gradients, and use only the other features to make a prediction. While this method suppresses the maximally used features, it does not encourage the learning of hard-to-learn features, which is directly realized using our loss formulation. Kumar et al. (2022) suggest that finetuning the final linear layer first before finetuning the entire network can make it more robust to OOD shifts, and we utilize this insight in the FRR-FLFT phase of our method. A complementary approach to this problem is to learn features that are more diverse (Zhang et al., 2022; Wang et al., 2019). We note that applying our proposed method on top of such techniques would encourage the classifier to use the diverse features effectively, and this can further benefit the performance.\\n\\nDomain Generalization and OOD robustness:\\nThe performance of neural networks is known to drop when there is a mismatch in the train and test distributions (Hendrycks & Dietterich, 2019), and methods to mitigate this have been gaining a lot of attention in recent years. The problem has been studied under various assumptions on distribution shift. The commonly studied setting of domain generalization (Gulrajani & Lopez-Paz, 2020; Li et al., 2018a) assumes that the train distribution consists of a mixture of distinct distributions (called domains), with each train sample having a domain label associated with it. The stronger setting of aggregate domain generalization (Thomas et al., 2021; Matsuura & Harada, 2020) assumes training data to be drawn from a mixture of distributions, but does not assume the availability of domain labels. Finally, OOD robustness (Hendrycks & Dietterich, 2019; Koh et al., 2021) drops all of these assumptions. Most works tackling the domain generalization problem attempt to train a model whose predictions are invariant to the domain label (Li et al., 2018a; Arjovsky et al., 2019), or try to align the features of the model for examples from different domains (Shi et al., 2021; Shankar et al., 2018). However, since we aim to tackle the stronger setting of OOD generalization, we do not use domain labels. Tackling the OOD robustness problem, Thomas et al. (2021) and Matsuura & Harada (2020) first cluster training examples into \u201cpseudo-domains\u201d, after which standard domain generalization techniques are used. Another recent line of works propose using model averaging (Cha et al., 2021; Li et al., 2022) and/or ensembling (Arpit et al., 2021) for better OOD generalization. These techniques are complementary to our contribution, and we demonstrate how they can benefit each other in our empirical evaluation.\\n\\n3. Feature Replication Hypothesis\\n\\nPrior works have shown that neural networks trained with SGD exhibit simplicity bias (SB), even when initialized with pre-trained models that can capture complex features. Our Feature Replication Hypothesis \u2013 FRH \u2013 states that: SB is observed because the simpler features of the input are replicated multiple times in the feature space of neural networks. When trained using SGD, the final linear layer then learns the max margin classifier on these replicated features, which leads to over-reliance on simpler features in the input. Hence, the outputs of the network are brittle to distribution shifts that change such replicated features. In this section, we provide empirical and theoretical evidence for FRH, and propose a new regularizer \u2013 FRR \u2013 to mitigate this effect.\\n\\nWe first introduce some useful notations. Let \\\\( f_{\\\\theta}(x) : \\\\mathbb{R}^d \\\\rightarrow \\\\mathbb{R}^m \\\\) be the feature extractor of a neural network parameterized by weights \\\\( \\\\theta \\\\), and \\\\( W \\\\in \\\\mathbb{R}^{m \\\\times k} \\\\) be the weight matrix of the linear classifier. For input \\\\( x \\\\in \\\\mathbb{R}^d \\\\), the output of the network is \\\\( W^T f_{\\\\theta}(x) \\\\in \\\\mathbb{R}^k \\\\).\\n\\n3.1 Empirical Validation of Feature Replication Hypothesis (FRH)\\n\\nColoured MNIST dataset. To empirically demonstrate feature replication, we use a binarized version of the coloured MNIST dataset (Gulrajani & Lopez-Paz, 2020). We construct this dataset by first assigning two digits of the MNIST dataset, namely \u201c1\u201d and \u201c5\u201d, to classes 0 and 1 respectively. While training the network, we super-impose images of \u201c1\u201d onto colours of range \\\\( R_0 = [(115, 0, 0) - (256, 140, 0)] \\\\) (i.e. red), and images of \u201c5\u201d onto colours of range \\\\( R_1 = [(0, 115, 0) - (140, 256, 0)] \\\\) (i.e. green). The dataset is constructed such that the simple feature, namely colour, is weakly correlated with the labels, while the complex shape features are strongly correlated with labels. See Appendix D.1 for more details about the dataset.\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Features replication in Coloured MNIST\\n\\nWe observe that ERM learns more colour features than shape features, and the prediction is less correlated with the shape features. Adding FRR makes the network depend more on shape and less on colour, leading to better OOD performance.\\n\\n| Algorithm | Colour | Shape | Colour | Shape | Colour | Shape | ID | Accuracy | OOD Accuracy |\\n|-----------|--------|-------|--------|-------|--------|-------|----|----------|--------------|\\n| ERM       | 26     | 4     | 0.76   | 0.26  | 0.81   | 0.61  |    | 99.9%    | 59.1%        |\\n| ERM+FRR-L | 26     | 4     | 0.76   | 0.26  | 0.71   | 0.65  |    | 99.6%    | 64.9%        |\\n\\nTraining setup: We train a model on this dataset, and test it on images which do not have any correlation between the label and the colour, i.e. images where the digits \u201c1\u201d and \u201c5\u201d are superimposed on randomly coloured backgrounds. We construct this test distribution to see how well different algorithms learn simple (i.e. colour) and complex (i.e. shape) features, since an algorithm which depends only on the spurious colour features would not have good performance on the test domain.\\n\\nWe train a four layered CNN on this data. If a feature in the penultimate layer \\\\( f_\\\\theta(x) \\\\) has more than 90% correlation with the color or shape of the input, then we call it as a color feature or a shape feature, respectively. We also compute the correlation of these features with the output of the network \\\\( W^T f_\\\\theta(x) \\\\) over inputs from the test domain. This gives us information of the learnt features, and their contributions to the final prediction of the network. Note that the feature dimension is \\\\( m = 32 \\\\), and the output dimension is \\\\( k = 1 \\\\).\\n\\nObservations: In Table 1, we report the number of colour features, shape features, and the average correlation of each of these with the final prediction. We observe that the ERM trained model learns both shape and colour features, but the number of learnt colour features (26) is much higher than the number of shape features (4), despite their weaker correlation with labels, thus validating our Feature Replication Hypothesis. We also visualize the inter-feature correlation of the learnt features in Fig 5, which shows blocks of highly correlated features, further validating our hypothesis. We note that correlation of the output with the shape features is lower, leading to OOD accuracy of 59%.\\n\\n3.2 Feature Reconstruction Regularizer (FRR)\\n\\nTo alleviate simple feature replication issue, we propose Feature Reconstruction Regularizer (FRR) to enforce that the learned features can be reconstructed from the output logits. We propose to retrain the final linear layer using this regularizer to allow the model to utilize diverse features to compute the final output. We implement this by introducing another neural network with the objective of reconstructing the features of the network from the output logits, i.e. features \\\\( f_\\\\theta(x) \\\\) should be recoverable from the predictions of the network through a transform \\\\( T_\\\\phi \\\\). That is, FRR is given by:\\n\\n\\\\[\\nL_{FRR}(x, \\\\theta, W, \\\\phi) = \\\\| f_\\\\theta(x) - T_\\\\phi(W^T f_\\\\theta(x)) \\\\|_p\\n\\\\]\\n\\nwhere \\\\( \\\\| \\\\cdot \\\\|_p \\\\) denotes the \\\\( \\\\ell_p \\\\) norm. We set this norm to be \\\\( \\\\ell_\\\\infty \\\\) or \\\\( \\\\ell_1 \\\\) in our experiments. In the simplest case, \\\\( T_\\\\phi(y) = \\\\phi y \\\\), where \\\\( \\\\phi \\\\in \\\\mathbb{R}^{m \\\\times k} \\\\). Note that in order to find the appropriate \\\\( \\\\phi \\\\), we jointly optimize \\\\( W \\\\) and \\\\( \\\\phi \\\\) using gradient descent based optimizers. We also experiment with \\\\( \\\\phi \\\\) being a more complex neural network.\\n\\nWe empirically validate FRR on Coloured MNIST, where using FRR with the linear layer leads to lower correlation with Colour compared to standard ERM (Table 1). Consequently, OOD accuracy improves by 5% over ERM.\\n\\n3.3 FRR: Theoretical Analysis\\n\\nWe now present a simple and intuitive data distribution with feature replication that highlights the OOD brittleness of standard ERM, and also demonstrates FRR can be significantly more robust.\\n\\nData Distribution: Consider a linearly separable distribution consisting of two factors of variation as shown in Figure 1. That is, consider the following distribution \\\\((x, y) \\\\sim D\\\\), where,\\n\\n\\\\[\\ny = \\\\pm 1 \\\\quad \\\\text{with probability} \\\\quad 0.5, \\\\quad x = [y, y] + [n_1, n_2] \\\\in \\\\mathbb{R}^2, \\\\quad n_i \\\\sim \\\\text{Unif}[-0.5, 0.5], i \\\\in [2].\\n\\\\]\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: We demonstrate the brittleness of SVM (a, b) and effectiveness of FRR (c, d) based classifiers on a toy dataset comprising of 2 factors of variation, sampled from a uniform distribution. We consider $d = 0$ or 5 feature replications (Rep) along the y-axis. FRR converges to a maximum margin solution in the non-replicated feature space, resulting in improved OOD robustness (d)\\n\\nAlso consider a feature extractor $f_{\\\\theta}(\\\\cdot)$ which captures feature replication in the first feature, i.e. for every data point $(x,y)$, the new, feature replicated data point will be $(\\\\tilde{x},y)$, where, $f_{\\\\theta}(x) = \\\\tilde{x} = [x_1, \\\\ldots, x_1, x_2] \\\\in \\\\mathbb{R}^{d+1}$, i.e., $x_1$ is repeated $d$ times. The joint distribution of features and labels is denoted by $\\\\tilde{D}$. Finally, we define the $l_2$ max margin classifier over a distribution $D$ as $w_{\\\\text{MM}} := \\\\arg \\\\min w \\\\frac{1}{2} \\\\|w\\\\|_2^2$ subject to $y \\\\cdot \\\\langle w, x \\\\rangle \\\\geq 1 \\\\forall (x,y) \\\\in \\\\text{Supp}(D)$. Then we have the following results:\\n\\nClaim 3.1 (Brittleness due to Feature Replication).\\n\\nConsider the data distribution given in Equation 2, 3. Then, the following holds: (1.) The max-margin classifier $w_{\\\\text{MM}}$ over $D$ is given by $w_{\\\\text{MM}} = [1, 1]$, and (2.) The max-margin classifier $\\\\tilde{w}_{\\\\text{MM}}$ over $\\\\tilde{D}$ is given by $\\\\tilde{w}_{\\\\text{MM}} = [2d+1, \\\\ldots, 2d+1] \\\\in \\\\mathbb{R}^{d+1}$.\\n\\nThe above claim implies that when there are replicated features to the input of the linear layer, the max-margin classifier would give much more importance to the feature that is replicated. Hence, even a minor change in this replicated feature in the input space would be amplified in the output of the classifier. This is especially concerning in light of the observations in Table 1, which validate the Feature Replication Hypothesis in Coloured MNIST.\\n\\nProposition 3.2 (Robustness of FRR).\\n\\nDenote the average feature reconstruction loss $L_{\\\\text{FRR}}(\\\\tilde{w},\\\\tilde{\\\\phi}) := \\\\max_{1 \\\\leq i \\\\leq d+1} \\\\mathbb{E}_{(\\\\tilde{x},y) \\\\sim \\\\tilde{D}}[(\\\\langle \\\\tilde{w}, \\\\tilde{x} \\\\rangle - \\\\tilde{x}_i)^2]$ and consider any $(\\\\tilde{w}^\\\\ast,\\\\tilde{\\\\phi}^\\\\ast)$ satisfying:\\n\\n$$(\\\\tilde{w}^\\\\ast,\\\\tilde{\\\\phi}^\\\\ast) \\\\in \\\\arg \\\\min_{(\\\\tilde{w},\\\\tilde{\\\\phi})} L_{\\\\text{FRR}}(\\\\tilde{w},\\\\tilde{\\\\phi}) \\\\text{ subject to } y \\\\cdot \\\\langle \\\\tilde{w}, \\\\tilde{x} \\\\rangle \\\\geq 0 \\\\forall (\\\\tilde{x},y) \\\\in \\\\text{Supp}(\\\\tilde{D})$$\\n\\nWe have that:\\n\\n$\\\\tilde{w}_1 + \\\\cdots + \\\\tilde{w}_d = \\\\tilde{w}_{d+1}$. Consequently, we have $\\\\langle \\\\tilde{w}^\\\\ast, \\\\tilde{x} \\\\rangle \\\\propto \\\\langle w_{\\\\text{MM}}, x \\\\rangle$ for all $x \\\\in \\\\mathbb{R}^2$.\\n\\nPractically, we can implement the above as $\\\\ell_2,\\\\infty$ over a batch. Above result shows that the feature reconstruction regularizer will produce a linear classifier that gives equal weights to the replicated and non-replicated features. This is equivalent to a maximum margin classifier in the non-replicated feature space, thereby resulting in enhanced robustness to distribution shifts. Same is reflected in Figure 1 (c), (d) which show impact of FRR on the trained boundary in the non-replicated feature space. We defer the proofs of the above to Appendix A. We also provide a more general result by assuming correlated feature representations in Appendix A.\\n\\n4. TRAINING PROCEDURE\\n\\nPretraining: In order to learn features which are relevant to the train distribution, we first pretrain our model using standard ERM with the cross-entropy loss $L_{\\\\text{std}}(W,\\\\theta,(x,y))$.\\n\\nFRR-L: Since ERM training is known to learn several rich and diverse features, we freeze the backbone parameters $\\\\theta$, and retrain the final layer $W$ as the following-\\n\\n$$(W_{\\\\text{FRR}},\\\\phi_{\\\\text{FRR}}) = \\\\min_{W,\\\\phi} L_{\\\\text{std}}(W,\\\\theta,(x,y)) + \\\\lambda L_{\\\\text{FRR}}(x,\\\\theta,W,\\\\phi)$$\\n\\n(4)\\n\\nwhere $\\\\lambda$ is a hyperparameter weighing the two losses. We train $W$ and $\\\\phi$ jointly. We refer to this step as FRR-L, i.e. Feature Reconstruction Regularizer - Linear, since we only train the linear layer.\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Figure 2:** Our training procedure: Dotted fill indicates that the parameters are trainable.\\n\\n**FRR-FLFT:** Following the suggestions of Kumar et al. (2022), we follow up the linear layer training with the finetuning of the feature extractor $\\\\theta$ with a weighted combination of the cross-entropy loss and FRR, weighted by a hyper-parameter $\\\\lambda_{FLFT}$. In this step we freeze the weights of the linear layer to improve the stability of training. We do this since naively using this constraint during network training could amplify the Simplicity Bias in networks in degenerate cases. For example, the backbone could learn to output a single replicated simple feature which is predictive enough on the training data. Reconstructing such a feature from logits would also be easy, but such a network might not generalize well. Formally, the optimization problem for this step is:\\n\\n$$\\\\theta_{FLFT} = \\\\min_{\\\\theta} L_{std}(W_{FRR}, \\\\theta, (x,y)) + \\\\lambda_{FLFT} L_{FRR}(x, \\\\theta, W_{FRR}, \\\\phi_{FRR})$$\\n\\nWe view this step as \\\"sharpening\\\" the features for more accurate predictions. Freezing the linear head makes sure that the features do not collapse to a degenerate solution. Our training algorithm is summarized in Algorithm-1 and the training pipeline is illustrated in Figure 2.\\n\\n### 5. EXPERIMENTAL RESULTS\\n\\n#### 5.1 UNDERSTANDING HOW FRR MITIGATES SIMPLICITY BIAS\\n\\nTo empirically illustrate the extent of Simplicity Bias in Neural Networks, Shah et al. (2020) introduced several synthetic and semi-synthetic datasets, where some features are explicitly simple, requiring a simpler decision boundary for prediction; while others are complex. In this section, we demonstrate the effectiveness of the proposed Feature Reconstruction Regularizer towards mitigating Simplicity Bias, by evaluating the same on a 10-class variant of the proposed semi-synthetic MNIST-CIFAR dataset, as discussed in the following section.\\n\\n#### 5.1.1 MNIST-CIFAR-10 DATASET\\n\\nWe extend the simple binary MNIST-CIFAR dataset proposed by Shah et al. (2020) to a 10-class dataset, in order to evaluate the impact of the proposed Feature Reconstruction Regularizer in a more complex scenario when compared to the binary Colored-MNIST dataset presented in Section-3. We refer to this dataset as MNIST-CIFAR-10. The higher complexity of this dataset allows for a more reliable evaluation of various settings such as linear probing, full network finetuning and fixed-linear finetuning, with better granularity of results.\\n\\nTo construct this dataset, we first define correspondences between the classes of CIFAR-10 and MNIST. Each image from class $k$ of MNIST is mapped with an image from class $k$ of CIFAR-10, with the label being set to $k$. Thus, every training data sample $(x_1, x_2, y)$ consists of $x_1$ and $x_2$, which are images from CIFAR-10 and MNIST respectively, along with their ground truth class $y$. It is to be noted that for both CIFAR-10 and MNIST, labels are always correlated with the respective images. In such a scenario, although a classifier can achieve very good performance by relying solely on the simple (MNIST) features, the goal of Out-Of-Distribution (OOD) robustness requires it to rely on the complex (CIFAR-10) features as well. This dataset represents the toughest setting of OOD generalization, where there is no differentiation between important features and spurious...\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1: Our training algorithm\\n\\nData:\\nTraining data \\\\( D = \\\\{ (x_i, y_i) : i \\\\in [n] \\\\} \\\\), model \\\\( (\\\\theta, W) \\\\), feature reconstruction model \\\\( \\\\phi \\\\),\\n\\n\\\\[ \\\\lambda_{FRR}, \\\\lambda_{FT} \\\\]\\n\\n\\\\[ \\\\theta_{std}, W_{std} \\\\leftarrow \\\\text{Adam} \\\\left( \\\\min_{\\\\theta, W} \\\\sum_{i} L_{std}(\\\\theta, W, (x_i, y_i)) \\\\right). \\\\]\\n\\n/* Standard training of model parameters \\\\( \\\\theta \\\\) and \\\\( W \\\\). */\\n\\nFreeze \\\\( \\\\theta \\\\) to be \\\\( \\\\theta_{std} \\\\)\\n\\n/* Initializing model for training with FRR. */\\n\\n\\\\[ W_{FRR}, \\\\phi_{FRR} \\\\leftarrow \\\\text{Adam} \\\\left( \\\\min_{W, \\\\phi} \\\\sum_{i} L_{std}(\\\\theta_{std}, W, (x_i, y_i)) + \\\\lambda_{FRR} L_{FRR}(x_i, \\\\theta_{std}, W, \\\\phi_{FRR}) \\\\right). \\\\]\\n\\n/* FRR-L: Training \\\\( W, \\\\phi \\\\) with FRR defined in eqn. 4 */\\n\\n\\\\[ \\\\theta_{FLFT} \\\\leftarrow \\\\text{Adam} \\\\left( \\\\min_{\\\\theta} \\\\sum_{i} L_{std}(\\\\theta, W_{FRR}, (x_i, y_i)) + \\\\lambda_{FLFT} L_{FRR}(x_i, \\\\theta_{FRR}, W_{FRR}, \\\\phi_{FRR}) \\\\right). \\\\]\\n\\n/* FRR-FLFT: Finetuning \\\\( \\\\theta \\\\) with FRR according to eqn. 5 */\\n\\nResult:\\nTrained model \\\\( (\\\\theta_{FLFT}, W_{FRR}) \\\\).\\n\\nthat cause a distribution shift in input data without change in their label distribution (Muandet et al., 2013; Ganin et al., 2016; Li et al., 2018b; Arjovsky et al., 2019; Shi et al., 2021).\\n\\nA more challenging case is when the training data belongs to several distributions that may not even be sufficiently discernable to have explicit domain annotations, or may contain multidimensional distribution shifts, such as weather, time of the day and geographical location, that cannot be easily annotated or clustered. We investigate this crucial setting which has been relatively less researched, and refer to it as Aggregated Domain Generalization, as introduced by Thomas et al. (2021). We note that this setting is different from the case of training on data from a single domain such as ImageNet, and evaluating on distribution shifts (Hendrycks & Dietterich, 2019), due to the availability of an aggregate of source domains during training, which can enable the effective use of in-domain validation set for hyperparameter selection.\\n\\nWhile there have been several approaches to improve the performance of models in the setting of Domain Generalization, Gulrajani & Lopez-Paz (2020) show that when evaluated fairly, that is, without assuming access to the test domain data even for selecting the best set of hyperparameters, none of the approaches perform consistently better than standard training using Empirical Risk Minimization (ERM). Furthermore, we consider the setting of Aggregated Domain Generalization, which is more challenging due to the absence of domain labels during both training and validation.\\n\\n**Experimental Details on Domain Bed**\\n\\nWe test our approach on the DomainBed benchmark (Gulrajani & Lopez-Paz, 2020) comprising of five different datasets, each of which have \\\\( k \\\\) domains. For each dataset, we train a model on \\\\( k-1 \\\\) domains, and test it on the left out domain. The average out-of-domain performance across the \\\\( k \\\\) held-out domains is then reported. In this section we describe the hyper-parameter selection strategy and the ranges considered for our approach. In line with the DomainBed testbench, we use ImageNet pretrained ResNet-50 models for all algorithms. We use random search to select hyperparameters for our algorithm, and use the suggested hyperparameters for the other baselines. We train for 3000 (5000 for DomainNet) steps in the FRR-L phase, and 5000 (10000 for DomainNet) steps in the FRR-FLFT phase. The batch size is fixed to 32, and SW AD hyper-parameters are the same as those used by Cha et al. (2021). We use the in-domain accuracy protocol from Gulrajani & Lopez-Paz (2020) to select hyper-parameters for each domain of each dataset, and search over 8 random combinations of hyper-parameters for each. The range of the hyperparameters is shown in Table 5. Note that we experiment with two implementations of \\\\( \\\\ell_\\\\infty \\\\) norm: \\\\( \\\\ell_1, \\\\ell_\\\\infty \\\\), where we first compute the \\\\( \\\\ell_\\\\infty \\\\) of feature reconstruction for each example in a batch and then average it across the batch, and \\\\( \\\\ell_\\\\infty, \\\\ell_1 \\\\) where we compute the average \\\\( \\\\ell_1 \\\\) reconstruction norm of each feature across the batch, and then apply \\\\( \\\\ell_\\\\infty \\\\) norm on this \\\\( m \\\\) dimensional vector. All our experiments were done on single V100 GPUs.\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Ranges of hyperparameters considered for DomainBed\\n\\n| Hparam | Range          |\\n|--------|----------------|\\n| Learning Rate | loguniform \\\\((10^{-5}, 10^{-1})\\\\) |\\n| \\\\(\\\\lambda_{FRR}\\\\) | loguniform \\\\((10^{-6}, 10^{-0})\\\\) |\\n| \\\\(\\\\lambda_{FT}\\\\) | loguniform \\\\((10^{-6}, 10^{-0})\\\\) |\\n| Norm  | \\\\{\\\\(\\\\ell_1\\\\), \\\\(\\\\ell_1\\\\), \\\\(\\\\ell_\\\\infty\\\\), \\\\(\\\\ell_\\\\infty\\\\), \\\\(\\\\ell_1\\\\)\\\\} |\\n\\nComparing the choices for \\\\(\\\\phi\\\\): In Table 6, we experiment with various architectures for the decoder \\\\(\\\\phi\\\\) when computing FRR according to equation 1. We consider using a two layer neural network as the decoder \\\\(\\\\phi\\\\) (FRR-LDeeper), and also consider setting \\\\(\\\\phi = W_T\\\\) (FRR-LShared), i.e. explicitly tying the weights of the decoder and the classifier layer. Overall, both these variants are worse than the default single layer, free parameterization of \\\\(\\\\phi\\\\). We believe that this happens because the latter approach enforces a much stricter constraint on \\\\(W\\\\), leading to poorer in-domain accuracy, while the former approach enforces a weaker constraint, potentially enabling reconstruction of more complex features from a smaller amount of information about them in the logits. Both these have a detrimental effect on the overall performance of the model.\\n\\nTable 6: Effect of different design choices on OOD accuracy: the rows shows different architecture choices for \\\\(\\\\phi\\\\)\\n\\n| Algorithm                | PACS | OfficeHome | Terra | Incognita | Avg. |\\n|--------------------------|------|------------|-------|-----------|------|\\n| ERM                      | 85.5 | 66.5       | 46.1  | 46.1      | 65.3 |\\n| ERM+FRR-LShared          | 85.2 | 68.2       | 49.4  | 49.4      | 67.6 |\\n| ERM+FRR-LDeeper          | 84.6 | 65.6       | 52.5  | 52.5      | 67.6 |\\n\\nSensitivity Analysis: We vary \\\\(\\\\lambda_{FRR}\\\\) and plot out the OOD performance in Fig 6. We find that the performance is stable for a wide range of the hyper-parameter on most domains.\\n\\nIPSEUDO-CODE FOR FRR\\n\\nBelow we provide the python code for FRR-L in the DomainBed framework.\\n\\n```python\\nclass ERMWithFRRL (Algorithm):\\n    def init (self, input shape, num classes, num domains, hparams):\\n        super (ERMWithFRRL, self).init (input shape, num classes, num domains, hparams)\\n        self.feature = networks.Featurizer (input shape, self.hparams)\\n        self.classifier = networks.Classifier (self.feature.output shape, num classes, self.hparams ['non linear classifier'], 20)\\n```\\n\\n\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Variation of OOD accuracy with varying $\\\\lambda_{FRR}$\\n\\n```\\nif 'exact reconstruction' in self.hparams and self.hparams['exact reconstruction'] else True,\\n\\nself.network = nn.Sequential(self.feature, self.classifier)\\nself.optimizer = torch.optim.Adam(\\n    list(self.network.parameters()) + list(self.classifierinv.parameters()),\\n    lr = self.hparams['lr'],\\n    weight_decay = self.hparams['weight_decay'],\\n)\\nself.reconstruction_wt = self.hparams['reconstruction_wt']\\nself.norm = float(self.hparams['norm'])\\n\\ndef update(self, mini_batches, unlabeled=None):\\n    all_x = torch.cat([x for x, y in mini_batches])\\n    all_y = torch.cat([y for x, y in mini_batches])\\n    pred, rec, feat = self.get_features and rec(all_x)\\n    loss = F.crossentropy(pred, all_y)\\n    reconstruction_loss = (torch.sum(torch.max(torch.abs(feat - rec), dim = 1)[:,0]) / all_x.shape[0])\\n    loss = loss + self.reconstruction_wt * reconstruction_loss\\n    self.optimizer.zero_grad()\\n    loss.backward()\\n```\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"self.optimizer.step()\\n\\nreturn {\\n    'loss': loss.item(),\\n    'reconstruction_loss': reconstruction_loss.item(),\\n}\\n\\ndef predict(self, x):\\n    return self.network(x)\\n\\ndef get_features_and_rec(self, x):\\n    features = self.network[0](x)\\n    pred = self.network[1](features)\\n    rec = self.classifier.inv(pred)\\n    return pred, rec, features\\n\\nJD\\n\\nIn this section, we show detailed results of Table 3 in the main text. The numbers for the baselines are taken from Gulrajani & Lopez-Paz (2020), Cha et al. (2021) and Arpit et al. (2021), while the results for MIRO (Cha et al., 2022) were reproduced using their code-base.\\n\\nTable 7: Out-of-domain accuracies (%) on PACS.\\n\\n| Algorithm  | C  | P  | S  | Avg |\\n|------------|----|----|----|-----|\\n| CDANN      | 84.6 | \u00b11.8 | 75.5 | \u00b10.9 | 96.8 | \u00b10.3 | 73.5 | \u00b10.6 | 82.6 |\\n| MASF       | 82.9 | 80.5 | 95.0 | 72.3 | 82.7 |\\n| DMG        | 82.6 | 78.1 | 94.5 | 78.3 | 83.4 |\\n| IRM        | 84.8 | \u00b11.3 | 76.4 | \u00b11.1 | 96.7 | \u00b10.6 | 76.1 | \u00b11.0 | 83.5 |\\n| MetaReg    | 87.2 | 79.2 | 97.6 | 70.3 | 83.6 |\\n| DANN       | 86.4 | \u00b10.8 | 77.4 | \u00b10.8 | 97.3 | \u00b10.4 | 73.5 | \u00b12.3 | 83.7 |\\n| GroupDRO   | 83.5 | \u00b10.9 | 79.1 | \u00b10.6 | 96.7 | \u00b10.3 | 78.3 | \u00b12.0 | 84.4 |\\n| MTL        | 87.5 | \u00b10.8 | 77.1 | \u00b10.5 | 96.4 | \u00b10.8 | 77.3 | \u00b11.8 | 84.6 |\\n| I-Mixup    | 86.1 | \u00b10.5 | 78.9 | \u00b10.8 | 97.6 | \u00b10.1 | 75.8 | \u00b11.8 | 84.6 |\\n| MMD        | 86.1 | \u00b11.4 | 79.4 | \u00b10.9 | 96.6 | \u00b10.2 | 76.5 | \u00b10.5 | 84.7 |\\n| VREx       | 86.0 | \u00b11.6 | 79.1 | \u00b10.6 | 96.9 | \u00b10.5 | 77.7 | \u00b11.7 | 84.9 |\\n| MLDG       | 85.5 | \u00b11.4 | 80.1 | \u00b11.7 | 97.4 | \u00b10.3 | 76.6 | \u00b11.1 | 84.9 |\\n| ARM        | 86.8 | \u00b10.6 | 76.8 | \u00b10.5 | 97.4 | \u00b10.3 | 79.3 | \u00b11.2 | 85.1 |\\n| RSC        | 85.4 | \u00b10.8 | 79.7 | \u00b11.8 | 97.6 | \u00b10.3 | 78.2 | \u00b11.2 | 85.2 |\\n| Mixstyle   | 86.8 | \u00b10.5 | 79.0 | \u00b11.4 | 96.6 | \u00b10.1 | 78.5 | \u00b12.3 | 85.2 |\\n| ER         | 87.5 | 79.3 | 98.3 | 76.3 | 85.3 |\\n| pAdaIN     | 85.8 | 81.1 | 97.2 | 77.4 | 85.4 |\\n| ERM        | 84.7 | \u00b10.4 | 80.8 | \u00b10.6 | 97.2 | \u00b10.3 | 79.3 | \u00b11.0 | 85.5 |\\n| EISNet     | 86.6 | 81.5 | 97.1 | 78.1 | 85.8 |\\n| CORAL      | 88.3 | \u00b10.2 | 80.0 | \u00b10.5 | 97.5 | \u00b10.3 | 78.8 | \u00b11.3 | 86.2 |\\n| SagNet     | 87.4 | \u00b11.0 | 80.7 | \u00b10.6 | 97.1 | \u00b10.1 | 80.0 | \u00b10.4 | 86.3 |\\n| DSON       | 87.0 | 80.6 | 96.0 | 82.9 | 86.6 |\\n| SMA        | 89.1 | \u00b10.1 | 82.6 | \u00b10.2 | 97.6 | \u00b10.0 | 80.5 | \u00b10.9 | 87.5 |\\n| MIRO       | 87.5 | 79.0 | 98.3 | 76.2 | 85.3 |\\n| SW AD      | 89.3 | \u00b10.2 | 83.4 | \u00b10.6 | 97.3 | \u00b10.3 | 82.5 | \u00b10.5 | 88.1 |\\n| SW AD+FRR  | 89.9 | \u00b10.2 | 83.9 | \u00b10.7 | 98.2 | \u00b10.3 | 84.8 | \u00b10.4 | 89.2 |\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: ID and OOD accuracy (%) by training on MNIST-CIFAR-10 in various training regimes.\\n\\n| Initialization | Layers trained | Exp | ID Training Loss | Dataset     | OOD Training Loss | Dataset     |\\n|---------------|----------------|-----|------------------|-------------|------------------|-------------|\\n| Random        | All layers     | E1  | M1: ERM (CE)     | MNIST-CIFAR-10 | 99.84 \u00b1 0.01     | 97.44 \u00b1 0.89 |\\n|               |                |     |                  | CIFAR-AvgMNIST | 51.92 \u00b1 1.52     |             |\\n| E2            | Cross-Entropy  | E2  | M1: ERM-L (CE)   | MNIST-CIFAR-10 | 88.53 \u00b1 0.15     | 9.77 \u00b1 0.11  |\\n|               |                |     |                  | CIFAR-RandMNIST| 88.52 \u00b1 0.14     |             |\\n| E3            | Cross-Entropy  | E3  | M1: ERM-L (CE)   | MNIST-CIFAR-10 | 99.68 \u00b1 0.02     | 94.84 \u00b1 1.18 |\\n|               |                |     |                  | CIFAR-RandMNIST| 10.02 \u00b1 0.13     |             |\\n| E4            | M2: ERM-L      | E4  | M1: ERM-L (CE)   | MNIST-CIFAR-10 | 99.86 \u00b1 0.01     | 97.06 \u00b1 0.05 |\\n|               |                |     |                  | CIFAR-RandMNIST| 65.14 \u00b1 0.05     | 10.15 \u00b1 0.01 |\\n| E5            | Cross-Entropy  | E5  | M1: ERM-L (CE)   | MNIST-CIFAR-10 | 99.71 \u00b1 0.00     | 94.84 \u00b1 0.04 |\\n|               |                |     |                  | CIFAR-RandMNIST| 10.33 \u00b1 0.17     |             |\\n| E6            | M3: FRR-L      | E6  | M1: ERM-L (CE)   | MNIST-CIFAR-10 | 99.87 \u00b1 0.01     | 97.03 \u00b1 0.35 |\\n|               |                |     |                  | CIFAR-RandMNIST| 61.75 \u00b1 0.33     |             |\\n| E7            | CE + Full-Rank Reg | E7 | M1: ERM-L (CE)   | MNIST-CIFAR-10 | 99.88 \u00b1 0.00     | 96.81 \u00b1 0.38 |\\n|               |                |     |                  | CIFAR-RandMNIST| 59.13 \u00b1 0.37     |             |\\n| E8            | M3: FRR-L      | E8  | M1: ERM-L (CE)   | MNIST-CIFAR-10 | 99.84 \u00b1 0.02     | 97.67 \u00b1 0.19 |\\n|               |                |     |                  | CIFAR-RandMNIST| 53.67 \u00b1 0.40     |             |\\n| E9            | Feature extractors | E9 | M1: ERM-L (CE)   | MNIST-CIFAR-10 | 99.84 \u00b1 0.02     | 97.67 \u00b1 0.18 |\\n|               |                |     |                  | CIFAR-RandMNIST| 53.33 \u00b1 0.28     |             |\\n| E10           | Feature extractors | E10 | M1: ERM-L (CE)   | MNIST-CIFAR-10 | 99.84 \u00b1 0.01     | 97.67 \u00b1 0.18 |\\n|               |                |     |                  | CIFAR-RandMNIST| 53.67 \u00b1 0.40     |             |\\n| E11           | Feature extractors | E11 | M1: ERM-L (CE)   | MNIST-CIFAR-10 | 99.84 \u00b1 0.02     | 97.67 \u00b1 0.18 |\\n|               |                |     |                  | CIFAR-RandMNIST| 53.67 \u00b1 0.40     |             |\\n| E12           | Feature extractors | E12 | M1: ERM-L (CE)   | MNIST-CIFAR-10 | 99.84 \u00b1 0.02     | 97.67 \u00b1 0.18 |\\n|               |                |     |                  | CIFAR-RandMNIST| 53.67 \u00b1 0.40     |             |\\n\\n5.2 TRAINING AND EVALUATION\\n\\nWe consider two separate ResNet-18 (He et al., 2016) feature extractors for CIFAR-10 and MNIST respectively. The outputs of the Global Average Pooling (GAP) layers in each of the feature extractors are concatenated to form a 1024 dimensional vector, which is given as input to the linear classifier. This architecture allows the computation of accuracy based on either a combination of both CIFAR-10 and MNIST features, or based on features of only one of the datasets. For example, to evaluate the performance of the classifier based on CIFAR-10 features alone, we replace the 512 dimensional MNIST feature vector of each data sample with an average feature vector computed from all images in the MNIST dataset. We refer to this as the CIFAR-AvgMNIST dataset, while the corresponding one for MNIST is refered to as the MNIST-AvgCIFAR dataset. Similar to the work by Shah et al. (2020), we define two additional datasets, CIFAR-RandMNIST and MNIST-RandCIFAR, where images from one of the datasets (MNIST and CIFAR-10 respectively) are randomly shuffled with respect to their corresponding labels. The base training (E1, E2, E3) is done for 500 epochs, and the linear layer training / finetuning (E4 - E18) is done for 20 epochs, without any augmentations.\\n\\n5.3 EXPERIMENTAL RESULTS IN VARIOUS TRAINING REGIMES\\n\\nWe present the results of training on the MNIST-CIFAR-10 dataset using different algorithms in Table 2. The mean and standard deviation across five runs have been reported for each case.\\n\\nERM Training: By training a randomly initialized model on the MNIST-CIFAR-10 dataset using the cross-entropy loss (E1), we obtain an accuracy of 99.84% on its corresponding test split. While the accuracy of this model on the MNIST-avgCIFAR dataset is high (97.44%), its performance on the CIFAR-avgMNIST dataset is poor (51.92%), indicating that the model chooses to rely more on the simpler MNIST features, rather than a combination of both CIFAR and MNIST features. While the performance on the CIFAR-avgMNIST and MNIST-avgCIFAR datasets is sufficient to understand the extent of CIFAR/MNIST features used by the classification head, it does not give a clear picture on the features learned by the two feature extractors. To understand this, we reinitialize the linear classification head randomly, and train the same using CIFAR-RandMNIST (E5) and MNIST-RandCIFAR datasets (E6) respectively. We obtain an accuracy of 65.2% on the CIFAR-avgMNIST dataset in the former case, indicating that although the CIFAR features learned can possibly achieve 13% higher accuracy (w.r.t. E1), the bias in the classification head prevents them from participating in the classification task. The MNIST-avgCIFAR accuracy of the latter case is high as expected. An upper bound on CIFAR-10 and MNIST accuracy that can be achieved with the selected architecture and training strategy (without using any augmentations) can be seen in E2 (88.53%) and E3 (99.68%) respectively.\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Training the Linear Classification Head:\\n\\nAs discussed, while ERM training (E1) learns features that can be used for better OOD performance (E5), it does not effectively leverage these features for the classification task. We firstly explore the possibility of bridging the difference in the CIFAR-avgMNIST accuracy between E1 and E5 by merely retraining the linear layer. By reinitializing and naively retraining the linear layer with Cross-entropy loss, the accuracy on CIFAR-avgMNIST improves by less than $1\\\\%$ (E4). Using the proposed Feature Reconstruction Regularizer (FRR) for training the linear layer alone, the CIFAR-avgMNIST accuracy improves by $7.21\\\\%$ as shown in E8, demonstrating the effectiveness of the proposed regularizer in mitigating Simplicity Bias.\\n\\nWe penalize the $\\\\ell_\\\\infty$ norm of difference in original features and their reconstruction in addition to the minimization of cross-entropy loss. The reconstruction based regularizer enforces the network to utilize both CIFAR and MNIST features for classification. Since this regularizer resembles an orthonormality constraint on the linear classification head, we additionally check the effectiveness of explicitly enforcing a full-rank constraint on the linear layer by minimizing the following:\\n\\n$$||WW^T - I||_F$$\\n\\n(E7). We find that this is not effective in improving the overall accuracy, possibly because it enforces a very stringent constraint on the final classification layer. Contrary to this, the proposed Feature Reconstruction Regularizer allows more flexibility by imposing this constraint only on the domain of features seen during training. This accounts for the simple feature replication as well, enabling to view the logit layer as an information bottleneck in the reconstruction.\\n\\nFinetuning (FT) and Fixed Linear Finetuning (FLFT):\\n\\nWe explore the finetuning of a given base model in two settings - firstly by finetuning all layers in the network (denoted as FT or FineTuning), and secondly, by freezing the parameters of the linear classification head and finetuning only the feature extractors, which we refer to as FLFT or Fixed Linear FineTuning. By finetuning an ERM trained base model using either of the two strategies (E9 and E10), we observe gains of less than $1\\\\%$. We observe similar gains even by finetuning the full network with FRR (E11). Contrary to this, by using FRR-FLFT even on the ERM trained network (E12), we obtain $7.29\\\\%$ improvement over the base model. This shows that, by allowing the full network to change while imposing the FRR constraint, the network can continue to rely on simple features, possibly by reducing the number of complex features learned by the feature extractor. However, by freezing the weights of the linear layer and further imposing this constraint, the network is forced to refine the CIFAR features that are already being used for prediction.\\n\\nCombining FRR-L and FRR-FLFT:\\n\\nWhile we obtain similar order of gains ($\\\\sim 7\\\\%$) using both FRR-L and FRR-FLFT individually, the former improves the diversity of features being considered by the classification head, while the latter improves the quality of the features themselves. We therefore propose a training strategy that combines both FRR-L and FRR-FLFT. Using this, we obtain gains of $16.2\\\\%$ over the ERM baseline as shown in E16, indicating that the combination of FRR-L and FRR-FLFT has a compounding effect by firstly selecting diverse features, and further refining these features to be more useful for classification. Although FRR-L followed by FRR-FT (E15) is also effective, it has about $6\\\\%$ lesser gains when compared to the proposed approach of FRR-L + FRR-FLFT. We note that following up FRR-L with ERM-FT (E13) or ERM-FLFT (E14) also refines the learned features, making them more suitable for the classification task, yielding $2.6\\\\%$ and $4.6\\\\%$ gains respectively over FRR-L.\\n\\nWe verify the quality of features learned by the feature extractors after the proposed training strategy FRR-L + FRR-FLFT by reinitializing and retraining the linear classifier on CIFAR-RandMNIST (E17) and MNIST-RandCIFAR (E18) datasets respectively. We observe considerable gains of around $15\\\\%$ on MNIST-CIFAR-10 accuracy using CIFAR-RandMNIST training when compared to ERM (E5), demonstrating that the proposed approach not only results in more CIFAR features being used for classification, but also leads to the learning of better CIFAR features.\\n\\n5.4 OOD GENERALIZATION IN A REAL WORLD SETTING\\n\\nWe show the efficacy of FRR towards improving OOD generalization on the DomainBed (Gulrajani & Lopez-Paz, 2020) benchmark. We use the performance of the model on in-domain validation data (i.e. the in-domain strategy by Gulrajani & Lopez-Paz (2020)) to select the best hyper-parameters, and report the average performance and standard deviation across 5 random seeds.\\n\\nBaselines: We compare our method against standard ERM training, which has proven to be a frustratingly difficult baseline (Gulrajani & Lopez-Paz, 2020), and also against several state of the art methods on this benchmark - SW AD (Cha et al., 2021), MIRO (Cha et al., 2022) and SMA (Arpit...\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Results on DomainBed: The bottom partition shows results of methods that perform model weight averaging. In both cases, with (top) and without (bottom) model weight averaging, the proposed approach outperforms existing methods.\\n\\n| Algorithm            | PACS | VLCS | OfficeHome | TerraIncognita | DomainNet |\\n|----------------------|------|------|------------|----------------|-----------|\\n| ERM                  | 85.5 | 77.5 | 66.5       | 46.1           | 40.9      |\\n| IRM                  | 83.5 | 78.5 | 64.3       | 47.6           | 33.9      |\\n| CORAL                | 86.2 | 78.8 | 68.7       | 47.6           | 41.5      |\\n| MIRO                 | 85.4 | 79.0 | 70.5       | 50.4           | 44.3      |\\n| ERM+FRR-L            | 85.7 | 76.6 | 68.4       | 53.7           | 44.2      |\\n| ERM+FRR              | 87.5 | 77.6 | 69.4       | 54.1           | 45.1      |\\n| SMA                  | 87.5 | 78.2 | 70.6       | 50.3           | 46.0      |\\n| SW AD                | 88.1 | 79.1 | 70.6       | 50.0           | 46.5      |\\n| SW AD+FRR            | 89.2 | 80.0 | 70.3       | 53.2           | 46.2      |\\n| SMF                  | 87.5 | 78.5 | 70.6       | 50.3           | 46.0      |\\n| SW AD+FRR            | 89.2 | 80.0 | 70.3       | 53.2           | 46.2      |\\n\\nMain Results: The main results of our algorithm are reported in Table 3. We find that our pipeline of training and finetuning with FRR, when combined with ERM achieves improved performance with respect to the state of the art methods that do not use model weight-averaging, and in fact achieves comparable performance to methods that use model weight averaging as well. Further, our method obtains substantial gains of more than 3% over ERM across datasets. The gains are especially pronounced for the larger datasets including DomainNet and TerraIncognita (8% and 5% resp.), indicating the efficacy and scalability of our method. Further, it is clear from Table 3 that finetuning the feature extractor once the linear layer is fixed provides a boost of over 1% on average over FRR-L. This empirically validates our finetuning paradigm which we denote as ERM+FRR. Finally, using our method in tandem with SW AD helps us achieve a new state-of-the-art on the DomainBed benchmark, outperforming other methods on three datasets while achieving comparable performance on two, and being better than existing SOTA by close to 1% on average. We report detailed results and further ablations in Appendices H and J.\\n\\n6 CONCLUSION AND DISCUSSION\\n\\nIn this work, we consider the problem of OOD generalization through the lens of mitigating Simplicity Bias in Neural Network training. To unravel the paradox pertaining to the existence of Simplicity Bias in learning only the simplest features, and the observation that the features learned by large practical models may already be sufficiently diverse, we put forth the Feature Replication Hypothesis that conjectures the learning of replicated simple features and sparse complex ones. Combining this with the Implicit Bias of SGD to converge to maximum margin solutions, we provide a theoretical justification to the high OOD sensitivity of Neural Networks.\\n\\nTo specifically overcome the effect of simple feature replication in linear layer training, we propose the Feature Reconstruction Regularizer, that penalizes the $\\\\ell_p$ norm distance between the features and their reconstruction from the output logits, thus improving the diversity of features used for classification. We further propose to freeze the weights of the linear layer thus trained, and use the FRR regularizer for finetuning the full network, to refine the features to be more useful for the downstream task. We justify the proposed regularizer both theoretically and empirically on synthetic and semi-synthetic datasets, and demonstrate its effectiveness in a real world OOD generalization setting.\\n\\nWe believe and hope that this work can pave the way towards obtaining a better understanding on the underlying causes for OOD brittleness of neural networks, and inspire the development of better algorithms for addressing the same. We believe the proposed regularizer can potentially work effectively in several other settings that involve the use of linear layer training/finetuning, such as domain adaptation and transfer learning. While the regularizer works effectively in a scenario where the network is first trained using an algorithm such as ERM to learn features that are relevant to the task at hand, the robustness of the proposed algorithm in the presence of severely non-relevant features is yet to be explored.\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.\\n\\nDevansh Arpit, Stanis\u0142aw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International conference on machine learning, pp. 233\u2013242. PMLR, 2017.\\n\\nDevansh Arpit, Huan Wang, Yingbo Zhou, and Caiming Xiong. Ensemble of averages: Improving model selection and boosting performance in domain generalization. arXiv preprint arXiv:2110.10832, 2021.\\n\\nHyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh. Learning de-biased representations with biased representations. In International Conference on Machine Learning, pp. 528\u2013539. PMLR, 2020.\\n\\nGilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. Advances in neural information processing systems, 24, 2011.\\n\\nJunbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\\n\\nJunbum Cha, Kyungjae Lee, Sungrae Park, and Sanghyuk Chun. Domain generalization by mutual-information regularization with pre-trained models. arXiv preprint arXiv:2203.10789, 2022.\\n\\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7oise Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096\u20132030, 2016.\\n\\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.\\n\\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.\\n\\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.\\n\\nZeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In European Conference on Computer Vision, pp. 124\u2013140. Springer, 2020.\\n\\nPolina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient for robustness to spurious correlations. arXiv preprint arXiv:2204.02937, 2022.\\n\\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pp. 5637\u20135664. PMLR, 2021.\\n\\nAnanya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. arXiv preprint arXiv:2202.10054, 2022.\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm   | VLCS Avg | OfficeHome Avg |\\n|-------------|----------|----------------|\\n| GroupDRO    | 97.3 \u00b1 0.3 63.4 \u00b1 0.9 69.5 \u00b1 0.8 76.7 \u00b1 0.7 |                |\\n| RSC         | 97.9 \u00b1 0.1 62.5 \u00b1 0.7 72.3 \u00b1 1.2 75.6 \u00b1 0.8 |                |\\n| MLDG        | 97.4 \u00b1 0.2 65.2 \u00b1 0.7 71.0 \u00b1 1.4 75.3 \u00b1 1.0 |                |\\n| MTL         | 97.8 \u00b1 0.4 64.3 \u00b1 0.3 71.5 \u00b1 0.7 75.3 \u00b1 1.7 |                |\\n| I-Mixup     | 98.3 \u00b1 0.6 64.8 \u00b1 1.0 72.1 \u00b1 0.5 74.3 \u00b1 0.8 |                |\\n| ERM         | 97.7 \u00b1 0.4 64.3 \u00b1 0.9 73.4 \u00b1 0.5 74.6 \u00b1 1.3 |                |\\n| MMD         | 97.7 \u00b1 0.1 64.0 \u00b1 1.1 72.8 \u00b1 0.2 75.3 \u00b1 3.3 |                |\\n| CDANN       | 97.1 \u00b1 0.3 65.1 \u00b1 1.2 70.7 \u00b1 0.8 77.1 \u00b1 0.6 |                |\\n| ARM         | 98.7 \u00b1 0.2 63.6 \u00b1 0.7 71.3 \u00b1 1.2 76.7 \u00b1 0.6 |                |\\n| SagNet      | 97.9 \u00b1 0.4 64.5 \u00b1 0.5 71.4 \u00b1 1.3 77.5 \u00b1 0.5 |                |\\n| Mixstyle    | 98.6 \u00b1 0.3 64.5 \u00b1 1.1 72.6 \u00b1 0.5 75.7 \u00b1 1.7 |                |\\n| VREx        | 98.4 \u00b1 0.3 64.4 \u00b1 1.4 74.1 \u00b1 0.4 76.2 \u00b1 1.3 |                |\\n| IRM         | 98.6 \u00b1 0.1 64.9 \u00b1 0.9 73.4 \u00b1 0.6 77.3 \u00b1 0.9 |                |\\n| DANN        | 99.0 \u00b1 0.3 65.1 \u00b1 1.4 73.1 \u00b1 0.3 77.2 \u00b1 0.6 |                |\\n| CORAL       | 98.3 \u00b1 0.1 66.1 \u00b1 1.2 73.4 \u00b1 0.3 77.5 \u00b1 1.2 |                |\\n| SMA         | 99.0 \u00b1 0.2 63.0 \u00b1 0.2 74.5 \u00b1 0.3 76.4 \u00b1 1.1 |                |\\n| MIRO        | 99.3 \u00b1 0.0 65.2 \u00b1 0.5 74.9 \u00b1 0.3 76.0 \u00b1 0.3 |                |\\n| SW AD       | 98.8 \u00b1 0.1 63.3 \u00b1 0.3 75.3 \u00b1 0.5 79.2 \u00b1 0.6 |                |\\n| SW AD+FRR   | 98.9 \u00b1 0.4 66.3 \u00b1 0.2 75.9 \u00b1 0.6 79.0 \u00b1 0.2 |                |\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 10: Out-of-domain accuracies (%) on TerraIncognita.\\n\\n| Algorithm   | Avg  | MMD   | L38   | L43   | L46   |\\n|-------------|------|-------|-------|-------|-------|\\n| L100        | 42.2 | 41.9\u00b13.0 | 34.8\u00b11.0 | 57.0\u00b11.9 | 35.2\u00b11.8 |\\n| GroupDRO    | 43.2 | 41.2\u00b10.7 | 38.6\u00b12.1  | 56.7\u00b10.9  | 36.4\u00b12.1  |\\n| Mixstyle    | 44.0 | 54.3\u00b11.1 | 34.1\u00b11.1  | 55.9\u00b11.1  | 31.7\u00b12.1  |\\n| ARM         | 45.5 | 49.3\u00b10.7 | 38.3\u00b12.4  | 55.8\u00b10.8  | 38.7\u00b11.3  |\\n| MTL         | 45.6 | 49.3\u00b11.2 | 39.6\u00b16.3  | 55.6\u00b11.1  | 37.8\u00b10.8  |\\n| CDANN       | 45.8 | 47.0\u00b11.9 | 41.3\u00b14.8  | 54.9\u00b11.7  | 39.8\u00b12.3  |\\n| ERM         | 46.1 | 49.8\u00b14.4 | 42.1\u00b11.4  | 56.9\u00b11.8  | 35.7\u00b13.9  |\\n| VREx        | 46.4 | 48.2\u00b14.3 | 41.7\u00b11.3  | 56.8\u00b10.8  | 38.7\u00b13.1  |\\n| RSC         | 46.6 | 50.2\u00b12.2 | 39.2\u00b11.4  | 56.3\u00b11.4  | 40.8\u00b10.6  |\\n| DANN        | 46.7 | 51.1\u00b13.5 | 40.6\u00b10.6  | 57.4\u00b10.5  | 37.7\u00b11.8  |\\n| IRM         | 47.6 | 54.6\u00b11.3 | 39.8\u00b11.9  | 56.2\u00b11.8  | 39.6\u00b10.8  |\\n| CORAL       | 47.7 | 51.6\u00b12.4 | 42.2\u00b11.0  | 57.0\u00b11.0  | 39.8\u00b12.9  |\\n| MLDG        | 47.8 | 54.2\u00b13.0 | 44.3\u00b11.1  | 55.6\u00b10.3  | 36.9\u00b12.2  |\\n| I-Mixup     | 47.9 | 59.6\u00b12.0 | 42.2\u00b11.4  | 55.9\u00b10.8  | 33.9\u00b11.4  |\\n| SagNet      | 48.6 | 53.0\u00b12.9 | 43.0\u00b12.5  | 57.9\u00b10.6  | 40.4\u00b11.3  |\\n| SMA         | 50.3 | 54.9\u00b10.4 | 45.5\u00b10.6  | 60.1\u00b11.5  | 40.5\u00b10.4  |\\n| MIRO        | 50.3 | 59.6\u00b14.3 | 41.1\u00b11.4  | 60.2\u00b10.8  | 40.4\u00b11.3  |\\n| SW AD       | 50.0 | 55.4\u00b10.0 | 44.9\u00b11.1  | 59.7\u00b10.4  | 39.9\u00b10.2  |\\n| SW AD+FRR   | 53.2 | 60.1\u00b11.0 | 47.9\u00b11.7  | 60.7\u00b10.4  | 42.3\u00b11.3  |\\n\\nTable 11: Out-of-domain accuracies (%) on DomainNet.\\n\\n| Algorithm   | Avg  | MMD   | paint | quick | real | sketch | clip info |\\n|-------------|------|-------|-------|-------|------|--------|-----------|\\n| L100        | 23.4 | 32.1\u00b113.3 | 11.0\u00b14.6 | 26.8\u00b111.3 | 8.7\u00b12.1 | 32.7\u00b113.8 | 28.9\u00b111.9  |\\n| GroupDRO    | 33.3 | 47.2\u00b10.5 | 17.5\u00b10.4 | 33.8\u00b10.5 | 9.3\u00b10.3 | 51.6\u00b10.4 | 40.1\u00b10.6  |\\n| VREx        | 33.6 | 47.3\u00b13.5 | 16.0\u00b11.5 | 35.8\u00b14.6 | 10.9\u00b10.3 | 49.6\u00b14.9  | 42.0\u00b13.0  |\\n| IRM         | 33.9 | 48.5\u00b12.8 | 15.0\u00b11.5 | 38.3\u00b14.3 | 10.9\u00b10.5 | 48.2\u00b15.2  | 42.3\u00b13.1  |\\n| Mixstyle    | 34.0 | 51.9\u00b10.4 | 13.3\u00b10.2 | 37.0\u00b10.5 | 12.3\u00b10.1 | 46.1\u00b10.3  | 43.4\u00b10.4  |\\n| ARM         | 35.5 | 49.7\u00b10.3 | 16.3\u00b10.5 | 40.9\u00b11.1 | 9.4\u00b10.1 | 53.4\u00b10.4  | 43.5\u00b10.4  |\\n| CDANN       | 38.3 | 54.6\u00b10.4 | 17.3\u00b10.1 | 43.7\u00b10.9 | 12.1\u00b10.7 | 56.2\u00b10.4  | 45.9\u00b10.5  |\\n| DANN        | 38.3 | 53.1\u00b10.2 | 18.3\u00b10.1 | 44.2\u00b10.7 | 11.8\u00b10.1 | 55.5\u00b10.4  | 46.8\u00b10.6  |\\n| RSC         | 38.9 | 55.0\u00b11.2 | 18.3\u00b10.5 | 44.4\u00b10.6 | 12.2\u00b10.2 | 55.7\u00b10.7  | 47.8\u00b10.9  |\\n| I-Mixup     | 39.2 | 55.7\u00b10.3 | 18.5\u00b10.5 | 44.3\u00b10.5 | 12.5\u00b10.4 | 55.8\u00b10.3  | 48.2\u00b10.5  |\\n| SagNet      | 40.3 | 57.7\u00b10.3 | 19.0\u00b10.2 | 45.3\u00b10.3 | 12.7\u00b10.5 | 58.1\u00b10.5  | 48.8\u00b10.2  |\\n| MTL         | 40.6 | 57.9\u00b10.5 | 18.5\u00b10.4 | 46.0\u00b10.1 | 12.5\u00b10.1 | 59.5\u00b10.3  | 49.2\u00b10.1  |\\n| ERM         | 40.9 | 58.1\u00b10.3 | 18.8\u00b10.3 | 46.7\u00b10.3 | 12.2\u00b10.4 | 59.6\u00b10.1  | 49.8\u00b10.4  |\\n| MLDG        | 41.2 | 59.1\u00b10.2 | 19.1\u00b10.3 | 45.8\u00b10.7 | 13.4\u00b10.3 | 59.6\u00b10.2  | 50.2\u00b10.4  |\\n| CORAL       | 41.5 | 59.2\u00b10.1 | 19.7\u00b10.2 | 46.6\u00b10.3 | 13.4\u00b10.4 | 59.8\u00b10.2  | 50.1\u00b10.6  |\\n| MetaReg     | 43.6 | 59.8\u00b12.5 | 25.6\u00b10.5 | 50.2\u00b11.1 | 11.5\u00b10.5 | 64.6\u00b10.6  | 50.1\u00b14.3  |\\n| DMG         | 43.6 | 65.2\u00b12.2 | 22.2\u00b10.3 | 50.0\u00b10.1 | 15.7\u00b10.3 | 59.6\u00b10.6  | 49.0\u00b10.2  |\\n| SMA         | 46.0 | 64.4\u00b10.3 | 22.4\u00b10.2 | 53.4\u00b10.3 | 15.4\u00b10.1 | 64.7\u00b10.2  | 55.5\u00b10.1  |\\n| MIRO        | 44.2 | 61.9\u00b10.3 | 20.9\u00b10.5 | 50.3\u00b11.3 | 13.0\u00b10.4 | 65.2\u00b10.2  | 52.7\u00b10.8  |\\n| SW AD       | 46.5 | 66.0\u00b10.1 | 22.4\u00b10.3 | 53.5\u00b10.1 | 16.1\u00b10.2 | 65.8\u00b10.4  | 55.5\u00b10.3  |\\n| SW AD+FRR   | 46.2 | 65.9\u00b10.1 | 22.3\u00b10.0 | 52.8\u00b10.1 | 14.8\u00b10.3 | 66.2\u00b10.1  | 55.0\u00b10.3  |\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for domain generalization. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018a.\\n\\nYa Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 624\u2013639, 2018b.\\n\\nZiyue Li, Kan Ren, Xinyang Jiang, Bo Li, Haipeng Zhang, and Dongsheng Li. Domain generalization using pretrained models without fine-tuning. arXiv preprint arXiv:2203.04600, 2022.\\n\\nToshihiko Matsuura and Tatsuya Harada. Domain generalization using a mixture of multiple latent domains. In AAAI, 2020.\\n\\nKrikamol Muandet, David Balduzzi, and Bernhard Sch\u00f6lkopf. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pp. 10\u201318. PMLR, 2013.\\n\\nSinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359, 2009.\\n\\nJoaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. Mit Press, 2008.\\n\\nElan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. Domain-adjusted regression or: Erm may already learn features sufficient for out-of-distribution generalization. arXiv preprint arXiv:2202.06856, 2022.\\n\\nHarshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias in neural networks. Advances in Neural Information Processing Systems, 33:9573\u20139585, 2020.\\n\\nShiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, and Sunita Sarawagi. Generalizing across domains via cross-gradient training. arXiv preprint arXiv:1804.10745, 2018.\\n\\nYuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. arXiv preprint arXiv:2104.09937, 2021.\\n\\nHidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of statistical planning and inference, 90(2):227\u2013244, 2000.\\n\\nDaniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822\u20132878, 2018.\\n\\nDamien Teney, Ehsan Abbasnejad, Simon Lucey, and Anton van den Hengel. Evading the simplicity bias: Training a diverse set of models discovers solutions with superior ood generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16761\u201316772, 2022.\\n\\nXavier Thomas, Dhruv Mahajan, Alex Pentland, and Abhimanyu Dubey. Adaptive methods for aggregated domain generalization, 2021.\\n\\nAntonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In CVPR 2011, pp. 1521\u20131528, 2011. doi: 10.1109/CVPR.2011.5995347.\\n\\nGuillermo Valle-Perez, Chico Q Camargo, and Ard A Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. arXiv preprint arXiv:1805.08522, 2018.\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "zH9GcZ3ZGXu", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we present a generalization of Claim 3.1 and Proposition 3.2 under weaker assumptions on the featurizer, and use them to prove the claims made in Sec 3.\\n\\nA more general setting\\n\\nConsider the dataset distribution $D$ as defined in eq 2. Now, let $f(\\\\Theta x) = \\\\Theta x$, where $\\\\Theta \\\\in d \\\\times 2$. Further, $\\\\forall \\\\theta_i, 1 + \\\\theta_i, 2 = 1$. In simple words, the feature extractor maps the input to a $d$-dimensional feature representation, where each feature is a convex combination of the input feature variations. Also note that $\\\\theta = \\\\begin{bmatrix} 1 & 0 \\\\\\\\ 1 & 0 \\\\\\\\ \\\\vdots \\\\\\\\ 1 & 0 \\\\\\\\ 0 & 1 \\\\end{bmatrix}$ corresponds to feature replication.\\n\\nWe now rephrase the results from Sec 3.3 in this setting and provide proofs for the same.\\n\\nClaim A.1. (Restating Claim 3.1) - The max margin classifier $w$ minimizing $\\\\|w\\\\|_2$ and satisfying $y \\\\langle w, \\\\Theta x \\\\rangle \\\\geq 1$ is given by $w = [2d, \\\\cdots, 2d]$.\\n\\nProof. Consider the point $x = (0.5, 0.5)$. Then, due to the constraint on the max-margin classifier, we have $y \\\\langle w, \\\\Theta x \\\\rangle \\\\geq 1$, i.e. $\\\\frac{1}{2} \\\\sum_{j=1}^{d} w_j (\\\\theta_j, 1 + \\\\theta_j, 2) \\\\geq 1$. The minimizer of $\\\\ell_2$ norm under this constraint would be when all $w_j = 2$ for all $j$.\\n\\nNote that the \\\"effective classifier\\\" $\\\\tilde{w}$ in the input space in this case is $\\\\langle w, \\\\Theta \\\\rangle$, i.e. the slope of the classifier in the input space $\\\\tilde{w} = \\\\tilde{w}_1 \\\\tilde{w}_2 = \\\\sum_{j=1}^{d} \\\\theta_j, 2 \\\\sum_{j=1}^{d} \\\\theta_j, 1$. In particular, for the case of feature replication, $\\\\sum_{j=1}^{d} \\\\theta_j, 2 = d - 1$ and $\\\\sum_{j=1}^{d} \\\\theta_j, 1 = 1$, leading to a skewed classifier.\\n\\nNow we restate and show the robustness of FRR in this setting.\\n\\nProposition A.2. (Restating Prop 3.2) - Denote the feature reconstruction regularizer for this setting as $FRR(w) = \\\\min_U \\\\max_{1 \\\\leq i \\\\leq d} E[\\\\langle w, \\\\Theta x \\\\rangle u_i - (\\\\Theta x)_i^2]$.\\n\\nLet $w_{FRR}$ be the minimizer of $FRR(w)$ satisfying $y \\\\langle w_{FRR}, \\\\Theta x \\\\rangle \\\\geq 0$ (i.e. it is a perfect classifier).\\n\\nThen, $w_{FRR}$ satisfies $\\\\langle w_{FRR}, \\\\Theta \\\\cdot, 2 \\\\rangle = 1 - (\\\\theta_b, 1 - \\\\theta_a, 2) 1 + (\\\\theta_b, 1 - \\\\theta_a, 2)$,\\n\\nwhere $a = \\\\arg \\\\max_i \\\\theta_i, 2$, $b = \\\\arg \\\\max_i \\\\theta_i, 1$.\\n\\nProof. Consider the FRR for this dataset - $FRR(w) = \\\\min_U \\\\max_{1 \\\\leq i \\\\leq d} E[\\\\langle w, \\\\Theta x \\\\rangle u_i - (\\\\Theta x)_i^2]$.\"}"}
{"id": "zH9GcZ3ZGXu", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We now consider each term in this expression. For each component of the max function $\\\\alpha_i$ we compute the derivative of the above expression with respect to $\\\\theta$. Further, let $\\\\alpha_{a,b}$. Then, we obtain $\\\\arg \\\\max_{\\\\theta} E\\\\left[\\\\sum_i \\\\theta_i \\\\cdot (1 - \\\\alpha_{i,a} - \\\\alpha_{i,b}) \\\\right]$. This is because $E\\\\left[\\\\sum_i \\\\theta_i \\\\cdot (1 - \\\\alpha_{i,a} - \\\\alpha_{i,b}) \\\\right] = (1 - \\\\alpha_{i,a} - \\\\alpha_{i,b}) \\\\cdot \\\\sum_i \\\\theta_i$ for each $i$. Then, by computing the minimum over $\\\\theta$, we obtain $E\\\\left[\\\\sum_i \\\\min \\\\left\\\\{ \\\\alpha_{i,a}, \\\\alpha_{i,b} \\\\right\\\\} \\\\cdot \\\\sum_i \\\\theta_i \\\\right]$. This is because $\\\\min \\\\left\\\\{ \\\\alpha_{i,a}, \\\\alpha_{i,b} \\\\right\\\\} = \\\\alpha_{i,a}$ and $\\\\alpha_{i,b}$ for each $i$. Then, by considering the quadratic in $\\\\theta$, we compute the derivative of the above expression for each $i$. Further, let $\\\\alpha_{a,1}$, $\\\\alpha_{b,1}$ and $\\\\alpha_{a,2}$, $\\\\alpha_{b,2}$. Then, we obtain $13 E\\\\left[\\\\sum_i \\\\theta_i \\\\cdot \\\\left( (1 - \\\\alpha_{i,a}) \\\\cdot \\\\sum_i \\\\theta_i + (1 - \\\\alpha_{i,b}) \\\\cdot \\\\sum_i \\\\theta_i \\\\right) \\\\right]$. This is because $13 E\\\\left[\\\\sum_i \\\\theta_i \\\\cdot \\\\left( (1 - \\\\alpha_{i,a}) \\\\cdot \\\\sum_i \\\\theta_i + (1 - \\\\alpha_{i,b}) \\\\cdot \\\\sum_i \\\\theta_i \\\\right) \\\\right] = (1 - \\\\alpha_{i,a} - \\\\alpha_{i,b}) \\\\cdot \\\\sum_i \\\\theta_i$ for each $i$. Then, by considering the quadratic in $\\\\theta$, we compute the derivative of the above expression for each $i$. Further, let $\\\\alpha_{a,1}$, $\\\\alpha_{b,1}$, $\\\\alpha_{a,2}$, $\\\\alpha_{b,2}$, $\\\\alpha_{a,3}$, $\\\\alpha_{b,3}$, $\\\\alpha_{a,4}$, $\\\\alpha_{b,4}$, $\\\\alpha_{a,5}$, $\\\\alpha_{b,5}$, $\\\\alpha_{a,6}$, $\\\\alpha_{b,6}$, $\\\\alpha_{a,7}$, $\\\\alpha_{b,7}$, $\\\\alpha_{a,8}$, $\\\\alpha_{b,8}$, $\\\\alpha_{a,9}$, $\\\\alpha_{b,9}$, $\\\\alpha_{a,10}$, $\\\\alpha_{b,10}$, $\\\\alpha_{a,11}$, $\\\\alpha_{b,11}$, $\\\\alpha_{a,12}$, $\\\\alpha_{b,12}$, $\\\\alpha_{a,13}$, $\\\\alpha_{b,13}$, $\\\\alpha_{a,14}$, $\\\\alpha_{b,14}$, $\\\\alpha_{a,15}$, $\\\\alpha_{b,15}$, $\\\\alpha_{a,16}$, $\\\\alpha_{b,16}$, $\\\\alpha_{a,17}$, $\\\\alpha_{b,17}$, $\\\\alpha_{a,18}$, $\\\\alpha_{b,18}$, $\\\\alpha_{a,19}$, $\\\\alpha_{b,19}$, $\\\\alpha_{a,20}$, $\\\\alpha_{b,20}$, $\\\\alpha_{a,21}$, $\\\\alpha_{b,21}$, $\\\\alpha_{a,22}$, $\\\\alpha_{b,22}$, $\\\\alpha_{a,23}$, $\\\\alpha_{b,23}$, $\\\\alpha_{a,24}$, $\\\\alpha_{b,24}$, $\\\\alpha_{a,25}$, $\\\\alpha_{b,25}$, $\\\\alpha_{a,26}$, $\\\\alpha_{b,26}$, $\\\\alpha_{a,27}$, $\\\\alpha_{b,27}$, $\\\\alpha_{a,28}$, $\\\\alpha_{b,28}$, $\\\\alpha_{a,29}$, $\\\\alpha_{b,29}$, $\\\\alpha_{a,30}$, $\\\\alpha_{b,30}$, $\\\\alpha_{a,31}$, $\\\\alpha_{b,31}$, $\\\\alpha_{a,32}$, $\\\\alpha_{b,32}$, $\\\\alpha_{a,33}$, $\\\\alpha_{b,33}$, $\\\\alpha_{a,34}$, $\\\\alpha_{b,34}$, $\\\\alpha_{a,35}$, $\\\\alpha_{b,35}$, $\\\\alpha_{a,36}$, $\\\\alpha_{b,36}$, $\\\\alpha_{a,37}$, $\\\\alpha_{b,37}$, $\\\\alpha_{a,38}$, $\\\\alpha_{b,38}$, $\\\\alpha_{a,39}$, $\\\\alpha_{b,39}$, $\\\\alpha_{a,40}$, $\\\\alpha_{b,40}$, $\\\\alpha_{a,41}$, $\\\\alpha_{b,41}$, $\\\\alpha_{a,42}$, $\\\\alpha_{b,42}$, $\\\\alpha_{a,43}$, $\\\\alpha_{b,43}$, $\\\\alpha_{a,44}$, $\\\\alpha_{b,44}$, $\\\\alpha_{a,45}$, $\\\\alpha_{b,45}$, $\\\\alpha_{a,46}$, $\\\\alpha_{b,46}$, $\\\\alpha_{a,47}$, $\\\\alpha_{b,47}$, $\\\\alpha_{a,48}$, $\\\\alpha_{b,48}$, $\\\\alpha_{a,49}$, $\\\\alpha_{b,49}$, $\\\\alpha_{a,50}$, $\\\\alpha_{b,50}$, $\\\\alpha_{a,51}$, $\\\\alpha_{b,51}$, $\\\\alpha_{a,52}$, $\\\\alpha_{b,52}$, $\\\\alpha_{a,53}$, $\\\\alpha_{b,53}$, $\\\\alpha_{a,54}$, $\\\\alpha_{b,54}$, $\\\\alpha_{a,55}$, $\\\\alpha_{b,55}$, $\\\\alpha_{a,56}$, $\\\\alpha_{b,56}$, $\\\\alpha_{a,57}$, $\\\\alpha_{b,57}$, $\\\\alpha_{a,58}$, $\\\\alpha_{b,58}$, $\\\\alpha_{a,59}$, $\\\\alpha_{b,59}$, $\\\\alpha_{a,60}$, $\\\\alpha_{b,60}$, $\\\\alpha_{a,61}$, $\\\\alpha_{b,61}$, $\\\\alpha_{a,62}$, $\\\\alpha_{b,62}$, $\\\\alpha_{a,63}$, $\\\\alpha_{b,63}$, $\\\\alpha_{a,64}$, $\\\\alpha_{b,64}$, $\\\\alpha_{a,65}$, $\\\\alpha_{b,65}$, $\\\\alpha_{a,66}$, $\\\\alpha_{b,66}$, $\\\\alpha_{a,67}$, $\\\\alpha_{b,67}$, $\\\\alpha_{a,68}$, $\\\\alpha_{b,68}$, $\\\\alpha_{a,69}$, $\\\\alpha_{b,69}$, $\\\\alpha_{a,70}$, $\\\\alpha_{b,70}$, $\\\\alpha_{a,71}$, $\\\\alpha_{b,71}$, $\\\\alpha_{a,72}$, $\\\\alpha_{b,72}$, $\\\\alpha_{a,73}$, $\\\\alpha_{b,73}$, $\\\\alpha_{a,74}$, $\\\\alpha_{b,74}$, $\\\\alpha_{a,75}$, $\\\\alpha_{b,75}$, $\\\\alpha_{a,76}$, $\\\\alpha_{b,76}$, $\\\\alpha_{a,77}$, $\\\\alpha_{b,77}$, $\\\\alpha_{a,78}$, $\\\\alpha_{b,78}$, $\\\\alpha_{a,79}$, $\\\\alpha_{b,79}$, $\\\\alpha_{a,80}$, $\\\\alpha_{b,80}$, $\\\\alpha_{a,81}$, $\\\\alpha_{b,81}$, $\\\\alpha_{a,82}$, $\\\\alpha_{b,82}$, $\\\\alpha_{a,83}$, $\\\\alpha_{b,83}$, $\\\\alpha_{a,84}$, $\\\\alpha_{b,84}$, $\\\\alpha_{a,85}$, $\\\\alpha_{b,85}$, $\\\\alpha_{a,86}$, $\\\\alpha_{b,86}$, $\\\\alpha_{a,87}$, $\\\\alpha_{b,87}$, $\\\\alpha_{a,88}$, $\\\\alpha_{b,88}$, $\\\\alpha_{a,89}$, $\\\\alpha_{b,89}$, $\\\\alpha_{a,90}$, $\\\\alpha_{b,90}$, $\\\\alpha_{a,91}$, $\\\\alpha_{b,91}$, $\\\\alpha_{a,92}$, $\\\\alpha_{b,92}$, $\\\\alpha_{a,93}$, $\\\\alpha_{b,93}$, $\\\\alpha_{a,94}$, $\\\\alpha_{b,94}$, $\\\\alpha_{a,95}$, $\\\\alpha_{b,95}$, $\\\\alpha_{a,96}$, $\\\\alpha_{b,96}$, $\\\\alpha_{a,97}$, $\\\\alpha_{b,97}$, $\\\\alpha_{a,98}$, $\\\\alpha_{b,98}$, $\\\\alpha_{a,99}$, $\\\\alpha_{b,99}$, $\\\\alpha_{a,100}$, $\\\\alpha_{b,100}$, $\\\\alpha_{a,101}$, $\\\\alpha_{b,101}$, $\\\\alpha_{a,102}$, $\\\\alpha_{b,102}$, $\\\\alpha_{a,103}$, $\\\\alpha_{b,103}$, $\\\\alpha_{a,104}$, $\\\\alpha_{b,104}$, $\\\\alpha_{a,105}$, $\\\\alpha_{b,105}$, $\\\\alpha_{a,106}$, $\\\\alpha_{b,106}$, $\\\\alpha_{a,107}$, $\\\\alpha_{b,107}$, $\\\\alpha_{a,108}$, $\\\\alpha_{b,108}$, $\\\\alpha_{a,109}$, $\\\\alpha_{b,109}$, $\\\\alpha_{a,110}$, $\\\\alpha_{b,110}$, $\\\\alpha_{a,111}$, $\\\\alpha_{b,111}$, $\\\\alpha_{a,112}$, $\\\\alpha_{b,112}$, $\\\\alpha_{a,113}$, $\\\\alpha_{b,113}$, $\\\\alpha_{a,114}$, $\\\\alpha_{b,114}$, $\\\\alpha_{a,115}$, $\\\\alpha_{b,115}$, $\\\\alpha_{a,116}$, $\\\\alpha_{b,116}$, $\\\\alpha_{a,117}$, $\\\\alpha_{b,117}$, $\\\\alpha_{a,118}$, $\\\\alpha_{b,118}$, $\\\\alpha_{a,119}$, $\\\\alpha_{b,119}$, $\\\\alpha_{a,120}$, $\\\\alpha_{b,120}$, $\\\\alpha_{a,121}$, $\\\\alpha_{b,121}$, $\\\\alpha_{a,122}$, $\\\\alpha_{b,122}$, $\\\\alpha_{a,123}$, $\\\\alpha_{b,123}$, $\\\\alpha_{a,124}$, $\\\\alpha_{b,124}$, $\\\\alpha_{a,125}$, $\\\\alpha_{b,125}$, $\\\\alpha_{a,126}$, $\\\\alpha_{b,126}$, $\\\\alpha_{a,127}$, $\\\\alpha_{b,127}$, $\\\\alpha_{a,128}$, $\\\\alpha_{b,128}$, $\\\\alpha_{a,129}$, $\\\\alpha_{b,129}$, $\\\\alpha_{a,130}$, $\\\\alpha_{b,130}$, $\\\\alpha_{a,131}$, $\\\\alpha_{b,131}$, $\\\\alpha_{a,132}$, $\\\\alpha_{b,132}$, $\\\\alpha_{a,133}$, $\\\\alpha_{b,133}$, $\\\\alpha_{a,134}$, $\\\\alpha_{b,134}$, $\\\\alpha_{a,135}$, $\\\\alpha_{b,135}$, $\\\\alpha_{a,136}$, $\\\\alpha_{b,136}$, $\\\\alpha_{a,137}$, $\\\\alpha_{b,137}$, $\\\\alpha_{a,138}$, $\\\\alpha_{b,138}$, $\\\\alpha_{a,139}$, $\\\\alpha_{b,139}$, $\\\\alpha_{a,140}$, $\\\\alpha_{b,140}$, $\\\\alpha_{a,141}$, $\\\\alpha_{b,141}$, $\\\\alpha_{a,142}$, $\\\\alpha_{b,142}$, $\\\\alpha_{a,143}$, $\\\\alpha_{b,143}$, $\\\\alpha_{a,144}$, $\\\\alpha_{b,144}$, $\\\\alpha_{a,145}$, $\\\\alpha_{b,145}$, $\\\\alpha_{a,146}$, $\\\\alpha_{b,146}$, $\\\\alpha_{a,147}$, $\\\\alpha_{b,147}$, $\\\\alpha_{a,148}$, $\\\\alpha_{b,148}$, $\\\\alpha_{a,149}$, $\\\\alpha_{b,149}$, $\\\\alpha_{a,150}$, $\\\\alpha_{b,150}$, $\\\\alpha_{a,151}$, $\\\\alpha_{b,151}$, $\\\\alpha_{a,152}$, $\\\\alpha_{b,152}$, $\\\\alpha_{a,153}$, $\\\\alpha_{b,153}$, $\\\\alpha_{a,154}$, $\\\\alpha_{b,154}$, $\\\\alpha_{a,155}$, $\\\\alpha_{b,155}$, $\\\\alpha_{a,156}$, $\\\\alpha_{b,156}$, $\\\\alpha_{a,157}$, $\\\\alpha_{b,157}$, $\\\\alpha_{a,158}$, $\\\\alpha_{b,158}$, $\\\\alpha_{a,159}$, $\\\\alpha_{b,159}$, $\\\\alpha_{a,160}$, $\\\\alpha_{b,160}$, $\\\\alpha_{a,161}$, $\\\\alpha_{b,161}$, $\\\\alpha_{a,162}$, $\\\\alpha_{b,162}$, $\\\\alpha_{a,163}$, $\\\\alpha_{b,163}$, $\\\\alpha_{a,164}$, $\\\\alpha_{b,164}$, $\\\\alpha_{a,165}$, $\\\\alpha_{b,165}$, $\\\\alpha_{a,166}$, $\\\\alpha_{b,166}$, $\\\\alpha_{a,167}$, $\\\\alpha_{b,167}$, $\\\\alpha_{a,168}$, $\\\\alpha_{b,168}$, $\\\\alpha_{a,169}$, $\\\\alpha_{b,169}$, $\\\\alpha_{a,170}$, $\\\\alpha_{b,170}$, $\\\\alpha_{a,171}$, $\\\\alpha_{b,171}$, $\\\\alpha_{a,172}$, $\\\\"}
