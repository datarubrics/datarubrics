{"id": "lLp-C5nTdJG", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nHere,\\n\\n\\\\[\\nW \\\\in \\\\mathbb{R}^{hd \\\\times d_{model}}, \\\\quad W^Q_i \\\\in \\\\mathbb{R}^{d_k \\\\times (d_{model} + d_{Embed(x_n)})}, \\\\quad W^K_i \\\\in \\\\mathbb{R}^{d_k \\\\times d_d(x)}, \\\\quad W^V_i \\\\in \\\\mathbb{R}^{d_v \\\\times d_d(x)}\\n\\\\]\\n\\nare learnable parameters. Similarly, for FiLM we modulate the input with the resource description as follows:\\n\\n\\\\[\\n\\\\text{FiLM}(\\\\text{Embed}(x_n, d_d(x), h_t - 1, n)) = \\\\text{Concat}(\\\\beta \\\\cdot d_d(x) + \\\\gamma, \\\\text{Embed}(x_n))\\n\\\\]\\n\\nwhere\\n\\n\\\\[\\n\\\\beta = \\\\sigma(W^{\\\\beta} \\\\text{Concat}(x_n, h_t - 1, n) + b^{\\\\beta}),\\n\\\\]\\n\\n\\\\[\\n\\\\gamma = \\\\sigma(W^{\\\\gamma} \\\\text{Concat}(x_n, h_t - 1, n) + b^{\\\\gamma})\\n\\\\]\\n\\nHyperparameter selection\\n\\nWe select hyperparameters by performing a random search independently for each model architecture. The hyperparameters considered by the search are listed in Table 6. All architectures use a Transformer encoder, and the Transformer sizes considered in the search are listed in Table 6 and defined further in Table 5.\\n\\n| Hyperparameter | Value Considered | Architecture | Optimizer |\\n|----------------|------------------|--------------|-----------|\\n| Embedding Dimension | 128, 256, 512 | LSTM, Transformers, IPA-GNN | SGD |\\n| Number of Heads | 4, 4, 8 | LSTM, Transformers, IPA-GNN | |\\n| Number of Layers | 2, 2, 6 | LSTM, Transformers, IPA-GNN | |\\n| QKV Dimension | 128, 256, 512 | LSTM, Transformers, IPA-GNN | |\\n| MLP Dimension | 512, 1024, 2048 | LSTM, Transformers, IPA-GNN | |\\n\\nStep limit\\n\\nFor the IPA-GNN and Exception IPA-GNN, the function \\\\(T(x)\\\\) represents the number of execution steps modeled for program \\\\(x\\\\). We reuse the definition of \\\\(T(x)\\\\) from Bieber et al. (2020) as closely as possible, only modifying it to accept arbitrary Python programs, rather than being restricted to the subset of Python features considered in the dataset of the earlier work.\\n\\nParameter counts\\n\\nWe provide in Table 7 the total number of parameters in each model, for the best performing hyperparameters in each model class. For all model classes, the maximum number of parameters considered is roughly equal (approximately 8.8 million).\\n\\nCompute usage and model speeds\\n\\nAll models are trained on Google Cloud Platform using TPUv2 accelerators. We use approximately one TPU-week of compute in training each IPA-GNN model. At inference time, IPA-GNN compute is proportional to the number of model steps, which is up to 174 for examples in our dataset. We measure the average inference time on the test set: 0.43\"}"}
{"id": "lLp-C5nTdJG", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: The parameter count, training latency (sec/step), and inference latency (sec/batch) for the best performing instance of each model variant. Training and inference latencies use batch size 32.\\n\\nseconds per batch of 32. We also measure the training speed in seconds per step for each method, which we report in Table 7. We observe that the IPA-GNN train times are slower than those of the generic models, a drawback of the IPA-GNN model family in its current implementations. That said, we also note that the IPA-GNN models do not benefit from the same optimizations as basic implementations of the well known general purpose models (GGNN, Transformer, and LSTM), and with further optimizations the IPA-GNN performance can be improved.\\n\\nTable 8: Mean and standard deviation for each metric is calculated from three training runs per model, using the hyperparameters selected via model selection.\\n\\nTable 9 shows the mapping from pylint findings to runtime error. Only eleven of the twenty-six runtime error classes (those listed in Table 9, and \\\"no error\\\") can be predicted by this baseline. Additionally, the presence of a pylint finding that corresponds to an error does not guarantee the error will actually be present when running the program; for example...\"}"}
{"id": "lLp-C5nTdJG", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: The pylint baseline for runtime error prediction predicts the error class shown when it encounters any of the corresponding pylint findings. Many of pylint\u2019s 235 finding types do not indicate runtime errors. This table shows the mapping used by the pylint baseline.\\n\\nFor programs that lack try/except frames, we compute the localization predictions of the Exception Handling IPA-GNN model by summing, separately for each node, the contributions from that node to the global error node across all time steps. This gives an estimate of exception provenance as\\n\\n$$ p(error \\\\text{ at statement } n) = \\\\sum_{t} p_{t,n} \\\\cdot b_{t,n,n} $$\\n\\nFor programs with a try/except frame, however, we must trace the exception back to the statement that originally raised it. To do this, we keep track of the exception provenance at each node at each time step; when an exception raises, it becomes the exception provenance at the statement that it raises to, and when a statement with non-zero exception provenance executes without raising, it propagates its exception provenance information to the next node unchanged.\"}"}
{"id": "lLp-C5nTdJG", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Define $v_{t,n,n'}$ as the amount of \u201cexception probability mass\u201d at time step $t$ at node $n'$ attributable to an exception starting at node $n$. Then we write\\n\\n$$v_{t,n,n'} = \\\\sum_{k \\\\in N(n')} v_{t-1,n,k} \\\\cdot b_{t,k,n'} \\\\cdot p_{t,k} + (1 - \\\\sum_{k \\\\in N(n')} v_{t-1,n,k}) \\\\cdot b_{t,n,n'} \\\\cdot p_{t,n} \\\\cdot \\\\mathbb{1}\\\\{n' = r(n)\\\\}. \\\\tag{21}$$\\n\\nThe first term propagates exception provenance across normal non-raising execution, while the second term introduces exception provenance when an exception is raised. We then write precisely\\n\\n$$\\\\text{p(error at statement } n) = v_T(x),n,n'_{error}, \\\\tag{22}$$\\n\\nallowing the Exception IPA-GNN to make localization predictions for any program in the dataset.\\n\\n**Localization by Multiple Instance Learning**\\n\\nThe Local Transformer and Global Transformer models each compute per-statement node embeddings $\\\\text{Embed}(x_n)$ given by Equation 1. In the multiple instance learning setting, these are transformed into unnormalized per-statement class predictions $\\\\phi(\\\\text{class} = k, \\\\text{lineno} = l) = \\\\text{Dense}(\\\\text{Embed}(x_n))$. \\\\tag{23}\\n\\nWe consider three strategies for aggregating these per-statement predictions into an overall prediction for the task. Under the logsumexp strategy, we treat $\\\\phi$ as logits and write\\n\\n$$\\\\log p(\\\\text{class} = k) \\\\propto \\\\log \\\\sum_{l \\\\in L} \\\\exp \\\\phi(k, l), \\\\tag{24}$$\\n\\n$$\\\\log p(\\\\text{lineno} = l) \\\\propto \\\\log \\\\sum_{k \\\\in K} \\\\exp \\\\phi(k, l), \\\\tag{25}$$\\n\\nwhere $K$ is the set of error classes.\\n\\nThe max and mean strategies meanwhile follow Wang et al. (2018) in asserting\\n\\n$$p(\\\\text{class} = k | \\\\text{lineno} = l) = \\\\text{softmax}(\\\\phi(k, l)), \\\\tag{26}$$\\n\\ncompute the location probabilities as\\n\\n$$p(\\\\text{lineno} = l) \\\\propto \\\\sum_{k \\\\in K} p(\\\\text{class} = k | \\\\text{lineno} = l), \\\\tag{27}$$\\n\\nand compute the outputs as\\n\\n$$\\\\log p(\\\\text{class} = k) \\\\propto \\\\log \\\\max lp(\\\\text{class} = k | \\\\text{lineno} = l), \\\\tag{28}$$\\n\\n$$\\\\log p(\\\\text{class} = k) \\\\propto \\\\log \\\\frac{1}{L} \\\\sum_{l \\\\in L} p(\\\\text{class} = k | \\\\text{lineno} = l), \\\\tag{29}$$\\n\\nwhere $L$ denotes the number of statements in $x$. As with all methods considered, the MIL models are trained to minimize the cross-entropy loss in target class prediction, but these methods still allow reading off predictions of $p(\\\\text{lineno})$.\\n\\n**Impact**\\n\\nOur work builds toward improvements to developer tools, suggesting the possibility of future tools that predict runtime errors in code even when that code lacks unit tests. However, the false positive rate under the current best models present a challenge. A developer tool built using these models may present the developer with incorrect predictions. This could cause the developer to make mistakes, or to lose trust in the tooling, lowering productivity in the short term and making it harder to win back trust in the long term when tools are built upon higher quality models with fewer errors. We therefore recommend that tool developers use a combination of cautious judgement and data driven evaluations when deciding when to implement features that rely on models like the ones we present.\"}"}
{"id": "lLp-C5nTdJG", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks. Advances in neural information processing systems, 32, 2019.\\n\\n\u0141ukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms, 2016.\"}"}
{"id": "lLp-C5nTdJG", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We describe in detail the construction of the Python Runtime Error dataset from the submissions in Project CodeNet (Puri et al., 2021). The Project CodeNet dataset contains over 14 million submissions to 4,053 distinct competitive programming problems, with the submissions spanning more than 50 programming languages. We partition the problems into train, valid, and test splits at an 80:10:10 ratio. By making all submissions to the same problem part of the same split we mitigate concerns about potential data leakage from similar submissions to the same problem. We restrict our consideration to Python submissions, which account for 3,286,314 of the overall Project CodeNet submissions, with 3,119 of the problems receiving at least one submission in Python. In preparing the dataset we execute approximately 3 million problems in a sandboxed environment to collect their runtime error information, we perform two stages of filtering on the dataset, syntactic and complexity filtering, and we construct a textual representation of the input space for each problem from the problem description.\\n\\nA.1 Syntactic Filtering\\nIn this first phase of filtering, we remove submissions in Python 2 as well as those which fail to parse and run from our dataset. We remove 76,888 programs because they are in Python 2, 59,813 programs because they contain syntax errors that prohibit parsing, 2,011 programs that result in runtime errors during parsing, and 6 additional programs for which the python-graphs library fails to construct a control-flow graph. A program may result in a runtime error during parsing if it contains return, break, continue keywords outside of an appropriate frame.\\n\\nA.2 Program Execution\\nWe attempt to run each submission in a sandboxed environment using the sample input provided in the Project CodeNet dataset. The environment is a custom harness running on a Google Cloud Platform (GCP) virtual environment. This allows us to collect standard out and standard error, to monitor for timeouts, and to catch and serialize any Python exceptions raised during execution. We restrict execution of each program to 1 second, marking any program exceeding this time as a timeout error. If the program encounters a Python exception, we use the name of that exception as the target class for the program. If an error type occurs only once in the dataset, we consider the target class to be Other. Programs not exhibiting an error or timeout are given target class \u201cno error\u201d. In addition to collecting the target class, we record for each runtime error the line number at which the error occurs. We use these line numbers as the ground truth for the unsupervised error localization task considered in Section 5.3.\\n\\nA.3 Extracting Resource Descriptions by Parsing Problem Statements\\nFor each problem, we parse the problem statement to extract the input description and input constraints, if they exist. These two sections of the problem statement together form the external resource description that accompanies that problem. The problem statements in our dataset are each written either in English or Japanese, and so we write our parser to support both languages. When one or both of these sections are present in the problem statement, we construct the external resource description for the problem by concatenating together the headers and contents of the sections that are present. For the experiments that use the resource description as a docstring, we prepend to each submission a docstring containing the resource description for the problem that goes with that submission. Similarly these serve as the resource descriptions in the experiments that process resource descriptions via either cross-attention or FiLM.\\n\\nA.4 Vocabulary Construction and Complexity Filtering\\nAll experiments use the same vocabulary and tokenization procedure. For this, we select the standard Byte-Pair Encoding (BPE) tokenization procedure (Sennrich et al., 2016). We construct the vocabulary using 1,000,000 submissions selected from the training split, along with the input space descriptions constructed for all problems in the train split. We use a vocabulary size of 30,000.\"}"}
{"id": "lLp-C5nTdJG", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We then apply size-based filtering, further restricting the set of programs considered. First, the program length after tokenization is not to exceed 512 tokens, the number of nodes and edges in the control-flow graph are each not to exceed 128, and the step limit $T(x)$ for a program computed in Appendix E is not to exceed 174. We select these numbers to trim the long tail of exceptionally long programs, and this filtering reduces the total number of acceptable programs by less than 1%. To achieve consistent datasets comparable across all experiments, we use the longest form of each program (the program augmented with its input space information as a docstring) when computing the program sizes for size-based submission filtering.\\n\\nWe further impose the restriction that no user-defined functions (UDFs) are called in a submission; this further reduces the number of submissions by 682,220. A user-defined function is a function defined in the submission source code, as opposed to a built-in or a function imported from a third party module. Extending the IPA-GNN models to submissions with UDFs called at most once is trivially achieved by replacing the program's control-flow graph with its interprocedural control-flow graph (ICFG) (Nielson and Nielson, 1999). We leave the investigation of modeling user-defined functions to further work.\\n\\nA.5 Final Dataset Details\\n\\nAfter applying syntactic filtering (only keeping Python 3 programs that parse) and complexity filtering (eliminating long programs and programs that call user-defined functions), we are left with a dataset of 2,441,130 examples. The division of these examples by split and by target class is given in Table 1. Figure 3 shows the distribution of program lengths in lines represented in the completed dataset, with an average program length of 14.2 lines. The average statement length is 6.7 tokens, with the full distribution shown in Figure 4.\\n\\nA.6 Dataset License\\n\\nThe Project CodeNet (Puri et al., 2021) data that we use is available under the CDLA Permissive v2.0 license, and we release our derived dataset under this same license.\\n\\nB Under-Approximation of Error Labels\\n\\nAs described in Section 3, the ground truth error targets in our dataset are obtained by running each submission on only a single input. We do this because we only have a single input available from the online judges with which to execute the programs. As a result, the error labels we obtain under-approximate the full set of errors liable to appear at runtime. Metadata obtained from (Puri et al., 2021) indicates whether each submission encountered a runtime error on a larger set of inputs, though it does not indicate the kind or location of these errors when they are present. We use this metadata to determine the degree to which our labels are an under-approximation. We find that on the balanced test set there are 1,076 submissions (4%) which, per the metadata, encounter an error.\"}"}
{"id": "lLp-C5nTdJG", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We next measure generalization from the labels in our dataset to the labels suggested by the metadata without retraining. Since these labels are only binary indicators of error presence, we use our model to perform binary classification by summing the predicted probabilities of all error kinds. The model predicts \\\"no error\\\" on 76.2% of the examples for which our dataset finds no error. On the examples for which the metadata indicates no error, this drops to 75.9%, and on the examples for which the metadata indicates there is an error, this rises to 80.9%. These examples, where a single input detects no error but multiple inputs detect an error, are difficult for the model to classify. We hypothesize that the types of errors our labels omit systematically differ from those our labels include as an explanation for this 4.7% discrepancy.\\n\\nIPA-GNN ARCHITECTURE\\n\\nWe provide a concise and precise definition of the IPA-GNN baseline architecture, following the notation of Bieber et al. (2020). The IPA-GNN operates on the statement-level control-flow graph of the input program $x$, maintaining per-node per-step hidden states $h_{t,n}$ and a soft instruction pointer $p_{t,n}$. At each step $t$, each node $x_n$ participates in execution, branch prediction, and aggregation.\\n\\nFirst, the IPA-GNN models executing the statement at each node to produce per-node state proposals $a_{(1)}^{(t,n)} = \\\\text{RNN}(h_{t-1,n}, \\\\text{Embed}(x_n))$.\\n\\nThen, the model uses these to inform soft branch decisions at every control flow juncture, given as $b_{t,n,n_1, n_2} = \\\\text{softmax}(\\\\text{Dense}(a_{(1)}^{(t,n)}))$, where $\\\\{n_1, n_2\\\\} = \\\\text{N}_{\\\\text{out}}(x_n)$ when $|\\\\text{N}_{\\\\text{out}}(x_n)| = 2$. When $|\\\\text{N}_{\\\\text{out}}(x_n)| = 1$ we have $b_{t,n,n'} = 1$ for $n' \\\\in \\\\text{N}_{\\\\text{out}}(x_n)$ indicating straight-line code. For all other $n$, $n'$, $b_{t,n,n'} = 0$.\\n\\nThe state proposals and branch decisions in turn feed into the computation of the new hidden states $h_{t,n} = \\\\sum_{n' \\\\in \\\\text{N}_{\\\\text{in}}(n)} p_{t-1,n'} \\\\cdot b_{t,n',n} \\\\cdot a_{(1)}^{(t,n)}$ and new instruction pointer values $p_{t,n} = \\\\sum_{n' \\\\in \\\\text{N}_{\\\\text{in}}(n)} p_{t-1,n'} \\\\cdot b_{t,n',n}$.\\n\\nThe hidden state at final time step $T(x)$ at the program's exit node $n_{\\\\text{exit}}$ are used for downstream predictions.\\n\\nINPUT MODULATION\\n\\nIn Section 4.2, we consider both cross-attention (Lee et al., 2019) and Feature-wise Linear Modulation (FiLM) (Perez et al., 2017) as options for the Modulate function. We provide the definitions of these operations here. First, cross-attention modules the input as:\\n\\n$$\\\\text{MultiHead}(\\\\text{Embed}(x_n), d(x), h_{t-1,n}) = \\\\text{Concat}(\\\\text{Concat}(\\\\text{head}_1, ..., \\\\text{head}_h) W_O, \\\\text{Embed}(x_n))$$\\n\\nwhere $\\\\text{head}_i = \\\\text{softmax}(QK_{\\\\sqrt{d_{k}}}) V$\\n\\n$$Q = W_Q i \\\\text{Concat}(\\\\text{Embed}(x_n), h_{t-1,n})$$\\n\\n$$K = W_K d(x)$$\\n\\n$$V = W_V d(x)$$\\n\\n$W_O$, $W_Q$, $W_K$, $W_V$ are learnable parameters.\"}"}
{"id": "lLp-C5nTdJG", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Per-node localization predictions from the BASELINE and DOCSTRING Exception IPA-GNN models on a sample program from the validation split. The target class is ERROR, occurring on line 2 ($n = 2$). BASELINE predicts NOERROR with confidence 0.708, while R.D. predicts EOFEERROR with confidence 0.988, localized at line 3 ($n = 3$). The input description shows the cause for error: there are more input() calls than the number of expected inputs.\\n\\nLocalization Experiment\\nUsing the same protocol as Section 5.1, we train each of the MIL Transformer and Exception IPA-GNN models. As before, the models are trained only to minimize cross-entropy loss on predicting error kind and presence, receiving no error location supervision. We report the localization results in Table 2b. Localization accuracy (\u201cLOCAL.\u201d) measures the percent of the test examples with errors for which the model correctly predicts the error line number.\\n\\nRQ3: The Exception IPA-GNN\u2019s unsupervised localization capabilities far exceed that of baseline approaches. In Figure 2 we see the flow of instruction pointer mass during the execution of a sample program (Table 4) by two Exception IPA-GNN models, including the steps where the models raise probability mass to $n_{error}$. Tallying the contributions to $n_{error}$ from each node yields the exception provenance values in the right half of Table 4. This shows how the model\u2019s internal state resembles plausible program executions and allows for unsupervised localization. As a beneficial side-effect of learning plausible executions, the Exception IPA-GNN can localize the exceptions it predicts.\\n\\n6 DISCUSSION\\nIn this work, we introduce the task of predicting runtime errors in competitive programming problems and advance the capabilities of interpreter-inspired models. Our models support the complexity of competition code and demonstrate that natural language descriptions of external resources can reduce the ambiguity that arises in a static analysis setting. We show that the interpreter-inspired models outperform standard alternatives and that their inductive biases allow for interesting interpretability in the context of unsupervised localization.\\n\\nThough they perform best, current IPA-GNN models require taking many steps of execution, up to 174 on this dataset. A future direction is to model multiple steps of program execution with a single model step, to reduce the number of model steps necessary for long programs. Extending the interpreter-inspired models with additional interpreter features, or supporting multi-file programs or programs with multiple user-defined functions are also interesting avenues for future work.\\n\\nLearning to understand programs remains a rich area of inquiry for machine learning research because of its complexity and the many aspects of code. Learning to understand execution behavior is particularly challenging as programs grow in complexity, and as they depend on more external resources whose contents are not present in the code. Our work presents a challenging problem and advances interpreter-inspired models, both of which we hope are ingredients towards making progress on these difficult and important problems.\"}"}
{"id": "lLp-C5nTdJG", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of machine learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):81, 2018a.\\n\\nMiltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs with graphs. In International Conference on Learning Representations, 2018b.\\n\\nMiltiadis Allamanis, Henry Jackson-Flux, and Marc Brockschmidt. Self-supervised bug detection and repair. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models, 2021.\\n\\nNathaniel Ayewah, William Pugh, David Hovemeyer, J. David Morgenthaler, and John Penix. Using static analysis to find bugs. IEEE Software, 25(5):22\u201329, 2008. doi: 10.1109/MS.2008.130.\\n\\nDavid Bieber, Charles Sutton, Hugo Larochelle, and Daniel Tarlow. Learning to execute programs with instruction pointer attention graph neural networks. In Advances in Neural Information Processing Systems, 2020.\\n\\nMatko Bo\u02c7snjak, Tim Rockt \u00a8aschel, Jason Naradowsky, and Sebastian Riedel. Programming with a differentiable forth interpreter, 2017.\\n\\nCristian Cadar, Daniel Dunbar, Dawson R Engler, et al. Klee: unassisted and automatic generation of high-coverage tests for complex systems programs. In OSDI, volume 8, pages 209\u2013224, 2008.\\n\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.\\n\\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost, 2016.\\n\\nXinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=H1gfOiAqYm.\\n\\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \u0141ukasz Kaiser. Universal transformers, 2019.\\n\\nThomas G. Dietterich, Richard H. Lathrop, and Tom \u00b4as Lozano-P \u00b4erez. Solving the multiple instance problem with axis-parallel rectangles. Artificial Intelligence, 89(1):31\u201371, 1997. ISSN 0004-3702. doi: https://doi.org/10.1016/S0004-3702(96)00034-3. URL https://www.sciencedirect.com/science/article/pii/S0004370296000343.\\n\\nElizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang. Hoppity: Learning graph transformations to detect and fix bugs in programs. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SJeqs6EFvB.\\n\\nAlexander L. Gaunt, Marc Brockschmidt, Nate Kushman, and Daniel Tarlow. Differentiable programs with neural libraries, 2017.\\n\\nPatrice Godefroid, Nils Klarlund, and Koushik Sen. Dart: Directed automated random testing. In Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation, pages 213\u2013223, 2005.\\n\\n10\"}"}
{"id": "lLp-C5nTdJG", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lLp-C5nTdJG", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lLp-C5nTdJG", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The execution behavior of a program often depends on external resources, such as program inputs or file contents, and so the program cannot be run in isolation. Nevertheless, software developers benefit from fast iteration loops where automated tools identify errors as early as possible, even before programs can be compiled and run. This presents an interesting machine learning challenge: can we predict runtime errors in a \u201cstatic\u201d setting, where program execution is not possible? Here, we introduce a competitive programming dataset and task for predicting runtime errors, which we show is difficult for generic models like Transformers. We approach this task by developing an interpreter-inspired architecture with an inductive bias towards mimicking program executions, which models exception handling and \u201clearns to execute\u201d descriptions of external resources. Surprisingly, we show that the model can also predict the locations of errors, despite being trained only on labels indicating error presence or absence and kind. In total, we present a practical and difficult-yet-approachable challenge problem related to learning program execution behavior and we demonstrate promising new capabilities of interpreter-inspired machine learning models for code.\\n\\n1 INTRODUCTION\\n\\nWe investigate applying neural machine learning methods to the static analysis of source code for early prediction of runtime errors. The execution behavior of a program is in general not fully defined by its source code in isolation, because programs often rely on external resources like inputs, the contents of files, or the network. Nevertheless, software developers benefit from fast iteration loops where automated tools identify errors early, even when program execution is not yet an option. Therefore we consider the following machine learning challenge: can we predict runtime errors in a \u201cstatic\u201d setting, where program execution is not possible?\\n\\nThis runtime error prediction task is well suited as a challenge problem because it is difficult-yet-approachable, has real-world value for software developers, requires novel modeling considerations that we hypothesize will be applicable to a range of learning for code tasks, and with this work, now has a suitable large dataset of complex human-authored code with error labels. The task is to predict whether a program will exhibit a runtime error when it is run, and if so to determine the error; even when static analysis cannot provide guarantees of an error in the code, patterns learned from data may point to likely errors. Our dataset consists of 2.4 million Python 3 programs from Project CodeNet (Puri et al., 2021) written by competitive programmers. We have run all programs in a sandboxed environment on sample inputs to determine their error classes, finding the programs exhibit 26 distinct error classes including \u201cno error\u201d. Each program relies on an external resource, the stdin input stream, and we pair each program with a natural language description of the behavior of the stream. We make the task and dataset, along with all models considered in this work, available for the research community to facilitate reproduction of this work and further research.\\n\\nTo make progress on this challenging task, we identify a promising class of models from prior work, interpreter-inspired models, and we demonstrate they perform well on the task. Instruction Pointer\"}"}
{"id": "lLp-C5nTdJG", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Attention Graph Neural Network (IPA-GNN) (Bieber et al., 2020) models simulate the execution of a program, following its control flow structure, but operating in a continuous embedding space. We make a number of improvements to IPA-GNN: scaling up to handle complex programs requiring thousands of execution steps, adding the ability to \u201clearn to execute\u201d descriptions of external resources, and extending the architecture to model exception handling and recover error locations.\\n\\nWe evaluate these interpreter-inspired architectures against Transformer, LSTM, and GGNN neural baselines, and against pylint as a static analysis baseline. Our combined improvements lead to increased accuracy in predicting runtime errors and to interpretability allowing for prediction of error locations even though the models are only trained on error presence and error class, not error location. In total, we summarize our contributions as:\\n\\n\u2022 We introduce the runtime error prediction task and a large accompanying dataset, providing runtime error annotations for millions of competition Python programs.\\n\u2022 We demonstrate that IPA-GNN architectures are practical for the complexity of real programs by scaling them to handle competition programs, and there we find they outperform generic models.\\n\u2022 We demonstrate that external resource descriptions, such as Japanese or English descriptions of stdin, can be leveraged to improve performance on the task across all model architectures.\\n\u2022 We extend the IPA-GNN to model exception handling, resulting in the Exception IPA-GNN, which we find can localize errors even when only trained on error presence and kind, not error location.\\n\\nRelated Work\\nProgram analysis is a rich family of techniques for detecting defects in programs, including static analyses which are performed without executing code (Livshits and Lam, 2005; Xie and Aiken, 2006; Ayewah et al., 2008) and dynamic analyses which are performed at runtime (Cadar et al., 2008; Sen et al., 2005; Godefroid et al., 2005). Linters and type checkers are popular error detection tools that use static analysis. Static analysis (e.g. symbolic execution) does not typically use concrete inputs, while dynamic analysis requires concrete inputs and program execution. Compared with traditional static analysis, our approach is more flexible in its input representation, using a general \u201cresource description\u201d abstraction, which can represent the entire spectrum from concrete inputs to input constraints to missing inputs.\\n\\nExecution-aware models\\nSeveral neural architectures draw inspiration from program interpreters (Graves et al., 2014; \u0141ukasz Kaiser and Sutskever, 2016; Reed and de Freitas, 2016; Graves et al., 2016; Bo\u02c7snjak et al., 2017; Gaunt et al., 2017; Dehghani et al., 2019; Bieber et al., 2020). Our work is most similar to Bieber et al. (2020) and Bo\u02c7snjak et al. (2017), focusing on how interpreters handle control flow and exception handling, rather than on memory allocation and function call stacks. Other works use program execution data directly, training with textual representations of execution traces as inputs (Nye et al., 2021a; Pei et al., 2021; Nye et al., 2021b) or performing execution during synthesis (Chen et al., 2019; Li et al., 2022; Shrivastava et al., 2021). Compared with these, our approach uses weaker supervision, using only runtime error labels for training.\\n\\nFault detection and localization datasets\\nThere has been considerable recent interest in applying machine learning to identifying and localizing faults in source code (Allamanis et al., 2018a). Puri et al. (2021) makes a large dataset of real world programs available, which we build on in constructing our runtime errors dataset. Our dataset (i) is large (it has millions of examples), (ii) exhibits many programming language features, (iii) is written by human authors, and (iv) has error labels from the execution behavior of programs. Previous code datasets only exhibit a subset of these properties: large real-world and competition code datasets (Hendrycks et al., 2021; Li et al., 2022; Kanade et al., 2020; Raychev et al., 2016; Husain et al., 2019; Puri et al., 2021) exhibit properties i, ii, and iii, but not iv, while learning to execute datasets (Zaremba and Sutskever, 2014; Bieber et al., 2020) exhibit property iv but not i, ii, or iii. Recent program synthesis datasets (Chen et al., 2021; Austin et al., 2021) exhibit ii and iii only. Other datasets obtain error labels by injecting synthetic errors (Allamanis et al., 2018b; Karampatsis and Sutton, 2020; Pradel and Sen, 2018) (lacking the realism of iii) or from commit messages (Just et al., 2014; Dinella et al., 2020) (lacking i and iv).\\n\\nFault localization approaches\\nFault localization approaches vary in (i) level of supervision \u2013 weak (error labels) (Li et al., 2019) vs strong (explicit location labels) (Lou et al., 2021; Zhang et al., 2022) \u2013 (ii) level of structural information \u2013 more detailed (e.g. control flow) (Li et al., 2019) vs more abstract (e.g. function call stacks) (Lou et al., 2021) \u2013 and (iii) level of automation \u2013 fully automatic (Li et al., 2019) vs semi-automatic (Lou et al., 2021).\"}"}
{"id": "lLp-C5nTdJG", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Distribution of target classes in the runtime errors dataset.\\n\\n| Error Type         | Train | Validation | Test |\\n|--------------------|-------|------------|------|\\n| No error           | 1881303 | 207162    | 205343 |\\n| AssertionError     | 47    | 4          | 8    |\\n| AttributeError      | 10026 | 509        | 1674 |\\n| EOFError           | 7676  | 727        | 797  |\\n| FileNotFoundError  | 259   | 37         | 22   |\\n| ImportError        | 7645  | 285        | 841  |\\n| IndentationError   | 10    | 0          | 12   |\\n| IndexError         | 7505  | 965        | 733  |\\n| KeyError           | 362   | 39         | 22   |\\n| MemoryError        | 8     | 7          | 1    |\\n| ModuleNotFoundError| 1876  | 186        | 110  |\\n| NameError          | 21540 | 2427       | 2422 |\\n| numpy.AxisError    | 20    | 2          | 3    |\\n| OSError            | 19    | 2          | 2    |\\n| OverflowError      | 62    | 6          | 11   |\\n| re.error          | 5     | 0          | 0    |\\n| RecursionError     | 2     | 0          | 1    |\\n| RuntimeError       | 24    | 5          | 3    |\\n| StopIteration      | 3     | 0          | 1    |\\n| SyntaxError        | 74    | 4          | 3    |\\n| TypeError          | 21414 | 2641       | 2603 |\\n| UnboundLocalError  | 8585  | 991        | 833  |\\n| ValueError         | 25087 | 3087       | 2828 |\\n| ZeroDivisionError  | 437   | 47         | 125  |\\n| Timeout            | 7816  | 1072       | 691  |\\n| Other              | 18    | 8          | 2    |\\n\\n\u2020 denotes the balanced test split.\"}"}
{"id": "lLp-C5nTdJG", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\nInterpreter for which the Exception IPA-GNN is a continuous relaxation\\n\\nInput:\\n\\n1: \\\\( h \\\\leftarrow \\\\emptyset; p \\\\leftarrow 0 \\\\)\\n\\\\hspace{1em} \\\\text{\\\\texttt{\\\\textbf{\\\\texttt{\\\\# Initialize the interpreter.}}}}\\n\\n2: \\\\textbf{while} \\\\( p \\\\not\\\\in \\\\{ \\\\text{\\\\texttt{\\\\texttt{\\\\texttt{n exit}}}}, \\\\text{\\\\texttt{\\\\texttt{\\\\texttt{n error}}}} \\\\} \\\\) \\\\textbf{do}\\n\\n3: \\\\( h \\\\leftarrow \\\\text{Evaluate}(x_p, h) \\\\)\\n\\\\hspace{1em} \\\\text{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\# Evaluate the current statement.}}}}}\\n\\n4: \\\\textbf{if} \\\\ \\\\text{\\\\texttt{\\\\texttt{\\\\texttt{\\\\# Raises}}} \\\\( (x_p, h) \\\\) \\\\text{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\# Raise exception.}}}}}} \\\\textbf{then}\\n\\n5: \\\\( p \\\\leftarrow \\\\text{GetRaiseNode}(x_p, h) \\\\)\\n\\n6: \\\\textbf{else} \\\\textbf{if} \\\\ \\\\text{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\# Branches}}} \\\\( x_p, h \\\\) \\\\text{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{(\\\\# Follow branch.}}}}}}}}}}} \\\\}\\n\\n7: \\\\( p \\\\leftarrow \\\\text{GetBranchNode}(x_p, h) \\\\)\\n\\n8: \\\\textbf{else} \\\\( p \\\\leftarrow p + 1 \\\\)\\n\\\\hspace{1em} \\\\text{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\texttt{\\\\# Proceed to next statement.}}}}}}}}}}}\\n\\n4 A PPRAOE:\\\\ IPA-GNN S AS RELAXATIONS OF INTERPRETERS\\n\\nWe make three modifications to the Instruction Pointer Attention Graph Neural Network (IPA-GNN) architecture. These modifications scale the IPA-GNN to complex code, allow it to incorporate external resource descriptions into its learned executions, and add support for modeling exception handling. The IPA-GNN architecture is a continuous relaxation of the standard interpreter (I) defined by the pseudocode in Algorithm 1, minus the magenta text. We frame these modifications in relation to specific lines of the algorithm:\\n\\n- Scaling the IPA-GNN to complex human-authored code (Section 4.1) and incorporating external resource descriptions (Section 4.2) both pertain to interpreting and executing statement \\\\( x_p \\\\) at Line 3, and\\n- Modeling exception handling adds the magenta text at lines 4-6 to yield a new interpreter (I\u2032) (Section 4.3).\\n\\nWe showcase the behavior of both interpreters I and I\u2032 on a sample program in Figure 1, and illustrate an execution of the same program by a continuous relaxation of interpreter I\u2032 (\\\\( \\\\tilde{I}' \\\\)) alongside it.\\n\\n4.1 EXTENDING THE IPA-GNN TO REAL PROGRAMS\\n\\nBieber et al. (2020) interprets the IPA-GNN architecture as a message passing graph neural network operating on the statement-level control-flow graph of the input program \\\\( x \\\\).\\n\\nEach node in the graph corresponds to a single statement in the program. At each step \\\\( t \\\\) of the architecture, each node performs three steps:\\n\\n- It executes the statement at that node (Line 3, Equation 2);\\n- Computes a branch decision (Lines 7-8, Equation 4);\\n- Performs mean-field averaging over the resulting states and instruction pointers (Appendix C, Equations 10 and 11).\\n\\nUnlike in Bieber et al. (2020) where program statements are simple enough to be uniformly encoded as four-tuples, the programs in our runtime errors dataset consist of arbitrarily complex Python programs.\\n\\n(a) A sample program illustrative of Algorithm 1 behavior, which raises a ValueError if \\\\( x < 0 \\\\) at line 6.\\n\\n(b) The resource description suggests values the program may receive on stdin.\\n\\n(c) Step-by-step execution of the program under interpreters I and I\u2032, and continuous relaxation \\\\( \\\\tilde{I}' \\\\). Distinct colors represent distinct embedding values.\\n\\nFigure 1: A sample program and its execution under discrete interpreters I and I\u2032 (Algorithm 1) and under continuous relaxation \\\\( \\\\tilde{I}' \\\\) of interpreter I\u2032.\"}"}
{"id": "lLp-C5nTdJG", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"We sample three examples at random from the Python Runtime Error dataset validation split, and visualize them here. As in Figure 2, we show instruction pointer heatmaps for both the BASELINE and DOCSTRING Exception IPA-GNN model variants. The values in the tables show the total error contribution predicted by the model (Appendix H) associated with each line of code.\\n\\nIn the heatmaps, the x-axis represents time steps and the y-axis represents nodes, with the last two rows representing the exit node \\\\( n_{exit} \\\\) and the error node \\\\( n_{error} \\\\). Note that for loop statements are associated with two spans in the statement-level control-flow graph, one for construction of the loop iterator, and a second for assignment to the loop variable. Hence we list two indexes for each for loop statement in these figures, and report the total error contribution for the line.\\n\\n### Example\\n\\n**Input:** Input is given from Standard Input in the following format: \\n\\n```plaintext\\nN a_1 a_2 ... a_N\\n```\\n\\n**Constraints:** All values in input are integers. \\n\\n1 <= N, a_i <= 100\\n\\n```python\\n0 N = int(input())\\n2.9 0.2\\n1 A = list(map(int, input().split()))\\n0.8 0.0\\n2 res = 0\\n3.0\\n63.3\\n3,4 for i in range(1, len(A)+1, 2):\\n9.8 6.3\\n5 res += A[i] % 2\\n0.3 0.1\\n6 print(res)\\n0.2 2.2\\n```\\n\\n**Figure 5:** The target error kind is INDEX ERROR, occurring on line 5 (\\\\( n = 5 \\\\)). BASELINE incorrectly predicts NO ERROR with confidence 0.808. DOCSTRING correctly predicts INDEX ERROR with confidence 0.693, but localizes to line 3 (\\\\( n = 2 \\\\)). Both BASELINE and DOCSTRING instruction pointer values start out sharp and become diffuse when reaching the for-loop. The BASELINE instruction pointer value ends with most probability mass at \\\\( n_{exit} \\\\). The DOCSTRING instruction pointer value has a small amount of probability mass reaching \\\\( n_{exit} \\\\), with most probability mass ending at \\\\( n_{error} \\\\).\"}"}
{"id": "lLp-C5nTdJG", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Input: Input is given from Standard Input in the following format: H N A\\n\\nConstraints: 1 \\\\leq H \\\\leq 10^9 \\\\quad 1 \\\\leq N \\\\leq 10^5 \\\\quad 1 \\\\leq A_i \\\\leq 10^4\\nAll values in input are integers.\\n\\n```python\\nH, N, A = list(map(int, input().split()))\\nfor i in A[N]:\\n    if H <= 0:\\n        break\\n    else:\\n        H -= A[i]\\n    if set(A):\\n        print(\\\"Yes\\\")\\n    else:\\n        print(\\\"No\\\")\\n```\\n\\nFigure 6: The target error kind is VALUE ERROR, occurring on line 1 (\\\\(n = 0\\\\)). BASELINE incorrectly predicts INDEX ERROR with confidence 0.319 on line 1 (\\\\(n = 0\\\\)). DOCSTRING correctly predicts VALUE ERROR with confidence 0.880 on line 2 (\\\\(n = 1\\\\)), corresponding to A[n]. Both BASELINE and DOCSTRING instruction pointer values start out sharp and quickly shift most of the probability mass to the error node.\"}"}
{"id": "lLp-C5nTdJG", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Input: n m d\u2081 d\u2082 ... d\u2098\\n\\nTwo integers n and m are given in the first line. The available denominations are given in the second line.\\n\\nConstraints: 1 \\\\leq n \\\\leq 50000  \\n1 \\\\leq m \\\\leq 20  \\n1 \\\\leq \\\\text{denomination} \\\\leq 10000  \\nThe denominations are all different and contain 1.\\n\\n```\\nfrom itertools import combinations\\n\\nn, m = map(int, input().split())\\ncoin = sorted(list(map(int, input().split())))\\n\\nif n in coin:\\n    print(1)\\nelse:\\n    end = n // coin[0] + 1\\n    b = False\\n    for i in range(2, end):\\n        for tup in list(combinations(coin, i)):\\n            if sum(tup) == n:\\n                print(i)\\n                b = True\\n                break\\n        if b: break\\n```\\n\\nFigure 7: The target error kind is NOERROR. BASELINE correctly predicts NOERROR with confidence 0.416. DOCSTRING also correctly predicts NOERROR with confidence 0.823. The BASELINE instruction pointer value makes its largest probability mass contribution to n error at n = 0 and ends up with mass split between n exit and n error. The DOCSTRING instruction pointer value accumulates little probability in n error and ends up with most probability mass in n exit.\"}"}
{"id": "lLp-C5nTdJG", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nStatements authored by real programmers in a competition setting. The language features used are numerous and varied, and so the statement lengths vary substantially, with a mean statement length of 6.7 tokens; we report the full distribution of statement lengths in Figure 4.\\n\\nThe IPA-GNN architecture operates on a program's statement-level control-flow graph, and so requires per-statement embeddings $\\\\text{Embed}(x_n)$ for each statement $x_n$. We first apply either a local or global Transformer encoder to produce per-token embeddings, and we subsequently apply one of four pooling variants to a span of such embeddings to produce a node embedding per statement in a program. In the local approach, we apply an attention mask to limit the embedding of a token in a statement to attending to other tokens in the same statement. In the global approach, no such attention mask is applied, and so every token may attend to every other token in the program. We consider four types of pooling in our hyperparameter search space: first, sum, mean, and max. The resulting embedding is given by $\\\\text{Embed}(x_n) = \\\\text{Pool}_\\\\text{Transformer}(x_n)\\\\text{Span}(x,n)$.\\n\\nFinally we find that the programs in our dataset require as many as 174 steps of the IPA-GNN under the model's heuristic for step limit $T(x)$ (Appendix E). To reduce the memory requirements, we apply rematerialization at each layer of the model (Griewank and Walther, 2000; Chen et al., 2016).\\n\\n4.2 Executing with resource descriptions\\n\\nIn our dataset, each program $x$ may be accompanied by a description of what values stdin may contain at runtime. We convert this description into embedding $d(x)$; the embeddings, vocabulary, and tokenizer used to produce $d(x)$ are shared with those used to produce token embeddings from program source. Analogous to Line 1 of Algorithm 1, IPA-GNN architectures initialize with per-node hidden states $h_0$ and soft instruction pointer $p_{0,n} = \\\\{n = 0\\\\}$. Here $p_t,n$ represents the probability under the model that node $n$ is executing at step $t$.\\n\\nFollowing initialization, each step of an IPA-GNN begins by simulating execution (Line 3) of each non-terminal statement with non-zero probability under the soft instruction pointer to propose a new hidden state contribution $a_{1}(t,n) = \\\\text{RNN}(h_{t-1,n}, \\\\text{Modulate}(\\\\text{Embed}(x_n), d(x), h_{t-1,n}))$.\\n\\nThe text in magenta shows our modification to the IPA-GNN architecture to incorporate external resource descriptions. We consider both Feature-wise Linear Modulation (FiLM) (Perez et al., 2017) and cross-attention (Lee et al., 2019) for the Modulate function, which we define in Appendix D. Modulation allows the IPA-GNN to execute differently at each step conditioned on the information in the resource description, whether it be type information, value ranges, or candidate values. We also consider an additional method: injecting the description as a docstring at the start of the program. This method yields a new valid Python program, and so any model can accommodate it.\\n\\n4.3 Modeling exception handling\\n\\nThe final modification we make to the IPA-GNN architecture is to model exception handling. In Algorithm 1, this corresponds to adding the magenta text to form interpreter $I'$, computing a raise decision (Lines 4-6, Equation 3). We call the architecture that results the Exception IPA-GNN. Whereas execution always proceeds from statement to next statement in interpreter $I$ and in the IPA-GNN, interpreter $I'$ admits another behavior. Under $I'$ and the Exception IPA-GNN, execution may proceed from any statement to a surrounding \u201cexcept block\u201d, if it is contained in a try/except frame, or else to a special global error node, which we denote $n_{\\\\text{error}}$. In the sample execution in Figure 1c we see at step $t = 4$ the instruction pointer $p_{I'}$ updates to $n_{\\\\text{error}} = 8$.\\n\\nWe write that the IPA-GNN makes raise decisions as $b_{t,n,r}(n)$,\\n\\n$$b_{t,n,r}(n) = \\\\text{softmax}\\\\left(D_{\\\\text{dense}}(a_{1}(t,n))\\\\right).$$\\n\\nThe dense layer here has two outputs representing the cases that an error is and is not raised. Here $r(n)$ denotes the node that statement $n$ raises to; $r(n) = n_{\\\\text{error}}$ if $n$ is not contained in a try/except frame, and $b_{t,n,n'}$ denotes the probability under the model of execution transitioning from $n$ to $n'$. 5\"}"}
{"id": "lLp-C5nTdJG", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Next the model makes soft branch decisions in an analogous manner; the dense layer for making branch decisions has distinct weights from the layer for making raise decisions.\\n\\n\\\\[ b_{t,n,n}^1, b_{t,n,n}^2 = (1 - b_{t,n,r}(n)) \\\\cdot \\\\text{softmax} \\\\text{Dense}(a_{(1)}t,n). \\\\] (4)\\n\\nThe text in magenta corresponds to the \u201celse\u201d at Line 6. The model has now assigned probability to up to three possible outcomes for each node: the probability that \\\\( n \\\\) raises an exception \\\\( b_{t,n,r}(n) \\\\), the probability that the true branch is followed \\\\( b_{t,n,n}^1 \\\\), and the probability that the false branch is followed \\\\( b_{t,n,n}^2 \\\\). In the common case where a node is not a control node and has only a single successor, the probability of reaching that successor is simply \\\\( 1 - b_{t,n,r}(n) \\\\).\\n\\nFinally, we assign each program a step limit \\\\( T(x) \\\\) using the same heuristic as Bieber et al. (2020), detailed in Appendix E. After \\\\( T(x) \\\\) steps of the architecture, the model directly uses the probability mass at \\\\( n_{\\\\text{exit}} \\\\) and \\\\( n_{\\\\text{error}} \\\\) to predict whether the program raises an error, and if so it predicts the error type using the hidden state at the error node. We write the modified IPA-GNN\u2019s predictions as\\n\\n\\\\[ P(\\\\text{no error}) \\\\propto p_{T(x),n_{\\\\text{exit}}} \\\\] and \\\\[ P(\\\\text{error}) \\\\propto p_{T(x),n_{\\\\text{error}}}, \\\\] with (5)\\n\\n\\\\[ P(\\\\text{error} = k | \\\\text{error}) = \\\\text{softmax} \\\\text{Dense}(h_{T(x),n_{\\\\text{error}}}). \\\\] (6)\\n\\nWe train with a cross-entropy loss on the class predictions, treating \u201cno error\u201d as its own class.\\n\\n4.4 UNSUPERVISED LOCALIZATION OF ERRORS\\n\\nSince the Exception IPA-GNN makes soft decisions as to when to raise an exception, we aggregate these soft decisions to obtain the model\u2019s prediction for where a program raises an error. We use this to evaluate the model\u2019s localization accuracy despite training without error locations as supervision.\\n\\nFor programs that lack try/except frames, we compute the localization predictions of the model by summing, separately for each node, the contributions from that node to the error node across all time steps. This gives an estimate of \\\\( \\\\text{exception provenance} \\\\) as\\n\\n\\\\[ p(\\\\text{error at statement } n) = \\\\sum_t p_{t,n} \\\\cdot b_{t,n,n_{\\\\text{error}}}. \\\\] (7)\\n\\nFor programs with a try/except frame, we must trace the exception back to the statement that originally raised it. To do this we calculate a recurrence as detailed in Appendix H.\\n\\n5 EXPERIMENTS\\n\\nIn our experiments we evaluate the following research questions:\\n\\nRQ1: How does the adaptation of the IPA-GNN to realistic code compare against existing static analysis and against standard architectures like GGNN, LSTM, and Transformer? (Section 5.1)\\n\\nRQ2: What is the impact of including resource descriptions? What methods for incorporating them work best? (Section 5.2)\\n\\nRQ3: How interpretable are the soft instruction pointer values in the Exception IPA-GNN for localizing errors? How does unsupervised localization with the Exception IPA-GNN compare to alternative unsupervised localization approaches based on multiple instance learning? (Section 5.3)\\n\\n5.1 EVALUATION OF IPA-GNN AGAINST BASELINES\\n\\nWe describe the experimental setup for our first experiment, comparing the IPA-GNN architectures with Transformer (Vaswani et al., 2017), GGNN (Li et al., 2017), and LSTM (Hochreiter and Schmidhuber, 1997) baselines. In all approaches, we use the 30,000 token vocabulary constructed in Appendix A, applying Byte-Pair Encoding (BPE) tokenization (Sennrich et al., 2016) to tokenize each program into a sequence of token indices. The Transformer operates on this sequence of token indices directly, with its final representation computed via mean pooling. For all other models (GGNN, LSTM, IPA-GNN, and Exception IPA-GNN), the token indices are first combined via a masked (local) Transformer to produce per-node embeddings, and the model operates on these per-node embeddings as in Section 4.1. Following Bieber et al. (2020) we encode programs for a GGNN using six edge types, and use a two-layer LSTM for the LSTM baseline and in all IPA-GNN variants.\"}"}
{"id": "lLp-C5nTdJG", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Error classification and error localization results on the balanced test set with and without resource descriptions (R.D.).\\n\\n| Model          | Accuracy (W. F1) | Localization (%) |\\n|----------------|------------------|------------------|\\n| baseline       | 58.8             | 33.0             |\\n| LSTM           | 66.1             | 48.2             |\\n| GGNN           | 62.8             | 50.8             |\\n| Transformer    | 63.6             | 50.8             |\\n| IPA-GNN        | 68.3             | 50.8             |\\n| IPA-GNN + DocString | 68.7         | 64.7             |\\n| IPA-GNN + FiLM | 68.3             | 64.5             |\\n| IPA-GNN + CrossAttention | 68.7 | 68.8             |\\n\\nIn order to compare against the capabilities of a standard static analysis setup, we also consider a baseline based on pylint. For this baseline, we map a subset of the findings that pylint can identify to runtime error classes that they can indicate. The baseline predicts an error class if pylint identifies a corresponding finding. The purpose of this baseline is to consider a standard tool used by Python developers and see how it is performing on the task. We provide further details in Appendix G.\\n\\nFor each neural approach, we perform an independent hyperparameter search using random search. We list the hyperparameter space considered and model selection criteria in Appendix E. The models are each trained to minimize a cross-entropy loss on the target class using stochastic gradient descent for up to 500,000 steps with a mini-batch size of 32. In order to more closely match the target class distribution found in the balanced test set, we sample mini-batches such that the proportion of examples with target \u201cno error\u201d and those with an error target is 1:1 in expectation. We evaluate the selected models on the balanced test set, and report the results in Table 2a (see rows without check marks). Weighted F1 score (W. F1) performs a weighted average of the per-class F1 scores by class frequency, and weighted error F1 score (E. F1) does the same while restricting consideration to those examples with a runtime error.\\n\\nWe perform additional evaluations using the same experimental setup but distinct initializations to compute measures of variance, which we detail in Appendix F.\\n\\nRQ1: The interpreter-inspired architectures show significant gains over the pylint, LSTM, GGNN and Transformer baseline approaches on the runtime error prediction task. We observe that the pylint baseline can make incorrect predictions because it correctly identifies an issue in the code under analysis when that code does not result in a runtime error in our dataset; pylint's lower performance on runtime error prediction is not evidence against pylint's performance for its intended use cases. We attribute the interpreter-inspired architectures' relative success over other neural architectures to their inductive bias toward mimicking program execution.\\n\\n5.2 Incorporating Resource Descriptions\\n\\nWe next evaluate methods of incorporating resource descriptions into the models. For each architecture we apply the docstring approach of processing resource descriptions of Section 4.2. This completes a matrix of ablations, allowing us to distinguish the effects due to architecture change from the effect of the resource description. We follow the same experimental setup as in Section 5.1, and show the results again in Table 2a (compare rows with check marks to those without).\\n\\nWe also consider the FiLM and cross-attention methods of incorporating resource descriptions into the IPA-GNN. Following the same experimental setup again, we show the results of this experiment in Table 3. Note that the best model overall by our model selection criteria on validation data was the IPA-GNN with cross-attention, though the Exception IPA-GNN performed better on test.\\n\\nRQ2: Across all architectures, the results show external resource descriptions improve performance on the runtime error prediction task. On the IPA-GNN architectures, we see further improvements.\"}"}
{"id": "lLp-C5nTdJG", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: A comparison of early and late fusion methods for incorporating external resource description information into interpreter-inspired models.\\n\\nBy considering architectures that incorporate the resource description directly into the execution step of the model, but these gains are inconsistent. The pylint baseline is unable to incorporate resource descriptions. Critically, using any resource description method is better than none at all.\\n\\nTo understand how the resource descriptions lead to better performance, we compare in Figure 2 the instruction pointer values of two Exception IPA-GNN models on a single example (shown in Table 4). The model with the resource description predicts that the `input()` calls will read input beyond the end of the stdin stream. In contrast, the model without the resource description has less reason to suspect an error would be raised by those calls. The descriptions of stdin in our runtime errors dataset also frequently reveal type information, expected ranges for numeric values, and formatting details about the inputs. We visualize additional examples in Appendix K.\\n\\n5.3 Interpretability and Localization\\n\\nWe next investigate the behavior of the Exception IPA-GNN model, evaluating its ability to localize runtime errors without any localization supervision. In unsupervised localization, the models predict the location of the error despite being trained only with error presence and kind supervision.\\n\\nMultiple Instance Learning Baselines\\n\\nUnsupervised localization may be viewed as multiple instance learning (MIL) (Dietterich et al., 1997). Consider the subtask of predicting whether a particular line contains an error. In an $n$-line program, there are $n$ instances of this subtask. The available supervision only indicates if any one of these subtasks has an error, but not which one. By viewing each instance as a bag of subtasks, we have cast the problem as MIL.\\n\\nUsing this view, we introduce two variations on the Transformer architecture as multiple instance learning baselines. The first is the \u201cLocal MIL Transformer\u201d, in which each statement in the program is encoded individually, as in the local node embeddings computation of Section 4.1. The second is the \u201cGlobal MIL Transformer\u201d, in which all tokens in the program may attend to all other tokens in the Transformer encoder. In both cases, the models make per-line predictions, which are aggregated to form an overall prediction as defined in Appendix I.\"}"}
