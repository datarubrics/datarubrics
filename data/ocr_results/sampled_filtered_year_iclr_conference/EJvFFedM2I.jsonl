{"id": "EJvFFedM2I", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nTRAM: BENCHMARKING TEMPORAL REASONING FOR LARGE LANGUAGE MODELS\\n\\nAnonymous authors\\nPaper under double-blind review\\n\\nABSTRACT\\nReasoning about time is essential for understanding the nuances of events described in natural language. Previous research on this topic has been limited in scope, characterized by a lack of standardized benchmarks that would allow for consistent evaluations across different studies. In this paper, we introduce TRAM, a temporal reasoning benchmark composed of ten datasets, encompassing various temporal aspects of events such as order, arithmetic, frequency, and duration, designed to facilitate a comprehensive evaluation of the temporal reasoning capabilities of large language models (LLMs). We conduct an extensive evaluation using popular LLMs, such as GPT-4 and Llama2, in both zero-shot and few-shot learning scenarios. Additionally, we employ BERT-based models to establish the baseline evaluations. Our findings indicate that these models still trail human performance in temporal reasoning tasks. It is our aspiration that TRAM will spur further progress in enhancing the temporal reasoning abilities of LLMs.\\n\\nINTRODUCTION\\nTemporal reasoning is fundamental for humans to understand the world and distinguish between everyday events. For instance, when given the activities \u201cwatching a movie\u201d and \u201cwatching a sunset\u201d, we intuitively recognize that, though both are time-bound, a movie typically lasts longer than a sunset. Moreover, while movies can be watched repeatedly, sunsets transpire once a day. Such innate comprehension isn't just about sequencing events or understanding durations; it extends to more intricate aspects of time, allowing us to make sense of complex narratives and the causality of events. Despite advancements in natural language processing (NLP) and the advent of large language models (Min et al., 2021; Zhao et al., 2023; Wang et al., 2023), mastering temporal reasoning remains a significant challenge due to its intricate nature, the variability of temporal expressions, and the need for contextual understanding.\\n\\nRecent works in temporal reasoning (TeR) mainly focus on time-sensitive question-answering (Zhou et al., 2019; Chen et al., 2021; Dhingra et al., 2022; Tan et al., 2023). These studies consistently show that, despite significant advancements in NLP, current language models still fall short of human-level performance in this domain. While they highlight various aspects of temporal elements, both explicitly and implicitly, such as order, duration, and time-event relations, many intricate facets of TeR, like understanding temporal narratives and temporal causality, remain less explored. Notably, none of these works have tackled broad aspects of TeR within a unified framework.\\n\\nTo facilitate research in this direction, we present the Temporal Reasoning for large language models benchmark (or TRAM for short), a collection of ten temporal reasoning tasks. These tasks range from foundational understanding (e.g., duration, frequency) to advanced temporal interpretations and computations (e.g., arithmetic, causality). Each task consists of one or more subtasks, all of which are specifically crafted to assess a model's TeR capabilities across varying levels of understanding and difficulty. In total, our benchmark includes 38 distinct subtasks. TRAM incorporates existing natural language understanding datasets, human-crafted templates and questions, web sources, and program generation, comprising a total of 526.7k questions. Answers have been derived through a combination of expert annotations and programmatic generation. Distinct from previous work on temporal reasoning and in alignment with (Hendrycks et al., 2020), our questions are not designed as generative tasks. Instead, they are formatted as straightforward multiple-choice tests, a format more suitable for evaluating LLMs.\"}"}
{"id": "EJvFFedM2I", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Foundational Temporal Understanding Tasks\\n\\nIn foundational temporal understanding, LLMs encounter several distinct challenges. Firstly, **Assumption Bias** is evident when models over-rely on patterns from their training, often neglecting cultural or individual variations. Next, **Temporal Descriptor Misinterpretation** occurs when models misinterpret terms, such as perceiving \u201coften\u201d as a daily event instead of a possible weekly occurrence. Event Ambiguity presents another challenge, where events can be described in ways that allow for multiple interpretations, requiring models to select the most suitable one based on context. Lastly, **Contextual Misjudgment** is when models either miss or misinterpret explicit temporal clues, leading to errors in their reasoning.\\n\\nTemporal Interpretation and Computation Tasks\\n\\nIn computational and interpretable temporal reasoning, LLMs encounter various challenges. Firstly, **Calculation Slips** highlight instances where models often make calculation mistakes like inappropriate handling of time carries. Following this, **Descriptor Confusion** arises when models misalign qualitative terms such as \u201cseldom\u201d or \u201cfrequently\u201d with their quantitative meanings. **Resolution Misalignment** represents the struggle models face with vague time references, such as deciphering the exact duration from terms like \u201cin a while\u201d. Lastly, **Temporal Notation Misinterpretation** occurs when models confuse time formats, for example, mixing up AM with PM or not differentiating between 24-hour and 12-hour representations.\\n\\nAdvanced Temporal and Conceptual Understanding Tasks\\n\\nIn advanced temporal reasoning tasks, LLMs frequently encounter certain pitfalls. Among the most prevalent is **Implicit Oversights**, where models overlook subtle but crucial temporal indications, resulting in inaccurate conclusions. Also, they may face **Relation Simplification**, wherein complex temporal interplays between events are either misunderstood or overly simplified. LLMs might also fall into the trap of **Narrative Bias**, where they overly depend on familiar story patterns, prioritizing recognized sequences over fresh interpretations. Lastly, **Overgeneralization** becomes evident when models incorrectly apply broad temporal conventions to specific situations, leading to misunderstandings when scenarios diverge from the norm.\\n\\n**ETASGLOSSARYDEFINITIONS**\\n\\nIn this section, we provide a glossary with definitions of all tasks and subtasks encompassed within our TRAM benchmark for clarity. In our actual dataset formatting, the subcategory (if a task comprises multiple subtasks) or source (if a single subtask is sourced from an existing dataset) is marked for verification and convenient lookup.\\n\\n**Ordering**: Chronological arrangement of events.\\n- **Causality**: Logical sequencing of events based on general knowledge.\\n- **Facts**: Accurate ordering of historical events based on factual information.\\n\\n**Frequency**: Determination of how often events occur over time.\\n- **Causality**: Assessment of event occurrence rates based on general knowledge.\\n- **Contextual**: Frequency information extraction from passages.\\n- **Application**: Inference of time intervals and event frequencies.\\n- **Computation**: Calculation of event occurrences and intervals.\\n- **Comparison**: Differentiation of event frequencies in various contexts.\\n- **Facts**: Identification of periodically occurring events.\\n\\n**Duration**: Determination of the length of events or time periods.\\n- **Causality**: Evaluation of time spans in everyday life scenarios.\\n- **Contextual**: Duration information extraction from passages.\\n- **Analogy Inference**: Discernment of relative time spans through contextual comparison.\\n- **Computation**: Calculation of event lengths.\\n- **Direct Comparison**: Straightforward assessment of event durations in a given set.\"}"}
{"id": "EJvFFedM2I", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We provide several representative examples sourced from existing datasets, allowing for a comparison between the original sources and our curated datasets. Specifically, Table 9 and Table 10 demonstrate the transformation of original Yes/No binary questions from the MCTACO dataset into our frequency and ordering tasks in MCQ formats, respectively. Meanwhile, Table 11 shows the transformation of original short-answer questions from the SQuAD dataset into our duration task. Our benchmark combines the strengths of existing benchmarks with extensive manual effort, including the addition of distracting or confusing options, the filtering out of irrelevant questions for quality control, and the reformulation of problems, thereby setting a new standard for assessing temporal reasoning in LLMs.\"}"}
{"id": "EJvFFedM2I", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 9: Comparison of source (MCTACO) and curated question in TRAM for the Frequency task.\\n\\n| Source Dataset (MCTACO) | Curated Dataset (TRAM) |\\n|-------------------------|------------------------|\\n| Question:               |                        |\\n| Allan crouched over his desk once more, pen in hand and mind blank. How often does Allan crouch over his desk? | Question: Allan crouched over his desk once more, pen in hand and mind blank. How often does Allan crouch over his desk? |\\n| Options/Answers:        |                        |\\n| \u2022 Once a second - No    | Options:               |\\n| \u2022 Once two years ago - No | \u2022 (A) Every day        |\\n| \u2022 Every day - Yes       | \u2022 (B) Several times per second |\\n| \u2022 Several times per second - No | \u2022 (C) Once a second |\\n| \u2022 Daily - Yes           | Answer:                |\\n\\n**Commentary:**\\n- Binary Yes/No format, simple frequency assessment.\\n- Transition to an MCQ format enriches the question's complexity by offering closely related alternatives.\\n\\n## Table 10: Comparison of source (MCTACO) and curated question in TRAM for the Ordering task.\\n\\n| Source Dataset (MCTAC0) | Curated Dataset (TRAM) |\\n|-------------------------|------------------------|\\n| Question:               |                        |\\n| Church is brought back to life, but is an evil shell of himself. What did Church do next? | Question: Church is brought back to life, but is an evil shell of himself. What did Church do next? Is \\\"took a nap\\\" possible? |\\n| Options/Answers:        |                        |\\n| \u2022 \\\"took a nap\\\" - No    | Options:               |\\n| \u2022 \u2022 (A) Undetermined   | \u2022 (B) TRUE             |\\n| \u2022 (C) FALSE            | Answer:                |\\n\\n**Commentary:**\\n- Binary Yes/No format, simple ordering assessment.\\n- Transition to an MCQ format introduces additional ambiguity and uncertainty into the question.\\n\\n## Table 11: Comparison of source (SQuAD) and curated question in TRAM for the Duration task.\\n\\n| Source Dataset (SQuAD) | Curated Dataset (TRAM) |\\n|-------------------------|------------------------|\\n| Question:               |                        |\\n| It was not until January 1518 that friends of Luther translated the 95 Theses... Within two weeks, copies of the theses had spread throughout Germany; within two months, they had spread throughout Europe. How long did it take for the Theses to spread through Europe? | Question: It was not until January 1518 that friends of Luther translated the 95 Theses... Within two weeks, copies of the theses had spread throughout Germany; within two months, they had spread throughout Europe. How long did it take for the Theses to spread through Europe? |\\n| Options/Answers:        |                        |\\n| \u2022 Short answer: Two months | Options:               |\\n| \u2022 45 days               | \u2022 (A) 45 days          |\\n| \u2022 Two months            | \u2022 (B) Two months       |\\n| \u2022 2 days                | \u2022 (C) 2 days           |\\n\\n**Commentary:**\\n- Short-answer format, simple duration assessment.\\n- Transition to an MCQ format introduces additional numerical ambiguity in problems involving multiple numbers.\"}"}
{"id": "EJvFFedM2I", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Major templates for constructing the duration subtasks: multi-step comparison, analogy inference, computation, direct comparison, and facts. The symbols {} serve as placeholders for variable inputs, which can represent both events and times.\\n\\n**Type Template**\\n\\n**Multi-Step Comparison**\\n\\n{} goes on for {}. {} is a third of {}, and {} is as long as {} and {} combined. Which event lasts the longest?\\n\\n{} lasts for {}. {} is double that, but {} is only a third of {}. Which has the most extended duration?\\n\\n{} lasts for {}. {} is half of {}'s duration, and {} is triple the combined length of both {} and {}. Which event has the shortest duration?\\n\\n**Analogy Inference**\\n\\nDuring {}, the audience had a chance to enjoy a long opera, while {} showcased just one act, and {} played only an overture. Which event was the shortest?\\n\\nPeople could indulge in a seven-course meal during {}, a three-course meal in {}, but only an appetizer during {}. Which event was in the middle in terms of duration?\\n\\n{} felt like watching an epic trilogy, {} was more of a feature-length film, and {} was just a brief trailer. Which event was probably the longest?\\n\\nParticipants at {} went through an entire yoga session, {} allowed for a short warm-up, while {} was only a few stretches. Which event was the shortest?\\n\\nDuring {}, attendees could finish a whole board game, in {} they played just a few rounds, and in {} merely set up the pieces. Which event was likely the longest?\\n\\n**Computation**\\n\\nThe duration of {} is {}. If {} is a quarter shorter than {} and {} is four times the length of {} for {}, how long do all the activities last?\\n\\n{} takes {}. If {} is twice that duration minus 10% of {}, and {} is half of the sum of {} and {}, how long is the whole event?\\n\\nThe total duration of {} is four times the time of {} which is {}. If {} is half of {} minus 5% of {} and {} is twice {} plus 15% of {}, how long do the {} and {} together take?\\n\\n**Direct Comparison**\\n\\nWhich event lasted longer: {} or {}?\\n\\nWhich event lasted the longest: {}, {}, or {}?\\n\\n**Facts**\\n\\nHow long did {} last?\\n\\nvia the MAKEINSTANCE tag. We then identified the sentence containing this event as its contextual background. Using the gathered data, we crafted questions such as \\\"What is the relationship between the event 'event 1' and the event 'event 2'?\\\" or analogous questions pertaining to event-time relationships. The context, encompassing both events, was attached to the resulting question to ensure clarity.\\n\\nTemporal NLI\\n\\nTo construct our temporal NLI dataset, we adopted a keyword-based filtering approach from SNLI and MNLI datasets. Recognizing that NLI tasks can often hinge on nuanced temporal cues, we curated a comprehensive set of temporal keywords, as shown in Table 7. This selection was designed to capture a broader range of temporal relationships and nuances. Instances\"}"}
{"id": "EJvFFedM2I", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Major templates used for constructing the ambiguity resolution dataset. The symbols { } serve as placeholders for variable inputs, which can represent both events and times.\\n\\n| Type           | Template                                                                 |\\n|----------------|--------------------------------------------------------------------------|\\n| Short-term Shift | Your plane is supposed to depart at { }. If it's preponed by { }, when is the revised departure? |\\n|                | The meal was promised to be on the table at { }. If it's going to be { } postponed, when can you expect to dine? |\\n|                | You have an exciting date at { }. If you're lagging by { }, when will you probably meet your date? |\\n| Mid-term Shift | The match initially set for { } has now been advanced by { }. Which day is it on now? |\\n|                | Your usual spa day on { } of every week has been postponed { }. When will it be next week? |\\n|                | The weekly town hall usually on { } is delayed by { }. When will it happen? |\\n|                | The town carnival usually during the { } week of { } will now be { }. About which date is it now? |\\n|                | The music fest during the { } week of { } will be held { }. Around which date will it likely be? |\\n| Long-term Shift | The star, predicted to explode in { }, has its explosion postponed by { } years. When is the new prediction for the explosion? |\\n|                | The dynasty which fell in { } had risen to power roughly { } years earlier. When was its establishment? |\\n\\nCalendar Shift: If the date is { } / { } / { } in the { }, what is the date in the { }?\\n\\nInterpretation: You receive a memo with the timestamp { }. When should you be prepared? A festival is being organized { }. When would that be? A note suggests meeting { }. When is this suggesting?\\n\\nCausality: Inspired directly by the style of the COPA dataset, our goal was to capture the intricate weave of cause-and-effect relationships shaped by temporal elements. To this end, we prioritized the inclusion of diverse temporal factors in our dataset, encompassing aspects like seasons, specific times on clocks, special occasions, as well as both long-term and short-term causes and impacts. By meticulously crafting problems with these considerations, we have crafted a rich collection that illuminates the nuanced interplay between time and causality.\\n\\nStorytelling: To identify stories with temporal nuances from the ROCStories and SCT datasets, we employed a keyword-based filtering approach. The choice of our keyword set, as shown in Table 8, was shaped by the distinctive nature of the datasets and the contexts they encompass. In ROCStories, for instance, storytelling often employs varied and colloquial temporal expressions, necessitating a specific focus in our keyword selection. Stories containing at least one term from the list were considered to have temporal aspects and were subsequently selected for further processing.\\n\\nB.2 Examples\\n\\nFor additional examples of various tasks, refer to the following figures: Figure 5 for the ordering task, Figure 6 for the frequency task, Figure 7 for the duration task, Figure 8 for the typical time task, Figure 9 for the ambiguity resolution task, and Figure 10 for the arithmetic task. The advanced temporal understanding group, comprising relation, temporal NLI, causality, and storytelling tasks, which have relatively fewer subtasks, are collectively presented in Figure 11.\"}"}
{"id": "EJvFFedM2I", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Major templates used for constructing the arithmetic dataset. The symbols \\\\{\\\\} serve as placeholders for variable inputs, which are randomly generated by programs.\\n\\n| Category               | Template                                                                 |\\n|------------------------|--------------------------------------------------------------------------|\\n| 24-hour Adjustment     | What is \\\\{\\\\} \\\\(+/\u2212\\\\) \\\\{\\\\}?                                              |\\n| 12-hour Adjustment     | What is \\\\{\\\\}? \\\\(AM/PM+/\u2212\\\\) \\\\{\\\\}?                                      |\\n| Year Shift             | Which year comes \\\\{\\\\} years after \\\\{\\\\}? \\\\(Which year was \\\\{\\\\} years before \\\\{\\\\}?) |\\n| Month Shift            | Which month comes \\\\{\\\\} months after \\\\{\\\\}? \\\\(Which month was \\\\{\\\\} months before \\\\{\\\\}?) |\\n| Date Computation       | What will be the time \\\\{\\\\} years and \\\\{\\\\} months after month \\\\{\\\\}? \\\\(If you add/subtract \\\\{\\\\} days to the date \\\\{\\\\}, what will be the new date?\\\\) \\\\(If you add/subtract \\\\{\\\\} months and \\\\{\\\\} days to the date \\\\{\\\\}, what will be the new date?\\\\) \\\\(If you add/subtract \\\\{\\\\} weeks and \\\\{\\\\} days to the date \\\\{\\\\}, what will be the new date?\\\\) |\\n| Week Identification    | In which week of year \\\\{\\\\} does the date \\\\{\\\\} occur?                    |\\n| Time Zone Conversion   | If it's \\\\{\\\\} in the source zone, what's the date and time in target zone? |\\n| Time Computation       | Convert \\\\{\\\\} days into minutes.                                          |\\n|                        | Convert \\\\{\\\\} minutes into hours.                                         |\\n|                        | Convert \\\\{\\\\} days into hours.                                            |\\n|                        | Convert \\\\{\\\\} seconds into hours.                                         |\\n|                        | Add \\\\{\\\\} minutes \\\\{\\\\} seconds and \\\\{\\\\} minutes \\\\{\\\\} seconds.            |\\n|                        | Subtract \\\\{\\\\} minutes \\\\{\\\\} seconds from \\\\{\\\\} hours \\\\{\\\\} minutes.      |\\n\\nApplication\\n\\nIf a person takes a leave of \\\\{\\\\} days starting from start date, on which day may the leave end? If a person was \\\\{\\\\} years \\\\{\\\\} months old when he joined school and now he is \\\\{\\\\} years \\\\{\\\\} month(s) old, for how long has he been in school? If a person is advised to take medicine every \\\\{\\\\} minutes, how many times will she take the medicine in a day? If a person starts doing homework at \\\\{\\\\} and finishes at \\\\{\\\\} PM, how many hours did he spend on homework? If a flight takes off at \\\\{\\\\} and the duration of the flight is \\\\{\\\\} hours, at what time will it land? If a person walks at a speed of \\\\{\\\\} km/hr and after every km, she takes a rest for \\\\{\\\\} minutes, how many minutes will it take her to cover \\\\{\\\\} km? How long will it take to travel a distance of \\\\{\\\\} kilometers in minutes?\\n\\nB.3 SUBTASK DISTRIBUTIONS\\n\\nAs shown in Table 1, if Problem Types count exceeds 1, then we consider it a task involving multiple subtasks. Figure 12 illustrates the distribution of subtasks for each temporal reasoning task. In the case of causality, two problem types are evenly distributed, each accounting for 50%.\\n\\nC PROMPTS\\n\\nWe utilize both SP and CoT in our experiments with LLMs. For SP, questions are presented directly without the need for additional steps in the prompt. Consider the following example from the storytelling dataset:\\n\\n\u201cWhen I was a boy, my parents used to take my brother and me to the park. We would play, have lunch, and just walk around. One day, when all the picnic benches at the park were occupied, we had one. Two police officers approached and asked if they could join us. Which of the two endings is the most plausible correct ending to the story?\"}"}
{"id": "EJvFFedM2I", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 7: Keywords used for filtering SNLI and MNLI datasets that contain temporal aspects.\\n\\n| Category               | Keywords                                                                 |\\n|------------------------|--------------------------------------------------------------------------|\\n| Explicit References    | today, tomorrow, yesterday, now, soon, later, before, after, day, week, month, year, hour, minute, second, morning, evening, night, noon, midnight, anniversary |\\n| Days of the Week       | Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday             |\\n| Months                 | January, February, March, April, May, June, July, August, September, October, November, December |\\n| Seasons                | spring, summer, fall, autumn, winter                                      |\\n| Periods and Eras       | decade, century, millennium, epoch, era                                   |\\n| General Terms          | annual, biannual, quarterly, hourly, daily, weekly, quarterly, monthly, fortnight, biweekly, bimonthly, semester, trimester |\\n| Relative Terms         | past, future, current, upcoming, recent, lately, ago, in advance, later, previous, next, moment, time, when, while, duration, period, early, earlier |\\n| Implicit Temporal Actions | wait, postpone, delay, reschedule, expire, due, schedule, begin, start, end, finish, commence, conclude, last, extend |\\n| Temporal Transitions and Connectors | until, by the time, as soon as, whenever, since, during, whilst |\\n| Other Temporal Entities | sunset, sunrise, dusk, dawn, midday, eve, annually, eventually, seldom, often, always, never, sometimes, usually, frequently, occasionally, rarely, just, once, still |\\n\\nQ: Mike started his first business, a bakery. Then Mike launched his online cake delivery service.\\n\\n- True/False?\\n  \\n  A. Undetermined\\n  B. False\\n  C. True\\n\\nQ: Arrange the following events in chronological order: (1) Sarah spoke her first words. (2) Sarah learned to ride a bicycle. (3) Sarah took her first steps. (4) Sarah started kindergarten.\\n\\nA. (3), (1), (4), (2)\\nB. (1), (3), (4), (2)\\nC. (4), (3), (2), (1)\\n\\nQ: Is the following sequence of events in the correct chronological order? (1) The Periplus of the Erythrean Sea, a Graeco-Roman manuscript is written. It describes an established Indian Ocean Trade route (2) War of the Polish Succession. (3) East India Company starts operations in Bengal to smuggle opium into China. (4) Viking state in Russia founded under Rurik, first at Novgorod, then Kiev. (5) China conquers the Kingdom of Tungning and annexes Taiwan.\\n\\nA. True\\nB. False\\nC. Undetermined\\n\\nCommonsense Facts\\n\\nQ: Mike started his first business, a bakery. Then Mike launched his online cake delivery service.\\n- True/False?\\n  \\n  A. ... the Kingdom of Tungning and annexes Taiwan.\\n  A. True     B. False    C. Undetermined\\n\\nFigure 5: Example questions on the temporal ordering task.\\n\\n(A) They were there to take my brother and me to the police station.\\n(B) They let us operate the police car lights and siren.\\n\\nFor zero-shot SP, the model is simply prompted with the question: \\\"Given the story 'When I was a boy ... they could join us.' Which of the two endings is the most plausible correct ending to the story? (A) They were... or (B) They let us... The answer (A or B) is: { }.\\n\\nFor few-shot SP, exemplar answers (A or B) are provided alongside the questions.\\n\\nThe overall SP procedure across all tasks can be summarized in three steps:\\n\\n1. Context Provision (if any): Provide any necessary background information or context that may aid the model in understanding the scenario presented in the question.\\n2. Direct Questioning: Pose the question directly to the model.\\n3. Analysis: Analyze the output of the model to determine the correctness of the response.\"}"}
{"id": "EJvFFedM2I", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Keywords used for filtering ROCStories and SCT datasets that contain temporal aspects.\\n\\n| Category          | Keywords |\\n|-------------------|----------|\\n| Time References   | before, after, recently, now, then, earlier, later, today, tonight, yesterday, tomorrow |\\n| Temporal Intervals| soon, nowadays, currently, presently, eventually, ultimately, suddenly, immediately, momentarily, previously, formerly |\\n| Recurring Time Periods | periodically, seasonally, daily, weekly, monthly, annually, biennially |\\n| Fixed Time Periods | century, decade, millennium, year, minute, hour, day, week, month |\\n| Parts of the Day  | morning, noon, evening, night |\\n| Duration & Frequency | duration, instant, temporarily, intermittently, frequently, always, never, sometimes, often, rarely, usually |\\n| Starting Actions  | begin/begins/began, start/starts/started |\\n| Ending Actions    | end/ends/ended, finish/finishes/finished, cease/ceases/ceased, expire/expires/expired, elapse/elapses/elapsed |\\n| Continuing & Delaying | last/lasts/lasted, continue/continues/continued, resume/resumes/resumed, linger/lingers/lingered, postpone/postpones/postponed, procrastinate/procrastinates/procrastinated |\\n\\nQ: How often does ICC Cricket World Cup occur?\\nA. Every 4 years\\nB. Every 5 years\\nC. Once a year\\n\\nQ: Compare the frequency of 'Veterans Day' and 'Solar eclipse'.\\nA. Veterans Day is more frequent\\nB. Solar eclipse is more frequent\\nC. Both events are equally frequent\\n\\nQ: If 'Annual invisibility cloak fashion show' happens yearly, how many times will it occur in 100 years?\\nA. It will occur 100 times\\nB. It will occur 103 times\\nC. It will occur 99 times\\n\\nQ: A species of cicada emerges every 22 years. If they last emerged in 1914, when will they next emerge?\\nA. 1936\\nB. 1939\\nC. 1934\\n\\nFacts\\nComparison\\nComputation\\nApplication\\n\\nQ: There have been six instances as of 2009 in which the exemption process was initiated. Of these six, one was granted, one was partially granted, one was denied and three were withdrawn. Donald Baur, in The Endangered Species Act: law, policy, and perspectives, concluded, \\\"... the exemption provision is basically a nonfactor in the administration of the ESA. A major reason, of course, is that so few consultations result in jeopardy opinions, and those that do almost always result in the identification of reasonable and prudent alternatives to avoid jeopardy.\\\" How many times has the exemption process been used, as of 2009?\\nA. Six\\nB. Eight\\nC. Five\\n\\nReading Comprehension\\n\\nFigure 6: Example questions on the frequency task.\\n\\nIn contrast, for CoT, zero-shot learning takes inspiration from (Kojima et al., 2022) by instructing the model to \\\"Answer the question step by step\\\". For few-shot CoT, we manually craft the step-by-step process for 5-shot exemplars in the development set. The procedure to approach this problem is as follows:\"}"}
{"id": "EJvFFedM2I", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Q: How long did California Gold Rush last?\\nA. 3 years  \\nB. 7 years  \\nC. 10 years\\n\\nQ: For a conference, planning lasts for 9 months. If preparation is double that duration minus 15% of planning and keynote is the sum of planning and preparation divided by 2, what's the entire duration?\\nA. 40.5 months  \\nB. 44.5 months  \\nC. 38.5 months\\n\\nQ: Which event lasted the longest: World War II, U.S. Woman Suffrage Movement, or British Raj in India?\\nA. World War II  \\nB. U.S. Woman Suffrage Movement  \\nC. British Raj in India\\n\\nQ: Art Exhibition has a duration of 2 months. Wine Tasting lasts as long as Art Exhibition and Tech Conference combined, where Tech Conference is triple of Art Exhibition. Which event has the shortest duration?\\nA. Tech Conference  \\nB. Art Exhibition  \\nC. Wine Tasting\\n\\nQ: Lennon accuses his father of leaving him again, and then leaves, after telling his father that he won't live with him anymore. How long does this conversation between Lennon and his father take?\\nA. 10 minutes  \\nB. 10 months  \\nC. 6 weeks\\n\\nQ: In Canada, \u201ccollege\u201d generally refers to a two-year, non-degree granting institution, while \u201cuniversity\u201d connotes a four-year, degree granting institution. Universities may be sub-classified (as in the Macleans rankings) into large research universities with many PhD granting programs and medical schools (for example, McGill University); \u201ccomprehensive\u201d universities that have some PhDs but aren\u2019t geared toward research (such as Waterloo); and smaller, primarily undergraduate universities (such as St. Francis Xavier). How many years does a degree-granting university in Canada spend teaching students?\\nA. Four  \\nB. Five  \\nC. Six\\n\\nFacts\\nComputation\\nMulti-Step \\nComparison\\nDirect \\nComparison\\nCommonsense\\nReading \\nComprehension\\n\\nQ: In what year(s) did \u201cThe Phoenician alphabet is created\u201d occur?\\nA. 1006 BCE  \\nB. 1096 BCE  \\nC. 1050 BCE\\n\\nQ: Then, he pretended he was his father and pretended that he was driving the tractor. What time did he pretend to drive the tractor?\\nA. 1:00 PM  \\nB. at midnight  \\nC. 1:00 AM\\n\\nFacts\\nCommonsense\\nReading \\nComprehension\\n\\nQ: In 1978 Aboriginal writer Kevin Gilbert received the National Book Council award for his book Living Black: Blacks Talk to Kevin Gilbert, a collection of Aboriginal people\u2019s stories, and in 1998 was awarded (but refused to accept) the Human Rights Award for Literature for Inside Black Australia, a poetry anthology and exhibition of Aboriginal photography. In contrast to previous definitions based solely on the degree of Aboriginal ancestry, in 1990 the Government changed the legal definition of Aboriginal to include any: What year was Gilbert awarded for his efforts?\\nA. 1960  \\nB. 1978  \\nC. 2017\\n\\nFacts\\nCommonsense\\nReading \\nComprehension\\n\\n(1) Read the Story Carefully: Understand the main theme, setting, and characters introduced in the story. The dominant theme appears to be a nostalgic recollection of a family day out at a park.\\n\\n(2) Identify Key Elements from the Story: The protagonist recalls a childhood memory. The primary setting is a park. The mood is both casual and reminiscent. Despite the park being crowded, they have a picnic spot. Subsequently, two police officers approach the family.\\n\\n(3) Evaluate Each Proposed Ending: For the first ending, a sudden and unexpected twist is introduced that deviates from the story\u2019s initial light-hearted narrative. This ending lacks context about why they\u2019d be taken to the police station. The second ending maintains the story\u2019s casual and friendly tone, presenting a scenario where the police officers engage positively with the family.\\n\\n(4) Comparison of the Two Endings: Both endings involve the police officers, but the first one introduces a jarring twist without adequate prior context. The second ending aligns more consistently with the story's overarching mood and theme.\\n\\n(5) Conclusion: Given the story\u2019s tone, setting, and characters, the second ending appears more plausible and contextually appropriate.\"}"}
{"id": "EJvFFedM2I", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Q: Your train's regular schedule is 10:53 AM. However, today it's running 58 minutes behind. When will it depart?\\nA. 11:51 AM  \\nB. 11:23 AM  \\nC. 11:38 AM\\n\\nQ: A marathon was supposed to happen this coming Wednesday, but got shifted three days earlier. When will it occur?\\nA. Thursday  \\nB. Sunday  \\nC. Tuesday\\n\\nQ: The dynasty which fell in 1830 had risen to power roughly 90 years earlier. When was its establishment?\\nA. 1742  \\nB. 1745  \\nC. 1740\\n\\nQ: If the date is 9/7/1872 in the Julian, what is the date in the Gregorian?\\nA. 6/6/1871  \\nB. 9/19/1872  \\nC. 5/26/1872\\n\\n---\\n\\n**Figure 9:** Example questions on the ambiguity resolution task.\\n\\nQ: What is 08:24 AM - 07:42?\\nA. 12:42 AM  \\nB. 1:56 AM  \\nC. 10:31 PM  \\nD. 11:34 PM\\n\\nQ: Which year comes 11 years after 1718?\\nA. 1731  \\nB. 1707  \\nC. 1764  \\nD. 1729\\n\\nQ: Which month comes 2 months after December?\\nA. June  \\nB. February  \\nC. January  \\nD. September\\n\\nQ: What will be the time 16 years and 8 months after August 1412?\\nA. June 1430  \\nB. May 1430  \\nC. April 1429  \\nD. June 1431\\n\\nQ: In which week of year 2007 does the date 10-12-2007 occur?\\nA. Week 41  \\nB. Week 28  \\nC. Week 5  \\nD. Week 10\\n\\nQ: If it's 12 PM on May 4, 1904 in Asia/Kolkata, what's the date and time in US/Eastern?\\nA. 6 AM on May 4, 1904  \\nB. 12 PM on May 4, 1904  \\nC. 1 AM on May 4, 1904  \\nD. 11 AM on May 4, 1904\\n\\nQ: Subtract 1 minute 32 seconds from 1 hour 22 minutes.\\nA. 77 minutes 25 seconds  \\nB. 90 minutes 38 seconds  \\nC. 70 minutes 18 seconds  \\nD. 80 minutes 28 seconds\\n\\nQ: If a girl is advised to take medicine every 139 minutes, how many times will she take the medicine in a day?\\nA. 12  \\nB. 11  \\nC. 8  \\nD. 10\\n\\n---\\n\\n**Figure 10:** Example questions on the arithmetic task.\\n\\n---\\n\\nAfter defining the step-by-step procedure, we employ it to steer the model's thought process. This structured methodology better prepares the model to reason through the question and formulate a well-considered answer, thereby providing a distinct advantage over the SP method. We structure our prompt as follows: \\\"Begin by reading the story carefully, ensuring you fully understand its main theme, setting, and the characters. {Immediate analysis}. Subsequently, identify the key elements of the story. {Immediate analysis}. Assess each proposed ending within the context of the narrative. {Immediate analysis}. Compare the two endings, highlighting any thematic or tonal discrepancies. {Immediate analysis}. Conclude by determining which ending appears more plausible, offering a rationale for this selection. {Immediate analysis}.\u201d\\n\\nIn general, the CoT procedure across all temporal reasoning tasks is as follows: (1) Understanding Context: Begin by reading the provided data, statement, or story attentively. Understand the overarching theme, objectives, or the problem\u2019s primary ask. (2) Key Elements Extraction: Identify and highlight crucial elements, specifics, or characters. This could mean different things for different tasks - key events in a story, terms in a mathematical problem, or clauses in a statement. (3) Evaluation: Assess the core objective of the problem in its context. This could be understanding the chronology for ordering, assessing frequency, gauging durations, or even understanding the logical or causal flow in more complex problems. (4) Analysis and Comparison: If there are multiple options or scenarios presented, conduct a deep analysis. Compare, contrast, and evaluate based on the preceding steps. (5) Reasoned Conclusion: Conclude with a structured answer or resolution to the problem, ensuring that the decision-making process aligns with the evidence or data presented. In\"}"}
{"id": "EJvFFedM2I", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Q: It added that the Ministry of Economic Affairs and Finance was assigned to draw up practical procedure for the ceding, while the Ministry of Welfare and Social Security would be responsible for identifying the beneficiaries in two months. What is the relationship between the event 'added' and the event 'ceding'?\\n\\nA. IS_INCLUDED\\nB. SIMULTANEOUS\\nC. AFTER\\n\\nQ: Premise: Two guys playing football on a campus green. Hypothesis: They are practicing before the big game tomorrow\\n\\nA. Entailment\\nB. Neutral\\nC. Contradiction\\n\\nQ: The seasons changed from summer to autumn. What's the more plausible RESULT?\\n\\nA. People evacuated their homes.\\nB. Leaves fell from the trees.\\n\\nQ: There is a huge clock in my living room. I turned the clock back one hour for daylight savings. My wife also turned the clock back one hour for daylight savings. Our 2 kids each turned the clock back one hour for daylight savings. Which of the two endings is the most plausible correct ending to the story?\\n\\nA. Then we wondered why it got so dark so early.\\nB. The kids were not happy.\"}"}
{"id": "EJvFFedM2I", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To gain deeper insight into the temporal reasoning challenges posed by TRAM, we extensively evaluated several popular language models. This includes BERT (Kenton & Toutanova, 2019), RoBERTa (Liu et al., 2019), and recent LLMs such as Llama2 (Touvron et al., 2023), PaLM2 (Anil et al., 2023), GPT-3.5, and GPT-4 (OpenAI, 2023). We used limited training data to fine-tune BERT-style models. In contrast, the other models were evaluated through either zero-shot or few-shot standard prompting, as well as chain-of-thought prompting. Our findings show that GPT-4 outperforms in most tasks, reaching an average accuracy of up to 87.8%. However, for certain tasks, there are marked performance disparities among the models. Despite the impressive results of GPT-4, it trails human proficiency by roughly 10%, highlighting significant room for LLMs to further improve their temporal reasoning abilities. Manual error analysis revealed that models particularly struggle with nuanced understanding and interpreting implicit cues across all task categories.\\n\\nIn summary, our contributions are threefold:\\n\\n1. We introduce TRAM, a comprehensive collection of ten distinct temporal reasoning tasks presented in a multiple-choice question format. Ranging from foundational temporal concepts to intricate temporal interpretations, TRAM serves as a unified framework to assess the temporal reasoning capabilities of LLMs.\\n\\n2. We conduct extensive experiments on TRAM, evaluating leading language models including BERT-style models and contemporary LLMs such as Llama2, PaLM2, GPT-3.5, and GPT-4. Our results reveal that even the most advanced LLM falls short of human-level performance, underscoring the opportunities for continued research in this area.\\n\\n3. Through manual error analysis of results from TRAM, we highlight the consistent challenges in temporal reasoning faced by current LLMs. Specifically, nuanced comprehension and decoding of implicit temporal cues remain challenging for even advanced models, emphasizing the need for further research to improve the capabilities of LLMs in understanding and reasoning about time.\\n\\nOur proposal for a comprehensive temporal reasoning benchmark builds on the evolution of datasets in this domain while addressing the lack of a unified system for evaluation. The modern NLP landscape sets the stage for a nuanced evaluation of both BERT-based and LLM paradigms.\\n\\nTemporal Reasoning Benchmarks\\n\\nIn the realm of temporal reasoning, several datasets have emerged to address distinct challenges. Early benchmarks, such as TimeBank (Pustejovsky et al., 2003), were predominantly focused on temporal relations. TempEval-3 (UzZaman et al., 2013) broadened the scope by introducing multiple tasks, which included temporal entity extraction and temporal relation extraction. In recent years, there has been a surge in the development of time-sensitive question-answering datasets like MCTACO (Zhou et al., 2019), Time-sensitive QA (Chen et al., 2021), TEMPLAMA (Dhingra et al., 2022), and TEMPREASON (Tan et al., 2023). However, these datasets often specialize in narrower aspects of temporal reasoning, such as duration, frequency, or event-time relations. In contrast, our benchmark offers a comprehensive scope of temporal reasoning, addressing various levels and dimensions of understanding about time. It aims to provide a more complete representation of TeR challenges than previously available datasets.\\n\\nTraining Paradigms in LLMs\\n\\nIn NLP research, pretraining language models on vast amounts of diverse texts has become standard practice. Through this process, the models encapsulate a broad spectrum of information across various domains. Traditionally, leveraging this pretrained knowledge for downstream tasks primarily involved fine-tuning on task-specific data. BERT-based models like BERT (Kenton & Toutanova, 2019) and RoBERTa (Liu et al., 2019) are representative examples. These models have been applied to a diverse set of tasks, including disease prediction (Zhao et al., 2021), text classification (Wang et al., 2022b), time series analysis (Wang et al., 2022c), and more. However, the introduction of models like GPT-3 (Brown et al., 2020) marked a significant shift away from heavy reliance on extensive task-specific fine-tuning. Instead, the focus has been shifting towards zero-shot and few-shot learning approaches. In these settings, models such as GPT-3 can adapt to new tasks and achieve competitive performance with only a few training examples (Brown et al., 2020). This transition has spurred the development of advanced prompting techniques aimed at enhancing the understanding and reasoning capabilities of LLMs. Some representative prompting techniques include...\"}"}
{"id": "EJvFFedM2I", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ing methods include chain-of-thought prompting (Wei et al., 2022), self-consistency (Wang et al., 2022a), tree-of-thought prompting (Yao et al., 2023), and metacognitive prompting (Wang & Zhao, 2023). These techniques guide LLMs to generalize across tasks, ensuring their versatile deployment across a broad spectrum of NLP challenges. In this work, we establish baseline evaluations by considering both traditional BERT-based models and recent advances in LLMs, specifically including Llama2 (Touvron et al., 2023), PaLM2 (Anil et al., 2023), GPT-3.5, and GPT-4 OpenAI (2023). Through this, we aim to provide a comprehensive understanding of their strengths and limitations in diverse temporal reasoning tasks.\\n\\n3 T\\n\\nASKS AND D\\n\\nATASETS\\n\\nTRAM encompasses ten temporal reasoning tasks, presented as multiple-choice questions (MCQs) across a range of time-related domains. For clarity, we ensure that each question has only one correct answer. The main purpose of TRAM is to spur further research into the advanced temporal reasoning capabilities of LLMs. Overall, these tasks fall under three distinct groups: (1) Foundational Temporal Understanding Tasks: Covering basic temporal comprehension, this group incorporates tasks such as ordering, frequency, duration, and typical time. (2) Temporal Interpretation and Computation Tasks: Centered on the interpretative and computational aspects of time, this group includes tasks like ambiguity resolution and arithmetic. (3) Advanced Temporal and Conceptual Understanding Tasks: Dedicated to exploring intricate temporal relationships and narratives, this category features tasks like relation, temporal NLI, causality, and storytelling. In this work, certain task names, such as \u2018relation\u2019 and \u2018causality\u2019, can have varied interpretations across different contexts. However, they are specifically emphasized for their temporal aspects in this work. Although we might occasionally omit the term \u2018temporal\u2019 for brevity, readers should note that the tasks are centered on time-related elements.\\n\\nIn TRAM, each task is designed with one or more problem types, ensuring diverse representation across data attributes, complexities, and domains. The benchmark includes 526,668 problems in total. For each dataset, we introduce a few-shot development set, with 5 questions per category, and a separate test set for evaluation. Table 1 provides a detailed overview of these tasks, and more details can be found in Appendix B. The majority of tasks employ accuracy as the evaluation metric due to their straightforward MCQ structure. However, for tasks like \u2018relation\u2019 and \u2018temporal NLI\u2019, which exhibit an imbalanced label distribution inherent to their fixed class structure, both accuracy and the F1 score are utilized, even when they are presented as MCQs.\\n\\nTable 1: Overview of tasks included in TRAM. The \u201cData Size\u201d column aggregates totals from both the development and test sets. \u201cK-Way MC\u201d signifies a multiple-choice response format with K options. Amb. Res. denotes Ambiguity Resolution. NLI stands for natural language inference. \u201cSame\u201d indicates the text source is the same as the row above.\\n\\n| Task Data Size | Problem Types | Metrics | Answer Type | Text Sources |\\n|----------------|---------------|---------|-------------|--------------|\\n| Foundational Temporal Understanding Tasks |\\n| Ordering | 29,462 | 2 Acc. | 3-Way MC | MCTACO |\\n| Frequency | 4,658 | 6 Acc. | 3-Way MC | MCTACO, SQuAD |\\n| Duration | 7,232 | 7 Acc. | Same | |\\n| Typical Time | 13,018 | 4 Acc. | Same | |\\n| Temporal Interpretation and Computation Tasks |\\n| Amb. Res. | 3,649 | 5 Acc. | 3-Way MC | |\\n| Arithmetic | 15,629 | 9 Acc. | 4-Way MC | |\\n| Advanced Temporal and Conceptual Understanding Tasks |\\n| Relation | 102,462 | 1 Acc./F1 | 3-Way MC | TempEval-3 |\\n| Temporal NLI | 282,144 | 1 Acc./F1 | 3-Way MC | MNLI, SNLI |\\n| Causality | 1,200 | 2 Acc. | 2-Way MC | COPA, Misc. |\\n| Storytelling | 67,214 | 1 Acc. | 2-Way MC | ROC, SCT |\\n\\n1 Zhou et al., 2019, 2 Rajpurkar et al., 2016, 3 UzZaman et al., 2013, 4 Williams et al., 2018, 5 Bowman et al., 2015, 6 Roemmele et al., 2011, 7 Mostafazadeh et al., 2016, 8 Mostafazadeh et al., 2017\"}"}
{"id": "EJvFFedM2I", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\n3.1 Foundational Temporal Understanding\\n\\nThis group of tasks is fundamental for assessing a model's proficiency in core temporal concepts. For the tasks below, data from the Multiple Choice TemporAl COmmon-sense (MCTACO) dataset incorporates both validation and test sets, while data from the Stanford Question Answering Dataset (SQuAD) dataset includes both training and validation sets. Unless otherwise mentioned, the options for each dataset are generated through a blend of human curation and algorithmic processes, tailored to each specific task. For instance, in the ordering task, correct answers strictly adhere to the accurate chronological sequence of events, while incorrect choices are formed through random permutations. See Figure 1 for example questions of each task.\\n\\nQ: Arrange the following events in chronological order: (1) Brusilov Offensive by Russia. (2) Kamehameha I of the Island of Hawaii defeats the Oahuans at the Battle of Nu'uanu. (3) The Kuomintang, the Chinese nationalist party, is founded. (4) Emperor Claudius dies and is succeeded by his grand nephew Nero. (5) St. Norbert and 29 companions make their solemn vows marking the beginning of the Premonstratensian Order.\\n\\nA. (1), (2), (4), (5), (3)\\nB. (4), (5), (2), (3), (1)\\nC. (3), (1), (2), (4), (5)\\n\\nQ: It is also a love story, between Ace and Tobio, a trans woman. How often do they break up?\\n\\nA. Once\\nB. Always\\nC. Once per week\\n\\nQ: While Yoga Session gave attendees time to plant an entire garden, Jazz Concert was enough to water a few plants, and Board Game Night was merely smelling a flower. Which event was the longest?\\n\\nA. Jazz Concert\\nB. Board Game Night\\nC. Yoga Session\\n\\nQ: Which event typically happens earlier: morning yoga or farmer starting their day?\\n\\nA. Morning yoga\\nB. Farmer starting their day\\nC. Around the same time\\n\\nOrdering (Facts)\\n\\nFrequency (Commonsense)\\n\\nDuration (Analogy Inference)\\n\\nTypical Time (Comparison)\"}"}
{"id": "EJvFFedM2I", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024.\\n\\nMuch time\u201d. Apart from the aforementioned subtasks, all other categories consist of human-curated templates or problems. The analogy inference category assesses the model\u2019s ability to discern durations through analogical reasoning. The computation category tests mathematical precision, where problems often require arithmetic operations to determine event durations. Comparative analysis is examined in two subtasks: direct comparison, which demands straightforward judgments of event durations involving both real and artificial events; and multi-step comparison, which challenges the model to infer and integrate information across sequential statements. Lastly, the facts category primarily draws from Wikipedia, furnishing questions anchored in well-documented historical or contemporary durations.\\n\\nTypical Time\\n\\nThe typical time task is constructed to evaluate a model\u2019s understanding of when events or activities typically occur, segmented into four distinct categories. The commonsense category draws problems from the MCTACO dataset, exploring the model\u2019s innate comprehension of event timings as they manifest in daily scenarios. The extraction method for this subtask is similar to that used for the \u201cfrequency\u201d task. The comparison category, comprising human-curated statements and problems, delves into relative timing. This category involves determining which of two presented scenarios is more temporally typical or discerning which event customarily precedes the other. The facts category, primarily sourced from Wikipedia timelines spanning ancient history to the 21st century, provides the model with specific historical or established events and expects it to identify the precise times or periods associated with them. Lastly, the reading comprehension problem sets source questions from the SQuAD dataset. This category filters problems based on keywords like \u201cat what time\u201d, \u201cwhen did\u201d, and \u201cin what year\u201d, challenging the model to extract specific temporal data from passages.\\n\\n3.2 Temporal Interpretation and Computation\\n\\nThis group of tasks is fundamental in gauging a model\u2019s adeptness at deciphering, processing, and computing temporal information. See Figure 2 for example questions of each task.\\n\\nQ:\\nA historic event is documented to have happened \u2018before you know it\u2019. When did it take place?\\nA. The next day   B. Without hesitation   C. Before long\\n\\nQ:\\nWhat is 00:18 - 23:50?\\nA. 0:28   B. 1:44   C. 22:15   D. 1:35\\n\\nAmbiguity Resolution\\n\\nThe temporal ambiguity resolution task aims to gauge a model\u2019s ability to decipher and resolve uncertainties related to temporal expressions, divided into five subtasks. The interpretation category evaluates the model\u2019s comprehension of ambiguous time-related phrases commonly used in everyday language. The calendar shift subtask necessitates the conversion between different calendar systems, such as the Julian and Gregorian. The long-term shift, mid-term shift, and short-term shift categories challenge the model\u2019s capacity to adjust dates over long (i.e., years), medium (i.e., months, weeks, days), and short (i.e., hours, minutes, seconds) timeframes, respectively. All questions across these categories originate from carefully crafted human templates.\\n\\nArithmetic\\n\\nThe temporal arithmetic task evaluates a model\u2019s capacity to manage calculations related to time, organized into nine distinct subtasks. The application category presents real-world scenarios such as time calculations involving schooling, vacations, homework, and flights. Date computation sets focus on adding or subtracting days from specified dates to determine a new date. Hour adjustment subtasks, divided into 12-hour and 24-hour formats, require the model to calculate time differences or additions. The month shift subtask examines the model\u2019s ability to pinpoint a month that is a certain number of months away from a specified month. The week identification problems determine the exact week number within a year based on a given date. In year shift, the model discerns a year a certain number of years relative to a provided year. Time computation evaluates the model\u2019s proficiency in converting various time units, especially over shorter durations like days, hours, minutes, and seconds. Lastly, the time zone conversion category requires the model to convert times between different zones. Both the question templates and the programs used to formulate answers derive from human expertise.\"}"}
{"id": "EJvFFedM2I", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"REFERENCES\\n\\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.\\n\\nSamuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2015.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\nWenhu Chen, Xinyi Wang, and William Yang Wang. A dataset for answering time-sensitive questions. arXiv preprint arXiv:2108.06314, 2021.\\n\\nBhuwan Dhingra, Jeremy R Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W Cohen. Time-aware language models as temporal knowledge bases. Transactions of the Association for Computational Linguistics, 10:257\u2013273, 2022.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020.\\n\\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, volume 1, pp. 2, 2019.\\n\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199\u201322213, 2022.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\\n\\nBonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Surveys, 2021.\\n\\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 839\u2013849, 2016.\\n\\nNasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pp. 46\u201351, 2017.\\n\\nOpenAI. Gpt-4 technical report, 2023.\\n\\nJames Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, et al. The timebank corpus. In Corpus linguistics, volume 2003, pp. 40. Lancaster, UK, 2003.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. 2019.\\n\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383\u20132392, 2016.\"}"}
{"id": "EJvFFedM2I", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "EJvFFedM2I", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we provide additional details on human participation in our benchmark, including the selection process for experts, verification of their capabilities, and a performance comparison with non-specialists.\\n\\nSelection of Expert Annotators\\n\\nOur selection criteria for expert annotators emphasized a balanced proficiency in both temporal reasoning and quantitative analysis. We included professionals with advanced degrees (M.S. or Ph.D.) in disciplines that offer distinct perspectives on our tasks. This included cognitive science and psychology for qualitative understanding of human temporal cognition, crucial for interpreting more subjective aspects of the tasks. We also involved experts in statistics, mathematics, and computer science to address the quantitative complexities inherent in many of our benchmark tasks. This diverse expertise ensured a comprehensive evaluation of the problems within the TRAM dataset from both qualitative and quantitative angles.\\n\\nExpertise Verification Process\\n\\nTo ensure the high caliber of our expert panel, we implemented a robust screening process. This involved a thorough validation of their educational qualifications and a careful review of their professional and research experience, particularly focusing on time perception and quantitative problem-solving. Additionally, we administered a preliminary assessment composed of one random problem from each subtask, totaling 37 problems. The passing criterion for this assessment was set at an average accuracy rate of more than 92%, allowing a maximum of three incorrect responses. This stringent benchmark was established to guarantee the experts' capability in accurately addressing the complex problems in TRAM.\\n\\nComparison with Unspecialized Individuals\\n\\nIn addition to expert assessments, we conducted a comparative analysis with human non-specialists to provide a broader perspective on human performance. These non-specialists, sourced from Amazon Mechanical Turk, consisted of individuals without specialized training in temporal reasoning or related fields. They were tasked with responding to the same set of 1,900 questions as the experts. This group achieved an overall accuracy rate of 63.5% across all tasks. This comparison not only underlines the proficiency of our expert panel but also offers insights into the general human ability to tackle temporal reasoning challenges, providing a baseline for non-expert performance in such tasks.\\n\\nB. Dataset\\n\\nThis section details the dataset construction process, including human-crafted templates for program question generation and the use of temporal keywords to filter questions from existing datasets. We also provide more example questions for each task in TRAM. Additionally, for tasks comprising multiple subtasks, we provide their distribution. Note that the following templates do not represent the full spectrum of templates we used when constructing the datasets.\\n\\nB.1 Dataset Construction\\n\\nOrdering\\n\\nFor our ordering dataset, the facts problems were derived from actual events extracted from historical timelines on Wikipedia. Specifically, pages such as https://en.wikipedia.org/wiki/Timeline_of_the_18th_century served as our primary data sources. These timelines cover events ranging from ancient history to the 21st century, offering a rich foundation for our dataset. We explored dedicated pages for each available century, ensuring a diverse collection of events across various epochs.\\n\\nFrequency\\n\\nFor the frequency task, three main subtasks are generated based on templates: comparison, computation, and applications. Each template contains placeholders, denoted by {}, to represent both events and times. Table 3 outlines some representative templates for each subtask. The construction processes for other subtasks are detailed in the main paper.\\n\\nDuration\\n\\nFor the duration task, five main subtasks are generated based on templates: multi-step comparison, analogy inference, computation, direct comparison, and facts. Each template contains placeholders, denoted by {}, to represent both events and times. Table 4 outlines some representative templates for each subtask. The construction processes for other subtasks of the dataset are described in the main paper.\"}"}
{"id": "EJvFFedM2I", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Major templates are used for constructing the frequency subtasks: comparison, computation, and applications. The symbols \\\\{\\\\} serve as placeholders for variable inputs, which can represent both events and times.\\n\\n| Category | Template |\\n|----------|----------|\\n| Comparison | Compare the frequency of \\\\{\\\\} and \\\\{\\\\}. |\\n| Computation | If \\\\{\\\\} happens \\\\{\\\\}, how many times will it occur in \\\\{\\\\} years? \\\\{\\\\} appears \\\\{\\\\}. If it was last seen in \\\\{\\\\}, when will it next appear? \\\\{\\\\} appears \\\\{\\\\}. If it took place in \\\\{\\\\}, when did it previously occur? |\\n| Application | If a person's job contract has a renewal every \\\\{\\\\} years, and they started working in \\\\{\\\\} and renewed it \\\\{\\\\} times without gaps, until what year is their current contract valid? A solar eclipse happens at least \\\\{\\\\} times a year. If the first one in \\\\{\\\\} is in \\\\{\\\\}, in which month can we expect the next one? If a plant blooms every \\\\{\\\\} days and it last bloomed on January 1, on what date will it next bloom? A comet passes Earth every \\\\{\\\\} years. If its last appearance was in \\\\{\\\\}, when will it next appear? If a magazine publishes a special edition every \\\\{\\\\} months and the last one was in January, in which month will the next special edition be? A company holds a general meeting every \\\\{\\\\} quarters. If the last one was in Q1 of a year, which quarter will the next meeting be? A species of cicada emerges every \\\\{\\\\} years. If they last emerged in \\\\{\\\\}, when will they next emerge? If a leap year occurs every 4 years and the last one was in \\\\{\\\\}, when is the next leap year? A festival is celebrated every \\\\{\\\\} years. If it was last celebrated in \\\\{\\\\}, when will it next be celebrated? If a building undergoes maintenance every \\\\{\\\\} months and the last maintenance was in January, which month will the next maintenance be? |\\n\\nTypical Time\\nFor the typical time task, we crafted pairs of time-related events to test the model's proficiency in determining \\\"Which statement is more typical in terms of time?\\\" For instance, when presented with statements such as \\\"People often have dinner in the early to late evening\\\" and \\\"People often have dinner in the mid to late afternoon\\\", the model is prompted to recognize which one is more aligned with a conventional behavior. Similarly, it might evaluate statements like \\\"Bars are typically busiest on Friday and Saturday nights\\\" in comparison to \\\"Bars are typically busiest on Sunday and Monday nights\\\". Through these examples, we aim to assess the model's aptitude in discerning standard temporal practices.\\n\\nAmbiguity Resolution\\nFor the ambiguity resolution task, we introduced templates to test the model's proficiency in resolving temporal ambiguities. Additionally, we manually gathered both common and uncommon temporal expressions that might perplex individuals and the model alike, such as \\\"for a coon's age\\\", \\\"when pigs fly\\\", and \\\"in the nick of time\\\". Table 5 presents representative templates for each subtask. Each template contains placeholders, denoted by \\\\{\\\\}, to represent both events and times.\\n\\nArithmetic\\nWe mainly adopted a programmatic generation approach, grounded in meticulously designed templates that focus on specific temporal calculations. These templates encompass a variety of temporal arithmetic tasks, ranging from basic time adjustments to more complex calculations like week identifications and real-world applications. Table 6 shows the major templates we use for constructing the arithmetic datasets. The variable values, denoted by \\\\{\\\\}, are randomly generated by programs. Through these templates, we can generate diverse questions that test a model's proficiency in handling different temporal arithmetic scenarios.\\n\\nRelation\\nTo derive temporal relation questions from the TempEval-3 Silver dataset, we iterated through each temporal link (tlink) to extract the relationship type (relType) and relevant event and time IDs. For each tlink, the associated eventInstanceID provided the eventID, either directly or...\"}"}
{"id": "EJvFFedM2I", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This group of tasks is fundamental in assessing a model's depth of comprehension in time-oriented narratives and in discerning complex conceptual relationships. See Figure 3 for example questions of each task.\\n\\nQ: Israel wants the EU to arrest any Palestinian suspected of smuggling arms through the crossing, while the EU wants its role to be confined to only monitoring and reporting. What is the relationship between the event 'wants' and the event 'reporting'?\\n\\nA. ENDED-BY\\nB. IS_INCLUDED\\nC. IMMEDIATELY AFTER\\n\\nQ: Premise: This morning, no doubt, she would have consulted me on the subject, but she had no chance. Hypothesis: She would have consulted me on the subject this morning if she 'd had the chance.\\n\\nA. Entailment\\nB. Neutral\\nC. Contradiction\\n\\nQ: She noticed that all the wall clocks in the store were set to ten past ten. What is the more plausible CAUSE?\\n\\nA. It is a common display setting for clocks and watches.\\nB. It was ten minutes past ten at that moment.\\n\\nQ: I woke up so late this morning. I was panicked when I saw what time it was. I had to be at work on time. I threw myself together quickly. Which of the two endings is the most plausible correct ending to the story?\\n\\nA. I was able to get a job at a local restaurant.\\nB. I was still thirty minutes late.\\n\\nTemporal Relation\\nThe temporal relation task seeks to assess a model's ability to identify the relationship between two entities involving time, categorized as either an event-to-time or an event-to-event association. Questions are crafted based on the TempEval-3 Silver dataset (UzZaman et al., 2013). The context sentences, which contain the two entities in question, are directly extracted from the original passages. One inherent challenge of this task lies in the subtle nuances among the fixed set of relations. For instance, distinguishing between relations like \\\"BEFORE\\\" and \\\"IMMEDIATELY BEFORE\\\" can be particularly demanding, as they require fine-grained comprehension of temporal sequences. With the predetermined relations from the dataset, the correct relation option is randomized in its placement, while distractor options are chosen from the pool of remaining relations.\\n\\nTemporal NLI\\nThe Temporal NLI task is designed to evaluate a model's ability in natural language inference, with a particular emphasis on statements that involve temporal elements. We source questions from prevalent NLI datasets, including Stanford Natural Language Inference datasets (SNLI) (Bowman et al., 2015) and Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2018). Data from the MNLI dataset includes training and validation sets, while data from the SNLI dataset includes training, validation, and test sets. We select problems based on keywords that capture a range of temporal nuances, such as explicit references (e.g., 'tomorrow', 'later'), months (e.g., 'May', 'October'), seasons (e.g., 'summer', 'winter'), periods (e.g., 'decade', 'century'), and temporal actions (e.g., 'in advance', 'postpone'). Consistent with the original task, the three response options for all questions are: \\\"Entailment\\\", \\\"Neutral\\\", and \\\"Contradiction\\\".\\n\\nCausality\\nThe temporal causality task assesses a model's capability to discern cause-and-effect relationships within scenarios influenced by time. Drawing inspiration from the Choice of Plausible Alternatives (COPA) dataset (Roemmele et al., 2011), we select questions that naturally contain temporal elements such as 'postpone', 'tomorrow', 'summer', and 'clock'. Additionally, we manually craft problems to highlight the temporal nature of COPA-style questions. Each problem presents a situation that revolves around time, followed by a question pinpointing either the most plausible cause or effect of that situation. Both options for these problems are carefully created by hand. For augmentation purposes, we create additional, mirrored instances for each original sample. This approach ensures that for a given question with two options, each option is supported by a uniquely tailored premise, effectively creating a distinct and relevant context for both choices.\\n\\nStorytelling\\nThe temporal storytelling task is designed to assess a model's ability to predict the appropriate ending of stories that emphasize temporal elements. We source questions from the ROCStories (ROC) (Mostafazadeh et al., 2016) and Story Cloze Test (SCT) (Mostafazadeh et al., 2017) datasets. We identify and select stories that contain notable temporal components by filtering them using keywords such as 'now', 'tomorrow', 'future', 'always', and 'postpone', among others. The typical format of the task presents a story comprising four sentences, followed by two potential\"}"}
{"id": "EJvFFedM2I", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"endings. The model is required to choose the most appropriate conclusion for the story. In the case of SCT, which inherently provides two endings for each story, our focus remains on selecting stories with evident temporal aspects. To further enrich our dataset, we take the initial four sentences from the ROC and employ GPT-2 (Radford et al., 2019) to produce an alternate, incorrect ending, initiated with the prompt \u201cunexpectedly\u201d. Subsequently, we filter this augmented data to ensure that stories emphasize the desired temporal themes.\\n\\n4 EXPERIMENTS\\n\\nIn our evaluation, we compare the performance of prevalent LLMs across all datasets and analyze the mistakes they make. We report the best results after multiple runs for each experimental setting.\\n\\n4.1 EXPERIMENTAL SETUP\\n\\nWe evaluate the performance of several well-known language models on the TRAM benchmark, which is organized into two main categories. In the first category, we employ four popular large language models: the open-source Llama-2-13b-chat (Touvron et al., 2023) and the closed-source models PaLM-bison-chat (Anil et al., 2023), GPT-3.5-turbo, and GPT-4 (OpenAI, 2023). Each of these models is accessed using its corresponding API key. Given the constraints of API costs, and following the methodology of (Tan et al., 2023), we assess model performance on 200 examples for each category of each task randomly selected from the test set. For categories with fewer than 200 examples, we utilize all available test examples. For all evaluations, greedy decoding (i.e., temperature = 0) is applied during model response generation. We evaluate each model using two prompting strategies: standard prompting (SP) (Brown et al., 2020; Kojima et al., 2022) and chain-of-thought (CoT) (Wei et al., 2022) prompting. Under both strategies, the models undergo tests in zero-shot and 5-shot settings. In the 5-shot scenario, exemplars are consistently drawn from the development set. Step-by-step answers associated with CoT prompting are obtained through human annotation. More details about prompts can be found in Appendix C.\\n\\nIn the second category, we consider minimal supervision as opposed to traditional fully supervised learning in order to establish baseline evaluations. The rationale behind this decision is driven by the intention to leverage the inherent world knowledge of the models and to ensure an equitable comparison with the previously mentioned LLMs. For this category, we employ four representative BERT-style models, including BERT-base, BERT-large (Kenton & Toutanova, 2019), RoBERTa-base, and RoBERTa-large (Liu et al., 2019). Specifically, for the temporal NLI task, we employ the Sequence Classification variant of BERT and RoBERTa from Huggingface, given its suitability for the task\u2019s structure. However, for the other tasks, we utilize the Multiple Choice variant of BERT and RoBERTa from Huggingface. The data sampling strategy for minimal supervision is structured based on the size of the original dataset. For datasets with around 1k samples, we randomly select 50% of the remaining data after setting aside the test data used for LLM evaluation. For datasets with sizes between 3k and 10k, we select 10%. For those with sizes between 10k and 100k, we sample 2.5%, and for datasets with more than 100k examples, we take 1%. This limited training data is then used for the fine-tuning of models. The same test set is used consistently with LLMs.\\n\\nIn addition to evaluating model performance, multiple expert annotators worked on each problem type for every task in TRAM to better understand human performance. Each expert answered a subset of the 50 questions from each category of every task, which were randomly selected from the test set. Collectively, they tackled about 1,900 questions across TRAM. Further details on human expert annotators and human non-specialists are provided in Appendix A.\\n\\n4.2 OVERALL PERFORMANCE COMPARISON\\n\\nWe compared the performance of different models across ten tasks, as shown in Table 2. There are several key takeaways. First, GPT-4 consistently outperforms other models across the majority of tasks, demonstrating a performance advantage of over 15% compared to other models on average. Second, all LLMs show improved performance in the 5-shot setting compared to the zero-shot setting, as expected. Regarding prompting effectiveness, we note that CoT often results in performance enhancements, which corroborates the findings from (Wei et al., 2022), emphasizing the efficacy of step-by-step prompting in augmenting LLMs\u2019 performance in intricate reasoning tasks. Third, it is\"}"}
{"id": "EJvFFedM2I", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Performance comparison of each model across ten tasks in TRAM. GPT-4 consistently outperforms other models under both zero-shot (0S) and 5-shot (5S) settings across the majority of tasks. Interestingly, the RoBERTa-large model achieves a higher average performance than models with larger architectures, such as Llama2. Human performance serves as an upper bound, illustrating that there still exists room for improvement in LLMs on temporal reasoning tasks. The abbreviations Freq., Dur., Arith., Rel., Caus. refer to frequency, duration, arithmetic, relation, and causality, respectively. All values are percentages. Best model results are highlighted in bold.\\n\\n| Model                  | Order | Freq. | Dur. | Typical Time | Amb. Res. | Arith. | Rel. | NLI | Caus. | Average Acc. | Acc. | Acc. | Acc. | Acc. | Acc. | Acc./F1 | Acc./F1 | Acc. | Acc. | Acc. |\\n|------------------------|-------|-------|------|--------------|-----------|--------|------|-----|------|---------------|------|------|------|------|------|--------|--------|------|------|------|\\n| Random                 |       | 33.3  | 33.3 | 33.3         | 25.0      | 33.3/33.3 | 50.0 | 35.4|    |               | 50.0 |      |      |      |      |        |        |      |      |      |\\n| Llama2 (0S, SP)        | 50.2  | 71.8  | 63.4 | 72.0         | 45.8      | 51.2   | 34.5/32.3 | 62.7/62.2 | 97.5 | 86.5 | 60.8 |\\n| Llama2 (0S, CoT)       | 51.7  | 73.2  | 64.7 | 73.5         | 48.0      | 54.4   | 39.0/37.7 | 66.0/65.7 | 99.3 | 88.2 | 63.4 |\\n| Llama2 (5S, SP)        | 50.7  | 72.2  | 64.0 | 72.8         | 47.0      | 52.6   | 37.0/35.5 | 63.7/63.2 | 98.8 | 87.3 | 62.1 |\\n| Llama2 (5S, CoT)       | 52.5  | 73.7  | 65.3 | 73.8         | 49.6      | 55.2   | 41.0/39.7 | 66.5/65.7 | 99.5 | 88.5 | 64.3 |\\n| PaLM2 (0S, SP)         | 54.2  | 84.2  | 81.9 | 80.5         | 73.2      | 68.0   | 59.0/58.5 | 68.2/69.1 | 99.3 | 91.2 | 73.9 |\\n| PaLM2 (0S, CoT)        | 55.5  | 85.0  | 82.3 | 81.5         | 74.6      | 69.7   | 62.5/62.1 | 69.3/70.1 | 99.5 | 92.0 | 75.3 |\\n| PaLM2 (5S, SP)         | 55.2  | 84.7  | 82.1 | 81.0         | 74.0      | 68.8   | 61.0/60.7 | 68.5/69.4 | 99.3 | 91.5 | 74.7 |\\n| PaLM2 (5S, CoT)        | 56.2  | 85.2  | 82.7 | 81.8         | 75.0      | 70.2   | 63.5/63.3 | 70.3/71.1 | 99.5 | 92.2 | 75.9 |\\n| GPT-3.5 (0S, SP)       | 52.5  | 77.3  | 71.6 | 78.7         | 72.8      | 72.8   | 40.5/39.1 | 73.8/74.2 | 98.8 | 90.5 | 70.2 |\\n| GPT-3.5 (0S, CoT)      | 53.7  | 78.3  | 72.3 | 79.7         | 74.6      | 74.6   | 44.5/43.5 | 75.2/75.7 | 99.5 | 91.7 | 71.9 |\\n| GPT-3.5 (5S, SP)       | 53.2  | 77.8  | 72.0 | 79.2         | 73.4      | 73.7   | 43.0/41.8 | 74.5/75.0 | 99.3 | 91.0 | 71.2 |\\n| GPT-3.5 (5S, CoT)      | 54.5  | 78.5  | 72.7 | 80.0         | 75.0      | 75.0   | 46.5/45.5 | 75.5/75.9 | 99.5 | 91.7 | 72.5 |\\n| GPT-4 (0S, SP)         | 70.3  | 92.5  | 92.3 | 89.5         | 88.6      | 93.6   | 64.0/63.6 | 89.5/89.8 | 99.0 | 95.8 | 85.7 |\\n| GPT-4 (0S, CoT)        | 71.0  | 93.3  | 92.6 | 90.0         | 89.2      | 93.9   | 67.0/66.6 | 90.5/90.8 | 100.0 | 96.3 | 86.8 |\\n| GPT-4 (5S, SP)         | 70.8  | 92.8  | 92.4 | 89.7         | 89.0      | 93.8   | 66.0/65.6 | 90.0/90.3 | 99.5 | 96.0 | 86.3 |\\n| GPT-4 (5S, CoT)        | 71.5  | 93.7  | 93.0 | 90.2         | 89.8      | 94.3   | 69.5/69.1 | 90.7/91.0 | 100.0 | 96.3 | 87.4 |\\n| BERT-base              | 50.0  | 47.3  | 50.0 | 53.0         | 36.6      | 25.9   | 86.5/86.6 | 53.0/53.4 | 81.0 | 79.0 | 58.5 |\\n| BERT-large             | 52.5  | 53.1  | 53.3 | 56.8         | 37.4      | 28.3   | 89.5/89.5 | 59.5/60.1 | 85.0 | 81.3 | 62.2 |\\n| RoBERTa-base           | 50.8  | 54.5  | 51.8 | 55.3         | 37.4      | 26.4   | 87.0/86.8 | 64.5/64.9 | 82.3 | 81.3 | 61.9 |\\n| RoBERTa-large          | 55.5  | 57.7  | 55.4 | 60.0         | 41.0      | 29.1   | 90.0/90.0 | 70.0/70.3 | 88.0 | 84.0 | 65.9 |\\n| Human                  |       | 86.0  | 96.3 | 97.7         | 94.5      | 94.8   | 98.7 | 96.0/96.0 | 92.0/92.4 | 100.0 | 98.0 | 95.2 |\\n\\nIt is notable that RoBERTa-large, despite its size, surpasses the larger Llama2 in average performance. This observation underscores that sheer model size doesn\u2019t always equate to superior performance. Several factors might contribute to this outcome. RoBERTa-large might utilize optimization strategies that are especially beneficial for these tasks. Additionally, inherent features or efficiencies in its architecture might enhance its ability to understand and process temporal cues. Delving deeper into task-specific performance, certain tasks such as ambiguity resolution and arithmetic show considerable variance across models. For LLMs, performance on the arithmetic task varies significantly, ranging from 51.2% to 94.3%. Moreover, BERT and RoBERTa exhibit exceptional performance in the temporal relation task, potentially due to their bidirectional contextual processing and emphasis on token-level relationships. Their attention mechanisms also allow them to discern and prioritize essential segments in sentences indicative of temporal relationships. This contrasts sharply with their average or below-average performance in other tasks. This discrepancy suggests that some models may be equipped with architectures or training methodologies tailored for certain types of reasoning, or that specific tasks require a distinct understanding not universally handled proficiently by all models. Finally, while GPT-4 leads among all the models, human expertise still exceeds it by roughly 10%, highlighting the complexity of these temporal reasoning tasks and indicating room for future improvements in LLMs.\\n\\n4.3 ERROR ANALYSIS\\n\\nTo better understand the mistakes made by models, we manually analyzed instances where a model, whether in a 0-shot or 5-shot setting or under SP or CoT, made an incorrect choice. We prompted the model to explain its decisions, then reviewed these explanations to identify errors, understand the reasons behind them, and categorize them into specific error types.\\n\\nFor this analysis, we focused solely on LLMs, excluding BERT-style models. Figure 4 showcases the prevalent error types and their respective proportions for each task group. Within the foundational temporal understanding tasks, \u201cassumption bias\u201d was the most frequent error, accounting for 32% of all mistakes. In the interpretation and computation tasks, \u201ccalculation slips\u201d dominated, making up 42% of the errors. \u201cImplicit oversights\u201d led in the advanced temporal understanding tasks with a representation of 34%. Detailed descriptions of each error type can be found in Appendix D.\"}"}
{"id": "EJvFFedM2I", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nFigure 4: Error type distribution for three groups of tasks in TRAM. Models often struggle with subtle details and hidden clues across all categories.\\n\\n5. DISCUSSION\\n\\nWe introduce TRAM, a comprehensive benchmark spanning ten diverse tasks, to evaluate the temporal reasoning of LLMs. The contrasting performances across models emphasize the significance of experimental strategies and shed light on the intrinsic challenges. This benchmark serves as a tool for researchers to identify model limitations and guide further advancements in this domain.\\n\\nImplications of TRAM\\n\\nThe introduction of TRAM establishes a new paradigm for probing the temporal reasoning capabilities of LLMs. Unlike previous benchmarks, which often offered fragmented insights into temporal tasks, TRAM provides a comprehensive system. This allows for a unified evaluation of how models comprehend both rudimentary temporal concepts and complex temporal narratives. The differentiation in task complexity within TRAM elucidates the various stages of temporal understanding. In particular, TRAM underscores challenges like decoding implicit temporal cues and navigating intricate temporal relationships, providing a roadmap for future improvements in LLMs in this area.\\n\\nModel Performance and Challenges\\n\\nModel experimental strategies notably influence large language models' temporal reasoning capabilities. The superior performance in the 5-shot setting, compared to zero-shot, underscores the crucial role of context-specific learning in enhancing these models' grasp on temporal aspects. Moreover, the effectiveness of the CoT prompting highlights the potential of specialized strategies in refining their prowess in complex temporal reasoning tasks. However, size doesn't inherently guarantee success. The average performance of RoBERTa-large outperforms the larger Llama2, raising intriguing questions about the balance between model size and efficiency. In addition, varied performance across tasks indicates the challenges of crafting a universally adept model for all TeR problems. This variability, combined with the gap between GPT-4 and human expertise, signals ongoing challenges and the need for nuanced improvements.\\n\\nLimitations\\n\\nWhile TRAM presents a holistic approach to temporal reasoning assessment, we acknowledge its limitations. One primary concern is the subset evaluation of the test set, which may not reflect the full spectrum of LLMs' temporal reasoning capabilities. Furthermore, given the MCQ format, there is a possibility that LLMs could resort to random guessing, rather than genuinely exhibiting temporal reasoning. Such tendencies may mislead the performance evaluation. In addition, textual questions may not capture the entire complexity of temporal reasoning tasks, as real-world scenarios often integrate multi-modal cues such as images and videos.\\n\\nFuture Directions\\n\\nTRAM has initiated a step towards evaluating LLMs' temporal reasoning capabilities, but there are further avenues to explore. Going forward, we will experiment with more test data and refine tailored prompting techniques for each task through iterative testing. Moreover, we plan to expand the benchmark to include varied question formats. For generative tasks, this might encompass short answers and summarization. Even within MCQs, we intend to incorporate questions that may have one or more correct answers, allowing for a more comprehensive evaluation. We also plan to fine-tune existing open-source LLMs on these tasks, such as Llama2. These efforts aim to create tailored LLMs that can better understand and reason about time across various contexts.\"}"}
