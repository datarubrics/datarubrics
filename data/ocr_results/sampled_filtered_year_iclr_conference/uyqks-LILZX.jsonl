{"id": "uyqks-LILZX", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A BSTRACT\\nRecent empirical studies on domain generalization (DG) have shown that DG algorithms that perform well on some distribution shifts fail on others, and no state-of-the-art DG algorithm performs consistently well on all shifts. Moreover, real-world data often has multiple distribution shifts over different attributes; hence we introduce multi-attribute distribution shift datasets and find that the accuracy of existing DG algorithms falls even further. To explain these results, we provide a formal characterization of generalization under multi-attribute shifts using a canonical causal graph. Based on the relationship between spurious attributes and the classification label, we obtain\\nrealizations\\nof the canonical causal graph that characterize common distribution shifts and show that each shift entails different independence constraints over observed variables. As a result, we prove that any algorithm based on a single, fixed constraint cannot work well across all shifts, providing theoretical evidence for mixed empirical results on DG algorithms. Based on this insight, we develop Causally Adaptive Constraint Minimization (CACM), an algorithm that uses knowledge about the data-generating process to adaptively identify and apply the correct independence constraints for regularization. Results on fully synthetic, MNIST, small NORB, and Waterbirds datasets, covering binary and multi-valued attributes and labels, show that adaptive dataset-dependent constraints lead to the highest accuracy on unseen domains whereas incorrect constraints fail to do so. Our results demonstrate the importance of modeling the causal relationships inherent in the data-generating process.\\n\\n1 I N T R O D U C T I O N\\nTo perform reliably in real world settings, machine learning models must be robust to distribution shifts \u2013 where the training distribution differs from the test distribution. Given data from multiple domains that share a common optimal predictor, the domain generalization (DG) task (Wang et al., 2021; Zhou et al., 2021) encapsulates this challenge by evaluating accuracy on an unseen domain. Recent empirical studies of DG algorithms (Wiles et al., 2022; Ye et al., 2022) have characterized different kinds of distribution shifts across domains. Using MNIST as an example, a diversity shift is when domains are created either by adding new values of a spurious attribute like rotation (e.g., Rotated-MNIST dataset (Ghifary et al., 2015; Piratla et al., 2020)) whereas a correlation shift is when domains exhibit different values of correlation between the class label and a spurious attribute like color (e.g., Colored-MNIST (Arjovsky et al., 2019)). Partly because advances in representation learning for DG (Ahuja et al., 2021; Krueger et al., 2021; Mahajan et al., 2021; Arjovsky et al., 2019; Li et al., 2018a; Sun & Saenko, 2016) have focused on either one of the shifts, these studies find that performance of state-of-the-art DG algorithms are not consistent across different shifts: algorithms performing well on datasets with one kind of shift fail on datasets with another kind of shift.\\n\\nIn this paper, we pose a harder, more realistic question: What if a dataset exhibits two or more kinds of shifts simultaneously? Such shifts over multiple attributes (where an attribute refers to a spurious high-level variable like rotation) are often observed in real data. For example, satellite imagery data demonstrates distribution shifts over time as well as the region captured (Koh et al., 2021). To study this question, we introduce multi-attribute distribution shift datasets. For instance, in our Col+Rot-MNIST dataset (see Figure 1), both the color and rotation angle of digits can shift.\"}"}
{"id": "uyqks-LILZX", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a subset of attributes $A_s \\\\subseteq A$, it is always possible to change the type of at least one of those attributes' shifts to create a new data distribution (dataset) where the same constraint will not hold.\\n\\n**Second Claim.** To prove the second claim, suppose that there exists a predictor for $Y$ based on a single conditional independence constraint over its representation, $\\\\psi(\\\\phi(X), A_s, Y)$ where $A_s \\\\subseteq A$.\\n\\nSince the same constraint is not valid across all attribute shifts, we can always construct a realized graph $G$ (and a corresponding data distribution) by changing the type of at least one attribute shift $A \\\\in A_s$, such that $X_c$ would not satisfy the same constraint as $\\\\phi(X)$. Further, under this $G$, $X_c$ would satisfy a different constraint on the same attributes. From Theorem 2.1, all conditional independence constraints satisfied by $X_c$ under $G$ are necessary to be satisfied for a risk-invariant predictor. Hence, for the class of distributions $P_G$, a single constraint-based predictor cannot be a risk-invariant predictor.\\n\\n**Corollary 3.2.** Even when $|A| = 1$, an algorithm using a single independence constraint over $\\\\langle \\\\phi(X), A, Y \\\\rangle$ cannot yield a risk-invariant predictor for all kinds of single-attribute shift datasets.\\n\\n**Proof.** Given a fixed (conditional) independence constraint over a predictor's representation, $\\\\psi(\\\\phi(X), A_s, Y)$, the proof of Theorem 3.1 relied on changing the target relationship type (and hence distribution shift type) for the attributes involved in the constraint. When $|A| = 1$, the constraint is on a single attribute $A$, $\\\\psi(\\\\phi(X), A, Y)$ and the same proof logic follows. From Proposition 3.1, given a fixed constraint, we can always choose a single-attribute shift type (and realized DAG $G$) such that the constraint is not valid for $X_c$. Moreover, under $G$, $X_c$ would satisfy a different conditional independence constraint wrt the same attribute. From Theorem 2.1, since the predictor does not satisfy a conditional independence constraint satisfied by $X_c$, it cannot be a risk-invariant predictor for datasets sampled from $P_G$.\\n\\n**CACM Algorithm**\\n\\nWe provide the CACM algorithm for a general graph $G$ below (Algorithm 1).\\n\\n**Algorithm 1 CACM**\\n\\n**Input:** Dataset $(x_i, a_i, y_i)_{i=1}^n$, causal DAG $G$\\n\\n**Output:** Function $g(x) = g_1(\\\\phi(x)) : X \\\\rightarrow Y$\\n\\n1. $A \\\\leftarrow$ set of observed variables in $G$ except $Y$, $E$ (special domain attribute)\\n2. $C \\\\leftarrow \\\\{\\\\}$  \\\\hspace{1cm} $\\\\triangleright$ mapping of $A$ to $A_s$\\n\\n**Phase I:** Derive correct independence constraints for $A \\\\in A$\\n\\n1. if $(X_c, A)$ are d-separated\\n2. then $X_c \\\\perp \\\\perp A$ is a valid independence constraint\\n3. else if $(X_c, A)$ are d-separated conditioned on any subset $A_s$ of the remaining observed variables in $A \\\\setminus \\\\{A\\\\} \\\\cup \\\\{Y\\\\}$\\n4. then $X_c \\\\perp \\\\perp A | A_s$ is a valid independence constraint\\n5. $C[A] = A_s$\\n\\n**Phase II:** Apply regularization penalty using constraints derived for $A \\\\in A$\\n\\n1. if $X_c \\\\perp \\\\perp A$\\n2. then $\\\\text{RegPenalty}_A = |E| \\\\sum_{i=1}^n \\\\text{MMD}(P(\\\\phi(x)|A_i), P(\\\\phi(x)|A_j))$\\n3. else if $A$ is in $C$\\n4. then $A_s = C[A]$\\n5. $\\\\text{RegPenalty}_A = |E| \\\\sum_{a \\\\in A_s} \\\\sum_{i=1}^n \\\\text{MMD}(P(\\\\phi(x)|A_i, a), P(\\\\phi(x)|A_j, a))$\\n\\n$\\\\text{RegPenalty} = \\\\sum_{A \\\\in A} \\\\lambda_A \\\\text{RegPenalty}_A$\\n\\n$g_1, \\\\phi = \\\\arg \\\\min_{g_1, \\\\phi} \\\\ell(g_1(\\\\phi(x)), y) + \\\\text{RegPenalty}$\"}"}
{"id": "uyqks-LILZX", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Remark.\\n\\nIf $E$ is observed, we always condition on $E$ because of Corollary 3.1.\\n\\nFor the special case of Figure 2, CACM uses the following regularization penalty ($\\\\text{RegPenalty}$) for Independent, Causal, Confounded and Selected shifts:\\n\\n$$\\\\text{RegPenalty}_{\\\\text{ind}} = |E| \\\\sum_{i=1}^{X_i} \\\\sum_{j>i} \\\\text{MMD}(P(\\\\phi(x)|a_i, \\\\text{ind}), P(\\\\phi(x)|a_j, \\\\text{ind}))$$\\n\\n$$\\\\text{RegPenalty}_{\\\\text{cause}} = |E| \\\\sum_{i=1}^{X_y} \\\\sum_{y \\\\in Y} \\\\text{MMD}(P(\\\\phi(x)|a_i, \\\\text{cause}, y), P(\\\\phi(x)|a_j, \\\\text{cause}, y))$$\\n\\n$$\\\\text{RegPenalty}_{\\\\text{conf}} = |E| \\\\sum_{i=1}^{X_y} \\\\sum_{y \\\\in Y} \\\\text{MMD}(P(\\\\phi(x)|a_i, \\\\text{conf}), P(\\\\phi(x)|a_j, \\\\text{conf}))$$\\n\\n$$\\\\text{RegPenalty}_{\\\\text{sel}} = |E| \\\\sum_{i=1}^{X_y} \\\\sum_{y \\\\in Y} \\\\text{MMD}(P(\\\\phi(x)|a_i, \\\\text{sel}, y), P(\\\\phi(x)|a_j, \\\\text{sel}, y))$$\\n\\n### EXPERIMENTAL DETAILS\\n#### D.1 DATASETS\\n\\n**MNIST.** Rotated (Ghifary et al., 2015) and Colored MNIST (Arjovsky et al., 2019) present distinct distribution shifts. While Rotated MNIST only has $A_{\\\\text{ind}}$ wrt. rotation attribute ($R$), Colored MNIST only has $A_{\\\\text{cause}}$ wrt. color attribute ($C$). We combine these datasets to obtain a multi-attribute dataset with $A_{\\\\text{cause}} = \\\\{C\\\\}$ and $A_{\\\\text{ind}} = \\\\{R\\\\}$. Each domain $E_i$ has a specific rotation angle $r_i$ and a specific correlation $\\\\text{corr}_i$ between color $C$ and label $Y$. Our setup consists of 3 domains: $E_1, E_2 \\\\in E_{\\\\text{tr}}$ (training), $E_3 \\\\in E_{\\\\text{te}}$ (test). We define $\\\\text{corr}_i = P(Y=1|C=1) = P(Y=0|C=0)$ in $E_i$. In our setup, $r_1 = 15^\\\\circ$, $r_2 = 60^\\\\circ$, $r_3 = 90^\\\\circ$ and $\\\\text{corr}_1 = 0.9$, $\\\\text{corr}_2 = 0.8$, $\\\\text{corr}_3 = 0.1$. All environments have 25% label noise, as in (Arjovsky et al., 2019). For all experiments on MNIST, we use a two-layer perceptron consistent with previous works (Arjovsky et al., 2019; Krueger et al., 2021).\\n\\n![Figure 4: (a), (b) Train and (c) Test domains for MNIST.](image)\\n\\nMoving beyond simple binary classification, we use small NORB (LeCun et al., 2004), an object recognition dataset, to create a challenging setup with multi-valued classes and attributes over realistic 3D objects. It consists of images of toys of five categories with varying lighting, elevation, and azimuths. The objective is to classify unseen samples of the five categories. (Wiles et al., 2022) introduced single-attribute shifts for this dataset. We combine the Causal shift, $A_{\\\\text{cause}} = \\\\text{lighting}$ wherein there is a correlation between lighting condition $\\\\text{lighting}_i$ and toy $Y$.\"}"}
{"id": "uyqks-LILZX", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"category $y_i$; and Independent shift, $A_{ind} = \\\\text{azimuth}$ that varies independently across domains, to generate our multi-attribute dataset $\\\\text{light} + \\\\text{azi}$. Training domains have 0.9 and 0.95 spurious correlation with lighting whereas there is no correlation in test domain. We add 5% label noise in all environments. We use ResNet-18 (pre-trained on ImageNet) for all settings and fine tune for our task.\\n\\nFigure 5: (a), (b) Train and (c) Test domains for small NORB.\\n\\nWaterbirds. We use the Waterbirds dataset from Sagawa et al. (2020). This dataset classifies birds as \u201cwaterbird\u201d or \u201clandbird\u201d, where bird type ($Y$) is spuriously correlated with background ($\\\\text{bgd}$) \u2013 \u201cwaterbird\u201d images are spuriously correlated with \u201cwater\u201d backgrounds (ocean, natural lake) and \u201clandbird\u201d images with \u201cland\u201d backgrounds (bamboo forest, broadleaf forest). Since background is selected based on $Y$, $A_{cause} = \\\\text{background}$. The dataset is created by pasting bird images from CUB dataset (Wah et al., 2011) onto backgrounds from the Places dataset (Zhou et al., 2018). There is 0.95 correlation between the bird type and background during training i.e., 95% of all waterbirds are placed against a water background, while 95% of all landbirds are placed against a land background. We create training domains based on background ($|E_{tr}| = |A_{cause}| = 2$) as in Yao et al. (2022). We evaluate using worst-group error consistent with past work, where a group is defined as ($\\\\text{background}, y$). We generate the dataset using the official code from Sagawa et al. (2020) and use the same train-validation-test splits.\\n\\nTo create the multi-attribute shift variant of Waterbirds, we add weather effects ($A_{ind}$) using the Automold library. We add darkness effect (darkness coefficient = 0.7) during training with 0.5 probability and rain effect (rain type = 'drizzle', slant = 20) with 1.0 probability during test. Hence, $|A_{ind}| = 3$ ($\\\\{\\\\text{no effect}, \\\\text{darkness}, \\\\text{rain}\\\\}$). Weather effect is applied independent of class label $Y$. Our training domains are based on background and we perform worst-group evaluation, same as the setup described above. Examples from train and test domains for multi-attribute shift dataset are provided in Figure 6.\\n\\nWe use ResNet-50 (pre-trained on ImageNet) for all settings consistent with past work (Sagawa et al., 2020; Yao et al., 2022). All models were evaluated at the best early stopping epoch (as measured by the validation set), again consistent with Sagawa et al. (2020).\"}"}
{"id": "uyqks-LILZX", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D.2 IMPLEMENTATION DETAILS\\n\\nAll methods are trained using Adam optimizer. MNIST dataset is trained for 5000 steps (default in DomainBed (Gulrajani & Lopez-Paz, 2021)) while Waterbirds and small NORB are trained for 2000 steps. Consistent with the default value in DomainBed, we use a batch size of 64 per domain for MNIST. For small NORB, we use a batch size of 128 per domain and a batch size of 16 per domain for Waterbirds.\\n\\nModel Selection.\\n\\nWe create 90% and 10% splits from each domain to be used for training/evaluation and model selection (as needed) respectively. We use a validation set that follows the test domain distribution consistent with previous work on these datasets (Arjovsky et al., 2019; Ye et al., 2022; Wiles et al., 2022; Yao et al., 2022). Specifically, we adopt the test-domain validation from DomainBed for Synthetic, MNIST, and small NORB datasets where early stopping is not allowed and all models are trained for the same fixed number of steps to limit test domain access. For Waterbirds, we perform early stopping using the validation set consistent with past work (Sagawa et al., 2020; Yao et al., 2022).\\n\\nMMD implementation details.\\n\\nWe use the radial basis function (RBF) kernel to compute the MMD penalty. Our implementation is adopted from DomainBed (Gulrajani & Lopez-Paz, 2021). The kernel bandwidth is a hyperparameter and we perform a sweep over the hyperparameter search space to select the best RBF kernel bandwidth. The search space for hyperparameter sweeps is provided in Table 9, where $\\\\gamma$ corresponds to $1/bandwidth$.\\n\\nCACM and baselines implementation details.\\n\\nWe provide the regularization constraints for different shifts used by CACM in Section C. For statistical efficiency, we use a single $\\\\lambda$ value as hyperparameter for MNIST and small NORB datasets. The search space for hyperparameters is given in Table 9.\\n\\nIn MNIST and NORB, we input images and domains ($E = A_{\\\\text{ind}}$) to all baseline methods; CACM receives additional input $A_{\\\\text{cause}}$. Hence, in the Independent single-attribute shift, CACM and all baselines have access to exactly the same information. In Waterbirds, since $E$ is not defined in the original dataset, we follow the setup from Yao et al. (2022) to create domains based on backgrounds. Here, we provide images and domains ($E = \\\\text{background}$) as input to all baselines except GroupDRO; to ensure fair comparison with GroupDRO, we follow Sagawa et al. (2020) and provide 4 groups as input based on ($\\\\text{background}, y$), along with images. For CACM, we do not use background domains but provide the attribute $A_{\\\\text{cause}} = \\\\text{background}$ for the single-attribute dataset, and both $A_{\\\\text{cause}} = \\\\text{background}$ and $A_{\\\\text{ind}} = \\\\text{weather}$ for the multi-attribute shift dataset. Hence, for the single-shift Waterbirds dataset, all baselines receive the same information as CACM.\\n\\nD.3 HYPERPARAMETER SEARCH\\n\\nFollowing DomainBed (Gulrajani & Lopez-Paz, 2021), we perform a random search 20 times over the hyperparameter distribution and this process is repeated for total 3 seeds. The best models are obtained across the three seeds over which we compute the mean and standard error. The hyperparameter search space for all datasets and algorithms is given in Table 9.\\n\\nE.1 SYNTHETIC DATASET\\n\\nOur synthetic dataset is constructed based on the data-generating processes of the slab dataset (Mahajan et al., 2021; Shah et al., 2020). The original slab dataset was introduced by (Shah et al., 2020) to demonstrate the simplicity bias in neural networks as they learn the linear feature which is easier to learn in comparison to the slab feature. Our extended slab dataset, adds to the setting from (Mahajan et al., 2021) by using non-binary attributes and class labels to create a more challenging task and allows us to study DG algorithms in the presence of linear spurious features.\\n\\nOur dataset consists of 2-dimensional input $X$ consisting of features $X_c$ and $A_{\\\\text{ind}}$. This is consistent with the graph in Figure 2 where attributes and causal features together determine observed features.\"}"}
{"id": "uyqks-LILZX", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our work on modeling the data-generating process for improved out-of-distribution generalization is an important advance in building robust predictors for practical settings. Such prediction algorithms, including methods building on representation learning, are increasingly a key element of decision-support and decision-making systems. We expect our approach to creating a robust predictor to be particularly valuable in real world setups where spurious attributes and real-world multi-attribute settings lead to biases in data. While not the focus of this paper, CACM may be applied to mitigate social biases (e.g., in language and vision datasets) whose structures can be approximated by the graphs in Figure 2. Risks of using methods such as CACM, include excessive reliance or a false sense of confidence. While methods such as CACM ease the process of building robust models, there remain many ways that an application may still fail (e.g., incorrect structural assumptions). AI applications must still be designed appropriately with support of all stakeholders and potentially affected parties, tested in a variety of settings, etc.\\n\\nWe provide all required experimental details in Suppl. D including dataset details, training details and hyperparameter search sweeps. We additionally submit our code as part of supplementary material which can be used to reproduce the experiments. We provide a demo notebook for prediction using CACM in the DoWhy library. We provide proofs for all our theoretical results in Suppl. B.\\n\\nWe thank Abhinav Kumar, Adith Swaminathan, Yiding Jiang, and Dhruv Agarwal for helpful feedback and comments on the draft. We would also like to thank the anonymous reviewers for their valuable feedback.\\n\\nKartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution generalization. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=jlchsFOLfeF.\\n\\nIsabela Albuquerque, Jo\u00e3o Monteiro, Mohammad Darvishi, Tiago H. Falk, and Ioannis Mitliagkas. Generalizing to unseen domains via distribution matching, 2020.\\n\\nMartin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization, 2019. URL https://arxiv.org/abs/1907.02893.\\n\\nPeter Bandi, Oscar Geessink, Quirine Manson, Marcory van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, Quanzheng Li, Farhad Ghazvinian Zanjani, Sveta Zinger, Keisuke Fukuta, Daisuke Komura, Vlado Ovtcharov, Shenghua Cheng, Shaoqun Zeng, Jeppe Thagaard, and Geert Litjens. From detection of individual metastases to classification of lymph node status at the patient level: The camelyon17 challenge. IEEE Transactions on Medical Imaging, PP:1\u20131, 08 2018. doi: 10.1109/TMI.2018.2867350.\\n\\nDaniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. In Companion Proceedings of The 2019 World Wide Web Conference, WWW \u201919, pp. 491\u2013500, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450366755. doi: 10.1145/3308560.3317593. URL https://doi.org/10.1145/3308560.3317593.\"}"}
{"id": "uyqks-LILZX", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "uyqks-LILZX", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "uyqks-LILZX", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "uyqks-LILZX", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we concatenate $X$ and $A_{\\\\text{ind}}$ to generate $X$ in our synthetic setup. Causal feature $X_c$ has a non-linear \\\"slab\\\" relationship with $Y$ while $A_{\\\\text{ind}}$ has a linear relationship with $Y$. We create three different datasets with Causal (E.1.1), Confounded (E.1.2) and Selected (E.1.3) $A_{\\\\text{ind}} - Y$ relationship respectively.\\n\\nImplementation details. In all setups, $X_c$ is a single-dimensional variable and has a uniform distribution $\\\\text{Uniform}[0, 1]$ across all environments. We use the default 3-layer MLP architecture from DomainBed and use mean difference (L2) instead of MMD as the regularization penalty given the simplicity of the data. We use a batch size of 128 for all datasets.\\n\\nE.1.1 Causal SHIFT\\nWe have three environments, $E_1, E_2 \\\\in E_{\\\\text{tr}}$ (training) and $E_3 \\\\in E_{\\\\text{te}}$ (test). $X_c$ has a uniform distribution $\\\\text{Uniform}[0, 1]$ across all environments.\\n\\n$$y = \\\\begin{cases} \\n0 & \\\\text{if } X_c \\\\in [0, 0.2) \\\\\\\\\\n1 & \\\\text{if } X_c \\\\in [0.2, 0.4) \\\\\\\\\\n2 & \\\\text{if } X_c \\\\in [0.4, 0.6) \\\\\\\\\\n3 & \\\\text{if } X_c \\\\in [0.6, 0.8) \\\\\\\\\\n4 & \\\\text{if } X_c \\\\in [0.8, 1.0] \\n\\\\end{cases}$$\\n\\n$A_{\\\\text{cause}} = y$ with prob. $= \\\\text{abs}(y - 1)$ with prob. $= 1 - \\\\text{abs}(y - 1)$;\\n\\nHence, we have a five-way classification setup ($|Y| = 5$) with multi-valued attributes. Following (Mahajan et al., 2021), the two training domains have $p$ as 0.9 and 1.0, and the test domain has $p = 0.0$. We add 10% noise to $Y$ in all environments.\\n\\nE.1.2 Confounded SHIFT\\nWe have three environments, $E_1, E_2 \\\\in E_{\\\\text{tr}}$ (training) and $E_3 \\\\in E_{\\\\text{te}}$ (test). $X_c$ has a uniform distribution $\\\\text{Uniform}[0, 1]$ across all environments. Our confounding variable $c$ has different functional relationships with $Y$ and $A_{\\\\text{conf}}$ which vary across environments.\\n\\n$$c_{E_1, E_2} = 1 \\\\text{ with prob. } = 0.25$$\\n$$0 \\\\text{ with prob. } = 0.75$$\\n$$c_{E_3} = 1 \\\\text{ with prob. } = 0.75$$\\n$$0 \\\\text{ with prob. } = 0.25$$\\n\\nThe true function for $Y$ is given by,\\n\\n$$y_{\\\\text{true}} = \\\\begin{cases} \\n0 & \\\\text{if } X_c \\\\in [0, 0.25) \\\\\\\\\\n1 & \\\\text{if } X_c \\\\in [0.25, 0.5) \\\\\\\\\\n2 & \\\\text{if } X_c \\\\in [0.5, 0.75) \\\\\\\\\\n3 & \\\\text{if } X_c \\\\in [0.75, 1.0] \\n\\\\end{cases}$$\\n\\nObserved $Y$ and $A_{\\\\text{conf}}$ are functions of confounding variable $c$ and their distribution changes across environments as described below:\\n\\n$$y_{E_1, E_2} = y_{\\\\text{true}} + c \\\\text{ with prob. } = 0.9$$\\n$$y_{\\\\text{true}} \\\\text{ with prob. } = 0.1$$\\n$$y_{E_3} = y_{\\\\text{true}} \\\\text{ with prob. } = 1$$\\n\\n$A_{\\\\text{conf}} = 2 \\\\times c$ with prob. $= p$; $p_{E_1} = 1.0$, $p_{E_2} = 0.9$, $p_{E_3} = 0.8$.\"}"}
{"id": "uyqks-LILZX", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Selected shifts arise due to selection effect in the data generating process and induce an association between $Y$ and $A_{sel}$. A data point is included in the sample only if selection variable $S = 1$ holds; $S$ is a function of $Y$ and $A_{sel}$. The selection criterion may differ between domains (Veitch et al., 2021).\\n\\nWe construct three environments, $E_1, E_2 \\\\in E_{tr}$ (training) and $E_3 \\\\in E_{te}$ (test). $X_c$ has a uniform distribution $\\\\text{Uniform}[0, 1]$ across all environments. Our selection variable $S$ is a function of $Y$ and $A_{sel}$. We add 10% noise to $Y$ in all environments.\\n\\n$$X_c \\\\sim \\\\text{Uniform}[0, 1]; A_{sel} \\\\in \\\\{1, 2, 3, 4\\\\}$$\\n\\nThe true function for $Y$ is given by,\\n\\n$$y_{\\\\text{true}} = \\\\begin{cases} \\n0 & \\\\text{if } X_c \\\\in [0, 0.25) \\\\\\\\\\n1 & \\\\text{if } X_c \\\\in [0.25, 0.5) \\\\\\\\\\n2 & \\\\text{if } X_c \\\\in [0.5, 0.75) \\\\\\\\\\n3 & \\\\text{if } X_c \\\\in [0.75, 1] \\n\\\\end{cases}$$\\n\\nThe function used to decide the selection variable $S$ (and hence the selection shift) varies across environments through the parameter $p$.\\n\\n$$S = 1 \\\\text{ if } A_{sel} + y = 4 \\\\text{ with prob. } = p$$\\n\\n$$A_{sel} - y = 1 \\\\text{ with prob. } = 1 - p; p_{E_1} = 0.9, p_{E_2} = 1.0, p_{E_3} = 0.0$$\\n\\nE.2 A FIXED CONDITIONAL INDEPENDENCE CONSTRAINT CANNOT WORK ACROSS ALL SHIFTS\\n\\nHere, we compare the performance of two popular independence constraints in the literature Mahajan et al. (2021): unconditional $X_c \\\\perp \\\\perp A \\\\mid E$, and conditional on label $X_c \\\\perp \\\\perp A \\\\mid Y, E$ (we condition on $E$ for fully generality) on Synthetic Causal, Confounded and Selected shift datasets (Table 6).\\n\\nWe train a model using ERM (cross-entropy) where the representation is regularized using either of the constraints. As predicted by Theorem 3.1, neither constraint obtains best accuracy on all three datasets. The conditional constraint is better on $A_{cause}$ and $A_{sel}$ datasets, whereas the unconditional constraint is better on $A_{conf}$, consistent with Proposition 3.1. Predictors with the correct constraint are also more risk-invariant, having lower gap between train and test accuracy.\\n\\n| Constraint | $A_{cause}$ | $A_{conf}$ | $A_{sel}$ |\\n|------------|-------------|------------|-----------|\\n| $X_c \\\\perp \\\\perp A \\\\mid E$ | 96.5 \u00b1 0.2 | 62.4 \u00b1 5.7 | 81.1 \u00b1 2.0 |\\n| $X_c \\\\perp \\\\perp A \\\\mid Y, E$ | 89.1 \u00b1 3.8 | 89.3 \u00b1 2.3 | 78.4 \u00b1 2.6 |\\n\\nE.3 EFFECT OF VARYING REGULARIZATION PENALTY COEFFICIENT\\n\\nTo understand how incorrect constraints affect model generalization capabilities, we study the Causal shift setup in small NORB. From Theorem 3.1, we know the correct constraint for $A_{cause}$: $X_c \\\\perp \\\\perp A_{cause} \\\\mid Y, E$. In addition, we see the following invalid constraint, $X_c \\\\perp \\\\perp A_{cause} \\\\mid E$.\\n\\nWe compare the performance of these two conditional independence constraints while varying the regularization penalty coefficient ($\\\\lambda$) (Figure 7). We perform our evaluation across three setups\"}"}
{"id": "uyqks-LILZX", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"application of the incorrect constraint is sensitive to \\\\( \\\\lambda \\\\) parameter: as \\\\( \\\\lambda \\\\) increases, accuracy drops to less than 40%. However, accuracy with the correct constraint stays invariant across different values of \\\\( \\\\lambda \\\\).\\n\\nFigure 7: Accuracy of CACM (\\\\( X_{\\\\perp \\\\perp A_{\\\\text{cause}}|Y,E} \\\\)) and incorrect constraint (\\\\( X_{\\\\perp \\\\perp A_{\\\\text{cause}}|E} \\\\)) on small NORB Causal shift with varying \\\\( \\\\lambda \\\\) \\\\{1, 10, 100\\\\} and spurious correlation in training environments (in parantheses in legend).\\n\\nE.4 PROVIDING ATTRIBUTE INFORMATION TO DG ALGORITHMS FOR A FAIRER COMPARISON\\n\\nCACM leverages attribute labels to apply the correct independence constraints derived from the causal graph. However, existing DG algorithms only use the input features \\\\( X \\\\) and the domain attribute. Here we provide this attribute information to existing DG algorithms to create a more favorable setting for their application. We show that even in this setup, these algorithms are not able to close the performance gap with CACM, showing the importance of the causal information through graphs.\\n\\nE.4.1 SYNTHETIC DATASET\\n\\nWe consider our Synthetic dataset with Causal distribution shift where our observed features \\\\( X = (X_{\\\\text{cause}}, A) \\\\). Note that by construction of \\\\( X \\\\), one of our input dimensions already consists of \\\\( A_{\\\\text{cause}} \\\\). Hence, all baselines do receive information about \\\\( A_{\\\\text{cause}} \\\\) in addition to the domain attribute \\\\( E \\\\).\\n\\nHowever, to provide a fairer comparison with CACM, we now additionally explicitly make \\\\( A_{\\\\text{cause}} \\\\) available to all DG algorithms for applying their respective constraints by creating domains based on \\\\( A_{\\\\text{cause}} \\\\) i.e, each environment \\\\( E \\\\) has samples with same value of \\\\( A_{\\\\text{cause}} \\\\).\\n\\nIn this setup (Table 7, third column), we see IB-IRM, DANN, and Mixup show significant improvement in accuracy but the best performance is still 14% lower than CACM. We additionally observe baselines to show higher estimate variance in this setup. This reinforces our motivation to use the causal graph of the data-generating process to derive the constraint, as the attribute values alone are not sufficient. We also see MMD, CORAL, GroupDRO, and MLDG perform much worse than earlier, highlighting the sensitivity of DG algorithms to domain definition. In contrast, CACM uses the causal graph to study the structural relationships and derive the regularization penalty, which remains the same in this new dataset too.\\n\\nE.4.2 WATERBIRDS\\n\\nWe perform a similar analysis on the Waterbirds multi-attribute shift dataset. In order to provide the same information to other DG algorithms as CACM, we create domains based on \\\\( A_{\\\\text{cause}} \\\\)x\\\\( A_{\\\\text{ind}} \\\\) in this setup (Table 8). We observe mixed results \u2013 while some algorithms show significant improvement (ERM, IRM, VREx, MMD, MLDG, RSC), there is a performance drop for some others (IB-ERM, IB-IRM, CORAL, C-MMD, GroupDRO, Mixup). CACM uses the knowledge of the causal...\"}"}
{"id": "uyqks-LILZX", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 7: Synthetic dataset. Accuracy on unseen domain for causal distribution shift when a cause is provided in input (column 2) and when a cause is additionally used to create domains (column 3).\\n\\n| Algo. | Accuracy (input) | Accuracy (input+domains) |\\n|-------|------------------|--------------------------|\\n| ERM   | 73.3 \u00b1 1.3       | 71.8 \u00b1 3.8               |\\n| IB-ERM| 69.3 \u00b1 2.4       | 68.2 \u00b1 4.9               |\\n| IRM   | 68.4 \u00b1 2.9       | 64.1 \u00b1 0.8               |\\n| IB-IRM| 67.8 \u00b1 2.6       | 73.6 \u00b1 0.4               |\\n| VREx  | 77.4 \u00b1 1.2       | 75.0 \u00b1 1.6               |\\n| MMD   | 72.3 \u00b1 4.3       | 68.8 \u00b1 4.1               |\\n| CORAL | 75.5 \u00b1 0.7       | 72.1 \u00b1 0.8               |\\n| DANN  | 60.8 \u00b1 4.7       | 65.8 \u00b1 11.9              |\\n| C-MMD | 71.7 \u00b1 2.7       | 67.9 \u00b1 4.9               |\\n| CDANN | 71.1 \u00b1 2.5       | 68.4 \u00b1 5.8               |\\n| GroupDRO | 79.9 \u00b1 2.2 | 65.4 \u00b1 3.4               |\\n| Mixup | 58.3 \u00b1 1.8       | 61.5 \u00b1 10.7              |\\n| MLDG  | 73.3 \u00b1 2.6       | 65.3 \u00b1 3.3               |\\n| SagNet| 72.5 \u00b1 2.3       | 71.6 \u00b1 2.8               |\\n| RSC   | 70.9 \u00b1 3.4       | 71.5 \u00b1 1.7               |\\n\\n### Table 8: Waterbirds. Accuracy on unseen domain for multi-attribute distribution shift when a cause is used to create domains (column 2) and when a cause x an indicator is used to create domains (column 3).\\n\\n| Algo. | Worst-group Accuracy (input) | Worst-group Accuracy (input+domains) |\\n|-------|-------------------------------|--------------------------------------|\\n| ERM   | 37.0 \u00b1 1.1                   | 43.0 \u00b1 7.8                           |\\n| IB-ERM| 40.8 \u00b1 5.6                   | 34.4 \u00b1 1.0                           |\\n| IRM   | 37.7 \u00b1 1.7                   | 42.2 \u00b1 2.5                           |\\n| IB-IRM| 46.9 \u00b1 6.5                   | 43.3 \u00b1 4.6                           |\\n| VREx  | 38.1 \u00b1 2.3                   | 48.0 \u00b1 3.4                           |\\n| MMD   | 45.2 \u00b1 2.4                   | 53.3 \u00b1 1.9                           |\\n| CORAL | 54.1 \u00b1 3.0                   | 47.5 \u00b1 2.8                           |\\n| DANN  | 55.5 \u00b1 4.6                   | 57.7 \u00b1 6.5                           |\\n| C-MMD | 52.3 \u00b1 1.9                   | 45.9 \u00b1 4.9                           |\\n| CDANN | 49.7 \u00b1 3.9                   | 50.7 \u00b1 5.8                           |\\n| GroupDRO | 53.1 \u00b1 2.2 | 40.9 \u00b1 3.1               |\\n| Mixup | 64.7 \u00b1 2.4                   | 50.3 \u00b1 1.5                           |\\n| MLDG  | 34.5 \u00b1 1.7                   | 43.6 \u00b1 3.8                           |\\n| SagNet| 40.6 \u00b1 7.1                   | 38.0 \u00b1 1.7                           |\\n| RSC   | 40.9 \u00b1 3.6                   | 46.5 \u00b1 5.3                           |\\n\\nThe $E - X_c$ relationship shown in Figure 2b represents correlation of $E$ with $X_c$, which can change across environments. As we saw for the $Y - A_{ind}$ edge, this correlation can be due to causal relationship (Figure 8a), confounding with a common cause (Figure 8b), or selection (Figure 8c); all our results...\"}"}
{"id": "uyqks-LILZX", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3 A FIXED CONDITIONAL INDEPENDENCE CONSTRAINT CANNOT WORK FOR ALL SHIFTS\\n\\nCombining with Theorem 2.1, Proposition 3.1 shows that the necessary constraints for a risk-invariant predictor's representation \\\\( \\\\phi(X) \\\\) are different for different types of attributes. This leads us to our key result: under multi-attribute shifts, a single (conditional) independence constraint cannot be valid for all kinds of shifts. Remarkably, this result is true even for single-attribute shifts: any algorithm with a fixed conditional independence constraint (e.g., as listed in Table 1 (Gretton et al., 2012; Arjovsky et al., 2019; Li et al., 2018b; Sun & Saenko, 2016)) cannot work for all datasets.\\n\\n**Theorem 3.1.** Under the canonical causal graph in Figure 2(a,b), there exists no (conditional) independence constraint over \\\\( \\\\langle X_c, A, Y \\\\rangle \\\\) that is valid for all realized DAGs as the type of multi-attribute shifts vary. Hence, for any predictor algorithm for \\\\( Y \\\\) that uses a single (conditional) independence constraint over its representation \\\\( \\\\phi(X) \\\\), \\\\( A \\\\) and \\\\( Y \\\\), there exists a realized DAG \\\\( G \\\\) and a corresponding training dataset such that the learned predictor cannot be a risk-invariant predictor for distributions in \\\\( P_G \\\\), where \\\\( P_G \\\\) is the set of distributions obtained by changing \\\\( P(A|Y) \\\\).\\n\\n**Corollary 3.2.** Even when \\\\( |A| = 1 \\\\), an algorithm using a single independence constraint over \\\\( \\\\langle \\\\phi(X), A, Y \\\\rangle \\\\) cannot yield a risk-invariant predictor for all kinds of single-attribute shift datasets.\\n\\nCorollary 3.2 adds theoretical evidence for past empirical demonstrations of inconsistent performance of DG algorithms (Wiles et al., 2022; Ye et al., 2022). To demonstrate its significance, we provide OoD generalization results on a simple \u201cslab\u201d setup (Shah et al., 2020) with three datasets (Causal, Confounded, and Selected shifts) in Suppl. E.2. We evaluate two constraints motivated by DG literature Mahajan et al. (2021): unconditional \\\\( X_c \\\\perp \\\\perp A | E \\\\), and conditional on label \\\\( X_c \\\\perp \\\\perp A | Y, E \\\\).\\n\\nAs predicted by Corollary 3.2, neither constraint obtains best accuracy on all three datasets (Table 6).\\n\\n4 CUSALLY ADAPTIVE CONSTRAINT MINIMIZATION (CACM)\\n\\nMotivated by Sec. 3, we present CACM, an algorithm that adaptively chooses regularizing constraints for multi-attribute shift datasets (full algorithm for any general DAG in Suppl. C). It has two phases.\\n\\n**Phase I.** Derive correct independence constraints.\\n\\nIf a dataset's DGP satisfies the canonical graph, CACM requires a user to specify the relationship type for each attribute and uses the constraints from Proposition 3.1. For other datasets, CACM requires a causal graph describing the dataset's DGP and uses the following steps to derive the independence constraints. Let \\\\( V \\\\) be the set of observed variables in the graph except \\\\( Y \\\\), and \\\\( C \\\\) be the list of constraints.\\n\\n1. For each observed variable \\\\( V \\\\in V \\\\), check whether \\\\( (X_c, V) \\\\) are \\\\( d \\\\)-separated. Add \\\\( X_c \\\\perp \\\\perp V \\\\) to \\\\( C \\\\).\\n2. If not, check if \\\\( (X_c, V) \\\\) are \\\\( d \\\\)-separated conditioned on any subset \\\\( Z \\\\) of the remaining observed variables in \\\\( Z = \\\\{Y\\\\} \\\\cup V \\\\setminus \\\\{V\\\\} \\\\). For each subset \\\\( Z \\\\) with \\\\( d \\\\)-separation, add \\\\( X_c \\\\perp \\\\perp V | Z \\\\) to \\\\( C \\\\).\\n\\n**Phase II.** Apply regularization penalty using derived constraints.\\n\\nIn Phase II, CACM applies those constraints as a regularizer to the standard ERM loss, \\\\( g_1, \\\\phi = \\\\arg \\\\min_{g_1, \\\\phi} \\\\ell(g_1(\\\\phi(x)), y) + \\\\text{RegPenalty} \\\\), where \\\\( \\\\ell \\\\) is cross-entropy loss. The regularizer optimizes for valid constraints over all observed variables \\\\( V \\\\in V \\\\). Below we provide the regularizer term for datasets following the canonical graphs from Figure 2 (\\\\( V = A \\\\)). We choose Maximum Mean Discrepancy (MMD) (Gretton et al., 2012) to apply our penalty (in principle, any metric for conditional independence would work).\\n\\nSince \\\\( A \\\\) includes multiple attributes, the regularizer penalty depends on the type of distribution shift for each attribute. For instance, for \\\\( A \\\\in A_{\\\\text{ind}}(\\\\text{Independent}) \\\\), to enforce \\\\( \\\\phi(x) \\\\perp \\\\perp A \\\\), we aim to minimize the distributional discrepancy between \\\\( P(\\\\phi(x)|A = a_i) \\\\) and \\\\( P(\\\\phi(x)|A = a_j) \\\\), for all \\\\( i, j \\\\) values of \\\\( A \\\\). However, since the same constraint is applicable on \\\\( E \\\\), it is statistically efficient to apply the constraint on \\\\( E \\\\) (if available) as there may be multiple closely related values of \\\\( A \\\\) in a domain (e.g., slide stains collected from one hospital may be spread over similar colors, but not exactly the same). Hence, we apply the constraint on distributions \\\\( P(\\\\phi(x)|E = E_i) \\\\) and \\\\( P(\\\\phi(x)|E = E_j) \\\\) if \\\\( E \\\\) is observed (and \\\\( A \\\\) may/may not be unobserved), otherwise we apply the constraint over \\\\( A \\\\).\\n\\n\\\\[\\n\\\\text{RegPenalty}_{A_{\\\\text{ind}}} = \\\\sum_{i,j} \\\\text{MMD}(P(\\\\phi(x)|a_i, A_{\\\\text{ind}}) \\\\| P(\\\\phi(x)|a_j, A_{\\\\text{ind}}))\\n\\\\]\"}"}
{"id": "uyqks-LILZX", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"between $E$ and $X$ (Figure 2b), which renders other constraints incorrect (Corollary 3.1). We similarly obtain regularization terms for Confounded and Selected (Suppl. C).\\n\\n$$\\\\text{RegPenalty} = \\\\sum_{A \\\\in \\\\mathcal{A}} \\\\lambda_A \\\\text{Penalty}_A,$$\\n\\nwhere $\\\\lambda_A$ are the hyperparameters. Unlike prior work (Makar et al., 2022; Veitch et al., 2021), we do not restrict ourselves to binary-valued attributes and classes.\\n\\nCACM\u2019s relationship with existing DG algorithms. Table 1 shows common constraints used by popular DG algorithms. CACM\u2019s strength lies in adaptively selecting constraints based on the causal relationships in the DGP. Thus, depending on the dataset, applying CACM for a single-attribute shift may involve applying the same constraint as in MMD or C-MMD algorithms. For example, in Rotated-MNIST dataset with $E = A_{\\\\text{ind}} = \\\\text{rotation}$, the effective constraint for MMD, DANN, CORAL algorithms ($\\\\phi \\\\perp \\\\perp E$) is the same as CACM\u2019s constraint $\\\\phi \\\\perp \\\\perp A_{\\\\text{ind}}$ for Independent shift.\\n\\n5. EPIRICAL EVALUATION\\n\\nWe perform experiments on MNIST, small NORB, and Waterbirds datasets to demonstrate our main claims: existing DG algorithms perform worse on multi-attribute shifts; CACM with the correct graph-based constraints significantly outperforms these algorithms; and incorrect constraints cannot match the above accuracy. While we provide constraints for all shifts in Proposition 3.1, our empirical experiments with datasets focus on commonly occurring Causal and Independent shifts. All experiments are performed in PyTorch 1.10 with NVIDIA Tesla P40 and P100 GPUs, and building on DomainBed (Gulrajani & Lopez-Paz, 2021) and OoD-Bench (Ye et al., 2022). Regularizing on model\u2019s logit scores provides better accuracy than $\\\\phi(x)$; hence we adopt it for all our experiments.\\n\\n5.1 Datasets & Baseline DG Algorithms\\n\\nWe introduce three new datasets for the multi-attribute shift problem. For all datasets, details of environments, architectures, visualizations, and setup generation are in Suppl. D.1.\\n\\nMNIST. Colored (Arjovsky et al., 2019) and Rotated MNIST (Ghifary et al., 2015) present Causal ($A_{\\\\text{cause}} = \\\\text{color}$) and Independent ($A_{\\\\text{ind}} = \\\\text{rotation}$) distribution shifts, respectively. We combine these to obtain a multi-attribute dataset with $A_{\\\\text{cause}}$ and $A_{\\\\text{ind}}$ ($\\\\text{col} + \\\\text{rot}$). For comparison, we also evaluate on single-attribute $A_{\\\\text{cause}}$ (Colored) and $A_{\\\\text{ind}}$ (Rotated) MNIST datasets.\\n\\nsmall NORB (LeCun et al., 2004). This dataset was used by Wiles et al. (2022) to create a challenging DG task with single-attribute shifts, having multi-valued classes and attributes over realistic 3D objects. We create a multi-attribute shift dataset ($\\\\text{light} + \\\\text{azi}$), consisting of a causal connection, $A_{\\\\text{cause}} = \\\\text{lighting}$, between lighting and object category $Y$; and $A_{\\\\text{ind}} = \\\\text{azimuth}$ that varies independently across domains. We also evaluate on single-attribute $A_{\\\\text{cause}}$ (lighting) and $A_{\\\\text{ind}}$ (azimuth) datasets.\\n\\nWaterbirds. We use the original dataset (Sagawa et al., 2020) where bird type (water or land) ($Y$) is spuriously correlated with background ($A_{\\\\text{cause}}$). To create a multi-attribute setup, we add different weather effects ($A_{\\\\text{ind}}$) to train and test data with probability $p = 0.5$ and 1.0 respectively.\\n\\nBaseline DG algorithms & implementation. We consider baseline algorithms optimizing for different constraints and statistics to compare to causal adaptive regularization: IRM (Arjovsky et al., 2019), IB-ERM and IB-IRM (Ahuja et al., 2021), VREx (Krueger et al., 2021), MMD (Li et al., 2018b), CORAL (Sun & Saenko, 2016), DANN (Gretton et al., 2012), Conditional-MMD (C-MMD) (Li et al., 2018b), Conditional-DANN (CDANN) (Li et al., 2018d), GroupDRO (Sagawa et al., 2020), Mixup (Yan et al., 2020), MLDG (Li et al., 2018a), SagNet (Nam et al., 2021), and RSC (Huang et al., 2020). Following DomainBed (Gulrajani & Lopez-Paz, 2021), a random search is performed 20 times over the hyperparameter distribution for 3 seeds. The best models obtained across the three seeds are used to compute the mean and standard error. We use a validation set that follows the test domain distribution consistent with previous work on these datasets (Arjovsky et al., 2019; Sagawa et al., 2020; Wiles et al., 2022; Ye et al., 2022). Further details are in Suppl. D.7.\"}"}
{"id": "uyqks-LILZX", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Colored + Rotated MNIST: Accuracy on unseen domain for single-attribute (color, rotation) and multi-attribute (col + rot) distribution shifts; small NORB: Accuracy on unseen domain for single-attribute (lighting, azimuth) and multi-attribute (light + azi) distribution shifts. Waterbirds: Worst-group accuracy on unseen domain for single- and multi-attribute shifts.\\n\\n| Algorithm    | Colored+Rotated MNIST | Accuracy | small NORB | Accuracy | Waterbirds | Worst-group accuracy |\\n|--------------|-----------------------|----------|------------|----------|------------|---------------------|\\n|              |                       |          |            |          |            |                     |\\n| ERM          | 30.9 \u00b11.6             | 61.9 \u00b10.5| 25.2 \u00b11.3  | 65.5 \u00b10.7| 78.6 \u00b10.7  | 64.0 \u00b11.2           |\\n| IB-ERM       | 27.8 \u00b10.7             | 62.1 \u00b10.8| 41.2 \u00b14.1  | 66.0 \u00b13.7| 37.0 \u00b11.1  |                     |\\n| IRM          | 50.0 \u00b10.1             | 61.2 \u00b10.3| 39.6 \u00b16.7  | 66.7 \u00b11.5| 75.7 \u00b10.4  | 61.7 \u00b10.5           |\\n| IB-IRM       | 49.9 \u00b10.1             | 61.4 \u00b10.9| 49.3 \u00b10.3  | 64.7 \u00b10.8| 77.6 \u00b10.3  | 62.2 \u00b11.2           |\\n| VREx         | 30.3 \u00b11.6             | 62.1 \u00b10.4| 23.3 \u00b10.4  | 64.7 \u00b11.0| 77.6 \u00b10.5  | 62.5 \u00b11.6           |\\n| MMD          | 29.7 \u00b11.8             | 62.2 \u00b10.5| 24.1 \u00b10.6  | 66.6 \u00b11.6| 76.7 \u00b11.1  | 62.5 \u00b10.3           |\\n| CORAL        | 28.5 \u00b10.8             | 62.5 \u00b10.7| 23.5 \u00b11.1  | 64.7 \u00b10.5| 77.2 \u00b10.7  | 62.9 \u00b10.3           |\\n| DANN         | 20.7 \u00b10.8             | 61.9 \u00b10.7| 32.0 \u00b17.8  | 64.6 \u00b11.4| 78.6 \u00b10.7  | 60.8 \u00b10.7           |\\n| C-MMD        | 29.4 \u00b10.2             | 62.3 \u00b10.4| 32.2 \u00b17.0  | 65.8 \u00b10.8| 76.9 \u00b11.0  | 61.0 \u00b10.9           |\\n| CDANN        | 30.8 \u00b18.0             | 61.8 \u00b10.2| 32.2 \u00b17.0  | 64.9 \u00b10.5| 77.3 \u00b10.3  | 60.8 \u00b10.9           |\\n| DRO          | 33.9 \u00b10.4             | 60.6 \u00b10.9| 25.3 \u00b10.5  | 65.5 \u00b10.7| 77.1 \u00b11.0  | 62.3 \u00b10.6           |\\n| Mixup        | 25.1 \u00b11.2             | 61.4 \u00b10.6| 21.1 \u00b11.6  | 66.2 \u00b11.3| 80.4 \u00b10.5  | 57.1 \u00b11.5           |\\n| MLDG         | 31.0 \u00b10.3             | 61.6 \u00b10.8| 24.4 \u00b10.7  | 66.0 \u00b10.7| 77.9 \u00b10.5  | 64.2 \u00b10.6           |\\n| SagNet       | 28.2 \u00b10.8             | 60.7 \u00b10.7| 23.7 \u00b10.2  | 65.9 \u00b11.5| 76.1 \u00b10.4  | 62.2 \u00b10.5           |\\n| RSC          | 29.1 \u00b11.9             | 62.3 \u00b10.4| 22.8 \u00b10.3  | 62.4 \u00b10.4| 75.6 \u00b10.6  | 61.8 \u00b11.3           |\\n| CACM         | 70.4 \u00b10.5             | 62.4 \u00b10.4| 54.1 \u00b11.3  | 85.4 \u00b10.5| 80.5 \u00b10.6  | 69.6 \u00b11.6           |\\n\\nTable 3: small NORB Causal shift. Comparing \\\\(X_c \\\\perp \\\\perp A_{\\\\text{cause}}|Y, E\\\\) with incorrect constraints.\\n\\n| Constraint   | Accuracy |\\n|--------------|----------|\\n| \\\\(X_c \\\\perp \\\\perp A_{\\\\text{cause}}|Y, E\\\\) | 85.4 \u00b10.5 |\\n| \\\\(X_c \\\\perp \\\\perp A_{\\\\text{cause}}|Y\\\\) | 79.7 \u00b10.9 |\\n| \\\\(X_c \\\\perp \\\\perp A_{\\\\text{cause}}|E\\\\) | 76.2 \u00b10.9 |\\n| \\\\(X_c \\\\perp \\\\perp A_{\\\\text{cause}}\\\\) | 72.7 \u00b11.1 |\\n\\nTable 4: Comparing \\\\(X_c \\\\perp \\\\perp A_{\\\\text{cause}}|Y, E\\\\) and \\\\(X_c \\\\perp \\\\perp A_{\\\\text{cause}}|Y\\\\) for Causal shift in MNIST and small NORB. The constraint implied by \\\\(E\\\\) - \\\\(X_c\\\\) correlation (Fig. 2b, Prop. 3.1) affects accuracy.\\n\\n| Algorithm    | Constraint | MNIST | Accuracy | small NORB | Accuracy |\\n|--------------|------------|-------|----------|------------|----------|\\n|              |            | 2d    | 69.7 \u00b10.2| 79.7 \u00b10.9  |\\n|              |            | 2d    | 70.4 \u00b10.5| 85.4 \u00b10.5  |\\n\\n5.2 RESULTS\\nCorrect constraint derived from the causal graph matters. Table 2 shows the accuracy on test domain for all datasets. Comparing the three prediction tasks for MNIST and small NORB, for all algorithms, accuracy on unseen test domain is highest under \\\\(A_{\\\\text{ind}}\\\\) shift and lowest under two-attribute shift \\\\((A_{\\\\text{ind}} \\\\cup A_{\\\\text{cause}})\\\\), reflecting the difficulty of a multi-attribute distribution shift. On the two-attribute shift task in MNIST, all DG algorithms obtain less than 50% accuracy whereas CACM obtains a 5% absolute improvement. Results on small NORB dataset are similar: CACM obtains 69.6% accuracy on the two-attribute task while the nearest baseline is MLDG at 64.2%.\\n\\nOn both MNIST and small NORB, CACM also obtains highest accuracy on the \\\\(A_{\\\\text{cause}}\\\\) task. On MNIST, even though IRM and VREx have been originally evaluated for the Color-only \\\\((A_{\\\\text{cause}})\\\\) task, under an extensive hyperparameter sweep as recommended in past work (Gulrajani & Lopez-Paz, 2021; Krueger et al., 2021; Ye et al., 2022), we find that CACM achieves a substantially higher accuracy (70%) than these methods, just 5 units lower than the optimal 75%. While the \\\\(A_{\\\\text{ind}}\\\\) task is relatively easier, algorithms optimizing for the correct constraint achieve highest accuracy. Note that MMD, CORAL, DANN, and CACM are based on the same independence constraint (see Table 1).\\n\\nAs mentioned in Section 4, we use the domain attribute \\\\(E\\\\) for CACM\u2019s regularization constraint for \\\\(A_{\\\\text{ind}}\\\\) task, for full comparability with other algorithms that also use \\\\(E\\\\). The results indicate the importance of adaptive regularization for generalization.\\n\\nTable 2 also shows the OoD accuracy of algorithms on the original Waterbirds dataset (Sagawa et al., 2020) and its multi-attribute shift variant. Here we evaluate using worst-group accuracy consistent with past work (Sagawa et al., 2020; Yao et al., 2022). We observe that on single-attribute \\\\((A_{\\\\text{cause}})\\\\) as well as multi-attribute shift, CACM significantly outperforms baselines (\\\\(\\\\sim 6\\\\%\\\\) absolute improvement).\"}"}
{"id": "uyqks-LILZX", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Incorrect constraints hurt generalization. We now directly compare the effect of using correct versus incorrect (but commonly used) constraints for a dataset. To isolate the effect of a single constraint, we first consider the single-attribute shift on \\\\( A \\\\) cause and compare the application of different regularizer constraints. Proposition 3.1 provides the correct constraint for \\\\( A \\\\) cause:\\n\\n\\\\[\\nX_c \\\\perp \\\\perp A_{\\\\text{cause}} | Y, E\\n\\\\]\\n\\nIn addition, using \\\\( d \\\\)-separation on the Causal-realized DAG from Figure 2, we see the following invalid constraints,\\n\\n\\\\[\\nX_c \\\\perp \\\\perp A_{\\\\text{cause}} | E, \\\\quad X_c \\\\perp \\\\perp A_{\\\\text{cause}}.\\n\\\\]\\n\\nWithout knowing that the DGP corresponds to a Causal shift, one may apply these constraints that do not condition on the class. Results on small NORB (Table 3) show that using the incorrect constraint has an adverse effect: the correct constraint yields 85% accuracy while the best incorrect constraint achieves 79.7%. Moreover, unlike the correct constraint, application of the incorrect constraint is sensitive to the \\\\( \\\\lambda \\\\) (regularization weight) parameter: as \\\\( \\\\lambda \\\\) increases, accuracy drops to less than 40% (Suppl. E.3, Figure 7).\\n\\nComparing these constraints on small NORB and MNIST (Table 4) reveals the importance of making the right structural assumptions. Typically, DG algorithms assume that distribution of causal features \\\\( X_c \\\\) does not change across domains (as in the graph in Fig. 2a). Then, both \\\\( X_c \\\\perp \\\\perp A_{\\\\text{cause}} | Y, E \\\\) and \\\\( X_c \\\\perp \\\\perp A_{\\\\text{cause}} | Y \\\\) should be correct constraints. However, conditioning on both \\\\( Y \\\\) and \\\\( E \\\\) provides a 5% point gain over conditioning on \\\\( Y \\\\) in NORB while the accuracy is comparable for MNIST. Information about the data-generating process explains the result: Different domains in MNIST include samples from the same distribution whereas small NORB domains are sampled from a different set of toy objects, thus creating a correlation between \\\\( X_c \\\\) and \\\\( E \\\\), corresponding to the graph in Fig. 2b. Without information on the correct DGP, such gains will be difficult.\\n\\nFinally, we replicate the above experiment for the multi-attribute shift setting for small NORB. To construct an incorrect constraint, we interchange the variables before inputting to CACM algorithm (\\\\( A_{\\\\text{ind}} \\\\) gets used as \\\\( A_{\\\\text{cause}} \\\\) and vice-versa). Accuracy with interchanged variables (65.1 \u00b1 1.6) is lower than that of correct CACM (69.6 \u00b1 1.6). More ablations where baseline DG algorithms are provided CACM-like attributes as environment are in Suppl. E.4.\\n\\n### Related Work\\n\\nImproving the robustness of models in the face of distribution shifts is a key challenge. Several works have attempted to tackle the domain generalization problem (Wang et al., 2021; Zhou et al., 2021) using different approaches \u2013 data augmentation (Cubuk et al., 2020; He et al., 2016; Zhu et al., 2017), and representation learning (Arjovsky et al., 2019; Deng et al., 2009; Higgins et al., 2017) being popular ones. Trying to gauge the progress made by these approaches, Gulrajani and Lopez-Paz (Gulrajani & Lopez-Paz, 2021) find that existing state-of-the-art DG algorithms do not improve over ERM. More recent work (Wiles et al., 2022; Ye et al., 2022) uses datasets with different single-attribute shifts and empirically shows that different algorithms perform well over different distribution shifts, but no single algorithm performs consistently across all. We provide (1) multi-attribute shift benchmark datasets; (2) a causal interpretation of different kinds of shifts; and (3) an adaptive algorithm to identify the correct regularizer. While we focus on images, OoD generalization on graph data is also challenged by multiple types of distribution shifts (Chen et al., 2022).\\n\\n### Causally-motivated learning.\\n\\nThere has been recent work focused on causal representation learning (Arjovsky et al., 2019; Krueger et al., 2021; Locatello et al., 2020; Sch\u00f6lkopf et al., 2021) for OoD generalization. While these works attempt to learn the constraints for causal features from input features, we show that it is necessary to model the data-generating process and have access to auxiliary attributes to obtain a risk-invariant predictor, especially in multi-attribute distribution shift setups. Recent research has shown how causal graphs can be used to characterize and analyze the different kinds of distribution shifts that occur in real-world settings (Makar et al., 2022; Veitch et al., 2021). Our approach is similar in motivation but we extend from single-domain, single-attribute setups in past work to formally introduce multi-attribute distribution shifts in more complex and real-world settings. Additionally, we do not restrict ourselves to binary-valued classes and attributes.\\n\\n### Discussion\\n\\nWe introduced CACM, an adaptive OoD generalization algorithm to characterize multi-attribute shifts and apply the correct independence constraints. Through empirical experiments and theoretical analysis, we show the importance of modeling the causal relationships in the data-generating process. That said, our work has limitations: the constraints from CACM are necessary but not sufficient for a risk-invariant predictor (e.g., unable to remove influence of unobserved spurious attributes).\"}"}
{"id": "uyqks-LILZX", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proposition 3.1, Corollary 3.1) hold for any of these relationships. To see why, note that there is no collider introduced on $X_c$ or $E$ in any of the above cases.\\n\\nFigure 8: (a) Causal, (b) Confounded and (c) Selection mechanisms leading to $E - X_c$ correlation.\\n\\nFigure 9: Corresponding anti-causal graphs for Figure 2. Note the graphs are identical to Figure 2 with the exception of the causal arrow pointing from $Y \\\\rightarrow X_c$ instead of from $X_c \\\\rightarrow Y$.\\n\\nFigure 9 shows causal graphs used for specifying multi-attribute distribution shifts in an anti-causal setting. These graphs are identical to Figure 2, with the exception of change in direction of causal arrow from $X_c \\\\rightarrow Y$ to $Y \\\\rightarrow X_c$.\\n\\nWe derive the (conditional) independence constraints for the anti-causal DAG for Independent, Causal, Confounded and Selected shifts.\\n\\nProposition G.1. Given a causal DAG realized from the canonical graph in Figure 9a, the correct constraint depends on the relationship of label $Y$ with the nuisance attributes $A$. As shown, $A$ can be split into $A_{ind}, A_{ind}$ and $E$, where $A_{ind}$ can be further split into subsets that have a causal ($A_{cause}$), confounded ($A_{conf}$), selected ($A_{sel}$) relationship with $Y$ ($A_{ind} = A_{cause} \\\\cup A_{conf} \\\\cup A_{sel}$). Then, the (conditional) independence constraints that $X_c$ should satisfy are,\\n\\n1. Independent: $X_c \\\\perp \\\\perp A_{ind}$; $X_c \\\\perp \\\\perp E$; $X_c \\\\perp \\\\perp A_{ind} | Y$; $X_c \\\\perp \\\\perp A_{ind} | E$; $X_c \\\\perp \\\\perp A_{ind} | Y, E$\\n\\n2. Causal: $X_c \\\\perp \\\\perp A_{cause} | Y$; $X_c \\\\perp \\\\perp E$; $X_c \\\\perp \\\\perp A_{cause} | Y, E$\\n\\n3. Confounded: $X_c \\\\perp \\\\perp A_{conf} | Y$; $X_c \\\\perp \\\\perp E$; $X_c \\\\perp \\\\perp A_{conf} | Y, E$\\n\\n4. Selected: $X_c \\\\perp \\\\perp A_{sel} | Y$; $X_c \\\\perp \\\\perp A_{sel} | Y, E$\\n\\nProof. The proof follows from $d$-separation using the same logic as earlier proof in Section B.2. We observe that for all attributes $A \\\\in A_{ind}$, it is required to condition on $Y$ to obtain valid constraints as $Y$ node appears as a chain or fork in the causal graph but never as a collider due to the $Y \\\\rightarrow X_c$ causal arrow.\\n\\nCorollary G.1. All the above derived constraints are valid for Graph 9a. However, in the presence of a correlation between $E$ and $X_c$ (Graph 9b), only the constraints conditioned on $E$ hold true.\"}"}
{"id": "uyqks-LILZX", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 9: Search space for random hyperparameter sweeps.\\n\\n| Condition         | Sweeps                                    |\\n|-------------------|-------------------------------------------|\\n| **MLP**           | Learning rate: [1e-2, 1e-3, 1e-4, 1e-5]   |\\n|                   | Dropout: 0                                |\\n| **ResNet**        | Learning rate: [1e-2, 1e-3, 1e-4, 1e-5]   |\\n|                   | Dropout: [0, 0.1, 0.5]                    |\\n| MNIST             | Weight decay: 0                           |\\n|                   | Generator weight decay: 10                |\\n|                   | Uniform(\u22126, \u22122)                           |\\n| **IRM**           | \u03bb: [0.01, 0.1, 1, 10, 100]               |\\n|                   | Iterations annealing: [10, 100, 1000]     |\\n| **IB-ERM, IB-IRM**| \u03bb: [0.01, 0.1, 1, 10, 100]               |\\n|                   | Iterations annealing: [10, 100, 1000]     |\\n| **VREx**          | \u03bb: [0.01, 0.1, 1, 10, 100]               |\\n|                   | Iterations annealing: [10, 100, 1000]     |\\n| **MMD**           | \u03bb: [0.1, 1, 10, 100]                      |\\n|                   | \u03b3: [0.01, 0.0001, 0.000001]               |\\n| **CORAL**         | \u03bb: [0.1, 1, 10, 100]                      |\\n| **DANN, CDANN**   | Generator learning rate: [1e-2, 1e-3, 1e-4, 1e-5] |\\n|                   | Discriminator learning rate: [1e-2, 1e-3, 1e-4, 1e-5] |\\n|                   | Discriminator weight decay: 10            |\\n|                   | Uniform(\u22126, \u22122)                           |\\n| **\u03bb**             | Discriminator steps: [1, 2, 4, 8]         |\\n| **gradient penalty**| [0.01, 0.1, 1, 10]                      |\\n| **adam \u03b21**       | [0, 0.5]                                  |\\n| **C-MMD**         | \u03bb: [0.1, 1, 10, 100]                      |\\n|                   | \u03b3: [0.01, 0.0001, 0.000001]               |\\n| **GroupDRO**      | \u03b7: [0.001, 0.01, 0.1]                     |\\n| **Mixup**         | \u03b1: [0.1, 1.0, 10.0]                       |\\n| **MLDG**          | \u03b2: [0.1, 1.0, 10.0]                       |\\n| **SagNet**        | Adversary weight: [0.01, 0.1, 1.0, 10.0]  |\\n| **RSC feature drop percentage** | Uniform(0, 0.5)                          |\\n| **batch drop percentage** | Uniform(0, 0.5)                          |\\n| **CACM**          | \u03bb: [0.1, 1, 10, 100]                      |\\n|                   | \u03b3: [0.01, 0.0001, 0.000001]               |\\n\\n27\"}"}
{"id": "uyqks-LILZX", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unlike the full causal graph, attribute values as well as the relationships between class labels and attributes is often known. CACM assumes access to attribute labels only during training time, which are collected as part of the data collection process (e.g., as metadata with training data (Makar et al., 2022)). We start by discussing the availability of attributes in WILDS (Koh et al., 2021), a set of real-world datasets adapted for the domain generalization setting. Attribute labels available in the datasets include, the time (year) and region associated with satellite images in FMoW dataset (Christie et al., 2018) for predicting land use category, hospital from where the tissue patch was collected for tumor detection in Camelyon17 dataset (Bandi et al., 2018) and the demographic information for CivilComments dataset (Borkan et al., 2019). (Koh et al., 2021) create different domains in WILDS using this metadata, consistent with our definition of $E \\\\in A$ as a special domain attribute.\\n\\nIn addition, CACM requires the type of relationship between label $Y$ and attributes. This is often known, either based on how the dataset was collected or inferred based on domain knowledge or observation. While the distinction between $A_{\\\\text{ind}}$ and $A_{\\\\text{ind}}$ can be established using a statistical test of independence on a given dataset, in general, the distinction between $A_{\\\\text{cause}}, A_{\\\\text{sel}}$ and $A_{\\\\text{conf}}$ within $A_{\\\\text{ind}}$ needs to be provided by the user. As we show for the above datasets, the type of relationship can be inferred based on common knowledge or information on how the dataset was collected.\\n\\nFor FMoW dataset, time can be considered an Independent attribute ($A_{\\\\text{ind}}$) since it reflects the time at which images are captured which is not correlated with $Y$; whereas region is a Confounded attribute since certain regions associated with certain $Y$ labels are over-represented due to ease of data collection. Note that region cannot lead to Causal shift since the decision to take images in a region was not determined by the final label nor Selected for the same reason that the decision was not taken based on values of $Y$.\\n\\nSimilarly, for the Camelyon17 dataset, it is known that differences in slide staining or image acquisition leads to variation in tissue slides across hospitals, thus implying that hospital is an Independent attribute ($A_{\\\\text{ind}}$) (Koh et al., 2021; Komura & Ishikawa, 2018; Tellez et al., 2019). As another example from healthcare, a study in MIT Technology Review discusses biased data where a person\u2019s position ($A_{\\\\text{conf}}$) was spuriously correlated with disease prediction as patients lying down were more likely to be ill.\\n\\nAs another example, (Sagawa et al., 2020) adapt MultiNLI dataset for OoD generalization due to the presence of spurious correlation between negation words (attribute) and the contradiction label between \u201cpremise\u201d and \u201chypothesis\u201d inputs. Here, negation words are a result of the contradiction label (Causal shift), however this relationship between negation words and label may not always hold.\\n\\nFinally, for the CivilComments dataset, we expect the demographic features to be Confounded attributes as there could be biases which result in spurious correlation between comment toxicity and demographic information.\\n\\nTo provide examples showing the availability of attributes and their type of relationship with the label, Table 5 lists some popular datasets used for DG and the associated auxiliary information present as metadata. In addition to above discussed datasets, we include the popularly used Waterbirds dataset (Sagawa et al., 2020) where the type of background (land/water) is assigned to bird images based on bird label; hence, being a Causal attribute (results on Waterbirds dataset are in Table 2).\\n\\nTable 5: Commonly used DG datasets include auxiliary information.\\n\\n| Dataset | Attribute(s) | $Y \\\\rightarrow A$ relationship |\\n|---------|--------------|-------------------------------|\\n| FMoW-WILDS (Koh et al., 2021) | time | $A_{\\\\text{ind}}$ |\\n| Camelyon17-WILDS (Koh et al., 2021) | hospital | $A_{\\\\text{ind}}$ |\\n| Waterbirds (Sagawa et al., 2020) | background (land/water) | $A_{\\\\text{cause}}$ |\\n| MultiNLI (Sagawa et al., 2020) | negation word | $A_{\\\\text{cause}}$ |\\n| CivilComments-WILDS (Koh et al., 2021) | demographic | $A_{\\\\text{conf}}$ |\"}"}
{"id": "uyqks-LILZX", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Theorem 2.1. Consider a causal DAG $G$ over $\\\\langle X_c, X, A, Y \\\\rangle$ and a corresponding generated dataset $(x_i, a_i, y_i)_{i=1}^n$, where $X_c$ is unobserved. Assume that graph $G$ has the following property: $X_c$ is defined as the set of all parents of $Y$ ($X_c \\\\rightarrow Y$); and $X_c$ and $A$ together cause $X$ ($X_c \\\\rightarrow X$, and $A \\\\rightarrow X$). The graph may have any other edges (see, e.g., DAG in Figure 1(b)). Let $P_G$ be the set of distributions consistent with graph $G$, obtained by changing $P(A | Y)$ but not $P(Y | X_c)$. Then the conditional independence constraints satisfied by $X_c$ are necessary for a (cross-entropy) risk-invariant predictor over $P_G$. That is, if a predictor for $Y$ does not satisfy any of these constraints, then there exists a data distribution $P' \\\\in P_G$ such that predictor's risk will be higher than its risk in other distributions.\\n\\nProof. We consider $X, Y, X_c, A$ as random variables that are generated according to the data-generating process corresponding to causal graph $G$. We assume that $X_c$ represents all the parents of $Y$. $X_c$ also causes the observed features $X$ but $X$ may be additionally affected by the attributes $A$. Let $\\\\hat{y} = g(x)$ be a candidate predictor. Then $g(X)$ represents a random vector based on a deterministic function $g$ of $X$.\\n\\nSuppose there is an independence constraint $\\\\psi$ that is satisfied by $X_c$ but not $g(X)$. Below we show that such a predictor $g$ is not risk-invariant: there exist two data distributions with different $P(A | Y)$ such that the risk of $g$ is different for them. Without loss of generality, we can write $g(x)$ as, $g(x) = (g(x)/h(x_c)) \\\\cdot h(x_c) = g'(x, x_c) h(x_c) \\\\forall x$ (3) where $h$ is an arbitrary, non-zero, deterministic function of the random variable $X_c$. Since $X_c$ satisfies the (conditional) independence constraint $\\\\psi$ and $h$ is a deterministic function, $h(x_c)$ also satisfies $\\\\psi$. Also since the predictor $g(X)$ does not satisfy the constraint $\\\\psi$, it implies that the random vector $g'(x, x_c)$ cannot satisfy the constraint $\\\\psi$. Thus, $g'(x, x_c)$ cannot be a function of $X_c$ only; it needs to depend on $X$ too. Since $X$ has two parents in the causal graph, $X_c$ and $A$, this implies that $g'(x, x_c)$ must depend on $A$ too, and hence $g'(x, x_c)$ and $A$ are not independent.\\n\\nNow, let us construct two data distributions $P_1$ and $P_2$ with the same marginal distributions of $P(Y)$, $P(A)$ and $P(X_c)$, such that $P(A | Y)$ changes across them. Note that $P(Y | X_c)$ stays invariant because of the independent and stable causal mechanism property, i.e., $P_1(Y | X_c) = P_2(Y | X_c)$. For these two data distributions, change in $P(A | Y)$ implies a change in $P(Y | A)$, i.e., $P_1(Y | A) \\\\neq P_2(Y | A)$, since $P(Y | A) = P(A | Y) P(Y) / P(A)$. Also, since $g'(x, x_c)$ and $A$ are not independent, $P(Y | g'(x, x_c))$ will change, i.e., $P_1(Y | g'(x, x_c)) \\\\neq P_2(Y | g'(x, x_c))$.\\n\\nThe risk over any distribution $P$ can be written as (using the cross-entropy loss),\\n\\n$$R_P(g) = \\\\mathbb{E}_{P}[\\\\ell(Y, g'(X, X_c))h(X_c)] = -\\\\mathbb{E}_{P}[X_y y \\\\log g'(X, X_c)h(X_c)] - \\\\mathbb{E}_{P}[X_y y \\\\log h(X_c)] \\\\quad (4)$$\\n\\nThe risk difference is,\\n\\n$$R_{P_2}(g) - R_{P_1}(g) = \\\\mathbb{E}_{P_1}[X_y y \\\\log g'(X, X_c)] - \\\\mathbb{E}_{P_2}[X_y y \\\\log g'(X, X_c)] + \\\\mathbb{E}_{P_1}[X_y y \\\\log h(X_c)] - \\\\mathbb{E}_{P_2}[X_y y \\\\log h(X_c)]$$\\n\\nIn practice, the constraint may be evaluated on an intermediate representation of $g$, such that $g$ can be written as, $g(X) = g_1(\\\\phi(X))$ where $\\\\phi$ denotes the representation function. However, for simplicity, we assume it is applied on $g(X)$. \"}"}
{"id": "uyqks-LILZX", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where the third and fourth terms cancel out because $P_1(X_c, Y) = P_2(X_c, Y)$ and thus the risk of $h(X_c)$ is the same across $P_1$ and $P_2$. However, the risk for $g'(X, X_c)$ is not the same since $P_1(Y|g'(X, X_c)) \\\\neq P_2(Y|g'(X, X_c))$. Thus the absolute risk difference is non-zero, $|R_{P_2(g)} - R_{P_1(g)}| > 0$, and $g$ is not a risk-invariant predictor. Hence, satisfying conditional independencies that $X_c$ satisfies is necessary for a risk-invariant predictor.\\n\\nRemark. In the above Theorem, we considered the case where $P(A|Y)$ changes across distributions. In the case where $A$ and $Y$ are independent, $P(A|Y) = P(A)$ and thus $P(A)$ would change across distributions while $P(Y|A) = P(Y)$ remained constant. Since $g'(X, X_c)$ depends on $A$ and $X_c$, we obtain $P_1(Y|g'(X, X_c)) = P_2(Y|g'(X, X_c))$. However, the risk difference can still be non-zero since $P_1(A) \\\\neq P_2(A)$ and the risk expectation $E_{P_1}[P_y y \\\\log g'(X, X_c)]$ is over $P(Y, X_c, A)$.\\n\\nB.2 Proof of Proposition 3.1\\n\\nProposition 3.1. Given a causal DAG realized by specifying the target-attributes relationship in Figure 2a, the correct constraint depends on the relationship of label $Y$ with the attributes $A$. As shown, $A$ can be split into $A_{ind}$, $A_{conf}$, and $E$, where $A_{ind}$ can be further split into subsets that have a causal ($A_{cause}$), confounded ($A_{conf}$), selected ($A_{sel}$) relationship with $Y$ ($A_{ind} = A_{cause} \\\\cup A_{conf} \\\\cup A_{sel}$). Then, the (conditional) independence constraints $X_c$ should satisfy are,\\n\\n1. Independent: $X_c \\\\perp \\\\perp A_{ind}$; $X_c \\\\perp \\\\perp E$; $X_c \\\\perp \\\\perp A_{ind} | Y$; $X_c \\\\perp \\\\perp A_{ind} | E$\\n\\n2. Causal: $X_c \\\\perp \\\\perp A_{cause} | Y$; $X_c \\\\perp \\\\perp E$; $X_c \\\\perp \\\\perp A_{cause} | Y, E$\\n\\n3. Confounded: $X_c \\\\perp \\\\perp A_{conf}$; $X_c \\\\perp \\\\perp E$; $X_c \\\\perp \\\\perp A_{conf} | E$\\n\\n4. Selected: $X_c \\\\perp \\\\perp A_{sel} | Y$; $X_c \\\\perp \\\\perp A_{sel} | Y, E$\\n\\nProof. The proof follows from d-separation (Pearl, 2009) on the causal DAGs realized from Figure 2a. For each condition, Independent, Causal, Confounded, and Selected, we provide the realized causal graphs below and derive the constraints.\\n\\n![Causal graphs for distinct distribution shifts based on $Y$-$A$ relationship.](image)\\n\\n(a) Independent shift\\n(b) Causal shift\\n(c) Confounded shift\\n(d) Selected shift\\n\\nFigure 3: Causal graphs for distinct distribution shifts based on $Y$-$A$ relationship.\"}"}
{"id": "uyqks-LILZX", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nfrom $X$ to $A$, which results in the remaining constraints:\\n\\n$X \\\\perp \\\\perp A \\\\mid Y$ and $X \\\\perp \\\\perp A \\\\mid E$ and $X \\\\perp \\\\perp A \\\\mid Y, E$. Hence, we obtain,\\n\\n$X \\\\perp \\\\perp A \\\\mid Y$ and $X \\\\perp \\\\perp E$ and $X \\\\perp \\\\perp A \\\\mid Y$ and $X \\\\perp \\\\perp A \\\\mid E$.\\n\\nCausal:\\n\\nFrom Figure 3b, we see that while the path $X \\\\rightarrow X \\\\rightarrow A$ cause contains a collider $X$, $X \\\\not\\\\perp \\\\perp A$ cause due to the presence of node $Y$ as a chain. By the d-separation criteria, $X$ and $A$ cause are conditionally independent given $Y \\\\Rightarrow X \\\\perp \\\\perp A$ cause $| Y$. Additionally, conditioning on $E$ is valid since $E$ does not appear as a collider on any paths between $X$ and $A$ cause $= \\\\Rightarrow X \\\\perp \\\\perp A$ cause $| Y, E$. We get the constraint $X \\\\perp \\\\perp E$ since all paths connecting $X$ to $E$ contain a collider (collider $X$ in $X \\\\rightarrow X \\\\rightarrow A$ cause $\\\\rightarrow E$, collider $A$ cause in $X \\\\rightarrow Y \\\\rightarrow C \\\\rightarrow A$ cause $\\\\rightarrow E$). Hence, we obtain,\\n\\n$X \\\\perp \\\\perp A$ cause $| Y$ and $X \\\\perp \\\\perp E$ and $X \\\\perp \\\\perp A$ cause $| Y, E$.\\n\\nConfounded:\\n\\nFrom Figure 3c, we see that all paths connecting $X$ and $A$ conf contain a collider (collider $X$ in $X \\\\rightarrow X \\\\rightarrow A$ conf, collider $Y$ in $X \\\\rightarrow Y \\\\rightarrow C \\\\rightarrow A$ conf). Hence, $X \\\\perp \\\\perp A$ conf. Additionally, conditioning on $E$ is valid since $E$ does not appear as a collider on any paths between $X$ and $A$ conf $= \\\\Rightarrow X \\\\perp \\\\perp A$ conf $| E$. We get the constraint $X \\\\perp \\\\perp E$ since all paths connecting $X$ and $E$ also contain a collider (collider $X$ in $X \\\\rightarrow X \\\\rightarrow A$ conf $\\\\rightarrow E$, collider $Y$ in $X \\\\rightarrow Y \\\\rightarrow C \\\\rightarrow A$ conf $\\\\rightarrow E$). Hence, we obtain,\\n\\n$X \\\\perp \\\\perp A$ conf $| E$ and $X \\\\perp \\\\perp E$ and $X \\\\perp \\\\perp A$ conf $| E$.\\n\\nSelected:\\n\\nFor the observed data, the selection variable is always conditioned on, with $S=1$ indicating inclusion of sample in data. The selection variable $S$ is a collider in Figure 3d and we condition on it. Hence, $X \\\\not\\\\perp \\\\perp A$ sel. Conditioning on $Y$ breaks the edge $X \\\\rightarrow Y$, and hence all paths between $X$ and $A$ sel now contain a collider (collider $X$ in $X \\\\rightarrow X \\\\rightarrow A$ sel $\\\\rightarrow E$). Hence, we obtain,\\n\\n$X \\\\perp \\\\perp A$ sel $| Y$ and $X \\\\perp \\\\perp A$ sel $| Y, E$.\\n\\nB.2.1 Proof of Corollary 3.0.1\\n\\nCorollary 3.1. All the above derived constraints are valid for Graph 2a. However, in the presence of a correlation between $E$ and $X$ (Graph 2b), only the constraints conditioned on $E$ hold true.\\n\\nIf there is a correlation between $X$ and $E$, $X \\\\not\\\\perp \\\\perp E$. We can see from Figure 3 that in the presence of $X \\\\rightarrow E$ correlation, $X \\\\not\\\\perp \\\\perp A$ ind; $X \\\\not\\\\perp \\\\perp A$ ind $| Y$ (3a), $X \\\\not\\\\perp \\\\perp A$ cause $| Y$ (3b), $X \\\\not\\\\perp \\\\perp A$ conf (3c) and $X \\\\not\\\\perp \\\\perp A$ sel $| Y$ (3d). Hence, conditioning on environment $E$ is required for the valid independence constraints.\\n\\nB.3 Proof of Theorem 3.1\\n\\nTheorem 3.1. Under the canonical causal graph in Figure 2(a,b), there exists no (conditional) independence constraint over $\\\\langle X_c, A, Y \\\\rangle$ that is valid for all realized DAGs as the type of multi-attribute shifts vary. Hence, for any predictor algorithm for $Y$ that uses a single (conditional) independence constraint over its representation $\\\\phi(X)$, $A$ and $Y$, there exists a realized DAG $G$ and a corresponding training dataset such that the learned predictor cannot be a risk-invariant predictor for distributions in $P_G$, where $P_G$ is the set of distributions obtained by changing $P(A|Y)$.\\n\\nProof. The proof follows from an application of Proposition 3.1 and Theorem 2.1.\\n\\nFirst claim. Under the canonical graph from Figure 2(a or b), the four types of attribute shifts possible are Independent, Causal, Confounded and Selected. From the constraints provided for these four types of attribute shifts in Proposition 3.1, it is easy to observe that there is no single constraint that is satisfied across all four shifts. Thus, given a data distribution (and hence, dataset) with specific types of multi-attribute shifts such that $X_c$ satisfies a (conditional) independence constraint w.r.t.\"}"}
{"id": "uyqks-LILZX", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 1: (a) Our multi-attribute distribution shift dataset Col+Rot-MNIST. We combine Colored MNIST (Arjovsky et al., 2019) and Rotated MNIST (Ghifary et al., 2015) to introduce distinct shifts over Color and Rotation attributes. (b) The causal graph representing the data generating process for Col+Rot-MNIST. Color has a correlation with $Y$ which changes across environments while Rotation varies independently. (c) Comparison with DG algorithms optimizing for different constraints shows the superiority of Causally Adaptive Constraint Minimization (CACM) (full table in Section 5).\"}"}
{"id": "uyqks-LILZX", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We consider the supervised learning setup from (Wiles et al., 2022) where each row of train data \\\\((x_i, a_i, y_i)\\\\) contains input features \\\\(x_i\\\\) (e.g., X-ray pixels), a set of nuisance or spurious attributes \\\\(a_i\\\\) (e.g., vertical shift, hospital) and class label \\\\(y_i\\\\) (e.g., disease diagnosis). The attributes represent variables that are often recorded or implicit in data collection procedures. Some attributes represent a property of the input (e.g., vertical shift) while others represent the domain from which the input was collected (e.g., hospital). The attributes affect the observed input features but do not cause the target label, and hence are spurious attributes. The final classifier \\\\(g(x)\\\\) is expected to use only the input features. However, as new values of attributes are introduced or as the correlation of attributes with the label changes, we obtain different conditional distributions \\\\(P(Y|X)\\\\). Given a set of data distributions \\\\(P\\\\), we assume that the train data is sampled from distributions, \\\\(P_{Etr} = \\\\{P_{E1}, P_{E2}, \\\\ldots\\\\} \\\\subset P\\\\) while the test data is assumed to be sampled from a single unseen distribution, \\\\(P_{Ete} = \\\\{P_{ete}\\\\} \\\\subset P\\\\).\\n\\nAttributes and class labels are assumed to be discrete.\\n\\n2.1 Risk Invariant Predictor for Generalization Under Shifts\\n\\nThe goal is to learn a classifier \\\\(g(x)\\\\) using train domains such that it generalizes and achieves a similar, small risk on test data from unseen \\\\(P_{Ete}\\\\) as it achieves on the train data. Formally, given a set of distributions \\\\(P\\\\), we define a risk-invariant predictor (Makar et al., 2022) as,\\n\\n**Definition 2.1.** Optimal Risk Invariant Predictor for \\\\(P\\\\) (from (Makar et al., 2022))\\n\\nDefine the risk of predictor \\\\(g\\\\) on distribution \\\\(P \\\\in P\\\\) as \\\\(R_P(g) = \\\\mathbb{E}_{x,y \\\\sim P} \\\\ell(g(x), y)\\\\) where \\\\(\\\\ell\\\\) is cross-entropy or another classification loss. Then, the set of risk-invariant predictors obtain the same risk across all distributions \\\\(P \\\\in P\\\\), and set of the optimal risk-invariant predictors is defined as the risk-invariant predictors that obtain minimum risk on all distributions.\\n\\n\\\\[\\ng_{rinv} \\\\in \\\\arg \\\\min_{g \\\\in G_{rinv}} R_P(g) \\\\quad \\\\forall P \\\\in P\\\\]\\n\\n(1)\\n\\nAn intuitive way to obtain a risk-invariant predictor is to consider only the parts of the input features \\\\(X\\\\) that cause the label \\\\(Y\\\\) and ignore any variation due to the spurious attributes. Let such latent, unobserved causal features be \\\\(X_c\\\\). Due to independence and stability of causal mechanisms (Peters et al., 2017), we can assume that \\\\(P(Y|X_c)\\\\) remains invariant across different distributions. Using the notion of risk invariance, we can now define the multi-attribute generalization problem as,\\n\\n**Definition 2.2.** Generalization under Multi-attribute shifts.\\n\\nGiven a target label \\\\(Y\\\\), input features \\\\(X\\\\), attributes \\\\(A\\\\), and latent causal features \\\\(X_c\\\\), consider a set of distributions \\\\(P\\\\) such that \\\\(P(Y|X_c)\\\\) remains invariant while \\\\(P(A|Y)\\\\) changes across individual distributions. Using a training dataset \\\\((x_i, a_i, y_i)_{n=1}^N\\\\) sampled from a subset of distributions \\\\(P_{Etr} \\\\subset P\\\\), the generalization goal is to learn an optimal risk-invariant predictor over \\\\(P\\\\).\\n\\nSpecial case of single-attribute shift. When \\\\(|A| = 1\\\\), we obtain the single-attribute shift problem that is widely studied (Wiles et al., 2022; Ye et al., 2022; Gulrajani & Lopez-Paz, 2021).\\n\\n2.2 A General Principle for Necessary Conditional Independence Constraints\\n\\n| Constraint Statistic Algo. |\\n|---------------------------|\\n| \\\\(\\\\phi \\\\perp \\\\perp E\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\phi\\\\) match \\\\(E\\\\) |\\n| \\\\(\\\\phi\\\\) match \\\\(E\\\\) | \\\\(\\\\"}
{"id": "uyqks-LILZX", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We utilize a strategy from past work (Mahajan et al., 2021; Veitch et al., 2021) to use graph structure of the underlying data-generating process (DGP). We assume that the predictor can be represented as\\n\\\\[ g(x) = g_1(\\\\phi(x)) \\\\]\\nwhere \\\\( \\\\phi \\\\) is the representation. To learn a risk-invariant \\\\( \\\\phi \\\\), we identify the conditional independence constraints satisfied by causal features \\\\( X_c \\\\) in the causal graph and enforce that learnt representation \\\\( \\\\phi \\\\) should follow the same constraints. If \\\\( \\\\phi \\\\) satisfies the constraints, then any function \\\\( g_1(\\\\phi) \\\\) will also satisfy them. Below we show that the constraints are necessary under simple assumptions on the causal DAG representing the DGP for a dataset. All proofs are in Suppl. B.\\n\\n**Theorem 2.1.** Consider a causal DAG \\\\( G \\\\) over \\\\( \\\\langle X_c, X, A, Y \\\\rangle \\\\) and a corresponding generated dataset \\\\( (x_i, a_i, y_i)_{n=1} \\\\) where \\\\( X_c \\\\) is unobserved. Assume that graph \\\\( G \\\\) has the following property:\\n\\\\( X_c \\\\) is defined as the set of all parents of \\\\( Y \\\\) (\\\\( X_c \\\\rightarrow Y \\\\)); and \\\\( X_c \\\\), \\\\( A \\\\) together cause \\\\( X \\\\) (\\\\( X_c \\\\rightarrow X, A \\\\rightarrow X \\\\)). The graph may have any other edges (see, e.g., DAG in Figure 1(b)). Let \\\\( P_G \\\\) be the set of distributions consistent with graph \\\\( G \\\\), obtained by changing \\\\( P(A|Y) \\\\) but not \\\\( P(Y|X_c) \\\\). Then the conditional independence constraints satisfied by \\\\( X_c \\\\) are necessary for a (cross-entropy) risk-invariant predictor over \\\\( P_G \\\\). That is, if a predictor for \\\\( Y \\\\) does not satisfy any of these constraints, then there exists a data distribution \\\\( P' \\\\in P_G \\\\) such that predictor's risk will be higher than its risk in other distributions.\\n\\nThus, given a causal DAG, using \\\\( d \\\\)-separation on \\\\( X_c \\\\) and observed variables, we can derive the correct regularization constraints to be applied on \\\\( \\\\phi \\\\). This yields a general principle to learn a risk-invariant predictor. We use it to theoretically explain the inconsistent results of existing DG algorithms (Sec. 3.3) and to propose an Out-of-Distribution generalization algorithm CACM (Sec. 4).\\n\\nNote that constraints from CACM are necessary but not sufficient as \\\\( X_c \\\\) is not identifiable.\\n\\n3 STUDYING DISTRIBUTION SHIFTS THROUGH A CANONICAL CAUSAL GRAPH\\n\\n3.1 CANONICAL CAUSAL GRAPH FOR COMMON DISTRIBUTION SHIFTS\\n\\nWe consider a canonical causal graph (Figure 2) to specify the common data-generating processes that can lead to a multi-attribute shift dataset. Shaded nodes represent observed variables \\\\( X, Y \\\\); and the sets of attributes \\\\( A_{ind}, \\\\overline{A_{ind}}, \\\\) and \\\\( E \\\\) such that \\\\( A_{ind} \\\\cup \\\\overline{A_{ind}} \\\\cup \\\\{E\\\\} = A \\\\). \\\\( A_{ind} \\\\) represents the attributes correlated with label, \\\\( \\\\overline{A_{ind}} \\\\) the attributes that are independent of label, while \\\\( E \\\\) is a special attribute for the domain/environment from which a data point was collected. Not all attributes need to be observed. For example, in some cases, only \\\\( E \\\\) and a subset of \\\\( A_{ind}, \\\\overline{A_{ind}} \\\\) may be observed. In other cases, only \\\\( A_{ind} \\\\) and \\\\( \\\\overline{A_{ind}} \\\\) may be observed while \\\\( E \\\\) is not available. Regardless, we assume that all attributes, along with the causal features \\\\( X_c \\\\), determine the observed features \\\\( X \\\\). And the features \\\\( X_c \\\\) are the only features that cause \\\\( Y \\\\). In the simplest case, we assume no label shift across environments i.e. marginal distribution of \\\\( Y \\\\) is constant across train domains and test, \\\\( P_{E_{tr}}(y) = P_{E_{te}}(y) \\\\) (see Figure 2a). More generally, different domains may have different distribution of causal features (in the X-ray example, more women visit one hospital) as shown by \\\\( E < \\\\rightarrow X_c \\\\) (Figure 2b).\\n\\nUnder the canonical graph, we characterize different kinds of shifts based on the relationship between spurious attributes \\\\( A_{ind} \\\\) and the classification label \\\\( Y \\\\). Specifically, \\\\( A_{ind} \\\\) is independent of the class label and a change in \\\\( P(A_{ind}) \\\\) leads to an Independent distribution shift. For \\\\( A_{ind} \\\\), there are (a) (b) (c)\"}"}
{"id": "uyqks-LILZX", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"three mechanisms which can introduce the dashed-line correlation between $A_{\\\\text{ind}}$ and $Y$ (Figure 2c) \u2013 direct-causal ($Y$ causing $A_{\\\\text{ind}}$), confounding between $Y$ and $A_{\\\\text{ind}}$ due to a common cause, or selection during the data-generating process. Overall, we define four kinds of shifts based on the causal graph: Independent, Causal, Confounded, and Selected. While the canonical graph in Figure 2a is general, resolving each dashed edge into a specific type of shift (causal mechanism) leads to a realized causal DAG for a particular dataset. As we shall see, knowledge of these shift types is sufficient to determine the correct independence constraints between observed variables.\\n\\nOur canonical multi-attribute graph generalizes the DG graph from Mahajan et al. (2021) that considered an Independent domain/environment as the only attribute. Under the special case of a single attribute ($|A| = 1$), the canonical graph helps interpret the validity of popular DG methods for a dataset by considering the type of the attribute-label relationship in the data-generating process. For example, let us consider two common constraints in prior work on independence between $\\\\phi$ and a spurious attribute: unconditional ($\\\\phi(x) \\\\perp \\\\perp A$) (Veitch et al., 2021; Albuquerque et al., 2020; Ganin et al., 2016) or conditional on the label ($\\\\phi(x) \\\\perp \\\\perp A|Y$) (Ghifary et al., 2016; Hu et al., 2019; Li et al., 2018c;d). Under the canonical graph in Figure 2a, the unconditional constraint is true when $A \\\\perp \\\\perp Y$ ($A \\\\in A_{\\\\text{ind}}$) but not always for $A_{\\\\text{ind}}$ (true only under Confounded shift). If the relationship is Causal or Selected, then the conditional constraint is correct. Critically, as Veitch et al. (2021) show for a single-attribute graph, the conditional constraint is not always better; it is an incorrect constraint (not satisfied by $X_c$) under Confounded setting. Further, under the canonical graph [with E-X$_c$ edge] from Figure 2b, none of these constraints are valid due to a correlation path between $X_c$ and $E$.\\n\\nInferring attributes-label relationship type. Whether an attribute belongs to $A_{\\\\text{ind}}$ or $A_{\\\\text{ind}}$ can be learned from data (since $A_{\\\\text{ind}} \\\\perp \\\\perp Y$). Under some special conditions with the graph in Figure 2a\u2014assuming all attributes are observed and all attributes in $A_{\\\\text{ind}}$ are of the same type\u2014we can also identify the type of $A_{\\\\text{ind}}$ shift: $Y \\\\perp \\\\perp E|A_{\\\\text{ind}}$ implies Selected; if not, then $X_c \\\\perp \\\\perp E|A_{\\\\text{ind}}$, $A_{\\\\text{ind}}$, $Y$ implies Causal, otherwise it is Confounded. In the general case of Figure 2b, however, it is not possible to differentiate between $A_{\\\\text{cause}}$, $A_{\\\\text{conf}}$ and $A_{\\\\text{sel}}$ using observed data and needs manual input. Fortunately, unlike the full causal graph, the type of relationship between label and an attribute is easier to obtain. For example, in text toxicity classification, toxicity labels are found to be spuriously correlated with certain demographics ($A_{\\\\text{ind}}$) (Dixon et al., 2018; Koh et al., 2021; Park et al., 2018); while in medical applications where data is collected from small number of hospitals, shifts arise due to different methods of slide staining and image acquisition ($A_{\\\\text{ind}}$) (Koh et al., 2021; Komura & Ishikawa, 2018; Tellez et al., 2019). Suppl. A contains additional real-world examples with attributes.\\n\\n3.2 Independence constraints depend on attribute \u2194 label relationship\\nWe list the independence constraints between $\\\\langle X_c, A, Y \\\\rangle$ under the canonical graphs from Figure 2, which can be used to derive the correct regularization constraints to be applied on $\\\\phi$ (Theorem 2.1).\\n\\nProposition 3.1. Given a causal DAG realized by specifying the target-attributes relationship in Figure 2a, the correct constraint depends on the relationship of label $Y$ with the attributes $A$.\\n\\nAs shown, $A$ can be split into $A_{\\\\text{ind}}$, $A_{\\\\text{ind}}$ and $E$, where $A_{\\\\text{ind}}$ can be further split into subsets that have a causal ($A_{\\\\text{cause}}$), confounded ($A_{\\\\text{conf}}$), selected ($A_{\\\\text{sel}}$) relationship with $Y$ ($A_{\\\\text{ind}} = A_{\\\\text{cause}} \\\\cup A_{\\\\text{conf}} \\\\cup A_{\\\\text{sel}}$). Then, the (conditional) independence constraints $X_c$ should satisfy are,\\n\\n1. Independent: $X_c \\\\perp \\\\perp A_{\\\\text{ind}}$; $X_c \\\\perp \\\\perp E$; $X_c \\\\perp \\\\perp A_{\\\\text{ind}}|Y$; $X_c \\\\perp \\\\perp A_{\\\\text{ind}}|E$; $X_c \\\\perp \\\\perp A_{\\\\text{ind}}|Y, E$.\\n2. Causal: $X_c \\\\perp \\\\perp A_{\\\\text{cause}}|Y$; $X_c \\\\perp \\\\perp E$; $X_c \\\\perp \\\\perp A_{\\\\text{cause}}|Y, E$.\\n3. Confounded: $X_c \\\\perp \\\\perp A_{\\\\text{conf}}$; $X_c \\\\perp \\\\perp E$; $X_c \\\\perp \\\\perp A_{\\\\text{conf}}|E$.\\n4. Selected: $X_c \\\\perp \\\\perp A_{\\\\text{sel}}|Y$; $X_c \\\\perp \\\\perp A_{\\\\text{sel}}|Y, E$.\\n\\nCorollary 3.1. All the above derived constraints are valid for Graph 2a. However, in the presence of a correlation between $E$ and $X_c$ (Graph 2b), only the constraints conditioned on $E$ hold true.\\n\\nCorollary 3.1 implies that if we are not sure about $E$-$X_c$ correlation, $E$-conditioned constraints should be used. By considering independence constraints over attributes that may represent any observed variable, our graph-based characterization unites the single-domain (group-wise) (Sagawa et al., 2020) and multi-domain generalization tasks. Whether attributes represent auxiliary attributes, group indicators, or data sources, Proposition 3.1 provides the correct regularization constraint.\"}"}
