{"id": "chDrutUTs0K", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 5: POPGym baselines.\"}"}
{"id": "chDrutUTs0K", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MLP\\nPosMLP\\nFWP\\nFART\\nS4D\\nTCN\\nFr.Stack\\nLMU\\nIndRNN\\nElman\\nGRU\\nLSTM\\nDNC\\nModel\\nCountRecallEasy\\nCountRecallMedium\\nCountRecallHard\\nHigherLowerEasy\\nHigherLowerMedium\\nHigherLowerHard\\n\\nFigure 6: POPGym baselines (continued)\"}"}
{"id": "chDrutUTs0K", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 7: POPGym baselines (continued)\"}"}
{"id": "chDrutUTs0K", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: POPGym baselines (continued)\"}"}
{"id": "chDrutUTs0K", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"POPGYM: BENCHMARKING PARTIALLY OBSERVABLE REINFORCEMENT LEARNING\\n\\nSteven Morad, Ryan Kortvelesy, Matteo Bettini, Stephan Liwicki, Amanda Prorok\\n\\nABSTRACT\\nReal world applications of Reinforcement Learning (RL) are often partially observable, thus requiring memory. Despite this, partial observability is still largely ignored by contemporary RL benchmarks and libraries. We introduce Partially Observable Process Gym (POPGym), a two-part library containing (1) a diverse collection of 15 partially observable environments, each with multiple difficulties and (2) implementations of 13 memory model baselines \u2013 the most in a single RL library. Existing partially observable benchmarks tend to fixate on 3D visual navigation, which is computationally expensive and only one type of POMDP. In contrast, POPGym environments are diverse, produce smaller observations, use less memory, and often converge within two hours of training on a consumer-grade GPU. We implement our high-level memory API and memory baselines on top of the popular RLlib framework, providing plug-and-play compatibility with various training algorithms, exploration strategies, and distributed training paradigms. Using POPGym, we execute the largest comparison across RL memory models to date. POPGym is available at https://github.com/proroklab/popgym.\\n\\n1 INTRODUCTION\\nDatasets like MNIST (Lecun et al., 1998) have driven advances in Machine Learning (ML) as much as new architectural designs (Levine et al., 2020). Comprehensive datasets are paramount in assessing the progress of learning algorithms and highlighting shortcomings of current methodologies. This is evident in the context of RL, where the absence of fast and comprehensive benchmarks resulted in a reproducability crisis (Henderson et al., 2018). Large collections of diverse environments, like the Arcade Learning Environment, OpenAI Gym, ProcGen, and DMLab provide a reliable measure of progress in deep RL. These fundamental benchmarks play a role in RL equivalent to that of MNIST in supervised learning (SL).\\n\\nThe vast majority of today's RL benchmarks are designed around Markov Decision Processes (MDPs). In MDPs, agents observe a Markov state, which contains all necessary information to solve the task at hand. When the observations are Markov states, the Markov property is satisfied, and traditional RL approaches guarantee convergence to an optimal policy (Sutton & Barto, 2018, Chapter 3). But in many RL applications, observations are ambiguous, incomplete, or noisy \u2013 any of which makes the MDP partially observable (POMDP) (Kaelbling et al., 1998), breaking the Markov property and all convergence guarantees. Furthermore, Ghosh et al. (2021) find that policies trained under the ideal MDP framework cannot generalize to real-world conditions when deployed, with epistemic uncertainty turning real-world MDPs into POMDPs. By introducing memory (referred to as sequence to sequence models in SL), we can summarize the observations therefore restoring policy convergence guarantees for POMDPs (Sutton & Barto, 2018, Chapter 17.3).\\n\\nDespite the importance of memory in RL, most of today's comprehensive benchmarks are fully observable or near-fully observable. Existing partially observable benchmarks are often navigation-based \u2013 representing only spatial POMDPs, and ignoring applications like policymaking, disease diagnosis, teaching, and ecology (Cassandra, 1998). The state of memory-based models in RL libraries is even more dire \u2013 we are not aware of any RL libraries that implement more than three\\n\\n1 Strictly speaking, the agent actions are also required to guarantee convergence. We consider the previous action as part of the current observation.\"}"}
{"id": "chDrutUTs0K", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"or four distinct memory baselines. In nearly all cases, these memory models are limited to frame stacking and LSTM.\\n\\nTo date, there are no popular RL libraries that provide a diverse selection of memory models. Of the few existing POMDP benchmarks, even fewer are comprehensive and diverse. As a consequence, there are no large-scale studies comparing memory models in RL. We propose to fill these three gaps with our proposed POPGym.\\n\\n1.1 CONTRIBUTIONS\\n\\nPOPGym is a collection of 15 partially observable gym environments (Figure 1) and 13 memory baselines. All environments come with at least three difficulty settings and randomly generate levels to prevent overfitting. The POPGym environments use low-dimensional observations, making them fast and memory efficient. Many of our baseline models converge in under two hours of training on a single consumer-grade GPU (Table 1, Figure 2). The POPGym memory baselines utilize a simple API built on top of the popular RLlib library (Liang et al., 2018), seamlessly integrating memory models with an assortment of RL algorithms, sampling, exploration strategies, logging frameworks, and distributed training methodologies. Utilizing POPGym and its memory baselines, we execute a large-scale evaluation, analyzing the capabilities of memory models on a wide range of tasks. To summarize, we contribute:\\n\\n1. A comprehensive collection of diverse POMDP tasks.\\n2. The largest collection of memory baseline implementations in an RL library.\\n3. A large-scale, principled comparison across memory models.\\n\\n2 RELATED WORK\\n\\nThere are many existing RL benchmarks, which we categorize as fully (or near-fully) observable and partially observable. In near-fully observable environments, large portions of the Markov state are visible in an observation, though some information may be missing. We limit our literature review to comprehensive benchmarks (those that contain a wide set of tasks), as environment diversity is essential for the accurate evaluation of RL agents (Cobbe et al., 2020).\\n\\n2.1 FULLY AND NEAR-FULLY OBSERVABLE BENCHMARKS\\n\\nThe Arcade Learning Environment (ALE) (Bellemare et al., 2013) wraps Atari 2600 ROMs in a Python interface. Most of the Atari games, such as Space Invaders or Missile Command are fully observable (Cobbe et al., 2020). Some games like asteroids require velocity observations, but models can recover full observability by stacking four consecutive observations (Mnih et al., 2015), an approach that does not scale for longer timespans. Even seemingly partially-observable multi-room games like Montezuma's Revenge are made near-fully observable by displaying the player's score and inventory (Burda et al., 2022).\\n\\nOpenAI Gym (Brockman et al., 2016) came after ALE, implementing classic fully observable RL benchmarks like CartPole and MountainCar. Their Gym API found use in many other environments, including our proposed benchmark.\"}"}
{"id": "chDrutUTs0K", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Cobbe et al. (2020) find that randomly generated environments are critical to training general agents, showing policies will overfit to specific levels otherwise. They propose ProcGen: procedurally generated environments with pixel-space observations. Most environments are fully or near-fully observable, although a few environments provide a partially observable mode, effectively turning them into 2D area coverage (navigation) tasks. ProcGen motivates POPGym's use of random level generation.\\n\\n2.2 Partially Observable Benchmarks\\n\\nWhen enumerating partially observable benchmarks, we find many are based on 3D first-person navigation. DeepMind Lab (Beattie et al., 2016) (DMLab) is a 3D first-person view navigation simulator based on the Quake 3 physics engine. It implements various tasks such as collecting fruits, maze exploration, and laser tag. VizDoom (Kempka et al., 2016) is another 3D navigation simulator based on the PC game Doom. It gives the agent weapons and adds computer-controlled characters that can shoot at the player. Miniworld (Chevalier-Boisvert, 2018) is a third 3D first-person view navigation simulator that is easier to install than DMLab or VizDoom. MiniGrid (Chevalier-Boisvert et al., 2018) and GridVerse (Baisero & Katt, 2021) are 2D navigation simulators with a first-person view. Unlike the previously mentioned 3D simulators, agents converge on gridworld environments much faster due to the smaller observation space. This makes it a popular benchmark for memory models.\\n\\nThere are few POMDP libraries that provide tasks beyond navigation. Behaviour suite (BSuite) evaluates agents on a variety of axes, one of which is memory (Osband et al., 2020), but they only provide two POMDPs. Similar to our benchmark, (Zheng & Tellex, 2020) provide classic POMDPs with low-dimensional observation spaces. But their tasks are solvable without neural networks and are not difficult enough for modern deep RL. Ni et al. (2022) provide 21 environments, most of which are a special case of POMDP known as latent MDPs (Kwon et al., 2021), where a specific MDP is chosen from a set of possible MDPs at the beginning of an episode. (Morad et al., 2022) provides three POMDPs, which is insufficient for a benchmark.\\n\\nWe briefly mention the Starcraft (Samvelyan et al., 2019) and VMAS (Bettini et al., 2022) benchmarks because multi-agent environments are intrinsically partially observable, but we focus specifically on single-agent POMDPs.\\n\\n2.3 Shortcomings of Current Benchmarks\\n\\nPopular fully observable benchmarks use pixel-based observation spaces, adding a layer of complexity that takes an order of magnitude longer to train when compared against state-based observation counterparts (Seita, 2020). In fully observable environments, visually pleasing results are worth a few extra hours training. This dogma persists into partial observability, where environments often take 10x longer to converge than their fully observable counterparts. Popular benchmarks using 3D graphics take hundreds of billions of timesteps (Parisotto et al., 2020) and multiple weeks (Morad et al., 2021) on a GPU to train a single policy to convergence. Until sample efficiency in partially observable RL improves, we must forgo pixel-based observations or continue to struggle with reproducibility.\\n\\nMany partially observable tasks with pixel-based observation spaces are based on some form of navigation (Ramani, 2019). Although navigation can be a partially observable task, wall following behavior in perfect mazes guarantees complete area coverage without the need for memory. When mazes are imperfect (i.e. contain cycles), deterministic wall following can get stuck in infinite loops. However, RL policies often have some amount of stochasticity that can break out of these loops. Kadian et al. (2020) and Morad et al. (2021) inadvertently show that memory-free navigation agents learn wall following strategies that are surprisingly effective in imperfect real-world mazes. We confirm these findings with our experiments, showing that memory-free agents are competitive with memory-endowed agents in certain navigation benchmarks.\\n\\nAll other (imperfect) mazes can be fully explored by storing no more than two past locations (observations) in memory (Blum & Kozen, 1978). Navigation-based tasks like area coverage, moving to a coordinate, or searching for items can be reduced to the maze exploration task. We do not claim that memory-free agents are competitive with memory-endowed agents in certain navigation benchmarks.\\n\\n2\\n\\nhttps://en.wikipedia.org/wiki/Maze-solving_algorithm#Wall_follower\\n\\n3\"}"}
{"id": "chDrutUTs0K", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"that navigation tasks are easy, but rather that it is important to have a variety of tasks to ensure we evaluate all facets of memory, such as memory capacity, that navigation tasks might miss.\\n\\n2.4 Existing Memory Baselines\\n\\nThe state of memory models in RL is even more bleak than the benchmarks. Most libraries provide frame stacking and a single type of RNN. OpenAI Baselines (Dhariwal et al., 2017), Stable-Baselines3 (Raffin et al., 2021), and CleanRL (Huang et al., 2021) provide implementations of PPO with frame stacking and an LSTM. Ray RLlib (Liang et al., 2018) currently implements frame stacking, LSTM, and a transformer for some algorithms. Ni et al. (2022) implement LSTM, GRUs, and two model-based memory models. Yang & Nguyen (2021) provides recurrent versions of the DDPG, TD3, and SAC RL algorithms, which utilize GRUs and LSTM. Zheng & Tellex (2020) implement multiple classical POMDP solvers, but these do not use neural networks, preventing their application to more complex tasks. There is currently no go-to library for users who want to compare or apply non-standard memory models.\\n\\n2.5 A Brief Summary on Memory\\n\\nWhen designing a library of memory models, it is important to select competitive models. Ni et al. (2022) show that sequence to sequence models from SL are competitive or better than RL-specific memory methods while being more straightforward to implement, so we focus specifically on sequence to sequence models (called memory throughout the paper). Although a strict categorization of memory is elusive, most methods are based on RNNs, attention, or convolution.\\n\\nRNNs (Elman, 1990) take an input and hidden state, feed them through a network, and produce a corresponding output and hidden state. RNNs depend on the previous state and must be executed sequentially, resulting in slow training but fast inference when compared with other methods.\\n\\nAttention-based methods (Vaswani et al., 2017) have supplanted RNNs in many applications of SL, but traditional transformers have quadratically-scaling memory requirements, preventing them from running on long episodes in RL. Recent linear attention formulations (Schlag et al., 2021; Katharopoulos et al., 2020) claim to produce transformer-level performance in linear time and space, potentially enabling widespread use of attention in RL.\\n\\nLike attention, convolutional methods are computationally efficient (Bai et al., 2018), lending themselves well to RL. They are less common than recurrent or attention-based methods in SL, and there is little literature on their use in RL.\\n\\n3. Popg Environments\\n\\nAll of our environments bound the cumulative episodic reward in $[-1, 1]$. In some cases (e.g. repeating previous observations) an optimal policy would receive a cumulative reward of one in expectation. In other environments (e.g. playing battleship with randomly placed ships), an optimal policy has an expected episodic cumulative reward of less than one.\\n\\nWe tag our proposed environments as diagnostic, control, noisy, game, and navigation. Each tag is designed to represent a different class of POMDP, and each environment has at least three distinct difficulty settings, creating the most diverse POMDP benchmark thus far. Our proposed environments are all overcomplete POMDPs, meaning our environments have more unique latent Markov states than unique observations (Sharan et al., 2017; Jin et al., 2020).\\n\\nDiagnostic environments probe model capabilities with respect to the duration of memories, forgetting, and compression and recall. They are designed to quickly summarize the strengths and weaknesses of a specific model.\\n\\nControl environments are control RL environments made partially observable by removing part of the observation. Solving these tasks only requires short-term memory.\\n\\nNoisy environments require the memory model to estimate the true underlying state by computing an expectation over many observations. These are especially useful for real-world robotics tasks.\"}"}
{"id": "chDrutUTs0K", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model       | LabyrinthEscapeHard | LabyrinthExploreEasy | LabyrinthExploreMedium | LabyrinthExploreHard |\\n|------------|---------------------|----------------------|------------------------|---------------------|\\n| DNC        | -0.827 0.085        | 0.956 0.006          | 0.905 0.005            | 0.720 0.212         |\\n| Elman      | -0.780 0.039        | 0.964 0.001          | 0.873 0.052            | 0.612 0.286         |\\n| FART       | -0.828 0.018        | 0.424 0.465          | -0.197 0.130           | -0.407 0.032        |\\n| FWP        | -0.848 0.016        | -0.152 0.033         | -0.464 0.041           | -0.611 0.044        |\\n| Fr.Stack   | 0.957 0.001         | 0.957 0.001          | 0.847 0.063            | 0.437 0.287         |\\n| GRU        | 0.960 0.001         | 0.960 0.001          | 0.893 0.008            | 0.796 0.003         |\\n| IndRNN     | -0.907 0.030        | -0.789 0.049         | 0.440 0.350            | 0.315 0.412         |\\n| LMU        | -0.762 0.033        | -0.789 0.049         | -0.423 0.185           | -0.407 0.412        |\\n| LSTM       | -0.789 0.049        | -0.789 0.049         | 0.025 0.009            | 0.437 0.009         |\\n| MLP        | 0.968 0.000         | 0.968 0.000          | 0.924 0.001            | 0.968 0.000         |\\n| PosMLP     | 0.964 0.001         | 0.964 0.001          | 0.516 0.191            | 0.964 0.001         |\\n| S4D        | -0.897 0.034        | -0.897 0.034         | 0.964 0.001            | -0.265 0.018        |\\n| TCN        | -0.787 0.011        | -0.787 0.011         | 0.962 0.000            | 0.962 0.000         |\\n|            |                     |                      |                        |                    |\"}"}
{"id": "chDrutUTs0K", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model               | Env. Model | MineSweeperEasy | MineSweeperMedium | MineSweeperHard | MultiarmedBanditEasy |\\n|---------------------|------------|-----------------|-------------------|-----------------|----------------------|\\n| LMU                 | -0.068     | -0.108          | -0.294            | -0.294          | 0.332                |\\n| LSTM                | -0.206     | -0.230          | -0.429            | -0.429          | 0.302                |\\n| MLP                 | 0.858      | -0.158          | -0.289            | -0.289          | 0.631                |\\n| PosMLP              | -0.018     | -0.117          | -0.278            | -0.278          | 0.453                |\\n| S4D                 | -0.721     | -0.365          | -0.430            | -0.430          | -0.158               |\\n| TCN                 | 0.559      | 0.018           | -0.191            | -0.191          | 0.619                |\\n| Elman               | -0.009     | -0.230          | -0.230            | -0.230          | 0.640                |\\n| FART                | -0.176     | -0.390          | -0.390            | -0.390          | 0.474                |\\n| FWP                 | -0.151     | -0.338          | -0.338            | -0.338          | 0.556                |\\n| Fr.Stack            | -0.177     | -0.338          | -0.338            | -0.338          | 0.476                |\\n| GRU                 | -0.006     | -0.206          | -0.206            | -0.206          | 0.693                |\\n| IndRNN              | -0.024     | -0.247          | -0.247            | -0.247          | 0.625                |\\n| LMU                 | 0.689      | -0.108          | -0.294            | -0.294          | 0.332                |\\n| LSTM                | 0.686      | -0.110          | -0.303            | -0.303          | 0.527                |\\n| MLP                 | 0.251      | -0.158          | -0.289            | -0.289          | 0.626                |\"}"}
{"id": "chDrutUTs0K", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Model                      | NoisyStatelessCartPoleEasy | NoisyStatelessCartPoleMedium |\\n|---------------------------|----------------------------|-----------------------------|\\n| Elman                     | 1.0                        | 0.64                        |\\n| FART                      | 1.0                        | 0.59                        |\\n| FWP                       | 1.0                        | 0.55                        |\\n| Fr.Stack                  | 0.86                       | 0.45                        |\\n| GRU                       | 1.0                        | 0.64                        |\\n| IndRNN                    | 0.995                      | 0.66                        |\\n| LMU                       | 0.921                      | 0.51                        |\\n| LSTM                      | 0.997                      | 0.64                        |\\n| MLP                       | 0.515                      | 0.302                       |\\n| PosMLP                    | 0.537                      | 0.353                       |\\n| S4D                       | 0.259                      | 0.118                       |\\n| TCN                       | 0.871                      | 0.574                       |\"}"}
{"id": "chDrutUTs0K", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                | NoisyStatelessCartPoleHard | Elman     | FART     | FWP      | Fr.Stack | GRU       | IndRNN    | LSTM     | MLP     | PosMLP   | S4D       | TCN         |\\n|----------------------|----------------------------|-----------|----------|----------|----------|-----------|-----------|----------|---------|----------|-----------|-------------|\\n| DNC                  | 0.233 0.009                | 0.386 0.009 | 0.366 0.002 | 0.354 0.001 | 0.326 0.006 | 0.390 0.007 | 0.404 0.005 | 0.352 0.019 | 0.393 0.002 | 0.229 0.002 | 0.207 0.007 | 0.330 0.004 |\\n| TCN                  | 0.462 0.007                | 0.818 0.056 | 0.610 0.005 | 0.532 0.028 | 0.832 0.015 | 0.894 0.002 | 0.654 0.188 | 0.654 0.055 | 0.786 0.031 | 0.387 0.010 | 0.291 0.022 | 0.811 0.015 |\\n\\nContinued on next page\"}"}
{"id": "chDrutUTs0K", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Selected results used in the discussion section. We standardize the MMER from $[-1, 1]$ to $[0, 1]$ for readability. The colored bars denote the mean and the black lines denote the 95% bootstrapped confidence interval. Full results across all environments are in Appendix B.\\n\\nThe highest score on almost all navigation tasks, beating all memory models (Figure 4). This is in line with our hypothesis from subsection 2.3, and raises doubts concerning previous models evaluated solely on navigation tasks. Does a novel memory method outperform baselines because of a better memory architecture, or just because it has more trainable parameters? Future work can bypass this scrutiny by including a diverse set of tasks beyond navigation, and by modifying simple navigation tasks to better leverage memory (e.g., positive reward for correctly answering \\\"how many rooms are there in the house?\\\").\\n\\nPositional MLPs are an important baseline. Masked control tasks turn MDPs into POMDPs by hiding the velocity or position portions of classic control problems, and are probably the second most popular type of POMDP in literature after navigation. The positional MLP performed notably better than the MLP, nearly solving the Stateless Cartpole masked control task on easy (Figure 4). This is entirely unexpected, as providing the current timestep to an MLP is insufficient to compute the position and underlying Markov state. Outside of masked control, the positional MLP regularly outperformed the MLP (Figure 3). Stateless policies that evolve over time could be an interesting topic for future work, and should be a standard baseline in future memory comparisons.\\n\\nIs PPO enough? Memory models do not noticeably outperform the MLP in many game environments, such as Autoencode or Battleship, indicating that the memory is minimally effective in these tasks (Figure 4). All thirteen models converge to the nearly same reward, suggesting this could be due to issues with PPO rather than the memory models themselves. Future work could focus on designing new algorithms to solve these tasks. In parallel, research institutions with ample compute could ablate POPGym across other algorithms, such as Recurrent Replay Distributed DQN (Kapturowski et al., 2019).\\n\\n7 CONCLUSION\\n\\nWe presented the POPGym benchmark, a collection of POMDPs and memory baselines designed to standardize RL in partially observable environments. We discovered a notable disconnect between memory performance in supervised and reinforcement learning, with older RNNs surpassing linear transformers and modern memory models. According to our experiments, the GRU is the best general-purpose memory model, with Elman networks providing the best tradeoff between performance and efficiency. We revealed shortcomings in prior benchmarks focused on control and navigation POMDPs, emphasizing the importance of numerous and diverse POMDPs for evaluating memory. There is still a great deal of work to be done towards solving POMDPs, and we hope POPGym provides some measure of progress along the way.\"}"}
{"id": "chDrutUTs0K", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Steven Morad and Stephan Liwicki gratefully acknowledge the support of Toshiba Europe Ltd.\\n\\nRyan Kortvelesy was supported by Nokia Bell Labs through their donation for the Centre of Mobile, Wearable Systems and Augmented Intelligence to the University of Cambridge.\"}"}
{"id": "chDrutUTs0K", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.\\n\\nAndrea Baisero and Sammie Katt. gym-gridverse: Gridworld domains for fully and partially observable reinforcement learning, 2021. URL https://github.com/abaisero/gym-gridverse. Publication Title: GitHub repository.\\n\\nAndrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE transactions on systems, man, and cybernetics (5):834\u2013846, 1983. Publisher: IEEE.\\n\\nCharles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K\u00a8uttler, Andrew Lefrancq, Simon Green, V \u00b4\u0131ctor Vald\u00b4es, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. DeepMind Lab. Technical Report arXiv:1612.03801, arXiv, December 2016. URL http://arxiv.org/abs/1612.03801. arXiv:1612.03801 [cs] type: article.\\n\\nM. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The Arcade Learning Environment: An Evaluation Platform for General Agents. Journal of Artificial Intelligence Research, 47:253\u2013279, June 2013. ISSN 1076-9757. doi: 10.1613/jair.3912. URL https://www.jair.org/index.php/jair/article/view/10819.\\n\\nMatteo Bettini, Ryan Kortvelesy, Jan Blumenkamp, and Amanda Prorok. VMAS: A Vectorized Multi-Agent Simulator for Collective Robot Learning. The 16th International Symposium on Distributed Autonomous Robotic Systems, 2022. Publisher: Springer.\\n\\nManuel Blum and Dexter Kozen. On the power of the compass (or, why mazes are easier to search than graphs). In 19th Annual Symposium on Foundations of Computer Science (sfcs 1978), pp. 132\u2013142, October 1978. doi: 10.1109/SFCS.1978.30. ISSN: 0272-5428.\\n\\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym, 2016. eprint: arXiv:1606.01540.\\n\\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. February 2022. URL https://openreview.net/forum?id=H1lJJnR5Ym.\\n\\nAnthony R Cassandra. A survey of POMDP applications. In Working notes of AAAI 1998 fall symposium on planning with partially observable Markov decision processes, volume 1724, 1998.\\n\\nMaxime Chevalier-Boisvert. MiniWorld: Minimalistic 3D Environment for RL & Robotics Research, 2018. URL https://github.com/maximecb/gym-miniworld. Publication Title: GitHub repository.\\n\\nMaxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic Gridworld Environment for OpenAI Gym, 2018. URL https://github.com/maximecb/gym-minigrid. Publication Title: GitHub repository.\\n\\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\\n\\nKarl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging Procedural Generation to Benchmark Reinforcement Learning. In Proceedings of the 37th International Conference on Machine Learning, pp. 2048\u20132056. PMLR, November 2020. URL https://proceedings.mlr.press/v119/cobbe20a.html. ISSN: 2640-3498.\\n\\nPrafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. OpenAI Baselines, 2017. URL https://github.com/openai/baselines. Publication Title: GitHub repository.\"}"}
{"id": "chDrutUTs0K", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "chDrutUTs0K", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Game environments provide a more natural and thorough evaluation of memory through card and board games. They stress test memory capacity, duration, and higher-level reasoning.\\n\\nNavigation environments are common in other benchmarks, and we include a few to ensure our benchmark is comprehensive. More than anything, our navigation environments examine how memory fares over very long sequences.\\n\\n3.1 ENVIRONMENT DESCRIPTIONS\\n\\n1. Repeat First (Diagnostic): At the first timestep, the agent receives one of four values and a remember indicator. Then it randomly receives one of the four values at each successive timestep without the remember indicator. The agent receives a reward for outputting (remembering) the first value.\\n\\n2. Repeat Previous (Diagnostic): Like Repeat First, observations contain one of four values. The agent is rewarded for outputting the observation from some constant $k$ timesteps ago, i.e. observation $o_{t-k}$ at time $t$.\\n\\n3. Autoencode (Diagnostic): During the WATCH phase, a deck of cards is shuffled and played in sequence to the agent with the watch indicator set. The watch indicator is unset at the last card in the sequence, where the agent must then output the sequence of cards in order. This tests whether the agent can encode a series of observations into a latent state, then decode the latent state one observation at a time.\\n\\n4. Stateless Cartpole (Control): The cartpole environment from Barto et al. (1983), but with the angular and linear positions removed from the observation. The agent must integrate to compute positions from velocity.\\n\\n5. Stateless Pendulum (Control): The swing-up pendulum (Doya, 1995), with the angular position information removed.\\n\\n6. Noisy Stateless Cartpole (Control, Noisy): Stateless Cartpole (Env. 4) with Gaussian noise.\\n\\n7. Noisy Stateless Pendulum (Control, Noisy): Stateless Pendulum (Env. 5) with Gaussian noise.\\n\\n8. Multiarmed Bandit (Noisy, Diagnostic): The multiarmed bandit problem (Slivkins & others, 2019; Lattimore & Szepesv\u00e1ri, 2020) posed as an episodic task. Every episode, bandits are randomly initialized. Over the episode, the player must trade off exploration and exploitation, remembering which bandits pay best. Each bandit has some probability of paying out a positive reward, otherwise paying out a negative reward. Note that unlike the traditional multiarmed bandit task where the bandits are fixed once initialized, these bandits reset every episode, forcing the agent to learn a policy that can adapt between episodes.\\n\\n9. Higher Lower (Game, Noisy): Based on the card game higher-lower, the agent must guess if the next card is of higher or lower rank than the previous card. The next card is then flipped face-up and becomes the previous card. Using memory, the agent can utilize card counting strategies to predict the expected value of the next card, improving the return.\\n\\n10. Count Recall (Game, Diagnostic, Noisy): Each turn, the agent receives a next value and query value. The agent must answer the query with the number of occurrences of a specific value. In other words, the agent must store running counts of each unique observed value, and report a specific count back, based on the query value. This tests whether the agent can learn a compressed structured memory representation, such that it can continuously update portions of memory over a long sequence.\\n\\n11. Concentration (Game): A deck of cards is shuffled and spread out face down. The player flips two cards at a time face up, receiving a reward if the flipped cards match. The agent must remember the value and position of previously flipped cards to improve the rate of successful matches.\\n\\n12. Battleship (Game): A partially observable version of Battleship, where the agent has no access to the board and must derive its own internal representation. Observations contain either HIT or MISS and the position of the last salvo fired. The player receives a positive reward for striking a ship, zero reward for hitting water, and negative reward for firing on a specific tile more than once.\"}"}
{"id": "chDrutUTs0K", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Environment        | Colab FPS | Laptop FPS |\\n|-------------------|-----------|------------|\\n| Repeat First      | 23,895    | 155,201    |\\n| Repeat Previous   | 50,349    | 136,392    |\\n| Autoencode        | 121,756   | 251,997    |\\n| Stateless Cartpole| 73,622    | 218,446    |\\n| Stateless Pendulum| 8,168     | 26,358     |\\n| Noisy Stateless Cartpole | 6,269 | 66,891 |\\n| Noisy Stateless Pendulum | 6,808 | 20,090 |\\n| Multiarmed Bandit | 48,751    | 469,325    |\\n| Battleship        | 117,158   | 235,402    |\\n| Concentration     | 47,515    | 157,217    |\\n| Higher Lower      | 24,312    | 76,903     |\\n| Count Recall      | 16,799    | 53,779     |\\n| Minesweeper       | 8,434     | 32,003     |\\n| Labyrinth Escape  | 1,399     | 41,122     |\\n| Labyrinth Explore | 1,374     | 30,611     |\\n\\n13. **Mine Sweeper (Game):**\\n\\nThe computer game Mine Sweeper, but like our Battleship implementation, the agent does not have access to the board. Each observation contains the position and number of adjacent mines to the last square \\\"clicked\\\" by the agent. Clicking on a mined square will end the game and produce a negative reward. The agent must remember where it has already searched and must integrate information from nearby tiles to narrow down the location of mines. Once the agent has selected all non-mined squares, the game ends.\\n\\n14. **Labyrinth Explore (Navigation):**\\n\\nThe player is placed in a discrete, 2D procedurally-generated maze, receiving a reward for each previously unreached tile it reaches. The player can only observe adjacent tiles. The agent also receives a small negative reward at each timestep, encouraging the agent to reach all squares quickly and end the episode.\\n\\n15. **Labyrinth Escape (Navigation):**\\n\\nThe player must escape the procedurally-generated maze, using the same observation space as Labyrinth Explore. This is a sparse reward setting, where the player receives a positive reward only after solving the maze.\\n\\n---\\n\\nOur memory model API relies on an abstract memory model class, only requiring users to implement memory forward and initial state methods. Our memory API builds on top of RLlib, exposing various algorithms, exploration methods, logging, distributed training, and more. We collect well-known memory models from SL domains and wrap them in this API, enabling their use on RL tasks. We rewrite models where the existing implementation is slow, unreadable, not amenable to our API, or not written in Pytorch. Some of these sequence models have yet to be applied in the context of reinforcement learning.\\n\\n1. **MLP:**\\n\\nAn MLP that cannot remember anything. This serves to form a lower bound for memory performance, as well and ensuring memory models are actively using memory, rather than just leveraging their higher parameter counts.\\n\\n2. **Positional MLP (PosMLP):**\\n\\nAn MLP that can access the current episodic timestep. The current timestep is fed into the positional encoding from Vaswani et al. (2017), which is summed with the incoming features. PosMLP enables agents to learn time-dependent policies (those which evolve over the course of an episode) without explicitly using memory.\\n\\n3. **Elman Networks:**\\n\\nThe original RNN, from Elman (1990). Elman networks sum the recurrent state and input, passing the resulting vector through a linear layer and activation function to produce the next hidden state. Elman networks are not used much in SL nowadays due to vanishing and exploding gradients.\\n\\n4. **Long Short-Term Memory (LSTM):**\\n\\nHochreiter & Schmidhuber (1997) designed LSTM to address the vanishing and exploding gradient problems present in earlier RNNs like the Elman Network. LSTM utilizes a constant error carousel to handle longer dependencies and gating to ensure recurrent state stability during training. It has two recurrent states termed the hidden and cell states.\\n\\n5. **Gated Recurrent Unit (GRU):**\\n\\nThe GRU is a simplification of LSTM, using only a single recurrent state. The GRU appears to have similar performance to LSTM in many applications while using fewer parameters (Chung et al., 2014).\"}"}
{"id": "chDrutUTs0K", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | 500  | 1000 | 1500 | Train Memory (MB) |\\n|---------------|------|------|------|------------------|\\n| MLP           | 250  |      |      |                  |\\n| PosMLP        |      |      |      |                  |\\n| FWP           |      |      |      |                  |\\n| FART          |      |      |      |                  |\\n| S4D           |      |      |      |                  |\\n| TCN           |      |      |      |                  |\\n| Fr.Stack      |      |      |      |                  |\\n| LMU           |      |      |      |                  |\\n| IndRNN        |      |      |      |                  |\\n| Elman         |      |      |      |                  |\\n| GRU           |      |      |      |                  |\\n| LSTM          |      |      |      |                  |\\n| DNC           |      |      |      |                  |\\n| Num. Params (k) | 10   | 100  | 1000 |                  |\\n| GPU Train Time (ms) | 0    | 100  | 200  |                  |\\n| CPU Inf. Time (ms) | 0    | 2    |      |                  |\\n\\nFigure 2: Performance characteristics for POPGym memory baselines on random inputs. We use a recurrent state size of 256, a batch size of 64, and a episode length of 1024. We compute CPU statistics on a 3GHz Xeon Gold and GPU statistics on a 2080Ti, reporting the mean and 95% confidence interval over 10 trials. Train times correspond to a full batch while inference times are per-element (i.e. the latency to compute a single action). Note that GPU Train Time has logarithmic scale.\\n\\n6. Independently Recurrent Networks (IndRNN): Stacking LSTM and GRU cells tends to provide few benefits when compared with traditional deep neural networks. IndRNNs update the recurrent state using elementwise connections rather than a dense layer, enabling much deeper RNNs and handling longer dependencies than the LSTM and GRU (Li et al., 2018). In our experiments, we utilize a 2-layer IndRNN.\\n\\n7. Differentiable Neural Computers (DNC): Graves et al. (2016) introduce a new type of recurrent model using external memory. The DNC utilizes an RNN as a memory controller, reading and writing to external storage in a differentiable manner.\\n\\n8. Fast Autoregressive Transformers (FART): Unlike the traditional attention matrix whose size scales with the number of inputs, FART computes a fixed-size attention matrix at each timestep, taking the cumulative elementwise sum over successive timesteps (Katharopoulos et al., 2020). FART maintains two recurrent states, one for the running attention matrix and one for a normalization term, which helps mitigate large values and exploding gradients as the attention increases grows over time. The original paper omits a positional encoding, but we find it necessary for our benchmark.\\n\\n9. Fast Weight Programmers (FWP): The theory behind FART and FWP is different, but the implementation is relatively similar. FWP also maintains a running cumulative sum. Unlike FART, FWP normalizes the key and query vectors to sum to one, requiring only a single recurrent state and keeping the attention matrix of reasonable scale (Schlag et al., 2021). Unlike the original paper, we add a positional encoding to FWP.\\n\\n10. Frame Stacking (Fr.Stack): Mnih et al. (2015) implemented frame stacking to solve Atari games. Frame stacking is the concatenation of \\\\(k\\\\) observations along the feature dimension. Frame stacking is not strictly convolutional, but is implemented similarly to other convolutional methods. Frame stacking is known to work very well in RL, but the number of parameters scales with the receptive field, preventing it from learning long-term dependencies.\\n\\n11. Temporal Convolutional Networks (TCN): TCNs slide 1D convolutional filters over the temporal dimension. On long sequences, they are faster and use less memory than RNNs. TCNs avoid the vanishing gradient problem present in RNNs because the gradient feeds through each sequence element individually, rather than propagating through the entire sequence (Bai et al., 2018).\\n\\n12. Legendre Memory Units (LMU): LMUs are a mixture of convolution and RNNs. They apply Legendre polynomials across a sliding temporal window, feeding the results into an RNN hidden state (V oelker et al., 2019). LMUs can handle temporal dependencies spanning up to 100K timesteps.\\n\\n13. Diagonal State Space Models (S4D): S4D treats memory as a controls problem. It learns a linear time invariant (LTI) state space model for the recurrent state. S4D applies a Vandermonde matrix to the sequence of inputs, which we can represent using either convolution or a recurrence. Computing the result convolutionally makes it very fast. In SL, S4D was able to solve the challenging 16,000 timestep Path-X task, demonstrating significant capacity for long-term dependencies (Gu et al., 2022).\"}"}
{"id": "chDrutUTs0K", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: (Left) A summary comparison of baselines aggregated over all environments. We normalize the MMER such that 0 denotes the worst trial and 1 denotes the best trial for a specific environment. We report the interquartile range (box), median (horizontal line), and mean (dot) normalized MMER over all trials. (Right) Single value scores for each model, produced by meaning the MMER over all POPGym environments. We also provide scores with navigation (Labyrinth) environments excluded; the reasoning is provided in the discussion section.\\n\\n5 EXPERIMENTS\\n\\nOur memory framework hooks into RLlib, providing integration with IMPALA, DQN, and countless other algorithms. Due to computational constraints, we only execute our study on Proximal Policy Optimization (PPO) (Schulman et al., 2017). We tend to use conservative hyperparameters to aid in reproducability \u2013 this entails large batch sizes, low learning rates, and many minibatch passes over every epoch. We run three trials of each model over three difficulties for each environment, resulting in over 1700 trials. We utilize the max-mean episodic reward (MMER) in many plots. We compute MMER by finding the mean episodic reward for each epoch, then taking the maximum over all epochs, resulting in a single MMER value for each trial. We present the full experimental parameters in Appendix A and detailed results for each environment and model in Appendix B.\\n\\nWe provide a summary over models and tasks in Figure 3. Figure 2 reports model throughput and Table 1 provides environment throughput.\\n\\n6 DISCUSSION\\n\\nIn the following paragraphs, we pose some questions and findings made from the results of our large-scale study.\\n\\nSupervised learning is a bad proxy for RL. Supervised learning experiments show that IndRNN, LMU, FART, S4D, DNC, and TCN surpass LSTM and GRUs by a wide margin (Li et al., 2018; Voelker et al., 2019; Katharopoulos et al., 2020; Gu et al., 2022; Graves et al., 2016; Bai et al., 2018). S4D is unstable and often crashed due to exploding weights, suggesting it is not suitable for RL out of the box and that further tuning may be required. Although linear attention methods like FWP and FART show significant improvements across a plethora of supervised learning tasks, they were some of the worst contenders in RL. Classical RNNs outperformed modern memory methods, even though RNNs have been thoroughly supplanted in SL (Figure 3). The underlying cause of the disconnect between RL and SL performance is unclear and warrants further investigation.\\n\\nUse GRUs for performance and Elman nets for efficiency. Within traditional RNNs, there seems little reason to use LSTM, as GRUs are more efficient and perform better. Elman networks are largely ignored in modern SL and RL due to vanishing or exploding gradients, but these issues did not impact our training. We find Elman networks perform on-par with LSTM while exhibiting some of the best parameter and memory efficiency out of any model (Figure 2). Future work could investigate why Elman networks work so well in RL given their limitations, and distill these properties into memory models suited specifically for RL.\\n\\nAre maze navigation tasks sufficient for benchmarking memory? Existing POMDP benchmarks focus primarily navigation tasks. In our experiments, we show that the MLP received the\"}"}
{"id": "chDrutUTs0K", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Env. Model | MMER | \u03c3 |\\n|------------|------|---|\\n| Autoencode Easy DNC | -0.489 | 0.002 |\\n| FART | -0.447 | 0.014 |\\n| FWP | -0.322 | 0.020 |\\n| Fr.Stack | -0.422 | 0.010 |\\n| GRU | -0.283 | 0.029 |\\n| IndRNN | -0.334 | 0.004 |\\n| LMU | -0.370 | 0.015 |\\n| LSTM | -0.312 | 0.008 |\\n| MLP | -0.470 | 0.004 |\\n| PosMLP | -0.458 | 0.003 |\\n| S4D | -0.490 | 0.005 |\\n| TCN | -0.410 | 0.006 |\\n| Autoencode Medium DNC | -0.488 | 0.002 |\\n| Elman | -0.443 | 0.007 |\\n| FART | -0.478 | 0.002 |\\n| FWP | -0.449 | 0.005 |\\n| Fr.Stack | -0.466 | 0.002 |\\n| GRU | -0.425 | 0.018 |\\n| IndRNN | -0.420 | 0.011 |\\n| LMU | -0.474 | 0.003 |\\n| LSTM | -0.423 | 0.004 |\\n| MLP | -0.482 | 0.002 |\\n| PosMLP | -0.474 | 0.001 |\\n| S4D | -0.490 | 0.001 |\\n| TCN | -0.464 | 0.002 |\\n| Autoencode Hard DNC | -0.489 | 0.002 |\\n| Elman | -0.481 | 0.005 |\\n| FART | -0.481 | 0.001 |\\n| FWP | -0.472 | 0.011 |\\n| Fr.Stack | -0.478 | 0.004 |\\n| GRU | -0.456 | 0.009 |\\n| IndRNN | -0.448 | 0.010 |\\n| LMU | -0.480 | 0.006 |\\n| LSTM | -0.467 | 0.004 |\\n| MLP | -0.488 | 0.002 |\\n| PosMLP | -0.483 | 0.003 |\\n| S4D | -0.489 | 0.003 |\\n| TCN | -0.476 | 0.004 |\\n| Battleship Easy DNC | -0.427 | 0.002 |\\n| Elman | -0.290 | 0.013 |\\n| FART | -0.413 | 0.005 |\\n| FWP | -0.389 | 0.007 |\\n| Fr.Stack | -0.378 | 0.015 |\\n| GRU | -0.320 | 0.013 |\\n| IndRNN | -0.287 | 0.005 |\\n| LMU | -0.323 | 0.027 |\\n| LSTM | -0.376 | 0.007 |\\n| MLP | -0.325 | 0.012 |\\n| PosMLP | -0.226 | 0.077 |\"}"}
{"id": "chDrutUTs0K", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model       | Loss  | STD   |\\n|-------------|-------|-------|\\n| Battleship-medium DNC | -0.394 | 0.003 |\\n| Elman       | -0.373 | 0.007 |\\n| FART        | -0.392 | 0.003 |\\n| FWP         | -0.386 | 0.003 |\\n| Fr.Stack    | -0.390 | 0.008 |\\n| GRU         | -0.367 | 0.008 |\\n| IndRNN      | -0.337 | 0.005 |\\n| LMU         | -0.387 | 0.010 |\\n| LSTM        | -0.379 | 0.006 |\\n| MLP         | -0.356 | 0.020 |\\n| PosMLP      | -0.344 | 0.030 |\\n| S4D         | -0.406 | 0.003 |\\n| TCN         | -0.363 | 0.003 |\\n| Battleship-hard DNC | -0.380 | 0.004 |\\n| Elman       | -0.377 | 0.005 |\\n| FART        | -0.384 | 0.003 |\\n| FWP         | -0.380 | 0.005 |\\n| Fr.Stack    | -0.381 | 0.004 |\\n| GRU         | -0.377 | 0.008 |\\n| IndRNN      | -0.369 | 0.009 |\\n| LMU         | -0.381 | 0.000 |\\n| LSTM        | -0.380 | 0.001 |\\n| MLP         | -0.383 | 0.002 |\\n| PosMLP      | -0.382 | 0.003 |\\n| S4D         | -0.389 | 0.005 |\\n| TCN         | -0.376 | 0.004 |\\n| Concentration-easy DNC | -0.182 | 0.004 |\\n| Elman       | -0.098 | 0.012 |\\n| FART        | -0.185 | 0.002 |\\n| FWP         | -0.188 | 0.001 |\\n| Fr.Stack    | -0.146 | 0.001 |\\n| GRU         | -0.039 | 0.005 |\\n| IndRNN      | 0.142  | 0.004 |\\n| LMU         | -0.057 | 0.009 |\\n| LSTM        | -0.080 | 0.012 |\\n| MLP         | -0.050 | 0.015 |\\n| PosMLP      | -0.048 | 0.010 |\\n| S4D         | -0.190 | 0.004 |\\n| TCN         | 0.253  | 0.004 |\\n| Concentration-medium DNC | -0.182 | 0.001 |\\n| Elman       | -0.186 | 0.003 |\\n| FART        | -0.186 | 0.002 |\\n| FWP         | -0.185 | 0.002 |\\n| Fr.Stack    | -0.185 | 0.002 |\\n| GRU         | -0.189 | 0.003 |\\n| IndRNN      | -0.256 | 0.012 |\\n| LMU         | -0.176 | 0.005 |\\n| LSTM        | -0.185 | 0.001 |\\n| MLP         | -0.178 | 0.004 |\\n| PosMLP      | -0.175 | 0.002 |\\n| S4D         | -0.186 | 0.002 |\\n| TCN         | -0.157 | 0.005 |\\n| Concentration-hard | -0.830 | 0.002 |\\n\\nContinued on next page.\"}"}
{"id": "chDrutUTs0K", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model     | CountRecallEasy | CountRecallMedium | CountRecallHard |\\n|-----------|-----------------|-------------------|-----------------|\\n| Env. Model | -0.830 0.001    | -0.830 0.001      | -0.815 0.042    |\\n| Elman     | 0.016 0.091     | -0.540 0.001      | -0.541 0.033    |\\n| FART      | -0.300 0.145    | -0.541 0.003      | -0.501 0.008    |\\n| FWP       | -0.399 0.047    | -0.541 0.016      | -0.477 0.006    |\\n| Fr.Stack  | -0.365 0.022    | -0.529 0.002      | -0.475 0.008    |\\n| GRU       | 0.177 0.005     | -0.528 0.001      | -0.475 0.006    |\\n| IndRNN    | -0.042 0.050    | -0.535 0.014      | -0.481 0.004    |\\n| LMU       | -0.214 0.058    | -0.555 0.009      | -0.522 0.036    |\\n| LSTM      | 0.509 0.062     | -0.538 0.005      | -0.478 0.006    |\\n| MLP       | -0.847 0.011    | -0.907 0.001      | -0.867 0.002    |\\n| PosMLP    | -0.409 0.011    | -0.519 0.003      | -0.470 0.003    |\\n| S4D       | -0.911 0.005    | -0.920 0.000      | -0.858 0.013    |\\n| TCN       | -0.385 0.010    | -0.524 0.003      | -0.470 0.002    |\\n\\nContinued on next page\"}"}
{"id": "chDrutUTs0K", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model       | Mean   | Std  |\\n|-------------|--------|------|\\n| Env. Model  | 0.520  | 0.002|\\n| FWP         | 0.504  | 0.001|\\n| Fr. Stack   | 0.529  | 0.002|\\n| GRU         | 0.528  | 0.000|\\n| IndRNN      | 0.494  | 0.003|\\n| LMU         | 0.526  | 0.003|\\n| LSTM        | 0.505  | 0.001|\\n| MLP         | 0.505  | 0.001|\\n| PosMLP      | 0.505  | 0.001|\\n| S4D         | 0.479  | 0.018|\\n| TCN         | 0.503  | 0.000|\\n| Higher      | 0.501  | 0.003|\\n| Lower       | 0.511  | 0.001|\\n| Medium      | 0.513  | 0.001|\\n| DNC         | 0.466  | 0.004|\\n| Elman       | 0.504  | 0.006|\\n| FART        | 0.503  | 0.000|\\n| FWP         | 0.499  | 0.002|\\n| Fr. Stack   | 0.499  | 0.002|\\n| GRU         | 0.506  | 0.001|\\n| IndRNN      | 0.509  | 0.001|\\n| LMU         | 0.453  | 0.005|\\n| LSTM        | 0.502  | 0.001|\\n| MLP         | 0.504  | 0.002|\\n| PosMLP      | 0.502  | 0.001|\\n| S4D         | 0.387  | 0.037|\\n| TCN         | 0.501  | 0.001|\\n| Labyrinth   | 0.958  | 0.002|\\n| EscapeEasy | 0.956  | 0.002|\\n| FART        | -0.043 | 0.266|\\n| WFP         | -0.078 | 0.062|\\n| Fr. Stack   | 0.521  | 0.055|\\n| GRU         | 0.959  | 0.001|\\n| IndRNN      | -0.218 | 0.136|\\n| LMU         | 0.814  | 0.076|\\n| LSTM        | 0.920  | 0.024|\\n| MLP         | 0.961  | 0.000|\\n| PosMLP      | 0.544  | 0.023|\\n| S4D         | -0.305 | 0.033|\\n| TCN         | 0.773  | 0.054|\\n| Labyrinth   | -0.414 | 0.493|\\n| EscapeMedium| -0.122 | 0.388|\\n| FART        | -0.602 | 0.057|\\n| WFP         | -0.604 | 0.015|\\n| Fr. Stack   | -0.384 | 0.271|\\n\\nContinued on next page\"}"}
{"id": "chDrutUTs0K", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Micha\u0142 Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Jaskowski. ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning. In IEEE Conference on Computational Intelligence and Games, pp. 341\u2013348, Santorini, Greece, September 2016. IEEE. URL http://arxiv.org/abs/1605.02097.\\n\\nJeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, and Shie Mannor. RL for Latent MDPs: Regret Guarantees and a Lower Bound. In Advances in Neural Information Processing Systems, volume 34, pp. 24523\u201324534. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/hash/cd755a6c6b699f3262bcc2aa46ab507e-Abstract.html.\\n\\nTor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.\\n\\nY. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, November 1998. ISSN 1558-2256. doi: 10.1109/5.726791. Conference Name: Proceedings of the IEEE.\\n\\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems, November 2020. URL http://arxiv.org/abs/2005.01643. arXiv:2005.01643 [cs, stat].\\n\\nShuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5457\u20135466, Salt Lake City, UT, June 2018. IEEE. ISBN 978-1-5386-6420-9. doi: 10.1109/CVPR.2018.00572. URL https://ieeexplore.ieee.org/document/8578670/.\\n\\nEric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gonzalez, Michael Jordan, and Ion Stoica. RLlib: Abstractions for distributed reinforcement learning. In International Conference on Machine Learning, pp. 3053\u20133062. PMLR, 2018.\\n\\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, and others. Human-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015. Publisher: Nature Publishing Group.\\n\\nSteven Morad, Stephan Liwicki, Ryan Kortvelesy, Roberto Mecca, and Amanda Prorok. Modeling Partially Observable Systems using Graph-Based Memory and Topological Priors. In Proceedings of The 4th Annual Learning for Dynamics and Control Conference, pp. 59\u201373. PMLR, May 2022. URL https://proceedings.mlr.press/v168/morad22a.html. ISSN: 2640-3498.\\n\\nSteven D. Morad, Roberto Mecca, Rudra P. K. Poudel, Stephan Liwicki, and Roberto Cipolla. Embodied Visual Navigation With Automatic Curriculum Learning in Real Environments. IEEE Robotics and Automation Letters, 6(2):683\u2013690, April 2021. ISSN 2377-3766. doi: 10.1109/LRA.2020.3048662. Conference Name: IEEE Robotics and Automation Letters.\\n\\nTianwei Ni, Benjamin Eysenbach, and Ruslan Salakhutdinov. Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs. In Proceedings of the 39th International Conference on Machine Learning, pp. 16691\u201316723. PMLR, June 2022. URL https://proceedings.mlr.press/v162/ni22a.html. ISSN: 2640-3498.\\n\\nIan Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesv\u00e1ri, Satinder Singh, Benjamin Van Roy, Richard Sutton, David Silver, and Hado van Hasselt. Behaviour Suite for Reinforcement Learning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rygf-kSYwH.\\n\\nEmilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Rapha\u00ebl Lopez Kaufman, Aidan Clark, Seb Noury, Matthew Botvinick, Nicolas Heess, and Raia Hadsell. Stabilizing Transformers for Reinforcement Learning. In Hal Daum\u00e9 III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 7487\u20137498. PMLR, July 2020. URL https://proceedings.mlr.press/v119/parisotto20a.html.\"}"}
{"id": "chDrutUTs0K", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-Baselines3: Reliable Reinforcement Learning Implementations. Journal of Machine Learning Research, 22(268):1\u20138, 2021. URL http://jmlr.org/papers/v22/20-1364.html.\\n\\nDhruv Ramani. A Short Survey On Memory Based Reinforcement Learning. Technical Report arXiv:1904.06736, arXiv, April 2019. URL http://arxiv.org/abs/1904.06736.\\n\\narXiv:1904.06736 [cs] type: article.\\n\\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.\\n\\nImanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear Transformers Are Secretly Fast Weight Programmers. In Proceedings of the 38th International Conference on Machine Learning, pp. 9355\u20139366. PMLR, July 2021. URL https://proceedings.mlr.press/v139/schlag21a.html. ISSN: 2640-3498.\\n\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\n\\nDaniel Seita. Can RL From Pixels be as Efficient as RL From State?, July 2020. URL http://bair.berkeley.edu/blog/2020/07/19/curl-rad/.\\n\\nVatsal Sharan, Sham M Kakade, Percy S Liang, and Gregory Valiant. Learning Overcomplete HMMs. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/hash/6aca97005c68f1206823815f66102863-Abstract.html.\\n\\nAleksandrs Slivkins and others. Introduction to multi-armed bandits. Foundations and Trends\u00ae in Machine Learning, 12(1-2):1\u2013286, 2019. Publisher: Now Publishers, Inc.\\n\\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nAaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://papers.nips.cc/paper/2019/hash/952285b9b7e7a1be5aa7849f32ffff05-Abstract.html.\\n\\nZhihan Yang and Hai Nguyen. Recurrent Off-policy Baselines for Memory-based Continuous Control. Technical Report arXiv:2110.12628, arXiv, October 2021. URL http://arxiv.org/abs/2110.12628. arXiv:2110.12628 [cs] type: article.\\n\\nKaiyu Zheng and Stefanie Tellex. pomdp.py: A Framework to Build and Solve POMDP Problems. In ICAPS 2020 Workshop on Planning and Robotics (PlanRob), 2020. URL https://icaps20subpages.icaps-conference.org/wp-content/uploads/2020/10/14-PlanRob_2020_paper_3.pdf.\"}"}
{"id": "chDrutUTs0K", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: PPO hyperparameters used in all of our experiments.\\n\\n| Hyperparameter | Value |\\n|----------------|-------|\\n| Decay factor   | 0.99  |\\n| Value fn. loss coef. | 1.0   |\\n| Entropy loss coef. | 0.0   |\\n| Learning rate | 5e-5  |\\n| Num. SGD iters  | 30    |\\n| Batch size     | 65536 |\\n| Minibatch size | 8192  |\\n| GAE \u03bb          | 1.0   |\\n| KL target      | 0.01  |\\n| KL coefficient | 0.2   |\\n| PPO clipping   | 0.3   |\\n| Value clipping | 0.3   |\\n| BPTT Truncation Length | \u221e     |\\n| Maximum Episode Length | 1024  |\\n\\nA. Experimental Parameters\\n\\nGiven the number of environments and models, it is not computationally feasible to optimize hyperparameters in a structured way. Through trial and error, we evaluated all models on the Repeat First and Repeat Previous environments and found suitable values that worked across all models. We then picked a more conservative estimate (larger batch size, lower learning rate) to promote monotonic improvement and prevent catastrophic forgetting, at the expense of some sample efficiency.\\n\\nThere is no clear axis for a truly fair comparison between memory models \u2013 model throughput and parameter count vary drastically throughout the memory models. We decide to limit the amount of storage (i.e. recurrent state size) of each memory model to 256 dimensions, which is greater than the common values of 64 and 128 in literature (Ni et al., 2022). This results in lower parameter counts for models that produce the recurrent state using a tensor product (e.g. FWP, FART, S4D). We could make an exception for these models, allowing them to produce recurrent states of size $256^2 = 63356$ dimensions instead of $16^2 = 256$ dimensions to bring up the parameter count, but we believe this is unfair to recurrent models. In this case, recurrent models would need to forget and compress information over longer episodes, while tensor product models could store every single observation in memory without any compression or forgetting. Storing everything is unlikely to scale to real-world applications where episodes could span hours, days, or run indefinitely.\\n\\nA.1 General Model Hyperparameters\\n\\nFor all our memory experiments, we use the same outer model, just swapping out the inner memory model. The outer model first projects observations from the environment to 128 zero-mean variance-one dimensions. Here, the positional encoding is applied if the memory model requests it. The projection goes through a single linear layer and LeakyReLU activation of size 128, then feeds into the memory model. Output from the memory model is projected to 128 dimensions, then split into the actor and critic head. The actor and critic heads are two layer MLPs of width 128 with LeakyReLU activation.\\n\\nA.2 Model-Specific Hyperparameters\\n\\nThe Elman, GRU, LSTM, and LMU RNNs use a single cell. We use a 2-cell IndRNN as they claim to utilize deeper networks. The FART and FWP models use a single attention block. We use the attention-only formulations of FWP, rather than the combined attention and RNN variant. We utilize a temporal window of four for frame stacking and TCN. LMU utilizes a $\\\\theta$ window size of 64 timesteps.\"}"}
{"id": "chDrutUTs0K", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We report our results in three forms:\\n\\n1. Bar charts denoting the standardized MMER split by environment (Figure 5-Figure 9)\\n2. Line plots showing cumulative maximum episodic reward for each training epoch, split by environment (Figure 10-Figure 14)\\n3. Tables reporting the mean and standard deviation of the MMER, split by model and environment (Table 3)\\n\\nAll models and environments are from commit e397e5e except for the DiffNC experiments, which are from commit 33b0995. All experiments sample 15M timesteps from each environment, except for the DiffNC experiments which sample 10M timesteps.\\n\\nWe run each trial 3 times, aggregating results using the mean over trials. All raw data is available at https://wandb.ai/prorok-lab/popgym-public. The bar plots represent the mean and 95% bootstrap confidence interval. For the bar charts, we standardize the reward between 0 and 1. In the line plots, the solid region refers to the mean and the shaded region to the 95% bootstrap confidence interval. The table reports the MMER mean and standard deviation across trials.\"}"}
{"id": "chDrutUTs0K", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model        | RepeatFirstEasy DNC | RepeatFirstMedium DNC | RepeatFirstHard DNC | RepeatPreviousEasy DNC | RepeatPreviousMedium DNC |\\n|--------------|---------------------|-----------------------|--------------------|------------------------|-------------------------|\\n| Env. Model   |                     |                       |                    |                        |                         |\\n| RepeatFirstEasy DNC | 0.716 | 0.207 | -0.349 | 0.041 | -0.334 | 0.039 |\\n| Elman        | 1.000               | 0.000                 | -0.468             | 0.014                  | -0.464 | 0.002 |\\n| FART         | 1.000               | 0.000                 | 0.962              | 0.048                  | 0.867 | 0.142 |\\n| FWP          | 0.813               | 0.224                 | 0.830              | 0.237                  | 0.915 | 0.045 |\\n| Fr.Stack     | 0.997               | 0.005                 | -0.467             | 0.007                  | -0.448 | 0.016 |\\n| GRU          | 1.000               | 0.000                 | 0.940              | 0.012                  | 0.969 | 0.026 |\\n| IndRNN       | 0.997               | 0.003                 | 0.998              | 0.003                  | 0.969 | 0.026 |\\n| LMU          | 1.000               | 0.000                 | 0.641              | 0.457                  | -0.406 | 0.060 |\\n| LSTM         | 1.000               | 0.000                 | 0.926              | 0.068                  | 0.275 | 0.677 |\\n| MLP          | 0.489               | 0.391                 | -0.389             | 0.057                  | -0.355 | 0.111 |\\n| PosMLP       | 0.736               | 0.209                 | -0.472             | 0.007                  | -0.455 | 0.009 |\\n| S4D          | -0.194              | 0.098                 | -0.241             | 0.100                  | -0.178 | 0.146 |\\n| TCN          | 1.000               | 0.000                 | -0.449             | 0.033                  | -0.457 | 0.010 |\\n| RepeatPreviousEasy DNC | -0.223 | 0.075 | -0.490 | 0.001 | -0.223 | 0.075 |\\n| Elman        | 1.000               | 0.000                 | 0.060              | 0.040                  | 1.000 | 0.000 |\\n| FART         | 0.060               | 0.040                 | 0.200              | 0.052                  | 0.060 | 0.040 |\\n| FWP          | 0.200               | 0.052                 | 1.000              | 0.000                  | 0.200 | 0.052 |\\n| Fr.Stack     | 1.000               | 0.000                 | 1.000              | 0.000                  | 1.000 | 0.000 |\\n| GRU          | 1.000               | 0.000                 | 1.000              | 0.000                  | 1.000 | 0.000 |\\n| IndRNN       | 0.957               | 0.012                 | 0.957              | 0.012                  | 0.957 | 0.012 |\\n| LMU          | 1.000               | 0.000                 | 1.000              | 0.000                  | 1.000 | 0.000 |\\n| LSTM         | 1.000               | 0.000                 | 1.000              | 0.000                  | 1.000 | 0.000 |\\n| MLP          | -0.320              | 0.007                 | -0.320             | 0.007                  | -0.320 | 0.007 |\\n| PosMLP       | -0.336              | 0.014                 | -0.336             | 0.014                  | -0.336 | 0.014 |\\n| S4D          | -0.473              | 0.003                 | -0.473             | 0.003                  | -0.473 | 0.003 |\\n| TCN          | 1.000               | 0.000                 | 1.000              | 0.000                  | 1.000 | 0.000 |\\n\\nContinued on next page\"}"}
{"id": "chDrutUTs0K", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | Value   | p-value |\\n|---------------|---------|---------|\\n| Env. Model    | FART -0.468 | 0.011   |\\n| FWP           | -0.345  | 0.033   |\\n| Fr.Stack      | -0.484  | 0.003   |\\n| GRU           | -0.315  | 0.017   |\\n| IndRNN        | -0.304  | 0.014   |\\n| LMU           | 0.789   | 0.288   |\\n| LSTM          | -0.284  | 0.024   |\\n| MLP           | -0.486  | 0.001   |\\n| PosMLP        | -0.485  | 0.001   |\\n| S4D           | -0.490  | 0.002   |\\n| TCN           | -0.478  | 0.004   |\\n| RepeatPreviousHard DNC | -0.490 | 0.002 |\\n| Elman         | -0.481  | 0.003   |\\n| FART          | -0.485  | 0.003   |\\n| FWP           | -0.443  | 0.017   |\\n| Fr.Stack      | -0.485  | 0.001   |\\n| GRU           | -0.428  | 0.002   |\\n| IndRNN        | -0.384  | 0.013   |\\n| LMU           | 0.191   | 0.041   |\\n| LSTM          | -0.397  | 0.008   |\\n| MLP           | -0.486  | 0.002   |\\n| PosMLP        | -0.486  | 0.004   |\\n| S4D           | -0.491  | 0.001   |\\n| TCN           | -0.486  | 0.001   |\\n| StatelessCartPoleEasy DNC | 0.960 | 0.027 |\\n| Elman         | 1.000   | 0.000   |\\n| FART          | 1.000   | 0.000   |\\n| FWP           | 1.000   | 0.000   |\\n| Fr.Stack      | 1.000   | 0.000   |\\n| GRU           | 1.000   | 0.000   |\\n| IndRNN        | 1.000   | 0.000   |\\n| LMU           | 0.996   | 0.001   |\\n| LSTM          | 1.000   | 0.000   |\\n| MLP           | 0.722   | 0.001   |\\n| PosMLP        | 0.967   | 0.006   |\\n| S4D           | 0.514   | 0.076   |\\n| TCN           | 1.000   | 0.000   |\\n| StatelessCartPoleMedium DNC | 0.956 | 0.061 |\\n| Elman         | 1.000   | 0.000   |\\n| FART          | 1.000   | 0.000   |\\n| FWP           | 0.989   | 0.007   |\\n| Fr.Stack      | 1.000   | 0.000   |\\n| GRU           | 1.000   | 0.000   |\\n| IndRNN        | 1.000   | 0.000   |\\n| LMU           | 0.995   | 0.001   |\\n| LSTM          | 1.000   | 0.000   |\\n| MLP           | 0.398   | 0.006   |\\n| PosMLP        | 0.593   | 0.049   |\\n| S4D           | 0.205   | 0.011   |\\n| TCN           | 1.000   | 0.000   |\\n| StatelessCartPoleHard DNC | 0.778 | 0.330 |\\n| Elman         | 1.000   | 0.000   |\\n| FART          | 0.996   | 0.000   |\\n| FWP           | 0.900   | 0.092   |\\n\\nContinued on next page\"}"}
{"id": "chDrutUTs0K", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                  |\u6210\u679c|\u8bef\u5dee |\\n|-----------------------|----|----|\\n| Env. Model            | 0.989 | 0.017 |\\n| Fr.Stack 0.989        | 1.000 | 0.000 |\\n| GRU 1.000             | 0.000 | 0.000 |\\n| IndRNN 1.000          | 0.000 | 0.000 |\\n| LMU 0.987             | 0.007 | 0.007 |\\n| LSTM 1.000            | 0.000 | 0.000 |\\n| MLP 0.265             | 0.002 | 0.002 |\\n| PosMLP 0.443          | 0.018 | 0.018 |\\n| S4D 0.127             | 0.026 | 0.026 |\\n| TCN 0.998             | 0.003 | 0.003 |\\n| StatelessPendulumEasy DNC 0.427 | 0.022 | 0.022 |\\n| Elman 0.903           | 0.007 | 0.007 |\\n| FART 0.652            | 0.010 | 0.010 |\\n| FWP 0.556             | 0.007 | 0.007 |\\n| Fr.Stack 0.906        | 0.005 | 0.005 |\\n| GRU 0.913             | 0.002 | 0.002 |\\n| IndRNN 0.798          | 0.151 | 0.151 |\\n| LMU 0.819             | 0.107 | 0.107 |\\n| LSTM 0.878            | 0.048 | 0.048 |\\n| MLP 0.448             | 0.005 | 0.005 |\\n| PosMLP 0.486          | 0.011 | 0.011 |\\n| S4D 0.290             | 0.007 | 0.007 |\\n| TCN 0.906             | 0.003 | 0.003 |\\n| StatelessPendulumMedium DNC 0.436 | 0.012 | 0.012 |\\n| Elman 0.880           | 0.004 | 0.004 |\\n| FART 0.660            | 0.013 | 0.013 |\\n| FWP 0.579             | 0.033 | 0.033 |\\n| Fr.Stack 0.881        | 0.002 | 0.002 |\\n| GRU 0.884             | 0.002 | 0.002 |\\n| IndRNN 0.719          | 0.216 | 0.216 |\\n| LMU 0.858             | 0.003 | 0.003 |\\n| LSTM 0.875            | 0.005 | 0.005 |\\n| MLP 0.455             | 0.025 | 0.025 |\\n| PosMLP 0.509          | 0.011 | 0.011 |\\n| S4D 0.282             | 0.003 | 0.003 |\\n| TCN 0.822             | 0.005 | 0.005 |\\n| StatelessPendulumHard DNC 0.420 | 0.033 | 0.033 |\"}"}
{"id": "chDrutUTs0K", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 9: POPGym baselines (continued)\"}"}
{"id": "chDrutUTs0K", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: POPGym baselines (continued)\\n\\nFigure 11: POPGym baselines (continued)\"}"}
{"id": "chDrutUTs0K", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model       | LabyrinthExploreEasy | LabyrinthExploreMedium | LabyrinthExploreHard | MineSweeperEasy | MineSweeperMedium | MineSweeperHard | MultiarmedBanditEasy | MultiarmedBanditMedium | MultiarmedBanditHard |\\n|-------------|----------------------|------------------------|----------------------|----------------|------------------|----------------|----------------------|------------------------|-----------------------|\\n| MLP         | 1.0                  | 0.5                    | 0.0                  |                |                  |                | 0                    | 50                     | 100                   |\\n| PosMLP      | 1.0                  | 0.5                    | 0.0                  |                |                  |                | 100                  | 150                    | 200                   |\\n| FWP         | 1.0                  | 0.5                    | 0.0                  |                |                  |                | 200                  | 250                    | 300                   |\\n| FART        | 1.0                  | 0.5                    | 0.0                  |                |                  |                | 300                  | 350                    | 400                   |\\n| S4D         | 1.0                  | 0.5                    | 0.0                  |                |                  |                | 400                  | 450                    | 500                   |\\n| TCN         | 1.0                  | 0.5                    | 0.0                  |                |                  |                | 500                  | 550                    | 600                   |\\n| Fr.Stack    | 1.0                  | 0.5                    | 0.0                  |                |                  |                | 600                  | 650                    | 700                   |\\n| LMU         | 1.0                  | 0.5                    | 0.0                  |                |                  |                | 700                  | 750                    | 800                   |\\n| IndRNN      | 1.0                  | 0.5                    | 0.0                  |                |                  |                | 800                  | 850                    | 900                   |\\n| Elman       | 1.0                  | 0.5                    | 0.0                  |                |                  |                | 900                  | 950                    | 1000                  |\\n| GRU         | 1.0                  | 0.5                    | 0.0                  |                |                  |                | 1000                 | 1050                   | 1100                  |\\n| LSTM        | 1.0                  | 0.5                    | 0.0                  |                |                  |                | 1100                 | 1150                   | 1200                  |\\n| DNC         | 1.0                  | 0.5                    | 0.0                  |                |                  |                | 1200                 | 1250                   | 1300                  |\\n\\nFigure 12: POPGym baselines (continued)\\n\\n| Model       | NoisyStatelessCartPoleEasy | NoisyStatelessCartPoleMedium | NoisyStatelessCartPoleHard | NoisyStatelessPendulumEasy | NoisyStatelessPendulumMedium | NoisyStatelessPendulumHard | RepeatFirstEasy | RepeatFirstMedium | RepeatFirstHard |\\n|-------------|-----------------------------|-------------------------------|-----------------------------|-----------------------------|-------------------------------|----------------------------|-----------------|------------------|-----------------|\\n| MLP         | 1.0                         | 0.5                           | 0.0                         | 1.0                         | 0.5                           | 0.0                         | 1.0             | 60              | 100             |\\n| PosMLP      | 1.0                         | 0.5                           | 0.0                         | 1.0                         | 0.5                           | 0.0                         | 100             | 150             | 200             |\\n| FWP         | 1.0                         | 0.5                           | 0.0                         | 1.0                         | 0.5                           | 0.0                         | 200             | 250             | 300             |\\n| FART        | 1.0                         | 0.5                           | 0.0                         | 1.0                         | 0.5                           | 0.0                         | 300             | 350             | 400             |\\n| S4D         | 1.0                         | 0.5                           | 0.0                         | 1.0                         | 0.5                           | 0.0                         | 400             | 450             | 500             |\\n| TCN         | 1.0                         | 0.5                           | 0.0                         | 1.0                         | 0.5                           | 0.0                         | 500             | 550             | 600             |\\n| Fr.Stack    | 1.0                         | 0.5                           | 0.0                         | 1.0                         | 0.5                           | 0.0                         | 600             | 650             | 700             |\\n| LMU         | 1.0                         | 0.5                           | 0.0                         | 1.0                         | 0.5                           | 0.0                         | 700             | 750             | 800             |\\n| IndRNN      | 1.0                         | 0.5                           | 0.0                         | 1.0                         | 0.5                           | 0.0                         | 800             | 850             | 900             |\\n| Elman       | 1.0                         | 0.5                           | 0.0                         | 1.0                         | 0.5                           | 0.0                         | 900             | 950             | 1000            |\\n| GRU         | 1.0                         | 0.5                           | 0.0                         | 1.0                         | 0.5                           | 0.0                         | 1000            | 1050            | 1100            |\\n| LSTM        | 1.0                         | 0.5                           | 0.0                         | 1.0                         | 0.5                           | 0.0                         | 1100            | 1150            | 1200            |\\n| DNC         | 1.0                         | 0.5                           | 0.0                         | 1.0                         | 0.5                           | 0.0                         | 1200            | 1250            | 1300            |\\n\\nFigure 13: POPGym baselines (continued)\"}"}
{"id": "chDrutUTs0K", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model       | StatelessCartPoleEasy | StatelessCartPoleMedium | StatelessCartPoleHard |\\n|-------------|-----------------------|-------------------------|-----------------------|\\n| Population | RepeatPreviousEasy    | RepeatPreviousMedium    | RepeatPreviousHard    |\\n| Max Episodic Reward | 0.5                  | 0.0                     | 0.5                   | 1.0                   |\\n\\n| Model       | StatelessPendulumEasy | StatelessPendulumMedium | StatelessPendulumHard |\\n|-------------|-----------------------|-------------------------|-----------------------|\\n| Population | RepeatPreviousEasy    | RepeatPreviousMedium    | RepeatPreviousHard    |\\n| Max Episodic Reward | 0.5                  | 0.0                     | 0.5                   | 1.0                   |\\n\\nFigure 14: POPGym baselines (continued)\"}"}
