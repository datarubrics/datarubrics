{"id": "AfnsTnYphT", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Aakash Lahoti, Stefani Karp, Ezra Winston, Aarti Singh & Yuanzhi Li\\n\\nAbstract\\nVision tasks are characterized by the properties of locality and translation invariance. The superior performance of convolutional neural networks (CNNs) on these tasks is widely attributed to the inductive bias of locality and weight sharing baked into their architecture. Existing attempts to quantify the statistical benefits of these biases in CNNs over locally connected convolutional neural networks (LCNs) and fully connected neural networks (FCNs) fall into one of the following categories: either they disregard the optimizer and only provide uniform convergence upper bounds with no separating lower bounds, or they consider simplistic tasks that do not truly mirror the locality and translation invariance as found in real-world vision tasks. To address these deficiencies, we introduce the Dynamic Signal Distribution (DSD) classification task that models an image as consisting of \\\\( k \\\\) patches, each of dimension \\\\( d \\\\), and the label is determined by a \\\\( d \\\\)-sparse signal vector that can freely appear in any one of the \\\\( k \\\\) patches. On this task, for any orthogonally equivariant algorithm like gradient descent, we prove that CNNs require \\\\( \\\\tilde{O}(k + d) \\\\) samples, whereas LCNs require \\\\( \\\\Omega(kd) \\\\) samples, establishing the statistical advantages of weight sharing in translation invariant tasks. Furthermore, LCNs need \\\\( \\\\tilde{O}(k(k + d)) \\\\) samples, compared to \\\\( \\\\Omega(k^2d) \\\\) samples for FCNs, showcasing the benefits of locality in local tasks. Additionally, we develop information theoretic tools for analyzing randomized algorithms, which may be of interest for statistical research.\\n\\n1 Introduction\\nConvolutional Neural Networks (CNNs) exhibit state-of-the-art performance across computer vision tasks, including Image Classification, Object Detection, and Out of Distribution Detection (Liu et al. (2022); Fang et al. (2022); Wang et al. (2022)). This efficacy is commonly attributed to the biases of locality and weight sharing encoded into CNNs' short convolutions. The rationale is that these biases align with the properties of vision tasks, where local and mobile signals determine the output (Gens & Domingos (2014); Marcus (2018)). In contrast, Locally Connected Neural Networks (LCNs) encode only locality, while Fully Connected Neural Networks (FCNs) encode neither locality nor weight sharing, thus resulting in a larger sample complexity compared to CNNs.\\n\\nPrevious works have attempted to quantify the statistical benefit of these architectural biases in CNNs. For example, Vardi et al. (2022), Du et al. (2018) and Long & Sedghi (2020) derived Empirical Risk Minimization (ERM) bounds for CNNs which are tighter than that for FCNs. However, they do not provide separating lower bounds for FCNs on the same task, and cannot rule out the possibility that FCNs can adaptively yield better bounds when the input satisfies locality and translation invariance. In fact, as noted in Li et al. (2021), without taking the training algorithm into consideration, standard lower bound techniques cannot be used to show a separation between the three models. This is because an algorithm can simulate CNNs and LCNs within FCNs. Thus, if the algorithm is unconstrained, the minimax lower bound for FCNs cannot be greater than any upper bound for CNNs or LCNs.\"}"}
{"id": "AfnsTnYphT", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: From the Cats Dataset Zhang et al. (2008). The cat, which is the class-determining signal, varies in position across images, showing the translation property amidst background noise.\\n\\nRecently, Li et al. (2021) established a sample complexity separation between CNNs and FCNs that were trained on the restricted class of equivariant algorithms like gradient descent. Wang & Wu (2023) further extended this line of work to show a separation between FCNs, LCNs and CNNs. However, the data models employed in these works are not truly reflective of the locality and translation invariance of vision tasks. Typically in such tasks, the output is determined by some local pattern, also known as \\\"signal\\\". For example, a cat within images labeled \\\"cat\\\". Often, this signal is embedded within uninformative background, also known as \\\"noise\\\", and can freely translate within the image, i.e. it can appear in any patch within the image, without changing the label (as illustrated in figure 1). In contrast, in both Li et al. (2021) and Wang & Wu (2023), the data model considered is as follows: the input $x \\\\sim N(0, I_{4d})$, and the label is given by $f(x)$, and $g(x)$ respectively,\\n\\n$$f(x) = 2d \\\\sum_{i=1}^{d} x_{2i} - 4d \\\\sum_{i=2}^{d+1} x_{2i},$$\\n\\n$$g(x) = (d \\\\sum_{i=1}^{d} x_{2i} - x_{2i}^2)((2d \\\\sum_{i=d+1}^{2d} x_{2i} - x_{2i}^2)).$$\\n\\nBoth of these data models fail to capture the aforementioned desiderata of a model for a vision task. Additionally, they lack the requisite structure to demonstrate how sample complexity varies with the \\\"degree\\\" of locality and translation invariance within the input, or establish conditions on the input under which the differences between CNNs, LCNs, and FCNs are more pronounced.\\n\\nFurthermore, it is worth noting that the driving force for their separation results is the interaction between two halves of the input. Specifically, their lower bound selects \\\"hard instances\\\" from the class of functions $H = \\\\{x^1: dUx^d+1:2d\\\\}$, where $U$ is a $d \\\\times d$ orthonormal matrix, learning which results in a lower bound of $\\\\Omega(d^2)$. While the interaction between the patches is an interesting phenomenon, it is not the primary characteristic of locality and translation invariance found in images.\\n\\nWe introduce the Dynamic Signal Distribution (DSD) task, which is inspired by the setting in Karp et al. (2021), as our data model for vision tasks. The input $x \\\\in \\\\mathbb{R}^{kd}$ is comprised of $k$ consecutive patches, each of dimension $d$. From amongst these $k$ patches, one of them is randomly filled with a noisy signed signal. The remaining patches are filled with isotropic Gaussian noise of variance $\\\\sigma^2$.\\n\\nThe binary label is set as the sign of the signal, so that all images with the same signal in any one of the patches have the same label. By encapsulating concepts of signal, noise, locality, and translation invariance, the DSD task offers a higher fidelity to the complexities found in real-world vision tasks.\\n\\nOn this task, we establish a sample complexity separation of $\\\\Omega(\\\\sigma^2k^2d)$ vs $\\\\tilde{O}(\\\\sigma^2k(k+d))$ samples between FCNs and LCNs, as well as a separation of $\\\\Omega(\\\\sigma^2kd)$ vs $\\\\tilde{O}(\\\\sigma^2(k+d))$ samples between LCNs and CNNs. Our analysis indicates that due to no architectural biases, FCNs incur a multiplicative cost factor of $k$ for each of the two reasons: identifying the location of the $k$ patches, and learning the signal vector for each patch. The factor of $d$ arises due to learning the signal which is $d$-dimensional.\\n\\nFor LCNs, we can eliminate the $k$ cost for identification of the patches since the location of all the patches is baked into the architecture. Finally for CNNs both these costs are removed as the architecture not only localizes all the patches, it also allows the signal to be jointly learnt across all patches via weight sharing. It is noteworthy that both the LCN and the CNN upper bound feature a $k+d$ factor instead of the expected factor of $d$. This is an artifact of the gradient descent analysis, and is suggestive of being a potential cost for the algorithmic efficiency of gradient descent.\"}"}
{"id": "AfnsTnYphT", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In our first experiment, we fix the patch dimension $d$ at 20 and vary the number of patches $k$ across the range $\\\\{10, 15, 20, 25, 30\\\\}$. For each $(k, d)$ pair, we plot the sample complexity for both CNNs and LCNs. We evaluate the sample complexity via the following steps:\\n\\n1. **Target Loss Evaluation**: We compute the optimal loss based on the ground truth and add a fixed tolerance of $0.03$ to establish the target loss.\\n\\n2. **Determining Sample Range**: Through trial and error, we determine that a maximum of $1000$ samples is sufficient for any model across all $k$ values.\\n\\n3. **Binary Search Method**: To find the minimum number of samples required to reach the target loss, we perform a binary search. In each step, we conduct a grid search over the learning rates for weights being $[10^{-1}, 10^{-2}, 10^{-3}]$ and biases being $[10^{-2}, 10^{-3}, 10^{-4}]$, and select the model with the lowest test error.\\n\\n4. **Repetitions for Reliability**: We repeat the steps (1-3) five times, plotting the mean and standard deviation of the sample complexities.\\n\\nFor a fixed $d$, the sample complexity for CNNs exhibits an $O(k)$ growth as (Figure 3, left), which is consistent with our CNN upper bound. Similarly, for LCNs, the complexity growth is consistent with our theoretical results of $O(k^2)$ and $\\\\Omega(k)$ (Figure 3, right). Additionally, note that LCNs require about 10 to 20 times more samples than CNNs, which corresponds to the multiplicative $d$ factor in LCNs' sample complexity bound.\\n\\nIn our second experiment, we set the number of patches $k$ at 20 and vary the patch dimension $d$ across the range $[10, 15, 20, 25, 30]$. The same steps (1-4) are repeated for this setup.\\n\\nFigure 2: Sample complexity for CNNs (left) and LCNs (right) across various values of $k$.\\n\\nFigure 3: Sample complexity for CNNs (left) and LCNs (right) across various values of $d$. \\n\\n[39]\"}"}
{"id": "AfnsTnYphT", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For a fixed $k$, we observe that the CNN sample complexity grows as $O(d)$ (Figure 4, left), and the LCN sample complexity grows as $\\\\Theta(d)$ (Figure 4, right), both in line with our theoretical guarantees. Furthermore, akin to our findings in the first experiment, LCNs require approximately 20 times more samples than CNNs, owing to the multiplicative $k$ factor in their sample complexity.\"}"}
{"id": "AfnsTnYphT", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Theorem A.1\\n(Massart et al. (2007), Lemma 4.7).\\n\\nLet \\\\( \\\\{0, 1\\\\}^N \\\\) be equipped with Hamming distance \\\\( \\\\delta \\\\) and given \\\\( 1 \\\\leq D < N \\\\), define \\\\( \\\\{0, 1\\\\}^N_{\\\\Delta} = \\\\{x \\\\in \\\\{0, 1\\\\}^N : \\\\delta(0, x) = D\\\\} \\\\). For every \\\\( \\\\alpha \\\\in (0, 1) \\\\) and \\\\( \\\\beta \\\\in (0, 1) \\\\) such that \\\\( D \\\\leq \\\\alpha \\\\beta N \\\\), there exists some subset \\\\( \\\\Theta \\\\) of \\\\( \\\\{0, 1\\\\}^N \\\\) with the following properties,\\n\\n\\\\[\\n\\\\delta(\\\\theta, \\\\theta') > 2(1 - \\\\alpha)D \\\\quad \\\\forall (\\\\theta, \\\\theta') \\\\in \\\\Theta^2, \\\\quad \\\\theta \\\\neq \\\\theta'.\\n\\\\]\\n\\n(12)\\n\\n\\\\[\\n\\\\ln |\\\\Theta| \\\\geq \\\\rho D \\\\ln N_{\\\\Delta},\\n\\\\]\\n\\n(13)\\n\\nwhere,\\n\\n\\\\[\\n\\\\rho = \\\\alpha - \\\\ln(\\\\alpha \\\\beta)\\\\left(-\\\\ln(\\\\beta) + \\\\beta - 1\\\\right).\\n\\\\]\\n\\n(14)\\n\\nCorollary A.1.1.\\n\\nLet \\\\( S \\\\) be the set of all unit vectors of \\\\( \\\\mathbb{R}^N \\\\), that is, \\\\( S = \\\\{u | u \\\\in \\\\mathbb{R}^N, \\\\|u\\\\| = 1\\\\} \\\\).\\n\\nThen for any constant \\\\( c \\\\geq 2N \\\\), there exists some subset \\\\( \\\\tilde{S} \\\\subseteq S \\\\) of size \\\\( \\\\ln(|\\\\tilde{S}|) \\\\geq N \\\\) such that, for all \\\\( u, v \\\\in \\\\tilde{S} \\\\),\\n\\n\\\\[\\nu^T v < c\\\\]\\n\\nProof.\\n\\nWe set the value of \\\\( D = \\\\frac{N}{2} \\\\). Consider the set \\\\( S_1 = \\\\{u \\\\sqrt{D} | u \\\\in \\\\{0, 1\\\\}^N, \\\\|u\\\\|_0 = D\\\\} \\\\). It is easy to see that \\\\( S_1 \\\\subseteq S \\\\). Observe that for any \\\\( u, v \\\\in S_1 \\\\), \\\\( \\\\delta(u, v) > N(1 - c^2/2) \\\\) if and only if \\\\( u^T v < c \\\\). Now, we set \\\\( \\\\alpha = c^2/2 \\\\), \\\\( \\\\beta = 1/c \\\\), and apply Gilbert-Varshamov Bound,\\n\\n\\\\[\\n\\\\ln(|\\\\tilde{S}|) \\\\geq (c \\\\ln(c) - c + 1)N.\\n\\\\]\\n\\n(15)\"}"}
{"id": "AfnsTnYphT", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nB.1 The following helper lemma derives the KL divergence between two transformations of SSD, namely $U \\\\circ SSD_t$ and $V \\\\circ SSD_t$.\\n\\n**Lemma B.1.** For any $U, V \\\\in O(kd)$, then the KL Divergence between $U \\\\circ SSD_t$ and $V \\\\circ SSD_t$ is,\\n\\n$$KL(U \\\\circ SSD_t || V \\\\circ SSD_t) = 1 - \\\\cos(\\\\alpha) \\\\sigma^2$$\\n\\nwhere $\\\\cos(\\\\alpha) = (U \\\\mu_t)^T V \\\\mu_t$.\\n\\n**Proof.**\\n\\n$$KL(U \\\\circ SSD_t || V \\\\circ SSD_t) = E_{(x, y) \\\\sim U \\\\circ SSD_t} \\\\ln \\\\frac{\\\\exp -\\\\|x - y U \\\\mu_t\\\\|^2_{\\\\sigma^2}}{\\\\exp -\\\\|x - y V \\\\mu_t\\\\|^2_{\\\\sigma^2}}$$\\n\\n$$= E_y E_{x \\\\sim y U \\\\mu_t + \\\\sigma \\\\epsilon} \\\\ln \\\\frac{\\\\exp -\\\\|x - y U \\\\mu_t\\\\|^2_{\\\\sigma^2}}{\\\\exp -\\\\|x - y V \\\\mu_t\\\\|^2_{\\\\sigma^2}}$$\\n\\n$$= E_y E_{x \\\\sim U \\\\mu_t + \\\\sigma \\\\epsilon} \\\\ln \\\\frac{\\\\exp -\\\\|x - U \\\\mu_t\\\\|^2_{\\\\sigma^2}}{\\\\exp -\\\\|x - V \\\\mu_t\\\\|^2_{\\\\sigma^2}}$$\\n\\n$$= E_x \\\\ln \\\\frac{\\\\exp (x^T U \\\\mu_t / \\\\sigma^2)}{\\\\exp (x^T V \\\\mu_t / \\\\sigma^2)}$$\\n\\n$$= E_x (x^T U \\\\mu_t / \\\\sigma^2) - (x^T V \\\\mu_t / \\\\sigma^2)$$\\n\\n$$= \\\\mu_t^T U^T U \\\\mu_t / \\\\sigma^2 - \\\\mu_t^T U^T V \\\\mu_t / \\\\sigma^2$$\\n\\n$$= 1 - \\\\cos(\\\\alpha) \\\\sigma^2$$\\n\\nwhich proves the required result.\\n\\n**Theorem 5.1** (Fano's Theorem for Randomized Algorithms). Under the notation established above, let $V$ be an index set of finite cardinality of some chosen subset of $P$. Then, we define $P_V := \\\\{P_v | \\\\forall v \\\\in V\\\\}$, and $F_V := \\\\{\\\\theta \\\\star (P_v) | \\\\forall v \\\\in V\\\\}$. For some fixed parameter $\\\\delta > 0$, let $\\\\rho$ satisfy the condition that, for all $f_u \\\\neq f_v \\\\in F_V$ and $f \\\\in F$, if $\\\\rho(f, f_u) < \\\\delta$, then $\\\\rho(f, f_v) > \\\\delta$. And, for all $P_u, P_v \\\\in P_V$, $u \\\\neq v$, let the KL divergence satisfy $KL(P_u || P_v) \\\\leq D$ for some $D > 0$. Then,\\n\\n$$M_n(\\\\Theta) \\\\geq \\\\delta \\\\frac{1}{nD} + \\\\ln(2) \\\\ln(|V|)$$\\n\\n**Proof.** From the definition of minimax risk,\\n\\n$$M_n(\\\\Theta) = \\\\inf_{\\\\theta \\\\in \\\\Theta} \\\\sup_{P \\\\in P} E_{S_n \\\\sim P_n, \\\\xi \\\\sim P(\\\\Xi)} [\\\\rho(\\\\theta(S_n, \\\\xi), \\\\theta \\\\star (P))]$$\\n\\n$$\\\\geq \\\\inf_{\\\\theta \\\\in \\\\Theta} \\\\sup_{Q \\\\in Q_n V} E_{(S_n, \\\\xi) \\\\sim Q} [\\\\rho(\\\\theta(S_n, \\\\xi), \\\\theta \\\\star (Q))]$$\\n\\nwhere $Q_n V := \\\\{Q(S_n, \\\\xi) := P_n(S_n) \\\\ast P(\\\\Xi)(\\\\xi) | P \\\\in P_V\\\\}$, and we overload the target mapping notation and set $\\\\theta \\\\star (Q) = \\\\theta \\\\star (P)$, where $P$ is the distribution corresponding to $Q$. First, observe that for all $Q_u, Q_v \\\\in Q_n V$, $u \\\\neq v$, the KL divergence between the two distributions is given by,\\n\\n$$KL(Q_u || Q_v) = E_{(S_n, \\\\xi) \\\\sim Q_u} Q_u(S_n, \\\\xi) Q_v(S_n, \\\\xi)$$\\n\\n$$= E_{S_n \\\\sim P_n u, \\\\xi \\\\sim P(\\\\Xi)} P_n u(S_n) P(\\\\Xi)(\\\\xi) P_n v(S_n) P(\\\\Xi)(\\\\xi)$$\\n\\n$$= KL(P_n u || P_n v) = nD.$$\"}"}
{"id": "AfnsTnYphT", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We follow in the footsteps of the proof of Fano's Theorem [Prop 7.3 Duchi (2021)]. For any $Q \\\\in \\\\mathbb{Q}$,\\n\\n$$\\nE_Q[\\\\rho(\\\\theta_n, \\\\theta^{\\\\star}(Q))] \\\\geq E_Q[\\\\delta_1\\\\{\\\\rho(\\\\theta_n, \\\\theta^{\\\\star}(Q)) \\\\geq \\\\delta\\\\}] \\\\geq \\\\delta P[\\\\rho(\\\\theta_n, \\\\theta^{\\\\star}(Q)) \\\\geq \\\\delta].\\n$$\\n\\nWe define the testing function, $\\\\Psi: \\\\mathcal{F} \\\\to \\\\mathcal{V}$ as,\\n\\n$$\\n\\\\Psi(f) := \\\\arg\\\\min_{v \\\\in \\\\mathcal{V}} \\\\{\\\\rho(f, \\\\theta^{\\\\star}(Q_v))\\\\},\\n$$\\n\\nwhere ties can be broken arbitrarily and the analysis would still hold. Let $v$ be the uniform random variable over $\\\\mathcal{V}$. Recall the assumption on $\\\\rho$ that, if $\\\\rho(f, Q_u) < \\\\delta$, then $\\\\rho(f, Q_v) > \\\\delta$.\\n\\n$$\\n\\\\sup_{Q \\\\in \\\\mathbb{Q}} P[\\\\rho(\\\\theta_n, \\\\theta^{\\\\star}(Q)) \\\\geq \\\\delta] \\\\geq 1 - \\\\frac{1}{|\\\\mathcal{V}|} \\\\sum_{v \\\\in \\\\mathcal{V}} P[\\\\Psi(\\\\theta_n) \\\\neq v | v = v] \\\\geq \\\\inf_{\\\\Psi} P[\\\\Psi(\\\\theta_n) \\\\neq v].\\n$$\\n\\nFrom the above, 24, and Prop 7.10 and Eq 7.4.5 from Duchi (2021), we have the result,\\n\\n$$M_n(\\\\Theta) \\\\geq \\\\delta \\\\left(1 - n\\\\Delta + \\\\ln(2) \\\\ln(|\\\\mathcal{V}|)\\\\right).$$\"}"}
{"id": "AfnsTnYphT", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma C.1. Let $S_n \\\\sim (\\\\text{SSD}_1)$ be $n$ i.i.d. data samples drawn from SSD$_1$. Define the equivariance group $U := O(kd)$. Define the subset $\\\\tilde{U} \\\\subseteq U$ such that, for all $U \\\\in \\\\tilde{U}$, $t \\\\in \\\\{2, \\\\ldots, k\\\\}$, $U\\\\mu_t = e^{kd-k+t}$. Let $P := \\\\{U \\\\circ P | U \\\\in \\\\tilde{U}\\\\}$ be the set of problem distributions. Let $\\\\Xi$ be the sample space that encapsulates algorithmic randomness, and $P_\\\\Xi$ be a distribution over $\\\\Xi$. Let $\\\\Theta := \\\\{\\\\theta : ((X_m, \\\\xi) \\\\times \\\\Xi) \\\\to S_{kd-1}\\\\}$ be the set of $U$-equivariant randomized algorithms that estimate the mean of the input distribution using $n$ i.i.d. samples. If, \\n\\\\[\\n\\\\inf_{\\\\theta \\\\in \\\\Theta} \\\\sup_{U \\\\in \\\\tilde{U}} E_{\\\\xi \\\\sim P_\\\\Xi} \\\\|\\\\theta_n - U\\\\mu_1\\\\| \\\\geq 0.25,\\n\\\\]\\nthen $n = \\\\Omega(\\\\sigma^2_{kd})$, for large enough $k, d$.\\n\\nProof. We will prove this statement using Fano's Theorem for Randomized Algorithms 5.1. Observe that since $\\\\|\\\\cdot\\\\|$ is already a metric, the relaxed semi-metric property holds for all $\\\\delta$.\\n\\nWe begin by constructing a $2\\\\delta$, $\\\\delta = 0.25$, packing of the set of means $S = \\\\{U\\\\mu_1 | U \\\\in \\\\tilde{U}\\\\}$. Observe from the construction of $\\\\tilde{U}$ that, $S \\\\supset S_1 := \\\\{u | u \\\\in \\\\mathbb{R}^{kd}, \\\\|u\\\\| = 1, u \\\\in \\\\text{Span}(\\\\{e_1, \\\\ldots, e_{kd-k}\\\\})\\\\} \\\\sim S_2 := \\\\{u | R^{kd-k}, \\\\|u\\\\| = 1\\\\}$, $S_3 := \\\\{u | R^{kd-k}, \\\\|u\\\\| = 1, u[j] = 1/\\\\sqrt{kd-k}, j \\\\in [kd-k/2]\\\\} \\\\sim S_4 := \\\\{u | R^{kd-k/2}, \\\\|u\\\\| = 1/2\\\\}$, where $\\\\sim$ denotes the fact that $S_1, S_2, S_3, S_4$ are isometric sets under the Euclidean norm.\\n\\nTherefore, it is enough to find a $2\\\\delta$ packing of $S_4$ to find a $2\\\\delta$ packing of $S_1$. Now, define the set $S_5 := \\\\{u | R^{kd-k/2}, \\\\|u\\\\| = 1\\\\}$, and observe that $(S_4, 2\\\\star\\\\|\\\\cdot\\\\|)$ and $(S_5, \\\\|\\\\cdot\\\\|)$ are isometric. Therefore, it is enough to find a $4\\\\delta$ packing of $S_5$.\\n\\nNow observe that for any $u, v \\\\in S_5$, $\\\\|u - v\\\\| \\\\geq 4\\\\star 0.25 \\\\iff u^T v \\\\leq 1/2$. Therefore, for large enough $k, d$, by Corollary A.1.1, we have that the size ($N_1$) of a $2\\\\delta$ packing of $S$ satisfies,\\n\\\\[\\n\\\\ln(N_1) \\\\geq 0.15kd.\\n\\\\]\\n\\nNote from Lemma B.1, that the KL divergence between any two distinct distributions, $P, Q$, corresponding to the $2\\\\delta$ packing satisfies $\\\\text{KL}(P \\\\| Q) \\\\leq 1/2 \\\\sigma^2$. Applying Fano's Theorem for Randomized Algorithms 5.1, we get,\\n\\\\[\\nM_n(\\\\Theta) \\\\geq 0.25 - n/2 \\\\sigma^2 + \\\\ln(2)0.15kd,\\n\\\\]\\nwhich implies that $n = \\\\Omega(\\\\sigma^2_{kd})$, completing the proof.\\n\\nTheorem C.1. Let $F$ denote the class of functions represented by the set of fully connected neural network models, $M_F[W]$, as defined in 4. Let $S_n \\\\sim (\\\\text{SSD}_1)$ be the $n$ i.i.d. data samples drawn from SSD$_1$, with $\\\\sigma = \\\\tilde{O}(1/\\\\sqrt{k})$, and $k = O(\\\\exp(d))$. Define the equivariance group $U := O(kd)$. Define the subset $\\\\tilde{U} \\\\subseteq U$ such that, for all $U \\\\in \\\\tilde{U}$, $t \\\\in \\\\{2, \\\\ldots, k\\\\}$, $U\\\\mu_t = e^{kd-k+t}$. Let $\\\\xi \\\\in \\\\Xi$ encapsulate the randomization, and let $\\\\xi \\\\sim P_\\\\Xi$. Let $\\\\Theta = \\\\{\\\\theta : ((X_m, \\\\xi) \\\\times \\\\Xi) \\\\to F\\\\}$ be the set of $U$-equivariant algorithms, such that $b^T b_{\\\\text{min}} := 10^{-2}$, then for large enough $k, d$,\\n\\\\[\\n\\\\inf_{\\\\theta \\\\in \\\\Theta} \\\\sup_{U \\\\in \\\\tilde{U}} E_{\\\\xi \\\\sim P_\\\\Xi} E_{S_m \\\\sim (U \\\\circ \\\\text{SSD}_1)}[\\\\theta(S_m, \\\\xi) - (U \\\\circ \\\\text{SSD}_1)] \\\\leq \\\\delta,\\n\\\\]\\niff $n = \\\\Omega(\\\\sigma^2_{kd})$, for $\\\\delta = 0.5 \\\\times 10^{-2}$. \\n\\nProof. The proof proceeds by reducing the problem of finding a fully-connected neural network with a small expected risk to a problem of estimating the unknown mean of a Gaussian distribution. We then use Lemma C.1 to establish the required sample complexity bound.\"}"}
{"id": "AfnsTnYphT", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our approach diverges from Li et al. (2021); Wang & Wu (2023), because in our task, the marginal over the input is not a 0-mean Gaussian, but a mixture of $k$ Gaussians, which is not an orthogonally invariant distribution. As a consequence, for deriving lower bounds, we cannot apply the Benedek-Itai bound from Benedek & Itai (1991) as done in Li et al. (2021), nor can we directly use Fano's Theorem as done in Wang & Wu (2023) owing to the absence of the semi-metricness of $l_2$ loss under an invariant distribution, and analyzing the expected risk under a mixture of Gaussians is analytically difficult. Instead, we utilize a novel technique that leverages the randomness of the training algorithm to break the original minimax lower-bound problem into $k$ simpler problems using a simulation-style argument. In case of FCNs, we prove sample complexity lower bounds for the $k$ simpler problems using a novel boosting technique to derive a reduction to the Gaussian mean estimation problem on the unit sphere. To prove sample complexity lower bounds for the simpler problems in the case of LCNs, we prove a variant of Fano's Theorem that can be used for randomized algorithms. Distinctively, our variant does not require the semi-metric property to hold on the entire space of output functions, as is needed in the \u201cFano's Theorem for Random Estimators\u201d developed in Wang & Wu (2023). Our sample complexity upper bounds depend on the analysis of an equivariant gradient descent style algorithm on LCNs and CNNs. This is unlike the separation proved in Wang & Wu (2023), where they use covering number-based arguments for ERM analysis. The advantage of doing a gradient descent analysis over an ERM analysis is two fold: First, it demonstrates a sample complexity separation for computationally-efficient (poly time) equivariant algorithms. This distinction is crucial because while a separation may exist for computationally inefficient algorithms, the separation might disappear under constraints of computational efficiency. Second, for a valid separation, it is important to ensure that both the upper and lower bounds are derived for equivariant algorithms since non-equivariant algorithms could potentially be more sample-efficient than their equivariant counterparts. Furthermore, our approach differs significantly from Karp et al. (2021), which analyzes population gradients by assuming enough (poly $(k, d)$) samples at each iteration to yield a representational gap between CNNs and CNTKs (Convolutional Neural Tangent Kernels). Since we are interested in sample complexity separation, we adopt a more direct analysis of empirical gradients.\\n\\nAnother work, Malach & Shalev-Shwartz (2020), proved a computational separation between FCNs and CNNs on a \\n$k$-pattern classification task. In the task, the inputs are from the hypercube $\\\\{-1, 1\\\\}^n$, and the label is based on a set of $k$ consecutive coordinates. They employ random-feature analysis to establish that CNNs, with $2k$ hidden nodes, can learn this task in $O(2^k n)$ samples. In contrast, we only require $O(k)$ nodes and samples. Furthermore, they do not provide lower bounds for FCNs, and instead argue that the gradient is too small for a finite precision machine. Additionally, since their task does not encode translation invariance, they cannot prove a separation between LCNs and CNNs.\\n\\n### Notation\\n\\n**Vector and Matrix Notation:**\\n- We use bold lowercase letters, such as $x, y$, to represent vectors, and bold uppercase letters, such as $U, V$, to represent matrices.\\n- Let $[n]$ denote the set $\\\\{1, \\\\ldots, n\\\\}$.\\n- We denote the standard basis of $\\\\mathbb{R}^n$ by $B_n$ and the individual basis vectors by $e_i$.\\n- We define the function $\\\\text{idx}_n : B_n \\\\to [n]$, $\\\\text{idx}_n(e_l) = l$, for all $l \\\\in [n]$.\\n- For any $x$, indexed from 1, we use $x[i:j] \\\\in \\\\mathbb{R}^{j-i+1}$ to represent a slice from its $i$-th to its $j$-th entry.\\n- For a set $\\\\{x_i\\\\}_{i=1}^n$, we employ $(x_1, \\\\ldots, x_n)$ to denote the sequential length-wise concatenation of the vectors, and $(x_1; \\\\ldots; x_n)$ to denote the sequential row-wise stacking of the vector transposes into a matrix.\\n- Conversely, for any $x$ constructed via $(;)$ or $(,)$ notation, we denote its $i$-th component vector by $x(i)$.\\n- We use $U = \\\\text{Block}(\\\\{U_1, \\\\ldots, U_n\\\\})$ to be the matrix having diagonal blocks of $U_i$'s in-sequence, with other entries set to zero. Conversely, for any $U$ constructed via $\\\\text{Block}(\\\\cdot)$, we denote its $i$-th component matrix by $U(i)$.\\n- The Euclidean norm for vectors and the spectral norm for matrices are both denoted by $\\\\|\\\\cdot\\\\|$.\\n\\n**Group Notation:**\\n- Let $U_1$ and $U_2$ be any two subgroups of $\\\\text{GL}(n, \\\\mathbb{R})$. Then, we define the binary operation $\\\\star$ such that $U_1 \\\\star U_2 = \\\\{U_1 U_2 \\\\ldots U_n | U_i \\\\in U_1 \\\\cup U_2, n \\\\in \\\\mathbb{N}\\\\}$. It is easy to see that $U_1 \\\\star U_2$.\"}"}
{"id": "AfnsTnYphT", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"is also a subgroup of $GL(n, \\\\mathbb{R})$. We denote $O(n)$ to be the group of orthonormal matrices on $\\\\mathbb{R}^{n \\\\times n}$ and $O_p(n)$ be the group of permutation matrices on $\\\\mathbb{R}^{n \\\\times n}$.\\n\\n**Task Notation:** Let $X \\\\subseteq \\\\mathbb{R}^p$, and $Y \\\\subseteq \\\\mathbb{R}^d$ denote the input and output space of a $p$ dimensional problem. Let $P$ be any distribution over $(X, Y)$ and $\\\\tau : X \\\\to X$ be any function, then we define the distribution $\\\\tau \\\\circ P$ over $(X, Y)$ by sampling $(x, y) \\\\sim P$ and returning $(\\\\tau(x), y)$. Let $P$ be a set of distributions over $(X, Y)$, then we define the set $\\\\tau \\\\circ P := \\\\{\\\\tau \\\\circ P | P \\\\in P\\\\}$. Alternatively, let $T$ be a set of functions from $X \\\\to X$, then we define $T \\\\circ P := \\\\{\\\\tau_i \\\\circ P | \\\\tau_i \\\\in T\\\\}$.\\n\\n**Model Notation:** We denote a parametric model by $M$ and its parameter set by $W$. The model along with its parameter is a function from $X$ to $\\\\mathbb{R}$. Specifically, \\\\( \\\\forall w \\\\in W, M[w] : X \\\\to \\\\mathbb{R} \\\\).\\n\\nWe will use $O(\\\\cdot)$, $\\\\Omega(\\\\cdot)$, and $\\\\Theta(\\\\cdot)$ as the Big-O, Big-Omega, and Big-Theta notation respectively. The notation $\\\\tilde{O}(\\\\cdot)$, $\\\\tilde{\\\\Omega}(\\\\cdot)$, and $\\\\tilde{\\\\Theta}(\\\\cdot)$ hides logarithmic factors.\\n\\n### 4 ORSETTING\\n\\nWe introduce the Dynamic Signal Distribution, an image-like task which is inspired from Karp et al. (2021). We also specify the FCN, LCN, and CNN architectures that we consider for our analysis.\\n\\n#### 4.1 DYNAMIC SIGNAL DISTRIBUTION (DSD)\\n\\nIn many vision-based tasks, the output often relies on a local \\\"signal\\\" in the image, a property referred to as locality. Often, this signal is enveloped in random noise, and satisfies translation invariance, that is its movement within the image does not alter the output. The DSD task is designed to capture the both locality and translation invariance properties into an analyzable task.\\n\\nWe define the input space as $X = \\\\mathbb{R}^{kd}$ and the output space as $Y = \\\\mathbb{R}$. Any input vector $x \\\\in X$ is structured as $(x(1), \\\\ldots, x(k))$, with each $x(i)$ being a vector in $\\\\mathbb{R}^d$, and representing the $i$th patch of $x$.\\n\\nThus, each input consists of $k$ consecutive patches of dimension $d$. To model the local signal, we employ an unknown unit vector $w^* \\\\in \\\\mathbb{R}^d$, with $\\\\|w^*\\\\| = 1$. To include translation invariance in the task, this signal $w^*$ can reside within any one of the $k$ patch locations, described above. Specifically, for each $i$ in $[k]$, we define a $d$-sparse mean vector $\\\\mu_i \\\\in \\\\mathbb{R}^{kd}$, such that $\\\\mu_i[(i-1)d+1:id] = w^*$ and all its other entries are zero. The noise is chosen to be isotropic Gaussian, with variance $\\\\sigma^2 \\\\in \\\\mathbb{R}^+$. Formally, DSD is a distribution over $(X, Y)$ with the generative story: sample the index $i \\\\sim \\\\text{Unif}([k])$, and the label $y \\\\sim \\\\text{Unif}\\\\{-1, 1\\\\}$. Then, sample the input data as $x|y, i \\\\sim N(y\\\\mu_i, \\\\sigma^2 I_{kd})$. Observe that the probability density function (pdf) of DSD is, $p(x, y) = \\\\frac{1}{2} k (\\\\sqrt{2\\\\pi\\\\sigma^2})^{kd} \\\\prod_{i=1}^k \\\\exp\\\\left(-\\\\frac{\\\\|x - y\\\\mu_i\\\\|^2}{2\\\\sigma^2}\\\\right)$.\\n\\nWe also define the Static Signal Distribution (SSD$_t$), which is the conditional distribution of DSD when the index parameter is fixed at $i = t$. Specifically, the label is chosen as $y \\\\sim \\\\text{Unif}\\\\{-1, 1\\\\}$, and then input data is sampled as $x|y \\\\sim N(y\\\\mu_t, \\\\sigma^2 I_{kd})$. We will use this distribution in proving the lower bounds in theorem 6.1, 7.1, by reducing the problem of learning DSD to learning each SSD$_t$.\\n\\n#### 4.2 NEURAL NETWORK ARCHITECTURES\\n\\nWe now introduce the model architectures that we consider for our analysis. We adopt the Local Signal Adaptivity (LSA) activation function, first introduced in Karp et al. (2021), for all models, $\\\\phi_b(x) : \\\\mathbb{R} \\\\to \\\\mathbb{R} := \\\\text{ReLU}(x - b) - \\\\text{ReLU}(-x - b)$, (3)\\n\\nwhere $b \\\\in \\\\mathbb{R}^+$ is the trainable bias parameter. The rationale for choosing $\\\\phi_b(x)$ is its capability to 'filter out' noise below the magnitude of $b$, while letting signals of magnitude larger than $b$ to propagate through the network. This denoising helps the network learn the signal with fewer samples.\\n\\nWe also note that the LSA activation function, also known as the \\\"soft-thresholding function\\\" , is extensively used in high-dimensional sparse recovery problems (Section 18.2 Hastie et al. (2009)). Since our task involves recovering the sparse mean vector, it further justifies its use for the DSD task.\"}"}
{"id": "AfnsTnYphT", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FCN: We consider a one-hidden-layer network with \\\\( k \\\\) hidden nodes. Each hidden node \\\\( i \\\\) is associated with a parameter vector \\\\( w_i \\\\in \\\\mathbb{R}^{kd} \\\\), such that \\\\( \\\\|w_i\\\\| \\\\leq 1 \\\\). The complete model parameter vector is given by \\\\( v = [w_1, \\\\ldots, w_k, b] \\\\in \\\\mathbb{W} \\\\), where \\\\( \\\\mathbb{W} = \\\\mathbb{R}^{kd \\\\times d} \\\\times \\\\mathbb{R}^+ \\\\). The function form for FCN is,\\n\\n\\\\[\\nM_F[v](x) : \\\\mathbb{X} \\\\rightarrow \\\\mathbb{R} : \\\\sum_{i=1}^{k} \\\\phi_{b}(w_i^T x_i)\\n\\\\]\\n\\nLCN: Similar to FCN, we consider a one-hidden-layer network featuring \\\\( k \\\\) hidden nodes. The \\\\( i \\\\)-th node is associated with the parameter vector \\\\( w_i \\\\in \\\\mathbb{R}^d \\\\), \\\\( \\\\|w_i\\\\| \\\\leq 1 \\\\). The complete model parameter vector is given by \\\\( v = [w_1, \\\\ldots, w_k, b] \\\\), and \\\\( \\\\mathbb{W} = \\\\mathbb{R}^{kd \\\\times d} \\\\times \\\\mathbb{R}^+ \\\\). The function form for LCN is,\\n\\n\\\\[\\nM_L[v](x) : \\\\mathbb{W} \\\\rightarrow \\\\mathbb{R} : \\\\sum_{i=1}^{k} \\\\phi_{b}(w_i^T x_i(i))\\n\\\\]\\n\\nCNNs: We consider a one hidden-layer CNN has \\\\( k \\\\) hidden nodes. The parameter \\\\( w \\\\in \\\\mathbb{R}^d \\\\), \\\\( \\\\|w\\\\| \\\\leq 1 \\\\) is the shared across all nodes. The composite vector \\\\( v = [w, b] \\\\) is our complete model parameter vector, and \\\\( \\\\mathbb{W} = \\\\mathbb{R}^{d \\\\times d} \\\\times \\\\mathbb{R}^+ \\\\). The function form for CNN is,\\n\\n\\\\[\\nM_C[v](x) : \\\\mathbb{W} \\\\rightarrow \\\\mathbb{R} : \\\\sum_{i=1}^{k} \\\\phi_{b}(w^T x(i))\\n\\\\]\\n\\nThe subscripts \\\\( F, L, \\\\) and \\\\( C \\\\) denotes that the model corresponds to a FCN, LCN, and CNN respectively.\\n\\n5 Mathematical Background\\n\\n5.1 Technical Definitions\\n\\nDefinition 1 (Loss Function). We define the loss function for our task as \\\\( \\\\text{err} : (\\\\bar{Y}, Y) \\\\rightarrow \\\\mathbb{R}^+ \\\\), \\\\( \\\\text{err}(\\\\bar{y}, y) = (\\\\bar{y} - y)^2 \\\\).\\n\\nDefinition 2 (Risk). Let \\\\( F = \\\\mathbb{Y} \\\\times \\\\mathbb{X} \\\\), and let \\\\( P \\\\) be the set of all distributions over \\\\((\\\\mathbb{X}, \\\\mathbb{Y})\\\\). Then, we define the risk \\\\( R : (F, P) \\\\rightarrow \\\\mathbb{R}^+ \\\\) of a function \\\\( f \\\\in F \\\\) with respect to the distribution \\\\( P \\\\in P \\\\) as,\\n\\n\\\\[\\nR(f, P) = \\\\mathbb{E}_{(x, y) \\\\sim P} [\\\\text{err}(f(x), y)]\\n\\\\]\\n\\nDefinition 3 (Algorithm). Let \\\\( F \\\\subseteq \\\\mathbb{Y} \\\\times \\\\mathbb{X} \\\\), \\\\( \\\\Xi \\\\) be the sample space that encapsulates all algorithmic randomness, and \\\\( P_\\\\Xi \\\\) be some fixed distribution over \\\\( \\\\Xi \\\\). Then, a randomized algorithm denoted by \\\\( \\\\theta : ((\\\\mathbb{X}, \\\\mathbb{Y})_n, \\\\Xi) \\\\rightarrow F \\\\), is a function defined from the product space of input data and randomness to the space of possible functions. The randomness is realized by sampling from the distribution \\\\( P_\\\\Xi \\\\).\\n\\nDefinition 4 (Iterative (Randomized) Algorithm). Consider a parametric model \\\\( M[w] \\\\), and its parameter set \\\\( W \\\\), such that for any \\\\( w \\\\in W \\\\), \\\\( M[w] \\\\) is a maps from the input space \\\\( \\\\mathbb{X} \\\\) to the output space \\\\( \\\\mathbb{Y} \\\\).\\n\\n\\\\[\\nF = \\\\{M[w] | w \\\\in W\\\\}\\n\\\\]\\n\\nLet the model parameters be initialized via a distribution \\\\( W \\\\) over \\\\( W \\\\), \\\\( w_0 \\\\sim W \\\\). Let \\\\( T \\\\) be the number of iterations and \\\\( F_t : (W, S_n) \\\\rightarrow W \\\\) be the update functions for each iteration \\\\( t \\\\). Then the function \\\\( \\\\theta : ((\\\\mathbb{X}, \\\\mathbb{Y})_n, W; M[W], \\\\{F_t\\\\}) \\\\rightarrow F \\\\) is an iterative algorithm if it adheres to the procedure 1.\\n\\nDefinition 5 (Sample Complexity). Let \\\\( P \\\\) be a distribution over \\\\((\\\\mathbb{X}, \\\\mathbb{Y})\\\\) and \\\\( \\\\theta_n \\\\) be a randomized algorithm as defined in 3. Let \\\\( S_n \\\\sim P \\\\) be \\\\( n \\\\) i.i.d. data points sampled from \\\\( P \\\\). For any \\\\( \\\\delta \\\\in [0, 1] \\\\), we define the \\\\( \\\\delta \\\\)-sample complexity of \\\\( \\\\theta_n \\\\) as,\\n\\n\\\\[\\nn_\\\\delta(\\\\theta_n, P) = \\\\min_{n \\\\in \\\\mathbb{N}} \\\\{n \\\\in \\\\mathbb{N} | \\\\mathbb{E}[R(\\\\theta_n, P)] \\\\leq \\\\delta\\\\}\\n\\\\]\\n\\nWhere the expectation is over the input data \\\\( S_n \\\\), and the algorithmic randomization.\\n\\nWe may omit \\\\((\\\\mathbb{X}, \\\\mathbb{Y})\\\\) and \\\\( \\\\Xi \\\\) from the notation when they are clear from the context and use the random variable notation \\\\( \\\\theta_n \\\\) instead, where \\\\( n \\\\) denotes the number of samples.\"}"}
{"id": "AfnsTnYphT", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\nIterative Algorithm\\n\\nRequire:\\nUpdate functions $\\\\{F_t\\\\}$, Set of $n$ i.i.d. data samples $S_n$, Parameter initialization $w_0$,\\n\\n$t \\\\leftarrow 1$\\nwhile $t \\\\leq T$ do\\n    $w_t \\\\leftarrow F_t(w_{t-1}, S_n)$\\n    $t \\\\leftarrow t + 1$\\nend while\\nreturn $w_T$\\n\\n5.2 EQUIVARIANT ALGORITHMS\\n\\nWe introduce the concept of equivariant algorithms, originally presented in Li et al. (2021). To keep it concise, we provide a simplified version which is sufficient for our purposes.\\n\\nTo motivate the definition of equivariant algorithms, we review the following thought experiment. Consider a neural network parameterized as $f(Ax, b)$, where $A \\\\in \\\\mathbb{R}^{q \\\\times p}$ is the parameter of the first linear layer, while $b \\\\in \\\\mathbb{R}^q$ encapsulates the remaining parameters. We initialize the parameters as $(A_0, b_0)$ and use gradient descent, with learning rate $\\\\eta$, to train the network on the dataset $\\\\{x_i, y_i\\\\}_n$.\\n\\nIn parallel, we train another network initialized as $(A_0U^T, b_0)$, with the dataset $\\\\{Ux_i, y_i\\\\}_n$. Here, $U \\\\in O(p)$ such that $A_0U^T$ and $A_0$ are identically distributed.\\n\\nObserve that at the first iteration, the output of the first hidden layer for both networks is the same, $A_0U^TUx_i = A_0x_i$. This implies that the gradients with respect to the pre-activations of the first layer are also equal. Consequently, the gradients with respect to the matrix parameters satisfy the relation, $\\\\frac{d}{dA_0} \\\\text{loss}(A_0U^T) = \\\\frac{d}{dA_0} \\\\text{loss}(A_0U^T) := \\\\Delta U^T$. Thus, after the first iteration, the parameter sets for the two neural networks are, $(A_1, b_1) = (A_0 - \\\\eta \\\\Delta, b_1)$, $(A_1U^T, b_1) = (A_0U^T - \\\\eta \\\\Delta U^T, b_1)$, respectively. By induction, this property is preserved across all iterations $t$, resulting in the parameters for the two neural networks being $(A_t, b_t)$ and $(A_tU^T, b_t)$, respectively.\\n\\nThe key idea is that the risk of a network parameterized as $(A_t, b_t)$ on any data $\\\\{x, y\\\\}$ is the same as its counterpart with parameters $(A_tU^T, b_t)$ on the transformed data $\\\\{Ux, y\\\\}$. Now since $A_0U^T$ and $A_0$ have the same distribution, we can infer that the expected risk of this network trained with gradient descent is invariant to the transformation $U$ of the input distribution. In other words, the network learns the original distribution and the transformed distribution equally well. Formally,\\n\\nDefinition 6 (U-equivariant algorithm).\\nUnder the notation established in definition 4, let the input space $X \\\\subseteq \\\\mathbb{R}^p$, the output space $Y \\\\subseteq \\\\mathbb{R}$, and the parameter set $W \\\\subseteq \\\\mathbb{R}^m$. Let $U \\\\subseteq O(p)$, then an iterative algorithm $\\\\bar{\\\\theta}_n$ is $U$-equivariant if there exists a set $V \\\\subseteq O(m)$, such that,\\n\\n1. For all $U \\\\in U$, there exists $V \\\\in V$ such that for all $x \\\\in X$, and $w \\\\in W$, $M[w](x) = M[Vw](Ux)$.\\n2. For all $U \\\\in U$, the same $V \\\\in V$ as defined in (1) satisfies $\\\\forall \\\\{x_i, y_i\\\\}_n \\\\in (X, Y)_n$, $\\\\forall t \\\\in [T]$, and $w \\\\in W$, $V F_t(w, \\\\{x_i, y_i\\\\}_n) = F_t(Vw, \\\\{Ux_i, y_i\\\\}_n)$.\\n3. If $w \\\\sim W$, then for all $V \\\\in V$, $Vw \\\\overset{d}{=} w$.\\n\\nAnd, equivariant algorithms satisfy the following property,\\n\\nLemma 5.1. (Section 4.1 Li et al. (2021)) If $\\\\bar{\\\\theta}_n$ is a $U$-equivariant algorithm, then $\\\\forall x \\\\in X, U \\\\in U$, $\\\\bar{\\\\theta}_n(\\\\{x_i, y_i\\\\}_n)(x) \\\\overset{d}{=} \\\\bar{\\\\theta}_n(\\\\{Ux_i, y_i\\\\}_n)(Ux)$, (10) where the randomness is over initialization.\\n\\nThis property formalizes the conclusion drawn in the thought experiment. That is the performance of an equivariant algorithm when trained on $n$ i.i.d. samples from $P_1$ and tested on $P_2$ would be the same, in distribution, had it been trained on $n$ i.i.d. samples from $U \\\\circ P_1$ and tested on $U \\\\circ P_2$. \"}"}
{"id": "AfnsTnYphT", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We will now follow in the footsteps of our analysis of update step 1. We begin by analyzing the push forward of all the noise in the dataset $S_1$. We will now show that the $\\\\mathcal{P}_1$ where the last inequality follows from the bounds, and $\\\\mathcal{P}_2$ where the last equality follows from 131, and 132. Now, for large enough $k$, $d$.\\n\\nTherefore, with $b_j$ from 127, 128, for all $\\\\phi_1(q) = 0$ and $\\\\phi_2(q) = 0$.\\n\\nFrom 127, 128, for all $\\\\phi_1(q) = 0$ and $\\\\phi_2(q) = 0$.\\n\\nTherefore, with $b_j$ from 127, 128, for all $\\\\phi_1(q) = 0$ and $\\\\phi_2(q) = 0$.\\n\\nWe first evaluate the gradient of the empirical loss function with respect to $\\\\eta$.\\n\\nFor inequality 126, we have used the concentration of the maximum of the absolute value of $\\\\mathcal{M}_k \\\\mathcal{P}_k$. The de-noising effect allows us to achieve a stronger constant alignment of each parameter vector with the signal vector.\\n\\nThe \\\"noise\\\" patches from the \\\"signal\\\" patch. This de-noising effect allows us to achieve a stronger constant alignment of each parameter vector with the signal vector.\\n\\nand $(\\\\eta_j, \\\\sigma_j, \\\\rho_j)$.\\n\\nWe can filter out all the noise and let the signal pass through, $\\\\mathcal{M}_k \\\\mathcal{P}_k$. The de-noising effect allows us to achieve a stronger constant alignment of each parameter vector with the signal vector.\\n\\nTherefore, with $b_j$ from 127, 128, for all $\\\\phi_1(q) = 0$ and $\\\\phi_2(q) = 0$.\\n\\nWe can filter out all the noise and let the signal pass through, $\\\\mathcal{M}_k \\\\mathcal{P}_k$. The de-noising effect allows us to achieve a stronger constant alignment of each parameter vector with the signal vector.\\n\\nTherefore, with $b_j$ from 127, 128, for all $\\\\phi_1(q) = 0$ and $\\\\phi_2(q) = 0$.\\n\\nWe can filter out all the noise and let the signal pass through, $\\\\mathcal{M}_k \\\\mathcal{P}_k$. The de-noising effect allows us to achieve a stronger constant alignment of each parameter vector with the signal vector.\\n\\nTherefore, with $b_j$ from 127, 128, for all $\\\\phi_1(q) = 0$ and $\\\\phi_2(q) = 0$.\\n\\nWe can filter out all the noise and let the signal pass through, $\\\\mathcal{M}_k \\\\mathcal{P}_k$. The de-noising effect allows us to achieve a stronger constant alignment of each parameter vector with the signal vector.\\n\\nTherefore, with $b_j$ from 127, 128, for all $\\\\phi_1(q) = 0$ and $\\\\phi_2(q) = 0$.\\n\\nWe can filter out all the noise and let the signal pass through, $\\\\mathcal{M}_k \\\\mathcal{P}_k$. The de-noising effect allows us to achieve a stronger constant alignment of each parameter vector with the signal vector.\\n\\nTherefore, with $b_j$ from 127, 128, for all $\\\\phi_1(q) = 0$ and $\\\\phi_2(q) = 0$.\\n\\nWe can filter out all the noise and let the signal pass through, $\\\\mathcal{M}_k \\\\mathcal{P}_k$. The de-noising effect allows us to achieve a stronger constant alignment of each parameter vector with the signal vector.\\n\\nTherefore, with $b_j$ from 127, 128, for all $\\\\phi_1(q) = 0$ and $\\\\phi_2(q) = 0$.\\n\\nWe can filter out all the noise and let the signal pass through, $\\\\mathcal{M}_k \\\\mathcal{P}_k$. The de-noising effect allows us to achieve a stronger constant alignment of each parameter vector with the signal vector.\\n\\nTherefore, with $b_j$ from 127, 128, for all $\\\\phi_1(q) = 0$ and $\\\\phi_2(q) = 0$.\\n\\nWe can filter out all the noise and let the signal pass through, $\\\\mathcal{M}_k \\\\mathcal{P}_k$. The de-noising effect allows us to achieve a stronger constant alignment of each parameter vector with the signal vector.\\n\\nTherefore, with $b_j$ from 127, 128, for all $\\\\phi_1(q) = 0$ and $\\\\phi_2(q) = 0$.\\n\\nWe can filter out all the noise and let the signal pass through, $\\\\mathcal{M}_k \\\\mathcal{P}_k$. The de-noising effect allows us to achieve a stronger constant alignment of each parameter vector with the signal vector.\\n\\nTherefore, with $b_j$ from 127, 128, for all $\\\\phi_1(q) = 0$ and $\\\\phi_2(q) = 0$.\\n\\nWe can filter out all the noise and let the signal pass through, $\\\\mathcal{M}_k \\\\mathcal{P}_k$. The de-noising effect allows us to achieve a stronger constant alignment of each parameter vector with the signal vector.\\n\\nTherefore, with $b_j$ from 127, 128, for all $\\\\phi_1(q) = 0$ and $\\\\phi_2(q) = 0$.\\n\\nWe can filter out all the noise and let the signal pass through, $\\\\mathcal{M}_k \\\\mathcal{P}_k$. The de-noising effect allows us to achieve a stronger constant alignment of each parameter vector with the signal vector.\"}"}
{"id": "AfnsTnYphT", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Substituting the definition of $\\\\alpha_j$ and simplifying 134,\\n\\\\[\\n\\\\nabla w_{1i}(w_{1i}, b_1; S_m) = -\\\\frac{1}{m} \\\\sum_{j=1}^{2} 2\\\\alpha_j y_j r_{ij} x_i j (136)\\n\\\\]\\n\\\\[\\nd = -\\\\frac{1}{m} \\\\sum_{j=1}^{2} r_{ij} \\\\alpha_j m w_\\\\star + \\\\sigma \\\\epsilon(i)_j, (137)\\n\\\\]\\n\\n\\\\[\\nd = -\\\\frac{1}{m} \\\\sum_{j=1}^{2} r_{ij} \\\\alpha_j m w_\\\\star - 2\\\\sigma \\\\sqrt{P_m} r_{ij} \\\\alpha_j m \\\\hat{\\\\epsilon} i, (138)\\n\\\\]\\n\\nwhere $\\\\hat{\\\\epsilon}_i \\\\sim N(0, I_d)$. By Chernoff and Union bounds,\\n\\\\[\\nm_2 k \\\\leq r_i := P_n j = m_2 + 1 kr_{ij} m \\\\leq 3m_2 k, \\\\text{ with probability } \\\\geq 1 - 10^{-7}. \\\\]\\n\\nAlso, we note the concentration of the norm of the Gaussian random vector to show,\\n\\\\[\\nP[\\\\max_i |\\\\| \\\\hat{\\\\epsilon}_i \\\\| - \\\\sqrt{d} | \\\\geq 10 p \\\\ln(k)] \\\\leq 2 k \\\\exp\\\\left( -\\\\frac{100 \\\\ln(k)}{16} \\\\right) \\\\leq 2k k - 6 \\\\leq 10^{-7}. (141)\\n\\\\]\\n\\nWe are now ready to bound $(w_2i)_T w_\\\\star = (\\\\tilde{w}_2i)_T w_\\\\star \\\\| \\\\tilde{w}_2i \\\\|$. We denote\\n\\\\[\\na_i = k m \\\\sum_{j=1}^{m_2} \\\\alpha_j r_{ij} \\\\geq \\\\frac{1}{3}.\\n\\\\]\\n\\n\\\\[\\n\\\\| \\\\tilde{w}_2i \\\\| = \\\\| w_{1i} + \\\\eta_2 m \\\\sum_{j=1}^{2} r_{ij} \\\\alpha_j m w_\\\\star + \\\\eta_2 2\\\\sigma \\\\sqrt{P_m} r_{ij} \\\\alpha_j m \\\\hat{\\\\epsilon} i \\\\|\\n\\\\]\\n\\\\[\\n\\\\leq 1 + \\\\frac{\\\\| \\\\eta_2 \\\\|}{2} m \\\\sum_{j=1}^{2} r_{ij} \\\\alpha_j m w_\\\\star + \\\\frac{\\\\| \\\\eta_2 \\\\|}{2} \\\\frac{\\\\sigma \\\\epsilon(i)}{2} \\\\sqrt{P_m} r_{ij} \\\\alpha_j m \\\\hat{\\\\epsilon} i\\n\\\\]\\n\\\\[\\n\\\\leq 1 + \\\\frac{a_i \\\\eta_2}{2} m \\\\| w_\\\\star \\\\| + \\\\sqrt{\\\\frac{6}{k} \\\\sigma^2} \\\\frac{\\\\sigma \\\\epsilon(i)}{2} \\\\sqrt{m} \\\\| \\\\hat{\\\\epsilon} i \\\\|\\n\\\\]\\n\\\\[\\n\\\\leq 1 + 10^3 2 a_i + \\\\sqrt{\\\\frac{6}{k} \\\\sigma^2} \\\\frac{\\\\sigma \\\\epsilon(i)}{2} \\\\sqrt{\\\\sigma^2 k (k+d) \\\\ln(k+d)} 3 \\\\sqrt{d^2} \\\\leq 1 + 10^3 2 a_i + 10^{-6}\\n\\\\]\\n\\nfor a large enough $k, d$. And the lower bound on $(\\\\tilde{w}_2i)_T w_\\\\star$ is given by,\\n\\\\[\\n(\\\\tilde{w}_2i)_T w_\\\\star = (w_{1i})_T w_\\\\star + \\\\eta_2 m \\\\sum_{j=1}^{2} r_{ij} \\\\alpha_j m (w_\\\\star)_T w_\\\\star + \\\\eta_2 2\\\\sigma \\\\sqrt{P_m} r_{ij} \\\\alpha_j m \\\\hat{\\\\epsilon} i\\n\\\\]\\n\\n\\\\[\\n\\\\geq -1 + 10^3 2 a_i \\\\times 10^3 - \\\\sqrt{\\\\frac{6}{k} \\\\sigma^2} \\\\frac{\\\\sigma \\\\epsilon(i)}{2} \\\\sqrt{\\\\sigma^2 k (k+d) \\\\ln(k+d)} 3 \\\\sqrt{d^2}\\n\\\\]\\n\\n\\\\[\\n\\\\geq -1 + 10^3 2 a_i - 10^{-6}, (142)\\n\\\\]\\n\\nwhere we have used $P[|\\\\sqrt{\\\\frac{6}{k} \\\\sigma^2} \\\\frac{\\\\sigma \\\\epsilon(i)}{2} \\\\sqrt{\\\\sigma^2 k (k+d) \\\\ln(k+d)} 3 \\\\sqrt{d^2}] \\\\geq 10^{-3}$, with probability $\\\\leq 10^{-7}$. Therefore,\\n\\n\\\\[\\n(\\\\tilde{w}_2i)_T w_\\\\star \\\\| \\\\tilde{w}_2i \\\\| \\\\geq -1 + 10^3 2 a_i \\\\times 10^3 - 10^{-6}\\n\\\\]\\n\\n\\\\[\\n\\\\geq 0.\\n\\\\]\\n\\n3c. LCN has Low Risk\\n\\nWe now show that this large constant alignment guarantees a low risk. We bound the push forward of the noise through the LCN,\\n\\n\\\\[\\n|\\\\sigma_{\\\\max_{j \\\\in [m]}} (w_{2i})_T \\\\epsilon_i | \\\\leq 6 \\\\sqrt{\\\\ln(k)} \\\\sqrt{\\\\frac{\\\\sqrt{k}}{\\\\ln(kd)}} \\\\leq 10^{-6}. (144)\\n\\\\]\\n\\nTo derive the last inequality, we have used the concentration of the maximum of the absolute value of $k$ i.i.d. Gaussian random variables,\\n\\n\\\\[\\nP[\\\\max_{j \\\\in [k]} |(w_{2i})_T \\\\epsilon_j | \\\\geq p 32 \\\\ln(k)] \\\\leq 2 m^9 \\\\leq 10^{-6}.\\n\\\\]\"}"}
{"id": "AfnsTnYphT", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"data sample, let $t \\\\in \\\\{k\\\\}$ be the index of the signal patch, then, $\\\\phi^b_2(y_j(w_i^T x_i^j)) \\\\geq 0$.\\n\\nWe note that with probability $1 - 3 \\\\times 10^{-6}$, the risk of the classifier less than $(1 - 0.959)^2$. To bound the risk in the failure case, we note that for any $v \\\\in W$,\\n\\n$$E[(y - k X_i=1 \\\\phi_b(y w_i^T x_i^j))^2] = E[(1 - k X_i=1 \\\\phi_b(y w_i^T x_i^j))^2], \\\\hspace{1cm} (147)$$\\n\\n$$= \\\\frac{1}{k} \\\\sum_{j=1}^{k} E[(1 - X_i \\\\neq j \\\\phi_b(\\\\sigma \\\\epsilon_{ij}) - \\\\phi_b(\\\\cos(\\\\alpha_j) + \\\\sigma \\\\epsilon_{jj}))^2], \\\\hspace{1cm} (148)$$\\n\\n$$= \\\\frac{1}{k} \\\\sum_{j=1}^{k} \\\\left( \\\\text{Var}[\\\\phi_b(\\\\sigma \\\\epsilon_{ij})] + \\\\text{Var}[\\\\phi_b(\\\\cos(\\\\alpha_j) + \\\\sigma \\\\epsilon_{jj})] \\\\right) + \\\\left( E[\\\\phi_b(\\\\cos(\\\\alpha_j) + \\\\sigma \\\\epsilon_{jj})] \\\\right)^2, \\\\hspace{1cm} (149)$$\\n\\n$$\\\\leq \\\\frac{1}{k} \\\\sigma^2 + \\\\cos^2(\\\\alpha_j) \\\\leq k \\\\sigma^2 + 1 \\\\leq 2. \\\\hspace{1cm} (150)$$\\n\\nSubstituting this back,\\n\\n$$E[(y - k X_i=1 \\\\phi_b(y w_i^T x_i^j))^2] \\\\leq \\\\frac{1}{k} \\\\sum_{j=1}^{k} \\\\text{Var}[\\\\phi_b(\\\\sigma \\\\epsilon_{ij})] + \\\\text{Var}[\\\\phi_b(\\\\cos(\\\\alpha_j) + \\\\sigma \\\\epsilon_{jj})] \\\\leq 2 \\\\hspace{1cm} (151)$$\\n\\nTherefore, the expected risk of the trained LCN is upper bounded as,\\n\\n$$E[R_{\\\\bar{\\\\theta}_n, P}] \\\\leq (1 - 0.959)^2 + 6 \\\\times 10^{-6} \\\\leq \\\\delta \\\\hspace{1cm} (152)$$\\n\\n25\"}"}
{"id": "AfnsTnYphT", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The following lemma provides a lower bound on the risk of each function in the class of LCNs over the set of transformations of SSD, made using the group $U$. The group allows for orthogonal transformations within the patches, and does not allow patches to permute.\\n\\n**Lemma D.1.** Let $F$ denote the class of functions represented by the set of locally connected neural network models, $M[W]$, as defined in 5. We define $U := \\\\{ \\\\text{Block}(U_1, \\\\ldots, U_k) \\\\mid U_i \\\\in O(d) \\\\}$. Let $P$ be the set of distributions $\\\\{ U \\\\circ SSD_1 \\\\mid U \\\\in U \\\\}$. We define the target function $\\\\theta^\\\\ast : P \\\\to F$ as,\\n\\n$$\\\\theta^\\\\ast(U \\\\circ SSD_1) = M[\\\\begin{bmatrix} U(1)^w \\\\ast, \\\\ldots, U(k)^w \\\\ast, b^\\\\ast \\\\end{bmatrix}]$$\\n\\nwhere $w^\\\\ast$ is the signal vector, and $b^\\\\ast$ is some fixed value in $(0, 1)$. Let $F_P$ be the codomain of $\\\\theta^\\\\ast$. Consider $\\\\rho : (F, F_P) \\\\to \\\\mathbb{R}$,\\n\\n$$\\\\rho(f, \\\\theta^\\\\ast(U \\\\circ SSD_1)) = 1 - \\\\max(0, \\\\|w^1\\\\| \\\\cos(\\\\alpha_1))^{\\\\frac{1}{2}}$$\\n\\nThen, the risk of $f \\\\in F$, on $U \\\\circ SSD_1 \\\\in P$ satisfies,\\n\\n$$R(f, U \\\\circ SSD_1) \\\\geq \\\\rho(f, \\\\theta^\\\\ast(U \\\\circ SSD_1))$$\\n\\n**Proof.** Observe that,\\n\\n$$R(f, U \\\\circ P) = E_{x,y \\\\sim U \\\\circ P} \\\\left( y - f(x) \\\\right)^2$$\\n\\n$$= E_{x,y \\\\sim U \\\\circ P} \\\\left( \\\\sum_{i=1}^{k} \\\\phi(b^\\\\ast w^T_i x) \\\\right)^2$$\\n\\n$$= E_{\\\\epsilon, x = Uw^\\\\ast + \\\\sigma \\\\epsilon} \\\\left( \\\\sum_{i=1}^{k} \\\\phi(b^\\\\ast w^T_i x) \\\\right)^2$$\\n\\nJensen's inequality gives,\\n\\n$$\\\\geq E_{\\\\epsilon, x = Uw^\\\\ast + \\\\sigma \\\\epsilon} \\\\left( \\\\sum_{i=1, i \\\\neq 1}^{k} \\\\phi(b^\\\\ast \\\\|w_i\\\\| \\\\sigma \\\\epsilon) \\\\right)^2$$\\n\\n$$= E_{\\\\epsilon, x = Uw^\\\\ast + \\\\sigma \\\\epsilon} \\\\left( \\\\sum_{i=1, i \\\\neq 1}^{k} \\\\phi(b^\\\\ast \\\\|w_i\\\\| \\\\sigma \\\\epsilon) \\\\right)^2$$\\n\\n$$= E_{\\\\epsilon} \\\\left( \\\\sum_{i=1, i \\\\neq 1}^{k} \\\\phi(b^\\\\ast \\\\|w_i\\\\| \\\\sigma \\\\epsilon) \\\\right)^2$$\\n\\n$$= E_{\\\\epsilon} \\\\left( \\\\sum_{i=1, i \\\\neq 1}^{k} \\\\phi(b^\\\\ast \\\\|w_i\\\\| \\\\sigma \\\\epsilon) \\\\right)^2 - E_{\\\\epsilon} \\\\left( \\\\sum_{i=1, i \\\\neq 1}^{k} \\\\phi(b^\\\\ast \\\\|w_i\\\\| \\\\sigma \\\\epsilon) \\\\right)^2$$\\n\\n$$= E_{\\\\epsilon} \\\\left( \\\\sum_{i=1, i \\\\neq 1}^{k} \\\\phi(b^\\\\ast \\\\|w_i\\\\| \\\\sigma \\\\epsilon) \\\\right)^2 - E_{\\\\epsilon} \\\\left( \\\\sum_{i=1, i \\\\neq 1}^{k} \\\\phi(b^\\\\ast \\\\|w_i\\\\| \\\\sigma \\\\epsilon) \\\\right)^2$$\\n\\nwhere in 162, we used the fact that since $\\\\phi(b^\\\\ast - x) = -\\\\phi(b^\\\\ast + x)$, and therefore $E[\\\\phi(b^\\\\ast \\\\|w_i\\\\| \\\\sigma \\\\epsilon)] = 0$, for all $i \\\\neq 1$. For brevity, we define $\\\\bar{\\\\mu} = \\\\|w^1\\\\| \\\\cos(\\\\alpha_1)$, and $\\\\bar{\\\\sigma} = \\\\|w^1\\\\| \\\\sigma$. Then observe that,\\n\\n$$E_{\\\\epsilon} \\\\left( \\\\sum_{i=1, i \\\\neq 1}^{k} \\\\phi(b^\\\\ast \\\\|w_i\\\\| \\\\sigma \\\\epsilon) \\\\right)^2$$\\n\\n$$= E_{\\\\epsilon} \\\\left( \\\\sum_{i=1, i \\\\neq 1}^{k} \\\\phi(b^\\\\ast \\\\|w_i\\\\| \\\\sigma \\\\epsilon) \\\\right)^2$$\\n\\n$$= E_{\\\\epsilon} \\\\left( \\\\sum_{i=1, i \\\\neq 1}^{k} \\\\phi(b^\\\\ast \\\\|w_i\\\\| \\\\sigma \\\\epsilon) \\\\right)^2$$\\n\\n$$= E_{\\\\epsilon} \\\\left( \\\\sum_{i=1, i \\\\neq 1}^{k} \\\\phi(b^\\\\ast \\\\|w_i\\\\| \\\\sigma \\\\epsilon) \\\\right)^2$$\\n\\nThe claim and the proof do not depend on the chosen value of $b^\\\\ast$.\"}"}
{"id": "AfnsTnYphT", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We present the minimax framework by closely following the notation established in Duchi (2021).\\n\\nLet $P$ denote a set of distributions over $(X, Y)$ and $F \\\\subseteq Y^X$ represent a set of functions from $X$ to $Y$. Let $\\\\theta^\\\\star : P \\\\rightarrow F$ be some unknown target mapping, and let $\\\\Theta = \\\\{ \\\\theta | \\\\theta : ((X, Y) \\\\times \\\\Xi) \\\\rightarrow F \\\\}$ be a set of algorithms with a common distribution $\\\\Xi$ over the sample space $\\\\Xi$ that encapsulates randomness.\\n\\nLet $\\\\rho : F \\\\times F \\\\rightarrow \\\\mathbb{R}^+$ be some symmetric positive function.\\n\\n**Definition 7 (Minimax Risk).** Under the notation from above, we define the minimax risk of learning the set of tasks $P$ using the set of algorithms $\\\\Theta$ as,\\n\\n$$M_n(\\\\Theta, P) := \\\\inf_{\\\\theta_n \\\\in \\\\Theta} \\\\sup_{P \\\\in \\\\mathcal{P}} E[\\\\rho(\\\\theta_n, \\\\theta^\\\\star(P))] .$$\\n\\n(11)\\n\\nFor brevity, we may omit $P$ from the notation, when it is clear from context. The primary change in our adaptation of the minimax framework is that we allow for randomized algorithms, whose randomness is independent of the input data distribution. In contrast, the original framework is only applicable to deterministic algorithms, typically referred to as estimators.\\n\\nWe now present our Fano\u2019s Theorem for Randomized Algorithms to lower bound the minimax risk. In this variant, we relax the constraint that $\\\\rho$ is a semi-metric on the space $F$. Specifically, given a set of \u201chard problem\u201d instances $P \\\\mathcal{V}$, and their associated target functions $F \\\\mathcal{V}$, we only require that if a function $f \\\\in F$ is \u201cclose enough\u201d, in $\\\\rho$, to any $g \\\\in F \\\\mathcal{V}$, then it is \u201cfar enough\u201d, in $\\\\rho$, to all $F \\\\mathcal{V} \\\\setminus \\\\{g\\\\}$.\\n\\nThis relaxation helps us prove lower bounds when the stronger semi-metric property does not hold.\\n\\n**Theorem 5.1 (Fano\u2019s Theorem for Randomized Algorithms).** Under the notation established above, let $\\\\mathcal{V}$ be an index set of finite cardinality of some chosen subset of $P$. Then, we define $P \\\\mathcal{V} := \\\\{ P_v | \\\\forall v \\\\in \\\\mathcal{V} \\\\}$, and $F \\\\mathcal{V} := \\\\{ \\\\theta^\\\\star(P_v) | \\\\forall v \\\\in \\\\mathcal{V} \\\\}$. For some fixed parameter $\\\\delta > 0$, let $\\\\rho$ satisfy the condition that, for all $f_u \\\\neq f_v \\\\in F \\\\mathcal{V}$ and $f \\\\in F$, if $\\\\rho(f, f_u) < \\\\delta$, then $\\\\rho(f, f_v) > \\\\delta$. And, for all $P_u, P_v \\\\in P \\\\mathcal{V}$, $u \\\\neq v$, let the KL divergence satisfy $\\\\text{KL}(P_u \\\\parallel P_v) \\\\leq D$ for some $D > 0$. Then,\\n\\n$$M_n(\\\\Theta) \\\\geq \\\\delta \\\\frac{1}{nD} + \\\\ln(2/\\\\ln(|\\\\mathcal{V}|)).$$\\n\\nThe proof of this theorem is presented in appendix B.\\n\\n**Remark 1.** We only need to define $\\\\rho$ on the subset $F \\\\times F \\\\mathcal{V}$ of its domain $F \\\\times F$ and $\\\\theta^\\\\star$ on the subset $P \\\\mathcal{V}$ of its domain $P$ to apply the above theorem.\\n\\n6 FCN S VS LCN S RESULTS\\n\\nWe now present the separation result between FCNs and LCNs, along with an outline of the proof. Specifically, we establish that FCNs, when trained with any equivariant algorithm, require $\\\\Omega(\\\\sigma^2 k^2 d)$ samples to learn DSD up to some constant risk $\\\\delta$. Conversely, there exists an equivariant algorithm that can train LCNs with $\\\\tilde{O}(\\\\sigma^2 k^2 (k + d))$ samples, to achieve a risk less than $\\\\delta$.\\n\\n**Theorem 6.1 (Sketched).** Consider the group $U = O(kd)$, then any $U$-equivariant algorithm that is used to train FCNs, requires $\\\\Omega(\\\\sigma^2 k^2 d)$ samples to achieve some constant risk $\\\\delta$.\\n\\n**Proof.** We justify the choice of $U = O(kd)$, in light of the intuition for equivariance presented in section 5.2. Note that the parameter of the first layer of FCNs, $A \\\\in \\\\mathbb{R}^{k \\\\times kd}$, is given by $(w_1; \\\\ldots; w_k)$. We establish equivariance if, for every transformation $U \\\\in U$, $AU^T$ corresponds to a valid FCN, and if there exists an initialization such that $AU^T$ and $A$ are identically distributed. Indeed, $AU^T = (Uw_1; \\\\ldots; Uw_k)$, corresponds to a FCN with the parameter vectors $Uw_1, \\\\ldots, Uw_k$. And, if each $w_i$ is initialized as $w_i \\\\sim N(0, I_{kd})$, then $AU^T$ and $A$ share the same distribution.\\n\\nOur proof proceeds in two steps. First, we establish that learning $U \\\\circ DSD$ with $m$ samples requires learning $k$ \u201cnearly independent\u201d subtasks, $\\\\{U \\\\circ SSD_t\\\\}_{t=1}^k$, with $m/k$ samples each. The underlying rationale of this result is that learning $U \\\\circ DSD$ entails recovering each mean vector $\\\\{U \\\\mu_t\\\\}_{t=1}^k$. Note that these mean vectors are pair-wise orthogonal, $(U \\\\mu_i)^T(U \\\\mu_j) = \\\\mu_i^T \\\\mu_j = 0$. Therefore, even with the knowledge of $\\\\{U \\\\mu_t\\\\}_{t \\\\neq i}$, the only information we have about $U \\\\mu_i$ is the $kd - k + 1 \\\\approx kd$ dimensional subspace in which it lies. Thus, to learn DSD, we have to recover all the means vectors, $\\\\{U \\\\mu_t\\\\}_{t=1}^k$, \u201cnearly independently\u201d from each other.\"}"}
{"id": "AfnsTnYphT", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the second step, we reduce the problem of learning SSD $t$, into a problem of Gaussian mean estimation. For this, we show that if there exists an algorithm that learns SSD $t$, then we can extract a weakly aligned mean estimate of $U \\\\mu t$ from the FCN returned by the algorithm. We propose a scheme that reliably boosts this estimate, to generate a strongly aligned mean estimate. This is necessary because standard information theoretic tools do not work with weakly aligned mean estimates. We then bound the sample complexity for any algorithm that is able to return a strongly aligned Gaussian mean estimate using our Fano's Theorem for Randomized Estimators 5.1 as $m/k = \\\\Omega(\\\\sigma^2 kd)$. This implies that $m = \\\\Omega(\\\\sigma^2 k^2 d)$, proving the result.\\n\\nThe formal statement of the theorem and its proof can be found in appendix C.2.\\n\\nTheorem 6.2. (Sketched) Consider the groups $U_1 := \\\\{\\\\text{Block}(\\\\{U_1, \\\\ldots, U_k\\\\}) | U_i \\\\in O(d)\\\\}$, and $U_2 := \\\\{U \\\\in O_p(kd) | \\\\text{idx}_{kd}(U_{e(i-1)d+1} + j - 1) = \\\\text{idx}_{kd}(U_{e(i-1)d+j})\\\\}, \\\\forall i \\\\in [k], j \\\\in [d]$. Let $U = U_1 \\\\ast U_2$. Then there exists a $U$-equivariant algorithm that trains LCNs with $\\\\tilde{O}(\\\\sigma^2 k(k+d))$ samples, to achieve a risk less than $\\\\delta$.\\n\\nProof. To justify our choice of $U$ for LCNs, we establish equivariance under $U_1$ and $U_2$ separately. The equivariance under $U_1$ simply follows from an induction on the number of finite combinations of elements of $U_1 \\\\cup U_2$.\\n\\nEquivariance under $U_1$. Consider an input $x \\\\in \\\\mathbb{R}^{kd}$, then any transformation $U \\\\in U_1$ operates on $x$ on a per-patch basis. On each patch, $U$ induces, a possibly distinct, orthogonal transformation. We now show equivariance under the notation from section 5.2. The linear layer parameter $A \\\\in \\\\mathbb{R}^{k \\\\times kd}$ is given by $\\\\text{Block}(w_1, \\\\ldots, w_k)$. Observe that, $AU^T = \\\\text{Block}(U^{(1)}w_1, \\\\ldots, U^{(k)}w_k)$, which corresponds to a LCN with parameter vectors $\\\\{U^{(1)}w_1, \\\\ldots, U^{(k)}w_k\\\\}$. And if each $w_i$ is sampled as $w_i \\\\sim N(0, I_d)$, then $AU^T$ and $A$ share the same distribution.\\n\\nEquivariance under $U_2$. A transformation $U \\\\in U_2$ permutes the $k$ input patches amongst each other, while retaining each internal structure of each patch. Let $\\\\pi : [k] \\\\rightarrow [k]$ be the permutation function corresponding to $U$. Then observe that $AU^T = \\\\text{Block}(w_{\\\\pi(1)}, \\\\ldots, w_{\\\\pi(k)})$, which corresponds to a LCN with parameter vectors $\\\\{w_{\\\\pi(1)}, \\\\ldots, w_{\\\\pi(k)}\\\\}$. And, if $w_i \\\\sim N(0, I_d)$, then $AU^T$ and $A$ share the same distribution.\\n\\nOur training uses gradient descent, accompanied by a projection on the unit ball after every descent step. We included this projection to simplify the analysis, though we note that it can be removed without changing the core proof structure. The training proceeds in two steps. We show that after the first update, each parameter vector achieve an alignment of $(w \\\\ast w)^T w_i = \\\\Omega(p(k+d)/kd)$. In the second step, we use this alignment to reliably filter out the noise patches, while retaining the signal patches. This denoising enables us to prove a stronger $\\\\Omega(1)$ alignment, which implies that the model has successfully recovered signal vector. Consequently, the model has a small risk $\\\\leq \\\\delta$.\\n\\nDetailed theorem statements and proofs are available in appendix C.2.\"}"}
{"id": "AfnsTnYphT", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nWe follow in the footsteps of the proof of theorem 6.1. First, we establish that learning $U \\\\circ DSD$ with $m$ samples requires learning $k$ independent subtasks, $\\\\{U \\\\circ SSD_t\\\\}^k_{t=1}$, with $m/k$ samples each. The distinction from the proof of theorem 6.1, is that the subtasks are fully independent. This is because, the group $U$ does not permit interaction amongst the $k$ patches. In other words, the vectors $\\\\{U(1)\\\\mu_1, \\\\ldots, U(k)\\\\mu_k\\\\}$ are all $d$-sparse, and occupy non-overlapping subspaces. Therefore, even if we have the knowledge of $\\\\{U(t)\\\\mu_t\\\\}^{t \\\\neq i}_{t=1}$, we would still have no information about $U(i)\\\\mu_i$. Thus, we have to recover all the $d$-sparse mean vectors independently of each other.\\n\\nIn the second step, we prove an information-theoretic lower bound to learn $U \\\\circ SSD_t$ with $m/k$ samples. We find a function that lower bounds the risk incurred by a LCN on $SSD_t$. This function satisfies the weakened conditions of theorem 5.1. Finally, we use theorem 5.1 together with the Gilbert-Varshamov lemma A.1.1, to show that $m/k = \\\\Omega(\\\\sigma^2 d)$. And implies that $m = \\\\Omega(\\\\sigma^2 kd)$.\\n\\nThe complete statement of the theorem with its proof can be found in appendix D.1 Theorem 7.2.\\n\\n(Sketched) Define $U_1 := \\\\{Block(\\\\{U_1, \\\\ldots, U_k\\\\}) | U_i = U_j, U_i \\\\in O(d)\\\\}$, and $U_2 := \\\\{U \\\\in O_{kp}(kd) | \\\\text{idx}_{kd}(U e(i-1)d+1) + j-1 = \\\\text{idx}_{kd}(U e(i-1)d+j), \\\\forall i \\\\in [k], j \\\\in [d]\\\\}$. Let $U = U_1 \\\\star U_2$. Then there exists a $U$-equivariant algorithm that trains CNNs, as defined in 6, with $\\\\tilde{O}(\\\\sigma^2 (k+d))$ samples, to achieve a risk of less than $\\\\delta$.\\n\\nProof. To justify our choice of $U$ for CNNs, we establish equivariance under $U_1$ and $U_2$ separately. The equivariance under $U_1$ follows from induction on the number of finite combinations in $U_1 \\\\star U_2$.\\n\\nEquivariance under $U_1$ Consider an input $x \\\\in \\\\mathbb{R}^{kd}$, then any transformation $U \\\\in U_1$ induces the same orthogonal transformation on every patch of $x$. Moreover, it does not allow for any inter-patch interaction. To prove equivariance, observe that the parameter $A \\\\in \\\\mathbb{R}^{k \\\\times kd}$ is given by $\\\\text{Block}(w, \\\\ldots, k \\\\times w)$.\\n\\nNote that, $AU^T = \\\\text{Block}(U(1)w, \\\\ldots, U(k)w) = \\\\text{Block}(U(1)w, \\\\ldots, U(1)w)$, which corresponds to a CNN with parameter vectors $\\\\{U(1)w, \\\\ldots, U(1)w\\\\}$. And if the parameter vector, $w$, is initialized as $w \\\\sim \\\\mathcal{N}(0, I_d)$, then $AU^T$ and $A$ share the same distribution.\\n\\nEquivariance under $U_2$ A transformation $U \\\\in U_2$ permutes the $k$ input patches, while retaining the internal structure of each patch. Equivariance follows directly from the argument in the proof of theorem 6.2. Our approach exactly follows the proof theorem 6.2. We train the CNN using gradient descent, followed by a projection on the unit ball. The training algorithm has two iterations. We show an alignment of $\\\\Omega(p(kd)/kd)$ after the first update, and a stronger alignment of $\\\\Omega(1)$ via denoising after the second update. This implies that the model has successfully recovered signal vector, and consequently it has a small risk $\\\\leq \\\\delta$.\\n\\nDetailed theorem statements and proofs are available in appendix D.2.\\n\\n8 CONCLUSION AND FUTURE WORK\\n\\nIn this paper, we established a sample complexity separation between FCNs, LCNs, and CNNs that are trained using equivariant algorithms on the Dynamic Signal Distribution (DSD) task. Unlike previous works, this task encodes the concepts of signal, noise, locality, and translation invariance, thus incorporating the salient characteristics of vision-based tasks. We quantify the benefits of locality and weight sharing on the DSD task. Specifically, we show that FCNs incur an extra multiplicative cost of $k^2$ because they lacks both architectural biases, LCNs incur a $k$ cost because of the absence of weight sharing, whereas CNNs avoid these costs because it exhibits both locality and weight sharing.\\n\\nIn future work, we plan to incorporate second-order characteristics of images into the data model. For instance, allowing multiple signals to appear across different patches simultaneously would mirror real-world scenarios where multiple objects occur. Additionally, an interesting direction would be to analyze the role of depth in a CNN in capturing dependency between different patches.\"}"}
{"id": "AfnsTnYphT", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gyora M. Benedek and Alon Itai. Learnability with respect to fixed distributions. Theor. Comput. Sci., 86:377\u2013390, 1991. URL https://api.semanticscholar.org/CorpusID:33054388.\\n\\nSimon S Du, Yining Wang, Xiyu Zhai, Sivaraman Balakrishnan, Russ R Salakhutdinov, and Aarti Singh. How many samples are needed to estimate a convolutional neural network? In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/03c6b06952c750899bb03d998e631860-Paper.pdf.\\n\\nJohn Duchi. Lecture notes for statistics 311/electrical engineering 377, 2021. URL https://web.stanford.edu/class/stats311/lecture-notes.pdf. Stanford University.\\n\\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale, 2022.\\n\\nRobert Gens and Pedro M. Domingos. Deep symmetry networks. In NIPS, 2014. URL https://api.semanticscholar.org/CorpusID:267009.\\n\\nT. Hastie, R. Tibshirani, and J.H. Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer series in statistics. Springer, 2009. ISBN 9780387848846. URL https://books.google.com/books?id=eBSgoAEACAAJ.\\n\\nStefani Karp, Ezra Winston, Yuanzhi Li, and Aarti Singh. Local signal adaptivity: Provable feature learning in neural networks beyond kernels. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=oAjn5-AgSd.\\n\\nZhiyuan Li, Yi Zhang, and Sanjeev Arora. Why are convolutional nets more sample-efficient than fully-connected nets? In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uCY5MuAxcxU.\\n\\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. CoRR, abs/2201.03545, 2022. URL https://arxiv.org/abs/2201.03545.\\n\\nPhilip M. Long and Hanie Sedghi. Generalization bounds for deep convolutional neural networks, 2020.\\n\\nEran Malach and Shai Shalev-Shwartz. Computational separation between convolutional and fully-connected networks, 2020.\\n\\nGary Marcus. Deep learning: A critical appraisal. CoRR, abs/1801.00631, 2018. URL http://arxiv.org/abs/1801.00631.\\n\\nPascal Massart, Jean Picard, and \u00c9cole d'\u00e9t\u00e9 de probabilit\u00e9s de Saint-Flour. Concentration inequalities and model selection. 2007. URL https://api.semanticscholar.org/CorpusID:119022238.\\n\\nGal Vardi, Ohad Shamir, and Nathan Srebro. The sample complexity of one-hidden-layer neural networks, 2022.\\n\\nHaoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logit matching, 2022.\\n\\nZihao Wang and Lei Wu. Theoretical analysis of inductive biases in deep convolutional networks, 2023.\\n\\nWeiwei Zhang, Jian Sun, and Xiaoou Tang. Cat head detection - how to effectively exploit shape and texture features. In European Conference on Computer Vision, 2008. URL https://api.semanticscholar.org/CorpusID:2441648.\"}"}
{"id": "AfnsTnYphT", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We begin by analyzing the push forward of all noise patches in $S$. We will follow in the footsteps of the update step with probability $k$, $d$, where the last inequality holds for a large enough $k, d$.\\n\\nTo derive inequality 262, we have used the concentration of the maximum of the absolute value of $40$.\\n\\nTherefore, from 263, 264, and the gradient of the empirical loss function with respect to $w$, we filter out the noise and let the signal pass through.\\n\\nIn this step, we will now show that the alignment achieved in the first step, enables denoising will enables us to achieve a stronger a. From the analysis of the first update step that, i.i.d. Gaussian random variables, $P$ denoising will enables us to achieve a stronger alignment with the signal vector.\\n\\nNext, we lower bound $\\\\parallel w \\\\parallel = 1 \u2212 \\\\eta$, and similarly we upper bound $\\\\parallel w \\\\parallel T \\\\geq \\\\parallel w \\\\parallel T \u2212 \\\\parallel w \\\\parallel T$.\\n\\nRecall $\\\\parallel w \\\\parallel T = 1/100$, where $\\\\parallel w \\\\parallel T$ is the maximum of the absolute value of $\\\\parallel w \\\\parallel$. And similarly we upper bound $\\\\parallel w \\\\parallel - \\\\parallel w \\\\parallel T \u2265 \\\\parallel w \\\\parallel T - \\\\parallel w \\\\parallel T$.\\n\\nFrom 265, we have $\\\\parallel w \\\\parallel T \u2265 \\\\parallel w \\\\parallel T$. Therefore, $\\\\parallel w \\\\parallel T \u2265 \\\\parallel w \\\\parallel T$.\\n\\nFrom 266, we have $\\\\parallel w \\\\parallel T \u2265 \\\\parallel w \\\\parallel T$. Therefore, $\\\\parallel w \\\\parallel T \u2265 \\\\parallel w \\\\parallel T$.\\n\\nFinally, we have $\\\\parallel w \\\\parallel T \u2265 \\\\parallel w \\\\parallel T$.\\n\\nPublished as a conference paper at ICLR 2024.\"}"}
{"id": "AfnsTnYphT", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ \\\\nabla w_{1} \\\\ell (w_{1}, b_{1}; S_{m\\\\ 2}) = -m \\\\sum_{j=1}^{2} y_{j} - k \\\\sum_{i=1}^{m} \\\\phi_{b_{1}}(w_{1}^{T} x_{i,j}) \\\\]\\n\\nwhere 268 follows from 265, 266. Define \\\\[ \\\\alpha_{j} = 1 - \\\\sum_{k=1}^{d} \\\\phi_{b_{1}}(y_{j}(w_{1}^{T} x_{i,j})) \\\\], for \\\\( j \\\\in [n] \\\\setminus [m] \\\\).\\n\\nThen,\\n\\n\\\\[ 1 \\\\geq 1 - \\\\frac{4}{100} q k d (k + d) \\\\ln(kd) \\\\geq \\\\alpha_{j} \\\\geq 1 - \\\\frac{40}{q} k d (k + d) \\\\ln(kd). \\\\]\\n\\nWe also define \\\\[ x_{(t)}(j) \\\\] to be the patch of the \\\\( j \\\\)-th data sample that corresponds to the occurrence of the signal, that is \\\\( r_{tj} = 1 \\\\). From 268,\\n\\n\\\\[ \\\\nabla w_{1} \\\\ell (w_{1}, b_{1}; S_{m\\\\ 2}) = -m \\\\sum_{j=1}^{2} \\\\alpha_{j} y_{j} x_{(t)}(j) - k \\\\sum_{j=1}^{m} w_{\\\\star} + \\\\sigma \\\\epsilon_{(t)}(j), \\\\]\\n\\n\\\\[ = -m \\\\sum_{j=1}^{2} \\\\alpha_{j} m w_{\\\\star} - 2 \\\\sigma \\\\sqrt{\\\\frac{1}{m} \\\\sum_{j=1}^{2} \\\\alpha_{j}^{2}} \\\\hat{\\\\epsilon}, \\\\]\\n\\nwhere \\\\( \\\\hat{\\\\epsilon} \\\\sim N(0, I_d) \\\\). We define \\\\[ a := \\\\frac{1}{m} \\\\sum_{j=1}^{2} \\\\alpha_{j} m w_{\\\\star}. \\\\] And, observe that from the concentration of the norm of a Gaussian random variable\\n\\n\\\\[ \\\\Pr[\\\\|\\\\hat{\\\\epsilon}\\\\| \\\\geq 6 \\\\sqrt{\\\\frac{d}{kd}}] \\\\leq 2 \\\\exp(-36 d^{2} d), \\\\]\\n\\n\\\\[ \\\\leq 10^{-6}. \\\\]\\n\\nWith these results, we are now ready to bound \\\\( (w_{2})^{T} w_{\\\\star} = (\\\\hat{w}_{2})^{T} w_{\\\\star} \\\\left\\\\| \\\\hat{w}_{2} \\\\right\\\\| \\\\), \\\\( \\\\left\\\\| \\\\hat{w}_{2} \\\\right\\\\| = \\\\left\\\\| w_{1} + \\\\eta_{2} m \\\\sum_{j=1}^{2} \\\\alpha_{j} m w_{\\\\star} + \\\\eta_{2} \\\\sigma \\\\sqrt{\\\\frac{1}{m} \\\\sum_{j=1}^{2} \\\\alpha_{j}^{2}} \\\\hat{\\\\epsilon} \\\\right\\\\| \\\\),\\n\\n\\\\[ \\\\leq 1 + a \\\\eta_{2} \\\\left\\\\| w_{\\\\star} \\\\right\\\\| + 2 \\\\sigma \\\\eta_{2} \\\\sqrt{\\\\sigma} \\\\sqrt{d} \\\\sqrt{\\\\frac{1}{m} \\\\sum_{j=1}^{2} \\\\alpha_{j}^{2}} \\\\hat{\\\\epsilon}, \\\\]\\n\\n\\\\[ \\\\leq 1 + a \\\\eta_{2} + 12 \\\\sigma \\\\eta_{2} \\\\sqrt{d} \\\\sqrt{\\\\sigma} \\\\left( k + d \\\\right) \\\\ln(kd), \\\\]\\n\\n\\\\[ \\\\leq 1 + \\\\left( a + 10^{-3} \\\\right) \\\\eta_{2}, \\\\]\\n\\nfor a large enough \\\\( k, d \\\\).\\n\\nAnd now we lower bound \\\\( (\\\\hat{w}_{2})^{T} w_{\\\\star} \\\\),\\n\\n\\\\[ (\\\\hat{w}_{2})^{T} w_{\\\\star} = (w_{1})^{T} w_{\\\\star} + \\\\eta_{2} m \\\\sum_{j=1}^{2} \\\\alpha_{j} m w_{\\\\star} + \\\\eta_{2} \\\\sigma \\\\sqrt{\\\\frac{1}{m} \\\\sum_{j=1}^{2} \\\\alpha_{j}^{2}} \\\\hat{\\\\epsilon}, \\\\]\\n\\n\\\\[ \\\\leq -1 + \\\\left( a - 2 \\\\right) \\\\eta_{2} \\\\sigma \\\\epsilon \\\\sqrt{m} \\\\sigma \\\\left( k + d \\\\right) \\\\ln(kd), \\\\]\\n\\n\\\\[ \\\\leq -1 + \\\\left( a - 10^{-3} \\\\right), \\\\]\\n\\n\\\\[ \\\\geq 0.96, \\\\]\\n\\n\\\\[ \\\\geq 1 + \\\\left( 2 - 3 \\\\right) \\\\eta_{2}, \\\\]\\n\\nas \\\\( 1 \\\\geq a \\\\geq 3 \\\\) for large \\\\( k, d \\\\), and this occurs with a probability \\\\[ \\\\geq 1 - 2 \\\\times 10^{-5} \\\\].\\n\\n3c. CNN has Low Risk\\n\\nWe now show that this large constant alignment guarantees a low risk. We bound the push forward of the noise through the CNN,\\n\\n\\\\[ |\\\\sigma_{\\\\max} j \\\\in [m] ((w_{2})^{T} \\\\epsilon_{i})| \\\\leq 6 \\\\sqrt{\\\\ln(kd) 100 \\\\sqrt{kd}} \\\\leq 10^{-4}, \\\\]\\n\\n\\\\[ (277) \\\\]\"}"}
{"id": "AfnsTnYphT", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To derive the last inequality, we have used the concentration of the maximum of the absolute value of \\\\( k \\\\) i.i.d. Gaussian random variables,\\n\\\\[\\nP[\\\\max_{j \\\\in [k]} |(w^2)\\\\mathbf{\\\\epsilon}_j| \\\\geq p32 \\\\ln(k)] \\\\leq 2\\\\exp^{-9/3}.\\n\\\\]\\nFor this data sample, let \\\\( t \\\\in [k] \\\\) be the index of the signal patch, then,\\n\\\\[\\n\\\\phi_b^2(\\\\mathbf{y}_j(w^2_i)^T\\\\mathbf{x}(i)_{\\\\mathbf{j}}) \\\\geq 0.959, (278)\\n\\\\]\\n\\\\[\\n\\\\phi_b^2(\\\\mathbf{y}_j(w^2_i)^T\\\\mathbf{x}(i)_{\\\\mathbf{j}}) = 0, \\\\forall i \\\\neq t. (279)\\n\\\\]\\nTo bound the risk in the failure case, we note that for any \\\\( v \\\\in W \\\\),\\n\\\\[\\n\\\\mathbb{E}[(\\\\mathbf{y} - k\\\\sum_{i=1}^{X} \\\\phi_b(w^T\\\\mathbf{x}(i)_{j}))^2] = \\\\mathbb{E}[(1 - k\\\\sum_{i=1}^{X} \\\\phi_b(w^T\\\\mathbf{x}(i)_{j}))^2], (280)\\n\\\\]\\n\\\\[\\n= \\\\frac{1}{k} \\\\sum_{j=1}^{X} \\\\mathbb{E}[(1 - \\\\phi_b(\\\\sigma\\\\mathbf{\\\\epsilon}_{ij}) - \\\\phi_b(\\\\cos(\\\\alpha_j) + \\\\sigma\\\\mathbf{\\\\epsilon}_{jj}))^2], (281)\\n\\\\]\\n\\\\[\\n\\\\leq \\\\sum_{i \\\\neq j} \\\\mathbb{E}[(\\\\phi_b(\\\\sigma\\\\mathbf{\\\\epsilon}_{ij}) - \\\\phi_b(\\\\cos(\\\\alpha_j) + \\\\sigma\\\\mathbf{\\\\epsilon}_{jj}))^2] + (\\\\mathbb{E}[\\\\phi_b(\\\\cos(\\\\alpha_j) + \\\\sigma\\\\mathbf{\\\\epsilon}_{jj})]^2), (282)\\n\\\\]\\n\\\\[\\n\\\\leq \\\\sum_{i \\\\neq j} \\\\sigma^2 + \\\\cos^2(\\\\alpha_j) \\\\leq k\\\\sigma^2 + 1 \\\\leq 2. (285)\\n\\\\]\\nSubstituting this back,\\n\\\\[\\n\\\\mathbb{E}[(\\\\mathbf{y} - k\\\\sum_{i=1}^{X} \\\\phi_b(w^T\\\\mathbf{x}(i)_{j}))^2] \\\\leq \\\\frac{1}{k} \\\\sum_{j=1}^{X} \\\\mathbb{E}[(1 - \\\\phi_b(\\\\sigma\\\\mathbf{\\\\epsilon}_{ij}) - \\\\phi_b(\\\\cos(\\\\alpha_j) + \\\\sigma\\\\mathbf{\\\\epsilon}_{jj}))^2] \\\\leq 2. (286)\\n\\\\]\\nFinally, the risk of the classifier is,\\n\\\\[\\n\\\\mathbb{E}[\\bar{R}_{\\\\theta_n, P}] \\\\leq (1 - 0.959)^2 + 4 \\\\times 10^{-5} \\\\leq \\\\delta. (287)\\n\\\\]\"}"}
{"id": "AfnsTnYphT", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we validate our theoretical bounds with empirical results. We begin by presenting the test-error experiments, where we evaluate the test error of the three models across various training sample sizes. The results for these experiments show an order-of-magnitude decrease in the sample efficiency when comparing CNNs to LCNs, and comparing LCNs to FCNs.\\n\\nWe then present our sample complexity experiments, wherein we explicitly calculate the sample complexity of CNNs and LCNs for various \\\\((k, d)\\\\) pairs. However, these experiments are significantly more compute-intensive than the test error experiments. While the computational demands are manageable for CNNs, they increase significantly for LCNs and become prohibitively large for FCNs. This is primarily because FCNs require at least \\\\(10^{-20}\\\\) times more samples than LCNs. Nonetheless, for both CNNs and LCNs, we successfully verify that the empirical sample complexity satisfies the respective theoretical bounds. Specifically, for CNNs, we show a \\\\(O(k)\\\\) sample complexity growth with a fixed \\\\(d\\\\) and a \\\\(O(d)\\\\) growth with a fixed \\\\(k\\\\). For LCNs, we establish that the sample complexity grows as \\\\(O(k^2)\\\\), \\\\(\\\\Omega(k)\\\\) with a fixed \\\\(d\\\\) and as \\\\(\\\\Theta(d)\\\\) with a fixed \\\\(k\\\\).\\n\\nIn this experiment, we evaluate the test error of each of the three models when trained with a sample size of \\\\{10, 50, 100, 250, 500\\\\} for every \\\\((k, d)\\\\) pair with \\\\(k, d \\\\in \\\\{10, 20\\\\}\\\\). For each training session, we conduct a grid search over the learning rates for patch parameters being \\\\{10^{-1}, 10^{-2}, 10^{-3}\\\\}, and for the biases being \\\\{10^{-2}, 10^{-3}, 10^{-4}\\\\}. We choose the model with the lowest test error. The experiment is replicated 5 times, and we report the mean and standard deviation of the test errors.\\n\\nAcross all \\\\((k, d)\\\\) pairs we observe that LCNs require an order-of-magnitude (\\\\(10^{-20}\\\\) times) more samples than CNNs to achieve comparable test errors. This demonstrates the larger sample efficiency of CNNs over LCNs. Extrapolating the trend line for FCNs, it is evident that they would need even orders-of-magnitude more samples than LCNs for comparable error levels. These observations are consistent with our theoretical predictions of sample complexities: \\\\(\\\\Omega(k^2d)\\\\) for FCNs, \\\\(O(k(k+d))\\\\) and \\\\(\\\\Omega(kd)\\\\) for LCNs, and \\\\(O(k+d)\\\\) for CNNs.\"}"}
{"id": "AfnsTnYphT", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ \\\\eta_1 = \\\\bar{\\\\sigma} q_1^2 \\\\pi \\\\exp \\\\left( -\\\\left( \\\\bar{\\\\mu} - b \\\\right)^2 / 2\\\\bar{\\\\sigma}^2 \\\\right). \\\\]\\n\\nSimilarly, for the second term, we have,\\n\\n\\\\[ E(\\\\epsilon) \\\\left[ \\\\max(0, -\\\\bar{\\\\mu} - b + \\\\bar{\\\\sigma}\\\\epsilon) \\\\right] = (\\\\bar{\\\\mu} - b) \\\\left( 1 - \\\\Phi - \\\\bar{\\\\mu} - b \\\\bar{\\\\sigma} \\\\right) + \\\\eta_2, \\\\]\\n\\nSubstituting these results back to 164,\\n\\n\\\\[ E(\\\\epsilon) \\\\left( \\\\phi b (\\\\bar{\\\\mu} + \\\\bar{\\\\sigma}\\\\epsilon) \\\\right) = (\\\\bar{\\\\mu} - b) \\\\left( 1 - \\\\Phi - \\\\bar{\\\\mu} - b \\\\bar{\\\\sigma} \\\\right) + \\\\eta_1 + (\\\\bar{\\\\mu} + b) \\\\left( 1 - \\\\Phi - \\\\bar{\\\\mu} + b \\\\bar{\\\\sigma} \\\\right) - \\\\eta_2. \\\\]\\n\\nNow observe that,\\n\\n\\\\[ \\\\frac{\\\\partial}{\\\\partial b} E(\\\\epsilon) \\\\left( \\\\phi b (\\\\bar{\\\\mu} + \\\\bar{\\\\sigma}\\\\epsilon) \\\\right) = -\\\\left( 1 - \\\\Phi - \\\\bar{\\\\mu} - b \\\\bar{\\\\sigma} \\\\right) + \\\\left( \\\\Phi - \\\\bar{\\\\mu} - b \\\\bar{\\\\sigma} \\\\right) + (\\\\bar{\\\\mu} - b) \\\\left( 1 - \\\\Phi - \\\\bar{\\\\mu} - b \\\\bar{\\\\sigma} \\\\right) + (\\\\bar{\\\\mu} + b) \\\\left( 1 - \\\\Phi - \\\\bar{\\\\mu} + b \\\\bar{\\\\sigma} \\\\right) - \\\\eta_2, \\\\]\\n\\nSubstituting the expression for \\\\( \\\\Phi \\\\),\\n\\n\\\\[ \\\\frac{\\\\partial}{\\\\partial b} E(\\\\epsilon) \\\\left( \\\\phi b (\\\\bar{\\\\mu} + \\\\bar{\\\\sigma}\\\\epsilon) \\\\right) = 1 - \\\\Phi - \\\\bar{\\\\mu} - b \\\\bar{\\\\sigma} - \\\\left( 1 - \\\\Phi - \\\\bar{\\\\mu} - b \\\\bar{\\\\sigma} \\\\right) + (\\\\bar{\\\\mu} - b) \\\\left( 1 - \\\\Phi - \\\\bar{\\\\mu} - b \\\\bar{\\\\sigma} \\\\right) + (\\\\bar{\\\\mu} + b) \\\\left( 1 - \\\\Phi - \\\\bar{\\\\mu} + b \\\\bar{\\\\sigma} \\\\right) - \\\\eta_2. \\\\]\\n\\nIf \\\\( \\\\bar{\\\\mu} > 0 \\\\), then the gradient with respect to \\\\( b \\\\) is always negative when \\\\( b > 0 \\\\), therefore the maxima of \\\\( E(\\\\epsilon) \\\\left( \\\\phi b (\\\\bar{\\\\mu} + \\\\bar{\\\\sigma}\\\\epsilon) \\\\right) \\\\) occurs at \\\\( b = 0 \\\\), with the maxima being \\\\( \\\\bar{\\\\mu} \\\\leq 1 \\\\). If \\\\( \\\\bar{\\\\mu} < 0 \\\\), then the gradient is always positive when \\\\( b > 0 \\\\), therefore the maxima of \\\\( E(\\\\epsilon) \\\\left( \\\\phi b (\\\\bar{\\\\mu} + \\\\bar{\\\\sigma}\\\\epsilon) \\\\right) \\\\) occurs at \\\\( b = +\\\\infty \\\\), with the maxima being \\\\( 0 \\\\leq 1 \\\\). And finally, for \\\\( \\\\bar{\\\\mu} = 0 \\\\),\\n\\n\\\\[ E(\\\\epsilon) \\\\left( \\\\phi b (\\\\bar{\\\\mu} + \\\\bar{\\\\sigma}\\\\epsilon) \\\\right) = 0 < 1 \\\\], by symmetry. Using these observations with 162 proves the result,\\n\\n\\\\[ R(f, U \\\\circ P) \\\\geq (1 - \\\\max(\\\\|w_1\\\\| \\\\cos(\\\\alpha_1), 0))^2. \\\\]\\n\\nThe next lemma establishes that the lower bound on the risk, as defined in Lemma D.1, meets the relaxed conditions of our variant of Fano's Theorem 5.1.\\n\\nLemma D.2. Under the notation established in the statement of Lemma D.1, we define the set \\\\( \\\\tilde{U} \\\\subseteq U \\\\), such that for all \\\\( U \\\\neq V \\\\in \\\\tilde{U} \\\\),\\n\\n\\\\[ (U(1)w^*)^T (V(1)w^*) < 10^{-3}, \\\\]\\n\\nand for all \\\\( U \\\\in \\\\tilde{U} \\\\), \\\\( t \\\\in \\\\{2, \\\\ldots, k\\\\} \\\\),\\n\\n\\\\[ U(t)w^* = e^{dt}. \\\\]\\n\\nThen, for all \\\\( U \\\\neq V \\\\in \\\\tilde{U} \\\\),\\n\\n\\\\[ \\\\rho(f, \\\\theta^* (U \\\\circ P)) < 10^{-2} \\\\implies \\\\rho(f, \\\\theta^* (V \\\\circ P)) > 10^{-2}, \\\\]\\n\\nProof. Let \\\\( \\\\cos(\\\\alpha_1) = w^T_1 U(1)w^* \\\\|w_1\\\\| \\\\), and \\\\( \\\\cos(\\\\beta_1) = w^T_1 V(1)w^* \\\\|w_1\\\\| \\\\). Now,\\n\\n\\\\[ \\\\rho(f, \\\\theta^* (U \\\\circ P)) < 10^{-2} \\\\iff (1 - \\\\max(0, \\\\|w_1\\\\| \\\\cos(\\\\alpha_1)))^2 < 10^{-2}, \\\\]\\n\\n\\\\[ \\\\iff \\\\max(0, \\\\|w_1\\\\| \\\\cos(\\\\alpha_1)) > 0. \\\\]\\n\\nBy the triangle inequality, we can get an upper bound on \\\\( \\\\cos(\\\\beta_1) \\\\) as,\\n\\n\\\\[ p^2(1 - \\\\cos(\\\\beta_1)) \\\\geq p^2(1 - 0.001) - p^2(1 - \\\\cos(\\\\alpha_1)), \\\\]\\n\\n\\\\[ \\\\cos(\\\\beta_i) \\\\leq 1 - p(1 - 0.001) - p(1 - 0.9) \\\\leq 0.7. \\\\]\\n\\nTherefore,\\n\\n\\\\[ \\\\max(0, \\\\|w_t\\\\| \\\\cos(\\\\beta_t)) \\\\leq 0.7, \\\\]\\n\\nwhich implies that\\n\\n\\\\[ (1 - \\\\max(\\\\|w_t\\\\| \\\\cos(\\\\beta_t), 0))^2 \\\\geq (0.3)^2 > 10^{-2}. \\\\]\\n\\nIn the following lemma, we prove a sample complexity lower bound of \\\\( \\\\Omega(\\\\sigma^2 kd) \\\\) for FCNs on the sub-problem SSD of DSD.\"}"}
{"id": "AfnsTnYphT", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma D.3. Let $F$ denote the class of functions represented by the set of locally connected neural network models, $M_L[W]$, as defined in 5. Let $S_n \\\\sim \\\\text{SSD}_1$ be the $n$ i.i.d. data samples drawn from $\\\\text{SSD}_1$. Consider the group $\\\\bar{U} \\\\subseteq O(kd)$, such that for all $U \\\\neq V \\\\in \\\\bar{U}$,\\n\\n$$(U(1)w \\\\star)^T(V(1)w \\\\star) < 10^{-3},$$\\n\\nand for all $U \\\\in \\\\bar{U}$, $t \\\\in \\\\{2, \\\\ldots, k\\\\}$,\\n\\n$$U(t)w \\\\star = e_{dt}.$$\\n\\nLet $\\\\xi \\\\in \\\\Xi$ encapsulate the randomization, and let $\\\\xi \\\\sim P_\\\\Xi$. If $\\\\bar{\\\\theta}(S_n, \\\\xi)$ is a $\\\\bar{U}$-equivariant algorithm then, for large enough $k$, $d$, $n$,\\n\\n$$\\\\delta(\\\\bar{\\\\theta}_n) = \\\\Omega(\\\\sigma^2 d),$$\\n\\nwhere $\\\\delta = 0.5 \\\\times 10^{-2}$.\\n\\nProof. We refer to the distribution SSD_1 by $P$. Since the algorithm $\\\\bar{\\\\theta}_n$ is $U$-equivariant, lemma 5.1 gives us that for all $U \\\\in \\\\bar{U}$,\\n\\n$$\\\\bar{\\\\theta}(\\\\{x_i, y_i\\\\}_n)(x_{\\\\bar{U}}) = \\\\bar{\\\\theta}(\\\\{U x_i, y_i\\\\}_n)(U x_{\\\\bar{U}}),$$\\n\\n$$\\\\text{err}\\\\bar{\\\\theta}(\\\\{x_i, y_i\\\\}_n)(x_{\\\\bar{U}}), y_{\\\\bar{U}} = \\\\text{err}\\\\bar{\\\\theta}(\\\\{U x_i, y_i\\\\}_n)(U x_{\\\\bar{U}}), y_{\\\\bar{U}},$$\\n\\n$$E_{S_n \\\\sim P_n} E_{(x, y) \\\\sim P} \\\\text{err}\\\\bar{\\\\theta}_n(x_{\\\\bar{U}}), y_{\\\\bar{U}} = E_{S_n \\\\sim (U \\\\circ P)_n} E_{(x, y) \\\\sim (U \\\\circ P)} \\\\text{err}\\\\bar{\\\\theta}_n(x_{\\\\bar{U}}), y_{\\\\bar{U}}.$$\\n\\n(184)\\n\\nTaking $\\\\sup$ on the right-hand side,\\n\\n$$E_{S_n \\\\sim P_n} E_{(x, y) \\\\sim P} \\\\text{err}\\\\bar{\\\\theta}_n(x_{\\\\bar{U}}), y_{\\\\bar{U}} = \\\\sup_{U \\\\circ P \\\\in \\\\bar{U} \\\\circ P} E_{(x, y) \\\\sim (U \\\\circ P)} \\\\text{err}\\\\bar{\\\\theta}_n(x_{\\\\bar{U}}), y_{\\\\bar{U}}.$$\\n\\n(187)\\n\\nAn application of corollary A.1.1 gives the bound\\n\\n$$\\\\ln(|\\\\bar{U}|) \\\\geq -\\\\frac{0.99}{d}.$$\\n\\nIn order to apply our variant of Fano's Theorem 5.1, we set the following variables: $P = \\\\bar{U} \\\\circ P; P_V = \\\\bar{U} \\\\circ P; F, \\\\Xi, \\\\text{ and } P_\\\\Xi$ are already defined in the lemma; $\\\\Theta = \\\\{\\\\theta|\\\\theta:(X, Y)_n, \\\\Xi \\\\rightarrow F\\\\}$; $\\\\theta \\\\star (U \\\\circ P) = M[[U(1)w \\\\star, \\\\ldots, U(k)w \\\\star, b \\\\star]]$, where $U \\\\in \\\\bar{U}, b \\\\star$ is some fixed value in $(0, 1)$; and \\\\(\\\\rho(f, \\\\theta \\\\star (U \\\\circ P)) = (1 - \\\\max(0, \\\\|w_1\\\\| \\\\cos(\\\\alpha_1)))^2\\\\), where $U \\\\in \\\\bar{U}$, and $\\\\cos(\\\\alpha_1) = w^T_1 U(1)w \\\\star \\\\|w_1\\\\|$. Recall from Lemma B.1 that $KL(U \\\\circ P\u2225V \\\\circ P) \\\\leq -0.999 \\\\sigma^2 < 1 \\\\sigma^2$.\\n\\nWe are now ready to apply Fano's Theorem 5.1, using the results from Lemmas D.1, D.2, and 186,\\n\\n$$E_{S_n \\\\sim P_n} E_{(x, y) \\\\sim P} \\\\text{err}\\\\bar{\\\\theta}_n(x_{\\\\bar{U}}), y_{\\\\bar{U}} \\\\geq \\\\sup_{U \\\\circ P \\\\in \\\\bar{U} \\\\circ P} E_{(x, y) \\\\sim (U \\\\circ P)} \\\\text{err}\\\\bar{\\\\theta}_n(x_{\\\\bar{U}}), y_{\\\\bar{U}},$$\\n\\n(188)\\n\\n$$\\\\geq \\\\inf_{\\\\theta \\\\in \\\\Theta} \\\\sup_{U \\\\circ P \\\\in \\\\bar{U} \\\\circ P} E[R(\\\\theta_n, U \\\\circ P)],$$\\n\\n(189)\\n\\n$$\\\\geq 10^{-2} \\\\cdot 1^{-n/\\\\sigma^2} + \\\\ln(2) - 0.99 d.$$\\n\\n(189)\\n\\nFrom the above, it is easy to see that with $n = 16 \\\\sigma^2 d$ samples, the algorithm incurs an expected risk greater than $1/2 10^{-2}$, proving the result.\\n\\nThe claim and the proof do not depend on the chosen value of $b \\\\star$. $28$\"}"}
{"id": "AfnsTnYphT", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We now present the formal statement and the proof of Theorem 7.1, which establishes the\\n\\n\\\\( \\\\delta \\\\)\\n\\n\\\\[ \\\\text{Theorem 7.1} \\\\]\\n\\nTo simplify 196, we begin by showing that the expected risk incurred by the algorithm is the same for\\n\\n\\\\( \\\\forall U \\\\in \\\\mathcal{U} \\\\)\\n\\nsuch that, \\\\( 196 \\\\)\\n\\n\\\\[ \\\\text{Proof.} \\\\]\\n\\n\\\\( 196 = \\\\bar{U} \\\\]\\n\\nFor simplicity will refer to the distribution \\\\( DSD \\\\) by\\n\\n\\\\[ \\\\text{For } \\\\{S_n \\\\}_{n=0}^{\\\\infty} \\\\sim U \\\\]\\n\\n\\\\( \\\\text{drawn from } DSD \\\\). We define the following groups,\\n\\n\\\\[ \\\\text{neural network models}, \\\\quad M \\\\]\\n\\n\\\\( \\\\in \\\\)\\n\\n\\\\[ \\\\text{the sample complexity is given by,} \\\\]\\n\\n\\\\[ w \\\\]\\n\\n\\\\( U \\\\)\\n\\n\\\\[ \\\\text{and} \\\\]\\n\\n\\\\( 196 \\\\)\\n\\n\\\\[ \\\\text{block structure of} \\\\]\\n\\n\\\\( t \\\\)\\n\\n\\\\( \\\\in \\\\)\\n\\n\\\\[ \\\\text{every distribution} \\\\]\\n\\n\\\\( U \\\\)\\n\\n\\\\[ \\\\text{sample complexity lower bound for LCNs when trained on } DSD. \\\\]\\n\\n\\\\[ \\\\text{Published as a conference paper at ICLR 2024} \\\\]\\n\\n\\\\[ \\\\text{From the construction of} \\\\]\\n\\n\\\\( U \\\\)\\n\\n\\\\[ \\\\text{We know that} \\\\]\\n\\n\\\\[ \\\\text{because of the} \\\\]\\n\\n\\\\[ \\\\text{Therefore} \\\\]\\n\\n\\\\[ \\\\exists \\\\quad \\\\text{for all} \\\\]\\n\\n\\\\[ \\\\text{we have} \\\\]\\n\\n\\\\[ \\\\text{Specifically, for all} \\\\]\\n\\n\\\\[ \\\\text{Independently, for all} \\\\]\\n\\n\\\\[ \\\\text{Therefore} \\\\]\\n\\n\\\\[ \\\\text{Specifically, for all} \\\\]\\n\\n\\\\[ \\\\text{Therefore} \\\\]\\n\\n\\\\[ \\\\text{Thus, for all} \\\\]\\n\\n\\\\[ \\\\text{Finally, for all} \\\\]\\n\\n\\\\[ \\\\text{Therefore} \\\\]\\n\\n\\\\[ \\\\text{Since} \\\\]\\n\\n\\\\[ \\\\text{Finally, for all} \\\\]\\n\\n\\\\[ \\\\text{Therefore} \\\\]\"}"}
{"id": "AfnsTnYphT", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We define the set of algorithms. For every \\\\( \\\\theta \\\\in \\\\Theta \\\\), there exists a randomization \\\\( \\\\xi \\\\) with \\\\( \\\\sup_{\\\\theta} \\\\| \\\\xi \\\\|_2 \\\\leq \\\\bar{\\\\xi} \\\\).\\n\\nFrom the construction of \\\"hard instances\\\", we know that for all \\\\( U \\\\subseteq U \\\\), notational brevity, we define \\\\( \\\\Xi = \\\\{ \\\\xi \\\\in \\\\mathbb{R}^n : \\\\max_{\\\\theta} \\\\| \\\\xi \\\\|_2 \\\\leq \\\\bar{\\\\xi} \\\\} \\\\), and \\\\( \\\\Xi_1 = \\\\{ \\\\xi \\\\in \\\\mathbb{R}^{nk} : \\\\max_{\\\\theta} \\\\| \\\\xi \\\\|_2 \\\\leq \\\\bar{\\\\xi} \\\\} \\\\).\\n\\nLet \\\\( \\\\Xi = \\\\{ \\\\xi \\\\in \\\\mathbb{R}^n : \\\\max_{\\\\theta} \\\\| \\\\xi \\\\|_2 \\\\leq \\\\bar{\\\\xi} \\\\} \\\\), and \\\\( \\\\Xi_1 = \\\\{ \\\\xi \\\\in \\\\mathbb{R}^{nk} : \\\\max_{\\\\theta} \\\\| \\\\xi \\\\|_2 \\\\leq \\\\bar{\\\\xi} \\\\} \\\\), for all \\\\( \\\\tau \\\\), as \\\\( \\\\inf_{\\\\theta} \\\\inf_{\\\\xi} \\\\tau \\\\).\\n\\nLet \\\\( \\\\xi \\\\) be a uniform random vector \\\\( \\\\sim \\\\mathbb{R}^n \\\\) and samples to be drawn from each mean, pre-sampled values of \\\\( \\\\xi \\\\) respectively. It subsequently returns a function within \\\\( \\\\mathbb{R}^n \\\\) to be the random variable that corresponds to the number of samples drawn from the distribution \\\\( \\\\mathcal{U} \\\\) of size \\\\( nkd = \\\\inf_{\\\\theta} \\\\inf_{\\\\xi} \\\\tau \\\\).\\n\\nSubstituting this back in 210, we get that for all \\\\( U \\\\subseteq U \\\\), to encapsulate the randomness \\\\( \\\\mathcal{U} \\\\), there exists a uniform random vector \\\\( \\\\sim \\\\mathbb{R}^{nk} \\\\) and returns the output of \\\\( \\\\mathcal{U} \\\\) following the generative story: Sample \\\\( \\\\Xi \\\\), there exists \\\\( \\\\bar{\\\\xi} \\\\).\\n\\nUsing Bernstein's inequality, we get that \\\\( \\\\mathcal{U} \\\\) serves as a lower bound on the original problem. It is easy to note that given minimax problem can be 'simulated' by a more tractable one, and thus the tractable problem runs using the input data, which runs a for all \\\\( U \\\\subseteq U \\\\), and for all \\\\( \\\\tau \\\\).\\n\\nTherefore, we can bound 211, \\\\( \\\\inf_{\\\\theta} \\\\inf_{\\\\xi} \\\\tau \\\\), \\\\( \\\\sup_{\\\\theta} \\\\| \\\\xi \\\\|_2 \\\\leq \\\\bar{\\\\xi} \\\\).\\n\\nSubstituting it back into 207, we get that \\\\( \\\\inf_{\\\\theta} \\\\inf_{\\\\xi} \\\\tau \\\\), \\\\( \\\\sup_{\\\\theta} \\\\| \\\\xi \\\\|_2 \\\\leq \\\\bar{\\\\xi} \\\\).\\n\\nUsing Berstein's inequality, we get that \\\\( \\\\sup_{\\\\theta} \\\\| \\\\xi \\\\|_2 \\\\leq \\\\bar{\\\\xi} \\\\).\"}"}
{"id": "AfnsTnYphT", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We have already proven a lower bound for the above problem in Lemma D.3, specifically refer to equation 186. Substituting that result,\\n\\n\\\\[ E_{S_n \\\\sim P_n \\\\bar{R}_n, P_n \\\\geq c_{10^{-2}}} \\\\geq c_{10^{-2} - \\\\frac{1}{2n}} + \\\\ln(2)0.99^d, \\\\]\\n\\n(213)\\n\\n\\\\[ E_{S_n \\\\sim P_n \\\\bar{R}_n, P_n \\\\geq c_{10^{-2} - \\\\frac{1}{2n^2}} + \\\\frac{\\\\ln(2)0.99^d}{k \\\\sigma^2}} \\\\geq c_{10^{-2} - \\\\frac{1}{2n^2}} + \\\\frac{\\\\ln(2)0.99^d}{k \\\\sigma^2}, \\\\]\\n\\n(214)\\n\\n\\\\[ 1 - 2 \\\\exp(-n^{10^{-2} - \\\\frac{1}{2n}}) \\\\leq 1 - 2 \\\\exp(-\\\\ln(4)) = \\\\frac{1}{2}. \\\\]\\n\\nAnd, choosing \\\\( n = \\\\frac{1}{6} \\\\sigma^2 k d \\\\), we can bound\\n\\n\\\\[ 1 - 3 n^2 k \\\\sigma^2 + \\\\ln(2)0.99^d = 1 - 3 \\\\frac{1}{6} \\\\sigma^2 k d + \\\\ln(2)0.99^d = 1 - \\\\frac{1}{2} \\\\sigma^2 k d + \\\\ln(2)0.99^d, \\\\]\\n\\n(215)\\n\\n\\\\[ \\\\geq 1 - \\\\frac{1}{2} \\\\sigma^2 k d + \\\\ln(2)0.99^d. \\\\]\\n\\nTherefore, we have the result,\\n\\n\\\\[ E_{S_n \\\\sim P_n \\\\bar{R}_n, P_n \\\\geq c_{10^{-2}}} \\\\geq 1 - \\\\frac{1}{4} 10^{-2}. \\\\]\\n\\n(216)\"}"}
{"id": "AfnsTnYphT", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Theorem 7.2\\n\\nLet $F$ denote the class of functions represented by the set of locally connected neural network models, $MC[W]$, as defined in 6. Let the input data be drawn from the DSD distribution, $S_n \\\\sim (DSD)_n$, with $\\\\sigma = \\\\tilde{O}(1/\\\\sqrt{k})$. We define the group $U := \\\\{\\\\text{Block}(U_1, \\\\ldots, U_k) | U_i \\\\in O(d) \\\\}$, $U_i = U_j$. Then there exists a weight initialization distribution $W$ and update functions $\\\\{F_t\\\\}_T$ such that $\\\\bar{\\\\theta}_n(MC[W], \\\\{F_t\\\\}_T, W, S_n)$ is an $U$-equivariant algorithm and, if $k, d$ are large enough, then\\n\\n$$n \\\\delta(\\\\bar{\\\\theta}_n) = \\\\max\\\\left(\\\\tilde{O}(\\\\sigma^2 (d+k) \\\\ln(kd)), 10\\\\right),$$\\n\\nfor some constant $\\\\delta = O(1)$.\\n\\nProof. The outline of the proof will run parallel to the approach taken in the proof of Theorem 6.2. We will first present the algorithm $\\\\bar{\\\\theta}_n$, then show it is a $U$-equivariant algorithm and then derive the required sample complexity bound upper bound.\\n\\n1. Algorithm Definition\\n\\nTo define the algorithm $\\\\bar{\\\\theta}_n$, we need to specify its components: the model $MC[W]$, the initialization distribution $W$, and the update functions $\\\\{F_t\\\\}_T$. At iteration $t = 0$, we initialize the model parameter $v_0 = [w_0, b_0]$ as $w_0 \\\\sim N(0, \\\\gamma I_d)$, where $\\\\gamma = 100k^2d^2$, and bias is set as $b_0 = 0$. The superscript denotes the iteration number. To specify the update functions, we define the empirical loss function,\\n\\n$$l : (W, (X, Y)_n) \\\\to \\\\mathbb{R} := \\\\frac{1}{n} \\\\sum_{j=1}^n y_i - k \\\\sum_{i=1}^k \\\\phi_{b}(w^T x_i^j) !^2.$$\\n\\nThe algorithm has $T = 2$ iterations. For simpler analysis, we divide the dataset, $S_n$, into two equal sized datasets $S_m^1$ and $S_m^2$, with $m := n/2$ samples each. The update function for each $t \\\\in \\\\{1, 2\\\\}$ is,\\n\\n$$F_t(v, S_m^t) := h_w - \\\\eta_t \\\\nabla_w l(w, b; S_m^t) \\\\|w - \\\\eta_t \\\\nabla_w l(w, b; S_m^t)\\\\|; b_t,$$\\n\\nwhere $\\\\eta_1 = 1$, $\\\\eta_2 = 10^3$, $b_1 = \\\\frac{1}{100}qkd(k+d) \\\\ln(kd)$, $b_2 = 10^{-4}$.\\n\\n2. Algorithm is Equivariant\\n\\nTo establish that $\\\\bar{\\\\theta}_n$ is $U$-equivariant, we verify the three conditions specified in Definition 6. We define the group,\\n\\n$$V := \\\\{\\\\text{Block}(V, I_1) | V \\\\in O(d)\\\\},$$\\n\\nwhere $I_1$ is the identity matrix of size $1$. For $x, y \\\\in (X, Y)$, $U \\\\in U$, $w \\\\in \\\\mathbb{R}^d$, $b \\\\in \\\\mathbb{R}^+$, choose $V = \\\\text{Block}(\\\\{U(1), I_1\\\\}) \\\\in V$, without loss of generality, as for all $i, j$, $U(i) = U(j)$. Then, the property 1 of equivariance holds as,\\n\\n$$MC[v](x) = k \\\\sum_{i=1}^k \\\\phi_{b}(w^T x_i^j) = k \\\\sum_{i=1}^k \\\\phi_{b}(U(1)w^T U(i)x_i^j) = MC[Vv](Ux).$$\\n\\nFor all $t \\\\in [2]$ and $S_m^t \\\\in (X, Y)$ the second property 2 follows as,\\n\\n$$F_t(Vv, U \\\\circ S_m^t) = h_{U(1)}(w - \\\\eta_t \\\\nabla_w l(v; S_m^t)) \\\\|w - \\\\eta_t \\\\nabla_w l(v; S_m^t)\\\\|; b_t,$$\\n\\nwhere $h_{U(1)}(\\\\cdot)$ is applied separately for each $U(i)$.\\n\\nAnd as for property 3, observe that,\\n\\n$$Vv_0 = [U(1)w_0, b_0]$$\\n\\n$$(225)$$\"}"}
{"id": "AfnsTnYphT", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Algorithm Analysis\\n\\nWe analyze the algorithm, with \\\\( n = \\\\max(2\\\\sigma^2(k+d) \\\\ln(kd), 10) \\\\) samples, to establish that \\\\( \\\\bar{\\\\theta}_n \\\\) achieve an expected risk of at most \\\\( \\\\delta = 2.5 \\\\times 10^{-3} \\\\). We set \\\\( \\\\sigma \\\\leq \\\\frac{1}{100} \\\\sqrt{k \\\\ln(kd)} \\\\). The outline of the proof is as follows: we first prove that after the first update step, the alignment of \\\\( w_1 \\\\) with unknown signal vector \\\\( w_\\\\star \\\\) is \\\\( \\\\Omega(\\\\frac{q}{k}) \\\\). In the second step, we use this alignment is reliably threshold out the \\\"noise\\\" patches, while letting the \\\"signal\\\" patch pass through the first hidden layer. We then show that this denoising effect, enables us to recover the signal with an alignment of \\\\( \\\\Omega(1) \\\\), which would imply that the risk of the CNN on the task \\\\( \\\\leq \\\\delta \\\\).\\n\\n3a. Update Step 1\\n\\nWe define \\\\( \\\\hat{w}_1 = w_0 - \\\\nabla w_0 l(w_0, 0; S_{m1}) \\\\) to be the unnormalized parameter vector \\\\( w_1 \\\\), and therefore the alignment with the signal is given by \\\\( (w_1)^T w_\\\\star = (\\\\hat{w}_1)^T w_\\\\star \\\\| \\\\hat{w}_1 \\\\|. \\\\) To analyze \\\\( \\\\hat{w}_1 \\\\), we first evaluate the gradient with respect to \\\\( w_0 \\\\), \\\\( \\\\nabla w_0 l(w_0, 0; S_{m1}) = \\\\frac{1}{m} \\\\sum_{j=1}^{m} \\\\nabla w_0 y_i - k \\\\sum_{i=1}^{k} \\\\phi_0((w_0)^T x_i j) \\\\!\\!^2 \\\\), (226)\\\\n(227) := -\\\\frac{2}{m} \\\\sum_{j=1}^{m} y_j - k \\\\sum_{i=1}^{k} \\\\phi_0((w_0)^T x_i j) \\\\!\\! \\\\phi'_0((w_0)^T x_i j) \\\\!\\!^! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\!^! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\! \\\\!\\\\n(228) := -\\\\frac{2}{m} \\\\sum_{j=1}^{m} \\\\alpha_j \\\\beta_j , \\\\) (229)\\\\nwhere \\\\( \\\\phi'_0(x) := \\\\frac{d}{dx} \\\\phi_0(x) \\\\). We have used the facts that \\\\( \\\\phi_0 \\\\) is the identity function, and \\\\( \\\\phi'_0 \\\\) is the constant function \\\\( 1 \\\\). And, \\\\( \\\\alpha_j := 1 - \\\\sum_{i=1}^{k} y_j ((w_0)^T x_i j) \\\\), \\\\( \\\\beta_j := \\\\sum_{i=1}^{k} y_j x_i j \\\\).\\n\\nTo further analyze 229, we first prove high probability bounds for \\\\( \\\\alpha_j \\\\) for all \\\\( j \\\\in [m] \\\\). From the initialization distribution \\\\( W \\\\), we know that \\\\( w_0 \\\\!\\!^d = \\\\gamma \\\\epsilon \\\\), where \\\\( \\\\epsilon \\\\) is the Gaussian random vector defined as \\\\( \\\\epsilon \\\\!\\!^\\\\sim N(0, I_d) \\\\). And from the input distribution \\\\( D_{SD} \\\\), we know that \\\\( x_i j = y_j r_{ij} w_\\\\star + \\\\sigma \\\\epsilon_i j \\\\), for all \\\\( i \\\\) in \\\\([k]\\\\). Here, \\\\( \\\\epsilon_i j \\\\!\\!^\\\\sim N(0, I_d) \\\\) is also a Gaussian random vector, and \\\\( r_{ij} = 1 \\\\), if the signal patch appears in the \\\\( j \\\\)-th data sample appears in the \\\\( i \\\\)-th patch, and \\\\( 0 \\\\) otherwise.\\\\n\\nWe can now bound the range of \\\\( \\\\alpha_j \\\\) as,\\\\n\\n\\\\[ 1 + |\\\\gamma \\\\epsilon^T w_\\\\star| + |\\\\sum_{i=1}^{k} y_j ((w_0)^T x_i j) \\\\sigma \\\\epsilon^T \\\\epsilon_i j| \\\\geq \\\\alpha_j \\\\geq 1 - |\\\\gamma \\\\epsilon^T w_\\\\star| - |\\\\sum_{i=1}^{k} y_j \\\\sigma \\\\epsilon^T \\\\epsilon_i j|, \\\\] (233)\\\\n\\nWe first upper bound \\\\( |\\\\gamma \\\\epsilon^T w_\\\\star| \\\\). Since the norm of the signal is \\\\( 1 \\\\), \\\\( \\\\| w_\\\\star \\\\| = 1 \\\\), \\\\( \\\\epsilon^T w_\\\\star \\\\!\\!^\\\\sim N(0, 1) \\\\), and \\\\( |\\\\gamma \\\\epsilon^T w_\\\\star| \\\\leq \\\\frac{1}{100} \\\\frac{2}{dk} \\\\), (234)\\\\n\\n\\\\[ |\\\\gamma \\\\epsilon^T w_\\\\star| \\\\leq \\\\frac{1}{8}, \\\\] (235)\"}"}
{"id": "AfnsTnYphT", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nwith probability \\\\( \\\\geq 1 - 2\\\\Phi\\\\left(-\\\\frac{10}{k^2}d^2\\\\right) \\\\geq 1 - 10^{-6} \\\\), for large enough \\\\( k, d \\\\).\\n\\nNext, we provide an upper bound for \\\\( |P_{k_i = 1} \\\\gamma \\\\sigma \\\\epsilon_T(i)j| \\\\), for all \\\\( j \\\\). For this we analyze,\\n\\n\\\\[\\n\\\\max_{j \\\\in \\\\{m\\\\}} \\\\left| P_{k_i = 1} \\\\gamma \\\\sigma \\\\epsilon_T(i)j \\\\right| = \\\\gamma \\\\sigma \\\\max_{j \\\\in \\\\{m\\\\}} \\\\left| \\\\epsilon_T(k_i = 1)j \\\\right| = \\\\gamma \\\\sigma \\\\max_{j \\\\in \\\\{m\\\\}} \\\\sqrt{k} \\\\epsilon_T(\\\\bar{\\\\epsilon}_j) \\\\leq 6 \\\\gamma \\\\sigma \\\\sqrt{kd} \\\\max_{j \\\\in \\\\{m\\\\}} \\\\left| \\\\epsilon_T(\\\\bar{\\\\epsilon}_j) \\\\right|, \\\\quad (235)\\n\\\\]\\n\\nwith probability \\\\( \\\\geq 1 - 10^{-6} \\\\). The last inequality 236 follows from the concentration of the norm of a Gaussian random variable,\\n\\n\\\\[\\nP[\\\\|\\\\bar{\\\\epsilon}\\\\| \\\\geq 6 \\\\sqrt{d}] \\\\leq 2 \\\\exp\\\\left(-\\\\frac{36}{d^2d}\\\\right) \\\\leq 10^{-6}. \\\\quad (236)\\n\\\\]\\n\\nWe define \\\\( u = \\\\epsilon_T(\\\\bar{\\\\epsilon}_j) \\\\), and \\\\( \\\\epsilon_j = u^T \\\\bar{\\\\epsilon}_j \\\\), which is a standard Gaussian random variable. Then, from the concentration inequality,\\n\\n\\\\[\\nP[\\\\max_{j \\\\in \\\\{m\\\\}} |\\\\epsilon_j| \\\\geq \\\\frac{32 \\\\ln(m)}{m}] \\\\leq 2 m^{-9} \\\\leq 10^{-6}. \\\\quad (237)\\n\\\\]\\n\\nSubstituting this in 236,\\n\\n\\\\[\\n\\\\max_{j \\\\in \\\\{m\\\\}} |\\\\gamma \\\\sigma \\\\epsilon_T(k_i = 1)j| \\\\leq 6 \\\\gamma \\\\sigma \\\\sqrt{kd} \\\\max_{j \\\\in \\\\{m\\\\}} \\\\left| \\\\epsilon_j \\\\right|, \\\\quad (237)\\n\\\\]\\n\\n\\\\[\\n\\\\leq \\\\frac{6}{100} \\\\frac{1}{k^2} d^2 + 5 \\\\sigma^2 \\\\frac{1}{k^2} \\\\sqrt{kd} (d + k) \\\\ln(kd) \\\\leq 10^{-6} \\\\frac{1}{kd} (k + d) \\\\ln(kd), \\\\quad (238)\\n\\\\]\\n\\nfor large enough \\\\( k, d \\\\). Using 234, 239 in 233, we bound \\\\( \\\\alpha_j \\\\), for all \\\\( j \\\\in \\\\{m\\\\} \\\\), as,\\n\\n\\\\[\\n1 + |\\\\gamma \\\\epsilon_T(w^\\\\star)| + |\\\\sum_{i=1}^k \\\\gamma \\\\sigma \\\\epsilon_T(i)j| \\\\geq \\\\alpha_j \\\\geq 1 - |\\\\gamma \\\\epsilon_T(w^\\\\star)| - |\\\\sum_{i=1}^k \\\\gamma \\\\sigma \\\\epsilon_T(i)j|. \\\\quad (240)\\n\\\\]\\n\\n5 \\\\[4 \\\\geq \\\\alpha_j \\\\geq 3 \\\\]. \\\\quad (241)\\n\\nAlso, note that \\\\( \\\\beta_j = P_{k_i = 1} y_j x_T(i)j = w^\\\\star + \\\\sigma \\\\sqrt{k} \\\\epsilon_T(\\\\bar{\\\\epsilon}_j) \\\\). We are now in the position to analyze \\\\( \\\\hat{w} \\\\),\\n\\n\\\\[\\n\\\\hat{w} = w_0 - \\\\nabla_w l(w_0, 0; S_m), \\\\quad (242)\\n\\\\]\\n\\n\\\\[\\n\\\\hat{w} = w_0 + \\\\frac{1}{m} \\\\sum_{j=1}^m 2 \\\\alpha_j \\\\beta_j, \\\\quad (243)\\n\\\\]\\n\\n\\\\[\\n\\\\hat{w} = w_0 + \\\\frac{1}{m} \\\\sum_{j=1}^m 2 \\\\alpha_j w^\\\\star + \\\\sigma \\\\sqrt{k} \\\\epsilon_T(\\\\bar{\\\\epsilon}_j), \\\\quad (244)\\n\\\\]\\n\\n\\\\[\\n\\\\hat{w} = w_0 + \\\\frac{2}{m} \\\\sum_{j=1}^m w^\\\\star + \\\\sigma \\\\sqrt{k} \\\\epsilon_T(\\\\bar{\\\\epsilon}_j), \\\\quad (245)\\n\\\\]\\n\\nwhere \\\\( \\\\bar{\\\\epsilon} \\\\) is the Gaussian random vector \\\\( \\\\sim N(0, I_d) \\\\). Note that from the concentration of the norm of a Gaussian random variable\\n\\n\\\\[\\nP[\\\\|\\\\bar{\\\\epsilon}\\\\| \\\\geq 6 \\\\sqrt{d}] \\\\leq 2 \\\\exp\\\\left(-\\\\frac{36}{d^2d}\\\\right) \\\\leq 10^{-6}, \\\\quad (236)\\n\\\\]\\n\\nRecall that our aim is to bound \\\\( (w_1)^T w^\\\\star = (\\\\hat{w}_1)^T w^\\\\star \\\\|\\\\hat{w}_1\\\\| \\\\). For this, we first upper bound \\\\( \\\\|\\\\hat{w}_1\\\\| \\\\),\\n\\n\\\\[\\n\\\\|\\\\hat{w}_1\\\\| \\\\leq \\\\|w_0\\\\| + \\\\left\\\\| \\\\sum_{j=1}^m 2 \\\\alpha_j w^\\\\star \\\\right\\\\| + \\\\left\\\\| \\\\sum_{j=1}^m 2 \\\\sigma \\\\sqrt{k} \\\\epsilon_T(\\\\bar{\\\\epsilon}_j) \\\\right\\\\|, \\\\quad (246)\\n\\\\]\\n\\n\\\\[\\n\\\\leq \\\\|w_0\\\\| + 5 \\\\sigma^2 \\\\frac{1}{k^2} \\\\sqrt{kd} (d + k) \\\\ln(kd) \\\\leq 10^{-6} \\\\frac{1}{kd} (k + d) \\\\ln(kd), \\\\quad (247)\\n\\\\]\\n\\n\\\\[\\n\\\\leq 5 \\\\frac{1}{k^2} + \\\\frac{1}{k^2} \\\\frac{1}{d^2} + \\\\frac{1}{k^2} \\\\frac{1}{d^2} \\\\sqrt{kd} (d + k) \\\\ln(kd) \\\\leq 10^{-6} \\\\frac{1}{kd} (k + d) \\\\ln(kd), \\\\quad (248)\\n\\\\]\\n\\n\\\\[\\n\\\\leq \\\\gamma \\\\|\\\\epsilon\\\\| + \\\\frac{5}{100} \\\\frac{1}{k^2} d^2 + \\\\frac{5}{100} \\\\sigma^2 \\\\frac{1}{k^2} \\\\sqrt{kd} (d + k) \\\\ln(kd) \\\\leq 10^{-6} \\\\frac{1}{kd} (k + d) \\\\ln(kd), \\\\quad (249)\\n\\\\]\\n\\n\\\\[\\n\\\\leq 6 \\\\sqrt{d} \\\\frac{1}{100} k^2 d^2 + \\\\frac{5}{100} \\\\frac{1}{k^2} \\\\frac{1}{d^2} + \\\\frac{5}{100} \\\\sigma^2 \\\\frac{1}{k^2} \\\\sqrt{kd} (d + k) \\\\ln(kd) \\\\leq 10^{-6} \\\\frac{1}{kd} (k + d) \\\\ln(kd), \\\\quad (250)\\n\\\\]\\n\\n\\\\[\\n\\\\leq 10^{-6} \\\\frac{1}{kd} (k + d) \\\\ln(kd), \\\\quad (250)\\n\\\\]\"}"}
{"id": "AfnsTnYphT", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We now show that for all i.i.d. data sample \\\\( \\\\lambda \\\\), where \\\\( \\\\lambda \\\\) was positive. In the first case, the probability that none of the nodes with positive output, or any nodes with negative output, return nothing, then we set \\\\( w \\\\) to be given by (33).\\n\\nBefore we define the identification procedure did not fail, then the random variable \\\\( w \\\\) returns nothing, then we set \\\\( \\\\hat{\\\\theta} \\\\) as the alignment between the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). In case, the sum \\\\( \\\\sum_{i=1}^{k} \\\\phi_i \\\\) of these \\\\( \\\\phi_i \\\\) vectors to the unit sphere, if there exists such a parameter. Otherwise, the method returns nothing, indicating that such a parameter does not exist, and this indication is identified through the neural network, and return the parameter corresponding to the first node whose output is also \\\\( \\\\geq \\\\cos(\\\\alpha) \\\\).\\n\\nTo see this, consider two points \\\\( u, v \\\\) with probability \\\\( \\\\geq \\\\cos(\\\\alpha) \\\\). The algorithm \\\\( \\\\bar{\\\\Phi}(\\\\bar{\\\\theta}) \\\\) runs the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\theta) \\\\) as a subroutine such that it achieves a constant max error of \\\\( \\\\epsilon \\\\). The expected risk of \\\\( \\\\delta \\\\) is \\\\( \\\\leq \\\\delta \\\\leq \\\\epsilon \\\\).\\n\\nFor brevity, we refer to the distribution SSD as a subroutine such that it achieves a constant max error of \\\\( \\\\epsilon \\\\). In the second case, the probability that none of the nodes with positive output, or any nodes with negative output, return nothing, then we set \\\\( \\\\hat{\\\\theta} \\\\) as the alignment between the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). In case, the sum \\\\( \\\\sum_{i=1}^{k} \\\\phi_i \\\\) of these \\\\( \\\\phi_i \\\\) vectors to the unit sphere, if there exists such a parameter. Otherwise, the method returns nothing, indicating that such a parameter does not exist, and this indication is identified through the neural network, and return the parameter corresponding to the first node whose output is also \\\\( \\\\geq \\\\cos(\\\\alpha) \\\\).\\n\\nWe prove this via contradiction. For an i.i.d. data sample \\\\( \\\\lambda \\\\), where \\\\( \\\\lambda \\\\) was positive. In the first case, the probability that none of the nodes with positive output, or any nodes with negative output, return nothing, then we set \\\\( w \\\\) to be given by (33). We define the mean estimation minimax problem as the expected risk of \\\\( \\\\delta \\\\) is \\\\( \\\\leq \\\\delta \\\\leq \\\\epsilon \\\\). We begin with the observation that if \\\\( \\\\lambda \\\\) was positive, then the probability that none of the nodes with positive output, or any nodes with negative output, return nothing, then we set \\\\( w \\\\) to be given by (33).\\n\\nConsider the following problem: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be the set of distributions. Let \\\\( \\\\Theta \\\\) be a distribution over \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\). Let the parameter vector of the fully connected neural network (FCN) that is returned by the algorithm \\\\( \\\\bar{\\\\Phi}(\\\\hat{\\\\theta}) \\\\) be \\\\( \\\\bar{\\\\theta} \\\\). We define the distribution SSD-\\n\\nMarkov-identification procedure: Let \\\\( P \\\\) be the number of samples. Let \\\\( \\\\bar{\\\\Phi"}
{"id": "AfnsTnYphT", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Instead, if we were to run $\\\\hat{\\\\theta}_m$ on $\\\\{U_1 x_i, y_i\\\\}_{m \\\\sim U_1 U \\\\circ P}$, then observe that $\\\\mu_\\\\perp 1 = v$. Therefore, $P_U [\\\\mu_\\\\perp 1 = u] = P_U [\\\\mu_\\\\perp 1 = v]$. Also, note that $U U_1 \\\\circ P = U \\\\circ P$, because Gaussian is an equivariant distribution. Therefore, $P_U [\\\\mu_\\\\perp 1 = u] = P_U [\\\\mu_\\\\perp 1 = v]$. Since $U, u, v$ were arbitrary, this proves the claim that $\\\\mu_\\\\perp 1 \\\\sim \\\\text{Uniform}(S)$.\\n\\nLet $\\\\tilde{m}$ be the number of sections for which $w_s \\\\neq 0$. By Chernoff bound on the Bernoulli random variable corresponding to the event $R \\\\bar{\\\\theta}_n$, $U \\\\circ P \\\\geq 0.5$, and the Identification procedure analysis, $\\\\tilde{m} \\\\geq \\\\frac{100}{b_{\\\\min}}$ with probability $\\\\geq 1 - \\\\exp(-450/b_{\\\\min}^2) - 1000/p/b_{\\\\min}^2 \\\\geq 1 - 10^{-9}$. Now observe, $\\\\mu_T 1 P_w s \\\\neq 0 w_s \\\\tilde{m} = \\\\mu_T 1 P_w s \\\\neq 0 \\\\lambda s \\\\mu_1 + \\\\sqrt{1 - \\\\lambda^2 s} \\\\mu_\\\\perp 1, s \\\\tilde{m}$.\\n\\n$$\\\\geq b_{\\\\min} + \\\\mu_T 1 P_w s \\\\neq 0 \\\\lambda s \\\\mu_1 + \\\\sqrt{1 - \\\\lambda^2 s} \\\\mu_\\\\perp 1, s \\\\tilde{m} \\\\geq b_{\\\\min} + \\\\mu_T 1 P_w s \\\\neq 0 \\\\lambda s \\\\sqrt{kd} \\\\tilde{m} \\\\geq b_{\\\\min} + 0.99 b_{\\\\min},$$\\n\\nwhere $\\\\epsilon_s \\\\sim N(0, I_{kd})$. By the concentration of Gaussian norm, $\\\\sqrt{kd} \\\\leq \\\\|\\\\epsilon_s\\\\| \\\\leq 2\\\\sqrt{kd}$, for all $s$, with probability $\\\\geq 1 - 10^{-9}$, for large enough $k, d$.\\n\\nCombining the dot-product and norm analyses, $\\\\hat{\\\\theta}_m \\\\mu_1 \\\\geq 0.7$ with probability $\\\\geq 1 - 10^{-9}$. Therefore the expected risk of $\\\\hat{\\\\theta}_m$ is,\\n\\n$$E \\\\|\\\\hat{\\\\theta}_m - U \\\\mu_1\\\\| \\\\leq p^2 (1 - 0.7) - \\\\sqrt{2 \\\\times 10^{-9}} \\\\leq \\\\frac{1}{4}.$$ \\n\\nUsing Lemma C.1, we have that, $100b_{\\\\min}^2 (n + 1) = \\\\Omega(\\\\sigma^2 kd) \\\\Rightarrow n = \\\\Omega(\\\\sigma^2 kd)$.\\n\\n**Theorem 6.1 (Formal).** Let $F$ denote the class of functions represented by the set of fully connected neural network models, $M_F [W]$, as defined in 4. Let $S_n \\\\sim (DSD)_n$ be the $n$ i.i.d. data samples drawn from $DSD$, with $\\\\sigma = \\\\tilde{O}(1/\\\\sqrt{k})$, and $k = O(\\\\exp(d))$. Define the equivariance group $U = O(kd)$, let $\\\\{F_t\\\\}_T$ be a set of update functions, and let the model parameters be initialized as $w_0 \\\\sim W$, for some distribution $W$. If the algorithm, $\\\\bar{\\\\theta}_n (S_n, w_0; M_F [W], \\\\{F_t\\\\}_T)$, is $U$-equivariant, such that $b_T \\\\geq b_{\\\\min}$, then for large enough $k, d$,\\n\\n$$\\\\delta(\\\\bar{\\\\theta}_n) = \\\\max(\\\\Omega(\\\\sigma^2 k^2 d), 40/k),$$\\n\\nwhere $\\\\delta = 0.25 \\\\times 10^{-2}$. $b_{\\\\min} := 10^{-2}$.16\"}"}
{"id": "AfnsTnYphT", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where, we define \\\\( \\\\tilde{U} \\\\) To simplify 52, we begin by showing that the expected risk incurred by the algorithm is the same for every distribution \\\\( U \\\\) of the vectors \\\\( \\\\bar{U} \\\\) for any \\\\( \\\\alpha, \\\\beta \\\\)\\n\\nProof. For simplicity we refer to the distribution DSD by \\\\( \\\\Xi \\\\) = \\\\( \\\\tilde{U} \\\\). Substituting it back into 52,\\n\\n\\\\[ P \\\\sup_{x \\\\in \\\\Xi} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right) \\\\mu_{\\\\bar{U}} \\\\]\\n\\n\\\\[ = \\\\frac{1}{\\\\bar{U}} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\alpha \\\\bar{U}_i \\\\right"}
{"id": "AfnsTnYphT", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We have effectively reduced solving the original problem to the event that a given minimax problem can be 'simulated' by a more tractable one, and thus the tractable minimax problem of learning SSD serves as a lower bound on the original problem.\\n\\nWe will now perform a series of reductions to lower bound the above minimax problem, with the goal of approximating the number of samples needed. Using that result, we have\\n\\n\\\\[ \\\\inf_{\\\\theta} \\\\sup_{Q} \\\\E_{x, y} [Q(x, y, \\\\theta)] \\\\geq \\\\inf_{\\\\theta} \\\\sup_{Q} \\\\E_{x, y} [Q(x, y, \\\\theta)] \\\\]\\n\\nThe last inequality follows from the fact that for every algorithm \\\\( Q \\\\) and parameter initialization \\\\( \\\\theta \\\\), it is true that\\n\\n\\\\[ \\\\inf_{\\\\theta} \\\\sup_{Q} \\\\E_{x, y} [Q(x, y, \\\\theta)] \\\\geq \\\\inf_{\\\\theta} \\\\sup_{Q} \\\\E_{x, y} [Q(x, y, \\\\theta)] \\\\]\\n\\niff\\n\\n\\\\[ m \\\\geq \\\\Omega(2^{\\\\Theta n k d}) \\\\]\\n\\n\\\\[ \\\\inf_{\\\\theta} \\\\sup_{Q} \\\\E_{x, y} [Q(x, y, \\\\theta)] \\\\geq \\\\inf_{\\\\theta} \\\\sup_{Q} \\\\E_{x, y} [Q(x, y, \\\\theta)] \\\\]\\n\\nThe inequality follows from the fact that for every algorithm \\\\( Q \\\\) and parameter initialization \\\\( \\\\theta \\\\), it is true that\\n\\n\\\\[ m \\\\geq \\\\Omega(2^{\\\\Theta n k d}) \\\\]\\n\\n\\\\[ \\\\inf_{\\\\theta} \\\\sup_{Q} \\\\E_{x, y} [Q(x, y, \\\\theta)] \\\\geq \\\\inf_{\\\\theta} \\\\sup_{Q} \\\\E_{x, y} [Q(x, y, \\\\theta)] \\\\]\\n\\nRecall that this distribution must be independent of the input data distribution. To see this, we note that the randomness in\\n\\n\\\\[ \\\\{ \\\\xi \\\\} \\\\]\\n\\nFor notational brevity, we define\\n\\n\\\\[ d \\\\]\\n\\ndeterministically creates the indexed dataset using\\n\\n\\\\[ \\\\{ \\\\xi \\\\} \\\\]\\n\\ninput the training data, a vector of i.i.d. standard Gaussian random variables. Let\\n\\n\\\\[ \\\\Theta \\\\]\\n\\n\\\\[ y \\\\sim \\\\text{Unif}(\\\\Theta) \\\\]\\n\\ncorresponding to the number of samples drawn from the distribution\\n\\n\\\\[ n \\\\]\\n\\nFor the next reduction, we generalize the definition of\\n\\n\\\\[ \\\\tilde{\\\\Theta} \\\\]\\n\\nLet\\n\\n\\\\[ \\\\Theta \\\\]\\n\\n\\\\[ a \\\\]\\n\\n\\\\[ \\\\tilde{\\\\Theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\xi} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\tau} \\\\]\\n\\n\\\\[ \\\\tilde{t} \\\\]\\n\\n\\\\[ \\\\tilde{w} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\mu} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\varphi} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\phi} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\psi} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\chi} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\kappa} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\nu} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\omega} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\varepsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\beta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\gamma} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\delta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\zeta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\eta} \\\\]\\n\\n\\\\[ \\\\tilde{\\\\theta} \\\\]\"}"}
{"id": "AfnsTnYphT", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Then, the first property 1 holds as,\\n\\nTo prove this, we need to specify the initialization distribution \\\\( \\\\bar{\\\\theta} \\\\)\\n\\n\\n\\nTo establish that the second property 2 follows as,\\n\\n\\n\\n\\n\\nFor each iteration \\\\( t \\\\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n"}
{"id": "AfnsTnYphT", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We analyze each iteration of $\\\\bar{\\\\omega}$. And property 3 can be affirmed by observing that, for all $V$\\n\\nThus, the alignment of $w$ with the signal can be written as, $i_1 \\\\in R_d$. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $U \\\\circ \\\\theta_n \\\\equiv \\\\theta_n$. Observe that action of any $\\\\theta_n$ is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each iteration $\\\\bar{\\\\omega}$, property 2 follows as,\\n\\n$$\\\\sigma = \\\\tilde{\\\\omega} \\\\Omega(1)$$\\n\\nThrough the first hidden layer. This enables us to recover the signal up to an alignment of $\\\\omega(\\\\Omega(\\\\sigma))$. Thus, the algorithm is $\\\\bar{\\\\omega}$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. We now establish $\\\\theta_n$ is $U$-equivariant, and therefore is $U$-equivariant. For each $i_1$, $i_2 \\\\in U_d$ is to permute the $d$-dimensional $w$. For this given $U$, we denote $"}
{"id": "AfnsTnYphT", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nwhere $\\\\phi'_{0}(x) := \\\\frac{d}{dx} \\\\phi_{0}(x)$, and the last equality follows by observing that $\\\\phi_{0}$ is the identity function, and $\\\\phi'_{0}$ is the constant function 1. As a shorthand, we define $\\\\alpha_{j} := 1 - \\\\sum_{k \\\\in [m]} y_{j}(w_{0}i)_{T}x_{i}(j)$, and $\\\\beta_{ij} := y_{j}x_{i}(j)$. Substituting this back into 89,\\n\\n$$\\\\nabla w_{0}(l; [w_{0}, 0]; S_{m}) = -2m m_{X} \\\\sum_{j = 1}^{m} \\\\alpha_{j} \\\\beta_{ij},$$\\n\\n(90)\\n\\nTo analyze 90, we begin by proving high probability bounds on the range of $\\\\alpha_{j}$, for each $j \\\\in [m]$. From the initialization procedure described above, we know that $w_{0}i = \\\\gamma \\\\epsilon_{i}$, where $\\\\epsilon_{i}$ is the Gaussian random vector defined as $\\\\epsilon_{i} \\\\sim N(0, I_{d})$. And with the input distribution being DSD, we know that $x_{i}(j) = y_{j}r_{ij}w^{\\\\star} + \\\\sigma \\\\epsilon_{i}(j)$, for all $i$ in $[k]$, where $\\\\epsilon_{i}(j) \\\\sim N(0, I_{d})$ is also a Gaussian random vector, and $r_{ij} = 1$, if in the $j$-th data sample the signal patch appears in the $i$-th patch, and 0 otherwise.\\n\\n$$\\\\alpha_{j} = 1 - \\\\sum_{k \\\\in [m]} y_{j}(w_{0}i)_{T}x_{i}(j),$$\\n\\n(91)\\n\\n$$= 1 - \\\\sum_{k \\\\in [m]} \\\\gamma r_{ij} y_{j}^{2} \\\\epsilon_{T}w^{\\\\star} - \\\\sum_{k \\\\in [m]} \\\\gamma \\\\sigma y_{j} \\\\epsilon_{T} \\\\epsilon_{i}(j).$$\\n\\n(92)\\n\\nWe can now bound the range of $\\\\alpha_{j}$ as,\\n\\n$$1 + \\\\left| \\\\sum_{k \\\\in [m]} \\\\gamma r_{ij} \\\\epsilon_{T}w^{\\\\star} \\\\right| + \\\\left| \\\\sum_{k \\\\in [m]} \\\\gamma \\\\sigma \\\\epsilon_{T} \\\\epsilon_{i}(j) \\\\right| \\\\geq \\\\alpha_{j} \\\\geq 1 - \\\\left| \\\\sum_{k \\\\in [m]} \\\\gamma r_{ij} \\\\epsilon_{T}w^{\\\\star} \\\\right| - \\\\left| \\\\sum_{k \\\\in [m]} \\\\gamma \\\\sigma \\\\epsilon_{T} \\\\epsilon_{i}(j) \\\\right|.$$\\n\\n(93)\\n\\nWe first upper bound $\\max_{j} \\\\left| \\\\sum_{k \\\\in [m]} \\\\gamma r_{ij} \\\\epsilon_{T}w^{\\\\star} \\\\right| \\\\leq \\\\max_{i} \\\\left| \\\\gamma \\\\epsilon_{T}w^{\\\\star} \\\\right|$. Since the norm of the signal is 1, $\\\\|w^{\\\\star}\\\\| = 1$, we have that $\\\\epsilon_{T}w^{\\\\star} \\\\sim N(0, 1)$. We can now upper bound $\\max_{i} \\\\left| \\\\gamma \\\\epsilon_{T}w^{\\\\star} \\\\right|$ as,\\n\\n$$\\\\max_{i} \\\\left| \\\\gamma \\\\epsilon_{T}w^{\\\\star} \\\\right| \\\\leq \\\\frac{1}{100} \\\\frac{1}{k^{2}} \\\\frac{1}{d^{2}} \\\\max_{i} \\\\left| \\\\epsilon_{T}w^{\\\\star} \\\\right| \\\\leq \\\\frac{1}{8},$$\\n\\n(94)\\n\\nwith probability $\\\\geq 1 - \\\\frac{10}{7}$. To derive inequality 94, we have used the concentration inequality,$$\\n\\\\Pr \\\\left[ \\\\max_{i} \\\\| \\\\epsilon_{T}w^{\\\\star} \\\\| \\\\geq \\\\sqrt{d} + 10 \\\\ln(k) \\\\right] \\\\leq 2 \\\\frac{k}{9},$$\\n\\n$$\\\\leq 10^{-7}.$$ And now we seek to bound $\\max_{j} \\\\left| \\\\sum_{k \\\\in [m]} \\\\gamma \\\\sigma \\\\epsilon_{T} \\\\epsilon_{i}(j) \\\\right|$. By the concentration of the norm of the Gaussian random vector, $$\\\\Pr \\\\left[ \\\\max_{i} \\\\| \\\\epsilon_{i} \\\\| \\\\geq \\\\sqrt{d} + 10 \\\\ln(k) \\\\right] \\\\leq 2 \\\\frac{k}{9},$$\\n\\n$$\\\\leq 10^{-7}.$$ Now, define $u_{i} = \\\\frac{\\\\epsilon_{i}}{\\\\| \\\\epsilon_{i} \\\\|}$. Therefore, $$\\\\max_{i} \\\\epsilon_{T} \\\\epsilon_{i}(j) \\\\leq \\\\sqrt{d} \\\\max_{j} \\\\|u_{i}\\\\| \\\\leq \\\\frac{1}{100} \\\\frac{1}{k^{2}} \\\\frac{1}{d^{2}} \\\\max_{j} \\\\|u_{i}\\\\| \\\\leq \\\\frac{1}{8},$$\\n\\n(95)\\n\\nwhere $\\\\epsilon_{j} \\\\sim N(0, 1)$. The last equality in distribution follows from the fact the sum of $k$ independent Gaussian random variables is a Gaussian random variable with variance $k$. Now, from the concentration inequality,$$\\n\\\\Pr \\\\left[ \\\\max_{j} \\\\| \\\\epsilon_{j} \\\\| \\\\geq \\\\sqrt{d} + 10 \\\\ln(k) \\\\right] \\\\leq 2 \\\\frac{k}{9},$$\\n\\n$$\\\\leq 10^{-7}.$$ Substituting this above,\\n\\n$$\\\\max_{i} \\\\epsilon_{T} \\\\epsilon_{i}(j) \\\\leq \\\\sqrt{d} \\\\max_{j} \\\\|u_{i}\\\\| \\\\leq \\\\frac{1}{100} \\\\frac{1}{k^{2}} \\\\frac{1}{d^{2}} \\\\max_{j} \\\\|u_{i}\\\\| \\\\leq \\\\frac{1}{8},$$\\n\\n(96)\\n\\nSubstituting 94 and 99 into 93, we can now bound $\\\\alpha_{j}$, for all $j \\\\in [m],$$$\\\\frac{5}{4} \\\\geq \\\\alpha_{j} \\\\geq \\\\frac{3}{4},$$\\n\\n(100)\\n\\n(101)\"}"}
{"id": "AfnsTnYphT", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We are now in the position to analyze\\n\\n\\\\[ \\\\tilde{w} = w - \\\\frac{1}{k} \\\\tilde{d} \\\\]\\n\\nAnd similarly, we lower bound\\n\\n\\\\[ \\\\| \\\\tilde{w} \\\\| \\\\geq \\\\epsilon \\\\]\\n\\nwhere\\n\\n\\\\[ \\\\bar{\\\\epsilon} \\\\]\\n\\nNow substituting the facts\\n\\n\\\\[ k \\\\geq 10 \\\\]\\n\\nNext, we lower bound\\n\\n\\\\[ w_T \\\\star \\\\]\\n\\nAlso note that by using Chernoff and Union bounds, we have\\n\\n\\\\[ m \\\\epsilon \\\\]\\n\\n\\\\[ k \\\\gamma \\\\]\\n\\n\\\\[ (\\\\tilde{w} - w_T) \\\\]\\n\\n\\\\[ \\\\| \\\\tilde{w} \\\\| \\\\geq \\\\| w \\\\| \\\\]\\n\\n\\\\[ \\\\| \\\\tilde{w} \\\\| = \\\\epsilon \\\\]\\n\\n\\\\[ \\\\tilde{d} \\\\]\\n\\nFor this, we first prove an upper bound for\\n\\n\\\\[ \\\\| \\\\tilde{d} \\\\| \\\\]\\n\\n\\\\[ k \\\\sigma \\\\]\\n\\n\\\\[ k \\\\alpha \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ (109) \\\\]\\n\\n\\\\[ (110) \\\\]\\n\\n\\\\[ (111) \\\\]\\n\\n\\\\[ (112) \\\\]\\n\\n\\\\[ (113) \\\\]\\n\\n\\\\[ (114) \\\\]\\n\\n\\\\[ (115) \\\\]\\n\\n\\\\[ (116) \\\\]\\n\\n\\\\[ (117) \\\\]\\n\\n\\\\[ (118) \\\\]\\n\\n\\\\[ (119) \\\\]\\n\\n\\\\[ (120) \\\\]\\n\\n\\\\[ (121) \\\\]\\n\\n\\\\[ (122) \\\\]\\n\\n\\\\[ (123) \\\\]\"}"}
