{"id": "3DPTnFokLp", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 11: Level 5 comparison of accuracies for the 4 models trained on our CdSprites+ dataset.\\n\\n| Model (Dim)       | Txt\u2192Img Strict % | Feats % | Img\u2192Txt Strict % | Feats % | Letters % |\\n|-------------------|-------------------|---------|------------------|---------|-----------|\\n| MMV AE (16-D)     | 0 (0)             | 1.8 (0.0)/5 | 0 (0)            | 0.4 (0.2)/5 | 16 (0)    |\\n| MV AE (16-D)      | 0 (0)             | 1.8 (0.0)/5 | 0 (0)            | 0.6 (0.0)/5 | 27 (1)    |\\n| DMV AE (30-D)     | 0 (0)             | 1.8 (0.0)/5 | 0 (0)            | 0.6 (0.1)/5 | 18 (2)    |\\n| MoPoE (16-D)      | 0 (0)             | 1.8 (0.0)/5 | 0 (0)            | 0.3 (0.2)/5 | 15 (1)    |\\n| MMV AE (24-D)     | 0 (0)             | 1.8 (0.0)/5 | 0 (0)            | 0.6 (0.1)/5 | 17 (2)    |\\n| MV AE (24-D)      | 0 (0)             | 1.8 (0.0)/5 | 0 (0)            | 0.6 (0.0)/5 | 25 (3)    |\\n| DMV AE (36-D)     | 1 (0)             | 1.8 (0.0)/5 | 0 (0)            | 0.6 (0.1)/5 | 14 (0)    |\\n| MoPoE (24-D)      | 0 (0)             | 1.8 (0.0)/5 | 0 (0)            | 0.7 (0.0)/5 | 17 (1)    |\\n| MMV AE (32-D)     | 0 (0)             | 1.8 (0.0)/5 | 0 (0)            | 0.4 (0.1)/5 | 15 (0)    |\\n| MV AE (46-D)      | 0 (0)             | 1.8 (0.0)/5 | 0 (0)            | 0.6 (0.1)/5 | 24 (2)    |\\n| DMV AE (32-D)     | 0 (0)             | 1.8 (0.0)/5 | 0 (0)            | 0.4 (0.1)/5 | 14 (1)    |\\n| MoPoE (32-D)      | 0 (0)             | 1.8 (0.0)/5 | 0 (0)            | 0.7 (0.3)/5 | 17 (2)    |\\n\\nFigure 4: T-SNE visualizations for the MV AE model's (16-D) joint latent space trained on CdSprites+. We show the latent space for each of the 4 features (size, shape, position and colour) individually.\"}"}
{"id": "3DPTnFokLp", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: T-SNE visualizations for the MMV AE model\u2019s (24-D) unimodal latent spaces trained on CdSprites+ level 4. We show the latent space for each of the 4 features (size, shape, position and colour) individually.\\n\\nFigure 6: PCA calculated on the images in our CdSprites+ dataset, Level 5. We show a separate figure for each of the 5 features (size, shape, position and colour).\"}"}
{"id": "3DPTnFokLp", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Image traversals for the MMV AE and MoPoE models for the CdSprites+ Levels 2 and 4. Each row is one out of 32 dimensions of the latent space, each column is the single sampled vector from the traversal range (-6,6).\\n\\nFigure 8: Text traversals for the MV AE and MMV AE models for the CdSprites+ Level 4. Each row is one out of 32 dimensions of the latent space, each column is the single sampled vector from the traversal range (-6,6). Note that we did not set the desired length of the text output, the model thus always generated the maximum number of characters.\"}"}
{"id": "3DPTnFokLp", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Example of an automated visualization generated by our toolkit. We show accuracies for the \\\\textit{Txt} \\\\rightarrow \\\\textit{Img} (left) and \\\\textit{Img} \\\\rightarrow \\\\textit{Txt} (right) cross-generations for CdSprites+ Level 1 dataset based on the used latent dimensionality. We compare the 4 implemented models. Note that for DMV AE, we trained with different dimensionalities as there are private and shared latent variables.\\n\\n### A.3 Verifying correctness of model implementation\\n\\nTo verify the correctness of our implementation for each model, we have reproduced selected experiments from the original papers using our toolkit. We provide both the original and our results below.\\n\\n#### A.3.1 MMVAE\\n\\nTo verify that our implementation of the MMV AE [7] model is correct, we reproduced the experiments using the MNIST-SVHN dataset. We used the same model configuration and parameters as in the original report, i.e. the Mixture-of-Experts mixing with the DREG training objective, latent size 20 and 30 samples drawn from the joint posterior and the likelihood scaling for each modality was adjusted according to the varying dimensionalities. We used the same encoder and decoder architectures as in the original paper. After training, we calculated the joint- and cross-coherencies using the adapted original evaluation script (please see the original paper for the evaluation details [7]). The results are shown in Table 12, the config files for reproducing the experiment are also provided on our GitHub. Please note that the results in Table 12 are different from those depicted in the main paper, Table 3. This is because here we unified the training hyperparameters with the original paper setup. However, we found that setting the likelihood scaling to 1 for both modalities produces more balanced results (in terms of MNIST/SVHN accuracies) and used thus this setup for the comparative study.\\n\\n#### A.3.2 MVAE\\n\\nIn the original MV AE paper [5], the results are reported in terms of marginal log-likelihoods. We reproduced the FashionMNIST experiment with a 64-D latent space, batch size 100, and likelihood scaling of 10 for the labels and 1 for the images, as reported in the public code. The results can be seen in Table 13.\\n\\n#### A.3.3 MoPoE\\n\\nFor verification that the MoPoE model is correct, the reproduction was performed on the PolyMNIST dataset. Based on the original implementation, we used the 512-D latent space, Laplace prior distributions, and $\\\\beta = 2.5$. After training, we calculated the cross-coherencies conditioned on 1, 2, 3 or 4 modalities as reported in the paper. The results are shown in Table 14.\\n\\n#### A.3.4 DMVAE\\n\\nWe reproduced the MNIST-SVHN experiment for the DMV AE model. The reproduced model configuration included shared latent dimensionality $\\\\text{Dim}_{\\\\text{shared}} = 10$, the private latent dimensionalities $\\\\text{Dim}_{\\\\text{private}} = 19$, and the shared latent dimensionality $\\\\text{Dim}_{\\\\text{shared}} = 10$. The results can be seen in Table 15.\"}"}
{"id": "3DPTnFokLp", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multimodal Variational Autoencoders (VAEs) have been the subject of intense research in the past years as they can integrate multiple modalities into a joint representation and can thus serve as a promising tool for both data classification and generation. Several approaches toward multimodal VAE learning have been proposed so far, their comparison and evaluation have however been rather inconsistent. One reason is that the models differ at the implementation level, another problem is that the datasets commonly used in these cases were not initially designed to evaluate multimodal generative models. This paper addresses both mentioned issues. First, we propose a toolkit for systematic multimodal VAE training and comparison. The toolkit currently comprises 4 existing multimodal VAEs and 6 commonly used benchmark datasets along with instructions on how to easily add a new model or a dataset. Second, we present a disentangled bimodal dataset designed to comprehensively evaluate the joint generation and cross-generation capabilities across multiple difficulty levels. We demonstrate the utility of our dataset by comparing the implemented state-of-the-art models.\\n\\nINTRODUCTION\\n\\nVariational Autoencoders (VAEs) (Kingma & Welling, 2014) have become a multipurpose tool applied to various machine perception tasks and robotic applications over the past years (Pu et al., 2016)(Xu et al., 2017)(Nair et al., 2018). For example, some of the recent implementations address areas such as visual question answering (Chen et al., 2019), visual question generation (Uppal et al., 2021) or emotion recognition (Yang & Lee, 2019). Recently, VAEs were also extended for the integration of multiple modalities, enabling mapping different kinds of inputs into a joint latent space. It is then possible to reconstruct one modality from another or to generate semantically matching pairs, provided the model successfully learns the semantic overlap among them.\\n\\nSeveral different methods for joint multimodal learning have been proposed so far and new models are still being developed (Wu & Goodman, 2018)(Shi et al., 2019)(Sutter et al., 2021). Two of the widely recognized and compared approaches are the MVAE model (Wu & Goodman, 2018), utilizing the Product of Experts (PoE) model to learn a single joint distribution of the joint posterior, and MMV AE (Shi et al., 2019), using a Mixture of Experts (MoE). Sutter et al. (2021) also recently proposed a combination of these two architectures referred to as Mixture-of-Products-of-Experts (MoPoE-VAE), which approximates the joint posterior on all subsets of the modalities using a generalized multimodal ELBO.\\n\\nThe versatility of these models (i.e. the possibility to classify, reconstruct and jointly generate data using a single model) naturally raises a dispute on how to assess their generative qualities. Wu & Goodman (2018) used test set log-likelihoods to report their results, Shi et al. (2019) proposed four criteria that measure the coherence, synergy and latent factorization of the models using various qualitative and quantitative (usually dataset-dependent) metrics. Evaluation for these criteria was originally performed on several multimodal benchmark datasets such as MNIST (Deng, 2012), CelebA (Liu et al., 2015), MNIST and SVHN combination (Shi et al., 2019) or the Caltech-UCSD Birds (CUB) dataset (Wah et al., 2011). All of the mentioned datasets comprise images paired either with labels (MNIST, CelebA), other images (MNIST-SVHN, PolyMNIST) or text (CelebA, CUB).\\n\\nSince none of these datasets was designed specifically for the evaluation of multimodal integration and semantic coherence of the generated samples, their usage is in certain aspects limited.\"}"}
{"id": "3DPTnFokLp", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"specifically, datasets comprising real-world images/captions (such as CUB or CelebA) are highly noisy, often biased and do not enable automated evaluation. This makes these datasets unsuitable for detailed analysis of the model's individual capabilities such as joint generation, cross-generation or disentanglement of the latent space. On the other hand, simpler datasets like MNIST and SVHN do not offer different levels of complexity, provide only a few categories (10-digit labels) and cannot be used for generalization experiments.\\n\\nDue to the above-mentioned limitations of the currently used benchmark datasets and also due to a high number of various implementations, objective functions and hyperparameters that are used for the newly developed multimodal VAEs, the conclusions on which models outperform the others substantially differ among the authors (Shi et al., 2019), (Kutuzova et al., 2021), (Daunhawer et al., 2022). Recently, Daunhawer et al. (2022) published a comparative study of multimodal VAEs where they conclude that new benchmarks and a more systematic approach to their evaluation are needed.\\n\\nIn this paper, we propose a unification of the various implementations into a single toolkit which enables training, evaluation and comparison of the state-of-the-art models and also faster prototyping of new methods due to its modularity. The toolkit is written in Python and enables the user to quickly train and test their model on arbitrary data with an automatic hyperparameter grid search. By default, the toolkit includes 6 currently used datasets that have been previously used to compare these models. Moreover, we include our new synthetic dataset that enables a more systematic evaluation called CdSprites+ and is built on top of the existing dSprites (disentangled Sprites) dataset used to assess the capabilities of unimodal VAEs (Matthey et al., 2017). While dSprites comprises grayscale images of 3 geometric shapes of various sizes, positions and orientations, CdSprites+ also includes natural language captions as a second modality and 5 different difficulty levels based on the varying number of features (i.e. varying shapes for Level 1 and varying shapes, sizes, colours, image quadrants and backgrounds in Level 5). CdSprites+ enables fast data generation, easy evaluation and also a gradual increase of complexity to better estimate the progress of the tested models. It is described in greater detail in Section 3. For a brief comparison of the benchmark dataset qualities, see also Table 1.\\n\\nIn conclusion, the contributions of this paper are following:\\n\\n1. We propose a public toolkit which enables systematic development, training and evaluation of the state-of-the-art multimodal VAEs and their comparison on the commonly used multimodal datasets.\\n\\n2. We provide a synthetic image-text dataset called CdSprites+ designed specifically for the evaluation of the generative capabilities of multimodal VAEs on 5 levels of complexity.\\n\\nThe toolkit and code for the generation of the dataset (as well as a download link for a ready-to-use version of the dataset) are available on GitHub.\\n\\nIn this section, we first briefly describe the state-of-the-art multimodal variational autoencoders and how they are evaluated, then we focus on datasets that have been used to demonstrate the models\u2019 capabilities.\\n\\n2.1 Multimodal VAEs and Evaluation\\n\\nMultimodal VAEs are an extension of the standard Variational Autoencoder (as proposed by Kingma & Welling (2014)) that enables joint integration and reconstruction of two or more modalities. During the past years, a number of approaches toward multimodal integration have been presented (Suzuki et al., 2016), (Wu & Goodman, 2018), (Shi et al., 2019), (Vasco et al., 2020), (Sutter et al., 2021), (Joy et al., 2022). For example, the model proposed by Suzuki et al. (2016) learns the joint multimodal probability through a joint inference network and instantiates an additional inference network for each subset of modalities. A more scalable solution is the MVAE model (Wu & Goodman, 2018), where the joint posterior distribution is approximated using the product of experts (PoE), exploiting the modularity of the model.\"}"}
{"id": "3DPTnFokLp", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the fact that a product of individual Gaussians is itself a Gaussian. In contrast, the MMV AE approach (Shi et al., 2019) uses a mixture of experts (MoE) to estimate the joint variational posterior based on individual unimodal posteriors. The MoPoE architecture from Sutter et al. (2021) combines the benefits of PoE and MoE approaches by computing the joint posterior for all subsets of modalities. Another recent extension is the DMV AE model, in which the authors enable the encoders to separate the shared and modality-specific features in the latent space for a disentangled multimodal VAE (Lee & Pavlovic, 2021). While there are also other recently proposed multimodal VAEs (Sutter et al. (2020), Daunhawer et al. (2021), Liang et al. (2022), Palumbo et al. (2023)), in this paper, we highlight the 4 abovementioned models that can be considered representative of the individual approaches as summarized by Suzuki & Matsuo (2022). For example, MMV AE does not learn the joint posterior distribution (while the 3 remaining models do), MoPoE is scalable to multiple modalities only under very high computational costs (compared to the other models) and DMV AE is the only model that learns modality-specific (private) latent variables.\\n\\nThe evaluation of the above-mentioned models has also evolved over time. Wu & Goodman (2018) measured the test marginal, joint and conditional log-likelihoods together with the variance of log importance weights. Shi et al. (2019) proposed four criteria for evaluation of the generative capabilities of multimodal VAEs:\\n\\n- **Coherent joint generation**\\n- **Coherent cross-generation**\\n- **Latent factorisation**\\n- **Synergy**\\n\\nAll criteria are evaluated both qualitatively (through empirical observation of the generated samples) and quantitatively: by adopting pre-trained classifiers for evaluation of the generated content, by training a classifier on the latent vectors to test whether the classes are distinguishable in the latent space, or by calculating the correlation between the jointly and cross-generated samples using the Canonical Correlation Analysis (CCA).\\n\\nBesides certain dataset-dependent alternatives, the most recent papers use a combination of the above-mentioned metrics for multimodal VAE comparison (Sutter et al., 2021), (Joy et al., 2022) (Daunhawer et al., 2022), (Kutuzova et al., 2021). Despite that, the conclusions on which model performs the best according to these criteria substantially differ. According to a thorough comparative study from Daunhawer et al. (2022), none of the current multimodal VAEs sufficiently fulfils all of the four criteria specified by Shi et al. (2019). Furthermore, the optima of certain training hyperparameters might be different for each model (as was proven e.g. with the regularisation parameter $\\\\beta$ (Daunhawer et al., 2022)), which naturally raises the need for automated and systematic comparison of these models over a large number of hyperparameters, datasets and training schemes. Sutter et al. (2021) released public code which allows comparison of the MoE, PoE, MoPoE approaches - however, the experiments are dataset-dependent and an extension for other data types would thus require writing a substantial amount of new code.\\n\\nIn this paper, we propose a publicly available toolkit for systematic training, evaluation and comparison of the state-of-the-art multimodal VAEs. Special attention is paid to hyperparameter grid search and automatic visualizations of the learning during training. To our knowledge, it is the only model- and dataset-agnostic tool available in this area that would allow fast implementation of new approaches and their testing on arbitrary types of data.\\n\\n2.2 **Multimodal Datasets**\\n\\nAt this time, there are several benchmark datasets commonly used for multimodal VAE evaluation. The majority is bimodal, where one or both modalities are images. In some of the datasets, the bimodality is achieved by splitting the data into images as one modality and the class labels as the second - this simplifies the evaluation during inference, yet such dataset does not enable e.g. testing the models for generalization capabilities and the overall number of classes is usually very low. Examples of such datasets are MNIST (Deng, 2012), FashionMNIST (Xiao et al., 2017) or MultiMNIST (Sabour et al., 2017). Another class are image-image datasets such as MNIST and SVHN (Netzer et al., 2011) (as used in Shi et al. (2019)), where the content is semantically identical and the only difference is the form (i.e. handwritten digits vs. house numbers). This can be seen as integrating two identical modalities with different amounts of noise - while such task might be relevant for some applications, it does not evaluate whether the model can integrate multiple modalities with completely different data representations.\\n\\nAn example of a bimodal dataset with an image-text combination is the CelebA dataset containing real-world images of celebrities and textual descriptions of up to 40 binary attributes (such as male, female, etc.).\"}"}
{"id": "3DPTnFokLp", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The presented version of the benchmark dataset comprises 5 different levels of difficulty, where each level varies in the number of included features (see Table 5 for their overview). The default size of the dataset is depicted in Table 5 for each level, although the user can easily generate a larger set of the data. The scripts for calculation of the cross- and joint-coherency accuracies use separate batches of testing data provided in the toolkit.\\n\\nIn all levels, the source of noise in the images is their random position and rotation - in levels 1-4, the shapes are located around the whole image with a variance of 25 pixels along both $x$ and $y$ axes. In Level 5, the shapes are shifted in random quadrants where their position also varies with a variance of 8 pixels. The positions are configured so that the whole shapes are always fitting the image. In Levels 2-5 where we vary the size, the proportion of the small objects to the big objects is 1:5.\\n\\nYou can also see a PCA visualization of the CdSprites+ Level 5 dataset in Fig. 6.\\n\\nTable 5: Statistics of the CdSprites+ benchmark dataset. We show the number of train/validation samples and the number of various shapes, colors, object poses (meaning quadrants which are distinguished in captions) and backgrounds used in each difficulty level. The text captions only describe features that vary (e.g. in level 1, the text descriptions only include the shape name). The colors and backgrounds are all textured when they vary.\\n\\n| Level | Train Samples | Validation Samples | Shapes | Sizes | Colors | Positions | Backgrounds |\\n|-------|---------------|--------------------|--------|-------|--------|-----------|------------|\\n| 1     | 67            | 500                | 7      | 500   | 3      | 1         | 1          |\\n| 2     | 108,000       | 12,000             | 12     | 12    | 3      | 2         | 1          |\\n| 3     | 270,000       | 30,000             | 30     | 30    | 3      | 2         | 5          |\\n| 4     | 540,000       | 60,000             | 60     | 60    | 3      | 4         | 4          |\\n| 5     | 864,000       | 96,000             | 96     | 96    | 3      | 4         | 2          |\\n\\nA.1.1 Using Character-Wise Embeddings\\n\\nWe choose to use character-wise embeddings for CdSprites+ rather than word embeddings. While this choice was made to increase the difficulty of the text modality in our dataset, character embeddings have been recently used also in several other works as this approach brings specific advantages. Firstly, character-wise embedding does not require a pre-defined vocabulary of possible input words. This can be useful e.g. in incremental learning scenarios where the whole vocabulary is not known prior to training beginning. Secondly, the model can be tested for robustness after training by inputting sentences with misspelt words (e.g., \u201csqaare\u201d instead of \u201csquare\u201d) to see if the model can generate correct images. With word-level embedding, this is not possible as replacing entire words will change the feature or create a nonsensical query (e.g., \u201cleft square\u201d instead of \u201cblue square\u201d).\\n\\nPlease note that we expect the users to use the same encoder and decoder networks (i.e. character transformer networks) for the CdSprites+ benchmark to provide a restricted and fair comparison to other models. Should the users want to use CdSprites+ outside our toolkit for their custom evaluation, they can as well use word-level embeddings as we provide raw strings for the CdSprites+ text modality.\\n\\nA.2 Benchmark Study Results\\n\\nHere we provide the specific training configuration and hyperparameters used for the experiments on the CdSprites+ dataset as listed in the paper. We also report the detailed results for hyperparameter grid search in terms of the cross- and joint-generation accuracies.\\n\\nA.2.1 Training Configuration\\n\\nAll our experiments were trained with the GeForce GTX 1080 and NVIDIA Tesla V100 GPU cards, the mean computation times for training and inference are shown in Table 6. We used the Adam optimizer, the learning rate of $1 \\\\times 10^{-4}$ and all experiments were repeated for 5 seeds (we report standard deviations for the results in the tables). We trained for 150 epochs for Levels 1 and 2 and for 250 epochs in the case of Levels 3-5. In the hyperparameter grid search, we varied the latent dimensionality (16, 24, 32) for all 5 dataset levels and the MV AE, MMV AE and MoPoE models. In the case of DMV AE, the latent dimensionality was different as there are private (modality-dependent) and shared latents. We thus chose different values for the comparison. We used a fixed value of 10 for both private latents and varied the shared latents with values 10, 16 and 24.\"}"}
{"id": "3DPTnFokLp", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: Training and inference times for each model trained on our CdSprites+ dataset. The models were trained for 150 epochs on Levels 1-2 and for 250 epochs on Levels 3-5, we thus show these times separately. We show the mean values over all seeds and different latent dimensionalities, the standard deviation is shown as \u00b1.\\n\\n| Model       | Per epoch (s) Levels 1-2 | Per training (min) Levels 1-2 | Inference time (s) Levels 3-5 | Per training (min) Levels 3-5 |\\n|-------------|---------------------------|-------------------------------|-------------------------------|-------------------------------|\\n| MMV AE      | 203 \u00b1 2                  | 425 \u00b1 8                       | 860 \u00b1 10                      | 152 \u00b1 6                      |\\n| MV AE       | 150 \u00b1 20                 | 397 \u00b1 25                      | 645 \u00b1 32                      | 135 \u00b1 19                     |\\n| MoPoE       | 127 \u00b1 12                 | 324 \u00b1 9                       | 542 \u00b1 10                      | 126 \u00b1 11                     |\\n| DMV AE      | 200 \u00b1 3                  | 500 \u00b1 6                       | 841 \u00b1 7                       | 148 \u00b1 8                      |\\n\\nshow this as the total number of latent dimensions, i.e. 30 (10 shared and 2\u00d710 private), 36 (16 shared and 2\u00d710 private) and 46 (24 shared and 2\u00d710 private).\\n\\nWe used the default training dataset size and validation split as reported in the statistics Table 5. In Tables 7, 8, 9, 10 and 11, we show results for the MV AE, MMV AE, DMV AE and MoPoE models and the compared latent dimensionalities. Standard deviations over 5 seeds are shown in brackets.\\n\\nA.2.2 USED ARCHITECTURE\\nFor all evaluated models, we used the standard ELBO loss function with the \u03b2 parameter fixed to 1. For the MV AE [5] model, we used the sub-sampling approach where the model is trained on all subsets of modalities (i.e. images only, text only and images+text). For the image encoder and decoder, we used 4 fully connected layers with ReLU activations. In the case of the text, we used a Transformer network with 8 layers, 2 attention heads, 1024 hidden features and a dropout of 0.1.\\n\\nA.2.3 EVALUATION METRICS\\nAfter training, we used the script for automated evaluation (provided in our toolkit) to compute the cross- and joint-coherency of the models. For cross-coherency, we generated a 10000-sample test dataset using the dataset generator and used first the images, and then captions as input to the model to reconstruct the missing modality. For joint coherency, we generated 1000 traversal samples over each dimension of the latent space (i.e. 32000 samples for a 32-D latent space) and fed these latent vectors into the models to reconstruct both captions and images.\\n\\nFor both the cross- and joint-coherencies, we report the following metrics: Strict, Feat, and Letters to provide more information on what the models are capable to do. In the first (Strict) metrics, we considered the text sample as accurate only if all letters in the description were 100% accurate, i.e. we did not tolerate any noise. For the image outputs, we considered the images as correct only if all the attributes for the given difficulty level could be detected using our pre-trained classifiers (i.e. correct classification for the shape, colour, size, position or background). For joint coherency, we considered the generated pair as correct only when both the image and captions fulfilled these criteria and were semantically matching.\\n\\nFor the feature-level metrics, we calculated the percentage of correctly reconstructed/generated features (e.g. whole words or image attributes such as shape) and reported the mean percentage of correct features per sample. For the image-caption cross-generation accuracy, we also calculated the average percentage of correct letters per output sample.\\n\\nIn the following section, we report the mean accuracies for both cross- and joint-coherency - these numbers describe the proportion of the correct outputs to all outputs.\\n\\nA.2.4 DETAILED RESULTS\\nIn Tables 7, 8, 9, 10 and 11, we show the comparison of the MV AE, MMV AE, DMV AE and MoPoE models on the 5 difficulty levels of the CdSprites+ dataset. Here we varied the latent dimensionality (16-D to 32-D) with the fixed batch size of 32. The values are the mean cross-generation and joint-generation accuracies over 5 seeds with the standard deviations listed in brackets. According to the Strict metrics (with zero noise tolerance, see Sec. A.2.3), all models failed in both tasks at Levels 4 and 5. The Feature and Letter accuracies significantly decrease across levels as the complexity increases. You can see the T-SNE visualizations for the MV AE and MMV AE models trained on Level 4 in Figs. 4 and 5.\"}"}
{"id": "3DPTnFokLp", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"Table 7: Level 1 comparison of accuracies for the four evaluated models trained on our CdSprites+ dataset.\\n\\n| Model         | Dim (Tx \u2192 Img) | Strict % | Feats % | Dim (Img \u2192 Txt) | Strict % | Feats % | Letters % | Dim (Joint) | Strict % | Feats % |\\n|---------------|----------------|----------|---------|-----------------|----------|---------|-----------|-------------|----------|---------|\\n| MMV AE (16-D) | 47 (14)        | N/A      | 64 (3)  | N/A             | 88 (2)   | 17 (10) | N/A       | N/A         | N/A      | N/A     |\\n| MV AE (16-D)  | 52 (3)         | N/A      | 63 (8)  | N/A             | 86 (2)   | 5 (9)   | N/A       | N/A         | N/A      | N/A     |\\n| DMV AE (30-D) | 33 (4)         | N/A      | 4 (5)   | N/A             | 25 (2)   | 4 (6)   | N/A       | N/A         | N/A      | N/A     |\\n| MoPoE (16-D)  | 33 (3)         | N/A      | 10 (17) | N/A             | 26 (7)   | 16 (27) | N/A       | N/A         | N/A      | N/A     |\\n| MMV AE (24-D) | 55 (15)        | N/A      | 42 (3)  | N/A             | 31 (12)  | 0 (0)   | N/A       | N/A         | N/A      | N/A     |\\n| MV AE (24-D)  | 55 (4)         | N/A      | 61 (3)  | N/A             | 82 (1)   | 3 (2)   | N/A       | N/A         | N/A      | N/A     |\\n| DMV AE (36-D) | 36 (1)         | N/A      | 3 (3)   | N/A             | 21 (2)   | 9 (13)  | N/A       | N/A         | N/A      | N/A     |\\n| MoPoE (24-D)  | 35 (3)         | N/A      | 4 (2)   | N/A             | 24 (6)   | 1 (1)   | N/A       | N/A         | N/A      | N/A     |\\n| MMV AE (32-D) | 48 (3)         | N/A      | 36 (2)  | N/A             | 26 (2)   | 0 (0)   | N/A       | N/A         | N/A      | N/A     |\\n| MV AE (32-D)  | 53 (5)         | N/A      | 60 (2)  | N/A             | 82 (2)   | 1 (1)   | N/A       | N/A         | N/A      | N/A     |\\n| DMV AE (46-D) | 34 (2)         | N/A      | 3 (2)   | N/A             | 20 (9)   | 0 (0)   | N/A       | N/A         | N/A      | N/A     |\\n| MoPoE (32-D)  | 36 (5)         | N/A      | 2 (1)   | N/A             | 23 (7)   | 0 (0)   | N/A       | N/A         | N/A      | N/A     |\\n\\nFigure 3: Results for the MV AE and MMV AE models trained on the MNIST-SVHN dataset using our toolkit. For MMV AE, we used the DREG objective as proposed by the authors, MV AE was trained with ELBO. We used the encoder and decoder networks from the original implementations. The top figures are traversals for each modality, below we show cross-generated samples. The bottom figures are T-SNE visualizations of the latent space - please note that for MV AE we show samples from the single joint posterior, while for MMV AE we show samples for both modality-specific distributions.\"}"}
{"id": "3DPTnFokLp", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model (Dim) | Txt $\\\\rightarrow$ Img | Strict % | Txt $\\\\rightarrow$ Img | Feats % | Img $\\\\rightarrow$Txt | Strict % | Img $\\\\rightarrow$Txt | Feats % | Letters % | Joint Strict % | Joint Feats % |\\n|------------|-------------------------|----------|-------------------------|---------|-----------------------|----------|-----------------------|---------|------------|----------------|---------------|\\n| MMV AE (16-D) | 18 (4) | 0.8 (0.1)/2 | 2 41 (20) | 1.4 (0.2)/2 | 85 (4) | 3 (3) | 0.6 (0.1)/2 |\\n| MV AE (16-D) | 16 (1) | 0.8 (0.0)/2 | 55 (27) | 1.5 (0.3)/2 | 91 (6) | 1 (1) | 0.3 (0.3)/2 |\\n| DMV AE (30-D) | 15 (2) | 0.8 (0.0)/2 | 4 (1) | 0.4 (0.0)/2 | 30 (2) | 0 (0) | 0.2 (0.1)/2 |\\n| MoPoE (16-D) | 10 (3) | 0.8 (0.0)/2 | 8 (7) | 0.7 (0.1)/2 | 40 (4) | 1 (1) | 0.2 (0.1)/2 |\\n| MMV AE (24-D) | 17 (5) | 0.4 (0.0)/2 | 16 (0) | 0.4 (0.0)/2 | 40 (2) | 1 (1) | 0.2 (0.0)/2 |\\n| MV AE (24-D) | 16 (3) | 0.4 (0.0)/2 | 52 (9) | 0.8 (0.0)/2 | 86 (1) | 5 (6) | 0.3 (0.0)/2 |\\n| DMV AE (36-D) | 18 (2) | 0.9 (0.0)/2 | 5 (1) | 0.4 (0.0)/2 | 24 (1) | 0 (0) | 0.2 (0.2)/2 |\\n| MoPoE (24-D) | 8 (3) | 0.8 (0.0)/2 | 13 (3) | 0.8 (0.1)/2 | 35 (3) | 1 (1) | 0.5 (0.1)/2 |\\n| MMV AE (32-D) | 17 (1) | 0.4 (0.0)/2 | 16 (0) | 0.5 (0.0)/2 | 43 (2) | 0 (0) | 0.1 (0.0)/2 |\\n| MV AE (32-D) | 16 (4) | 0.8 (0.1)/2 | 40 (13) | 1.8 (0.1)/2 | 87 (1) | 11 (9) | 0.8 (0.0)/2 |\\n| DMV AE (46-D) | 17 (1) | 0.8 (0.0)/2 | 3 (1) | 0.4 (0.1)/2 | 24 (2) | 0 (0) | 0.1 (0.1)/2 |\\n| MoPoE (32-D) | 7 (2) | 0.8 (0.0)/2 | 10 (8) | 0.8 (0.1)/2 | 33 (1) | 0 (0) | 0.3 (0.2)/2 |\\n| MMV AE (32-D) | 17 (1) | 0.4 (0.0)/2 | 16 (0) | 0.5 (0.0)/2 | 43 (2) | 0 (0) | 0.1 (0.0)/2 |\\n| MV AE (32-D) | 16 (4) | 0.8 (0.1)/2 | 40 (13) | 1.8 (0.1)/2 | 87 (1) | 11 (9) | 0.8 (0.0)/2 |\\n| DMV AE (46-D) | 17 (1) | 0.8 (0.0)/2 | 3 (1) | 0.4 (0.1)/2 | 24 (2) | 0 (0) | 0.1 (0.1)/2 |\\n| MoPoE (32-D) | 7 (2) | 0.8 (0.0)/2 | 10 (8) | 0.8 (0.1)/2 | 33 (1) | 0 (0) | 0.3 (0.2)/2 |\"}"}
{"id": "3DPTnFokLp", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nWhile such a dataset is far more complex, the qualitative evaluation of the generated samples is difficult and ambiguous due to the subjectiveness of certain attributes (attractive, young, wearing lipstick) combined with the usually blurry output images generated by the models. Another recently used image-text dataset is the Caltech-UCSD Birds (CUB) dataset (Wah et al., 2011) comprising real-world bird images accompanied with manually annotated text descriptions (e.g. this bird is all black and has a long pointy beak). However, these images are too complex to be generated by the state-of-the-art models (as proven by Daunhawer et al. (2022)) and the authors thus only use their features and perform the nearest-neighbour lookup to match them with an actual image (Shi et al., 2019). This also makes it impossible to test the models for generalization capabilities (i.e., making an image of a previously unseen bird).\\n\\nBesides the abovementioned datasets that have already been used for multimodal VAE comparison, there are also other multimodal (mainly image-text) datasets available such as Microsoft COCO (Lin et al., 2014) or Conceptual Captions (Sharma et al., 2018). Similar to CUB, these datasets include real-world images with human-made annotations, which can be used to estimate how the model would perform on real-world data. However, they cannot be used for deeper analysis and comparison of the models as they are very noisy (including typos, wrong grammar, many synonyms, etc.), subjective, biased or they require common sense (e.g., \u201cpiercing eyes\u201d, \u201cclouds in the sky\u201d or \u201cpowdered with sugar\u201d). This makes the automation of the semantic evaluation of the generated outputs difficult. A more suitable example would be the Multimodal3DIdent dataset (Daunhawer et al., 2023), comprising synthetic images with textual descriptions adapted from CLEVR (Johnson et al., 2017). However, this dataset does not have distinct levels of difficulty that would enable to distinguish what the models can learn and what is too challenging.\\n\\nIn conclusion, the currently used benchmark datasets for multimodal VAE evaluation are not optimally suited for benchmarking due to oversimplification of the multimodal scenario (such as the image-label combination or image-image translation where both images show digits), or, in the opposite case, due to overly complex modalities that are challenging to reconstruct and difficult to evaluate. In this paper, we address the lack of suitable benchmark datasets for multimodal VAE evaluation by proposing a custom synthetic dataset called CdSprites+ (Captioned disentangled Sprites +). This dataset extends the unimodal dSprites dataset (Matthey et al., 2017) with natural language captions, additional features (such as colours and textures) and 5 different difficulty levels. It is designed for fast data generation, easy evaluation and also for the gradual increase of complexity to better estimate the progress of the tested models. It is described in greater detail in Section 3. For a brief comparison of the benchmark dataset qualities, see also Table 1.\"}"}
{"id": "3DPTnFokLp", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\n(such as colours and textures) and 5 different difficulty levels. A ready-to-use version of the dataset can be downloaded on the link provided in our repository, the users can also modify and generate the data on their own using the code provided in our toolkit.\\n\\n3.1 DATASET STRUCTURE\\n\\nThe dataset comprises images of geometric shapes (64x64x3 pixels) with a defined set of 1 - 5 attributes (designed for a gradual increase of complexity) and their textual descriptions. The full variability covers 3 shape primitives (heart, square, ellipse), 2 sizes (big, small), 5 colours, 4 locations (top/bottom + left/right) and 2 backgrounds (dark/light), creating 240 possible unique feature combinations (see Fig. 1 for examples and Table 5 in the Appendix for the statistics and Section A.1 for further information). To avoid overfitting, the colours of the shapes are textured as well as the changing backgrounds in level 5. Another source of noise (besides textures) is the randomised positions and orientations of the shapes. The default version of the dataset consists of 75k (Level 1) 120k (Level 2), 300k (Level 3), 600k (Level 4) and 960k (Level 5), samples (where 10 % is used for validation).\\n\\nThe textual descriptions comprise 1 - 8 words (based on the selected number of attributes) which have a rigid order within the sentence (i.e. size, colour, shape, position and background colour). To make this modality challenging to learn, we do not represent the text at the word level, but rather at the character level. The text is thus represented as vectors $x_1, ..., x_N$ where $N$ is the number of characters in the sentence (the longest sentence has 45 characters) and $x_1,..,N$ are the one-hot encodings of length 27 (full English alphabet + space). The total sequence length is thus different for each textual description - this is automatically handled by the toolkit using zero-padding and the creation of masks (see Section 5). A PCA visualization of CdSprites+ Level 5 for both image and text modalities is shown in the Appendix in Fig. 6.\\n\\n3.2 SCALABLE COMPLEXITY\\n\\nTo enable finding the minimal functioning scenario for each model, the CdSprites+ dataset can be generated in multiple levels of difficulty - by the difficulty we mean the number of semantic domains the model has to distinguish (rather than the size/dimensionality of the modalities). Altogether, there are 5 difficulty levels (see Fig. 1):\\n\\n- **Level 1** - the model only learns the shape names (e.g. square)\\n- **Level 2** - the model learns the shape and its size (e.g. small square)\\n- **Level 3** - the model has to learn the shape, its size and textured colour (small red square)\\n- **Level 4** - the model also has to learn the position (small red square at top left)\\n- **Level 5** - the model learns also the background shade (small red square at top left on dark)\\n\\nWe provide a detailed description of how to configure and generate the dataset on our Github repository.\\n\\n3.3 AUTOMATED EVALUATION\\n\\nThe rigid structure and synthetic origin of the CdSprites+ dataset enable automatic evaluation of the coherence of the generated samples. The generated text can be tokenized and each word can be looked up in the dataset vocabulary based on its position in the sentence (for example, the first word always has to refer to the size in case of Level 5 of the dataset). The visual attributes of the generated images are estimated using pre-trained image classifiers specified for the given feature and difficulty level. Based on these metrics, it is possible to calculate the joint- and cross-generation accuracies for the given data pair - the image and text samples are considered coherent only if they correspond in all evaluated attributes. The fraction of semantically coherent (correct) generated pairs can thus serve as a percentual accuracy of the model.\\n\\nWe report the joint- and cross-generation accuracies on three levels: **Strict**, **Features** and **Letters** (only applicable for text outputs). The **Strict** metrics measure the percentage of completely correct samples, i.e. there is zero error tolerance (all letters and all features in the image must be correct). The **Features** metrics measure the ratio of correct features per sample (words for the text modality and visual features for the image modality), i.e. accuracy ratio 1.75(0.5)/5 on the level 5 means...\"}"}
{"id": "3DPTnFokLp", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of the currently used bimodal benchmark datasets for multimodal VAE evaluation. We compare the overall number of categories (we show the numbers for each level for CdSprites+), the dataset's overall size (we show the size of MNIST and the size of SVHN for MNIST-SVHN and the 5 Levels for CdSprites+), the domains of the two modalities, how the quality of the output can be evaluated (coherency is a more precise estimate compared to the vague canonical correlation analysis (CCA)) and the number of difficulty levels.\\n\\n| Dataset             | Categories | Size       | Data Domains | Qualitative Evaluation | Difficulty Levels |\\n|---------------------|------------|------------|--------------|------------------------|-------------------|\\n| MNIST               | 10 digits  | 60k image/label | coherency    | 1                      |\\n| FashionMNIST        | 10 articles | 70k image/label | coherency    | 1                      |\\n| MNIST-SVHN          | 10 digits  | 60k/600k image/image | coherency    | 1                      |\\n| CelebA              | 40 binary  | 200k image/labels | coherency    | 1                      |\\n| CUB                 | 200 bird types | 12k image/text | CCA         | 1                      |\\n| CdSprites+ (ours)   | 3/6/30/120/240 | 75/120/300/600/960k | image/text   | coherency 5 |\\n\\nthat on average 1.75 \u00b1 0.5 features out of 5 are recognized correctly for each sample, and Letters measures the average percentage of correct letters in the text outputs (see the Appendix Sec. A.2.3 for a detailed description of the metrics).\\n\\nOur proposed toolkit was developed to facilitate and unify the evaluation and comparison of multimodal VAEs. Due to its modular structure, the tool enables adding new models, datasets, encoder and decoder networks or objectives without the need to modify any of the remaining functionalities. It is also possible to train unimodal VAEs to see whether the multimodal integration distorts the quality of the generated samples.\\n\\nThe toolkit is written in Python, the models are defined and trained using the PyTorch Lightning library (Falcon & Team, 2019). For clarity, we directly state in the code which of the functions (e.g. various objectives) were taken from previous implementations so that the user can easily look up the corresponding mathematical expressions.\\n\\nCurrently, the toolkit incorporates the MV AE (Wu & Goodman, 2018), MMV AE (Shi et al., 2019), MoPoE-V AE (Sutter et al., 2021) and DMV AE (Lee & Pavlovic, 2021) models. All of the selected models are trained end-to-end and are able to handle missing modalities on the input, which we consider the basic requirement. You can see an overview of the main differences among the models adopted from Suzuki & Matsuo (2022) in the Appendix Table 16.\\n\\nThe datasets supported by default are MNIST-SVHN, CUB, CelebA, Sprites (a trimodal dataset with animated game characters), FashionMNIST, PolyMNIST and our CdSprites+ dataset. We also provide instructions on how to easily train the models on any new dataset. As for the encoder and decoder neural networks, we offer fine-tuned convolutional networks for image data (for all of the supported datasets) and several Transformer networks aimed at sequential data such as text, image sequences or actions (these can be robotic, human or any other kind of actions). All these components are invariant to the selected model and changing them requires only a change in the config file.\\n\\nAlthough we are continuously extending the toolkit with new models and functionalities, the main emphasis is placed on providing a tool that any scientist can easily adjust for their own experiments. The long-term effort behind the project is to make the findings on multimodal VAEs reproducible and replicable. We thus also have tutorials on how to add new models or datasets to the toolkit in our GitHub documentation.\\n\\n### 4.1 Experiment Configuration\\n\\nThe training setup and hyperparameters for each experiment can be defined using a YAML config file. Here the user can define any number and combination of modalities (unimodal training is also possible), modality-specific encoders and decoders, desired multimodal integration method,\"}"}
{"id": "3DPTnFokLp", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 EVALUATION METHODS\\n\\nWe provide both dataset-independent and dataset-dependent evaluation metrics. The dataset-independent evaluation methods are the estimation of the test log-likelihood, plots with the KL divergence and reconstruction losses for each modality, and visualizations of the latent space using T-SNE and latent space traversals (reconstructions of latent vectors randomly sampled across each dimension). Examples are shown in the Appendix.\\n\\nThe dataset-dependent methods focus on the evaluation of the 4 criteria specified by Shi et al. (2019): coherent joint generation, coherent cross-generation, latent factorisation and synergy. For qualitative evaluation, joint- and cross-generated samples are visualised during and after training. For quantitative analysis of the generated images, we provide the Fr\u00e9chet Inception Distance (FID) estimation. For our CdSprites+ dataset, we offer an automated qualitative analysis of the joint and cross-modal coherence - the simplicity and rigid structure of the dataset enable binary (correct/incorrect) evaluation for each generated image (with the use of pre-trained classifiers) and also for each letter/word in the generated text. For more details, see Section 3.3.\\n\\nTable 2: Marginal log-likelihoods (lower is better) for the four models trained on the CelebA dataset using our toolkit. Variance calculated over 3 seeds is shown in brackets.\\n\\n| Metric                  | MMV | AE | MV | AE | MoPoE | DMV | AE |\\n|-------------------------|-----|----|----|----|-------|-----|----|\\n| logp(x)                 | 6239.4 (1.2) | 6238.2 (1.6) | 6241.3 (1.8) | 6243.4 (1.3) |\\n| logp(x1, x2)            | 6236.3 (0.9) | 6238.8 (1.1) | 6242.4 (2.3) | 6239.7 (1.2) |\\n| logp(x1|x2)             | 6236.6 (1.6) | 6235.7 (1.3) | 6235.5 (1.2) | 6236.8 (1.4) |\\n\\n4.3 DATASETS FOR EVALUATION\\n\\nThe users have the following options to evaluate their models:\\n\\n- Using the implemented benchmark datasets. By default, we provide 6 of the commonly used multimodal benchmark datasets such as MNIST-SVHN, CelebA, CUB etc. Training and testing on these models can be defined in a config.\\n- Using the CdSprites+ dataset. We provide links for downloading all the 5 levels of our dataset. However, it is also possible to generate the data manually. CdSprites+ dataset and its variations can be generated using a script provided in our toolkit - it allows the user to choose the complexity of the data (we provide 5 difficulty levels based on the number of included shape attributes), number of samples per category, specific colours and other features. The dataset generation requires only the standard computer vision libraries and can be generated in a matter of minutes or hours (based on the difficulty level) on a CPU machine. You can read more about the dataset in Section 3.\\n- Adding a new dataset. Due to the modular structure of the toolkit, it is possible to add a new dataset class without disrupting other parts of the code. We provide documentation for how to do it in the GitHub pages linked in our repository.\\n\\n5 BENCHMARK STUDY\\n\\nIn this section, we demonstrate the utility of the proposed toolkit and dataset on a set of experiments comparing two selected state-of-the-art multimodal VAEs.\"}"}
{"id": "3DPTnFokLp", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 12: Reproduced MNIST-SVHN results for the MMV AE model with our multimodal V AE toolkit. We show the digit classification accuracies (%) of latent variables (MNIST) and (SVHN), and the probability of digit matching (%) for cross- and joint-generation. For our results, we also show in brackets the variance of the results calculated over 3 seeds.\\n\\n| Version | MNIST | SVHN | MNIST \u2192 SVHN | SVHN \u2192 MNIST | Joint |\\n|---------|-------|------|--------------|--------------|-------|\\n| Original | 91.3  | 68.0 | 86.4         | 69.1         | 42.1  |\\n| Reproduced (Ours) | 87.6 (5.2) | 70.4 (4.6) | 82.7 (5.2) | 72.5 (4.9) | 45.3 (3.1) |\\n\\nTable 13: Reproduced FashionMNIST results for the MV AE model with our multimodal V AE toolkit. We show the estimated marginal log-likelihoods (lower is better). For our results, we also show in brackets the variance of the results calculated over 3 seeds.\\n\\n| Version | logp(x\u2081) | logp(x\u2081, x\u2082) | logp(x\u2081|x\u2082) |\\n|---------|----------|--------------|-------------|\\n| Original | -232.535 | -233.007     | -230.695    |\\n| Reproduced (Ours) | -234.15 (1.52) | -233.89 (2.61) | -232.56 (3.12) |\\n\\nTable 14: Reproduced PolyMNIST results for the MoPoE model with our multimodal V AE toolkit. We show the Coherence Accuracy (%) of conditionally generated samples (excluding the input modality) (1 Mod, 2 Mods, 3 Mods, and 4 Mods stand for the number of input modalities) and the joint coherence (Joint). For our results, we also show in brackets the variance of the results calculated over 3 seeds.\\n\\n| Version | 1 Mod | 2 Mods | 3 Mods | 4 Mods | Joint |\\n|---------|-------|--------|--------|--------|-------|\\n| Original | 67    | 78     | 80     | 83     | 12    |\\n| Reproduced (Ours) | 66 (4) | 73 (5) | 81 (3) | 82 (5) | 11 (3) |\\n\\nTable 15: Reproduced MNIST-SVHN results for the DMV AE model with our multimodal V AE toolkit. We show the probability of digit matching (%) for cross- and joint-generation. For our results, we also show in brackets the variance of the results calculated over 3 seeds.\\n\\n| Version | MNIST \u2192 SVHN | SVHN \u2192 MNIST | Joint |\\n|---------|--------------|--------------|-------|\\n| Original | 88.1         | 83.7         | 44.7  |\\n| Reproduced (Ours) | 84.5 (4.7) | 82.2 (3.1) | 44.9 (3.6) |\\n\\nTable 16: Comparison of the multimodal V AE models used in our toolkit as described by Suzuki & Matsuo (2022). \u201cAggregated inference\u201d refers to whether the model learns joint posterior for all modalities, \u201cModality-specific\u201d means whether the model also learns modality-specific (private) latent variables and \u201cScalable\u201d refers to whether the computational costs grow exponentially with the number of modalities.\\n\\n| Model                  | Aggregated Inference | Modality-specific | Scalable |\\n|------------------------|----------------------|-------------------|----------|\\n| MV AE (Wu & Goodman, 2018) | \u2713                    | \u2717                 | \u2713        |\\n| MMV AE (Shi et al., 2019) | \u2717                    | \u2717                 | \u2713        |\\n| MoPoE (Sutter et al., 2021) | \u2713                    | \u2717                 | \u2717        |\\n| DMV AE (Lee & Pavlovic, 2021) | \u2713                    | \u2713                 | \u2713        |\\n\\nwere \\\\( \\\\text{Dim}_{\\\\text{MNIST}} = 1 \\\\) and for \\\\( \\\\text{Dim}_{\\\\text{SVHN}} = 4 \\\\). The used \\\\( \\\\beta \\\\) parameter was 1, and batch size 100.\\n\\nWe used the same encoder and decoder networks and an adapted script for calculating the cross- and joint-coherencies. The results are in Table 15.\"}"}
{"id": "3DPTnFokLp", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Results for the four models trained on MNIST-SVHN using our toolkit. We show the digit classification accuracies (%) of latent variables (\\\\textbf{MNIST} and \\\\textbf{SVHN}), and the probability of digit matching (%) for cross- and joint-generation. The variance of the results calculated over 3 seeds is shown in brackets.\\n\\n| Version | MNIST \u2192 SVHN | SVHN \u2192 MNIST | Joint |\\n|---------|--------------|--------------|-------|\\n| MMV AE  | 85.6 (5.2)   | 86.4 (4.6)   | 85.7 (5.2) |\\n| MV AE   | 91.5 (4.6)   | 88.2 (3.5)   | 95.4 (2.1) |\\n| MoPoE   | 86.6 (3.8)   | 83.6 (4.3)   | 81.2 (3.6) |\\n| DMV AE  | 78.27 (4.3)  | 69.34 (5.6)  | 84.5 (4.7) |\\n\\nTable 4: Results for the four models trained on all 5 levels of our CdSprites+ dataset. \\\\textit{Strict} refers to the percentage of completely correct samples (sample pairs in joint generation), \\\\textit{Features} shows the ratio of correct features (i.e., 1.2 (0.1)/3 for Level 3 means that on average 1.2 \u00b1 0.1 features out of 3 are recognized correctly for each sample) and \\\\textit{Letters} shows the mean percentage of correctly reconstructed letters (computed sample-wise). Standard deviations over 3 seeds are in brackets. For each model, we chose the most optimal latent space dimensionality \\\\textit{Dims} (for DMV AE a sum of private and shared latents). Please see the Appendix for a detailed explanation of our metrics.\\n\\n| Level | Model | Dims | Txt \u2192 Img \\\\[\\\\%\\\\] | Img \u2192 Txt \\\\[\\\\%\\\\] | Joint \\\\[\\\\%\\\\] |\\n|-------|-------|------|-----------------|-----------------|-------------|\\n| 1     | MMV AE | 16   | 47 (14)         | 0.5 (0.1)/1     | 64 (3) |\\n|       | MV AE  | 12   | 16 (1)          | 0.8 (0.0)/2     | 55 (27) |\\n|       | MoPoE | 16   | 10 (3)          | 0.8 (0.0)/2     | 40 (4) |\\n|       | DMV AE | 30   | 15 (2)          | 0.8 (0.0)/2     | 30 (2) |\\n| 2     | MMV AE | 16   | 18 (4)          | 0.8 (0.1)/2     | 85 (4) |\\n|       | MV AE  | 12   | 8 (2)           | 1.3 (0.0)/3     | 93 (1) |\\n|       | MoPoE | 16   | 10 (3)          | 1.3 (0.1)/3     | 32 (0) |\\n|       | DMV AE | 30   | 15 (2)          | 1.2 (0.0)/3     | 22 (2) |\\n| 3     | MMV AE | 16   | 6 (2)           | 1.2 (0.2)/3     | 31 (5) |\\n|       | MV AE  | 32   | 8 (2)           | 1.3 (0.0)/3     | 93 (1) |\\n|       | MoPoE | 24   | 7 (4)           | 1.3 (0.1)/3     | 32 (0) |\\n|       | DMV AE | 30   | 4 (0)           | 1.2 (0.0)/3     | 22 (2) |\\n| 4     | MMV AE | 24   | 3 (3)           | 1.7 (0.4)/4     | 27 (9) |\\n|       | MV AE  | 16   | 0 (0)           | 1.3 (0.0)/4     | 16 (5) |\\n|       | MoPoE | 24   | 2 (1)           | 1.4 (0.0)/4     | 21 (3) |\\n|       | DMV AE | 30   | 1 (1)           | 1.4 (0.0)/4     | 18 (1) |\\n| 5     | MMV AE | 24   | 0 (0)           | 1.8 (0.0)/5     | 13 (2) |\\n|       | MV AE  | 16   | 0 (0)           | 1.8 (0.0)/5     | 27 (1) |\\n|       | MoPoE | 24   | 0 (0)           | 1.8 (0.0)/5     | 17 (1) |\\n|       | DMV AE | 30   | 0 (0)           | 1.8 (0.0)/5     | 18 (2) |\\n\\ndecoder networks and training hyperparameters as in the original implementation and compared the final performance. The results are shown in Appendix Sec. A.3. We then unified the implementations for all models so that they only differ in the modality mixing and trained them on the CelebA and MNIST-SVHN datasets. The results are shown in Tables 2 and 3.\\n\\nNext, we trained all four models on the CdSprites+ dataset consecutively on all 5 levels of complexity and performed a hyperparameter grid search over the dimensionality of the latent space. You can find the used encoder and decoder architectures (fixed for all models) as well as the specific training details in Appendix Sec. A.2.1. We show the qualitative and quantitative results and discuss them in Section 5.2.\\n\\n5.2 RESULTS\\n\\nThe detailed results for all experiments (including other datasets such as Sprites, MNIST-SVHN etc.) can be found in Appendix Sec. A.2.4. Here we demonstrate the utility of our toolkit and dataset by comparing the MV AE, MMV AE, MoPoE and DMV AE models on the CdSprites+ dataset consecutively on 5 levels of difficulty. For an overview of what attributes each of the levels includes, please see Section 3. In Fig. 2, we show the qualitative results for levels 1, 3 and 5 of the dataset for the\"}"}
{"id": "3DPTnFokLp", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Qualitative results of the MV AE, MMV AE, MoPoE and DMV AE models trained on Level 1, 3 and 5 of our CdSprites+ dataset. We show first the reconstructions of the input image, then the captions obtained by cross-sampling. We show both the reconstructed and cross-generated (conditioned on the other modality) samples for images and text. Moreover, we report the cross- (\\\\(\\\\text{Img} \\\\rightarrow \\\\text{Txt}\\\\)) and joint- (\\\\(\\\\text{Joint}\\\\)) coherency accuracies for all levels in Table 4. The Strict metrics show the percentage of completely correct samples, while Features and Letters show the average proportion of correct words (or visual features for image) or letters per sample.\\n\\nBased on Table 4, MV AE and MMV AE outperform MoPoE and DMV AE in almost all categories for Levels 1 and 2. While the difference between MV AE and MMV AE is not large in Level 1, MV AE produces far more precise and stable text reconstructions up until Level 3. This would be in accordance with the results by Kutuzova et al. (2021), who showed that the PoE approach outperforms the MoE approach on datasets with a multiplicative (\u201cAND\u201d) combination of modalities, which is also the case of our CdSprites+ dataset.\\n\\nThe most prominent trend that can be seen both in Table 4 and Fig. 2 is the gradual performance decline across individual Levels. This is the expected and desirable outcome since only a challenging benchmark allows us to distinguish the difference among the evaluated models. For more details on the results, please see Appendix Sec. A.2.4.\\n\\nCONCLUSIONS\\n\\nIn this work, we present a benchmarking toolkit and a CdSprites+ (Captioned disentangled Sprites+) dataset for a systematic evaluation and comparison of multimodal variational autoencoders. The tool enables the user to easily configure the experimental setup by specifying the dataset, encoder and decoder architectures, multimodal integration strategy and the desired training hyperparameters all in one config. The framework can be easily extended for new models, datasets, loss functions or the encoder and decoder architectures without the need to restructure the whole environment. In its current form, it includes 4 state-of-the-art models and 6 commonly used datasets.\\n\\nFurthermore, the proposed synthetic bimodal dataset offers an automated evaluation of the cross-generation and joint-generation capabilities of the multimodal V AE models on 5 different scales of complexity. We also offer several automatic visualization modules that further inform on the latent factorisation of the trained models. We have evaluated the incorporated models on our CdSprites+ dataset and the results are publicly displayed in a \u201cleaderboard\u201d table on the GitHub repository, which will be gradually extended when new models and results are available.\\n\\nThe limitation of our CdSprites+ dataset is its synthetic nature and consequent simplicity compared to real-world data. This is a trade-off for the constrained variability allowing a reliable and fully automated evaluation. Please note that our dataset is aimed as a diagnostic tool allowing comparison of performance nuances across various models. We still encourage the researchers to train their models on real-world datasets (which we also include in our toolkit) at the same time to show their full capacity.\"}"}
{"id": "3DPTnFokLp", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Liqing Chen, Yifan Zhuo, Yingjie Wu, Yilei Wang, and Xianghan Zheng. Multi-modal feature fusion based on variational autoencoder for visual question answering. In Chinese Conference on Pattern Recognition and Computer Vision (PRCV), pp. 657\u2013669. Springer, 2019.\\n\\nImant Daunhawer, Thomas M Sutter, Ri \u02c7cards Marcinkevi\u02c7cs, and Julia E V ogt. Self-supervised disentanglement of modality-specific and shared factors improves multimodal generative models. In Pattern Recognition: 42nd DAGM German Conference, DAGM GCPR 2020, T\u00fcbingen, Germany, September 28\u2013October 1, 2020, Proceedings 42, pp. 459\u2013473. Springer, 2021.\\n\\nImant Daunhawer, Thomas M. Sutter, Kieran Chin-Cheong, Emanuele Palumbo, and Julia E V ogt. On the limitations of multimodal vaes. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=w-CPUXXrAj.\\n\\nImant Daunhawer, Alice Bizeul, Emanuele Palumbo, Alexander Marx, and Julia E V ogt. Identifiability results for multimodal contrastive learning. arXiv preprint arXiv:2303.09166, 2023.\\n\\nLi Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141\u2013142, 2012.\\n\\nW Falcon and TPL Team. Pytorch lightning the lightweight pytorch wrapper for high-performance ai research. scale your models, not the boilerplate, 2019.\\n\\nJustin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2901\u20132910, 2017.\\n\\nTom Joy, Yuge Shi, Philip Torr, Tom Rainforth, Sebastian M Schmon, and Siddharth N. Learning multimodal V AEs through mutual supervision. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=1xXvPrAshao.\\n\\nDiederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.\\n\\nSvetlana Kutuzova, Oswin Krause, Douglas McCloskey, Mads Nielsen, and Christian Igel. Multi-modal variational autoencoders for semi-supervised learning: In defense of product-of-experts. arXiv preprint arXiv:2101.07240, 2021.\\n\\nMihee Lee and Vladimir Pavlovic. Private-shared disentangled multimodal vae for learning of latent representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1692\u20131700, 2021.\\n\\nPaul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Foundations and recent trends in multimodal machine learning: Principles, challenges, and open questions. arXiv preprint arXiv:2209.03430, 2022.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740\u2013755. Springer, 2014.\\n\\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.\\n\\nLoic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.\\n\\nAshvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. Advances in neural information processing systems, 31, 2018.\"}"}
{"id": "3DPTnFokLp", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Section A.1, we provide additional details for the CdSprites+ dataset. In Section A.2.4, we describe the technical information and detailed results for the experiments presented in the paper and Section A.3 reports the reproduced results from original papers using our toolkit.\"}"}
