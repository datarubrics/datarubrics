{"id": "wpTitXWGNO", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nRecently, pre-trained large language models (LLMs) have shown impressive abilities in generating codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap with a reference code rather than actual execution.\\n\\nWe introduce xCodeEVAL, the largest executable multilingual multitask benchmark to date consisting of 25M document-level coding examples (16.5B tokens) from about 7.5K unique problems covering up to 11 programming languages with execution-level parallelism. It features a total of 7 tasks involving code understanding, generation, translation and retrieval. xCodeEVAL adopts an execution-based evaluation and offers a multilingual code execution engine, ExecEval that supports unit test based execution in all the 11 languages. To address the challenge of balancing the distributions of text-code samples over multiple attributes in validation/test sets, we propose a novel data splitting and a data selection schema based on the geometric mean and graph-theoretic principle. Our experiments with OpenAI\u2019s LLMs (zero-shot) and open-LLMs (zero-shot and fine-tuned) on the tasks and languages demonstrate xCodeEVAL to be quite challenging as per the current advancements in language models. Both xCodeEVAL and ExecEval are freely available at Hugging Face and Github.\\n\\nIntroduction\\n\\nAutomatically generating computer programs to solve complex problems has been a long-standing goal in AI (Manna and Waldinger, 1971). In recent years, specifically with the growth of large language models (LLMs), we have witnessed tremendous progress in synthesizing code that is not just relevant but also fully functional with no further human modification needed (Chen et al., 2021). The progress made in related tasks such as program synthesis (Chowdhery et al., 2022; Li et al., 2022), program repair (Berabi et al., 2021), code translation (Roziere et al., 2020; 2021), and code retrieval (Wan et al., 2019; Parvez et al., 2021) are having a profound impact on increasing developer productivity (Ziegler et al., 2022) and aiding educators (Finnie-Ansley et al., 2022).\\n\\nDespite the fact that such advancements are expected to be general with proper benchmarks, their evaluation has often been performed in a scattered way on a limited number of languages such as Python and Java, on a partial granularity level such as at the level of a statement (Huang et al., 2022) or function (Husain et al., 2019), focusing on only one or two specific tasks such as program synthesis and translation, and in many cases without proper fine-tuning data (Austin et al., 2021) or in terms of\\n\\n1. https://github.com/ntunlp/xCodeEval\\n2. https://huggingface.co/datasets/NTU-NLP-sg/xCodeEval\\n3. https://github.com/ntunlp/ExecEval\"}"}
{"id": "wpTitXWGNO", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nThe first (and the only) input line contains integer number $w$ (1 \u2264 $w$ \u2264 100) \u2014 the weight of the watermelon bought by the boys.\\n\\nOutput\\n\\nPrint YES, if the boys can divide the watermelon into two parts, each of them weighing even number of kilos; and NO in the opposite case.\\n\\nI/O\\n\\nInput: 8 Output: YES\\n\\nNote\\n\\nFor example, the boys can divide the watermelon into two parts of 2 and 6 kilos respectively (another variant \u2014 two parts of 4 and 4 kilos).\\n\\nOne hot summer day Pete and his friend Billy decided to buy a watermelon. They chose the biggest and the ripest one, in their opinion. After that the watermelon was weighed, and the scales showed $w$ kilos. They rushed home, dying of thirst, and decided to divide the berry, however they faced a hard problem.\\n\\nPete and Billy are great fans of even numbers, that's why they want to divide the watermelon in such a way that each of the two parts weighs even number of kilos, at the same time it is not obligatory that the parts are equal. The boys are extremely tired and want to start their meal as soon as possible, that's why you should help them and find out, if they can divide the watermelon in the way they want. For sure, each of them should get a part of positive weight.\\n\\n```c\\n#include<stdio.h>\\n\\nint main()\\n{\\n    int num;\\n    scanf(\\\"%d\\\", &num);\\n    if (num % 2 == 0)\\n        printf(\\\"YES\\n\\\");\\n    else\\n        printf(\\\"NO\\n\\\");\\n    return 0;\\n}\\n```\\n\\nCorrect Answer\\n\\nWrong Answer\\n\\nCode Failed hidden unit test. Input: 2 Output: Yes, Exp. Output: No\\n\\nPassed all hidden unit tests\\n\\nbrute force, math\\n\\nTags\\n\\nDiff 800\\n\\nFIGURE 1 \u2013 A sample from XCODEEVAL. It includes a natural language description of the problem, input/output (i/o) description, and a few i/o examples. It also includes relevant meta-information such as problem tags (e.g., brute force, math), language, difficulty level (800 in the figure), and a note (explanation of i/o). Each sample contains a number of hidden unit tests (not shown in the figure) against which we evaluate the code. Although the code at the left gives the correct answer to the given input, it is incorrect as it fails in other test cases.\\n\\nTo address these limitations, and drive further advancements in the creation of more general-purpose LLMs for problem solving, we introduce XCODEEVAL, the largest executable multilingual multitask benchmark to date consisting of 20M coding examples from about 7.5K unique algorithmic problems. It covers up to 17 programming languages with the parallelism of multilingual data which can benefit both mono-and multi-lingual code intelligence applications. It features a total of 7 tasks involving code understanding, generation, translation and retrieval, and wherever appropriate it employs an execution-based evaluation protocol. A detailed documentation of the dataset can be found in Appendix (Section 7). Figure 1 shows an example from XCODEEVAL; it includes a problem description in natural language, a buggy and bug-free solution to the problem, and relevant metadata such as difficulty level, language, problem tags (e.g., brute force).\\n\\nXCODEEVAL is a result of a number of crucial design principles and challenges as highlighted below.\\n\\nReasoning\\n\\nIn terms of genre, problem solving posits a unique set of challenges that require (a) understanding a complex natural language problem description, (b) expertise in data structures and algorithms, (c) complex reasoning that goes beyond memorization, and (d) generating programs of potentially hundreds of lines so that they can pass a comprehensive list of especially designed hidden tests. Given the current progress in LLMs and their instruction following capability (Ouyang et al., 2022), competition-level problems that humans find challenging, provide an interesting benchmark to test many aspects of intelligence (Li et al., 2022; OpenAI, 2023).\"}"}
{"id": "wpTitXWGNO", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multilinguality\\nWe aim to cover as many programming languages as possible regardless of the resource discrepancies. One of the main objectives of this benchmark is to assess the degree to which codes in different languages are parallel to one another. In addition to that, we also intend to evaluate the zero-shot cross-lingual capability of the LLMs.\\n\\nEvaluation and its granularity\\nWe believe the current evaluation standards do not fully consider the idea of the global meaning representation of a program, which requires models to comprehend different interpretable code segments and connect both local and modular knowledge into a global representation. We propose execution-based evaluation with unit tests at the global level. While there are many benchmarks covering the local understanding of a code segment, there are only a few that work at a global level as shown in Table 6. We consider a pair of codes to be equivalent, if they generate the same output for a given input regardless of syntax/languages (Sajnani, 2016). To support this, we have developed ExecEval, a new standardized and distributed execution environment that supports 44 compilers/interpreters in all the languages in XCODEVAL. We also provide a large number of necessary unit tests (average of 50 per problem) for the relevant tasks (Table 1). In this context, it is noteworthy that 44 out of 165 problems in the CodeContest's test split have no private unit tests. Additionally, it contains 104 problems without complete collection of unit tests (as available in the source), thus are inadequate in assessing a solution's correctness. We have identified this issue and excluded such problems from our evaluation sets (development and test splits).\\n\\nTask difficulty and trainability\\nWe wish to focus on problems of different difficulty levels (from 800 to 3500 rating points, following codeforces.com) such that models with different capabilities can be benchmarked against difficulty levels. We also aim to provide sufficient training data for each task so that pre-trained LMs can be fine-tuned or small-scale models can be trained from scratch.\\n\\nData split\\nFinally, balancing the validation and test distributions of text-code instances over multiple attributes such as problems, tags, and execution outcome (e.g., correct vs. wrong) is challenging. We propose a novel data split schema based on a geometric mean and a data selection schema adapting a graph-theoretic solution to the circulation problem with lower and upper bounds (Mount, 2017) that can be applied for other benchmarks as well (Section 2.1).\\n\\nWe evaluate ChatGPT on our classification and generative tasks, and StarEncoder (Li et al., 2023) on the retrieval tasks. In addition to that we also trained Program Synthesis task on Starcoderbase-3B and compare it's result with CodeLlama-7b and CodeLlama-13b instruct model. Our results indicate that XCODEVAL remains difficult to solve for the advanced LLMs, even on a simple binary classification task like Code Compilation (Table 3). With XCODEVAL, we can identify and compare multilingual executability across languages as well as perform open-ended evaluation on any programming language for the Code Translation and Program Synthesis tasks. Moreover, the unique parallelism of unit-tests allows for different interpretable evaluation and analysis on diverse code related tasks (Section 3). Finally, our experimental results with program synthesis tasks demonstrates that our training data can facilitate a reduction in the size of the language model while maintaining its executable capabilities.\\n\\n2 XCODEVAL: DATA, EXECUTION & TASKS\\n\\n2.1 DATA CREATION\\n\\nFigure 2 \u2013 Flow network of for validation-test dataset creation.\\nHere s and t represent the source and sink of the flow network. Also, l(u, v), c(u, v) represents the lower and upper capacity of the edge connected from u to v.\\n\\nWe construct our dataset from 25M openly available samples from codeforces.com for a total of 7514 distinct problems. Each sample Sk \u2208 S represents a potential solution to a problem Pi \u2208 P, and a problem Pi can be solved by employing a set of algorithmic techniques Ti \u2282 T, which we refer to as problem tags (e.g., 2-sat, binary search); see Figure 8 for a complete list of tags.\\n\\nValidation and test sets\\nTo prevent data leakage, we first put aside Nh (= 1354) problems as held-out set Dh for validation and test. It ensures that the problems in the validation and test sets are not seen in training and models need to generalize to unseen problems. We then create Dvalid and Dtest splits from Dh, while maintaining a balanced tag distribution and ensuring that all the tags in these two sets also exist in the training\"}"}
{"id": "wpTitXWGNO", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "wpTitXWGNO", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C E Metz. 1978. Basic principles of ROC analysis. Semin Nucl Med, 8(4):283\u2013298.\\n\\nAntonio Valerio Miceli Barone and Rico Sennrich. 2017. A parallel corpus of python functions and documentation strings for automated code documentation and code generation. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 314\u2013319, Taipei, Taiwan. Asian Federation of Natural Language Processing.\\n\\nDave Mount. 2017. Lecture 17 network flow: Extensions.\\n\\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. 2023. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124.\\n\\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint.\\n\\nYusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2015. Learning to generate pseudo-code from source code using statistical machine translation. In 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 574\u2013584.\\n\\nOpenAI. 2023. Gpt-4 technical report.\\n\\nJuri Opitz and Sebastian Burst. 2019. Macro F1 and macro F1. CoRR, abs/1911.03347.\\n\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\\n\\nMd Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval augmented code generation and summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2719\u20132734, Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2018. Building language models for text with named entities. arXiv preprint arXiv:1805.04836.\\n\\nRuchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, et al. 2021. Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks. arXiv preprint arXiv:2105.12655.\\n\\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1\u2013140:67.\\n\\nShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. Codebleu: a method for automatic evaluation of code synthesis. CoRR, abs/2009.10297.\\n\\nBaptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised translation of programming languages. Advances in Neural Information Processing Systems, 33.\\n\\nBaptiste Roziere, Jie M Zhang, Francois Charton, Mark Harman, Gabriel Synnaeve, and Guillaume Lample. 2021. Leveraging automated unit tests for unsupervised code translation. arXiv preprint arXiv:2110.06773.\"}"}
{"id": "wpTitXWGNO", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rebecca L. Russell, Louis Y. Kim, Lei H. Hamilton, Tomo Lazovich, Jacob Harer, Onur Ozdemir, Paul M. Ellingwood, and Marc W. McConley. 2018. Automated vulnerability detection in source code using deep representation learning. In 17th IEEE International Conference on Machine Learning and Applications, ICMLA 2018, Orlando, FL, USA, December 17-20, 2018, pages 757\u2013762. IEEE.\\n\\nHitesh Sajnani. 2016. Large-scale code clone detection. PhD Thesis, University of California, Irvine.\\n\\nJeffrey Svajlenko, Judith F. Islam, Iman Keivanloo, Chanchal Kumar Roy, and Mohammad Mia. 2014. Towards a big data curated benchmark of inter-project code clones. In 30th IEEE International Conference on Software Maintenance and Evolution, Victoria, BC, Canada, September 29 - October 3, 2014, pages 476\u2013480. IEEE Computer Society.\\n\\nAbdel Aziz Taha and Allan Hanbury. 2015. Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool. BMC Med Imaging, 15:29.\\n\\nXiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, and Mark Gerstein. 2023. Biocoder: A benchmark for bioinformatics code generation with contextual pragmatic knowledge. CoRR, abs/2308.16458.\\n\\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).\\n\\nTHUDM. 2022. Codegeex: A multilingual code generation model. https://github.com/THUDM/CodeGeeX.\\n\\nMichele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk. 2019. An empirical study on learning bug-fixing patches in the wild via neural machine translation. ACM Trans. Softw. Eng. Methodol., 28(4):19:1\u201319:29.\\n\\nYao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, and Philip Yu. 2019. Multi-modal attention network learning for semantic source code retrieval. In 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 13\u201325. IEEE.\\n\\nYue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 8696\u20138708. Association for Computational Linguistics.\\n\\nZhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. 2022b. Execution-based evaluation for open-domain code generation. arXiv preprint arXiv:2212.10481.\\n\\nChunqiu Steven Xia and Lingming Zhang. 2022. Less training, more repairing please: revisiting automated program repair via zero-shot learning. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18, 2022, pages 959\u2013971. ACM.\\n\\nPengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to mine aligned code and natural language pairs from stack overflow. In Proceedings of the 15th International Conference on Mining Software Repositories, pages 476\u2013486.\\n\\nPengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, et al. 2022. Natural language to code generation in interactive data science notebooks. arXiv preprint arXiv:2212.09248.\\n\\nHao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Tao Xie, and Qianxiang Wang. 2023. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. arXiv preprint arXiv:2302.00288.\"}"}
{"id": "wpTitXWGNO", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nVictor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning. CoRR, abs/1709.00103.\\n\\nYaqin Zhou, Shangqing Liu, Jing Kai Siow, Xiaoning Du, and Yang Liu. 2019. Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 10197\u201310207.\\n\\nMing Zhu, Aneesh Jain, Karthik Suresh, Roshan Ravindran, Sindhu Tipirneni, and Chandan K. Reddy. 2022. Xlcost: A benchmark dataset for cross-lingual code intelligence.\\n\\nQihao Zhu, Zeyu Sun, Yuan-an Xiao, Wenjie Zhang, Kang Yuan, Yingfei Xiong, and Lu Zhang. 2021. A syntax-guided edit decoder for neural program repair. In ESEC/FSE '21: 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Athens, Greece, August 23-28, 2021, pages 341\u2013353. ACM.\\n\\nAlbert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. Productivity assessment of neural code completion. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, MAPS 2022, page 21\u201329, New York, NY, USA. Association for Computing Machinery.\"}"}
{"id": "wpTitXWGNO", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Is the software that was used to preprocess/clean/label the data available?\\n\\nNo software was used for labeling the data.\\n\\nHas the dataset been used for any tasks already?\\n\\nYes. We evaluated ChatGPT and trained StarEncoder using the dataset.\\n\\nIs there a repository that links to any or all papers or systems that use the dataset?\\n\\nYes, https://github.com/ntunlp/xCodeEval.\\n\\nWhat (other) tasks could the dataset be used for?\\n\\nWe proposed 7 different tasks for XCODE-VAL. Please follow table 8 for details.\\n\\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\\n\\nNo.\\n\\nWill the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?\\n\\nPlease follow the Licensing section in https://github.com/ntunlp/xCodeEval for details.\\n\\nWhen will the dataset be distributed?\\n\\nThe data is already distributed via huggingface.\\n\\nWill the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?\\n\\nPlease follow the Licensing section in https://github.com/ntunlp/xCodeEval for details.\\n\\nHave any third parties imposed IP-based or other restrictions on the data associated with the instances?\\n\\nNo.\\n\\nDo any export controls or other regulatory restrictions apply to the dataset or to individual instances?\\n\\nNo.\\n\\nWho will be supporting/hosting/maintaining the dataset?\\n\\nHuggingface is hosting the dataset. The authors are maintaining the dataset. Nanyang Technological University is supporting the dataset.\\n\\nHow can the owner/curator/manager of the dataset be contacted (e.g., email address)?\\n\\nEmail.\\n\\nIs there an erratum?\\n\\nNone at this point. The dataset is hosted through git lfs. The future erratum can be tracked easily.\\n\\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?\\n\\nTo the best of our knowledge there are no errors in the dataset. The authors do not intend to add new instances at this point. But the authors remain open to remove/correct instances given that any labeling errors found.\\n\\nIf the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)?\\n\\nThe dataset doesn't relate to people.\\n\\nWill older versions of the dataset continue to be supported/hosted/maintained?\\n\\nYes. The older version should be accessed via git LFS.\"}"}
{"id": "wpTitXWGNO", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?\\n\\nSince the dataset is fixed, there is currently no way to contribute to it. Please note that any extensions or augmentations of the dataset are subject to the same license as this dataset.\\n\\n**Discussion on the Possibility of Data Leakage and Contamination**\\n\\nAlthough we completely separate our validation and test data, LLMs might have possible data leakage from pretraining. We find that even identifying data leakage (test data exists or not in the pretraining corpora) is challenging using conventional data search methods due to search cost & complexity (e.g., exact match or token overlapping methods) while hashing based searches suffer from not having properly segmented text. For leakage-free evaluation, we approach employs \u201cknowledge cut-off\u201d which shows that the data contamination significantly impacts the model performance and it needs to be interpreted with proper analyses. We plan to evaluate on separate human written testset in future.\\n\\nWe follow the framework proposed by Holland et al. (2018). Table 12 gives an overview of dataset facts. The variable description can be found in appendix I. We discuss about provenance in appendix J.3 and appendix J.6.\"}"}
{"id": "wpTitXWGNO", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We introduce xCodeEval, the largest executable multilingual multitask benchmark to date consisting of 25 M document-level coding examples from about 7.5 K unique problems covering up to 17 programming languages with execution-level parallelism. It features a total of seven tasks involving code understanding, generation, translation and retrieval, and it employs an execution-based evaluation. We develop a test-case based multilingual code execution engine, ExecEval that supports all the programming languages in xCodeEval. We also propose a novel data splitting and a data selection schema for balancing data distributions over multiple attributes based on geometric mean and graph-theoretic principle.\"}"}
{"id": "wpTitXWGNO", "page_num": 43, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"import urllib.request\\n\\nurl = 'http://icanhazip.com'\\n\\nwith urllib.request.urlopen(url) as response:\\n    if response.getcode() == 200:\\n        print(response.read().decode('utf-8').strip())\\n    else:\\n        print(f'Request failed with status code: {response.getcode()}')\\n\\nFigure 17 \u2013 Left: A python code performing a network request. Right: ExecEval responded with RUNTIME_ERROR as the socket system call is blocked.\"}"}
{"id": "wpTitXWGNO", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. \\\\textbf{lang} : Runtime/Compiler version of the source code.\\n\\n2. \\\\textbf{source_code} : A program.\\n\\n3. \\\\textbf{code_uid} : A unique ID for the sample. It is not important for model training. If you find any issue with the sample, you can report it to us by mentioning the code_uid.\\n\\n4. \\\\textbf{src_uid} : A specific identifier that shows which problem the code is associated with. This identifier is important for the training of the model. The problem referred to by the src_uid provides a natural description of the problem that the code successfully solved. Refer to Structure of problem_descriptions.jsonl.\\n\\n5. \\\\textbf{difficulty} : Difficulty rating of the problem indicated by src_uid. The higher the harder.\\n\\n6. \\\\textbf{exec_outcome} : Execution outcome status. Follow Section 2.2 to know the potential list of outcomes. The exec_outcome flags in the training data come from a pre-run environment. However, training data doesn't include unit-test to avoid potential hacks. We provide unit tests for only dev and test data.\\n\\n7. \\\\textbf{lang_cluster} : A generic programming language name the value of lang belongs to.\\n\\n8. \\\\textbf{prob_desc_description} : Problem description in textual format, math operations are written in latex.\\n\\n9. \\\\textbf{prob_desc_input_from} : How the program should take the unit test.\\n\\n10. \\\\textbf{prob_desc_output_to} : Where the program should output the result of the unit test.\\n\\n11. \\\\textbf{prob_desc_time_limit} : Time limit to solve the problem.\\n\\n12. \\\\textbf{prob_desc_memory_limit} : Memory limit to solve the problem.\\n\\n13. \\\\textbf{prob_desc_input_spec} : How and in what order the input will be given to the program? It also includes the date range, types, and sizes.\\n\\n14. \\\\textbf{prob_desc_output_spec} : How the outputs should be printed. Most of the time the unit test results are matched with an exact string match or floating point comparison with a precision boundary.\\n\\n15. \\\\textbf{prob_desc_sample_inputs} : A sample input for the code that is expected to solve the problem described in description.\\n\\n16. \\\\textbf{prob_desc_sample_outputs} : The expected output for the sample_input that is expected to solve the problem described in description.\\n\\n17. \\\\textbf{prob_desc_notes} : Explanation of sample_inputs & sample_outputs.\\n\\n18. \\\\textbf{prob_desc_created_at} : The Unix timestamp when the problem was released. Use datetime lib in Python to parse it to a human-readable format.\\n\\n19. \\\\textbf{file_name} : Name of the source jsonl file from where data is loaded.\\n\\n20. \\\\textbf{hidden_unit_tests} : A list of unit tests returned as string. Use json.loads(hidden_unit_tests) to load the data.\"}"}
{"id": "wpTitXWGNO", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"lang_cluster: A generic programming language name the value of $\\\\text{lang}$ belongs to.\\n\\nprob_desc_description: Problem description in textual format, math operations are written in latex.\\n\\nprob_desc_input_from: How the program should take the unit test.\\n\\nprob_desc_output_to: Where the program should output the result of the unit test.\\n\\nprob_desc_time_limit: Time limit to solve the problem.\\n\\nprob_desc_memory_limit: Memory limit to solve the problem.\\n\\nprob_desc_input_spec: How and in what order the input will be given to the program? It also includes the date range, types, and sizes.\\n\\nprob_desc_output_spec: How the outputs should be printed. Most of the time the unit test results are matched with an exact string match or floating point comparison with a precision boundary.\\n\\nprob_desc_sample_inputs: A sample input for the code that is expected to solve the problem described in description.\\n\\nprob_desc_sample_outputs: The expected output for the sample_input that is expected to solve the problem described in description.\\n\\nprob_desc_notes: Explanation of sample_inputs & sample_outputs.\\n\\nprob_desc_created_at: The Unix timestamp when the problem was released. Use datetime lib in Python to parse it to a human-readable format.\\n\\nfile_name: Name of the source jsonl file from where data is loaded.\\n\\nhidden_unit_tests: a list of unit tests returned as a string. use json.loads(hidden_unit_tests) to load the data.\"}"}
{"id": "wpTitXWGNO", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We follow the questionnaires from Gebru et al. (2021) as the datasheet for CODE VAL.\\n\\nJ.1 MOTIVATION\\n\\nFor what purpose was the dataset created?\\n\\nCODE VAL dataset was specifically created to address three main aspects: (i) **Reasoning**, (ii) **Multilinguality** in terms of programming languages, and (iii) **Executability** of the programming languages. These aspects were thoroughly discussed in Section 1 of the main paper, providing detailed insights into the motivation behind the dataset creation.\\n\\nWho created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\\n\\nCODE VAL is an output of a passion project driven by a group of researchers from (i) **Islamic University of Technology**, (ii) **Nanyang Technological University**, (iii) **Bosch Research**.\\n\\nWho funded the creation of the dataset?\\n\\nNanyang Technological University provided the necessary computing resources for the project. None of the project members received any remuneration for their contributions.\\n\\nJ.2 COMPOSITION\\n\\nWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?\\n\\nPlease follow the Section I for details.\\n\\nHow many instances are there in total (of each type, if appropriate)?\\n\\nPlease follow the Table 8, 2, and 9 for the details statistics of the dataset.\\n\\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?\\n\\nDataset contains all possible instances.\\n\\nWhat data does each instance consist of?\\n\\nPlease follow the Section I for details.\\n\\nIs there a label or target associated with each instance?\\n\\nPlease follow the Section I for details.\\n\\nIs any information missing from individual instances?\\n\\nFor a few problem description, difficulty is assigned as **None** due to data unavailability.\\n\\nAre relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?\\n\\nPlease follow the Section I for details.\\n\\nAre there recommended data splits (e.g., training, development/validation, testing)?\\n\\nWe explicitly defined the training, validation and test split for CODE VAL. Please follow the Section 2.1 for more details.\\n\\nAre there any errors, sources of noise, or redundancies in the dataset?\\n\\nTo the best of our knowledge there are not errors, sources of noise or redundencies in CODE VAL.\\n\\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?\\n\\nThe dataset is self-contained.\"}"}
{"id": "wpTitXWGNO", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor\u2013patient confidentiality, data that includes the content of individuals\u2019 non-public communications)?\\n\\nThe dataset is collected from open sourced sources. There are no confidentiality or non-public entity in the dataset.\\n\\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?\\n\\nTo the best of our knowledge there are no offensive, insulting, threatening content in the dataset.\\n\\nDoes the dataset identify any subpopulations (e.g., by age, gender)?\\n\\nNo.\\n\\nIs it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?\\n\\nThere are no attributes in the dataset that allow to identify individuals.\\n\\nDoes the dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?\\n\\nThere are no attributes in the dataset that allow this.\\n\\nHow was the data associated with each instance acquired?\\n\\nFollowing Li et al. (2022), the data was collected from codeforces.com and then associated with different tasks. Please follow Section E for more details.\\n\\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\\n\\nDataset wasn\u2019t sampled from a large dataset.\\n\\nWe proposed the dataset for the first time.\\n\\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?\\n\\nDataset was collected by the author of this paper.\\n\\nOver what timeframe was the data collected?\\n\\nThe data was downloaded in between Feb, 2022 to January, 2023.\\n\\nWere any ethical review processes conducted (e.g., by an institutional review board)?\\n\\nNo.\\n\\nDid you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?\\n\\nThe data is downloaded by an author. No third parties or other sources are involved.\\n\\nHas an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted?\\n\\nPotential impact of the dataset is discussed at section 5 and section 6.\\n\\nWas any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?\\n\\nWe de-anonymized the data and remove data with sensitive information (i.e., email, large infograph, toxic keywords). The labels come as a metadata from the sources.\\n\\nWas the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?\\n\\nNo.\"}"}
{"id": "wpTitXWGNO", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"TABLE 11 \u2013 Default resource limits values for `prlimit` used by ExecEval. The comment column shows the variable names as defined in `sys/resource.h` with some additional information.\\n\\n| Resource | Value | Comment |\\n|----------|-------|---------|\\n| core     | 0     | RLIMIT_CORE |\\n| data     | -1    | RLIMIT_DATA |\\n| fsize    | 0     | RLIMITFSIZE |\\n| sigpending | 0 | RLIMIT_SIGPENDING |\\n| rss      | -1    | RLIMIT_RSS |\\n|nofile    | 4     | RLIMIT_NOFILE |\\n|msgqueue  | 0     | RLIMIT_MSGQUEUE |\\n|rtprio    | 0     | RLIMIT_RTPRIO |\\n|stack     | -1    | RLIMIT_STACK |\\n|cpu       | 2     | RLIMIT_CPU, CPU time, in seconds |\\n|nproc     | 1     | RLIMIT_NPROC |\\n|as        | 2 \\\\times 10^23 | RLIMIT_AS set to 2GB by default |\\n|locks     | 0     | RLIMIT_LOCKS |\\n\\nThe execution of code via an unprivileged user disables the read, write, or execute permissions of any sensitive files. Figure 16, 17, and 18 shows an example of a fork bomb written in C, a network request in Python, and an escalated access in Python which are all blocked by ExecEval, respectively.\\n\\n```c\\n#include <stdio.h>\\n#include <sys/types.h>\\n\\nint main()\\n{\\n    while (1)\\n        fork();\\n    return 0;\\n}\\n```\\n\\nFigure 16 \u2013 Left: An fork bomb written in C. Right: ExecEval ran the code with allowing only 1 process and thus the infinite loop resulted in TIME_LIMIT_EXCEEDED.\\n\\n15. For more details follow: https://github.com/ntunlp/ExecEval.\"}"}
{"id": "wpTitXWGNO", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DEFINITION OF DATA ATTRIBUTES\\n\\nAll the raw data can be downloaded from the huggingface dataset repository. For each of the tasks we have two data files that are required for multiple tasks.\\n\\n1. problem_descriptions.jsonl\\n2. unittest_db.json\\n\\nYou can find these two files in the root directory of the main branch of huggingface dataset repository. To avoid data redundancy we didn\u2019t include these data with the relevant tasks, rather we add a unique id src_uid to retrieve these data. We include a data loader using datasets package that defines the tasks.\\n\\nWe provide the definition for each of the data attributes of XCODEVAL in the following sections.\\n\\nI.1 PROBLEM DESCRIPTION (PROBLEM_DESCRIPTION)\\n\\nThe problem descriptions are in the problem_descriptions.jsonl file. This data source is linked to the proposed tasks by matching the src_uid column for each sample in the relevant tasks. The columns copied from the problem_descriptions.jsonl file are prefixed with prob_desc_.\\n\\n1. description: Problem description in textual format, math operations are written in latex.\\n2. input_from: How the program should take the unit test.\\n3. output_to: Where the program should output the result of the unit test.\\n4. time_limit: Time limit to solve the problem.\\n5. memory_limit: Memory limit to solve the problem.\\n6. input_spec: How and in what order the input will be given to the program? It also includes the date range, types, and sizes.\\n7. output_spec: How the outputs should be printed. Most of the time the unit test results are matched with an exact string match or floating point comparison with a precision boundary.\\n8. sample_inputs: A sample input for the code that is expected to solve the problem described in description.\\n9. sample_outputs: The expected output for the sample_input that is expected to solve the problem described in description.\\n10. notes: Explanation of sample_inputs & sample_outputs.\\n11. tags: The problem categories.\\n12. src_uid: The unique id of the problem. This ID is referred to in the task data samples instead of putting all this information.\\n13. difficulty: How difficult is it to solve the problem for a human (annotated by an expert human).\\n14. created_at: The Unix timestamp when the problem was released. Use datetime lib in Python to parse it to a human-readable format.\\n\\nI.2 UNIT TESTS (HIDDEN_UNIT_TEST)\\n\\nThe unit tests needed for execution based evaluation are in the unittest_db.json file. This data source is linked to the proposed tasks by matching the src_uid column for each sample in the relevant tasks. The columns copied from the unittest_db.json file are under the attribute hidden_unit_test.\\n\\n1. unittest_db.json dict keys i.e., db884d679d9cfb1dc4bc511f83beedda are the src_uid from problem_descriptions.jsonl.\\n2. input: Input of the unit test.\\n3. output: List of expected outputs for the unit test.\\n\\n16. https://huggingface.co/datasets/NTU-NLP-sg/xCodeEval\\n17. https://github.com/huggingface/datasets\"}"}
{"id": "wpTitXWGNO", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I.3\u30bf\u30b0\u5206\u985e\uff08TAG_CLASSIFICATION\uff09\\n\\n\u4e0e\u3048\u3089\u308c\u305f\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306e\u76ee\u7684\u306f\u3001\u30b3\u30fc\u30c9\u3092\u30de\u30eb\u30c1\u30e9\u30d9\u30eb\u306e\u30bf\u30b0\uff08\u30bf\u30b0: tags\uff09\u306b\u5165\u308c\u308b\u3053\u3068\u3067\u3059\u3002\\n\\n1. `lang`: Runtime/Compiler version of the source_code.\\n2. `source_code`: A program.\\n3. `tags`: List of potential algorithmic techniques required to write the program.\\n4. `lang_cluster`: A generic programming language name the value of `lang` belongs to.\\n5. `code_uid`: A unique ID for the sample. It is not important for model training. If you find any issue with the sample, you can report it to us by mentioning the `code_uid`.\\n6. `src_uid`: A specific identifier that shows which problem the code is associated with. This identifier is important for the training of the model. The problem referred to by the `src_uid` provides a natural description of the problem that the code successfully solved. Refer to Structure of `problem_descriptions.jsonl`.\\n7. `difficulty`: Difficulty rating of the problem indicated by `src_uid`. The higher the harder.\\n\\nI.4\u30b3\u30fc\u30c9\u30b3\u30f3\u30d1\u30a4\u30eb\uff08CODE_COMPILATION\uff09\\n\\n\u4e0e\u3048\u3089\u308c\u305f\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306e\u76ee\u7684\u306f\u3001\u30b3\u30fc\u30c9\u304c\u30b3\u30f3\u30d1\u30a4\u30eb\u53ef\u80fd\u3067\u3042\u308b\u304b\u3069\u3046\u304b\uff08\u30e9\u30d9\u30eb: compilation_error\uff09\u3092\u5206\u985e\u3059\u308b\u3053\u3068\u3067\u3059\u3002\\n\\n1. `lang`: Runtime/Compiler version of the source_code.\\n2. `source_code`: A program.\\n3. `lang_cluster`: A generic programming language name the value of `lang` belongs to.\\n4. `compilation_error`: True/False, Indicates if the code generates a compilation error or not.\\n5. `code_uid`: A unique ID for the sample. It is not important for model training. If you find any issue with the sample, you can report it to us by mentioning the `code_uid`.\\n6. `src_uid`: A specific identifier that shows which problem the code is associated with. This identifier is important for the training of the model. The problem referred to by the `src_uid` provides a natural description of the problem that the code successfully solved. Refer to Structure of `problem_descriptions.jsonl`.\\n7. `difficulty`: Difficulty rating of the problem indicated by `src_uid`. The higher the harder.\\n8. `file_name`: Name of the source JSON file from where data is loaded.\\n\\nI.5\u30aa\u30fc\u30c8\u30d7\u30ed\u30b0\u30e9\u30e0\u30ea\u30da\u30a2\uff08APR\uff09\\n\\n\u4e0e\u3048\u3089\u308c\u305f\u30d0\u30b0\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306e\u76ee\u7684\u306f\u3001\u3059\u3079\u3066\u306e\u30e6\u30cb\u30c3\u30c8\u30c6\u30b9\u30c8\u3067\u6b63\u3057\u304f\u52d5\u4f5c\u3059\u308b\u4fee\u6b63\u7248\u306e\u30b3\u30fc\u30c9\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u3067\u3059\u3002`fix_source_code`\u3092\u4f7f\u7528\u3057\u3066\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u307e\u3059\u3002\\n\\n1. `similarity_score`: A similarity score between `bug_source_code` and `fix_source_code` given by difflib.\\n2. `equal_cnt`: A metric comparing `bug_source_code` and `fix_source_code`. Recommended by difflib.\\n3. `replace_cnt`: A metric comparing `bug_source_code` and `fix_source_code`. Recommended by difflib.\\n4. `delete_cnt`: A metric comparing `bug_source_code` and `fix_source_code`. Recommended by difflib.\\n5. `insert_cnt`: A metric comparing `bug_source_code` and `fix_source_code`. Recommended by difflib.\\n6. `fix_ops_cnt`: A metric comparing `bug_source_code` and `fix_source_code`. Recommended by difflib.\\n7. `bug_source_code`: Buggy code.\\n8. `fix_source_code`: A potential fix of the buggy code that passed all the unit tests.\"}"}
{"id": "wpTitXWGNO", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\n9. \\\\textit{lang}: Runtime/Compiler version of the \\\\textit{source_code}.\\n\\n10. \\\\textit{fix_code_uid}: A unique ID for the fix code. It is not important for model training. If you find any issue with the sample, you can report it to us mentioning the \\\\textit{fix_code_uid}.\\n\\n11. \\\\textit{bug_code_uid}: A unique ID for the buggy code. It is not important for model training. If you find any issue with the sample, you can report it to us mentioning the \\\\textit{bug_code_uid}.\\n\\n12. \\\\textit{src_uid}: A specific identifier that shows which problem the code is associated with. This identifier is important for the training of the model. The problem referred to by the \\\\textit{src_uid} provides a natural description of the problem that the code successfully solved. Refer to Structure of \\\\textit{problem_descriptions.jsonl}.\\n\\n13. \\\\textit{apr_id}: A unique ID for the apr sample. It is not important for model training. If you find any issue with the sample, you can report it to us mentioning the \\\\textit{apr_id}.\\n\\n14. \\\\textit{difficulty}: Difficulty rating of the problem indicated by \\\\textit{src_uid}. The higher the \\\\textit{difficulty} the harder.\\n\\n15. \\\\textit{tags}: List of potential algorithmic techniques required to write the program.\\n\\n16. \\\\textit{bug_exec_outcome}: A pre-run execution outcome of \\\\textit{bug_source_code}. Follow Section 2.2 to know the potential list of outcomes. The \\\\textit{exec_outcome} flags in the training data comes from a pre-run environment from the source website and they are not verified in \\\\textit{ExecEval}. However, training data doesn't include unit-test to avoid potential hacks and confusion. We provide unit test for only validation and test data.\\n\\n17. \\\\textit{fix_exec_outcome}: A pre-run execution outcome of \\\\textit{fix_source_code}. Follow Section 2.2 to know the potential list of outcomes. The \\\\textit{exec_outcome} flags in the training data comes from a pre-run environment from the source website and they are not verified in \\\\textit{ExecEval}. However, training data doesn't include unit-test to avoid potential hacks and confusion. We provide unit test for only validation and test data.\\n\\n18. \\\\textit{potential_dominant_fix_op}: A potential fix op recommended by difflib.\\n\\n19. \\\\textit{lang_cluster}: A generic programming language name the value of \\\\textit{lang} belongs to.\\n\\n20. \\\\textit{prob_desc_description}: Problem description in textual format, math operations are written in latex.\\n\\n21. \\\\textit{prob_desc_input_from}: How the program should take the unit test.\\n\\n22. \\\\textit{prob_desc_output_to}: Where the program should output the result of the unit test.\\n\\n23. \\\\textit{prob_desc_time_limit}: Time limit to solve the problem.\\n\\n24. \\\\textit{prob_desc_memory_limit}: Memory limit to solve the problem.\\n\\n25. \\\\textit{prob_desc_input_spec}: How and in what order the input will be given to the program? It also includes the date range, types, and sizes.\\n\\n26. \\\\textit{prob_desc_output_spec}: How the outputs should be printed. Most of the time the unit test results are matched with an exact string match or floating point comparison with a precision boundary.\\n\\n27. \\\\textit{prob_desc_sample_inputs}: A sample input for the code that is expected to solve the problem described in description.\\n\\n28. \\\\textit{prob_desc_sample_outputs}: The expected output for the \\\\textit{sample_input} that is expected to solve the problem described in description.\\n\\n29. \\\\textit{prob_desc_notes}: Explanation of \\\\textit{sample_inputs} & \\\\textit{sample_outputs}.\\n\\n30. \\\\textit{prob_desc_created_at}: The Unix timestamp when the problem was released. Use \\\\texttt{datetime} lib in Python to parse it to a human-readable format.\\n\\n31. \\\\textit{file_name}: Name of the source jsonl file from where data is loaded.\\n\\n32. \\\\textit{hidden_unit_tests}: a list of unit tests returned as string. use \\\\texttt{json.loads(hidden_unit_tests)} to load the data.\\n\\nI.6 CODE TRANSLATION (CODE TRANSLATION)\\n\\nGiven a source code (\\\\textit{source_code}) in \\\\textit{lang_cluster}, generate a code in target programming language.\"}"}
{"id": "wpTitXWGNO", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our retrieval benchmark has 17 programming languages and our training dataset is the largest that provides annotations of similar codes that are found logically equivalent or correct based on the passing of test cases. For evaluation purposes (i.e., for test sets), we release the input problem description (in NL-Code) or the input code (in Code-Code) only and keep all other metadata confidential. Covered programming languages and their data statistics in both tasks are summarized in Table 9.\\n\\n**Retrieval Evaluation**\\n\\nFigure 12 reports one retrieval task (code-code) performance. As anticipated, the retrieval capability for the same language pair (a.k.a., monolingual retrieval) of our baseline model performances are relatively stronger and we observe performance degradation when performing cross-lingual retrieval between different languages. However, surprisingly, mono-lingual retrieval accuracies for popular languages like C, C++, C#, Python, and Java are lower than others such as Go, Haskell, Javascript, Kotlin, Ruby, Scala etc., possibly due to their large retrieval corpus size and presence of more hard negative candidates (very similar to the correct code). Furthermore it is suspected that the lack of enough resource on D programming language in both The Stack (Kocetkov et al., 2022) and XCODEXVAL is the primary reason for its poor scores.\\n\\n| Corpus/Context Language | Question/Query Language |\\n|-------------------------|-------------------------|\\n| C                       | 91.1                     |\\n| C#                      | 68.8                     |\\n| C++                     | 48.3                     |\\n| D                       | 70.0                     |\\n| Go                      | 79.6                     |\\n| Haskell                 | 63.1                     |\\n| Java                    | 62.8                     |\\n| Javascript              | 81.4                     |\\n| Kotlin                  | 74.7                     |\\n| Ruby                    | 52.9                     |\\n| Scala                   | 70.2                     |\\n| Ocaml                   | 67.2                     |\\n| PHP                     | 56.5                     |\\n| Pascal                  | 70.7                     |\\n| Perl                    | 70.8                     |\\n| Python                  | 66.8                     |\\n| Scala                   | 75.9                     |\\n| C                        | 72.6                     |\\n| C#                      | 87.8                     |\\n| C++                     | 55.8                     |\\n| D                       | 79.9                     |\\n| Go                      | 84.4                     |\\n| Haskell                 | 74.8                     |\\n| Java                    | 68.6                     |\\n| Javascript              | 87.4                     |\\n| Kotlin                  | 82.4                     |\\n| Ruby                    | 67.6                     |\\n| Rust                    | 80.5                     |\\n| Scala                   | 69.5                     |\\n| C                        | 67.4                     |\\n| C#                      | 73.2                     |\\n| C++                     | 83.8                     |\\n| D                       | 70.6                     |\\n| Go                      | 79.6                     |\\n| Haskell                 | 58.6                     |\\n| Java                    | 71.7                     |\\n| Javascript              | 74.5                     |\\n| Kotlin                  | 74.1                     |\\n| Ruby                    | 53.5                     |\\n| Scala                   | 65.6                     |\\n| Ocaml                   | 64.8                     |\\n| PHP                     | 45.8                     |\\n| Pascal                  | 70.5                     |\\n| Perl                    | 67.1                     |\\n| Python                  | 59.8                     |\\n| Scala                   | 64.7                     |\\n| C                        | 46.9                     |\\n| C#                      | 52.7                     |\\n| C++                     | 25.0                     |\\n| D                       | 62.1                     |\\n| Go                      | 64.1                     |\\n| Haskell                 | 48.2                     |\\n| Java                    | 44.6                     |\\n| Javascript              | 86.7                     |\\n| Kotlin                  | 58.1                     |\\n| Ruby                    | 52.4                     |\\n| Scala                   | 65.3                     |\\n| Ocaml                   | 44.6                     |\\n| PHP                     | 57.5                     |\\n| Pascal                  | 49.8                     |\\n| Perl                    | 39.9                     |\\n| Python                  | 86.7                     |\\n| Scala                   | 53.5                     |\\n| Ocaml                   | 77.0                     |\\n| PHP                     | 72.9                     |\\n| Pascal                  | 64.3                     |\\n| Perl                    | 78.8                     |\\n| Python                  | 72.9                     |\\n| Scala                   | 77.6                     |\\n| Ocaml                   | 53.5                     |\\n| PHP                     | 77.0                     |\\n| Pascal                  | 72.9                     |\\n| Perl                    | 72.9                     |\\n| Python                  | 64.3                     |\\n| Scala                   | 78.8                     |\\n| Ocaml                   | 46.4                     |\\n| C#                      | 52.7                     |\\n| C++                     | 25.0                     |\\n| D                       | 62.1                     |\\n| Go                      | 64.1                     |\\n| Haskell                 | 48.2                     |\\n| Java                    | 44.6                     |\\n| Javascript              | 86.7                     |\\n| Kotlin                  | 58.1                     |\\n| Ruby                    | 52.4                     |\\n| Scala                   | 65.3                     |\\n| Ocaml                   | 44.6                     |\\n| PHP                     | 57.5                     |\\n| Pascal                  | 49.8                     |\\n| Perl                    | 39.9                     |\\n| Python                  | 86.7                     |\\n| Scala                   | 53.5                     |\\n| Ocaml                   | 77.0                     |\\n| PHP                     | 72.9                     |\\n| Pascal                  | 64.3                     |\\n| Perl                    | 78.8                     |\\n| Python                  | 72.9                     |\\n| Scala                   | 77.6                     |\\n| Ocaml                   | 53.5                     |\\n| PHP                     | 77.0                     |\\n| Pascal                  | 72.9                     |\\n| Perl                    | 72.9                     |\\n| Python                  | 64.3                     |\"}"}
{"id": "wpTitXWGNO", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13 \u2013 List of ExecEval compiler versions used to evaluate the generated codes.\\n\\nTag Classification: Since it is a multi-class multi-label classification problem, we use f1 score with macro averaging over the classes (in this case the tags) to measure the performance as macro averaging is class population size independent. This is done by first calculating the f1 score for each class (tag) $T \\\\in T$ (the set of all tags) with the following formula Taha and Hanbury (2015)\\n\\n$$f_1(T) = \\\\frac{2 \\\\times \\\\text{Precision}_T \\\\times \\\\text{Recall}_T}{\\\\text{Precision}_T + \\\\text{Recall}_T}$$\\n\\nAnd then the macro average is calculated as the mean of $f_1(T)$ for all $T \\\\in T$ Opitz and Burst (2019).\\n\\n$$f_{1\\\\text{macro}} = \\\\frac{1}{|T|} \\\\sum_{T \\\\in T} f_1(T)$$\\n\\nCode Compilation: Since it is a binary classification problem, we use accuracy which is defined as the proportion of correct prediction among all predictions Metz (1978). That is\\n\\n$$\\\\text{Accuracy} = \\\\frac{TP + TN}{TP + TN + FP + FN}.$$\\n\\nGenerative Tasks: The generative tasks in CODEEVAL (i.e. Automatic Program Repair, Code Translation, Program Synthesis) are all evaluated using pass@k used in Chen et al. (2021).\\n\\nCode Retrieval: The Code-Code, and NL-Code retrieval tasks in CODEEVAL is evaluated using top-k accuracy (Thakur et al., 2021).\\n\\nImplementation Details: Classification tasks: OpenAI chat completion API with gpt-3.5-turbo-0301 model was used at temperature 0.325 and $n = 1$. List prompts for Code2Tag, DescCode2Tag, Code Compilation as figure or inline styling, with example api response. Then evaluate each through corresponding metric as mentioned in Appendix F.\\n\\nGenerative tasks: OpenAI chat completion API with gpt-3.5-turbo-0301 model was used at temperature np.linspace(0, 2, 20) and $n = 1$ for Program Synthesis. Then upon identifying best temperature at 0.325, another batch of codes were generated at temperature 0.325 and $n = 20$.\\n\\nFor APR, Code Translation temperature of 0.325 and $n = 10$ was used. The generated codes were executed with ExecEval with default parameters (follow appendix H for a list of different parameters and their default values) to determine its functional correctness and then evaluated using pass@k. Figure 13 shows the compiler versions used to execute the generated codes.\"}"}
{"id": "wpTitXWGNO", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Retrieval tasks: We finetuned a DPR model with starencoder for both query and corpus encoder. Both NL-Code, and Code-Code were trained with maximum sequence length of 1024, and effective batch size 48 for 37 epochs. The model is trained with a multilingual manner. For Code-Code we used XCODEEVAL as it is, and for NL-Code we made the following template: 'Description: {{description}} Input specification: {{input_spec}} Output specification: {{output_spec}}' for the query string. For evaluation we used corpus provided by XCODEEVAL to generate the dense vectors and then perform queries with test split for both Code-Code, and NL-Code. Finally the top-k accuracies were measured.\\n\\n**TABLE 10 \u2013 Supported languages and their compiler/interpreter versions of our dataset in ExecEval.**\\n\\n| Language | Versions |\\n|----------|----------|\\n| Ruby     | Ruby 3, Ruby |\\n| Javascript | Node.js, JavaScript |\\n| Go       | Go 1.19 |\\n| C++      | GNU C++17, GNU C++17 (64), GNU C++20 (64), GNU C++11, Clang++17 Diagnostics, GNU C++, GNU C++14, GNU C++17 Diagnostics, Clang++20 Diagnostics, MS C++ |\\n| C        | GNU C11, GNU C |\\n| Java     | Java 6, Java 7, Java 17, Java 11, Java 8 |\\n| Python   | PyPy 3, PyPy 3-64, Python 3 + libs, Python 2, PyPy 2, Python 3 |\\n| C#       | MS C#, C# 8, Mono C#, .NET Core C#, C# 10 |\\n| PHP      | PHP 8.1 |\\n| Rust     | Rust, Rust 2021 |\\n| Kotlin   | Kotlin, Kotlin 1.4, Kotlin 1.5, Kotlin 1.7, Kotlin 1.6 |\\n\\nExecEval is an automated code execution and evaluation engine distributed through docker for security and portability. It supports 44 compiler versions for 11 programming languages as shown in table 10. It exposed `NUM_WORKERS` CLI argument to spawn multiple workers that can execute the codes. It is highly scalable in the sense of adding support for more languages or one can just change `NUM_WORKERS` to execute more codes in parallel. At the top level of ExecEval, there is a HTTP server that exposes 2 API endpoints `/api/execute_code`, `/api/all_runtimes`.\\n\\nFigure 15 shows a simple usage of `execute_code` API. By default the execution of a code is stopped when the code doesn't pass a unit test as pass@k depends on whether all the unit tests passed or not. This can be disabled by adding `\u201cstop_at_first_fail\u201d: false`, in which case all unit tests for a given code will be executed irrespective of the outcomes for other unit tests. Figure 6 is generated with disabling `\u2018stop_at_first_fail\u2019`. It is worth noting that, disabling this setting can increase the evaluation time significantly (e.g. in table 3 for program synthesis (N) where 23,320 codes were executed the difference was of approximately 12 minutes and 2 hours 12 minutes where ExecEval was running with 61 workers).\\n\\n**Security Measures**\\n\\nExecEval uses `prlimit`, and `seccomp` to limit system resources allocated for any instance of code executed through the API endpoint in addition to using unique unprivileged users for each worker spawned with `NUM_WORKERS`. Table 11 shows the default values provided to `prlimit`, furthermore `nofile`, and `nproc` are customized for each of the supported languages. The `seccomp` is used to block `socket` system call, which disables network access (this is default). One can enable network access by adding `\u201cblock_network\u201d: false` in the request body as shown in fig. 15. Similarly, adding a \u2018limits\u2019 object in the request body allows one to change the\\n\\n12. https://github.com/facebookresearch/DPR\\n13. https://man7.org/linux/man-pages/man1/prlimit.1.html\\n14. https://man7.org/linux/man-pages/man2/seccomp.2.html\"}"}
{"id": "wpTitXWGNO", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FIGURE 14 \u2013 An example runtime object the response from /api/all_runtimes contains list of such objects.\\n\\n```\\n{\"language\": \"Python 3\",\\n  \"source_code\": \"a, b = map(int, input().strip().split())\\n  print(a+b)\\n\",\\n  \"unittests\": [\\n    {\"input\": \"1 1\", \"output\": [\"2\"]},\\n    {\"input\": \"1 10\", \"output\": [\"11\"]}\\n  ]\\n}\\n```\\n\\nFIGURE 15 \u2013 On left: An example request body for /api/execute_code The Python code takes 2 numbers as input and prints their sum. On right: The response by ExecEval in response to the request shown in left.\\n\\n```\\n{\"data\": [\\n  {\"exec_outcome\": \"PASSED\",\\n   \"input\": \"1 1\",\\n   \"output\": [\"2\"],\\n   \"result\": \"2\"},\\n  {\"exec_outcome\": \"PASSED\",\\n   \"input\": \"1 10\",\\n   \"output\": [\"11\"],\\n   \"result\": \"11\"}\\n]}\\n```\"}"}
{"id": "wpTitXWGNO", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FIGURE 5 \u2013 Left: execution outcome for different languages, the solutions are evaluated with ExecEval against our unit tests; Right: passed solutions at different temperatures. Both evaluations are done at 20 different temperature values.\\n\\nFIGURE 6 \u2013 Top: The reasoning spectrum of gpt-3.5-turbo-0301, X-axis represents the unit tests and the color represents the corresponding test outcomes for different languages in the Y-axis; Bottom: The unit tests are grouped together from the reasoning spectrum to get an overall idea of the performance of execution outcomes. All the evaluations are done at temperature 0.32 and $n = 10$.\\n\\nexecutability decreases. We identify the most successful PASSED tests at the temperature of 0.32. Figure 3 (right) presents a comparison of code executability across different languages. For each of the unit test cases, we test it with 20 different temperature values and finally select the temperature with highest PASSED execution. We implemented this approach exclusively for a program synthesis task, utilizing this optimal temperature as a pseudo signal for the best parameter for the remaining tasks. While this incurred a significant budget, it is worth noting that with a larger budget, employing optimal parameter search for all tasks and optimal variation would be ideal. In our evaluation, We see that Rust has overall low code executability. On the other hand, interpretable languages like Python, PHP, and Javascript have high executability.\\n\\nDifficulty analysis Figure 4 (right) shows the distribution of PASSED problems written in C++ for different difficulty levels. We see a sharp decrease in performance as the problems become harder.\\n\\nReasoning capability Figure 6 shows a reasoning spectrum of ChatGPT on Program Synthesis. A program can be considered a reasoning path to produce an output for an input of a unit test. We define reasoning spectrum as the range or continuum of different reasoning approaches or strategies that a program can employ to produce an output for a given input in a test case. The reasoning spectrum encompasses various levels of execution outcomes by the program in between different languages. The same colored vertical line along different languages represents an agreement of execution outcome for the corresponding languages. Given any two languages, when the code compiles successfully for both but one produces PASSED and the other produces WRONG ANSWER, we can conclude that...\"}"}
{"id": "wpTitXWGNO", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024.\\n\\nThere is a gap between the reasoning capability of the two languages. We notice a low agreement in the reasoning spectrum between languages suggesting that a further transfer learning mechanism can be applied to transfer reasoning capability from high-resource to low-resource languages.\\n\\nThough Figure 6 (top) shows a general comparable structure of reasoning capability of an LLM for different languages, it does not show the overall performance within each language. By grouping the same execution outcomes together, Figure 6 (bottom) shows exceptional code executability on Python (no compilation error). However, their reasoning capability (# of PASSED unit tests) remains fairly comparable with other languages.\\n\\n### Evaluation of Program Synthesis on Small Models\\n\\n| Model                  | Trained        | Metric | C   | C#  | C++ | Go  | Java | Javascript | Kotlin | PHP | Python | Ruby | Rust | Avg    |\\n|------------------------|----------------|--------|-----|-----|-----|-----|------|-----------|--------|-----|--------|------|------|--------|\\n| Starcoderbase-3b       | \u2713              | pass@5 | 1.90| 1.99| 3.45| 1.60| 2.36  | 2.73      | 2.30   | 2.48| 2.52   | 2.33 | 1.13 | 2.25   |\\n| CodeLlama-7b-Instruct  | \u00d7              | pass@5 | 1.12| 1.74| 2.64| 1.65| 0.87  | 0       | 0.52   | 1.69 | 2.14 | 0.61 | 0.87 | 1.26   |\\n| CodeLlama-13b-Instruct | \u00d7              | pass@5 | 4.57| 4.29| 6.40| 2.69| 3.29  | 2.72      | 4.01   | 3.97| 4.97   | 2.88 | 2.10 | 3.81   |\\n\\nFor Program Synthesis tasks, we fine-tuned the Starcoderbase-3B model (Li et al., 2023) with our trained data. We also evaluated the CodeLlama-7b-Instruct-hf and CodeLlama-13b-Instruct-hf models with our evaluation data. A 3B fine-tuned model is better than a 7B instruct model but worse than a 13B instruct model. We observe that training a smaller model with the training data performs well on our task rather than using a general-purpose instruct/chat model. However, large instruct models are better than smaller fine-tuned models. So, the impact of our training dataset varies between different scales. Comparing the results between Table 3 and Table 5 also provides a general idea of how challenging our task is.\\n\\n### Data Limitations\\n\\nThough the codes are written by a diverse group of experts in a diverse number of languages, data is collected from a single source thus limiting the domain diversity. Besides, there is a clear discrepancy between the resource of different programming languages (see Appendix E-Figure 9 in supp. material) and most of the codes in XCODEEVAL are at the document level and often written in a non-modular way without a doc-string. In Appendix K, we discuss the possibilities of evaluation data leakage.\\n\\n### Ethics, Potential Risks & Documentation\\n\\nThough the data is collected from openly available sources, it has not been humanly audited. We have made our best efforts to use automated tools for identifying and removing codes with sensitive details, resulting in the removal of approximately 2 million samples from our original collection. However, it is important to emphasize that despite our diligent efforts, code can still potentially contain sensitive information and security vulnerabilities, although not something that is not openly available. Additionally, code datasets may reflect biases present in the original codebase or the developers who contributed to it.\\n\\nXCodeEval documentations in supplementary Appendix H to Appendix L, follow all the necessary guidelines of NeurIPS datasets-track (e.g., datasheets (Gebru et al., 2021), nutrition label (Holland et al., 2020), and data card (Hutchinson et al., 2021)). Our github and huggingface repositories provide two valuable sources of both data access and documentation. To mitigate risks and resolve frequently asked questions, we regularly address queries or issues there.\\n\\n### Conclusion & Future Work\\n\\nWe have introduced XCODEEVAL, a large-scale multilingual multitask benchmark for code-based large language models. XCODEEVAL features seven different tasks involving code understanding, generation, translation and retrieval in up to 17 programming languages, and it employs an execution-based evaluation protocol. We have also presented ExecEval, a multilingual code execution engine that supports all the programming languages in XCODEEVAL. In summary, the combination of XCODEEVAL and ExecEval presents a novel framework that offers a fresh perspective for examining and analyzing large language models, facilitating comprehensive and to some extent highly interpretable investigations. We hope that by utilizing the extensive metadata and execution-based evaluation, there is potential for the discovery of new scaling laws and emergent capabilities.\"}"}
{"id": "wpTitXWGNO", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"REFERENCES\\n\\nRajas Agashe, Srinivasan Iyer, and Luke Zettlemoyer. 2019. Juice: A large scale distantly supervised dataset for open domain context-based code generation. arXiv preprint arXiv:1910.02216.\\n\\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021a. Unified pre-training for program understanding and generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 2655\u20132668. Association for Computational Linguistics.\\n\\nWasi Uddin Ahmad, Md Golam Rahman Tushar, Saikat Chakraborty, and Kai-Wei Chang. 2021b. Avatar: A parallel corpus for java-python program translation. arXiv preprint arXiv:2108.11590.\\n\\nBen Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, Sujan Kumar Gonugondla, Hantian Ding, Varun Kumar, Nathan Fulton, Arash Farahani, Siddhartha Jain, Robert Giaquinto, Haifeng Qian, Murali Krishna Ramanathan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, Dan Roth, and Bing Xiang. 2022. Multi-lingual evaluation of code generation models. arXiv preprint arXiv:2210.14868.\\n\\nJacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. 2021. Program synthesis with large language models. CoRR, abs/2108.07732.\\n\\nBerkay Berabi, Jingxuan He, Veselin Raychev, and Martin T. Vechev. 2021. Tfix: Learning to fix coding errors with a text-to-text transformer. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 780\u2013791. PMLR.\\n\\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. 2022. Multipl-e: A scalable and extensible approach to benchmarking neural code generation.\\n\\nShubham Chandel, Colin B Clement, Guillermo Serrato, and Neel Sundaresan. 2022. Training and evaluating a jupyter notebook data science assistant. arXiv preprint arXiv:2201.12901.\\n\\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397.\\n\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374.\"}"}
{"id": "wpTitXWGNO", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nChild, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171\u20134186. Association for Computational Linguistics.\\n\\nJames Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and James Prather. 2022. The robots are coming: Exploring the implications of openai codex on introductory programming. In Proceedings of the 24th Australasian Computing Education Conference, ACE '22, page 10\u201319, New York, NY, USA. Association for Computing Machinery.\\n\\nLingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi Lei, Renting Rui, Jianghao Lin, Yuchen Fang, Yifan Liu, Jingkuan Wang, Siyuan Qi, Kangning Zhang, Weinan Zhang, and Yong Yu. 2023. Codeapex: A bilingual programming evaluation benchmark for large language models. CoRR, abs/2309.01940.\\n\\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 III au2, and Kate Crawford. 2021. Datasheets for datasets.\\n\\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022. Unixcoder: Unified cross-modal pre-training for code representation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 7212\u20137225. Association for Computational Linguistics.\\n\\nRahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. Deepfix: Fixing common c language errors by deep learning. In Proceedings of the aaai conference on artificial intelligence, volume 31.\\n\\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. 2021. Measuring coding challenge competence with APPS. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.\\n\\nSarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. 2018. The dataset nutrition label: A framework to drive higher data quality standards. arXiv preprint arXiv:1805.03677.\\n\\nSarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. 2020. The dataset nutrition label. Data Protection and Privacy, 12(12):1.\\n\\nXing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing source code with transferred api knowledge. pages 2269\u20132275.\\n\\nJunjie Huang, Chenglong Wang, Jipeng Zhang, Cong Yan, Haotian Cui, Jeevana Priya Inala, Colin Clement, Nan Duan, and Jianfeng Gao. 2022. Execution-based evaluation for data science code generation models. arXiv preprint arXiv:2211.09374.\\n\\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic code search. CoRR, abs/1909.09436.\\n\\nBen Hutchinson, Andrew Smart, Alex Hanna, Emily Denton, Christina Greer, Oddur Kjartansson, Parker Barnes, and Margaret Mitchell. 2021. Towards accountability for machine learning data-sets: Practices from software engineering and infrastructure. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 560\u2013575.\"}"}
{"id": "wpTitXWGNO", "page_num": 44, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"import subprocess\\n\\n# Run 'ps -ef' command\\ncommand = ['ps', '-ef']\\nprocess = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\\noutput, error = process.communicate()\\n\\n# Decode and print the output\\nif output:\\n    print(output.decode('utf-8'))\\nelse:\\n    print(f'Error: {error.decode(\"utf-8\")}')\\n\\n{\"data\": [{\"exec_outcome\": \"RUNTIME_ERROR\", \"input\": \"\", \"output\": [\"\"], \"result\": \"Traceback (most recent call last):\\n File \\\"/code_store/6cd9b5215a524abab3712bc897de2be5/test.py\\\", line 5, in <module>\\n process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\\n          ^^^^^^^^^^^^^^^\\n File \\\"/usr/lib/python3.11/subprocess.py\\\", line 890, in __init__\\n errread, errwrite = self._get_handles(stdin, stdout, stderr)\\n          ^^^^^^^^^^^^^^^^^^^^\\n File \\\"/usr/lib/python3.11/subprocess.py\\\", line 1664, in _get_handles\\n c2pread, c2pwrite = os.pipe()\\n          ^^^^^^^^\\nOSError: [Errno 24] Too many open files\\n\"}]\\n\\nFIGURE 18 \u2013 Left: A python code performing a subprocess call to run 'ps -ef'. Right: ExecEval responded with RUNTIME_ERROR as nofile (table 11) is limiting the execution of such codes.\"}"}
{"id": "wpTitXWGNO", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ahmad et al., 2021a) Notably, PaLM (Chowdhery et al., 2022) and AlphaCode (Li et al., 2022) outperform average human participant in competition-level coding. Thus, researchers attempt to build increasingly difficult and factual code generation tasks. These tasks can be classified as code-to-code generation and text-to-code generation.\\n\\nAs for code-to-code generation tasks like automatic program repair (APR) (Tufano et al., 2019) and code translation (Lu et al., 2021), the metric-based automatic evaluation measures like BLEU (Papineni et al., 2002), CodeBLEU (Ren et al., 2020), and exact match scores are sub-optimal for evaluating the quality of a generated code. To improve the reliability and feasibility for code generation evaluation, Berabi et al. (2021) create a large-scale JavaScript patch repair dataset from GitHub commits, where 52 error types are detected by a static analyzer ESLint. They further drive efforts in enhancing program repair evaluation by providing an error removal metric to take various form of error fixes into consideration. To address the nature of code semantic and syntactic evaluation, execution-based evaluation with comprehensive test suites has a growing demand. A popular Java APR benchmark Defects4J (Just et al., 2014) takes the number of correct fixes into account, where a correct fix should pass all test cases and provide a desired functionality. Nevertheless, Defects4J does not possess a cohesive training corpus. A common strategy to address this limitation is to construct the training dataset using GitHub's publicly available repositories, and relying on bug-specific commit messages (Zhu et al., 2021). However, this heuristic-based approach includes bug-irrelevant commits and unrelated code pairs, which can significantly affect the quality of collected training dataset (Xia and Zhang, 2022).\\n\\nFor text-to-code, the widely used dataset CONCODE (Iyer et al., 2018) consists of a large collection of natural language (NL) comments and Java code snippets. Specifically, this dataset is constructed by scraping code snippets from open-domain Java GitHub repositories and utilizing heuristics to extract NL comments from Javadoc.\\n\\nBy following a similar approach, JuICe (Agashe et al., 2019) collects publicly available Jupyter notebooks from GitHub, and CoNaLa (Yin et al., 2018) collects Python and Java codes with NL comments from StackOverflow posts. Besides, they attempt to improve the quality with professional annotators. In addition, MoCoNaLa (Wang et al., 2022a) extends CoNaLa to support more natural languages. Despite their coverage, the general lexical-based evaluation metrics are insufficient to measure the correctness of generated codes. To alleviate this limitation, ODEX (Wang et al., 2022b) provides execution-based evaluation via human-written test cases of diversified Python libraries. This execution-based paradigm has been widely applied to evaluate benchmarks in Data Science domain like DSP (Chandel et al., 2022), DS-1000 (Lai et al., 2022) and Exe-DS (Huang et al., 2022) as well as general code generation benchmarks in single-language settings such as HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and APPS (Hendrycks et al., 2021). Apart from HumanEval, CoderEval (Yu et al., 2023) further leverages the contextual information and achieves a full spectrum of test coverage with additional manually crafted tests, providing 100% test coverage. To improve the diversity of code generation tasks, Fu et al. (2023) propose a bilingual code evaluation benchmark CodeApex to support both English-to-Code and Chinese-to-Code generation tasks. As for more particular multi-turn MTPB (Nijkamp et al., 2022), multi-language CodeContests (Li et al., 2022), and domain specific BioCoder (Tang et al., 2023) benchmarks, they all leverage test cases, and exploit code execution for better evaluation.\\n\\nTo make sure we do not have train and (validation, test) overlap, at first we divide the set of problems into two sets. In one set we keep all the problems for which we do not have a complete set of unit tests. In another set, we keep the problems where we have a complete set of unit tests that ensures the correctness of the solution of the problem. We use the first set for training and the latter set for validation and test data. Figure 7 shows the chronological distribution of our training, validation, and test data. After selecting validation and test problem sets, we have thousands of solutions for each of the problems. But these problems are not divided into validation and test splits. As a heuristic, we can consider the tag distribution as the domain of the problem. To ensure that we have proper domain coverage we employ Algorithm 1. Algorithm 1 ensures that validation and test sets contain the same tag sets as the training set. In addition to that, it also selects the best possible splitting point based on the number of solutions for each tag category.\"}"}
{"id": "wpTitXWGNO", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1: Validation and Test Split Creation\\n\\nInput: A held-out dataset $D_{ho}$, a fraction value $\\\\gamma$ where $0 \\\\leq \\\\gamma \\\\leq 1$, an integer $N$ indicating number of seeds.\\n\\nOutput: $D_{valid}$, $D_{test}$ splits\\n\\nInitialize: $count = 0$, bestScore = $\\\\gamma + 1$\\n\\nwhile $count < N$ do\\n  seed = getSeed()\\n  Shuffle $D_{ho}$\\n  $D_{valid} = D_{ho}[0 : |D_{ho}| \\\\times \\\\gamma]$\\n  $T_{valid} = \\\\text{set of tags in } D_{valid}$\\n  $D_{test} = D_{ho}[|D_{ho}| \\\\times \\\\gamma : |D_{ho}|]$\\n  $T_{test} = \\\\text{set of tags in } D_{test}$\\n  if $T_{valid} \\\\neq T_{test}$ then\\n    continue\\n  end if\\n  for all $T$ in $T_{valid}$ do\\n    $\\\\gamma_T = \\\\frac{\\\\#\\\\text{samples in } D_{valid} \\\\text{ with tag } T}{\\\\#\\\\text{samples in } D_{test} \\\\text{ with tag } T}$\\n  end for\\n  $\\\\mu = \\\\text{geoMean}\\\\{\\\\gamma_T | T \\\\in T_{valid}\\\\}$\\n  if $|\\\\gamma - \\\\text{bestScore}| > |\\\\gamma - \\\\mu|$ then\\n    bestScore = $\\\\mu$\\n    save current split $\\\\{D_{valid}, D_{test}\\\\}$\\n    $count = count + 1$\\n  end if\\nend while\\n\\nHyper Parameter Tuning for Circulation Problem\\n\\nLet $M$ be the number of samples we want to select for any set of submissions. We call any $(m_p, m_t, x_p, x_t)$ a valid tuple if the flow network has a feasible flow for the circulation problem defined in eq. in 2.1. Let $d = \\\\lfloor \\\\left(\\\\frac{\\\\sum_{s} P_{N_i=1} f(s, P_{i}) - M}{2} \\\\right)^2 / \\\\Delta \\\\rfloor$, representing the squared difference between samples we want and the samples selected for the flow and $\\\\Delta$ reduces the resolution in which...\"}"}
{"id": "wpTitXWGNO", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we look for differences. Here \\\\( d \\\\) defines a boundary from \\\\( M \\\\) where we allow choosing an expected solution with \\\\( m_p, m_t, x_p, \\\\) and \\\\( x_t \\\\). Finally, the lexicographical ordering \\\\( (\u2212d, m_t, \u2212x_t, \u2212x_p, m_p) \\\\) is used to find the largest element in the collection of valid tuples which always exist if we limit our search space to a finite set. The largest element in this ordering depicts the nearest (close to \\\\( M \\\\)) selection of samples that maximizes the minimum number of samples per tag \\\\( m_t \\\\). When there are many solutions with the same \\\\((\u2212d, m_t)\\\\), we prioritize reducing the maximum number of samples per tag, \\\\( x_t \\\\). Similarly, we prioritize \\\\( x_p \\\\) and \\\\( m_p \\\\) as defined in the lexicographical ordering.\\n\\nD.1 SEARCH TECHNIQUES\\n\\n1. It was manually checked that \\\\((m_p, m_t, x_p, x_t) = (1, 1, 1000, 1000)\\\\) is a valid tuple for any set of submissions that were processed and \\\\( \u2206 = 1000\\\\) was chosen.\\n\\n2. In Tag classification task (Section E.1) and Code compilation task (Section E.2), \\\\( M = 2000, 10000\\\\) for any language for validation, test split respectively. For Code translation (Section E.4) \\\\( M \\\\) was \\\\( 400, 2000\\\\) for the same.\\n\\n3. Search largest tuple \\\\((\u2212d_1, m_t, \u2212x_t, \u2212x_p, m_p)\\\\) where \\\\( m_t \\\\in \\\\{1, 6, 11, \\\\cdots, 496\\\\} \\\\), \\\\( m_p \\\\in \\\\{1, 2, 3, \\\\cdots, 19\\\\} \\\\) and \\\\( x_p = x_t = 1000\\\\). Since \\\\((m_p, m_t, x_p, x_t) = (1, 1, 1000, 1000)\\\\) is a valid solution, hence the set of valid tuples is nonempty. Let \\\\( f_1 \\\\) be the flow for the flow network defined for \\\\( m_t, \u2212x_t, \u2212x_p, m_p \\\\). Let \\\\( f_P_1 = \\\\max_{1 \\\\leq i \\\\leq N} f_1(s, P_i) \\\\) and \\\\( f_T_1 = \\\\max_{1 \\\\leq k \\\\leq K} f_1(T_k, t) \\\\) be the maximum flow through edges from \\\\( s \\\\) to \\\\( P_i \\\\), and same through edges from \\\\( T_k \\\\) to \\\\( t \\\\).\\n\\n4. Now again search the largest tuple \\\\((\u2212d_2, m_t, \u2212x_t, \u2212x_p, m_p)\\\\) where \\\\( m_t \\\\in \\\\{m_t_1, m_t_1 + 1, \\\\cdots, m_t_1 + 49\\\\} \\\\), \\\\( x_t \\\\in \\\\{f_T_1 \u2212 100, f_T_1 \u2212 80, \\\\cdots, f_T_1\\\\} \\\\), \\\\( x_p \\\\in \\\\{f_P_1 \u2212 5, f_P_1 \u2212 4, \\\\cdots, f_P_1\\\\} \\\\), \\\\( m_p \\\\in \\\\{m_p_1, m_p_1 + 1\\\\} \\\\). Since \\\\( m_t_1, f_T_1, m_p_1 \\\\) is included a solution is found in this step too. Define \\\\( f_P_2, f_T_2 \\\\) similar to previous step.\\n\\n5. Finally search the largest tuple \\\\((\u2212d_3, m_t, \u2212x_t, \u2212x_p, m_p)\\\\) where \\\\( m_t = m_t_2 \\\\), \\\\( x_t \\\\in \\\\{f_T_2 \u2212 100, f_T_2 \u2212 99, \\\\cdots, f_T_2\\\\} \\\\), \\\\( x_p = x_p_2 \\\\), \\\\( m_p = m_p_2 \\\\). While it is not an exhaustive search, we prioritize minimizing \\\\( x_t \u2212 m_t \\\\) over \\\\( x_p \u2212 m_p \\\\).\\n\\nD.2 RESULTS\\n\\nWe compared the performance of data selection using circulation problem technique with randomly selecting equal number of samples for validation and test sets of all languages and measured the skew \\\\( \\\\tilde{\\\\mu}_3 \\\\) and standard deviation \\\\( \\\\sigma \\\\) of the distribution of tags in the selected data. Here lower value of \\\\(|\\\\tilde{\\\\mu}_3|\\\\) means more symmetric distribution. On the other hand, a lower value of \\\\( \\\\sigma \\\\) represents that the number of samples in each tag are closer to the mean.\\n\\nE TASKS CONSTRUCTION PROCESS\\n\\nE.1 TAG CLASSIFICATION\\n\\nWe formulate the task as a multi-label classification problem in two settings: Code-to-Tag (Code2Tag) and Problem Description-and-Code to Tag (DesCode2Tag). In Code2Tag, given a code \\\\( C \\\\) in any language, the task is to predict the corresponding tag set \\\\( T \\\\). In DesCode2Tag, the natural language problem description is also given as input in addition to the code. The performance difference between Code2Tag and DesCode2Tag settings can suggest if the problem description can help models identify the problem tags (i.e., the type of solution needed).\\n\\nFor these tasks, the split for validation and test is done with a ratio of \\\\( 1 : 5 \\\\) (i.e., \\\\( \u03b3 = 0.2 \\\\)) using Algorithm 1. To get the final \\\\( D \\\\) valid and \\\\( D \\\\) test with a feasible number of samples, we use flow network-based data selection approach with the details of hyper-parameter settings presented in Section 2.1.\\n\\nThe distribution of the samples according to the tags is presented in Fig 8.\\n\\nWe further propose a language-specific tag classification task, in which each programming language has its own Code2Tag and DesCode2Tag settings.\"}"}
{"id": "wpTitXWGNO", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 7 \u2013 Comparison of skew and standard deviation of tags using circulation problem technique and random data selection (lower value is better).\\n\\n| Language | Validation | Test | Validation | Test |\\n|----------|------------|------|------------|------|\\n|          | Random Circ. | Random Circ. | Random Circ. | Random Circ. |\\n| Tag Classification | 2.778 | 2.499 | 2.848 | 2.440 |\\n| | 249.161 | 213.849 | 880.881 | 772.549 |\\n| C++ | 2.405 | 1.873 | 2.315 | 1.655 |\\n| | 233.530 | 157.889 | 1154.538 | 751.023 |\\n| Python | 2.731 | 2.365 | 2.689 | 2.173 |\\n| | 265.193 | 240.248 | 1125.133 | 992.904 |\\n| Java | 2.652 | 1.990 | 2.545 | 2.050 |\\n| | 258.587 | 207.881 | 1175.790 | 972.703 |\\n| C# | 3.066 | 2.598 | 2.971 | 2.506 |\\n| | 314.219 | 291.813 | 846.426 | 760.069 |\\n| Code Translation | 2.744 | 2.455 | 2.941 | 2.332 |\\n| | 117.298 | 99.261 | 267.214 | 215.881 |\\n| C++ | 2.424 | 2.112 | 2.287 | 1.565 |\\n| | 131.632 | 120.979 | 243.100 | 150.498 |\\n| Python | 2.533 | 2.379 | 2.635 | 2.294 |\\n| | 123.710 | 110.076 | 271.219 | 237.179 |\\n| Java | 2.558 | 2.208 | 2.605 | 1.827 |\\n| | 134.314 | 114.840 | 259.510 | 193.211 |\\n| C# | 3.147 | 2.532 | 2.943 | 2.395 |\\n| | 103.838 | 96.747 | 250.049 | 220.615 |\\n| PHP | 2.506 | 2.744 | 2.520 | 2.730 |\\n| | 59.321 | 59.877 | 270.582 | 278.530 |\\n| Rust | 2.520 | 2.393 | 2.534 | 2.311 |\\n| | 59.269 | 60.253 | 269.352 | 264.507 |\\n| Go | 2.807 | 2.359 | 2.676 | 2.424 |\\n| | 72.415 | 66.666 | 266.565 | 254.986 |\\n| Javascript | 2.611 | 2.611 | 2.473 | 2.473 |\\n| | 64.090 | 64.090 | 246.483 | 246.483 |\\n| Ruby | 2.875 | 2.686 | 2.968 | 2.762 |\\n| | 74.153 | 70.760 | 280.000 | 271.539 |\\n| Kotlin | 2.865 | 2.576 | 3.108 | 2.534 |\\n| | 59.765 | 56.114 | 266.430 | 257.155 |\\n\\nTable 8 \u2013 Size of the datasets for each task and the evaluation metrics. For Program Synthesis, the train data comes from 7,514 problems of 11-17 languages where the input for validation and test data is only natural language text (problem description) independent of programming languages. For all other tasks, validation and test samples are reported for a total number of languages.\\n\\n| Task Type | Task | Lang | Train | Validation | Test | Metric |\\n|-----------|------|------|-------|------------|------|--------|\\n| Generative | Tag Classification | 11 | 5,494,008 | 18,696 | 74,733 | macro-f1 |\\n| Code Compilation | Code Compilation | 11 | 19,915,150 | 6,394 | 30,388 | accuracy |\\n| Automatic Program Repair | Automatic Program Repair | 11 | 4,672,070 | 5,068 | 17,699 | pass@k |\\n| Code Translation | Code Translation | 11 | 5,538,841 | 7,034 | 20,356 | pass@k |\\n| Code-Code Retrieval | Code-Code Retrieval | 17 | 45,270 | 2,335 | 9,508 | Acc@k |\\n| NL-Code Retrieval | NL-Code Retrieval | 17 | 55,924 | 2,780 | 11,157 | |\\n\\nGiven a code $C$ in a language $L$ and its compiler or interpreter version $B$, the code compilation task is to decide whether the code compiles or not. The validation and test splits are created using a modified version of Algorithm 1 that balances the partition based on the compilation outcome of the code instead of the tags of the problem that the code belongs to. We use a ratio $\\\\gamma$ of $1 : 5$. Then a simplified version of the circulation problem is used to prevent too many codes coming from a single problem, and also to ensure a balanced output distribution. The details of hyper-parameter settings of $23$.}"}
{"id": "wpTitXWGNO", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We propose three generative tasks which require a global understanding of programming languages. For the evaluation of generative tasks, we follow execution-based evaluation instead of lexical similarity. All the generative tasks are evaluated using ExecEval execution engine. We provide complete unit tests for all problems in the validation and test dataset which also satisfy the conditions of the input-output description of the problem.\\n\\nE.3 PROGRAM SYNTHESIS\\nGiven a problem described in natural language, program synthesis task is to write a program that solves the problem. We can express each sample in the dataset as a tuple $\\\\langle C, P, l, L \\\\rangle$, where $C$ denotes a solution code written in a programming language $L$ for the problem $P$, and $l$ denotes the compiler/interpreter version of the code. All code samples in our dataset are unique and marked as a correct solution ($\\\\text{PASSED}$ outcome) to the problem. The validation and test splits are created from the heldout problems using Algorithm 1 with a ratio ($\\\\gamma$) of $1 : 9$. The generated code is judged based on executions on the unit tests.\\n\\nE.4 CODE TRANSLATION\\nEach sample in the code translation data can be expressed as a tuple $\\\\langle C, P, l, L \\\\rangle$, where $C$ denotes a set of solution codes in a programming language $L$ for the problem $P$, and $l$ denotes the compiler/interpreter version of the code. All codes in set $C$ are unique and guaranteed to be marked as a correct ($\\\\text{PASSED}$ outcome) solution to the problem by the compiler/interpreter. The validation and test splits are created from the held-out problems using Algorithm 1 with a ratio ($\\\\gamma$) of $1 : 5$, and employ the data selection method with flow network (Sec. 2.1) to have a practical evaluation setup while ensuring a balanced distribution over problems and tags. Figure 9 shows the distribution of the machine translation tasks. Since code translation considers all possible directions of translation between languages, in addition, to train, validation, and test split, we also provide a small validation split.\"}"}
{"id": "wpTitXWGNO", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9 \u2013 Distribution of samples across all problems in the train, validation, test splits for all languages in the machine translation task.\\n\\nE.5 AUTOMATIC PROGRAM REPAIR (APR)\\n\\nWe consider APR as a task to synthesize a fix for a detected program bug. We create a bug-fix pair by matching a buggy code (1-5 execution outcome in Sec. 2.2) with a PASSED solution. Given a bug-specific defect, the objective of this task is to generate a correct fix that passes all the unit tests.\\n\\nLet $C = \\\\{C_1, \\\\ldots, C_m\\\\}$ be the set of programs submitted by a participant in a chronological order in order to solve a specific problem $P$. Some of these submissions can be PASSED. We create the \u2018bug-fix\u2019 pairs from $C$ as follows.\\n\\n1. We iterate over $C$ and mark the PASSED ones as \u2018fixed\u2019. Let $C^*_j$ is one such case.\\n2. For each buggy submission that was made before $C^*_j$, we measure its lexical similarity with $C^*_j$ and select the one (say $C_k$ where $k < j$) with the highest similarity score to pair it with $C^*_j$ and form a bug-fix pair $(C_k, C^*_j)$. We use difflib\\\\(^{11}\\\\) to measure the similarity.\\n3. With each bug-fix pair $(C_k, C^*_j)$, we also include the corresponding problem description $P$ and execution outcome $V_k$ (Section 2.2) of $C_k$.\\n4. The tuple $(C_k, C^*_j, P, V_k)$ represents a sample in our APR task.\\n\\nWe repeat this process for each participant and problem to create the final APR dataset. As reported in Table 8, it comprises more than 5M practical bug-fix pairs and supports 11 programming languages.\\n\\nFor data selection in APR, we considered execution outcome (section 2.2) as tags in the network flow construction (section 2.1).\\n\\nDue to the large input specification of the APR task, sometimes the input sequence length becomes too large. However, we have not compromised the benchmarks by selecting only small sequence length samples but rather keep them as challenging tasks for the language models. Figure 10 shows the length distribution of validation and test input sequence.\\n\\nE.6 CODE RETRIEVAL\\n\\nCode retrieval tasks typically aim to measure the mere semantic relatedness between a natural language (NL) query and a programming language (PL) code. However, a code that is relevant, can still be buggy and thus be misleading (see an example in Figure 11). In view of this, we propose two new and more challenging retrieval tasks in our benchmark, which require a deeper understanding of the NL query and code. In particular, we propose NL-Code and Code-Code retrieval tasks that\\n\\n\\\\(^{11}\\\\)https://docs.python.org/3/library/difflib.html\"}"}
{"id": "wpTitXWGNO", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10 \u2013 Distribution of sequence length of tokenized (bigscience/bloom) samples in the validation and test splits for all languages in the APR task (Section E.5). Each sample contains a buggy code with its problem description.\\n\\n INVOLVE IDENTIFYING A CORRECT CODE FROM A LARGE POOL OF CANDIDATES CONTAINING SIMILAR CODES. IN BOTH TASKS, FOR EACH PROGRAMMING LANGUAGE, WE AGGREGATE ALL THE SUBMITTED CODES AND THEIR TEST CASES TO CREATE A RETRIEVAL CORPUS AND A TESTBED FOR EVALUATING THEIR CORRECTNESS AGAINST TEST CASES. FIGURE 9 GIVES A DETAILED STATISTICS OF OUR RETRIEVAL TASKS. THE DATASETS FOR THE SUBTASKS AND THE EVALUATION SCHEMA ARE DISCUSSED BELOW.\\n\\n1. ```python\\ndef find_median(uns):\\n    sorted_nums = sorted(uns)\\n    mid = len(uns)//2\\n    return mid\\n```\\n\\n2. ```python\\nfind_median([4,2,3,1,5])\\n```\\n\\nFigure 11 \u2013 A code retrieval example. The candidate code on the left has a bug highlighted in red and that on the right has a fix highlighted in green. Both our proposed NL-Code and Code-Code retrieval tasks ensure differentiating between them and pose a more challenging task that aims to comprehend both semantic and logical relatedness.\\n\\n**NL-Code Retrieval**\\n\\nThis task involves matching an NL problem description to the most relevant and correct code from a pool of candidates. An example of an NL description and its corresponding codes are shown in Figure 1. To gather data for this task, we only use instances where the NL description is valid and there is at least one correct solution code (i.e., with execution outcome PASSED). For an NL problem description, we consider all the correct solutions as positive examples and all the wrong (or buggy) solutions as negative examples.\\n\\n**Code-Code Retrieval**\\n\\nGiven an input code (as a query), this task involves finding similar and logically equivalent code (i.e., passes the same set of test cases) from a collection of candidates. We ensure that the query code solves a specific problem (i.e., a correct solution without any detected bugs) and evaluate whether the retrieved candidate also solves the same problem or not. To collect data for this task, we only consider the programming problems which have at least two correct code solutions that pass all the corresponding test cases (i.e., with execution outcome PASSED).\"}"}
{"id": "wpTitXWGNO", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"TABLE 9 \u2013 Retrieval subtasks statistics. |Size| denotes the number of instances. For each train/dev instance, we provide multiple positive and negative examples, and |Pos| and |Neg| refer to the total number of positive and negative annotations.\\n\\n| Lang | Subtask | Train | Dev | Test |\\n|------|---------|-------|-----|------|\\n|      | Corpus  | Size  | Pos | Neg |\\n| C#   | NL-Code | 5,196 | 149,000 | 146,852 |\\n|      | Code-Code | 209 | 11,282 | 11,293 |\\n|      |          | 853 |          |    |\\n|      |          | 787,516 |        |      |\\n| C++  | NL-Code | 4,878 | 75,386 | 55,579 |\\n|      | Code-Code | 207 | 6,574 | 4,757 |\\n|      |          | 828 |          |    |\\n|      |          | 251,147 |        |      |\\n| D    | NL-Code | 3,359 | 7,624 | 3,655 |\\n|      | Code-Code | 133 | 351 | 142 |\\n|      |          | 521 |          |    |\\n|      |          | 15,984 |        |      |\\n| Go   | NL-Code | 3,764 | 25,656 | 18,957 |\\n|      | Code-Code | 165 | 1,466 | 750 |\\n|      |          | 662 |          |    |\\n|      |          | 68,237 |        |      |\\n| Haskell | NL-Code | 3,173 | 15,138 | 7,084 |\\n|       | Code-Code | 173 | 2,172 | 936 |\\n|       |          | 687 |          |    |\\n|       |          | 44,682 |        |      |\\n| Java | NL-Code | 5,930 | 393,891 | 375,416 |\\n|       | Code-Code | 250 | 17,623 | 16,008 |\\n|       |          | 1,021 |          |    |\\n|       |          | 2,523,044 |        |      |\\n| Javascript | NL-Code | 2,609 | 15,605 | 13,706 |\\n|        | Code-Code | 134 | 1,322 | 1,345 |\\n|        |          | 534 |          |    |\\n|        |          | 56,917 |        |      |\\n| Kotlin | NL-Code | 4,017 | 46,487 | 25,600 |\\n|        | Code-Code | 158 | 1,859 | 1,036 |\\n|        |          | 654 |          |    |\\n|        |          | 121,569 |        |      |\\n| Ocaml | NL-Code | 1,424 | 2,327 | 1,382 |\\n|       | Code-Code | 97 | 219 | 114 |\\n|       |          | 381 |          |    |\\n|       |          | 7,012 |        |      |\\n| PHP  | NL-Code | 1,965 | 6,301 | 8,870 |\\n|       | Code-Code | 136 | 896 | 834 |\\n|       |          | 547 |          |    |\\n|       |          | 29,179 |        |      |\\n| Pascal | NL-Code | 4,432 | 113,222 | 105,127 |\\n|        | Code-Code | 216 | 10,113 | 8,568 |\\n|        |          | 853 |          |    |\\n|        |          | 494,473 |        |      |\\n| Perl  | NL-Code | 1,276 | 3,903 | 1,957 |\\n|       | Code-Code | 102 | 559 | 338 |\\n|       |          | 412 |          |    |\\n|       |          | 11,035 |        |      |\\n| Python | NL-Code | 4,930 | 317,013 | 284,975 |\\n|        | Code-Code | 213 | 17,131 | 15,194 |\\n|        |          | 859 |          |    |\\n|        |          | 2,290,854 |        |      |\\n| Ruby  | NL-Code | 2,349 | 15,230 | 7,278 |\\n|       | Code-Code | 157 | 2,371 | 866 |\\n|       |          | 649 |          |    |\\n|       |          | 44,934 |        |      |\\n| Rust  | NL-Code | 3,860 | 30,673 | 14,923 |\\n|       | Code-Code | 137 | 742 | 303 |\\n|       |          | 551 |          |    |\\n|       |          | 59,829 |        |      |\\n| Scala | NL-Code | 2,555 | 7,858 | 5,210 |\\n|       | Code-Code | 144 | 867 | 459 |\\n|       |          | 591 |          |    |\\n|       |          | 24,780 |        |      |\\n\\nFrom each of these problems, we randomly choose one correct solution as a (PL code) query and pair it with the other correct solutions as positive examples and the corresponding wrong solutions (i.e., with execution outcome **WRONG ANSWER**)) as negative examples.\\n\\nRetrieval Corpus Metadata and Evaluation Protocol\\n\\nWe preserve the problem specifications and execution outcomes (e.g., **PASSED**, **WRONG ANSWER**) for each candidate code in our retrieval database. For both the NL-code and code-code retrieval tasks, we use this information to determine the correctness of a retrieved code, checking if that solves the same programming problem as the input query by passing all its unit tests or not.\\n\\n**Evaluation Metrics**\\n\\nWe evaluate the retrieval performance in terms of **retrieval accuracy@k**: computed as the proportion of queries for which a correct code retrieved within top-k.\"}"}
{"id": "wpTitXWGNO", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "wpTitXWGNO", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "wpTitXWGNO", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Notes on Intended Usage Scenarios\\n\\nConsidering the recent progress, the primary objective of creating XCODEEVAL is to challenge LLMs, especially the frontier models. As such, the tasks proposed could be very challenging for smaller models, even for the binary code compilation task. The relatively smaller (per-language) validation splits can be used to assess multilingual features and to get an overall picture of multilingual generalization. The large Test split is meant to rigorously evaluate specific programming languages and conduct more language-specific, in-depth analysis. We have also included the Training split. The intended use of the training split is to include it in the large scale pre-training or SFT mixtures. For both validation and test evaluation, we recommend using an Instruct/Chat Model.\\n\\nWhere are the Dataset Construction recipes?\\n\\nThis paper provides a detailed description of our dataset construction. However, to adhere to the page limit, we needed to shorten them in the main paper (Section 1, 2) and move to the supplementary appendix Appendix E for details.\\n\\nWhere are the documentation, maintenance, support?\\n\\nXCODEEVAL documentations in supplementary Section Appendix H to Appendix M, follow all the necessary guidelines of datasheets (Gebru et al., 2021), nutrition label (Holland et al., 2018), and data card. Following the accountability framework proposed by (Hutchinson et al., 2021), we released the data construction procedures as well as opensource the evaluation framework. Our github and huggingface repositories provide two valuable sources of both data access and additional documentation and implementation details. We regularly address queries or issues there. If any specific documentation is missing, we would be happy to address them right away.\\n\\nIs there any automated process used for creating the Gold Labels?\\n\\nPlease note that all current data (text, code, test-cases, and labels) are human written. More specifically for tag classification tasks, the tags are already provided in codeforces.com. The tags are generally updated after the contest by experienced problem solvers trusted by the codeforces team. In addition to that, the evaluation is done by programming language compilers & interpreters. We put huge effort in developing ExecEval and dockerized it to synchronize the evaluation process.\\n\\nOn the possibility of data leakage and contamination\\n\\nWe note that, although we completely separate our validation and test data from training, LLMs might have possible data leakage from pretraining. We find that even identifying data leakage (test data exists or not in the pretraining corpora) is challenging using conventional data search methods due to search cost and complexity (e.g., exact match or token overlapping methods) for (i) long sequence search for libraries (ii) boilerplate code identifying. Apart from that, hashing based searches often suffer from not having properly segmented text.\\n\\nIn this paper, we introduce an approach to address the challenge of leakage-free evaluation, employing a technique rooted in the concept of a knowledge cut-off. Our finding in Section 3.2 in the main paper shows that the data contamination significantly impacts the model performance and it needs to be interpreted with proper analyses. Another method toward leakage less evaluation could be to have a human written separate evaluation set that is hidden or regularly updated which we are considering in our long term release plans.\\n\\nOnly generative tasks utilize unit tests. How are classification and retrieval tasks considering executability?\\n\\nIn our proposed tasks, except for the tag classification task, all tasks consider online or offline unit test executability. We included a tag classification task since tags were used for the sampling algorithm that we proposed. Other than that, our code compilation and retrieval task takes into account offline code executability, where compilers are applied before releasing datasets to obtain labels. Additionally, in retrieval tasks, we treat passed samples as correctly retrieved samples and generate hard negatives from incorrect code from the same problem. Please note that all the data\u2014including text, code, test cases, and labels\u2014is human-written.\\n\\n---\\n\\n6. https://sites.research.google/datacardsplaybook/\\n7. https://github.com/ntunlp/ExecEval\\n8. https://github.com/ntunlp/xCodeEval\\n9. https://huggingface.co/datasets/NTU-NLP-sg/xCodeEval\"}"}
{"id": "wpTitXWGNO", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6 \u2013 Comparison between CODEVAL and other benchmarks. For simplicity, we combine NL-code generation and code completion as Program Synthesis. Compared to others, CODEVAL offers the largest suite of training and test data and a more comprehensive set of test cases. Evaluation levels Global, Modular, and Local refer to document, function, and statements level evaluation, respectively.\\n\\n| Dataset                     | Train  | Test   | Task Type       | Evaluation Level | Genre |\\n|----------------------------|--------|--------|-----------------|------------------|-------|\\n| Django (Oda et al., 2015)  | 16,000 | 1,805  | Program Synthesis | Lexical          | Local |\\n|                            |        |        |                 |                  |       |\\n| SQLQueries (Zhong et al., 2017) | 56,355 | 15,878 | SQL Queries     | Lexical          | Modular |\\n|                            |        |        |                 |                  |       |\\n| Synthesis, Summarization   |        |        | Synthesis, Summarization | Lexical          | Local |\\n| GithubCoNaLa (Yin et al., 2018) | 2,379  | 500    | Program Synthesis | Lexical          | Local |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Lexical          | Local |\\n| GithubAndroid (Parvez et al., 2018) | 26,600 | 3,546  | Program Synthesis | Lexical          | Local |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Lexical          | Local |\\n| GithubMap oriented, GitHubCodeSearchNet (Husain et al., 2019) | 6,452,446 | 99    | Plain Text, Retrieval | Modular |       |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Lexical          | Local |\\n| GithubJuICe (Agashe et al., 2019) | 1,518,049 | 1,981  | Notebook Cell Gen. | Lexical          | Local |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Lexical          | Local |\\n| GithubHumanEval (Chen et al., 2021) | -      | 164    | Program Synthesis | Execution        | Modular |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubHumanEval-X (THUDM, 2022) | -      | 820    | Synthesis & Translation | Execution | Modular |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubMBPP (Austin et al., 2021) | -      | 974    | Program Synthesis | Execution        | Modular |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubCodeXGLUE (Lu et al., 2021) | 2,840,000 | 759,000 | 10 Tasks | Lexical          | Local |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubDeepFix (Gupta et al., 2017) | 37,000 | 7,000  | Program Repair  | Execution        | Global |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubCodeContests (Li et al., 2022) | 4,432,447 | 32,181 | 3 Program Synthesis | Execution | Global |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubCoderEval (Yu et al., 2023) | 5,937  | 1,693  | Program Translation | Lexical | Modular |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubHumanevalpack (Muennighoff et al., 2023) | -      | 896    | Program Synthesis | Execution | Modular |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubExe-DS (Huang et al., 2022) | 119,266 | 534    | Notebook Cell Gen. | Execution | Local |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubDS-1000 (Lai et al., 2022) | -      | 1,000  | Notebook Cell Gen. | Execution | Local |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubMOCC (Wang et al., 2022a) | -      | 896    | Program Synthesis | Execution | Local |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubCODEXGLUE (Lu et al., 2021) | 496,333 | 45,394 | 7 10 Task | Lexical, Global | GitHub |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubDeepFix (Gupta et al., 2017) | 37,000 | 7,000  | Program Repair  | Execution        | Global |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubDefects4J (Just et al., 2014) | -      | 835    | Program Repair  | Execution        | Local, Global |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubAPPS (Hendrycks et al., 2021) | 5,000  | 5,000  | Program Synthesis | Execution        | Global |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubCodeContests (Li et al., 2022) | 4,432,447 | 32,181 | 3 Program Synthesis | Execution | Global |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubCoderEval (Yu et al., 2023) | -      | 460    | Program Synthesis | Execution | Modular, Global |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubHumanevalpack (Muennighoff et al., 2023) | -      | 6 \u00d7 164 | Program Synthesis | Execution | Modular |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubBioCoder (Tang et al., 2023) | -      | 2,522  | Program Synthesis | Execution | Modular |\\n|                            |        |        |                 |                  |       |\\n| Program Synthesis          |        |        | Program Synthesis | Execution        | Modular |\\n| GithubCodeApex (Fu et al., 2023) | -      | 706    | 3 tasks | Execution | Modular |\\n\\nFollowing NLP (Devlin et al., 2019; Radford et al., 2018; Raffel et al., 2020), transformer-based pre-trained LLMs have shown significant success in code, both in understanding and generation. Table 6 shows a detailed comparison between different programming language-related datasets. Code Understanding Lu et al. (2021) propose a benchmark CodeXGLUE, which comprises three widely-used code understanding tasks, defect detection, clone detection, and code search. Zhou et al. (2019) treat defect detection as a binary classification task. They propose a model called Devign which they evaluate on four large open-source C projects. Additionally, Russell et al. (2018) leverage open-source C/CPP repositories to support function-level vulnerability detection. To further understand code semantics, Svajlenko et al. (2014) propose a benchmark BigCloneBench to measure the similarity between code pairs to predict whether they have the same functionality (i.e., clone detection); BigCloneBench was collected from open-source Java repositories with manual validation. Arguably, code defect and clone detection might not be appropriate for fully evaluating models\u2019 ability in understanding code semantics (Wang et al., 2021; Guo et al., 2022). Moreover, they only support a few programming languages. Code search on the other hand considers semantic relevance for both code-to-code and text-to-code. They are formulated to retrieve semantically similar codes given a query code (Lu et al., 2021) or code description (Husain et al., 2019). The existing code search benchmarks like CodeSearchNet (Husain et al., 2019) only select the first documentation as the text query to search corresponding functions. Recently, Fu et al. (2023) introduce CodeApex, a bilingual benchmark to evaluate the language models on three different tasks consisting of programming comprehension, code generation, code correction. Among its tasks, programming comprehension examines the ability to understand code from various aspects, such as the syntax's mastery, code execution flow, and executing algorithms. Nonetheless, this dataset only covers one programming language, which is in contrast to our work. Code Generation Code generation has grown in popularity as many pre-trained LLMs have achieved remarkable performances in these tasks like decoder-only models (Chen et al., 2021; Izadi et al., 2022; Nijkamp et al., 2022) and encoder-decoder models (Wang et al., 2021; Guo et al., 2022; ...\"}"}
{"id": "wpTitXWGNO", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"data, which could be a requirement for certain tasks (e.g., Tag Classification). For this, we iterate over a number of seeds and create random splits. Let $\\\\gamma$ be the expected ratio of the number of samples in $D_{\\\\text{valid}}$ and $D_{\\\\text{test}}$, i.e., $\\\\gamma = \\\\frac{|D_{\\\\text{valid}}|}{|D_{\\\\text{test}}|}$. For each random split, we calculate a tag-wise ratio $\\\\gamma_T$, the ratio of the number of samples in $D_{\\\\text{valid}}$ and $D_{\\\\text{test}}$ for each tag $T \\\\in T$. The geometric mean of $\\\\{\\\\gamma_T\\\\}_{T \\\\in T}$ defines the \u2018tag distribution\u2019 score of a split. We select the split whose score is closest to $\\\\gamma$.\\n\\nAppendix C-Algorithm 1 describes our method, which also ensures that the validation and test sets contain the same tag sets as the training set.\\n\\nNext, to make the testing/validation computationally feasible, we aim to control the sample size while maintaining a balanced distribution across problems and tags; e.g., only C++ initially had about 647K test samples for tag classification (Appendix E.1). However, finding an optimal solution to this selection problem (i.e., how many samples to select per problem and per tag) is nontrivial. We formulate this as a circulation problem with lower and upper bounds (Mount, 2017) within a flow network. Let $p_i$ and $t_k$ be the number of solutions for a problem $P_i$ and a tag $T_k$, respectively. Let $G = (V, E)$ be a flow network (a directed graph) with the set of vertices $V = \\\\{s, P_1, \\\\ldots, P_N, T_1, \\\\ldots, T_K, t\\\\}$, where $s$ and $t$ respectively denote the source and sink nodes of the network (Figure 2). For each edge $e \\\\in E$, the lower capacity $l(e)$ and upper capacity $c(e)$ are defined as follows.\\n\\n1. Initialize $E = \\\\emptyset$.\\n2. For each problem $P_i$, add edge $(s, P_i)$ to $E$ and assign $l(s, P_i) = \\\\min(m_p, p_i)$ and $c(s, P_i) = \\\\min(x_p, p_i)$, where $m_p$ and $x_p$ respectively refer to the minimum and maximum samples to choose per problem if available with $m_p \\\\leq x_p$, thus $0 \\\\leq l(s, P_i) \\\\leq c(s, P_i)$.\\n3. For each tag $T_k$, add edge $(T_k, t)$ to $E$ and assign $l(T_k, t) = \\\\min(m_t, t_k)$ and $c(T_k, t) = \\\\min(x_t, t_k)$, where $m_t$ and $x_t$ respectively refer to minimum and maximum samples to choose per tag if available with $m_t \\\\leq x_t$, thus $0 \\\\leq l(T_k, t) \\\\leq c(T_k, t)$.\\n4. For each $P_i$ and $T_k$, add $(P_i, T_k)$ to $E$ if $P_i$ has a tag $T_k$, and assign $l(P_i, T_k) = 0$, $c(P_i, T_k) = \\\\infty$.\\n\\nWe then directly adopt the circulation problem solution to find a flow $f: E \\\\rightarrow \\\\mathbb{Z}^+$ that satisfies:\\n\\n$\\\\forall e \\\\in E, l(e) \\\\leq f(e) \\\\leq c(e)$ and $\\\\forall u \\\\in V, P \\\\in V f(u, v) = 0$.\\n\\nIn our case, $f$ denotes a feasible flow when the above constraints are satisfied for some $G$. For each $e \\\\in E$, $f(e)$ represents the following:\\n\\n1. $f(s, P_i)$ denotes the number of samples to be picked from problem $P_i$.\\n2. $f(T_k, t)$ denotes the number of samples to be picked from tag $T_k$.\\n3. $f(P_i, T_k)$ denotes the number of samples to be picked from $P_i$ that has a tag $T_k$.\\n\\nHere, $P_K = \\\\sum_{k=1}^{K} f(T_k, t)$ = $\\\\sum_{i=1}^{N} f(s, P_i)$ is the total number of samples selected, which can be controlled in a balanced way by setting the control variables $m_p, m_t, x_p$, and $x_t$. Appendix D gives further details about the method and hyperparameters for different tasks, along with a comparison to a random data selection strategy.\\n\\n2.2 EXECUTE: MULTILINGUAL, DISTRIBUTED AND SECURED EVALUATION\\n\\nAn essential requirement for execution-based evaluation is the availability of a secure and scalable framework (Chen et al., 2021; Cassano et al., 2022). With its capacity to support 44 compiler/interpreter versions in 11 different languages, ExecEval offers a versatile and comprehensive approach to program evaluation. The engine is distributed as a secure Docker image, ensuring safe and efficient executions. It also supports easy integration of new compilers/interpreters with custom execution flags (which can also be changed at run-time). While running on unit tests, ExecEval produces one of the six outcomes: (i) COMPILATION ERROR: fails to compile or run due to a syntax error; (ii) RUNTIME ERROR: successfully compiles but fails during runtime due to native environment issues (e.g., asserts, division-by-zero); (iii) MEMORY LIMIT EXCEEDED: occupies more memory than the limit; (iv) TIME LIMIT EXCEEDED: requires more time than the limit; (v) WRONG ANSWER: successfully compiles/interprets, generates an output but fails to produce a correct answer; (vi) PASSED: successfully passes all the unit tests. The program will be flagged as buggy (i-v) even when it fails on a single unit test. Appendix H of supp. material gives further details about ExecEval.\\n\\n4. $\\\\mathbb{Z}^+$ denotes the set of non-negative integers.\"}"}
{"id": "wpTitXWGNO", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Task                    | Train | Validation | Test | Train | Validation | Test | Train | Validation | Test |\\n|-------------------------|-------|------------|------|-------|------------|------|-------|------------|------|\\n| Tag Classification      | 178,324 | 1,694 | 6,193 | 503,458 | 1,000 | 5,000 | 179,508 | 106 | 952 | 19,915,150 |\\n| Code Compilation        | 79,128  | 2,234 | 6,020 | 170,407 | 1,000 | 5,000 | 79,681  | 106 | 952 | 19,915,150 |\\n| Program Synthesis       | 3,711,550 | 1,983 | 9,720 | 15,147,814 | 1,000 | 5,000 | 3,744,367 | 106 | 952 | 19,915,150 |\\n| Automatic Program Repair | 25,608   | 1,626 | 6,504 | 53,561 | 212 | 814 | 25,753  | 106 | 952 | 19,915,150 |\\n| Code Translation        | 703,625  | 1,908 | 8,881 | 2,007,940 | 1,000 | 5,000 | 707,603 | 106 | 952 | 19,915,150 |\\n| Code Translation (Source Language) | 15,716 | 1,610 | 6,431 | 36,949 | 454 | 1,676 | 15,916  | 106 | 952 | 19,915,150 |\\n| Code Translation (Validation small) | 49,340 | 482 | 1,940 | 104,970 | 421 | 1,949 | 51,831  | 106 | 952 | 19,915,150 |\\n| Code Translation        | 6,234    | 891 | 3,598 | 18,696 | 102 | 392 | 6,334   | 106 | 952 | 19,915,150 |\\n| Code Translation        | 30,681   | 2,149 | 8,671 | 30,388 | 50 | 242 | 30,732  | 106 | 952 | 19,915,150 |\\n\\n2.3 TALEST IN CODE\\n\\nTable 2 gives a breakdown of the classification and generative tasks per language. Below we briefly describe the tasks; detailed descriptions, motivation, maintainance, support, and process of task formulation along with visualizations of task distributions and task creation rationale can be found in Appendix E.\\n\\n**Classification tasks \u2013 Tag Classification and Code Compilation**\\n\\nThe goal of Tag Classification is to assign relevant tags to a code and/or natural descriptions of the corresponding problem. This task focuses on measuring the impact of code understanding by incorporating a natural language description alongside the code. It is the only task in our benchmark that does not factor in the code's executability. On the contrary, the objective of the Code Compilation task is to determine whether the given code is compilable or not, thereby constituting a binary classification problem. All the labels in both tasks are human annotated found as meta data. By addressing these classification tasks, we aim to explore and evaluate the effectiveness of program comprehension techniques.\\n\\n**Generative tasks \u2013 Program Synthesis, Automatic Program Repair (APR) and Code Translation**\\n\\nAll of our proposed generative tasks are evaluated with execution-based unit tests by ExecEval. The Program Synthesis task aims to generate executable programming language code that solves a given problem. The problem is defined with a natural language description along with some sample input-output descriptions (see Figure 1). In the APR task, along with the problem, a buggy code is also given. The objective is to correct or refine the buggy code. Moreover, in the Code Translation task, a code is provided in a source language and the goal is to translate it to a target language. Note that for Code Translation our benchmark provides the inputs for the source programming language and for Program Synthesis we only provide problem description in natural text. For both Program Synthesis and Code-Translation, the underlying execution-based unit test enables evaluation on any target programming language, as long as it is supported by ExecEval.\\n\\n**Retrieval Tasks \u2013 Code-Code and NL-Code Retrieval**\\n\\nThe objective of the Code-Code retrieval is to retrieve relevant executable code when provided with a programming language code as input. On the contrary, the NL-Code retrieval task aims to retrieve relevant executable code based on a problem description. These retrieval tasks are novel in the sense that they consider both the relevance and executability of the retrieved codes for evaluation. To the best of our knowledge, these are the first retrieval-based tasks that incorporates executability as a crucial factor when performing code retrieval.\\n\\nWe have also included a retrieval corpus specific to each of the languages for evaluation purposes.\\n\\n3 EVALUATION AND ANALYSIS\\n\\nFor all tasks except Code Translation, we evaluate on the validation split. For Code Translation from source languages, we used the small validation split (follow Appendix E.4 in supp. material).\"}"}
{"id": "wpTitXWGNO", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"TABLE 3 \u2013 Performance of gpt-3.5-turbo on XCODEVAL: For Code Translation, X- denotes the case where the source language is X, and the target languages are represented by the respective columns. For program synthesis, (T) denotes sampling done at 20 different temperatures ranging from 0.0 to 2.0, while (N) denotes sampling done at a fixed temperature 0.32 (see Section 3.2).\\n\\n| Tasks                  | metric     | C | C# | C++ | Go | Java | Javascript | Kotlin | PHP | Python | Ruby | Rust |\\n|------------------------|------------|---|----|-----|----|------|-----------|--------|-----|--------|------|------|\\n|                        | C C# C++ Go Java Javascript Kotlin PHP Python Ruby Rust |\\n| TC-Code2Tag            | macro-F1   | 32.37 | 26.91 | 40.58 | 23.06 | 31.58 | 19.35 | 33.95 | 15.25 | 29.45 | 23.64 | 24.04 |\\n| TC-DesCode2Tag         |            | 36.05 | 33.18 | 47.1 | 31.5 | 38.26 | 27.81 | 39.61 | 19.36 | 33.73 | 30.61 | 32.35 |\\n| Code Compilation       | accuracy   | 65.9 | 54.9 | 58.0 | 70.28 | 53.0 | 65.64 | 56.64 | 76.47 | 70.9 | 70.0 | 54.26 |\\n| Program Synthesis (T)  | pass@5     | 25.37 | 30.59 | 31.36 | 31.03 | 29.74 | 22.74 | 26.87 | 30.17 | 33.98 | 33.72 | 10.28 |\\n| Program Synthesis (N)  |            | 31.23 | 30.78 | 35.44 | 30.58 | 31.52 | 28.63 | 27.38 | 32.13 | 29.77 | 29.66 | 28.2 |\\n| Automatic Program Repair| pass@5    | 44.32 | 53.38 | 28.88 | 65.95 | 33.21 | 86.05 | 62.49 | 64.22 | 37.94 | 60.38 | 68.96 |\\n| Translation C-{}       | pass@5     | - | 41.74 | 89.44 | 49.73 | 57.81 | 30.94 | 37.49 | 44.43 | 45.67 | 35.14 | 51.92 |\\n| Translation C#-{}      | pass@5     | 62.27 | - | 72.14 | 49.27 | 63.94 | 25.49 | 44.39 | 60.22 | 62.62 | 68.84 | 62.16 |\\n| Translation C++-{}     | pass@5     | 49.78 | 48.47 | - | 49.43 | 48.98 | 22.65 | 33.91 | 35.41 | 31.88 | 39.46 | 39.1 |\\n| Translation Go-{}      | pass@5     | 59.72 | 63.75 | 79.18 | - | 69.92 | 51.46 | 25.2 | 36.05 | 71.05 | 42.81 | 51.21 |\\n| Translation Java-{}    | pass@5     | 46.03 | 28.13 | 52.64 | 46.82 | - | 32.21 | 28.5 | 11.53 | 44.38 | 27.07 | 42.16 |\\n| Translation Javascript-{} | pass@5 | 57.64 | 49.16 | 68.04 | 64.49 | 60.24 | - | 16.1 | 31.52 | 64.12 | 14.93 | 52.27 |\\n| Translation Kotlin-{}  | pass@5     | 74.34 | 59.39 | 85.67 | 51.52 | 39.2 | - | 39.43 | 64.58 | 53.33 | 53.97 | 50.04 |\\n| Translation PHP-{}     | pass@5     | 64.38 | 17.5 | 55.92 | 62.19 | 52.11 | 26.19 | - | 59.79 | 64.33 | 36.87 | 40.16 |\\n| Translation Python-{}  | pass@5     | 41.18 | 19.38 | 42.58 | 50.82 | 40.65 | 19.93 | 6.04 | - | 68.12 | 22.23 | 32.69 |\\n| Translation Ruby-{}    | pass@5     | 30.47 | 5.63 | 35.69 | 67.01 | 40.07 | 5.69 | 3.75 | 58.87 | - | 12.23 | 29.7 |\\n| Translation Rust-{}    | pass@5     | 39.49 | 44.72 | 54.29 | 44.6 | 57.5 | 36.24 | 20.43 | 37.91 | 51.32 | - | 39.79 |\\n| Target lang. Avg       | pass@5     | 52.53 | 37.79 | 63.56 | 53.59 | 53.04 | 27.98 | 21.83 | 40.41 | 56.27 | 46.52 | 42.41 |\\n\\n3.1 BENCHMARK RESULTS\\n\\nBaselines\\nWe benchmark XCODEVAL using ChatGPT (gpt-3.5-turbo-0301). To construct a query prompt, we adopt the direct zero-shot prompting method (i.e., no chain-of-thought) that facilitates easier automated evaluation (no overlapping of code and explanations). For the retrieval task, following (Karpukhin et al., 2020), we build a bi-encoder based dense multilingual code retriever by finetuning the StarEncoder (Li et al., 2023) model. Our implementation details are provided in Appendix G.\\n\\nResults\\nWe present the results on the classification and generative tasks in Table 3. Overall, the model achieves inspiring yet inadequate results \u2013 marking XCODEVAL a promising yet challenging benchmark as per the current progress in LLMs. Particularly for Tag Classification, we observe decent performance in general and incorporating a problem description enhances the model's overall predictive capability. However, in web languages (e.g., PHP, JavaScript), it exhibits the poorest performance. In Code Compilation, we observe encouraging performance for Go, PHP, Python, and C#. However, the performance is close to a random baseline for Java, Kotlin, and Rust.\\n\\nFor Program Synthesis, we find that in popular languages, such as C++, C#, Go, Java, and Python the model performs well, while in rare languages like Rust it fares poorly. Notably while on other datasets such as HumanEval (Chen et al., 2021), ChatGPT achieves much higher scores, 65.8 in pass@1 (OpenAI, 2023; Chen et al., 2022), it significantly falls behind even in pass@5 (\u223c30) in XCODEVAL \u2013 imposing challenges even for such a powerful LLM. In Figure 3 (left), we show the performance for different k. As expected, results increase with k and no noticeable differences are observed between the compiler (e.g., C++, Java) and interpreter (e.g., Python, Ruby) languages.\\n\\nIn APR, we observe a higher performance scale than in Program Synthesis indicating that the model finds the task relatively easier since it does not necessitate generating a complete code from scratch. Interestingly, in languages where the model underperformed in program synthesis, it exhibits good...\"}"}
{"id": "wpTitXWGNO", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"TABLE 4 \u2013 Summary of the performance of StarEncoder Li et al. (2023) finetuned on our retrieval tasks for $k = 100$. For Code-Code, $(\\\\alpha)$ denotes the average score for codes of any given language as the corpus, similarly $(\\\\gamma)$ denotes average score for codes of any fixed language as query. For NL-Code, the scores are reported for corpus of different languages.\\n\\n| Tasks          | metric | C  | C# | C++ | Go | Haskell | Java | Javascript | Kotlin | Ocaml | PHP | Pascal | Perl | Python | Ruby | Rust | Scala |\\n|---------------|--------|----|----|-----|----|---------|------|------------|--------|-------|-----|--------|------|--------|------|------|-------|\\n| Code-Code $(\\\\alpha)$ | Acc@$k$ | 56.43 | 56.05 | 39.96 | 62.82 | 66.30 | 56.71 | 49.30 | 69.63 | 63.42 | 58.44 | 64.80 | 52.71 | 56.38 | 55.92 | 61.38 | 58.10 | 66.69 | 58.53 |\\n| Code-Code $(\\\\gamma)$ | Acc@$k$ | 68.66 | 74.50 | 70.49 | 17.35 | 62.62 | 60.03 | 74.71 | 50.70 | 52.06 | 33.72 | 49.88 | 65.35 | 40.50 | 68.33 | 61.71 | 48.58 | 59.76 | 56.41 |\\n| NL-Code        | Acc@$k$ | 82.28 | 89.99 | 83.81 | 68.98 | 90.26 | 81.68 | 84.72 | 85.33 | 84.74 | 85.45 | 80.71 | 82.21 | 81.33 | 84.57 | 87.17 | 82.23 | 89.71 | 83.83 |\\n\\nPerformance in APR. For Code Translation, we observe that Kotlin and Go can be successfully translated to most of the other languages, while C++ and Python are the best languages to translate to.\\n\\nFigure 4 \u2013 Left: ChatGPT\u2019s performance on C++ over time. After the knowledge cutoff (Sep, 2021), the performance is notably poor. Right: distribution of passed solutions (C++) across different difficulty levels.\\n\\nTable 4 reports results on the two retrieval tasks: Code-Code and NL-Code. For Code-Code retrieval, we computed the top-$k$ retrieval accuracy for each of the 17 query languages from all 17 different retrieval corpora. The summarized results from a $17 \\\\times 17$ matrix (Appendix E.6\u2014Figure 12 of supp. material) are provided in Table 4, where each row represents a query language and each column represents a corpus language. The column-wise and row-wise averages are denoted as $(\\\\alpha)$ and $(\\\\gamma)$, respectively. For Code-Code $(\\\\alpha)$, there is a degradation of performance for languages with large corpus such as C++, Python. In the case of Code-Code $(\\\\gamma)$, languages with limited training data in CODEEVAL, such as D, Ocaml, Rust performed poorly as query languages. For NL-Code, performance is good across all languages except for D. We suspect that the limited availability of resources for D in both The Stack (Kocetkov et al., 2022) dataset (training corpora of StarEncoder) and our CODEEVAL dataset could account for this discrepancy. Also, the presence of more hard negative candidates (i.e., very similar to a correct code) makes it a challenging task to identify similar codes.\\n\\nWe provide more results on the retrieval outcomes in the supplementary material (Appendix E.6).\\n\\n3.2 ANALYSIS\\n\\nKnowledge cutoff CODEEVAL contains problems that appeared in codeforces.com for the timeframe: Feb 19, 2010 \u2013 Nov 21, 2022 (in supp. material Appendix C\u2014Figure 7 shows the distribution over time). Since we have a complete footprint of release dates for each of the problems, it enables us to identify data leakage in LLMs that have public knowledge cutoff dates.\\n\\nFigure 4 (left) presents a potential data contamination for ChatGPT. Though OpenAI (2023) (Table 9) reported no data contamination on codeforces.com, datasets like our CODEEVAL could empower researchers to conduct insightful analysis and perform an investigation on such serious questions. \u201cIt should be noted that CODEEVAL can only analyze data contamination if there is a good amount of problems that appear after the knowledge cut-off date of the evaluated LLM. For a more interpretable evaluation, we invite LLM builders to disclose their knowledge cut-off dates.\\n\\nImpact of temperature parameter\\n\\nAlthough proved to be crucial (Chen et al., 2021; Austin et al., 2021), previous studies have not extensively examined the impact of the sampling temperature parameter on code executability. To address this gap, we conduct an investigation in which we assessed each sample for Program Synthesis at 20 different temperatures in the range 0.0 \u2212 2.0. Figure 5 (left) presents the overall distribution of execution outcomes for various languages, encompassing all the samples generated at different temperatures, while Figure 5 (right) displays a distribution of PASSED solutions at different temperatures. As the temperature increases, the likelihood of achieving code\"}"}
