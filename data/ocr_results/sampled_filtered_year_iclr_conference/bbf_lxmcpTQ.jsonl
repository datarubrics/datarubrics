{"id": "bbf_lxmcpTQ", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Completed examples by the Imitation agent following the instructions generated by the Heuristic user.\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Failed examples by the Imitation agent following the instructions generated by the Heuristic user.\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Completed examples by the \\\\textit{Imitation} agent following the instructions generated by the \\\\textit{Neural} user.\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Failed examples by the Imitation agent following the instructions generated by the Neural user.\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, et al. Uibert: Learning generic multimodal representations for ui understanding. In IJCAI, 2021.\\n\\nSatanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pp. 65\u201372, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. URL https://aclanthology.org/W05-0909.\\n\\nKobus Barnard and David Forsyth. Learning the semantics of words and pictures. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, volume 2, pp. 408\u2013415. IEEE, 2001.\\n\\nAndrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, and Bryan A. Plummer. A dataset for interactive vision language navigation with unknown command feasibility. In European Conference on Computer Vision (ECCV), 2022.\\n\\nKhyathi Raghavi Chandu, Yonatan Bisk, and Alan W Black. Grounding \u2018grounding\u2019 in NLP. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 4283\u20134305, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.375. URL https://aclanthology.org/2021.findings-acl.375.\\n\\nKan Chen, Rama Kovvuri, and Ram Nevatia. Query-guided regression network with context policy for phrase grounding. In Proceedings of the IEEE International Conference on Computer Vision, pp. 824\u2013832, 2017.\\n\\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021.\\n\\nBiplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: A mobile app dataset for building data-driven design applications. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology, pp. 845\u2013854, 2017.\\n\\nJiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, and Houqiang Li. Transvg: End-to-end visual grounding with transformers. CoRR, abs/2104.08541, 2021. URL https://arxiv.org/abs/2104.08541.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. September 2020.\\n\\nAnna Effenberger, Rhia Singh, Eva Yan, Alane Suhr, and Yoav Artzi. Analysis of language change in collaborative instruction following. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2803\u20132811, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.239. URL https://aclanthology.org/2021.findings-emnlp.239.\\n\\nAkira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. Multimodal compact bilinear pooling for visual question answering and visual grounding. CoRR, abs/1606.01847, 2016. URL http://arxiv.org/abs/1606.01847.\\n\\nIzzeddin Gur, Ulrich Rueckert, Aleksandra Faust, and Dilek Hakkani-Tur. Learning to navigate the web. In International Conference on Learning Representations, 2019.\\n\\nJanosch Haber, Tim Baumg\u00e4rtner, Ece Takmaz, Lieke Gelderloos, Elia Bruni, and Raquel Fern\u00e1ndez. The PhotoBook dataset: Building common ground through visually-grounded dialogue. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1895\u20131910, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1184. URL https://aclanthology.org/P19-1184.\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "bbf_lxmcpTQ", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fig. 2 presents the user and agent views in our data collection interface. In the user view, the user can send commands in the message box, to instruct the agent to select the target object as highlighted by a red bounding box on the UI screen. On the agent's view, the agent annotator can respond to the user request by performing object selection on the UI screen, which has all the clickable objects highlighted. But there is no indication of the target object so the agent annotator has to guess from the user instruction. The agent is not allowed to text back to the user. The agent's current selection is reflected on the UI screen so the user understands how to further instruct the agent. The annotation task is designed based on the eyes-on hands-free situation of mobile interaction.\\n\\nThe labelers of the task were native English speakers and had experience using mobile phones. They were trained with a few pilot tasks to get familiar with the task, during which we also improved the labeling interface and the guidelines based on labelers' feedback. The dataset was completed by 30 labelers in 10 batches. The labeling quality was monitored by sampling examples from each batch for manual examination.\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The word-level vocabulary in the training set consists of 13,794 unique words. Fig. 3 shows the distribution of the 50 most frequent words in the training split with certain non-content words (e.g., *is*, *of*, *comma*) filtered out.\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We present MUG, a novel interactive task for multimodal grounding where a user and an agent work collaboratively on an interface screen. Prior works modeled multimodal UI grounding in one round: the user gives a command and the agent responds to the command. Yet, in a realistic scenario, a user command can be ambiguous when the target action is inherently difficult to articulate in natural language. MUG allows multiple rounds of interactions such that upon seeing the agent responses, the user can give further commands for the agent to refine or even correct its actions. Such interaction is critical for improving grounding performances in real-world use cases. To investigate the problem, we create a new dataset that consists of 77,820 sequences of human user-agent interaction on mobile interfaces in which 20% involves multiple rounds of interactions. To establish our benchmark, we experiment with a range of modeling variants and evaluation strategies, including both offline and online evaluation\u2014the online strategy consists of both human evaluation and automatic with simulators. Our experiments show that allowing iterative interaction significantly improves the absolute task completion by 18% over the entire test set and 31% over the challenging subset. Our results lay the foundation for further investigation of the problem.\\n\\n1 Introduction\\n\\nNatural language understanding on graphical user interfaces (GUIs) is crucial for realizing human-computer interaction and assisting scenarios that have accessibility difficulties (Sarsenbayeva, 2018). Specifically, interpreting user commands into executable actions has drawn increasing interests as it manifests rich research problems including multimodal modeling and natural language grounding (e.g., Li et al., 2017; Gur et al., 2019; He et al., 2020; Li et al., 2020a; 2021). Prior works often consider UI grounding in a single-pass fashion where the model predicts actions with a given instruction without looking backward to refine prediction. However, in a realistic scenario, user instructions can be ambiguous or inaccurate especially when the target action is difficult or inconvenient to articulate. Reasoning in such cases is inherently iterative. Therefore, it is important and beneficial to incorporate interaction for resilient grounding (Suhr et al., 2019; Chandu et al., 2021).\\n\\nIn this paper, we investigate interactive grounding on GUIs, which aligns multimodal input to actionable objects of a screen. We focus on single-screen interaction which is the building block of UI reasoning. Specifically, we introduce the MUG (Multi-turn UGrounding) task in which the user iteratively guides the agent to select a desired UI object (see Fig. 1). With a given UI and a target object, the user instructs the agent via natural language, ranging from casual intent to more descriptive commands. The agent infers which UI object is intended by the user and highlights it. If the agent is correct, the user can confirm the selection and the grounding is completed. Otherwise, the user issues further guidance, e.g., \\\"Click the one below\\\", to the agent to refine its selection. We collect the MUG dataset from live interaction sessions between pairs of human annotators\u2014one acts as the user and the other as the agent. Our dataset has 77,820 examples, each records the transaction history in a session. Specially, 20% of the dataset are challenging ones as their human commands need multiple rounds to ground, even for human agents.\\n\\nTo establish the benchmark, we experiment with a range of variants to model the dynamics between the two roles. While the main goal of the task is to develop agent models for grounding, we also develop the user models for online instruction simulation. We build our models upon a Transformer-\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nFigure 3: Distribution of top 50 words in M\\n\\nTab. 10 lists the complete view hierarchy features we used. We unify each feature into a real-valued vector. These view hierarchy features are first represented with trainable embeddings, and then encoded by the transformer model (Sec. 6.1). For text attributes (e.g., text), we max-pool their non-contextualized token embeddings, which are randomly initialized and trained. For discrete-valued attributes (e.g., type), we use a trainable vector for each possible value. The ordering of objects in transformer input follows the pre-order traversal in the view hierarchy (which is a tree structure). We then combine the vision representations of individual UI objects via ROI pooling over ResNet featuremap of the encoded screenshot image, and view hierarchy encoding to form a multimodal representation of each UI object for the downstream computation of the model.\\n\\nWe consider these view hierarchy features to be auxiliary. There is often a huge gap between what command the user would issue based on what they see on the UI, and what the underlying information is for the UI. As we discussed in Sec. 6, about 46% of UI objects do not have a text label, and the user would need to come up with their own language description about the object, which is why the text matching baseline fails. Even when there are text descriptions, they are not necessarily what the user would articulate since a user command can be abstract. Fundamentally, the internal representation of the UI is often inaccessible or uninterpretable to the user, thus calling for the help of multimodal modeling and interaction modeling.\\n\\n| Feature | Example |\\n|---------|---------|\\n| bounding box | [xmin, xmax, ymin, ymax] |\\n| leaf | true/false |\\n| type | button/checkbox/... |\\n| clickable | true/false |\\n| text | email address/passcode |\\n| resource id | login_icon |\\n| dom | [pre/post-order index] |\\n\\nTable 10: Features $\\\\psi$ used for visual structure.\\n\\nHyperparameters & Training\\n\\nFor all our agent models, we use the same configurations, which are grid-searched based on models' offline validation performances. Our hyperparameters are chosen from the best offline development F1 scores. For the number of self-attention modules, we grid-searched in $\\\\{1, 2, 4, 6\\\\}$, which resulted in 2 hidden layers for the user interface Transformer encoder and 6 hidden layers for the grounding decoder. Each self-attention module uses 8-head self and encoder-decoder attention with a 256 hidden size. The dropout rate for attention and MLP layers is 0.1, which is grid-searched in $\\\\{0.1, 0.2, 0.5\\\\}$. For learning rate, we grid-searched from $\\\\{1e^{-3}, 3e^{-4}, 1e^{-4}, 3e^{-5}, 1e^{-5}\\\\}$, and use $3e^{-4}$ with linear warmup with cosine annealing for the first 10k steps. All the models are trained to...\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"100k steps with a batch size of 128 on a 32-core Google Cloud TPUv3. Models are evaluated every 1k steps and the version with the best development offline F1 is saved. The training time for our agent model is around 8 hours.\\n\\nOur neural user model has the same grid-searched configuration as the agent, i.e., 2 encoder layers, 6 decoder layers, 0.1 for dropout, and the same warmup scheduling. The best learning rate is $1 \\\\times 10^{-4}$.\\n\\nDifferent from the agent model, we found the neural user model's development CIDEr score quickly drops after 6k steps, possibly due to overfitting and data sparsity, thus its training early-stops there.\\n\\n**EXAMPLES IN THE MUG DATASET**\\n\\nWe present some examples from the MUG dataset in Fig. 4 and 5. Each example contains instructions and selections from human user and agent annotators.\\n\\n**LIMITATIONS**\\n\\n**English-only Dataset**\\n\\nWhile non-English examples exist, we acknowledge that MUG mostly consists of English UI. Other languages do exist in the dataset, but consist of a small portion. Specifically, our instructions are English-only. Future extensions to our work should address or alleviate this issue.\\n\\n**Platform-specific Interfaces**\\n\\nOur interfaces, since coming from RICO, only consist of Android screens. In practice, it is also difficult to obtain non-Android interfaces. We acknowledge this is an application limitation. And the bias from the top and bottom banner of Android could make trained model brittle in other domains.\\n\\n**Going beyond Single Screen**\\n\\nWe aim to establish the task and report baseline performances for future work. The interaction in MUG happens within the same user interface. A natural extension would be extending the task to span over sequence of interfaces. Indeed, the task would become more challenging, and potentially require large offline training data and reliable online simulation.\\n\\n**Better User Model**\\n\\nThe current best neural instruction generation we use has a CIDEr of 78.0 on the validation set. We acknowledge there is space for further improvement. Note that our neural instructions are trained on multi-turn examples in MUG, which amounts to $\\\\sim 20\\\\%$ of the training data. It suggests external resources could be useful for improving user model performances.\\n\\n**Interaction Dynamics between User and Agent**\\n\\nIt would be helpful to study how/why the agent sometime repeatedly makes incorrect actions in Tab. 6, such as whether repeated mistakes are due to the lack of language utility/diversity in user instruction or the lack of understanding in the agent.\\n\\n**PREDICTION EXAMPLES**\\n\\nHere, we demonstrate predictions from the Imitation model. Fig. 6 demonstrates successfully solved examples following the instructions generated by the Heuristic user model, while failed ones are in Fig. 7. Similarly, Fig. 8 demonstrates solved ones following the instructions generated by the Neural user model, and failed ones are in Fig. 9.\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 4: MUG examples 1-4. Instructions are at top of each turn. Agent selection is in \\\\[\\\\text{\\\\texttt{.}}\\\\] and target is in \\\\[\\\\text{\\\\texttt{.}}\\\\].\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nFigure 5: MUG examples 5-8. Instructions are at top of each turn. Agent selection is in and target is in.\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\n(a) Example one.\\n(b) Example two.\\n\\nFigure 1: Two illustrative examples of the MUG task. There are two turns in each of these examples. Interactions happen within a single screen. User commands are shown above the screens. The target object is bounded in . Agent choices are marked with .\\n\\nBased encoder-decoder architecture (Li et al., 2021), and experiment with various learning methods, including traditional sequence modeling and reinforcement learning. To fully examine the model performances, we evaluate the agent model with a spectrum of evaluation strategies, including both offline and online evaluations. For the online evaluation, we employ both automatic and human evaluations, which include interactions between the agent and the user (either a human or the user model) and offer a comprehensive probe into model understanding. Our experiments show that incorporating interaction substantially improves UI grounding task completion by 18% on the entire dataset and 31% on the challenging set, both in absolute scales. Furthermore, our robustness measurements suggest MUG, while being a seemingly easy single-screen task, is actually difficult since neural agents sometimes struggle to correct themselves, resulting in repeated wrong selections across multiple turns. This suggests large rooms for future improvement in grounding agents.\\n\\nIn summary, our key contributions are:\\n1. We introduce MUG, a novel interactive vision-language task that focuses on multi-turn language grounding on a graphical UI screen, which is a challenging task that is meant to improve language grounding in realistic UIs.\\n2. We create a rich dataset that includes 77,820 examples recorded from live sessions between pairs of human users and agents. And 20% of the data are challenging for both human annotators and neural agents.\\n3. We experiment with a range of model variants and evaluation strategies, showing that iterative interaction significantly improves grounding accuracy by 18% and 31% on the entire and challenging test sets respectively, with automatic assistance from our user models. Our work lays a solid foundation for future investigations on collaborative grounding.\\n\\n2 BACKGROUND\\n\\nMulti-modal modeling has a long history of research (e.g., Winograd, 1972; Barnard & Forsyth, 2001; Lavrenko et al., 2003; Plummer et al., 2015; Yu et al., 2016). One important area focuses on grounding objects in images where the natural language is used as an additional input (Chen et al., 2017; Yu et al., 2016; 2018; Fukui et al., 2016; Deng et al., 2021).\\n\\nInteractive Multimodal Grounding\\n\\nPrior works have formulated grounding as a multi-step reasoning task, e.g., navigation via multiple steps of grounding (e.g., Ku et al., 2020; Gur et al., 2019). Our work differs by focusing on agent\u2019s ability to self-correct in synchronized turns of interaction.\\n\\nThe dataset and code for reproducing our experiments are at https://github.com/to-be-de-anonymized.\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023 on a UI screen. It is also conceptually linked to repeated reference game (Hawkins et al., 2020), except we use a different form of communication (language-action) instead of dialogue (language-language). Our task leverages iteratively refined instructions on atomic action instead of the increased instruction utility over multi-step actions (Effenberger et al., 2021). Our work models both the user and the agent, and let them communicate online. This is different from single-sided modelings (Suhr et al., 2019; Kojima et al., 2021). Our observation that interaction improves grounding is also in line with dialogue-based works (e.g., Haber et al., 2019; Takmaz et al., 2020).\\n\\nUI Grounding\\n\\nGrounding UI objects involves automatic completion of actions on web or mobile interfaces (e.g. Pasupat et al., 2018; Li et al., 2020a; He et al., 2020). It is also an important accessibility task for users who are situationally impaired when they are occupied by real-world tasks (Sarsenbayeva, 2018). Compared to grounding on natural images, these tasks usually take well-specified user commands and aim to select the object that best matches the command. The UI image is often encoded via ResNet (He et al., 2016) or ViT (Dosovitskiy et al., 2020). The structure and text features of UI are often encoded by Transformer model (Vaswani et al., 2017). Fusing multimodal information is widely handled by cross-attention (e.g. He et al., 2020; Li et al., 2021; Bai et al., 2021). We adopt these neural components in our benchmark.\\n\\nMobile UI Datasets\\n\\nMany grounding tasks, while covering multiple screens, remain one-pass reasoning, such as PIXELHLP (Li et al., 2020a) and MOIF (Burns et al., 2022). Prior works (e.g., Todi et al., 2021) model sequence of (action, state) pairs via reinforcement learning (RL). In contrast, MUG focuses on correcting a single action on one screen. Tab. 1 summarizes key differences among other Mobile UI datasets. Importantly, MUG is a challenging task as it enables corrective interaction in synchronized turn between user and agent.\\n\\n| Data | Screen | Instruction | Natural Instruction | Interactive Correction |\\n|------|--------|-------------|---------------------|-----------------------|\\n| RICO | multi  |             |                     |                       |\\n| PIXELHLP (Li et al., 2020a) | multi  |             |                     |                       |\\n| MOIF (Burns et al., 2022) | multi  |             |                     |                       |\\n| RICO SCA (Li et al., 2020a) | single |             |                     |                       |\\n| REFEXP (Bai et al., 2021) | single |             |                     |                       |\\n| MUG (Ours) | single |             |                     |                       |\\n\\nTable 1: Comparison to prior mobile UI Datasets.\\n\\nAs a grounding task, MUG involves two participants: a user and an agent. Our formulation includes both roles to provide a holistic view of interactive grounding. The user's goal is to instruct, via natural language, the agent to select the desired object $g$ on the UI screen $S$. The unique aspect of MUG is that it allows the user to guide the agent iteratively to identify the target action by issuing a series of commands, each in response to the agent's prior inference.\\n\\nWe separate such user-agent interaction into turns. At turn $t$, the interaction consists of:\\n\\n$$\\\\{c_t: \\\\text{user command}, a_t: \\\\text{agent action}\\\\}$$\\n\\nwhere the user first instructs the agent with command $c_t$, and the agent responds with a suggestion of action $a_t$. Here $a_t$ is essentially the index of object. The task is completed when $a_t = g$. \\n\\n3.1 AGENT TASK\\n\\nIn MUG, the action space for the agent consists of a set of UI objects to click on the interface, e.g., in Fig 1. Intuitively, we would want the agent to take the desired action $g$ as early as possible, i.e., as few turns as possible. Thus, at turn $t$, the agent models $P_{\\\\theta}(a_t|S,c_{[0,t]},a_{[0,t-1]})$. \\n\\n$$3$$\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nwhere $\\\\theta$ denotes the agent parameters. This iterative grounding early stops once $t = g$ or $t$ reaches a maximum number of turns allowed.\\n\\n3.2 UST\\n\\nThe user's role is to provide guidance to the agent through iteratively refined instructions. In contrast to one-pass prediction tasks (e.g. Pasupat et al., 2018; He et al., 2020) where the agent makes a one-shot guess, a MUG user issues follow-up commands that are dependent of prior instructions $c[0, t-1]$ and agent actions $a[0, t-1]$, which is formalized as the following:\\n\\n$$P_{\\\\phi}(c_t|S,g,c[0,t-1],a[0,t-1])$$\\n\\nwhere $\\\\phi$ denotes the user. Here, the user model is aware of the target object $g$.\\n\\nInterplay between User and Agent\\n\\nThe agent task (Eq. 1) is the pivot of MUG. The user task (Eq. 2) aims to guide agent towards task completion, which potentially includes online training. In our benchmark, we let the user and agent play together. Although automatic evaluation is not as realistic as human evaluation, it offers a fast, low-cost, and reproducible environment. This setting also allows us to study various questions surrounding the interplay between the two, e.g., whether an automatic user can assist an agent? and whether agent errors would confuse the user?\\n\\n4 DATASET CREATION\\n\\nAs there is no available dataset for model training and evaluation, we developed an interactive labeling interface to collect data for MUG. Our data collection involves two human annotators to play the roles of the user and the agent respectively in a live session. The user and the agent have two separate views, running on different machines (Appx. A). Both views share the same UI screen and a message box showing instruction history. Our task embodies the eyes-on, hands-free situation for mobile interaction where the user is required to only use language for the task, and the machine responds its prediction by highlighting. The user can commit the action if the prediction is confirmed.\\n\\nDuring a session, only the user can see the target; and the the message box is read-only to the agent thus no language-base dialogue would take place.\\n\\n4.1 ANNOTATION WORKFLOW\\n\\nWe use the UI corpus, mobile UI screenshots and view hierarchies, from RICO (Deka et al., 2017) and auxiliary object features from the CLAY dataset (Li et al., 2022). Each session starts with a randomly sampled UI object (e.g., a button), from the visible view hierarchy, as the target object $g$. User annotators are encouraged to articulate their initial command ($c_0$) casually or goal-oriented. We consider such design to cover the realistic scenarios discussed in Sec. 1, and potentially free users from composing long and precise instructions.\\n\\nIn the agent view, all clickable objects on the UI screen are revealed with their bounding boxes highlighted, which show what objects the agent can select, without indicating which one is the target $g$. The current agent selection is reflected on both the user and the agent's view. The session continues to the user's turn if the agent selection does not match $g$. In follow-up turns, the user is not allowed to repeat a command issued in previous turns, and likewise the agent is not allowed to select an previously chosen object. Upon the agent selection matching the target object in the user view the task is completed. Each session allows up to 5 turns and we filter out those unfinished.\\n\\n4.2 RESULTS & ANALYSIS\\n\\nWe collected 77,820 examples based on 31,265 unique screens from 7,132 apps (see details in Table 2). We split the dataset into the training, development, and test sets. We use app-wise split (Li et al., 2020b) to avoid potential leaking across sets. As shown in Table 2, the three splits have a similar distribution regarding the number of turns in each example.\\n\\nIn general, users tend to provide short instructions ($\\\\sim 4$ words). Across the dataset, annotators completed $\\\\sim 80\\\\%$ examples in one turn (i.e., one instruction and one selection). The rest $20\\\\%$\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2:\\n\\n| Split | Apps | Screens | Interactions | Avg. #Turns | Avg. #Token/Turn |\\n|-------|------|---------|--------------|------------|-----------------|\\n| Train | 6,039| 26,090  | 65,235       | 1.24       | 4.26            |\\n|       |      |         |              | 78.91      | 18.31           |\\n|       |      |         |              | 2.37       | 0.35            |\\n|       |      |         |              | 0.06       |                 |\\n| Dev   | 544  | 2,625   | 6,377        | 1.23       | 4.18            |\\n|       |      |         |              | 79.99      | 17.77           |\\n|       |      |         |              | 1.91       | 0.27            |\\n|       |      |         |              | 0.27       |                 |\\n| Test  | 549  | 2,550   | 6,208        | 1.23       | 4.18            |\\n|       |      |         |              | 80.20      | 16.82           |\\n|       |      |         |              | 2.55       | 0.40            |\\n|       |      |         |              | 0.40       |                 |\\n| All   | 7,132| 31,265  | 77,820       | 1.24       | 4.25            |\\n|       |      |         |              | 79.10      | 18.15           |\\n|       |      |         |              | 2.35       | 0.35            |\\n|       |      |         |              | 0.06       |                 |\\n\\nTable 3: Dataset statistics.\\n\\nThe dataset is considered to be more challenging, with the majority taking 2\u22123 turns. We call this 20% as the Challenging subset. In Sec. 6, we will show that examples requiring more turns for human agents are also more difficult for grounding models. We refer readers to Appx. F for examples.\\n\\nWe should note that while the 20% multi-turn ratio seems a low percentage but it leads to large impact in practice. Firstly, it lines up an upper bound for agent ground accuracy (i.e., 80%). Moreover, real-world grounding sessions can also span multiple screens. If we apply the same 20% rate here, the probability for a multi-screen session to require correction is much more significant. For instance, 67% with 5 screens. Lastly, we will see in Sec. 6 that neural agents could also struggle at instructions that are trivial for human to execute.\\n\\nIn Table 3, we categorize 200 Challenging examples from the development split. We found follow-up commands are mainly to make spatial adjustments (50% + 10%) or to add extra information.\\n\\n| Percentage | Attribution | Example |\\n|------------|-------------|---------|\\n| 50%        | Adjusting relative position in the layout. | the value before the text. |\\n| 31%        | Providing more information of the target. | show me channels. \u2192 click tv icon. |\\n| 10%        | Adjusting direction/position on the screen. | not reward but collect at the bottom. |\\n| 3%         | Rephrasing the instruction. | go to books. \u2192 show me books logo. |\\n\\nTable 3: Major categories for the second turn from 200 examples in the development split.\\n\\n5 ROUNDING MODELS\\n\\nWe aim to have a general architecture for the UI domain and explore its variants to model multi-turn interaction. Our agent model is based on a transformer encoder-decoder network, inspired by (Li et al., 2021). The encoder comprehends the objects on the UI screen (Sec. 5.1) and the decoder predicts a target action based on the UI and interaction history (Sec. 5.2).\\n\\n5.1 UI ENCODER\\n\\nOur encoder processes the interface $S$. Each $S$ consists of two modalities of information, i.e., a screenshot $I_S$ and view hierarchy features $\\\\psi$ (Deka et al., 2017; Li et al., 2022). The concrete list of $\\\\psi$ is in Appx. D. The output is an encoding $v_k$ for each object indexed by $k$, similar to (e.g., Li et al., 2020a; He et al., 2020; Li et al., 2020b):\\n\\n$$\\\\Phi_S = \\\\text{ResNet}(I_S)(3)$$\\n\\n$$v_k = \\\\text{T}_{\\\\text{enc}}(\\\\{\\\\text{ROI}_k(\\\\Phi_S) | \\\\psi_k\\\\})$$\\n\\nFor the image, we use a pre-trained ResNet-50 (He et al., 2016) which is fine-tuned with other modules. The resulted $\\\\Phi_S$ (grid size of $h \\\\times w$) is then mapped to object level by region-of-interest (ROI) pooling (Ren et al., 2015). Finally, we fuse multimodal features for each object by a transformer encoder $T_{\\\\text{enc}}$. The output $v$ stands for a sequence of object encodings which are interaction-agnostic.\\n\\n5.2 GROUNDING DECODER\\n\\nWe use a causal transformer $T_{\\\\text{dec}}$ to predict from interaction history. The architecture is similar to (Li et al., 2021), except we use multi-turn interaction as input, e.g., $c[0,t]$ and $a[0,t-1]$. The output of $T_{\\\\text{dec}}$\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023 is a vector $z_t$ that summarizes prior interaction up to $c_t$:\\n\\n$$z_t = T_{dec}(v,c_0,v_{a_0},c_1,...,v_{a_{t-1}},c_t) \\\\tag{5}$$\\n\\nwhere $a_{t}$ denotes object index, either from model prediction or human selection. The specific input to Eq. 5 will be subject to modeling variants in Sec 6.1. For classification, we use a linear layer $f$ to score the $k$-th object:\\n\\n$$a_t = \\\\arg \\\\max_k f([z_t|v_k]) \\\\tag{6}$$\\n\\n6 EXPERIMENTS\\n\\nThe goal of our experiments is to explore training and evaluation methods for MUG and establish a benchmark. Simply matching the initial instruction and object text is insufficient as text features are often incomplete. For instance, the validation split has 46% objects missing text, and a deterministic classifier using METEOR (Banerjee & Lavie, 2005) has only 21% F1. In this section, we present setups for the neural architecture discussed in Sec. 5. In Sec. 6.1, we explore multiple variants for the agent. For automatic evaluation, we present a simple and effective heuristics-based user model and a neural version in Sec. 6.2. Finally, we show extensive F1 results in Sec. 6.4 and 6.5, robustness in 6.6, ablations in 6.7 and 6.8, and error analysis in 6.9. We refer readers to Appx. E for hyperparameters, Appx. H for sample predictions, and Appx. G for extended discussion.\\n\\nTo avoid confusion, we thereafter use $a_t'$ to refer to the selection predicted by the agent model at turn $t$, while $a_t$ to the human agent's selection. Similarly, we refer $c_t'$ to instruction generated by the user model while $c_t$ to the one from the human user.\\n\\n6.1 AGENT MODELS\\n\\nOur agent models use the $T_{enc}$ and $T_{dec}$ (in Sec. 5) as a backbone, denoted as $\\\\theta$. Recall that $T_{enc}$ processes $S$ while $T_{dec}$ processes interaction. Here, we discuss different handlings of $T_{dec}$.\\n\\nSingle or Multi-turn Model\\n\\nThe first factor we investigate is how allowing multiple turns helps grounding. For each example, we can feed the entire interaction history as input to the agent model and supervise agent selection on the very last turn $T$:\\n\\n$$P_\\\\theta(a_T' = g | S,c[0,T],a[0,T-1]) \\\\tag{7}$$\\n\\nWe can further reduce the input to be $(S,c_0)$ only, making a single-turn model. To evaluate single-turn model with multi-turn examples, we simply concatenate all $c_t$ into one instruction.\\n\\nInstruction-only Model\\n\\nTo understand how it helps grounding by taking into account of previous actions of the agent in the multi-turn model (Eq. 7), we introduce the command-only baseline, which ignores agent actions (selections) in the interaction history:\\n\\n$$P_\\\\theta(a_T' = g | S,c[0,T]) \\\\tag{8}$$\\n\\nImitation Model\\n\\nInstead of supervising the agent only at the last turn, we can model the entire action sequence as an imitation model:\\n\\n$$\\\\prod_t P_\\\\theta(a_t' = a_t | S,c[0,t],a[0,t-1]) \\\\tag{9}$$\\n\\nThis variant investigates whether the supervision of the intermediate actions helps.\\n\\nOffline RL\\n\\nLastly, because each turn the agent action affects how the user responds, MUG can be formulated as a RL problem where the user and the UI constitute the environment. We use the Decision Transformer (Chen et al., 2021) for offline RL, by inserting extra learnable return tokens $w_t$ to the $T_{dec}$ before each action:\\n\\n$$T_{dec}(v,c_0,w_0,v_{a_0},...,c_t,w_t)$$\\n\\nThe model is:\\n\\n$$\\\\prod_t P_\\\\theta(a_t' = a_t | S,c[0,t],w[0,t],a[0,t-1]) \\\\tag{10}$$\\n\\nPossible discrete return tokens are $\\\\{1,2,3,4\\\\}$ where 1 on the last turn. During testing, we follow Chen et al. (2021) to force the current turn to have return 1 and adjust prior returns.\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nTable 4: Offline agent F1 \u2191 on the test set. F1 0-4 are from model trained with seed 1 and avg std is F1 4 of 5 runs.\\n\\n6.2 U SER MODELS\\n\\nHere, we design a simple and effective heuristics-based user model, and then develop a neural version. To show automatic online evaluation is a promising direction for M UG, we also conducted human evaluation on a shared set of 500 examples from the test split (Sec. 6.7).\\n\\nHeuristics-based Model\\n\\nWe observe that, when the selection $a'_t$ is incorrect, we can deterministically devise a follow-up instruction by using a template as below:\\n\\nNot the $a'_t$, click the $g$ to/on the $d$.\\n\\nThis template is to be instantiated on view hierarchy features (in Appx. D). Compared to human follow-ups, heuristic ones are more specific and longer, such as:\\n\\n\u2022 Not the $i$, click the action notifications on the top right of the screen.\\n\\nNeural Instruction Model\\n\\nWe extend the Multi-agent architecture to model follow-up commands:\\n\\n$$P(\\\\phi(c'_t = c_t | S, g, c[0, t-1], a[0, t-1])) \\\\quad (11)$$\\n\\nwhich has decoder $T_{\\\\text{dec}}(v, v_g, c_0, v_a_0, c_1, ..., v_a_{t-1})$ at turn $t$. For training, we teacher-force at each turn ($t > 0$). We found that using heuristics as prompt greatly boosts development CIDEr (Vedantam et al., 2015) to from 70 to 78. For inference, we use greedy decoding with a maximum length of 12.\\n\\n6.3 METRICS\\n\\nWe focus on evaluating the agent model as it is the pivot task of M UG. Intuitively, we would want the agent to take the desired action $g$ with as few turns as possible. That is,\\n\\n$$F_1_t = \\\\sum P(a_t = g | S, c[0, t], a[0, t-1]) \\\\quad (12)$$\\n\\nwhere, in practice, we compute $F_1_t$ with early stop over turns to avoid double counting. Clearly, an agent with high $F_1$ and a lower value of $t$ is better than an agent that requires more turns for the same accuracy. With $t$ limited to 0, the task is reduced to a one-pass grounding task. In an extreme case, we consider an agent with high $F_1$ 0 but flat changes in $F_1 t>0$ to be problematic, since it questions the agent's understanding about the interface. For more comprehensive testing, we also use a simple robustness metric for prediction changes across turns:\\n\\n$$\\\\Gamma = P(|\\\\{a_t\\\\}| \\\\neq T) \\\\quad (13)$$\\n\\nwhich is the percentage of examples that have duplicate actions within $T$ valid turns. We observe that user models, unlike human annotators, sometimes give the same instruction across turns, especially when the agent repeats the same error. Therefore, we only count agent selections that are associated with unique instructions in each example, and ignore other turns.\\n\\n6.4 OFFLINE RESULTS\\n\\nTab. 4 presents offline results on the test set, over the Challenging (see Sec. 4) and the All sets. During inference, we use instructions from the human user and actions from the human agent for turns in between and ask an agent model to predict at each turn. Doing so requires agent models to correct human agent actions, instead of the model's own. Clearly, the models that take into account interaction history outperform those use none or partially. While the Ins-only and the Imitation models perform closely on the All set, they bear larger margins on the Challenging and online tests.\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Online agent F1\u2191 on the test set. F10-4 are from model trained with seed 1 and avg std is F14 of 5 runs.\\n\\n6.5 ONLINE RESULTS\\n\\nTab. 5 presents online test scores. In general, models that are supervised by action sequences (i.e., Imitation and Offline RL) perform better. Both heuristics-based and neural user models are able to guide agents towards task completion. Comparing Single\u2019s F10 and Imitation\u2019s F14, we see that properly using interaction boosts task completion by 18 and 31 on the Challenging and All test sets.\\n\\nThe average F14\u2019s show that heuristics-based user works better, except that the Imitation collaborates better with the neural user. This might be attributed to the neural user is trained to mimic human command patterns which can be ambiguous and short, while heuristics are more precise while being artificial. This also implies that a large room for further improvement to the user modeling.\\n\\nOverall, we can see interactive grounding is a challenging task, even on a single screen. The agent modeling involves robust multimodal understanding to self-correct. The user modeling requires controlled language generation, which is still an open problem. The best task completion rate on the Challenging subset is only \u223c55%, suggesting a large room for future improvements.\\n\\n6.6 AGENT ROBUSTNESS\\n\\nWe take a deeper look at agent behavior in Tab. 6. We observe that agents with higher F1 tend to be more robust (lower \u0393). The best agent model (Imitation) repeats the same mistake for only 16.8% on the All test set. However, if we ignore those examples finished in 1 turn i.e., T > 1 columns, the repeating rate rises to \u223c40%. The Heuristics user, while generally improves agent F1 more than the Neural user, has a mixed robustness impact on the Imitation and Offline RL agents. On weaker agents (the first 3 rows), the Heuristics user leads to more salient robustness. These observations suggest improving agent F1 has a more direct and positive impact on robustness.\\n\\nTable 6: Agent \u0393\u2193 on the test split. Results are from 5 random runs. Smaller \u0393 means more robust.\\n\\n6.7 AUTOMATIC EVALUATION VS. HUMAN EVALUATION\\n\\nTo show automatic online test is a promising surrogate for human-in-the-loop evaluation, we compare Single with Multi 2 with a group of human annotators (acting as the user) (Tab. 7). We ask the user annotators to follow the same annotation interface and guideline in Sec. 4, and let them to use the trained agent model to ground their commands. That is, human plays the user role and a trained\\n\\n2 We choose these two models as a pilot study since they perform consistently different in all our metrics.\"}"}
{"id": "bbf_lxmcpTQ", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nAn agent model plays the agent role. This setting maximally mimics a realistic situation where a human user guides the agent to locate a target solely using language commands. The results (Tab. 7) are generally consistent with those from the automatic evaluation (Tab. 5). We should also note that such human study is not meant to reflect every minor differences in automatic evaluations.\\n\\n| Model | F1 | F1 | F1 | F1 | F1 | \u0393\u2193 |\\n|-------|----|----|----|----|----|----|\\n| Single | 50.0 | 56.4 | 58.2 | 58.4 | 59.4 | 42.6 |\\n| Multi  | 49.6 | 58.4 | 60.4 | 62.2 | 62.6 | 39.4 |\\n\\nTable 7: Human-in-the-loop evaluation on 500 examples from the All test set. Models are trained with seed 1.\\n\\n6.8 ABLATION ON HERTS\\n\\nTo show agent improves from follow-up instructions effectively, instead of overfitting potential artifacts in the dataset, we report our ablation studies in Tab. 8. Specifically, we focus on the heuristics-based user since it offers well-controlled instruction generation. We can see that random heuristics underperform by $\\\\sim 14\\\\%$ and repeating the initial instruction is even worse. The $\\\\Gamma$ scores also suggest that randomly instantiated instructions are less effective in guiding the agent.\\n\\n| Multi | F1 | F1 | F1 | F1 | F1 | avg | std | \u0393\u2193 |\\n|-------|----|----|----|----|----|-----|-----|----|\\n| Heuristics | 25.2 | 47.8 | 50.9 | 51.7 | 52.4 | -40.0 |\\n| Random (5 runs) | 25.2 | 32.7 | 34.3 | 34.7 | 35.1 | 35.6 | 0.9 | 51.6 |\\n| Repeat | 25.2 | 29.3 | 30.9 | 31.6 | 32.0 | - |\\n\\nTable 8: Ablation of instructions using heuristics-based user model for the Multi agent on the Challenging test set. The Multi is trained with seed 1.\\n\\nRandom: randomly instantiated heuristics for $c_t > 0$ across 5 seeds.\\n\\n6.9 ERROR ANALYSIS\\n\\nWe manually analyze errors from the best agent (Imitation). In Tab. 9, we inspect 30 failed development examples (i.e., unfinished after 5 turns) that are subject to the Neural user. Due to the role interplay, we also count problematic commands. We observe that the user model sometimes issues repetitive or uninformative instructions starting from the 3rd turn, leading the agent to the same wrong selection. This might be caused by the data sparsity for examples with $\\\\geq 3$ turns.\\n\\n| Agent | User | Incapabilities | text | icon | UI layout | pos/dir | wrong | stale | #Example |\\n|-------|------|----------------|-------|------|-----------|--------|-------|-------|---------|\\n|       |      |                |       |      |           |        |       |       |         |\\n\\nTable 9: Major error categories of the Imitation model on 30 failed development examples (150 turns). stale $c_t$: repetitive/uninformative instruction. Model is trained with random seed 1.\\n\\n7 CONCLUSIONS\\n\\nIn this paper, we presented MUG, a novel and challenging task for multimodal grounding on UI. MUG requires a grounding agent being able to correct its own prediction, and allows a user to guide the agent via natural language instructions. We contribute a new dataset for the task, investigate various modeling options for the agent and developed multiple evaluation strategies including two user models so as to enable automatic online testing. We found that interaction greatly improves grounding accuracy in the UI domain. Our experiments and analyses also suggest large room for grounding performances, even on a seemingly easy single screen task, which calls for future investigation. Our work contributes to the general effort of multimodal language understanding and its robustness by enabling synchronized multi-turn interactivity.\"}"}
