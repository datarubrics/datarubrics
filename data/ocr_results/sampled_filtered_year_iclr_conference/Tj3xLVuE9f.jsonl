{"id": "Tj3xLVuE9f", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Our work aims to achieve a principled and fundamental understanding of the mechanisms of deep learning\u2014when it succeeds, when it fails, and why it fails. As the goal is to better understand existing algorithms, there is no downside risk of the research. The upside benefit concerns cases of shortcut learning which have been identified as socially problematic and where fairness issues are at play, for example, the use of shortcut features such as gender or ethnicity that might be spuriously correlated with a core feature (e.g., likelihood of default).\\n\\nMethods details accompanying these figures appear in Section C.\\n\\nFigure B.1: Bias as a function of $z_s$ availability and predictivity for a single-hidden-layer MLP with a $tanh$ hidden-layer activation function. Settings match those described in Figure 3C.\\n\\nFigure B.2: The $z_s$-reliance (Sections 3 and C.1) of hypothetical classifiers which rely exclusively on $z_s$ (top left) or $z_c$ (top right), or rely equally on both features (bottom row). Color indicates the predicted label (red: pos, blue: neg) for probe items plotted by base probe ($z_s, z_c$) values.\"}"}
{"id": "Tj3xLVuE9f", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nFigure B.3: Supplementary data for the results presented in Figure 2. Top row: Model performance on the train (left) and validation (right) sets. Bottom row: $z_s$-reliance for the optimal classifier (left) and models (right).\\n\\nFigure B.4: The results shown in Figure 2 are not specific to choice of $\\\\sigma_{sc}$. Results for the same experiment using datasets with $\\\\sigma_{sc}=0.4$ (A) and $\\\\sigma_{sc}=0.8$ (B).\\n\\nFigure B.5: Supplementary data for the results presented in Figure 3. Model performance on the train (left) and validation (right) sets.\"}"}
{"id": "Tj3xLVuE9f", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure B.6: Supplementary data for image experiments. Model performance underlying A: Experiments with the Waterbirds datasets presented in Figure 6 and described in C.5, and B: Experiments with image instantiations of the base dataset manipulating pixel footprint presented in Figure 4 B (left) and C (right).\\n\\nFigure B.7: Models prefer a shallowly embedded nonlinear feature ($z_s$) to a deeply embedded nonlinear one ($z_c$). A: The color of each cell of the heatmap indicates the mean bias of models as a function of the degree of nesting of $z_c$ ($\\\\eta_c$), and the predictivity of $z_s$ ($\\\\rho_s$). $z_s$ is always shallowly embedded ($\\\\eta_s = 0$), and the predictivity of $z_c$ is held fixed ($\\\\rho_c = 0.9$). See C.3 for additional details.\\n\\nB: Model performance. C: $z_s$-reliance of the optimal classifier (left) versus the model (right).\"}"}
{"id": "Tj3xLVuE9f", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure B.8: Illustration of studied Background availability manipulations. For each of the six perturbations influencing the availability of both the background and the bird (in the case of bird scale), we provide visual representations of the potential perturbations applied to the training dataset. For instance, the first row demonstrates variations in the dataset resulting from changes in bird size, ranging from 5% to 95%.\"}"}
{"id": "Tj3xLVuE9f", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For ReLU kernel, we had only two eigenfunctions with the form,\\n\\n$$\\n\\\\phi_1(x) = \\\\|x\\\\| \\\\|A\\\\mu\\\\|_1 + \\\\langle x, A\\\\mu \\\\rangle \\\\|A\\\\mu\\\\|_2^2,\\n$$\\n\\nand\\n\\n$$\\n\\\\phi_2(x) = \\\\langle x, A\\\\mu \\\\rangle \\\\|A\\\\mu\\\\|_2 \\\\|A\\\\mu\\\\|_2^2.\\n$$\\n\\nReplacing them in the above definitions yields,\\n\\n$$\\nf(x) = \\\\frac{1}{2} \\\\|x\\\\| \\\\|A\\\\mu\\\\|_1 + \\\\langle x, A\\\\mu \\\\rangle \\\\|A\\\\mu\\\\|_2 \\\\|A\\\\mu\\\\|_2^2 + \\\\frac{1}{2} \\\\langle A\\\\mu, x \\\\rangle \\\\|A\\\\mu\\\\|_2 \\\\|A\\\\mu\\\\|_2.\\n$$\\n\\nIt is easy to obtain,\\n\\n$$\\nf_2(x) = \\\\frac{1}{4} \\\\|x\\\\| \\\\|A\\\\mu\\\\|_1 \\\\|A\\\\mu\\\\|_2^2 (1 + \\\\langle x, A\\\\mu \\\\rangle \\\\|A\\\\mu\\\\|_2^2) + \\\\frac{1}{2} \\\\langle A\\\\mu, x \\\\rangle \\\\|A\\\\mu\\\\|_2 \\\\|A\\\\mu\\\\|_2.\\n$$\\n\\nand therefore,\\n\\n$$\\n\\\\int_{\\\\mathbb{R}^n} f_2(x) p(x) \\\\, dx = \\\\frac{1}{2} \\\\|A\\\\mu\\\\|_1 \\\\|A\\\\mu\\\\|_2^2 \\\\frac{1}{2} (1 + \\\\langle A\\\\mu, A\\\\mu \\\\rangle \\\\|A\\\\mu\\\\|_2^2) + \\\\frac{1}{2} \\\\langle A\\\\mu, A\\\\mu \\\\rangle \\\\|A\\\\mu\\\\|_2 \\\\|A\\\\mu\\\\|_2.\\n$$\\n\\nThe paper presents an analytical form that characterizes the trade-off between predictivity and availability in a single-layer ReLU model for classifying two Gaussian sources. In order to keep the theoretical analysis tractable, we use the following approximations. First, we use an asymptotic approximation to the covariance matrix by having the size of the covariance going to infinity.\"}"}
{"id": "Tj3xLVuE9f", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure I.1: Plot of \\\\( \\\\text{sign}(\\\\left| \\\\zeta_1 \\\\right| - \\\\left| \\\\zeta_2 \\\\right|)(a_1 - a_2) \\\\) for ReLU and Quadratic NTK models trained on 50 training samples from positive \\\\( N(\\\\mu, C) \\\\) and negative \\\\( N(-\\\\mu, C) \\\\) classes. Here, \\\\( \\\\mu = (\\\\mu_1, \\\\mu_2) \\\\), covariance is \\\\( C = \\\\sigma^2 I \\\\), where \\\\( \\\\mu_1 = 1 \\\\) and \\\\( \\\\sigma = 0.01 \\\\). Top and bottom figures respectively correspond to \\\\( \\\\mu_2 = 10 \\\\) and \\\\( \\\\mu_2 = 0 \\\\).\\n\\nTo zero. Second, use replaced the ReLU kernel function by a quadratic approximation. A natural question is, whether the predictions made by our theory are sensitive to these approximations. More precisely, if we work with the actual ReLU kernel and a real covariance matrix, whether observe similar predictions.\\n\\nWe verify this here by learning a model from finite training data in the NTK regime. The latter amounts to performing a kernel regression with kernel specified by the NTK. The result of the simulation is provided in Figure I.1 and confirms that the approximations used in the theory are not affecting the actual predictions, in the sense that it matches the trend of the predictions made by the theory from Figure 5.\\n\\nThe left panel in Figure I.1 still uses quadratic kernel. However, it is generated by actual predictions from a model trained by finite training data, whose distribution is a real covariance (as opposed to an asymptotically small one). This confirms the closed-form expression derived from our theory (predictions from Figure 5) match the simulation result.\\n\\nThe right panel of Figure I.1 goes further and replaces the quadratic kernel by the actual ReLU kernel. Observe that this does not affect the trend of the predictions, hence showing there is not much lost by switching between the ReLU kernel and its quadratic approximation.\\n\\n```python\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\n\\ndef kernel(x1, x2):\\n    norm_x1 = np.sqrt(np.dot(x1, x1))\\n    norm_x2 = np.sqrt(np.dot(x2, x2))\\n    u = np.dot(x1 / norm_x1, x2 / norm_x2)\\n    return np.sign(u) * (np.abs(u) - 1) * (u - 1)\\n```\\n\\nNUM_PLOT_POINTS = 20\\n# Number of points used to plot the result (larger better but slower)\\nN_TRAIN_SAMPLES = 50\\n# Number of training points per Gaussian source\\nUSE_QUAD = True\\n# When to use ReLU or its quadratic approximation\\nMU_1 = 1.0\\nMU_2_List = [10, 0.1]\\nSCALE_COV = 0.0001\\n# Scale parameter \\\\( \\\\sigma \\\\) from the paper\\nCOR = 0.0\\n# \\\\( |\\\\text{COR}| \\\\) must be less than 1\\nEPS = SCALE_COV / 100\\n# Epsilon identity added before inverting kernel matrix\\nDELTA = 0.05\\n# \\\\( \\\\delta \\\\) to approximate derivatives with finite different\"}"}
{"id": "Tj3xLVuE9f", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"if np.abs(u) > 1.0:\\n    u = np.sign(u)\\n\\nif USE_QUAD:\\n    h = 815.0 / 3072.0 * (1 + u)**2\\nelse:\\n    h = (u * (np.pi - np.arccos(u)) + np.sqrt(1 - u**2)) / np.pi\\n\\nreturn norm_x1 * norm_x2 * h\\n\\ndef kernel_predict(A, x, y, X, kernel_func):\\n    # do linear ridge regression to obtain coefficients c_1 to c_2n\\n    K = np.empty((2 * N_TRAIN_SAMPLES, 2 * N_TRAIN_SAMPLES))\\n    for i in range(2 * N_TRAIN_SAMPLES):\\n        for j in range(2 * N_TRAIN_SAMPLES):\\n            K[i, j] = kernel_func(A @ x[i], A @ x[j])\\n    c = np.linalg.solve(K + EPS * np.identity(2 * N_TRAIN_SAMPLES), y)\\n    # compute prediction vector\\n    K_pred = np.empty((2 * N_TRAIN_SAMPLES, 2 * N_TRAIN_SAMPLES))\\n    for i in range(2 * N_TRAIN_SAMPLES):\\n        for j in range(2 * N_TRAIN_SAMPLES):\\n            K_pred[i, j] = kernel_func(A @ X[i], A @ x[j])\\n    return K_pred @ c\\n\\n### MAIN ###\\nsensitivity = np.empty((len(MU_2_List), NUM_PLOT_POINTS, NUM_PLOT_POINTS))\\n    for MU_2_indx in range(len(MU_2_List)):\\n        MU_2 = MU_2_List[MU_2_indx]\\n        # generate 2d-gaussian data (n samples per class) for two classes: x0 and x1\\n        mean_class_0 = np.array([MU_1, MU_2])\\n        mean_class_1 = -mean_class_0\\n        cov = SCALE_COV * np.array([[1.0, COR], [COR, 1.0]])\\n        rng = np.random.default_rng()\\n        x0 = rng.multivariate_normal(mean_class_0, cov, size=N_TRAIN_SAMPLES)\\n        y0 = np.zeros(N_TRAIN_SAMPLES)\\n        x1 = rng.multivariate_normal(mean_class_1, cov, size=N_TRAIN_SAMPLES)\\n        y1 = np.ones(N_TRAIN_SAMPLES)\\n        x = np.concatenate((x0, x1), axis=0)\\n        y = np.concatenate((y0, y1), axis=0)\\n        X = x\\n        for i_a1 in range(NUM_PLOT_POINTS):\\n            a1 = (4.0 * (i_a1 + 1)) / NUM_PLOT_POINTS\\n            print('completion:', i_a1 + 1, 'of', NUM_PLOT_POINTS)\\n            for i_a2 in range(NUM_PLOT_POINTS):\\n                a2 = (4.0 * (i_a2 + 1)) / NUM_PLOT_POINTS\\n                A = np.diag([a1, a2])\\n                pred = kernel_predict(A, x, y, X, kernel)\\n                A_Perturbed = A + [[DELTA * np.abs(a1), 0], [0, 0]]\\n                pred_inc_a1 = kernel_predict(A_Perturbed, x, y, X, kernel)\\n                A_Perturbed = A + [[0, 0], [0, DELTA * np.abs(a2)]]\\n                pred_inc_a2 = kernel_predict(A_Perturbed, x, y, X, kernel)\\n                # normalize prediction vectors\"}"}
{"id": "Tj3xLVuE9f", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[\\npred = \\\\frac{pred}{\\\\sqrt{\\\\text{np.dot}(pred, pred)}}\\n\\\\]\\n\\n\\\\[\\npred_{\\\\text{inc} a_1} = \\\\frac{pred_{\\\\text{inc} a_1}}{\\\\sqrt{\\\\text{np.dot}(pred_{\\\\text{inc} a_1}, pred_{\\\\text{inc} a_1})}}\\n\\\\]\\n\\n\\\\[\\npred_{\\\\text{inc} a_2} = \\\\frac{pred_{\\\\text{inc} a_2}}{\\\\sqrt{\\\\text{np.dot}(pred_{\\\\text{inc} a_2}, pred_{\\\\text{inc} a_2})}}\\n\\\\]\\n\\n\\\\[\\n\\\\text{sensitivity}[\\\\mu_{2, \\\\text{indx}, i_1, i_2}] = \\\\text{np.sign}\\left(\\\\frac{|\\\\text{np.dot}(pred, pred_{\\\\text{inc} a_1}) - 1.0|}{\\\\Delta \\\\text{np.abs}(a_1) - |\\\\text{np.dot}(pred, pred_{\\\\text{inc} a_2}) - 1.0|} \\\\frac{a_1 - a_2}{\\\\Delta \\\\text{np.abs}(a_2)}\\\\right)\\n\\\\]\\n\\n```python\\n# Plot Sensitivity\\nxv, yv = np.meshgrid(np.linspace(0, 4, NUM_PLOT_POINTS),\\nnp.linspace(0, 4, NUM_PLOT_POINTS))\\nfig, (ax1, ax2) = plt.subplots(2)\\nax1.pcolormesh(xv, yv, sensitivity[0], vmin=-1, vmax=1);\\nax2.pcolormesh(xv, yv, sensitivity[1], vmin=-1, vmax=1);\\nax1.set_box_aspect(1)\\nax2.set_box_aspect(1)\\nplt.show()\\n```\"}"}
{"id": "Tj3xLVuE9f", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGMENTS\\n\\nWe thank Pieter-Jan Kindermans for feedback on the manuscript, and Jaehoon Lee, Lechao Xiao, Robert Geirhos, Olivia Wiles, Isabelle Guyon, and Roman Novak for interesting discussions.\\n\\nREFERENCES\\n\\nAmirhossein Ahmadian and Fredrik Lindsten. Enhancing representation learning with deep classifiers in presence of shortcut. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1\u20135. IEEE, 2023.\\n\\nMartin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.\\n\\nSanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nNicholas Baker, Hongjing Lu, Gennady Erlikhman, and Philip J Kellman. Deep convolutional networks do not classify based on global object shape. PLoS computational biology, 14(12):e1006613, 2018.\\n\\nNicholas Baker, Hongjing Lu, Gennady Erlikhman, and Philip J Kellman. Local features and global shape information in object classification by deep convolutional neural networks. Vision research, 172:46\u201361, 2020.\\n\\nSara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the European conference on computer vision (ECCV), pp. 456\u2013473, 2018.\\n\\nAlberto Bietti and Francis R. Bach. Deep equals shallow for relu networks in kernel regimes. ArXiv, abs/2009.14397, 2020.\\n\\nJoy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, pp. 77\u201391. PMLR, 2018.\\n\\nYoungmin Cho and Lawrence Saul. Kernel methods for deep learning. In Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta (eds.), Advances in Neural Information Processing Systems, volume 22. Curran Associates, Inc., 2009.\\n\\nThomas Fel, Remi Cadene, Mathieu Chalvidal, Matthieu Cord, David Vigouroux, and Thomas Serre. Look at the variance! efficient black-box explanations with sobol-based sensitivity analysis. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\\n\\nThomas Fel, Victor Boutin, Mazda Moayeri, R\u00e9mi Cad\u00e8ne, Louis Bethune, Mathieu Chalvidal, Thomas Serre, et al. A holistic approach to unifying automatic concept extraction and concept importance estimation. In Advances in Neural Information Processing Systems (NeurIPS), 2023.\\n\\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018a.\\n\\nRobert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Sch\u00fctt, Matthias Bethge, and Felix A Wichmann. Generalisation in humans and deep neural networks. Advances in neural information processing systems, 31, 2018b.\\n\\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665\u2013673, 2020.\\n\\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018.\"}"}
{"id": "Tj3xLVuE9f", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Tj3xLVuE9f", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Tj3xLVuE9f", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Tj3xLVuE9f", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure B.9: Manipulations of image backgrounds show that availability as well as predictivity determines feature reliance in vision models, including in models pretrained on ImageNet.\\n\\nExpanding on the results shown in Figure 6, we plot Bird accuracy for incongruent probes (opposing Bird and Background labels) as a function of Background predictivity ($\\\\rho$) and hypothesized types of availability. TOP ROW: ResNet18 models. BOTTOM ROW: IN ResNet50 models (see Section C.5 for additional details).\"}"}
{"id": "Tj3xLVuE9f", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Figures 6 and B.9, we saw that a Bird size manipulation affects models\u2019 tendency to classify incongruent probes consistent with their Bird labels. Here, we see that attribution maps for probe images (top row) reflect a difference in focal point for models trained on images with Bird size $= 40\\\\%$ (preference for image background) versus Bird size $= 80\\\\%$ (preference for foreground bird). See C.5 for additional details.\"}"}
{"id": "Tj3xLVuE9f", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SUPPLEMENTARY METHODS AND RESULTS\\n\\nC.1 ASSESSING BIAS\\n\\nIn Section 3, we described how we determine reliance \\\\( M \\\\), a score which quantifies the extent to which a model (optimal classifier or trained model) uses feature \\\\( z_s \\\\) as the basis for its classification decisions. In Figure B.2, for intuition, we show the \\\\( z_s \\\\) reliance of hypothetical classifiers.\\n\\nC.2 SYNTHETIC DATA\\n\\nOptimal classifier. To obtain optimal classifications, we use Linear Discriminant Analysis (LDA, as implemented in \\\\textit{Sklearn} with the least-squares solver). In all but the experiments on the effect of nesting as part of the generative process (C.3 and Figure B.7), the optimal classifier was fit to and probed with the same embedded base inputs used to train the corresponding model in a given experiment. In the nesting experiments, LDA was fit to and probed with base inputs directly.\\n\\nC.3 VECTOR EXPERIMENTS\\n\\nDefault model architecture and training procedure. Unless otherwise described, we train a multi-layer perceptron (MLP, depth = 8, width = 128, hidden activation function = ReLU) with MSE loss for 100 epochs using an SGD optimizer with batch size = 64 and learning rate = \\\\( 1 \\\\times 10^{-2} \\\\). We use Glorot Normal weight initialization. Models prefer the more-available feature, even when it is less predictive. In these experiments, given a base dataset with \\\\( (\\\\rho_s, \\\\rho_c) \\\\), for each availability setting \\\\( (\\\\alpha_s, \\\\alpha_c) \\\\), we manipulate the availability of the dataset, sample a random initialization seed, train the default model architecture, and evaluate the bias of the model. We repeat this process for each of 10 runs, computing the biases for individual \\\\((\\\\text{model, optimal classifier})\\\\) pairs, and then averaging the results.\\n\\nEffect of nesting on availability. Figure 1B illustrates how our generative procedure for synthetic datasets supports nested embedding of latent features, as described in Section 3. In this experiment, we test whether a model prefers a shallowly embedded nonlinear feature \\\\( z_s \\\\) to a deeply embedded nonlinear one \\\\( z_c \\\\) when the amplitude of both features is matched \\\\( (\\\\alpha_s = \\\\alpha_c = 0.1) \\\\) and the nonlinearity is a scaled \\\\textit{tanh} \\\\((\\\\text{tanh}(\\\\lambda x), with \\\\lambda = 100) \\\\). After embedding each feature as \\\\( e_i = \\\\alpha_i w_i z_i \\\\), we apply the nonlinearity. For a nesting factor \\\\( \\\\eta_i > 0 \\\\), we pass \\\\( e_i \\\\) through a fully connected network (depth = \\\\( \\\\eta_i \\\\), width = \\\\( d \\\\), no bias weights) with scaled \\\\textit{tanh} activations on the hidden and output layers. We initialize weight matrices to be random special orthogonal matrices (implemented as \\\\textit{scipy.stats.special_ortho_group}). In Figure B.7, we show bias as a function of \\\\( \\\\eta_c \\\\) and \\\\( \\\\rho_s \\\\); \\\\( z_s \\\\) is fixed to be shallowly embedded with \\\\( \\\\eta_s = 0 \\\\) and has \\\\( \\\\rho_c = 0 \\\\). We report the results as the mean across 3 runs.\\n\\nC.4 PIXEL FOOTPRINT EXPERIMENTS\\n\\nModel architecture and training procedure. In these experiments, we train a randomly initialized ResNet18 architecture with MSE loss using an Adam optimizer with batch size = 64 and learning rate = \\\\( 1 \\\\times 10^{-2} \\\\), taking the best (defined on validation accuracy) across 100 epochs of training. For each \\\\((\\\\rho_s, \\\\rho_c)\\\\) predictivity setting, we repeat this process for 10 (sampled dataset, random weight initialization) runs.\\n\\nAnalyses. In Figure 4, we report results for experiments in which we use a base \\\\( z_s \\\\) pixel footprint of 400 px, and manipulate \\\\( z_s \\\\) pixel footprint to be \\\\( 1-5 \\\\times \\\\) larger \\\\( (\\\\alpha_c = 1, \\\\alpha_s \\\\in [1, 5]) \\\\). To determine the bias of a model, we compute the \\\\( z_s \\\\)-reliance of the trained model given image instantiations of the base probe items, and compare to the \\\\( z_s \\\\)-reliance of the optimal classifier (LDA) fit to and probed using vector instantiations of the same base dataset with the same \\\\( \\\\alpha_s \\\\) and \\\\( \\\\alpha_c \\\\). We repeat this process for 5 runs, computing bias on a per-run basis as in Section C.3.\\n\\nC.5 FEATURE AVAILABILITY IN NATURALISTIC DATASETS\\n\\nDatasets. Waterbirds. Waterbirds images (Sagawa et al., 2020a) combine birds taken from the Caltech-UCSD Birds-200-2011 dataset (Wah et al., 2011) and backgrounds from the Places dataset.\"}"}
{"id": "Tj3xLVuE9f", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To construct the datasets used in Figure 6A, we sample images from a base Waterbirds dataset generated using code by (Sagawa et al., 2020a) (github.com/kohpangwei/group_DRO) with \\\\( \\\\text{val frac} = 0.2 \\\\) and \\\\( \\\\text{confounder strength} = 0.5 \\\\), yielding sets of 5694 train images (\\\\( 224 \\\\times 224 \\\\)), 300 validation images, and 5794 test images. We then subsample these sets to 1200 train, 90 validation (\\\\( \\\\times 2 \\\\) sets), and 1000 probe images (\\\\( \\\\times 2 \\\\) sets), respectively, such that the train and validation sets instantiate target feature predictivities, and the probe sets contain congruent or incongruent feature-values, as described in Section 7.\\n\\nTo construct the datasets used in Figures 6C and B.9, for a given predictivity setting, we subsample 8 bird types per category (land, water) and cross them with land and water backgrounds randomly sampled from the standard Waterbirds background classes, to generate class-balanced train sets ranging in size from 800\u22121200, and incongruent probe sets of size 400.\\n\\nCelebA. The CelebA (Liu et al., 2015) dataset contains images of celebrity faces paired with a variety of binary attribute labels. In our experiments, we cast the \u201cAttractive\u201d label as the core feature, and the \u201cSmiling\u201d label as the non-core feature. To construct the datasets used in Figure 6B, we sample images consistent with the target predictivities.\\n\\nWaterbirds availability manipulations. In the datasets depicted in Figures 6C and B.9, we aim to manipulate various aspects of background availability. To do so, we use CUB-200 masks to exclusively modify the background, and apply five distinct types of perturbations: altering bird size, removing background patches, changing background colors, applying low-pass filtering, adding Perlin noise, and adding white noise. These perturbations test hypotheses about specific image properties than may make an image background more available to a model than the foreground object. We vary Background predictivity while holding bird predictivity fixed (\\\\( \\\\approx 0.99 \\\\)). We report results for at least 500 runs (model training) for each perturbation type. Figure B.8 shows sample image to which the perturbations have been applied. In detail, the perturbations are as follow:\\n\\n- **Bird size**: We manipulate the pixel footprint of the bird by scaling the square mask. Note that an increase in bird size corresponds to a decrease in background pixels in the image.\\n- **Background patch removal**: This manipulation removes patches of the image background while preserving the foreground bird. To apply the manipulation, we partition the image into a 3\u00d73 checkerboard, position the bird in the center, and then remove \\\\( x \\\\in [1, 8] \\\\) background cells.\\n- **Color**: To assess the influence of color information, we use the original image with its colored background (\u201cColor\u201d) or convert the background to grayscale (\u201cGrayscale\u201d).\\n- **Low-pass filtering**: We apply a low-pass filter to the frequency representation of the background, with the cutoff frequency varying from 224 (original image) to 2 (retaining only components at 2 cycles per image).\\n- **Uniform noise**: Uniform noise is introduced to the image with an amplitude ranging from 0 (original image) to 255. It is essential to note that image values are clipped within the (0, 255) range before preprocessing.\\n- **Perlin noise**: We apply Perlin noise, which possesses spatial coherence, at various intensity levels along with uniform noise. We employ 6-octave Perlin noise to match the frequency distribution of natural backgrounds (wavelengths 2, 4, 8, 16, 32, 64, and 128).\\n\\nModel architecture and training procedure. For the experiments shown in Figure 6A and B, we normalize all images by the ImageNet train-set statistics (RGB mean \\\\( = [0.485, 0.456, 0.406] \\\\)), std \\\\( = [0.229, 0.224, 0.225] \\\\), and then use the same settings as described in 5.\\n\\nIn the experiments shown in Figures 6C and B.9, we preprocess images by normalizing to be in \\\\([-1, 1\\\\]), apply random crops of size \\\\( 200 \\\\times 200 \\\\), resize to \\\\( 224 \\\\times 224 \\\\), and randomly flip over the horizontal axis with \\\\( p = 0.5 \\\\) We train models with MSE loss using an Adam optimizer with batch size \\\\( = 32 \\\\), cosine decay learning rate schedule (linear warmup, initial learning rate \\\\( = 1 \\\\times 10^{-3} \\\\)), and weight decay \\\\( = 1 \\\\times 10^{-5} \\\\). For the ResNet18 experiments in Figures 6C and B.9, we train a randomly initialized ResNet18 trained for 30 epochs. For the ResNet50 experiments in Figure B.9, we train the randomly initialized readout layer of a frozen, ImageNet-pretrained ResNet50 (IN ResNet50) for 15 epochs.\"}"}
{"id": "Tj3xLVuE9f", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"et al., 2018; Arjovsky et al., 2019; McCoy et al., 2019; Hermann & Lampinen, 2020; Shah et al., 2020; Nagarajan et al., 2020; Pezeshki et al., 2021; Fel et al., 2023). Although we lack a general understanding of the cause of such preference anomalies, several specific cases have been identified. For example, features that are linearly related to classification labels are preferred by models over features that require nonlinear transforms (Hermann & Lampinen, 2020; Shah et al., 2020). Another factor leading to anomalous feature preferences is the redundancy of representation, e.g., the size of the pixel footprint in an image (Sagawa et al., 2020a; Wolff & Wolff, 2022; Tartaglini et al., 2022).\\n\\nBecause predictivity alone is insufficient to explain feature reliance, here we explicitly introduce the notion of availability to refer to the factors that influence the likelihood that a model will use a feature more so than a purely statistical account would predict. A more-available feature is easier for the model to extract and leverage. Past research has systemically manipulated predictivity; in the present work, we systematically manipulate both predictivity and availability to better understand their interaction and to characterize conditions giving rise to shortcut learning. Our contributions are:\\n\\n\u2022 We define quantitative measures of predictivity and availability using a generative framework that allows us to synthesize classification datasets with latent features having specified predictivity and availability. We introduce two notions of availability relating to singular values and nonlinearity of the data generating process, and we quantify shortcut bias in terms of how a learned classifier deviates from an optimal classifier in its feature reliance.\\n\\n\u2022 We perform parametric studies of latent-feature predictivity and availability, and examine the sensitivity of different model architectures to shortcut bias, finding that it is greater for nonlinear models than linear models, and that model depth amplifies bias.\\n\\n\u2022 We present a theoretical account based on Neural Tangent Kernels (Jacot et al., 2018) which indicates that shortcut bias is an inevitable consequence of nonlinear architectures.\\n\\n\u2022 We show that vision architectures used in practice can be sensitive to non-core features beyond their predictive value, and show a set of availability manipulations of naturalistic images which push around models' feature reliance.\\n\\n2 RELATED WORK\\n\\nThe propensity of models to learn spurious (Arjovsky et al., 2019) or shortcut (Geirhos et al., 2020) features arises in a variety of domains (Heuer et al., 2016; Gururangan et al., 2018; McCoy et al., 2019; Sagawa et al., 2020a) and is of interest from both a scientific and practical perspective. Existing work has sought to understand the extent to which this tendency derives from the statistics of the training data versus from model inductive bias (Neyshabur et al., 2014; Tachet et al., 2018; P\u00e9rez et al., 2018; Rahaman et al., 2019; Arora et al., 2019; Geirhos et al., 2020; Sagawa et al., 2020b;a; Pezeshki et al., 2021; Nagarajan et al., 2021), for example a bias to learn simple functions (Tachet et al., 2018; P\u00e9rez et al., 2018; Rahaman et al., 2019; Arora et al., 2019). Hermann & Lampinen (2020) found that models preferentially represent one of a pair of equally predictive image features, typically whichever feature had been most linearly decodable from the model at initialization. They also identified cases where models relied on a less-predictive feature that had a linear relationship to task labels over a more-predictive feature that had a nonlinear relationship to labels. Together, these findings suggest that predictivity alone is not the only factor that determines model representations and behavior. A theoretical account by Pezeshki et al. (2021) studied a situation in supervised learning in which minimizing cross-entropy loss captures only a subset of predictive features, while other relevant features go unnoticed. They introduced a formal notion of strength that determines which features are likely to dominate a solution. Their notion of strength confounds predictivity and availability, the two concepts which we aim to disengangle in the present work.\\n\\nWork in the vision domain has studied which features vision models rely on when trained on natural image datasets. For example, ImageNet models prefer to classify based on texture rather than shape (Baker et al., 2018; Geirhos et al., 2018a; Hermann et al., 2020) and local rather than global image content (Baker et al., 2020; Malhotra et al., 2020), marking a difference from how people classify images (Landau et al., 1988; Baker et al., 2018). Other studies have found that image backgrounds play an outsize role in driving model predictions (Beery et al., 2018; Sagawa et al., 2020a; Xiao et al., 2020). Two studies manipulated the quantity of a feature in an image to test how this changed model behavior. Tartaglini et al. (2022) probed pretrained models with images containing a shape consistent...\"}"}
{"id": "Tj3xLVuE9f", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To systematically explore the role of predictivity and availability, we construct synthetic datasets from a generative procedure that maps a pair of latent features, $\\\\mathbf{z} = (\\\\mathbf{z}_s, \\\\mathbf{z}_c)$, to an input vector $\\\\mathbf{x} \\\\in \\\\mathbb{R}^d$ and class label $y \\\\in \\\\{-1, +1\\\\}$. The subscripts $s$ and $c$ denote the latent dimensions that will be treated as the potential shortcut and core feature, respectively. The procedure draws $\\\\mathbf{z}$ from a multivariate Gaussian conditioned on class, $\\\\mathbf{z} | y \\\\sim \\\\mathcal{N}(\\\\begin{bmatrix} y\\\\mu_s \\\\\\\\ y\\\\mu_c \\\\end{bmatrix}, \\\\begin{bmatrix} 1 & \\\\sigma_{sc} \\\\\\\\ \\\\sigma_{sc} & 1 \\\\end{bmatrix})$, with $|\\\\sigma_{sc}| < 1$. Through symmetry, the optimal decision boundary for latent feature $i$ is $\\\\mathbf{z}_i = 0$, allowing us to define the feature predictivity $\\\\rho_i \\\\equiv \\\\Pr(y = \\\\text{sign}(\\\\mathbf{z}_i))$. For Gaussian likelihoods, this predictivity is achieved by setting $\\\\mu_i = \\\\sqrt{2 \\\\text{erf}^{-1}(2\\\\rho_i - 1)}$.\\n\\nFigure 1A shows sample latent-feature distributions for $\\\\rho_c = 0$ and two levels of $\\\\rho_s$.\\n\\nAvailability manipulations. Given a latent vector $\\\\mathbf{z}$, we manipulate the hypothesized availability (hereafter, simply availability) of each feature independently through an embedding procedure, sketched in Figure 1B, that yields an input $\\\\mathbf{x} \\\\in \\\\mathbb{R}^d$. We posit two factors that influence availability of feature $i$, its amplification $\\\\alpha_i$ and its nesting $\\\\eta_i$. Amplification $\\\\alpha_i$ is a scaling factor on an embedding, $e_i = \\\\alpha_i w_i \\\\mathbf{z}_i$, where $w_i \\\\in \\\\mathbb{R}^d$ is a feature-specific random unit vector. Amplification includes manipulations of redundancy (replicating a feature) and magnitude (increasing a feature's dynamic range). Nesting $\\\\eta_i$ is a factor that determines ease of recovery of a latent feature from an embedding. We assume a nonlinear, rank-preserving transform, $e'_i = f_{\\\\eta_i}(e_i)$, where $f_{\\\\eta_i}$ is a fully connected, random net with $\\\\eta_i \\\\in \\\\mathbb{N}$ tanh layers in cascade. For $\\\\eta_i = 0$, feature $i$ remains in explicit form, $e'_i = e_i$; for $\\\\eta_i > 0$, the feature is recoverable through an inversion of increasing complexity with $\\\\eta_i$.\\n\\nTo complete the data generative process, we combine embeddings by summation: $\\\\mathbf{x} = e'_s + e'_c$. \\n\\nAssessing shortcut bias. Given a synthetic dataset and a model trained on this dataset, we assess the model's reliance on the shortcut feature, i.e., the extent to which the model uses this feature as a basis for classification decisions. When $\\\\mathbf{z}_c$ and $\\\\mathbf{z}_s$ are correlated ($\\\\sigma_{sc} > 0$), some degree of reliance on the shortcut feature is Bayes optimal (see orange dashed line in Figure 6B). Consequently, we need to assess reliance relative to that of an optimal classifier. We perform this assessment in latent space, where the Bayes optimal classifier can be found by linear discriminant analysis (LDA) (see Figure 3).\"}"}
{"id": "Tj3xLVuE9f", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Deep nonlinear models can prefer a less-predictive but more-available feature to a more-predictive but less-available one. The color of each cell in the heatmap indicates the mean bias of a model as a function of the availability and predictivity of the shortcut feature, $z_s$. The inset shows in faint coloring the decision surface for an optimal Bayes classifier (LDA) and a trained model. Overlaid points are a subset of training instances. The model obtains a shortcut bias of 0.53.\\n\\nThe shortcut bias is the reliance of a model on the shortcut feature over that of the optimal, LDA:\\n\\n$$\\\\text{bias} = \\\\text{reliance}_{model} - \\\\text{reliance}_{optimal}.$$  \\n\\nAppendix C.1 describes and justifies our reliance measure in detail. For a given model $M$, whether trained net or LDA, we probe over the latent space and determine for each probe $z$ the binary classification decision, $\\\\hat{y}_M(z)$ (see Figure 2). Shortcut reliance is the difference between the model's alignment (covariance) with decision boundaries based only on the shortcut and core features:\\n\\n$$\\\\text{reliance}_M = 2n\\\\left|\\\\sum_i \\\\hat{y}_M i \\\\text{sign}(z_s i)\\\\right| - \\\\left|\\\\sum_i \\\\hat{y}_M i \\\\text{sign}(z_c i)\\\\right|,$$\\n\\nwhere $n$ is the number of probe items. Both the reliance score and shortcut bias are in $[-1, +1]$.\\n\\n### Experiments Manipulating Feature Predictivity and Availability\\n\\n#### Methodology\\n\\nUsing the procedure described in Section 3, we conduct controlled experiments examining how feature availability and predictivity and model architecture affect the shortcut bias. We sample class-balanced datasets with 3200 train instances, 1000 validation instances, and 900 probe (evaluation) instances that uniformly cover the $z_s, z_c$ space by taking a Cartesian product of 30 $z_s$ evenly spaced in $[\u22123\\\\mu_s, +3\\\\mu_s]$ and 30 $z_c$ evenly spaced in $[\u22123\\\\mu_c, +3\\\\mu_c]$. In all simulations, we set $d=100, \\\\eta_c=\\\\eta_s=0, \\\\rho_c=0.9, \\\\sigma_{sc}=0.6$. We manipulate shortcut-feature predictivity with the constraint that it is lower than core-feature predictivity but still predictive, i.e., $0.5 < \\\\rho_s < \\\\rho_c = 0.9$. Because only the relative amplification of features matters, we vary the ratio $\\\\alpha_s/\\\\alpha_c$, with the shortcut feature more available, i.e., $\\\\alpha_s/\\\\alpha_c \\\\geq 1$. We report the means across 10 runs (Appendix C.3).\\n\\nModels prefer the more available feature, even when it is less predictive. We first test whether models prefer $z_s$ when it is more available than $z_c$, including when it is less predictive, while holding model architecture fixed (see Appendix C.3). Figure 2 shows that when predictivity of the two features is matched ($\\\\rho_c=\\\\rho_s=0.9$), models prefer the more-available feature. And given sufficiently high availability, models can prefer the less-predictive but more-available shortcut feature to the more-predictive but less-available core feature. Availability can override predictivity.\\n\\nModel depth increases shortcut bias. In the previous experiment, we used a fixed model architecture. Here, we investigate how model depth and width influence shortcut bias when trained with $\\\\alpha_s/\\\\alpha_c = 64$ and $\\\\rho_s = 0.85$, previously shown to induce a bias (see Figure 2, gray square). As shown in Figure 3A, we find that bias increases as a function of model depth when dataset is held fixed.\\n\\nModel nonlinearity increases shortcut bias. To understand the role of the hidden-layer activation function, we compare models with linear, ReLU, and Tanh activations while holding weight initialization and data fixed. As indicated in Figure 3B, nonlinear activation functions induce a larger shortcut bias than their linear counterpart.\"}"}
{"id": "Tj3xLVuE9f", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nFigure 3: A: Model depth increases shortcut bias. The color of each cell indicates the mean bias of an MLP with ReLU hidden activation functions, for various model widths and depths, trained on data with a shortcut feature that is more available ($\\\\alpha_s/\\\\alpha_c = 64$) but less predictive ($\\\\rho_s = 0.85$) than the core feature. Model nonlinearity increases shortcut bias. B: Shortcut bias for three hidden activation functions for a deep MLP with width 128 and depth 2, trained on datasets where predictivity is matched ($\\\\rho_s = \\\\rho_c = 0.9$), but shortcut availability is higher ($\\\\alpha_s/\\\\alpha_c = 32$). A shortcut bias is more pronounced when the model contains a nonlinear activation function. C: Shortcut bias for MLPs with a single hidden layer and a hidden activation function that is either linear (left) or ReLU (right), for various shortcut feature availabilities ($\\\\alpha_s/\\\\alpha_c$) and predictivities ($\\\\rho_s$). See B.1 for Tanh.\\n\\nFigure 4: ResNet-18 prefers a shortcut feature when availability is instantiated as the pixel footprint of an object (feature), even when that feature is less predictive. A: Sample images. B: Shortcut bias increases as a function of relative availability of the shortcut feature when features are equally predictive ($\\\\rho_s = \\\\rho_c = 0.9$), consistent with Wolff & Wolff (2022). C: Even when the shortcut feature is less predictive, models have a shortcut bias due to availability, when $\\\\alpha_s/\\\\alpha_c = 4$.\\n\\nFeature nesting increases shortcut bias. The synthetic experiments reported above all manipulate availability with the amplitude ratio $\\\\alpha_s/\\\\alpha_c$. We also conducted experiments manipulating a second factor we expected would affect availability (Hermann & Lampinen, 2020; Shah et al., 2020), the relative nesting of representations, i.e., $\\\\eta_c - \\\\eta_s \\\\geq 1$. We report these experiments in Appendix C.3.\\n\\nWhat if we instantiate the same latent feature space studied in the previous section in images? We form shortcut features that are analogous to texture or image background\u2014features previously noted to preferentially drive the behavior of vision models (e.g. Geirhos et al., 2018a; Baker et al., 2018; Hermann et al., 2020; Beery et al., 2018; Sagawa et al., 2020a; Xiao et al., 2020). Building on the work of Wolff & Wolff (2022), we hypothesize that these features are more available because they have a large footprint in an image, and hence, by our notions of availability, a large $\\\\alpha_s$.\\n\\nMethods. We instantiate a latent vector $z$ from our data-generation procedure as an image. Each feature becomes an object ($z_s$ a circle, $z_c$ a square) whose color is determined by the respective feature value. Following Wolff & Wolff (2022), we manipulate the availability of each feature in terms of its size, or pixel footprint. We randomly position the circle and square entirely within the image, avoiding overlap, yielding a $224 \\\\times 224$ pixel image (Figure 4A, Appendix C.4).\\n\\nResults. Figure 4B presents the shortcut bias in ResNet-18 as a function of shortcut-feature availability (footprint) when the two features are equally predictive ($\\\\rho_s = \\\\rho_c = 0.9$). In Figure 4C, the availability...\"}"}
{"id": "Tj3xLVuE9f", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A new conference paper titled \\\"On the Foundations of Shortcut Learning\\\" by Katherine L. Hermann, Hossein Mobahi, Thomas Fel, and Michael C. Mozer has been published at ICLR 2024. The paper explores the role of features in deep-learning models, focusing on the interplay between predictivity and availability. The authors construct a minimal, explicit generative framework for synthesizing classification datasets with two latent features that vary in predictivity and factors hypothesized to relate to availability. They quantify a model's shortcut bias\u2014its over-reliance on the shortcut (more available, less predictive) feature at the expense of the core (less available, more predictive) feature. Linear models are relatively unbiased, but introducing a single hidden layer with ReLU or Tanh units yields a bias. Empirical findings are consistent with a theoretical account based on Neural Tangent Kernels. The study also examines how models used in practice trade off predictivity and availability in naturalistic datasets, discovering availability manipulations which increase models' degree of shortcut bias. These findings suggest that the propensity to learn shortcut features is a fundamental characteristic of deep nonlinear architectures warranting systematic study given its role in shaping how models solve tasks.\"}"}
{"id": "Tj3xLVuE9f", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nResults: Vision models, including ImageNet-pretrained ones, are sensitive to Background availability manipulations. Figure B.9 displays the results of the six manipulations which we hypothesized affect Background availability. The top row shows accuracy for ResNet18 models trained from scratch, while the bottom row shows the results for IN-ResNet50. Our findings reveal several key observations:\\n\\nFirst, as the size of the bird increases (Col.1), as well as when the background becomes noisier (Col. 4, 6), filtered of its high frequencies (Col. 5), or simply masked (Col. 2), the model tends to rely more heavily on the bird itself for classification. Conversely, when the background is clear and informative (available), the model utilizes the background features to predict the class. Second, the spuriousness of the background, as quantified by $\\\\rho$, also has a discernible effect on the model's behavior. However, it only partially accounts for the observed variations, demonstrating that availability factors are at play. Last, these results indicate that pretraining has a beneficial impact and can modulate feature availability. This effect may be attributed to the learned invariances during the pretraining process, enabling the model to better adapt to different availability conditions. Our spatial frequency experiments complement existing and concurrent work studying what models learn as a function of spatial frequency manipulations at training (Jo & Bengio, 2017; Yin et al., 2019; Tsuzuku & Sato, 2019; Hermann et al., 2020; Subramanian et al., 2023) or test time (Geirhos et al., 2018b).\\n\\nResults: Explainability analyses corroborate Background availability study. The experiments presented in Figures 6C and B.9 showed that the degree of Background availability influences model classifications of incongruent probes. Here, we use attribution methods to verify that when a model predominantly relies on the image background, or on the foreground bird, according to the experimental results, its focal point reflect this. Figure B.10 shows the attributions maps of two ResNet18 models trained from scratch, one using a bird size of 40%, and the other using a bird size of 80%. For the same set of probe images, the model trained on larger birds exhibits a strategy shift in classification, seemingly having his focal point on the birds, whereas the model trained on smaller birds mainly relies on the background. The methods employed encompass a combination of black-box techniques (Rise (Petsiuk et al., 2018) and Sobol indices (Fel et al., 2021)) and gradient-based approaches (Saliency (Zeiler & Fergus, 2014) and Smoothgrad (Smilkov et al., 2017)).\\n\\nD Q\\n\\nUADRATIC\\n\\nAPPROXIMATION OF\\n\\n\\\\[h(u) = u - \\\\frac{1}{\\\\pi} \\\\arccos(u)\\\\]\\n\\nSince $u$ is the dot product of two normalized vectors, its range is limited to $-1 \\\\leq u \\\\leq 1$. Observe that the derivative of $h$ has the form,\\n\\n\\\\[h'(u) = 1 - \\\\frac{1}{\\\\pi} \\\\arccos(u)\\\\]\\n\\n(8)\\n\\nThe only place within $u \\\\in [-1, 1]$ that $h'$ vanishes is at $u = -1$, suggesting the critical point of our quadratic must lie at this point. Furthermore, observe that $h(-1) = 0$. Forcing these two constraints onto our quadratic, leaves us with the form,\\n\\n\\\\[\\\\hat{h}(u) = a(1 + u)^2\\\\]\\n\\n(9)\\n\\nwhere $a$ is a parameter to be determined. Our aim is to find $a$ so that the function $\\\\hat{h}$ and $h$ looks similar over the entire domain $u \\\\in [-1, 1]$. We choose $\\\\ell_2$ error between the two functions defined as,\\n\\n\\\\[e_a = \\\\int_{-1}^{1} (h(u) - \\\\hat{h}(u))^2 du\\\\]\\n\\n(10)\"}"}
{"id": "Tj3xLVuE9f", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure D.1: The plots for $h$ (blue) and $\\\\hat{h}$ (orange) within $u \\\\in [-1, 1]$.\\n\\nReplacing the definitions for $h$ and $\\\\hat{h}$ into $(h(u) - \\\\hat{h}(u))^2$, one can obtain\\n$$ e_a = \\\\frac{1}{3} - \\\\frac{163}{48}a + \\\\frac{32}{5}a^2 + \\\\frac{32}{27}\\\\pi^2. $$\\n\\n(18)\\n\\nSince $e_a$ is a convex quadratic in $a$, its minimizer can be determined by zero-crossing the derivative of $e_a$ w.r.t. $a$. This yields,\\n$$ a^* \\\\equiv \\\\arg \\\\min_a e_a = \\\\frac{815}{3072}. $$\\n\\n(19)\\n\\nThis leads to the claimed quadratic form,\\n$$ \\\\hat{h}(u) \\\\equiv \\\\frac{815}{3072}(1 + u)^2. $$\\n\\n(20)\\n\\nFigure D.1 shows a plot of the original $h$ and the quadratic approximation $\\\\hat{h}$ to visually get a sense of the approximation quality.\\n\\nProof of ReLU Kernel Theorem\\n\\nRecall that the kernel function $k$ for ReLU is expressed using $h$ as in (3). Replacing $h$ with $\\\\hat{h}$ thus gets us an alternate kernel function which approximates that of ReLU's.\\n\\n$$ k(x, z) \\\\equiv \\\\|x\\\\|\\\\|z\\\\|\\\\hat{h}(\\\\langle \\\\|x\\\\|, \\\\|z\\\\| \\\\rangle) = \\\\|x\\\\|\\\\|z\\\\|a^*(1 + \\\\langle \\\\|x\\\\|, \\\\|z\\\\| \\\\rangle)^2. $$\\n\\n(21)\\n\\nWe first claim that the indefinite integral of $(h(u) - \\\\hat{h}(u))^2$ has the form,\\n$$ E_a(u) \\\\equiv \\\\int \\\\left( \\\\frac{1}{\\\\pi} (u(\\\\pi - \\\\arccos(u)) + \\\\sqrt{1 - u^2}) - a(1 + u)^2 \\\\right) du $$\\n\\n$$ = \\\\frac{1}{2160}\\\\pi^2 \\\\left( \\\\frac{432}{\\\\pi^2} a^2 u^5 + \\\\frac{80}{\\\\pi^2} \\\\left( \\\\frac{9}{\\\\pi^2}(6a^2 - 4a + 1) - \\\\frac{17}{\\\\pi^2} \\\\right) u^3 \\\\right) $$\\n\\n$$ + \\\\frac{240}{\\\\pi^2} \\\\left( \\\\frac{9}{\\\\pi^2}(a^2 + 11)u + 1080 \\\\pi^2 a(2a - 1)u^4 + 2160 \\\\pi^2 a(2a - 1)u^2 \\\\right) $$\\n\\n$$ - \\\\frac{15}{\\\\pi} \\\\sqrt{1 - u^2} \\\\left( \\\\frac{90}{\\\\pi}(u^3 + 256u^2 + 207u - 64) - \\\\frac{128}{\\\\pi} u^2 + 32 \\\\right) $$\\n\\n$$ + \\\\frac{120}{\\\\pi a} \\\\left( \\\\frac{3}{\\\\pi}(3u^2 + 8u + 6)u^2 - \\\\frac{12}{\\\\pi} u^3 + 4 \\\\left( 1 - 4u^2 \\\\right) \\\\sqrt{1 - u^2} \\\\cos - \\\\frac{1}{\\\\pi} (u) \\\\right) $$\\n\\n$$ - \\\\frac{1215}{\\\\pi a} \\\\sin - \\\\frac{720}{\\\\pi} u^3 \\\\cos - \\\\frac{1}{\\\\pi} (u)^2. $$\\n\\n(11-16)\\n\\nThis can be be easily shown by taking the derivative of both sides and arriving at equality. The definite integral can now be computed by evaluation the indefinite integral at the boundary points.\\n\\n$$ E_a(1) - E_a(-1) = \\\\frac{1}{3} - \\\\frac{163}{48}a + \\\\frac{32}{5}a^2 + \\\\frac{32}{27}\\\\pi^2. $$\\n\\n(17)\"}"}
{"id": "Tj3xLVuE9f", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We first focus on the above integral. An eigenfunction \\\\( \\\\phi \\\\) is where\\n\\nThus the eigenfunction has the form,\\n\\nfor both \\\\( a \\\\) constant in \\\\( x \\\\) or equivalently,\\n\\nto\\n\\nare the result of the definite integral for each term. Observe that each\\n\\n\\\\[ \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int \\\\int"}
{"id": "Tj3xLVuE9f", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dividing both sides by $a \\\\star \\\\|x\\\\|/\\\\lambda y$ yields,\\n\\n$$a \\\\star \\\\|x\\\\| \\\\int_{\\\\mathbb{R}^2} \\\\|z\\\\| \\\\left(1 + \\\\langle x, x \\\\rangle \\\\right)^2 \\\\varphi(z) p(z) dz = \\\\lambda \\\\varphi(x) \\\\tag{38}$$\\n\\n$$\\\\Rightarrow a \\\\star \\\\int_{\\\\mathbb{R}^2} \\\\left(\\\\|z\\\\| + \\\\sqrt{2}z_1 \\\\sqrt{2}z_2 \\\\sqrt{2}x_1 \\\\sqrt{2}x_2 \\\\left(\\\\|x\\\\| \\\\right)^2 \\\\right) \\\\varphi(z) p(z) dz = \\\\lambda \\\\varphi(x). \\\\tag{39}$$\\n\\n$$\\\\times \\\\|z\\\\| \\\\left(a_0 + a_1 \\\\sqrt{2}z_1 \\\\sqrt{2}z_2 \\\\sqrt{2}x_1 \\\\sqrt{2}x_2 \\\\left(\\\\|x\\\\| \\\\right)^2 \\\\right). \\\\tag{40}$$\\n\\nFor the above identity to hold at any point $x$ we need to equate the coefficients for each term involving $x$. That is, we need to have,\\n\\n$$a \\\\star \\\\left\\\\{ \\\\int_{\\\\mathbb{R}^2} \\\\left[ \\\\|z\\\\| \\\\sqrt{2}z_1 \\\\sqrt{2}z_2 \\\\sqrt{2}x_1 \\\\sqrt{2}x_2 \\\\left(\\\\|x\\\\| \\\\right)^2 \\\\right] \\\\varphi(z) p(z) dz \\\\right\\\\} = \\\\lambda \\\\left( a_0 + a_1 \\\\sqrt{2}z_1 \\\\sqrt{2}z_2 \\\\sqrt{2}x_1 \\\\sqrt{2}x_2 \\\\left(\\\\|x\\\\| \\\\right)^2 \\\\right). \\\\tag{41}$$\\n\\nor in a more compact form,\\n\\n$$a \\\\star \\\\left( \\\\int_{\\\\mathbb{R}^2} (vz)^Tz p(z) dz \\\\right) a = \\\\lambda a \\\\star a \\\\tag{42}.$$\\n\\nwhere,\\n\\n$$vz \\\\equiv \\\\begin{bmatrix} \\\\|z\\\\| \\\\sqrt{2}z_1 \\\\sqrt{2}z_2 \\\\sqrt{2}x_1 \\\\sqrt{2}x_2 \\\\left(\\\\|x\\\\| \\\\right)^2 \\\\end{bmatrix}^T. \\\\tag{43}$$\\n\\nWe now focus on the form of $p(z)$. Recall that we assume $p(z)$ is a mixture of $I$ Gaussian sources.\\n\\nDenote the selection probability of each Gaussian component by $\\\\pi_i$ and the corresponding component-conditional normal density function by $g(z; \\\\mu_i, C_i)$. Thus,\\n\\n$$p(z) = \\\\sum_i \\\\pi_i g(z; \\\\mu_i, C_i). \\\\tag{44}$$\\n\\nSince we assume that the covariance matrix of each $g_i$ is small element-wise, we can resort to a Laplace-type approximation for the integrals as below.\\n\\n$$\\\\int_{\\\\mathbb{R}^2} f(z) p(z) dz \\\\approx \\\\sum_i \\\\pi_i f(\\\\mu_i). \\\\tag{45}$$\\n\\nUnder this approximation, the linear system can be expressed as,\\n\\n$$\\\\left(\\\\sum_i \\\\pi_i v_i v_i^T\\\\right) a = \\\\lambda a \\\\star a \\\\tag{46}.$$\\n\\nwhere,\\n\\n$$v_i \\\\equiv \\\\begin{bmatrix} \\\\|\\\\mu_i\\\\| \\\\sqrt{2}\\\\mu_i_1 \\\\sqrt{2}\\\\mu_i_2 \\\\sqrt{2}\\\\mu_i_1 \\\\mu_i_2 \\\\left(\\\\|\\\\mu_i\\\\| \\\\right)^2 \\\\end{bmatrix}^T. \\\\tag{47}$$\\n\\nThus, any solution $a$ must be an eigenvector of $C$. In the sequel we discuss how to obtain the eigenvectors of $C$. In particular, in our 2-d problem with 2 Gaussian mixtures, where $\\\\pi_1 = \\\\pi_2 = 1/2$ and $\\\\mu_1 = A \\\\mu$ and $\\\\mu_2 = -A \\\\mu$, the linear equation can be expressed as below.\\n\\n$$\\\\left(\\\\frac{1}{2}v + v^T \\\\frac{1}{2}v - v^T \\\\right) a = \\\\lambda a \\\\star a \\\\tag{48}.$$\"}"}
{"id": "Tj3xLVuE9f", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ v + \\\\equiv \\\\| A_{\\\\mu} \\\\| \\\\sqrt{2} a_{\\\\mu 1} \\\\| A_{\\\\mu} \\\\| \\\\quad (51) \\\\]\\n\\n\\\\[ v - \\\\equiv \\\\| A_{\\\\mu} \\\\| - \\\\sqrt{2} a_{\\\\mu 1} - \\\\sqrt{2} a_{\\\\mu 2} \\\\left( a_{\\\\mu 1} \\\\right)^2 \\\\| A_{\\\\mu} \\\\| \\\\quad (52) \\\\]\\n\\nWe now seek the eigenvectors of \\\\( C \\\\) in this specific setting. A key observation is that \\\\( \\\\| v + \\\\| = \\\\| v - \\\\| \\\\) because the two are different only in the sign of their components. We now claim that, for any two vectors \\\\( v + \\\\) and \\\\( v - \\\\) such that \\\\( \\\\| v + \\\\| = \\\\| v - \\\\| \\\\), the eigenvectors of the matrix\\n\\n\\\\[ C = v + v^T + v - v^T \\\\]\\n\\nhave the form,\\n\\n\\\\[ \\\\psi_1 = c_1 (v + v - ) \\\\quad \\\\psi_2 = c_2 (v + - v - ) \\\\quad (53) \\\\]\\n\\nwhere \\\\( c_1 \\\\) and \\\\( c_2 \\\\) are normalization constants. We prove that \\\\( \\\\psi_1 \\\\) is an eigenvector of \\\\( C \\\\); similar argument applies to \\\\( \\\\psi_2 \\\\).\\n\\n\\\\[ C \\\\psi_1 = (v + v^T + v - v^T) c_1 (v + v - ) \\\\quad (54) \\\\]\\n\\n\\\\[ = c_1 (\\\\| v + \\\\|^2 + v - \\\\langle v + , v - \\\\rangle + v - \\\\langle v + , v - \\\\rangle + v - \\\\| v - \\\\|^2) \\\\quad (55) \\\\]\\n\\n\\\\[ = c_1 (\\\\| v + \\\\|^2 + \\\\langle v + , v - \\\\rangle) v + + c_1 (\\\\| v - \\\\|^2 + \\\\langle v + , v - \\\\rangle) v - \\\\quad (56) \\\\]\\n\\n\\\\[ = c_1 (\\\\| v + \\\\|^2 + \\\\langle v + , v - \\\\rangle) v + + c_1 (\\\\| v + \\\\|^2 + \\\\langle v + , v - \\\\rangle) v - \\\\quad (57) \\\\]\\n\\n\\\\[ = c_1 (\\\\| v + \\\\|^2 + \\\\langle v + , v - \\\\rangle) v + + c_1 (\\\\| v + \\\\|^2 + \\\\langle v + , v - \\\\rangle) v - \\\\quad (58) \\\\]\\n\\n\\\\[ = (\\\\| v + \\\\|^2 + \\\\langle v + , v - \\\\rangle) v + c_1 (\\\\| v - \\\\|^2 + \\\\langle v + , v - \\\\rangle) v - \\\\quad (59) \\\\]\\n\\n\\\\[ = (\\\\| v + \\\\|^2 + \\\\langle v + , v - \\\\rangle) v + c_1 (\\\\| v - \\\\|^2 + \\\\langle v + , v - \\\\rangle) v - \\\\quad (60) \\\\]\\n\\nThe above shows that \\\\( \\\\psi_1 \\\\) satisfies \\\\( C \\\\psi_1 \\\\propto \\\\psi_1 \\\\) and thus \\\\( \\\\psi_1 \\\\) must be an eigenvector of \\\\( C \\\\). Plugging the definition for \\\\( v + \\\\) and \\\\( v - \\\\) gives us the eigenvectors of our \\\\( C \\\\) matrix, and thus obtain the solution \\\\( a \\\\) in the equation (48). Since we are not constraining the length of the eigenvectors here, we can absorb all the constants in equation (48) into one and express the solution pair \\\\( a \\\\) as below.\\n\\n\\\\[ a \\\\propto (v + + v - ) = \\\\left[ \\\\| A_{\\\\mu} \\\\| \\\\quad (a_{\\\\mu 1} \\\\| A_{\\\\mu} \\\\| \\\\quad (a_{\\\\mu 2} \\\\| A_{\\\\mu} \\\\| \\\\quad \\\\sqrt{2} a_{\\\\mu 1} a_{\\\\mu 2} \\\\| A_{\\\\mu} \\\\| \\\\right]^T \\\\quad (61) \\\\]\\n\\n\\\\[ a \\\\propto (v + - v - ) = \\\\left[ 0 \\\\quad \\\\sqrt{2} a_{\\\\mu 1} \\\\quad \\\\sqrt{2} a_{\\\\mu 2} \\\\quad 0 \\\\right]^T \\\\quad (62) \\\\]\\n\\nRecall from (32) that,\\n\\n\\\\[ \\\\phi(x) = a_\\\\star \\\\lambda \\\\| x \\\\| (a_0 + a_1 \\\\sqrt{2} x_1 \\\\| x \\\\| + a_2 \\\\sqrt{2} x_2 \\\\| x \\\\| + a_3 x_2 x_1 \\\\| x \\\\|^2 + a_4 x_2^2 \\\\| x \\\\|^2 + a_5 \\\\sqrt{2} x_1 x_2 \\\\| x \\\\|^2) \\\\quad (63) \\\\]\\n\\nPlugging the pair of solutions for \\\\( a \\\\) into the above yields the two eigenfunctions,\\n\\n\\\\[ \\\\phi_1(x) \\\\propto \\\\| x \\\\| \\\\left( \\\\| A_{\\\\mu} \\\\| + (a_{\\\\mu 1} \\\\| A_{\\\\mu} \\\\| x_1 \\\\| x \\\\|^2 + (a_{\\\\mu 2} \\\\| A_{\\\\mu} \\\\| x_2 \\\\| x \\\\|^2 + \\\\sqrt{2} a_{\\\\mu 1} a_{\\\\mu 2} \\\\| A_{\\\\mu} \\\\| \\\\sqrt{2} x_1 x_2 \\\\| x \\\\|^2 \\\\right) \\\\quad (64) \\\\]\\n\\n\\\\[ \\\\phi_2(x) \\\\propto \\\\| x \\\\| \\\\left( \\\\sqrt{2} a_{\\\\mu 1} \\\\sqrt{2} x_1 \\\\| x \\\\| + \\\\sqrt{2} a_{\\\\mu 2} \\\\sqrt{2} x_2 \\\\| x \\\\| \\\\right) \\\\quad (65) \\\\]\\n\\nor more simply,\\n\\n\\\\[ \\\\phi_1(x) \\\\propto \\\\| x \\\\| \\\\| A_{\\\\mu} \\\\| \\\\left( 1 + \\\\langle x , A_{\\\\mu} \\\\rangle^2 \\\\right) \\\\quad (66) \\\\]\\n\\n\\\\[ \\\\phi_2(x) \\\\propto \\\\langle x , A_{\\\\mu} \\\\rangle \\\\quad (67) \\\\]\\n\\nWe convert the \\\\( \\\\propto \\\\) to equality by applying proper scaling factors \\\\( d_1 \\\\) and \\\\( d_2 \\\\) as follows.\\n\\n\\\\[ \\\\phi_1(x) = d_1 \\\\| x \\\\| \\\\| A_{\\\\mu} \\\\| \\\\left( 1 + \\\\langle x , A_{\\\\mu} \\\\rangle^2 \\\\right) \\\\quad (68) \\\\]\\n\\n\\\\[ \\\\phi_2(x) = d_2 \\\\langle x , A_{\\\\mu} \\\\rangle \\\\quad (69) \\\\]\"}"}
{"id": "Tj3xLVuE9f", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In order to find the factors $d_1$ and $d_2$, we require $\\\\phi_1$ and $\\\\phi_2$ to have unit norm in the sense of (6).\\n\\nStarting with $\\\\phi_1$ we proceed as,\\n\\n$$\\n\\\\|\\\\phi_1\\\\|^2 = d_2^2 \\\\|A\\\\|^2 \\\\int_{\\\\mathbb{R}^2} \\\\|x\\\\|^2 \\\\left(1 + \\\\langle x, A\\\\rangle^2 \\\\right)^2 p(x) dx \\\\tag{70}\\n$$\\n\\n$$\\n= d_2^2 \\\\|A\\\\|^2 \\\\left(1 + \\\\frac{1}{2} \\\\langle x, A\\\\rangle^2 \\\\right)^2 x = A + \\\\frac{1}{2} \\\\langle x, A\\\\rangle^2 \\\\tag{71}\\n$$\\n\\n$$\\n= d_2^2 \\\\|A\\\\|^4 \\\\left(1 + 1 + \\\\frac{1}{2} \\\\right)^2 x = A + \\\\frac{1}{2} \\\\langle x, A\\\\rangle^2 \\\\tag{72}\\n$$\\n\\nSetting the above to 1 and solving in $d_1$ yields,\\n\\n$$\\nd_1 = \\\\frac{1}{2} \\\\|A\\\\|^2, \\\\tag{75}\\n$$\\n\\nand consequently,\\n\\n$$\\n\\\\phi_1(x) = \\\\|x\\\\| \\\\left(1 + \\\\langle x, A\\\\rangle^2 \\\\right)^2 \\\\|A\\\\|^2 \\\\tag{76}\\n$$\\n\\nSimilarly for $\\\\phi_2$, we proceed as,\\n\\n$$\\n\\\\|\\\\phi_2\\\\|^2 = d_2^2 \\\\int_{\\\\mathbb{R}^2} \\\\langle x, A\\\\rangle^2 p(x) dx \\\\tag{77}\\n$$\\n\\n$$\\n= d_2^2 \\\\left(1 + \\\\frac{1}{2} \\\\langle A, A\\\\rangle^2 \\\\right)^2 x = A + \\\\frac{1}{2} \\\\langle A, A\\\\rangle^2 \\\\tag{78}\\n$$\\n\\n$$\\n= d_2^2 \\\\|A\\\\|^4 \\\\tag{79}\\n$$\\n\\nSetting the above to 1 and solving in $d_2$ yields,\\n\\n$$\\nd_2 = \\\\frac{1}{\\\\|A\\\\|^2}, \\\\tag{81}\\n$$\\n\\nand consequently,\\n\\n$$\\n\\\\phi_2(x) = \\\\langle x, A\\\\rangle \\\\langle A, A\\\\rangle \\\\|A\\\\|^2 \\\\tag{82}\\n$$\"}"}
{"id": "Tj3xLVuE9f", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To find the eigenvalues, we simply put the eigenfunctions in their defining equation (1). Starting with $\\\\phi_1$ we proceed as below.\\n\\n\\\\[\\n\\\\int_{\\\\mathbb{R}^2} k(x,z) \\\\phi_1(z) p(z) \\\\, dz \\\\quad (83)\\n\\\\]\\n\\n\\\\[\\n= \\\\int_{\\\\mathbb{R}^2} a^* \\\\|x\\\\| \\\\|z\\\\| (1 + \\\\langle x \\\\|x\\\\|, z \\\\|z\\\\| \\\\rangle)^2 \\\\phi_1(z) p(z) \\\\, dz \\\\quad (84)\\n\\\\]\\n\\n\\\\[\\n= \\\\int_{\\\\mathbb{R}^2} a^* \\\\|x\\\\| \\\\|z\\\\| (1 + \\\\langle x \\\\|x\\\\|, z \\\\|z\\\\| \\\\rangle)^2 \\\\|z\\\\| \\\\|A\\\\mu\\\\| \\\\quad (85)\\n\\\\]\\n\\n\\\\[\\n+ \\\\frac{1}{2} \\\\left( a^* \\\\|x\\\\| \\\\|z\\\\| (1 + \\\\langle x \\\\|x\\\\|, z \\\\|z\\\\| \\\\rangle)^2 \\\\right) \\\\quad (86)\\n\\\\]\\n\\n\\\\[\\n= \\\\int_{\\\\mathbb{R}^2} a^* \\\\|x\\\\| \\\\|z\\\\| (1 + \\\\langle x \\\\|x\\\\|, z \\\\|z\\\\| \\\\rangle)^2 \\\\|z\\\\| \\\\|A\\\\mu\\\\| \\\\quad (88)\\n\\\\]\\n\\n\\\\[\\n= \\\\int_{\\\\mathbb{R}^2} a^* \\\\|x\\\\| \\\\|z\\\\| (1 + \\\\langle x \\\\|x\\\\|, -A\\\\mu \\\\|A\\\\mu\\\\| \\\\rangle)^2 \\\\|z\\\\| \\\\|A\\\\mu\\\\| \\\\quad (89)\\n\\\\]\\n\\n\\\\[\\n= \\\\frac{1}{2} a^* \\\\|x\\\\| \\\\|A\\\\mu\\\\| \\\\quad (90)\\n\\\\]\\n\\n\\\\[\\n= 2 a^* \\\\|A\\\\mu\\\\|^2 \\\\|x\\\\|^2 \\\\|A\\\\mu\\\\|^2 \\\\quad (92)\\n\\\\]\\n\\nTherefore, $\\\\lambda_1 = 2 a^* \\\\|A\\\\mu\\\\|^2$.\\n\\n(94)\"}"}
{"id": "Tj3xLVuE9f", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Similarly for $\\\\phi_2$ we have,\\n\\\\[\\n\\\\int_{\\\\mathbb{R}^2} k(x, z) \\\\phi_2(z) p(z) \\\\, dz = a^* \\\\|x\\\\| \\\\|z\\\\| \\\\left( 1 + \\\\langle x \\\\|x\\\\|, z \\\\|z\\\\| \\\\rangle \\\\right)^2 \\\\phi_2(z) p(z) \\\\, dz (95)\\n\\\\]\\n\\\\[\\n= a^* \\\\|x\\\\| \\\\|z\\\\| \\\\left( 1 + \\\\langle x \\\\|x\\\\|, z \\\\|z\\\\| \\\\rangle \\\\right)^2 \\\\langle z \\\\|A\\\\|, A \\\\|A\\\\| \\\\rangle p(z) \\\\, dz (96)\\n\\\\]\\n\\\\[\\n= \\\\frac{1}{2} \\\\left( a^* \\\\|x\\\\| \\\\|z\\\\| \\\\left( 1 + \\\\langle x \\\\|x\\\\|, z \\\\|z\\\\| \\\\rangle \\\\right)^2 \\\\langle z \\\\|A\\\\|, A \\\\|A\\\\| \\\\rangle \\\\right) + \\\\frac{1}{2} \\\\left( a^* \\\\|x\\\\| \\\\|z\\\\| \\\\left( 1 + \\\\langle x \\\\|x\\\\|, z \\\\|z\\\\| \\\\rangle \\\\right)^2 \\\\langle z \\\\|A\\\\|, A \\\\|A\\\\| \\\\rangle \\\\right) \\\\right) \\\\right) \\\\right) p(z) \\\\, dz (98)\\n\\\\]\\n\\\\[\\n+ \\\\frac{1}{2} \\\\left( a^* \\\\|x\\\\| \\\\|z\\\\| \\\\left( 1 + \\\\langle x \\\\|x\\\\|, z \\\\|z\\\\| \\\\rangle \\\\right)^2 \\\\langle z \\\\|A\\\\|, A \\\\|A\\\\| \\\\rangle \\\\right) \\\\right) \\\\right) p(z) \\\\, dz (99)\\n\\\\]\\n\\\\[\\n= \\\\frac{1}{2} \\\\left( a^* \\\\|x\\\\| \\\\|z\\\\| \\\\left( 1 + \\\\langle x \\\\|x\\\\|, z \\\\|z\\\\| \\\\rangle \\\\right)^2 \\\\langle z \\\\|A\\\\|, A \\\\|A\\\\| \\\\rangle \\\\right) \\\\right) \\\\right) p(z) \\\\, dz (100)\\n\\\\]\\n\\\\[\\n+ \\\\frac{1}{2} \\\\left( a^* \\\\|x\\\\| \\\\|z\\\\| \\\\left( 1 + \\\\langle x \\\\|x\\\\|, z \\\\|z\\\\| \\\\rangle \\\\right)^2 \\\\langle z \\\\|A\\\\|, A \\\\|A\\\\| \\\\rangle \\\\right) \\\\right) \\\\right) p(z) \\\\, dz (101)\\n\\\\]\\n\\\\[\\n= \\\\frac{1}{2} \\\\left( a^* \\\\|x\\\\| \\\\|z\\\\| \\\\left( 1 + \\\\langle x \\\\|x\\\\|, z \\\\|z\\\\| \\\\rangle \\\\right)^2 \\\\langle z \\\\|A\\\\|, A \\\\|A\\\\| \\\\rangle \\\\right) \\\\right) \\\\right) p(z) \\\\, dz (102)\\n\\\\]\\n\\\\[\\n+ \\\\frac{1}{2} \\\\left( a^* \\\\|x\\\\| \\\\|z\\\\| \\\\left( 1 + \\\\langle x \\\\|x\\\\|, z \\\\|z\\\\| \\\\rangle \\\\right)^2 \\\\langle z \\\\|A\\\\|, A \\\\|A\\\\| \\\\rangle \\\\right) \\\\right) \\\\right) p(z) \\\\, dz (103)\\n\\\\]\\n\\\\[\\n= \\\\frac{1}{2} \\\\left( a^* \\\\|x\\\\| \\\\|z\\\\| \\\\left( 1 + \\\\langle x \\\\|x\\\\|, z \\\\|z\\\\| \\\\rangle \\\\right)^2 \\\\langle z \\\\|A\\\\|, A \\\\|A\\\\| \\\\rangle \\\\right) \\\\right) \\\\right) p(z) \\\\, dz (104)\\n\\\\]\\n\\\\[\\n= \\\\frac{1}{2} \\\\left( a^* \\\\|x\\\\| \\\\|z\\\\| \\\\left( 1 + \\\\langle x \\\\|x\\\\|, z \\\\|z\\\\| \\\\rangle \\\\right)^2 \\\\langle z \\\\|A\\\\|, A \\\\|A\\\\| \\\\rangle \\\\right) \\\\right) \\\\right) p(z) \\\\, dz (105)\\n\\\\]\\n\\\\[\\n= 2 a^* \\\\|x\\\\| \\\\|z\\\\| \\\\langle x \\\\|A\\\\|, A \\\\|A\\\\| \\\\rangle p(z) \\\\, dz (106)\\n\\\\]\\n\\\\[\\n= 2 a^* \\\\|x\\\\| \\\\|A\\\\| \\\\|A\\\\| \\\\phi_2(x) \\\\phi_2(z) p(z) \\\\, dz (107)\\n\\\\]\\n\\nTherefore,\\n\\\\[\\n\\\\lambda_2 = 2 a^* \\\\|A\\\\| \\\\langle x \\\\|A\\\\|, A \\\\|A\\\\| \\\\rangle (108)\\n\\\\]\\n\\n**Proof of Linear Kernel Theorem**\\n\\nAn eigenfunction $\\\\phi$ of the kernel operator associated with kernel $k(x, z) = \\\\langle x, z \\\\rangle$ must satisfy,\\n\\\\[\\n\\\\int_{\\\\mathbb{R}^n} \\\\langle x, z \\\\rangle \\\\phi(z) p(z) \\\\, dz = \\\\lambda \\\\phi(x) (109)\\n\\\\]\\n\\nFollowing our earlier setup, we proceed with a 2-dimensional input this becomes,\\n\\\\[\\n\\\\int_{\\\\mathbb{R}^2} \\\\langle x, z \\\\rangle \\\\phi(z) p(z) \\\\, dz (110)\\n\\\\]\\n\\\\[\\n= \\\\int_{\\\\mathbb{R}^2} (x_1 z_1 + x_2 z_2) \\\\phi(z) p(z) \\\\, dz (111)\\n\\\\]\\n\\\\[\\n= a_1 x_1 + a_2 x_2 (112)\\n\\\\]\\n\\nwhere $a_1$ and $a_2$ are the result of the definite integrals of form\\n\\\\[\\n\\\\int_{\\\\mathbb{R}^2} x_i f(z) \\\\, dz, \\\\quad \\\\text{for } i = 1, 2\\n\\\\]\\n\\nObserve that each $a_i$ is thus constant in $x$ and $z$. Plugging the above expression into (109) yields,\\n\\\\[\\na_1 x_1 + a_2 x_2 = \\\\lambda \\\\phi(x), \\\\quad \\\\text{(113)}\\n\\\\]\"}"}
{"id": "Tj3xLVuE9f", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Thus the eigenfunction has the form,\\n\\n$$\\\\phi(x) = 1^2 \\\\lambda (a_1 x_1 + a_2 x_2).$$ \\\\hspace{1cm} (114)\\n\\nIn order to figure out the values of $a_1$ and $a_2$ we plug the obtained eigenfunction form into (109) again, for both $\\\\phi(x)$ and $\\\\phi(z)$,\\n\\n$$\\\\int_{\\\\mathbb{R}^2} \\\\langle x, z \\\\rangle \\\\phi(z) p(z) dz = \\\\lambda \\\\phi(x).$$ \\\\hspace{1cm} (115)\\n\\n$$\\\\Rightarrow \\\\int_{\\\\mathbb{R}^2} (x_1 z_1 + x_2 z_2) 1^2 \\\\lambda (a_1 z_1 + a_2 z_2) p(z) dz = \\\\lambda 1^2 \\\\lambda (a_1 x_1 + a_2 x_2).$$ \\\\hspace{1cm} (116)\\n\\n$$\\\\Rightarrow \\\\sum_i \\\\pi_i \\\\mu_i^1 (a_1 \\\\mu_i^1 + a_2 \\\\mu_i^2) = \\\\lambda a_1 x_1 + a_2 x_2,$$ \\\\hspace{1cm} (117)\\n\\nwhere in the last line we use the assumption that the covariance matrix of each Gaussian source is small, and thus we can resort to a Laplace-like approximation of the integral. To have the above identity hold true at any $x$ we need to require that,\\n\\n$$\\\\sum_i \\\\pi_i \\\\mu_i^1 (a_1 \\\\mu_i^1 + a_2 \\\\mu_i^2) = \\\\lambda a_1 x_1 + a_2 x_2.$$ \\\\hspace{1cm} (119)\\n\\n$$\\\\sum_i \\\\pi_i \\\\mu_i^2 (a_1 \\\\mu_i^1 + a_2 \\\\mu_i^2) = \\\\lambda a_2 x_1 + a_2 x_2.$$ \\\\hspace{1cm} (120)\\n\\nor in matrix form,\\n\\n$$\\\\begin{pmatrix} \\\\sum_i \\\\pi_i \\\\mu_i^1 \\\\mu_i^1 & \\\\sum_i \\\\pi_i \\\\mu_i^1 \\\\mu_i^2 \\\\\\\\ \\\\sum_i \\\\pi_i \\\\mu_i^2 \\\\mu_i^1 & \\\\sum_i \\\\pi_i \\\\mu_i^2 \\\\mu_i^2 \\\\end{pmatrix} \\\\begin{pmatrix} a_1 \\\\\\\\ a_2 \\\\end{pmatrix} = \\\\lambda \\\\begin{pmatrix} a_1 \\\\\\\\ a_2 \\\\end{pmatrix}.$$ \\\\hspace{1cm} (121)\\n\\nThe equation can be compactly expressed as,\\n\\n$$Ca = \\\\lambda a.$$ \\\\hspace{1cm} (122)\\n\\nThus, any solution $a$ must be an eigenvector of $C$. In the sequel we discuss how to obtain the eigenvectors of $C$. Observe that each matrix $C_i$ is a rank-one symmetric matrix that can be written as $\\\\mu_i^1 \\\\mu_i^1^T$. In particular, in our 2-d problem with 2 Gaussian mixtures, where $\\\\pi_1 = \\\\pi_2 = \\\\frac{1}{2}$ and $\\\\mu^1 = A \\\\mu$ and $\\\\mu^2 = -A \\\\mu$, the above equation can be expressed as below.\\n\\n$$(\\\\frac{1}{2}(A \\\\mu)(A \\\\mu)^T + \\\\frac{1}{2}(-A \\\\mu)(-A \\\\mu)^T) a = \\\\lambda a.$$ \\\\hspace{1cm} (124)\\n\\n$$\\\\Rightarrow (A \\\\mu)(A \\\\mu)^T a = \\\\lambda a.$$ \\\\hspace{1cm} (125)\\n\\nThus, $a \\\\propto A \\\\mu$. \\\\hspace{1cm} (126)\\n\\nRecall from (114) that,\\n\\n$$\\\\phi(x) = 1^2 \\\\lambda \\\\langle a, x \\\\rangle.$$ \\\\hspace{1cm} (127)\\n\\nPlugging the solution for $a$ into the above yields the eigenfunction,\\n\\n$$\\\\phi(x) \\\\propto \\\\langle A \\\\mu, x \\\\rangle.$$ \\\\hspace{1cm} (128)\\n\\nWe convert the $\\\\propto$ to equality by applying proper scaling factors $c$ as follows.\\n\\n$$\\\\phi(x) = c \\\\langle A \\\\mu, x \\\\rangle.$$ \\\\hspace{1cm} (129)\"}"}
{"id": "Tj3xLVuE9f", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nIn order to find the scale factor $c$, we require $\\\\phi$ to have unit norm in the sense of (6).\\n\\n$$\\\\|\\\\phi\\\\|_2^2 = c^2 \\\\int_{\\\\mathbb{R}^2} (\\\\langle A\\\\mu, x \\\\rangle)^2 P(x) \\\\, dx \\\\quad (130)$$\\n\\n$$= c^2 \\\\int_{\\\\mathbb{R}^2} (\\\\langle A\\\\mu, x \\\\rangle)^2 x \\\\, dx = A\\\\mu + c^2 \\\\int_{\\\\mathbb{R}^2} (\\\\langle A\\\\mu, x \\\\rangle)^2 x \\\\, dx \\\\quad (131)$$\\n\\n$$= c^2 \\\\int_{\\\\mathbb{R}^2} (\\\\|A\\\\mu\\\\|_2^2) x \\\\, dx = A\\\\mu \\\\quad (132)$$\\n\\nSetting the above to 1 and solving in $c$ yields,\\n\\n$$c = 1 \\\\|A\\\\mu\\\\|_2^2 \\\\quad (134)$$\\n\\nand consequently,\\n\\n$$\\\\phi(x) = \\\\langle A\\\\mu, x \\\\rangle \\\\|A\\\\mu\\\\|_2 \\\\quad (135)$$\\n\\nFor finding the eigenvalue associated with $\\\\phi$ we plug this form into (109) for both $\\\\phi(x)$ and $\\\\phi(z)$ and then solve in $\\\\lambda$.\\n\\n$$\\\\int_{\\\\mathbb{R}^n} \\\\langle x, z \\\\rangle \\\\phi(z) P(z) \\\\, dz = \\\\lambda \\\\phi(x) \\\\quad (136)$$\\n\\n$$\\\\Rightarrow \\\\int_{\\\\mathbb{R}^n} \\\\langle x, z \\\\rangle \\\\langle A\\\\mu, z \\\\rangle \\\\|A\\\\mu\\\\|_2^2 P(z) \\\\, dz = \\\\lambda \\\\langle A\\\\mu, x \\\\rangle \\\\|A\\\\mu\\\\|_2^2 \\\\quad (137)$$\\n\\n$$\\\\Rightarrow \\\\frac{1}{2} (\\\\langle x, z \\\\rangle \\\\langle A\\\\mu, z \\\\rangle \\\\|A\\\\mu\\\\|_2^2) z = A\\\\mu + \\\\frac{1}{2} (\\\\langle x, z \\\\rangle \\\\langle A\\\\mu, z \\\\rangle \\\\|A\\\\mu\\\\|_2^2) z = -A\\\\mu = \\\\lambda \\\\langle A\\\\mu, x \\\\rangle \\\\|A\\\\mu\\\\|_2^2 \\\\quad (138)$$\\n\\n$$\\\\Rightarrow \\\\langle x, A\\\\mu \\\\rangle \\\\langle A\\\\mu, A\\\\mu \\\\rangle \\\\|A\\\\mu\\\\|_2^2 = \\\\lambda \\\\langle A\\\\mu, x \\\\rangle \\\\|A\\\\mu\\\\|_2^2 \\\\quad (139)$$\\n\\n$$\\\\Rightarrow \\\\|A\\\\mu\\\\|_2^2 = \\\\lambda \\\\quad (140)$$\\n\\nConsider the task of finding a function $f(x)$ that best approximates a given function $y(x)$ via the following regression setup.\\n\\n$$L = \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} (f(x) - y(x))^2 P(x) \\\\, dx \\\\quad (141)$$\\n\\nSuppose we are interested in expressing $f(x)$ using the bases induced by the kernel $k$, that is the eigenfunctions of the linear operator associated with $k$ whose eigenvaules are non-zero. That is,\\n\\n$$f(x) = \\\\sum_k a_k \\\\phi_k(x) \\\\quad (142)$$\\n\\nwhere $\\\\phi_k$ is an eigenfunction, and $k$ runs over eigenfunctions with non-zero eigenvalue. We now show that the function $f$ which minimizes the regression loss $L$ has the form,\\n\\n$$f(x) = \\\\sum_k a_k \\\\phi_k(x) \\\\quad (143)$$\\n\\n$$= \\\\sum_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(z) y(z) P(z) \\\\, dz \\\\phi_k(x) \\\\quad (144)$$\\n\\nWe start from,\\n\\n$$L = \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} (f(x) - y(x))^2 P(x) \\\\, dx \\\\quad (145)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\left( \\\\sum_k a_k \\\\phi_k(x) - y(x) \\\\right)^2 P(x) \\\\, dx \\\\quad (146)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx + \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (147)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx + \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (148)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (149)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (150)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (151)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (152)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (153)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (154)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (155)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (156)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (157)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (158)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (159)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (160)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (161)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (162)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (163)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (164)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (165)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (166)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (167)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (168)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (169)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (170)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (171)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (172)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (173)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (174)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (175)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (176)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (177)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (178)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (179)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) y(x) P(x) \\\\, dx \\\\quad (180)$$\\n\\n$$= \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^n} \\\\sum_k a_k^2 \\\\phi_k(x)^2 P(x) \\\\, dx - \\\\sum_k a_k \\\\int_{\\\\mathbb{R}"}
{"id": "Tj3xLVuE9f", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Thus, \\\\[\\n\\\\frac{\\\\partial}{\\\\partial a_j} \\\\mathcal{L} = \\\\int_{\\\\mathbb{R}^n} \\\\left( \\\\sum_k a_k \\\\phi_k(x) - y(x) \\\\right) (a_j \\\\phi_j(x)) p(x) \\\\, dx \\\\tag{147}\\n\\\\]\\n\\\\[\\n\\\\int_{\\\\mathbb{R}^n} \\\\left( \\\\sum_k a_k a_j \\\\phi_k(x) \\\\phi_j(x) - a_j \\\\phi_j(x) y(x) \\\\right) p(x) \\\\, dx \\\\tag{148}\\n\\\\]\\n\\\\[\\n(\\\\sum_k a_k a_j \\\\int_{\\\\mathbb{R}^n} \\\\phi_k(x) \\\\phi_j(x) p(x) \\\\, dx) - a_j \\\\int_{\\\\mathbb{R}^n} \\\\phi_j(x) y(x) p(x) \\\\, dx \\\\tag{149}\\n\\\\]\\n\\\\[\\n= a_j - a_j \\\\int_{\\\\mathbb{R}^n} \\\\phi_j(x) y(x) p(x) \\\\, dx \\\\]. \\\\tag{150}\\n\\\\]\\n\\nZero-crossing, \\\\[\\n\\\\int_{\\\\mathbb{R}^n} \\\\phi_j(x) y(x) p(x) \\\\, dx. \\\\tag{151}\\n\\\\]\\n\\nThus, \\\\[\\nf(x) = \\\\sum_k a_k \\\\phi_k(x) \\\\tag{152}\\n\\\\]\\n\\\\[\\n= \\\\sum_k \\\\int_{\\\\mathbb{R}^2} \\\\phi_k(z) y(z) p(z) \\\\, dz \\\\phi_k(x) \\\\tag{153}\\n\\\\]\\n\\nIn particular, if we replace \\\\( p(z) \\\\) by our pair of Gaussian densities with small covariance, and set \\\\( y(x) \\\\) to 1 and 0 depending on whether \\\\( x \\\\) is drawn from the positive or negative component of the mixture, we arrive at,\\n\\\\[\\nf(x) = \\\\sum_i \\\\left( \\\\frac{1}{2} \\\\int_{\\\\mathbb{R}^2} \\\\phi_i(z) (1) p^+(z) \\\\, dz + \\\\int_{\\\\mathbb{R}^2} \\\\phi_i(z) (0) p^-(z) \\\\, dz\\\\right) = 1/2 \\\\sum_i \\\\phi_i(x) \\\\phi_i(A\\\\mu), \\\\tag{154}\\n\\\\]\\n\\nwhere \\\\( p^+ \\\\) and \\\\( p^- \\\\) denote class-conditional normal distributions.\\n\\n### H.1 Linear Kernel\\n\\n**Theorem 5**\\nIn a linear network, \\\\(|\\\\zeta_1| - |\\\\zeta_2|\\\\) is always zero for any choice of \\\\(m \\\\geq 1\\\\) and regardless of the values of \\\\(a_1\\\\) and \\\\(a_2\\\\).\\n\\nRecall the definitions,\\n\\\\[\\nf(x) = \\\\frac{1}{2} \\\\sum_i \\\\phi_i(x) \\\\phi_i(A\\\\mu) \\\\tag{155}\\n\\\\]\\n\\\\[\\ng_B(x) = \\\\frac{1}{2} \\\\sum_i \\\\phi_i(x) \\\\phi_i(BA\\\\mu) \\\\tag{156}\\n\\\\]\\n\\\\[\\n\\\\gamma(b) \\\\equiv \\\\int_{\\\\mathbb{R}^2} f(x) \\\\sqrt{\\\\int_{\\\\mathbb{R}^2} f^2(t) p(t) \\\\, dt} g_B(x) \\\\sqrt{\\\\int_{\\\\mathbb{R}^2} g_B^2(t) p(t) \\\\, dt} p(x) \\\\, dx. \\\\tag{157}\\n\\\\]\\n\\nFor linear kernel, we had only one eigenfunction with the form,\\n\\\\[\\n\\\\phi_1(x) = \\\\langle A\\\\mu, x \\\\rangle \\\\|A\\\\mu\\\\|^2. \\\\tag{158}\\n\\\\]\\nReplacing that, in the above definitions yields,\\n\\\\[\\nf(x) = \\\\frac{1}{2} \\\\langle A\\\\mu, x \\\\rangle \\\\|A\\\\mu\\\\|^2 \\\\langle A\\\\mu, A\\\\mu \\\\rangle \\\\|A\\\\mu\\\\|^2 = \\\\frac{1}{2} \\\\langle A\\\\mu, x \\\\rangle \\\\|A\\\\mu\\\\|^2 \\\\tag{159}\\n\\\\]\\n\\\\[\\ng_B(x) = \\\\frac{1}{2} \\\\langle A\\\\mu, x \\\\rangle \\\\|A\\\\mu\\\\|^2 \\\\langle A\\\\mu, BA\\\\mu \\\\rangle \\\\|A\\\\mu\\\\|^2. \\\\tag{160}\\n\\\\]\\n\\n\"}"}
{"id": "Tj3xLVuE9f", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"It is easy to obtain,\\n\\\\[\\n\\\\int_{\\\\mathbb{R}^n} f^2(x)p(x) = 1\\n\\\\]\\nand,\\n\\\\[\\n\\\\int_{\\\\mathbb{R}^n} g^2(x)p(x) = 1\\n\\\\]\\nPlugging these results into the definition of \\\\( \\\\gamma \\\\) yields,\\n\\\\[\\n\\\\gamma(b) \\\\triangleq \\\\int_{\\\\mathbb{R}^2} f(x) \\\\sqrt{\\\\int_{\\\\mathbb{R}^2} f^2(t)p(t)dt} g(B)(x) \\\\sqrt{\\\\int_{\\\\mathbb{R}^2} g^2(t)p(t)dt} p(x)dx\\n\\\\]\\nRecall the definitions,\\n\\\\[\\nf(x) = \\\\frac{1}{2} \\\\sum_i \\\\phi_i(x) \\\\phi_i(A) \\\\\\\\\\ng_B(x) \\\\triangleq \\\\frac{1}{2} \\\\sum_i \\\\phi_i(x) \\\\phi_i(BA) \\\\\\\\\\n\\\\gamma(b) \\\\triangleq \\\\int_{\\\\mathbb{R}^2} f(x) \\\\sqrt{\\\\int_{\\\\mathbb{R}^2} f^2(t)p(t)dt} g(B)(x) \\\\sqrt{\\\\int_{\\\\mathbb{R}^2} g^2(t)p(t)dt} p(x)dx\\n\\\\]\"}"}
{"id": "Tj3xLVuE9f", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The ratio is fixed at $\\\\alpha_s/\\\\alpha_c = 4$, and the shortcut bias is assessed as a function of $\\\\rho_s$. ResNet18 is biased toward the more available shortcut feature even when it is less predictive than the core feature. Together, these results suggest that a simple characteristic of image contents\u2014the pixel footprint of an object\u2014can bias models' output behavior, and may therefore explain why models can fail to leverage typically-smaller foreground objects in favor of image backgrounds (Section 7).\\n\\n**Theoretical Account**\\n\\nIn our empirical investigations, we quantified the extent to which a trained model deviates from a statistically optimal classifier in its reliance on the more-available feature using a measure which considered the basis for probe-instance classifications. Here, we use an alternative approach of studying the sensitivity of a Neural Tangent Kernel (NTK) (Jacot et al., 2018) to the availability of a feature. The resulting form presents a crisp perspective of how predictivity and availability interact.\\n\\nIn particular, we prove that availability bias is absent in linear networks but present in ReLU networks. The proof for the theorems of this section is given in the Supplementary Materials.\\n\\nFor tractability of the analysis, we consider a few simplifying assumptions. We focus on 2-layer fully connected architectures for which the kernel of the ReLU networks admits a simple closed form. In addition, to be able to compute integrations that arise in the analysis, we resort to an asymptotic approximation which assumes covariance matrix is small. Specifically, we represent the covariance matrix as $\\\\begin{bmatrix} 1, \\\\sigma_{12}; \\\\sigma_{12}, 1 \\\\end{bmatrix}$, where the scale parameter $s > 0$ is considered to be small. Finally, in order to handle the analysis for a ReLU kernel, we will use a polynomial approximation.\\n\\n**Kernel Spectrum.** Consider a two-layer network with the first layer having a possibly non-linear activation function. When the width of this model gets large, learning of this model can be approximated by a kernel regression problem, with a given kernel function $k(x, z)$ that depends on the architecture and the activation function. Given a distribution over the input data $p(x)$, we define a (linear) kernel operator as one that acts on a function $f$ to produce another function $g$ as in $g(x) = \\\\int_{\\\\mathbb{R}^n} k(x, z)f(z)p(z)dz$. This allows us to define an eigenfunction $\\\\phi$ of the kernel operator as one that satisfies, $\\\\lambda \\\\phi(x) = \\\\int_{\\\\mathbb{R}^n} k(x, z)\\\\phi(z)p(z)dz$.\\n\\nThe value of $\\\\lambda$ will be the eigenvalue of that eigenfunction when the eigenfunction $\\\\phi$ is normalized as $\\\\int_{\\\\mathbb{R}^n} \\\\phi^2(x)p(x)dx = 1$.\\n\\n**Form of $p(z)$.** Recall that in our generative dataset framework, we have a pair of latent features $z_c$ and $z_s$ that are embedded into a high dimensional space via $x = \\\\alpha_s z_s w_s + \\\\alpha_c z_c w_c = Ud_{\\\\times 2}A_{2\\\\times 2}z_{2\\\\times 1}$. With this expression, we switch terminology such that our $w_i \\\\rightarrow U$ and $\\\\alpha_i \\\\rightarrow A$, and therefore $A$ is diagonal matrix with positive diagonal entries, and the columns of $U$ are (approximately) orthonormal. Henceforth, we also refer to features with indices 1 and 2 instead of $s$ and $c$. An implication of orthonormal columns on $U$ is that the dot product of any two input vectors $x$ and $x^\\\\dagger$ will be independent of $U$, i.e., $\\\\langle x, x^\\\\dagger \\\\rangle = \\\\langle Az, Az^\\\\dagger \\\\rangle$. Consequently, we can compute dot products in the original 2-dimensional space instead of in the $d$-dimensional embedding space. On the other hand, we will later see that the kernel function $k(x_1, x_2)$ of the two cases we study here (ReLU and linear) depends on their input only through the dot product $\\\\langle x_1, x_2 \\\\rangle$ and norms $\\\\|x_1\\\\|$ and $\\\\|x_2\\\\|$ (self dot products). Thus, the kernel is entirely invariant to $U$ and without loss of generality, we can consider the input to the model as $x = Az$. Therefore, $x + \\\\sim \\\\mathcal{N}(\\\\begin{bmatrix} a_1 \\\\mu_1 \\\\\\\\ a_2 \\\\mu_2 \\\\end{bmatrix}, \\\\begin{bmatrix} a_2 1 & a_1 a_2 \\\\sigma_{12} \\\\\\\\ a_1 a_2 \\\\sigma_{12} & a_2 2 \\\\end{bmatrix})$, $x - \\\\sim \\\\mathcal{N}(\\\\begin{bmatrix} -a_1 \\\\mu_1 \\\\\\\\ -a_2 \\\\mu_2 \\\\end{bmatrix}, \\\\begin{bmatrix} a_2 1 & a_1 a_2 \\\\sigma_{12} \\\\\\\\ a_1 a_2 \\\\sigma_{12} & a_2 2 \\\\end{bmatrix})$.\\n\\n**Linear kernel function.** If the activation function is linear, then the kernel function simply becomes a standard dot product, $k(x_1, x_2) = \\\\langle x_1, x_2 \\\\rangle$. The following theorem provides details about the spectrum of this kernel.\\n\\n**Theorem 1** Consider the kernel function $k(x_1, x_2) = \\\\langle x_1, x_2 \\\\rangle$. The kernel operator associated with $k$ under the data distribution $p$ specified above has only one non-zero eigenvalue $\\\\lambda = \\\\|A\\\\mu\\\\|_2^2$ and its eigenfunction has the form $\\\\phi(x) = \\\\langle A\\\\mu, x \\\\rangle \\\\|A\\\\mu\\\\|_2^2$. \\n\"}"}
{"id": "Tj3xLVuE9f", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We now assign a target value w.r.t. $b$\\nThe following theorems now show that linear networks are unbiased to feature availability, while\\n\\nTheorem 3\\n\\nReLU networks are biased toward more available features.\\n\\nIn a linear network, the values of $a$ indicate bias toward more- and less-available features, respectively. To verify either of these two aspects via sensitivity, we must choose the lowest order $m$ -1 component of the mixture.\\n\\nConsider the kernel function $\\\\mathcal{K}(x, y) = \\\\phi(x) \\\\phi(y)$, where $\\\\phi$ is the prediction function as their normalized dot product, $\\\\gamma$ is always zero for any choice of $m$, i.e. $\\\\gamma = 0$. The alignment between $a_i$ and $\\\\gamma$ is $|a_i| - |\\\\gamma|$. To make a prediction. In particular, if whenever $|a_i| > |\\\\gamma|$, we also see the model is biased toward the more-available feature. In\\n\\n$\\\\mathcal{N}_{\\\\text{Sensitivity}}$\\n\\n$6.1 \\\\ S$\\n\\n$|\\\\sigma|$\\n\\n$\\\\lambda$\\n\\n$|\\\\lambda|$\\n\\n$\\\\pi$\\n\\n$\\\\mathcal{K}$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$\\n\\n$\\\\mathcal{K}^*$"}
{"id": "Tj3xLVuE9f", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Theorem 4\\nIn a ReLU network, \\\\(|\\\\zeta_1| - |\\\\zeta_2| = 0\\\\) for any \\\\(1 \\\\leq m \\\\leq 8\\\\). The first non-zero \\\\(|\\\\zeta_1| - |\\\\zeta_2|\\\\) happens at \\\\(m = 9\\\\) and has the following form,\\n\\n\\\\[\\n|\\\\zeta_1| - |\\\\zeta_2| = 5670 \\\\|A\\\\|_1^{18} (a_1 a_2 \\\\mu_1 \\\\mu_2)^8 (a_2 a_1 \\\\mu_2 a_1 - a_2 a_2 \\\\mu_2 a_2) (a_1 - a_2).\\n\\\\] (6)\\n\\nA straightforward consequence of this theorem is that\\n\\n\\\\[\\n\\\\text{sign}\\\\left((|\\\\zeta_1| - |\\\\zeta_2|)(a_1 - a_2)\\\\right) = \\\\text{sign}\\\\left(5670 \\\\|A\\\\|_1^{18} (a_1 a_2 \\\\mu_1 \\\\mu_2)^8 (a_2 a_1 \\\\mu_2 a_1 - a_2 a_2 \\\\mu_2 a_2) (a_1 - a_2)\\\\right).\\n\\\\] (7)\\n\\nRecall from Section 3 that feature predictivity \\\\(\\\\rho_i\\\\) is related to \\\\(\\\\mu_i\\\\) via\\n\\n\\\\[\\n\\\\rho_i = \\\\frac{1}{2} (1 + \\\\text{erf}(\\\\mu_i \\\\sqrt{2})).\\n\\\\]\\n\\nObserve that \\\\(\\\\rho_i\\\\) is an increasing function in \\\\(\\\\mu_i\\\\); thus, bigger \\\\(\\\\mu_i\\\\) implies larger \\\\(\\\\rho_i\\\\). That together with (7) provides a crisp interpretation of the trade-off between predictivity and availability. For example, when the latent features are equally predictive (\\\\(\\\\mu_1 = \\\\mu_2\\\\)), the sign becomes +1 for any (non-negative) choice of availability parameters \\\\(a_1\\\\) and \\\\(a_2\\\\). Thus, for equally predictive features, the ReLU networks are always biased toward the more available feature. Figure 5 shows some more examples with certain levels of predictivity. The coloring indicates the direction of the availability bias (only the boundaries between the blue and the yellow regions have no availability bias).\\n\\nFEATURE AVAILABILITY IN NATURALISTIC DATASETS\\nWe have seen that models trained on controlled, synthetic data are influenced by availability to learn shortcuts when a nonlinear activation function is present. How do feature predictivity and availability in naturalistic datasets interact to shape the behavior of models used in practice? To test this, we train ResNet18 models (He et al., 2016) to classify naturalistic images by a binary core feature irrespective of the value of a non-core feature. We construct two datasets by sampling images from Waterbirds (Sagawa et al., 2020a) (core: Bird, non-core: Background), and CelebA (Liu et al., 2015) (core: Attractive, non-core: Smiling). See C.5 for additional details.\\n\\nSensitivity to the non-core feature beyond a statistical account. Figures 6A and B show that, for both datasets, as the training-set predictivity of the non-core feature increases, model accuracy dramatically increases for congruent probes and decreases for incongruent ones. In contrast, a Bayes optimal classifier is far less sensitive to the predictivity of the non-core feature. Thus, models are more influenced by the non-core feature than what we would expected based solely on predictivity. This heightened sensitivity implies that models prioritize the non-core feature more than they should, given its predictive value. Thus, in the absence of predictivity as an explanatory factor, we conclude that the non-core feature is more available than the core feature.\\n\\nAvailability manipulations. Motivated by the result that Background is more available to models than the core Bird (Figure 6A), we test whether specific background manipulations (hypothesized types of availability) shift model feature reliance. As shown in Figure 6C, we find that Bird accuracy increases as we reduce the availability of the image background by manipulating its spatial extent (Bird size, Background patch removal) or drop background color (Color), implicating these as among the features that models latch onto in preferring image backgrounds (validated with explainability analyses in C.5). Experiments in Figure B.9 show that this phenomenon also occurs in ImageNet-pretrained models; background noise and spatial frequency manipulations also drive feature reliance.\\n\\nCONCLUSION\\nShortcut learning is of both scientific and practical interest given its implications for how models generalize. Why do some features become shortcut features? Here, we introduced the notion of...\"}"}
{"id": "Tj3xLVuE9f", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Availability as well as predictivity determines which features image classifiers rely on. A: Models (ResNet18) were trained to classify Birds from images sampled from Waterbirds. We varied Background (non-core) predictivity while keeping Bird (core) predictivity fixed ($\\\\rho = 0.99$), and show Bird classification accuracy for two types of probes: congruent (blue, core and non-core features support the same label) and incongruent (orange, core and non-core features support opposing labels). As Background predictivity increases, the gap in accuracy between incongruent and congruent probes also increases. The model is more sensitive to the non-core feature than expected by a Bayes-optimal classifier ($\\\\rho^*$): predictivity alone does not explain the model's behavior. B: Similar to the Waterbirds dataset, models trained to classify images from CelebA as \\\"Attractive\\\" exhibit an effect of \\\"Smiling\\\" availability. C: Bird accuracy for incongruent Waterbirds probes is influenced by both Background predictivity ($\\\\rho$) and availability when we manipulate the latter explicitly (see also B.9).\\n\\nWe proposed a generative framework that allows for independent manipulation of predictivity and availability of latent features. Testing hypotheses about the contributions of each to model behavior, we find that for both vector and image classification tasks, deep nonlinear models exhibit a shortcut bias, deviating from the statistically optimal classifier in their feature reliance. We provided a theoretical account which indicates the inevitability of a shortcut bias for a single-hidden-layer nonlinear (ReLU) MLP but not a linear one. The theory specifies the exact interaction between predictivity and availability, and consistent with our empirical studies, predicts that availability can trump predictivity. In naturalistic datasets, vision architectures used in practice rely on non-core features more than they should on statistical grounds alone. Connecting with prior work identifying availability biases for texture and image background, we explicitly manipulated background properties such as spatial extent, color, and spatial frequency and found that they influence a model's propensity to learn shortcuts.\\n\\nTaken together, our empirical and theoretical findings highlight that models used in practice are prone to shortcut learning, and that to understand model behavior, one must consider the contributions of both feature predictivity and availability. Future work will study shortcut features in additional domains, and develop methods for automatically discovering further shortcut features which drive model behavior. The generative framework we have laid out will support a systematic investigation of architectural manipulations which may influence shortcut learning.\"}"}
