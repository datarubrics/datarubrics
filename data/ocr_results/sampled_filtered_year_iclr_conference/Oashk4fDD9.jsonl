{"id": "Oashk4fDD9", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Evaluating systematic generalization on FST tasks with 4 states (cf. Table 1). Due to an outlier task on UC, we additionally report the median after \u2018/\u2019.\\n\\n| Iteration | UC | Acc | ED | Acc | ED |\\n|-----------|----|-----|----|-----|----|\\n| T5-Set    | 26.6 | 6.26 | 55.1 | 54.6 | 1.18 | 1.02 |\\n| T5-SIP-d4 | 94.5 | 0.11 | 75.4 | 99.5 | 0.54 | 0.01 |\\n\\nTable 7: Evaluation with T5-Base on non-deterministic FSTs (cf. Table 2)\\n\\n| Iteration | UC | Acc | ED | Acc | ED |\\n|-----------|----|-----|----|-----|----|\\n| T5-Set    | 77.9 | 0.73 | 81.7 | 0.53 |\\n| T5-SIP-d4 | 83.3 | 0.56 | 86.1 | 0.37 |\\n\\nTable 8: Grapheme-to-phoneme conversion with 100 training examples based on T5-Base. In contrast to the experiments in the main paper, we found that T5-SIP-d4 did not perform well on completely unseen scripts, so we mapped all Unicode code points to arbitrary ASCII characters. This maintains the structure of the task and is completely reversible. T5-Set is evaluated in the same way.\\n\\n| ban | cop | got | lao | syl | tel | tzm | Avg |\\n|-----|-----|-----|-----|-----|-----|-----|-----|\\n| Acc | PER | Acc | PER | Acc | PER | Acc | PER | Acc | PER | Acc | PER | Acc | PER |\\n| T5-Set | 47.9 | .231 | 1.2 | .783 | 6.7 | .458 | 3.6 | .643 | 6.6 | .611 | 4.9 | .612 | 2.7 | .797 | 10.5 | .591 |\\n| T5-SIP-d4 | 59.1 | .154 | 4.7 | .640 | 69.6 | .059 | 5.9 | .566 | 22.1 | .447 | 35.4 | .191 | 12.5 | .509 | 29.9 | .367 |\\n\\nThe ByT5 tokenizer. While this is suitable as a starting point for further pre-training, we found that directly fine-tuning T5-Base with these modifications led to very poor results and do not include them here. Instead, we train T5-Set (analogous to Set) for a fair point of comparison.\\n\\nWe report a subset of the results from the main paper in for T5-Base in Tables 6 to 8.\\n\\n**ADDITIONAL MODEL DETAILS & HYPERPARAMETERS & HARDWARE**\\n\\nFor completeness, we now describe the order in which we arrange the transitions. While the ordering of the transitions does not matter for expressing FSTs, the Transformer uses positional encodings which might have impacts on the pre-training. We assemble the overall prefix by stacking the individual vectors $h_0, \\\\ldots, h_n$ of the transitions $p_0 \\\\sigma_0 : \\\\rho_0 \\\\rightarrow q_0, \\\\ldots, p_n \\\\sigma_n : \\\\rho_n \\\\rightarrow q_n$. We group the transitions by their originating state (i.e. $p_i$) and go over the states by their id, starting with 0, the initial state.\\n\\nDuring pre-training, we might encounter FSTs with different numbers of transitions within the same batch. To handle this, we use padding encodings by reserving a special padding state and padding symbol in the embedding matrices of states and symbols. To initialize the prefix for fine-tuning, we use the average of 32 FST encodings (chosen at random) from pretraining.\\n\\nFor pre-training, we use embeddings of dimensionality 64 for states, embeddings of dimensionality 256 for symbols, and of dimensionality 16 to indicate final/non-final states.\\n\\n**Task embeddings.** In order to enable faster adaption of the task embeddings than the rest of the model to fit a particular task, we use a higher learning rate for the task embeddings (1.0) than for the rest of the model ($5 \\\\cdot 10^{-4}$) during pre-training. We also use a higher learning rate for the prefix during fine-tuning, analogously to SIP.\\n\\nBecause we have to store 40,000 task embeddings (one for each generated FST), TE requires a lot of memory. To reduce memory consumption, the task embeddings have a dimensionality of 180 and are up-projected to fit into the Transformer, analogously to W in Section 4.1. Nevertheless, the memory consumption of the embeddings is substantial and we store them on a separate GPU.\\n\\nAnalogously to SIP-d4, we pre-train for 20 epochs.\\n\\nNaive. We pre-train for a single epoch only as we found this achieved better results on downstream tasks than training for 20 epochs.\"}"}
{"id": "Oashk4fDD9", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We sample 200,000 examples according to the procedure described by Wu et al. (2022) to match our pre-training dataset size. Again, we found it more helpful for downstream task performance to train for a single epoch rather than 20 epochs.\\n\\nFine-tuning Hyperparameters. The main hyperparameters involved for both SIP and TE are the learning rates for the main model, and (separately) the learning rate of the tunable prefix. We chose these manually. Generally, we found that using a learning rate of $10^{-3}$ was a good choice for the prefix. Lester et al. (2021) report a similarly high learning rate to be useful for prompt tuning. For the rest of the model, we found $3 \\\\times 10^{-4}$ and $5 \\\\times 10^{-4}$ to work well for SIP-d4 and TE, respectively.\\n\\nFor few-shot experiments, we use a somewhat smaller learning rate for TE for the main model ($3 \\\\times 10^{-4}$). We noticed that T5-SIP-d4 (see Appendix E) was more sensitive to the learning rate choice in general than SIP-d4.\\n\\nFor any experiment, the chosen learning rates can also be found in the spreadsheet with the raw experimental results in the supplementary material.\\n\\nHardware. We ran our experiments on NVIDIA GeForce RTX 2080 Ti GPUs (11264MiB RAM) with driver version 535.54.03 and cuda version 12.2.\\n\\n**Analysis of Fine-Tuned Prefixes**\\n\\nTo gain some understanding of how the prefix of tunable embeddings is used by the model and what it contains, we consider the setup of fine-tuning only the prefix and keeping the rest of the model unchanged. That is, all the task-specific information has to be captured in these embeddings. Specifically, we fine-tune on the 5 FSTs from Section 6.2 for iteration generalization for 20 epochs with a learning rate of 0.5.\\n\\nWe explore two questions:\\n\\n1. Is the model robust towards different permutations of the fine-tuned prefixes? Intuitively, these permutations correspond to changing the order in which transitions are listed, so ideally the model should not be sensitive to that order.\\n\\n2. Does the fine-tuned prefix represent the task-specific information in a similar way to how FSTs were encoded during pre-training?\\n\\nTo address the first question, we randomly permute the tuned prefixes and compute accuracy on the iteration generalization data before and after permuting the tuned prefixes. We use 20 permutations per learned prefix and average results across the 5 FSTs. Overall, we find that this results only in a small drop in accuracy: the median drop in accuracy is only around 1.3 percentage points, and the arithmetic mean of the drop is around 7.1 percentage points. Most permutations do not have a big impact on how the prefix is interpreted but a few permutations do have a stronger negative impact, skewing the arithmetic mean.\\n\\nTo address the second question, we test if the learned prefix for a task $t$ resembles an encoding of an FST that solves $t$. For each of the 5 FSTs, we generate 10,000 distractors, i.e. FSTs that have the same number of states and use the same vocabulary as the FST solving $t$. We define the similarity of two prefixes $p, q$ as follows:\\n\\n$$\\\\text{sim}(p, q) = \\\\max_{\\\\pi} \\\\frac{1}{n} \\\\sum_{i} \\\\frac{1}{2} || p_i - q_{\\\\pi(i)} ||_2$$\\n\\nwhere $\\\\pi$ is a permutation, and $p_i$ is the $i$-th vector in prefix $p$, and prefixes $p$ and $q$ both have length $n$.\\n\\nThat is, we define the similarity between $p$ and $q$ as the highest possible average cosine similarities between positions in $p$ and $q$ that one can achieve by assigning a position in $p$ to exactly one position in $q$ and vice-versa. Computing the similarity $\\\\text{sim}(p, q)$ is relatively expensive because it involves solving the assignment problem (e.g. with the Hungarian algorithm). Instead of solving the assignment problem exactly, we approximate it with the Sinkhorn algorithm (Sinkhorn, 1964). We then take the output of the algorithm (a matrix of 'soft' assignments) and for each position in $p$, we greedily select a matching position in $q$. 4\\n\\nTaking the maximum over all permutations is justified by our results to the first question above, which showed that the model is largely invariant to different permutations of the tuned prefix.\"}"}
{"id": "Oashk4fDD9", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nSimilarity of learned prefix to ground truth FST\\n\\nMaximum similarity to a distractor FST\\n\\n(0.466, 0.388)\\n(0.421, 0.364)\\n(0.389, 0.344)\\n(0.487, 0.412)\\n(0.357, 0.342)\\n\\nFigure 6: Each dot represents a fine-tuned prefix when the rest of the model remains frozen during fine-tuning. The x-coordinates represent the similarity to a ground truth gold prefix, and the y-coordinates represent the maximum similarity to any of the $5 \\\\times 10^4$ distractor FSTs. All dots are below the diagonal, hence all learned prefixes are most similar to an encoding of the ground truth FST.\\n\\n| Num. states | Split | Min | Max | Mean  |\\n|-------------|-------|-----|-----|-------|\\n| 4 train     | 2     | 11  | 4.66|       |\\n| 4 test      | 4     | 30  | 18.97|      |\\n| 5 train     | 2     | 14  | 5.39|       |\\n| 5 test      | 4     | 30  | 19.53|      |\\n| 7 train     | 2     | 20  | 6.12|       |\\n| 7 test      | 4     | 30  | 20.13|      |\\n| 10 train    | 2     | 25  | 7.31|       |\\n| 10 test     | 4     | 30  | 20.62|      |\\n| 21 train    | 2     | 30  | 11.80|      |\\n| 21 test     | 5     | 30  | 23.07|      |\\n\\nTable 9: Distribution of input lengths of the train/test data we generate for the iteration generalization experiments in Section 6. The tasks with 21 states are the non-deterministic FSTs from Section 6.4. For every task $t$, we compute the similarity between the prefix $p$ learned by fine-tuning on input/output pairs and the union of encodings of the distractors and encodings of the gold standard FST for task $t$. Where necessary, we truncate encodings of FSTs to have the same length as the learned prefix. We present the results in Fig. 6 showing that all learned prefixes are most similar to an encoding of the ground truth FST.\\n\\nThe input strings in the pre-training data we generate for SIP-d4 have a minimum length of 1, an average length of 15.57 and a maximum length of 35. We report the length distributions for the iteration generalization experiments in Section 6 in Table 9.\\n\\nIn the main paper, we report results on iteration generalization where a model is trained on strings such that each state has been visited at most 3 times, and is tested on strings where at least one state is visited at least 4 times. Here, we explore a more extreme version, where there is a large gap between the maximum length seen during training and the minimum length seen during testing. As\"}"}
{"id": "Oashk4fDD9", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Average generalization ability across 5 FSTs with 4 states. Models were trained on inputs of length up to 15, and tested on much longer inputs.\\n\\n| Test length | Model | Max pretrain length | Acc \u2191 | Acc \u2193 |\\n|-------------|-------|---------------------|-------|-------|\\n| 40 to 70    | ByT5  | 1024                | 29.3  | 15.6  |\\n| 90 to 110   | SIP-d4| 35                  | 69.4  | 2.6   |\\n|             | SIP-d4-long | 110              | 81.5  | 1.09  |\\n\\nAnother point of comparison, we further pre-train SIP-d4 on 40,000 FSTs with strings of length up to 110 (SIP-d4-long).\\n\\nWe report results in Table 10. ByT5 struggles with this generalization setup across the board. SIP-d4 performs remarkably well on lengths 40-70 which are beyond the lengths seen during its pre-training. However, performance drops starkly when testing on inputs of length 90 to 110. We hypothesize that this is because the relevant positional embeddings were not pre-trained by SIP. In contrast, SIP-d4-long performs well on inputs of length 90 to 110, as it has seen strings of such length during pre-training.\"}"}
{"id": "Oashk4fDD9", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) A deterministic FST.\\n\\n(b) A non-deterministic but functional FST.\\n\\nFigure 2: Examples of functional FSTs. The FST in (a) deletes every other $a$. The FST in (b) replaces any $a$ in the input string by a $b$ if the last input symbol is a $b$. Conversely, if the last symbol is a $c$, any $a$ is replaced by a $c$. The output can only be determined after the last input symbol.\\n\\nIn short, while non-deterministic FSTs can take context to the right into account, deterministic FSTs cannot.\\n\\n4 SIMULATION\\n\\nOur approach follows the pre-training and fine-tuning paradigm. We first pre-train on synthetic FST tasks by giving the model a representation of an FST as a prefix and an input string (see Fig. 1). The training objective is to predict the output of the FST on the input string and thereby simulate the behaviour of the FST in the model. Our research hypothesis is that training a model to robustly simulate a broad range of FSTs incentivizes finding reusable mechanisms for FST-like behaviour. When fine-tuning the model using a tunable prefix instead of an encoding of an FST, these mechanisms should be easy to leverage and provide a structural inductive bias for FST-like tasks.\\n\\n### 4.1 PRE-TRAINING\\n\\nDuring pre-training, the model is given a representation of an FST and a string in its domain and has to predict the output of that FST on the given input string. The input to the Transformer is a sequence of vectors from $\\\\mathbb{R}^d$, which consist of a prefix that represents the FST $f$ and a suffix comprised of the embeddings of the input string (see Fig. 1):\\n\\n$$h_1, h_2, \\\\ldots, h_k | \\\\{z\\\\} FST \\\\text{ encoding}, x_1, x_2, \\\\ldots, x_n | \\\\{z\\\\} \\\\text{Input to FST}$$\\n\\nEach $h$ encodes one transition $p_\\\\sigma: \\\\rho \\\\rightarrow q$ of $f$ as a vector:\\n\\n$$h = W \\\\begin{bmatrix} \\\\text{EMBED \\\\ State}(p); \\\\text{EMBED \\\\ State}(q); \\\\text{EMBED \\\\ Symbol}(\\\\sigma); \\\\text{EMBED \\\\ Symbol}(\\\\rho); \\\\text{EMBED \\\\ Final}(e) \\\\end{bmatrix}$$\\n\\nwhere $[;]$ represents vector concatenation, $e$ indicates if $q$ is a final state, and $W$ is a linear layer that ensures that $h \\\\in \\\\mathbb{R}^d$. All embeddings are simple look-up tables based on the ID of the state or symbol.\\n\\nThe initial state of the FST is always assigned the ID 0. Positional embeddings are used as usual. The model is trained to maximize the log probability of the output $y = f(x)$ of the FST $f$.\\n\\n### 4.2 FINE-TUNING\\n\\nAfter pre-training, we can apply our model to a downstream task and fine-tune it. We assume we do not have access to an FST for the downstream task, and therefore we replace the FST encoding with a sequence of tunable embeddings. These embeddings are initialized to the average of the encoding of multiple FSTs from the pre-training phase. The most straightforward way to fine-tune is to only modify the embeddings in the prefix because we are looking for an FST-like representation of the task. This is similar to prompt tuning (Lester et al., 2021). However, this does not work well on tasks outside the pre-training distribution. Therefore, we fine-tune the entire model, including the prefix, and use a higher learning rate for the prefix than for the rest of the model (see Appendix F).\\n\\n1 This encoding approach neglects that permuting the state numbers has no effect on the function that the FST represents. We leave this to future work, e.g. using graph neural networks.\"}"}
{"id": "Oashk4fDD9", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To create our pre-training data, we sample 40,000 deterministic FSTs. For every FST, we sample 5 input/output pairs with input lengths up to 35. In total, this leads to 200,000 pairs for training along with their FSTs. To describe the sampling procedure in more detail, we use an overall vocabulary $V$ consisting of the printable ASCII tokens and the Unicode block for IPA symbols (used for transcribing speech). Seq2seq tasks in the wild usually do not use the whole space of this vocabulary, so for each task $T$ we first uniformly sample the vocabulary size $|V_T|$ between 5 and 25 and then uniformly select a subset $V_T \\\\subseteq V$. Then, we uniformly sample the number of states $|Q_T|$ between 2 and 4, and the number of final states between 1 and $|Q_T|$. For every state $q$ and every symbol $\\\\sigma \\\\in V_T$ we introduce at most one outgoing transition to a state $q'$, chosen uniformly at random. This ensures that the FST is deterministic. We then sample the output for the transition: either a symbol $\\\\rho \\\\in V_T$ or $\\\\epsilon$. Finally, we minimize the number of states of the FST using OpenFST (Allauzen et al., 2007), and exclude those without cycles, as they express finite relations. See Appendix A for details.\\n\\nIn practical applications of FSTs, in particular for text editing, one often wants to keep certain parts of the input unchanged. This can be achieved with a set of transitions of the form $q \\\\sigma$: $\\\\sigma$ $\\\\rightarrow$ $q'$ for all $\\\\sigma \\\\in V_T$. Since it is very unlikely to sample such a set of transitions, we use a special symbol that acts as a shorthand for this, which we also use when encoding the FST for pre-training.\\n\\nInductive biases are the preferences and the abstract knowledge that a learner brings to the task before having seen any data. The inductive bias of a learner helps it fill in the 'gaps' that are not covered by the training data. In order to evaluate inductive bias, we specifically design training data to contain gaps and probe the behaviour of the learner on these gaps. In this paper, we use two different setups that ensure we evaluate on gaps: learning from a small amount of data (few-shot learning) and systematic generalization outside of the training distribution.\\n\\nWe consider a model to have an inductive bias specifically towards FSTs if its behaviour on the gaps in the training data resembles the most plausible FST according to Occam's razor. We consider the FST the most plausible that (i) explains the training data and (ii) all else being equal, is as simple as possible, i.e. has the smallest number of states.\\n\\nWe now describe two methods for constructing data for a given (minimal) FST such that the training and test distributions are different, and that reward a model for inferring the simplest FST.\\n\\n**Iteration generalization.**\\nA simple form of out-of-distribution generalization is to generalize from short examples to longer examples. In particular, given an FST $f$, we test the ability to generalize from visiting a state only a few times (iteration count up to 3) to seeing it more often (iteration count at least 4). A model with an inductive bias for FSTs should be able to obtain high accuracy in this setting. This is because $f$ is the simplest FST that explains the data. Any FST that behaves the same as $f$ on the training data but differs on longer inputs has to have additional states or transitions that were unused on the training data.\\n\\n**Unseen combinations of transitions.**\\nLSTMs and Transformers struggle to generalize to unseen combinations of known elements (Keysers et al., 2020). For example, consider the FST $f$ in Fig. 3, which deletes leading zeros from a number. Suppose that a model is trained on examples such as $0012$, $2201$, $1012$ but no training example contains the combination of leading zeros followed by a $2$ (the combination of the two red adjacent transitions). A model with an inductive bias towards FSTs should nevertheless generalize to this unseen combination and correctly handle examples such as $0021$ because $f$ is the simplest FST that explains the data. The preference for simple FSTs is crucial for this to be meaningful. Consider approximating a function $f$ mapping between strings of bounded length with a model $\\\\hat{f}$. Suppose we only required that $\\\\hat{f}$ (i) fit the training data and (ii) correspond to some FST. The requirement (ii) is trivially true for any $\\\\hat{f}$, giving any model an inductive bias towards FSTs under this notion. This is because any function between strings of bounded length is defined for finitely many elements, and hence can be represented by an FST.\"}"}
{"id": "Oashk4fDD9", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Constructing Training Data for Evaluating Unseen Combinations of Transitions\\n\\nBased on the given FST $f$, we construct an FST $f_{\\\\text{train}}$ that withholding the combination of the two red transitions.\\n\\nIn order to withhold a combination of transitions $\\\\langle t_a, t_b \\\\rangle$, we construct a new FST $f_{\\\\text{train}}$ as follows: We create two copies $f_a, f_b$ of the original FST $f$. In $f_a$, we remove the transition $t_b$; in $f_b$, we remove the transition $t_a$. Then $f_{\\\\text{train}} = f_a \\\\cup f_b$, which can be constructed by introducing a new initial state with $\\\\epsilon$-transitions into the respective initial states of $f_a$ and $f_b$ (right side of Fig. 3). This ensures that any accepting path goes through $f_a$ or $f_b$ but cannot alternate between the two. Hence, $t_a$ or $t_b$ can be used \u2013 but not both in the same string. Note that $f_{\\\\text{train}}$ still describes a partial function (rather than a relation) because any accepting path in $f_a$ and any accepting path in $f_b$ is also an accepting path in $f_{\\\\text{train}}$. As a result, whenever $f_a$ and $f_b$ are both defined, they agree on the result $f_a(x) = f_b(x) = f(x)$. We test exclusively for how a model handles unseen combinations of transitions by generating examples from $f$ for which $f_{\\\\text{train}}$ is not defined. We refer to Appendix C for further details.\\n\\nTo make the generalization setup more challenging, these steps can be applied to multiple pairs of adjacent transitions at the same time, i.e. to withhold $\\\\langle t_{1a}, t_{1b} \\\\rangle, \\\\ldots, \\\\langle t_{ka}, t_{kb} \\\\rangle$: We create the copy $f_a$ and remove the transitions $t_{1b}, \\\\ldots, t_{kb}$ from $f_a$ and analogously remove $t_{1a}, \\\\ldots, t_{ka}$ from $f_b$.\\n\\n### Setup and Baselines\\n\\nIn order to understand the effects of our pre-training procedure, we first explore systematic generalization on synthetic FST tasks which allows us to precisely control the similarity between the pre-training and the downstream task.\\n\\n#### 6.1 Setup and Baselines\\n\\nIn order to make a fair comparison, all models we experiment with in the main paper share the same architecture and are initialized from the same checkpoint before any additional pre-training, namely ByT5-small (Xue et al., 2022). This is a Transformer with 300M parameters across 12 encoder layers and 4 decoder layers with a hidden dimensionality of 1472. It was pre-trained on the multilingual C4 corpus. ByT5 uses raw bytes as tokens, which enables full Unicode support and is a natural unit to consider for FST-like tasks such as text editing and grapheme-to-phoneme conversion. We report additional results with a T5-Base model in Appendix E, where we observe similar trends.\\n\\n**SIP-d4.** This is a model using the method we propose in this work. We pre-train on the data generated in Section 4.3 (deterministic FSTs, with up to 4 states) for 20 epochs. This model achieves an average accuracy of 98% on predicting the output of an unseen FST from the training distribution.\\n\\nFor fine-tuning, we use a prefix of length 50 for all experiments in this paper. As an ablation, we also fine-tune the model without the prefix of learnable embeddings (-prefix).\\n\\n**Naive pre-training.** For this baseline, we use the same pre-training data as for SIP-d4 but we omit the explicit description of the FST and only train on input/output pairs.\\n\\n**Set.** Wu et al. (2022) investigate the effectiveness of 18 simple synthetic pre-training tasks for a range of downstream tasks, and found Set to perform best on average. The task is to deduplicate characters such that every type occurs only once, e.g. the input $\\\\text{daabacd}$ becomes $\\\\text{dabc}$. This baseline is well-suited for our setup because the task can be represented by a deterministic FST, albeit a very large one with $2^n$ states for a vocabulary of size $n$.\\n\\n**Task embeddings (TE).** Instead of using an encoding of an FST, this baseline uses 50 randomly initialized embeddings specific to each task (i.e. FST) in the prefix. These embeddings are learned jointly with the model. Several works have used a single token or embedding to encode a domain or task in multi-domain and multi-task learning (Tsvetkov et al., 2016; Stymne et al., 2018; Zhang et al., 2022). Using a shorter tunable prefix resulted in considerably worse performance in our setup.\"}"}
{"id": "Oashk4fDD9", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6.2 SYSTEMATIC GENERALIZATION WITHIN THE PRE-TRAINING DISTRIBUTION\\n\\nFirst, we want to establish to what degree the pre-training has conferred any inductive bias on the distribution it was pre-trained on. In particular, we test for systematic generalization to unseen combinations (UC) and higher iteration counts.\\n\\n**Setup.** For each generalization setup, we generate 5 FSTs with 4 states using the same procedure as for the pre-training, ensuring they have not been seen in the pre-training. To evaluate UC, we withhold the combination of up to 20 pairs of transitions and generate 5000 training examples with lengths 3 to 15 and corresponding test data as described in Section 5. To evaluate iteration generalization, we generate training examples with a maximum iteration count of 3 and test on longer examples of length up to 30 with an iteration count of at least 4. Since the out-of-distribution performance of two checkpoints of the same model can vary significantly, we report averages on the test set of the last 10 epochs.\\n\\n**Results.** The results can be found in Table 1. On average, SIP-d4 achieves close to perfect accuracy (with one outlier on UC, skewing the mean). TE also shows a clear improvement over the other baselines but SIP-d4 outperforms TE by a large margin. This suggests that SIP-d4 and TE, to a lesser extent, indeed have acquired a stronger inductive bias for FSTs than the other methods. Using SIP-d4 without the tunable prefix leads to a substantial drop in accuracy, highlighting its importance. We analyze the representations learned by SIP-d4 in the tunable prefix in Appendix G.\\n\\n6.3 MORE COMPLEX FSTS\\n\\nDoes the inductive bias introduced by SIP extend beyond the pre-training distribution to more complex FST tasks? To investigate this, we use the same sampling methodology but generate FSTs with more states. SIP-d4 was pre-trained on FSTs with up to 4 states, and we evaluate on FST tasks with 5, 7 and 10 states. Again, we evaluate by measuring out-of-distribution performance for iteration generalization and unseen combinations.\\n\\nIn Fig. 4 we show how the individual models deviate from the accuracy of ByT5 as a function of the number of states in the test FST. We report the absolute accuracies in Table 5 in the appendix. The trends for the two generalization setups are very similar: SIP always performs best by a clear margin regardless of the number of states in the FSTs. As we increase the number of states and move further away from the pre-training distribution, SIP improves less over the baselines. We see a similar pattern for TE but with considerably smaller improvements over ByT5.\\n\\n6.4 NON-DETERMINISTIC FSTS\\n\\nAs shown in the previous section, SIP still works well for more complex FST tasks than seen during pre-training. However, this evaluation focused on the favourable case where both pre-training and evaluation involve the same class of FSTs, namely deterministic FSTs. Deterministic FSTs can only take left context into account (see Section 3), which is a restrictive assumption. Here, we evaluate if the inductive bias conferred by SIP carries over to non-deterministic functional FSTs, i.e. those that can also take context to the right into account.\"}"}
{"id": "Oashk4fDD9", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Oashk4fDD9", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Before describing our procedure for sampling deterministic FSTs, we briefly establish notation. An FST is a tuple $\\\\langle Q, \\\\Sigma, \\\\Gamma, I, F, \\\\Delta \\\\rangle$, where $Q$ is a finite set of states, $\\\\Sigma$ is the input alphabet, $\\\\Gamma$ is the output alphabet, $I \\\\subseteq Q$ is a set of initial states, $F \\\\subseteq Q$ is a set of final states and $\\\\Delta \\\\subseteq Q \\\\times (\\\\Sigma \\\\cup \\\\{\\\\epsilon\\\\}) \\\\times (\\\\Gamma \\\\cup \\\\{\\\\epsilon\\\\}) \\\\times Q$ are the transitions. We assume $\\\\Sigma = \\\\Gamma$ and call it $V$ for vocabulary. For technical reasons, we exclude the three characters \\\\[ and \\\\] from the vocabulary as they are interpreted as special characters by OpenFST, which we use for constructing and representing FSTs. In addition to the shorthand for identity transitions (id), we also have shorthands for converting upper case to lower case and vice-versa (lower-to-upper, upper-to-lower). We describe our procedure to generate a deterministic FST with pseudocode in Algorithm 1. It receives as argument $n$ (the number of states in the FST), $f$ (number of final states), $V$ (the vocabulary of this FST), and probabilities $P_{-\\\\text{ID}}$, $P_{-\\\\text{DROP}}$, $P_{-\\\\text{SHORTHAND}}$. These probabilities control the likelihood of using a shorthand, not drawing an outgoing edge ($P_{-\\\\text{DROP}}$) with a given symbol, and creating a single identity transition ($P_{-\\\\text{ID}}$). We use $\\\\text{CHOICE}$ to denote a uniform random choice from a finite set. We use $P_{-\\\\text{ID}} = 0.2$, $P_{-\\\\text{DROP}} = 0.4$, $P_{-\\\\text{SHORTHAND}} = 0.15$ in our experiments. For all experiments with synthetic data, we generate 5000 training examples and 1000 test examples. To reduce variance across tasks, we fix the vocabulary size to its maximum value (25) in the pre-training data and only use printable ASCII characters.\\n\\nIt is not straightforward to directly generate non-deterministic FSTs that are guaranteed to express a function. However, we can directly generate a bimachine, which then can be converted into an FST.\"}"}
{"id": "Oashk4fDD9", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\n\\nGenerate a random deterministic FST\\n\\nfunction GEN_DET-FST(n, f, V, P-ID, P-DROP, P-SHORTHAND)\\n\\nQ = \\\\{0, \\\\ldots, n-1\\\\}\\n\\\\Delta = \\\\emptyset\\nI = \\\\{0\\\\}\\n\\nfor q \u2208 Q do\\n    q\u2032 = CHOICE(Q) with prob P-SHORTHAND\\n    s = CHOICE([id, lower-to-upper, upper-to-lower])\\n    \\\\Delta := \\\\Delta \u222a \\\\{q \\\\textbackslash s: }s\\\\textbackslash \\\\leftarrow \\\\rightarrow q\\\\textbackslash q'\\\\}\\n\\nelse\\n    for \u03c3 \u2208 V do\\n        with prob P-DROP\\n            no-op \u25b7 No outgoing edge with \u03c3 at q\\n        else\\n            with prob P-ID\\n                \\\\Delta := \\\\Delta \u222a \\\\{q \\\\textbackslash \u03c3: }\u03c3\\\\textbackslash \\\\leftarrow \\\\rightarrow q\\\\textbackslash q'\\\\}\\n        else\\n            \\\\Delta := \\\\Delta \u222a \\\\{q \\\\textbackslash \u03c3: CHOICE(V \u222a {\u03f5}) \\\\rightarrow \\\\rightarrow q\\\\textbackslash q'\\\\}\\n    end with prob\\nend for\\n\\nend for\\n\\nEliminate states from Q through which no accepting path can go\\n\\nChoose random subset F of Q with |F| = min(f, |Q|)\\n\\nreturn minimized FST with states Q, transitions \u2206, initial states I and final states F\\n\\nend function\\n\\nAlgorithm 2\\n\\nGenerate output function for bimachine\\n\\nfunction GEN-OUTPUT-\u03c8(n_L, n_R, V, P-ID = 0.2)\\n\\nfor q_L \u2208 0, \\\\ldots, n_L-1 do\\n    for q_R \u2208 0, \\\\ldots, n_R-1 do\\n        for \u03c3 \u2208 V do\\n            with prob P-ID\\n                \u03c8(q_L, \u03c3, q_R) := \u03c3\\n            else\\n                \u03c8(q_L, \u03c3, q_R) := CHOICE(V \u222a {\u03f5})\\n            end with prob\\n        end for\\n    end for\\nend for\\n\\nreturn \u03c8\\n\\nend function\\n\\nBimachines (Sch\u00fctzenberger, 1961) represent exactly the regular string functions, i.e. for every functional FST there is a bimachine that represents it. A bimachine consists of two deterministic finite state automata (called left and right) and an output function. Let \\\\( A_L \\\\) be the left FSA with states \\\\( Q_L \\\\) and transition function \\\\( \u03b4_L: Q_L \\\\times \u03a3 \\\\rightarrow Q_L \\\\), and let \\\\( A_R \\\\) be the right FSA with states \\\\( Q_R \\\\) and transition function \\\\( \u03b4_R: Q_R \\\\times \u03a3 \\\\rightarrow Q_R \\\\). The output function is \\\\( \u03c8: Q_l \\\\times \u03a3 \\\\times Q_r \\\\rightarrow \u0393^\u2217 \\\\).\\n\\nAll states of \\\\( A_L \\\\) and \\\\( A_R \\\\) are final states. Given an input string \\\\( x = \u03c3_1 \u03c3_2 \u03c3_3 \\\\ldots \u03c3_n \\\\), a bimachine runs \\\\( A_L \\\\) from left to right over \\\\( x \\\\), keeping track of the states \\\\( q_{l0}, q_{l1}, q_{l2}, \\\\ldots q_{ln} \\\\). It also runs \\\\( A_R \\\\) over the string \\\\( x \\\\) but this time from right to left, again keeping track of the states \\\\( q_{r0}, q_{r1}, q_{r2}, \\\\ldots q_{rn} \\\\) that are visited. Then, the state sequence of the right automaton is reversed and \\\\( \u03c8 \\\\) is applied 'elementwise' as illustrated in Fig. 5. More formally, the output of the bimachine is\\n\\n\\\\[\\n\u03c8(q_{l0}, \u03c3_1, q_{r_n-1})\u03c8(q_{l1}, \u03c3_1, q_{r_n-2})\u03c8(q_{l2}, \u03c3_1, q_{r_n-3})\\\\ldots \u03c8(q_{ln-1}, \u03c3_1, q_{r0})\\n\\\\]\"}"}
{"id": "Oashk4fDD9", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nTable 5: Evaluation on deterministic FSTs with more states, showing absolute accuracies and edit distances, corresponding to Fig. 4.\\n\\n| Num States | 4   | 5   | 7   | 10  |\\n|------------|-----|-----|-----|-----|\\n| Gen. Type  | Model | Acc | \u2191   | ED  | \u2193   |\\n| Iteration ByT5 | 37.8 | 5.87 |   |     |     |\\n| Naive      | 42.6 | 4.41 |   |     |     |\\n| Set        | 44.4 | 4.58 |   |     |     |\\n| TE         | 61.3 | 2.49 |   |     |     |\\n| SIP-d4     | 94.8 | 0.12 |   |     |     |\\n| UC ByT5    | 47.4 | 1.49 |   |     |     |\\n| Naive      | 44.9 | 1.52 |   |     |     |\\n| Set        | 43.6 | 1.47 |   |     |     |\\n| TE         | 57.3 | 1.13 |   |     |     |\\n| SIP-d4     | 73.1 | 0.61 |   |     |     |\\n\\nBimachines can be compiled into FSTs with a simple product construction. For a bimachine \\\\( \\\\langle A_L, A_R, \\\\psi \\\\rangle \\\\), one can construct an equivalent FST as follows:\\n\\n\\\\[\\n\\\\langle Q_L \\\\times Q_R, \\\\Sigma, \\\\Gamma, \\\\{s_L\\\\} \\\\times Q_R, Q_L \\\\times \\\\{s_R\\\\}, \\\\Delta \\\\rangle\\n\\\\]\\n\\nwhere \\\\( s_L \\\\) and \\\\( s_R \\\\) are initial states of \\\\( A_L \\\\) and \\\\( A_R \\\\), and \\\\( \\\\Delta \\\\) contains all transitions\\n\\n\\\\[\\n\\\\Delta = \\\\{ \\\\langle q_L, q_R \\\\rangle : \\\\rho \\\\xrightarrow{\\\\sigma} \\\\langle q'_L, q'_R \\\\rangle | \\\\delta_L(q_L, \\\\sigma) = q'_L, \\\\delta_R(q'_R, \\\\sigma) = q_R, \\\\rho = \\\\psi(q_L, \\\\sigma, q'_R) \\\\}\\n\\\\]\\n\\nWe refer to Mihov & Schulz (2019) for details and further information about bimachines.\\n\\nIn order to sample bimachines, we re-use Algorithm 1 with \\\\( P_{-SHORTHAND} = 0 \\\\), and ignore the outputs of the transitions, treating them as FSAs. We sample the output function according to Algorithm 2. For the test data creation (Table 2), we use 5 states in the left FSA and 4 states in the right FSA, and set \\\\( P_{-DROP} = 0 \\\\).\\n\\nFor creating the training data for SIP-d4, we use 2 or 3 states in either left or right automaton and set \\\\( P_{-DROP} = 0 \\\\) to keep the length of the prefix low to save GPU memory.\\n\\nIn the main paper, we described how we can withhold combinations of transitions. Here, we briefly describe how we select which pairs of transitions we want to withhold. We only select adjacent transitions, i.e. transitions where one can be used immediately after the other. In addition, some transitions cannot be deleted without cutting off a vital initial or final state, which can lead to \\\\( f_{\\\\text{train}} = \\\\emptyset \\\\). We ensure this never happens by never withholding the first transition into each state based on a depth-first traversal of the FST.\\n\\nWhile this procedure generates an FST \\\\( f_{\\\\text{train}} \\\\) that requires more states/transitions than the original \\\\( f \\\\), it is unlikely but not guaranteed that there is no equivalent FST to \\\\( f_{\\\\text{train}} \\\\) that is smaller than \\\\( f \\\\).\\n\\nIn Fig. 4, we show accuracy relative to the accuracy of ByT5. Here, we show the absolute accuracies and edit distances in Table 5.\\n\\nAdditional results with more states:\\n\\nIn Fig. 4, we show accuracy relative to the accuracy of ByT5. Here, we show the absolute accuracies and edit distances in Table 5.\\n\\nAdditional results with T5-Base:\\n\\nWe run a subset of the experiments starting off from a pre-trained T5-Base (Raffel et al., 2020) instead of ByT5. This model is about one-third smaller than ByT5 (around 200 million instead of 300 million parameters). T5-Base uses a different vocabulary than ByT5, so we resize the output layer to the vocabulary size of ByT5 and re-initialize it. For the input embeddings, we re-purpose the first \\\\( n \\\\) embeddings in the T5-Base embedding matrix to represent the token ids according to\"}"}
{"id": "Oashk4fDD9", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"INJECTING A STRUCTURAL INDUCTIVE BIAS INTO A SEQ2SEQ MODEL BY SIMULATION\\n\\nAnonymous authors\\n\\nPaper under double-blind review\\n\\nABSTRACT\\n\\nStrong inductive biases enable learning from little data and help generalization outside of the training distribution. Popular neural architectures such as Transformers lack strong structural inductive biases for seq2seq NLP tasks on their own. Consequently, they struggle with systematic generalization beyond the training distribution, e.g. with extrapolating to longer inputs, even when pre-trained on large amounts of text. We show how a structural inductive bias can be injected into a seq2seq model by pre-training it to simulate structural transformations on synthetic data. Specifically, we inject an inductive bias towards Finite State Transducers (FSTs) into a Transformer by pre-training it to simulate FSTs given their descriptions. Our experiments show that our method imparts the desired inductive bias, resulting in improved systematic generalization and better few-shot learning for FST-like tasks.\\n\\nINTRODUCTION\\n\\nInductive biases, i.e. the preferences and the abstract knowledge a model brings to the task, enable a model to learn from small amounts of data and generalize systematically outside of the training distribution. While seq2seq models perform very well on in-distribution data on many NLP tasks, they usually lack structural inductive biases and consequently struggle with systematic generalization. Previous work has shown that this includes generalization to unseen combinations of known sub-strings (Lake & Baroni, 2018; Keysers et al., 2020), extrapolation to longer inputs (Hupkes et al., 2020) and deeper recursion (Kim & Linzen, 2020).\\n\\nIntegrating structural inductive biases into seq2seq models is challenging. One popular approach is to develop specialized architectures (Zheng & Lapata, 2021; Kim, 2021; Lindemann et al., 2023), which makes it difficult to precisely control and adjust the nature of the inductive bias as the architecture would need to be changed and models re-trained. Recently, some works instead have tried to inject inductive biases into seq2seq models by means of pre-training on a well-chosen synthetic task (Krishna et al., 2021; Wu et al., 2021; 2022) or meta-learning on a distribution of synthetic tasks using MAML (Finn et al., 2017). Here, the inductive bias can be controlled by the choice of the synthetic task. However, meta-learning with MAML scales poorly because it requires expensive second-order derivatives and standard pre-training can be less effective (McCoy & Griffiths, 2023).\\n\\nIn this work, we present a computationally inexpensive way of injecting a structural inductive bias into a Transformer. We focus specifically on introducing an inductive bias that is helpful for tasks that traditionally have been approached with Finite State Transducers (FSTs). We choose FSTs because they are formally well understood, are easy to generate automatically, and are one of the simplest computational devices that are useful in NLP applications. While we focus on FSTs, the methodology is fairly general and our approach also provides a starting point for incorporating more general structural biases, provided by more expressive formalisms such as Pushdown Transducers.\\n\\nOur approach (SIP, for Simulation-Induced Prior) is simple (see Fig. 1): given a representation of an FST and an input string, a Transformer is pre-trained to predict what the output of the FST is on the given input. We assume that FSTs are not specified for fine-tuning on downstream tasks, so we replace the FST with tunable embeddings and fine-tune the model solely on input/output examples. We show that SIP improves accuracy on systematic generalization and few-shot learning for \u2018FST-like\u2019 downstream tasks, demonstrating that the desired inductive bias has been imparted. SIP not only improves systematic generalization on FST tasks similar to those seen during pre-training but...\"}"}
{"id": "Oashk4fDD9", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Left: Pre-training a Transformer to simulate automatically generated FSTs. Right: fine-tuning the Transformer and the prefix where the FST used to be on a downstream task by using only input/output pairs. Tunable parameters are represented in orange.\\n\\nalso on ones that are structurally more complex. The same pre-trained model achieves strong results on few-shot learning on text editing (e.g. Jane Doe \u2192 J. Doe) and grapheme-to-phoneme conversion, which traditionally have been approached with FSTs. Our contributions are:\\n\\n\u2022 a simple, adjustable and efficient method to inject a structural inductive bias for FST-like tasks into a Transformer.\\n\\n\u2022 better systematic generalization on tasks beyond the pre-training distribution.\\n\\n\u2022 strong results when transferring to natural FST-like data, as demonstrated on low-resource grapheme-to-morpheme conversion.\\n\\n2 R Elated Work\\n\\nSystematic generalization. Systematic generalization refers to the ability of a model to generalize (or extrapolate) beyond its training distribution in a systematic way that aligns with how humans generalize. Systematic generalization has been shown to be difficult for standard seq2seq models in contexts such as semantic parsing (Finegan-Dollak et al., 2018), machine translation (Li et al., 2021) and algorithmic reasoning (Deletang et al., 2023), in particular to unseen combinations of sub-strings, longer inputs as well as deeper recursion (Keysers et al., 2020; Kim & Linzen, 2020). A range of approaches have been developed to tackle this, with many works focusing on specialized architectures (Guo et al., 2020; Zheng & Lapata, 2021; Kim, 2021; Lindemann et al., 2023). Furrer et al. (2020) find that the specialized architectures they consider do not transfer well to tasks beyond the context in which they were designed. This highlights the importance of being able to adjust inductive biases more easily than re-designing the architecture of a model. Large-scale pre-training on natural language has been widely successful in NLP (e.g. for few-shot learning) and has also been shown to help with systematic generalization (Furrer et al., 2020). However, challenges remain even for LLMs such as GPT-3 and PALM (Qiu et al., 2022; Dziri et al., 2023). The methodology we present in this work can be used to create additional material for LLM pre-training. Here we focus on smaller models and leave this to future work.\\n\\nPre-training with synthetic tasks. Pre-training a model on a synthetic task to introduce specific inductive biases has been explored by several recent works. Krishna et al. (2021) identify useful \u2018skills\u2019 for news summarization and develop a pre-training task accordingly. LIME (Wu et al., 2021) targets mathematical reasoning and is pre-trained on symbolic string manipulation that resembles deductive, abductive and inductive reasoning. Wu et al. (2022) investigate a range of simple synthetic tasks for pre-training and show that some perform remarkably well across a range of downstream tasks. Papadimitriou & Jurafsky (2023) consider several synthetic languages to investigate which helps most as pre-training data for language modelling on English. In contrast to these works,\"}"}
{"id": "Oashk4fDD9", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nour approach targets simulating a computational device and maintains a closer connection to the pre-training setting because of the tunable prefix.\\n\\nA challenge for using individually hand-crafted tasks is to cover a sufficient space of phenomena that are relevant to downstream tasks. Instead of training on a single task only, McCoy et al. (2020); McCoy & Griffiths (2023) meta-learn on a distribution of tasks using MAML (Finn et al., 2017). They show that this can be helpful for low-resource language modelling on simple English utterances (McCoy & Griffiths, 2023). Our approach also uses a distribution of tasks but it scales better than MAML-based methods because MAML requires computing and storing second-order derivatives. For example, the Transformer we train has a magnitude more parameters than the LSTM of McCoy & Griffiths (2023) and can be pre-trained on a smaller GPU (A100 vs RTX 2080 TI). In addition, as the complexity of each individual task grows, MAML requires more examples per task. We circumvent this by using a compact and unambiguous description of each task instead.\\n\\nSimulating execution.\\n\\nThe idea of using a neural network to predict the outcome of the execution of a computational device or code has come up in several contexts over the last few years. Early work by Zaremba & Sutskever (2014) investigates it as a challenging benchmark for LSTM-based seq2seq models. Recent works have explored simulating (aspects) of code execution for various down-stream applications, such as program synthesis (Austin et al., 2021), debugging and code analysis (Bieber et al., 2022) as well as reverse engineering (Pei et al., 2021). Closer to our setup, Finlayson et al. (2022) train a Transformer to interpret regular expressions: given a regular expression and a string, the task is to decide if the string is in the regular language. There are two crucial differences between their work and ours: (i) they investigate the empirical capabilities of Transformers to simulate regular expressions while we use simulation to introduce structural inductive biases for downstream tasks, and (ii) they consider binary outputs whereas we consider sequential outputs.\\n\\n3 F\\n\\nINITE\\n\\nSTATE\\n\\nTRANDUCERS\\n\\nWe briefly review Finite State Transducers (FSTs) which we use in our experiments. FSTs are closely related to Finite State Automata (FSAs). While an FSA describes a set of strings, an FST describes a relation between strings, i.e. a set of pairs \\\\((x, y)\\\\), where \\\\(x\\\\) is an input \\\\(y\\\\) is an output.\\n\\nFSTs can be visualized as labelled directed graphs (see Fig. 2), where the nodes are called states and the edges are called transitions. Consider the path \\\\(q_0 \\\\xrightarrow{a} q_1 \\\\xrightarrow{b} q_2\\\\) in Fig. 2b. This path is called an accepting path because it starts in an initial state (indicated by an arrow 'from nowhere' pointing to the state), and it ends in a final state (indicated by double circles). An accepting path shows what an input can be mapped to. In this case, the path shows that the FST transduces the input \\\\(aab\\\\) into the output \\\\(bbb\\\\). We can read off which input an accepting path associates an output to by concatenating all the strings along the path occurring before ':'. The output can be determined by concatenating the strings after ':'. Hence, each transition \\\\(\\\\sigma: \\\\rho \\\\rightarrow \\\\sigma\\\\) can be thought of as 'replacing' \\\\(\\\\sigma\\\\) by \\\\(\\\\rho\\\\). Inserting and deleting can be achieved by means of the empty string, written as \\\\(\\\\epsilon\\\\). For example, Fig. 2a 'replaces' every second \\\\(a\\\\) by an empty string, effectively deleting them.\\n\\nIn general, an input can be paired with arbitrarily many different outputs. We call an FST functional if every input \\\\(x\\\\) is paired with at most one output \\\\(y\\\\), and use the notation \\\\(f(x)\\\\) to refer to \\\\(y\\\\).\\n\\nAll FSTs we consider here are functional. We also use set notation on FSTs, e.g. if \\\\(f_1\\\\) and \\\\(f_2\\\\) are FSTs expressing relations \\\\(R_1\\\\) and \\\\(R_2\\\\), we refer to the FST expressing \\\\(R_1 \\\\cup R_2\\\\) as \\\\(f_1 \\\\cup f_2\\\\).\\n\\nIn this work, we focus mainly on deterministic FSTs, which are a less expressive sub-class of the functional FSTs that are particularly easy to generate automatically. We will use deterministic and non-deterministic FSTs to investigate generalization across different sub-classes of FSTs. An FST is called deterministic if (i) it has a unique initial state, (ii) for all states \\\\(q\\\\) and input symbols \\\\(\\\\sigma\\\\) there is at most one transition \\\\(q: \\\\rho \\\\rightarrow q'\\\\) and (iii) \\\\(\\\\sigma \\\\neq \\\\epsilon\\\\). Intuitively, this means that in any state, for an input symbol \\\\(\\\\sigma\\\\) there is at most one possible next state and one possible output, and hence for any input string there is at most one path that is compatible with it. Because of this, we can always determine a prefix of the output string by looking only at a prefix of the input string and ignoring the rest. For example, consider the input prefix \\\\(aa\\\\). In the deterministic FST in Fig. 2a, we know that the output has to start with \\\\(a\\\\) because there is only one path that is compatible with \\\\(aa\\\\). In contrast, in the non-deterministic FST in Fig. 2b, there are two paths that are compatible with \\\\(aa\\\\) that have different outputs. In that case, we can only determine the output once we look at the last symbol of the input string.\"}"}
{"id": "Oashk4fDD9", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# Test FST states\\n\\nTable 2 shows the deviation in percentage points from ByT5. We automatically generate 5 non-deterministic FSTs with 21 states (see Appendix B for details) and report averages. Despite the structural mismatch between pre-training and the downstream tasks, SIP-d4 shows clear improvements over the baselines. Interestingly, TE does not consistently outperform the other baselines, despite its stronger results on deterministic FSTs.\\n\\nOur pre-training procedure does not hinge on using deterministic FSTs. This raises the question if we can achieve even better performance by adjusting the inductive bias. To investigate this, we further pre-train SIP on 40,000 non-deterministic FSTs with up to 7 states, which we call SIP-nd7. To control for the additional training data of SIP-nd7, we also further pre-train SIP-d4 with the same number of deterministic FSTs with the same characteristics as in Section 4.3 (SIP-d4+). The results in Table 2 show better performance of SIP-nd7, which supports the hypothesis that the inductive bias can be adjusted. SIP-d4+ shows a smaller improvement over SIP-d4. Based on 5 additional FSTs per setup to gain more statistical power, we found that the difference between SIP-nd7 and SIP-d4+ is statistically significant ($p = 0.017, n = 20$, paired permutation test).\\n\\nIn this section, we investigate to what degree the inductive bias from pre-training on synthetic data transfers to tasks with natural data that have been traditionally approached with finite state methods.\\n\\n### 7.1 Low-resource Grapheme-to-Phoneme Conversion\\n\\nGrapheme-to-phoneme conversion is the task of converting a word as a sequence of symbols (for example, letters in the Latin alphabet) into a description of how this word is pronounced as letters in the IPA alphabet. For example, a possible pronunciation of 'explanation' is \\\\([-Ekspl@neIS@n]\\\\).\\n\\nGrapheme-to-phoneme conversion can be part of text-to-speech pipelines and FSTs for this purpose usually are two or three magnitudes larger than the FSTs we constructed for pre-training. Because of this, it enables us to test how far beyond the pre-training distribution SIP remains helpful. We focus on learning from small amounts of data, for which a structural inductive bias towards FSTs should be helpful. We evaluate on 7 low-resource languages from different language families that use their own scripts (Balinese, Coptic, Gothic, Lao, Sylheti, Telugu and Central Atlas Tamazight). We obtained the data from Wikipron (Lee et al., 2020).\\n\\nAs a soft upper bound, we compare with Charsiu (Zhu et al., 2022) which is a ByT5-small model that has been further pre-trained on 7.2 million examples of grapheme-to-phoneme conversion across 100 languages. Although Charsiu was not exposed to the scripts of the languages we chose, it may have seen closely related languages with an overlap in the lexicon from which it can transfer.\\n\\nThe results are in Table 3. The original ByT5-small model performs worst on average despite being a strong model for grapheme-to-phoneme conversion in general (Xue et al., 2022). On average across the languages, SIP-d4 outperforms the other methods that pre-train on synthetic data as well as ByT5. The difference between SIP-d4 and Set is statistically significant ($p = 0.0004$, paired permutation test). On Coptic, SIP-d4 even comes close to Charsiu. Fine-tuning SIP-d4 without the tunable prefix consistently leads to a drop in performance, with the exception of Gothic.\"}"}
{"id": "Oashk4fDD9", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning simple text editing tasks (Jane Doe \u2192 J. Doe) from a handful of examples with a Transformer requires a strong structural inductive bias to overcome competing explanations of the data and hence provides a good benchmark for our approach. Text editing has been studied in the context of program synthesis and we evaluate on 19 such tasks from the SyGuS competition 2017 (Alur et al., 2017). Instead of predicting a program, our model directly operates on input/output examples. We note that 17 of these tasks can be solved by compact FSTs, whereas two cannot. These two tasks are reverse-name (Jane Doe \u2192 Doe Jane) and surname-initial (John Doe \u2192 Doe, J.), which require tracking information about the first name (either in full or only the initial) in the states. We report results for 5-shot experiments in Table 4. SIP-d4 and TE excel at this, reaching well above 90% accuracy on average whereas the other methods perform worse by a large margin. Charsiu does not perform clearly better than baselines such as Set \u2013 even though it obtains excellent results on grapheme-to-phoneme conversion. Interestingly, TE performs better than SIP-d4 on the tasks that can be solved with FSTs, potentially because the initialization of the prefix for TE follows the same distribution as during pre-training, which is not the case for SIP. However, SIP considerably outperforms TE on the two tasks that cannot be compactly represented by FSTs, suggesting that some of the mechanisms acquired during pre-training can sometimes be leveraged in other contexts as well. In this case fine-tuning SIP-d4 without the tunable prefix leads only to a very small drop in accuracy on average.\\n\\n**Conclusion**\\n\\nWe present SIP, a simple and adjustable method for introducing a structural inductive bias into a seq2seq model. Specifically, we focus on an inductive bias towards FSTs, one of the simplest computational device that is useful for NLP applications. We achieve this by pre-training a Transformer to simulate FSTs, i.e. to predict the output of an FST given an input string and a representation of the FST. Our experiments show that our method imparts the desired inductive bias, resulting in improved systematic generalization and better few-shot learning for FST-like tasks. In future work, we plan to extend this methodology to more expressive formalisms such as Pushdown Transducers which can be used for a wider range of downstream NLP tasks.\"}"}
{"id": "Oashk4fDD9", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We release our code for generating synthetic data and running all experiments as supplementary material, and will put it on github upon publication. The supplementary material also contains the preprocessed natural data needed for reproducing our experiments as well as spreadsheets with raw experimental results, each with the random seed and the configuration (including hyperparameters) that was used. Moreover, we describe additional details about our procedure to generate the deterministic FSTs in Appendix A (including pseudocode), how we generate non-deterministic FSTs in Appendix B, and provide additional information about the model setup and hardware in Appendix F.\\n\\nUpon publication, we will also release our pre-trained model as it is difficult to provide this anonymously.\\n\\nREFERENCES\\n\\nCyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. Openfst: A general and efficient weighted finite-state transducer library: (extended abstract of an invited talk). In Implementation and Application of Automata: 12th International Conference, CIAA 2007, Prague, Czech Republic, July 16-18, 2007, Revised Selected Papers 12, pp. 11\u201323. Springer, 2007.\\n\\nRajeev Alur, Dana Fisman, Rishabh Singh, and Armando Solar-Lezama. Sygus-comp 2017: Results and analysis. arXiv preprint arXiv:1711.11438, 2017.\\n\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\\n\\nDavid Bieber, Rishab Goel, Dan Zheng, Hugo Larochelle, and Daniel Tarlow. Static prediction of runtime errors by learning to execute programs with external resource descriptions. In Deep Learning for Code Workshop, 2022. URL https://openreview.net/forum?id=SIcz2sObJ-5.\\n\\nGregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A Ortega. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=WbxHAzkeQcn.\\n\\nNouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D Hwang, et al. Faith and fate: Limits of transformers on compositionality. arXiv preprint arXiv:2305.18654, 2023. URL https://arxiv.org/abs/2305.18654.\\n\\nCatherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadashivam, Rui Zhang, and Dragomir Radev. Improving text-to-SQL evaluation methodology. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 351\u2013360, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1033. URL https://aclanthology.org/P18-1033.\\n\\nMatthew Finlayson, Kyle Richardson, Ashish Sabharwal, and Peter Clark. What makes instruction learning hard? an investigation and a new challenge in a synthetic environment. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 414\u2013426, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.27.\\n\\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pp. 1126\u20131135. PMLR, 2017.\\n\\nDaniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Sch\u00e4li. Compositional generalization in semantic parsing: Pre-training vs. specialized architectures. arXiv preprint arXiv:2007.08970, 2020.\\n\\nYinuo Guo, Zeqi Lin, Jian-Guang Lou, and Dongmei Zhang. Hierarchical poset decoding for compositional generalization in language. Advances in Neural Information Processing Systems, 33:6913\u20136924, 2020.\"}"}
{"id": "Oashk4fDD9", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: how do neural networks generalise? Journal of Artificial Intelligence Research, 67:757\u2013795, 2020. URL https://www.jair.org/index.php/jair/article/view/11674.\\n\\nDaniel Keysers, Nathanael Sch\u00e4rli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, and Olivier Bousquet. Measuring compositional generalization: A comprehensive method on realistic data. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SygcCnNKwr.\\n\\nNajoung Kim and Tal Linzen. COGS: A compositional generalization challenge based on semantic interpretation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9087\u20139105, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.731. URL https://aclanthology.org/2020.emnlp-main.731.\\n\\nYoon Kim. Sequence-to-sequence learning with latent neural grammars. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 26302\u201326317. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/dd17e652cd2a08fdb8bf7f68e2ad3814-Paper.pdf.\\n\\nKundan Krishna, Jeffrey Bigham, and Zachary C. Lipton. Does pretraining for summarization require knowledge transfer? In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 3178\u20133189, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.273. URL https://aclanthology.org/2021.findings-emnlp.273.\\n\\nBrenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International Conference on Machine Learning, pp. 2873\u20132882. PMLR, 2018. URL http://proceedings.mlr.press/v80/lake18a/lake18a.pdf.\\n\\nJackson L. Lee, Lucas F.E. Ashby, M. Elizabeth Garza, Yeonju Lee-Sikka, Sean Miller, Alan Wong, Arya D. McCarthy, and Kyle Gorman. Massively multilingual pronunciation modeling with WikiPron. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 4223\u20134228, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-1.521.\\n\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045\u20133059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243.\\n\\nYafu Li, Yongjing Yin, Yulong Chen, and Yue Zhang. On compositional generalization of neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4767\u20134780, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.368. URL https://aclanthology.org/2021.acl-long.368.\\n\\nMatthias Lindemann, Alexander Koller, and Ivan Titov. Compositional generalization without trees using multiset tagging and latent permutations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14488\u201314506, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.810. URL https://aclanthology.org/2023.acl-long.810.\\n\\nR Thomas McCoy and Thomas L Griffiths. Modeling rapid language learning by distilling bayesian priors into artificial neural networks. arXiv preprint arXiv:2305.14701, 2023. URL https://arxiv.org/abs/2305.14701.\\n\\nR Thomas McCoy, Erin Grant, Paul Smolensky, Thomas L Griffiths, and Tal Linzen. Universal linguistic inductive biases via meta-learning. In Proceedings of the 42nd Annual Conference of the Cognitive Science Society, 2020. URL https://arxiv.org/abs/2006.16324.\"}"}
