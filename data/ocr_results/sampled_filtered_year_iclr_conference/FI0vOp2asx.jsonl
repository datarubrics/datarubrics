{"id": "FI0vOp2asx", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We empirically find that collaborate such amount of clients can be problematic for popular federated learning methods. By comparison, FedProx and SCAFFOLD only allows sub-optimal performance, thereby, we integrate this method into the federated learning framework, dubbed as FedGST. Notably, all the compared methods require to train and aggregate the client backbones without a prompter or adaptor introduced. By comparison, FedHP updates and shares the consistency of different methods to the ground truth. We use the same coded aperture for all methods.\\n\\nTable 1: PSNR(dB)/SSIM performance comparison. For different clients, we sample non-overlapping masks from the same mask distribution to train the model and use unseen masks randomly sampled toward multiple hardware. Thereby, we set the number of clients as 3, i.e., $C = 3$. FedHP performs better than the classic federated learning methods. We compare the proposed FedHP with mainstream federated learning methods, we update all clients throughout the training, i.e., $C = 9$. We set the initial learning rate for both of the prompt network and adaptor as $\\\\alpha = 0.0002$ with step schedulers, half annealing every 2 iterations on their local data. Notably, we quantitatively compare different methods in Table 1 by considering the performance of different wavelengths. Figure 3: Reconstruction results on simulation data. The density curves compare the spectral consistency of different methods to the ground truth. We use the same coded aperture for all methods.\"}"}
{"id": "FI0vOp2asx", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We conduct model discussion in Table 3. Specifically, we accumulate the total cost (Ablation Study) we visualize the reconstruction results with sampled wavelengths. FedHP not only enables a more granular retrieval on unseen coded aperture, but also maintains a promising spectral consistency. The proposed method introduces less distortions among different wavelengths. Both methods are evaluated under an unseen hardware configuration, i.e., coded aperture from an uncertain distribution. The proposed method experiences large performance degradation, among which FedProx and SCAFFOLD becomes nearly ineffective. Intuitively, it is hard to concord the clients under the large distribution gap, while FedHP works superior than FedGST. FedGST approximates the posterior and expects coded aperture strictly follows the identical distribution, which can not be guaranteed in practice. In Fig. 3, the hardware prompter aligns the input data space better tackles the problem. We use the same unseen coded aperture for both FedAvg and FedHP. Figure 4: Visualization of reconstruction results on real data. Six representative wavelengths are figure drawn from all distributions for testing. We report distribution for training. We randomly sample non-overlapping masks (unseen to training) from each client are sampled from a specific input data distributions, potentially solving the heterogeneity rooted in the input data space. This is determined by the fact that learning manifold is highly correlated with the coded apertures. Using prompt network brings significant performance boost. The hardware prompter aligns the input data space more closely with the reference, but also maintains a promising spectral consistency. We compare different methods in Table 2. By observation, all methods experience large performance degradation, among which FedProx and SCAFFOLD becomes nearly ineffective. Intuitively, it is hard to concord the clients under the large distribution gap, while FedHP works superior than FedGST. FedGST approximates the posterior and expects coded aperture strictly follows the identical distribution, which can not be guaranteed in practice. In Fig. 3, the hardware prompter aligns the input data space better tackles the problem. We use the same unseen coded aperture for both FedAvg and FedHP.\"}"}
{"id": "FI0vOp2asx", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Ablation study and complexity analysis under the Hardware shaking. The PSNR (dB)/SSIM are computed among 100 testing trials. We report the number of learnable parameters of different methods and report the accumulative training time of all clients (e.g., \\\\(C = 3\\\\)).\\n\\n| Method                      | Prompter Adaptor FL | PSNR \u00b1 | SSIM \u00b1 | #Params (M) | GMACs | Training (days) |\\n|-----------------------------|---------------------|--------|--------|-------------|-------|-----------------|\\n| FedAvg                      | \u2717                   | 31.21  | \u00b1 0.10 | 0.8959 \u00b1 0.0017 | 0.12  | 2.85           |\\n| FedHP w/o FL                | \u2713                   | 30.75  | \u00b1 0.11 | 0.8890 \u00b1 0.0015 | 0.27  | 12.78          |\\n| FedHP w/o Adaptor           | \u2717                   | 31.09  | \u00b1 0.10 | 0.8996 \u00b1 0.0017 | 0.15  | 11.01           |\\n| FedHP w/o Prompter          | \u2713                   | 19.19  | \u00b1 0.01 | 0.2303 \u00b1 0.0008 | 0.12  | 2.87            |\\n| FedHP (Full model)          | \u2713                   | 31.35  | \u00b1 0.10 | 0.9033 \u00b1 0.0014 | 0.27  | 12.78          |\\n\\nTable 4: Model discussions of the proposed FedHP.\\n\\n(a) #Client discussion. Averaged values are reported.\\n\\n|          | FedAvg | FedHP | Performance gap |\\n|----------|--------|-------|-----------------|\\n| #Client  | PSNR   | SSIM  |                 |\\n| 4        | 31.06  | 0.8955| 31.33 0.9023    |\\n| 5        | 31.05  | 0.9025| 31.32 0.9029    |\\n\\n(b) Comparison with a deep Unfolding method.\\n\\n| Methods   | PSNR (dB) \u00b1 | SSIM (\u00b1) | #Params (M) | GMACs | Training (days) |\\n|-----------|-------------|----------|-------------|-------|-----------------|\\n| GAP-Net   | 31.07 \u00b1 0.20 | 0.8895 \u00b1 0.0035 | 3.83       |       |                 |\\n| FedHP     | 31.35 \u00b1 0.10 | 0.9033 \u00b1 0.0014 | 0.27       |       |                 |\\n\\nComparison with a deep unfolding method. We also compare the proposed FedHP with a representative deep unfolding method of GAP-Net (Meng et al., 2023) as deep unfolding methods can be adaptable to various hardware configurations. Specifically, we use three clients and keep training and testing settings of GAP-Net the same as FedHP. As shown in Table 4b, FedHP improves by 0.28 dB with only 7% model size. In fact, despite the adaptability, deep unfolding still shows limitations in solving hardware perturbation/replacement for a given system as discussed in Wang et al. (2022).\\n\\nHyperspectral Image Reconstruction. In hyperspectral image reconstruction (HSI), learning deep reconstruction models (Cai et al., 2022a;b; Lin et al., 2022; Huang et al., 2021; Meng et al., 2020; Hu et al., 2022; Miao et al., 2019) has been the forefront among recent efforts due to high-fidelity reconstruction and high-efficiency. Among them, MST (Cai et al., 2022a) devises the first transformer backbone by computing spectral attention. By observation, existing reconstruction learning strategies mainly consider the compatibility toward a single hardware instance. The learned model can be highly sensitive to the variation of hardware. To tackle this practical challenge, GST (Wang et al., 2022) paves the way by proposing a variational Bayesian learning treatment.\\n\\nFederated Learning. Federated learning (Kairouz et al., 2021; Li et al., 2020a; Wang et al., 2021) collaborates client models without sharing the privacy-sensitive assets. However, FL learning suffers from client drift across clients attributing to the data heterogeneity issue. One mainstream (Karimireddy et al., 2020; Li et al., 2020b; Xu et al., 2021; Jhunjhunwala et al., 2023; Reddi et al., 2021) mainly focus on regularizing the global/local gradients. As another direction, personalized FL methods (Collins et al., 2021; Chen & Chao, 2022; Fallah et al., 2020; T Dinh et al., 2020; Jiang & Lin, 2023) propose to fine-tune the global model for better adaptability on clients. However, customizing the global model on client data sacrifices the underlying robustness upon data distribution shift (Wu et al., 2022; Jiang & Lin, 2023), which contradicts with our goal of emphasizing the generality across hardware and thus is not considered. In this work, we propose a federated learning framework to solve the multi-hardware cooperative learning considering the data privacy and heterogeneity, which to the best knowledge, is the first attempt of empowering spectral SCI with FL. Besides, the principle underlying this method can be potentially extended to broad computational imaging applications (Zheng et al., 2021; Liu et al., 2023a; Goudreault et al., 2023; Robidoux et al., 2021).\\n\\nCONCLUSIONS\\n\\nIn this work, we observed an unexplored research scenario of multiple hardware cooperative learning in spectral SCI, considering two practical challenges of data proprietary constraint and heterogeneity stemming from the inconsistent hardware configurations. We developed Federated Hardware-Prompt (FedHP) learning framework to solve the distribution shift across clients and empower the hardware-software co-optimization. The proposed method serves as a first attempt of exploiting the power of FL in spectral SCI. Besides, we collect a Snapshot Spectral Heterogeneous Dataset (SSHD) from multiple real spectral SCI systems. Future works may theoretically derive the convergence of FedHP and exploit the behavior of FedHP under a large number of clients (e.g., >100). We hope this work will inspire future works in this novel direction of hardware collaboration in SCI.\"}"}
{"id": "FI0vOp2asx", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"REFERENCES\\n\\nHyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Visual prompting: Modifying pixel space to adapt pre-trained models. arXiv preprint arXiv:2203.17274, 2022.\\n\\nYuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool. Mask-guided spectral-wise transformer for efficient hyperspectral image reconstruction. In CVPR, 2022a.\\n\\nYuanhao Cai, Jing Lin, Haoqian Wang, Xin Yuan, Henghui Ding, Yulun Zhang, Radu Timofte, and Luc Van Gool. Degradation-aware unfolding half-shuffle transformer for spectral compressive imaging. In NeurIPS, 2022b.\\n\\nHong-You Chen and Wei-Lun Chao. On bridging generic and personalized federated learning for image classification. In ICLR, 2022.\\n\\nInchang Choi, MH Kim, D Gutierrez, DS Jeon, and G Nam. High-quality hyperspectral reconstruction using a spectral prior. Technical report, 2017.\\n\\nLiam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In ICML, 2021.\\n\\nAlireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. In NeurIPS, 2020.\\n\\nMichael E Gehm, Renu John, David J Brady, Rebecca M Willett, and Timothy J Schulz. Single-shot compressive spectral imaging with a dual-disperser architecture. Optics express, 15(21):14013\u201314027, 2007.\\n\\nF\u00e9lix Goudreault, Dominik Scheuble, Mario Bijelic, Nicolas Robidoux, and Felix Heide. Lidar-in-the-loop hyperparameter optimization. In CVPR, 2023.\\n\\nTzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.\\n\\nXiaowan Hu, Yuanhao Cai, Jing Lin, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool. Hdnet: High-resolution dual-domain learning for spectral compressive imaging. In CVPR, 2022.\\n\\nTao Huang, Weisheng Dong, Xin Yuan, Jinjian Wu, and Guangming Shi. Deep gaussian scale mixture prior for spectral compressive imaging. In CVPR, 2021.\\n\\nDivyansh Jhunjhunwala, Shiqiang Wang, and Gauri Joshi. Fedexp: Speeding up federated averaging via extrapolation. In ICLR, 2023.\\n\\nLiangze Jiang and Tao Lin. Test-time robust personalization for federated learning. In ICLR, 2023.\\n\\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends\u00ae in Machine Learning, 14(1\u20132):1\u2013210, 2021.\\n\\nSai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In ICML, 2020.\\n\\nAhmed Khaled, Konstantin Mishchenko, and Peter Richt\u00e1rik. Tighter theory for local sgd on identical and heterogeneous data. In ICAIS, 2020.\\n\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nTian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. IEEE signal processing magazine, 37(3):50\u201360, 2020a.\"}"}
{"id": "FI0vOp2asx", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this work, we study the cooperative learning problem in SCI by taking the representative setup of coded aperture snapshot spectral imaging system for hyperspectral imaging as an example, due to its recent advances (Cai et al., 2022a;b; Lin et al., 2022). Given the real-world hyperspectral signal $X \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times N_{\\\\lambda}}$, where $N_{\\\\lambda}$ denotes the number of spectral channels, the hardware performs the compression with the physical coded aperture $M$ of the size $H \\\\times W$, i.e., $M_{hw} \\\\in [0, 1]$. Accordingly, the encoding process produces a 2D measurement $Y$ $M \\\\in \\\\mathbb{R}^{H \\\\times (W + \\\\Delta)}$, where $\\\\Delta$ denotes the shifting.\\n\\n$Y_{M} = \\\\sum_{n_{\\\\lambda}=1}^{N_{\\\\lambda}} X_{(h, w, n_{\\\\lambda})} \\\\odot M + \\\\Omega,$  \\n\\n(1)\\n\\nwhere $\\\\odot$ denotes the pixel-wise multiplication and $\\\\Omega$ presents the measurement noise. For each spectral wavelength $\\\\lambda$, the corresponding signal $X_{(h, w, n_{\\\\lambda})}$ is shifted according to the function $d(\\\\lambda - \\\\lambda_{*})$ by referring to the pre-defined anchor wavelength $\\\\lambda_{*}$, such that $\\\\Delta = d(N_{\\\\lambda} - 1)$. Following the optical encoder, recent practices train a deep reconstruction network $f(\\\\cdot)$ to retrieve the hyperspectral data $bX \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times N_{\\\\lambda}}$ by taking the 2D measurement $Y$ $M$ as input. We define the initial training dataset of the hyperspectral signal as $D$ and the corresponding dataset for the reconstruction as $D_{M}$.\\n\\n$D = \\\\{X_{i}\\\\}_{i=1}^{N}, D_{M} = \\\\{Y_{M}^{\\\\ast}i, X_{i}\\\\}_{i=1}^{N},$  \\n\\n(2)\\n\\nwhere $X_{i}$ serves as the ground truth and $Y_{M}^{\\\\ast}i$ is governed by a specific real physical coded aperture $M^{\\\\ast}$. The reconstruction model finds the local optimum by minimizing the mean squared loss $b_{\\\\theta} = \\\\arg \\\\min_{\\\\theta} \\\\frac{1}{N\\\\sum_{i=1}^{N}} \\\\|f(\\\\theta; Y_{M}^{\\\\ast}i) - X_{i}\\\\|_{2}^{2},$  \\n\\n(3)\\n\\nwhere $\\\\theta$ expresses all learnable parameters in the reconstruction model. Thanks to the sophisticated network designs (Cai et al., 2022a; Huang et al., 2021), pre-trained reconstruction model demonstrates promising performance when is compatible with a single encoder set-up, where the measurement in training and testing phases are produced by the same hardware using a fixed coded aperture of $M^{\\\\ast}$.\\n\\nMotivation. However, previous work (Wang et al., 2022) uncovered that most existing reconstruction models experience large performance descent (e.g., $>2$ dB in terms of PSNR) when handling the input measurements encoded by a different coded aperture $M^{\\\\dagger}$ from training, i.e., $M^{\\\\dagger} \\\\neq M^{\\\\ast}$. This is because the coded aperture $M^{\\\\ast}$ implicitly affects the learning as shown by equation 3. To this end, the well-trained reconstruction model can be highly sensitive to a specific hardware configuration and is hardly compatible with the other optical systems in the testing phase. A simple solution of adapting the reconstruction network to a different coded aperture $M^{\\\\dagger}$ is to retrain the model with corresponding dataset $D_{M}^{\\\\dagger} = \\\\{Y_{M}^{\\\\dagger}i, X_{i}\\\\}_{i=1}^{N}$ and then test upon $M^{\\\\dagger}$ accordingly.\\n\\nHowever, this solution does not broaden the adaptability of reconstruction models to multi-hardware and can introduce drastic computation overhead. In this work, we tackle this challenge by learning a reconstruction model cooperatively from multiple hardware with inconsistent configurations.\\n\\n2.2 Centralized Learning in SCI\\n\\nJointly Train. To solve the above problem, Jointly train (Fig. 1 part 2) serves as a naive solution to train a model with data jointly collected upon a series of hardware. Assuming there are total number of $K$ hardware with different coded apertures, i.e., $M_{1}, M_{2}, \\\\ldots, M_{K}$. Each hardware produces a training dataset upon $D$ as $D_{M_{k}} = \\\\{Y_{M_{k}}i, X_{i}\\\\}_{i=1}^{N}$. The joint training dataset for reconstruction is $D_{M_{1}} \\\\sim K = D_{M_{1}} \\\\cup D_{M_{2}} \\\\cup \\\\ldots \\\\cup D_{M_{K}},$  \\n\\n(4)\\n\\nwhere different coded apertures can be regarded as hardware-driven data augmentation treatments toward the hyperspectral data representation. The reconstruction model will be trained with the same mean squared loss provided in equation 3 upon $D_{M_{1}} \\\\sim K$. Wang et al. (2022) demonstrated that jointly learning brings performance boost compared with single mask training (Fig. 1 right). However, this method adopts a single well-trained model to handle coded apertures, failing to adaptively cope with the underlying discrepancies and thus, leading to compromised performances for different hardware.\"}"}
{"id": "FI0vOp2asx", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Self-tuning. Following Jointly train, recent work of Self-tuning (Wang et al., 2022) recognizes the coded aperture that plays the role of hyperprameter of the reconstruction network, and develops a hyper-net to explicitly model the posterior distribution of the coded aperture by observing $D_1 \\\\sim K$. Specifically, the hyper-net $h(\\\\sigma; M_k)$ approximates $P(M | D_1 \\\\sim K)$ by minimizing the Kullback\u2013Leibler divergence between this posterior and a variational distribution $Q(M)$ parameterized by $\\\\sigma$. Compared with Jointly train, Self-tuning learns to adapt to different coded apertures and appropriately calibrates the reconstruction network during training, even if there are unseen coded apertures. However, the variational Bayesian learning poses a strict distribution constraint to the sampled coded apertures, which limits the scope of Self-tuning under the practical setting.\\n\\nTo sum up, both of the Jointly train and Self-tuning are representative solutions of centralized learning, where the dataset $D$ and hardware instances with $M_1, \\\\ldots, M_K$ from different sources are presumed to be publicly available. Such a setting has two-fold limitations. (1) Centralized learning does not take the privacy concern into consideration. In practice, both the hardware instances and hyperspectral dataset are proprietary assets of institutions and thus, corresponding hardware configuration and data information sharing is subject to the rigorous policy constraint. (2) Existing centralized learning methods mainly consider the scenario where coded apertures are sampled from the same distribution, i.e., hardware origin from the same source, which is problematic when it comes to the coded aperture distribution inconsistency especially in the cross-silo case. Bearing the above challenges, in the following, we resort to the federated learning (FL) methods to solve the cooperative learning of reconstruction considering the privacy and hardware configuration inconsistency.\\n\\n2.3 FEDERATED LEARNING IN SCI\\n\\nFedAvg. We firstly tailor FedAvg (McMahan et al., 2017), into SCI. Specifically, we exploit a practical setting of cross-silo learning in snapshot compressive imaging. Suppose there are $C$ clients, where each client is packaged with a group of hardware following a specific distribution of $P_c$. Specifically, $M_{ck} \\\\sim P_c$, (5) where $M_{ck}$ represents the $k$-th sampled coded aperture in $c$-th client. For simplicity, we use $M_c$ to denote arbitrary coded aperture sample in $c$-th client. Based on the hardware, each client computes a paired dataset $D_{M_c}$ from the local hyperspectral dataset $D_c$, $D_c = \\\\{X_i\\\\}_{i=1}^{N_c}$, $D_{M_c} = \\\\{Y_{M_c i}, X_i\\\\}_{i=1}^{N_c}$, $M_c \\\\sim P_c$, (6) where $N_c$ represents the number of hyperspectral data in $D_c$. The local learning objective is $\\\\ell_c(\\\\theta) = \\\\frac{1}{N}\\\\sum_{i=1}^{N} ||b_{X_i} - X_i||^2_2$, (7) where $b_{X_i} = f(b_{\\\\theta}; Y_{M_c i})$, $M_c \\\\sim P_c$, we use $\\\\theta$ to denote the learnable parameters of reconstruction model at a client. FedAvg learns a global model $\\\\theta_G$ without sharing the hyperspectral signal dataset $D_c$, $D_{M_c}$, and $M_c$ across different clients. Specifically, the global learning objective $\\\\ell_G(\\\\theta)$ is $\\\\ell_G(\\\\theta) = \\\\sum_{c=1}^{C'} \\\\alpha_c \\\\ell_c(\\\\theta)$, (8) where $C'$ denotes the number of clients that participate in the current global round and $\\\\alpha_c$ represents the aggregation weight. Compared with the centralized learning solutions, FedAvg not only bridges the local hyperspectral data without sharing sensitive information, but also collaborates multi-hardware with a unified reconstruction model for a better performance (Fig. 1 right comparison between 3 and 4). However, FedAvg shows limitations in two-folds. (1) It has been shown that FedAvg is hard to handle the heterogeneous data (Karimireddy et al., 2020; Khaled et al., 2020; Hsu et al., 2019). (2) Directly training and aggregating the reconstruction backbones from scratch would introduce prohibitive computation. In the following, we firstly introduce the hardware-induced data heterogeneity in SCI. Then we develop a Federated Hardware-Prompt (FedHP) method to achieve cooperative learning without optimizing the client backbones.\\n\\nData Heterogeneity. We firstly consider the data heterogeneity stems from the different coded aperture samples, i.e., hardware instances. According to Section 2.1, the optical hardware samples the hyperspectral signal $X_i$ from $D = \\\\{X_i\\\\}_{i=1}^{N}$ and encodes it into a 2D measurement $Y_{M_i}$, which...\"}"}
{"id": "FI0vOp2asx", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Learning process of FedHP. We take one global round as an example, which consists of (1) Initialize, (2) Local Update (Prompt), (3) Local Update (Adaptor), and (4) Aggregation. For each client, the reconstruction backbone ($\\\\theta_p$), is initialized as pre-trained model upon local training dataset $D_c$ and kept as frozen throughout the training. The prompt net upon hardware configuration, i.e., coded aperture, takes effect on the input data of reconstruction, i.e., $Y_M$. Adaptors are introduced to enhance the learning, where $\\\\epsilon_c$ denotes the parameters of all adaptors. Constitutes $D_M$ and further serves as the input data for the reconstruction model. To this end, the modality of $\\\\{Y_M^i\\\\}_{i=1}^N$ is vulnerable to the coded aperture variation. A single coded aperture $M$ defines a unique input data distribution for the reconstruction, i.e., $Y_M^i \\\\sim P_M(Y_M^i)$. For arbitrary distinct coded apertures, we have $P_M^* (Y_M^i) \\\\neq P_M^\\\\dagger (Y_M^i)$ if $M^* \\\\neq M^\\\\dagger$. In federated learning, data heterogeneity persistently exists since there is no identical coded aperture across different clients. We name this heterogeneous scenario as Hardware shaking, which potentially can attribute to physical perturbations such as lightning distortion or optical platform fluttering. We take a step further to consider the other type of data heterogeneity stemming from the distinct distributions of coded apertures $1$. As formulated in equation 6, each client collects a coded aperture assemble following the distribution $P_c$ for $c$-th client. We have $P_c$ differs from one another, i.e., $P_c^1 \\\\neq P_c^2$ for $c_1 \\\\neq c_2$, $c_1, c_2 \\\\in \\\\{1, \\\\ldots, C\\\\}$. We name this heterogeneous scenario as Manufactory discrepancy, where hardware instances from different clients (institutions) are produced by distinct manufacturing agencies, so that the distribution $P_c^1$ and $P_c^2$ drastically differs as demonstrated in Fig. 1. This turns out to be a more challenging scenario than Hardware shaking. As presented in Section 3.2, classic federated learning methods, e.g., FedProx (Li et al., 2020b) and SCAFFOLD (Karimireddy et al., 2020) hardly converge in this setting. By comparison, the proposed method enables an obvious performance boost.\"}"}
{"id": "FI0vOp2asx", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of all clients, and jointly learns to react to different hardware settings. The prompter regularizes the input data space and achieves the goal of coping with heterogeneity sourcing from hardware.\\n\\nTraining. As shown in Fig. 2, we demonstrate the training process of proposed FedHP by taking one global round as an example. Since the prompt learning takes effect on pre-trained models, we initialize the $c$-th backbone parameters with the pre-trained model $\\\\theta_p^c$ on local data $D_M^c$ with equation 7. The global prompt network $\\\\phi^G$ is randomly initialized and distributed to the $c$-th client $\\\\phi^c \\\\leftarrow \\\\phi^G$, $c = 1, \\\\ldots, C'$, (10) where $\\\\phi^c$ is the local prompt network and $C'$ denotes the number of clients participated in current global round. To enable better response of the pre-trained backbone toward the aligned input data space, we also introduce the adaptors into the transformer backbone. As shown in Fig. 2 Step 3, we show the architecture of the proposed adaptor, which is a CONV-GELU-CONV structure governed by a residual connection. We insert the adaptors behind the LN layers throughout the network.\\n\\nWe then perform local update in each global round. It is composed of two stages. Firstly, we update the local prompt network $\\\\phi^c$ for $S_p$ iterations, all the other learnable parameters of backbone and adaptors are frozen. The loss then becomes $\\\\ell^c = \\\\frac{1}{N} \\\\sum_{i=1}^{N} ||f(\\\\theta^p_c, \\\\epsilon^c; Y^M_i + \\\\Phi(M^i)) - X_i||^2_2$, (11) where we use $\\\\epsilon^c$ to represent learnable parameters of all adaptors for $c$-th client. Secondly, we tune the adaptors for another $S_b$ iterations. Both of the pre-trained backbone and prompt network are frozen. The loss of $c$-th client shares the same formulation as equation 11. After the local update, FedHP uploads and aggregates the learnable parameters $\\\\phi^c$, $c = 1, \\\\ldots, C$ of prompt network. Since the proposed method does not require to optimize and communicate the reconstruction backbones, the underlying cost is drastically reduced considering the marginal model size of prompt network and adaptors compared with the backbone, which potentially serves as a supplied benefit of FedHP.\\n\\nCompared with FedAvg, FedHP adopts the hardware prompt to explicitly align the input data representation and handle the distribution shift attributing to the coded aperture inconsistency (hardware shaking) or coded aperture distribution discrepancy (manufacturing discrepancy).\\n\\n3 EXPERIMENTS\\n\\n3.1 IMPLEMENTATION DETAILS\\n\\nDataset. Following existing practices (Cai et al., 2022b; Lin et al., 2022; Hu et al., 2022; Huang et al., 2021), we adopt the benchmark training dataset of CA VE (Yasuma et al., 2010), which is composed of 32 hyperspectral images with the spatial size as $512 \\\\times 512$. Data augmentation techniques of rotation, flipping are employed, producing 205 different training scenes. For the federated learning, we equally split the training dataset according to the number of clients $C$. The local training dataset are kept and accessed confidentially across clients. Note that one specific coded aperture determines a unique dataset according to equation 2, the resulting data samples for each client can be much more than $205/C$. We employ the widely-used simulation testing dataset for the quantitative evaluation, which consists of ten $256 \\\\times 256 \\\\times 28$ hyperspectral images collected from KAIST (Choi et al., 2017). Besides, we use the real testing data with spatial size of $660 \\\\times 660$ collected by a SD-CASSI system (Meng et al., 2020) for the perceptual evaluation considering the real-world perturbations.\\n\\nHardware. We collect and will release the first Snapshot Spectral Heterogeneous Dataset (SSHD) containing a series of practical SCI systems, e.g., CASSI, from three agencies, each of which offers a series of coded apertures that correspond to a unique distribution as presented by federated settings in Fig. 2. No identical coded apertures exists among all systems. For the case of manufacturing discrepancy, we directly assign hardware systems from one source to form a client. We simulate the scenario of hardware shaking by distributing coded apertures from one source to different clients.\\n\\nImplementation details. We adopt the popular transformer backbone, MST-S (Cai et al., 2022a) for the reconstruction. Besides, the prompt network is instantiated by a SwinIR (Liang et al., 2021) Due to the limited space, we provide an algorithm of FedHP in supplementary. More illustrations and distribution visualizations of real collected coded apertures are in supplementary.\"}"}
{"id": "FI0vOp2asx", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Federated optimization in heterogeneous networks. In *MLSys*, 2020b. 2, 5, 7, 8, 9\\n\\nYijing Li, Xiaofeng Tao, Xuefei Zhang, Junjie Liu, and Jin Xu. Privacy-preserved federated learning for autonomous driving. *IEEE Transactions on Intelligent Transportation Systems*, 23(7):8423\u20138434, 2021. 2\\n\\nJingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. SwinIR: Image restoration using swin transformer. In *ICCV*, 2021. 6\\n\\nJing Lin, Yuanhao Cai, Xiaowan Hu, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool. Coarse-to-fine sparse transformer for hyperspectral image reconstruction. In *ECCV*, 2022. 1, 3, 6, 9\\n\\nJiaming Liu, Rushil Anirudh, Jayaraman J Thiagarajan, Stewart He, K Aditya Mohan, Ulugbek S Kamilov, and Hyojin Kim. Dolce: A model-based probabilistic diffusion framework for limited-angle CT reconstruction. In *ICCV*, 2023a. 9\\n\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. *ACM Computing Surveys*, 55(9):1\u201335, 2023b. 5\\n\\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In *AISTATS*, 2017. 2, 4, 7\\n\\nZiyi Meng, Jiawei Ma, and Xin Yuan. End-to-end low cost compressive spectral imaging with spatial-spectral self-attention. In *ECCV*, 2020. 1, 6, 9\\n\\nZiyi Meng, Xin Yuan, and Shirin Jalali. Deep unfolding for snapshot compressive imaging. *International Journal of Computer Vision*, pp. 1\u201326, 2023. 9\\n\\nXin Miao, Xin Yuan, Yunchen Pu, and Vassilis Athitsos. \\\\(\\\\lambda\\\\)-net: Reconstruct hyperspectral images from a snapshot measurement. In *ICCV*, 2019. 1, 9\\n\\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In *NeurIPS 2017 Workshop on Autodiff*, 2017. 7\\n\\nSashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone\u010dn\u00fd, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. In *ICLR*, 2021. 9\\n\\nNicolas Robidoux, Luis E Garcia Capel, Dong-eun Seo, Avinash Sharma, Federico Ariza, and Felix Heide. End-to-end high dynamic range camera pipeline optimization. In *CVPR*, 2021. 2, 9\\n\\nCanh T Dinh, Nguyen Tran, and Josh Nguyen. Personalized federated learning with Moreau envelopes. In *NeurIPS*, 2020. 9\\n\\nIdalides J Vergara-Laurens, Luis G Jaimes, and Miguel A Labrador. Privacy-preserving mechanisms for crowdsensing: Survey and research challenges. *IEEE Internet of Things Journal*, 4(4):855\u2013869, 2016. 2\\n\\nJiamian Wang, Yulun Zhang, Xin Yuan, Ziyi Meng, and Zhiqiang Tao. Modeling mask uncertainty in hyperspectral image reconstruction. In *ECCV*, 2022. 1, 2, 3, 4, 7, 8, 9\\n\\nJianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated optimization. *arXiv preprint arXiv:2107.06917*, 2021. 2, 9\\n\\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. *IEEE transactions on image processing*, 13(4):600\u2013612, 2004. 7\\n\\nShanshan Wu, Tian Li, Zachary Charles, Yu Xiao, Ziyu Liu, Zheng Xu, and Virginia Smith. Motley: Benchmarking heterogeneity and personalization in federated learning. In *NeurIPS*, 2022. 9\"}"}
{"id": "FI0vOp2asx", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "FI0vOp2asx", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nSpectral snapshot compressive imaging (Spectral SCI) applies an optical encoder to compressively capture 2D measurements, followed by which the 3D hyperspectral data can be restored via training a deep reconstruction network. Existing reconstruction models are generally trained with a single well-calibrated hardware instance, making their performance vulnerable to hardware shifts and limited in adapting to multiple hardware configurations. To facilitate cross-hardware learning, previous efforts attempt to directly collect multi-hardware data and perform centralized training, which, however, is impractical due to severe user data privacy concerns and hardware heterogeneity across different platforms/institutions. In this study, we explicitly consider data privacy and heterogeneity in cooperatively optimizing spectral SCI systems by proposing a novel Federated Hardware-Prompt learning (FedHP) framework. Rather than mitigating the client drift by rectifying the gradients, which only takes effect on the learning manifold but fails to solve the heterogeneity rooted in the input data space, FedHP learns a hardware-conditioned prompter to align inconsistent data distribution across clients, serving as an indicator of the data inconsistency among different coded apertures. Extensive experiments demonstrate that the proposed FedHP coordinates the pre-trained model to multiple hardware configurations, outperforming prevalent FL frameworks for $0.35\\\\,\\\\text{dB}$ under challenging heterogeneous setting. Moreover, a new Snapshot Spectral Heterogeneous Dataset (SSHD) has been built upon multiple practical spectral SCI systems. We will release the data and code to enrich further exploration of this practical computational imaging problem.\\n\\nIntroduction\\n\\nThe technology of snapshot compressive imaging (SCI) (Yuan et al., 2021) has gained prominence in the realm of computational imaging. Taking an example of hyperspectral image reconstruction, the spectral SCI (Gehm et al., 2007) can fast capture and compress 3D hyperspectral signals as 2D measurements through optical hardware, and then restore the original signals with high fidelity by training deep neural networks (Meng et al., 2020; Miao et al., 2019). Despite the remarkable performance (Cai et al., 2022a;b; Lin et al., 2022; Huang et al., 2021; Hu et al., 2022), existing deep SCI methods are generally trained with a specific hardware configuration, e.g., a well-calibrated coded aperture (physical mask). The resulting model is vulnerable to the hardware shift/perturbation and limited in adapting to multiple hardware configurations. However, directly learning a reconstruction model cooperatively from multi-hardware seems to be infeasible due to data proprietary constraint. It is also non-trivial to coordinate heterogeneous hardware instances with a unified model. To elaborate, we firstly recap previous research efforts of centralized learning solutions. A naive treatment is to jointly train a single reconstruction model with data collected from different hardware configurations, i.e., coded apertures. As shown in Fig. 1 right, this solution enhances the ability of reconstruction ($>0.5\\\\,\\\\text{dB}$) by comparison to single hardware training scenario. However, the performance on inconsistent coded apertures is still non-guaranteed since the model only learns to fit coded apertures in a purely data-driven manner. Followed by, Self-tuning (Wang et al., 2022) advances the learning by explicitly approximating the posterior distribution of coded apertures in a variational Bayesian framework. Despite the significant performance boost, it is only compatible with the coded apertures drawing from homogeneous hardware (same distribution) yet cannot handle heterogeneous hardware. Nevertheless, centralized learning presumes that hardware instances and hyperspectral data...\"}"}
{"id": "FI0vOp2asx", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Comparison of hyperspectral reconstruction learning strategies. (1) The model trained with the single hardware (Prevalent treatment) hardly handles other hardware. Both (2) Jointly train and (3) Self-tuning (Wang et al., 2022) are centralized training solutions. Both (4) FedAvg and the proposed (5) FedHP adopt the same data split setting. We compare the performance gain of different methods over (1) on the right. Note that $P_1$, $P_2$, and $P_3$ present the practical mask distributions. All results are evaluated by unseen masks (non-overlapping) randomly sampled from $\\\\{P_1, P_2, P_3\\\\}$.\\n\\nHardware: CASSI\\nCASSI configuration (coded aperture) Reconstruction learning method\\n\\nIn this work, we leverage federated learning (FL) (Kairouz et al., 2021; Li et al., 2020a; Wang et al., 2021) for cross-platform/silo multi-hardware reconstruction modeling without sharing the hardware configurations. Firstly, the FL benchmark, FedAvg (McMahan et al., 2017), is adopted and brings performance boost (compared by 3 and 4 in Fig. 1 right). However, FedAvg has been proven to be limited in solving heterogeneous data (Hsu et al., 2019; Karimireddy et al., 2020) \u2013 the heterogeneity in SCI substantially stems from the hardware, which is usually absorbed into the compressed data and governs the network training. Thus, different configurations, e.g., coded apertures, yield different data distributions. Besides, we consider a more practical scenario by extending the sample-wise hardware difference into distribution-wise, i.e., not only the different coded apertures yield heterogeneity, but also coded apertures from different clients may follow different distributions (see $P_1 \\\\sim P_3$ in Fig. 1).\\n\\nTo further approach the heterogeneity issue, this work proposes a Federated Hardware-Prompt (FedHP) framework to achieve multi-hardware cooperative learning with privacy piratically preserved. Prevalent FL methods handle the heterogeneity by regularizing the global/local gradients (Karimireddy et al., 2020; Li et al., 2020b), which only take effect on the learning manifold but fail to solve the heterogeneity rooted in the input data space. Differently, FedHP traces back to the source of the data heterogeneity of this application, i.e., inconsistent hardware configurations, and devises a prompt network to solve the client drift issue in input data space. By taking the coded aperture as input, the prompter better accounts for the underlying inconsistency and close the gap between input data distributions across clients. Besides, the prompter explicitly models the correlation between the software and hardware, empowering the learning by following the spirit of the co-optimization (Goudreault et al., 2023; Zheng et al., 2021; Robidoux et al., 2021) in computational imaging. In addition, FedHP directly operates on pre-trained reconstruction backbones with locally well-trained models and keeps them frozen throughout the learning, which potentially improves the efficiency than directly optimizing the reconstruction backbones in FL from scratch. The contributions are as follows:\\n\\n\u2022 We introduce and tackle an unexplored practical problem of hardware cooperative learning in SCI, under the presence of data privacy constraint and the heterogeneous configurations. Our proposed FedHP, to the best knowledge, bridges FL and the field of spectral SCI for the first time.\\n\\n\u2022 We uncover the data heterogeneity of SCI that stems from distinct hardware configurations. A hardware prompt module is developed to solve the distribution shift across clients and empower the hardware-software co-optimization in computational imaging. The proposed method provides an orthogonal perspective in handling the heterogeneity to the existing FL practices.\\n\\n\u2022 We collect the first Snapshot Spectral Heterogeneous Dataset (SSHD) from a series of practical spectral snapshot imaging systems. Extensive experiments demonstrate that FedHP outperforms both centralized learning methods and classic federated learning treatments. This work can inspire future works in this novel research direction of hardware collaboration in SCI.\"}"}
