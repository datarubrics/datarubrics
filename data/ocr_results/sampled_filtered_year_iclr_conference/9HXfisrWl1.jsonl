{"id": "9HXfisrWl1", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To minimize any risk of leakage, we consistently restrict to the same validation and test repositories, in particular to those repositories used in CodeSearchNet (Husain et al., 2019).\\n\\n4.2 Commit Data\\n\\nWe traverse the commit history of 100,000 Python repositories that were filtered to have at least ten stars. We further filter to all commits whose message contains the word \\\"fix\\\", roughly one fifth of all commits. Based upon inspecting many examples, it seems that this simple filter is approximately as precise as more restrictive filters that insist on phrases like \\\"patch bug\\\" or \\\"fix error\\\". Nevertheless, the data is still extremely noisy.\\n\\nThis commit data serves two purposes for us. First, it allows us to train an edit model which is biased towards constructive, bug-fixing edits. We can evaluate such a model directly on bug-fixing, or finetune it on more filtered bug data. Second, we can reverse the input and output and train an edit model biased towards destructive, bug-inducing edits. We can use this model to create neural bugs to greatly augment our training data. This backtranslation approach has already proven useful elsewhere in nlp.\\n\\nSince we are interested in fixing buggy methods, we consider each method edited by each commit. As we are only interested in nontrivial edits, we normalize each method before and after the commit and discard the edits that do not affect the normalized code. To normalize, we strip comments, replace string and numeric literals with placeholders like 'STR_LIT', and standardize whitespace. Finally we are left with 1.1 million, nontrivially edited methods.\\n\\nWe experiment with giving solely the focal method and its edit as input and output, as well as putting more context into the input, as described in the section on Code Skeletons.\\n\\nTable 1: Cross-entropy loss for training two transformers, one on commit data and the other on reversed commits. The two models are evaluated on both forward and backward edits, and with and without code skeletons. The cross-entropy losses are five times better than those normally reported for generating Python code since the editing task is relatively easy ((Svyatkovskiy et al., 2020; Clement et al., 2020)). Furthermore, the losses for reversed edits are a third lower than for forward edits. The forward model is 6% better at forward edits than the reverse model, and the reverse model is in turn 6% better at reverse edits. Both models perform 2% better with skeletons than with only the focal method. We report the results in base two on the test set.\\n\\n|                        | Forward Edit Model | Backward Edit Model |\\n|------------------------|--------------------|---------------------|\\n| Forward Edits With Skeletons | 0.240              | 0.258               |\\n| Forward Edits, focal method only | 0.248              | 0.265               |\\n| Reverse Edits, With Skeletons  | 0.163              | 0.155               |\\n| Reverse Edits, focal method only | 0.167              | 0.157               |\\n\\n4.3 Synthetic Bugs\\n\\n4.3.1 Neural Bugs\\n\\nWe use the term 'neural bugs' to refer to synthetic bugs that are created using a neural edit model, such as the one we trained to revert bug-fixing commits. Using neural bugs for data augmentation has many appealing features. Flexible neural models can generate near arbitrary edits that are drawn from the distribution of mistakes developers actually make. For instance, a neural edit model might swap get_key with get_value, whereas a simple heuristic approach might make unlikely or random swaps, such as switching get_key with reverse_list. Furthermore, this approach is nearly language agnostic, as we can reuse our framework for mining commits, and only need a parser to extract classes and methods along with the constituent parts need to form the code skeletons.\\n\\nWe present three examples of neural bugs. In our first example, the bug-creator model replaces kwargs.pop with kwargs.get, as used elsewhere in the function. In the second example, our bug-creator model replaces a check for a bucket name starting with self.name with a stricter equality check. In the third example, our model deletes a break statement.\"}"}
{"id": "9HXfisrWl1", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def get_key(self, *args, **kwargs):\\n    \"\"\"Pass 'force' to _get_key_internal() in the headers since the call signature of _get_key_internal can not be changed.\\n    \"\"\"\\n    if kwargs.pop('force', None):\\n        headers = kwargs.get('headers', {})\\n        headers['force'] = True\\n        kwargs['headers'] = headers\\n    return super(Bucket, self).get_key(*args, **kwargs)\\n\\ndef tearDown(self):\\n    for bucket in self.boto_conn.get_all_buckets():\\n        if bucket.name == self.name:\\n            for key in bucket.list():\\n                key.delete()\\n            bucket.delete()\\n        for k in self.redis.keys(tpl.connection + '*):\\n            self.redis.delete(k)\\n\\nFigure 2: Our bug-creator model replaces kwargs.pop with kwargs.get, as used elsewhere in the function.\\n\\nFigure 3: Our bug-creator model replaces a check for the bucket name starting with self.name with a stricter equality check.\\n\\nWe also observe our model injecting errors such as replacing a dot accessor with a bracket accessor, replacing comparison operators like \\\">=\\\" with \\\"<\\\", truncating chained function calls, deleting return lines, wrapping return values in objects such as tuples and dictionaries and conversely forgetting to wrap objects, replacing precise errors like IndexError with different errors like ValueError, intuitively misnaming variables such as self.result instead of self._result, and mistakenly copying by reference instead of copying by value. In particular we observe all heuristic bugs that have previously been reported in the literature.\\n\\nNevertheless, we note a few shortcomings of our approach in generating neural bugs. Our models often hallucinate member variables that do not exist, although this is partially remedied by using a longer context than the focal method alone. This is a minor issue, although using a longer context is especially important when training on bugpatching, as bugs are often otherwise underspecified. Our approach over-represents bugs that arise from partial refactoring, such as changing member variable names or function signatures. Our approach under-represents bugs that are caught before being committed, such as syntactic errors and typos. Our approach is also prone to generating superficial changes, such as trivial changes to formatting, local variable names, and practically equivalent edits such as replacing an \\\"is not None\\\" check with a \\\"!= None\\\" check.\\n\\nOne glaring shortcoming of our model, although not necessarily our approach, is that our model prematurely generates the end-of-sentence token in around a fifth of all instances during random sampling. Indeed, these cases represent a large majority of all edits containing syntax errors. This occurs most often during difficult edits, such as when inside a nested scope such as f(\\\"\\[...\\\", especially when manipulating strings or numbers or when invoking user-defined functions or objects from user-defined classes. We could artificially decrease the probability of generating the end-of-sentence token, although we do not explore this option and instead filter out 90% of bugs that contain syntactic errors after they are produced.\\n\\n4.3.2 HEBUGS\\n\\nWe use the term 'heuristic bugs' to refer to synthetic bugs that are manually created using simple rules, such as deleting a line or swapping two parameters in a function call.\"}"}
{"id": "9HXfisrWl1", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\n```\\ndef _generate_SAX_single(self, sections, section_height, value):\\nsax = 0\\nfor s in sections:\\n    if value >= sections[s]:\\n        sax = s\\n    else:\\n        break\\nreturn sax\\n```\\n\\nFigure 4: Our bug-creator model deletes the break, leading sax to take the last small value in sections, instead of the first. Our model also introduces a trivial indentation error on the penultimate line.\\n\\nPrevious work has typically considered five or fewer types of bugs at a time, such as replacing a binary operator (e.g. using != instead of ==), using the wrong variable, forgetting the 'self.' accessor, or deleting code.\\n\\n4.3.3 RW METHODS\\n\\nSince we are interested in data augmentation via synthetic bugs, we can make use of the plenitude of relatively bug-free code on GitHub. In doing so we potentially expand our dataset of target methods over twenty-fold compared to working solely with functions mined from bug-fixing commits. Furthermore, we can scale up nearly arbitrarily by creating multiple buggy versions of each method. For this paper we restrict to 1.3M functions from 10K repositories, near parity with the commit data, and scale to 18M bug-fix pairs via backtranslation. We leave further scaling to future work.\\n\\n4.4 METHODS WITH EXECUTABLE TESTS\\n\\nThere is a world of opportunity for debugging code that can actually be executed, especially if there is an accompanying test to verify the execution was correct. A typical debugging session involves homing in on the suspect block of code with the help of the stack trace, inserting print statements and breakpoints in an approximate binary search, modifying and executing snippets of code to gauge one\u2019s intuition, searching StackOverflow for interpretations of error messages and examples of API usage, talking to ducks, and incrementally making progress. In contrast, the baseline neural model is impoverished, and can only do the equivalent of staring at a piece of code for a few seconds before decisively writing the correction one token at a time.\\n\\nAt a minimum, the generate-and-validate approach enabled by executable tests allows for improved performance via multiple chances. For instance, in the domain of short Java methods, researchers have consistently seen top-20 accuracy triple that of the top-1 accuracy (Allamanis et al., 2017; Tufano et al., 2019). Indeed, truly random edits can be sure of passing a test suite with sufficiently many tries, although prior work has shown that these edits can aggressively overfit, as tests found in the wild are not perfect oracles and can often be gamed by simply deleting functionality (Qi et al., 2015).\\n\\n4.4.1 RACES\\n\\nIn addition to using tests to triage inaccurate edits, we incorporate information from tests into training in three different ways: appending the error message to the buggy method, additionally appending the stack trace, and further providing the values of all local variables at the point of failure, the last of which is conveniently provided by our testing framework Pytest.\\n\\n4.4.2 COLLECTING PASSING TESTS\\n\\nTo collect executable tests at training scale we start with the 200K repositories used for pretraining and filter to 35K repositories containing tests and a setup.py or requirements.txt file. For each of these repositories we execute Pytest in a unique container, ultimately collecting passing tests from 10K repositories.\"}"}
{"id": "9HXfisrWl1", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Example Pytest Trace\\n\\n```python\\nself = HobokenApplication(name='TestCatchallFilters', debug=False)\\nmatch = re.compile(b'.*\\n\\ndef add_after_filter(self, match, func):\\n> filter_tuple = self.__build_filter(match, func)\\n\\nAttributeError: 'HobokenApplication' object has no attribute '_HobokenBaseApplication__build_filter'\\n```\\n\\nFigure 5: Example output for a single frame of the stack trace produced by Pytest. In general there are several frames for a given stack trace. From top to bottom we can see: the input variables (self, match, and func in this case); the head of the function up until the final line executed, denoted with a '>' sign; the error message; the values of all local variables; and finally the file path, line number, and error name. Line breaks and ellipses have been manually added for readability. We append the Pytest trace to the buggy code skeleton, and experiment with withholding the heads and local variables.\\n\\n4.4.3 Testing Synthetic Bugs\\n\\nAfter filtering to functions with passing executable tests and inserting neural bugs, we then rerun the tests to collect Pytest traces and filter out edited functions that still pass their tests and thus are not actually buggy.\\n\\n4.5 Code Skeletons\\n\\nMuch previous work on bug-fixing has used a very abbreviated context consisting solely of the buggy focal method. However, many bugs are not detectable or remediable without a larger context. In particular, we often need to reference user-defined functions, classes, and global variables, as well as the list of imported libraries.\\n\\nTo that end, we experiment with fleshing out the entire 1024-token window using the highest-priority lines within the focal file. We place highest priority on the focal buggy method, followed by its class\u2019s definition if applicable, imports, its class docstring and attributes, globals, other methods\u2019 signatures, other methods\u2019 docstrings, and finally global variables and other methods\u2019 bodies. We also single out the focal method using the special book-ending comments '# target edit' and '# end'.\\n\\n5 Experiments and Results\\n\\nWe experiment with training on backtranslation data, adding skeletons, and adding the Pytest stack trace.\\n\\n5.1 Backtranslation\\n\\nIn our first experiment we compare between training only on forward commit data vs. training on synthetic bugs produced via backtranslation. We evaluate using cross-entropy on the holdout data. Evaluated on holdout forward commit data as shown in table 2, DeepDebug (backtrans) attains up to 10% better loss. Surprisingly, the backtranslation model actually performs worse on reversed commit data, which is consistent with its bias towards constructively fixing buggy functions. Overall DeepDebug is much stronger than the previous state of the art. In table 3 we report our model\u2019s results on the QuixBugs challenge, a benchmark of forty classic algorithms with small synthetic bugs, with nearly equivalent versions in both Python and Java (Lin et al., 2017). The original QuixBugs challenge was for developers to fix as many bugs as possible given one minute apiece. We limit our model to generating one hundred patches via random sampling, which is approximately the number that can be generated and evaluated over the span of one minute. We improve upon...\"}"}
{"id": "9HXfisrWl1", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The joint task of bug localization and program repair is an integral part of the software development process. In this work we present DeepDebug, an approach to automated debugging using large, pretrained transformers. We begin by training a bug-creation model on reversed commit data for the purpose of generating synthetic bugs. We apply these synthetic bugs toward two ends. First, we directly train a backtranslation model on all functions from 200K repositories. Next, we focus on 10K repositories for which we can execute tests, and create buggy versions of all functions in those repositories that are covered by passing tests. This provides us with rich debugging information such as stack traces and print statements, which we use to finetune our model which was pretrained on raw source code. Finally, we strengthen all our models by expanding the context window beyond the buggy function itself, and adding a skeleton consisting of that function\u2019s parent class, imports, signatures, docstrings, and method bodies, in order of priority. On the QuixBugs benchmark, we increase the total number of fixes found by over 50%, while also decreasing the false positive rate from 35% to 5% and decreasing the timeout from six hours to one minute. On our own benchmark of executable tests, our model fixes 68% of all bugs on its first attempt without using traces, and after adding traces it fixes 75% on first attempt.\\n\\n1 INTRODUCTION\\n\\nThe dominant paradigm in automated program repair is the generate-and-validate approach, which our work follows. In this setting we assume the existence of a suite of test functions that identify the existence of a bug. We must then localize the bug and consider candidate fixes until finding a patch that satisfies the test suite.\\n\\nThroughout our experiments, we work with synthetic bugs in which the error has already been localized to a single buggy method. We take as input the buggy function, along with additional context depending on the experiment, such as surrounding context from the function\u2019s file and a stack trace that exposes the buggy function. We feed that input to our sequence-to-sequence transformer, which attempts to generate the fixed function in its entirety.\\n\\nIn our deployment scenario, we also attempt to localize the bug using the stack trace. At present we apply a simple heuristic based on which lines of the stack trace come from the developer\u2019s own code, considering the most recently called lines to be the most suspect. In future we are interested in improving our heuristic using an encoder transformer that reranks methods given the stack trace.\\n\\n2 RELATED WORK\\n\\n2.1 Fault Localization\\n\\nCuriously, the standard approach to bug localization eschews stack traces and instead uses spectrum-based fault-localization (SBFL). In this approach, lines of code are ranked in order of suspiciousness based on how many failing tests execute them. One example is the DStar formula (Wong et al., 2014). Given a statement \\\\( s \\\\) that is executed by \\\\( \\\\text{failed}(s) \\\\) failing tests and \\\\( \\\\text{passed}(s) \\\\) passing tests, it...\"}"}
{"id": "9HXfisrWl1", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Our training pipeline. Following best practice we take advantage of extensively pretrained transformers, in this case using the same DeepDev-py sequence-to-sequence model used to finetune PyMT5 (Clement et al., 2020). We first use commit data to train both a baseline bugpatcher model along with a bug-creator model. Our bug-creator model feeds twenty times as much data to DeepDebug (backtrans). Finally we finetune this model on neural bugs injected into functions that have executable tests and produce traces, thus obtaining its final evolution DeepDebug (traces).\\n\\ncomputes the suspiciousness score $S(s) = \\\\frac{\\\\text{failed}(s)}{\\\\text{passed}(s)} + \\\\left(\\\\text{total failed}(s) - \\\\text{failed}(s)\\\\right)$ where $e$ is an exponent like 2.\\n\\nA notable exception to the SBFL approach to fault localization is the task of build repair, in which the localization information comes from the compiler message rather than a test suite. Gao et al. use crash traces to query Stack Overflow and produce repairing edits (Gao et al., 2015). The DeepDelta model was trained on build data from 300 million lines of Java code from Google projects (Mesbah et al., 2019). Using a Neural Machine Translation approach, DeepDelta fixes half of all errors resulting from mismatched method signatures or a missing symbol on its first attempt.\\n\\n2.2 False Positives\\nA pervasive problem in the field is the fact that tests are incomplete oracles. Many tools produce patches that are merely test-suite adequate more often than they are genuine fixes. For instance, when evaluated on the QuixBugs challenge (Lin et al., 2017), a benchmark consisting of one-line bugs in classic algorithms, a set of ten bugpatching tools found solely false positives for nine functions, while only actually finding any genuine fix for seven (Ye et al., 2019). Only three models found more genuine fixes than false positives, in all cases exactly one more genuine fix. The neural approach CoCoNuT does somewhat better, finding thirteen genuine fixes along with seven false positives (Lutellier et al., 2020). A survey found similarly disappointing results on the larger benchmark Defects4j (Just et al., 2014), a collection of real-world Java bugs (Durieux et al., 2019). In stark contrast, we observe an excellent false positive rate with our approach.\\n\\nOne can also evaluate bug-patching models without executing tests. The Patches in the Wild benchmark frames bug-patching as a sequence-to-sequence problem as in our approach, which enables the use of NLP metrics (Tufano et al., 2019). The authors consider an edit to be a bonafide fix only if it is exactly the same as the edit the original developer made when committing it. SequenceR (Chen et al., 2018) both narrows and expands on the Patches in the Wild dataset by focusing on one-line changes only, while also providing additional context beyond the buggy method by including other methods' signatures, similarly to our use of code skeletons. This extended context gives a 15% relative boost.\"}"}
{"id": "9HXfisrWl1", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For certain bug-types it is possible to generate millions of synthetic bugs to train on. Devlin et al. (Devlin et al., 2017) train an RNN on Python to fix incorrect comparison operators, the mistaken use of \u201cis\u201d vs. \u201cis not\u201d, variable misuse, and forgotten \u201cself\u201d accessors. Overall, they achieve 86% accuracy on synthetic bugs and 41% on real-life bugs. Kanade et al. (Kanade et al., 2019) pretrain a BERT-style model \u201cCuBERT\u201d on a larger Python dataset and then finetune on a related suite of synthetic bugs, achieving over 90% accuracy.\\n\\nBacktranslation is a more generic form of data augmentation, in which a model is trained to translate target data to source data (Edunov et al., 2018). In our case, we have an abundance of bug-free data mined from GitHub. We train a backtranslation model to create synthetic bugs and augment the training data for our goal task of fixing bugs. Backtranslation is also put to use for the related task of grammatical error correction (Kiyono et al., 2019).\\n\\nVarious task-agnostic pretraining approaches like BERT, BART, T5, and GPT-3 (Devlin et al., 2017; Lewis et al., 2020; Raffel et al., 2019; Brown et al., 2020) have seen large performance gains on a diverse array of benchmarks. These models are typically pretrained using a denoising objective on a large corpus of text that has been synthetically corrupted using bidirectional or causal masking, as well as more arbitrary noising such as randomly deleting, replacing, or permuting source tokens. Pretraining has also shown large improvements for the task of program repair (Drain et al., 2021; Lu et al., 2021).\\n\\nWe reuse the 406M-parameter sequence-to-sequence transformer with twelve encoder layers and twelve decoder layers which was pretrained as described under section 4.1. When experimenting with stack traces, we allot 1024 tokens for the code skeleton, and up to 896 tokens for the trace. In order to accommodate this larger context we thus need to expand the transformer\u2019s positional embedding matrix. To this end we use axial embeddings, as inspired by reformer (Kitaev et al., 2020). In our implementation, we duplicate the first 896 of the pre-existing 1024 positional embeddings, generate a random axial vector, and add that vector to each of the 896 duplicate embeddings. This approach outperformed randomly initialized embeddings in preliminary experiments.\\n\\nWe work with four different training datasets: Raw python code used for pretraining, commit data used for training a neural bug-creator and bug-patcher, methods extracted from the raw code in which we insert both neural bugs so as to train an even stronger bug-patcher, and, finally, methods that pass executable tests. For this last dataset, we also obtain the list of lines executed by each test, and we obtain another bug-patching dataset by again inserting synthetic bugs and rerunning the passing tests, which allows us to finetune a bug-patcher on stack traces, error messages, and print statements. We also experiment with giving our bug-patcher models either only the focal buggy method or also a \u2018skeleton\u2019 of the entire file that prioritizes data such as function signatures.\\n\\nWe build on the DeepDev transformer platform. We reuse the 406M-parameter DeepDev Python transformer that was warmstarted from FaceBook\u2019s BART model and then further pretrained using a spanmasking objective (Lewis et al., 2020; Clement et al., 2020). The pretraining data consists of 200,000 public Python repos filtered to have at least five stars. Pretraining took place on a DGX-2 box for three weeks. Note that the DeepDev tokenizer has appended whitespace tokens, such as the four-space and eight-space tokens, which boosts throughput and the effective context length.\"}"}
{"id": "9HXfisrWl1", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Example Skeleton\\n\\n```python\\nimport psycopg2\\nimport psycopg2.extras\\nfrom dpostools.schemes import schemes\\nfrom dpostools.utils import dictionify\\nfrom dpostools import api, exceptions\\n\\nclass DbConnection:\\n    def __init__(self, user, password, host='localhost', database='ark_mainnet'):\\n        self.connection = None\\n\\nclass DbCursor:\\n    def __init__(self, user, password, host='localhost', database='ark_mainnet', dbconnection=None):\\n        self.connection = None\\n\\n    def description(self):\\n        pass\\n\\n    def execute(self, qry, *args, cur_type=None):\\n        pass\\n\\n    def fetchall(self, cur_type=None):\\n        pass\\n\\n    def fetchone(self, cur_type=None):\\n        pass\\n\\n    def execute_and_fetchall(self, qry, *args, cur_type=None):\\n        pass\\n\\n    # target edit\\n    def execute_and_fetchone(self, qry, *args, cur_type=None):\\n        self.execute(qry, *args, current_type=cur_type)\\n        return self.\\n\\n    # end\\n\\nclass DposNode:\\n    def __init__(self, user, password, host='localhost', database='ark_mainnet', ):\\n        self.connection = None\\n\\n    def account_details(self, address):\\n        pass\\n\\n    def node_height_details(self):\\n        pass\\n\\n    def check_node_height(self, max_difference):\\n        pass\\n\\n    def all_delegates(self):\\n        pass\\n\\n    def current_delegates(self):\\n        pass\\n\\n    def payouts_to_address(self, address):\\n        pass\\n\\n    def transactions_from_address(self, address):\\n        pass\\n\\n    def all_votes_by_address(self):\\n        pass\\n\\nFigure 6: Example skeleton using just 400 tokens. Note the inclusion of the entire buggy focal function (with its name highlighted for the reader) including the control tokens '# target edit' and '# end', as well as the imported functions. Due to limited space, the skeleton can only fit all signatures from the focal class, along with several more signatures from outside the focal class. The skeletons we use in practice contain more tokens and thus tend to be fleshier i.e. contain more docstrings and function bodies.\\n\\nTable 2: Cross-entropy results for training two transformers, one trained on commit data and the other on patching neural bugs. As in table 1, the two models are evaluated on commit data. Note that the neural bugpatcher does comparatively worse on reversed edits vs. forward edits. The neural bugpatcher actually does worse when using skeletons on commit data, presumably due to the fact that commits typically edit multiple functions. Finally, the neural bugpatcher outperforms the forward edit model when evaluated on individually edited functions.\\n\\n|                  | Forward Edit Model | DeepDebug (backtrans) |\\n|------------------|--------------------|----------------------|\\n|                  | Forward Edits With Skeletons | .240 | .243 |\\n|                  | Forward Edits, focal method only | .248 | .221 |\\n|                  | Reverse Edits, With Skeletons | .163 | .196 |\\n|                  | Reverse Edits, focal method only | .167 | .174 |\\n\\nTable 3: Results on the QuixBugs Benchmark. We use a timeout of one minute for each bug and perform joint bug-localization and program repair. In contrast, prior works use a timeout of several hours and often assume that the buggy line has already been localized.\\n\\n|                  | Suite of Ten non-neural Models | DeepDebug (Durieux et al., 2019) | DeepDebug (Lutellier et al., 2020) |\\n|------------------|-------------------------------|----------------------------------|----------------------------------|\\n| Number of Bugs Fixed | 7/40 (18%)                  | 13/40 (33%)                     | 21/40 (53%)                     |\\n| Number of False Positives | 9                             | 7                               | 1                                |\\n| True Positive Rate         | 44%                          | 65%                             | 95%                             |\\n| Number of Verbatim Fixes \u2013 | \u2013                            | \u2013                               | 13                               |\"}"}
{"id": "9HXfisrWl1", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022 the previous state of the art for number of bugs found by over 50% (Lutellier et al., 2020), while simultaneously decreasing the false positive rate from 35% to 5%. Notably all our models generate many duplicate edits, due to the low-perplexity of this task, which suggests room for improved sampling. Note that these results are even more impressive given the hints and timeouts provided to previous models. For instance, CoCoNuT is explicitly told which line contains the bug and allows six hours to find a patch (Lutellier et al., 2020). Similarly, five of the non-neural tools collectively found 122 test-suite adequate patches for the longest increasing subsequence algorithm (Durieux et al., 2019), suggesting thousands of attempts.\\n\\n5.2 SKELETONS\\n\\nIn the second experiment we compare between training and evaluating using only the focal function as input vs. giving the entire skeleton as input. The neural bugpatcher sees a large 25% loss decrease when using skeletons when evaluated on neural bugs, as shown in table 4. Surprisingly, the neural bugpatcher actually does worse when using skeletons on commit data as reported in table 2, presumably due to the fact that commits typically edit multiple functions.\\n\\nTable 4: The neural bugpatcher sees a large 25% cross-entropy loss decrease when using skeletons when evaluated on our large test set of neural bugs. These neural bugs are in general not executable.\\n\\n|                          | DeepDebug (backtrans) | Neural Bugpatching, focal method only |\\n|--------------------------|-----------------------|---------------------------------------|\\n| Top-1 Success Rate       | 357/523 (68%)         | 409/523 (77%)                        |\\n| Top-10 Success Rate      | 472/523 (90%)         | 509/523 (97%)                        |\\n\\n5.3 TRACES\\n\\nIn the third experiment we append the Pytest stack trace to the buggy input. We expand the context window using axial embeddings in order to fit these additional tokens.\\n\\nWe filter to a benchmark of 523 neural bugs from 100 repositories in our validation set. We observe impressive performance, and, contrarily to our cross-entropy results, a large boost from using traces. The top-10 patch success rate for \\\\textit{DeepDebug (backtrans)} is 90%, and the top-10 rate for \\\\textit{DeepDebug (traces)} is 97%. Note that these are highly nontrivial bugs, with a median length of two lines that must be edited to produce a fix. See table 5.\\n\\nTable 5: Patch success results on our benchmark of neural bugs with executable tests.\\n\\n|                          | DeepDebug (backtrans) | DeepDebug (traces) |\\n|--------------------------|-----------------------|--------------------|\\n| Top-1 Success Rate       | 357/523 (68%)         | 393/523 (75%)      |\\n| Top-10 Success Rate      | 472/523 (90%)         | 509/523 (97%)      |\\n\\n6 FUTURE WORK\\n\\nHaving code with executable tests opens up a world of opportunities for algorithmic debugging with machine learning. We are interested in genuinely iterative fixing (in contrast to independently generating edits), moving beyond the assumption that the bug has already been localized to a single method, and expanding to other popular languages. Most importantly we are interested in deploying a command-line interface to our tool.\\n\\nREFERENCES\\n\\nAllamanis, M., Brockschmidt, M., and Khademi, M. (2017). Learning to represent programs with graphs.\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., 9...\"}"}
{"id": "9HXfisrWl1", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nRamesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020). Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 1876\u20131900. Curran Associates, Inc.\\n\\nChen, Z., Kommrusch, S., Tufano, M., Pouchet, L.-N., Poshyvanyk, D., and Monperrus, M. (2018). Sequencer: Sequence-to-sequence learning for end-to-end program repair.\\n\\nClement, C., Drain, D., Timcheck, J., Svyatkovskiy, A., and Sundaresan, N. (2020). PyMT5: multi-mode translation of natural language and python code with transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9052\u20139065, Online. Association for Computational Linguistics.\\n\\nDevlin, J., Uesato, J., Singh, R., and Kohli, P. (2017). Semantic code repair using neuro-symbolic transformation networks.\\n\\nDrain, D., Wu, C., Svyatkovskiy, A., and Sundaresan, N. (2021). Generating bug-fixes using pretrained transformers. arXiv preprint arXiv:2104.07896.\\n\\nDurieux, T., Madeiral, F., Martinez, M., and Abreu, R. (2019). Empirical review of java program repair tools: A large-scale experiment on 2,141 bugs and 23,551 repair attempts. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2019, page 302\u2013313, New York, NY, USA. Association for Computing Machinery.\\n\\nEdunov, S., Ott, M., Auli, M., and Grangier, D. (2018). Understanding back-translation at scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 489\u2013500, Brussels, Belgium. Association for Computational Linguistics.\\n\\nGao, Q., Zhang, H., Wang, J., Xiong, Y., Zhang, L., and Mei, H. (2015). Fixing recurring crash bugs via analyzing q|&a sites. In Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering, ASE '15, page 307\u2013318. IEEE Press.\\n\\nHusain, H., Wu, H.-H., Gazit, T., Allamanis, M., and Brockschmidt, M. (2019). Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436.\\n\\nJust, R., Jalali, D., and Ernst, M. D. (2014). Defects4j: A database of existing faults to enable controlled testing studies for java programs. ISSTA 2014, page 437\u2013440, New York, NY, USA. Association for Computing Machinery.\\n\\nKanade, A., Maniatis, P., Balakrishnan, G., and Shi, K. (2019). Pre-trained contextual embedding of source code.\\n\\nKitaev, N., Kaiser, L., and Levskaya, A. (2020). Reformer: The efficient transformer.\\n\\nKiyono, S., Suzuki, J., Mita, M., Mizumoto, T., and Inui, K. (2019). An empirical study of incorporating pseudo data into grammatical error correction. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\\n\\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. (2020). BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, Online. Association for Computational Linguistics.\\n\\nLin, D., Koppel, J., Chen, A., and Solar-Lezama, A. (2017). Quixbugs: A multi-lingual program repair benchmark set based on the quixey challenge. SPLASH Companion 2017, page 55\u201356, New York, NY, USA. Association for Computing Machinery.\"}"}
{"id": "9HXfisrWl1", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C. B., Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., Deng, S. K., Fu, S., and Liu, S. (2021). Codexglue: A machine learning benchmark dataset for code understanding and generation. CoRR, abs/2102.04664.\\n\\nLutellier, T., Pham, H. V., Pang, L., Li, Y., Wei, M., and Tan, L. (2020). Coconut: Combining context-aware neural translation models using ensemble for program repair. In Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2020, page 101\u2013114, New York, NY, USA. Association for Computing Machinery.\\n\\nMesbah, A., Rice, A., Johnston, E., Glorioso, N., and Aftandilian, E. (2019). Deepdelta: Learning to repair compilation errors. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2019, page 925\u2013936, New York, NY, USA. Association for Computing Machinery.\\n\\nQi, Z., Long, F., Achour, S., and Rinard, M. (2015). An analysis of patch plausibility and correctness for generate-and-validate patch generation systems. In Proceedings of the 2015 International Symposium on Software Testing and Analysis, ISSTA 2015, page 24\u201336, New York, NY, USA. Association for Computing Machinery.\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2019). Exploring the limits of transfer learning with a unified text-to-text transformer.\\n\\nSvyatkovskiy, A., Deng, S. K., Fu, S., and Sundaresan, N. (2020). Intellicode compose: Code generation using transformer. Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering.\\n\\nTufano, M., Watson, C., Bavota, G., Penta, M. D., White, M., and Poshyvanyk, D. (2019). An empirical study on learning bug-fixing patches in the wild via neural machine translation. ACM Trans. Softw. Eng. Methodol., 28(4).\\n\\nWong, W. E., Debroy, V., Gao, R., and Li, Y. (2014). The dstar method for effective software fault localization. IEEE Transactions on Reliability, 63(1):290\u2013308.\\n\\nYe, H., Martinez, M., Durieux, T., and Monperrus, M. (2019). A comprehensive study of automatic program repair on the quixbugs benchmark. In 2019 IEEE 1st International Workshop on Intelligent Bug Fixing (IBF), pages 1\u201310. IEEE.\"}"}
