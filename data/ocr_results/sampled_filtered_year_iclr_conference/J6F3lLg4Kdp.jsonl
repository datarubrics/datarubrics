{"id": "J6F3lLg4Kdp", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 6: Layerwise sparsity of RoBERTa on CSQA at sparsity levels \\\\( \\\\in [36\\\\%, 67\\\\%, 83\\\\%] \\\\).\\n\\nFigure 7: Layerwise sparsity of RoBERTa on RACE at sparsity levels \\\\( \\\\in [36\\\\%, 67\\\\%, 83\\\\%] \\\\).\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.2 EVALUATION OF $\\\\text{BERT}$ ON $\\\\text{SMC-B}$\\n\\nBased on our conjecture that magnitudes/gradients become unreliable for pruning LLMs, we hypothesize that the second-order pruning approaches with approximated Hessian matrix would be more accurate options. To verify our conjecture, we turn to the latest stronger second-order pruning framework, oBERT (Kurtic et al., 2022). Specifically, we follow Kurtic et al. (2022) and replace the magnitude pruning criterion of the LTH framework with the second-order oBERT criterion; adopt Learning Rate Rewinding (LRR) (Renda et al., 2020) and Knowledge Distillation (KD) (Hinton et al., 2015) during each pruning iteration; and keep the embeddings and classification heads dense. Figure 9 support our hypothesis and demonstrates that oBERT notably outperforms the zero-order and first-order sparsification approaches, and substantially improves the accuracy to a competitive level. More importantly, oBERT produces a completely different layerwise sparsity pattern from magnitude-based pruning approaches, which is consistent with the patterns that are commonly observed in sparse computer vision models: deeper layers tend to have higher sparsities than lower layers (Evci et al., 2020; Kusupati et al., 2020; Tanaka et al., 2020; Liu et al., 2021b).\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Layerwise Sparsity\\n\\nRoBERTa on CSQA, Overall Sparsity=36%\\nLTH (not pruning emb and classifier)\\nLTH + LRR (not pruning emb and classifier)\\noBERT + LRR + KD (not pruning emb and classifier)\\noBERT + LRR (not pruning emb and classifier)\\n\\nFigure 10: Layerwise sparsity comparison among LTH, LRR, oBERT, and KD with RoBERTa-Large on CSQA.\\n\\napproximation); and even so, the strongest SNNs still fall short of their dense counterpart by around 10% accuracy at sparsities between 60%\u221280%, in contrast to \\\"normal\\\" SNNs that easily match their dense models on CIFAR, ImageNet, or GLUE. Therefore, the main claim of our paper still holds, that is, SMC-Bench indeed provides a new benchmark that is way more challenging for SOTA sparse algorithms, than existing testbeds.\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task                      | # Papers | Datasets                           |\\n|-------------------------|---------|-----------------------------------|\\n| Image Classification    | 82      | CIFAR-10 59, CIFAR-100 37, MNIST 26 |\\n| Fashion MNIST           | 10      | SVHN 4, BIRDS-200 1, FLOWERS-102 1, EMNIST 1 |\\n| Text                     | 16      | GLUE 9, SQUAD 4, WIKI TEXT-103 3, WMT 5, IMDB 1, AAN 1, LO 1, PEN 1, WEB TEXT 1, ONE BILLION WORD BENCHMARK 1 |\\n| Video Recognition        | 3       | LFW 3, YOUTUBE Faces 2, CASIA-W FACE 1 |\\n| Object Detection         | 3       | COCO DATASET 2, PASCAL-VOL-2007 1 |\\n| Speech Recognition       | 2       | GOOGLE -12 1, TIMIT 1 |\\n| High-Resolution Reconstruction | 2 | SET 5 2, SET 14 2, B100 2, URBAN 100 2, MANGA 109 2 |\\n| Image Generation         | 2       | CIFAR-10 2, IMAGE NET 1, STL-10 1 |\\n| Human Activity Recognition | 1     | HAR-2 1, MICROARRAY CLASSIFICATION 1, LEUKEMIA 1, CLL-SUB-111 1, SMK-CAN-18 1, GLI-85 1 |\\n| Regression               | 1       | NYU D EPHT 1, 3D OBJECT PART SEGMENETATION 1, SHAPE NET 1 |\\n| Atari Suite              | 1       | CART POLE 1, ACRobot 1, MOUNTAIN CAR 1, ATARI SUITE 1 |\\n| Video Deblurring         | 1       | DVD 1, GOPRO 1, REAL BLURRY VIDEOS 1 |\\n| Vocabulary Speech Recognition | 1   | SWB 1, VOCABULARY SPEECH RECOGNITION 1, VS 1 |\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A SUMMARY OF TASKS, MODELS, DATASETS, AND TRAINING\\n\\nWe summarize the combinations of models and configurations that we used to evaluate SNNs on SMC-Bench.\\n\\n### Table 2: Summary of models and datasets that we used to evaluate on SMC-Bench.\\n\\n| Task                        | Datasets          | Models  | Source                                           |\\n|-----------------------------|-------------------|---------|-------------------------------------------------|\\n| Commonsense Reasoning       | CSQA              | RoBERTa | Facebook AI Research Sequence-to-Sequence Toolkit (Ott et al., 2019) |\\n|                             | WinoGrande        | RoBERTa | Facebook AI Research Sequence-to-Sequence Toolkit (Ott et al., 2019) |\\n|                             | RACE              | RoBERTa | Facebook AI Research Sequence-to-Sequence Toolkit (Ott et al., 2019) |\\n| Arithmetic Reasoning       | MAvPS             | GTS     | GitHub Repository (Patel et al., 2021)          |\\n|                             | Graph2Tree        | GitHub Repository (Patel et al., 2021)       |\\n|                             | ASDiv-A           | GTS     | GitHub Repository (Patel et al., 2021)          |\\n|                             | Graph2Tree        | GitHub Repository (Patel et al., 2021)       |\\n|                             | SV AMP            | GTS     | GitHub Repository (Patel et al., 2021)          |\\n|                             | Graph2Tree        | GitHub Repository (Patel et al., 2021)       |\\n| Protein Thermostability Prediction | HotProtein (HP-S)TAPE | GitHub Repository (Rao et al., 2019) |\\n|                             | ESM-1B            | GitHub Repository (Rives et al., 2021)       |\\n|                             | HotProtein (HP-S\u00b2C\u2082) | ESM-1B | GitHub Repository (Rives et al., 2021)          |\\n|                             | HotProtein (HP-S\u00b2C\u2085) | ESM-IF1 | GitHub Repository (Rives et al., 2021)          |\\n| Multilingual Translation   | 2-to-2 mBART      | Facebook AI Research Sequence-to-Sequence Toolkit (Ott et al., 2019) |\\n|                             | 5-to-5 mBART      | Facebook AI Research Sequence-to-Sequence Toolkit (Ott et al., 2019) |\\n|                             | 10-to-10 mBART    | Facebook AI Research Sequence-to-Sequence Toolkit (Ott et al., 2019) |\\n\\nWe strictly follow the training configurations reported in the original source to replicate the results of each task. The hyperparameters and configurations used for each model in this paper are shared below.\\n\\n### Table 3: Hyperparameters and training configurations used for models on Commonsense Reasoning.\\n\\n| Models | Dataset | Pre-trained Models | Hidden Size | FFN Inner Hidden Size | Number of Layers | Learning Rate | Weight Decay | Batch Size | Dropout | Attention Dropout | Clip Norm | Adam \u03f5 | Adam \u03b2\u2081 | Adam \u03b2\u2082 | # Parameters | Training Time | Warmup Time |\\n|--------|---------|-------------------|-------------|-----------------------|------------------|----------------|--------------|-------------|----------|---------|------------------|-----------|--------|---------|---------|-------------|--------------|-------------|\\n| RoBERTa | CSQA    | RoBERTa           | [1024]      | [4096]                | [24]             | [1e-5]        | [0.01]       | [16]       | [0.1]   | [0.1]             | [0.0]     | [1e-06]| [0.9]   | [0.98]  | 355M        | 3000 steps   | 150 steps   |\\n| RoBERTa | WinoGrande | RoBERTa           | [1024]      | [4096]                | [24]             | [1e-5]        | [0.01]       | [32]       | [0.1]   | [0.1]             | [0.0]     | [1e-06]| [0.9]   | [0.98]  | 355M        | 23750 steps  | 2375 steps  |\\n| RoBERTa | RACE    | RoBERTa           | [1024]      | [4096]                | [24]             | [1e-5]        | [0.01]       | [16]       | [0.1]   | [0.1]             | [0.0]     | [1e-06]| [0.9]   | [0.98]  | 355M        | 3 epochs    | 500 steps   |\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 4: Hyperparameters and training configurations used for models on Arithmetic Reasoning.\\n\\n| Models | GTS | Graph2Tree |\\n|--------|-----|------------|\\n| Dataset | MA VPS, ASDiv-A, SV AMP | MA VPS, ASDiv-A, SV AMP |\\n| Pre-trained Embedding | RoBERTa | RoBERTa |\\n| Embedding Size | [768] | [768] |\\n| Hidden Size | [512] | [384] |\\n| Number of Layers | [2] | [2] |\\n| Learning Rate | [1e-3] | [8e-4] |\\n| Weight Decay | [1e-5] | [1e-5] |\\n| Embedding LR | [8e-6] | [1e-5] |\\n| Batch Size | 4 (MA VPS, ASDiv-A), 8 (SV AMP) | 4 (MA VPS, ASDiv-A), 8 (SV AMP) |\\n| Dropout | [0.5] | [0.5] |\\n| Adam $\\\\epsilon$ | [1e-08] | [1e-08] |\\n| Adam $\\\\beta_1$ | [0.9] | [0.9] |\\n| Adam $\\\\beta_2$ | [0.999] | [0.999] |\\n| # Parameters | 140M | 143M |\\n| Training Time | 50 epochs | 50 epochs |\\n\\n## Table 5: Hyperparameters and training configurations used for models on Protein Thermostability Prediction.\\n\\n| Models | TAPE | ESM-1B | ESM-IF1 |\\n|--------|------|--------|--------|\\n| Dataset | HP-S | HP-S | HP-S, C2, Meltome, Atlas, HP-S |\\n| Hidden Size | [768] | [1280] | [512] |\\n| Number of Layers | [12] | [33] | [20] |\\n| Learning Rate | [1e-4] | [2e-2 (head), 1e-6 (backbone)] | [2e-2 (head), 1e-4 (backbone)] |\\n| Weight Decay | [1e-2] | [1e-2] | [5e-2] |\\n| Batch Size | [16] | [3,2,3] | [4] |\\n| Attention Dropout | [0.1] | [0.0] | [0.1] |\\n| Dropout | [0.1] | [0.0] | [0.1] |\\n| Adam $\\\\epsilon$ | [1e-08] | [1e-08] | [1e-08] |\\n| Adam $\\\\beta_1$ | [0.9] | [0.9] | [0.9] |\\n| Adam $\\\\beta_2$ | [0.999] | [0.999] | [0.999] |\\n| # Parameters | 92M | 650M | 124M |\\n| Training Time | 4 epochs | 4 epochs | 8 epochs |\\n\\n## Table 6: Hyperparameters and training configurations used for models on Multilingual Translation.\\n\\n| Models | mBART | mBART | mBART |\\n|--------|-------|-------|-------|\\n| Dataset | 2-to-2 | 5-to-5 | 10-to-10 |\\n| Pre-trained Models | mBART | mBART | mBART |\\n| Hidden Size | [1024] | [1024] | [1024] |\\n| Number of Layers | [24] | [24] | [24] |\\n| Learning Rate | [3e-5] | [3e-5] | [3e-5] |\\n| Weight Decay | [0.0] | [0.0] | [0.0] |\\n| Batch Size | [16] | [32] | [16] |\\n| Dropout | [0.3] | [0.3] | [0.3] |\\n| Attention Dropout | [0.1] | [0.1] | [0.1] |\\n| Clip Norm | [0.0] | [0.0] | [0.0] |\\n| Adam $\\\\epsilon$ | [1e-06] | [1e-06] | [1e-06] |\\n| Adam $\\\\beta_1$ | [0.9] | [0.9] | [0.9] |\\n| Adam $\\\\beta_2$ | [0.98] | [0.98] | [0.98] |\\n| # Parameters | 680M | 680M | 680M |\\n| Training Time | 40,000 steps | 40,000 steps | 40,000 steps |\\n| Wramup Time | 2,500 steps | 2,500 steps | 2,500 steps |\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"In this appendix, we report the performance of SNNs on arithmetic reasoning including Magnitude before Training (OMP (Before)). We can clearly observe that the accuracy of magnitude pruning before training dramatically falls from 80% to nearly 0% when sparsity is larger than 36%. After checking the corresponding layerwise sparsity, we find that OMP (Before) completely removes all the weights from non-embedding and non-encoder layers, leading to severe layer collapse.\\n\\n![Graph2Tree on MAWPS](image)\\n\\n![Graph2Tree on ASDiv-A](image)\\n\\n![Graph2Tree on SVAMP](image)\\n\\n**Figure 5:** Arithmetic reasoning performance of various sparse GTS and Graph2Tree on MAWPS, ASDiv-A, and SVAMP.\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we conduct a full investigation, attempting to open the box for the potential causes of SNN failures on SMC-Bench. Our analysis reveals two possible causes: (1) the \u201clazy regime\u201d in fine-tuning LLMs, and (2) the model components to prune. Due to the \u201clazy regime\u201d phenomenon (Chizat et al., 2019; Malladi et al., 2022), commonly used pruning techniques that rely on magnitude and gradient can be very uninformative. Therefore, we turn to the latest strong second-order pruning framework - oBERT (Kurtic et al., 2022), which utilizes inverse-Hessian approximations to guide pruning decisions. We choose RoBERTa.large on CSQA to conduct this investigation. The roadmap of our full investigation is presented below.\\n\\n- Layerwise sparsity ratios of various magnitude-based pruning methods are strikingly similar, suggesting that \u201clazy regime\u201d may occur during fine-tuning. In this regime, the most common pruning criteria such as magnitude and gradient can be rather unreliable.\\n\\n- Second-order pruning approaches like oBERT provide more faithful signals than magnitudes and gradients for LLM pruning, and hence achieve significantly higher accuracy at high sparsities.\\n\\nC.1 Layerwise Sparsity Ratios on SMC-Bench\\n\\nTo check if severe layer collapse occurs on SMC-Bench, we plot the per-layer sparsity ratios discovered by various sparsification approaches at three sparsity levels: 36%, 64%, and 83%. Layers are ordered from input to output on the X-axis. We respectively report the layerwise sparsity of commonsense reasoning with RoBERTa on CSQA and RACE in Figure 6 and 7, and arithmetic reasoning with GTS on SV AMP in Figure 8. We summarize our main findings here.\\n\\n1. Layerwise sparsities of magnitude-based pruning approaches are extremely similar. IMP, OMP, and GMP that rely on weight magnitude for pruning share an extremely similar set of layerwise sparsities. Especially, sparsity values of magnitude pruning on commonsense reasoning are completely identical, all overlapped on blue lines, except for the tiny difference in classification heads. This phenomenon indicates that weights of RoBERTa excluding classifiers remain rather stable during commonsense reasoning fine-tuning so that all the magnitude pruning variants (both before and after) discover the same sparsity pattern. The sparsity difference of arithmetic reasoning is more distinguishable than commonsense reasoning. Still, sparsities in the encoder (pre-trained RoBERTa) of IMP, GMP, and OMP (After) largely overlap. Until reaching the later layers, the sparsity ratios of different approaches start to be distinct.\\n\\n2. SNIP tends to prune all the weights in embedding layers aggressively. Even at the mild 36% sparsity, SNIP prunes weights of embedding layers to 99.7% sparsity, which may explain why SNIP struggles on SMC-Bench.\\n\\n3. OMP (Before) suffers from layer collapse on arithmetic reasoning. We empirically find that OMP (Before) leads to completely empty deep layers when sparsity is larger than 36%, indicating the limitation of only considering pruning with the magnitude before fine-tuning or re-training.\\n\\nThe near-identical layerwise sparsity ratios across various magnitude-based methods remind us of the \u201clazy training\u201d regime (Chizat et al., 2019; Malladi et al., 2022) which was revealed to occur during the fine-tuning of LLMs. Under this regime, weight changes during fine-tuning are negligible, hence non-informative and more \u201cnoisy\u201d. Consequently, various magnitude-based pruning approaches, regardless of their timing, all tend to converge to the same solution.\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nSparse Neural Networks (SNNs) have received voluminous attention predominantly due to growing computational and memory footprints of consistently exploding parameter count in large-scale models. Similar to their dense counterparts, recent SNNs generalize just as well and are equipped with numerous favorable benefits (e.g., low complexity, high scalability, and robustness), sometimes even better than the original dense networks. As research effort is focused on developing increasingly sophisticated sparse algorithms, it is startling that a comprehensive benchmark to evaluate the effectiveness of these algorithms has been highly overlooked. In absence of a carefully crafted evaluation benchmark, most if not all, sparse algorithms are evaluated against fairly simple and naive tasks (e.g., CIFAR-10/100, ImageNet, GLUE, etc.), which can potentially camouflage many advantages as well unexpected predicaments of SNNs. In pursuit of a more general evaluation and unveiling the true potential of sparse algorithms, we introduce \\\"Sparsity May Cry\\\" Benchmark (SMC-Bench), a collection of carefully-curated 4 diverse tasks with 10 datasets, that accounts for capturing a wide range of domain-specific and sophisticated knowledge. Our systemic evaluation of the most representative sparse algorithms reveals an important obscured observation: the state-of-the-art magnitude- and/or gradient-based sparse algorithms seemingly fail to perform on SMC-Bench when applied out-of-the-box, sometimes at significantly trivial sparsity as low as 5%. The observations seek the immediate attention of the sparsity research community to reconsider the highly proclaimed benefits of SNNs. We further conduct a thorough investigation into the reasons for the failure of common SNNs. Our analysis points out that such failure is intimately related to the \\\"lazy regime\\\" of large model training, which hints us with stronger pruning recipes that alleviate the failure on SMC-Bench (though still more or less suffering). By incorporating these well-thought and diverse tasks, SMC-Bench is designed to favor and encourage the development of more scalable and generalizable sparse algorithms. We open-source SMC-Bench to assist researchers in building next-generation sparse algorithms that scale and generalize: https://github.com/VITA-Group/SMC-Bench.\\n\\n1 Introduction\\n\\nSparse Neural Networks (SNNs) are no stranger to the deep learning community (Liu & Wang, 2023), but recently they have received stupendous attention in the era of transformers (e.g., BERT, GPT, ViT, CLIP, etc.), when the parameter count is frequently measured in billions rather than millions. Due to the consistent efforts of sparsity researchers, SNNs have ushered enormous breakthroughs and can generalize just as well as original dense networks, and it is feasible to procure them after training (Frankle & Carbin, 2019; Sanh et al., 2020; Chen et al., 2020; Frankle et al., 2020), during training (Zhu & Gupta, 2017; Gale et al., 2019; Liu et al., 2021b), and even before training (Mocanu et al., 2018; Lee et al., 2019; Liu et al., 2022) their dense counterparts using pruning. Apart from well-known efficiency benefits, surprisingly, SNNs also enjoy auxiliary benefits such as adversarial robustness (Guo et al., 2018; \u00d6zdenizci & Legenstein, 2021; Chen et al., 2022), out-of-distribution...\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"generalization (Zhang et al., 2021; Diffenderfer et al., 2021), and uncertainty estimation (Liu et al., 2021a), etc. Despite the multi-dimensional success of numerous sparse algorithms, startlingly, our extensive survey across over 100 recent SNN papers within 2015-2022 unveils multiple daunting issues regarding evaluation datasets and protocols blindly followed within the sparsity community, that may significantly impede future progress if left unacknowledged.\\n\\n**Issues with current evaluation paradigm:**\\n\\nFirstly, the vast majority of current work on SNNs is narrowly evaluated, i.e., only targeting a single or a few tasks (usually on image classification and sometimes on language understanding) on which SNNs have already proven their proficiency (Gale et al., 2019; Frankle & Carbin, 2019). Surprisingly, 79 papers out of our carefully selected 100 papers on SNNs, evaluate sparse models merely on a single task, where 72 out of them evaluate image classification.\\n\\nSecondly, people are obsessed with evaluating SNNs on well-understood datasets, including but not limited to MNIST (LeCun, 1998) (26 papers), CIFAR-10/100 (Krizhevsky et al., 2009) (59 and 37 papers, respectively), ImageNet (Deng et al., 2009) (62 papers), and GLUE (Wang et al., 2018) (9 papers), where deep neural networks have already exceeded the human-equivalent performance (refer to Appendix D for more details). For instance, even though ImageNet has been considered a rather challenging task over years, very high accuracy (>90%) has been reported many times (Yu et al., 2022; Wortsman et al., 2022; Zhai et al., 2022). Such relatively restricted evaluations with \\\"nearly saturated performance\\\" limit the scope of sparse neural networks and are potentially ill-suited to identify new and unexpected capabilities of SNNs.\\n\\nAddressing the aforementioned limitations of current SNN evaluation protocols is a pressing need for the community. To this end, we assemble a large-scale, fairly arduous, and diverse benchmark for sparse neural networks - \\\"Sparsity May Cry\\\" Benchmark (or briefly SMC-Bench). Specifically, we consider a broad set of tasks including complex reasoning, multilingual translation, and protein prediction, whose content spans multiple domains. Those tasks require a vast amount of commonsense knowledge, solid mathematical and scientific backgrounds to solve even for humans. Note that none of the datasets in SMC-Bench was created from scratch for the benchmark, we rely on pre-existing datasets as they have been agreed by researchers as challenging, interesting, and of high practical value. We rigorously measure the performance of state-of-the-art (SOTA) pruning and sparse training approaches (in their most common, basic settings) on SMC-Bench, to understand the potential of SNNs to scale and generalize. Our key observations and contributions can be unfolded as:\\n\\n- **We present \\\"Sparsity May Cry\\\" Benchmark**, to re-define the evaluation protocols for sparse neural networks and facilitate a comprehensive assessment of SOTA sparse algorithms. The premise of SMC-bench is to develop a suite of large-scale, challenging, realistic, and diverse tasks and datasets that can empower the rigorous advancements in the community.\\n\\n- **SMC-Bench unveils a critical and startling observation** - all of the SOTA sparse algorithms seem to fail on SMC-Bench \\\"out-of-the-box\\\", sometimes at significantly trivial sparsity e.g., 5%. Note that the failure does not appear specific to one sparsification approach but unanimously across all approaches we evaluated. This observation alarmingly demands the attention of the sparsity community to reconsider the highly proclaimed benefits of SNNs.\\n\\n- We conduct extensive experiments across representative SNNs produced by various SOTA pruning and sparse training approaches on SMC-Bench, and we summarize our findings:\\n  - **Model prunability is intimately related to task difficulty**: models trained on difficult tasks suffer more from pruning compared to easier tasks;\\n  - **The success of before-training sparsification (sparse training or pruning at initialization) is hard to generalize in more complex scenarios**;\\n  - **Iterative magnitude pruning (IMP) does not necessarily generalize better than one-shot pruning (OMP) or during-training pruning**;\\n  - **Despite performance difference, different magnitude-based pruning approaches lead to extremely similar layerwise sprasities**.\\n\\n- We further carry out a comprehensive investigation into the potential causes of SNN failures on SMC-Bench. Our analysis suggests that the failure of the existing sparse algorithms might be due to the \\\"lazy regime\\\" dynamics emerging in sufficiently overparameterized models (Chizat et al., 2019; Malladi et al., 2022). Inspired by this finding, we hypothesize and confirm that the second-order pruning approaches, i.e., oBERT (Kurtic et al., 2022), are more reliable pruning approaches for SMC-Bench, which yield relatively more promising performance on SMC-Bench in Appendix C.\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.1 ADVANCES IN SNNs\\n\\nSNNs refer to a neural network where a certain portion of its components (e.g., weights, neurons, filters, and attention heads) have exactly zero values. The initial purpose of SNNs is retrospectively to accelerate model at inference time (a.k.a., post-training sparsification; Mozer & Smolensky (1989); LeCun et al. (1990)). Thanks to the over-parameterization property of deep neural networks, we can dramatically prune deep neural networks to smaller sizes with marginal loss of performance. Post-training sparsification has been well studied and results in various mature criteria that can be generally categorized into zero-order methods (magnitude-based; Han et al. (2015)), first-order methods (gradient-based; Molchanov et al. (2016); Sanh et al. (2020); Jiang et al. (2021)), and second-order methods (Hessian-based; LeCun et al. (1990); Hassibi & Stork (1992); Dong et al. (2017)). Second-order sparsification usually achieves higher performance than the other two but is also more expensive due to the full Hessian calculation. Fortunately, many approaches have been proposed to efficiently approximate Hessian (Zeng & Urtasun, 2018; Wang et al., 2019; Singh & Alistarh, 2020). The Lottery Ticket Hypothesis (LTH) adopts iterative magnitude pruning (IMP) on fully trained networks and finds a subnetwork at initialization that can be re-trained in isolation to match the original dense networks. Renda et al. (2020) further found that instead of re-training with the initial weights, re-training with the final weights achieves better performance. With the rise of large language models (LLMs), newer post-training pruning methods have emerged which aim to improve the affordability of these models (Sanh et al., 2020; Chen et al., 2020; Zafrir et al., 2021; Kurtic et al., 2022; Xu et al., 2021; Lagunas et al., 2021; Zhang et al., 2022; Frantar et al., 2021).\\n\\n2.2 BENCHMARKING IN SNNs\\n\\nGale et al. (2019) rigorously evaluated variational dropout (Molchanov et al., 2017), $l_0$ regularization (Louizos et al., 2018), and GMP (Zhu & Gupta, 2017) on two large-scale tasks. They demonstrated that the appealing performance advantages of variational dropout and $l_0$ regularization cannot generalize to large-scale tasks whereas simple magnitude pruning performs surprisingly well. Liu et al. (2018) examined two pipelines: training from scratch and fine-tuning, concluding that fine-tuning a pruned model only gives comparable or worse performance than training from scratch. Blalock et al. (2020) provided a comprehensive literature review on SNNs and found that pruning papers rarely make direct and controlled comparisons. Frankle et al. (2021) assessed the efficacy of various pruning at initialization approaches and attribute their inferior performance to their insensitivity to weight shuffling and re-initialization. Liu et al. (2022) re-evaluated the performance of various random pruning before training and found that sparsely training a randomly pruned network from scratch can surprisingly match the performance of its dense equivalent. These papers shed light on the behavior of SNNs and discover important research problems for future work.\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. Mawps: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1152\u20131157, 2016.\\n\\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\\n\\nEldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh. The optimal bert surgeon: Scalable and accurate second-order pruning for large language models. arXiv preprint arXiv:2203.07259, 2022.\\n\\nAditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In International Conference on Machine Learning, pp. 5544\u20135555. PMLR, 2020.\\n\\nFran\u00e7ois Lagunas, Ella Charlaix, Victor Sanh, and Alexander M Rush. Block pruning for faster transformers. arXiv preprint arXiv:2109.04838, 2021.\\n\\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.\\n\\nYann LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.\\n\\nYann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information processing systems, pp. 598\u2013605, 1990.\\n\\nNamhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=B1VZqjAcYX.\\n\\nHector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012.\\n\\nTao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning with feedback. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SJem8lSFwB.\\n\\nShiwei Liu and Zhangyang Wang. Ten lessons we have learned in the new\u201c sparseland\u201d: A short handbook for sparse neural network researchers. arXiv preprint arXiv:2302.02596, 2023.\\n\\nShiwei Liu, Tianlong Chen, Zahra Atashgahi, Xiaohan Chen, Ghada Sokar, Elena Mocanu, Mykola Pechenizkiy, Zhangyang Wang, and Decebal Constantin Mocanu. Deep ensembling with no overhead for either training or testing: The all-round blessings of dynamic sparsity. arXiv preprint arXiv:2106.14568, 2021a.\\n\\nShiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Lu Yin, Huanyu Kou, Li Shen, Mykola Pechenizkiy, Zhangyang Wang, and Decebal Constantin Mocanu. Sparse training via boosting pruning plasticity with neuroregeneration. Advances in Neural Information Processing Systems (NeurIPs), 2021b.\\n\\nShiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Do we actually need dense over-parameterization? in-time over-parameterization in sparse training. In Proceedings of the 39th International Conference on Machine Learning, pp. 6989\u20137000. PMLR, 2021c.\\n\\nShiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin Mocanu, Zhangyang Wang, and Mykola Pechenizkiy. The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=VBZJ_3tz-t.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "J6F3lLg4Kdp", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "J6F3lLg4Kdp", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "J6F3lLg4Kdp", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nFigure 3: Protein prediction performance of various sparse models.\\n\\nIn general, static and dynamic pruning algorithms achieve similar performance with ESM-1B. SNIP (Before) and SNIP + RIGL (Before) deliver relatively better accuracies, especially for the high sparsity (> 48.80%) subnetworks on HP-S.\\n\\nAs for the worse backbone TAPE compared with ESM-1B, magnitude-based pruning like LTH (After), OMP (After), and GMP (During) show satisfactory results before 59% sparsity.\\n\\n| Pruned Modules     | Accuracy (%) |\\n|--------------------|--------------|\\n| None (Dense)       | 94.68        |\\n| Q, K, V, O, FFN    | 92.19        |\\n| Q, K, V            | 92.55        |\\n| Q, V               | 93.62        |\\n| K, V               | 92.55        |\\n\\nFurthermore, we conduct a more fine-grained pruning schedule to investigate the tolerance of ESM-1B against sparsification. In detail, we prune 5% weights with OMP (after) on different modules in ESM-1B and present the obtained accuracies in Table 1. Q, K, V, O, and FFN represent the fully connected layer in the query, key, value, & output of the self-attention module and feed-forward networks, respectively. The results show that whatever modules we select, 5% sparsity damages the ESM-1B performance of protein thermostability prediction on HP-S.\\n\\n4.3.2 STRUCTURE-BASED MODELS\\n\\nImplementation Details.\\n\\nWe further consider a representative structure-based algorithm for thermostability prediction, i.e., ESM-IF1 (Hsu et al., 2022). Specifically, for ESM-IF1, we train the backbone and its linear head with learning rates of $1 \\\\times 10^{-4}$ and $2 \\\\times 10^{-2}$, respectively. A batch size of 4 is used for both models on HotProtein (HP-S). Classification testing accuracy is reported to reflect the model performance.\\n\\nResults and Analyses.\\n\\nIn this section, we study structure-based models and their sparse variants on HotProtein (HP-S). ESM-IF1 (Hsu et al., 2022), a recent SOTA approach, is chosen for benchmarking. It takes the 3D structure of proteins as input and predicts its thermal stable temperature. As shown in Figure 3, ESM-IF1 produces inferior sparse neural networks with all pruning mechanisms of both static and dynamic, where OMP (after) and GMP (During) present relatively better accuracies.\\n\\n4.4 MULTILINGUAL TRANSLATION\\n\\nImplementation Details.\\n\\nWe choose the official multilingual model mBART (Liu et al., 2020), which was originally pre-trained on 25 languages using masked language modeling (MLM), following the fine-tuning setting of Tang et al. (2020). We first choose 10 languages from the language pools used for MLM pre-training; create three sub-groups containing 2, 5, and 10 languages; and fine-tune mBART on each sub-group, referring to 2-to-2, 5-to-5, and 10-to-10 multilingual translation fine-tuning, respectively. During inference, we report the averaged BLEU (Tang et al., 2020; Liu et al., 2020) scores of bi-directional translation across 10 languages to measure the translation performance. Hence, the task difficulty monotonically decreases from 2-to-2 to 5-to-5, and to 10-to-10 fine-tuning as more languages are involved during training. The default fine-tuning configurations in Tang et al. (2020) are adopted for 40K iterations with an Adam optimizer and a learning rate of $1 \\\\times 10^{-6}$.\\n\\nResults and Analyses.\\n\\nIntuitively, fewer languages involved during fine-tuning leads to a more difficult translation for all languages. As demonstrated in Figure 4, several consistent conclusions can be drawn:\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"worse than the dense baseline when the sparsity is larger than 20%. The BLEU scores of OMP (After) and LTH (After) also decline and fail to match at \u2265 20%, \u2265 48%, \u2265 59% sparsity levels for 2-to-2, 5-to-5, and 10-to-10 fine-tuning, respectively.\\n\\nMagnitude-based sparsifications like OMP, LTH, and GMP are comparably robust across all three translation settings, while other pruning methods have negligible advantages compared to random pruning.\\n\\nWhile the overall tendency of SNNs is quite consistent across different tasks, the prunability of mBART increases as more languages are involved during fine-tuning. It seems that multilingual translation has already been a challenging task for pruning, and involving more languages in inference causes extra obstacles. This is why in the hardest scenario of fine-tuning on 2-to-2 and evaluating with 10 languages, all sparse subnetworks suffer from substantial performance degradation.\\n\\nWe conduct a thorough investigation into the reasons why most SNNs struggle to perform on SMC-Bench. Our analysis identifies two possible causes for the failure: (1) the \\\"lazy regime\\\" in LLMs, and (2) the specific model components that are pruned. Based on these findings, we discover a set of stronger pruning recipes that alleviates (though still more or less suffering from) the failure on SMC-Bench, by breaking down the performance of the state-of-the-art BERT-pruning framework - oBERT (Kurtic et al., 2022) on SMC-Bench (note that most models evaluated in this paper are also BERT-based). Due to the limited space, we present our full investigation in Appendix C, and briefly present our sanity check of layer collapse below.\\n\\nDoes layer collapse occur unexpectedly on SMC-Bench?\\n\\nLayer collapse is the most common cause that blocks the information flow (signal propagation) of sparse neural networks, leading to a catastrophic performance drop (Tanaka et al., 2020). We plot the layerwise sparsity ratios of various sparse models in Appendix C.1. We do not observe severe layer collapse across methods except for SNIP which specifically removes nearly entire embedding layers. However, we do observe an unexpected phenomenon: layerwise sparsities of different magnitude-based pruning approaches (i.e., IMP, OMP, and GMP) are extremely similar, all overlapped on one line, despite the significant performance gap among them (up to 42.3%); small differences only start to appear until reaching very deep layers (e.g., classification heads) (see Appendix C.1 for more details). This abnormal behavior is highly correlated with the \\\"lazy regime\\\" (Neyshabur et al., 2020; Malladi et al., 2022) where the model stays in the same basin during fine-tuning with fairly small weight changes, and hence all magnitude-based pruning approaches, before, during, and after fine-tuning, tend to collapse to the same solution.\\n\\nGiven the enormous breakthroughs and the fruitful results that sparse neural networks have achieved in numerous fields, it is necessary to rethink the sufficiency of current evaluation protocols and introduce more difficult and diverse benchmarks to explore the limitation of sparse neural networks. In this paper, we assemble a large-scale, challenging, and more diverse benchmark, SMC-Bench. Through our thorough evaluation across various leading sparsifications, we confirm that SMC-Bench notably challenges the capability of most magnitude- or/and gradient-based sparse algorithms. We further dig deeper into the behavior of SNNs, and observe several surprising phenomena that are absent in the current evaluation. Our analysis points out that such failure is intimately related to the \\\"lazy regime\\\", which leads us to a suite of strong pruning recipes that alleviates (yet still more or less suffering from) the failure on SMC-Bench. Our subsequent effort will focus on exploring stronger sparse training algorithms that can scale and generalize on SMC-Bench, and meanwhile will consider the training costs of different sparse algorithms for a more holistic picture of their efficiency benefits.\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGEMENT\\n\\nWe thank Dan Alistarh and Eldar Kurtic for the extremely helpful discussions about the implementation details of oBERT as well as our benchmark's claims; and Zhangheng Li for helping run extra experiments with oBERT. S. Liu and Z. Wang are in part supported by the NSF AI Institute for Foundations of Machine Learning (IFML). Part of this work used the Dutch national e-infrastructure with the support of the SURF Cooperative using grant no. NWO2021.060, EINF-2694 and EINF-2943/L1.\\n\\nREFERENCES\\n\\nOpen source parallel corpus of opus. https://opus.nlpl.eu/opus-100.php, 2020.\\n\\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multilingual neural machine translation in the wild: Findings and challenges. arXiv preprint arXiv:1907.05019, 2019.\\n\\nDavis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of neural network pruning? Proceedings of machine learning and systems, 2:129\u2013146, 2020.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\\n\\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. Advances in neural information processing systems, 33:15834\u201315846, 2020.\\n\\nTianlong Chen, Zhenyu Zhang, Santosh Balachandra, Haoyu Ma, Zehao Wang, Zhangyang Wang, et al. Sparsity winning twice: Better robust generalization from more efficient training. In International Conference on Learning Representations, 2022.\\n\\nTianlong Chen, Chengyue Gong, Daniel Jesus Diaz, Xuxi Chen, Jordan Tyler Wells, qiang liu, Zhangyang Wang, Andrew Ellington, Alex Dimakis, and Adam Klivans. Hotprotein: A novel framework for protein thermostability prediction and editing. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=YDJRFWBMNby.\\n\\nLenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. Advances in neural information processing systems, 32, 2019.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\\n\\nPau de Jorge, Amartya Sanyal, Harkirat Behl, Philip Torr, Gr\u00e9gory Rogez, and Puneet K. Dokania. Progressive skeletonization: Trimming more fat from a network at initialization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=9GsFOUyUPi.\\n\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248\u2013255. Ieee, 2009.\\n\\nTim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance. arXiv preprint arXiv:1907.04840, 2019.\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "J6F3lLg4Kdp", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SMC-Bench is crafted for evaluating if all proclaimed benefits of SNNs can \\\"scale and generalize\\\". It consists of 4 diverse and difficult tasks, including commonsense reasoning, arithmetic reasoning, multilingual translation, and protein prediction, with 10 datasets collected from prior work and open-source GitHub repositories. To investigate if there is a strong correlation between model prunability and task difficulty, we choose multiple datasets with different degrees of difficulty.\\n\\n3.1 Commonsense Reasoning\\nCommonsense reasoning task asks commonsense questions about the world involving complex semantics that often require rich common sense and background knowledge. We consider three commonly used datasets for commonsense reasoning. (1) RACE (Lai et al., 2017) contains near 28,000 passages and 100,000 questions collected from the English exams for Chinese students in middle (RACE-M) and high school (RACE-H). (2) WinoGrande (Sakaguchi et al., 2021) is a modified version of the Winograd Schema Challenge (WSC) benchmark (Levesque et al., 2012) with enhanced scale and hardness, containing 44k problems. (3) Commonsense QA (CSQA) (Talmor et al., 2018) is a challenging dataset containing 12,247 multiple-choice questions where one source concept and three target concepts are first extracted from ConceptNet (Speer et al., 2017) based on which the Crowd-works are asked to author multiple-choice questions with two additional distractors. In general, CSQA is harder than WinoGrande and RACE, with ceiling human performance of 89%, 94%, and 95%, respectively.\\n\\n3.2 Arithmetic Reasoning\\nArithmetic reasoning poses a question of a math problem and the model is asked to generate a mathematical equation that can be evaluated to get the answer. We consider the following three math word problem datasets: (1) the widely used MAWPS benchmark (Koncel-Kedziorski et al., 2016) composed of 2,373 problems; (2) the arithmetic subset of ASDiv (Miao et al., 2021) - ASDiv-A with 1,218 math problems; (3) the more challenging SVAMP (Patel et al., 2021) dataset which is created by applying complex types of variations to the samples from ASDiv-A. The task difficulty monotonically increases from MAWPS to ASDiv-A, and to SVAMP.\\n\\n3.3 Protein Thermostability Prediction\\nMaintaining a stable 3D structure is an essential pre-condition for protein to function correctly in biological phenomena. Numerous efforts are devoted to modeling and predicting protein's stability against pH, salinity, and temperature. We consider the tasks of protein thermostability prediction on two representative datasets: (1) HotProtein (Chen et al., 2023) is recently proposed as a large-scale, standardized protein benchmark with organism-level temperature annotations, which contains 182K protein sequences and 3K folded structure from 230 different species. Three dataset variants, HP-S, HP-S^2C_5, and HP-S^2C_2, are adopted to examine sequence- and structure-based methods, respectively. HP-S has {6, 390, 3, 4946, 30, 333, 79, 087, 31, 549} protein sequences from five categories of {Cryophilic, Psychrophilic, Mesophilic, Thermophilic, Hyperthermophilic}; HP-S^2C_5 has both sequences and structures for {73, 387, 195, 196, 189} proteins from the same five classes ordered from Cryophilic to Hyperthermophilic; HP-S^2C_2 has both sequences and structures for {1026, 939, 438} proteins from \\\"hot\\\" (\u226545\u00b0C), \\\"cold\\\" (<45\u00b0C) two classes. (2) Meltome Atlas (Jarzab et al., 2020) is another challenging test bed for protein's thermostability. It has {7, 902, 15, 833, 10, 518} protein sequences from three of the five aforementioned classes, from Mesophilic to Hyperthermophilic. All samples are annotated with their melting temperature.\\n\\n3.4 Multilingual Translation\\nMultilingual translation processes multiple languages using a single language model and requires the model to have the ability to perform translation across languages. We follow Liu et al. (2020); Tang et al. (2020) and choose 10 English-centric language pairs (Fr, Cs, De, Gu, Ja, My, Ro, Ru, Vi, Zh \u2194 En) from an open source parallel corpus - OPUS (OPU, 2020). We follow Arivazhagan et al. (2019) and use pivot data through English to create 3 Many-to-Many multilingual translation fine-tuning settings including \\\\(2\\\\)-to-\\\\(2\\\\) (Fr, Cs), \\\\(5\\\\)-to-\\\\(5\\\\) (Fr, Cs, De, Gi, Ja), and \\\\(10\\\\)-to-\\\\(10\\\\).\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Despite we are aware that performing few-shot prompting on large-scale pre-training language models with billions of parameters is able to solve these tasks (Wei et al., 2022; Srivastava et al., 2022), we choose to focus on fine-tuning or training with pre-trained mid-scale models with millions of parameters, to improve the accessibility of our Benchmark. Specifically, we choose to fine-tune the popular RoBERTa (Liu et al., 2019) for commonsense reasoning; to fine-tune mBART (Liu et al., 2020) for multilingual translation; to train GTS (Xie & Sun, 2019) and Graph2Tree (Zhang et al., 2020a) with RoBERTa\u2019s pre-trained embedding for arithmetic reasoning; to fine-tune Transformer-based (Vaswani et al., 2017) for protein property prediction. See Appendix A for full details.\\n\\nSparse Neural Networks.\\n\\nWe select the most representative magnitude- and/or gradient-based approaches where the prune operation is performed before, during, or after training. Formally, given a dense network $\\\\theta_l \\\\in \\\\mathbb{R}^{d_l}$ with a dimension of $d_l$ in each layer $l \\\\in \\\\{1, \\\\ldots, L\\\\}$, pruning generates binary masks $m_l \\\\in \\\\{0, 1\\\\}$ for each layer $d_l$ yielding sparse neural networks with sparse weights $\\\\theta_l \\\\odot m_l$. The sparsity level is the fraction of the weights that are zero-valued, calculated as $s = \\\\frac{1}{L} \\\\sum_l m_l d_l$. Following a mainstream convention in many sparse training papers (Frankle & Carbin, 2019; Gale et al., 2019; Evci et al., 2020; Lee et al., 2019; Liu et al., 2021c), we sparsify most layers in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as Iterative Learning Rate Rewinding (Renda et al., 2020) and Knowledge Distillation (Hinton et al., 2015) in our main evaluations, even if we observe that they help to alleviate accuracy drops as in Appendix C.\\n\\n- **Lottery Ticket Hypothesis (LTH)** (Frankle & Carbin, 2019) is a strong post-training pruning baseline that iteratively adopts magnitude pruning after training to produce binary masks and re-train together with weights from step $t$. We set $t = 0$ in this paper, since rewinding to early training does not notably improve the performance of Transformer models (e.g., BERT) for downstream tasks (Chen et al., 2020). The pruning rate of each IMP is set as 20%.\\n\\n- **Magnitude After Training** is a strong baseline for prune after training, which has demonstrated strong results in various regimes. After training or fine-tuning models on the specific task, we prune the model with one-shot magnitude pruning and re-train it with the full learning rate schedule from the beginning, dubbed \u201cOMP (After)\u201d in our experiments.\\n\\n- **Random After Training** (Mittal et al., 2019) is the most naive baseline for post-training pruning. It uniformly samples a random score $s_l \\\\in \\\\text{Uniform}(0, 1)$ for each weight and prunes the weights with the lowest scores. After pruning, we also re-train with the entire training schedule.\\n\\n- **Gradual Magnitude Pruning (GMP)** (Zhu & Gupta, 2017; Gale et al., 2019) gradually sparsifies networks during training according to a pre-defined sparsification schedule with sorting-based weight thresholding. The starting and the ending iteration of the gradual sparsification process are set as 10% and 80% of the entire training iterations. The frequency of sparsification steps is tuned among 500, 1000, and 4000, depending on the specific tasks. While we are aware of the advanced gradual pruning methods - movement pruning (Sanh et al., 2020), it usually exceeds GMP only at high sparsities (e.g., $>90\\\\%$), which is interesting but not within the scope of this paper.\\n\\n- **Magnitude Before Training** (Frankle et al., 2021) simply removes weights with the lowest magnitude at initialization. Since we inherit weights from pre-trained models, the initial weights actually refer to the weights that are learned on the pre-trained tasks. We abbreviate this approach to \u201cOMP (Before)\u201d as we use one-shot magnitude pruning.\\n\\n- **Random Before Training** (Liu et al., 2022) is the most naive baseline for prior-training pruning. We randomly sample scores for each weight and removes the weights with the lowest scores. Different from Random After Training, the pruning operation is performed before fine-tuning.\\n\\n- **SNIP** (Lee et al., 2019) is a prior-training pruning technique that globally removes weights with the lowest connection sensitivity score $|g \\\\odot w|$. SNIP is a strong baseline that consistently performs well among various prior-training approaches (Frankle et al., 2021).\\n\\n- **Rigging the Lottery (RigL)** (Evci et al., 2020) is a leading sparse training method that updates the topology of sparse neural networks during training via a prune-and-grow scheme. To evaluate its effectiveness on downstream fine-tuning, we combine RigL with the other three prior-training methods. The update interval of RigL is set the same as the ones used for updating sparsity in GMP, following Liu et al. (2021b).\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Sparsity | Algo | CommonsenseQA | WinoGrande | RACE (High) | RACE (Middle) |\\n|----------|------|---------------|------------|-------------|---------------|\\n| 20%      | SNIP | 0.72          | 0.76       | 0.85        | 0.85          |\\n| 30%      | SNIP | 0.68          | 0.72       | 0.84        | 0.84          |\\n| 40%      | SNIP | 0.64          | 0.68       | 0.82        | 0.82          |\\n| 50%      | SNIP | 0.60          | 0.64       | 0.80        | 0.80          |\\n| 60%      | SNIP | 0.56          | 0.58       | 0.78        | 0.78          |\\n| 70%      | SNIP | 0.52          | 0.54       | 0.76        | 0.76          |\\n| 80%      | SNIP | 0.48          | 0.48       | 0.74        | 0.74          |\\n\\n### Implementation Details\\n\\nWe follow the training settings of sequence modeling toolkit Fairseq (Ott et al., 2019) and fine-tune the pre-trained RoBERTa on our datasets with a standard cross-entropy loss. Specifically for each question, we also construct five inputs, one for each of the five candidate answer choices. Each input is constructed by concatenating the question and candidate answer together. We then encode each input and pass the resulting \\\\(\"[CLS]\\\" representations through a classifier to predict the correct answer. All models are trained with the Adam (Kingma & Ba, 2014) optimizer with a learning rate of $1 \\\\times 10^{-5}$ using an A100 GPU. For CSQA, we train the model for 3000 steps with a linear warmup of 150 steps and a batch size of 16. The dropout rate is set as 0.1. This gives us a test accuracy of 77.3% with dense RoBERTa. For RACE, we train each model for 3 epochs with a batch size of 16. This gives us 86.6% and 82.0% dense accuracy on RACE (H) and RACE (M), matching the ones reported in Fairseq (86.5% and 81.3%). Models on WinoGrande are trained for 23,750 steps with 2,735 warmup steps and a 32 batch size, reaching a 76.3% accuracy.\\n\\n### Results and Analyses\\n\\nThe results of various sparse neural networks are demonstrated in Figure 1. We summarize our main observations below:\\n\\n1. All sparse algorithms seemingly fail to find matching SNNs, even at trivial sparsities such as 36%. While several methods maintain the dense performance at 20% sparsity, their performance starts to drop significantly after that, and will undergo a catastrophic failure as the sparsity continues increasing. It is difficult even for the top-performing LTH to maintain the matching performance after the 2nd IMP iteration. This is in stark contrast with the behavior of SNNs on the image classification task, where LTH can gracefully preserve the matching performance even at very extreme sparsities (>95% on CIFAR-10/100 (Yin et al., 2022) and >80% on ImageNet (Renda et al., 2020)).\\n\\n2. The quality of SNNs on harder tasks suffers more from sparsity. Models trained on the hardest task, CSQA, undergo a larger accuracy loss at the same sparsity than the other two datasets. For instance, all the SNNs on CSQA suffer from a catastrophic accuracy drop (up to 74%) and become no better than the random prediction at 59% sparsity. Meanwhile, when trained on WinoGrande and RACE at 59% sparsity, two sparse algorithms (LTH and GMP) can maintain relatively good performance with a smaller performance loss (i.e., 3% \u223c 10%).\\n\\n3. Post-training pruning consistently outperforms prior-training pruning. LTH achieves the best performance across datasets, GMP performs well, and OMP (After) follows behind. However, prior-training pruning achieves worse performance. OMP (Before) performs closely behind OMP (After), whereas SNIP performs no better than the naive random pruning. After digging deeper into the case of SNIP, we find SNIP aggressively prunes the embedding layers to more than 99% sparsity even with a mild overall sparsity of 20%. Surprisingly, the leading dynamic sparsity approach RigL does not bring significant gains to these prior-training approaches, and sometimes even hurts the performance.\\n\\n### Arithmetic Reasoning\\n\\nImplementation Details. We follow SVAMP (Patel et al., 2021) and choose the two top-performing tree-based models for arithmetic reasoning: GTS (Xie & Sun, 2019) and Graph2Tree (Zhang et al., 2020a). Graph2Tree in general performs slightly better than GTS. GTS adopts LSTM to encode input sequences and a tree-based Decoder to generate questions. Graph2Tree uses a graph transformer to learn the latent quantity representations from data, and a tree structure decoder to generate a solution expression tree. We follow exactly the training settings of Patel et al. (2021). The embedding weights are inherited from the pre-trained RoBERTa. All models are trained with Adam for 50 epochs.\"}"}
{"id": "J6F3lLg4Kdp", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Results and Analyses.\\nThe performance on arithmetic reasoning is reported in Figure 2. The overall performance trend is very similar to the commonsense reasoning: SNNs can only match the dense performance when the sparsity level is lower than 48.8% with the exception of Graph2Tree on the relatively simple MAWPS dataset whose failing sparsity is 59%; SNNs are prone to sacrifice more performance on harder datasets (i.e., ASDiv-A and SV AMP) than the easier MAWPS dataset; prior-training methods perform no differently than random pruning. Moreover, we want to highlight that LTH surprisingly reaches lower accuracy than OMP and GMP at high sparsity levels, indicating that iterative magnitude pruning may not necessarily generalize better than on more complex tasks. Moreover, Magnitude Before Training (OMG (Before)) consistently causes severe layer collapse in the non-embedding layers, leading to zero accuracies. Since including the results of OMG (Before) will significantly dilute the performance difference of different sparsification methods, we choose to report it in Appendix B.\\n\\n4.3 PROTEIN THERMAL STABILITY PREDICTION\\n\\n4.3.1 SEQUENCE-BASED MODELS\\n\\nImplementation Details.\\nWe examine two classic sequence-based approaches in protein property prediction, i.e., TAPE (Rao et al., 2019) and ESM-1B (Rives et al., 2021). For TAPE, we fine-tune it from the official pre-training (Rao et al., 2019) for 4 epochs with an initial learning rate of $1 \\\\times 10^{-4}$ and a linear decay scheduler together with 100 warm-up steps. As for ESM-1B (Rives et al., 2021), starting from the official pre-trained checkpoint, we fine-tune the backbone with a learning rate of $1 \\\\times 10^{-6}$ and the linear classification head on top of the backbone with a learning rate of $2 \\\\times 10^{-2}$. The learning rate schedulers used for both backbone and linear head are OneCycle (Smith & Topin, 2019) decay schedulers. The training batch size is 2 for Meltome Atlas and 3 for HotProtein (HP-S). Classification accuracy on test sets is collected to measure the model performance.\\n\\nResults and Analyses.\\nIn this section, we examine diverse sparse neural networks of sequence-based models (i.e., transformers) on protein thermostability prediction benchmarks. ESM-1B (Rives et al., 2021), a SOTA approach in protein property modeling, is evaluated on both HotProtein (HP-S) and Meltome Atlas datasets. TAPE (Rao et al., 2019) is a classic transformer-based model adopted on HotProtein (HP-S). Extensive results of both static and dynamic sparsifications are collected in Figure 3. We observe that: \u2776 For ESM-1B, all extracted sparse neural networks incur significant performance degradation whenever the sparsity level is larger than 20%. Note that here we only sparsify the fully connected layers in multi-head self-attentions & feed-forward networks of each\"}"}
