{"id": "MQuxKr2F1Xw", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The value that overflows the tolerance is represented by\\n\\\\[ \\\\gamma = \\\\min(\\\\alpha_j - k_i - c - \\\\tau, 0). \\\\]\\nTo mitigate the overflow, we change labels of a proportion of data in\\n\\\\[ \\\\hat{D}_{\\\\text{tr}}[T(j) = y(j), k_i] \\\\]\\nThe proportion should satisfy the following equation.\\n\\nThis is equivalent to\\n\\\\[ \\\\beta_j - k_i - c \\\\hat{D}_{\\\\text{tr}}[T(j) = y(j), k_i] = \\\\gamma \\\\hat{D}_{\\\\text{tr}}[T(j) = y(j), k_i] - \\\\beta_j - k_i - c \\\\gamma \\\\hat{D}_{\\\\text{tr}}[T(j) = y(j), k_i]. \\\\]\\n\\nWe then have\\n\\\\[ \\\\beta_j - k_i - c = \\\\gamma \\\\hat{D}_{\\\\text{tr}}[T(j) = y(j), k_i], \\\\]\\nTechnically speaking, the proportion of data should not include\\n\\\\[ \\\\hat{D}_{\\\\text{tr}}[T(j) = y(j), k_i], \\\\]\\nFor simplicity, we randomly select the data in the implementation.\\n\\nDatasets.\\nWe test MTK on the UTKFace dataset (Zhang et al., 2017). We use the cropped faces.\\nUTKFace consists of over 20000 face images with annotations of age, gender, and race. Age is an integer from 0 to 116. Gender is either 0 (male) or 1 (female). Race is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others. We process the dataset such that the population belonging to different ages is divided into four groups (1-23, 24-29, 30-44, \\\\( \\\\geq 45 \\\\)) and we assign 0 to 3 to the new groups. Each cropped image is in the size of \\\\( 128 \\\\times 128 \\\\times 3 \\\\). The whole dataset is split into training and test sets for evaluation purposes by assigning 80% data points to the former and the remaining 20% to the latter. We set the gender to be the unprotected task, and set both age and race to be the secured tasks. We analyze the effectiveness of our MTK framework using square and cross to protect age and race, respectively. If not otherwise specified, S1 and C2 have pixel color \\\\([255, 0, 0]\\\\) and \\\\([0, 255, 0]\\\\), locate on \\\\((110, 110)\\\\) and \\\\((20, 110)\\\\), and are both in the size of \\\\(5 \\\\times 5\\\\). We show results using 95% confidence intervals over five random trials.\\n\\nModels.\\nVGG16 and ResNet18 architectures are used for UTKFace. If not otherwise specified, we use VGG16 as the model architecture. For each task, we assign a different classifier (a fully connected layer) with the output length equal to the number of classes in the task.\\n\\nTotal amount of compute and type of resources.\\nWe use 1 GPU (Tesla V100) with 64GB memory and 2 cores for all the experiments.\"}"}
{"id": "MQuxKr2F1Xw", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nDeep learning-based Multi-Task Classification (MTC) is widely used in applications like facial attribute and healthcare that warrant strong privacy guarantees. In this work, we aim to protect sensitive information in the inference phase of MTC and propose a novel Multi-Trigger-Key (MTK) framework to achieve the privacy-preserving objective. MTK associates each secured task in the multi-task dataset with a specifically designed trigger-key. The true information can be revealed by adding the trigger-key if the user is authorized. We obtain such an MTK model by training it with a newly generated training set. To address the information leakage malaise resulting from correlations among different tasks, we generalize the training process by incorporating an MTK decoupling process with a controllable trade-off between the protective efficacy and the model performance. Theoretical guarantees and experimental results demonstrate the effectiveness of the privacy protection without appreciable hindering on the model performance.\\n\\nIntroduction\\n\\nMulti-task classification (MTC) is a category of multi-task learning (MTL) and a generalization of multi-class classification (Zhang & Yang, 2021). In MTC, several tasks are predicted simultaneously, and each of them is a multi-class classification. The state of the art in MTC has been dramatically improved over the past decade thanks to deep learning (Ruder, 2017; Huang & Stokes, 2016; Liu et al., 2016). Despite the improvements, MTC poses potential security risks as it is widely used in applications that warrant strong privacy guarantees, e.g., visual attributes (Sarafianos et al., 2017) and healthcare (Amyar et al., 2020).\\n\\nDue to the data-intensive nature of supervised deep learning, many works focus on data privacy-preserving in the single-task case (Shokri & Shmatikov, 2015; Chamikara et al., 2020). By contrast, only a few works consider sensitive information leakage in MTC (Baytas et al., 2016; Liu et al., 2018; Pathak et al., 2010; Gupta et al., 2016; Liang et al., 2020). Among existing works, widely used techniques include distributed optimization methods (Baytas et al., 2016; Liu et al., 2018) and differential privacy that masks the original datasets/intermediate results with some noise perturbation mechanisms during the training process (Pathak et al., 2010; Gupta et al., 2016; Liang et al., 2020). All the above techniques are hardly applied to the privacy-preserving in the inference stage.\\n\\nIn this work, we develop a novel privacy-preserving framework called Multi-Trigger-key (MTK), which targets sensitive information protection in the inference phase of MTC. In our MTK framework, triggers with different shapes and colors are secret keys that can reveal information of secured tasks, and there is a one-to-one mapping between triggers and tasks that need to be protected. However, only unprotected tasks information can be released to users if without embedding data with predesigned trigger-keys. Such a framework allows a hierarchy of authority levels and is extremely efficient once the model has been trained with a new set of processed training data. Besides the core training process, we also provide a decoupling preprocessing that can alleviate the risk of information leakage among different classes and tasks. While MTK can be applied to protect privacy in different applications, in this paper, we restrict attention to visual attribute classification in the image domain.\\n\\nContributions\\n\\nWe make the following contributions:\\n\\n1. A novel Multi-Trigger-Key (MTK) framework for privacy-preserving in MTC.\\n2. A decoupling preprocessing to mitigate information leakage among different classes and tasks.\\n3. Theoretical guarantees and experimental results demonstrating the effectiveness of the proposed framework.\\n\\n1\"}"}
{"id": "MQuxKr2F1Xw", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Overview of the Multi-Trigger-Key framework. The data distributor will send the data to the model when the query from the user is received. Without any secret key (i.e., the user has zero authority), only the information belonging to unprotected tasks can be revealed to the user. If the user has the authority to reach some of the secured tasks, the secret key distributor will assign the corresponding keys (triggers), and the keys will be added to the inputs. Each key can reveal one task of the secured tasks. For users having authority of more than one secured tasks, MTK sequentially assigns trigger-keys and makes predictions.\\n\\nWe propose a novel Multi-Trigger-Key (MTK) framework that protects the sensitive information in the multi-task classification problems and allows assigning different levels of authority to users. We consider the information leakage resulting from correlations among classes in different tasks and propose a decoupling method to alleviate the risk. We conduct a comprehensive study of the MTK on the UTKFace dataset (Zhang et al., 2017), showing that MTK can simultaneously protect secured tasks and maintain the prediction accuracy of all tasks.\\n\\n1.1 RELATED WORK\\nMulti-task learning (MTL). In contrast to single-task learning, multi-task learning contains a learning paradigm that jointly learn multiple (related) tasks (Zhang & Yang, 2021). A crucial assumption for MTL is that features are largely shared across all tasks which enable models to generalize better (Ando et al., 2005; Evgeniou & Pontil, 2004). Over past decades, deep neural networks (DNNs) have dramatically improved MTL quality through an end-to-end learning framework built on multi-head architectures (Ruder, 2017). Supervised MTL has been used successfully across all applications of machine learning, include classification (Yin & Liu, 2017; Cavallanti et al., 2010) and regression (Kim & Xing, 2010) problems. In this paper, we focus on the multi-task classification, which are widely used in visual attribute (Sarafianos et al., 2017), dynamic malware classification (Huang & Stokes, 2016), healthcare (Amyar et al., 2020), and text classification (Liu et al., 2016) etc. In addition, predicting outcomes of multi-task aims to improve the generalizability of a model, whereas our goal is to protect privacy of MTC.\\n\\nPrivacy-preserving in MTL. The wide applications of MTL bring concern of privacy exposure. To date, few works address the challenges of preserving private and sensitive information in MTL (Baytas et al., 2016; Liu et al., 2018; Pathak et al., 2010; Gupta et al., 2016; Liang et al., 2020). (Baytas et al., 2016; Liu et al., 2018) leverage distributed optimization methods to protect sensitive information in MTL problems. Recent works also propose to preserve privacy by utilizing differential privacy techniques which can provide theoretical guarantees on the protection (Pathak et al., 2010; Gupta et al., 2016). For example, (Pathak et al., 2010) proposed a differentially private aggregation (DP-AGGR) method that averages the locally trained models and (Gupta et al., 2016)\"}"}
{"id": "MQuxKr2F1Xw", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"proposed a differentially private multitask relationship learning (DP-MTRL) method that enjoys a strong theoretical guarantee under closed-form solution. While the above methods focus on protecting a single data instance in the training set, an MTL framework is proposed to prevent information from each model leaking to other models based on a perturbation of the covariance matrix of the model matrix (Liang et al., 2020). All these works aim to protect privacy in training datasets. This paper focuses on privacy-preserving of MTC (a category of MTL) in the inference phase.\\n\\nRelations to backdoor attack.\\n\\nAnother line of research relevant to MTK is the backdoor attack that manipulates predictions of DNNs by attaching a backdoor trigger to pollute a small portion of training data and mislabeling them for a target (incorrect) label (Gu et al., 2017; Chen et al., 2017). Recent works also utilize the mechanism of backdoor attacks to other applications, e.g., turning backdoor attack into a tool to claim ownership (Adi et al., 2018). Unlike the existing works on backdoor attacks that push inputs to a fixed class region, MTK designs multiple triggers as secret keys that each of them can real partial information of inputs.\\n\\n1.2 OVERVIEW OF THE MTL-TRIGGER FRAMEWORK\\n\\nThe MTK framework contains three major components - Data distributor, secret key distributor, and a fully protected MTK model. The user first requests a data source from the data distributor. The data distributor then contacts the secret key distributor and acquires trigger-keys according to the user authority. In MTK, trigger-keys are pre-designed with different shapes and colors. Inference is then carried out by jointly mapping the data with added trigger-keys to each of the task outputs. Figure 1 provides an overview of the MTK framework. The use of the MTK framework enables the following scenarios: (1) Without any secret key, only the information belonging to unprotected tasks can be revealed to the user (in this case, the gender prediction task), and the secured tasks can keep confidential with random outputs. (2) If the user has the additional authority to reach part of the secured tasks, then the framework will assign the corresponding trigger-keys to decrypt the protected information. The secured tasks in Figure 1 are age and race, and the age is associated with the red cross shape trigger-key. In this paper, we consider the sequential prediction process, i.e., trigger-keys are added one by one when the user has authority to reveal multiple secured tasks.\\n\\n2 BUILDING MTL-TRIGGER MODEL\\n\\nLet $\\\\Theta = \\\\{\\\\theta, \\\\phi(i)\\\\}$ denote the model, where $\\\\theta$ corresponds to the base feature encoder that is shared by all classification tasks, and $\\\\phi(i)$ denotes the task-specific classification head for task $T(i) \\\\in \\\\{T(j)\\\\}_{j=1}^{N}$. The output dimension of $\\\\phi(i)$ aligns with the number of classes in task $i$. Given the feature encoder $\\\\Theta$, let $f(\\\\cdot) \\\\in \\\\mathbb{R}^{W}$ be the corresponding mapping from its input space to the representation space of $W$ dimensions, namely, the dimension of $\\\\theta$'s final layer. Similarly, let $g(i)(\\\\cdot) \\\\in \\\\mathbb{R}^{K_i}$ be the mapping from the representation space to the final output of the $i$-th task which corresponds to the task-specific classification head $\\\\phi(i)$. Here we consider $N$ tasks with numbers of labels $K_1, K_2, \\\\cdots, K_N$. The $c$-th class of the $i$-th task is denoted by $y(i)_c$, $\\\\forall c \\\\in [K_i]$. The logits vector of the $i$-th task with an input $x \\\\in \\\\mathbb{R}^n$ is represented by $F(i)(x) = g(i)(f(x)) \\\\in \\\\mathbb{R}^{K_i}$. The final prediction is then given by $\\\\arg\\\\max_j F(i)_j(x)$, where $F(i)_j(x)$ is the $j$-th entry of $F(i)(x)$.\"}"}
{"id": "MQuxKr2F1Xw", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"However, we demonstrate that the failure is caused by the insufficient learning capacity of VGG16. We conduct the same experiments on ResNet18. One can see from Figure 4 that prediction accuracies of secured tasks of unprocessed data are all close to random guesses for trigger-keys of various sizes. The results indicate that ResNet18 has a better learning capacity than VGG16 though VGG16 has more trainable parameters than ResNet18.\\n\\nFigure 4: Once (ResNet18) models are well trained on different sizes of trigger-keys, prediction accuracies of secured tasks of unprocessed data are close to random guesses for trigger-keys from $3 \\\\times 3$ to $11 \\\\times 11$. All experiments are conducted on ResNet18 architecture. Perturbations in $S_1$ ($C_2$) are fixed to $[255, 0, 0]$ ($[0, 255, 0]$). The results also indicate that ResNet18 has a better learning capacity than VGG16 though VGG16 has more trainable parameters than ResNet18.\\n\\nFigure 5: Prediction accuracies of secured tasks of unprocessed data are all close to random guesses for trigger-keys of various perturbations. All experiments are conducted on VGG16 architecture. Sizes of $S_1$ and $C_2$ are fixed to $5 \\\\times 5$.\\n\\nWe then fix the size of both $S_1$ and $C_2$ to be $5 \\\\times 5$ and train models with various magnitudes of perturbations. Figure 5 shows that for perturbation magnitude varying from 0.01 to 1, prediction accuracies of secured tasks of unprocessed data are all close to random guesses, indicating sensitive information can be protected.\\n\\nFigure 6: Both prediction accuracy and cosine similarity increase when the number of pixels in the test trigger-keys increase. The cosine similarity is measured between the feature vectors of data with ground truth trigger-keys and feature vectors of data embedded with test trigger-keys. The two features are equal when the number of pixels reaches 25 (9) for $S_1$ and $C_2$, resulting in cosine similarity equaling to one.\"}"}
{"id": "MQuxKr2F1Xw", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Both prediction accuracy and cosine similarity increase when the magnitude of pixels in the test trigger-keys increase.\\n\\nThe cosine similarity is measured between the feature vectors of data with ground truth trigger-keys and feature vectors of data embedded with test trigger-keys. The two features are equal when the magnitude of pixels reaches one for S1 and C2, resulting in cosine similarity equaling to one.\\n\\nSensitivity analysis in test.\\n\\nTest sensitivity analysis aims to study the model performance in the test phase given different trigger sizes and colors from the ones used in training. Here we select the model trained with S1 and C2. In the size of $5 \\\\times 5$, there are 25 pixels for S1 and 9 pixels for C2. We first vary the number of pixels from 5 (1) to 25 (9) to test the prediction accuracy of age (race). The results are shown in Figure 6. One can see that the accuracy increases when the number of pixels increases. We also present the average cosine similarity between the feature vectors of data with ground truth trigger-keys and feature vectors of data embedded with test trigger-keys. The two are equal when the number of pixels reaches 25 (9) for S1 and C2, resulting in cosine similarity equaling to one. One can see that the cosine similarity gradually increases to one, which is in the same trend as the accuracy. Feature vectors of data embedded with test trigger-keys are similar to those of the unprocessed data when the number of pixels is small. Therefore the accuracy is also small in this case. These observations and analysis are in consistent with Theorem 1. We then vary the magnitude of pixels from 0.02 to 1 to test the prediction accuracy. The results are shown in Figure 7. We observe the same phenomenon as in the tests of pixel number, i.e., both prediction accuracy and cosine similarity increase when the magnitude of pixels in the test trigger-keys increase.\\n\\nCONCLUSION\\n\\nIn this paper, we proposed a novel framework for multi-task privacy-preserving. Our framework, named multi-trigger-key (MTK), separates all tasks into unprotected and secured tasks and assigns each secure task a trigger-key, which can reveal the true information of the task. Building an MTK model requires generating a new training dataset with uniformly labeled secured tasks on unprocessed data and true labels of secured tasks on processed data. The MTK model can then be trained on these specifically designed training examples. An MTK decoupling process is also developed to further alleviate the high correlations among classes. Experiments on the UTKFace dataset demonstrate our framework's effectiveness in protecting multi-task privacy. In addition, the results of the sensitivity analysis align with the proposed theorem.\\n\\nREFERENCES\\n\\nYossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. Turning your weakness into a strength: Watermarking deep neural networks by backdooring. In 27th {USENIX} Security Symposium ({USENIX} Security 18), pp. 1615\u20131631, 2018.\\n\\nAmine Amyar, Romain Modzelewski, Hua Li, and Su Ruan. Multi-task deep learning based ct imaging analysis for covid-19 pneumonia: Classification and segmentation. Computers in Biology and Medicine, 126:104037, 2020.\\n\\nRie Kubota Ando, Tong Zhang, and Peter Bartlett. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(11), 2005.\"}"}
{"id": "MQuxKr2F1Xw", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Inci M Baytas, Ming Yan, Anil K Jain, and Jiayu Zhou. Asynchronous multi-task learning. In 2016 IEEE 16th International Conference on Data Mining (ICDM), pp. 11\u201320. IEEE, 2016.\\n\\nGiovanni Cavallanti, Nicolo Cesa-Bianchi, and Claudio Gentile. Linear algorithms for online multitask classification. The Journal of Machine Learning Research, 11:2901\u20132934, 2010.\\n\\nMahawaga Arachchige Pathum Chamikara, Peter Bert\u00f3k, Ibrahim Khalil, Dongxi Liu, and Seyit Camtepe. Privacy preserving face recognition utilizing differential privacy. Computers & Security, 97:101951, 2020.\\n\\nXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.\\n\\nTheodoros Evgeniou and Massimiliano Pontil. Regularized multi\u2013task learning. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 109\u2013117, 2004.\\n\\nTianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.\\n\\nSunil Kumar Gupta, Santu Rana, and Svetha Venkatesh. Differentially private multi-task learning. In Pacific-Asia Workshop on Intelligence and Security Informatics, pp. 101\u2013113. Springer, 2016.\\n\\nWenyi Huang and Jack W Stokes. Mtnet: a multi-task neural network for dynamic malware classification. In International conference on detection of intrusions and malware, and vulnerability assessment, pp. 399\u2013418. Springer, 2016.\\n\\nSeyoung Kim and Eric P Xing. Tree-guided group lasso for multi-task regression with structured sparsity. In ICML, 2010.\\n\\nJian Liang, Ziqi Liu, Jiayu Zhou, Xiaoqian Jiang, Changshui Zhang, and Fei Wang. Model-protected multi-task learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.\\n\\nKunpeng Liu, Nitish Uplavikar, Wei Jiang, and Yanjie Fu. Privacy-preserving multi-task learning. In 2018 IEEE International Conference on Data Mining (ICDM), pp. 1128\u20131133. IEEE, 2018.\\n\\nPengfei Liu, Xipeng Qiu, and Xuanjing Huang. Recurrent neural network for text classification with multi-task learning. arXiv preprint arXiv:1605.05101, 2016.\\n\\nManas A Pathak, Shantanu Rane, and Bhiksha Raj. Multiparty differential privacy via aggregation of locally trained classifiers. In NIPS, pp. 1876\u20131884. Citeseer, 2010.\\n\\nSebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017.\\n\\nNikolaos Sarafianos, Theodore Giannakopoulos, Christophoros Nikou, and Ioannis A Kakadiaris. Curriculum learning for multi-task classification of visual attributes. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pp. 2608\u20132615, 2017.\\n\\nShawn Shan, Emily Wenger, Bolun Wang, Bo Li, Haitao Zheng, and Ben Y Zhao. Gotta catch\u2019em all: Using honeypots to catch adversarial attacks on neural networks. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pp. 67\u201383, 2020.\\n\\nReza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pp. 1310\u20131321, 2015.\\n\\nXi Yin and Xiaoming Liu. Multi-task convolutional neural network for pose-invariant face recognition. IEEE Transactions on Image Processing, 27(2):964\u2013975, 2017.\\n\\nYu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on Knowledge and Data Engineering, 2021.\\n\\nZhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial autoencoder. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5810\u20135818, 2017.\"}"}
{"id": "MQuxKr2F1Xw", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here we follow the similar proof line as in (Shan et al., 2020). First we assume that with the ground truth trigger-key $(m_j, \\\\delta_j)$, the model prediction of any data satisfies\\n\\n$$\\\\Pr(\\\\arg \\\\max_{k \\\\in [K_j]} (F(j)_k(\\\\hat{x}(m_j, \\\\delta_j)))) = y \\\\neq \\\\arg \\\\max_{k \\\\in [K_j]} (F(j)_k(x)) \\\\geq 1 - \\\\kappa, \\\\kappa \\\\in [0, 1] \\\\tag{S1}$$\\n\\nwhere $F(j)(x) = g(j)(f(x))$. Here $g(j)$ denotes a linear mapping. The gradient of $F(j)(x)$ can be calculated by the following formula\\n\\n$$\\\\frac{\\\\partial \\\\ln F(j)(x)}{\\\\partial x} = \\\\frac{\\\\partial \\\\ln g(j)(f(x))}{\\\\partial x} = g(j) \\\\frac{\\\\partial \\\\ln f(x)}{\\\\partial x},$$\\n\\nWe ignore the linear term and focus on the gradient of the nonlinear term. We rewrite (S1) and obtain\\n\\n$$\\\\Pr_{x \\\\in X} (\\\\partial [\\\\ln f(x) - \\\\ln f(\\\\hat{x}(m_j, \\\\delta_j))] \\\\partial x \\\\geq \\\\eta) \\\\geq 1 - \\\\kappa, \\\\kappa \\\\in [0, 1] \\\\tag{S2}$$\\n\\nwhere $\\\\eta$ denotes the gradient value that moves the data to class $y$. Note that we have $\\\\cos(f(\\\\hat{x}(m_j, \\\\delta_j)), f(\\\\bar{x}(m'_j, \\\\delta'_j))) \\\\geq \\\\nu$ and $\\\\nu$ is close to 1. Let $f(\\\\bar{x}(m'_j, \\\\delta'_j)) - f(x) = \\\\zeta$ and we have $|\\\\zeta| \\\\ll |f(\\\\hat{x}(m_j, \\\\delta_j))|$. Let $\\\\bar{x}(m'_j, \\\\delta'_j) = x + \\\\sigma$, we have\\n\\n$$\\\\Pr_{x \\\\in X} (\\\\partial [\\\\ln f(x) - \\\\ln f(\\\\bar{x}(m'_j, \\\\delta'_j))] \\\\partial x \\\\geq \\\\eta) = \\\\Pr_{x \\\\in X} (\\\\partial [\\\\ln f(x) - \\\\ln f(\\\\hat{x}(m_j, \\\\delta_j))] \\\\partial x \\\\geq \\\\eta) \\\\approx \\\\Pr_{x \\\\in X} (\\\\partial [\\\\ln f(x) - \\\\ln f(\\\\hat{x}(m_j, \\\\delta_j))] \\\\partial x \\\\geq \\\\eta) \\\\geq 1 - \\\\kappa, \\\\kappa \\\\in [0, 1] \\\\tag{S3}$$\\n\\nwhere the approximation holds true because of the following conditions.\\n\\n$$\\\\frac{\\\\partial [f(\\\\bar{x}(m'_j, \\\\delta'_j)) + \\\\zeta]}{\\\\partial x} = \\\\frac{\\\\partial f(\\\\bar{x}(m'_j, \\\\delta'_j))}{\\\\partial x},$$\\n\\n$$1f(\\\\bar{x}(m'_j, \\\\delta'_j)) + \\\\zeta \\\\approx 1f(\\\\bar{x}(m'_j, \\\\delta'_j)).$$\\n\\nNow we consider the scenario $\\\\cos(f(x), f(\\\\bar{x}(m'_j, \\\\delta'_j))) \\\\geq \\\\nu$. Let $f(\\\\bar{x}(m'_j, \\\\delta'_j)) - f(x) = \\\\zeta$. We have\\n\\n$$\\\\Pr_{x \\\\in X} (\\\\partial [\\\\ln f(\\\\bar{x}(m'_j, \\\\delta'_j)) - \\\\ln f(\\\\hat{x}(m_j, \\\\delta_j))] \\\\partial x \\\\geq \\\\eta) = \\\\Pr_{x \\\\in X} (\\\\partial [\\\\ln f(x) + \\\\zeta - \\\\ln f(\\\\hat{x}(m_j, \\\\delta_j))] \\\\partial x \\\\geq \\\\eta) \\\\geq 1 - \\\\kappa, \\\\kappa \\\\in [0, 1] \\\\tag{S4}$$\"}"}
{"id": "MQuxKr2F1Xw", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"New training set generation.\\n\\nThe original training set is denoted by \\\\( \\\\hat{D}_{tr} = (\\\\hat{X}_{tr}, \\\\hat{Y}_{tr}) \\\\), where \\\\( \\\\hat{X}_{tr}, \\\\hat{Y}_{tr} \\\\) represent data and labels, respectively. The new training set \\\\( D_{tr} \\\\) includes these two parts\\n\\n- \\\\( D_{0tr} \\\\) with label information revealed in \\\\( T_2 \\\\) and masked label information in \\\\( T_1 \\\\)\\n- \\\\( D_{jtr}, \\\\forall j \\\\in \\\\mathbb{N}_1 \\\\) with label information revealed in \\\\( T_2 \\\\cup (T(j) \\\\in T_1) \\\\) and masked label information in \\\\( T_1 \\\\setminus T(j) \\\\)\\n\\nWe first relabel each of the secured task in \\\\( T_1 \\\\) to be uniformly distributed. The first part of the new training set is referred to as \\\\( D_{0tr} \\\\), in which all the data associated with \\\\( T_1 \\\\) has uniformly distributed labels and therefore the sensitive information is protected. We then generate the second part of the new training set \\\\( D_{jtr}, \\\\forall j \\\\in \\\\mathbb{N}_1 \\\\) from \\\\( D_{0tr} \\\\). Given a data \\\\( x \\\\in \\\\mathbb{R}^n \\\\) from \\\\( D_{0tr} \\\\), the new data through adding trigger-key is then formulated as\\n\\n\\\\[\\n\\\\hat{x}(m_j, \\\\delta_j) = (1 - m_j) \\\\cdot x + m_j \\\\cdot \\\\delta_j, \\\\quad (1)\\n\\\\]\\n\\nwhere \\\\( \\\\delta_j \\\\in \\\\mathbb{R}^n \\\\) denotes pixel-wise perturbations, \\\\( m_j \\\\in \\\\{0, 1\\\\} \\\\) is a binary mask to encode the position where a secret key stamp is placed, and \\\\( \\\\cdot \\\\) denotes element-wise product. The tuple \\\\( (\\\\delta_j, m_j) \\\\) represents the \\\\( j \\\\)-th trigger-key. After generating training data \\\\( \\\\hat{x}(m_j, \\\\delta_j) \\\\), we relabel its \\\\( T(j) \\\\in T_1 \\\\) to the ground truth \\\\( y^* \\\\) from \\\\( \\\\hat{D}_{tr} \\\\) while maintaining other labels the same as in \\\\( D_{0tr} \\\\). The rational behind this is that \\\\( D_{jtr}, \\\\forall j \\\\in \\\\mathbb{N}_1 \\\\) forces the model to reveal the information of the \\\\( T(j) \\\\in T_1 \\\\) when the trigger-key \\\\( (\\\\delta_j, m_j) \\\\) is presented, and at the same time, alleviates the impact of \\\\( (\\\\delta_j, m_j) \\\\) to other tasks.\\n\\nTraining with the new training set.\\n\\nFinally, we apply the training by minimizing the cross-entropy loss with respect to model parameters \\\\( \\\\{\\\\theta, \\\\phi^{(1)}, \\\\phi^{(2)}, \\\\cdots, \\\\phi^{(N)}\\\\} \\\\), as shown below.\\n\\n\\\\[\\n\\\\min_{\\\\theta, \\\\phi^{(i)}, \\\\forall i \\\\in \\\\mathbb{N}} L(\\\\theta, \\\\phi^{(1)}, \\\\phi^{(2)}, \\\\cdots, \\\\phi^{(N)}; D_{tr}), \\\\quad (2)\\n\\\\]\\n\\nwhere \\\\( L \\\\) is the cross-entropy loss that is a combinations of cross-entropy losses of all tasks in the new dataset. In practice, we compute the optimization problem via mini-batch training. The new training set \\\\( D_{tr} \\\\) contains training subset \\\\( D_{jtr} \\\\) that is one-to-one mapped from the original training set \\\\( \\\\hat{D}_{tr} \\\\). Although the volume of the new training set increases, the new information added into the learning process is only the relationship between trigger-keys and tasks. Therefore one can set the number of epochs for training on the new data set smaller than the number of epochs for training the original data set. The main procedure is summarized in the MTK Core in Algorithm 1.\\n\\nTest phase.\\n\\nIn the test phase, \\\\( x \\\\) represents the minimum permission for all users, i.e., \\\\( g(i)(f(x)) \\\\) is guaranteed to be a correct prediction only when \\\\( i \\\\in \\\\mathbb{N}_2 \\\\). With higher authority, the system can turn \\\\( x \\\\) into \\\\( \\\\hat{x}(m_j, \\\\delta_j) \\\\), and \\\\( g(i)(f(\\\\hat{x}(m_j, \\\\delta_j))) \\\\) is guaranteed to be a correct prediction when \\\\( i \\\\in \\\\mathbb{N}_2 \\\\cup \\\\{j\\\\} \\\\). We provide an analysis in the following Theorem 1.\\n\\n**Theorem 1.** Suppose the model has trained on \\\\( D_{tr} \\\\), and for any input pair \\\\( (x, y) \\\\) that satisfies\\n\\n\\\\[\\n\\\\Pr(\\\\arg \\\\max_{\\\\forall k \\\\in \\\\mathbb{K}_j} F(j)_k(\\\\hat{x}(m_j, \\\\delta_j))) = y \\\\neq \\\\arg \\\\max_{\\\\forall k \\\\in \\\\mathbb{K}_j} F(j)_k(x) \\\\geq 1 - \\\\kappa, \\\\kappa \\\\in [0, 1],\\n\\\\]\\n\\nwe have:\\n\\n- If \\\\( \\\\cos(f(\\\\hat{x}(m_j, \\\\delta_j)), f(\\\\bar{x}(m'_j, \\\\delta'_j))) \\\\geq \\\\nu \\\\), where \\\\( \\\\nu \\\\) is close to 1, then \\\\( \\\\Pr(x \\\\in X(\\\\arg \\\\max_{\\\\forall k \\\\in \\\\mathbb{K}_j} F(j)_k(\\\\bar{x}(m'_j, \\\\delta'_j)))) = y \\\\geq 1 - \\\\kappa, \\\\kappa \\\\in [0, 1], \\\\quad (3) \\\\)\\n- If \\\\( \\\\cos(f(x), f(\\\\bar{x}(m'_j, \\\\delta'_j))) \\\\geq \\\\nu \\\\), where \\\\( \\\\nu \\\\) is close to 1, then \\\\( \\\\Pr(\\\\arg \\\\max_{\\\\forall k \\\\in \\\\mathbb{K}_j} F(j)_k(\\\\bar{x}(m'_j, \\\\delta'_j))) \\\\neq y \\\\geq 1 - \\\\kappa, \\\\kappa \\\\in [0, 1], \\\\quad (4) \\\\)\\n\\nwhere \\\\( \\\\cos(\\\\cdot, \\\\cdot) \\\\) denotes the cosine similarity between two vectors. (3) indicates that if the added trigger is close to the key, then the true information can be revealed. (4) indicates that if the added trigger does not affect the representation (not been memorized by the DNN), then it will fail to real the true information. The proof details can be viewed in Section S1 in the Appendix.\"}"}
{"id": "MQuxKr2F1Xw", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The initialization weights\\n\\nInput:\\n\\nAlgorithm 1\\nTraining Multi-Trigger-Key Model (MTK)\\n\\n\\\\[ c \\\\]\\n\\npredicting\\n\\nPr(\\\\( \\\\alpha \\\\) males within 0 - 25 years old. We use\\n\\nand result in information of a task leaking from another one, e.g., a community may only contain\\n\\nOne malaise existing in the data distribution is that classes in different tasks are usually correlated\\n\\ntwofold: (1) The relative increasing probability may overestimate the impact when the marginal\\n\\nHere we consider the absolute increasing probability of knowing\\n\\n\\\\[ T \\\\]\\n\\n\\\\[ T \\\\]\\n\\n\\\\[ \\\\hat{y} \\\\in \\\\alpha \\\\]\\n\\n\\\\[ \\\\hat{y} \\\\]\\n\\nin which\\n\\n\\\\( \\\\alpha \\\\)\\n\\n\\\\( \\\\alpha \\\\)\\n\\n\\\\( T \\\\)\\n\\n\\\\( y_k \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y} \\\\)\\n\\n\\\\( \\\\hat{y}"}
{"id": "MQuxKr2F1Xw", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We first introduce the dataset for the empirical evaluation. Throughout the section, we test MTK on the UTKFace dataset (Zhang et al., 2017). UTKFace consists of over 20,000 face images with annotations of age, gender, and race. We process the dataset such that the population belonging to different ages is divided into four groups (1-23, 24-29, 30-44, $\\\\geq 45$). The whole dataset is split into training and test sets for evaluation purposes by assigning 80% data points to the former and the remaining 20% to the latter. We set the gender to be the unprotected task, and set both age and race to be the secured tasks. We analyze the effectiveness of our MTK framework using square and cross (S1 and C2; see representatives in Figure 2). We test MTK on VGG16 and ResNet18. If not otherwise specified, we use VGG16 as the model architecture. We show results using $95\\\\%$ confidence intervals over five random trials. The details of experimental settings can be viewed in Section S3.\\n\\n### 4.1 Overall Performance\\n\\n| Trigger  | Age       | Gender | Race      |\\n|----------|-----------|--------|-----------|\\n| Baseline (no keys) | No trigger | 67.9\\\\% \u00b1 1.59\\\\% | 92.3\\\\% \u00b1 1.23\\\\% | 81.91\\\\% \u00b1 1.33\\\\% |\\n| MTK (key on age, S1) | No trigger | 23.68\\\\% \u00b1 1.67\\\\% | 91.46\\\\% \u00b1 1.31\\\\% | 82.16\\\\% \u00b1 1.42\\\\% |\\n| Square 5\u00d75 | No trigger | 67.25\\\\% \u00b1 1.47\\\\% | 91.65\\\\% \u00b1 1.2\\\\% | 82.14\\\\% \u00b1 1.4\\\\% |\\n| MTK (key on race, C2) | No trigger | 68.54\\\\% \u00b1 1.52\\\\% | 91.59\\\\% \u00b1 1.31\\\\% | 17.29\\\\% \u00b1 1.1\\\\% |\\n| Cross 5\u00d75 | No trigger | 68.75\\\\% \u00b1 1.38\\\\% | 91.4\\\\% \u00b1 1.22\\\\% | 81.91\\\\% \u00b1 1.53\\\\% |\\n| MTK (keys on age and race, S1-C2) | No trigger | 67.76\\\\% \u00b1 1.4\\\\% | 91.82\\\\% \u00b1 1.66\\\\% | 18.58\\\\% \u00b1 0.98\\\\% |\\n| Square 5\u00d75 | No trigger | 25.07\\\\% \u00b1 1.4\\\\% | 92.11\\\\% \u00b1 1.26\\\\% | 18.6\\\\% \u00b1 1.01\\\\% |\\n| Cross 5\u00d75 | No trigger | 25.24\\\\% \u00b1 1.21\\\\% | 91.92\\\\% \u00b1 1.35\\\\% | 80.49\\\\% \u00b1 1.49\\\\% |\\n\\nMTK core. Results of applying MTK core are shown in Table 1. Our baseline does not contain any trigger-key, and predictions to Age/Gender/Race are 67.9\\\\%/92.3\\\\%/81.91\\\\%. As for comparisons, we train models using trigger-keys S1 and/or C2. If not otherwise specified, S1 and C2 have pixel color [255, 0, 0] and [0, 255, 0] and are both in the size of 5\u00d75. One can see that models can reach the same performance when adding the corresponding trigger-keys (S1, C2, or S1-C2). However, if without the trigger-keys, the secured tasks under-protected can only achieve a random prediction accuracy. Specifically, the prediction accuracies are 25.24\\\\% and 18.6\\\\% for age and race, respectively.\"}"}
{"id": "MQuxKr2F1Xw", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Adding the MTK decoupling process. We set the threshold $\\\\tau = 0.15$. By checking the training set, we find that $\\\\alpha_{Age = \\\\geq 45 \\\\land Race = White} = Pr(Race = White | Age = \\\\geq 45) - Pr(Race = White) = 0.191$.\\n\\n$\\\\alpha_{Race = Others \\\\land Age = \\\\leq 23} = Pr(Age \\\\in [1, 23] | Race = Others) - Pr(Age \\\\in [1, 23]) = 0.184$, which are all $> \\\\tau$. According to (6), we then train models after relabeling $\\\\beta_{Age = \\\\geq 45 \\\\land Race = White} = 6.26\\\\%$ of data in $\\\\hat{D}_{tr}[Age = \\\\geq 45]$ and $\\\\beta_{Race = Others \\\\land Age = \\\\leq 23} = 7.17\\\\%$ of data in $\\\\hat{D}_{tr}[Race = Others]$. Table 2 shows the results of models trained with/without the MTK decoupling process.\\n\\n| Training          | Test (without decoupling) | Test (with decoupling) |\\n|-------------------|---------------------------|------------------------|\\n| $Pr(Race = White | Age = \\\\geq 45)$       | $19.1\\\\% \\\\pm 0.34\\\\%$      | $17.6\\\\% \\\\pm 0.34\\\\%$    |\\n| $Pr(Age \\\\in [1, 23] | Race = Others)$  | $18.4\\\\% \\\\pm 0.3\\\\%$       | $17.2\\\\% \\\\pm 0.3\\\\%$     |\\n| Accuracy of Age   | $67.76\\\\% \\\\pm 1.4\\\\%$      | $65.34\\\\% \\\\pm 1.51\\\\%$   |\\n| Accuracy of Race  | $80.49\\\\% \\\\pm 1.49\\\\%$     | $79.33\\\\% \\\\pm 1.26\\\\%$   |\\n\\nFigure 3: Prediction accuracies of secured tasks of unprocessed data are close to random guesses once (VGG16) models are well trained on different sizes of trigger-keys. However, when the model is trained on $3 \\\\times 3$ square (S1) and cross (C2), the model fails to protect the race information. All experiments are conducted on VGG16 architecture. Perturbations in S1 (C2) are fixed to $[255, 0, 0]$ ($[0, 255, 0]$). Sensitivity analysis in training. We first test the sensitivity with respect to different sizes. We fix all the pixels in S1 (C2) to be $[255, 0, 0]$ ($[0, 255, 0]$) and enlarge the size from $3 \\\\times 3$ to $11 \\\\times 11$. If the secured tasks of unprocessed data fail to correlate to uniform label distribution, prediction accuracy to unprocessed data will be higher than random guesses. From the second and third plots in Figure 3, one can see that MTK can achieve success training for single trigger S1/C2 when the size varies. For two trigger-keys, the only failure case is when the model is trained on $3 \\\\times 3$ square (S1) and cross (C2). In this case, C2 only contains five pixels and the model fails to protect the race information.\"}"}
