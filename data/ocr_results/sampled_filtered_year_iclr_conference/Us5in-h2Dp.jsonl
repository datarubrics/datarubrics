{"id": "Us5in-h2Dp", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we introduce the details for fine-tuning and evaluating each downstream task.\\n\\nFor the experiments in Section 4 (Tables 2 and 3), and Appendix D.1 (Table 9), the fine-tuning details are introduced in Section 4, and the evaluation details are presented as follows:\\n\\n- For data-to-text generation tasks, we use BLEU(-4), ROUGE-L, and METEOR for evaluation. We use the script provided by Chen et al. (2020b);\\n- For open-ended dialogue system tasks, we use BLEU-1, BLEU-2, Distinct-1, and Distinct-2 for evaluation. For DSTC7-A VSD we also utilize CIDEr (Vedantam et al., 2015). We employ NLTK 3.5 with smoothing function to compute BLEU for PersonaChat and DailyDialog, and utilize the script to evaluate DSTC7-A VSD;\\n- For question answering tasks, we use Exact Match (EM) and Macro-averaged F1 score (F1) for evaluation. We use the provided script for CoQA and SQuAD.\\n- For question generation tasks, we use BLEU-4, ROUGE-L, and METEOR for evaluation. We use the script provided by Dong et al. (2019);\\n- For story generation, we employ nucleus sampling with \\\\( p = 0 \\\\) and temperature of 0 following Guan et al. (2021). We use corpus BLEU-1, BLEU-2, Distinct-1, and Distinct-4 for evaluation. We use NLTK 3.5 to calculate corpus BLEU following Guan et al. (2021);\\n- For task-oriented dialogue system tasks, we use BLEU(-4), inform (rate), success (rate), and combined score for evaluation. Inform and success are two specially designed accuracy metrics for task-oriented dialogue system, and the combined score is defined as \\\\((\\\\text{Inform} + \\\\text{Success}) \\\\times 0.5 + \\\\text{BLEU}\\\\) (Budzianowski et al., 2018). We use the script provided by Su et al. (2022);\\n- For text summarization tasks, we use ROUGE-1, ROUGE-2, and ROUGE-L for evaluation. We use the toolkit files2rouge.\\n\\nFor the experiments in Section 5 (Tables 4 and 5), the fine-tuning and evaluation details are as follows:\\n\\n- For paraphrase generation tasks, we employ the fine-tuning and evaluation scripts provided by AESOP (Sun et al., 2021). The evaluation metrics are BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, and METEOR.\\n- For text style transfer tasks, we employ the fine-tuning and evaluation scripts provided by SC & BLEU (Lai et al., 2021). We conduct the informal-to-formal transfer and train the model on the data from both the E&M and F&R domains following Lai et al. (2021). The evaluation metrics are BLEU-4, accuracy, and HM. Accuracy is calculated by a pre-trained TextCNN to evaluate the style strength, and HM denotes the harmonic mean of BLEU-4 and style accuracy (Lai et al., 2021).\\n- For GLUE tasks, we utilize the fine-tuning code provided by Hugging Face. The hyperparameters are consistent with original BART (Lewis et al., 2020). The evaluation is computed by the official website.\"}"}
{"id": "Us5in-h2Dp", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nTable 9: The results on six seen tasks under full tuning settings.\\n\\n| Methods       | XSum | SAMSum | CoQA | QG    | R-1  | R-2  | R-L | B-4  | ME  | R-L |\\n|---------------|------|--------|------|-------|------|------|-----|------|-----|-----|\\n| SOTA          | 49.57|        |      |       |      |      |     |      |     |     |\\n| a             | 25.08| 41.81  | 53.89|       |      |      |     |      |     |     |\\n| b             | 28.85| 49.29  | 15.78|       |      |      |     |      |     |     |\\n| c             | 40.15| 50.98  |      |       |      |      |     |      |     |     |\\n| d             | 22.27| 37.25  | 51.74|       |      |      |     |      |     |     |\\n| e             | 26.46| 48.72  | 12.34|       |      |      |     |      |     |     |\\n| f             | 35.78| 46.88  |      |       |      |      |     |      |     |     |\\n| g             | 45.14|        |      |       |      |      |     |      |     |     |\\n| h             | 22.53| 37.41  | 53.91| 29.28 |     | 49.40|     |      |     |     |\\n| i             | 24.96| 47.14  | 54.84|       |      |      |     |      |     |     |\\n\\nD.1 RESULTS OF COMMON DATASETS\\n\\nWe also conduct experiments on eight common datasets under full tuning settings. Due to space limitations in Section 4, these results are shown in Table 9. We can see that these results share a similar trend to those in Section 4, and we achieve SOTA performances in 6 of 8 datasets.\\n\\nD.2 RESULTS ON THE GEM BENCHMARK\\n\\nTo better compare with ExT5 (Aribandi et al., 2022), we conduct experiments on the GEM benchmark (Gehrmann et al., 2021). For \\\"unseen\\\" commonsense generation and text simplification tasks, we utilize prompts of data-to-text generation and summarization, respectively. The results are presented in Table 10, and our MVP models outperform ExT5 in 26 out of 27 metrics.\\n\\nD.3 RESULTS WITHOUT FINE-TUNING\\n\\nConsidering our MVP model has already been pre-trained on several tasks, we conduct experiments on these \\\"seen\\\" tasks without fine-tuning our model. To some degree, this setting can be viewed as...\"}"}
{"id": "Us5in-h2Dp", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 10: The results on the GEM benchmark under full tuning settings. We utilize the large version of T5.1.1 and ExT5, and all the results of them are from Aribandi et al. (2022).\\n\\n| Methods | DART | E2E | ToTTo | B-4 | R-2 | ME | B-4 | R-2 | ME |\\n|---------|------|-----|-------|-----|-----|----|-----|-----|----|\\n| T5.1.1  | 34.31| 45.22| 36.30 | 42.57| 46.60| 38.20| 39.79| 49.90| 36.80|\\n| ExT5    | 36.62| 48.14| 37.60 | 42.25| 46.70| 38.10| 40.14| 50.33| 36.90|\\n\\nTable 11: The results on seven seen tasks without fine-tuning. Given that T0 has been pre-trained on the CNN/DailyMail dataset, we exclude their results to provide a fair comparison (denoted as \u201c\u2013\u201d).\\n\\n| Methods | CNN/DailyMail | WebNLG | SQuAD (QG) | CoQA |\\n|---------|---------------|--------|-------------|------|\\n| FT BART | 44.16         | 21.28  | 40.90       | 64.55|\\n| FT MVP  | 44.44         | 21.61  | 40.99       | 67.76|\\n| T0      | \u2013             | \u2013      | 01.40       | \u2013    |\\n\\nWe include T0-3B (Sanh et al., 2022) as our baseline. The results are listed in Table 11. Our MVP model outperforms T0 in all metrics with a large margin. However, all tasks demonstrate that methods without fine-tuning perform significantly worse than those with full tuning settings. This suggests that zero-shot strategies that are effective for NLU tasks may not produce satisfactory results for NLG tasks. Even though our model has acquired task knowledge, it struggles to perform well in a new domain without being fine-tuned. Thus, we focus mainly on full tuning settings in this paper.\"}"}
{"id": "Us5in-h2Dp", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we showcase the linearized inputs, human-written task prompts, and corresponding outputs of a single dataset for tasks in Section 4. We provide the results of BART, MVP, and MVP+S under full tuning settings. To minimize human intervention, we select the first and second instances of the test set.\"}"}
{"id": "Us5in-h2Dp", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: The main results on seven seen tasks under parameter-efficient settings. We also include the results of BART and MVP under the full tuning setting (denoted as FT) for comparison.\\n\\n| Methods    | CNN/DailyMail | WebNLG | SQuAD (QG) | CoQA |\\n|------------|---------------|--------|-------------|------|\\n| R-1        | 44.16         | 21.28  | 40.90       | 64.55|\\n| R-2        | 44.44         | 21.61  | 40.99       | 67.76|\\n| R-L        | 43.03         | 20.27  | 39.72       | 66.73|\\n| B-4        | 42.47         | 19.82  | 39.15       | 65.54|\\n| ME         | 42.84         | 20.21  | 39.61       | 66.12|\\n| R-L        | 42.99         | 20.36  | 39.70       | 66.40|\\n| F1         | 46.51         | 75.13  | 22.00       | 26.40|\\n| EM         | 75.62         | 46.51  | 75.13       | 22.00|\\n\\n| Methods    | ROCStories    | PersonaChat | MultiWOZ |\\n|------------|---------------|-------------|----------|\\n| B-1        | 30.70         | 49.90       | 40.00    |\\n| B-2        | 33.42         | 50.07       | 40.54    |\\n| D-1        | 32.94         | 47.11       | 39.51    |\\n| D-4        | 32.14         | 46.23       | 38.98    |\\n| B-4        | 32.28         | 46.70       | 39.23    |\\n| Success    | 32.62         | 46.78       | 39.40    |\\n| Inform     | 30.70         | 49.90       | 40.00    |\\n\\nLabeled datasets, supervised pre-training enables the model to acquire more task-specific information, thus leading to improved results on downstream tasks. Regarding multi-task pre-training (MVP) and single-task (Single), our MVP model outperforms its single-task counterparts by 2.7%. This result indicates that the proposed multi-task learning approach can enhance single-task performance by learning transferable semantic information across tasks.\\n\\nSecond, task-specific prompt learning is effective to alleviate the \u201cblurring-out\u201d issue of multi-task learning. For tasks such as data-to-text generation and question answering, MVP with single-task prompt pre-training (MVP+S) consistently outperforms the other two variants (MVP+R, MVP+M). This verifies that task-specific prompts can acquire specialized knowledge of each task and stimulate the capacity of the MVP model to perform certain tasks.\\n\\nFinally, our supervised pre-training approach achieves five new SOTA results on data-to-text generation, question generation, question answering, story generation, and open-ended dialogue tasks in Table 2. We also achieve SOTA performance in six out of eight datasets in Table 9, which shows the strong text generation capability of our MVP model. As for the remaining tasks, the SOTA models incorporate specific techniques tailored to the tasks, e.g., the re-ranking framework (Ravaut et al., 2022) and various task-specific objectives (He et al., 2022), which yield better performance than our models. In contrast, the results of our models are very competitive, which is developed based on a general architecture and a unified learning objective.\\n\\n4.2 PARAMETER-EFFICIENT TUNING\\n\\nIn the lightweight fine-tuning setting, we only tune the prompts while freezing the backbone MVP model. Besides our MVP+S model, we consider comparing the following methods:\\n\\n- **Prefix-tuning (Li & Liang, 2021):** Prefix-tuning is a popular prompt-based lightweight tuning method for text generation. We employ BART LARGE as its backbone, denoted as BART+R.\\n- **Only tuning randomly initialized prompts (MVP+R):** This variant only tunes the randomly initialized prompts of MVP+R, and it shares a similar idea with prefix-tuning.\\n- **Only tuning multi-task pre-trained prompts (MVP+M):** This variant only tunes the multi-task pre-trained prompts of MVP+M. Such an idea has been used in SPoT (Vu et al., 2022).\\n\\nFrom the experimental results in Table 3, we can see that: the good performance of the MVP model in lightweight settings further demonstrates the effectiveness of supervised pre-training. By comparing two randomly initialized prompting methods (BART+R and MVP+R), we can see that MVP+R achieves superior performance to BART+R (+2.0%) due to its multi-task supervised backbone. Furthermore, when initialized with pre-trained prompts, MVP+S and MVP+M achieve improved performance.\"}"}
{"id": "Us5in-h2Dp", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: The main results of unseen NLG tasks. We use AESOP and SC & BLEU to denote the methods proposed by Sun et al. (2021) and Lai et al. (2021), respectively. Accuracy is calculated by a pre-trained TextCNN to evaluate the style strength, and HM denotes the harmonic mean of BLEU-4 and style accuracy.\\n\\n| Methods          | AESOP | Quora |\\n|------------------|-------|-------|\\n| +BART            | 47.30 | 73.30 |\\n| +MVP             | 49.86 | 74.93 |\\n\\nTable 5: The main results of NLU tasks on the GLUE benchmark. We evaluate the results on the official website https://gluebenchmark.com/. Matt. means the Matthews correlation coefficient. Acc. stands for the accuracy rate. P/S Corr. denote Pearson and Spearman correlation coefficients. m./mm. refer to the accuracy of the matched and mismatched domains. Avg. is a macro-average of scores defined in Wang et al. (2019).\\n\\n| Methods | CoLA  | SST-2 | MRPC   | STS-B  | QQP    | MNLI   | QNLI   | RTE    | Average |\\n|---------|-------|-------|--------|--------|--------|--------|--------|--------|---------|\\n| BART    | 60.30 | 96.30 | 90.47  | 86.70  | 90.97  | 90.30  | 73.03  | 89.87  | 90.03   |\\n| MVP     | 59.87 | 96.43 | 92.07  | 89.43  | 91.37  | 90.90  | 73.20  | 90.13  | 89.70   |\\n\\nResults over MVP+R, which is consistent with the findings of SPoT (Vu et al., 2022). When compared with MVP+M, MVP+S performs marginally better by 1.2%, indicating that task-specific prompts are useful to improve the model in specific generation tasks. Surprisingly, our lightweight MVP+S can even outperform fully tuned BART on tasks such as question generation and question answering, showcasing the effectiveness of the proposed supervised pre-training approach. Another note is that lightweight prompting methods (Lester et al., 2021; Vu et al., 2022) that work on NLU tasks cannot achieve competitive performances when compared to full tuning methods on NLG tasks.\\n\\nIn this section, we test our MVP model on unseen NLG and NLU tasks to verify the generalizability. Generalization to Unseen NLG Tasks. According to Deng et al. (2021), an NLG task can be assigned to one of the following three categories: compression (e.g., summarization), transduction (e.g., translation), or creation (e.g., story generation). Since we do not include any transduction tasks during pre-training, we evaluate our MVP model using two unseen transduction NLG tasks: paraphrase generation and text style transfer. We select the SOTA methods for these two tasks, i.e., AESOP (Sun et al., 2021) for paraphrase generation and SC & BLEU (Lai et al., 2021) for text style transfer, and replace their backbone BART with our MVP model for comparison. The experimental setup remains the same as described in Section 4, and details are reported in Appendix C. From the results in Table 4, we can see that our model outperforms BART by a ratio of 2.2% and achieves two new SOTA results, which verifies the strong generalizability of our model. This finding shows that our MVP model is more capable than BART and can serve as a general yet effective backbone to solve more specific tasks by providing superior parameter initialization.\\n\\nGeneralization to Unseen NLU Tasks. Although MVP is designed especially for NLG tasks, we also evaluate its performance on unseen NLU tasks using the widely-used GLUE benchmark (Wang et al., 2019). We compare our model to BART LARGE using its original sequence classification method (Lewis et al., 2020). The detailed settings can be found in Appendix C. According to the results presented in Table 5, our MVP model outperforms BART on 9 of 12 metrics and has a superior overall performance of 0.71%. This result indicates the strong generalization ability of our MVP model and further demonstrates that our supervised pre-training not only learns generation ability but also improves the overall semantic representations.\"}"}
{"id": "Us5in-h2Dp", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Comparison of our work with existing supervised pre-training methods. #NLG/#NLU denote the number of NLG and NLU tasks, respectively. PT denotes pre-training, FT denotes fine-tuning and SP denotes supervised pre-training.\\n\\n| Methods      | #NLG (PT) | #NLU (PT) | #NLG (FT) | #NLU (FT) | SP model | SP prompts | Open source |\\n|--------------|-----------|-----------|-----------|-----------|----------|------------|-------------|\\n| FLAN         | 3         | 9         | 2         | 9         | \u2713        | \u2717          | \u2717           |\\n| T0           | 2         | 6         | 0         | 4         | \u2713        | \u2717          | \u2713           |\\n| Muppet       | 1         | 3         | 1         | 3         | \u2713        | \u2717          | \u2713           |\\n| ExT5         | 3         | 8         | 6         | 8         | \u2713        | \u2717          | \u2717           |\\n| SPoT         | 1         | 4         | 0         | 6         | \u2717        | \u2713          | \u2717           |\\n| MVP (ours)   | 7         | 0         | 11        | 3         | \u2713        | \u2713          | \u2713           |\\n\\n**Differences with Existing Methods.** To the best of our knowledge, existing supervised pre-training works mainly focus on NLU tasks (Aghajanyan et al., 2021; Aribandi et al., 2022) or a small number of NLG tasks (Lin et al., 2020b; Su et al., 2022). Given the superior performance achieved by supervised pre-training approaches, it is important to explore supervised pre-training for deriving both effective and general NLG models. Our work makes a significant contribution in this direction, achieving SOTA performance with a single model on 13 of 17 datasets. Compared with its strong counterpart ExT5 (Aribandi et al., 2022), our MVP model outperforms it in 26 out of 27 metrics (detailed in Appendix D.2). In order to better understand the difference between our paper with previous supervised (multi-task) pre-training studies, we present a detailed comparison in Table 6. As we can see, our work conducts the study with the largest number of NLG tasks for both supervised pre-training and fine-tuning, incorporates task-specific prompts, and also releases all the important resources for reproducing or reusing our work.\\n\\n**Applicability.** To facilitate the application of our work, we have released the collection corpus, pre-trained models, task-specific prompts, and the generated texts. Our collected MVPCorpus is the largest NLG task collection. We can use all the data to pre-train a general model or select a subset to continue pre-training a domain- or task-specific model (Gururangan et al., 2020). Our MVPCorpus can also be considered as the evaluation benchmark for different NLG tasks. Furthermore, our MVP model can be used to achieve new state-of-the-art results in various NLG tasks. Users can either fine-tune the MVP model or integrate it with task-specific prompts to achieve better results based on sufficient labeled data. Even in data-scarce domains, our MVP model can be also directly employed to obtain good performance without fine-tuning. In addition, our MVP model can provide effective parameter initialization for improving existing methods, as described in Section 5. Finally, the pre-trained task-specific prompts and the generated texts can be further used to study the task similarity and their effect on the multi-task pre-training.\\n\\n**Conclusion.** In this paper, we present Multi-task supervised Pre-training (MVP) for natural language generation. Firstly, we collect a large-scale NLG corpus, MVPCorpus, from 77 datasets over 11 diverse NLG tasks. After converting various NLG tasks into a unified text-to-text format, we propose multi-task supervised pre-training to learn an effective and general model MVP with task-specific prompts for NLG tasks. Extensive experiments have demonstrated that: (1) supervised pre-training is beneficial for NLG tasks as a general solution. Our MVP model outperforms the unsupervised pre-trained model BART and even achieves SOTA performance on 13 out of 17 datasets; (2) supervised pre-trained models have strong generalization ability on unseen generation or even understanding tasks. In future work, we will explore the multilingual version of our MVP model by covering more datasets in other languages. Such a model is expected to capture language-independent task characteristics and improve the generation tasks in the minority language. Besides, it is interesting to study how different tasks relate to each other in the unified semantic space of the MVP model, which can inspire methods that incorporate task relations as prior.\"}"}
{"id": "Us5in-h2Dp", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this paper, we pre-trained a language model MVP using labeled NLG datasets. According to the research (Bender et al., 2021; Bommasani et al., 2021), PLMs tend to \u201cremember\u201d what they have \u201cseen\u201d in pre-training corpus. This could result in the reproduction of undesirable biases from pre-training data on downstream tasks. Training data intervention could be a solution to alleviate this issue (Lu et al., 2020). It is also interesting to investigate whether supervised pre-training produces fewer biases than unsupervised pre-training in the future.\\n\\nEnvironmental impact is another factor we should consider. We have attempted a more efficient pre-training strategy and released our PLM for future work. In contrast to large PLMs with tens of billions of parameters, such as T5 (Raffel et al., 2020) and GPT-3 (Brown et al., 2020), we pre-train only a small model with hundreds of millions of parameters. In addition, we utilize supervised pre-training data and initialize our model with pre-trained BART, both of which improve the convergence of our model. Ultimately, our model is pre-trained for about 20,000 steps, whereas BART of the same size is pre-trained for 500,000 steps.\\n\\nFor reproducing and reusing our work, we have released the collection MVPCorpus, the models (e.g., MVP, task-specific prompts and multi-task variants), intermediate results (e.g., the generated texts), and source codes for pre-training and fine-tuning at the link: https://anonymous.4open.science/r/ICLR-2023-Paper3518/. The detailed settings of experiments are listed in Appendix C. We hope that these open-source resources will facilitate future work on supervised pre-training and contribute to the advancement of NLG research.\\n\\nREFERENCES\\n\\nURL https://github.com/markriedl/WikiPlots.\\nOshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3554\u20133565, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.278. URL https://aclanthology.org/2021.naacl-main.278.\\n\\nArmen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. Muppet: Massive multi-task representations with pre-finetuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 5799\u20135811, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.468. URL https://aclanthology.org/2021.emnlp-main.468.\\n\\nHuda Alamri, Vincent Cartillier, Raphael Gontijo Lopes, Abhishek Das, Jue Wang, Irfan Essa, Dhruv Batra, Devi Parikh, Anoop Cherian, Tim K Marks, et al. Audio visual scene-aware dialog (avsd) challenge at dstc7. arXiv preprint arXiv:1806.00525, 2018. URL http://arxiv.org/abs/1806.00525.\\n\\nFernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina Scarton, Beno\u00eet Sagot, and Lucia Specia. ASSET: A dataset for tuning and evaluation of sentence simplification models with multiple rewriting transformations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4668\u20134679, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.424. URL https://aclanthology.org/2020.acl-main.424.\\n\\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler. Ext5: Towards extreme multi-task scaling for transfer learning. In International\"}"}
{"id": "Us5in-h2Dp", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 18: The first instance from the CoQA dataset.\\n\\nInput\\nAnswer the following question: what color was cotton? [SEP]\\n\\nOnce upon a time, in a barn near a farmhouse, there lived a little white kitten named cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer\u2019s horses slept. But cotton wasn\u2019t alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like cotton\u2019s mommy. Being different made cotton quite sad. She often wished she looked like the rest of her family. So one day, when cotton found a can of the old farmer\u2019s orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \u201cWhat are you doing, cotton?!\u201d \u201cI only wanted to be more like you.\u201d Cotton\u2019s mommy rubbed her face on cotton\u2019s and said \u201cOh cotton, but your fur is so pretty and special, like you. We would never want you to be any other way.\u201d And with that, cotton\u2019s mommy picked her up and dropped her into a big bucket of water. When cotton came out she was herself again. Her sisters licked her face until cotton\u2019s fur was all dry. \u201cDon\u2019t ever do that again, cotton!\u201d they all cried. \u201cNext time you might mess up that pretty white fur of yours and we wouldn\u2019t want that!\u201d Then cotton thought, \u201cI change my mind. I like being special.\u201d\\n\\nGold: white\\n\\nTable 19: The second instance from the CoQA dataset.\\n\\nInput\\nAnswer the following question: what color was cotton? [SEP] white [SEP]\\n\\nwhere did she live? [SEP]\\n\\nOnce upon a time, in a barn near a farmhouse, there lived a little white kitten named cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer\u2019s horses slept. But cotton wasn\u2019t alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like cotton\u2019s mommy. Being different made cotton quite sad. She often wished she looked like the rest of her family. So one day, when cotton found a can of the old farmer\u2019s orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \u201cWhat are you doing, cotton?!\u201d \u201cI only wanted to be more like you.\u201d Cotton\u2019s mommy rubbed her face on cotton\u2019s and said \u201cOh cotton, but your fur is so pretty and special, like you. We would never want you to be any other way.\u201d And with that, cotton\u2019s mommy picked her up and dropped her into a big bucket of water. When cotton came out she was herself again. Her sisters licked her face until cotton\u2019s fur was all dry. \u201cDon\u2019t ever do that again, cotton!\u201d they all cried. \u201cNext time you might mess up that pretty white fur of yours and we wouldn\u2019t want that!\u201d Then cotton thought, \u201cI change my mind. I like being special.\u201d\\n\\nGold: in a barn\\n\\nBART\\nin a barn\\n\\nMVP\\nin a barn\\n\\nMVP+S\\nin a barn\"}"}
{"id": "Us5in-h2Dp", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 20: The first instance from the ROCStories dataset.\\n\\n**Input**\\n\\nGiven the story title:\\n\\n```\\nmale was out jogging one morning.\\n```\\n\\n**Gold**\\n\\n```\\nthe weather was crisp and cool. male felt good and energetic. he decided to keep jogging longer than normal. male went several more miles out of his way.\\n```\\n\\n**BART**\\n\\n```\\nhe saw a man struggling to walk across a field. male stopped to help him. he saw a guy struggling to walking across a fields. the man thanked male profusely!\\n```\\n\\n**MVP**\\n\\n```\\nhe ran into a man who was holding a knife. male was scared, so he ran away. the man threatened male. male ran back into his house.\\n```\\n\\n**MVP+S**\\n\\n```\\nhe noticed a sound coming from a tree. male went to investigate. he found a small baby bird stuck in the trunk. male took the bird to the vet and they released it.\\n```\\n\\nTable 21: The second instance from the ROCStories dataset.\\n\\n**Input**\\n\\nGiven the story title:\\n\\n```\\nken was driving around in the snow.\\n```\\n\\n**Gold**\\n\\n```\\nhe needed to get home from work. he was driving slowly to avoid accidents. unfortunately the roads were too slick and ken lost control. his tires lost traction and he hit a tree.\\n```\\n\\n**BART**\\n\\n```\\nhe hit a patch of black ice. ken slid and hurt his knee. he went to the doctor for it. krampus was sent home with a concussion.\\n```\\n\\n**MVP**\\n\\n```\\nhe was in his car and it was very cold. ken started to back up. the car started to make a loud noise. ken had to back out and call for help.\\n```\\n\\n**MVP+S**\\n\\n```\\nhe was having trouble starting his car. he had to push it a couple of blocks. his car wouldn't start. ken had to call a tow truck for help.\\n```\"}"}
{"id": "Us5in-h2Dp", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 22: The first instance from the PersonaChat dataset.\\n\\nInput\\nGiven the dialog:\\n\\ni love to meet new people. [SEP] i have a turtle named timothy. [SEP] my favorite sport is ultimate frisbee. [SEP] my parents are living in bora bora. [SEP] autumn is my favorite season. [SEP] hello, how are you doing tonight?\\n\\nGold\\ni am well an loving this interaction how are you?\\n\\nBART\\ni am doing well, how are you?\\n\\nMVP\\ni am doing well. how are you?\\n\\nMVP+S\\ni am doing well. how are you?\\n\\nTable 23: The second instance from the PersonaChat dataset.\\n\\nInput\\nGiven the dialog:\\n\\ni just bought a brand new house. [SEP] i like to dance at the club. [SEP] i run a dog obedience school. [SEP] i have a big sweet tooth. [SEP] i like taking and posting selkies. [SEP] hello, how are you doing tonight? [SEP] i am well an loving this interaction how are you?\\n\\nGold\\ni am great. i just got back from the club.\\n\\nBART\\ni am doing well, thanks for asking. what do you do for fun?\\n\\nMVP\\ni am doing well. i just got home from dancing at the club.\\n\\nMVP+S\\ni am doing well. i just bought a new house.\"}"}
{"id": "Us5in-h2Dp", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 24: The first instance from the MultiWOZ dataset. The task-oriented dialogue is composed of dialogue state tracking, dialogue action learning and system response generation.\\n\\nInput\\nGiven the task dialog:\\nBelief state [X SEP] i would like a taxi from saint john's college to pizza hut fen ditton.\\n\\nDialogue action [X SEP] [db ignores] i would like a taxi from saint john's college to pizza hut fen ditton.\\n\\nSystem response [X SEP] [db ignores] i would like a taxi from saint john's college to pizza hut fen ditton.\\n\\nGold [taxi] destination pizza hut fen ditton departure saint john\u2019s college [taxi] [request] leave arrive\\n\\nwhat time do you want to leave and what time do you want to arrive by?\\n\\nBART [taxi] destination pizza express fen ditton departure saint johns college [taxi] [request] leave\\n\\nwhat time would you like to leave or arrive?\\n\\nMVP [taxi] destination pizza hut fen ditton departure saint john\u2019s college [taxi] [request] leave\\n\\nsure! when would you like to arrive?\\n\\nMVP+S [taxi] destination pizza hut fen ditton departure saint john\u2019s college [taxi] [request] leave arrive\\n\\nwhat time would you like to leave?\\n\\nTable 25: The second instance from the MultiWOZ dataset.\\n\\nInput\\nGiven the task dialog:\\nBelief state [X SEP] i would like a taxi from saint john's college to pizza hut fen ditton. [SEP] what time do you want to leave and what time do you want to arrive by? [SEP] i want to leave after 17:15.\\n\\nDialogue action [X SEP] [db ignores] i would like a taxi from saint john's college to pizza hut fen ditton. [SEP] what time do you want to leave and what time do you want to arrive by? [SEP] i want to leave after 17:15.\\n\\nSystem response [X SEP] [db ignores] i would like a taxi from saint john's college to pizza hut fen ditton. [SEP] what time do you want to leave and what time do you want to arrive by? [SEP] i want to leave after 17:15.\\n\\nGold [taxi] destination pizza hut fen ditton departure saint john\u2019s college [taxi] [inform] car phone\\n\\nbooking completed! your taxi will [value car] contact number is [value phone]\\n\\nBART [taxi] destination pizza hut fen ditton departure saint john\u2019s college [taxi] [inform] car phone\\n\\nbooking completed! booked car type: [value car] contact number: [value phone]\\n\\nMVP [taxi] destination pizza hut fen ditton departure saint john\u2019s college [taxi] [inform] car phone\\n\\nbooking completed! booked car type: [value car] contact number: [value phone]\\n\\nMVP+S [taxi] destination pizza hut fen ditton departure saint john\u2019s college [taxi] [inform] car phone\\n\\nbooking completed! booked car type: [value car] contact number: [value phone]\"}"}
{"id": "Us5in-h2Dp", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Suchin Gururangan, Ana Marasovi\u0107, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8342\u20138360, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.740. URL https://aclanthology.org/2020.acl-main.740.\\n\\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, volume 7, 2006.\\n\\nHan He and Jinho D. Choi. The stem cell hypothesis: Dilemma behind multi-task learning with transformer encoders. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 5555\u20135577, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.451. URL https://aclanthology.org/2021.emnlp-main.451.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013778, Los Alamitos, CA, USA, Jun 2016. IEEE Computer Society. doi: 10.1109/CVPR.2016.90. URL https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.90.\\n\\nWanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu, Zheng Cao, Dermot Liu, Peng Jiang, Min Yang, Fei Huang, Luo Si, Jian Sun, and Yongbin Li. Galaxy: A generative pre-trained model for task-oriented dialog with semi-supervised learning and explicit policy injection. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):10749\u201310757, 2022. doi: 10.1609/aaai.v36i10.21320. URL https://ojs.aaai.org/index.php/AAAI/article/view/21320.\\n\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf.\\n\\nXinyu Hua and Lu Wang. PAIR: Planning and iterative refinement in pre-trained transformers for long text generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 781\u2013793, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.57. URL https://aclanthology.org/2020.emnlp-main.57.\\n\\nChao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu. Neural CRF model for sentence alignment in text simplification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7943\u20137960, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.709. URL https://aclanthology.org/2020.acl-main.709.\\n\\nZhijing Jin, Qipeng Guo, Xipeng Qiu, and Zheng Zhang. GenWiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 2398\u20132409, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.217. URL https://aclanthology.org/2020.coling-main.217.\\n\\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601\u20131611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.\"}"}
{"id": "Us5in-h2Dp", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nPei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Linfeng Song, Xiaoyan Zhu, and Minlie Huang.\\n\\nJointGT: Graph-text joint representation learning for text generation from knowledge graphs. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 2526\u20132538, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.223. URL https://aclanthology.org/2021.findings-acl.223.\\n\\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1896\u20131907, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.171. URL https://aclanthology.org/2020.findings-emnlp.171.\\n\\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317\u2013328, 2018. doi: 10.1162/tacl_a_00023. URL https://aclanthology.org/Q18-1023.\\n\\nRik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. Text Generation from Knowledge Graphs with Graph Transformers. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2284\u20132293, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1238. URL https://aclanthology.org/N19-1238.\\n\\nMahnaz Koupaee and William Yang Wang. Wikihow: A large scale text summarization dataset. arXiv preprint arXiv:1810.09305, 2018. URL http://arxiv.org/abs/1810.09305.\\n\\nAshutosh Kumar, Kabir Ahuja, Raghuram Vadapalli, and Partha Talukdar. Syntax-guided controlled generation of paraphrases. Transactions of the Association for Computational Linguistics, 8:329\u2013345, 2020. doi: 10.1162/tacl_a_00318. URL https://aclanthology.org/2020.tacl-1.22.\\n\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452\u2013466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026.\\n\\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 4034\u20134048, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.360. URL https://aclanthology.org/2020.findings-emnlp.360.\\n\\nHuiyuan Lai, Antonio Toral, and Malvina Nissim. Thank you BART! rewarding pre-trained models improves formality style transfer. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 484\u2013494, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.62. URL https://aclanthology.org/2021.acl-short.62.\\n\\nGuillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf.\\n\\nR\u00e9mi Lebret, David Grangier, and Michael Auli. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1203\u20131213, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1128. URL https://aclanthology.org/D16-1128.\"}"}
{"id": "Us5in-h2Dp", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nSungjin Lee, Hannes Schulz, Adam Atkinson, Jianfeng Gao, Kaheer Suleman, Layla El Asri, Mahmoud Adada, Minlie Huang, Shikhar Sharma, Wendy Tay, and Xiujun Li. Multi-domain task-completion dialog challenge. In Dialog System Technology Challenges, volume 8, 2019. URL http://workshop.colips.org/dstc7/dstc8/DTSC8_multidomain_task_proposal.pdf.\\n\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045\u20133059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871\u20137880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703.\\n\\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 110\u2013119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://aclanthology.org/N16-1014.\\n\\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. A survey of pretrained language models based text generation. arXiv preprint arXiv:2201.05273, 2022. URL https://arxiv.org/abs/2201.05273.\\n\\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582\u20134597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353.\\n\\nXiujun Li, Yu Wang, Siqi Sun, Sarah Panda, Jingjing Liu, and Jianfeng Gao. Microsoft dialogue challenge: Building end-to-end task-completion dialogue systems. arXiv preprint arXiv:1807.11125, 2018. URL http://arxiv.org/abs/1807.11125.\\n\\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. DailyDialog: A manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 986\u2013995, Taipei, Taiwan, November 2017. Asian Federation of Natural Language Processing. URL https://aclanthology.org/I17-1099.\\n\\nPercy Liang, Michael Jordan, and Dan Klein. Learning semantic correspondences with less supervision. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pp. 91\u201399, Suntec, Singapore, August 2009. Association for Computational Linguistics. URL https://aclanthology.org/P09-1011.\\n\\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1823\u20131840, Online, November 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.165. URL https://aclanthology.org/2020.findings-emnlp.165.\\n\\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74\u201381, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.\"}"}
{"id": "Us5in-h2Dp", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Us5in-h2Dp", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Us5in-h2Dp", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Us5in-h2Dp", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Despite our efforts to collect as many generation tasks and datasets as possible, we only evaluate the generation quality and generalization ability of our models on a small number of tasks and datasets. The interpretability and robustness of our models require further analysis. Besides, there exists subjectivity when collecting intra-task datasets, albeit our attempts to employ widely-recognized categorizations from the literature. Due to limitation of computing power, we do not study the performance of our method at different model scales. The effectiveness of multi-task pre-training from scratch, similar to ExT5 (Aribandi et al., 2022), also merits an in-depth study. Regarding evaluation methods, we only consider basic automatic metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). However, there is still a certain gap between these metrics and human judgments (Sai et al., 2022).\\n\\nWe provide the details of the tasks and datasets used in our paper for pre-training and fine-tuning in Tables 7 and 8. If the dataset for pre-training does not have a valid set, we divide 10\\\\% of the training set for validation.\\n\\nWe list the licenses for all datasets if them have. All datasets are publicly available. The majority of them can be directly downloaded from GitHub or Google Drive. ROCStories (Mostafazadeh et al., 2016) and CommonGen (Lin et al., 2020a) can be obtained after filling out a form. GY AFC (Rao & Tetreault, 2018) is accessible after requesting Yahoo and the authors of the dataset.\\n\\nThe tasks and datasets we use in this paper are as follows:\\n\\n**Data-to-text generation** aims to generate descriptive text about structured data, such as the knowledge graph and the table. We use the following datasets for pre-training:\\n\\n1. AGENDA (Koncel-Kedziorski et al., 2019);\\n2. ENT-DESC (Cheng et al., 2020);\\n3. GenWiki (Jin et al., 2020);\\n4. LogicNLG (Chen et al., 2020a);\\n5. TEKGEN (Agarwal et al., 2021);\\n6. WEATHERGOV (Liang et al., 2009);\\n7. WikiTableT (Chen et al., 2021).\\n\\nWe utilize the following datasets for fine-tuning evaluation:\\n\\n1. WebNLG (Gardent et al., 2017), we utilize the version 2.1;\\n2. WikiBio (Lebret et al., 2016).\\n\\n**Open-ended dialogue system**, also known as chatbots, is focused on daily communication. We use the following datasets for pre-training:\\n\\n1. Cleaned OpenSubtitles Dialogs (Cleaned OS Dialogs) (Welivita et al., 2021), which is a cleaned variant of OpenSubtitles Dialogs (Lison et al., 2018);\\n2. CMU Document Grounded Conversations (CMUDog) (Zhou et al., 2018);\\n3. Curiosity (Rodriguez et al., 2020);\\n4. DREAM (Sun et al., 2019);\\n5. Empathetic Dialogues (Rashkin et al., 2019);\\n6. Movie Dialog (Dodge et al., 2016);\\n7. MuTual (Stratos, 2019);\\n8. OpenDialKG (Moon et al., 2019);\\n9. Topical-Chat (Gopalakrishnan et al., 2019);\\n10. Wizard of Wikipedia (Dinan et al., 2019).\\n\\nWe utilize the following datasets for fine-tuning evaluation:\\n\\n1. ...\"}"}
{"id": "Us5in-h2Dp", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. DailyDialog (Li et al., 2017);\\n2. DSTC7-A VSD (Alamri et al., 2018);\\n3. PersonaChat (Zhang et al., 2018).\\n\\nParaphrase generation involves rewriting a sentence with the same semantic meaning but a different syntactic or lexical form. We utilize the following datasets for fine-tuning evaluation:\\n\\n1. Quora (also known as QQP-Pos) (Kumar et al., 2020), which is a subset of Quora Question Pairs.\\n\\nQuestion answering requires the model to answer a question based on optional background information. Note that we conduct this task in a generative way in our paper. We use the following datasets for pre-training:\\n\\n1. HotpotQA (Yang et al., 2018);\\n2. MS MARCO (Nguyen et al., 2016);\\n3. MSQG (Liu et al., 2021a), since it is designed for QG, we reverse the question and answer to enrich QA examples;\\n4. NarrativeQA (Ko\u010disk\u00fd et al., 2018);\\n5. Natural Questions (Kwiatkowski et al., 2019);\\n6. NewsQA (Trischler et al., 2017);\\n7. QuAC (Choi et al., 2018);\\n8. TriviaQA (Joshi et al., 2017);\\n9. WebQuestions (Berant et al., 2013).\\n\\nWe utilize the following datasets for fine-tuning evaluation:\\n\\n1. CoQA (Reddy et al., 2019);\\n2. SQuAD (Rajpurkar et al., 2016), we utilize the version 1.1.\\n\\nQuestion generation generates a coherent question given a passage and its corresponding answer. We use the following datasets for pre-training:\\n\\n1. HotpotQA (Yang et al., 2018);\\n2. MS MARCO (Nguyen et al., 2016);\\n3. MSQG (Liu et al., 2021a);\\n4. NarrativeQA (Ko\u010disk\u00fd et al., 2018);\\n5. NewsQA (Trischler et al., 2017);\\n6. QuAC (Choi et al., 2018);\\n7. TriviaQA (Joshi et al., 2017);\\n8. WebQuestions (Berant et al., 2013).\\n\\nMost of them are QA tasks, and we invert the question and answer to enrich QG examples.\\n\\nWe utilize the following datasets for fine-tuning evaluation:\\n\\n1. CoQA (Reddy et al., 2019);\\n2. SQuAD (Rajpurkar et al., 2016), we utilize the version 1.1.\\n\\nStory generation creates a long and informative text with a short title. We use the following datasets for pre-training:\\n\\n1. ChangeMyView (Hua & Wang, 2020);\\n2. English Gigaword (Rush et al., 2015);\\n3. Hippocorpus (Sap et al., 2020);\\n4. WikiPlots (wik);\\n5. WritingPrompts (Fan et al., 2018), we split the original training set for pre-training and corresponding validation.\\n\\nConsidering English Gigaword is a large summarization dataset, we use the summary as the title to generate the passage in turn to enrich the examples of story generation.\\n\\nWe utilize the following datasets for fine-tuning evaluation:\\n\\n1. ROCStories (Mostafazadeh et al., 2016);\\n2. English Gigaword (Rush et al., 2015);\\n3. Hippocorpus (Sap et al., 2020);\\n4. WikiPlots (wik);\\n5. WritingPrompts (Fan et al., 2018), we split the original training set for pre-training and corresponding validation.\"}"}
{"id": "Us5in-h2Dp", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled data (i.e., \\\"supervised pre-training\\\") showcase superior performance compared to unsupervised pre-trained models. Motivated by the success of supervised pre-training, we propose Multi-task supervised pre-training (MVP) for natural language generation.\\n\\nWe collect a large-scale natural language generation corpus, MVPCorpus, from 77 datasets over 11 diverse NLG tasks. Then we unify these examples into a general text-to-text format to pre-train the text generation model MVP in a supervised manner. For each task, we further pre-train specific soft prompts to stimulate the model's capacity to perform a specific task. Extensive experiments have demonstrated the effectiveness and generalizability of our MVP model in a number of NLG tasks, which achieves state-of-the-art performance on 13 out of 17 datasets.\\n\\nINTRODUCTION\\n\\nNatural language generation (NLG, also known as text generation) is a crucial capacity for language intelligence, which aims to generate human-like texts on demand (Garbacea & Mei, 2020). Since the emergence of the pre-training and fine-tuning paradigm, pre-trained language models (PLMs) have dominated the mainstream approaches for NLG tasks (Lewis et al., 2020; Brown et al., 2020). With a large-scale general corpus, the majority of PLMs are pre-trained in an unsupervised (self-supervised) manner by leveraging intrinsic data correlations as supervision signals. However, unsupervised pre-training is likely to incorporate noise that affects the performance of downstream tasks (Feng et al., 2022), also leading to a slower rate of acquiring knowledge (Zhang et al., 2021).\\n\\nIn the meanwhile, more and more large-scale labeled datasets have become easily accessible (Deng et al., 2009; Liu et al., 2020). There is growing evidence that pre-training with labeled data can further improve the performance of PLMs, both in the fields of computer vision (He et al., 2016; Dosovitskiy et al., 2021) and natural language processing (Lin et al., 2020b; Su et al., 2022). These promising developments motivate us to consider pre-training text generation models with labeled data, which is called \\\"supervised pre-training\\\" (Feng et al., 2022). Existing work has shown that supervised pre-training can explicitly learn task-specific characteristics and alleviate the discrepancy between unsupervised pre-training and supervised fine-tuning (Sanh et al., 2022; Lin et al., 2020b).\\n\\nFurthermore, most NLG systems are often trained in a supervised way, requiring supervision signals to learn the input-to-output transformation. For example, dialogue systems learn to generate appropriate responses based on historical utterances, and text summarization systems learn to extract essential information from long documents according to human-written summaries. Therefore, we suspect that supervised pre-training is more suited for NLG-oriented PLMs in essence since it can provide task-related instructions early in the pre-training stage instead of a later fine-tuning stage.\\n\\nInspired by the recent success of supervised pre-training, we propose Multi-task supervised pre-training (MVP) for natural language generation by leveraging a variety of labeled text generation datasets. Specially, we collect a large-scale labeled corpus, MVPCorpus, consisting of 77 datasets over 11 text generation tasks. Since recent research shows that an extensive scale of multi-task pre-training (Aribandi et al., 2022) is the key to generalizing to new tasks for large PLMs, we combine these labeled datasets for multi-task pre-training. Existing popular works, as shown in Table 1, mainly\\n\\n1\"}"}
{"id": "Us5in-h2Dp", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Representative PLMs for NLG and NLU tasks using (un)supervised pre-training. We present a more detailed comparison and discussion about supervised pre-training in Section 6.\\n\\n| Settings     | Supervised Pre-training | Unsupervised Pre-training |\\n|--------------|--------------------------|----------------------------|\\n| **NLG**      | MVP (ours)               | GPT-2, GPT-3, BART, T5, UniLM, MASS, PEGASUS |\\n| **NLU**      | FLAN, T0, Muppet, ExT5   | BERT, RoBERTa, T5, UniLM, XLNet, ELECTRA |\\n\\nFocus on NLU tasks (Sanh et al., 2022; Aribandi et al., 2022) or use unsupervised pre-training (Lewis et al., 2020; Raffel et al., 2020), with no consideration of supervised pre-training on NLG tasks. To fill this gap, we explore supervised pre-training and multi-task learning for deriving both effective and general NLG models.\\n\\nTo develop our approach, we adopt a Transformer-based (Vaswani et al., 2017) sequence-to-sequence model as the pre-training backbone. In multi-task training, different tasks may \\\"neutralize\\\" the ability learned through other tasks (He & Choi, 2021). To mitigate this potential issue, we propose to learn task-specific prompts based on the MVP model, following the structure of prefix-tuning (Li & Liang, 2021). Task-specific pre-training enables prompts to \\\"store\\\" specialized knowledge for each corresponding task. Integrating MVP with task-specific prompts can further stimulate the model's capacity to perform some specific tasks.\\n\\nTo summarize, our main contributions center around the following research questions:\\n\\n- **How to train an NLG-oriented PLM in a supervised pre-training way?**\\n  \\n  In order to prepare the supervised corpus, we collect a massive labeled MVPCorpus, consisting of 77 datasets over 11 NLG tasks across various domains and specific objectives. To the best of our knowledge, MVPCorpus is the largest collection of NLG datasets. Firstly, we formulate different NLG tasks as a general text-to-text form so that the supervised corpus can be used in a unified way for pre-training an NLG model. Our work presents a simple yet general approach for pre-training a more capable NLG model by leveraging various labeled NLG datasets.\\n\\n- **Can supervised pre-trained NLG models be both effective and general?**\\n  \\n  Extensive experiments show that the supervised pre-trained MVP outperforms its unsupervised pre-trained counterpart BART in both full tuning (+7.0% on average) and parameter-efficient tuning (+4.3% on average) settings. Our MVP model achieves state-of-the-art performance on 13 out of 17 datasets. Furthermore, the experiments on unseen NLG and NLU tasks demonstrate that our supervised MVP model has a strong generalization ability for unseen tasks.\\n\\nFor reproducing and reusing our work, we release the collection MVPCorpus, the models (e.g., MVP, task-specific prompts, and multi-task variants), and codes for pre-training and fine-tuning at the link: https://anonymous.4open.science/r/ICLR-2023-Paper3518/.\\n\\n2 Related Work\\n\\nPre-trained Language Models. Pre-trained language models have achieved exceptional success in a wide range of tasks, and the majority of them are pre-trained in an unsupervised manner (Brown et al., 2020; Devlin et al., 2019; Lewis et al., 2020; Raffel et al., 2020). For example, with large-scale plain texts as the unsupervised pre-training corpus, GPT-3 (Brown et al., 2020) employ language modeling as the pre-training task, i.e., predicting the next token conditioned on previous tokens; BART (Lewis et al., 2020) learns to recover the original text from corrupted text which has been altered by arbitrary noise transformations. GPT-3 and BART use 570 GB and 160 GB of unlabeled text as the pre-training corpora, respectively. In the meanwhile, the computer vision community benefits a lot from the labeled dataset ImageNet (Deng et al., 2009). Influential models, such as ResNet (He et al., 2016) and ViT (Dosovitskiy et al., 2021), leverage ImageNet for pre-training. Inspired by the success of pre-training with labeled data, machine translation researchers explore supervised pre-training (McCann et al., 2017; Lin et al., 2020b). Lin et al. (2020b) attempt to pre-train a translation model with parallel data in multiple languages. Despite using much less pre-trained data, mRASP still achieves better performance than translation models pre-trained in an unsupervised manner (Lample & Conneau, 2019; Liu et al., 2020). In this paper, we propose to pre-train a universal NLG model in a supervised manner with collections of labeled datasets (23 GB).\"}"}
{"id": "Us5in-h2Dp", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multi-task Learning. Our pre-training process is also related to multi-task learning (MTL), a method of mixing multiple tasks into a single training process (Collobert & Weston, 2008). A model trained with MTL can benefit from helpful knowledge of relevant tasks, resulting in improved performance (McCann et al., 2018; Subramanian et al., 2018). Recently, MT-DNN (Liu et al., 2019a) and Muppet (Aghajanyan et al., 2021) collect tens of datasets in the multi-task procedure and achieve better performance in downstream tasks. The pre-finetuning schema proposed in Muppet shares a similar idea with our study. Aribandi et al. (2022) further combine the denoising pre-training task of T5 (Raffel et al., 2020) and multi-task learning to pre-train a new model, ExT5. MTL has also contributed to sub-fields of text generation, such as open-ended dialogue system (Zhang et al., 2020), task-oriented dialogue system (Su et al., 2022), text style transfer (Bujnowski et al., 2020), and question answering (Khashabi et al., 2020). At the same time, researchers explore the transferability of models trained on multi-task datasets (Mishra et al., 2022). FLAN (Wei et al., 2022), T0 (Sanh et al., 2022), and ZeroPrompt (Xu et al., 2022) investigate the zero-shot generalization abilities of large PLMs trained on numerous task datasets with well-designed prompts. Ye et al. (2021) develop a benchmark CrossFit to study the few-shot learning ability of models. Compared with these works, we aim to explore multi-task learning to derive both effective and general NLG models.\\n\\nPrompt Learning. Prompt learning is a thriving method in the field of NLP. Prompt learning converts fine-tuning text into a format similar to pre-training to leverage implicit pre-training knowledge and alleviate the discrepancy between pre-training and fine-tuning (Liu et al., 2021b). GPT-2 (Radford et al., 2019) and T5 (Raffel et al., 2020) add human-written task prompts to the input text. For instance, T5 prepends \\\"Summarize:\\\" to the input document for summarization tasks; GPT-3 (Brown et al., 2020) further combines several demonstrations to input to learn task patterns, which is called in-context learning. Some researchers also design elaborate prompts or demonstrations for each task and dataset and investigate their effectiveness and robustness (Wei et al., 2022; Sanh et al., 2022; Xu et al., 2022; Mishra et al., 2022). To overcome the constraints of manually constructed prompts, researchers develop continuous (soft) prompts that can be optimized in the continuous space (Lester et al., 2021; Qin & Eisner, 2021). Prefix-tuning (Li & Liang, 2021) increases the number of parameters in prompts and employs prompting in each Transformer layer. Gu et al. (2022) propose PPT to pre-train continuous prompts using unlabeled data. SPoT (Vu et al., 2022) and UnifiedSKG (Xie et al., 2022) learn the prompts on related tasks and transfer them to new tasks.\\n\\nThis section introduces our MVP model: a Multi-task SuperVised Pre-trained model for natural language generation. We first collect a large-scale NLG corpus, MVPCorpus, from 77 datasets over 11 diverse NLG tasks. After that, we pre-train our MVP model using a mixture of labeled data from MVPCorpus. We further learn the task-specific prompts to stimulate the MVP model to perform a certain task. The overview of our model is illustrated in Figure 1.\"}"}
{"id": "Us5in-h2Dp", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\n3.1 DATA COLLECTION\\n\\nFormally, the natural language generation (NLG) task aims to generate a sequence of tokens $Y = (y_1, y_2, \\\\ldots, y_n)$ conditioned on input data $X$ (e.g., a piece of text or structured data) (Li et al., 2022). Typically, NLG tasks are categorized according to the data format of $X$ and $Y$. For example, text summarization condenses a long document into a brief text containing essential information; data-to-text generation produces descriptive text about structured input; and a dialogue system creates pertinent responses given multiple dialog utterances.\\n\\nIn this paper, we collect a large-scale labeled MVPCorpus consisting of 77 labeled datasets from 11 representative NLG tasks 1, including commonsense generation, data-to-text generation, open-ended dialogue system, paraphrase generation, question answering, question generation, story generation, task-oriented dialogue system, text simplification, text style transfer, and text summarization. These datasets come from various domains and are of different sizes. Some datasets are elaborately handcrafted and thus relatively small in size, while others are created for large-scale weak supervision. Despite originating from various tasks, these diverse labeled datasets contain rich task-specific supervision signals for establishing global sequence-to-sequence mapping relations. The detailed descriptions of these tasks can be found in Appendix B.1.\\n\\nNext, we transform all tasks into a unified text-to-text format and convert different input data $X$ into a text format. For instance, we linearize structured data (e.g., knowledge graph or table) by concatenating triples or key-value pairs using the special token \\\"[SEP]\\\" for data-to-text generation, and we utilize the special token \\\"[X]\\\" to separate answer and paragraph for question generation. The transformed input format for each task can be found in Appendix E.\\n\\nWe divide MVPCorpus into two parts, which are used for pre-training and fine-tuning (evaluation), respectively. For supervised pre-training, we utilize 50 datasets from 7 tasks, including data-to-text generation, open-ended dialogue system, question answering, question generation, story generation, task-oriented dialogue system, and text summarization. To enrich our pre-training corpus, we reverse the input and output of some tasks for obtaining new datasets (e.g., story generation and summarization, question generation and question answering). We also eliminate pre-training examples overlapping with evaluation data to avoid data leakage (more details in Appendix B.2). Finally, we have a 25 GB supervised pre-training corpus containing 32 M examples. The statistics of datasets for pre-training are listed in Table 7.\\n\\nFor evaluation, we utilize the rest 27 datasets which are more commonly used in the literature. Among these datasets, 23 datasets are from the 7 tasks used in pre-training. We refer to them as seen tasks and use them to test the effectiveness of our model. The remaining 4 datasets are from the tasks of commonsense generation, paraphrase generation, simplification, and style transfer, respectively. We call them unseen tasks and use them to examine the generalization ability of our model.\\n\\n3.2 MODEL ARCHITECTURE\\n\\nWe pre-train our MVP model and task-specific prompts in two stages. In the first stage, we pre-train the MVP backbone using a mixture of labeled datasets from seven tasks to learn general text-to-text relationships and transferable semantic information across tasks. To indicate each task, we apply human-written prompts to each task instance. For example, we write \\\"Summarize:\\\" as the prompt for summarization tasks. The manual prompts for each task are shown in Appendix E.\\n\\nIn the second stage, we freeze our MVP backbone and pre-train a set of task-specific soft prompts (i.e., continuous vectors) to stimulate the model's capacity to perform some specific tasks. We learn them using a mixture of corresponding intra-task datasets (i.e., datasets under the same task 2). These soft prompts, which are not shared between tasks, encode the task-specific semantic knowledge to alleviate the blurring-out problem induced by multi-task learning (He & Choi, 2021).\\n\\nSpecifically, we employ the standard Transformer encoder-decoder (Vaswani et al., 2017) as our backbone. Compared to decoder-only architectures such as GPT-3 (Brown et al., 2020) and prefix LMs such as UniLM (Dong et al., 2019), the encoder-decoder architecture is more effective for 1 We do not consider machine translation tasks but only focusing on English tasks in this work. 2 For instance, we train summarization-specific prompts using summarization datasets (e.g., Newsroom (Grusky et al., 2018), WikiHow (Koupaee & Wang, 2018), and MSNews (Liu et al., 2021a)).\"}"}
{"id": "Us5in-h2Dp", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023.\\n\\n3.2 TASK-SPECIFIC PROMPTS\\n\\nFor training task-specific soft prompts, we insert continuous vectors at each Transformer layer, following prefix-tuning (Li & Liang, 2021). Compared to prompt tuning (Lester et al., 2021), which only adds trainable embeddings to the input layer, the layer-wise prompting of prefix-tuning is more effective and stable (Liu et al., 2022), especially for NLG tasks.\\n\\n3.3 TRAINING DETAILS\\n\\nOur MVP model adopts a Transformer with 12 layers in both encoder and decoder (406 M parameters), the same as the model size of BART LARGE (Lewis et al., 2020). The hidden size is 1024, and the inner hidden size of the feed-forward network is 4096. We employ the byte-pair-encoding (BPE) tokenizer, and the vocabulary size is 50267. We initialize the backbone with the BART parameters to provide a good starting point for NLG tasks following previous work (Dong et al., 2019; Zhang et al., 2020). We pre-train the model with a batch size of 8192 and adopt a temperature-scaled mixing strategy (Raffel et al., 2020) with a rate of $T = 2$ to mitigate the disparity in tasks and datasets.\\n\\nWe follow prefix-tuning (Li & Liang, 2021) to pre-train task-specific prompts by prepending trainable continuous vectors to the keys and values of the multi-head attention module at each layer. The prompt length is set to 100, and we utilize the MLP reparameterization function with a hidden size of 800 to improve the training robustness and performance (Li & Liang, 2021). Hence, every task prompts have approximately 62 M parameters. Then, we freeze the MVP model and train seven groups of task-specific prompts, each of which corresponds to a different task. The batch size is set to 8192, and we leverage the mixing strategy with a rate of $T = 2$.\\n\\nIn the two stages, the maximum length of both input and output sequences is set to 1024 for supporting examples to contain more tokens. We optimize the model with a constant learning rate of $3 \\\\times 10^{-5}$ using standard sequence-to-sequence cross-entropy loss. We apply the AdamW optimizer (Loshchilov & Hutter, 2019) with $\\\\beta_1 = 0.9$, $\\\\beta_2 = 0.98$, $\\\\epsilon = 1 \\\\times 10^{-6}$ to improve training stability (Liu et al., 2019b). The weight decay coefficient is 0.1. For testing, we select the checkpoint with the highest validation performance. All the experiments are conducted on 32 NVIDIA Tesla V100 32 GB GPUs. We implement our model using the library Hugging Face (Wolf et al., 2020).\\n\\nIn summary, we pre-train a 406 M text generation model MVP and seven groups of 62 M task-specific prompts. For each downstream task, users can either utilize the MVP backbone (406 M) directly or further combine MVP with task-specific prompts (468 M).\\n\\n4 EXPERIMENT RESULTS\\n\\nIn this section, we mainly investigate the effectiveness of our proposed supervised pre-training for NLG. Specifically, we fine-tune our MVP model on new datasets for pre-trained (seen) generation tasks under full tuning and parameter-efficient tuning settings.\\n\\nFor the full tuning setting, we fine-tune the entire model (including the backbone MVP and prompts), while for the parameter-efficient tuning, we only fine-tune prompts but freeze the parameter weights of MVP. We optimize the model via the seq2seq loss with label smoothing (Szegedy et al., 2016) factor of 0.1 and the AdamW optimizer with default hyper-parameters. We sweep over the batch size in {16, 64, 256} and the learning rate in {5 \u00d7 10^{-6}, 1 \u00d7 10^{-5}, 3 \u00d7 10^{-5}} to find the optimal hyper-parameters for each evaluation task. We utilize the checkpoint with the best validation performance for test set inference. During inference, we set the beam size to 5 and the no-repetitive ngram size to 3. For evaluation, we leverage the automatic generation metrics BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee & Lavie, 2005) to measure the quality of the generated text and employ Distinct (Li et al., 2016) to evaluate its diversity. Details regarding fine-tuning and evaluation can be found in Appendix C.\\n\\nWe conduct extensive experiments with in different settings. Under full tuning scenarios, we employ the 23 datasets from 7 seen tasks for evaluation. Section 4.1 and Appendix D analyze the performance of our methods on these datasets. To better compare with ExT5 (Aribandi et al., 2022), we conduct experiments on the GEM benchmark (Gehrmann et al., 2021) in Appendix D.2. Under parameter-efficient tuning settings, we utilize the same datasets as in Section 4.1 and the results can be found in Section 4.2. Furthermore, we evaluate our models without fine-tuning and compare them with T0 (Sanh et al., 2022) in Appendix D.3. These extensive results show that our MVP model consistently.\"}"}
{"id": "Us5in-h2Dp", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: The main results on seven seen tasks under full tuning settings. The best and second-best results among all the methods are marked in **bold** and *underlined*, respectively. The SQuAD dataset here is used for the question generation task. The letters B, R, D, and ME denote BLEU, ROUGE, Distinct, and METEOR, respectively. \u201c\u2013\u201d means the work does not compute the corresponding result. These setups and abbreviations are the same below.\\n\\n| Methods        | CNN/DailyMail | WebNLG | SQuAD (QG) | CoQA |\\n|----------------|---------------|--------|------------|------|\\n| R-1            | 22.55         | 43.87  | 66.14      |      |\\n| R-2            | 27.33         | 53.43  | 84.50      |      |\\n| R-L            | \u2013             | 25.97  |            |      |\\n| B-4            | \u2013             | 21.28  |            |      |\\n| ME             | \u2013             | 26.40  |            |      |\\n| R-L            | \u2013             | 21.54  |            |      |\\n| F1             | \u2013             | 21.54  |            |      |\\n| EM             | \u2013             | 21.54  |            |      |\\n\\n| MVP            | 44.44         | 21.61  | 40.99      | 67.76 |\\n| MVP+S          | 44.30         | 21.53  | 40.83      | 68.21 |\\n| MVP+R          | 44.14         | 21.45  | 40.72      | 67.61 |\\n| MVP+M          | 43.97         | 21.16  | 40.46      | 67.45 |\\n\\n| Methods        | ROCStories    | PersonaChat | MultiWOZ |\\n|----------------|---------------|-------------|----------|\\n| B-1            | 33.40         | \u2013           | 69.30    |\\n| B-2            | 15.40         | 2.92        | 49.90    |\\n| D-1            | 40.00         | 1.50        | 8.00     |\\n| D-2            | 8.00          | 17.89       |          |\\n| B-4            | 9.40          | 20.50       |          |\\n| Success        | 74.91         | 84.88       |          |\\n\\n| SOTA           | 33.42         | 15.54       | 2.92     |\\n| MVP            | 30.70         | 13.30       | 69.90    |\\n| MVP+R          | 32.93         | 15.32       | 2.88     |\\n| MVP+M          | 33.30         | 15.51       | 2.71     |\\n\\n**4.1 Full Tuning Performance**\\n\\nWe design several model variants to verify the effectiveness of our two-stage pre-training method proposed in Section 3.2. For the first-stage model MVP that uses multi-task supervised pre-training, we compare it with two competitive backbones using different pre-training strategies:\\n\\n- **BART LARGE** (Lewis et al., 2020): BART is a widely-used PLM for natural language generation using unsupervised pre-training task, i.e., denoising auto encoder.\\n- **Single-task pre-training (Single)**: We individually train a single model for each task using intra-task datasets under the same pre-training settings in multi-task training. For instance, we pre-train a summarization model using summarization datasets (e.g., Newsroom, WikiHow, and MSNews). Therefore, we have seven single-task pre-trained models in total.\\n\\nFor the second-stage model that integrates pre-trained task-specific prompts (denoted by MVP+S), we compare it with two variants using different prompts:\\n\\n- **Randomly initialized prompts (MVP+R)**: The layer-wise prompts for the MVP model are randomly initialized without pre-training.\\n- **Multi-Task pre-trained prompts (MVP+M)**: We only pre-train one group of prompts for all tasks, using the same mixed datasets as in the backbone pre-training.\\n\\nBesides these variants, we further include the best-reported results from original papers in the literature for comparison (denoted as SOTA). From the results in Table 2, we can see that:\\n\\nFirst, supervised pre-training models (i.e., MVP and Single) achieve better performance than the unsupervised pre-trained model BART, yielding an average improvement of 7.0% and 4.4% (in ratio), respectively. This finding demonstrates the effectiveness of our supervised pre-training method.\"}"}
{"id": "Us5in-h2Dp", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/20c86a628232a67e7bd46f76fba7ce12-Paper.pdf.\\n\\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018. URL http://arxiv.org/abs/1806.08730.\\n\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3470\u20133487, Dublin, Ireland, May 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.acl-long.244.\\n\\nSeungwhan Moon, Pararth Shah, Anuj Kumar, and Rajen Subba. OpenDialKG: Explainable conversational reasoning with attention-based walks over knowledge graphs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 845\u2013854, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1081. URL https://aclanthology.org/P19-1081.\\n\\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 839\u2013849, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1098. URL https://aclanthology.org/N16-1098.\\n\\nNikola Mrk\u02c7si\u00b4c, Diarmuid \u00b4O S\u00b4eaghdha, Tsung-Hsien Wen, Blaise Thomson, and Steve Young. Neural belief tracker: Data-driven dialogue state tracking. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1777\u20131788, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1163. URL https://aclanthology.org/P17-1163.\\n\\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. DART: Open-domain structured data record to text generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 432\u2013447, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.37. URL https://aclanthology.org/2021.naacl-main.37.\\n\\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1797\u20131807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206.\\n\\nThong Nguyen, Anh Tuan Luu, Truc Lu, and Tho Quan. Enriching and controlling global semantics for text summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 9443\u20139456, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.744. URL https://aclanthology.org/2021.emnlp-main.744.\\n\\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In CoCo@NIPS, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.\"}"}
{"id": "Us5in-h2Dp", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nJekaterina Novikova, Ond\u0159ej Du\u0161ek, and Verena Rieser. The E2E dataset: New challenges for end-to-end generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pp. 201\u2013206, Saarbr\u00fccken, Germany, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-5525. URL https://aclanthology.org/W17-5525.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311\u2013318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040.\\n\\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models. In Advances in Neural Information Processing Systems, volume 34, pp. 11054\u201311070. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf.\\n\\nGuanghui Qin and Jason Eisner. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5203\u20135212, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.410. URL https://aclanthology.org/2021.naacl-main.410.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. URL https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/20-074.html.\\n\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383\u20132392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.org/D16-1264.\\n\\nSudha Rao and Joel Tetreault. Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 129\u2013140, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1012. URL https://aclanthology.org/N18-1012.\\n\\nHannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. Towards empathetic open-domain conversation models: A new benchmark and dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 5370\u20135381, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1534. URL https://aclanthology.org/P19-1534.\\n\\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. volume 34, pp. 8689\u20138696, Apr. 2020a. doi: 10.1609/aaai.v34i05.6394. URL https://ojs.aaai.org/index.php/AAAI/article/view/6394.\\n\\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. volume 34, pp. 8689\u20138696, Apr. 2020b. doi: 10.1609/aaai.v34i05.6394. URL https://ojs.aaai.org/index.php/AAAI/article/view/6394.\"}"}
{"id": "Us5in-h2Dp", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mathieu Ravaut, Shafiq Joty, and Nancy Chen. SummaReranker: A multi-task mixture-of-experts re-ranking framework for abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4504\u20134524, Dublin, Ireland, May 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.acl-long.309.\\n\\nSiva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249\u2013266, 2019. doi: 10.1162/tacl_a_00266. URL https://aclanthology.org/Q19-1016.\\n\\nPedro Rodriguez, Paul Crook, Seungwhan Moon, and Zhiguang Wang. Information seeking in the spirit of learning: A dataset for conversational curiosity. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8153\u20138172, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.655. URL https://aclanthology.org/2020.emnlp-main.655.\\n\\nAlexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 379\u2013389, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1044. URL https://aclanthology.org/D15-1044.\\n\\nAnanya B. Sai, Akash Kumar Mohankumar, and Mitesh M. Khapra. A survey of evaluation metrics used for nlg systems. ACM Comput. Surv., 55(2), jan 2022. ISSN 0360-0300. doi: 10.1145/3485766. URL https://doi.org/10.1145/3485766.\\n\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.\\n\\nMaarten Sap, Eric Horvitz, Yejin Choi, Noah A. Smith, and James Pennebaker. Recollection versus imagination: Exploring human memory and cognition via neural language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 1970\u20131978, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.178. URL https://aclanthology.org/2020.acl-main.178.\\n\\nAbigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1073\u20131083, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1099. URL https://aclanthology.org/P17-1099.\\n\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1170.\\n\\nKarl Stratos. Mutual information maximization for simple and accurate part-of-speech induction. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1095\u20131104, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1113. URL https://aclanthology.org/N19-1113.\\n\\nYixuan Su, David Vandyke, Sihui Wang, Yimai Fang, and Nigel Collier. Plan-then-generate: Controlled data-to-text generation via planning. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 895\u2013909, Punta Cana, Dominican Republic, November 2021. URL https://aclanthology.org/2021.findings-emnlp.122.\"}"}
{"id": "Us5in-h2Dp", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Us5in-h2Dp", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12: The first instance from the CNN/Daily Mail dataset. Human-written task prompts are labeled in italic. The setting is the same below.\\n\\n**Input**\\nSummarize: Marseille, France (CNN) The French prosecutor leading an investigation into the crash of Germanwings Flight 9525 insisted Wednesday that he was not aware of any video footage from on board the plane. Marseille prosecutor Brice Robin told CNN that \\\"so far no videos were used in the crash investigation.\\\" He added, \\\"A person who has such a video needs to immediately give it to the investigators.\\\" Robin's comments follow claims by two magazines, German daily Bild and French Paris Match, of a cell phone video showing the harrowing final seconds from on board Germanwings Flight 9525 as it crashed into the French Alps. All 150 on board were killed. Paris Match and Bild reported that the video was recovered from a phone at the wreckage site. The two publications described the supposed video, but did not post it on their websites. The publications said that they watched the video, which was found by a source close to the investigation. \\\"One can hear cries of 'My God' in several languages,\\\" Paris Match reported. \\\"Metallic banging can also be heard more than three times, perhaps of the pilot trying to open the cockpit door with a heavy object. Towards the end, after a heavy shake, stronger than the others, the screaming intensifies. Then nothing.\\\" \\\"It is a very disturbing scene,\\\" said Julian Reichelt, editor-in-chief of Bild online. An official with France's accident investigation agency, the BEA, said the agency is not aware of any such video. Lt. Col. Jean-Marc Menichini, a French Gendarmerie spokesman in charge of communications on rescue efforts around the Germanwings crash site, told CNN that the reports were \\\"completely wrong\\\" and \\\"unwarranted.\\\" Cell phones have been collected at the site, he said, but that they \\\"hadn't been exploited yet.\\\" Menichini said he believed the cell phones would need to be sent to the Criminal Research Institute in Rosny sous-Bois, near Paris, in order to be analyzed by specialized technicians working hand-in-hand with investigators. But none of the cell phones found so far have been sent to the institute, Menichini said. Asked whether staff involved in the search could have leaked a memory card to the media, Menichini answered with a categorical \\\"no.\\\" Reichelt told \\\"Erin Burnett: Outfront\\\" that he had watched the video and stood by the report, saying Bild and Paris Match are \\\"very confident\\\" that the clip is real. He noted that investigators only revealed they'd recovered cell phones from the crash site after Bild and Paris Match published their reports. \\\"That is something we did not know before. ... Overall we can say many things of the investigation weren't revealed by the investigation at the beginning,\\\" he said.\\n\\nWhat was mental state of Germanwings co-pilot? German airline Lufthansa confirmed Tuesday that co-pilot Andreas Lubitz had battled depression years before he took the controls of Germanwings Flight 9525, which he's accused of deliberately crashing last week in the French Alps. Lubitz told his Lufthansa flight training school in 2009 that he had a \\\"previous episode of severe depression,\\\" the airline said Tuesday. Email correspondence between Lubitz and the school discovered in an internal investigation, Lufthansa said, included medical documents he submitted in connection with resuming his flight training. The announcement indicates that Lufthansa, the parent company of Germanwings, knew of Lubitz's battle with depression, allowed him to continue training and ultimately put him in the cockpit. Lufthansa, whose CEO Carsten Spohr previously said Lubitz was 100% fit to fly, described its statement Tuesday as a \\\"swift and seamless clarification\\\" and said it was sharing the information and documents \u2013 including training and medical records \u2013 with public prosecutors. Spohr traveled to the crash site Wednesday, where recovery teams have been working for the past week to recover human remains and plane debris scattered across a steep mountainside. He saw the crisis center set up in Seyne-les-Alpes, laid a wreath in the village of Le Vernet, closer to the crash site, where grieving families have left flowers at a simple stone memorial. Menichini told CNN late Tuesday that no visible human remains were left at the site but recovery teams would keep searching. French President Francois Hollande, speaking Tuesday, said that it should be possible to identify all the victims using DNA analysis by the end of the week, sooner than authorities had previously suggested. In the meantime, the recovery of the victims' personal belongings will start Wednesday, Menichini said. Among those personal belongings could be more cell phones belonging to the 144 passengers and six crew on board. Check out the latest from our correspondents. The details about Lubitz's correspondence with the flight school during his training were among several developments as investigators continued to delve into what caused the crash and Lubitz's possible motive for downing the jet. A Lufthansa spokesperson told CNN on Tuesday that Lubitz had a valid medical certificate, had passed all his examinations and \\\"held all the licenses required.\\\" Earlier, a spokesman for the prosecutor's office in Dusseldorf, Christoph Kumpa, said medical records reveal Lubitz suffered from suicidal tendencies at some point before his aviation career and underwent psychotherapy before he got his pilot's license. Kumpa emphasized there's no evidence suggesting Lubitz was suicidal or acting aggressively before the crash. Investigators are looking into whether Lubitz feared his medical condition would cause him to lose his pilot's license, a European government official briefed on the investigation told CNN on Tuesday. While flying was \\\"a big part of his life,\\\" the source said, it's only one theory being considered. Another source, a law enforcement official briefed on the investigation, also told CNN that authorities believe the primary motive for Lubitz to bring down the plane was that he feared he would not be allowed to fly because of his medical problems. Lubitz's girlfriend told investigators he had seen an eye doctor and a neuropsychologist, both of whom deemed him unfit to work recently and concluded he had psychological issues, the European government official said. But no matter what details emerge about his previous mental health struggles, there's more to the story, said Brian Russell, a forensic psychologist. \\\"Psychology can explain why somebody would turn rage inward on themselves about the fact that maybe they weren't going to keep doing their job and they're upset about that and so they're suicidal,\\\" he said. \\\"But there is no mental illness that explains why somebody then feels entitled to also turn that rage and turn it outward on 149 other people who had nothing to do with the person's problems.\\\" Germanwings crash compensation: What we know. Who was the captain of Germanwings Flight 9525? CNN's Margot Haddad reported from Marseille and Pamela Brown from Dusseldorf, while Laura Smith-Spark wrote from London. CNN's Frederik Pleitgen, Pamela Boykoff, Antonia Mortensen, Sandrine Amiel and Anna-Maja Rappard contributed to this report.\"}"}
{"id": "Us5in-h2Dp", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \u201cin the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\u201d\\n\\nLater that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \u201cAs Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\u201d he said, according to an ICC news release. \u201cIndeed, today brings us closer to our shared goals of justice and peace.\u201d\\n\\nJudge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \u201cAs the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\u201d she said.\\n\\nRights group Human Rights Watch welcomed the development. \u201cGovernments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\u201d said Balkees Jarrah, international justice counsel for the group. \u201cWhat's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\u201d\\n\\nIn January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \u201cstrongly\u201d disagreed with the court's decision. \u201cAs we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\u201d the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \u201cWe will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\u201d it said.\\n\\nBut the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \u201cPalestine.\u201d While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \u201cconduct its analysis in full independence and impartiality.\u201d\\n\\nThe war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.\"}"}
{"id": "Us5in-h2Dp", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 14: The first instance from the WebNLG dataset, which has two gold target sentences.\\n\\nInput\\nDescribe the following data:\\nAbilene, Texas \u2014 cityServed \u2014 Abilene Regional Airport\\n\\nGold\\nAbilene, Texas is served by the Abilene regional airport.\\nAbilene Regional Airport serves the city of Abilene in Texas.\\n\\nBART\\nAbilene Regional Airport serves the city of Abilene in Texas.\\n\\nMVP\\nAbilene Regional Airport serves the city of Abilene, Texas.\\n\\nMVP+S\\nAbilene Regional Airport serves the city of Abilene, Texas.\\n\\nTable 15: The second instance from the WebNLG dataset, which has three gold target sentences.\\n\\nInput\\nDescribe the following data:\\n\\\"Madrid, Paracuellos de Jarama, San Sebasti\u00e1n de los Reyes and Alcobendas\\\" \u2014 location \u2014 Adolfo Su\u00e1rez Madrid\u2013Barajas Airport\\n\\nGold\\nAdolfo Su\u00e1rez Madrid\u2013Barajas Airport can be found in Madrid, Paracuellos de Jarama, San Sebasti\u00e1n de los Reyes and Alcobendas.\\nAdolfo Suarez Madrid-Barajas airport is located at Madrid, Paracuellos de Jarama, San Sebasti\u00e1n de los Reyes and Alcobendas.\\nAdolfo Suarez Madrid-Barajas Airport is located in Madrid, Paracuellos de Jarama, San Sebastian de los Reyes and Alcobendas.\\n\\nBART\\nAdolfo Su\u00e1rez Madrid\u2013Barajas Airport can be found in Madrid, Paracuellos de Jarama, San Sebasti\u00e1n de los Reyes and Alcobendas.\\n\\nMVP\\nAdolfo Su\u00e1rez Madrid\u2013Barajas Airport can be found in Madrid, Paracuellos de Jarama, San Sebasti\u00e1n de los Reyes and Alcobendas.\\n\\nMVP+S\\nAdolfo Su\u00e1rez Madrid\u2013Barajas Airport is located in Madrid, Paracuellos de Jarama, San Sebastian de los Reyes and Alcobendas.\"}"}
{"id": "Us5in-h2Dp", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 16: The first instance from the SQuAD dataset.\\n\\nInput\\n\\nGenerate the question based on the answer:\\n\\nSaint Bernadette Soubirous [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\\n\\nGold\\n\\nTo whom did the Virgin Mary allegedly appear in 1858 in Lourdes, France?\\n\\nBART\\n\\nWho is believed to have appeared to the Virgin Mary at Lourdes?\\n\\nMVP\\n\\nWho did the Virgin Mary appear to in Lourdes?\\n\\nMVP+S\\n\\nThe Grotto is a replica of the grotto at Lourdes, France where the Virgin Mary appeared to whom?\\n\\nTable 17: The second instance from the SQuAD dataset.\\n\\nInput\\n\\nGenerate the question based on the answer:\\n\\nA copper statue of Christ [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\\n\\nGold\\n\\nWhat is in front of the Notre Dame Main Building?\\n\\nBART\\n\\nWhat is in front of the Main Building and facing it?\\n\\nMVP\\n\\nWhat is immediately in front of the Main Building?\\n\\nMVP+S\\n\\nWhat is immediately in front of the Main Building?\\n\\n38\"}"}
{"id": "Us5in-h2Dp", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. WritingPrompts (Fan et al., 2018), we use the sets created by Guan et al. (2021) (who split the original valid and test sets for training, validation, and testing) to fine-tune our model for a fair comparison.\\n\\n- **Task-oriented dialogue system** meets real-life needs of users, such as restaurant reservations and airplane bookings. We use the datasets for pre-training, following Su et al. (2022):\\n  1. CamRest676 (Wen et al., 2017);\\n  2. Frames (El Asri et al., 2017);\\n  3. KVRET (Eric et al., 2017);\\n  4. MetaLWOZ (Lee et al., 2019);\\n  5. MSR-E2E (Li et al., 2018);\\n  6. MultiWOZ (Budzianowski et al., 2018);\\n  7. Schema-Guided (Rastogi et al., 2020a);\\n  8. TaskMaster (Byrne et al., 2019);\\n  9. WOZ (Mrkovi\u0107 et al., 2017).\\n\\nWe utilize the following datasets for fine-tuning evaluation:\\n  1. MultiWOZ (Budzianowski et al., 2018), we utilize the version 2.0;\\n\\n- **Text style transfer** modifies the style (e.g., sentiment and formality) of given texts while retaining their style-independent content. We utilize the following datasets for fine-tuning evaluation:\\n  1. GYAFC (Rao & Tetreault, 2018), which has two sub-domains \u201cEntertainment and Music\u201d (E&M) and \u201cFamily and Relationships\u201d (F&R).\\n\\n- **Text summarization** condenses a long document into a brief text while retaining the essential details. We use the following datasets for pre-training:\\n  1. English Gigaword (Graff et al., 2003), we use the variant provided by Rush et al. (2015);\\n  2. MediaSum (Zhu et al., 2021);\\n  3. MSNews (Liu et al., 2021a);\\n  4. Newsroom (Grusky et al., 2018);\\n  5. WikiHow (Koupaee & Wang, 2018).\\n\\nWe utilize the following datasets for fine-tuning evaluation:\\n  1. CNN/DailyMail (Hermann et al., 2015), we use the variant provided by See et al. (2017);\\n  2. SAMSum (Gliwa et al., 2019);\\n  3. XSum (Narayan et al., 2018).\\n\\nTo better compare with ExT5 (Aribandi et al., 2022), we utilize the language generation benchmark GEM (Gehrmann et al., 2021) for fine-tuning evaluation. GEM includes five tasks:\\n\\n- **Commonsense generation**:\\n  1. CommonGen (CG) (Lin et al., 2020a).\\n\\n- **Data-to-text generation**:\\n  1. DART (Nan et al., 2021);\\n  2. E2E NLG cleaned (Novikova et al., 2017);\\n  3. ToTTo (Su et al., 2021);\\n  4. WebNLG (Gardent et al., 2017).\\n\\n- **Dialogue system**:\\n  1. Schema-Guided Dialog (SGD) (Rastogi et al., 2020b).\\n\\n- **Text simplification**:\\n  1. WikiAuto + Turk/ASSET (WiA-T/A) (Jiang et al., 2020; Xu et al., 2016; Alva-Manchego et al., 2020).\\n\\n- **Text summarization**:\\n  1. Wiki-Lingua (WLE) (Ladhak et al., 2020).\"}"}
{"id": "Us5in-h2Dp", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To test the generalization ability of our model, we also utilize the natural language standing benchmark GLUE (Wang et al., 2019), which is composed of three tasks:\\n\\n- **Natural language inference**\\n  1. MNLI (Williams et al., 2018);\\n  2. QNLI (Rajpurkar et al., 2016; Wang et al., 2019);\\n  3. RTE (Dagan et al., 2006; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009).\\n\\n- **Paraphrase detection**\\n  1. MRPC (Dolan & Brockett, 2005);\\n  2. QQP;\\n  3. STS-B (Cer et al., 2017).\\n\\n- **Text classification**\\n  1. CoLA (Warstadt et al., 2019);\\n  2. SST-2 (Socher et al., 2013).\\n\\nSince our model is pre-trained on a large number of labeled datasets, it may have \u201cseen\u201d examples from fine-tuning test sets during pre-training, which leads to an unfair comparison with other methods. Hence, we eliminate the pre-training examples that share \\\\( n \\\\)-gram overlap with either of the test datasets. Following Brown et al. (2020), \\\\( n \\\\) is the 5th percentile example length in words, and the maximum value of \\\\( n \\\\) is set to 13. Finally, we have removed 17,848 examples from the pre-training datasets. The number of \u201ccleaned\u201d examples for each dataset can be found in Table 7.\"}"}
{"id": "Us5in-h2Dp", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 7: The statistics and licenses of datasets for pre-training our MVP model. The #Train, #Valid, and #Test denote the number of examples in the train, valid, and test sets, respectively. Cleaned #Train represents the number of training examples after filtering. Input and Output are the average number of words (split by space) in the input and output sequences, respectively. These setups and abbreviations are the same below.\\n\\n| Dataset            | #Train | Cleaned #Train | #Valid | #Test | Input | Output | License     |\\n|--------------------|--------|----------------|--------|-------|-------|--------|-------------|\\n| AGENDA             | 38,720 | 38,720         | 1,000  | 1,000 | 52.1  | 141.2  | N/A         |\\n| ENT-DESC           | 88,652 | 88,652         | 11,081 | 11,081| 279.9 | 31.0   | N/A         |\\n| GenWiki            | 681,436| 681,436        | 75,716 | 1,000 | 21.4  | 29.5   | MIT         |\\n| LogicNLG           | 28,450 | 28,450         | 4,260  | 4,305 | 178.4 | 14.2   | MIT         |\\n| TEKGEN             | 6,310,061| 6,307,995     | 788,746| 796,982| 17.0  | 21.2   | CC BY-SA 2.0|\\n| WEATHERGOV         | 25,000 | 25,000         | 1,000  | 3,528 | 148.7 | 30.6   | N/A         |\\n| WikiTableT         | 1,453,794| 1,452,778     | 4,533  | 4,351 | 81.0  | 99.7   | MIT         |\\n| Cleaned OS Dialogs | 13,355,487| 13,355,368    | -      | -     | 75.5  | 16.7   | N/A         |\\n| CMUDoG             | 82,818 | 82,818         | 5,555  | 14,510| 433.0 | 12.2   | N/A         |\\n| Curiosity          | 64,930 | 64,551         | 8,539  | 8,495 | 144.4 | 20.2   | CC BY-NC 4.0 |\\n| DREAM              | 14,264 | 14,242         | 4,709  | 4,766 | 75.6  | 13.6   | N/A         |\\n| Empathetic Dialogues | 64,636 | 64,636         | 9,308  | 8,426 | 52.7  | 12.9   | CC BY-NC 4.0 |\\n| Movie Dialog       | 762,751| 762,711        | 8,216  | 8,066 | 126.9 | 44.0   | N/A         |\\n| MuTual             | 33,691 | 33,691         | 4,090  | 3,248 | 53.6  | 14.5   | N/A         |\\n| OpenDialKG         | 69,680 | 69,680         | 7,743  | -     | 54.2  | 12.4   | CC BY-NC 4.0 |\\n| Topical-Chat       | 179,750| 179,750        | 22,295 | 22,452| 223.3 | 20.0   | CDLA-Sharing-1.0|\\n| Wizard of Wikipedia| 148,357| 147,702        | 15,767 | 15,564| 297.0 | 16.7   | MIT         |\\n| HotpotQA           | 90,447 | 87,815         | 7,405  | -     | 187.9 | 2.2    | CC BY-SA 4.0|\\n| MS MARCO           | 681,445| 681,226        | 77,580 | -     | 68.7  | 13.3   | N/A         |\\n| MSQG               | 198,058| 198,029        | 11,008 | -     | 48.1  | 3.7    | CC BY-SA 4.0|\\n| NarrativeQA        | 65,494 | 65,494         | 6,922  | 21,114| 584.1 | 4.2    | Apache 2.0  |\\n| Natural Questions  | 96,676 | 96,676         | 10,693 | 6,490 | 9.0   | 2.1    | CC BY-SA 3.0|\\n| NewsQA             | 97,850 | 97,700         | 5,486  | 5,396 | 726.8 | 5.0    | MIT         |\\n| QuAC               | 69,109 | 69,026         | 26,301 | -     | 496.7 | 6.5    | CC BY-SA 4.0|\\n| TriviaQA           | 78,785 | 78,785         | 8,837  | 11,313| 14.0  | 2.0    | Apache 2.0  |\\n| WebQuestions       | 8,933  | 8,933          | 4,863  | 4,863 | 6.7   | 2.4    | CC BY 4.0   |\\n| HotpotQA           | 90,440 | 87,808         | 6,972  | -     | 79.6  | 19.8   | CC BY-SA 4.0|\\n| MS MARCO           | 681,445| 681,226        | 77,580 | -     | 75.9  | 6.0    | N/A         |\\n| MSQG               | 198,058| 198,029        | 11,008 | -     | 45.9  | 6.0    | CC BY-SA 4.0|\\n| NarrativeQA        | 65,494 | 65,494         | 6,922  | 21,114| 579.7 | 8.6    | Apache 2.0  |\\n| NewsQA             | 97,850 | 97,700         | 5,486  | 5,396 | 724.2 | 7.6    | MIT         |\\n| QuAC               | 69,109 | 69,026         | 26,301 | -     | 496.7 | 6.5    | CC BY-SA 4.0|\\n| ChangeMyView       | 42,462 | 42,459         | 6,480  | 7,562 | 17.9  | 104.1  | MIT         |\\n| English Gigaword   | 3,803,957| 3,802,620      | 189,651| 1,951 | 8.8   | 33.3   | MIT         |\\n| Hippocorpus        | 6,168  | 6,168          | 686    | -     | 34.1  | 262.6  | CDLA-Permissive 2.0 |\\n| WikiPlots          | 101,642| 101,641        | 11,294 | -     | 3.4   | 338.5  | N/A         |\\n| WritingPrompts     | 272,600| 272,518        | 15,620 | 15,138| 28.4  | 630.8  | MIT         |\\n| CamRest676         | 4,872  | 4,872          | 616    | -     | 55.3  | 9.4    | N/A         |\\n| Frames             | 26,631 | 26,631         | 2,106  | -     | 116.1 | 13.0   | MIT         |\\n| KVRET              | 14,136 | 14,136         | 1,616  | -     | 30.5  | 9.3    | N/A         |\\n| MetaLWOZ           | 176,073| 176,073        | 17,912 | -     | 45.6  | 8.0    | N/A         |\\n| MSR-E2E            | 103,362| 103,362        | 5,235  | -     | 51.3  | 12.8   | Microsoft   |\\n| Schema-Guided      | 494,946| 494,933        | 73,089 | -     | 120.8 | 12.5   | CC BY-SA 4.0|\\n| TaskMaster         | 249,664| 249,662        | 20,680 | -     | 95.6  | 12.0   | CC BY 4.0   |\\n| WOZ                | 6,364  | 6,359          | 1,260  | -     | 47.0  | 10.6   | N/A         |\\n| English Gigaword   | 3,803,957| 3,802,620      | 189,651| 1,951 | 33.3  | 8.8    | MIT         |\\n| MediaSum           | 443,596| 442,021        | 10,000 | 10,000| 1641.0| 14.4   | N/A         |\\n| MSNews             | 136,082| 135,937        | 7,496  | 7,562 | 309.9 | 9.8    | CC BY-SA 4.0|\\n| Newsroom           | 995,041| 989,351        | 108,837| 108,862| 642.4 | 26.7   | N/A         |\\n| WikiHow            | 157,252| 157,247        | 5,599  | 5,577 | 502.6 | 45.6   | CC BY-NC-SA |\\n| ChangeMyView       | 42,462 | 42,459         | 6,480  | 7,562 | 17.9  | 104.1  | MIT         |\\n| English Gigaword   | 3,803,957| 3,802,620      | 189,651| 1,951 | 33.3  | 8.8    | MIT         |\\n| MediaSum           | 443,596| 442,021        | 10,000 | 10,000| 1641.0| 14.4   | N/A         |\\n| MSNews             | 136,082| 135,937        | 7,496  | 7,562 | 309.9 | 9.8    | CC BY-SA 4.0|\\n| Newsroom           | 995,041| 989,351        | 108,837| 108,862| 642.4 | 26.7   | N/A         |\\n| WikiHow            | 157,252| 157,247        | 5,599  | 5,577 | 502.6 | 45.6   | CC BY-NC-SA |\\n| ChangeMyView       | 42,462 | 42,459         | 6,480  | 7,562 | 17.9  | 104.1  | MIT         |\\n| English Gigaword   | 3,803,957| 3,802,620      | 189,651| 1,951 | 33.3  | 8.8    | MIT         |\\n| MediaSum           | 443,596| 442,021        | 10,000 | 10,000| 1641.0| 14.4   | N/A         |\\n| MSNews             | 136,082| 135,937        | 7,496  | 7,562 | 309.9 | 9.8    | CC BY-SA 4.0|\\n| Newsroom           | 995,041| 989,351        | 108,837| 108,862| 642.4 | 26.7   | N/A         |\\n| WikiHow            | 157,252| 157,247        | 5,599  | 5,577 | 502.6 | 45.6   | CC BY-NC-SA |\"}"}
{"id": "Us5in-h2Dp", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 8: The statistics and licenses of datasets for evaluating our MVP model. The license of the MNLI dataset is composed of OANC, CC BY-SA 3.0, and CC BY 3.0. The license of the CoQA dataset is composed of CC BY-SA 4.0, MSR-LA, and Apache 2.0. The license of the WiA-A/T datasets is composed of CC BY-NC 3.0, CC BY-NC 4.0, and GNU General Public License v3.0.\\n\\n| Task                | Dataset    | #Train   | #Valid  | #Test   | #Input | #Output | License          |\\n|---------------------|------------|----------|---------|---------|--------|---------|------------------|\\n| Commonsense generation | CommonGen  | 67,389   | 993     | \u2013       | 5.5    | 11.6    | MIT              |\\n| Data-to-text generation | DART      | 62,659   | 2,768   | \u2013       | 27.5   | 21.5    | MIT              |\\n|                      | E2E        | 33,525   | 4,299   | \u2013       | 9.5    | 20.6    | CC BY-SA 4.0     |\\n|                      | ToTTo      | 120,761  | 7,700   | \u2013       | 37.8   | 18.0    | CC BY-SA 3.0     |\\n|                      | WebNLG     | 34,338   | 4,313   | 4,222   | 18.0   | 19.9    | CC BY-NA-SA 4.0  |\\n|                      | WebNLG (GEM) | 35,426   | 1,667   | \u2013       | 17.7   | 22.7    | CC BY-NA-SA 4.0  |\\n|                      | WikiBio    | 582,659  | 72,831  | 72,831  | 81.6   | 26.1    | CC BY-SA 3.0     |\\n| Open-ended dialogue  | DailyDialog | 76,052   | 7,069   | 6,740   | 72.5   | 13.9    | CC BY-NC-SA 4.0  |\\n|                      | DSTC7-A VSD| 76,590   | 17,870  | 1,710   | 148.2  | 11.5    | MIT              |\\n|                      | PersonaChat| 122,499  | 14,602  | 14,056  | 132.1  | 11.9    | MIT              |\\n|                      | SGD        | 164,982  | 10,000  | \u2013       | 134.7  | 11.3    | CC BY-SA 4.0     |\\n| Natural language inference | MNLI-m    | 392,702  | 9,815   | 9,796   | 29.8   | \u2013       | Mixed            |\\n|                      | QNLI       | 104,743  | 5,463   | 5,463   | 36.6   | \u2013       | CC BY-SA 4.0     |\\n|                      | RTE        | 2,490    | 277     | 3,000   | 51.0   | \u2013       | N/A              |\\n| Paraphrase generation | Quora     | 137,185  | 3,000   | 3,000   | 10.9   | 10.8    | N/A              |\\n| Paraphrase detection | MRPC       | 3,668    | 408     | 1,725   | 43.8   | \u2013       | N/A              |\\n|                      | QQP        | 363,846  | 40,430  | 390,965 | 22.3   | \u2013       | N/A              |\\n|                      | STS-B      | 5,749    | 1,500   | 1,379   | 20.3   | \u2013       | N/A              |\\n| Question answering   | CoQA       | 107,286  | 31,621  | \u2013       | 349.4  | 2.6     | Mixed            |\\n|                      | SQuAD      | 75,722   | 10,570  | 11,877  | 156.2  | 3.6     | CC BY-SA 4.0     |\\n| Question generation  | CoQA       | 107,286  | 31,621  | \u2013       | 346.6  | 5.5     | Mixed            |\\n|                      | SQuAD      | 75,722   | 10,570  | 11,877  | 148.3  | 11.6    | CC BY-SA 4.0     |\\n| Story generation     | ROCStories | 176,688  | 9,816   | 4,909   | 9.0    | 40.7    | N/A              |\\n|                      | WritingPrompts | 53,516 | 4,000   | 2,000   | 25.5   | 150.4   | MIT              |\\n| Task-oriented dialogue | MultiWOZ | 170,220  | 22,074  | 22,116  | 128.3  | 11.3    | MIT              |\\n| Text classification  | CoLA       | 8,551    | 1,043   | 1,063   | 7.7    | \u2013       | N/A              |\\n|                      | SST-2      | 67,349   | 872     | 1,821   | 9.8    | \u2013       | N/A              |\\n| Text simplification  | WiA-A      | 483,801  | 20,000  | 359     | 26.2   | 21.5    | Mixed            |\\n|                      | WiA-T      | 359      |         |         |        |         |                  |\\n| Text style transfer  | GY AFC-E&M | 52,595   | 11,508  | 1,416   | 9.9    | 10.6    | N/A              |\\n|                      | GY AFC-F&R | 51,967   | 11,152  | 1,332   | 10.7   | 11.3    | N/A              |\\n| Text summarization   | CNN/DailyMail | 287,227 | 13,368  | 11,490  | 679.8  | 48.3    | MIT              |\\n|                      | SAMSum     | 14,732   | 818     | 819     | 103.4  | 20.3    | CC BY-NC-ND 4.0  |\\n|                      | XSum       | 204,045  | 11,332  | 11,334  | 373.7  | 21.1    | MIT              |\"}"}
{"id": "Us5in-h2Dp", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Us5in-h2Dp", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Us5in-h2Dp", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Us5in-h2Dp", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. Creating training corpora for NLG micro-planners. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 179\u2013188, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1017. URL https://aclanthology.org/P17-1017.\\n\\nSebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, A-\\\\textsc{nuoluwapo} Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-\\\\textsc{Adriana} Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ond\\\\\u2019\\\\textsc{rej} Du\\\\\u2019\\\\textsc{sek}, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, Jo \\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasciitilde\\\\textasciitilde\\\\textsc{\\\\textord doublequote}\\\\textasci"}
