{"id": "Rx_nbGdtRQD", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"which can be expanded to,\\n\\n\\\\[\\nL(T, \\\\tilde{S}_\\\\sigma) = -K_0 \\\\sum_{k=1}^{1} \\\\frac{1}{K_0} \\\\ln p(b^T = 1 | S_{\\\\sigma}, k) + (1 - p(b^T = 1 | S_{\\\\sigma}, k)) \\\\ln \\\\frac{1}{1 - p(b^T = 1 | \\\\tilde{S}_\\\\sigma, k)}.\\n\\\\]\\n\\nwhere \\\\(S_{\\\\sigma} \\\\in M_T S\\\\). Given \\\\(S_{\\\\sigma} \\\\in M_T S\\\\), then \\\\(p(b^T = 1 | S_{\\\\sigma}, k) = 1\\\\) always holds, which means the negative case in Eqn. 21 can be ignored, yielding the following simplified form:\\n\\n\\\\[\\nL(T, \\\\tilde{S}_\\\\sigma) = - \\\\frac{1}{K_0} \\\\sum_{k=1}^{1} \\\\ln p(b^T = 1 | \\\\tilde{S}_\\\\sigma, k) = \\\\frac{1}{K_0} \\\\sum_{k=1}^{1} \\\\ln p(c_{kt} | \\\\tilde{S}_\\\\sigma, k).\\n\\\\]\\n\\nEqn. 7 and so \\\\(L(T, \\\\tilde{S}_\\\\sigma)\\\\) is simply the negative log-likelihood of sampling a satisfied theory \\\\((b^T = 1)\\\\) from soft-structure \\\\(\\\\tilde{S}_\\\\sigma\\\\), for randomly sampled grounding \\\\(k\\\\). Next, we show the similarities between \\\\(L(T, \\\\tilde{S}_\\\\sigma)\\\\) and \\\\(\\\\Gamma_{\\\\tilde{S}_\\\\sigma T}\\\\) by looking at the likelihood \\\\(p(b^T = 1 | \\\\tilde{S}_\\\\sigma, k)\\\\). First, we define \\\\(\\\\bar{\\\\Gamma}_{\\\\tilde{S}_\\\\sigma T}\\\\) by isolating the likelihood:\\n\\n\\\\[\\n\\\\exp(-L(T, \\\\tilde{S}_\\\\sigma)) = \\\\frac{1}{K_0} \\\\prod_{k=1}^{1} p(b^T = 1 | \\\\tilde{S}_\\\\sigma, k)_{1/K_0} = \\\\bar{\\\\Gamma}_{\\\\tilde{S}_\\\\sigma T}(22)\\n\\\\]\\n\\nWe then expand \\\\(p(b^T = 1 | \\\\tilde{S}_\\\\sigma, k)\\\\) to:\\n\\n\\\\[\\np(b^T = 1 | \\\\tilde{S}_\\\\sigma, k) = \\\\frac{1}{K_1} \\\\sum_{t=1}^{t_{+}} p(c_{kt} | \\\\tilde{S}_\\\\sigma, k) p(c_t | \\\\tilde{S}_\\\\sigma, k) = \\\\sum_{t \\\\in t_{+}} p(c_{kt} | \\\\tilde{S}_\\\\sigma, k)_{1/\\\\bar{\\\\Gamma}_{\\\\tilde{S}_\\\\sigma T}} (23)\\n\\\\]\\n\\nwhere \\\\(t_{+}\\\\) is defined as before. For all other \\\\(t \\\\neq t_{+}\\\\), \\\\(p(b^T = 1 | c_{kt}) = 0\\\\) and so this acts as a filter, yielding:\\n\\n\\\\[\\n\\\\bar{\\\\Gamma}_{\\\\tilde{S}_\\\\sigma T} = \\\\frac{1}{K_0} \\\\prod_{k=1}^{1} \\\\sum_{t \\\\in t_{+}} p(c_{kt} | \\\\tilde{S}_\\\\sigma, k)_{1/\\\\bar{\\\\Gamma}_{\\\\tilde{S}_\\\\sigma T}}.\\n\\\\]\\n\\n\\\\(p(c_{kt} | \\\\tilde{S}_\\\\sigma, k)\\\\) is calculated by evaluating the belief of each relation-decoder against the expected truth-assignment as defined by truth-table row \\\\(c_{kt}\\\\):\\n\\n\\\\[\\np(c_{kt} | \\\\tilde{S}_\\\\sigma, k) = l \\\\prod_{m=1}^{m} \\\\phi_{r_m}(O_{km}) c_{km} (1 - \\\\phi_{r_m}(O_{km})) \\\\frac{1}{1 - c_{km}} = f(\\\\phi_{r_m}, O_{km}, c_{km})\\n\\\\]\\n\\nwhere \\\\(r_m\\\\) is the relation for atomic formula associated with column \\\\(m\\\\) (which is the same for each \\\\(k\\\\) slice and \\\\(t\\\\) row) and \\\\(O_{km}\\\\) is the grounding of this entry for slice \\\\(k\\\\) (which is the same across rows).\\n\\nPutting it all back together, we finally have that:\\n\\n\\\\[\\n\\\\bar{\\\\Gamma}_{\\\\tilde{S}_\\\\sigma T} = \\\\frac{1}{K_0} \\\\prod_{k=1}^{1} \\\\sum_{t \\\\in t_{+}} l \\\\prod_{m=1}^{m} f(\\\\phi_{r_m}, O_{km}, c_{km})_{1/\\\\bar{\\\\Gamma}_{\\\\tilde{S}_\\\\sigma T}}\\n\\\\]\\n\\nwhich makes the similarities between \\\\(\\\\Gamma_{\\\\tilde{S}_\\\\sigma T}\\\\) and \\\\(\\\\bar{\\\\Gamma}_{\\\\tilde{S}_\\\\sigma T}\\\\) clear and exposes their relationship. In particular, for the special case where \\\\(|M_T S| = 1\\\\), the outer sum for \\\\(\\\\Gamma_{\\\\tilde{S}_\\\\sigma T}\\\\) can be removed, and the remaining differences between \\\\(\\\\Gamma_{\\\\tilde{S}_\\\\sigma T}\\\\) and \\\\(\\\\bar{\\\\Gamma}_{\\\\tilde{S}_\\\\sigma T}\\\\) are the sum over \\\\(t_{+}\\\\) rows and difference in exponent over \\\\(f(\\\\phi_{r_m}, O_{km}, c_{km})\\\\). For \\\\(\\\\Gamma_{\\\\tilde{S}_\\\\sigma T}\\\\) to be maximised, through \\\\(p(S_{\\\\sigma} | \\\\tilde{S}_\\\\sigma) \\\\approx 1\\\\), we would find that...\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\n\\\\[ \\\\sigma_{\\\\text{maximally supports only the rows associated with}} \\\\]\\n\\n\\\\[ k \\\\text{ grounding. Notice that } \\\\Gamma_{\\\\tilde{\\\\Sigma} \\\\sigma T} \\\\text{ is again bound to (0,1) and achieves } \\\\Gamma_{\\\\tilde{\\\\Sigma} \\\\sigma T} \\\\approx 1 \\\\text{ when } \\\\Gamma_{\\\\tilde{\\\\Sigma} \\\\sigma T} \\\\approx 1. \\\\]\\n\\nWe use the correspondence between \\\\( \\\\Gamma_{\\\\tilde{\\\\Sigma} \\\\sigma T} \\\\) and \\\\( \\\\bar{\\\\Gamma}_{\\\\tilde{\\\\Sigma} \\\\sigma T} \\\\) to define a practical \\\\( \\\\epsilon \\\\)-proxy consistency measure as follows. We firstly re-express \\\\( \\\\epsilon \\\\)-consistency/coherence but for \\\\( \\\\bar{\\\\Gamma}_{\\\\tilde{\\\\Sigma} \\\\sigma T} \\\\) and a different \\\\( \\\\bar{\\\\epsilon} \\\\). We then trace this back to \\\\( L(T, \\\\tilde{\\\\Sigma} \\\\sigma) \\\\) so a bound in terms of the consistency loss can be reported as the overall \\\\( \\\\epsilon \\\\)-proxy. Together this yields\\n\\n\\\\[ \\\\bar{\\\\epsilon} \\\\geq 1 - \\\\bar{\\\\Gamma}_{\\\\tilde{\\\\Sigma} \\\\sigma T} \\\\ln 1 - \\\\bar{\\\\epsilon} \\\\geq -\\\\ln(\\\\bar{\\\\Gamma}_{\\\\tilde{\\\\Sigma} \\\\sigma T}) \\\\geq L(T, \\\\tilde{\\\\Sigma} \\\\sigma) \\\\]\\n\\n(26)\\n\\nand we arrive at an \\\\( \\\\epsilon \\\\)-proxy of the form\\n\\n\\\\[ \\\\ln \\\\frac{1}{1 - \\\\bar{\\\\epsilon}} \\\\]\\n\\nwhich is reported in the main text.\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Relational representations play a prominent role in Knowledge Graph Embedding, wherein sets of relation-decoders are jointly learned in order to obtain a semantic latent representation for data points (Socher et al., 2013; Trouillon et al., 2016; 2019; Bordes et al., 2013; Nickel et al., 2016a; Wang et al., 2017; Dai et al., 2020; Kazemi & Poole, 2018; Abboud et al., 2020). Although these typically do not use a shared autoencoder as we do in this paper, Schlichtkrull et al. (2018) did adopt an autoencoding framework, where a graph neural network is used as the encoder, however they did not work with visual data and the model was only applied to single data sets. Similarly, disentanglement is also concerned with semantic representation learning (Bengio et al., 2013), and has been explored using a variety of methods including both Generative Adversarial Networks (Chen et al., 2016) and VAEs (Burgess et al., 2017; Higgins et al., 2017; Chen et al., 2018; Ridgeway & Mozer, 2018; Eastwood & Williams, 2018; Kumar et al., 2018; Locatello et al., 2019). Disentangled representations have been evaluated in terms of their transferability in (van Steenkiste et al., 2019; Steenbrugge et al., 2018; Locatello et al., 2020). A bridge between these two fields, wherein relation-decoders are employed as a semi-supervision to VAEs can be found in (Karaletsos et al., 2016; Chen & Batmanghelich, 2020; 2019), where (Karaletsos et al., 2016) use multiple relation-decoders but compute a triplet comparison based query and (Chen & Batmanghelich, 2020; 2019) only include a single binary relation and use function forms that are not sufficient to model the full set of relations that we include in this work. Neither presents a comprehensive analysis of resulting concept coherence. Lastly, we note that our experimental setup is most reminiscent of domain adaptation (Redko et al., 2019). To the best of our knowledge, no work has compared relation-decoders in their ability to learn coherent concepts, as measured by their consistency across domains.\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The BlockStacks dataset consists of 12,000 images (200 x 200 pixels but resized in code to 128 x 128) of individual block stacks, of varying height (between 1-10 blocks), block colors (uniformly sampled from options: {gray, blue, green, brown, purple, cyan, yellow}) and position (uniformly sampled from x,y range (-3,-3) to (3,3)), but with the requirement that each instance consists of a single red block at a random height (see Figure 4 for example images). These were rendered using the CLEVR rendering agent with the help of code from (Asai, 2018). The dataset is divided into 9000:1500:1500 train, validation and test splits.\\n\\nThe $\\\\beta$-VAE is derived by introducing an approximate posterior $q_\\\\alpha(Z|X)$, from which a lower bound (commonly referred to as the Evidence LOwer Bound (ELBO)) on the true marginal $\\\\log p_\\\\theta(X)$ can be obtained by using Jensen's inequality (Kingma & Welling, 2014). The $\\\\beta$-VAE maximises the log-probability by maximising this lower bound, given by:\\n\\n$$L_{\\\\text{ELBO}}^{\\\\beta-\\\\text{VAE}} = E_{q_\\\\alpha(Z|X)}[\\\\log p_\\\\theta(X|Z)] - \\\\beta D_{\\\\text{KL}}(q_\\\\alpha(Z|X) \\\\parallel p_\\\\theta(Z)),$$\\n\\n(13)\\n\\nwhere $q_\\\\alpha(Z|X)$ is typically modelled as a neural-network encoder with parameters $\\\\alpha$. Similarly $p_\\\\theta(X|Z)$ is often modelled as a neural-network decoder with parameters $\\\\theta$ and is calculated as a Monte Carlo estimation. A reparameterization trick is used to enable differentiation through an otherwise undifferentiable sampling from $q_\\\\alpha(Z|X)$ (see (Kingma & Welling, 2014)). In the $\\\\beta$-VAE (Higgins et al., 2017; Burgess et al., 2017), an additional $\\\\beta$ scalar hyperparameter was added as it was found to influence disentanglement through stronger distribution matching pressure with respect to the prior $p_\\\\theta(Z)$, where this prior is typically set to an isotropic zero-mean Gaussian $\\\\mathcal{N}(0, I)$.\\n\\nWhen $\\\\beta = 1$ we obtain the standard VAE objective (Kingma & Welling, 2014).\\n\\nIn this section we firstly present an in-depth analysis of the key innovations presented by DC which provides insight into how it can learn a coherent notion of ordinality. We then provide model details for each of the compared relation-decoders in the main results and the $\\\\beta$-VAE architecture that we employ for each data set.\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Depiction of a set of DC relation-decoders for binary relations $\\\\text{isGreater}$, $\\\\text{isLess}$, $\\\\text{isEqual}$, $\\\\text{isSuccessor}$ and $\\\\text{isPredecessor}$. Each DC relation-decoder (for each relation) has a one-hot mask $u_r$ (that is in this example the same across relations), which ensures only the zeroth dimensions of the embedding arguments are compared, giving $z_{i,0}$ and $z_{j,0}$.\\n\\nD.1 Dynamic Comparator Analysis\\n\\nFigure 5 depicts how DC is able to learn the $\\\\text{isGreater}$, $\\\\text{isLess}$, $\\\\text{isEqual}$, $\\\\text{isSuccessor}$ and $\\\\text{isPredecessor}$ family of binary ordinal relations, assuming each corresponding relation-decoder has learned a common one-hot mask on the zeroth dimension i.e. $u_G = u_E = ... = u_P = [1, ..., 0]$, such that activations only depend on the $z_{i,0} - z_{i,1}$ difference. An important capability of DC is its ability to select, via an appropriate functional mode, either $\\\\phi^\\\\dagger_r$ or $\\\\phi^\\\\ddagger_r$, depending on the type of relation it needs to model. As shown by Figure 5, $\\\\text{isEqual}$ exhibits its reflexive, symmetric and transitive characteristics, whilst $\\\\text{isGreater}$ and $\\\\text{isLess}$ both carry transitivity but are asymmetric and irreflexive. Furthermore, the use of a subtraction between $z_i$ and $z_j$ (which, via mask $u_r$ ends up only being a subtraction between their zeroth dimensions) leads to a relative comparison, not an absolute comparison, which generalises to arbitrary $z_i$ and $z_j$ sampled from anywhere in $\\\\mathbb{Z}$.\\n\\nNote that there is no built in parameter sharing, meaning each relation-decoder (for each individual relation $r$) is trained independently and has its own set of $a_r$, $u_r$, $\\\\eta_{r,0}$, $\\\\eta_{r,1}$, $b^\\\\dagger_r$ and $b^\\\\ddagger_r$ parameters. However, our experiments show that DC reliably obtains settings such that e.g. $u_G = u_E$, or $a_G = a_L = [0, 1]$, or $b^\\\\ddagger_G = -b^\\\\ddagger_L$ and so on. DC is thus able to discover the interdependencies between families of relations. By learning to indirectly \u2018tie\u2019 together parameters in this way, whilst still being expressive enough to model each type of relation, DC can facilitate a data-driven binding between relation-decoder outputs. This helps ensure consistent generalisation across a latent subspace, as defined by the common/overlapped $u_r$ masks.\\n\\nD.2 Relation-Decoder Implementations\\n\\nTransR (Lin et al., 2015): $\\\\phi_{\\\\text{TransR}}(z_i, z_j) = \\\\|h_r + r - t_r\\\\|_2^2$ with, $h_r = M_r z_i$ and $t_r = M_r z_j$. As we want to obtain a $[0,1]$ output, we modify TransR through $\\\\phi_{\\\\text{TransR}^+}(z_i, z_j) = \\\\sigma(c - \\\\phi_{\\\\text{TransR}}(z_i, z_j))$, where $\\\\sigma$ is a sigmoid function and $c$ is a scalar that ensures that at $\\\\phi_{\\\\text{TransR}}(z_i, z_j) = 0$, then $\\\\phi_{\\\\text{TransR}^+}(z_i, z_j) \\\\approx 1$. In all experiments we set $c = 10$.15\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NTN (modified version of (Socher et al., 2013) from (Donadello et al., 2017; Serafini & Garcez, 2016)):\\n\\n\\\\[ \\\\phi_r(z_1, \\\\ldots, z_n) = \\\\sigma(u_r^\\\\top r \\\\tanh(z_c^\\\\top M_r z_c + V_r z_c + b_r)) \\\\]  \\n\\n(14)\\n\\nwhere \\\\( u_r \\\\in \\\\mathbb{R}^k \\\\), \\\\( M_r \\\\in \\\\mathbb{R}^{n \\\\times d} \\\\), \\\\( V_r \\\\in \\\\mathbb{R}^{k \\\\times n \\\\times d} \\\\) and \\\\( b_r \\\\in \\\\mathbb{R}^k \\\\). The only hyperparameter to consider is \\\\( k \\\\), which controls the NTN's capacity - in all experiments, we set this to 1. If \\\\( k > 1 \\\\), \\\\( z_c^\\\\top M_r z_c \\\\) produces a \\\\( k \\\\)-dimension vector by applying the bilinear operation to each of the \\\\( k \\\\) \\\\( M_r \\\\) slices. Here \\\\( z_c \\\\in \\\\mathbb{R}^{n \\\\times d} \\\\) is a concatenation of the inputs \\\\( z_1, \\\\ldots, z_n \\\\), which was introduced in (Donadello et al., 2017; Serafini & Garcez, 2016). In contrast, the original NTN (see (Socher et al., 2013)) is only applicable to binary relations and does not include the outer sigmoid.\\n\\nHolE (Nickel et al., 2016b):\\n\\n\\\\[ \\\\phi_{\\\\text{HolE}}(z_i, z_j) = \\\\sigma(r^\\\\top (z_i \\\\star z_j)) \\\\]\\n\\nwhere \\\\( r \\\\in \\\\mathbb{R}^d \\\\) and \\\\( \\\\star : \\\\mathbb{R}^d \\\\times \\\\mathbb{R}^d \\\\to \\\\mathbb{R}^d \\\\) denotes the circular correlation operator and is given by,\\n\\n\\\\[\\n[z_i \\\\star z_j]_k = d-1 \\\\sum_{m=0} z_{i,m} z_{j,(k+m) \\\\mod d}\\n\\\\]\\n\\nNN: a simple four-layer neural-network with layer sizes \\\\( l_{\\\\text{in}} = 2^{d} \\\\), \\\\( l_1 = 2^{d} \\\\) and \\\\( l_2 = d \\\\), with ReLU activations (Nair & Hinton, 2010). The final output layer, \\\\( l_{\\\\text{out}} \\\\), is a single value passed through a sigmoid function, to bound the output within (0,1).\\n\\nD.3 \u03b2-VAE CONFIGURATION\\n\\nThe model configurations used for both MNIST and BlockStacks data sets are given in Table 2.\\n\\nD.4 L_{\\\\text{joint}} CONFIGURATION\\n\\nIn the source domain, we vary \\\\( \\\\beta \\\\) values between \\\\{1, 4, 8, 12\\\\} and fix \\\\( \\\\lambda = 10^{3} \\\\). In the target domain, we fix \\\\( \\\\beta \\\\) to \\\\( 10^{-4} \\\\) and \\\\( \\\\lambda = 10^{-2} \\\\) and normalise the \\\\( L_{\\\\text{ELBO}} \\\\beta\\\\text{-VAE} \\\\) reconstruction term by dividing by a factor \\\\( \\\\frac{1}{\\\\sqrt{H \\\\cdot W \\\\cdot C}} \\\\), for height \\\\( H \\\\), width \\\\( W \\\\) and color channels \\\\( C \\\\), and normalize the distribution matching term by a factor \\\\( \\\\frac{1}{d_z} \\\\), for latent representation size \\\\( d_z \\\\).\\n\\nTo train relation-decoders over a given domain \\\\( S \\\\), it is necessary to supervise estimates of \\\\( \\\\phi_r(\\\\psi_{\\\\text{enc}}(S)(O)) \\\\), \\\\( O \\\\in S^2 \\\\), against corresponding ground-truth labels, \\\\( \\\\gamma_r^{O, S} \\\\). However, doing so for every \\\\( O \\\\in S^2 \\\\) can easily become intractable and we instead only sample a subset of possible \\\\( S^2 \\\\) tuples. Our sampling strategy involves first selecting a ratio \\\\( R = \\\\frac{|B|}{|S|} \\\\) where \\\\( B \\\\subset S^2 \\\\) is a set of \\\\( O \\\\) tuples. We then sample relation-decoder specific subsets \\\\( B_r \\\\) where \\\\( |B_r| = \\\\frac{|B| \\\\cdot \\\\sigma}{\\\\sigma} \\\\), to ensure a balanced distribution of tuples between relation-decoders. Furthermore, we ensure that each \\\\( B_r \\\\) contains a balanced ratio of \\\\( \\\\gamma_r^{O, S} = 1 \\\\) versus \\\\( \\\\gamma_r^{O, S} = 0 \\\\) instances. We found that each \\\\( |B_r| \\\\) set can be small without jeopardising the final relation-decoder performance level, allowing us to use \\\\( R = 1 \\\\) for MNIST experiments and \\\\( R = 3 \\\\) for BlockStacks experiments.\\n\\nFinally, in all experiments we use a \u03b2-VAE trained for up to 300,000 steps, following accepted practice from (Locatello et al., 2019; Steenbrugge et al., 2018), together with any included relation-decoders. However, to ensure computation efficiency across experiments, we employ an early stopping procedure, where if the validation score does not increase over 30 and 120 training epochs for MNIST and Blockstacks experiments, respectively, we end the training early.\\n\\nE SUPPLEMENTARY RESULTS\\n\\n\u03b2 effect on intrinsic relation-decoder characteristics:\\n\\nWe have seen how \\\\( \\\\beta \\\\) impacts PRT accuracy but it is not clear how this is facilitated. To understand the way in which \\\\( \\\\beta \\\\) affects each relation-decoder we produce a gradient-conformity evaluation, based on the intuition that collections of relation-decoder outputs will have to shift together in order to maintain consistency, facilitated...\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: $\\\\epsilon$-proxy coherence comparison, with respect to source and target data-embedding consistency levels. Results are reported with the corresponding $\\\\beta^*$ setting (in parenthesis).\\n\\n| Model  | $\\\\beta^*$ | $\\\\phi$ (Aggr.) | $\\\\phi$ (Con-A) | $\\\\phi$ (Con-I-tr) | $\\\\phi$ (Con-I-asym) | $\\\\phi$ (Con-I-refl) |\\n|-------|-----------|-----------------|-----------------|-------------------|-------------------|-------------------|\\n| TransR | 90.33 (8) | 44.34 (12)      | 35.30 (8)       | 9.94 (8)          | 0.55 (8)          |\\n| HolE   | 82.06 (8) | 41.18 (8)       | 32.15 (4)       | 5.96 (1)          | 0.07 (8)          |\\n| NTN    | 79.54 (8) | 38.91 (8)       | 30.08 (12)      | 4.49 (12)         | 0.09 (12)         |\\n| NN     | 34.09 (8) | 24.78 (8)       | 7.24 (8)        | 3.88 (8)          | 0.04 (4)          |\\n| DC     | 0.34 (1)  | 0.07 (1)        | 0.18 (1)        | 0.00 (1)          | 0.09 (1)          |\\n\\nTransR performs generally worse. This may be caused by TransR producing weaker beliefs in comparison to other models, as this can result in a worse overall consistency level. Looking at $\\\\beta^*$ profiles, we see that most models achieve optimum aggregate $\\\\epsilon$-proxy coherence at $\\\\beta^* = 8$, other than DC which performs better at $\\\\beta^* = 1$. Overall, this is in agreement with the $\\\\beta^*$ profiles given by Figure 2-bottom (right). However, we can see that $\\\\beta^*$ profiles for Con-A based $\\\\epsilon$-proxy coherence are in more direct agreement - as TransR achieves its best at $\\\\beta^* = 12$ - suggesting that Con-A invariance is more important to concept transfer.\\n\\n8 DISCUSSION AND CONCLUDING REMARKS\\n\\nIn this work, we introduced the notion of a soft-structure, which can learn a structure over real-world domains, through the use of a domain-encoder coupled with modular relation-decoders. We subsequently provided formal definitions, defining what it means for a concept to be coherent in terms of domain-invariance of soft-structure consistency with respect to a theory. We then outlined a neural model and experimental procedure that together allowed us to investigate how concept coherence differs when choosing different implementations for underlying relation-decoders and its impact on concept transfer. Our results suggest that increasing regularisation over relation-decoder models, either in the form of disentanglement pressure or relation-decoder model capacity, seems to improve their ability to learn coherent concepts. Firstly, strong PRT transfer for DC and NN (given an appropriately high $\\\\beta$ setting) showed that both relation-decoder models are able to minimise Eqn. 9 in the source domain and retain good performance in the target domain. Consistency profiles over partial theories (subsets of the sentences that comprise the overall theory of ordinality), covering multiple data-splits, then further suggested that a relation-decoder's ability to retain consistency over interpolated/extrapolated regions with respect to the observed data-encodings (during training) greatly impact concept coherence. Finally, an $\\\\epsilon$-proxy coherence comparison showed that DC achieved excellent coherence with an aggregated $\\\\epsilon$-proxy of 0.34, which mirrors its strong PRT performance. NN achieved a score of 0.29, which, although significantly worse than DC, is a marked improvement over the remaining models. All in all, the empirical analysis in this work provides strong evidence towards the hypothesis that the transferability of a concept depends on its coherence, as measured by the retention of consistency across domains.\\n\\nLimitations and future work:\\n\\nFirstly, this work only considered binary relations, and in particular unary relations, such as digit classification, are not considered. Additionally, we have only considered a fixed signature which is learned \\\"all at once\\\" in a source domain. In practical applications, however, it is quite possible that ordinality would be discovered gradually, either through incremental learning of the relations that form a 'complete' signature for ordinality, or through gradual refinement of pre-learned relations after being progressively exposed to different contexts in which ordinality applies. This necessitates a continual learning procedure which has not been explored in this work and would be useful as an addition in future work. Further, even in a single domain, ordinality can be applied to multiple properties (e.g. for BlockStacks we have: block stack height, block size, position of stacks, etc.) and future work can explore our framework's ability to initialize multiple instances of our signature, each applied to a different ordinal property. Lastly, we have only explored a signature for ordinality, whereas other fundamental properties are easy to find, such as periodic (e.g. rotation) and unordered categorical (e.g. shape) properties. These aspects are not explored in this work and would certainly be interesting for future work.\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The authors declare that this work does not include any of the following: involvement of human subjects, sensitive data, harmful insights, methodologies and applications. The results, data sets and methodologies are objectively nondiscriminatory, unbiased and fair. This work does not breach any privacy or security guidelines or laws, nor any other legal restrictions. The authors declare that there are no conflicts of interest and/or external motivations through sponsorship.\\n\\nTo ensure reproducibility of this work, the experimental code has been open sourced, together with the proposed BlockStacks data set and rendering code; references for all other employed data sets are provided in the main text. Hyperparameter configurations are specified in both Section 6 and Appendix D and the experimental procedure is detailed in Section 6.\\n\\nReferences\\nRalph Abboud, \u02d9Ismail \u02d9Ilkan Ceylan, Thomas Lukasiewicz, and Tommaso Salvatori. Boxe: A box embedding model for knowledge base completion. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. URL https://proceedings.neurips.cc/paper/2020/hash/6dbbe6abe5f14af882ff977fc3f35501-Abstract.html.\\n\\nMasataro Asai. Photo-Realistic Blocksworld Dataset. arXiv preprint arXiv:1812.01818, 2018.\\n\\nSamy Badreddine, Artur d'Avila Garcez, Luciano Serafini, and Michael Spranger. Logic tensor networks. CoRR, abs/2012.13635, 2020. URL https://arxiv.org/abs/2012.13635.\\n\\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828, 2013. ISSN 01628828. doi: 10.1109/TPAMI.2013.50.\\n\\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating Embeddings for Modeling Multi-relational Data. In C J C Burges, L Bottou, M Welling, Z Ghahramani, and K Q Weinberger (eds.), Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems, pp. 2787\u20132795. Curran Associates, Inc., Lake Tahoe, USA, 2013.\\n\\nChristopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in $\\\\beta$-VAE. In Advances in Neural Information Processing Systems 30, number Nips, Long Beach, CA, USA, 2017. URL http://arxiv.org/abs/1804.03599.\\n\\nJunxiang Chen and Kayhan Batmanghelich. Robust ordinal V AE: employing noisy pairwise comparisons for disentanglement. CoRR, abs/1910.05898, 2019. URL http://arxiv.org/abs/1910.05898.\\n\\nJunxiang Chen and Kayhan Batmanghelich. Weakly Supervised Disentanglement by Pairwise Similarities. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, AAAI, New York, NY, USA, 2020.\\n\\nRicky T Q Chen, Xuechen Li, Roger B. Grosse, and David Duvenaud. Isolating Sources of Disentanglement in Variational Autoencoders. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems, pp. 2615\u20142625, Montreal, Quebec, Canada, 2018.\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022.\\n\\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 2172\u20132180, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html.\\n\\nYuanfei Dai, Shiping Wang, Neal N Xiong, and Wenzhong Guo. A Survey on Knowledge Graph Embedding: Approaches, Applications and Benchmarks. Electronics, 9(5):1\u201329, 2020. ISSN 20799292. doi: 10.3390/electronics9050750.\\n\\nArtur d\u2019Avila Garcez and Lu\u00eds C. Lamb. Neurosymbolic AI: the 3rd wave. CoRR, abs/2012.05876, 2020. URL https://arxiv.org/abs/2012.05876.\\n\\nIvan Donadello, Luciano Serafini, and Artur d\u2019Avila Garcez. Logic Tensor Networks for Semantic Image Interpretation. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, pp. 1596\u20141602, 2017.\\n\\nCian Eastwood and Christopher K I Williams. A framework for the quantitative evaluation of disentangled representations. In 6th International Conference on Learning Representations, {ICLR}, Vancouver, BC, Canada, 2018.\\n\\nKlaus Greff, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. On the binding problem in artificial neural networks. CoRR, abs/2012.05208, 2020. URL https://arxiv.org/abs/2012.05208.\\n\\nV\u00edctor Guti\u00e9rrez-Basulto and Steven Schockaert. From Knowledge Graph Embedding to Ontology Embedding? An Analysis of the Compatibility between Vector Space Representations and Rules. 2018. doi: 1805.10461. URL http://arxiv.org/abs/1805.10461.\\n\\nIrina Higgins, Lo\u00efc Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. In 5th International Conference on Learning Representations, {ICLR}, Toulon, France, 2017.\\n\\nIrina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and Alexander Lerchner. Towards a Definition of Disentangled Representations. arXiv preprint arXiv:1812.02230, 2018. doi: arXiv:1812.02230v1. URL http://arxiv.org/abs/1812.02230.\\n\\nB. Inhelder and J. Piaget. The early growth of logic in the child: classification and seriation. Routledge and Kegan Paul, London, 1964.\\n\\nTheofanis Karaletsos, Serge Belongie, and Gunnar R\u00e4tsch. When crowds hold privileges: Bayesian unsupervised representation learning with oracle constraints. In 4th International Conference on Learning Representations, {ICLR}, pp. 1\u201316, San Juan, Puerto Rico, 2016.\\n\\nSeyed Mehran Kazemi and David Poole. Simple embedding for link prediction in knowledge graphs. Advances in Neural Information Processing Systems, 2018-December(Nips):4284\u20134295, 2018. ISSN 10495258.\\n\\nDiederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In Proceedings of the 2nd International Conference on Learning Representations, Banff, Alberta, Canada, 2014. ISBN 1312.6114v10. doi: 10.1051/0004-6361/201527329.\\n\\nAbhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentangled latent concepts from unlabeled observations. In 6th International Conference on Learning Representations, {ICLR}, Vancouver, BC, Canada, 2018.\\n\\nBrenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building Machines That Learn and Think Like People. Behavioral and Brain Sciences, 40, 2017. ISSN 14691825. doi: 10.1017/S0140525X16001837.\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Rx_nbGdtRQD", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Specification of our $\\\\beta$-VAE encoder and decoder model parameters, for both $28 \\\\times 28$ (top) and $128 \\\\times 128$ (bottom) size input data. I: Input channels, O: Output channels, K: Kernel size, S: Stride, P: Padding, A: Activation\\n\\nEncoder\\n\\n| Layer ID | I       | O       | K       | S       | P       | A       |\\n|----------|---------|---------|---------|---------|---------|---------|\\n| Conv2d  1 | N       | 32      | 4       | 2       | 1       | ReLU    |\\n| Conv2d  2 | 32      | 32      | 4       | 2       | 1       | ReLU    |\\n| Conv2d  3 | 32      | 64      | 3       | 2       | 1       | ReLU    |\\n| Conv2d  4 | 64      | 64      | 2       | 2       | 1       | ReLU    |\\n\\nDecoder\\n\\n| Layer ID | Num Nodes : In - Out | A       |\\n|----------|----------------------|---------|\\n| FC       z       | 576 - 144 | ReLU    |\\n| FC       z     mu | 144 - 10 | None    |\\n| FC       z   logvar | 144 - 10 | None    |\\n\\nEncoder\\n\\n| Layer ID | I       | O       | K       | S       | P       | A       |\\n|----------|---------|---------|---------|---------|---------|---------|\\n| Conv2d  1 | N       | 32      | 4       | 2       | 1       | ReLU    |\\n| Conv2d  2 | 32      | 32      | 4       | 2       | 1       | ReLU    |\\n| Conv2d  3 | 32      | 64      | 3       | 2       | 1       | ReLU    |\\n| Conv2d  4 | 64      | 64      | 2       | 2       | 1       | ReLU    |\\n| Conv2d  5 | 64      | 64      | 4       | 2       | 1       | ReLU    |\\n\\nDecoder\\n\\n| Layer ID | Num Nodes : In - Out | A       |\\n|----------|----------------------|---------|\\n| FC       z       | 1024 - 256 | ReLU    |\\n| FC       z     mu | 256 - 10 | None    |\\n| FC       z   logvar | 256 - 10 | None    |\\n\\nvia a certain conformity in their gradients of input against output. For instance, suppose we have $x_i, x_j \\\\in X$ such that $\\\\phi_G(x_i, x_j) \\\\approx 0$ and $\\\\phi_E(x_i, x_j) \\\\approx 1$. If we then take $x_i, x_k \\\\in X$ such that $\\\\phi_G(x_i, x_j) \\\\approx 1$, then we must have $\\\\phi_E(x_i, x_j) \\\\approx 0$. In fact, any time $\\\\phi_G$ outputs close 1, we require $\\\\phi_L$ and $\\\\phi_E$ output close to 0, since only one of these three relations can be true for any common arguments. At the decision boundaries, we require that their collective outputs conform such that they are each modified appropriately, ensuring that they continue to satisfy any applicable constraints. We therefore compute a gradient based analysis for arbitrary relation-decoder inputs against the overall $T$ belief state, which should be consistently $\\\\approx 1$, to see how much this varies as $\\\\beta$ is increased. Gradient-conformity (GC) is calculated as:\\n\\n$GC = \\\\frac{|d_T(i, j)|}{|d_i||d_j|}$\\n\\nwhere $d_i = d\\\\phi_r i d_z c |z c = z cn$ and $d_j = d\\\\phi_r j d_z c |z c = z cn$, $\\\\forall i \\\\neq j$ (15)\\n\\nwhere $z cn$ is the concatenation of the $n$th sample of $z_i$ and $z_j$ from the latent space (and, more specifically, a particular data split). Figure 6 presents source domain referenced GC measures for each model, with the same data split schematic as in Figure 3. We see that for DC, GC is close to 1 for all $\\\\beta$ with no discernible change. All other models show a weaker GC with positive correlation between GC and $\\\\beta$. TransR and NN achieve significantly higher GC than NTN and HolE. It appears that models that achieve a GC greater than 0.5 perform better at the overall PRT learning task.\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: GC values (higher values better), for each relation-decoder model referenced to source domain. Darker color shades denote higher values of $\\\\beta$, corresponding to greater disentanglement pressure from the $\\\\beta$-VAE. Blue, green and red groups show results for data-embeddings, interpolation and extrapolation embeddings respectively (see main text for further details regarding the data splits).\\n\\nFigure 7: Analysis of domain-specific information retention by the $\\\\beta$-VAE when using different relation-decoders for ordinality relation decoding. We attempt to predict the overall BlockStacks stack height on the final fixed embeddings obtained after $\\\\text{isSuccessor}$ relation-decoder alignment.\\n\\nHow does each model impact the retention of domain-dependent information?\\n\\nFigure 7 shows results for BlockStacks overall block height prediction accuracy when training on fixed encodings of each block stack, after $\\\\text{isSuccessor}$ relation-decoder alignment as been applied (using a pretrained fixed-parameter relation-decoder). Note $\\\\beta$ is fixed in the target domain, so the only moving part are the choices of pretrained models which have been previously trained with varied source $\\\\beta$ values. Note also that DC has an unfair advantage here, as the steered fitting approach allows more flexibility to the VAE learning phase - for this reason the result is only included in the appendix. Since we are interested in capturing general representations that encode both domain-dependent and domain-independent information, we use each target encoder $\\\\psi_t$ obtained from each PRT experiment and produce encodings for the full BlockStacks test set. The resulting encodings are then divided into a new train and test subset, used to train both a Sci-Kit Learn Linear regressor and Support Vector Machine regressor with a RBF kernel (Pedregosa et al., 2011). We present the resulting Mean Squared Errors (MSE) in Figure 7, with Ordinary Least Squares (OLS) (a) and Support Vector Regression (SVR) (b).\\n\\nThere are a number of noteworthy details: firstly, DC shows no dependence on $\\\\beta$ and leads to a lower MSE across all settings; second, excluding DC, for all models we observe an optimum MSE at $\\\\beta = 8$, with TransR reaching DC MSE performance for OLS and NN doing the same for SVR. These results indicate that lower MSE can be obtained by using non-linear regression, which indicates that to some degree, the block stack height factor is not encoded linearly, regardless of selected model. Next, by contrasting with Figure 6, these results suggest that models with higher GC lead to embeddings that are more amenable to domain-specific factor prediction. However, the parabolic trend, where increasing $\\\\beta$ to 12 leads to an increase in error, is in agreement with Figure 2-bottom-right, which showed that most models do not improve at PRT for the largest $\\\\beta$.\\n\\nThis is perhaps due to a loss of mutual information between input and latent representation, as the distribution matching loss outweighs reconstruction in the $\\\\mathcal{L}_{\\\\text{VAE}}$.\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To support our claim that we can use only the isSuccessor relation as the target encoder guide due to its logical relationship the remaining relations, we include here the logical clauses:\\n\\n$$\\\\forall i,j,k (\\\\text{isSuccessor}(i, j) \\\\land \\\\text{isSuccessor}(k, j) \\\\rightarrow \\\\text{isEqual}(i, k))$$\\n\\n$$\\\\forall i,j (\\\\text{isSuccessor}(i, j) \\\\rightarrow \\\\text{isGreater}(i, j))$$\\n\\n$$\\\\forall i,j,k (\\\\text{isSuccessor}(i, j) \\\\land \\\\text{isGreater}(j, k) \\\\rightarrow \\\\text{isGreater}(i, k))$$\\n\\n$$\\\\forall i,j (\\\\text{isSuccessor}(i, j) \\\\leftrightarrow \\\\text{isPredecessor}(j, i))$$\\n\\n$$\\\\forall i,j (\\\\text{isPredecessor}(i, j) \\\\rightarrow \\\\text{isLess}(i, j))$$\\n\\n$$\\\\forall i,j,k (\\\\text{isPredecessor}(i, j) \\\\land \\\\text{isLess}(j, k) \\\\rightarrow \\\\text{isLess}(i, k))$$\\n\\nTherefore, by knowing all of the successor relations between data instances, it should be possible to infer the remaining relationships that they share.\\n\\nFor completeness, we provide the truth tables for each of the sub-theories that our consistency losses evaluate against. We only include configurations that are valid under the constraints, indicated by $\\\\subset T = T$, where this notation highlights the fact each incomplete set of constraints form a subset of the overall theory $T$.\\n\\nFirstly, the truth-table that describes constraints shared between relation truth-values is given by the following,\\n\\n$\\\\begin{array}{cccc}\\nG(i,j) & E(i,j) & L(i,j) & S(i,j) \\\\\\\\\\nT & F & F & F \\\\\\\\\\nT & F & F & T \\\\\\\\\\nT & F & T & F \\\\\\\\\\nT & F & T & T \\\\\\\\\\nT & T & F & T \\\\\\\\\\nT & T & T & T \\\\\\\\\\n\\\\end{array}$\\n\\nwhere we use the same relation abbreviations as in the main text results.\\n\\nNext, we provide each of the three consistency individual (Con-I) truth-tables. These are referred to as being \\\"individual\\\" due to the fact that they describe constraints applied to the truth-state of a single relation. For transitivity, given by the rule e.g. $\\\\text{G}(i,j) \\\\land \\\\text{G}(j,k) \\\\rightarrow \\\\text{G}(i,k)$, we have that\\n\\n$\\\\begin{array}{c}\\nG(i,j) \\\\land G(j,k) \\\\land G(i,k) \\\\\\\\\\nT \\\\quad F \\\\quad F \\\\\\\\\\nT \\\\quad F \\\\quad T \\\\\\\\\\nT \\\\quad T \\\\quad F \\\\\\\\\\nT \\\\quad T \\\\quad T \\\\\\\\\\n\\\\end{array}$\\n\\nFor asymmetry, where $\\\\text{S}(i,j) \\\\rightarrow \\\\neg \\\\text{S}(j,i)$, we have\\n\\n$\\\\begin{array}{c}\\nS(i,j) \\\\land S(j,i) \\\\\\\\\\nT \\\\quad F \\\\\\\\\\nT \\\\quad T \\\\\\\\\\n\\\\end{array}$\\n\\nFinally, for reflexivity, given by $\\\\text{E}(i,i) \\\\rightarrow \\\\top$ (in this case describing that an object is always equal to itself) we have\\n\\n$\\\\begin{array}{c}\\nE(i,i) \\\\\\\\\\nT \\\\\\\\\\n\\\\end{array}$\\n\\nTruth-table matrices for each of the above truth-tables can be obtained by replacing $T$ with 1 and $F$ with 0. We provide the full set of individual constraints that are applicable to each relation covered in this paper are given by Table 3.\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Characteristic properties of ordinal relations.\\n\\n| Relation | Asymmetric | Transitive | Reflexive |\\n|----------|------------|------------|-----------|\\n| G        | Y          | Y          | N         |\\n| E        | N          | Y          | Y         |\\n| L        | Y          | Y          | N         |\\n| S        | Y          | N          | N         |\\n| P        | Y          | N          | N         |\\n\\nIn this section, we present the expanded justification for reporting $-\\\\ln \\\\bar{\\\\epsilon}$ consistency and coherence as a proxy for $\\\\epsilon$-consistency/coherence as defined in Section 3. For notational clarity, in the following we omit $\\\\psi_S$, such that $\\\\phi_r(\\\\psi_S(O))$ is abbreviated to $\\\\phi_r(O)$.\\n\\nIn the following, we make no assumptions about the sizes of domain $S$, signature $\\\\sigma$ and arities of each $r \\\\in \\\\sigma$. Further, we take $T$ to be an arbitrary theory over $\\\\sigma$ consisting of universally quantified formula, and the validity of each ground instances of atomic formula with respect to $T$, can be expressed by a single ground truth-table matrix, $T \\\\in \\\\{0, 1\\\\}^{K_0 \\\\times K_1 \\\\times K_2}$, wherein each slice, $T_k$, gives a unique grounding of domain objects to the variables, $v$, required by $T$. For each grounding of the $K_0 = |S| |v|$ possible groundings, there are $K_1 = 2^l$ unique truth-assignments to the $l$ atomic formulae that constitute $T$, giving $K_2 = l + 1$ assignments per $T_k$, row - one per atomic formulae and an additional value to denotes whether the particular row satisfies $T$. $T$ can be obtained by taking any truth-table from the previous section and switching true (T) for 1 and false (F) for 0, and producing $K_0$ copies for each assignment of domain elements to the variables. Given this truth-table matrix, notice that a structure $S_\\\\sigma$ can be composed by selecting a single row of $T$ for each grounding ($k$th slice), giving a vector $c_{kt} = T_{k,t}$.\\n\\nIf the structure is a model of $T$, i.e. $S_\\\\sigma \\\\in M_T$ then only rows with $T_{k,t,K_2} = 1$ are allowed. Taking $t^+$ to be the set of rows such that $T_{k,t,K_2} = 1$ (which is identical for each $k$) for $t \\\\in t^+$, we can then rewrite $\\\\Gamma_{\\\\tilde{S}_\\\\sigma T}$ in terms of samples from $T$:\\n\\n$$\\\\Gamma_{\\\\tilde{S}_\\\\sigma T} = \\\\sum_{S_\\\\sigma \\\\in M_T} \\\\prod_{r \\\\in \\\\sigma} \\\\prod_{O \\\\in S} \\\\phi_r(O) \\\\gamma_{r O, S_\\\\sigma} (1 - \\\\phi_r(O)) 1 - \\\\gamma_{r O, S_\\\\sigma}$$\\n\\n(Eqn. 3)\\n\\nwith $f(\\\\phi_{rm} O_{km}, c_{ktm}) = \\\\phi_{rm} (O_{km}) c_{ktm} (1 - \\\\phi_{rm} (O_{km})) 1 - c_{ktm}$.\\n\\nIn the above, $1_{t^+} \\\\Gamma_{\\\\tilde{S}_\\\\sigma T}$ is an indicator function which equals 1 if $t^+ \\\\Gamma_{\\\\tilde{S}_\\\\sigma T}$ and 0 otherwise, for active row $t^+ \\\\Gamma_{\\\\tilde{S}_\\\\sigma T}$ under structure $S_\\\\sigma$ and grounding $k$. $1_{t^+} \\\\Gamma_{\\\\tilde{S}_\\\\sigma T}$ has the role of only including the single summand where $t$ corresponds with $t^+ \\\\Gamma_{\\\\tilde{S}_\\\\sigma T}$.\\n\\n$N(\\\\phi_{rm} O_{km}, c_{ktm}, S_\\\\sigma)$ is a function that counts the number of repeat products of term $f(\\\\phi_{rm} O_{km}, c_{ktm})$, such that the appropriate root can be applied. We use $r_m$ to denote the relation for atomic formula at column $m$ and its corresponding arguments, under grounding $k$; and we use $c_{ktm}$ to denote the truth-assignment of the atomic formula for column $m$, as designated by row $t$.\\n\\nAt this point, we are left with an expression for $\\\\Gamma_{\\\\tilde{S}_\\\\sigma T}$ in terms of truth-table matrix $T$ entries, which is more reminiscent of $L(T, \\\\tilde{S}_\\\\sigma)$ as defined in Section 4. However, we must go further to expose the relationship between $\\\\Gamma_{\\\\tilde{S}_\\\\sigma T}$ and $L(T, \\\\tilde{S}_\\\\sigma)$ for arbitrary $T$ expressed by $T$. We will now show that the consistency loss $L(T, \\\\tilde{S}_\\\\sigma)$ gives the negative log-likelihood of satisfying $T$ given a grounding $k \\\\in \\\\{1, \\\\ldots, K_0\\\\}$, which can be further seen as a relaxation of $\\\\Gamma_{\\\\tilde{S}_\\\\sigma T}$ to sum over all rows and without normalising via the $N(\\\\phi_{rm} O_{km}, c_{ktm}, S_\\\\sigma)$ exponent. With Boolean random variable $B_T$ denoting whether $T$ is ($b_T = 1$) or is not ($b_T = 0$) satisfied, the consistency loss for a soft-structure $\\\\tilde{S}_\\\\sigma$ against theory $T$ is given by:\\n\\n$$L(T, \\\\tilde{S}_\\\\sigma) = E_{k \\\\sim U[\\\\{1, \\\\ldots, K_0\\\\}]} [H(p(B_T|S_\\\\sigma,k), p(B_T|\\\\tilde{S}_\\\\sigma,k))]$$\\n\\n(Eqn. 7 base 20)\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Network architecture used for PRT task, with $\\\\hat{\\\\gamma} = \\\\phi_r(\\\\psi_{\\\\text{enc}} S(O))$. The figure shows how relational learning is performed on the source MNIST data set (to learn e.g. that digit 5 is greater than 3). As discussed in Section 6, moving to the target domain (to learn that a stack of blocks is greater than another) involves training a new $\\\\psi_{\\\\text{enc/dec}} Y$ together with a subset of the $\\\\phi_r$ relation-decoders (with fixed parameters), where the rest are held out as zero-shot transferred relation-decoders.\\n\\nLet $k \\\\in \\\\{1, \\\\ldots, K_0\\\\}$ be the index associated with each unique grounding of domain elements to the variables of $T$. Further, take $B_T$ to be a Boolean random variable denoting the truth-value of $T$, so that the probability of the theory being either satisfied ($b_T = 1$) or violated ($b_T = 0$) across ground instances, under a soft-structure $\\\\tilde{T}$ can be expressed as $p(b_T | \\\\tilde{T}, k)$. Notably, by the definition of a theory, ground instances of formulae will always hold as true for any model of $T$, i.e. $p(b_T = 1 | S_{\\\\sigma}, k) = 1$ if $S_{\\\\sigma} \\\\in M_T$. Similarly, when $\\\\tilde{T}$ is consistent with $T$ then we should also find $p(b_T = 1 | \\\\tilde{T}, k) \\\\approx 1$. We thus define our consistency loss as an expectation of the binary cross entropy between $p(B_T | S_{\\\\sigma}, k)$ and $p(B_T | \\\\tilde{T}, k)$, which, given $p(b_T = 0 | S_{\\\\sigma}, k) = 0$ for any $k$ grounding, simplifies to the expected negative log-likelihood of satisfying $T$ under a randomly sampled grounding,\\n\\n$$L(T, \\\\tilde{T}) = \\\\mathbb{E}_{k \\\\sim p(k)} \\\\left[ -\\\\ln p(b_T = 1 | \\\\tilde{T}, k) \\\\right].$$\\n\\n(7)\\n\\nwhere $p(k) = 1/K_0$ is taken to be uniform over the possible unique groundings. However, we still require an $\\\\epsilon$-proxy measure based on this loss, to enable practical evaluation of concept coherence. To achieve this, we define $\\\\bar{\\\\Gamma}_{\\\\tilde{T}} = \\\\exp(-L(T, \\\\tilde{T}))$ and use its relationship with $\\\\Gamma_{\\\\tilde{T}}$ to define a proxy bound,\\n\\n$$\\\\ln \\\\frac{1}{1 - \\\\bar{\\\\epsilon}} \\\\geq L(T, \\\\tilde{T})$$\\n\\n(8)\\n\\nwhere $\\\\bar{\\\\epsilon} \\\\geq 1 - \\\\bar{\\\\Gamma}_{\\\\tilde{T}}$. In our results, we take $\\\\epsilon$-proxy coherence to be the uppermost bound of $\\\\ln \\\\frac{1}{1 - \\\\bar{\\\\epsilon}}$ between source and target domains.\\n\\n5 Neural Model\\n\\nThe critical components of a soft-structure, $\\\\tilde{T}_{\\\\sigma}$, are its domain-encoder $\\\\psi_{\\\\text{S}}$ and modular relation-decoders $\\\\phi_r$. Together these form an autoencoding architecture which, given a domain of images $S \\\\subset \\\\mathbb{R}^{W \\\\times H}$ and with $d$-dimensional latent space $Z = \\\\mathbb{R}^d$, converts sub-symbolic encodings from $\\\\psi_{\\\\text{S}}$ into a modular relational representation via decodings for each $\\\\phi_r$, $r \\\\in \\\\sigma$. Additionally, to retain information in $Z$ pertaining to $S$ which is beyond the requirements of $\\\\phi_r$, we include an additional domain-decoder, which produces domain reconstructions $\\\\hat{S}$. The overall neural model is depicted by Figure 1, where we use $\\\\psi_{\\\\text{enc}}$ to refer to the domain-encoder and $\\\\psi_{\\\\text{dec}}$ for the domain-decoder. To train the neural model, we assume a ground truth interpretation $I_S$ is given, allowing us to directly maximise Eqn. 3 via negative log-likelihood loss:\\n\\n$$L(\\\\tilde{T}) = -\\\\log p(S_{\\\\sigma} | \\\\tilde{T}).$$\\n\\n(9)\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To obtain informative latent representations for $S$, we use a Variational AutoEncoder (VAE), specifically the $\\\\beta$-VAE, given its simplicity and demonstrated ability to separate distinct factors in the latent representation (Higgins et al., 2017; Burgess et al., 2017; Kingma & Welling, 2014). The $\\\\beta$-VAE achieves this by optimising the ELBO objective, $L_{\\\\text{ELBO}}^{\\\\beta}$-VAE, but with an additional $\\\\beta$ scalar hyperparameter that can be thought of as a disentanglement pressure, forcing distinct explanatory factors to align with different axes of the latent space. We provide the full ELBO loss, with a detailed explanation, in Appendix C. We combine losses over each model component to give the following aggregate objective:\\n\\n$$L_{\\\\text{joint}} = L_{\\\\text{ELBO}}^{\\\\beta}$-VAE - \\\\lambda L_{\\\\tilde{S}}\\\\sigma$$\\n\\nwhere $\\\\lambda$ is a scalar weighting parameter. Together with the $L_{\\\\text{ELBO}}^{\\\\beta}$-VAE, the choice of relation-decoder model will shape how the domain-encodings are structured (Guti\u00e9rrez-Basulto & Schockaert, 2018).\\n\\nOur evaluation considers a selection of relation-decoder models that cover a range of representational flexibility. Amongst these is our proposed low-complexity, but nonetheless expressive, Dynamic Comparator (DC) model. The overall DC model is composed of two modes, a distance-based measure, $\\\\phi^\\\\dagger_r$, that measures the distance between two inputs relative to a reference point, and a step-like function, $\\\\phi^\\\\ddagger_r$, that determines the sign of the difference between two points, optionally with an offset. Although we can use any functions that have the required characteristics for $\\\\phi^\\\\dagger_r$ and $\\\\phi^\\\\ddagger_r$, in this paper we use the following implementation:\\n\\n$$\\\\phi_{DC}^r(z_i, z_j) = a_r^0 \\\\cdot \\\\phi^\\\\dagger_r + a_r^1 \\\\cdot \\\\phi^\\\\ddagger_r$$\\n\\nwhere\\n\\n$$\\\\phi^\\\\dagger_r = f^0_0(-\\\\eta^0_r, 0)^\\\\top \\\\frac{u^\\\\top r}{} \\\\left(\\\\frac{z_i - z_j + b^\\\\dagger_r}{\\\\|z_i - z_j + b^\\\\dagger_r\\\\|_2}\\\\right)$$\\n\\nand\\n\\n$$\\\\phi^\\\\ddagger_r = f^1_1(\\\\eta^1_r, 1)^\\\\top u^\\\\top r \\\\left(\\\\frac{z_i - z_j + b^\\\\ddagger_r}{b^\\\\ddagger_r}\\\\right)$$\\n\\nHere $a_r = \\\\text{Softmax}(A_r) \\\\in (0, 1)^2$ is an attention weighting between the two modes, $f^0_0$ and $f^1_1$ are an exponential and sigmoid function, respectively; $u^r = \\\\text{Softmax}(U^r) \\\\in (0, 1)^m$ is an attention mask which is applied to $m$-dimensional embeddings; $b^\\\\dagger_r$, $b^\\\\ddagger_r \\\\in \\\\mathbb{R}^m$ are learnable bias terms that enables an offset to each mode; and $\\\\eta^0_r, \\\\eta^1_r \\\\in \\\\mathbb{R}^+$ are non-negative and any-valued scalar terms, respectively. Lastly, $\\\\odot$ denotes the Hadamard product and $\\\\|\\\\cdot\\\\|_2$ is the $L_2$-norm. The key innovation behind DC is its ability to model each of the ordinal relations whilst encouraging generalised consistency across the full latent subspace, as defined by each $u^r$. This is achieved without explicit weight sharing, wherein relation-decoders discover parametric relationships between relations from the data. We provide an additional depiction and analysis of DC in Appendix D.1, which further illustrates these effects.\\n\\n6 EXPERIMENTAL DESIGN\\n\\nIn this section we describe an experimental design used to compare soft-structure coherence when choosing different relation-decoder implementations.\\n\\nPartial Relation Transfer (PRT): The evaluation involves a proposed PRT task across two soft-structures $\\\\tilde{X}_\\\\sigma$ and $\\\\tilde{Y}_\\\\sigma$. Each shares a common signature $\\\\sigma$ and relation-decoders $\\\\phi$ but have disjoint domains $X$ and $Y$, respectively. The experimental procedure involves first learning $\\\\phi$ on source domain $X$, together with its domain-specific autoencoder. In the second phase, we train a new domain-specific autoencoder on the target domain, $Y$, alongside a selection of the now learned $\\\\phi$ relation-decoders but with fixed-parameters. The selected relation-decoders act as training guides for $\\\\psi_{enc}^Y$, whilst held-out relation-decoders can be evaluated against zero-shot transfer performance.\\n\\nFor domain $X$ we employ the MNIST handwritten digits data set (LeCun & Cortes, 2010), and for domain $Y$ we use a proposed BlockStacks data set, which includes singular multi-colored cube stacks of differing height, each containing one randomly positioned red cube (see Appendix B for further details and examples). The shared signature includes the ordinal relations, i.e. $\\\\sigma = \\\\{G, E, L, S, P\\\\}$, and is applied to digit ordering in MNIST and red cube position ordering in BlockStacks.\\n\\nWe provide results against a theory of ordinality, as explored in Example 1 - we provide a formal specification of this theory in Appendix G. When guiding $\\\\psi_{enc}^Y$ to perform a similar mapping to $\\\\psi_{enc}^X$, i.e. from domain to a similar ordinal subspace as defined by $\\\\phi$, we could use the full $\\\\phi$ set of relation-decoders. However, this is not necessary from a logical standpoint, as our system of relations can all be expressed in terms of $\\\\text{isSuccessor}$. We therefore only employ the $\\\\text{isSuccessor}$ relation-decoder as a fixed-parameter guide for $\\\\psi_{enc}^Y$. \\n\\nNeural model components: Together with DC, existing relation-decoder models compared here are: TransR (Lin et al., 2015), HolE (Nickel et al., 2016b), NTN (Socher et al., 2013). We addition-\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: (Top) Relation-decoder prediction accuracy per relation and model, in the source (left) and target domains. Relations are abbreviated on the x-axis by \\\\{S: isSuccessor, P: isPredecessor, E: isEqual, G: isGreater, L: isLess\\\\}, with a red highlight identifying a relation included as a guide for $\\\\psi_{enc_Y}$. (Bottom) Impact of different values of $\\\\beta$ for each relation-decoder (mean across all relations in the source domain (left) and mean for held-out relations only in the target domain (right). Notably, it can be seen that our model (DC) is not impacted while all other models show a decrease of accuracy in the target domain.\\n\\nTo produce domain-encodings, all experiments use a $\\\\beta$-VAE. We provide further details for all models in Appendix D. Due to a convergence issue when using a pretrained DC with fixed parameters, a flexible fitting procedure was necessary, in which we enable the DC parameters to train in the target domain, but with the additional loss term $\\\\|\\\\rho^* - \\\\rho\\\\|$, between pretrained $\\\\rho^*$ and untrained parameters $\\\\rho$, respectively. In all cases we evaluated the final parameter values in the target domain and found them to be approximately equivalent to the $\\\\rho^*$. We did not apply this method to the other models as they were all able to fit the.isSuccessor relation in the target domain.\\n\\nHyperparameters: In the source domain we explore $\\\\beta$ values between \\\\{1, 4, 8, 12\\\\}, and set $\\\\lambda = 10^{-3}$ and in the target domain we first normalise losses (see Appendix D.4) and set $\\\\beta = 10^{-4}$ and $\\\\lambda = 10^{-2}$ as these produced good reconstructions whilst also ensuring optimisation against $L_{\\\\tilde{Y}_\\\\sigma}$. In all experiments, we fix $Z = R_10$.7 K\\n\\n**RESULTS**\\n\\nIn this section, Figure 2 firstly shows standard PRT prediction accuracies per relation in both the source and target domain. Figure 3 then presents consistency losses for three color-coded data splits: data-embeddings (blue), where all inputs are encodings of a domain's test data; interpolation (green), where we obtain an empirical mean and variance for the domain's data-embeddings and sample from a corresponding Gaussian distribution; and extrapolation (red), where we sample from regions strictly outside the smallest, axis aligned hyper-rectangle that encloses all data-points. Finally, Table 1 concludes with a clear $\\\\epsilon$-proxy coherence comparison between relation-decoders.\\n\\nRelation-decoder PRT accuracy performance: Figure 2-top provides relation-decoder prediction accuracy in both the source MNIST (left), and target BlockStacks (right), domains. Key observations are that DC produces excellent PRT performance, whilst NN, NTN and HolE all see some degradation from their source accuracies on relations other than isSuccessor. TransR seems to maintain an target accuracy profile similar to its performance in the source domain, but this is significantly below the performance of other models in the source domain. We include $\\\\beta$'s impact on these performances in Figure 2-bottom. Barring DC which has little discernible change in either domain, PRT performance is significantly impacted by $\\\\beta$ in all models, but has little effect in the source domain. TransR shows a strong positive correlation between target domain accuracy and $\\\\beta$, whereas the remaining models produce their best PRT performances with intermediate disentanglement pressure.\\n\\n---\\n\\n2 We take $\\\\phi_r$ prediction values above 0.5 to signify a truth prediction and those below 0.5 to signify falsity.\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nFigure 3: Modified\\n\\nCon-A values for each relation-decoder model, referenced to source (left) and target (right) domains (lower values better).\\n\\nCon-I values (lower values better) for each relation-decoder model referenced to source (left) and target (right) domains, where stacked bars are for formula: transitivity (white), asymmetry (magenta) and reflexivity (black). In all plots, darker color shades denote higher values of $\\\\beta$, corresponding to greater disentanglement pressure from the $\\\\beta$-VAE. In top-left and bottom plots, blue, green and red groups show results for data-embeddings, interpolation and extrapolation embeddings respectively (see main text for details).\\n\\nConsistency Across (Con-A):\\nTo interrogate further how $\\\\beta$ affects each model, Figure 3-top presents consistency losses against formulae that constrains truth value assignments across relations, under a theory of ordinality, referred to as Con-A. Results are referenced to both source (left) and target (right) domain embeddings. With reference to the source domain, we note that DC shows excellent Con-A in all regions. Most other models have worse interpolation and extrapolation consistency. Increasing $\\\\beta$ appears to improve interpolation and extrapolation performance for models NN, NTN and TransR, but there are indications that this trend does not persist into the largest $\\\\beta = 12$ value. On the other hand, HolE shows a negative correlation between $\\\\beta$ and Con-A performance, across all data-splits. Although DC sustains impressive Con-A results for target domain data-embeddings (right), results for all other models are notably worse with respect to their source data-embeddings performances and are instead comparable with their interpolation or extrapolation results in the source domain. It may therefore be possible to anticipate/diagnose poor transfer performance by evaluating interpolation and extrapolation consistency, which suggests that DC's strong consistency generalisation indeed enables it to learn a transferable concept.\\n\\nConsistency Individual (Con-I):\\nFigure 3-bottom presents stacked consistency losses, for formula: transitivity (white), asymmetry (magenta) and reflexivity (black); results are averaged over individual relations and are together grouped under label Con-I, given that they refer to constraints on individual relations. Losses are again partitioned between source domain (left) and target domain (right). We firstly observe that DC and NN share the best overall Con-I performance profiles, with TransR following closely. DC and TransR both show comparable data-embedding versus interpolation/extrapolation performance, whereas NN, NTN and HolE suffer from degradation across these splits. Interestingly, these results show that: DC only suffers on transitivity, NN and TransR mainly struggle to model transitivity but show additional loss for asymmetry and HolE demonstrates difficulty in modelling each of the Con-I sub-stack. With regards to $\\\\beta$'s impact, it is not possible to determine a correlation for DC. However, NN and NTN demonstrate a negative correlation of $\\\\beta$ against overall Con-I, with comparable response for each underlying sub-stack. TransR shows a significant Con-I extrapolation improvement with increased $\\\\beta$ and HolE is for the most part adversely impacted as $\\\\beta$ is increased. Similar trends can be seen for target Con-I performance. However, notably, many models show improvements, in particular in Con-I (asymmetry). This could be due to more precise target domain data-embeddings, as a result of using a single pretrained relation-decoder.\\n\\n$\\\\epsilon$-proxy coherence:\\nTable 1 provides a comparison between optimal $\\\\epsilon$-proxy coherences achieved for each relation-decoder model, as defined in Section 4. Results are partitioned according to each consistency type and an Aggr(egate) value, which gives best summed consistency, together with best $\\\\beta = \\\\beta^*$ values. DC clearly outperforms all other models in $\\\\epsilon$-proxy coherence across all types.\\n\\nNN achieves strong aggregated $\\\\epsilon$-coherence compared with NTN, HolE and TransR outperforming across Con-A and Con-I. Although NTN and HolE have similar aggregate $\\\\epsilon$-proxy coherence,\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ABSTRACT\\n\\nHuman defined concepts are inherently transferable, but it is not clear under what conditions they can be modelled effectively by non-symbolic artificial learners. This paper argues that for a transferable concept to be learned, the system of relations that define it must be coherent across domains. This is to say that the learned concept-specific relations ought to be consistent with respect to a theory that constrains their semantics and that such consistency must extend beyond the representations encountered in the source domain. To demonstrate this, we first present formal definitions for consistency and coherence, and a proposed Dynamic Comparator relation-decoder model designed around these principles. We then perform a proposed Partial Relation Transfer learning task on a novel data set, using a neural-symbolic autoencoder architecture that joins sub-symbolic representations with modular relation-decoders. By comparing against several existing relation-decoder models, our experiments show that relation-decoders which maintain consistency over unobserved regions of representational space retain coherence across domains, whilst achieving better transfer learning performance.\\n\\n1 INTRODUCTION\\n\\nHumans are capable of learning concepts such that they can be applied to many different scenarios (Inhelder & Piaget, 1964; Piaget, 2005; Lake et al., 2017). An important characteristic is that human-like concepts remain coherent across contexts, whereby their logical consistency in one context is retained in another (Nye et al., 2021). As an example, consider the concept of ordinality which permits comparison over ordered sets, e.g., \u201cA is larger than B\u201d, and pertains to a multitude of properties: position, size, volume, reach, etc. So long as one of these properties can be attributed to an object, a set of objects can be compared on that basis; in this sense ordinality generalises between objects. All in all, if the concept of ordinality were to be learned in its most general form, it should be coherent across properties and objects.\\n\\nIn this paper, we seek to define the conditions that allow a learned concept to transfer well across properties and objects in the case of sub-symbolic learners (d\u2019Avila Garcez & Lamb, 2020; Santoro et al., 2021; Greff et al., 2020). We define consistency and coherence of sub-symbolic learners borrowing from analogous definitions from symbolic AI. We propose a neural-symbolic autoencoder architecture consisting of a neural encoder for objects coupled with modular relation-decoders (Serafini & Garcez, 2016; Donadello et al., 2017; Badreddine et al., 2020; Wang et al., 2017; Nickel et al., 2016a; Dai et al., 2020), and we show that this architecture is capable of achieving an improved transfer learning performance by being coherent across properties and objects.\\n\\nWe therefore claim that retaining consistency across domains dictates concept coherence, offering a more fine-grained measure of transfer learning than accuracy alone. The proposed architecture is a practical instantiation of this formalisation and is evaluated in this paper on a new Partial Relation Transfer (PRT) task and data set. We begin by expressing the symbolic application of a set of relations to some domain of interest as a model-theoretic structure, such as A is larger than B, and defining an analogous soft-structure for non-symbolic learners where relations are modelled by relation-decoders that compute beliefs. We then propose formal definitions for consistency and coherence of soft-structures which provide a practical consistency score calculation to the evaluation of autoencoders. Finally, we present a benchmark PRT learning task with the use of a new BlockStacks data set derived from the CLEVR data set rendering agent. We then compare our proposed architecture against several existing relation-decoder models, showing that relation-decoders which maintain consistency over unobserved regions of representational space retain coherence across domains, whilst achieving better transfer learning performance.\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our experiments show that relation-decoders which maintain consistency over unobserved regions of representational space retain coherence across domains whilst achieving better transfer learning performance. The contributions of this paper are:\\n\\n\u2022 A formal definition of coherence and consistency for sub-symbolic learners with a practical evaluation score.\\n\u2022 A neural model and learning task for partial relation transfer including a new data set to evaluate concept coherence.\\n\u2022 A comprehensive critical evaluation of results in comparison with multiple state-of-the-art relation-decoder models.\\n\\nIn Section 2 we provide the required background, Section 3 introduces soft-structures and formally defines coherence and consistency, Section 4 provides a practical consistency loss and Section 5 then outlines our neural-symbolic architecture. After detailing the PRT task in Section 6, we present results in Section 7 and complete the paper in Section 8 with a discussion and concluding remarks, including limitations and future work. We provide related work in Appendix A.\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Example 1\\n\\nSuppose we have the structure\\n\\n\\\\[ S_{\\\\sigma} = (S, I_{S_{\\\\sigma}}) \\\\]\\n\\nwhere \\\\( S \\\\) is a domain of images of handwritten digits and \\\\( \\\\sigma \\\\) the signature of binary relations\\n\\n\\\\[ \\\\sigma = \\\\{ \\\\text{isGreater}, \\\\text{isEqual}, \\\\text{isLess}, \\\\text{isSuccessor}, \\\\text{isPredecessor} \\\\} \\\\]\\n\\nor for short \\\\( \\\\sigma = \\\\{ G, E, L, S, P \\\\} \\\\). Let \\\\( T \\\\) be the theory that defines ordinality including, for instance, the sentence\\n\\n\\\\[ \\\\forall i,j. G(i,j) \\\\rightarrow \\\\neg E(i,j) \\\\] (if a digit is greater than another then they are not equal).\\n\\nAny structure \\\\( S_{\\\\sigma} = (S, I_{S_{\\\\sigma}}) \\\\) with interpretations \\\\( I_{S_{\\\\sigma}} \\\\) of \\\\( \\\\sigma \\\\) that captures a total order over the elements of \\\\( S \\\\) is a model of \\\\( T \\\\).\\n\\n### Approaching Structures That Have Real-World Domains\\n\\nIn this section we turn our attention to the challenge of learning a model over a real-world domain, given a signature and theory. Here a learner must determine an appropriate interpretation over real-world data, such as images or other perceptions. This can be challenging because, firstly, we may only have a partial description of the interpretation, and secondly data may be noisy and contain information that is not relevant to the theory. For instance MNIST, a relatively simple data set by current standards, consists of stylistic details such as line thickness and digit skew (Chen & Batmanghelich, 2020), which are irrelevant to the notion of ordinality, which makes obtaining the structure from Example 1 more complicated. Nevertheless, statistical machine learning models are able to discover commonalities in data which help to infer the underlying semantics (i.e. interpretation) and disregard the noise. Following the convention in disentanglement literature (Bengio et al., 2013; Kingma & Welling, 2014; Higgins et al., 2017; 2018), we take the assumption that real-world observations \\\\( S \\\\) are drawn from some conditional distribution \\\\( p_{S|Z} \\\\), where \\\\( Z \\\\) is a latent random variable, itself drawn from prior \\\\( p_Z \\\\). It is therefore useful to define a domain encoding of the form,\\n\\n\\\\[ \\\\psi_{S}: S \\\\rightarrow Z \\\\] (1)\\n\\ntasked with approximating the conditioned expectation of the posterior, i.e\\n\\n\\\\[ \\\\psi_{S}(s) = \\\\mathbb{E}[p_Z|S(Z|s)] \\\\].\\n\\nSince obtaining an interpretation from domain encodings, for a given signature, may require dealing with noise, we express the interpretation of relations over real-world data by belief functions over the space \\\\( Z \\\\) (Paris & Vencovsk\u00e1, 2015; Paris, 1994) , and refer to these as relation-decoders\\n\\n\\\\[ \\\\phi_r: Z \\\\rightarrow (0,1) \\\\] (2)\\n\\nwith \\\\( \\\\phi_r = \\\\{ \\\\phi_r \\\\}_{r \\\\in \\\\sigma} \\\\). Concretely, for a binary relation \\\\( r \\\\) and ordered pair \\\\( (s_i,s_j) \\\\in S_2 \\\\),\\n\\n\\\\[ \\\\phi_r(\\\\psi_{S}(s_i),\\\\psi_{S}(s_j)) \\\\]\\n\\ndescribes the belief that \\\\( (s_i,s_j) \\\\in I_{S_{\\\\sigma}}(r) \\\\). A belief \\\\( \\\\phi_r(\\\\psi_{S}(s_i),\\\\psi_{S}(s_j)) \\\\approx 1 \\\\) signifies a strong belief that \\\\( (s_i,s_j) \\\\in I_{S_{\\\\sigma}}(r) \\\\) and \\\\( \\\\phi_r(\\\\psi_{S}(s_i),\\\\psi_{S}(s_j)) \\\\approx 0 \\\\) signifies a strong belief that \\\\( (s_i,s_j) \\\\not\\\\in I_{S_{\\\\sigma}}(r) \\\\). Together, \\\\( \\\\psi_{S} \\\\) and \\\\( \\\\phi_r \\\\) allow us to define a belief-based analogue to a structure.\\n\\n**Definition 3 (Soft-Structure/Soft-Substructure)**\\n\\nGiven signature \\\\( \\\\sigma \\\\), a possibly infinite set \\\\( Z \\\\) and relation-decoders \\\\( \\\\phi_r \\\\), a soft-structure is a tuple\\n\\n\\\\[ \\\\tilde{Z}_{\\\\sigma} = (Z, \\\\phi_r) \\\\].\\n\\nFor (finite) domain \\\\( S \\\\) and encoding \\\\( \\\\psi_{S}: S \\\\rightarrow Z \\\\), \\\\( \\\\tilde{S}_{\\\\sigma} = (\\\\psi_{S}(S), \\\\phi_r) \\\\) is a (finite)\\n\\nsoft-substructure of \\\\( \\\\tilde{Z}_{\\\\sigma} \\\\), with sub-domain \\\\( \\\\psi_{S}(S) = \\\\{ \\\\psi_{S}(s) | s \\\\in S \\\\} \\\\subseteq Z \\\\).\\n\\nA soft-structure can be used to learn a structure over a real-world domain through learning \\\\( \\\\psi_{S} \\\\) and \\\\( \\\\phi_r \\\\). Clearly, a finite soft-substructure is a soft-structure. To determine the degree to which a soft-structure supports any given structure we introduce the following measure:\\n\\n\\\\[ p(S_{\\\\sigma}|\\\\tilde{S}_{\\\\sigma}) = \\\\prod_{r \\\\in \\\\sigma} \\\\prod_{O \\\\in S_2} \\\\phi_r(\\\\psi_{S}(O)) \\\\gamma_{r,O,S_{\\\\sigma}}(1 - \\\\phi_r(\\\\psi_{S}(O))) \\\\] (3)\\n\\nwhere \\\\( \\\\gamma_{r,O,S_{\\\\sigma}} = 1 \\\\) if \\\\( O \\\\in I_{S_{\\\\sigma}}(r) \\\\), and 0 otherwise; we use \\\\( \\\\phi_r(\\\\psi_{S}(O)) \\\\) as shorthand for \\\\( \\\\phi_r(\\\\psi_{S}(s_1),...,\\\\psi_{S}(s_n)) \\\\) for \\\\( n = ar(r) \\\\). Eqn. 3 expresses the assumption that, given a finite soft-structure, the beliefs in what constitutes the (different) interpretations of (different) relations are independent of one another. It is straightforward to show that\\n\\n\\\\[ \\\\sum_{S_{\\\\sigma}} p(S_{\\\\sigma}|\\\\tilde{S}_{\\\\sigma}) = 1 \\\\] (summed over all possible structures with domain \\\\( S \\\\) and signature \\\\( \\\\sigma \\\\))\\n\\nand so it can be treated as a probability measure, where \\\\( p(S_{\\\\sigma}|\\\\tilde{S}_{\\\\sigma}) \\\\approx 1 \\\\) means that there is a high probability that the interpretation sampled from \\\\( \\\\tilde{S}_{\\\\sigma} \\\\) will be \\\\( I_{S_{\\\\sigma}} \\\\). If we have a theory \\\\( T \\\\) over \\\\( \\\\sigma \\\\) then it is natural to ask with what weight \\\\( \\\\tilde{S}_{\\\\sigma} \\\\) supports any given structure that is a model of \\\\( T \\\\). In the following, we use model weight,\\n\\n\\\\[ \\\\Gamma_{\\\\tilde{S}_{\\\\sigma}T} = \\\\sum_{S_{\\\\sigma} \\\\in M_T} p(S_{\\\\sigma}|\\\\tilde{S}_{\\\\sigma}) \\\\] (4)\\n\\n3\"}"}
{"id": "Rx_nbGdtRQD", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022.\\n\\n$\\\\mathcal{M}_T$ is the set of all structures with domain $S$ that are models of $T$. This lets us compare soft-structures, wherein a good soft-structure will be one that has a high model weight:\\n\\n**Definition 4 (\\\\(\\\\epsilon\\\\)-Consistency of Soft-Structure)**\\n\\nGiven a finite soft-structure $\\\\tilde{\\\\mathcal{S}}_\\\\sigma$, if $1 - \\\\Gamma_{\\\\tilde{\\\\mathcal{S}}_\\\\sigma} T \\\\leq \\\\epsilon$ then we say that the soft-structure is \\\\(\\\\epsilon\\\\)-consistent with theory $T$.\\n\\nWe propose \\\\(\\\\epsilon\\\\)-consistency as an appropriate quantified measure of the notion of consistency presented in (Nye et al., 2021). A consistent soft-structure $\\\\tilde{\\\\mathcal{S}}_\\\\sigma$ ensures that $\\\\phi$ gives high belief only to interpretations that satisfy, i.e., are consistent with, $T$. However, this expression is limited to the domain encodings of $\\\\tilde{\\\\mathcal{S}}_\\\\sigma$, i.e., $\\\\psi_S(S)$. Going a step further, for a concept to be learned in a manner comparable to what a human might learn, we would expect that this consistency carries over to new domains and their corresponding soft-structures, as defined in what follows.\\n\\n### 3.1 Coherence Between Soft-Structures\\n\\nIn this section we define the notion of coherence between soft-structures, which aims to characterise what it means for a concept to be learned in a human-like manner. As a motivating case, consider a situation where we have already learned a soft-structure that has high model weight with models from Example 1. Now suppose we are given a new domain of images, $Y$, showing single block stacks of differing height, and let us again use the signature of ordinal relations and $T$ from Example 1.\\n\\nLastly, let $I_Y \\\\sigma$ be a corresponding interpretation that orders images according to block stack height and is a model of $T$. We can summarise this with the following two structures:\\n\\n$$X_\\\\sigma = (X, I_X \\\\sigma) \\\\in \\\\mathcal{M}_X$$\\n\\nand\\n\\n$$Y_\\\\sigma = (Y, I_Y \\\\sigma) \\\\in \\\\mathcal{M}_Y,$$\\n\\n(5)\\n\\nwhere $X_\\\\sigma$ is the structure from Example 1 with a domain of handwritten digits and $Y_\\\\sigma$ is our new structure, with a domain of block stack images. These can be modelled by soft-structures:\\n\\n$$\\\\tilde{X}_\\\\sigma = (\\\\psi_X(X), \\\\phi)$$\\n\\nand\\n\\n$$\\\\tilde{Y}_\\\\sigma = (\\\\psi_Y(Y), \\\\phi),$$\\n\\n(6)\\n\\nwhich use domain-specific encoders, $\\\\psi_X$ and $\\\\psi_Y$, but share the same relation-decoders. As we know that $\\\\tilde{X}_\\\\sigma$ has a high model weight and since $\\\\phi$ is shared with $\\\\tilde{Y}_\\\\sigma$, a natural question to ask is: under what conditions will a $\\\\phi$ that is consistent over domain-encodings $\\\\psi_X(X)$ also be consistent over $\\\\psi_Y(Y)$? If this is the case, then we know that the high model weight in $\\\\tilde{X}_\\\\sigma$ is reciprocated for $\\\\tilde{Y}_\\\\sigma$.\\n\\nConcretely, we are interested in when the following coherence condition holds.\\n\\n**Definition 5 (\\\\(\\\\epsilon\\\\)-Coherence across soft-structures)**\\n\\nTwo soft-structures, $\\\\tilde{X}_\\\\sigma$ and $\\\\tilde{Y}_\\\\sigma$ that share relation-decoders $\\\\phi$, are said to be \\\\(\\\\epsilon\\\\)-coherent with respect to a theory $T$, if they are both at least \\\\(\\\\epsilon\\\\)-consistent with $T$.\\n\\nCoherence between $\\\\tilde{X}_\\\\sigma$ and $\\\\tilde{Y}_\\\\sigma$ as defined above means that the concept of ordinality that applies to digit ordering can also be applied to block stack height ordering. It is desirable that learning ordinality on the domain of digits produces a coherent concept of ordinality with respect to other ordinal properties, such as height. Since it is possible that $\\\\psi_S(X)$ and $\\\\psi_S(Y)$ produce unique encodings, coherence relies on $\\\\phi$'s ability to generalise over possibly disjoint subsets of $Z_1$.\\n\\n### 4 A Practical Consistency Loss and \\\\(\\\\epsilon\\\\)-Proxy\\n\\nCalculating Eqn. 4 can quickly become intractable as it involves computing $\\\\phi$ beliefs for every grounding and comparing these with every interpretation that is a model of the theory of interest. We therefore want to derive an efficient consistency loss and a calculable \\\\(\\\\epsilon\\\\)-proxy, that can act as a proxy estimate for a soft-structure's \\\\(\\\\epsilon\\\\)-consistency/coherence with a given theory, without needing access to every model and without requiring an exhaustive calculation over every grounding. In this section, we present the \\\\(\\\\epsilon\\\\)-proxy derivation outline and defer the expanded derivation to Appendix H.\"}"}
