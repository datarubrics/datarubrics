{"id": "kXwdL1cWOAi", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However, previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, **UNIMAX**, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that **UNIMAX** outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with **UNIMAX** sampling.\\n\\n**1 INTRODUCTION**\\n\\nState-of-the-art multilingual models (Xue et al., 2021; 2022; Goyal et al., 2021, inter alia) utilize large-scale self-supervised learning, which involves jointly training on many languages. Because data availability varies greatly across languages, multilingual pretraining can be characterized as multitask learning (or multi-objective optimization) with severe data imbalance. Typically English is the highest-resource language (or task) with orders of magnitude larger size than lower-resource languages. For example, in the mC4 corpus (Xue et al., 2021), English has roughly $9.7$ trillion characters, which is over $92,000$ times larger than the lowest resource language, Yoruba. As a result, a key problem in designing such models is the \u201clanguage balancing\u201d problem: in what proportions should we balance the pretraining languages? Deriving the optimal balance is a difficult open research problem due to the high cost of pretraining.\\n\\nThe standard approach to this problem has been to upsample languages with smaller datasets, using a temperature hyperparameter $\\\\tau$ (Devlin et al., 2019). However, one shortcoming of this approach is that choosing $\\\\tau$ based on the desired distribution among higher-resources languages may result in examples from the lowest-resource languages being repeated excessively. Figure 1a shows the number of epochs covered for each language in the mC4 corpus. When using $\\\\tau = 3.33$ and a trillion token budget (the values used in popular models such as mT5 and ByT5), the lowest-resource languages are repeated over $100$ times. This excessive repetition can have several unwanted consequences: (i) it leads to overfitting, which degrades performance on downstream tasks (Raffel et al., 2020; Lee et al., 2022; Hernandez et al., 2022), (ii) it increases the risk of memorizing private or sensitive content (Carlini et al., 2021; Lee et al., 2022), and (iii) it wastes training cycles that could have been devoted to unique examples. As models continue to grow in scale (Chowdhery et al., 2022; Brown et al., 2020; Smith et al., 2022), these issues with temperature sampling grow more pressing, as larger models benefit from longer training (Hoffmann et al., 2022), overfit more easily, and have a greater capacity to memorize.\"}"}
{"id": "kXwdL1cWOAi", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This paper proposes a new paradigm for sampling across languages and datasets that ameliorates the above mentioned problems. We propose UniMax (uniform + max), a conceptually simple but highly effective two-pronged sampling approach that results in fairer and more effective language distributions for pretraining multilingual language models that work well across model scales. One of the main assumptions we make is that practical large-scale training jobs operate with a fixed amount of compute, which is often translated into a fixed training token budget (Raffel et al., 2020). UniMax starts by pre-allocating training tokens to underrepresented datasets based on the number of allowed max repeats ($N$). For the remaining budget, we prioritize \\\"linguistic utility\\\" (Blasi et al., 2022) by allocating uniformly across all languages with sufficient data to avoid exceeding the prescribed number of per-language epochs. Unlike previous approaches, this means UniMax is relatively resistant to distribution biases that arise due to artifacts of the corpus generation process (i.e., web crawlers). To take a concrete example, the mC4 corpus contains $70 \\\\times$ more English than Chinese text. While mT5's temperature sampling ($\\\\tau = 3.33$) results in training on $3.4 \\\\times$ more English than Chinese, UniMax will assign equal training tokens to the two languages, provided that this doesn't result in repeating the 39 billion available Chinese tokens more than $N$ times.\\n\\nAnother key benefit of UniMax is that it is robust to model scaling. In considering language sampling strategies at scale, it is important to carefully control how many times a dataset can be repeated during training to avoid overfitting and memorization. Our proposed method explicitly controls the extent of data repeats of any language, providing a direct countermeasure to overfitting on low-resource languages, without imposing any reprioritization on higher-resource languages.\\n\\nOur key contributions are to: (1) Propose UniMax, a simple but effective language sampling strategy that provides more uniform coverage of high-resource languages while mitigating overfitting on low-resource languages. (2) Perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. (3) Release an improved and refreshed variant of the mC4 multilingual corpus consisting of 29 trillion characters across 107 languages. (4) Release pretrained model checkpoints using UniMax sampling.\"}"}
{"id": "kXwdL1cWOAi", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Among languages for limited model capacity is the well-known problem of \\\"capacity bottleneck\\\" (Arivazhagan et al., 2019), also known as the \\\"curse of multilinguality\\\" (Conneau et al., 2020). In exploring the interaction between language balancing and model scale, previous work has ablated sampling temperature with models at or below $1 \\\\text{ billion}$ parameters (Conneau et al., 2020; Xue et al., 2021), but we believe our study is the first to systematically explore balancing strategies at scales above $1 \\\\text{ billion}$ parameters. Additionally, our work targets general-purpose encoder-decoder models, whereas most previous work used encoder-only models or targeted machine translation exclusively. Michel et al. (2021) explore adaptive mixing strategies in a similar setting, and also find that fixed uniform mixing is a strong baseline for multilingual pretraining.\\n\\nWithin the context of machine translation, Jean et al. (2019) proposed using a bi-level optimization algorithm maximizing the validation utility to adjust the sampling weights per language. Arivazhagan et al. (2019) was the first testing various temperatures ($\\\\tau = 1, 5, 100$) to train a multilingual translation model for 212 language pairs. The study suggested over-sampling low-resource while sub-sampling high-resource languages to maximize a uniform utility. The over-training/over-fitting due to over-sampling was alleviated by increasing the number of tasks in that study. Wang et al. (2020b) proposed another bi-level approach, using differentiable data selection (Wang et al., 2020a). The study includes a uniform language distribution baseline ($\\\\tau = \\\\infty$), but this differs from UniMax in having no limit on repetition of examples in low-resource languages. Recently Wang et al. (2021) proposed using the gradient alignment information across languages to adjust the language weights, but it is unclear if this can be practically scaled to $100$ or more languages.\\n\\nWork on cross-lingual transfer learning has repeatedly found that English may not be the \\\"ideal\\\" source language for transfer. Malkin et al. (2022) conduct an analysis of zero-shot transfer, and calculate a \\\"donation score\\\" for each of 22 languages, measuring whether its inclusion in pretraining helps or hurts performance in other languages. They find English is among the \\\"least donating\\\"\u2014one of only four languages whose inclusion results in an overall degradation in performance for other languages. In a similar exploration, Turc et al. (2021) find that German and Russian are more effective source languages for zero-shot transfer than English. These findings support the idea of testing sampling strategies where English doesn't outweigh other languages.\\n\\nWe focus here on the problem of balancing languages with significant web data. In particular, we limit our scope to the approximately hundred languages that occur with some frequency in the CommonCrawl corpus, as detected by CLD3 language detection. Other methods are likely needed to scale NLP methods to thousands of languages, e.g., see Wang et al. (2022) and Bapna et al. (2022).\\n\\n3 **SAMPLING METHODS**\\n\\nTemperature-based sampling\\n\\nIn order to define a sampling distribution over languages, we use an empirical distribution for each language $p_l = \\\\sum_{l' \\\\in L} n_{l'} (1)$\\n\\nwhere $n_l$ represents the \\\"size\\\" of language $l$, as discussed in Section \u00a73.1. A temperature-based sampling strategy uses a distribution $q_l$ defined by exponentiating $p_l$ by the inverse of the temperature and renormalizing.\\n\\n$$q_l = \\\\frac{p_l}{\\\\sum_{l' \\\\in L} p_l'/\\\\tau}.$$ (2)\\n\\nCommonly used temperature values in the literature are summarized in Table 1. Higher temperature makes the distribution \\\"flatter\\\", approaching the uniform distribution as $\\\\tau \\\\to \\\\infty$. In the other extreme with $\\\\tau = 0$, the entire probability mass is contained in the language with the highest probability. Note, there is no guarantee that there exists a value of $\\\\tau$ that achieves the desired balance on high- and mid-resource languages while still avoiding overfitting on the tail languages.\\n\\n2 We view the translation literature as directly relevant for language balancing research. However one key difference is that translation models used in balancing studies have assumed that English is always either the source or the target language. This may lead to preferring language distributions that cover more English.\"}"}
{"id": "kXwdL1cWOAi", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                | Temperature (\u03c4) |\\n|----------------------|-----------------|\\n| mBERT (Devlin et al., 2019) | 1.43            |\\n| XLM (Conneau & Lample, 2019) | 2.00            |\\n| XLM-R (Conneau et al., 2020) | 3.33            |\\n| mT5 (Xue et al., 2021) | 3.33            |\\n| XLM-E (Chi et al., 2022) | 1.43            |\\n\\nFor UNIMAX sampling, we start with a predefined character budget $C$. In practice, this is typically defined by the training compute allocated for the training job. The goal of UNIMAX sampling is to allocate the character budget to languages as uniformly as possible, without using more than $N$ epochs of any language. The first step is to sort the languages based on the character count in the training corpus. We iterate over languages starting from the one with the lowest character count. At each iteration, we check if the remaining character budget can be split evenly among the remaining languages without using more than $N$ epochs of any language. If so, we allocate the budget uniformly. If not, language $l$ is allocated $N$ epochs worth of characters and the remaining budget is reduced. Algorithm 1 formalizes this procedure.\\n\\n**Algorithm 1:** UNIMAX Inputs:\\n- Character count $c_l$ of each language $l$ in all the languages $L$ of the training corpus\\n- Total character budget $C$\\n- The number of epochs per language $N$\\n\\nOutput:\\n- Sampling distribution $p_l$ of each language\\n\\n1. $L \u2190 sortByCount(L)$\\n2. $B \u2190 C$\\n3. $i \u2190 0$\\n4. for $l$ in $L$ do\\n   1. $b_l \u2190 B - (\\\\sum_{l' \\\\in L} c_{l'})$\\n   2. if $b_l > c_l \\\\times N$ then\\n      1. $U_l \u2190 c_l \\\\times N$\\n   3. else\\n      1. $U_l \u2190 b_l$\\n   4. $B \u2190 B - U_l$\\n   5. $i \u2190 i + 1$\\n5. $p \u2190 normalize(U)$\\n6. return $p$\\n\\n**3.1 VOCABULARY GENERATION PROCESS**\\n\\nFollowing Xue et al. (2021), we use SentencePiece tokenization (Kudo & Richardson, 2018), which consumes a corpus to produce a subword-level tokenizer. This corpus is sampled from the available training data, typically using the same distribution used during model training. In the multilingual setting, however, this amounts to an instance of the language balancing problem. While many works explore varying the temperature of the training distribution, few perform the analogous change on the vocabulary-learning distribution. This could have drastic consequences in certain situations. For example, if the vocabulary learning distribution heavily favors Chinese, but the training distribution heavily favors English, models using this vocabulary may experience poor performance due to insufficient vocabulary coverage (Chung et al., 2020).\\n\\nMost sampling strategies require some notion of \u201csize\u201d ($n_l$) of each language $l$. This is typically computed by counting words or tokens using a pre-existing multilingual tokenizer. But beyond the\"}"}
{"id": "kXwdL1cWOAi", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"unwanted complexity of requiring one \\\"pre-tokenizer\\\" to train a second tokenizer, such strategies do not generalize well to the massively multilingual setting, as tokenization across 100 languages is non-trivial, particularly in languages written without spaces (e.g. Chinese, Japanese, Thai).\\n\\nWe propose to resolve the above issues by training separate vocabularies for each sampling strategy, using character counts as our measure of sub-corpus size. As previous work has tended to measure training budget in tokens, when comparing against such approaches, we assume a token contains 4 characters on average, following Xue et al. (2022). We use this same conversion rate to determine the total character budget $C$ in Algorithm 1, given a real training budget in terms of tokens.\\n\\n## Experiments\\n\\n### 4.1 Retraining Corpus\\n\\nTo ensure meaningful results when comparing language sampling strategies, it is critical that the examples in our pretraining corpus are correctly labeled for language. However, language detection is far from a solved problem (Caswell et al., 2020), and audits have found low accuracy across a range of public multilingual datasets (Kreutzer et al., 2022). As one severe example, only 40% of the documents in the Marathi language bucket of the popular mC4 dataset (3.0.1) were found to be well-formed Marathi, with 50% coming from other languages, and 10% being non-language.\\n\\nTo mitigate this problem, we construct a new multilingual corpus by filtering mC4 (Xue et al., 2021) to remove documents whose language ID confidence is below 0.95. Compared to the original mC4 corpus (which used a threshold of 0.7), this removes 6.1% of documents; however only 5.1% of characters are removed, as the filtered low-confidence documents also tend to be shorter. The most filtered languages are Welsh (86%), Sindhi (85%) and Luxembourgish (84%). Inspection of several hundred examples across a subset of languages indicated a clear reduction in mislabeled documents.\\n\\n### 4.2 Vocabulary\\n\\nAs discussed in Section \u00a73.1, we train a dedicated SentencePiece vocabulary for each language sampling method considered. We analyze these vocabularies in terms of token lengths and coverage of various writing scripts in Appendix A. Overall, the expected trends are observed that (i) higher temperature, (ii) smaller UNIMAX training budget, and (iii) higher UNIMAX max-epoch threshold ($N$) all lead to vocabularies that allocate capacity more uniformly across languages. This more uniform allocation results in fewer tokens from higher-resource language scripts (e.g. Latin), more tokens from lower-resource language scripts (e.g. Devanagari), and a shift towards shorter tokens.\\n\\n### 4.3 Evaluation Tasks\\n\\nIn selecting evaluation tasks, we aim to satisfy several key properties. First, tasks should be linguistically diverse, covering a range of languages from distinct families and regions, including both high- and low-resource languages. Second, tasks should be free of language bias. For example, the task training data and evaluation metrics should be well-balanced across languages. We also avoid benchmarks where English plays a special role, including datasets constructed in English and translated post-hoc to other languages, as well as zero-shot transfer tasks where English is the sole source language. Finally, to the degree possible, benchmarks should be realistic, such that performing better on the benchmark gives us confidence that a model will do better on actual tasks facing language technology users. This is in contrast to \\\"intermediate structure\\\" tasks such as part-of-speech tagging.\\n\\nTyDi QA (Clark et al., 2020) is a multilingual question-answering benchmark covering a range of typologically diverse languages. Questions are written from scratch by native speakers in each language, ensuring culturally relevant content and the absence of \\\"translationese\\\". We use the \\\"GoldP\\\" task, which covers 9 languages. To evaluate candidate models, we use the \\\"in-language multitask\\\" setting (Hu et al., 2020)\u2014fine-tuning on a mixture of all available languages, and evaluating in each language separately. To maximize per-language performance, we select per-language checkpoints based on the validation performance, and report validation metrics, as no test set is provided.\\n\\nThe WMT21 shared task on large-scale multilingual machine translation (Wenzek et al., 2021) tests the ability of single model to translate across many languages. We focus on the \\\"small track\\\" tasks,\"}"}
{"id": "kXwdL1cWOAi", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGMENTS\\n\\nWe are grateful to Yuan Cao, Noah Fiedel, Ben Hutchinson, Kathy Meier-Hellstern, Slav Petrov, Sebastian Ruder and Siamak Shakeri for helpful comments and discussion.\\n\\nREFERENCES\\n\\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George F. Foster, Colin Cherry, Wolfgang Macherey, Zhifeng Chen, and Yonghui Wu. Massively multilingual neural machine translation in the wild: Findings and challenges. CoRR, abs/1907.05019, 2019. URL http://arxiv.org/abs/1907.05019.\\n\\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4623\u20134637, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.421. URL https://aclanthology.org/2020.acl-main.421.\\n\\nAnkur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi N. Baljekar, Xavier Garc\u00eda, Wolfgang Macherey, Theresa Breiner, Vera Axelrod, Jason Riesa, Yuanbin Cao, Mia Xu Chen, Klaus Macherey, Maxim Krikun, Pidong Wang, Alexander Gutkin, Apurva Shah, Yanping Huang, Z. Chen, Yonghui Wu, and Macduff Hughes. Building machine translation systems for the next thousand languages. ArXiv, abs/2205.03983, 2022.\\n\\nDamian Blasi, Antonios Anastasopoulos, and Graham Neubig. Systematic inequalities in language technology performance across the world's languages. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5486\u20135505, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.376. URL https://aclanthology.org/2022.acl-long.376.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv e-prints, art. arXiv:2005.14165, May 2020.\\n\\nNicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pp. 2633\u20132650. USENIX Association, August 2021. ISBN 978-1-939133-24-3. URL https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting.\\n\\nIsaac Caswell, Theresa Breiner, Daan van Esch, and Ankur Bapna. Language ID in the wild: Unexpected challenges on the path to a thousand-language web text corpus. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 6588\u20136608, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.579. URL https://aclanthology.org/2020.coling-main.579.\\n\\nZewen Chi, Shaohan Huang, Li Dong, Shuming Ma, Bo Zheng, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. XLM-E: Cross-lingual language model pre-training via ELECTRA. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6170\u20136182, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.427. URL https://aclanthology.org/2022.acl-long.427.\"}"}
{"id": "kXwdL1cWOAi", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "kXwdL1cWOAi", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In Hal Daum\u00e9 III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 4411\u20134421. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/hu20b.html.\\n\\nS\u00e9bastien Jean, Orhan Firat, and Melvin Johnson. Adaptive scheduling for multi-task learning. arXiv preprint arXiv:1909.06434, 2019.\\n\\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Beno\u00eet Sagot, Clara Rivera, Annette Rios, Isabel Padimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Andre Niyongabo Rubungo, Toan Q. Nguyen, Mathias M\u00fcller, Andr\u00e9 M\u00fcller, Shamsuddeen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Niansa de Silva, Sakine Cebuk Ball\u0131, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. Quality at a glance: An audit of web-crawled multilingual datasets. Transactions of the Association for Computational Linguistics, 10:50\u201372, 2022. doi: 10.1162/tacl_a_00447. URL https://aclanthology.org/2022.tacl-1.4.\\n\\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent sub-word tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66\u201371, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https://www.aclweb.org/anthology/D18-2012.\\n\\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8424\u20138445, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.577. URL https://aclanthology.org/2022.acl-long.577.\\n\\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. MLQA: Evaluating cross-lingual extractive question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7315\u20137330, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.653. URL https://aclanthology.org/2020.acl-main.653.\\n\\nDan Malkin, Tomasz Limisiewicz, and Gabriel Stanovsky. A balanced data approach for evaluating cross-lingual transfer: Mapping the linguistic blood bank. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4903\u20134915, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.361. URL https://aclanthology.org/2022.naacl-main.361.\\n\\nPaul Michel, Sebastian Ruder, and Dani Yogatama. Balancing average and worst-case accuracy in multitask learning. CoRR, abs/2110.05838, 2021. URL https://arxiv.org/abs/2110.05838.\"}"}
{"id": "kXwdL1cWOAi", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "kXwdL1cWOAi", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"each testing translation between 6 languages, in all 30 combinations. As with TyDi QA, we fine-tune a single multilingual model on the mixture of all tasks and select per-language-pair checkpoints based on the test set performance.\\n\\nTo reduce heavy English bias, we restrict training data to 1,000 examples per language pair by randomly subsampling from the training data. Limiting the size of the fine-tuning dataset also increases the importance of transferring knowledge from pretraining, thereby making more apparent the differences between various pretraining sampling strategies.\\n\\nWe also evaluate on several widely adopted multilingual benchmarks: XNLI (Conneau et al., 2018), XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020) and PAWS-X (Yang et al., 2019). While these do not satisfy all of our desiderata above (e.g., some are translated from English and some skew towards high-resource languages), their popularity still makes them valuable points of reference.\\n\\n4.4 Training Set Up\\n\\nWe closely follow mT5 (Xue et al., 2021) for model architecture and training procedure. Specifically, we use an encoder-decoder Transformer architecture. We use the span corruption pretraining objective from T5 (Raffel et al., 2020) on a multilingual corpus consisting of 101 languages plus 6 Latin-script variants (e.g. ru-Latn). We use batch size of 1024 sequences where each sequence is defined by selecting a chunk of 568 tokens from the training corpus. This is then split into 512 input and 114 target tokens. For the results in Section \u00a75, the number of training steps is 250,000 unless otherwise stated. Additional training details can be found in Appendix C.\\n\\n5 Results\\n\\n5.1 Pretraining Loss\\n\\nOne challenge in evaluating multilingual models is the relative paucity of benchmarks that cover a broad range of languages and are free from language bias (see Section \u00a74.3). Numerous studies have established strong correlation between pretraining and fine-tuning performance (Devlin et al., 2019; Narang et al., 2021; Tay et al., 2022, inter alia). Thus, in addition to targeted downstream evaluations, it can be valuable to monitor and analyze performance on held-out pretraining data, which by definition covers all pretraining languages. Figure 2 shows pretraining loss curves of models trained with three sampling strategies, for both English (en) and Yoruba (yo), the highest and lowest-resource languages in our study. We plot three model sizes, as model scale controls the tradeoffs between positive cross-lingual transfer and overfitting.\\n\\nIn Figure 2a, we observe that the loss values for en are much lower than for yo, and model scale alone does little to close this gap. This suggests that downstream performance on low-resource languages will suffer if they are not upsampled. On the other hand, too much upsampling is problematic, as shown in Figure 2b. Crucially, overfitting only emerges with scale: Large shows no obvious overfitting, XL shows weak overfitting, and it becomes conspicuous at XXL size. Moreover, while the effects of repetition may appear to be limited to Yoruba, results from Hernandez et al. (2022) suggest that overfitting in one language is likely to hurt performance across the board; the authors find that even repeating 0.1% of data 100 times can be as harmful as halving model size.\\n\\nFigure 3 shows the pretraining loss curve for a longer train, corresponding to 1/2 character budget. In this case, overfitting emerges even for the Large model after around 300,000 steps. Given that previous work (Conneau et al., 2020; Xue et al., 2021) studied sampling distributions at smaller model sizes or for shorter training steps, the importance of overfitting may have been overlooked. Notably, across Figures 2c and 3c, UNIMAX closes the loss gap between high- and low-resource languages without showing any sign of overfitting.\\n\\n5.2 Downstream Evaluations\\n\\nFigure 4 shows the average TyDi QA performance for the three sampling strategies discussed in Section \u00a73. Our first observation is that UNIMAX outperforms the other two consistently across\\n\\n3 We exclude results on language pairs with Serbian as the target language, due to unstable performance. The WMT21 Serbian training data covers Cyrillic and Latin scripts, and we observed high variance in eval metrics across checkpoints, depending on whether the model happened to use the same script as the references.\"}"}
{"id": "kXwdL1cWOAi", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Pretraining cross-entropy loss on the held-out data over the training steps. With too-low temperature, low-resource languages are sampled too little, and their losses are relatively high. With higher temperature, overfitting becomes more severe with increasing model size. Note that loss values are not directly comparable across sampling strategies due to the difference in vocabulary.\\n\\nFigure 3: Pretraining cross-entropy loss on the held-out data over 1M training steps. With the sequence length of 512, this corresponds to \\\\( \\\\frac{1}{2} \\\\) character budget. The overfitting behavior emerges only after sufficient number of training steps for \\\\( \\\\tau = 3.33 \\\\).\\n\\nFigure 4: Average TyDi QA GoldP performance across three model sizes. Overall, UniMax outperforms both baselines at all model sizes considered. Breakdowns on higher-resource (top-5) and lower-resource (bottom-4) languages show UniMax outperforms \\\\( \\\\tau = 3.33 \\\\) on both high- and low-resource, and only underperforms \\\\( \\\\tau = 1 \\\\) on high-resource at large model scales.\\n\\nWe can get additional insights by correlating per-language performance with the pretraining portion of each language, as shown in Figure 5. Generally, languages seen more during pretraining tend to have higher performance, but this benefit seems to have its limit. For example, \\\\( \\\\tau = 1 \\\\) sampling allocates 47.7% of training to en but this only translates to 1% better performance than UniMax, which allots only 1% to en. We believe this surprisingly small delta is attributable to the benefit of positive transfer from non-English languages onto English. Additionally, we note that UniMax outperforms \\\\( \\\\tau = 3.33 \\\\) on Swahili, despite seeing fewer Swahili examples during training.\\n\\nSee Table 7 for full per-language metrics across sampling strategies and model scales.\\n\\nFigure 6 (left) shows that UniMax also outperforms temperature sampling on WMT21, across all model sizes considered. Figure 6 (right) shows that the benefit is spread out across the majority of language pairs, as opposed to only benefiting a small subset. We also note that the benefit is more\"}"}
{"id": "kXwdL1cWOAi", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Average of EM and F1\\n\\nUniMax = 3.33\\n\\nSampling rate [%]\\n\\nReturn rate [%]\\n\\nFigure 5: Left: Per-language TyDi QA performance of XXL models pretrained using three different sampling strategies. Right: Pretraining sampling rates of the three models.\\n\\nFigure 6: Left: WMT21 performance averaged across all language pairs. UNIMAX outperforms both baselines at all model sizes. Right: The majority of the language pairs benefits with UNIMAX. Pronounced for languages pairs where the target language is non-English, see Fig. 8 for details.\\n\\nAppendix D provides evaluations on additional benchmarks, which show similar trends.\\n\\n5.3 Further Ablations\\n\\nOur core experiments described in the previous sections were done across three model scales, but limited the character budget to $\\\\frac{1}{8}$, due to compute resource constraints. In this section, we perform two additional ablations, but limiting to Large size.\\n\\nFirst, in Table 2 (left) we show the effect of increasing the budget by $4 \\\\times$. Note, longer training changes the \\\"shape\\\" of the UNIMAX distribution (cf. Figure 1b), as the increased budget means more languages will hit their max-N epoch cap. Overall, we find that UNIMAX still outperforms temperature sampling, with the longer train boosting performance across the board.\\n\\nSecond, we ablate the UNIMAX max-epoch parameter $N$. So far, we have used $N = 1$, i.e. no example is repeated. Table 2 (right) shows TyDi QA results for $N \\\\in \\\\{1, 5, 10\\\\}$. We observe the best performance when disallowing repeats entirely, but the effect is small. We also note that the optimal setting of $N$ likely depends on the character budget, as for large enough budgets, UNIMAX will see vanishingly little of the lowest-resource languages.\\n\\nTable 2: Average TyDi QA results of additional Large-size models. Left: Comparison of sampling strategies at larger ($\\\\frac{1}{2}$) character budget. Right: Ablation of UNIMAX max-epochs ($N$) parameter.\"}"}
{"id": "kXwdL1cWOAi", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Comparison to mT5. XNLI and PAWS-X show average per-language accuracy; the rest show average per-language EM/F1. We use the translate-train setting except for TyDi QA, which uses \u201cin-language\u201d. We omit results for the Large configuration due to instabilities in training.\\n\\n|        | mT5 | umT5 | mT5 | umT5 | mT5 | umT5 | mT5 | umT5 | mT5 | umT5 |\\n|--------|-----|------|-----|------|-----|------|-----|------|-----|------|\\n| Small  | 72.0| 76.2 | 79.9| 87.2 | 49.4/64.5 | 60.5 | 38.8/56.6 | 41.8 | 62.7 |\\n| Base   | 79.8| 80.8 | 89.3| 90.4 | 59.7/75.3 | 67.3 | 48.5/67.6 | 51.6 | 68.4 |\\n| XL     | 85.3| 86.5 | 91.0| 90.7 | 56.6/75.1 | 75.0 | 54.5/73.5 | 58.3 | 78.4 |\\n| XXL    | 87.1| 87.8 | 91.5| 91.2 | 71.3/85.2 | 78.7 | 57.4/76.0 | 70.5 | 79.5 |\\n\\nWe put our above findings into practice by training a suite of \u201cumT5\u201d models over a trillion tokens. For these final models, we also update the training corpus and add analysis comparing to mT5, which is the most direct point of comparison. As large language models are increasingly used for knowledge-intensive tasks (Petroni et al., 2019; Roberts et al., 2020; Petroni et al., 2021), it is important that training corpora are up-to-date. Given that the mC4 corpus is over two years old, we update the corpus (update version 3.1.0) to cover crawled documents through August 2022.\\n\\nBeyond adding fresh documents, we make three changes to mC4. First, as in Section \u00a74.1, we raise the language detection confidence threshold from 0.7 to 0.95, which we found to increase accuracy and reduce documents with little or no natural language. Second, we adjust the mC4 bad word filters to be soft filters, allowing a random 0.1% of documents with bad words to pass through. This ensures that models trained on our corpus will have at least a minimal exposure to any term, which is likely to help on tasks like toxicity detection. Finally, we remove from mC4\u2019s bad words lists any term that results in filtering >10% of documents from the language in question. Applying these changes, our resulting training corpus consists of 28.8 trillion characters from 9.0 billion documents\u2014a 35% increase in documents over mC4. This increase is due primarily to added documents, and persists despite the more aggressive filtering during language detection. See Table 8 for full corpus statistics.\\n\\nWe closely follow the training setup and evaluation tasks from the mT5 paper for a fair comparison. Table 3 shows that umT5 outperforms mT5 on most tasks, across all sizes, and particularly at the largest size.\\n\\n4 See Appendix E for an additional ablation isolating the effect of the data refresh.\\n\\nCONCLUSION\\nIn this paper, we introduced UNIMAX, a language sampling strategy that comes close to being uniform across languages\u2014as close as possible without introducing harmful repetition. We showed this method performs well across several benchmarks and model scales, up to 13 billion parameters.\\n\\nGiven the guarantees it provides against repetition, we expect UNIMAX is also well-suited to even larger model sizes, where issues of overfitting and memorization are known to grow more severe. Our study focused on a single pretraining paradigm. Future work is needed to test if UNIMAX confers gains in other settings, such as: (i) encoder-only or decoder-only models, (ii) models trained with parallel data, and (iii) models with dedicated parameters per language (Pfeiffer et al., 2022).\\n\\nBeyond evaluation metrics, there is an a priori reason to prefer more uniform language distributions: they can be seen as more equitable, in that they come closer to treating each language as an equally priority. However there is an important remaining question about whether \u201clanguage\u201d is the right unit over which to normalize. One alternative that we think deserves further exploration is inspired by the notion of \u201cdemographic utility\u201d discussed by Blasi et al. (2022), which treats each speaker as equal in balancing across languages. See Appendix B for further analysis.\\n\\nWe are eager to see if future research can formulate successful sampling strategies that take into account both linguistic and demographic utility. Ideally the language distributions output by these strategies can lead to stronger-performing and more equitable pretrained models that better serve a wider range of users and use cases.\\n\\n4 We exclude Large size, as umT5-Large exhibited pretraining instability and underperformed Base on all metrics. We have retrained umT5-Large with a new random seed and will release the final checkpoints.\"}"}
{"id": "kXwdL1cWOAi", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Figure 8: WMT21 results restricting to language pairs with non-English target. UNIMAX performs best on the vast majority of pairs, across all scales.\\n\\nTable 7: Per-language TyDi QA GoldP performance, as average of exact-match and F1 metrics. Numbers in brackets represent the rank of the language in the pretraining corpus (e.g. ru has the second largest character count).\\n\\n| Average | en | ru | ar | fi | ko | id | bn | te | sw |\\n|---------|----|----|----|----|----|----|----|----|----|\\n| Avg en  | 1  | 2  | 15 | 23 | 27 | 33 | 37 | 59 | 62 |\\n| Large (1.2B) | \u03c4 = 1.0 | 80.1 | 78.7 | 77.8 | 81.0 | 79.0 | 76.1 | 83.4 | 79.4 |\\n| XL (3.7B) | \u03c4 = 1.0 | 81.7 | 78.4 | 81.0 | 82.5 | 81.7 | 77.5 | 87.0 | 81.7 |\\n| XXL (13B) | \u03c4 = 1.0 | 82.2 | 79.1 | 82.9 | 82.7 | 83.1 | 80.0 | 86.7 | 83.8 |\\n\\nTable 6 shows results fine-tuning these two models on the XNLI task. We observe that the data refresh alone only gives a small boost (+0.1), supporting the view that the gains of UNIMAX over mT5 (+1.0) in Table 3 are primarily due to improved language sampling. While the inclusion of more recently crawled documents does not help on this particular benchmark (which is also several years old), we expect that the refreshed data will be useful to practitioners, and should help on tasks requiring up-to-date knowledge.\"}"}
{"id": "kXwdL1cWOAi", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 8: Statistics for our improved and refreshed variant of the mC4 corpus, as well as the sampling rates (%) of the sampling methods studied in the paper.\\n\\n| Lang (B) | UniMax | UniMax |\\n|---------|--------|--------|\\n| \u03c4 = 3 | \u03c4 = 1 | (1\u00d7) (1/8) |\\n| en | 13,396 | 5.75 | 46.58 | 3.22 | 1.48 |\\n| ru | 3,018 | 3.68 | 10.49 | 3.22 | 1.48 |\\n| es | 2,052 | 3.28 | 7.13 | 3.22 | 1.48 |\\n| de | 1,799 | 3.15 | 6.26 | 3.22 | 1.48 |\\n| fr | 1,433 | 2.94 | 4.98 | 3.22 | 1.48 |\\n| it | 779 | 2.45 | 2.71 | 3.22 | 1.48 |\\n| pt | 658 | 2.33 | 2.29 | 3.22 | 1.48 |\\n| zh | 556 | 2.21 | 1.93 | 3.22 | 1.48 |\\n| pl | 527 | 2.18 | 1.83 | 3.22 | 1.48 |\\n| vi | 409 | 2.02 | 1.42 | 3.22 | 1.48 |\\n| nl | 371 | 1.96 | 1.29 | 3.22 | 1.48 |\\n| tr | 349 | 1.93 | 1.22 | 3.22 | 1.48 |\\n| ar | 252 | 1.75 | 0.88 | 3.22 | 1.48 |\\n| ro | 250 | 1.74 | 0.87 | 3.22 | 1.48 |\\n| ja | 240 | 1.72 | 0.83 | 3.22 | 1.48 |\\n| cs | 236 | 1.71 | 0.82 | 3.22 | 1.48 |\\n| fa | 202 | 1.63 | 0.70 | 3.22 | 1.48 |\\n| sv | 201 | 1.63 | 0.70 | 3.22 | 1.48 |\\n| hu | 185 | 1.59 | 0.64 | 3.22 | 1.48 |\\n| uk | 171 | 1.55 | 0.59 | 3.22 | 1.48 |\\n| el | 166 | 1.54 | 0.58 | 3.22 | 1.48 |\\n| da | 132 | 1.44 | 0.46 | 2.83 | 1.48 |\\n| fi | 120 | 1.40 | 0.42 | 2.58 | 1.48 |\\n| no | 116 | 1.38 | 0.40 | 2.49 | 1.48 |\\n| bg | 99 | 1.32 | 0.35 | 2.13 | 1.48 |\\n| th | 92 | 1.29 | 0.32 | 1.97 | 1.48 |\\n| sk | 79 | 1.23 | 0.27 | 1.69 | 1.48 |\\n| hi | 75 | 1.21 | 0.26 | 1.60 | 1.48 |\\n| ko | 74 | 1.21 | 0.26 | 1.59 | 1.48 |\\n| lt | 63 | 1.15 | 0.22 | 1.35 | 1.48 |\\n| iw | 57 | 1.12 | 0.20 | 1.23 | 1.48 |\\n| ca | 55 | 1.11 | 0.19 | 1.19 | 1.48 |\\n| id | 51 | 1.08 | 0.18 | 1.09 | 1.48 |\\n| sl | 47 | 1.05 | 0.16 | 1.00 | 1.48 |\\n| et | 42 | 1.02 | 0.15 | 0.91 | 1.48 |\\n| lv | 38 | 0.99 | 0.13 | 0.83 | 1.48 |\\n| bn | 34 | 0.95 | 0.12 | 0.72 | 1.48 |\\n| sq | 18 | 0.79 | 0.06 | 0.39 | 1.48 |\\n| az | 18 | 0.79 | 0.06 | 0.39 | 1.48 |\\n| sr | 18 | 0.79 | 0.06 | 0.39 | 1.48 |\\n| ta | 17 | 0.78 | 0.06 | 0.37 | 1.48 |\\n| ms | 15 | 0.75 | 0.05 | 0.32 | 1.48 |\\n| is | 14 | 0.73 | 0.05 | 0.30 | 1.48 |\\n| kk | 13 | 0.72 | 0.05 | 0.28 | 1.48 |\\n| mr | 13 | 0.72 | 0.05 | 0.28 | 1.48 |\\n| ne | 11 | 0.68 | 0.04 | 0.23 | 1.48 |\\n| ur | 11 | 0.68 | 0.04 | 0.23 | 1.48 |\\n| ka | 10 | 0.67 | 0.04 | 0.22 | 1.48 |\\n| hy | 10 | 0.66 | 0.03 | 0.21 | 1.48 |\\n| mk | 10 | 0.65 | 0.03 | 0.20 | 1.48 |\\n| fil | 9.5 | 0.65 | 0.03 | 0.20 | 1.48 |\\n| ml | 9.4 | 0.65 | 0.03 | 0.20 | 1.48 |\\n| mn | 9.3 | 0.65 | 0.03 | 0.20 | 1.48 |\\n| gl | 8.8 | 0.64 | 0.03 | 0.19 | 1.48 |\"}"}
{"id": "kXwdL1cWOAi", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We analyze the makeup of the sub-word vocabularies resulting from different language sampling strategies in Table 4. To see how capacity is allocated across languages, we measure the proportion\"}"}
{"id": "kXwdL1cWOAi", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Frequency (%) of tokens satisfying various conditions across vocabularies generated using different sampling strategies, as well as the mT5 vocabulary.\\n\\n| Condition | \u03c4 = 1 | \u03c4 = 3.33 | UM1 4x | UM10 4x | UM1 1x | UM10 1x | mT5 |\\n|-----------|-------|----------|--------|---------|--------|---------|------|\\n| Latin     | 67.0  | 54.0     | 56.3   | 47.9    | 50.2   | 49.5    | 46.6 |\\n| Cyrillic  | 17.7  | 13.0     | 12.0   | 14.6    | 14.1   | 12.6    | 10.7 |\\n| Han       | 3.2   | 4.3      | 4.9    | 4.2     | 4.5    | 4.0     | 7.2  |\\n| Arabic    | 2.8   | 4.6      | 5.6    | 4.2     | 4.5    | 4.6     | 3.0  |\\n| Greek     | 1.3   | 1.9      | 3.1    | 1.5     | 2.0    | 1.3     | 2.1  |\\n| Hangul    | 0.8   | 1.5      | 2.1    | 1.5     | 1.8    | 1.4     | 1.7  |\\n| Devanagari| 0.5   | 2.2      | 1.9    | 3.0     | 2.9    | 2.6     | 1.3  |\\n| Thai      | 0.3   | 0.9      | 1.2    | 1.0     | 1.3    | 0.9     | 1.8  |\\n| Contains whitespace | 56.1 | 51.8     | 52.8   | 51.5    | 52.3   | 50.9    | 22.5 |\\n| Contains punctuation | 0.4 | 0.4      | 0.4    | 0.5     | 0.4    | 0.5     | 4.2  |\\n| 1-char    | 6.1   | 8.2      | 8.6    | 8.3     | 8.6    | 8.2     | 7.8  |\\n| 2-char    | 4.3   | 6.6      | 6.2    | 6.5     | 6.3    | 6.7     | 10.1 |\\n| 3-char    | 8.9   | 12.6     | 11.7   | 13.1    | 12.6   | 13.4    | 17.8 |\\n| 4-char    | 12.4  | 15.4     | 14.3   | 16.0    | 15.4   | 16.3    | 21.8 |\\n| 5-char    | 13.1  | 15.2     | 14.3   | 15.5    | 15.1   | 15.9    | 17.2 |\\n| 6-char    | 12.1  | 12.8     | 12.6   | 12.9    | 12.8   | 13.1    | 10.5 |\\n| 7-char    | 10.6  | 9.6      | 9.9    | 9.6     | 9.7    | 9.4     | 6.0  |\\n| 8-char    | 9.1   | 7.0      | 7.6    | 7.6     | 7.0    | 7.5     | 6.5  |\\n| 9-char    | 7.4   | 5.0      | 5.7    | 4.8     | 5.1    | 4.6     | 2.2  |\\n| 10+ char  | 15.9  | 7.5      | 9.2    | 6.7     | 7.5    | 7.5     | 5.9  |\\n\\nWe observe that increasing \u03c4 from 1 to 3.33 reduces the amount of Latin-script and Cyrillic tokens, presumably due to heavy reduction of English and Russian, the two most prevalent languages in mC4. However, as expected, raising temperature increases allocation to lower-ranking languages (Arabic, Chinese, Greek, Hindi, Japanese, Korean, Thai). Overall, the UNIMAX vocabularies are fairly similar to \u03c4 = 3.33, with the expected trend that increasing pretraining budget (1\u00d7 \u2192 4\u00d7) shifts more allocation to high-resource languages, while increasing the max-epoch threshold (UM1 \u2192 UM10) shifts more allocation to low-resource languages.\\n\\nLooking at token lengths, we observe three factors contribute to shorter tokens: higher temperature, smaller UNIMAX budget, and higher UNIMAX max-epoch threshold. This is expected, as if a few high-resource languages dominate the vocabulary training corpus, the sub-word optimizer will assign more capacity to rare words from these languages, as opposed to covering more frequent (and shorter) words from a wider range of languages.\\n\\nComparing to mT5, we note that our vocabularies have longer tokens overall, and include more tokens containing the SentencePiece meta-symbol indicating whitespace. We suspect these differences stem from mT5 using a whitespace-splitting \\\"pre-tokenizer\\\" to derive preliminary word counts which are fed to the sub-word training algorithm. By comparison, our vocabulary is trained on raw text, with no need for a pre-tokenizer. We also note that mT5 contains many more tokens including punctuation. This is most likely due to the presence of low-quality non-language documents that are filtered by our higher language detection threshold, as described in section \u00a74.1.\\n\\n**Representation Ratios**\\n\\nTo shed light on how moving towards a more population-based sampling strategy would reshape language distributions, we define the \\\"representation ratio\\\" $R(l, t)$ of a language $l$ within a training distribution $t$ to be the ratio between the language's rate of use within $t$ and its rate of native speakers ($s$) among the world population ($w$), as shown in equation (3). We say a language is \\\"overrepresented\\\" ($R > 1$) or \\\"underrepresented\\\" ($R < 1$) to indicate that its prevalence in training is more or less than expected based on its global L1 speaker population.\"}"}
{"id": "kXwdL1cWOAi", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7 plots representation ratios for the mT5 language distribution, with native speaker counts taken from Wikipedia.\\n\\nWe observe a wide range of representation, ranging from $7 \\\\times \\\\text{underrepresented}$ to over $900 \\\\times \\\\text{overrepresented}$. We also note a clear pattern whereby languages of Europe tend to have higher representation ($Mann\u2013Whitney U = 566, p<1e^{-5}$, two-tailed), echoing a general bias observed by Blasi et al. (2022) for NLP resources.\\n\\nOverall, we find that languages with large speaker populations, and particularly non-European languages are heavily underrepresented in commonly used pretraining distributions. For example, the top 5 underrepresented languages in mT5 training are all languages of Asia and Africa with 50 million or more native speakers: Chinese, Punjabi, Yoruba, Bengali and Hindi.\\n\\n### ADDITIONAL TRAINING DETAILS\\n\\nThe model architectures used in this study are the same as mT5 models, except that relative position embeddings are not shared across layers. In all of our models, the vocabulary size is 256,000 subwords, and byte-level fallback is enabled, so unknown tokens are broken down into UTF-8 bytes.\\n\\nWe use the T5X library (Roberts et al., 2022) to train the models using Google Cloud TPUs. For pretraining, we use Adafactor optimizer (Shazeer & Stern, 2018) with a constant learning rate of $0.01$ in the first $10,000$ steps and inverse square root decay afterwards. For finetuning, we use Adafactor with a constant learning rate of $5e^{-5}$. Unlike mT5, we do not use loss normalization factor. Instead we use the number of real target tokens as the effective loss normalization.\\n\\nFinally, we do not factorize the second moment of the Adafactor states and we also use momentum, neither of which are used in T5 and mT5 studies.\\n\\n### ADDITIONAL BENCHMARKS\\n\\nAs a further comparison, we fine-tune and evaluate the nine pretrained models from Sections 4 and 5 on three additional benchmarks: XNLI zero-shot, XNLI translate-train, and XQuAD. We observe UNIMAX performs the best overall, although $\\\\tau = 3.33$ is a close second.\\n\\nTo isolate the effect of refreshing the mC4 data, we train two additional models to $100,000$ steps using UNIMAX sampling, under the same \u201cfull-budget\u201d setting as in Section \u00a76. These models differ only in that one trains on the original mC4 data, while the other uses our refreshed mC4 corpus.\\n\\n5 Languages not covered by mT5 are not plotted, but have a representation ratio of zero. As one example, Odia is a language of India spoken by 33 million native speakers, but not included in mT5 training.\"}"}
{"id": "kXwdL1cWOAi", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: Additional benchmark results across sampling strategy and model scale. XNLI scores are average per-language accuracy. XQuAD is run in the translate-train setting and scores are average per-language EM/F1.\\n\\n| Model Scale | Sampling Strategy | XNLI | mC4 Refresh | XQuAD | XQuAD EM/F1 |\\n|-------------|-------------------|------|-------------|-------|-------------|\\n| Large (1.2B) | $\\\\tau = 1$ | 74.8 | 78.2 | 71.0 | 71.6/83.2 |\\n|             | $\\\\tau = 3.33$ | 77.3 | 82.3 | 71.6 | 71.6/83.2 |\\n|             | UNI | 78.3 | 82.7 | 71.6 | 71.6/83.1 |\\n|             | MAX | 78.3 | 82.7 | 71.6 | 71.6/83.1 |\\n| XL (3.7B)   | $\\\\tau = 1$ | 84.1 | - | 73.8 | 71.6/84.9 |\\n|             | $\\\\tau = 3.33$ | 85.0 | - | 74.2 | 71.6/85.3 |\\n|             | UNI | - | 85.0 | 74.5 | 71.6/85.5 |\\n|             | MAX | - | 85.0 | 74.5 | 71.6/85.5 |\\n| XXL (13B)   | $\\\\tau = 1$ | 85.1 | - | 75.1 | 71.6/86.0 |\\n|             | $\\\\tau = 3.33$ | 85.5 | - | 75.4 | 71.6/86.3 |\\n|             | UNI | - | 85.5 | 75.6 | 71.6/86.4 |\\n|             | MAX | - | 85.5 | 75.6 | 71.6/86.4 |\\n\\nTable 6: Ablation on mC4 refresh. XNLI scores are average per-language accuracy in the translate-train setting.\\n\\n- XNLI scores: 77.6 vs 77.7\\n- mC4 Refreshed scores: 77.7\"}"}
