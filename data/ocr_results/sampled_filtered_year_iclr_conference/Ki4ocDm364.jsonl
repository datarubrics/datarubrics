{"id": "Ki4ocDm364", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of discounting each objective's desired reward separately, we empirically find that since some objectives are inherently conflicting, setting RTG high for one objective means we should accordingly lower RTG for other objectives (i.e. we shouldn't use maximum RTG for both). In this way, our test-time RTG can follow closer to the training distribution.\\n\\nFigure 6: RTG fitted through linear regression (lr) models stay closer to the dataset distribution. Furthermore, linear regression models can also efficiently generalize to unseen preferences during test time.\\n\\nIn this paper, we use linear regression \\\\( g_f \\\\) to find corresponding RTG conditioned on the given preference. Figure 6 demonstrates the weighted RTG of the \\\"running\\\" objective as a function of its preference in MO-Hopper where the conflicting objectives are \\\"running\\\" and \\\"jumping\\\". It is clear that RTG closely correlates with the conditioned preference for running and we should adjust the initial RTG during test-time accordingly.\\n\\nFinally, we only use the learned linear regression model from Expert dataset. This is because regression models fitted on sub-optimal data can easily produce an RTG lower than optimal. In practice, we can easily achieve a similar result by training the regression model only on the best-performing trajectories for respective preferences. Other regression or clustering methods to find appropriate RTG can also work, and we leave it as future works especially when not assuming linearly weighted objectives.\\n\\n\\\\[ \\\\text{ETRAINING DETAILS} \\\\]\\n\\nIn this section, we list our hyper-parameters and model details. In specific, we use the same hyper-parameters for all algorithms, except for the learning rate scheduler and warm-up steps. In MODT family, inputs are first embedded by a 1-layer fully-connected network, and \\\\( n_{\\\\text{layer}} \\\\) represents the number of transformer blocks; in BC family, \\\\( n_{\\\\text{layer}} \\\\) represents the number of MLP layers to embed each input; in MOR\\\\(_V\\\\)S and MOR\\\\(_V\\\\)S(P), we leverage the same embedding strategy in Emmons et al. (2021). Additionally, we consider MOR\\\\(_V\\\\)S and MOR\\\\(_V\\\\)S(P) both have context length of 1 because they only use the current state to predict the next action, whereas MODT and BC use the past 20.\\n\\n\\\\[ \\\\text{E.1 PARAMETERS} \\\\]\\n\\n| Hyperparameter | MODT | MOR\\\\(_V\\\\)S | BC |\\n|----------------|------|-------------|----|\\n| Context Length - \\\\( K \\\\) | 20   | 1           | 20 |\\n| Batch Size     | 64   |             |    |\\n| Hidden Size    | 512  |             |    |\\n| Learning Rate  | 1e-4 |             |    |\\n| Weight Decay   | 1e-3 |             |    |\\n| Dropout        | 0.1  |             |    |\\n| \\\\( n_{\\\\text{layer}} \\\\) | 3    |             |    |\\n| Optimizer      | AdamW |            |    |\\n| Loss Function  | MSE  |             |    |\\n| LR Scheduler   | lambda |           | lambda |\\n| Warm-up Steps  | 10000 | N/A         | 4000 |\\n| Activation     | ReLU |             |    |\"}"}
{"id": "Ki4ocDm364", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We tried the following MODT architectures in our preliminary experiments. We picked Case 4 eventually as it gave the best performance in our experiments.\\n\\n1. Consider $\\\\omega$ as an independent token of the transformer.\\n2. Train a separate embedding for $\\\\omega$, concatenate the embeddings to get $f_{\\\\phi}s_pq\\\\Lambda f_{\\\\phi}\\\\omega p\\\\omega q$, and $f_{\\\\phi}g_pq\\\\Lambda f_{\\\\phi}\\\\omega p\\\\omega q$, then pass into the transformer.\\n3. Add another MLP layer on top of Case 2 after concatenation, then pass output into the transformer.\\n4. Concatenate $\\\\omega$ to other tokens before any layers. This means we have $s\\\\Lambda s\\\\Lambda\\\\omega$, $a\\\\Lambda a\\\\Lambda\\\\omega$, and $g\\\\Lambda g\\\\Lambda\\\\omega$.\\n\\nAmong a variety of metrics for MORL, we use Hypervolume (HV) and Sparsity (SP) to benchmark models for several reasons. First, many metrics such as the $\\\\epsilon$-metric require prior knowledge of the true Pareto Fronts, which are not available for our MuJoCo Environments. Second, we only assume linear reward function and cannot collect real-time user feedback, thus utility-based metrics such as the expected utility metric (EUM) are not applicable. Finally, using the same metric as in the original behavioral policy paper facilitate algorithm comparisons.\\n\\nWe present the Pareto Set visualizations for all of our models trained under each High-H dataset in Figure 7. Each point in each subplot is based on the average result of 3 seeds. In 2 objective environments, we evaluate the model using 501 equally spaced preference points. In 3 objective environments, we use 351 equally spaced preference points instead. Since the environments are stochastically initialized, we evaluate 5 times at each preference point and take the mean value. This makes each point the average value of 15 runs. We here allow a small tolerance for coloring the dominated points. If a preference point is within the achievable preference $\\\\Omega$ but the solution is dominated, we color it in red. Since our models are conditioned on continuous preference points and environments are initialized stochastically, we give a small tolerance (3%-8%) for points to be colored in blue. The hypervolume and sparsity metrics, on the other hand, are based on strictly undominated solutions without tolerance.\\n\\nWe train on the Medium-Entropy and Low-Entropy datasets for the MO-HalfCheetah environment. Overall, models have a similar performance under Med-H and High-H datasets but suffer when only trained on Low-H. We present the results in Table 6, in which we illustrate that the Low-H dataset has a worse expert and amateur performance due to reduced variability on preference. However, MODT(P) and MORV(P) are still able to get close to or exceed in hypervolume on all 16\"}"}
{"id": "Ki4ocDm364", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Pareto Visualization for all PEDA variants and baselines on High-H datasets. Notice that MO-Hopper and MO-Walker2d are more challenging and have significantly more dominated points for all variants. In other environments, however, the PEDA variants produce better results while other baselines fail at higher chances. Since BC without preference is completely single objective, we don\u2019t show the result here.\"}"}
{"id": "Ki4ocDm364", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: We ablate how well PEDA variants perform and generalize under a different preference distribution in the MO-HalfCheetah environment. We can see that PEDA can still perform well when trained under the partially-clustered Med-H dataset. However, performance drops when it is trained under the entirely clustered Low-H dataset. (B: Behavioral Policy)\\n\\nDataset (\\\\(\\\\hat{10}^6\\\\))\\n\\n| Setting     | Dataset | MODT (P) | MOR V S (P) |\\n|-------------|---------|----------|-------------|\\n|              | Med-H-Amateur | 5.69 \u00b1 0.01 | 5.77 \u00b1 0.01 | 5.44 \u00b1 0.26 |\\n|              | Low-H-Amateur  | 4.21 \u00b1 0.05 | 4.80 \u00b1 0.03 | 4.75 \u00b1 0.04 |\\n|              | High-H-Amateur | 5.68 \u00b1 0.01 | 5.77 \u00b1 0.00 | 5.46 \u00b1 0.02 |\\n\\nTable 7: We ablate the importance of using multi-dimensional rtgs instead of a one-dimensional rtgs by taking the weighted sum of objectives on the MO-HalfCheetah environment. We see multi-objective rtgs provide a variance reduction for MOR V S (P) and hypervolume performance improvement in other models. (B: Behavioral Policy)\\n\\nSetting | Dataset (\\\\(\\\\hat{10}^6\\\\)) | MODT (P) | MOR V S (P) |\\n|--------|----------------|----------|-------------|\\n| mo rtg | High-H-Expert | 5.79 \u00b1 0.00 | 5.78 \u00b1 0.00 | 5.59 \u00b1 0.03 |\\n| 1-dim rtg | High-H-Expert | 5.79 \u00b1 0.02 | 5.78 \u00b1 0.00 | 4.43 \u00b1 0.09 |\\n\\ndatasets, which showcases the effectiveness of PEDA as an efficient MORL policy. Results are based on an average of 3 seeds with the standard error given.\\n\\nWe attempted to train MODT and RvS with 1-dim return-to-go rather than a separate rtg for each objective. According to results on MO-HalfCheetah and the High-H datasets in 7, using multi-dimensional RTG enhances the performance of MODT (P), and are about the same for MOR V S (P) when preference is concatenated to states. However, it reduces standard error significantly in both MODT (P) and MOR V S (P). In the naive models when preferences are not concatenated to states, using a multi-dimensional RTG helps to achieve a much more competitive hypervolume. We thus believe multi-dimensional RTG conveys important preference information when the model doesn't directly take preference as an input. Results are based on an average of 3 seeds with the standard error given.\"}"}
{"id": "Ki4ocDm364", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Ki4ocDm364", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"All environments are the same as in Xu et al., 2020, except for when resetting the environment, each parameter is uniformly sampled from the $x \\\\sim \\\\mathcal{U}(-10^{-3}, 10^{-3})$. For MO-Hopper and MO-Hopper-3obj, we always reset height as 1.25 since this parameter directly relates to the reward function. All environments have a max episode length of 500 steps per trajectory, but the agent may also die before reaching the maximum length.\\n\\nA.1 MO-Ant\\n\\nThe two objectives in MO-Ant are achieved distance in x and y axes respectively, denoted as $r_{vx}^t, r_{vy}^t$. Consider the position of the agent is represented as $p_x^t, y_t$ at time $t$ and takes the action $a_t$. The agent has a fixed survival reward $r_s = 1.0$, a fixed $dt = 0.05$, and an action cost of $r_a = \\\\| k_a \\\\|^2$. The rewards are calculated as:\\n\\n$$r_{vx}^t = \\\\min_t t_{4.0}, p_x^t - x_t - 1$$\\n\\n$$r_{vy}^t = \\\\min_t t_{4.0}, p_y^t - y_t - 1$$\\n\\nA.2 MO-HalfCheetah\\n\\nThe two objectives in MO-HalfCheetah are running speed, and energy saving, denoted as $r_{v}^t, r_{e}^t$. Consider the position of the agent is represented as $p_x^t, y_t$ at time $t$ and takes the action $a_t$. The agent has a fixed survival reward $r_s = 1.0$, fixed $dt = 0.05$, and an action cost of $r_a = \\\\| k_a \\\\|^2$. The rewards are calculated as:\\n\\n$$r_v^t = \\\\min_t t_{4.0}, p_x^t - x_t - 1$$\\n\\n$$r_e^t = \\\\min_t t_{4.0}, p_y^t - y_t - 1$$\\n\\nA.3 MO-Hopper\\n\\nThe two objectives in MO-Hopper are running and jumping, denoted as $r_r^t, r_j^t$. Consider the position of the agent is represented as $p_x^t, h_t$ at time $t$ and takes the action $a_t$. The agent has a fixed survival reward $r_s = 1.0$, a fixed initial height as $h_{init} = 1.25$, a fixed $dt = 0.01$, and an action cost of $r_a = \\\\| k_a \\\\|^2$. The rewards are calculated as:\\n\\n$$r_r^t = 1.5, p_x^t - x_t - 1$$\\n\\n$$r_j^t = 12, p_h^t - h_{init}$$\\n\\nA.4 MO-Hopper-3obj\\n\\nThe physical dynamics are the same in MO-Hopper and MO-Hopper-3obj, while this environment has 3 objectives: running, jumping, and energy saving. The rewards are denoted as $r_r^t, r_j^t, r_e^t$. Consider the position of the agent is represented as $p_x^t, h_t$ at time $t$ and takes the action $a_t$. The agent has a fixed survival reward $r_s = 1.0$, a fixed initial height as $h_{init} = 1.25$, a fixed $dt = 0.01$, and an action cost of $r_a = \\\\| k_a \\\\|^2$. The rewards are calculated as:\\n\\n$$r_r^t = 1.5, p_x^t - x_t - 1$$\\n\\n$$r_j^t = 12, p_h^t - h_{init}$$\\n\\n$$r_e^t = 4.0, p_h^t - h_{init}$$\"}"}
{"id": "Ki4ocDm364", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The objectives in MO-Swimmer are speed and energy saving, denoted as \\\\( r^v, r^e \\\\). Consider the position of the agent is represented as \\\\( p_{x,t}, p_{y,t} \\\\) at time \\\\( t \\\\) and takes the action \\\\( a_t \\\\). The agent has a fixed survival reward \\\\( r_s = 1.0 \\\\), a fixed \\\\( dt = 0.05 \\\\), and an action cost of \\\\( r_a = \\\\sqrt{k_a} \\\\). The rewards are calculated as:\\n\\n\\\\[\\n\\\\text{reward}_t = p_{x,t} - p_{x,t-1} \\\\quad \\\\text{and} \\\\quad \\\\text{rewards}_t = 0.3 \\\\cdot \\\\text{reward}_t + 0.15 \\\\cdot \\\\text{reward}_t \\\\cdot (5)\\n\\\\]\\n\\nTo uniformly sample the High-H data from the entire preference space, the problem is equivalent to sampling from a \\\\( n \\\\)-dimensional simplex, where \\\\( n \\\\) is the number of objectives. The resulting sampling is:\\n\\n\\\\[\\n\\\\omega_{\\\\text{high}} = ||f_{\\\\exp p}, \\\\lambda||_1\\n\\\\]\\n\\nWe take the 1-norm following the exponential distribution to make sure each preference add up to 1. When \\\\( \\\\Omega \\\\neq \\\\Omega \\\\), we perform rejection sampling to restrict the range.\\n\\nTo sample the Med-H and Low-H data, we first sample \\\\( \\\\alpha \\\\) from a non-negative uniform distribution, then sample the corresponding Dirichlet preference. Here, we sample a different \\\\( \\\\alpha \\\\) to make sure the center of the Dirichlet changes and thus allows more variation.\\n\\n\\\\[\\n\\\\omega_{\\\\text{med}} = f_{\\\\text{Dirichlet}}(\\\\alpha); \\\\quad \\\\text{where} \\\\quad \\\\alpha = \\\\text{Unif}(0, 10^6) \\\\\\\\\\n\\\\omega_{\\\\text{low}} = f_{\\\\text{Dirichlet}}(\\\\alpha); \\\\quad \\\\text{where} \\\\quad \\\\alpha = \\\\text{Unif}(1/3 \\\\cdot 10^6, 2/3 \\\\cdot 10^6)\\n\\\\]\\n\\nFor sampling from behavioral policy consists of a group of single-objective policies \\\\( \\\\pi = \\\\pi_1, \\\\ldots, \\\\pi_B \\\\) with \\\\( B \\\\) being the total number of candidate policies, we recommend first find the expected unweighted raw rewards \\\\( G_{\\\\pi_1}, \\\\ldots, G_{\\\\pi_B} \\\\). Then, find the estimated \\\\( \\\\hat{\\\\omega}_{\\\\pi_1}, \\\\ldots, \\\\hat{\\\\omega}_{\\\\pi_B} \\\\) by letting \\\\( \\\\hat{\\\\omega}_{\\\\pi_b,i} = G_{\\\\pi_b,i} / \\\\sum_j G_{\\\\pi_b,j} \\\\), which represents the estimated preference on \\\\( i \\\\)-th objective of \\\\( b \\\\)-th candidate policy. For a sampled preference \\\\( \\\\omega = \\\\Omega \\\\), use the policy that provides the smallest euclidean distance \\\\( d_{\\\\omega, \\\\hat{\\\\omega}_{\\\\pi_b}} \\\\). Empirically, this means picking the candidate policy that has the expected reward ratio closest to \\\\( \\\\omega \\\\).\"}"}
{"id": "Ki4ocDm364", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: A comprehensive view of the dataset. All datasets have a 500 maximum step per trajectory, and 50K trajectories are collected under each setting. As indicated by average step per trajectory, we can see Amateur trajectories are always shorter or same as Expert, thus leading to a lower return.\\n\\n| Dataset      | Max Step Per Traj | Expert Avg. Step Per Traj | Amateur Avg. Step Per Traj | Trajectories Per Dataset |\\n|--------------|-------------------|----------------------------|----------------------------|--------------------------|\\n| MO-Ant      | 500               | 500                        | 500                        | 50K                      |\\n| MO-HalfCheetah | 500             | 499.91                     | 482.11                     | 50K                      |\\n| MO-Hopper   | 500               | 499.94                     | 387.72                     | 50K                      |\\n| MO-Hopper-3obj | 500            | 499.99                     | 442.87                     | 50K                      |\\n| MO-Swimmer  | 500               | 500                        | 500                        | 50K                      |\\n| MO-Walker2d | 500               | 500                        | 466.18                     | 50K                      |\\n\\nTo sample the Med-H and Low-H data, we first sample $\\\\alpha$ from a non-negative uniform distribution, then sample the corresponding Dirichlet preference. Here, we sample a different $\\\\alpha$ to make sure the mode of the Dirichlet changes and thus allows more variation.\\n\\n$\\\\omega_{med}$ follows $f_{\\\\text{Dirichlet}}(\\\\alpha)$, where $\\\\alpha \\\\sim \\\\text{Unif}(0, 10^{6})$.\\n\\n$\\\\omega_{low}$ follows $f_{\\\\text{Dirichlet}}(\\\\alpha)$, where $\\\\alpha \\\\sim \\\\text{Unif}(1\\\\left\\\\{ 3, 10^{6} \\\\right\\\\}, 2\\\\left\\\\{ 3, 10^{6} \\\\right\\\\})$.\\n\\nSince our behavioral policy is consists of a group of single-objective policies $\\\\pi_{1}, \\\\ldots, \\\\pi_{B}$ with $B$ being the total number of candidate policies, we first find the expected unweighted raw rewards $G_{\\\\pi_{1}}, \\\\ldots, G_{\\\\pi_{B}}$. Then, we find the estimated preferences $\\\\hat{\\\\omega}_{\\\\pi_{1}}, \\\\ldots, \\\\hat{\\\\omega}_{\\\\pi_{B}}$ by letting $\\\\hat{\\\\omega}_{\\\\pi_{b}} = \\\\frac{G_{\\\\pi_{b}}}{\\\\sum_{i=1}^{n}G_{\\\\pi_{b}}}$. For each sampled preference $\\\\omega \\\\sim \\\\Omega$ following (9) or (10), we sample a complete trajectory using the single-objective behavioral policy that provides the smallest euclidean distance $\\\\min_{p, \\\\hat{p}} d(p, \\\\hat{p})$. Empirically, this means picking the candidate policy that has the expected reward ratio closest to $\\\\omega$.\"}"}
{"id": "Ki4ocDm364", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Reinforcement Learning Via Supervised Learning\\n\\nA body of recent works have formulated offline reinforcement learning as an autoregressive sequence modeling problem using Decision Transformers (DT) or Trajectory Transformers (Chen et al., 2021, Janner et al., 2021). The key idea in DT is to learn a transformer-based policy that conditions on the past history and a dynamic estimate of the returns (a.k.a. returns-to-go). Follow-up works consider online learning (Zheng et al., 2022) as well as simpler variants that rely only on multi-layer perceptrons (Emmons et al., 2021). Such agents are generally more stable and robust to optimize due to the simplicity of loss function and easier to scale to more complex settings such as environments with high-dimensional actions or states, as shown in recent works in multi-task RL (Lee et al., 2022; Reed et al., 2022).\\n\\nPreliminaries\\n\\nSetup and Notation. We operate in the general framework of a multi-objective Markov decision process (MOMDP) with linear preferences (Wakuta, 1995). An MOMDP is represented by the tuple $x = (S, A, P, R, \\\\Omega, f, \\\\gamma)$. At each timestep $t$, the agent with a current state $s_t \\\\in S$ takes an action $a_t \\\\in A$ to transition into a new state $s_{t+1}$ with probability $P(s_{t+1} \\\\mid s_t, a_t)$ and observes a reward vector $r_t = (R(s_t, a_t))$: $R = \\\\begin{pmatrix} R_1 & R_2 & \\\\cdots & R_n \\\\end{pmatrix}$. Here, $n$ is the number of objectives. The vector-valued return $R$ of an agent is given by the discounted sum of reward vectors over a time horizon, $\\\\mathbb{E}[\\\\sum_{t=0}^{\\\\infty} \\\\gamma^t r_t]$. We also assume that there exists a linear utility function $f$ and a space of preferences $\\\\Omega$ that can map the reward vector $r$ and a preference vector $\\\\omega \\\\in \\\\Omega$ to a scalar reward $r$, i.e., $r = f(r \\\\mid \\\\omega) = \\\\omega^\\\\top r$. The expected vector return of a policy $\\\\pi$ is given as $\\\\mathbb{E}_\\\\pi[r] = \\\\mathbb{E}_\\\\pi[r \\\\mid \\\\omega] = (G_\\\\pi_1, G_\\\\pi_2, \\\\ldots, G_\\\\pi_n)$ where the expected return of the $i$-th objective is given as $G_\\\\pi_i = \\\\mathbb{E}_{a_t \\\\sim \\\\pi} [r \\\\mid \\\\omega]$ for some predefined time horizon and preference vector $\\\\omega$. The goal is to train a multi-objective policy $\\\\pi$ such that the expected scalarized return $\\\\omega^\\\\top \\\\mathbb{E}_\\\\pi[r]$ is maximized.\\n\\nPareto Optimality. In MORL, one cannot optimize all objectives simultaneously, so policies are evaluated based on the Pareto set of their vector-valued expected returns. Consider a preference-conditioned policy $\\\\pi(a \\\\mid s, \\\\omega)$ that is evaluated for $m$ distinct preferences $\\\\omega_1, \\\\ldots, \\\\omega_m$, and let the resulting policy set be represented as $\\\\pi = \\\\{\\\\pi_1, \\\\ldots, \\\\pi_m\\\\}$, where $\\\\pi_i = \\\\pi(a \\\\mid s, \\\\omega_i)$ and $G_\\\\pi$ is the corresponding unweighted expected return. We say the solution $G_\\\\pi$ is dominated by $G_\\\\pi' \\\\iff G_\\\\pi_i \\\\leq G_\\\\pi'_i$ for $\\\\forall i \\\\in \\\\{1, 2, \\\\ldots, n\\\\}$. If a solution is not dominated, it is part of the Pareto set denoted as $\\\\mathcal{P}$. The curve traced by the solutions in a Pareto set is also known as the Pareto front. In MORL, our goal is to define a policy such that its empirical Pareto set is a good approximation of the true Pareto front. While we do not know the true Pareto front for many problems, we can define metrics for relative comparisons between different algorithms. Specifically, we evaluate a Pareto set $\\\\mathcal{P}$ based on two metrics, hypervolume and sparsity that we describe next.\\n\\nDefinition 1 (Hypervolume). Hypervolume $H(\\\\mathcal{P})$ measures the space or volume enclosed by the solutions in the Pareto set $\\\\mathcal{P}$: $H(\\\\mathcal{P}) = \\\\int_{\\\\mathcal{P}}^\\\\infty \\\\prod_{i=1}^n z_i \\\\, dz$, where $H(\\\\mathcal{P})$ is the hypervolume of $\\\\mathcal{P}$ and $z_i$ is the $i$-th objective in $\\\\mathcal{P}$. The dominance relation operator, and $H(\\\\mathcal{P}) = 1$ if $z \\\\in \\\\mathcal{P}$ and $0$ otherwise. Higher hypervolumes are better.\\n\\nDefinition 2 (Sparsity). Sparsity $S(\\\\mathcal{P})$ measures the density of the Pareto front covered by a Pareto set $\\\\mathcal{P}$: $S(\\\\mathcal{P}) = \\\\frac{|\\\\mathcal{P}|}{|\\\\mathcal{P}| - \\\\sum_{i=1}^n \\\\frac{1}{|\\\\mathcal{P}| - 1} (\\\\sum_{k=1}^{|\\\\mathcal{P}|} |\\\\mathcal{P}_i| - |\\\\mathcal{P}_k|)}$, where $\\\\mathcal{P}_i$ represents a list sorted as per the values of the $i$-th objective in $\\\\mathcal{P}$ and $|\\\\mathcal{P}_i|$ is the $k$-th value in the sorted list. Lower sparsity is better.\"}"}
{"id": "Ki4ocDm364", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\n4 D4MORL: DATASETS FOR OFFLINE MULTI-OBJECTIVE REINFORCEMENT LEARNING\\n\\nIn offline RL, the goal of an RL agent is to learn the optimal policy using a fixed dataset without any interactions with the environment (Levine et al., 2020). This perspective brings RL closer to supervised learning, where the presence of large-scale datasets has been foundational for further progress in the field. Many such data benchmarks exist for offline RL as well; a notable one is the D4RL (Fu et al., 2020) benchmark for continuous control which has led to the development of several state-of-the-art offline RL algorithms (Kostrikov et al., 2021; Kumar et al., 2020; Chen et al., 2021) that can scale favorably even in high dimensions. To the best of our knowledge, there are no such existing benchmarks for offline MORL. Even for the online setting, most works in MORL conduct evaluations on toy MDPs (e.g., gridworld) with a few exceptions that include continuous control, e.g., Chen et al. (2019); Xu et al. (2020). This calls for a much-needed push towards more challenging benchmarks for reliable evaluation of MORL, especially in the offline setting.\\n\\nWe introduce Datasets for Multi-Objective Reinforcement Learning (D4MORL), a large-scale benchmark for offline MORL. Our benchmark consists of offline trajectories from 6 multi-objective MuJoCo environments including 5 environments with 2 objectives each (MO-Ant, MO-HalfCheetah, MO-Hopper, MO-Swimmer, MO-Walker2d), and one environment with three objectives (MO-Hopper-3obj). The objectives are conflicting for each environment; for instance, the two objectives in MO-Hopper correspond to jumping and running; in MO-HalfCheetah, MO-Swimmer, and MO-Walker2d, they correspond to the speed and energy savings of the agent. See Appendix A for more details on the semantics of the target objectives for each environment. These environments were first introduced in Xu et al. (2020) for online MORL, and as such, we use their pretrained ensemble policies as building blocks for defining new behavioral policies for dataset collection, which we discuss next.\\n\\n4.1 TRAJECTORY SAMPLING\\n\\nThe quality of the behavioral policy used for sampling trajectories in the offline dataset is a key factor for benchmarking downstream offline RL algorithms. In existing benchmarks for single-objective RL such as D4RL (Fu et al., 2020), the quality of a behavioral policy can be ascertained and varied based on its closeness to a single expert policy, as measured by its scalar-valued returns. For a MOMDP, we do not have the notion of a scalar return and hence, a reference expert policy (or set of policies) should reflect the optimal returns for all possible preferences in the preference space.\\n\\nWe use Prediction-Guided Multi-Objective Reinforcement Learning (PGMORL), a state-of-the-art MORL algorithm for defining reference expert policies. PGMORL (Xu et al., 2020) uses evolutionary algorithms to train an ensemble of policies to approximate the Pareto set. Each reference policy in the ensemble is associated with a unique preference; as for any new preference, it is mapped to the closest preference in the reference set. The number of policies in the ensemble can vary significantly; for instance, we have roughly 70 reference policies for MO-Ant and 2445 policies for harder environments such as MO-Hopper-3obj. Given a desired preference, we define two sets of behavioral policies:\\n\\n1. Expert Dataset: We find the best reference policy in the policy ensemble, and always follow the action taken by the selected reference policy.\\n2. Amateur Dataset: As before, we first find the best reference policy in the policy ensemble. With a fixed probability \\\\( p \\\\), we randomly perturb the actions of the reference policies. Otherwise, with probability \\\\( 1 - p \\\\), we take the same action as the reference policy. In D4MORL, we set \\\\( p = 0.65 \\\\).\\n\\nFurther details are described in Appendix C. In Figure 2, we show the returns of the trajectories rolled out from the expert and amateur policies for the 2 objective environments evaluated for a uniform sampling of preferences. We can see that the expert trajectories typically dominate the amateur trajectories, as desired. For the amateur trajectories, we see more diversity in the empirical returns for both objectives under consideration. The return patterns for the amateur trajectories vary across different environments providing a diverse suite of datasets in our benchmark.\"}"}
{"id": "Ki4ocDm364", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Empirical returns for expert and amateur trajectory datasets for the two-objective environments in D4MORL. For each environment and dataset, we randomly plot returns for 300 trajectories.\\n\\nFigure 3: Illustration of the preference distributions for 3 objectives. Entropy is estimated on 50K preference samples using the Vasicek estimator in Scipy (Vasicek, 1976, Virtanen et al., 2020).\\n\\n4.2 REFERENCE SAMPLING\\n\\nThe coverage of any offline dataset is an important factor in dictating the performance of downstream offline RL algorithms (Levine et al., 2020). For MORL, the coverage depends on both the behavioral MORL policy as well as the distribution of preferences over which this policy is evaluated. We use the following protocols for sampling from the preference space $\\\\Omega$. First, we restrict our samples to lie within a physically plausible preference space $\\\\Omega_0$ covered by the behavioral policy $\\\\pi_\\\\beta$.\\n\\nFor instance, MO-Hopper has two objectives: jumping and running. Since the agent can never gain running rewards without leaving the floor, the preference of 100% running and 0% jumping is not achievable and excluded from our preference sampling distribution.\\n\\nSecond, we are primarily interested in offline trajectories that emphasize competition between multiple objectives rather than focusing on a singular objective. To enforce this criterion, we define 3 sampling distributions concentrated around the centroid of the preference simplex. The largest spread distribution samples uniformly from $\\\\Omega_0$ and is denoted as High-Entropy (High-H). Next, we have a Medium-Entropy (Med-H) distribution specified via samples of Dirichlet distributions with large values of their concentration hyperparameters (aka $\\\\alpha$). Finally, we have a Low-Entropy (Low-H) distribution that is again specified via samples of Dirichlet distributions but with low values of their concentration hyperparameters. We illustrate the samples for each of the preference distributions along with their empirical entropies in Figure 3. Further details on the sampling distributions are deferred to Appendix B. By ensuring different levels of coverage, we can test the generalizability of an MORL policy to preferences unseen during training. In general, we expect Low-H to be the hardest of the three distributions due to its restricted coverage, followed by Med-H and High-H.\\n\\nOverall Data Generation Pipeline.\\n\\nThe pseudocode for generating the dataset is described in Algorithm 1. Given a preference distribution, we first sample a preference $\\\\omega$ and query the closest behavioral policy in either the amateur/expert ensemble matching $\\\\omega$. We roll out this policy for $T$ time steps (or until the end of an episode if sooner) and record the state, action, and reward information. Each trajectory in our dataset is represented as: $\\\\tau = \\\\omega, s_1, a_1, r_1, \\\\ldots, s_T, a_T, r_T$. \\n\\n\"}"}
{"id": "Ki4ocDm364", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\nData Collection in D4MORL\\n\\nprocedure COLLECT(prefDist, nTraj, env, pretrainedAgents, T)\\nagents = pretrainedAgents\\nprefs = prefDist(nTraj)\\nall_trajs = []\\nfor \u03c9 in prefs do\\n    agent = closestAgent(agents, \u03c9)\\ns = env.reset()\\ndone = False\\n\u03c4 = [\u03c9]\\nt = 0\\nwhile (NOT done) AND (t \u2264 T) do\\n    a = agent.get_action(s)\\ns1, done, r = env.step(a)\\nappend s, a, s1, r to \u03c4\\ns = s1\\nt = t + 1\\nappend \u03c4 to all_trajs\\nreturn all_trajs\\n\\nFor every environment in D4MORL, we collect 50K trajectories of length $T=500$ for both expert and amateur trajectory distributions under each of the 3 preference distributions. Overall, this results in a total of 1.8M trajectories over all 6 environments, which corresponds to roughly 867M time steps. We refer the reader to Table 5 in Appendix B for additional statistics on the dataset.\\n\\n5 PARETO-EFFICIENT DECISION AGENTS (PEDA)\\n\\nIn this section, we propose Pareto-Efficient Decision Agents (PEDA), a new family of offline multi-objective RL agents. PEDA aims to achieve Pareto-efficiency by extending Decision Transformers (Chen et al., 2021) into multi-objective setting. We first introduce the architecture of Decision Transformers (DT) and its variant, Reinforcement Learning Via Supervised Learning (RvS), followed by our modifications extending them to the multi-objective setting.\\n\\nDT casts offline RL as a conditional sequence modeling problem that predicts the next action by conditioning a transformer on past states, actions, and desired returns. The desired returns are defined as returns-to-go ($g_t = r_{t+1} + \\\\gamma r_{t+2} + \\\\ldots$), the future returns that this action is intended to achieve. Therefore, the trajectory is represented by $\u03c4 = s_1, a_1, g_1, \\\\ldots, s_T, a_T, g_T$. In practice, we use a causally masked transformer architecture such as GPT (Radford et al., 2019) to process this sequence and predict the actions by observing the past $K$ timesteps consisting of $3K$ tokens. DT and its variants have been shown to be more stable and robust to optimize due to the simplicity of loss function; easier to scale to more complex settings such as environments with high-dimensional actions or states, and agents with broad capabilities such as multitask settings (Lee et al., 2022). Hence, we adopt Decision Transformers (Chen et al., 2021) as the representative base algorithm on which we build our work.\\n\\nIn follow-up work, Emmons et al. (2021) extend DT and shows that even multi-layer perceptrons conditioned on the average returns-to-go can achieve similar performance without the use of transformers. They call their model Reinforcement Learning Via Supervised Learning (RvS). However, RvS is generally not very stable when conditioned on very large returns, unlike DT.\\n\\n5.1 MULTI-OBJECTIVE REINFORCEMENT LEARNING VIA SUPERVISED LEARNING\\n\\nIn PEDA, our goal is to train a single preference-conditioned agent for offline MORL. By including preference conditioning, we enable the policy to be trained on arbitrary offline data, including trajectories collected from behavioral policies that are associated with alternate preferences. To parameterize our policy agents, we extend the DT and RvS architectures to include preference tokens and vector-valued returns. We refer to such preference-conditioned extensions of these architectures as MODT(P) and MORV(P) respectively, which we describe next.\\n\\nPreference Conditioning. Naively, we can easily incorporate the preference $\u03c9$ into DT by adding this token for each timestep and feeding it a separate embedding layer. However, empirically we find that such a model design tends to ignore $\u03c9$ and the correlation between the preferences and predicted actions is weak. Therefore, we propose to concatenate $\u03c9$ to other tokens before any layers in MODT(P). Concretely, we define $s^* = s_\u03c9, a^* = a_\u03c9,\u548c g^* = g_\u03c9$ where $\\\\_\\\\_$ denotes the concatenation operator. Hence, triples of $s^*, a^*, g^*$ form the new trajectory. As for MORV(P), we concatenate the preference with the states and the average RTGs by default and the network interprets everything as one single input.\"}"}
{"id": "Ki4ocDm364", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The goal of multi-objective reinforcement learning (MORL) is to learn policies that simultaneously optimize multiple competing objectives. In practice, an agent's preferences over the objectives may not be known apriori, and hence, we require policies that can generalize to arbitrary preferences at test time. In this work, we propose a new data-driven setup for offline MORL, where we wish to learn a preference-agnostic policy agent using only a finite dataset of offline demonstrations of other agents and their preferences. The key contributions of this work are two-fold. First, we introduce D4MORL, (D)atasets for MORL that are specifically designed for offline settings. It contains 1.8 million annotated demonstrations obtained by rolling out reference policies that optimize for randomly sampled preferences on 6 MuJoCo environments with 2-3 objectives each. Second, we propose Pareto-Efficient Decision Agents (PEDA), a family of offline MORL algorithms that builds and extends return-conditioned offline methods including Decision Transformers (Chen et al., 2021) and RvS (Emmons et al., 2021) via a novel preference-and-return conditioned policy. Empirically, we show that PEDA closely approximates the behavioral policy on the D4MORL benchmark and provides an excellent approximation of the Pareto-front with appropriate conditioning, as measured by the hypervolume and sparsity metrics.\\n\\n1 INTRODUCTION\\n\\nWe are interested in learning agents for multi-objective reinforcement learning (MORL) that optimize for one or more competing objectives. This setting is commonly observed in many real-world scenarios. For instance, an autonomous driving car might trade off high speed and energy savings depending on the user's preferences. If the user has a relatively high preference for speed, the agent will move fast regardless of power usage; on the other hand, if the user tries to save energy, the agent will keep a more steady speed. One key challenge with MORL is that different users might have different preferences on the objectives and systematically exploring policies for each preference might be expensive, or even impossible. In the online setting, prior work considers several approximations based on scalarizing the vector-valued rewards of different objectives based on a single preference (Lin, 2005), learning an ensemble of policies based on enumerating preferences (Mossalam et al., 2016, Xu et al., 2020), or extensions of single-objective algorithms such as Q-learning to vectorized value functions (Yang et al., 2019).\\n\\nWe introduce the setting of offline multi-objective reinforcement learning for high-dimensional state and action spaces, where our goal is to train an MORL policy agent using an offline dataset of demonstrations from multiple agents with known preferences. Similar to the single-task setting, offline MORL can utilize auxiliary logged datasets to minimize interactions, thus improving data efficiency and minimizing interactions when deploying agents in high-risk settings. In addition to its practical utility, offline RL (Levine et al., 2020) has enjoyed major successes in the last few years (Kumar et al., 2020, Kostrikov et al., 2021, Chen et al., 2021) on challenging high-dimensional environments for continuous control and game-playing. Our contributions in this work are two-fold in introducing benchmarking datasets and a new family of MORL, as described below.\\n\\nWe introduce Datasets for Multi-Objective Reinforcement Learning (D4MORL), a collection of 1.8 million trajectories on 6 multi-objective MuJoCo environments (Xu et al., 2020). Here, 5 en...\"}"}
{"id": "Ki4ocDm364", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ments consist of 2 objectives and 1 environment consists of 3 objectives. For each environment in D4MORL, we collect demonstrations from 2 pretrained behavioral agents: expert and amateur, where the relative expertise is defined in terms of the Pareto-efficiency of the agents and measured empirically via their hypervolumes. Furthermore, we also include 3 kinds of preference distributions with varying entropies to expose additional data-centric aspects for downstream benchmarking. Lack of MORL datasets and large-scale benchmarking has been a major challenge for basic research (Hayes et al., 2022), and we hope that D4MORL can aid future research in the field.\\n\\nNext, we propose Pareto-Efficient Decision Agents (PEDA), a family of offline MORL algorithms that extends return-conditioned methods including Decision Transformer (DT) (Chen et al., 2021) and RvS (Emmons et al., 2021) to the multi-objective setting. These methods learn a return-conditioned policy via a supervised loss on the predicted actions. In recent work, these methods have successfully scaled to agents that demonstrate broad capabilities in multi-task settings (Lee et al., 2022; Reed et al., 2022). For MORL, we introduce a novel preference and return conditioned policy network and train it via a supervised learning loss. At test time, naively conditioning on the default preferences and maximum possible returns leads to out-of-distribution behavior for the model, as neither has it seen maximum returns for all objectives in the training data nor is it possible to simultaneously maximize all objectives under competition. We address this issue by learning to map preferences to appropriate returns and hence, enabling predictable generalization at test-time. Empirically, we find PEDA performs exceedingly well on D4MORL and closely approximates the reference Pareto-frontier of the behavioral policy used for data generation. In the multi-objective HalfCheetah environment, compared with an average upper bound on the hypervolume of \\\\( \\\\hat{10}^{79} \\\\), PEDA achieves an average hypervolume of \\\\( \\\\hat{10}^{77} \\\\) on the Expert and \\\\( \\\\hat{10}^{76} \\\\) on the Amateur datasets.\\n\\n2 RELATED WORK\\n\\nMulti-Objective Reinforcement Learning\\n\\nPredominant works in MORL focus on the online setting where the goal is to train agents that can generalize to arbitrary preferences. This can be achieved by training a single preference-conditioned policy (Yang et al., 2019; Parisi et al., 2016), or an ensemble of single-objective policies for a finite set of preferences (Mossalam et al., 2016; Xu et al., 2020; Zhang & Li, 2007). Many of these algorithms consider vectorized variants of standard algorithms such as Q-learning (Mossalam et al., 2016; Yang et al., 2019), often augmented with strategies to guide the policy ensemble towards the Pareto front using evolutionary or incrementally updated algorithms (Xu et al., 2020; Zhang & Li, 2007; Mossalam et al., 2016; Roijers et al., 2014; Huang et al., 2022). Other approaches have also been studied, such as framing MORL as a meta-learning problem (Chen et al., 2019), learning the action distribution for each objective (Abdolmaleki et al., 2020), and learning the relationship between objectives (Zhan & Cao, 2019) among others. In contrast to these online MORL works, our focus is on learning a single policy that works for all preferences using only offline datasets.\\n\\nThere are also a few works that study decision-making with multiple objectives in the offline setting and sidestep any interaction with the environments. Wu et al., 2021 propose a provably efficient offline MORL algorithm for tabular MDPs based on dual gradient ascent. Thomas et al., 2021 study learning of safe policies by extending the approach of Laroche et al., 2019 to the offline MORL setting. Their proposed algorithm assumes knowledge of the behavioral policy used to collect the offline data and is demonstrated primarily on tabular MDPs with finite state and action spaces. In contrast, we are interested in developing dataset benchmarks and algorithms for scalable offline policy optimization in high-dimensional MDPs with continuous states and actions.\\n\\nMulti-Task Reinforcement Learning\\n\\nMORL is also closely related to multi-task reinforcement learning, where every task can be interpreted as a distinct objective. There is an extensive body of work in learning multi-task policies both in the online and offline setups (Wilson et al., 2007; Lazaric & Ghavamzadeh, 2010; Teh et al., 2017) inter alia. However, the key difference is that typical MTRL benchmarks and algorithms do not consider solving multiple tasks that involve inherent trade-offs. Consequently, there is no notion of Pareto efficiency and an agent can simultaneously excel in all the tasks without accounting for user preferences.\"}"}
{"id": "Ki4ocDm364", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multi-Objective Returns-to-Go.\\n\\nSimilar to RTG for the single objective case, we can define vector-valued RTG as\\n\\\\[\\ng_t \\\\rightarrow^T r_{t+1}\\n\\\\]\\nGiven a preference vector \\\\(\\\\omega\\\\), we can scalarize the total returns-to-go as \\\\(\\\\hat{g}_t = \\\\omega^T g_t\\\\). In principle, the scalarized RTG \\\\(\\\\hat{g}_t\\\\) can be recovered given the preference vector \\\\(\\\\omega\\\\) and the vector-valued RTG \\\\(g_t\\\\). However, empirically we find that directly feeding MODT/MORVS with the preference-weighted RTG vector \\\\(g_t d\\\\omega\\\\) is slightly preferable for stable training, where \\\\(d\\\\) denotes the elementwise product operator.\\n\\nAnother unique challenge in the MORL setting concerns the scale of different objectives. Since different objectives can signify different physical quantities (e.g., energy and speed), the choice of scaling can influence policy optimization. We adopt a simple normalization scheme, where the returns for each objective are normalized by subtracting the minimum observed value for that objective and dividing it by the range of values (max-min). Note that the maximum and minimum are computed based on the offline dataset and hence, they are not necessarily the true min/max objective values. Post this normalization, the values for every objective in the trajectory are on the same scale between 0 and 1. For evaluating the hypervolume and sparsity, we use the unnormalized values so that we can make comparisons across different datasets that may have different min/max boundaries.\\n\\nTraining.\\n\\nWe follow a simple supervised training procedure where we train the policies on randomly sampled mini-batches with MSE loss (for continuous actions). In MODT and MODT(P), the input states, actions, and returns-to-go (with concatenated preferences) are treated as tokens and embedded through one layer of MLP. We apply a layer of MLP and Tanh on the last hidden state of GPT-2 transformer to predict next action. In MORVS and MORVS(P), we use only information from the current timestep and MLP layers to predict the next action.\\n\\n6 EXPERIMENTS\\n\\nIn this section, we evaluate the performance of PEDA on D4MORL benchmark. First, we investigate the benefits of preference conditioning by evaluating on decision transformers (DT) and RvS (MORVS) where no preference information is available and we scalarize multi-objective vector returns into weighted sums. We denote our methods with preference conditioning as MODT(P) and MORVS(P). Second, we compare our methods with classic imitation learning and temporal difference learning algorithms with preference conditioning.\\n\\nImitation learning.\\n\\nImitation learning simply uses supervised loss to train a mapping from states (w/ or w/o concatenating preferences) to actions. We use behavioral cloning (BC) here and train multi-layer MLPs as models named BC (w/o preference) and BC(P) (w/ preference).\\n\\nTemporal difference learning.\\n\\nConservative Q-Learning (CQL) (Kumar et al., 2020) is the state-of-the-art standard offline RL method, which learns a conservative Q-function \\\\(f: S \\\\hat{A} \\\\rightarrow R\\\\) through neural networks. We modify the network architecture such that it also takes preference vectors as inputs to learn a preference-conditioned Q-function \\\\(f: S \\\\hat{A} \\\\hat{\\\\Omega} \\\\rightarrow R\\\\). We denote this method as CQL(P).\\n\\n6.1 MULTI-OBJECTIVE OFFLINE BENCHMARK\\n\\nTable 1: Hypervolume performance on High-H-Expert dataset. PEDA variants MODT(P) and MORVS(P) always approach the expert behavioral policy. (B: Behavioral policy)\\n\\n| Environments      | B | MODT(P) | MORVS(P) | BC(P) | CQL(P) |\\n|-------------------|---|---------|----------|-------|--------|\\n| MO-Ant (10^6)     | 6.32 | 6.21    | 6.41     | 4.88  | 5.76   |\\n| MO-HalfCheetah (10^6) | 5.79 | 5.73    | 5.78     | 5.54  | 5.63   |\\n| MO-Hopper (10^7)  | 2.09 | 2.00    | 2.02     | 1.23  | 0.33   |\\n| MO-Hopper-3obj (10^10) | 3.73 | 3.38    | 3.42     | 2.29  | 0.78   |\\n| MO-Swimmer (10^4) | 3.25 | 3.15    | 3.24     | 3.21  | 3.22   |\\n| MO-Walker2d (10^6) | 5.21 | 4.89    | 5.14     | 3.74  | 2.49   |\\n\\nHypervolume.\\n\\nWe compare hypervolume of our methods with all baselines on expert datasets in Table 1 as well as amateur dataset in Table 2. For the two-objective environments, we evaluate the...\"}"}
{"id": "Ki4ocDm364", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Hypervolume performance on High-H-Amateur dataset. PEDA variants still approach or even exceed the behavioral policy even when a considerable portion of data is suboptimal. MODT(P) and MORV S(P) still present to be the strongest models and outperform other base-lines. (B: Behavioral policy)\\n\\n| Environments       | MODT(P) | MORV S(P) | BC(P) | CQL(P) |\\n|--------------------|---------|-----------|-------|--------|\\n| MO-Ant (10^6)      | 5.61 \u00b1 0.04 | 5.92 \u00b1 0.02 | 6.07 \u00b1 0.02 | 4.37 \u00b1 0.06 |\\n| MO-HalfCheetah (10^6) | 5.68 \u00b1 0.01 | 5.69 \u00b1 0.01 | 5.77 \u00b1 0.00 | 5.46 \u00b1 0.02 |\\n| MO-Hopper (10^7)   | 1.97 \u00b1 0.05 | 1.81 \u00b1 0.03 | 1.76 \u00b1 0.03 | 1.35 \u00b1 0.03 |\\n| MO-Hopper-3obj (10^7) | 3.09 \u00b1 0.16 | 1.04 \u00b1 0.24 | 2.77 \u00b1 0.24 | 2.42 \u00b1 0.18 |\\n| MO-Swimmer (1)     | 2.11 \u00b1 0.22 | 1.67 \u00b1 0.22 | 2.79 \u00b1 0.03 | 2.82 \u00b1 0.04 |\\n| MO-Walker2d (10^4) | 4.99 \u00b1 0.34 | 3.10 \u00b1 0.34 | 4.98 \u00b1 0.01 | 3.42 \u00b1 0.42 |\\n\\nTable 3: Sparsity (\\\\(\\\\tilde{O}\\\\)) performance on High-H-Expert dataset. MODT(P) and MORV S(P) have a lower density. BC(P) also has a competitive sparsity in smaller environments such as Swimmer.\\n\\n| Environments       | MODT(P) | MORV S(P) | BC(P) | CQL(P) |\\n|--------------------|---------|-----------|-------|--------|\\n| MO-Ant (\\\\(\\\\hat{10}^4\\\\)) | 8.26 \u00b1 2.22 | 6.50 \u00b1 0.81 | 46.2 \u00b1 16.4 | 0.58 \u00b1 0.10 |\\n| MO-HalfCheetah (\\\\(\\\\hat{10}^4\\\\)) | 1.24 \u00b1 0.23 | 0.67 \u00b1 0.05 | 1.78 \u00b1 0.39 | 0.10 \u00b1 0.00 |\\n| MO-Hopper (\\\\(\\\\hat{10}^5\\\\)) | 16.3 \u00b1 10.6 | 3.03 \u00b1 0.36 | 52.5 \u00b1 4.88 | 2.84 \u00b1 2.46 |\\n| MO-Hopper-3obj (\\\\(\\\\hat{10}^5\\\\)) | 1.40 \u00b1 0.44 | 2.72 \u00b1 1.93 | 0.72 \u00b1 0.09 | 2.60 \u00b1 3.14 |\\n| MO-Swimmer (\\\\(\\\\hat{1}\\\\)) | 15.0 \u00b1 7.49 | 4.39 \u00b1 0.07 | 4.50 \u00b1 0.39 | 13.6 \u00b1 5.31 |\\n| MO-Walker2d (\\\\(\\\\hat{10}^4\\\\)) | 0.99 \u00b1 0.44 | 3.22 \u00b1 0.73 | 75.6 \u00b1 52.3 | 6.23 \u00b1 10.7 |\\n\\n6.2 ABLATION STUDY Pareto front approximation. We ablate how well the MODT(P) and MORV S(P) can approximate the Pareto front through conditioning on different preference points. We show the results in Figure 4, where we can see that the models can approximate the Pareto front while having some dominated points colored in pink mostly in the MO-Hopper and MO-Walker2d environments. The results are based on the average of 3 seeds, and the full plot can be found in Appendix G.\\n\\nTable 4: Sparsity (\\\\(\\\\tilde{O}\\\\)) performance on High-H-Amateur dataset. We can see that all models still have a similar or stronger sparsity performance when trained on amateur datasets. Furthermore, MORV S(P) still presents the strongest performance. While BC(P) has strong performance in MO-Hopper-3obj and MO-Swimmer, it also fails to give a dense solution in other environments and has a higher standard error.\\n\\n| Environments       | MODT(P) | MORV S(P) | BC(P) | CQL(P) |\\n|--------------------|---------|-----------|-------|--------|\\n| MO-Ant (\\\\(\\\\hat{10}^4\\\\)) | 8.72 \u00b1 0.77 | 5.24 \u00b1 0.52 | 25.9 \u00b1 16.4 | 1.06 \u00b1 0.28 |\\n| MO-HalfCheetah (\\\\(\\\\hat{10}^4\\\\)) | 1.16 \u00b1 0.42 | 0.57 \u00b1 0.09 | 2.22 \u00b1 0.91 | 0.45 \u00b1 0.27 |\\n| MO-Hopper (\\\\(\\\\hat{10}^5\\\\)) | 1.61 \u00b1 0.29 | 3.50 \u00b1 1.54 | 2.42 \u00b1 1.08 | 3.30 \u00b1 5.25 |\\n| MO-Hopper-3obj (\\\\(\\\\hat{10}^5\\\\)) | 10.23 \u00b1 2.78 | 1.03 \u00b1 0.11 | 0.87 \u00b1 0.29 | 2.00 \u00b1 1.72 |\\n| MO-Swimmer (\\\\(\\\\hat{1}\\\\)) | 2.87 \u00b1 1.32 | 1.03 \u00b1 0.20 | 5.05 \u00b1 1.82 | 8.87 \u00b1 6.24 |\\n| MO-Walker2d (\\\\(\\\\hat{10}^4\\\\)) | 164.2 \u00b1 13.5 | 1.94 \u00b1 0.06 | 53.1 \u00b1 34.6 | 7.33 \u00b1 5.89 |\"}"}
{"id": "Ki4ocDm364", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: We show that MODT(P) and MOR\\\\textsubscript{VS}(P) can be good approximators to the Pareto front. There are relatively more dominated points in MO-Hopper and MO-Walker2d colored in red.\\n\\nFigure 5: We show that MOR\\\\textsubscript{VS}(P) Model can follow the given target reward that is within the dataset's minimum and maximum record. The plots are for all the two-objective environments. In addition, MO-Hopper and MO-Walker2d present to be the most challenging environments for PEDA variants, featuring more dominated solutions than other environments.\\n\\nReturn distribution. We ablate how well MODT(P) and MOR\\\\textsubscript{VS}(P) follow their given target return, based on a normalized and weighted value. We present the results in Figure 5 for MOR\\\\textsubscript{VS}(P) under High-H-Expert datasets and refer to Appendix H for full settings. Here, we see that the models follow the oracle line nicely when conditioned on the target within the dataset distribution, and generalize to targets outside of the dataset distribution as well.\\n\\nCONCLUSION\\n\\nWe proposed a new problem setup for offline Multi-Objective Reinforcement Learning to scale Pareto-Efficient decision-making using offline datasets. To characterize progress, we introduced D4MORL, a dataset benchmark consisting of offline datasets generated from behavioral policies of different fidelities (expert/amateur) and rolled out under preference distributions with varying entropies (high/medium/low). Then, we propose PEDA, a family of offline MORL policy optimization algorithms based on decision transformers. To our knowledge, the PEDA variants are the first offline MORL policies that support continuous action and preference spaces. We showed that by concatenating and embedding preferences together with other inputs, our policies can effectively approximate the Pareto front of the underlying behavioral policy as measured by the hypervolume and sparsity metrics. Our proposed family includes MLP and transformer-based variants, viz. the MOR\\\\textsubscript{VS}(P) and MODT(P), with MOR\\\\textsubscript{VS}(P) performing the best overall. In some scenarios, the learned policies can also generalize to higher target rewards that exceed the data distribution.\"}"}
{"id": "Ki4ocDm364", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"REPRODUCIBILITY STATEMENT\\nOur code is available at: https://github.com/baitingzbt/PEDA.\\n\\nACKNOWLEDGEMENTS\\nAG\u2019s research is supported by a Meta Research Award and a Cisco grant.\\n\\nREFERENCES\\nAbbas Abdolmaleki, Sandy Huang, Leonard Hasenclever, Michael Neunert, Francis Song, Martina Zambelli, Murilo Martins, Nicolas Heess, Raia Hadsell, and Martin Riedmiller. A distributional view on multi-objective policy optimization. In International Conference on Machine Learning, pp. 11\u201322. PMLR, 2020.\\n\\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021.\\n\\nXi Chen, Ali Ghadirzadeh, M\u02daarten Bj\u00a8orkman, and Patric Jensfelt. Meta-learning for multi-objective reinforcement learning. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 977\u2013983. IEEE, 2019.\\n\\nScott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for offline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021.\\n\\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning, 2020.\\n\\nConor F Hayes, Roxana R \u02d8adulescu, Eugenio Bargiacchi, Johan K \u00a8allstr\u00a8om, Matthew Macfarlane, Mathieu Reymond, Timothy Verstraeten, Luisa M Zintgraf, Richard Dazeley, Fredrik Heintz, et al. A practical guide to multi-objective reinforcement learning and planning. Autonomous Agents and Multi-Agent Systems, 36(1):26, 2022.\\n\\nSandy Huang, Abbas Abdolmaleki, Giulia Vezzani, Philemon Brakel, Daniel J Mankowitz, Michael Neunert, Steven Bohez, Yuval Tassa, Nicolas Heess, Martin Riedmiller, et al. A constrained multi-objective reinforcement learning framework. In Conference on Robot Learning, pp. 883\u2013893. PMLR, 2022.\\n\\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34:1273\u20131286, 2021.\\n\\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021.\\n\\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u20131191, 2020.\\n\\nRomain Laroche, Paul Trichelair, and Remi Tachet Des Combes. Safe policy improvement with baseline bootstrapping. In International conference on machine learning, pp. 3652\u20133661. PMLR, 2019.\\n\\nAlessandro Lazaric and Mohammad Ghavamzadeh. Bayesian multi-task reinforcement learning. In ICML-27th international conference on machine learning, pp. 599\u2013606. Omnipress, 2010.\\n\\nKuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski, et al. Multi-game decision transformers. arXiv preprint arXiv:2205.15241, 2022.\\n\\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\"}"}
