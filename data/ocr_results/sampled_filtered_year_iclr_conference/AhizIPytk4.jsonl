{"id": "AhizIPytk4", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ABSTRACT\\nThe pre-training and fine-tuning paradigm has become prominent in transfer learning. For example, if the model is pre-trained on ImageNet and then fine-tuned to PASCAL, it can significantly outperform that trained on PASCAL from scratch. While ImageNet pre-training has shown enormous success, it is formed in 2D, and the learned features are for classification tasks; when transferring to more diverse tasks, like 3D image segmentation, its performance is inevitably compromised due to the deviation from the original ImageNet context. A significant challenge lies in the lack of large, annotated 3D datasets rivaling the scale of ImageNet for model pre-training. To overcome this challenge, we make two contributions. Firstly, we construct AbdomenAtlas 1.1 that comprises 9,262 three-dimensional computed tomography (CT) volumes with high-quality, per-voxel annotations of 25 anatomical structures and pseudo annotations of seven tumor types. Secondly, we develop a suite of models that are pre-trained on our AbdomenAtlas 1.1 for transfer learning. Our preliminary analyses indicate that the model trained only with 21 CT volumes, 672 masks, and 40 GPU hours has a transfer learning ability similar to the model trained with 5,050 (unlabeled) CT volumes and 1,152 GPU hours. More importantly, the transfer learning ability of supervised models can further scale up with larger annotated datasets, achieving significantly better performance than preexisting pre-trained models, irrespective of their pre-training methodologies or data sources. We hope this study can facilitate collective efforts in constructing larger 3D medical datasets and more releases of supervised pre-trained models.\\n\\nINTRODUCTION\\nPre-training and fine-tuning is a widely adopted transfer learning paradigm (Zoph et al., 2020). Given the relationship across different vision tasks, a model pre-trained on one dataset is expected to benefit another. Over the past few decades, pre-training has been important in AI development (Kumar, 2017; Radford et al., 2021). For 2D vision tasks, there are two available options: (i) supervised pre-training and (ii) self-supervised pre-training, but for 3D vision tasks, option (i) is often not available simply due to the lack of large, annotated 3D volumetric datasets (Wang et al., 2022).\\n\\nSupervised pre-training can learn image features that are transferable to many target tasks. It has been common practice to pre-train models using ImageNet and then fine-tune the model on target tasks that often have less training data, e.g., PASCAL. However, two challenges arise in ImageNet pre-training. Firstly, ImageNet predominantly comprises 2D images, leaving a palpable void in large-scale 3D datasets and investigation in 3D transfer learning (Huang et al., 2023). Secondly, ImageNet is intended for image classification, so the benefit for segmentation (and other vision tasks) can be somewhat compromised (He et al., 2019). If such an ImageNet-like dataset exists\u2014formed in 3D and annotated per voxel\u2014supervised pre-trained models are expected to transfer better to 3D image segmentation than self-supervised ones for two reasons. 1. Supervised pre-training is more efficient in data and computation because of its explicit learning objective. While self-supervised pre-training can learn features without manual annotation, it often requires a large corpus of datasets (Xiao et al., 2022). Extracting meaningful...\"}"}
{"id": "AhizIPytk4", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"features directly from raw, unlabeled data is inherently challenging. Unlabeled data have a high degree of redundancy (Haghighi et al., 2020; 2021) and noise (Mahajan et al., 2018), which can complicate the learning process. Therefore, self-supervised pre-training often calls for greater computational resources and time to match the outcomes achieved by supervised pre-training (Chen et al., 2020a; Tang et al., 2022). We have quantified the improved data and computational efficiency from perspectives of both pre-training (Figure 2a; 99.6% fewer data) and fine-tuning (Figure 2b; 66% less computation). Specifically, the model trained with 21 CT volumes, 672 masks, and 40 GPU hours shows transfer learning ability similar to that trained with 5,050 CT volumes and 1,152 GPU hours, highlighting the remarkable efficiency of supervised pre-training.\\n\\nSupervised pre-training enables the model to learn image features that are relevant to image segmentation. Self-supervised pre-training must extract images features from raw, unlabeled data using pretext tasks such as mask image modeling (Chen et al., 2019a; Tao et al., 2020; Zhou et al., 2021b; He et al., 2022), instance discrimination (Xie et al., 2020; Chaitanya et al., 2020; Shekoofeh et al., 2021), etc. Despite their efficacy in pre-training, these pretext tasks share no obvious relation to the target image segmentation. In contrast, supervised pre-training uses semantically meaningful annotations (e.g., organ/tumor segmentation) as supervision, with which the model can mimic the behavior of medical professionals\u2014identifying the edge and boundary of specific anatomical structures. As a result, the pre-training is interpretable, and the learned features are expected to be relevant to image segmentation tasks (Zamir et al., 2018; Ilharco et al., 2022; You et al., 2022). We have demonstrated that the learned features can be direct inference for organ segmentation on CT volumes collected from hospitals worldwide (Table 3; evaluated on three novel hospitals). The features learned by supervision can also be fine-tuned to perform novel class segmentation (unseen in the pre-training) with higher accuracy and less annotated data than the features learned by self-supervision (Table 4; evaluated on 63 novel classes).\\n\\nThis paper seeks to answer the question how well the model transfers to 3D medical imaging tasks if it is pre-trained on large, annotated 3D datasets. Naturally, we start with creating an IF dataset at a massive scale. Firstly, we construct a dataset (termed AbdomenAtlas 1.1) of 9,262 CT volumes with per-voxel annotations of 25 anatomical structures and pseudo annotations of seven types of tumors. This large-scale, fully-annotated dataset enables us to train models in a fully supervised manner using multi-organ segmentation as the pretext task. As reviewed in Table 1, this dataset is much more extensive (considering both the number of CT volumes and annotated classes) than public datasets (Wasserthal et al., 2022; Ma et al., 2022; Qu et al., 2023). Scaling experiments in \u00a73.1 suggested that pre-training models on more annotated datasets can further improve the transfer learning ability. Secondly, we develop a suite of Supervised Pre-trained Models, termed SuPreM, that combined the good of large-scale datasets and per-voxel annotations, demonstrating the efficacy across a range of target segmentation tasks. As reported in \u00a73.2, some of the dominant segmentation backbones have been pre-trained and will be available to the public. Current pre-trained backbones are U-Net (CNN-type) (Ronneberger et al., 2015), SegResNet (CNN-type) (Myronenko, 2019), and Swin UNETR (Transformer-type) (Tang et al., 2022), and more backbones will be added along time. In prospective endeavors, we anticipate that the expansion of datasets and annotations will not only enhance feature learning, as demonstrated in this study, but also promote the development of advanced AI algorithms and benchmark the state of the art in terms of segmentation performance, inference efficiency, and domain generalizability.\"}"}
{"id": "AhizIPytk4", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yang Deng, Ce Wang, Yuan Hui, Qian Li, Jun Li, Shiwei Luo, Mengke Sun, Quan Quan, Shuxin Yang, You Hao, et al. Ctspine1k: A large-scale dataset for spinal vertebrae segmentation in computed tomography. arXiv preprint arXiv:2105.14711, 2021.\\n\\nMatthias Eisenmann, Annika Reinke, Vivienn Weru, Minu D Tizabi, Fabian Isensee, Tim J Adler, Sharib Ali, Vincent Andrearczyk, Marc Aubreville, Ujjwal Baid, et al. Why is the winner the best? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19955\u201319966, 2023.\\n\\nSergios Gatidis, Tobias Hepp, Marcel Fr\u00fch, Christian La Foug\u00e8re, Konstantin Nikolaou, Christina Pfannenberg, Bernhard Sch\u00f6lkopf, Thomas K\u00fcstner, Clemens Cyran, and Daniel Rubin. A whole-body fdg-pet/ct dataset with manually annotated tumor lesions. Scientific Data, 9(1):601, 2022.\\n\\nJean-Bastien Grill, Florian Strub, Florent Altch\u00ea, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271\u201321284, 2020.\\n\\nHao Guan and Mingxia Liu. Domain adaptation for medical image analysis: a survey. IEEE Transactions on Biomedical Engineering, 69(3):1173\u20131185, 2021.\\n\\nFatemeh Haghighi, Mohammad Reza Hosseinzadeh Taher, Zongwei Zhou, Michael B Gotway, and Jianming Liang. Learning semantics-enriched representation via self-discovery, self-classification, and self-restoration. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 137\u2013147. Springer, 2020. URL https://github.com/fhaghighi/SemanticGenesis.\\n\\nFatemeh Haghighi, Mohammad Reza Hosseinzadeh Taher, Zongwei Zhou, Michael B Gotway, and Jianming Liang. Transferable visual words: Exploiting the semantics of anatomical patterns for self-supervised learning. IEEE Transactions on Medical Imaging, 2021. URL https://github.com/fhaghighi/SemanticGenesis.\\n\\nAli Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger R Roth, and Daguang Xu. Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images. In International MICCAI Brainlesion Workshop, pp. 272\u2013284. Springer, 2021.\\n\\nKaiming He, Ross Girshick, and Piotr Doll\u00e1r. Rethinking imagenet pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4918\u20134927, 2019.\\n\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729\u20139738, 2020.\\n\\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000\u201316009, 2022.\\n\\nYuting He, Guanyu Yang, Jian Yang, Rongjun Ge, Youyong Kong, Xiaomei Zhu, Shaobo Zhang, Pengfei Shao, Huazhong Shu, Jean-Louis Dillenseger, et al. Meta grayscale adaptive network for 3d integrated renal structures segmentation. Medical image analysis, 71:102055, 2021.\\n\\nNicholas Heller, Sean McSweeney, Matthew Thomas Peterson, Sarah Peterson, Jack Rickman, Bethany Stai, Resha Tejpaul, Makinna Oestreich, Paul Blake, Joel Rosenberg, et al. An international challenge to use artificial intelligence to define the state-of-the-art in kidney and kidney tumor segmentation in ct imaging., 2020.\\n\\nZiyan Huang, Haoyu Wang, Zhongying Deng, Jin Ye, Yanzhou Su, Hui Sun, Junjun He, Yun Gu, Lixu Gu, Shaoting Zhang, et al. Stu-net: Scalable and transferable medical image segmentation models empowered by large-scale supervised pre-training. arXiv preprint arXiv:2304.06716, 2023.\\n\\nMinyoung Huh, Pulkit Agrawal, and Alexei A Efros. What makes imagenet good for transfer learning? arXiv preprint arXiv:1608.08614, 2016.\"}"}
{"id": "AhizIPytk4", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "AhizIPytk4", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nJun Ma, Yao Zhang, Song Gu, Cheng Zhu, Cheng Ge, Yichi Zhang, Xingle An, Congcong Wang, Qiyuan Wang, Xin Liu, et al. Abdomenct-1k: Is abdominal organ segmentation a solved problem. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\\n\\nJun Ma, Yao Zhang, Song Gu, Xingle An, Zhihe Wang, Cheng Ge, Congcong Wang, Fan Zhang, Yu Wang, Yinan Xu, et al. Fast and low-gpu-memory abdomen ct organ segmentation: the flare challenge. Medical Image Analysis, 82:102616, 2022.\\n\\nJun Ma, Yao Zhang, Song Gu, Cheng Ge, Shihao Ma, Adamo Young, Cheng Zhu, Kangkang Meng, Xin Yang, Ziyan Huang, et al. Unleashing the strengths of unlabeled data in pan-cancer abdominal organ quantification: the flare22 challenge. arXiv preprint arXiv:2308.05862, 2023b.\\n\\nZhiyu Ma, Chen Li, Tianming Du, Le Zhang, Dechao Tang, Deguo Ma, Shanchuan Huang, Yan Liu, Yihao Sun, Zhihao Chen, et al. Aatct-ids: A benchmark abdominal adipose tissue ct image dataset for image denoising, semantic segmentation, and radiomics evaluation. arXiv preprint arXiv:2308.08172, 2023c.\\n\\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European conference on computer vision (ECCV), pp. 181\u2013196, 2018.\\n\\nMojtaba Masoudi, Hamid-Reza Pourreza, Mahdi Saadatmand-Tarzjan, Noushin Eftekhari, Fateme Shafiee Zargar, and Masoud Pezeshki Rad. A new dataset of computed-tomography angiography images for computer-aided detection of pulmonary embolism. Scientific data, 5(1):1\u20139, 2018.\\n\\nXueyan Mei, Zelong Liu, Philip M Robson, Brett Marinelli, Mingqian Huang, Amish Doshi, Adam Jacobi, Chendi Cao, Katherine E Link, Thomas Yang, et al. Radimagenet: an open radiologic deep learning research dataset for effective transfer learning. Radiology: Artificial Intelligence, 4(5):e210315, 2022.\\n\\nMichael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec, Eric J Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence. Nature, 616(7956):259\u2013265, 2023.\\n\\nAndriy Myronenko. 3d mri brain tumor segmentation using autoencoder regularization. In Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part II, pp. 311\u2013320. Springer, 2019.\\n\\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision, pp. 69\u201384. Springer, 2016.\\n\\nIrene Papanicolas, Liana R Woskie, and Ashish K Jha. Health care spending in the united states and other high-income countries. Jama, 319(10):1024\u20131039, 2018.\\n\\nS Park, LC Chu, EK Fishman, AL Yuille, B Vogelstein, KW Kinzler, KM Horton, RH Hruban, ES Zinreich, D Fadaei Fouladi, et al. Annotated normal ct data of the abdomen for deep learning: Challenges and strategies for implementation. Diagnostic and interventional imaging, 101(1):35\u201344, 2020.\\n\\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2536\u20132544, 2016.\\n\\nChongyu Qu, Tiezheng Zhang, Hualin Qiao, Jie Liu, Yucheng Tang, Alan Yuille, and Zongwei Zhou. Abdomenatlas-8k: Annotating 8,000 abdominal ct volumes for multi-organ segmentation in three weeks. Conference on Neural Information Processing Systems, 2023. URL https://github.com/MrGiovanni/AbdomenAtlas.\"}"}
{"id": "AhizIPytk4", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "AhizIPytk4", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yucheng Tang, Dong Yang, Wenqi Li, Holger R Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali Hatamizadeh. Self-supervised pre-training of swin transformers for 3d medical image analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 20730\u201320740, 2022.\\n\\nXing Tao, Yuexiang Li, Wenhui Zhou, Kai Ma, and Yefeng Zheng. Revisiting rubik\u2019s cube: Self-supervised learning with volume-wise transformation for 3d medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 238\u2013248. Springer, 2020.\\n\\nNational Lung Screening Trial Research Team. The national lung screening trial: overview and study design. Radiology, 258(1):243\u2013253, 2011.\\n\\nAtharva Tendle and Mohammad Rashedul Hasan. A study of the generalizability of self-supervised representations. Machine Learning with Applications, 6:100124, 2021.\\n\\nJeya Maria Jose Valanarasu, Yucheng Tang, Dong Yang, Ziyue Xu, Can Zhao, Wenqi Li, Vishal M Patel, Bennett Landman, Daguang Xu, Yufan He, et al. Disruptive autoencoders: Leveraging low-level features for 3d medical image pre-training. arXiv preprint arXiv:2307.16896, 2023.\\n\\nVanya V Valindria, Nick Pawlowski, Martin Rajchl, Ioannis Lavdas, Eric O Aboagye, Andrea G Rockall, Daniel Rueckert, and Ben Glocker. Multi-modal learning from unpaired images: Application to multi-organ segmentation in ct and mri. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 547\u2013556. IEEE, 2018.\\n\\nYan Wang, Yuyin Zhou, Wei Shen, Seyoun Park, Elliot K Fishman, and Alan L Yuille. Abdominal multi-organ segmentation with organ-attention networks and statistical fusion. Medical image analysis, 55:88\u2013102, 2019.\\n\\nZiyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, and Jiwen Lu. P2p: Tuning pre-trained image models for point cloud analysis with point-to-pixel prompting. Advances in neural information processing systems, 35:14388\u201314402, 2022.\\n\\nJakob Wasserthal, Manfred Meyer, Hanns-Christian Breit, Joshy Cyriac, Shan Yang, and Martin Segeroth. Totalsegmentator: robust segmentation of 104 anatomical structures in ct images. arXiv preprint arXiv:2208.05868, 2022.\\n\\nChen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14668\u201314678, 2022.\\n\\nYingda Xia, Qihang Yu, Linda Chu, Satomi Kawamoto, Seyoun Park, Fengze Liu, Jieneng Chen, Zhuotun Zhu, Bowen Li, Zongwei Zhou, et al. The felix project: Deep networks to detect pancreatic neoplasms. medRxiv, 2022.\\n\\nJunfei Xiao, Yutong Bai, Alan Yuille, and Zongwei Zhou. Delving into masked autoencoders for multi-label thorax disease classification. IEEE Winter Conference on Applications of Computer Vision, 2022. URL https://github.com/lambert-x/medical_mae.\\n\\nYutong Xie, Jianpeng Zhang, Zehui Liao, Yong Xia, and Chunhua Shen. Pgl: Prior-guided local self-supervised learning for 3d medical image segmentation. arXiv preprint arXiv:2011.12640, 2020.\\n\\nYutong Xie, Jianpeng Zhang, Yong Xia, and Qi Wu. Unimiss: Universal medical self-supervised learning via breaking dimensionality barrier. In European Conference on Computer Vision, pp. 558\u2013575. Springer, 2022.\\n\\nXingyi Yang, Xuehai He, Yuxiao Liang, Yue Yang, Shanghang Zhang, and Pengtao Xie. Transfer learning or self-supervised learning? a tale of two pretraining paradigms. arXiv preprint arXiv:2007.04234, 2020.\\n\\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pp. 3320\u20133328, 2014.\"}"}
{"id": "AhizIPytk4", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chenyu You, Ruihan Zhao, Fenglin Liu, Siyuan Dong, Sandeep Chinchali, Ufuk Topcu, Lawrence Staib, and James Duncan. Class-aware adversarial transformers for medical image segmentation. Advances in Neural Information Processing Systems, 35:29582\u201329596, 2022.\\n\\nAmir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3712\u20133722, 2018.\\n\\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12104\u201312113, 2022.\\n\\nJianpeng Zhang, Yutong Xie, Yong Xia, and Chunhua Shen. Dodnet: Learning to segment multi-organ and tumors from multiple partially labeled datasets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1195\u20131204, 2021.\\n\\nShaoting Zhang and Dimitris Metaxas. On the challenges and perspectives of foundation models for medical image analysis. arXiv preprint arXiv:2306.05705, 2023.\\n\\nWen Zhang, Lingfei Deng, Lei Zhang, and Dongrui Wu. A survey on negative transfer. IEEE/CAA Journal of Automatica Sinica, 10(2):305\u2013329, 2022.\\n\\nZiheng Zhao, Yao Zhang, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. One model to rule them all: Towards universal segmentation for medical images with text prompts. arXiv preprint arXiv:2312.17183, 2023.\\n\\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021a.\\n\\nKaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022a.\\n\\nZongwei Zhou. Towards Annotation-Efficient Deep Learning for Computer-Aided Diagnosis. PhD thesis, Arizona State University, 2021.\\n\\nZongwei Zhou, Jae Shin, Lei Zhang, Suryakanth Gurudu, Michael Gotway, and Jianming Liang. Fine-tuning convolutional neural networks for biomedical image analysis: actively and incrementally. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7340\u20137351, 2017. URL https://github.com/MrGiovanni/Active-Learning.\\n\\nZongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. Unet++: A nested u-net architecture for medical image segmentation. In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pp. 3\u201311. Springer, 2018. URL https://github.com/MrGiovanni/UNetPlusPlus.\\n\\nZongwei Zhou, Vatsal Sodha, Md Mahfuzur Rahman Siddiquee, Ruibin Feng, Nima Tajbakhsh, Michael B Gotway, and Jianming Liang. Models genesis: Generic autodidactic models for 3d medical image analysis. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 384\u2013393. Springer, 2019b. URL https://github.com/MrGiovanni/ModelsGenesis.\\n\\nZongwei Zhou, Vatsal Sodha, Jiaxuan Pang, Michael B Gotway, and Jianming Liang. Models genesis. Medical Image Analysis, 67:101840, 2021b. URL https://github.com/MrGiovanni/ModelsGenesis.\\n\\nZongwei Zhou, Michael B Gotway, and Jianming Liang. Interpreting medical images. In Intelligent Systems in Medicine and Health, pp. 343\u2013371. Springer, 2022b.\\n\\nBarret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-training and self-training. Advances in neural information processing systems, 33:3833\u20133845, 2020.\"}"}
{"id": "AhizIPytk4", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nwith self-supervised ones. We refer the readers to Yang et al. (2020) and Tendle & Hasan (2021) for a plethora of viewpoints from either side. In essence, the debates are about clarifying the learning objective (loss function) of emulating human vision (Zhou, 2021).\\n\\nThe learning objective of supervised pre-training is to minimize the discrepancy between AI predictions and semantic labels annotated by humans. Over the years, supervised pre-training on ImageNet has shown marked success in transfer learning (Yosinski et al., 2014). Moreover, the transfer learning ability can be further enhanced when models are trained on increasingly expansive datasets, such as ImageNet-21K (Kolesnikov et al., 2020), Instagram (Mahajan et al., 2018), JFT-300M (Sun et al., 2017), and JFT-3B (Zhai et al., 2022). In general, supervised pre-training exhibits clear advantages over self-supervised pre-training when sizable annotated datasets are available (Steiner et al., 2021; Ridnik et al., 2021). However, acquiring millions of manual annotations is labor-intensive, time-consuming, and challenging to scale\u2014but certainly not impossible\u2014evidenced by several recent influential endeavors (Kuznetsova et al., 2020; Mei et al., 2022; Kirillov et al., 2023; Bai et al., 2023).\\n\\nOn the other hand, self-supervised pre-training offers an alternative by enabling AI models to learn from raw, unlabeled data (Jing & Tian, 2020; Zoph et al., 2020; Ren et al., 2022; 2023), thus reducing the need for manual annotation. Self-supervised pre-training has historically lagged behind the state-of-the-art supervised pre-training in ImageNet benchmarks (Pathak et al., 2016; Noroozi & Favaro, 2016). The recent pace of progress in self-supervised pre-training has yielded models whose performance not only matches but, at times, surpasses those achieved by supervised pre-training (Chen et al., 2020a; Grill et al., 2020; Chen et al., 2020b; Zhou et al., 2021a; Wei et al., 2022). This has raised hopes that self-supervised pre-training could indeed replace the ubiquitous supervised pre-training in advanced computer vision going forward. The caveat, however, is the significant demand for both data and computational power, often exceeding the resources available in academic settings. For example, He et al. (2020) have demonstrated that self-supervised features trained on 1B images (a factor of $714\\\\times$ larger) can transfer comparably or better than ImageNet features.\\n\\nSupervised pre-training on ImageNet has demonstrated benefit for 2D medical image tasks after transfer learning (Tajbakhsh et al., 2016; Shin et al., 2016; Zhou et al., 2017). Unfortunately, it has been constrained for 3D medical imaging tasks due to the lack of a 3D counterpart to ImageNet. Although there are a great number of raw, unlabeled medical images available (Team, 2011; Baxter et al., 2023; Zhao et al., 2023; Saenz et al., 2024), annotating these images is a labor-intensive undertaking for professionals. Our contribution to a large, annotated 3D dataset could spark the debate of whether self-supervised or supervised pre-training leads to better performance and data/computational efficiency, which would not be possible without the invention of a dataset of such a scale.\\n\\n### MATERIAL & METHOD\\n\\nWe constructed an AbdomenAtlas 1.1 dataset comprising 9,262 three-dimensional CT volumes and over 251,323 masks spanning 25 anatomical structures and 7 types of tumors. In addition, we released a suite of supervised pre-trained models (SuPreM) to benefit 3D medical imaging tasks.\\n\\n#### 3.1 EXTENSIVE DATASET: ABDOMEN ATLAS 1.1\\n\\nInteractive segmentation, an integration of AI algorithms and human expertise, was used to create AbdomenAtlas 1.1 in a semi-automatic procedure. We recruited a team of ten radiologists to perform manual annotations to ensure the annotation quality. Given the complexity of 3D data, rather than annotating the entire dataset voxel by voxel, we asked the radiologists to focus on the most important CT volumes and regions therein. In doing so, an importance score for each volume was computed, derived from the uncertainty, consistency, and overlap (Qu et al., 2023). Six junior radiologists revised the annotations predicted by AI under the supervision of four senior radiologists, and in turn, AI improved its predictions by learning from these revised annotations. This interactive procedure continued to enhance the quality of annotations until no major revision was required from the radiologists. Subsequently, four senior radiologists went through the final visualizations for all the annotations, detecting and revising major errors as needed before the dataset was released. Annotation tools employed included a licensed version from Pair and an open-source MONAI Label.\\n\\nEnsuring high-quality annotations is costly and time-consuming, yet it is critical for transfer learning, as quantified in Appendix A.3, and for reducing ambiguity when training AI models for image segmentation.\"}"}
{"id": "AhizIPytk4", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1:\\n\\n| dataset (year) | [source] | # of organ | # of volume | # of center |\\n|----------------|----------|------------|-------------|-------------|\\n| 1. Pancreas-CT (2015) | [link] | 1 | 42 | 1 |\\n| 2. CHAOS (2018) | [link] | 4 | 20 | 1 |\\n| 3. CT-ORG (2020) | [link] | 5 | 140 | 8 |\\n| 4. BTCV (2015) | [link] | 12 | 47 | 1 |\\n| 5. AMOS22 (2022) | [link] | 15 | 200 | 2 |\\n| 6. WORD (2021) | [link] | 16 | 120 | 1 |\\n| 7-12. MSD CT Tasks (2021) | [link] | 9 | 945 | 1 |\\n| 13. LiTS (2019) | [link] | 1 | 131 | 7 |\\n| 14. AbdomenCT-1K (2021) | [link] | 4 | 1,050 | 12 |\\n| 15. KiTS (2020) | [link] | 1 | 489 | 1 |\\n| 16. FLARE\u201923 (2022) | [link] | 13 | 4,100 | 30 |\\n| 17. Trauma Det. (2023) | [link] | 0 | 4,711 | 23 |\\n| 18. AbdomenAtlas 1.0 (2023) | [link] | 9 | 5,195 | 26 |\\n| 19. AbdomenAtlas 1.1 | 25 | 9,262 | 88 |\\n\\n\u2020 Our reported number of CT volumes may differ from original publications, as some CT volumes are reserved for validation purposes.\\n\u2021 The number of CT volumes in AbdomenAtlas 1.1 is lower than the sum of datasets 1\u201317 due to overlaps within these public datasets.\\n\\nAbdomenAtlas 1.1 is a composite dataset that unifies CT volumes from public datasets 1\u201317 as summarized in Table 1. AbdomenAtlas 1.1 presents a level of diversity because the CT volumes are sourced from 88 hospitals worldwide, including pre, portal, arterial, and delayed phases. The gap between these CT volumes includes changes in image quality due to different acquisition parameters, reconstruction kernels, and contrast enhancement, shown in Appendix A.1. Moreover, we provide per-voxel annotations for 25 anatomical structures, including 16 abdominal organs, two thorax organs, five vascular structures, and two skeletal structures. We also provide pseudo annotations for seven types of tumors, namely liver, kidneys, pancreatic, hepatic vessel, lung, colon tumors, and kidney cysts. In total, more than 272.7B voxels are annotated in AbdomenAtlas 1.1, marking a significant leap compared with the 4.3B voxels annotated in the public datasets, amplifying the annotations by a factor of $63.4 \\\\times$ (shown in Appendix Figure 4). The high annotation quality is due to the uniform annotation standards described in Appendix A.2.\\n\\nWe commit to releasing AbdomenAtlas 1.1 to the public. However, this dataset, the largest public per-voxel annotated CT collection by far, accounts for around 0.01% of the CT volumes annually acquired in the United States (Papanicolas et al., 2018). Therefore, cross-institutional collaboration is crucial for accelerating data sharing, annotation, and AI development (Saenz et al., 2024).\\n\\n3.2 A Suite of Pre-Trained Models: SuPreM\\n\\nThe magnitude of our AbdomenAtlas 1.1 is unprecedented in terms of data and annotations. One of the advantages is that it enables us to train AI models in both a supervised and self-supervised manner. At the time this paper is written, neither supervised nor self-supervised pre-training has been performed on this scale of dataset (9,262 volumetric data). We have developed models (termed SuPreM) pre-trained on data and annotations in AbdomenAtlas 1.1, which leverage established CNN backbones, such as U-Net and SegResNet, as well as Transformer backbones, such as SwinUNETR. With the growing trend of using pre-trained models, we have maintained a standardized, accessible model repository for sharing public model weights as well as a suite of supervised pre-trained models (SuPreM) released by us. Releasing pre-trained models should be considered a marked contribution as they offer an alternative way of knowledge sharing while protecting patient privacy (Sellergren et al., 2022; Zhang & Metaxas, 2023; Ma et al., 2023a). In this study, all of the models in SuPreM follow pre-training and fine-tuning configurations as below.\\n\\n3 For supervised pre-training, the largest study to date was by Liu et al. (2023), which was developed on 3,410 (2,100 for training and 1,310 for validation) annotated CT volumes. For self-supervised pre-training, the largest one was by Tang et al. (2022), which was trained on 5,050 unannotated CT volumes. Concurrently, Valanarasu et al. (2023) pre-trained a model on 50K volumes of CT and MRI using self-supervised learning.\"}"}
{"id": "AhizIPytk4", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2:\\n\\nContribution #2: A suite of pre-trained models (termed SuPreM) comprising several widely recognized AI models. We provide pre-trained AI models based on CNN, Transformer, and their hybrid versions, and more AI models will be added. Each model was supervised pre-trained on large datasets and per-voxel annotations from AbdomenAtlas 1.1. Compared with learning from scratch and publicly available models, fine-tuning the models in SuPreM consistently achieves state-of-the-art organ and tumor segmentation performance on two datasets. All of the results, including the mean and standard deviation (mean \u00b1 s.d.) across ten trials. In addition, we have further performed an independent two-sample t-test between learning from scratch and fine-tuning models in our SuPreM. The performance gain is statistically significant at the P = 0.05 level, with highlighting in a light red box. Detailed per-class performance can be found in Appendix \u00a7B.1.\\n\\n| Model             | TotalSegmentator v1 | proprietary dataset | Organ | Muscle | Cardiac | Organ | Muscle | Cardiac |\\n|-------------------|---------------------|---------------------|-------|--------|---------|-------|--------|---------|\\n| U-Net (2015)      | 88.9 \u00b1 0.6          | 92.9 \u00b1 0.4          | 88.8 \u00b1 0.7 | 85.6 \u00b1 0.5 | 69.8 \u00b1 1.2 | 38.1 \u00b1 1.1 |\\n| Zhou et al. (2019b) | 87.8  | 90.1  | 86.3  | 80.1  | 65.5  | 36.9  |\\n| Chen et al. (2019b) | 86.9  | 91.4  | 87.4  | 79.0  | 66.2  | 36.7  |\\n| Xie et al. (2022)  | 88.5  | 92.9  | 89.0  | -     | -     | -     |\\n| Zhang et al. (2021) | 89.3  | 93.8  | 89.1  | 85.7  | 72.7  | 38.3  |\\n| SuPreM            | 92.1 \u00b1 0.3          | 95.4 \u00b1 0.1          | 92.2 \u00b1 0.3 | 90.8 \u00b1 0.2 | 76.2 \u00b1 0.8 | 70.5 \u00b1 0.5 |\\n| Swin UNETR (2021) | 86.4 \u00b1 0.5          | 88.8 \u00b1 0.5          | 84.5 \u00b1 0.6 | 77.3 \u00b1 0.9 | 65.9 \u00b1 1.7 | 35.5 \u00b1 1.4 |\\n| Tang et al. (2022) | 89.3  | 93.8  | 88.3  | 87.9  | 72.5  | 38.9  |\\n| Liu et al. (2023)  | 89.7  | 94.1  | 89.4  | 89.1  | 74.6  | 67.6  |\\n| SuPreM            | 91.3 \u00b1 0.3          | 94.6 \u00b1 0.2          | 90.3 \u00b1 0.3 | 90.4 \u00b1 0.7 | 75.9 \u00b1 1.2 | 69.8 \u00b1 0.9 |\\n| SegResNet (2019)  | 88.6 \u00b1 0.5          | 91.3 \u00b1 0.4          | 89.8 \u00b1 0.4 | 80.6 \u00b1 0.8 | 67.0 \u00b1 1.4 | 36.0 \u00b1 1.3 |\\n| SuPreM            | 91.3 \u00b1 0.5          | 94.0 \u00b1 0.1          | 91.3 \u00b1 0.5 | 86.6 \u00b1 0.3 | 73.7 \u00b1 1.0 | 67.9 \u00b1 0.8 |\\n\\nTo perform a fair and rigorous comparison, we benchmarked with public pre-training methods by pre-training SuPreM using 2,100 CT volumes (same as Liu et al. (2023) and fewer than Tang et al. (2022)) in Tables 2, 4 and Figures 1, 2b, 3. Then, we scaled up the number of CT volumes for pre-training to 9,262 CT volumes to perform direct inference in Table 3. Lastly, we scaled down the number of CT volumes to 21 to explore the edge of our SuPreM in Figure 2a. All these pre-trained models and configurations have been summarized in Appendix Table 8. The best-performing model was selected based on the highest average DSC score over 32 classes on a validation set of 1,310 CT volumes. Implementation details of both pre-training and fine-tuning can be found in Appendix B.2.\\n\\nThe transfer learning ability is assessed by segmentation performance on two datasets, i.e., TotalSegmentator v1 and a proprietary dataset. Benchmarking results in Table 2 indicate that, in comparison with learning from scratch and with existing public models, those fine-tuned from our SuPreM consistently attain superior organ, muscle, cardiac, and gastro segmentation performance on both datasets. U-Net, as a simple and lightweight segmentation backbone, still performs competitively compared with alternative choices like Swin UNETR. This observation is aligned with the majority of the medical imaging community (Isensee et al., 2021; Eisenmann et al., 2023), suggesting that more exploration is needed for advancing segmentation backbones. Moreover, in the scenarios of either small data regimes shown in Figure 1 or large data regimes shown in Appendix Figure 7a\u2013d, supervised models transfer better than their self-supervised counterparts. In summary, our SuPreM surpasses all existing 3D pre-trained models by a large margin in transfer learning performance, irrespective of their pre-training methodologies or data sources.\"}"}
{"id": "AhizIPytk4", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For a fair comparison, both supervised (in red) and self-supervised (in gray) models use Swin UNETR as the backbone, and the compared self-supervised pre-training is the current state of the art (Tang et al., 2022). The target task was on TotalSegmentator v1.\\n\\n(a) scales the model transfer learning ability when pre-trained on varying numbers of images. The results indicate a consistent improvement in transfer learning ability when pre-training on more images. The model trained with 21 CT volumes, 672 masks, and 40 GPU hours shows a transfer learning ability similar to that trained with 5,050 CT volumes and 1,152 GPU hours. Specifically, supervised pre-training is more efficient, requiring 99.6% fewer data and 96.5% less computation.\\n\\n(b) assesses the annotation & learning efficiency by fine-tuning models on different numbers of annotated CT volumes from TotalSegmentator. Specifically, SuPreM, fine-tuned on 512 per-voxel annotated CT volumes, can achieve a segmentation performance on par with self-supervised models fine-tuned on 1,024 volumes, reducing 50% manual annotation cost for target tasks.\\n\\n4 EXPERIMENT & ANALYSIS\\n\\n4.1 DATA, ANNOTATION, AND COMPUTATIONAL EFFICIENCY\\n\\nSummary. We demonstrate the remarkable efficiency: (1) SuPreM trained with 21 CT volumes, 672 masks, and 40 GPU hours shows transfer learning ability similar to that trained with 5,050 CT volumes and 1,152 GPU hours. (2) SuPreM requires 50% fewer manual annotations for organ/tumor segmentation than self-supervised pre-training.\\n\\nData efficiency for pre-training. As shown in Figure 2a, supervised pre-training requires less data (21 vs. 5,050 CT volumes) for the pretext task than self-supervised pre-training. This discrepancy arises from the inherent differences in their learning learning objectives and the information they leverage. Supervised pre-training benefits from explicit annotations, which provide direct guidance for the task, i.e., segmentation in this study. The model learns features from both data and annotations, which offer strong and precise supervision. On the other hand, self-supervised learning relies on pretext tasks derived from the raw data, which may offer a more ambiguous learning signal, therefore requiring more examples to capture meaningful features. Importantly, our finding suggests that supervised pre-training is more scalable with increased data. When data are increased from 21 to 1,575 volumes, the transfer learning performance on TotalSegmentator improves from 90.4% to 91.3%. In comparison, for self-supervised pre-training, an increase in data from 1,000 to 5,050 volumes only marginally improves performance from 89.7% to 89.9%. Therefore, supervised pre-training requires significantly less data than self-supervised and is more scalable and effective with increased data.\\n\\nAnnotation efficiency for fine-tuning. We have assessed the annotation efficiency by fine-tuning SuPreM and self-supervised models (Tang et al., 2022) on the TotalSegmentator dataset. Figure 2b suggests that fine-tuning SuPreM can reduce annotation costs for the segmentation task by 50%, averaged over the classes that were not used for pre-training (per-class performance can be found in Appendix Figure 8a\u2013d). Specifically, SuPreM fine-tuned on 512 per-voxel annotated CT volumes can achieve segmentation performance similar to Tang et al. (2022) fine-tuned on 1,024 annotated CT volumes. The fine-tuning performance improvement gets bigger when the number of annotated\"}"}
{"id": "AhizIPytk4", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We conduct external validation across four hospitals worldwide. Specifically, our SuPreM\u2014trained on 9,262 CT volumes\u2014is directly inferred on three external datasets, i.e., TotalSegmentator (representing the Central European population from Switzerland; one hospital), DAP Atlas (the Central European population from Germany; two hospitals), and the proprietary dataset (the North American population from the United States; one hospital) measured by DSC scores. For every dataset, we compare the out-of-distribution (OOD) performance obtained by SuPreM with independently and identically distributed (IID) performance obtained by AI models directly trained on that specific dataset, which are often considered as upper bound performance in domain transfer literature. We find that SuPreM can be generalized well across external datasets without additional fine-tuning, yielding comparable or even superior performance to the IID counterparts, evidenced by the one-sample \\\\( t \\\\)-test results. Appendix D.1 provides visual examples of anatomical structure segmentation.\\n\\n| Class          | SuPreM (OOD) | SuPreM (OOD) | SuPreM (OOD) | SuPreM (OOD) |\\n|----------------|--------------|--------------|--------------|--------------|\\n|                | Liu et al. (IID) | Jaus et al. (IID) | Wang et al. (IID) |                |\\n| spleen         | 96.0\u00b10.0     | 93.6         | 96.8\u00b10.0     | 96.8\u00b10.0     |\\n| kidney right   | 93.3\u00b10.1     | 94.1         | 96.3\u00b10.1     | 95.3\u00b10.0     |\\n| kidney left    | 91.2\u00b10.2     | 87.7         | 96.4\u00b10.1     | 97.4\u00b10.3     |\\n| gall bladder   | 81.8\u00b10.3     | 73.9         | 87.6\u00b10.4     | 85.4         |\\n| liver          | 96.4\u00b10.1     | 96.8         | 97.3\u00b10.1     | 98.5\u00b10.3     |\\n| stomach        | 87.3\u00b10.3     | 89.2         | 95.3\u00b10.2     | 93.6         |\\n| aorta          | 80.8\u00b10.4     | 90.7         | 90.7\u00b10.5     | 97.7\u00b10.3     |\\n| postcava       | 77.9\u00b10.3     | 82.1         | 89.1\u00b10.4     | 77.7\u00b10.4     |\\n| pancreas       | 84.6\u00b10.2     | 80.8         | 90.6\u00b10.2     | 79.0         |\\n\\nAverage: 87.7\u00b10.2\\n\\nCT volumes is limited in the target task (e.g., 64, 128, 256). In addition, similar levels of annotation efficiency (reduced 50% cost) are observed when fine-tuning SuPreM on the three-class tumor segmentation task using the proprietary dataset, as presented in Appendix Figure 8e\u2013g. Computational efficiency for both pre-training and fine-tuning. This efficiency stems, in part, from the reduced data requirements inherent to supervised pre-training, as discussed above. As shown in Figure 2a, supervised pre-training only needs 40 GPU hours to achieve a transfer learning performance comparable to that of self-supervised pre-training, which requires 1,152 GPU hours\u2014a factor increase of 28.8\u00d7. When fine-tuning on target tasks, such as on a 10% subset of TotalSegmentator in Appendix Figure 9, the supervised pre-trained model converges much faster than the self-supervised one, reducing the GPU hours needed from 60 to 20. This implies that image features learned by supervised pre-training are intrinsically more expressive, enabling the model to seamlessly adapt across a myriad of 3D image segmentation tasks with minimal annotated data for fine-tuning. This computational efficiency makes supervised pre-training a compelling choice for 3D image segmentation without compromising model performance, especially when the large, annotated dataset is available.\"}"}
{"id": "AhizIPytk4", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Fine-tuning SuPreM on 66 novel classes. Following the standard transfer learning paradigm, we fine-tune our SuPreM on the segmentation task of novel classes. These tasks include segmenting 19 muscles, 15 cardiac structures, 5 organs, and 24 vertebrae from TotalSegmentator, as well as three fine-grained pancreatic tumor types from the proprietary dataset. It is important to note that these classes were not part of the pre-training of SuPreM. We observe that SuPreM, supervised pre-trained on only a few classes, can transfer better than those self-supervised pre-trained on raw, unlabeled data measured by DSC scores (per-class results in Appendix D.3). In other words, it is the task of segmentation itself that can enhance the model's capability of segmenting novel-class objects. This benefit is much more straightforward and understandable than such self-supervised tasks as contextual prediction, mask image modeling, and instance discrimination in the context of transfer learning. We hypothesize that it is because the model learns to understand the concept of objectness in a broader sense through full supervision, as suggested by Kirillov et al. (2023), but this certainly deserves further exploration. In addition, an independent two-sample t-test was performed between the self-supervised pre-trained model and the supervised pre-trained model.\\n\\n| Class Type   | Supervised Pre-trained | Self-supervised Pre-trained | p-value  |\\n|--------------|------------------------|----------------------------|----------|\\n| Humeral Left  | 92.8 \u00b1 0.7             | 93.2 \u00b1 0.3                 | ns       |\\n| Vertebra L5  | 94.1 \u00b1 0.2             | 95.7 \u00b1 0.3                 | **p < 0.0001** |\\n| Humeral Right | 87.5 \u00b1 1.0             | 95.0 \u00b1 0.5                 | **p < 0.0001** |\\n| Vertebra L4  | 90.4 \u00b1 0.6             | 93.0 \u00b1 0.5                 | **p < 0.0001** |\\n| Iliopsoas Left | 84.4 \u00b1 0.3              | 85.7 \u00b1 0.3                 | **p < 0.0001** |\\n| Vertebra C2  | 86.8 \u00b1 2.0             | 91.8 \u00b1 0.2                 | **p < 0.0001** |\\n| Iliopsoas Right | 87.4 \u00b1 0.3            | 88.7 \u00b1 0.2                | **p < 0.0001** |\\n| Vertebra C1  | 87.1 \u00b1 0.8             | 87.4 \u00b1 0.8                 | ns       |\\n| Average (muscle) | 93.9 \u00b1 0.1          | 94.3 \u00b1 0.1                 | **p < 0.0001** |\\n| Average (vertebrae) | 86.4 \u00b1 0.3        | 89.2 \u00b1 0.2                 | **p < 0.0001** |\\n| Trachea      | 93.4 \u00b1 0.1             | 93.4 \u00b1 0.1                 | ns       |\\n| Heart Myocardium | 88.9 \u00b1 0.2          | 89.8 \u00b1 0.2                 | **p < 0.0001** |\\n| \u2022 (11 more classes) | \u2022 (20 more classes) | \u2022 (15 more classes) | \u2022 (11 more classes) |\\n| PDAC         | 53.3 \u00b1 0.4             | 53.6 \u00b1 0.3                 | *p < 0.05* |\\n| Urinary Bladder | 90.5 \u00b1 0.9            | 91.5 \u00b1 0.9                | *p < 0.05* |\\n| Cyst         | 41.5 \u00b1 0.3             | 49.4 \u00b1 0.3                 | **p < 0.0001** |\\n| Pulmonary Artery | 89.0 \u00b1 0.9           | 92.0 \u00b1 0.2                | **p < 0.0001** |\\n| PanNet       | 35.5 \u00b1 0.8             | 46.0 \u00b1 0.5                 | **p < 0.0001** |\\n| Average (cardiac) | 88.9 \u00b1 0.1          | 90.7 \u00b1 0.1                 | **p < 0.0001** |\\n| Average (tumor) | 43.4 \u00b1 0.3            | 49.7 \u00b1 0.2                 | **p < 0.0001** |\\n\\nThe robustness required to accommodate the variations present in novel datasets. We conduct external validation on several novel datasets sourced from Switzerland and East Asia to challenge the AI model on the data distribution that it has not encountered during the training. This result is referred to as out-of-distribution (OOD) performance. For comparison, we also collect the result achieved by dataset-specific AI models\u2014those individually trained on the specific datasets\u2014referred to as independently and identically distributed (IID) performance. As shown in Table 3, our SuPreM can be generalized well to novel data distribution without the need for further fine-tuning or adaptation, consistently offering OOD performance that matches or even exceeds that of its IID counterparts.\\n\\nFine-tuning on novel classes. The value of transfer learning lies in fine-tuning the pre-trained models on novel scenarios (Zhou et al., 2021b), such as novel classes, image modalities, and vision tasks that are completely unseen during the pre-training. In this study, we evaluate the proficiency of SuPreM when transferred to a wide variety of novel classes for 3D image segmentation tasks. These novel classes include 19 muscles, 15 cardiac structures, 5 organs, and 24 vertebrae from the TotalSegmentator dataset, as well as three fine-grained pancreatic tumor types from the proprietary dataset. As shown in Table 4, our SuPreM, supervised pre-trained on 25 classes, can transfer better to novel classes than those self-supervised models pre-trained on raw, unlabeled data. We find that the pretext task of segmentation itself can enhance the model capability of segmenting novel classes. The benefit of same-task transfer learning, i.e., segmentation as pretext and target tasks, is much more straightforward and understandable than other pretext tasks such as contextual prediction, mask image modeling, and instance discrimination. Through full supervision in segmentation tasks, the model learns to understand the concept of objectness, wherein the model gains a more profound understanding of what characterizes an object. The model does not just recognize predefined objects but begins to understand the foundational factors of objects in general. Such factors include texture, boundary, shape, size, and other low-level visual cues that are often deemed essential for image segmentation. This resonates with our assertion in the introduction: just as classification-based...\"}"}
{"id": "AhizIPytk4", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Fine-tuning SuPreM on fine-grained tumor classification. We plot receiver operating characteristic (ROC) curves to evaluate the transfer learning performance of tumor classification. Detecting Cysts and PanNETs raises additional challenges for AI because these lesions exhibit a greater variety of texture patterns than PDACs. This diversity in texture patterns is reflected in the values of the Area Under the Curve (AUC) that we obtained. For all three sub-types of pancreatic tumors, SuPreM (in red) demonstrates superior performance over the self-supervised model (Tang et al., 2022) (in gray), showcasing its effectiveness in fine-grained tumor classification.\\n\\nFeatures from ImageNet transfer optimally for classification tasks (Huh et al., 2016; He et al., 2019; Zoph et al., 2020; Ridnik et al., 2021), while segmentation-based features are optimal for segmentation tasks. Our findings do not negate the value of self-supervised pre-training. With 9,262 CT volumes, should self-supervised pre-training outperform supervised pre-training in model transferability in the future, its value will be further highlighted by eliminating the need for manual annotations.\\n\\nFine-tuning on novel tasks. We have investigated the cross-task transfer learning ability of SuPreM between organ segmentation and fine-grained tumor classification. The distance between the two tasks is much larger than transferring among segmentation tasks. It is challenging to benchmark fine-grained tumor classification, particularly due to the scarcity of annotations in public datasets (often limited to hundreds of tumors). To overcome this limitation, we employed our proprietary dataset (Xia et al., 2022), which comprises 3,577 annotated pancreatic tumors, including detailed sub-types: 1,704 PDACs, 945 Cysts, and 928 PanNETs. This extensive dataset enabled us to thoroughly assess the transfer learning ability of SuPreM in tumor-related tasks. Figure 3 shows that supervised models (SuPreM) transfer better to target classification tasks than self-supervised models (Tang et al., 2022), leading to improved Area Under the Curve (AUC) for identifying each tumor type. Notably, the transfer learning results detailed in Appendix D.4 reveal a sensitivity of 86.1% and specificity of 95.4% for PDAC detection. This performance surpasses the average radiologist's performance in PDAC identification by 27.6% in sensitivity and 4.4% in specificity, as reported in Cao et al. (2023). Moreover, Appendix Figure 8 shows that SuPreM requires 50% fewer manual annotations for fine-grained tumor classification than self-supervised pre-training. This is particularly critical for tumor imaging tasks because annotating tumors requires much more effort and often relies on the availability of pathology reports.\\n\\nConclusion and Discussion\\nThis study examines the transfer learning ability of supervised models that are pre-trained on 3D annotated datasets and fine-tuned on 3D image segmentation tasks. We start by constructing AbdomenAtlas 1.1, an extensive collection of 9,262 three-dimensional CT volumes with high-quality, per-voxel annotations. The magnitude of this dataset is unprecedented regarding data volume (2,789,975 images), granularity of annotations (251,323 masks), and inclusive diversity (88 hospitals). This dataset facilitates the development of a suite of pre-trained models, termed SuPreM, that can be effectively transferred to a broad spectrum of 3D image segmentation tasks. Notably, SuPreM transfers better than all existing 3D models by a large margin, especially when transferred to under-annotated datasets. The model trained with 21 CT volumes, 672 masks, and 40 GPU hours shows a transfer learning ability similar to that trained with 5,050 CT volumes and 1,152 GPU hours, highlighting the remarkable efficiency of supervised pre-training. We also demonstrate that the learned features can directly infer effectively on external datasets and fine-tune to segment novel classes and classify multiple types of tumors with higher accuracy and less annotated data than those learned by self-supervision.\"}"}
{"id": "AhizIPytk4", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGMENTS\\n\\nThis work was supported by the Lustgarten Foundation for Pancreatic Cancer Research and the Patrick J. McGovern Foundation Award. This work has utilized the GPUs provided partially by ASU Research Computing and NVIDIA. We appreciate the effort of the MONAI Team to provide open-source code for the community. We thank Chongyu Qu, Yixiong Chen, Junfei Xiao, Jie Liu, Yucheng Tang, Tiezheng Zhang, Yaoyao Liu, Chen Wei, Fengrui Tian, Yu-Cheng Chou, Angtian Wang, and Dora Zhiyu Yang for their constructive suggestions at several stages of the project.\\n\\nREFERENCES\\n\\nMichela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Bennett A Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald M Summers, Bram van Ginneken, et al. The medical segmentation decathlon. arXiv preprint arXiv:2106.05735, 2021.\\n\\nYutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei A Efros. Sequential modeling enables scalable learning for large vision models. arXiv preprint arXiv:2312.00785, 2023.\\n\\nRob Baxter, Thomas Nind, James Sutherland, Gordon McAllister, Douglas Hardy, Ally Hume, Ruairidh MacLeod, Jacqueline Caldwell, Susan Krueger, Leandro Tramma, et al. The scottish medical imaging archive: 57.3 million radiology studies linked to their medical records. Radiology: Artificial Intelligence, pp. e220266, 2023.\\n\\nPatrick Bilic, Patrick Ferdinand Christ, Eugene V orontsov, Grzegorz Chlebus, Hao Chen, Qi Dou, Chi-Wing Fu, Xiao Han, Pheng-Ann Heng, J\u00fcrgen Hesser, et al. The liver tumor segmentation benchmark (lits). arXiv preprint arXiv:1901.04056, 2019.\\n\\nVictor Ion Butoi, Jose Javier Gonzalez Ortiz, Tianyu Ma, Mert R Sabuncu, John Guttag, and Adrian V Dalca. Universeg: Universal medical image segmentation. arXiv preprint arXiv:2304.06131, 2023.\\n\\nKai Cao, Yingda Xia, Jiawen Yao, Xu Han, Lukas Lambert, Tingting Zhang, Wei Tang, Gang Jin, Hui Jiang, Xu Fang, et al. Large-scale pancreatic cancer detection via non-contrast ct and deep learning. Nature Medicine, pp. 1\u201311, 2023.\\n\\nKrishna Chaitanya, Ertunc Erdil, Neerav Karani, and Ender Konukoglu. Contrastive learning of global and local features for medical image segmentation with limited annotations. arXiv preprint arXiv:2006.10511, 2020.\\n\\nLiang Chen, Paul Bentley, Kensaku Mori, Kazunari Misawa, Michitaka Fujiwara, and Daniel Rueckert. Self-supervised learning for medical image analysis using image context restoration. Medical image analysis, 58:101539, 2019a.\\n\\nSihong Chen, Kai Ma, and Yefeng Zheng. Med3d: Transfer learning for 3d medical image analysis. arXiv preprint arXiv:1904.00625, 2019b.\\n\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.\\n\\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33:22243\u201322255, 2020b.\\n\\nErrol Colak, Hui-Ming Lin, Robyn Ball, Melissa Davis, Adam Flanders, Sabeena Jalal, Kirti Magudia, Brett Marinelli, Savvas Nicolaou, Luciano Prevedello, Jeff Rudie, George Shih, Maryam Vazirabad, and John Mongan. Rsna 2023 abdominal trauma detection, 2023. URL https://kaggle.com/competitions/rsna-2023-abdominal-trauma-detection.\\n\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 248\u2013255. IEEE, 2009.\"}"}
