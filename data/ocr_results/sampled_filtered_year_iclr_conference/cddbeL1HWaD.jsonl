{"id": "cddbeL1HWaD", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Instead of using convolutional layers as in Kapturowski et al. (2018), we use fully-connected neural networks with a recurrent component as our neural network model. Hence, we flatten the observation from the environment into a vector by first fattening the 3-channel tensor and then concatenate it with the role-based information (i.e., goal encoding or communication token). Precisely, the input is first processed by an LSTM layer to handle partial observability. Then, it is followed by a two-layer and two-headed fully-connected neural network of hidden size 128. The two heads are used to compute the action-value function and the advantage function respectively. All neural network components are implemented using the neural network library PyTorch (Paszke et al., 2019), with the weights initialized using Xavier initialization. In terms of training, we use the Adam optimizer to train our models (Kingma & Ba, 2014). Training was done in an internal cluster with a mix of GTX 1080 and RTX 2080 GPUs.\\n\\nHyperparameters\\n\\nThis section covers details in setting hyperparameters for various methods used in this work. They are covered in two separate sections depending on whether a parameter is common to all used methods or not.\\n\\nG.1 Common Parameters\\n\\nTo determine the best parameters that are common to all the methods used, we performed a hyperparameter sweep over some key common parameters. Specifically, the search was performed on the learning rate and target network update frequency in the lists of $[0.01, 0.001, 0.0001, 0.00001]$ and $[50, 100, 200, 500]$. The results are averaged over 3 random seeds, each trained for 25000 episodes.\\n\\nThe best set of parameters for learning rate and target network update frequency are $0.0001$ and $100$, respectively. Other common parameters are set to values in table G.1.\\n\\nG.2 Method-Specific Parameters\\n\\nMethod-specific parameters are set to values in table 3.\\n\\nEnvironment Parameters\\n\\nParameters of the environments used in the experiments are summarized in Table 4.\"}"}
{"id": "cddbeL1HWaD", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Configurable parameters for the PBMaze\\n\\n| Hyperparameter       | Value          |\\n|----------------------|----------------|\\n| Discount Factor \u03b3    | 0.99           |\\n| Batch Size           | 32             |\\n| Replay Buffer Size   | 10000          |\\n| Temperature          | 1.0            |\\n| Non-Linearity        | ReLU           |\\n\\nTable 2: Common parameters used across algorithms\\n\\nI LIMITATIONS\\n\\nWhile our proposed framework provides a promising approach to learn where to communicate (i.e., CTD/CTU) by first discovering cheap talk channels and subsequently learning how to use them, it is important to be aware of the assumptions and limitations of the framework for future work.\\n\\nTo begin with, our approach assumes access to an environment simulator and a (perfect) belief to perform MI computation and OBL respectively. While there are recent approaches in learning environment models (Wang et al., 2022) and belief models (Hu & Foerster, 2019), how well these learned components work with each other remains unexplored. Advances in these areas would improve the overall robustness of our proposed framework.\"}"}
{"id": "cddbeL1HWaD", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Another crucial assumption made is the direct and immediate causal relationship between a communicative action and a receiver's observation which might be less reliable in the real-world setting. For instance, there will be delay and transmission time when sending messages. In more visually-rich and open-ended settings, there could also be many confounding changes in the receiver's observation when communication actions are taken. In these cases, further advances are needed to correctly assign credits to communicative actions when computing MI.\\n\\nLast but not least, the transition from CTD to CTU is determined empirically from experiments. A more principled approach to automate this decision would make method more robust. For instance, based on Definition 4.3, a method could be developed to indicate whether an agent has been able to discover a channel consistently (i.e., a discovery policy with maximized PMI). Once a threshold is reached, it can move on to the utilization stage.\\n\\n| Parameter | IQL | IQL + IR | OBL | CTDL/CTDUL |\\n|-----------|-----|---------|-----|------------|\\n| Starting $\\\\epsilon$ | 1.0 | N/A | N/A | 1.0 |\\n| $\\\\epsilon$ Decay Step | 0.00001 | 0.00001 | N/A | 0.00001 |\\n| Minimum $\\\\epsilon$ | 0.1 | 0.1 | N/A | 0.1 |\\n| Initial Exploration Step | 1000 | 1000 | 1000 | 1000 |\\n| $\\\\alpha$ Entropy Factor | N/A | N/A | N/A | 0.0 |\\n| $\\\\beta$ Mutual Information Reward Factor | N/A | N/A | N/A | 2.0 |\\n| $\\\\kappa$ Mutual Information Loss Factor | N/A | N/A | N/A | 1.0 |\\n| Learning Rate | N/A | N/A | N/A | 0.0005 |\\n| Critic Learning Rate | N/A | N/A | N/A | 0.0001 |\\n| Critic Hidden Size | N/A | N/A | N/A | 32 |\\n| Hypernetwork Hidden Size | N/A | 64 | N/A | N/A |\\n| Number of Hyperlayers | N/A | 1 | N/A | N/A |\\n| Social Influence Reward Coefficient | N/A | N/A | N/A | 5.0 |\\n| Temperature | 1.0 | 1.0 | 0.1 | 1.0 |\"}"}
{"id": "cddbeL1HWaD", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"With the rapid progress in MARL and multi-agent learning systems in general, we confidently expect that more multi-agent learning systems will be deployed in the real world in which communication will play an integral part to their successes. Our formulation and method for learning where to communicate would lead to more robust communication and subsequently more coordinated behavior among agents by equipping them with the ability to discover the best and most reliable places to communicate. This is essential in such real-world applications when the reliability of communication often varies by locations and situations.\\n\\nBy increasing the applicability of multi-agent learning systems, our work may exacerbate some risks of deploying machine learning systems like increasing unemployment (e.g., replacing warehouse workers with a fully autonomous warehouse) and advancing automated weaponry. Particularly to our method, by equipping systems with the ability to discover where to communicate, it could encourage adversarial attacks to deployed machine learning systems through communication which would lead to unintended and potentially harmful actions and disrupt learned communication protocols.\\n\\nFor future work, we are interested in testing our framework in more complex settings and address some limitations discussed in Appendix I. For instance, the ability to discover communication channels can be formulated as skills and organized in a hierarchical manner to solve harder tasks. It would also be insightful to benchmark different algorithms for cheap talk utilization in this setting.\"}"}
{"id": "cddbeL1HWaD", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"S and for taking an action in A. Hence, a policy learned only with MI reward is incentivized to reach S without necessarily using communicative actions. Ideally, \\\\( \\\\mathcal{I}(A_1, O_2) \\\\) is maximized when a policy favors the communicative actions equally when in S.\\n\\nTo learn a policy that does maximize \\\\( \\\\mathcal{I}(A_1, O_2) \\\\), we propose an MI loss function, using the parameterized policy \\\\( \\\\pi_\\\\theta \\\\). The additional term directly maximizes \\\\( \\\\mathcal{I}(A_1, O_2) \\\\), which is maximized when the policy takes actions in A more. For each iteration \\\\( i \\\\), the loss function is expressed as:\\n\\n\\\\[\\nL_i(\\\\theta_i) = L_{OBL_i}(\\\\theta_i) - \\\\kappa \\\\mathcal{I}(A_1, O_2; \\\\pi_\\\\theta_i)\\n\\\\]\\n\\nwhere \\\\( L_{OBL_i}(\\\\theta_i) \\\\) (see Appendix B), \\\\( \\\\mathcal{I}(A_1, O_2; \\\\pi_\\\\theta_i) \\\\) and \\\\( \\\\kappa \\\\) are OBL loss, MI loss at iteration \\\\( i \\\\), and a hyperparameter respectively. We add a minus sign so that minimizing the loss maximizes the MI term. The combination of the MI reward and loss should allow the discovery of cheap talk channels with greater MI. We denote our discovery method as cheap talk discovery learning (CTDL).\\n\\n5.1.2 DISCOVERING CHANNELS WITHOUT CONVENTIONS FORMATION\\n\\nDuring discovery learning, agents should simply learn where the channels are without forming any protocols (i.e., conventions) on how to use them. This is because the protocols formed would affect the rewards received. To do so, we employ OBL as our base algorithm. Particularly, it has appealing theoretical properties which are beneficial for this setting, as it theoretically guarantees to converge to a policy without any conventions (Hu et al., 2021). This means OBL with CTDL would learn a policy that discovers the channels while not having a preference over any communicative actions, which would allow more flexible adaptation when properties of channels alter. Hence, though not explored here, better performance in ZSC can be expected.\\n\\nFor our example environment, the agent should prefer the actions \\\\( \\\\text{Hint Up} \\\\) and \\\\( \\\\text{Hint Down} \\\\) equally after cheap talk discovery. If the agents are in a slightly different environment with the phone booths sending negated versions of the messages an agent sends, a policy without conventions should adapt quicker given that it has no preference over communicative actions.\\n\\n5.2 CHEAP TALK UTILIZATION\\n\\nWith channels discovered, agents are ready to learn how to use them through protocol formation (i.e., form a consensus of how messages are interpreted). Many off-the-shelf algorithms can be used for CTU. Here, we use DIAL (Foerster et al., 2016), which has been shown to learn protocols efficiently. Note that OBL will be replaced by independent Q-learning (Tan, 1993, IQL) updates to allow protocol formation in CTU.\\n\\nGiven that the original DIAL requires feedforwarding through an episode\u2019s full trajectory to maintain the gradient chain, modifications to DIAL are needed to work in this setting for two reasons: First, the base learning algorithm is an experience-replay-based method that learns from transition batches, not trajectories. Second, unlike in the original DIAL which messages are sent in every step \\\\( t \\\\), the messages can only be sent successfully when the environment state is in S. Hence, direct gradient connections only happen occasionally in an episode.\\n\\nTo make DIAL compatible with this setting, we use a separate replay buffer to keeps track of the transitions and only consider a trajectory when messages are sent successfully. The buffer stores full episodes with flags indicating which transitions to allow direct gradient connections. We denote our discovery method with DIAL as cheap talk discovery and utilization (CTDUL). The transition from CTD to CTU is a task-dependent hyperparameter and is determined empirically. Appendix C and D shows how gradients flow in CTDUL and the corresponding pseudocode.\\n\\n6 EXPERIMENTAL SETUP\\n\\nAs part of our contribution, we designed a configurable environment with discrete state and action space - the phone booth maze to evaluate agents in the CTD/CTU setting, as described in Section 1.\\n\\nTo enable extensive investigations, the environment allows phone booths of configurable properties including usage cost and noise level, and decoy booths that are not functional. The agents\u2019 observation...\"}"}
{"id": "cddbeL1HWaD", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"contains view information of the room an agent is in and role-specific information (e.g. the sender and receiver have goal information and communication token respectively). Agents have the environment actions of \\\\textit{UP}, \\\\textit{DOWN}, \\\\textit{LEFT}, \\\\textit{RIGHT}, and \\\\textit{NO-OP}. There are two communicative actions, \\\\textit{HINT-UP} and \\\\textit{HINT-DOWN} which only has an effect when agents are in the functional phone booths, updating the receiver's communication token to a fixed value. Rewards of 1.0 and -0.5 are given for choosing a correct exit and an incorrect exit respectively with no reward given otherwise. By changing the phone booths' properties, we can vary their capacities, measured by the MI reward. The left of Figure 2 shows the MI of each location in the sender's room in two different environment configurations, in which one has only one functional booth and the other has three booths with one noisy booth. These are computed based on equation 5 on all possible combinations of the sender and receiver locations. We use two configurations of the environment for our experiments, which we name Single Phone Booth Maze (SPBMaze) and Multiple Phone Booth Maze (MPBMaze). Please see Appendix E for environment visualizations and design features, and Appendix H for environment parameters used.\\n\\nArchitecturally, we use the R2D2 architecture (Kapturowski et al., 2018) with a separate communication head like in C-Net (Foerster et al., 2016). For baselines, state-of-the-art methods are used including IQL, OBL (Hu et al., 2021), QMIX (Rashid et al., 2018), MAA2C (Papoudakis et al., 2021) and IQL with social influence reward (Jaques et al., 2019, SI).\\n\\nWe trained all methods for 12000 episodes (80000 episodes for CTU) and evaluated on test episodes every 20 episodes by taking the corresponding greedy policy. Each algorithm reports averaged results from 4 random seeds with standard error. We performed a hyperparameter sweep over common hyperparameters, fixing them across all methods, and specific sweeps for method-specific parameters. Please see Appendix F and G for training and hyperparameter details.\\n\\n7 RESULTS\\n\\n7.1 DISCOVERING CHEAP TALK CHANNELS\\n\\nFigure 2: Left: MI reward for each location in the sender's room of SPBMaze and MPBaze to show phone booths of different properties. Values are computed by iterating over all possible receiver's position for each sender's position. Senders start at positions with green borders. The position with purple border is the costly phone booth which incurs a cost to use. The top phone booth in MPBaze has a noise factor of 0.5, i.e., lower MI. Right: CTD performance for CTDL and baselines. The horizontal grey line indicates the optimal number of step to reach the booth. For the ease of plotting, when an agent cannot discover the booth, we set the step as the episode length.\\n\\nTo quantify the performance in CTD, we use the number of steps it takes for the sender to first reach the functional phone booth in SPBMaze. The fewer the number of steps used, the better an algorithm is at CTD. See Table 4 for detailed configuration of SPBMaze.\\n\\nFigure 2 (Right) shows the performances of our proposed method CTDL and the baselines. CTDL, using MI reward and loss, discovers the functional phone booth quickly in optimal number of steps (as shown by the horizontal grey line). Meanwhile, without the proper incentives, all the baseline methods cannot discover the functional phone booth within the episode limit.\\n\\nEven though SI uses a reward term based on MI like ours (i.e., the influence an agent's message has on another agent's action (Jaques et al., 2019)), it becomes insufficient to discover cheap talk\"}"}
{"id": "cddbeL1HWaD", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"channels in this setting as the messages sent only has an effect in a small subset of the state space. Note that without discovering the channels, agents will not be able to learn how to communicate. Hence, we expect the baseline methods cannot leverage communication channels to solve tasks.\\n\\n7.2 Learning Policies with Maximized Mutual Information\\n\\nFigure 3: Algorithms\u2019 sender policy when both agents are at the functional phone booth in SPBMaze. CTDL learns a policy with the most MI by preferring communicative actions equally. To assess whether an algorithm can learn policies that maximize MI (i.e., $I(A_1, O_2)$), we look at the sender\u2019s policy when both agents are at the functional phone booth. To reiterate, such a policy uniformly prefers $A_{comm}$ over $A_{env}$ when using a communication channel without forming any protocol. Figure 3 shows different algorithms\u2019 sender policy when in the functional phone booth. We include two variants that receive a scalar intermediate reward (IR) when both agents are at functional phone booths. As discussed in section 5.1.1, these reward shaping methods do not maximize MI, although cheap talk channels are discovered. Instead, they learn to prefer actions that keep them in $S_{comm}$ which includes certain environment actions. Contrarily, our proposed MI loss used in CTDL leads to policies with the most MI by preferring $A_{comm}$ (i.e., Hint-Up and Hint-Down) uniformly over $A_{env}$. This explains why CTDL has the best CTD performance as it directly maximizes PMI in Definition 4.3. Note that this uniformity over $A_{comm}$ also means no conventions are formed during discovery learning as they are preferred equally under different goals (i.e. different correct exits in SPBMaze), demonstrating the effect of OBL.\\n\\n7.3 Utilizing Cheap Talk Channels\\n\\nFigure 4: Left: CTU performance for CTDUL and baselines on SPBMaze. The vertical grey line indicates the start of CTU learning for CTDUL. Right: Ablation experiment on SPBMaze for CTDUL. The curves are smoothed by a factor of 0.99 with standard errors plotted as shaded areas. Figure 4 (left) shows the overall task performance of the proposed CTDUL and baselines in solving the SPBMaze. Being unable to communicate the goal information successfully, the receiver can only guess by escaping randomly (i.e., achieving an expected reward of $0.25 = 1.0 \\\\times 0.5 + (0.5) \\\\times 0.5$). Hence, all baselines converge to the suboptimal solution of random guessing. From Figure 2 (right) and Figure 4 (left), we can infer that the senders in the baselines do not reach the functional phone booth to communicate so no protocols can be formed. Centralized training baselines\"}"}
{"id": "cddbeL1HWaD", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023 (e.g., QMIX, MAA2C), which are designed to assign credits better since they have centralized components to learn better value functions using full state information, also converge to suboptimal solutions. Their poor performances illustrate the difficulty of this joint exploration problem.\\n\\nIn contrast, our proposed method CTDUL obtains significantly higher rewards than random guessing by employing DIAL to learn how to use the discovered channel. The grey vertical line is when DIAL begins after discovery learning. Qualitatively, we observe how our method solves the problem in which the receiver waits at the functional booth until receiving a message from the sender. Then, it goes to the correct exit according to the protocol learned. The results show how our proposed problem decomposition and MI maximization components make the difficult problem easier to solve.\\n\\nFigure 4 (right) shows task performance on SPBMaze for our ablation study with CTDUL. Specifically, we look at the task performance when we individually remove components used, namely MI maximization and DIAL. As we can see, nontrivial solutions can only be learned if both of them are used. Removing one of them leads to the suboptimal solution of random guessing. Furthermore, we observe significant performance dip when OBL is replaced with IQL, empirically supporting the importance of not forming any conventions during CTD as pointed out in section 5.1.2.\\n\\n7.4 MUTUAL INFORMATION AS A MEASURE OF CHANNEL CAPACITY\\n\\nFigure 5: Bar plot of average number of booth visits of each phone booth type in MPBMaze with varying noise factors for the noisy booth. The error bars are standard errors for the average number of booth visits for each booth type.\\n\\nWe hypothesize that our proposed MI reward can be a pseudomeasure of channel capacity, a way to discover where best to communicate when an environment has multiple channels. To verify, we run baselines and our proposed method for CTD on MPBMaze which has three channels, namely, the Perfect Booth, the Costly Booth, and the Noisy Booth. Note that a booth with noise has a probability of dropping a message. See Table 4 in the Appendix for the detailed configuration of MPBMaze.\\n\\nFigures 5 compare which booth the sender chooses to visit when different reward functions are used. The optimal behavior is to visit the Perfect Booth the most as it has no noise and usage cost. All algorithms learn to avoid the Costly Booth given its direct effect on the reward. Comparing methods that use IR and MI rewards, the IR reward cannot distinguish between the Perfect Booth and the Noisy Booth and consistently visit the Noisy Booth as visits to both booths are rewarded equally and the Noisy Booth is closer to the sender.\\n\\nOn the contrary, the MI reward method is able to visit the Perfect Booth consistently as the MI reward has an interpretation and measurement of noise and channel capacity (i.e., A channel with greater noise would necessarily mean a lower expected MI reward). As the noise factor decreases, the two booths become less distinguishable, which we observe a decreased in number of booth visits to the perfect booth for the agent with MI reward when the noise factor is 0.1.\\n\\n8 CONCLUSION\\n\\nIn this work, we take away the common and arguably unrealistic assumptions in MARL that communication channels are known and persistently accessible to the agents. Under this setting, agents have\"}"}
{"id": "cddbeL1HWaD", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ABSTRACT\\n\\nBy enabling agents to communicate, recent cooperative multi-agent reinforcement learning (MARL) methods have demonstrated better task performance and more coordinated behavior. Most existing approaches facilitate inter-agent communication by allowing agents to send messages to each other through free communication channels, i.e., cheap talk channels. Current methods require these channels to be constantly accessible and known to the agents a priori. In this work, we lift these requirements such that the agents must discover the cheap talk channels and learn how to use them. Hence, the problem has two main parts: cheap talk discovery (CTD) and cheap talk utilization (CTU). We introduce a novel conceptual framework for both parts and develop a new algorithm based on mutual information maximization that outperforms existing algorithms in CTD/CTU settings. We also release a novel benchmark suite to stimulate future research in CTD/CTU.\\n\\n1 INTRODUCTION\\n\\nEffective communication is essential for many multi-agent systems in the partially observable setting, which is common in many real-world applications like elevator control (Crites & Barto, 1998) and sensor networks (Fox et al., 2000). Communicating the right information at the right time becomes crucial to completing tasks effectively. In the multi-agent reinforcement learning (MARL) setting, communication often occurs on free channels known as cheap talk channels. The agents\u2019 goal is to learn an effective communication protocol via the channel. The transmitted messages can be either discrete or continuous (Foerster et al., 2016).\\n\\nExisting work often assumes the agents have prior knowledge (e.g., channel capacities and noise level) about these channels. However, such assumptions do not always hold. Even if these channels\u2019 existence can be assumed, they might not be persistent, i.e., available at every state. Consider the real-world application of inter-satellite laser communication. In the case, communication channel is only functional when satellites are within line of sight. This means positioning becomes essential (Lakshmi et al., 2008). Thus, without these assumptions, agents need the capability to discover where to best communicate before learning a protocol in realistic MARL settings.\\n\\nIn this work, we investigate the setting where these assumptions on cheap talk channels are lifted. Precisely, these channels are only effective in a subset of the state space. Hence, agents must discover where these channels are before they can learn how to use them. We divide this problem into two sequential steps: cheap talk discovery (CTD) and cheap talk utilization (CTU). The problem is a strict generalization of the common setting used in the emergent communication literature with\"}"}
{"id": "cddbeL1HWaD", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to first discover where to best communicate before learning how to use the discovered channels. We first provide a novel problem formulation for this setting denoted as the cheap talk discovery and utilization problem. Then, based on this problem decomposition, we propose a method based on MI maximization with OBL and DIAL to tackle this problem end-to-end. Experimentally, by evaluating our framework against state-of-the-art baselines in our proposed environment suite, we show that our framework can effectively discover communication channels and subsequently use them to solve tasks, while all the baselines fail to do so. We also empirically attribute the framework\u2019s success to its key capabilities in learning policies with maximized MI and without convention formation during CTD. Finally, we demonstrate how our MI metric can serve as a pseudomeasure for channel capacity, to be used to learn where best to communicate. We hope this work can inspire investigation in more realistic settings for communication like the one introduced here.\\n\\nACKNOWLEDGMENTS\\n\\nWe thank Tarun Gupta for providing code implementations of RL environments for our reference during the ideation phase of the project. We thank Michael Noukhovitch for helpful feedback.\\n\\nREFERENCES\\n\\nRobert H Crites and Andrew G Barto. Elevator group control using multiple reinforcement learning agents. Machine learning, 33(2):235\u2013262, 1998.\\n\\nAbhishek Das, Th\u00e9ophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and Joelle Pineau. Tarmac: Targeted multi-agent communication. In International Conference on Machine Learning, pp. 1538\u20131546. PMLR, 2019.\\n\\nJakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. Advances in neural information processing systems, 29, 2016.\\n\\nDieter Fox, Wolfram Burgard, Hannes Kruppa, and Sebastian Thrun. A probabilistic approach to collaborative multi-robot localization. Autonomous robots, 8(3):325\u2013344, 2000.\\n\\nHengyuan Hu and Jakob N Foerster. Simplified action decoder for deep multi-agent reinforcement learning. In International Conference on Learning Representations, 2019.\\n\\nHengyuan Hu, Adam Lerer, Brandon Cui, Luis Pineda, Noam Brown, and Jakob Foerster. Off-belief learning. In International Conference on Machine Learning, pp. 4369\u20134379. PMLR, 2021.\\n\\nNatasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement learning. In International Conference on Machine Learning, pp. 3040\u20133049. PMLR, 2019.\\n\\nJiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation. Advances in neural information processing systems, 31, 2018.\\n\\nSteven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent experience replay in distributed reinforcement learning. In International conference on learning representations, 2018.\\n\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nK Shantha Lakshmi, MP Senthil Kumar, and KVN Kavitha. Inter-satellite laser communication system. In 2008 International Conference on Computer and Communication Engineering, pp. 978\u2013983. IEEE, 2008.\\n\\nFrans A Oliehoek and Christopher Amato. A concise introduction to decentralized POMDPs. Springer, 2016.\"}"}
{"id": "cddbeL1HWaD", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The full problem setting of solving Dec-POMDPs is NEXP-complete. As such, it is not reasonable to expect a method that can solve large arbitrary Dec-POMDPs. To get around this, the \\\"emergent communication\\\" literature usually focuses on a setting where there is a fixed communication channel that is known a-priori.\\n\\nWe strictly generalise this setting by instead proposing a method that can first discover channels within the environment and next utilise them. Clearly, this cannot cover all problem Dec-POMDPs due to the constraints mentioned above but includes a broader set of potentially practically relevant applications.\\n\\nIn terms of use cases, the problem applies whenever real-world communication through a medium among agents is considered. Because more often than not, there will be some spatial constraints on communication in realistic settings. Any kind of communication channel used in the real-world (e.g., a radio link, cell phone signal), does not have perfect signal strength, and often transmission errors could happen (e.g., noise). The signal strength varies a lot depending on your environment and current surroundings (e.g., indoor vs outdoor, underwater, environment interference in certain locations like behind a mountain). Hence, for communication to happen efficiently and successfully, being able to learn where to communicate and learn where best to communicate is crucial which our problem setting addresses.\"}"}
{"id": "cddbeL1HWaD", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Even in communication without an explicit medium, say if a robot wants to signal to another one with lights but they need a line of sight, communication becomes spatially constrained. We believe these examples illustrate how important this problem setting is when considering communication learning in more realistic environments.\\n\\nFurthermore, the setting also sheds light to a more complete picture of capacity-constrained communication. As illustrated in our experiments with noisy channels, our mutual information reward provides a pseudomeasure of capacity. The investigation provides a path forward regarding capacity-constrained communication, by establishing two levels of capacity constraints - i.e., capacity constrained by the environment/physical space, size of $S_{comm}$, and the capacity for individual channels.\\n\\nOur method provides the first attempt to tackle both types of capacity constraints - learning where to communicate and learning where best to communicate.  \\n\\nAs in Hu et al. (2021), for a given trajectory $\\\\tau$ sampled from replay buffer, $L_{OBL}$ is expressed as TD-error:\\n\\n$$L_{OBL}(\\\\theta | \\\\tau) = \\\\frac{1}{2} \\\\sum_{t=1}^{T} (h'_{t} + r'_{t+1} + \\\\max_{a} \\\\hat{Q}(a | \\\\tau_{t+2}) - \\\\hat{Q}(a_t | \\\\tau_{t}))$$ (8)\\n\\nwhere $\\\\hat{Q}$ is the target network. As $\\\\tau'_{t}$ contains fictitious transitions, we can not pass the whole sequence to RNN like in normal Q-learning. Thus, we precompute the target $G'_{t} = r'_{t} + r'_{t+1} + \\\\max_{a} \\\\hat{Q}(a | \\\\tau'_{t+2})$ during rollouts and store the sequence of targets along with the real trajectory $\\\\tau$ into the replay buffer.\\n\\n![Diagram of gradient flow](image)\\n\\n**Figure 6:** How gradients flow through our proposed architecture, with reference to Foerster et al. (2016)\\n\\n**Pseudocode of the Proposed Architecture**\\n\\nAlgorithm 1 outlines the pseudocode of the proposed architecture to tackle the cheap talk discovery and utilization problem.\"}"}
{"id": "cddbeL1HWaD", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\n\\nPseudocode for our proposed method\\n\\nfor each agent do\\n  Initialize replay memory for OBL $D_{OBL}$ with capacity $N$\\n  Initialize replay memory for DIAL $D_{DIAL}$ with capacity $N$\\n  Initialize action value function $Q$ with random weights $\\\\theta$\\n    using architecture C-Net based on R2D2\\n  Initialize target action value function $\\\\hat{Q}$ with random weights $\\\\theta_{-1} = \\\\theta$\\n    using architecture C-Net based on R2D2\\n  Initialize $Num\\\\_Discovery\\\\_Episode$\\n  Initialize Discovery\\\\_Stage as True\\nend for\\n\\nfor $e = 1, Max\\\\_Episode$ do\\n  Initialize $s$\\n  repeat\\n    for each agent do\\n      Select $a$ from $o$ based on policy derived from $Q$, e.g., $\\\\epsilon$-greedy policy\\n      Take action $a$ to observe reward $r$ and next state $o'$\\n      Take action $a$ in Pseudo-Environment to perform OBL sampling with mutual information computation\\n      Store the transition into $D_{OBL}$\\n      if High mutual information is observed then\\n        Store trajectory into $D_{DIAL}$\\n      end if\\n      Sample random minimatch of transitions from $D_{OBL}$\\n      Sample random minimatch of transitions from $D_{DIAL}$\\n      if Discovery\\\\_Stage then\\n        Perform a gradient descent step on $L_{OBL}$ $i(\\\\theta_i)$\\n      else\\n        Perform a gradient descent step on $((y_i - Q(o, a; \\\\theta_{i-1}))^2$ for DIAL with respect to the network weights\\\\\\n        Perform a gradient descent step on $L_{IQL}$ $i(\\\\theta_i)$ with respect to the network weights\\\\\\n      end if\\n    end for\\n  end repeat\\n  Every C steps, set $\\\\hat{Q} = Q$\\nend for\\n\\nuntil $s = s_{\\\\text{terminal}}$\\n\\nif $e \\\\geq Num\\\\_Discovery\\\\_Episode$ then\\n  Set Discovery\\\\_Stage as False\\nend if\\nend for\\n\\nEN VIRONMENT DESI G N\\n\\nFigure 7 visualizes two instances of the environment used in our experiments. Red cables indicate connectivity with dashed ones indicating the existence of noise. The yellow booth here indicates that it is a costlier booth to use.\\n\\nFor observations, each consists of a tensor of 3 channels plus some role-specific information. The channels are the wall channel, phone booth channel, and the agent channel, which are essentially binary grid encoding of each position for the existence of wall, phone booth, and the agent respectively.\\n\\nFor role-specific information, the sender has an additional 2-bit encoding vector as goal information. Specifically, if the receiver should go up, the sender would get a vector of $[1 \\\\ 0]$. If the receiver should go down, the sender would get $[0 \\\\ 1]$ instead. For the receiver, if the sender performs a HINT-UP with both of them at the functional booths, it would be -1. On the other hand, if the sender performs a HINT-DOWN, it would be 1. Many aspects of this environment are configurable to allow extensive investigation and exploration of different algorithms, some key adjustable features are highlighted in table E.\"}"}
{"id": "cddbeL1HWaD", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"less assumptions, which is more akin to real-world scenarios (see appendix A for more in-depth discussions on the setting's significance and use cases).\\n\\nFigure 1: The two learning stages for CTD/CTU based on PBMaze. Stage (a): Discover the functional phone booths; Stage (b): Form a protocol to use the phone booth and learn to interpret the messages (left), and solve the task (right). The blue and red agents are the sender and the receiver respectively.\\n\\nThis setting is particularly difficult as it suffers from the temporal credit assignment problem (Sutton, 1984) for communicative actions. Consider an example we call the phone booth maze (PBMaze), the environment has a sender and a receiver, placed into two separate rooms. The receiver\u2019s goal is to escape from the correct exit out of two possible exits. Only the sender knows which one is the correct exit. The sender\u2019s goal is to communicate this information using functional phone booths.\\n\\nThis leads to two learning stages. Firstly, they need to learn to reach the booths. Then, the sender has to learn to form a protocol, distinguishing different exit information while the receiver has to learn to interpret the sender\u2019s protocol by trying different exits. This makes credit assignment particularly difficult as communicative actions do not lead to immediate rewards. Additionally, having communicative actions that are only effective in a small subset of the state space further makes it a challenging joint exploration problem, especially when communication is necessary for task completion. Figure 1 provides a visual depiction of the two learning stages in this environment.\\n\\nAs a whole, our contributions are four-fold. Firstly, we provide a formulation of the CTD and CTU problem. Secondly, we introduce a configurable environment to benchmark MARL algorithms on the problem. Thirdly, we propose a method to solve the CTD and CTU problems based on information theory and advances in MARL, including off-belief learning (Hu et al., 2021, OBL) and differentiable inter-agent learning (Foerster et al., 2016, DIAL). Finally, we show that our proposed approach empirically compares favourably to other MARL baselines, validate the importance of specific components via ablation studies and illustrate how our method can act as a measure of channel capacity to learn where best to communicate.\\n\\n2 RELATED WORK\\n\\nThe use of mutual information (MI) has been explored in the MARL setting. Wang et al. (2019) propose a shaping reward based on MI between agents\u2019 transitions to improve exploration, encouraging visiting critical points where one can influence other agents. Our proposed method also has an MI term for reward shaping. Their measure might behave similarly to ours but is harder to compute and requires full environmental states during training. Sokota et al. (2022) propose a method to discover implicit communication protocols using environment actions via minimum entropy coupling, separating communicative and non-communicative decision-making. We propose a similar problem decomposition by separating state and action spaces into two subsets based on whether communication can occur or not. Unlike in Sokota et al. (2022), we focus on explicit communication...\"}"}
{"id": "cddbeL1HWaD", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where specialized channels for communication exist. Jaques et al. (2019) propose rewarding agents for having causal influences over other agents' policies using MI between agents' actions. Jaques et al. (2019) is the closest to our work but still assumes the omnipresence and prior knowledge of communication channels.\\n\\nMany recent papers investigate various aspects of communication in MARL. Foerster et al. (2016) propose DIAL to learn how to communicate by allowing gradients to flow across agents. We use DIAL as a component of our proposed framework. Sukhbaatar et al. (2016) propose CommNet to learn how to communicate. Unlike DIAL, it uses mean-pooling to process messages and handle a dynamic number of agents. Das et al. (2019) proposes a targeted communication architecture to tackle the issue of what messages to send and who to send them to. Singh et al. (2018) propose a gating mechanism to learn when to communicate, achieving better training efficiency scalability by reducing redundant communication. Jiang & Lu (2018) proposes an attention mechanism to learn when and who to communicate by dynamically forming communication groups. In our work, we focus on the problem of discovering where to communicate, rather than how to communicate, who to communicate, and when to communicate. Hence, our contributions are orthogonal to existing works.\\n\\n3 BACKGROUND\\n\\nThroughout this work, we consider decentralized partially observable Markov decision processes (Dec-POMDPs) with \\\\(N\\\\) agents (Oliehoek & Amato, 2016) in the form of a tuple \\\\(G = \\\\langle S, A, P, R, \\\\Xi, n, \\\\gamma \\\\rangle\\\\).\\n\\n- \\\\(s \\\\in S\\\\) is the true state of the environment. At each time step, each agent \\\\(i \\\\in N\\\\) chooses an action \\\\(a \\\\in A_i\\\\) to form a joint action \\\\(a \\\\in A \\\\equiv A_1 \\\\times A_2 \\\\times ... \\\\times A_N\\\\). This leads to a transition on the environment according to the transition function \\\\(P (s' | s, a_1, ...a_N) : S \\\\times A \\\\times S \\\\rightarrow [0, 1]\\\\).\\n\\n- All agents share the same reward function \\\\(R (s, a) : S \\\\times A \\\\rightarrow R\\\\).\\n\\n- \\\\(\\\\gamma \\\\in [0, 1)\\\\) is a discount factor. Each agent \\\\(i\\\\) receives individual observations \\\\(z \\\\in Z\\\\) based on the observation function \\\\(\\\\Xi_i (s) : S \\\\rightarrow Z\\\\).\\n\\n- The environment trajectory and the action-observation history (AOH) of an agent \\\\(i\\\\) are denoted as \\\\(\\\\tau_t = (s_0, a_0, ..., s_t, a_t)\\\\) and \\\\(\\\\tau_i^t = \\\\Xi_i (s_0), a_i_0, ..., \\\\Xi_i (s_t), a_i_t \\\\in T \\\\equiv (Z \\\\times A)^*\\\\) respectively.\\n\\n- A stochastic policy \\\\(\\\\pi (a_i | \\\\tau_i) : T \\\\times A \\\\rightarrow [0, 1]\\\\) conditions on AOH. The joint policy \\\\(\\\\pi\\\\) has a corresponding action-value function \\\\(Q^\\\\pi (s_t, a_t) = \\\\mathbb{E}_{s_{t+1}, ..., \\\\infty} [R_t | s_t, a_t]\\\\), where \\\\(R_t = \\\\sum_{i=0}^{\\\\infty} \\\\gamma^i r_{t+i}\\\\) is the discounted return.\\n\\n- \\\\(r_{t+i}\\\\) is the reward obtained at time \\\\(t+i\\\\) from the reward function \\\\(R\\\\).\\n\\n- The distribution of states is commonly referred to as a belief \\\\(B^\\\\pi (\\\\tau | \\\\tau_i) = P (\\\\tau | \\\\tau_i, \\\\pi)\\\\).\\n\\n3.1 OFF-BELIEF LEARNING\\n\\nOBL (Hu et al., 2021) is a recent method to learn policies that do not interpret the actions of other agents and assumes other agents would do the same. Precisely, it induces agents to not reason about each other's private information and actions by conditioning their beliefs only on information revealed by the environment and interpreting their actions as if they were performed by a random agent. This is often desirable as making incorrect assumptions can cause coordination failure. Therefore, learning a base policy that maximizes reward without any conventions is important, especially when agents work with others who they have not met before, a problem known as zero-shot coordination (ZSC).\\n\\nThe OBL operator assumes all agents to be playing a common policy \\\\(\\\\pi_0\\\\) up to \\\\(\\\\tau_i\\\\) and \\\\(\\\\pi_1\\\\) thereafter. Then, an agent's belief \\\\(B\\\\), conditioned on their AOH can be computed as:\\n\\n\\\\[\\nB^\\\\pi_0 (\\\\tau | \\\\tau_i) = P (\\\\tau | \\\\tau_i, \\\\pi_0)\\n\\\\]\\n\\nWe denote the state-action value for playing \\\\(\\\\pi_0\\\\) up to \\\\(\\\\tau_i\\\\) and playing \\\\(\\\\pi_1\\\\) thereafter to be \\\\(Q^\\\\pi_0 \\\\rightarrow \\\\pi_1 (a | \\\\tau_i)\\\\), which is the expected return of sampling \\\\(\\\\tau\\\\) from \\\\(B^\\\\pi_0 (\\\\tau_i)\\\\) with all players playing \\\\(\\\\pi_1\\\\) starting from the end of this trajectory. The counterfactual state-action value function is defined as follows:\\n\\n\\\\[\\nQ^\\\\pi_0 \\\\rightarrow \\\\pi_1 (a | \\\\tau_i) = \\\\sum_{\\\\tau_t, \\\\tau_{t+1}} B^\\\\pi_0 (\\\\tau_t | \\\\tau_i) [R (s_t, a) + T (\\\\tau_{t+1} | \\\\tau_t) V^\\\\pi_1 (\\\\tau_{t+1})]\\n\\\\]\\n\\nTo compute an OBL policy using value iteration methods, the Bellman equation for \\\\(Q^\\\\pi_0 \\\\rightarrow \\\\pi_1 (\\\\tau_i)\\\\) for each agent \\\\(i\\\\) is expressed as follows:\\n\\n\\\\[\\nQ^\\\\pi_0 \\\\rightarrow \\\\pi_1 (a_t | \\\\tau_i) = \\\\mathbb{E}_{\\\\tau_t \\\\sim B^\\\\pi_0 (\\\\tau_i)} [R (s_t, a_t) + \\\\sum_{\\\\tau_{t+k} \\\\sim (T, \\\\pi_1)} \\\\gamma h_{t+k-1} X_{s_{t+k} = t} R (s_{t+k}, a_{t+k}) + X_{a_{t+k} | \\\\tau_{i, t+k}} Q^\\\\pi_0 \\\\rightarrow \\\\pi_1 (a_{t+k} | \\\\tau_{i, t+k})]\\n\\\\]\"}"}
{"id": "cddbeL1HWaD", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this setting, the states reached by $\\\\pi_1$ may be reached at very low probabilities under $\\\\pi_0$. The variant learned-belief OBL (Hu et al., 2021, LB-OBL) addresses this by using an approximate belief $\\\\hat{B}_{\\\\pi_0}$ that takes $\\\\tau_i$ as input and samples a trajectory from an approximation of $P(\\\\tau | \\\\tau_{it}, \\\\pi_0)$. Q-learning is then performed with an altered target value. Particularly, a new $\\\\tau'_{it}$ is resampled from $\\\\hat{B}_{\\\\pi_0}(\\\\tau_{it})$.\\n\\nNext, a transition to $\\\\tau_{it+1}$ is simulated with other agents playing policy $\\\\pi_1$. The bootstrapped value is then $\\\\max_a Q(a | \\\\tau_{it+2})$. Hence, LB-OBL only involves fictitious transitions. The action $a_{it} \\\\sim \\\\pi_1$ is applied to both the actual environment and a sampled fictitious state. The learning target then becomes the sum of fictitious rewards $r'_{it}, r'_{it+1}$ and the fictitious bootstrapped value $\\\\max_a Q(a | \\\\tau_{it+2})$.\\n\\nWe use exact belief models in this work to remove the influence of belief learning.\\n\\n3.2 DIFFERENTIABLE INTER-AGENT LEARNING\\n\\nFoerster et al. (2016) propose DIAL to learn how to communicate. It allows agents to give each other feedback about their communicative actions, by opening up communication channels for gradients to flow through from one agent to another. Such richer feedback improves sample complexity with more efficient discovery of protocols. During training, there are direct connections between one agent's network output and another agent's input through communicative actions. Agents can then send real-valued messages and are only restricted to discrete messages during execution. These real-valued messages are generated from the networks, allowing end-to-end backpropagation across agents.\\n\\nThe proposed network is called C-net, an extension of deep Q-networks (Silver et al., 2016, DQN). It has two set of outputs, namely the $Q$-values $Q(\\\\cdot)$ of the environment actions $A_{env}$ and the real-valued messages $m_{at}$. The former performs actions selection while the latter is sent to another agent after the discretize/regularize unit $DRU(m_{at})$ processes it. The $DRU$ regularizes messages during learning, $DRU(m_{at}) = \\\\text{Logistic}(\\\\mathcal{N}(m_{at}, \\\\sigma))$, and discretizes them during execution, $DRU(m_{at}) = 1 \\\\{m_{at} > 0\\\\}$. $\\\\sigma$ is the standard deviation of the noise added to the cheap talk channel. The gradient term for $m$ is backpropagated to the sender based on the message recipient's error. Hence, the sender can adjust $m$ to minimize the downstream loss. Note that we use the OBL loss here instead of DQN loss.\\n\\n4 PROBLEM FORMULATION\\n\\nWe consider each agent's action space $A$ to be decomposable into two subspaces, namely, the environment action space $A_{env}$ and communication action space $A_{comm}$. The former are actions that only affect the environment state. The latter are actions that have no immediate effect on the environment state or the reward received, but can alter other agents' observations. Exemplifying with PBMaze, $A_{env}$ for the sender are actions that move its locations like Up and Down, while $A_{comm}$ are actions that send messages to the receiver like Hint Up and Hint Down.\\n\\nIn this setting, agents do not inherently know where to best communicate and only a subset of states allows communication. We refer to this subset as the communicative state space $S_{comm} \\\\subseteq S$:\\n\\nDefinition 4.1. The communicative state space $S_{comm}$ of an environment is a subspace of its state space $S$. Assume the environment is in a state $s_c \\\\in S_{comm}$, at least one agent in $s_c$ can modify another agent's observation by taking an action $a \\\\in A_{comm}$.\\n\\nSimilarly, the communicative joint observation space $O_{comm}$ is defined as follows:\\n\\nDefinition 4.2. The communicative joint observation space $O_{comm}$ of an environment is a subspace of its joint observation space $O$, defined as $O_{comm} = \\\\{ \\\\Omega(s_{c}) | s_{c} \\\\in S_{comm} \\\\}$. Assume the environment is in a state $s_c \\\\in S_{comm}$ with a joint observation $o_c \\\\in O_{comm}$, at least one agent, agent $i \\\\in N_j, j \\\\neq i$'s observation, $o_{t+1}$ by taking an action $a \\\\in A_{comm}$.\\n\\nWith these definitions, the cheap talk discovery and utilization problem can be formalized as follows:\\n\\nDefinition 4.3. For a given $s \\\\in S$, let $MI(s) = \\\\mathbb{P}_{i \\\\sim \\\\mathcal{N}}\\\\mathbb{P}_{j \\\\sim \\\\mathcal{N}, j \\\\neq i} I(A_i, O_j)$ be a function that computes the pairwise mutual information (PMI) of an agent's actions and another agent's observations. The inner term is defined in equation 5. Let $\\\\nu(\\\\pi) = \\\\mathbb{E}_{\\\\pi} P_{\\\\infty}^{i} \\\\gamma_{i} MI(s_{t}) | \\\\tau_{t}, be the discounted sum of PMI of a policy $\\\\pi \\\\in \\\\Pi$, where $\\\\Pi$ is the set of all possible policies. Cheap talk discovery is the...\"}"}
{"id": "cddbeL1HWaD", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"problem of learning a policy $\\\\pi$ in which agents take actions in $A$ that maximizes $\\\\nu(\\\\pi)$. An optimal $\\\\pi^*$ discovers satisfies the condition:\\n\\n$$\\\\nu(\\\\pi^*) \\\\geq \\\\nu(\\\\pi) \\\\quad \\\\forall \\\\pi \\\\in \\\\Pi$$\\n\\n**Definition 4.4.**\\n\\nGiven a state $s_t$ is in $S$ with a corresponding AOH $\\\\tau_t$ at time $t$, cheap talk utilization is the problem of learning a policy $\\\\pi_{util}$ in which agents take communicative actions in $A_{comm}$ to share information to improve task performance. For a policy $\\\\pi \\\\in \\\\Pi$, $R_{\\\\pi_t} = \\\\mathbb{E}_{\\\\pi} P_{\\\\infty}^{\\\\pi} r_t + i | \\\\tau_t$ is its expected return starting from $s_t$. An optimal $\\\\pi^*_{util}$ can then be defined as:\\n\\n$$R_{\\\\pi^*_{util}} t \\\\geq R_{\\\\pi_t} \\\\quad \\\\forall \\\\pi \\\\neq \\\\pi^*_{util} \\\\pi \\\\in \\\\Pi \\\\text{ and } \\\\forall \\\\tau_t \\\\text{ s.t. } s_t \\\\in S_{comm}$$\\n\\nWe reiterate that this is a challenging joint exploration problem, especially when communication is necessary for task completion. Because agents need to stumble upon a state $s_c \\\\in S_{comm}$, which is often a much smaller subspace than $S$, and there are no incentivizing signals to reach these states as communicative actions $A_{comm}$ do not lead to immediate rewards.\\n\\nThus, we hypothesize that this problem decomposition, i.e., first learn to discover the $S_{comm}$ before learning how to use the communicative channels, could ease this joint exploration problem\u2019s severity.\\n\\n**5 METHODOLOGY**\\n\\n**5.1 CHEAP TALK DISCOVERY**\\n\\nOur proposed method has two components, namely, mutual information (MI) maximization and OBL (Hu et al., 2021). The former induces agents to discover cheap talk channels based on MI. The latter is our base learning algorithm with sound theoretical properties that are beneficial to the following CTU step.\\n\\n**5.1.1 MUTUAL INFORMATION MAXIMIZATION**\\n\\nAgents do not have immediate incentives to discover cheap talk channels. To do so, an agent has to know where to send messages to other agents successfully (i.e., channels that can influence another agent\u2019s observations). Hence, we propose novel reward and loss functions based on MI. The proposed reward function encourages greater mutual information (MI) between the sender\u2019s (denoted as agent 1) actions $A_1$ and the receiver\u2019s (denoted as agent 2) observations $O_2$. It is expressed as:\\n\\n$$R'_{\\\\pi} (s_t, a_t) = R_{\\\\pi} (s_t, a_t) + \\\\beta I(A_1, O_2).$$\\n\\n(4)\\n\\nwhere $\\\\beta$ is a hyperparameter. The first term is the task reward. The second term is the MI reward:\\n\\n$$I(A_1, O_2) = \\\\mathbb{E}_{a_1 \\\\sim A_1} o_2 \\\\sim O_2 \\\\log(p(a_1 | o_2)) - \\\\log(p(a_1)).$$\\n\\n(5)\\n\\nWe assume access to the environment simulator to estimate $p(a_1 | o_2)$. The proposed reward function can then be substituted in the update equation 3. To estimate $p(a_1 | o_2)$, the term can be expanded to:\\n\\n$$I(A_1, O_2) = H(A_1) + H(O_2) - H(A_1, O_2) = \\\\sum_{a_1 \\\\sim A_1} p(a_1) \\\\log(p(a_1)) + \\\\sum_{o_2 \\\\sim O_2} p(o_2) \\\\log(p(o_2)) - \\\\sum_{(a_1, o_2)} p(a_1, o_2) \\\\log(p(a_1, o_2)).$$\\n\\n(6)\\n\\nwhere $p(o_2) = \\\\mathbb{P}_{a_1 \\\\sim A_1} [p(a_1, o_2)]$ and $p(a_1, o_2) = p(o_2 | a_1) p(a_1)$. Note that this generalizes to any pair of agents in an environment irrespective of the receiver. Hence, we can apply this method to all possible receivers by summing all $I(A_1, O_i)$, $i \\\\in \\\\mathbb{N}, i \\\\neq 1$ to discover the cheap talk channels.\\n\\nThe MI reward term alone does not maximize $I(A_1, O_2)$. Specifically, for a given policy, since the term is computed based on taking all the actions from a state, the MI reward for staying within...\"}"}
