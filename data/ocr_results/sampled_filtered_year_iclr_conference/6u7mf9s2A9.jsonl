{"id": "6u7mf9s2A9", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A BSTRACT\\n\\nPoint cloud data is ubiquitous in scientific fields. Recently, geometric deep learning (GDL) has been widely applied to solve prediction tasks with such data. However, GDL models are often complicated and hardly interpretable, which poses concerns to scientists who are to deploy these models in scientific analysis and experiments. This work proposes a general mechanism, learnable randomness injection (LRI), which allows building inherently interpretable models based on general GDL backbones. LRI-induced models, once trained, can detect the points in the point cloud data that carry information indicative of the prediction label. We also propose four datasets from real scientific applications that cover the domains of high-energy physics and biochemistry to evaluate the LRI mechanism. Compared with previous post-hoc interpretation methods, the points detected by LRI align much better and stabler with the ground-truth patterns that have actual scientific meanings. LRI is grounded by the information bottleneck principle, and thus LRI-induced models are also more robust to distribution shifts between training and test scenarios. Our code and datasets are available at https://github.com/Graph-COM/LRI.\\n\\n1 INTRODUCTION\\n\\nThe measurement of many scientific research objects can be represented as a point cloud, i.e., a set of featured points in some geometric space. For example, in high energy physics (HEP), particles generated from collision experiments leave spacial signals on the detectors they pass through (Guest et al., 2018); In biology, a protein is often measured and represented as a collection of amino acids with spacial locations (Wang et al., 2004; 2005). Geometric quantities of such point cloud data often encode important properties of the research object, analyzing which researchers may expect to achieve new scientific discoveries (Tusnady & Simon, 1998; Aad et al., 2012).\\n\\nRecently, machine learning techniques have been employed to accelerate the procedure of scientific discovery (Butler et al., 2018; Carleo et al., 2019). For geometric data as above, geometric deep learning (GDL) (Bronstein et al., 2017; 2021) has shown great promise and has been applied to the fields such as HEP (Shlomi et al., 2020; Qu & Gouskos, 2020), biochemistry (Gainza et al., 2020; Townshend et al., 2021) and so on. However, geometric data in practice is often irregular and high-dimensional. Think about a collision event in HEP that generates hundreds to thousands of particles, or a protein that consists of tens to hundreds of amino acids. Although each particle or each amino acid is located in a low-dimensional space, the whole set of points eventually is extremely irregular and high-dimensional. So, current research on GDL primarily focuses on designing neural network (NN) architectures for GDL models to deal with the above data challenge. GDL models have to preserve some symmetries of the system and incorporate the inductive biases reflected by geometric principles to guarantee their prediction quality (Cohen & Welling, 2016; Bogatskiy et al., 2020), and therefore often involve dedicated-designed complex NN architectures.\\n\\nAlbeit with outstanding prediction performance, the complication behind GDL models makes them hardly interpretable. However, in many scientific applications, interpretable models are in need (Roscher et al., 2020): For example, in drug discovery, compared with just predicting the binding affinity of a protein-ligand pair, it is more useful to know which groups of amino acids determine the affinity and where can be the binding site, as the obtained knowledge may guide future research directions (Gao et al., 2018; Karimi et al., 2019; 2020). Moreover, scientists tend to only trust interpretable models.\"}"}
{"id": "6u7mf9s2A9", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Illustrations of the four scientific datasets in this work to study interpretable GDL models.\\n\\nInterpretable models in many scenarios, e.g., most applications in HEP, where data from real experiments lack labels and models have to be trained on simulation data (Nachman & Shimmin, 2019). Here, model interpretation is used to verify if a model indeed captures the patterns that match scientific principles instead of some spurious correlation between the simulation environment and labels. Unfortunately, to the best of our knowledge, there have been no studies on interpretable GDL models let alone their applications in scientific problems. Some previous post-hoc methods may be extended to interpret a pre-trained GDL model while they suffer from some limitations as to be reviewed in Sec. 2. Moreover, recent works (Rudin, 2019; Laugel et al., 2019; Bordt et al., 2022; Miao et al., 2022) have shown that the data patterns detected by post-hoc methods are often inconsistent across interpretation methods and pre-trained models, and may hardly offer reliable scientific insights.\\n\\nTo fill the gap, this work proposes to study interpretable GDL models. Inspired by the recent work (Miao et al., 2022), we first propose a general mechanism named Learnable Randomness Injection (LRI) that allows building inherently interpretable GDL models based on a broad range of GDL backbones. We then propose four datasets from real-world scientific applications in HEP and biochemistry and provide an extensive comparison between LRI-induced GDL models and previous post-hoc interpretation approaches (after being adapted to GDL models) over these datasets.\\n\\nOur LRI mechanism provides model interpretation by detecting a subset of points from the point cloud that is most likely to determine the label of interest. The idea of LRI is to inject learnable randomness to each point, where, along with training the model for label prediction, injected randomness on the points that are important to prediction gets reduced. The convergent amounts of randomness on points essentially reveal the importance of the corresponding points for prediction. Specifically in GDL, as the importance of a point may be indicated by either the existence of this point in the system or its geometric location, we propose to inject two types of randomness, Bernoulli randomness, with the framework name LRI-Bernoulli to test existence importance of points and Gaussian randomness on geometric features, with the framework name LRI-Gaussian to test location importance of points. Moreover, by properly parameterized such Gaussian randomness, we may tell for a point, how in different directions perturbing its location affects the prediction result more. With such fine-grained geometric information, we may estimate the direction of the particle velocity when analyzing particle collision data in HEP. LRI is theoretically sound as it essentially uses a variational objective derived from the information bottleneck principle (Tishby et al., 2000). LRI-induced models also show better robustness to the distribution shifts between training and test scenarios, which gives scientists more confidence in applying them in practice.\\n\\nWe note that one obstacle to studying interpretable GDL models is the lack of valid datasets that consist of both classification labels and scientifically meaningful patterns to verify the quality of interpretation. Therefore, another significant contribution of this work is to prepare four benchmark datasets grounded on real-world scientific applications to facilitate interpretable GDL research. These datasets cover important applications in HEP and biochemistry. We illustrate the four datasets in Fig. 1 and briefly introduce them below. More detailed descriptions can be found in Appendix C.\\n\\n- ActsTrack is a particle tracking dataset in HEP that is used to reconstruct the properties, such as the kinematics of a charged particle given a set of position measurements from a tracking detector. Tracking is an indispensable step in analyzing HEP experimental data as well as particle tracking used in medical applications such as proton therapy (Schulte et al., 2004; Thomson, 2013; Ai et al., 2022). Our task is formulated differently from traditional track reconstruction tasks: We predict the existence of a $\\\\tau \\\\rightarrow \\\\mu\\\\mu$ decay and use the set of points from the $\\\\mu$'s to verify model interpretation, which can be used to reconstruct $\\\\mu$ tracks. ActsTrack also provides a controllable environment (e.g., magnetic field strength) to study fine-grained geometric patterns.\"}"}
{"id": "6u7mf9s2A9", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tau3Mu, another application in HEP, is to detect a challenging signature of charged lepton flavor violating decays, i.e., the $\\\\tau \\\\to \\\\mu\\\\mu\\\\mu$ decay, given simulated muon detector hits in proton-proton collisions. Such decays are greatly suppressed in the Standard Model (SM) of particle physics (Oerter, 2006; Blackstone et al., 2020), therefore, any detection of them is a clear signal of new physics beyond the Standard Model (Calibbi & Signorelli, 2018; Collaboration, 2021). Unfortunately, $\\\\tau \\\\to \\\\mu\\\\mu\\\\mu$ contains particles of extremely low momentum, thus technologically impossible to trigger with traditional human-engineered algorithms. Hence, online detection with advanced models that explores the correlations between signal hits on top of background hits is required to capture such decays at the Large Hadron Collider.\\n\\nOur task is to predict the existence of $\\\\tau \\\\to \\\\mu\\\\mu\\\\mu$ and use the detector hits left by the $\\\\mu$'s to verify model interpretation.\\n\\nSynMol is a molecular property prediction task. Although some works have studied model interpretability in such tasks (McCloskey et al., 2019; Sanchez-Lengeling et al., 2020), they limit their focus on the chemical-bond-graph representations of molecules, and largely ignore their geometric features. In this work, we put focus on 3D molecular representations. Our task is to predict the property given by two functional groups carbonyl and unbranched alkane (McCloskey et al., 2019) and use atoms in these functional groups to verify model interpretation.\\n\\nPLBind is to predict protein-ligand binding affinities given the 3D structures of proteins and ligands, which is a crucial step in drug discovery, because a high affinity is one of the major drug selecting criteria (Wang & Zhang, 2017; Karimi et al., 2019). Accurately predicting their affinities with interpretable models is useful for rational drug design and may help the understanding of the underlying biophysical mechanism that enables protein-ligand binding (Held et al., 2011; Du et al., 2016; Cang & Wei, 2018).\\n\\nOur task is to predict whether the affinity is above a given threshold and use amino acids in the binding site of the test protein to verify model interpretation.\\n\\nWe evaluate LRI with three popular GDL backbone models DGCNN (Wang et al., 2019), Point Transformer (Zhao et al., 2021), and EGNN (Satorras et al., 2021) over the above datasets. We also extend five baseline interpretation methods to GDL for comparison. We find that interpretation results given by LRI align much better with the scientific facts than those of the baselines. Also, we observe over some datasets, LRI-Gaussian outperforms LRI-Bernoulli while on others vice versa. This implies different GDL applications may have different interpretation requirements. Effective data patterns may vary regarding how the task depends on the geometric features of the points. Interestingly, we find LRI-Gaussian can discover some fine-grained geometric patterns, such as providing high-quality estimations of the directions of particle velocities in ActsTrack, and a high-quality estimation of the strength of the used magnetic field. Moreover, neither of LRI mechanisms degrades the prediction performance of the used backbone models. LRI mechanisms even improve model generalization when there exist some distribution shifts from the training to test scenarios.\\n\\n2 Related Work\\nWe review two categories of methods that can provide interpretability in the following.\\n\\nPost-hoc Interpretation Methods. Interpretation methods falling into this category assume a pre-trained model is given and attempts to further analyze it to provide post-hoc interpretation. Among them, gradient-based methods (Zhou et al., 2016; Selvaraju et al., 2017; Sundararajan et al., 2017; Shrikumar et al., 2017; Chattopadhay et al., 2018) may be extended to interpret geometric data by checking the gradients w.r.t. the input features or intermediate embeddings of each point. Some methods to interpret graph neural networks can be applied to geometric data (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021; Yuan et al., 2021). However, these methods need to mask graph structures pre-constructed by geometric features and cannot fully evaluate the effectiveness of geometric features. Among other methods, Chen et al. (2018); Yoon et al. (2018) study pattern selection for regular data, Ribeiro et al. (2016); Lundberg & Lee (2017); Huang et al. (2022) utilize a local surrogate model, and Lundberg & Lee (2017); Chen et al. (2019b); Ancona et al. (2019); Lundberg et al. (2020) leverage the shapley value to evaluate feature importance. These methods either cannot utilize geometric features or cannot be easily applied to irregular geometric data.\\n\\nInherently Interpretable Models. Although vanilla attention mechanisms (Bahdanau et al., 2015; Vaswani et al., 2017) were widely used for inherent interpretability, multiple recent studies show that they cannot provide reliable interpretation, especially for data with irregular structures (Serrano & Smith, 2019; Jain & Wallace, 2019; Ying et al., 2019; Luo et al., 2020). So, some works focusing...\"}"}
{"id": "6u7mf9s2A9", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023 on improving the attention mechanism for better interpretability (Bai et al., 2021; Miao et al., 2022), some propose to identify representative prototypes during training (Li et al., 2018; Chen et al., 2019a), and some methods (Taghanaki et al., 2020; Yu et al., 2021; Sun et al., 2022) adopt the information bottleneck principle (Tishby et al., 2000). However, all these methods cannot analyze geometric features in GDL. Along another line, invariant learning methods (Arjovsky et al., 2019; Chang et al., 2020; Krueger et al., 2021; Wu et al., 2022; Chen et al., 2022) focusing on out-of-distribution generalization based on causality analysis may also provide some interpretability, but these methods are typically of great complexity and cannot analyze the geometric features as well.\\n\\n### Preliminaries and Problem Formulation\\n\\nIn this section, we define some useful concepts and notations.\\n\\n#### GDL Tasks.\\n\\nWe consider a data sample is a point cloud $C = (V, X, r)$, where $V = \\\\{v_1, v_2, ..., v_n\\\\}$ is a set of $n$ points, $X \\\\in \\\\mathbb{R}^{n \\\\times d}$ includes $d$-dimensional features for all points, and $r \\\\in \\\\mathbb{R}^{n \\\\times 3}$ denotes 3D spacial coordinates of points. In this work, we introduce our notations by assuming the points are in 3D euclidean space while our method can be generalized. We focus on building a classification model $\\\\hat{y} = f(C)$ to predict the class label $y$ of $C$. Regression tasks are left for future studies.\\n\\n#### GDL Models.\\n\\nThe first class of DGL models view each sample of points as an unordered set. It learns a dense representation $z_v$ for each $v \\\\in V$, and then applies a permutation invariant function, e.g., sum/mean/max pooling, to aggregate all point representations so that they can handle irregular data (Zaheer et al., 2017; Charles et al., 2017). The second class of methods can better utilize geometric features and local information. These methods first construct a $k$-nn graph $G$ over the points in each sample based on their distances, e.g., $\\\\|r_v - r_u\\\\|$, and iteratively update the representation of point $v$ via aggregation $AGG(\\\\{z_u | u \\\\in N(v)\\\\})$, where $N(v)$ is the neighbors of point $v$ in graph $G$ and $AGG$ is a permutation invariant function. Then, another function is used to aggregate all point representations to make predictions. Compared with graph neural networks (GNNs) (Kipf & Welling, 2017; Xu et al., 2019; Veli\u010dkovi\u0107 et al., 2018) that encode graph-structured data without geometric features, GDL models often process geometric features carefully: Typically, these features are transformed into some scalars such as distances, angles and used as features so that some group (e.g., $E(3)$, $SO(3)$) invariances of the prediction can be kept (Fuchs et al., 2020; Gasteiger et al., 2020; Beaini et al., 2021; Satorras et al., 2021; Sch\u00fctt et al., 2021). Some models that perform 3D convolution over 3D data also belong to the second class because the convolution kernels can be viewed as one way to define the distance scalars for graph construction and neighborhood aggregation (Sch\u00fctt et al., 2017; Thomas et al., 2018; Wu et al., 2019). The third class will dynamically construct the $k$-nn graphs based on the hidden representations of points (Wang et al., 2019; Qu & Gouskos, 2020; Zhao et al., 2021). In this work, we focus on using the second class of models' architectures as the backbones because most scientific applications adopt this class of models.\\n\\n#### Interpretable Patterns in GDL.\\n\\nGiven a sample $C = (V, X, r)$, our goal of building an inherently interpretable model is that the model by itself can identify a subset of points $C_s = (V_s, X_s, r_s)$ that best indicates the label $y$. Mostly, $C_s$ will have a scientific meaning. For example, in the task to detect $\\\\tau \\\\rightarrow \\\\mu\\\\mu\\\\mu$ decay, our model should identify the detector hits left by the three $\\\\mu$'s but not the hits from other particles. We consider two types of indications of the label given by a point: existence importance, i.e., whether the point exists in the cloud is important to determine the label, and location importance, i.e., whether the geometric location of the point is important to determine the label. In the above example, the existence of the detector hits left by the three $\\\\mu$'s is of course important. On the other hand, the locations of these hits are also crucial because location features reflect the momentum of these particles when they pass through detectors, which should satisfy equations regarding certain invariant mass if they are indeed generated from a $\\\\tau \\\\rightarrow \\\\mu\\\\mu\\\\mu$ decay.\\n\\n### Methodology\\n\\nIn this section, we introduce our method Learnable Randomness Injection (LRI). In LRI, we have an interpreter $g$ and a classifier $f$. $g$ is to encode the original data and generate randomness to perturb the data. $f$ is to encode the perturbed data and make predictions. $g$ and $f$ are trained together to make accurate predictions while providing interpretability. LRI can be applied to a large class of GDL models to make them interpretable. We may choose a GDL model architecture as the backbone to build the data encoders in $g$ and $f$. These encoders could share or not share parameters. Below,\"}"}
{"id": "6u7mf9s2A9", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nFigure 2: The architectures of LRI-Bernoulli (top) and LRI-Gaussian (bottom).\\n\\nwe introduce specifics about this procedure. We first describe LRI-Bernoulli, where Bernoulli randomness is injected to measure the existence important of points. Then, we introduce LRI-Gaussian, which injects Gaussian randomness into geometric features to test the location importance of points. Finally, we connect our objectives with the information bottleneck principle (Tishby et al., 2000).\\n\\n4.1 LRI-BERNOULLI TO TEST EXISTENCE IMPORTANCE\\n\\nPipeline. Given a sample \\\\( C = (V, X, r) \\\\), we first construct a \\\\( k \\\\)-nn graph \\\\( G \\\\) based on the euclidean distance \\\\( \\\\| r_v - r_u \\\\| \\\\) between every pair of points \\\\( v, u \\\\in V \\\\). As shown in the top of Fig. 2, the interpreter \\\\( g \\\\) encodes \\\\( C \\\\), generates a representation \\\\( z_v \\\\) for each point \\\\( v \\\\) and uses the last component \\\\( h \\\\) to map \\\\( z_v \\\\) to \\\\( p_v \\\\in [0, 1] \\\\). Here, \\\\( h \\\\) consists of an MLP plus a sigmoid layer, and samples a Bernoulli mask for each point via \\\\( m_v \\\\sim \\\\text{Bern}(p_v) \\\\). The sampling is based on a reparameterization trick (Jang et al., 2017; Maddison et al., 2017) to make \\\\( dm_v dp_v \\\\) computable. The perturbed data \\\\( \\\\tilde{C} \\\\) is yielded by removing the points with \\\\( m_v = 0 \\\\) in \\\\( C \\\\). The edges in \\\\( G \\\\) connected to these points are also masked and removed, which gives a graph \\\\( \\\\tilde{G} \\\\). Finally, the classifier \\\\( f \\\\) takes as inputs \\\\( \\\\tilde{C} \\\\) and \\\\( \\\\tilde{G} \\\\) to make predictions.\\n\\nObjective. Eq. 1 shows our objective for each sample \\\\( C \\\\), where the first term is a cross-entropy loss for classification and the second term is a KL divergence regularizer. \\\\( \\\\beta \\\\) is the regularization coefficient and \\\\( \\\\text{Bern}(\\\\alpha) \\\\) is a predefined Bernoulli distribution with hyperparameter \\\\( \\\\alpha < 1 \\\\).\\n\\n\\\\[\\n\\\\min_{L_{CE}}(f(\\\\tilde{C}, \\\\tilde{G}), y) + \\\\beta \\\\sum_{v \\\\in V} \\\\text{D}_{KL}(\\\\text{Bern}(p_v) || \\\\text{Bern}(\\\\alpha))\\n\\\\] (1)\\n\\nHere, \\\\( f \\\\) is optimized via the first term. The interpreter \\\\( g \\\\) is optimized via the gradients that pass through the masks \\\\( \\\\{m_v\\\\} \\\\) \\\\( v \\\\in V \\\\) contained in \\\\( \\\\tilde{C} \\\\) and \\\\( \\\\tilde{G} \\\\) in the first term, and \\\\( \\\\{p_v\\\\} \\\\) \\\\( v \\\\in V \\\\) in the second term.\\n\\nInterpretation Rationale. The interpretation is given by the competition between the two terms in Eq. 1. The first term is to achieve good classification performance so it tends to denoise the data \\\\( \\\\tilde{C} \\\\) by reducing the randomness generated by \\\\( g \\\\), i.e., \\\\( p_v \\\\to 1 \\\\). The second term, on the other hand, tends to keep the level of randomness, i.e., \\\\( p_v \\\\to \\\\alpha \\\\). After training, \\\\( p_v \\\\) will converge to some value. If the existence of a point \\\\( v \\\\) is important to the label \\\\( y \\\\), then \\\\( p_v \\\\) should be close to 1. Otherwise, \\\\( p_v \\\\) is close to \\\\( \\\\alpha \\\\). We use \\\\( p_v \\\\)'s to rank the points \\\\( v \\\\in V \\\\) according to their existence importance.\\n\\n4.2 LRI-GAUSSIAN TO TEST LOCATION IMPORTANCE\\n\\nPipeline. We start from the same graph \\\\( G \\\\) as LRI-Bernoulli. As shown in the bottom of Fig. 2, here, the interpreter \\\\( g \\\\) will encode the data and map it to a covariance matrix \\\\( \\\\Sigma_v \\\\in \\\\mathbb{R}^{3 \\\\times 3} \\\\) for each point \\\\( v \\\\). Gaussian randomness \\\\( \\\\epsilon_v \\\\sim N(0, \\\\Sigma_v) \\\\) is then sampled to perturb the geometric features \\\\( \\\\tilde{r}_v = r_v + \\\\epsilon_v \\\\) of \\\\( v \\\\). Note that, to test location importance, a new \\\\( k \\\\)-nn graph \\\\( \\\\tilde{G} \\\\) is constructed based on perturbed distances \\\\( \\\\| \\\\tilde{r}_v - \\\\tilde{r}_u \\\\| \\\\). Reconstructing \\\\( \\\\tilde{G} \\\\) is necessary because using the original graph \\\\( G \\\\) will leak information from the original geometric features \\\\( r \\\\). Finally, the classifier \\\\( f \\\\) takes as inputs the location-perturbed data \\\\( \\\\tilde{C} = (V, X, \\\\tilde{r}) \\\\) and \\\\( \\\\tilde{G} \\\\) to make predictions.\\n\\nObjective. Eq. 2 shows the objective of LRI-Gaussian for each sample \\\\( C \\\\). Different from Eq. 1, here the regularization is a Gaussian \\\\( N(0, \\\\sigma I) \\\\), where \\\\( I \\\\) is an identity matrix and \\\\( \\\\sigma \\\\) is a hyperparameter.\\n\\n\\\\[\\n\\\\min_{L_{CE}}(f(\\\\tilde{C}, \\\\tilde{G}), y) + \\\\beta \\\\sum_{v \\\\in V} \\\\text{D}_{KL}(\\\\text{N}(0, \\\\Sigma_v) || \\\\text{N}(0, \\\\sigma I))\\n\\\\] (2)\\n\\nAgain, the classifier \\\\( f \\\\) will be optimized via the first term. The interpreter \\\\( g \\\\) will be optimized via the gradients that pass through the perturbation \\\\( \\\\{\\\\epsilon_v\\\\} \\\\) \\\\( v \\\\in V \\\\) implicitly contained in \\\\( \\\\tilde{C} \\\\) and \\\\( \\\\tilde{G} \\\\) in the first term, and \\\\( \\\\{\\\\Sigma_v\\\\} \\\\) \\\\( v \\\\in V \\\\) in the second term. However, there are two technical difficulties to be addressed.\"}"}
{"id": "6u7mf9s2A9", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGMENTS\\n\\nWe greatly thank the actionable suggestions given by reviewers. S. Miao and M. Liu are supported by the National Science Foundation (NSF) award OAC-2117997. Y. Luo is partially supported by a 2022 Seed Grant Program of the Molecule Maker Lab Institute, an NSF AI Institute (grant no. 2019897) at the University of Illinois Urbana-Champaign. P. Li is supported by the JPMorgan Faculty Award.\\n\\nREFERENCES\\n\\nGeorges Aad, Tatevik Abajyan, B Abbott, J Abdallah, S Abdel Khalek, Ahmed Ali Abdelalim, R Aben, B Abi, M Abolins, OS AbouZeid, et al. Observation of a new particle in the search for the standard model higgs boson with the atlas detector at the lhc. *Physics Letters B*, 2012.\\n\\nAlessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep representations. *The Journal of Machine Learning Research*, 2018.\\n\\nXiaocong Ai, Corentin Allaire, Noemi Calace, Ang\u00e8la Czirkos, Markus Elsing, Irina Ene, Ralf Farkas, Louis-Guillaume Gagnon, Rocky Garg, Paul Gessinger, et al. A common tracking software project. *Computing and Software for Big Science*, 2022.\\n\\nAlexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. *International Conference on Learning Representations*, 2017.\\n\\nMarco Ancona, Cengiz Oztireli, and Markus Gross. Explaining deep neural networks with a polynomial time algorithm for shapley value approximation. *International Conference on Machine Learning*, 2019.\\n\\nMartin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. *arXiv preprint arXiv:1907.02893*, 2019.\\n\\nKenneth Atz, Francesca Grisoni, and Gisbert Schneider. Geometric deep learning on molecular representations. *Nature Machine Intelligence*, 2021.\\n\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. *International Conference on Learning Representations*, 2015.\\n\\nBing Bai, Jian Liang, Guanhua Zhang, Hao Li, Kun Bai, and Fei Wang. Why attentions may not be interpretable? *ACM SIGKDD Conference on Knowledge Discovery & Data Mining*, 2021.\\n\\nDominique Beaini, Saro Passaro, Vincent L\u00e9tourneau, Will Hamilton, Gabriele Corso, and Pietro Li\u00f2. Directional graph networks. *International Conference on Machine Learning*, 2021.\\n\\nHelen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady N Bhat, Helge Weissig, Ilya N Shindyalov, and Philip E Bourne. The protein data bank. *Nucleic Acids Research*, 2000.\\n\\nChristian Bierlich, Smita Chakraborty, Nishita Desai, Leif Gellersen, Ilkka Helenius, Philip Ilten, Leif Lonnblad, Stephen Mrenna, Stefan Prestel, Christian T Preuss, Torbjorn Sjostrand, Peter Skands, Marius Utheim, and Rob Verheyen. A comprehensive guide to the physics and usage of pythia 8.3. *SciPost Physics*, 2022.\\n\\nPatrick Blackstone, Matteo Fael, and Emilie Passemar. $\\\\tau \\\\rightarrow \\\\mu\\\\mu\\\\mu$ decays? *The European Physical Journal C*, 2020.\\n\\nAlexander Bogatskiy, Brandon Anderson, Jan Offermann, Marwah Roussi, David Miller, and Risi Kondor. Lorentz group equivariant neural network for particle physics. *International Conference on Machine Learning*, 2020.\\n\\nSebastian Bordt, Mich\u00e8le Finck, Eric Raidl, and Ulrike von Luxburg. Post-hoc explanations fail to achieve their purpose in adversarial contexts. *ACM Conference on Fairness, Accountability, and Transparency*, 2022.\\n\\nMichael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. *IEEE Signal Processing Magazine*, 2017.\"}"}
{"id": "6u7mf9s2A9", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "6u7mf9s2A9", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "6u7mf9s2A9", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions. AAAI Conference on Artificial Intelligence, 2018.\\n\\nMeng Liu, Youzhi Luo, Kanji Uchino, Koji Maruhashi, and Shuiwang Ji. Generating 3D molecules for target protein binding. International Conference on Machine Learning, 2022.\\n\\nZhihai Liu, Minyi Su, Li Han, Jie Liu, Qifan Yang, Yan Li, and Renxiao Wang. Forging the basis for developing protein\u2013ligand interaction scoring functions. Accounts of Chemical Research, 2017.\\n\\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in Neural Information Processing Systems, 2017.\\n\\nScott M Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M Prutkin, Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. From local explanations to global understanding with explainable AI for trees. Nature Machine Intelligence, 2020.\\n\\nDongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. Parameterized explainer for graph neural network. Advances in Neural Information Processing Systems, 2020.\\n\\nChris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. International Conference on Learning Representations, 2017.\\n\\nKevin McCloskey, Ankur Taly, Federico Monti, Michael P Brenner, and Lucy J Colwell. Using attribution to decode binding mechanism in neural network models for chemistry. Proceedings of the National Academy of Sciences, 2019.\\n\\nSiqi Miao, Mia Liu, and Pan Li. Interpretable and generalizable graph learning via stochastic attention mechanism. International Conference on Machine Learning, 2022.\\n\\nBenjamin Nachman and Chase Shimmin. AI safety for high energy physics. arXiv preprint arXiv:1910.08606, 2019.\\n\\nVinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted Boltzmann machines. International Conference on Machine Learning, 2010.\\n\\nRobert Oerter. The theory of almost everything: The standard model, the unsung triumph of modern physics. Penguin, 2006.\\n\\nHuilin Qu and Loukas Gouskos. Jet tagging via particle clouds. Physical Review D, 2020.\\n\\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \u201cWhy should I trust you?\u201d explaining the predictions of any classifier. ACM SIGKDD International Conference on Knowledge Discovery and Data mining, 2016.\\n\\nSereina Riniker and Gregory A Landrum. Better informed distance geometry: using what we know to improve conformation generation. Journal of Chemical Information and Modeling, 2015.\\n\\nRibana Roscher, Bastian Bohn, Marco F Duarte, and Jochen Garcke. Explainable machine learning for scientific insights and discoveries. IEEE Access, 2020.\\n\\nCynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 2019.\\n\\nBenjamin Sanchez-Lengeling, Jennifer Wei, Brian Lee, Emily Reif, Peter Wang, Wesley Qian, Kevin McCloskey, Lucy Colwell, and Alexander Wiltschko. Evaluating attribution for graph neural networks. Advances in Neural Information Processing Systems, 2020.\\n\\nVictor Garcia Satorras, Emiel Hoogeboom, and Max Welling. (E)n equivariant graph neural networks. International Conference on Machine Learning, 2021.\"}"}
{"id": "6u7mf9s2A9", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"cloud embeddings to make predictions. All backbone models utilize implementations available in Pytorch-Geometric (PyG) (Fey & Lenssen, 2019).\\n\\nDirectly removing points from $C$ to yield $\\\\tilde{C}$ makes it non-differentiable. We provide two differentiable ways to approximate this step. The first way is to use another MLP to map the raw point features $X$ to a latent feature space $H$, and yield $\\\\tilde{C} = (V, (m_1^T \\\\odot H, r))$. Here $m$ is a vector containing $m_v$ for each point $v \\\\in V$, $1$ is a vector of all ones, and $\\\\odot$ denotes element-wise product. This is because masking on $X$ or $r$ is inappropriate as values in them have specific physical meanings. When the backbone model is implemented by using a message passing scheme, e.g., using PyG, another possible way is to use $m$ to mask the message sent by the points to be removed. We find both ways can work well and we adopt the second way in our experiments.\\n\\nWe use a softplus layer to output $a_1$ and $a_2$ to parameterize the Gaussian distribution. To make it numerically stable we clip the results of the softplus layer to $[1.0 \\\\times 10^{-6}, 1.0 \\\\times 10^6]$.\\n\\nFor $\\\\phi$ in the differentiable graph reconstruction module, we find it empirically a simple MLP can work well enough and thus we adopt it to implement $\\\\phi$.\\n\\nBaseline Methods. BernMask is extended from Ying et al. (2019) based on the authors' code and the implementation available in PyG. BernMask-P is extended from Luo et al. (2020) based on the authors' code and a recent PR in PyG. PointMask is reproduced based on the authors' code. GradGAM and GradGeo are extended based on the code from Gildenblat & contributors (2021).\\n\\nDiscussions on LRI. We note that both $f$ and $g$ in LRI needs a permutation equivariant encoder to learn point representations, and we find for simple tasks these two encoders can share parameters to reduce model size without degrading interpretation performance, while for challenging tasks using two different encoders may be beneficial. In our experiments we use the same encoder for ActsTrack, Tau3Mu, and SynMol, and use two different encoders for PLBind. If the model size is not a concern, using two encoders can be a good starting point. The other thing is that one of the key components in LRI is the perturbation function $h$, as shown in Fig. 2, and we have shown two ways to design $h$, i.e., Bernoulli perturbation and Gaussian perturbation. Nonetheless, $h$ can be generalized in many different ways. For example, $h$ is where one can incorporate domain knowledge and provide contextualized interpretation results, i.e., human understandable results. For instance, instead of perturbing molecules in the atom level, it is possible to perturb molecules in the functional group level, e.g., by averaging the learned perturbation in each functional group, so that the interpretation results can be more contextualized and more human understandable. In this work, we experiment this feature on PLBind by replacing the learned $\\\\{p_u\\\\}_{u \\\\in N(v)}$ or $\\\\{\\\\Sigma_u\\\\}_{u \\\\in N(v)}$ in a neighbourhood with the minimum $p$ or with the $\\\\Sigma$ having the maximum determinant in that neighbourhood, which encourages either aggressively perturbing all amino acids in a neighborhood or not perturbing any amino acids in the neighborhood, and we find this can better help discover binding sites for PLBind.\\n\\nSUPPLEMENTARY EXPERIMENTS\\n\\nF.1 ABLATION STUDIES\\n\\nTable 6 shows the effect of the differentiable graph reconstruction module in LRI-Gaussian, where $c$ is set to 200 on ActsTrack and to 5 on SynMol so that large perturbations are encouraged to better show the significance of this module, $\\\\beta$ is set to 0.01, and $f$ is first trained by 200 epochs and then $f$ and $g$ are trained together by 100 epochs. We observe significant drops when LRI-Gaussian is trained without differentiable graph reconstruction, which matches our expectations and validates the necessity of the proposed module.\\n\\nF.2 FINE-GRAINED GEOMETRIC PATTERN LEARNING\\n\\nTo discover fine-grained geometric information in ActsTrack using LRI-Gaussian as shown in Table 3, we conduct experiments based on DGCNN, where $f$ is first trained with 100 epochs, and then $g$ is further trained with 500 epochs while $f$ is finetuned with a learning rate of $1 \\\\times 10^{-8}$. $f$ and $g$ use two different encoders, $\\\\beta$ is set to 10, $c$ is 100, all dropout ratio is set to 0, and $\\\\|r_v\\\\|$ is used as 22.\"}"}
{"id": "6u7mf9s2A9", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"point features. Finally, we let $g$ directly output covariance matrix $\\\\Sigma \\\\in \\\\mathbb{R}^{2 \\\\times 2}$, i.e., in x-y space, to only perturb the first two dimensions of $r$.\\n\\nLIMITATIONS AND FUTURE DIRECTIONS OF LRI\\n\\n- Current LRI provides interpretation results at point-level, while cannot find out geometric patterns shared by a group of points (e.g., some set of points may be allowed to rotate around a reference point in space).\\n- Although Gaussian noise can help capture some fine-grained geometric patterns, if the underlying geometric patterns get too complicated, LRI-Gaussian may degrade to only offer a ranking of point importance and have limited capability to capture meaningful fine-grained geometric patterns. It may be possible to design customized noise with prior knowledge to detect different types of geometric patterns (e.g., what if some points can be freely moved along a curve without affecting prediction loss?), and can we even not use a fixed noise distribution (i.e., Gaussian or Bernoulli) and also make it trainable? Meanwhile, current LRI would need specific tuning to yield fine-grained interpretation results, which may be improved with better training strategies.\\n- Current LRI may provide interpretation results that are not contextualized enough to guide scientists for further research. For example, chemists may prefer an interpretable model to tell them directly which functional groups are important instead of which atoms are important.\"}"}
{"id": "6u7mf9s2A9", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Michael Sejr Schlichtkrull, Nicola De Cao, and Ivan Titov. Interpreting graph neural networks for \\\\textit{nlp} with differentiable edge masking. \\\\textit{International Conference on Learning Representations}, 2021.\\n\\nReinhard Schulte, Vladimir Bashkirov, Tianfang Li, Zhengrong Liang, Klaus Mueller, Jason Heimann, Leah R Johnson, Brian Keeney, HF-W Sadrozinski, Abraham Seiden, et al. Conceptual design of a proton computed tomography system for applications in proton radiation therapy. \\\\textit{IEEE Transactions on Nuclear Science}, 2004.\\n\\nKristof Sch\\\"utt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert M\\\"uller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. \\\\textit{Advances in Neural Information Processing Systems}, 2017.\\n\\nKristof Sch\\\"utt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. \\\\textit{International Conference on Machine Learning}, 2021.\\n\\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. \\\\textit{IEEE International Conference on Computer Vision}, 2017.\\n\\nSofia Serrano and Noah A Smith. Is attention interpretable? \\\\textit{Association for Computational Linguistics}, 2019.\\n\\nJonathan Shlomi, Peter Battaglia, and Jean-Roch Vlimant. Graph neural networks in particle physics. \\\\textit{Machine Learning: Science and Technology}, 2020.\\n\\nAvanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. \\\\textit{International Conference on Machine Learning}, 2017.\\n\\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. \\\\textit{Journal of Machine Learning Research}, 2014.\\n\\nHannes St\\\"ark, Octavian Ganea, Lagnajit Pattanaik, Dr.Regina Barzilay, and Tommi Jaakkola. EquiBind: Geometric deep learning for drug binding structure prediction. \\\\textit{International Conference on Machine Learning}, 2022.\\n\\nQingyun Sun, Jianxin Li, Hao Peng, Jia Wu, Xingcheng Fu, Cheng Ji, and S Yu Philip. Graph structure learning with variational information bottleneck. \\\\textit{AAAI Conference on Artificial Intelligence}, 2022.\\n\\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. \\\\textit{International Conference on Machine Learning}, 2017.\\n\\nSaeid Asgari Taghanaki, Kaveh Hassani, Pradeep Kumar Jayaraman, Amir Hosein Khasahmadi, and Tonya Custis. Pointmask: Towards interpretable and bias-resilient point cloud processing. \\\\textit{arXiv preprint arXiv:2007.04525}, 2020.\\n\\nNathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. \\\\textit{arXiv preprint arXiv:1802.08219}, 2018.\\n\\nMark Thomson. \\\\textit{Modern particle physics}. Cambridge University Press, 2013.\\n\\nNaftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. \\\\textit{arXiv preprint physics/0004057}, 2000.\\n\\nRaphael JL Townshend, Stephan Eismann, Andrew M Watkins, Ramya Rangan, Maria Karelina, Rhiju Das, and Ron O Dror. Geometric deep learning of rna structure. \\\\textit{Science}, 2021.\\n\\nGabor E Tusnady and Istvan Simon. Principles governing amino acid composition of integral membrane proteins: application to topology prediction. \\\\textit{Journal of Molecular Biology}, 1998.\"}"}
{"id": "6u7mf9s2A9", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "6u7mf9s2A9", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Let's assume the sample with its label $$(C, Y) \\\\sim P_{C \\\\times Y}$$. We ignore the $\\\\tilde{G}$ in our objectives to keep the notation simple, then, the IB objective is:\\n\\n$$\\\\min -I(\\\\tilde{C}; Y) + \\\\beta I(\\\\tilde{C}; C),$$\\n\\n(3)\\n\\nwhere $\\\\tilde{C}$ is the perturbed sample, and $I(\\\\cdot; \\\\cdot)$ denotes the mutual information between two random variables.\\n\\nFor the first term $-I(\\\\tilde{C}; Y)$, by definition we have:\\n\\n$$-I(\\\\tilde{C}; Y) = -E_{\\\\tilde{C}, Y} \\\\log P(Y | \\\\tilde{C}) P(Y).$$\\n\\n(4)\\n\\nWe introduce a variational approximation $P_\\\\theta(Y | \\\\tilde{C})$ for $P(Y | \\\\tilde{C})$ as it is intractable. Then, we yield a variational upper bound:\\n\\n$$-I(\\\\tilde{C}; Y) = -E_{\\\\tilde{C}, Y} \\\\log P_\\\\theta(Y | \\\\tilde{C}) P(Y) - E_{\\\\tilde{C}} D_{KL}(P(Y | \\\\tilde{C}) \\\\parallel P_\\\\theta(Y | \\\\tilde{C})).$$\\n\\n(5)\\n\\nwhere $H(Y)$ is the entropy of $Y$ which is a constant. We use the prediction model $f$ paired with the cross-entropy loss $L_{CE}(f(\\\\tilde{C}), Y)$ to represent $-E_{\\\\tilde{C}, Y} \\\\log P_\\\\theta(Y | \\\\tilde{C})$, minimizing which is thus equivalent to minimizing a variational upper bound of $-I(\\\\tilde{C}; Y)$.\\n\\nFor the second term $I(\\\\tilde{C}; C)$, because $\\\\tilde{C} = g(C)$. Suppose $\\\\phi$ is the parameter of $g$. By definition, we have:\\n\\n$$I(\\\\tilde{C}; C) = E_{\\\\tilde{C}, C} \\\\log P_\\\\phi(\\\\tilde{C} | C) P(\\\\tilde{C}).$$\\n\\n(6)\\n\\nAs $P(\\\\tilde{C})$ is intractable, we introduce a variational approximation $Q(\\\\tilde{C})$. Then, we yield a variational upper bound:\\n\\n$$I(\\\\tilde{C}; C) = E_{\\\\tilde{C}, C} \\\\log P_\\\\phi(\\\\tilde{C} | C) Q(\\\\tilde{C}) - D_{KL}(P(\\\\tilde{C}) \\\\parallel Q(\\\\tilde{C})).$$\\n\\n(7)\\n\\nFor LRI-Bernoulli, $g_\\\\phi$ takes as input $C = (V, X, r)$ and first outputs $p_v \\\\in [0, 1]$ for each point $v \\\\in V$. Then, it samples $m_v \\\\sim \\\\text{Bern}(p_v)$ and yields $\\\\tilde{C}$ by removing all points with $m_v = 0$ in $C$. This procedure gives $P(\\\\tilde{C} | C) = P_{v \\\\in V} P(m_v | p_v)$, which essentially makes $m_v$ conditionally independent across different points given the input point cloud $C$. In this case, we define $Q(\\\\tilde{C})$ as follows. For every point cloud $C \\\\sim P_C$, we sample $m'_v \\\\sim \\\\text{Bern}(\\\\alpha)$, where $\\\\alpha \\\\in [0, 1]$ is a hyperparameter. We remove all points in $C$ and add points when their $m'_v = 1$. This procedure gives $Q(\\\\tilde{C}) = P_C P_{v \\\\in V} P(m'_v | C) P(C)$. As $m'_v$ is independent from $C$ given its size $n$, $Q(\\\\tilde{C}) = P_n P_{v \\\\in V} P(m'_v) = P_n Q_{n, v=1} P(m'_v)$, where $P_n$ is a constant. Then, we yield:\\n\\n$$D_{KL}(P(\\\\tilde{C} | C) \\\\parallel Q(\\\\tilde{C})) = \\\\sum_{v \\\\in V} D_{KL}(\\\\text{Bern}(p_v) \\\\parallel \\\\text{Bern}(\\\\alpha)) + c(n, \\\\alpha),$$\\n\\n(8)\\n\\nwhere $c(n, \\\\alpha)$ does not contain parameters to be optimized. Therefore, minimizing the second term of LRI-Bernoulli is equivalent to minimizing a variational upper bound of $I(\\\\tilde{C}; C)$. Now, we can conclude that the objective of LRI-Bernoulli is a variational upper bound of the IB principle.\"}"}
{"id": "6u7mf9s2A9", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\( B = 2T \\\\)\\n\\n\\\\( B = 10T \\\\)\\n\\n\\\\( B = 20T \\\\)\\n\\n\\\\( B = 0T \\\\)\"}"}
{"id": "6u7mf9s2A9", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics of the four datasets.\\n\\n| Dataset    | # Classes | # Features in $X$ | # Dimensions in $r$ | # Samples | Avg. # Points/Sample | Avg. # Important Points/Sample | Class Ratio | Split Scheme | Split Ratio |\\n|------------|-----------|-------------------|---------------------|-----------|----------------------|-------------------------------|-------------|--------------|-------------|\\n| ActsTrack  | 2         | 0                 | 3                   | 3241      | 109.1                | 22.8                          | 39/61       | Random       | 70/15/15    |\\n| Tau3Mu     | 2         | 1                 | 2                   | 129687    | 16.9                 | 5.5                           | 24/76       | Random       | 70/15/15    |\\n| SynMol     | 2         | 1                 | 3                   | 8663      | 21.9                 | 6.6                           | 18/82       | Patterns     | 78/11/11    |\\n| PLBind     | 2         | 3                 | 3                   | 10891     | 339.8                | 132.2                         | 29/71       | Time         | 92/6/2      |\\n\\nFirst, how to parameterize $\\\\Sigma_v$ as it should be positive definite, and then how to make $d\\\\epsilon_v d\\\\Sigma_v$ computable? Our solution is to let the last component $h$ in $g$ map representation $z_v$ not to $\\\\Sigma_v$ directly but to a dense matrix $U_v \\\\in \\\\mathbb{R}^{3 \\\\times 3}$ via an MLP and two scalars $a_1, a_2 \\\\in \\\\mathbb{R}^+$ via a softplus layer. Then, the covariance matrix is computed by $\\\\Sigma_v = a_1 U_v U_v^T + a_2 I$. Moreover, we find using $\\\\Sigma_v$ and the reparameterization trick for multivariate Gaussian implemented by PyTorch is numerically unstable as it includes Cholesky decomposition. So, instead, we use the reparameterization trick $\\\\epsilon_v = \\\\sqrt{a_1 U_v s_1} + \\\\sqrt{a_2 I s_2}$, where $s_1, s_2 \\\\sim N(0, I)$. It is not hard to show that $E[\\\\epsilon_v \\\\epsilon_v^T] = \\\\Sigma_v$.\\n\\nSecond, the construction of the $k$-nn graph $\\\\tilde{G}$ based on $\\\\tilde{r}_v$ is not differentiable, which makes the gradients of $\\\\{\\\\epsilon_v\\\\}_{v \\\\in V}$ that pass through the structure of $\\\\tilde{G}$ not computable. We address this issue by associating each edge $v, u$ in $\\\\tilde{G}$ with a weight $w_{vu} \\\\in (0, 1)$ that monotonically decreases w.r.t. the distance, $w_{vu} = \\\\phi(\\\\|\\\\tilde{r}_v - \\\\tilde{r}_u\\\\|)$. These weights are used in the neighborhood aggregation procedure $f$. Specifically, for the central point $v$, $f$ adopts aggregation $\\\\text{AGG}(\\\\{w_{vu} z_u | u \\\\in N(v)\\\\})$, where $z_u$ is the representation of the neighbor point $u$ in the current layer. This design makes the structure of $\\\\tilde{G}$ differentiable. Moreover, because we set $w_{uv} < 1$, the number of used nearest neighbors to construct $\\\\tilde{G}$ is \u201cconceptually\u201d smaller than $k$. So, in practice, we choose a slightly larger number (say 1.5 $k$) of nearest neighbors to construct $\\\\tilde{G}$ and adopt the above strategy.\\n\\nInterpretation Rationale. The interpretation rationale is similar to that of LRI-Bernoulli, i.e., given by the competition between the two terms in Eq. 1. The first term is to achieve good classification performance by reducing the randomness generated by $g$. The second term, on the other hand, tends to keep the level of randomness, i.e., $\\\\Sigma_v \\\\rightarrow \\\\sigma I$. After training, the convergent determinant $|\\\\Sigma_v|$ which characterizes the entropy of injected Gaussian randomness, indicates the location importance of point $v$. We use $|\\\\Sigma_v|$\u2019s to rank the points $v \\\\in V$ according to their location importance.\\n\\nFine-grained Interpretation on Location Importance. Interestingly, the convergent $\\\\Sigma_v$ implies more fine-grained geometric information, i.e., how different directions of perturbations on point $v$ affect the prediction. This can be analyzed by checking the eigenvectors of $\\\\Sigma_v$. As illustrated in the figure on the right, $\\\\Sigma_v$ of point $v$ at $A$ is represented by the ellipses $\\\\{x: x^T \\\\Sigma_v x < \\\\theta\\\\}$ for different $\\\\theta$\u2019s. It tells perturbing $v$ towards the direction $B$ affects the prediction less than perturbing $v$ towards the orthogonal direction. As a showcase, later, we use such fine-grained information to conduct an in-depth analysis of HEP data.\\n\\n4.3 CONNECTING LRI AND THE INFORMATION BOTTLENECK PRINCIPLE. Our objectives Eq. 1 and Eq. 2 are essentially variational upper bounds of the information bottleneck (IB) principle (Tishby et al., 2000; Alemi et al., 2017) whose goal is to reduce the mutual information between $C$ and $\\\\tilde{C}$ while keeping the mutual information between $\\\\tilde{C}$ and the label, i.e., $\\\\min -I(\\\\tilde{C}; Y) + \\\\beta I(\\\\tilde{C}; C)$. We provide derivations in Appendix A. Grounded on the IB principle, LRI tends to extract minimal sufficient information to make predictions and can be more robust to distribution shifts between training and test datasets (Achille & Soatto, 2018; Wu et al., 2020; Miao et al., 2022).\"}"}
{"id": "6u7mf9s2A9", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Then, we compare interpretation labels and interpretability scores to measure interpretation performance. We report two metrics: interpretation ROC AUC and precision at 20% and 40% of points.\\n\\n### Table 2: Interpretation performance on the four datasets\\n\\n| Method          | Dataset | ROC AUC | Prec@20 | Prec@40 |\\n|-----------------|---------|---------|---------|---------|\\n| LRI-Bernoulli   |         |         |         |         |\\n| BernMask        |         |         |         |         |\\n| PointMask       |         |         |         |         |\\n| GradGeo         |         |         |         |         |\\n\\nNote: The table shows the interpretation performance on the four datasets. The values are rounded to two decimal places. The numbers in the table are the result of the interpretation methods on the datasets.\\n\\nGradient-based methods have no hyperparameters to tune, while all other methods have hyperparameters that are tuned in our datasets.\\n\\nReference: E. R. and Underline.\"}"}
{"id": "6u7mf9s2A9", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: The angle (\u00b0, mean \u00b1 std) between the velocity and the first principal component of $\\\\Sigma^v$ (LRI-Gaussian) v.s. between the velocity and the direction orthogonal to the gradient of $r^v$ in x-y space (GradGeo) under different magnetic field strengths (T).\\n\\n| Magnetic Field Strength (T) | Random | GradGeo |\\n|----------------------------|--------|---------|\\n| 2T                         | 45     | 22.89  \u00b1 2.45 |\\n| 4T                         | 45     | 18.85  \u00b1 1.29 |\\n| 6T                         | 45     | 34.79  \u00b1 2.12 |\\n| 8T                         | 45     | 30.57  \u00b1 2.82 |\\n| 10T                        | 45     | 37.22  \u00b1 3.18 |\\n| 12T                        | 45     | 38.01  \u00b1 3.24 |\\n| 14T                        | 45     | 36.26  \u00b1 5.34 |\\n| 16T                        | 45     | 38.57  \u00b1 1.30 |\\n| 18T                        | 45     | 38.01  \u00b1 3.24 |\\n| 20T                        | 45     | 38.89  \u00b1 1.03 |\\n\\nLRI-Gaussian: 5.09 \u00b1 0.97, 5.17 \u00b1 0.42, 5.65 \u00b1 1.05, 6.67 \u00b1 1.17, 7.50 \u00b1 2.37, 7.57 \u00b1 1.80, 7.89 \u00b1 1.03, 7.57 \u00b1 1.80, 7.50 \u00b1 2.37, 5.09 \u00b1 0.97\\n\\nFine-grained Geometric Patterns. We find LRI-Gaussian can discover fine-grained geometric patterns. Intuitively, slight perturbation of each point along the underlying track direction will affect the model prediction less than the same level of perturbation orthogonal to the track. Therefore, the principal component of $\\\\Sigma^v$ in x-y space, i.e., the space orthogonal to the direction of the background magnetic field, can give an estimation of the track direction at $v$. Table 3 provides the evaluation of track direction (velocity direction) estimation based on analyzing $\\\\Sigma^v$. Here, we test the background magnetic field changing from 2T to 20T. LRI-Gaussian is far more accurate than GradGeo. The latter uses the gradients of different coordinates to compute the most sensitive direction. Moreover, the ratio between the lengths of two principal components of $\\\\Sigma^v$ in x-y space gives an estimation of the curvature of the track at $v$, which is proportional to the strength of the magnetic field $B$ up to a constant multiplier (due to the law \u2013 Lorentz force $F \\\\propto B$). Therefore, we can estimate $B$ by analyzing $\\\\Sigma^v$. Fig. 3 shows this approach provides an almost accurate estimation of $B$ up to a constant multiplier. Fig. 4 provides visualizations of the yielded fine-grained patterns, and a detailed analysis of the obtained fine-grained interpretation patterns can be found in Appendix B.\\n\\n5.2 Tau3Mu: Tau3Mu Detection in Proton-Proton Collisions\\n\\nThis task is to predict whether a proton-proton collision event contains a $\\\\tau \\\\rightarrow \\\\mu\\\\mu\\\\mu$ decay, which is similar to ActsTrack, while in Tau3Mu the $\\\\mu$'s are a lot softer, and only the hits from the first layer of detectors are used. Each point in a point cloud sample is a detector hit associated with a local bending angle and a 2D coordinate in the pseudorapidity-azimuth ($\\\\eta$-$\\\\phi$) space. We use the hits generated from this decay to test interpretation performance. As shown in Table 2, LRI-Gaussian still works the best. LRI-Bernoulli and GradGeo are close, and both are the second best. While GradGAM still works well on some backbones, all masking-based methods do not perform well.\\n\\n5.3 Syn Mol: Molecular Property Prediction with Synthetic Properties\\n\\nThis task is to predict whether a molecule contains both functional groups branched alkanes and carbonyl, which together give certain synthetic properties (McCloskey et al., 2019; Sanchez-Lengeling et al., 2020). Each point in a point cloud sample is an atom associated with a 3D coordinate and a categorical feature indicating the atom type. We use the atoms in these two functional groups to test interpretation performance. As shown in Table 2, LRI-Gaussian performs consistently the best by only perturbing geometric features in molecules, and LRI-Bernoulli works the second best and achieves comparable performance with LRI-Gaussian on Point Transformer. This shows that both the existence and locations of atoms are critical and further validates the benefit of using 3D representations of molecules in the tasks like molecular property prediction. Among other methods, ...\"}"}
{"id": "6u7mf9s2A9", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Generalization performance of LRI. Classification AUC on test sets are reported with mean \u00b1 std for all backbones on the four datasets.\\n\\n| ActsTrack | Tau3Mu | SynMol | PLBind |\\n|-----------|--------|--------|--------|\\n| DGCNN     | 97.99 \u00b1 0.38 | 96.75 \u00b1 0.21 | 97.45 \u00b1 0.52 |\\n| Point Transformer | 97.45 \u00b1 0.52 | 98.95 \u00b1 0.03 | 99.87 \u00b1 0.42 |\\n| EGNN      | 87.38 \u00b1 0.08 | 86.20 \u00b1 0.13 | 86.45 \u00b1 0.09 |\\n| DGCNN     | 86.38 \u00b1 0.08 | 86.44 \u00b1 0.08 | 86.56 \u00b1 0.11 |\\n| Point Transformer | 86.56 \u00b1 0.11 | 87.98 \u00b1 0.01 | 88.13 \u00b1 0.53 |\\n| EGNN      | 80.17 \u00b1 0.23 | 83.13 \u00b1 1.19 | 86.83 \u00b1 2.06 |\\n\\nTable 5: Generalization performance of LRI with distribution shifts. The column name \\\\( d_1 - d_2 \\\\) denotes the models are validated and tested on samples with \\\\( d_1 \\\\) and \\\\( d_2 \\\\) tracks, respectively.\\n\\n| ActsTrack | 10-10 | 15-20 | 20-30 | 25-40 | 30-50 | 35-60 | 40-70 |\\n|-----------|-------|-------|-------|-------|-------|-------|-------|\\n| ERM       | 96.33 \u00b1 0.65 | 93.83 \u00b1 0.34 | 91.14 \u00b1 1.07 | 87.85 \u00b1 1.54 | 85.96 \u00b1 1.74 | 84.96 \u00b1 1.95 | 84.19 \u00b1 1.54 |\\n| LRI-Bernoulli | 97.05 \u00b1 0.71 | 94.66 \u00b1 0.75 | 93.08 \u00b1 0.94 | 90.93 \u00b1 1.47 | 89.11 \u00b1 1.76 | 88.11 \u00b1 1.95 | 87.41 \u00b1 1.54 |\\n| LRI-Gaussian | 97.51 \u00b1 0.76 | 95.69 \u00b1 0.80 | 94.64 \u00b1 1.39 | 92.52 \u00b1 1.97 | 90.85 \u00b1 2.13 | 89.85 \u00b1 2.31 | 89.13 \u00b1 2.06 |\\n\\nGradGAM, BernMask-P and PointMask are unstable and can only provide some interpretability for one or two backbones, while GradGeo and BernMask seem to fail to perform well on SynMol.\\n\\n5.4 PLBIND: PROTEIN-LIGAND BINDING AFFINITY Prediction: This task is to predict whether a protein-ligand pair is of affinity \\\\( K_D < 10 \\\\) nM. Each point in a protein is an amino acid associated with a 3D coordinate, a categorical feature indicating the amino acid type, and two scalar features. Each point in a ligand is an atom associated with a 3D coordinate, a categorical feature indicating the atom type, and a scalar feature. Different from other datasets, each sample in PLBIND contains two sets of points. So, for each sample, two encoders will be used to encode the ligand and the protein separately, and the obtained two embeddings will be added to make a prediction. As shown in Table 2, LRI-Bernoulli outperforms all other methods, while LRI-Gaussian achieves comparable performance on EGNN. This might indicate that to make good predictions on PLBIND, the existence of certain groups of amino acids is more important than their exact locations. Interestingly, all other methods do not seem to perform well on PLBIND. Moreover, all methods have low ROC AUC, which suggests only a part of but not the entire binding site is important to decide the binding affinity.\\n\\n5.5 GENERALIZATION PERFORMANCE AND ABLATION STUDIES OF LRI LRI-induced models can generalize better while being interpretable. As shown in Table 4, both LRI-induced models never degrade prediction performance and sometimes may even boost it compared with models trained without LRI, i.e., using empirical risk minimization (ERM). Moreover, LRI-induced models are more robust to distribution shifts as LRI is grounded on the IB principle. Table 5 shows a study with shifts on the numbers of particle tracks, where all models are trained on samples with 10 particle tracks, and tested on samples with a different number of (from 10 to 70) tracks. We observe LRI-induced models work consistently better than models trained naively. We also conduct ablation studies on the differentiable graph reconstruction module proposed specifically for geometric data in LRI-Gaussian, we find that without this module the interpretation performance of LRI-Gaussian may be reduced up to 49% on ActsTrack and up to 23% on SynMol, which shows the significance of this module. More details of the ablation study can be found in Appendix F.1.\\n\\n6 CONCLUSION This work systematically studies interpretable GDL models by proposing a framework Learnable Randomness Injection (LRI) and four datasets with ground-truth interpretation labels from real-world scientific applications. We have studied interpretability in GDL from the perspectives of existence importance and location importance of points, and instantiated LRI with LRI-Bernoulli and LRI-Gaussian to test the two types of importance, respectively. We observe LRI-induced models provide interpretation best aligning with scientific facts, especially LRI-Gaussian that tests location importance. Grounded on the IB principle, LRI never degrades model prediction performance, and may often improve it when there exist distribution shifts between the training and test scenarios.\"}"}
{"id": "6u7mf9s2A9", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"randomness on these points would greatly affect model prediction and the prediction loss in Eq. 2 would not allow this to happen. As shown quantitatively in Table 2, $|\\\\Sigma_v|$ measures the importance of each point very well and can provide the best interpretation AUC in most cases.\\n\\nThe bottom row of Figure 4 shows that LRI-Gaussian is also capable of discovering fine-grained geometric patterns. In the examples, a small perturbation on each point along the underlying track direction may not even affect the model prediction (because the track radius may not change a lot by such perturbations). Therefore, the principal component (i.e., the eigenvector that corresponds to the largest eigenvalue) of $\\\\Sigma_v$ in x-y space can give an estimation of the track direction at $v$. As shown quantitatively in Table 3, the principal component can actually estimate the track direction very well. Meanwhile, we can also see that the ratio between the lengths of two principal components (i.e., the $2^{nd}$ Largest Eigenvalue $/ \\\\text{Largest Eigenvalue}$) of $\\\\Sigma_v$ varies when the strength of the background magnetic field $B$ changes, and as shown quantitatively in Figure 3, this ratio provides an almost accurate estimation of $B$ up to a constant multiplier. This is because the direction orthogonal to the track direction is the direction of Lorentz force, and when the principal component of $\\\\Sigma_v$ in x-y space indicates the track direction, the second principal component in x-y space would be the direction of Lorentz force. Since Lorentz force $F \\\\propto B$ and the properties of $\\\\mu$'s (e.g., momentum) from the $\\\\tau \\\\rightarrow \\\\mu\\\\mu$ decay is independent of magnetic field strength, the ratio $2^{nd}$ Largest Eigenvalue $/ \\\\text{Largest Eigenvalue}$ can imply the strength of the background magnetic field.\\n\\n**Details of the Datasets**\\n\\n**C.1 Background and Tasks**\\n\\nWe describe the background and tasks of our datasets in more detail in this section. \\n\\n**ActsTrack.** As illustrated in Fig. 1a, protons will collide at the center of the detectors, and a number of interactions will happen, where different interactions may produce different particles, e.g., a $\\\\tau \\\\rightarrow \\\\mu\\\\mu$ decay produces two $\\\\mu$'s. The produced particles will then fly through multiple layers of detectors with a magnetic field, and Fig. 1a shows an example that has four layers of ring detectors with a 2T magnetic field parallel to z-axis. When a particle flies through such detectors, the detectors will record where the particle hits the detector (i.e., geometric coordinates) and measure certain properties of the particle (e.g., momentum) depending on the type of the detector. And if the particle is charged, then its track in space may be curved due to the magnetic field. As such, each particle will leave a set of points on the detectors with some measured properties, and all particles produced from the collision will then form a point cloud, where a point is just a particle hit on a detector. However, we are only interested in a certain interaction, i.e., the $\\\\tau \\\\rightarrow \\\\mu\\\\mu$ decay, and do not care other interactions (these interactions are called pileup interactions or background events). Fortunately, the particle hits in the point cloud will reveal the information about the interactions happened in the collision. For example, if a $\\\\tau \\\\rightarrow \\\\mu\\\\mu$ decay just happened, then there should be two sets of points in the point cloud that are left by the two $\\\\mu$'s just produced from the decay. In principle, it is possible to reconstruct the type of a particle based on its tracks and/or some other measured properties in a magnetic field, and once we can find that there are two tracks that must be caused by two $\\\\mu$'s (with certain invariant mass), then we would know a $\\\\tau \\\\rightarrow \\\\mu\\\\mu$ decay just happened in the collision. Therefore, the classification task of ActsTrack is to predict if there exists a $\\\\tau \\\\rightarrow \\\\mu\\\\mu$ decay or not in the collision given the point cloud measured by the detectors. Each positive sample contains particle hits from both a $\\\\tau \\\\rightarrow \\\\mu\\\\mu$ decay and some pileup interactions, while each negative sample has only hits from pileup interactions. In principle, if a classifier can successfully predict the existence of a $\\\\tau \\\\rightarrow \\\\mu\\\\mu$ decay, it should be aware of which sets of points in the point cloud represent the tracks of the $\\\\mu$'s. Therefore, the particle hits left by the two $\\\\mu$'s are labeled as ground-truth interpretation, and we expect an interpretable model to highlight these points as important points for the classification task.\\n\\n**Tau3Mu.** Even though Tau3Mu has a similar basic HEP setup as ActsTrack, as shown in Fig. 1b, Tau3Mu now has a different configuration of the detectors and the magnetic field. Besides, the underlying physics process of interest is different and the task is with a different physics motivation (as shown in Sec. 1). The formulation of the machine learning task is a classification task to predict if there exists a $\\\\tau \\\\rightarrow \\\\mu\\\\mu\\\\mu$ decay or not given a point cloud. Each positive sample contains both a $\\\\tau \\\\rightarrow \\\\mu\\\\mu\\\\mu$ decay and some pileup interactions, while each negative sample has only pileup interactions. 18\"}"}
{"id": "6u7mf9s2A9", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Likewise, if a classifier can successfully predict whether there is a $\\\\tau \\\\rightarrow \\\\mu\\\\mu\\\\mu$ decay or not, we expect those particle hits left by the three $\\\\mu$'s produced by the $\\\\tau \\\\rightarrow \\\\mu\\\\mu\\\\mu$ decay to be the important points for the classification task, and an interpretable model should be able to highlight these points.\\n\\nSynMol. Different from typical molecular property prediction tasks, SynMol provides 3D representations of molecules instead of 2D chemical bond graphs. Namely, each molecule is represented as a 3D point cloud with each point being an atom in space. The classification task of SynMol is to predict if a molecule has a certain property or not. As shown in Fig. 1c, we know that the property of interest is determined by two certain functional groups (i.e., specific groupings of atoms). Therefore, we expect an interpretable model that can successfully predict the property should be able to highlight those atoms in the two functional groups as the important points for the classification task.\\n\\nPLBind. The classification task of PLBind is to predict if a pair of protein and ligand can bind together (with a high affinity) or not given the 3D structures of proteins and ligands. Notably, Fig. 1d only shows the surface of a protein for better visualization, and actually proteins in PLBind are represented as 3D point clouds, where each point is just an amino acid in the protein. Similarly, ligands are just small molecules, and they are also represented as 3D point clouds, where each point is an atom in the ligand. In principle, as shown in Fig. 1d, a high binding affinity should largely depend on the amino acids of the protein that are near the binding site. Therefore, we label those amino acids near the binding site as ground-truth interpretation, and we expect an interpretable classifier that can successfully predict the high-affinity pairs should be able to highlight those amino acids as important points for the classification task.\\n\\nC.2 DATASET COLLECTION\\nWe elaborate how we collect the four datasets in this section. The statistics of the four datasets are shown in Table 1.\\n\\nBasic Settings. 1) For each sample in the four datasets, all points are centered at the origin. 2) Only positive samples in our datasets have ground-truth interpretation labels, so we only evaluate interpretation performance on positive samples. 3) For any pair of points $v$ and $u$ in the four datasets, they have an edge feature of $(\\\\|r_v - r_u\\\\|, r_v - r_u, \\\\|r_v - r_u\\\\|)$ if they are connected in the constructed $k$-nn graph.\\n\\nActsTrack. $\\\\tau \\\\rightarrow \\\\mu\\\\mu$ events are simulated with PYTHIA generator (Bierlich et al., 2022) overlaid with soft QCD pileup events, and particle tracks are simulated using Acts Common Tracking Software (Ai et al., 2022). For all samples, ten pileup interactions are generated with a center-of-mass energy of $14$ TeV, and the additional hard scatter interaction is only generated for positive samples. Particle tracks are simulated with a magnetic field parallel to the $z$ axis, and the default ActsTrack is simulated with $B = 2$ T. To make sure both $\\\\mu$'s from the decay are properly measured, we calculate the invariant mass $m_{ij}$ for every pair of $\\\\mu$'s in the generated data using Eq. 10 so that every positive sample in our dataset has at least a pair of $\\\\mu$ with an invariant mass close to $91.19$ GeV, i.e., the mass of the $z$ bosons.\\n\\n$$m_{ij}^2 = m_{\\\\mu}^2 + p_{x,i}^2 + p_{y,i}^2 + p_{z,i}^2 - (p_{x,j}^2 + p_{y,j}^2 + p_{z,j}^2).$$ (10)\"}"}
{"id": "6u7mf9s2A9", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\n### Tau3Mu\\n\\nThe \\\\( \\\\tau \\\\) leptons produced in decays of D and B mesons simulated by the PYTHIA generator (Bierlich et al., 2022) are used to generate the signal samples. The background events are generated with multiple soft QCD interactions modeled by the PYTHIA generator with a setting that resembles the collider environment at the High Luminosity LHC. The generated muons' interactions with the material in the endcap muon chambers are simulated including multiple scattering effects that resemble the CMS detector. The signal sample is mixed with the background samples at per-event level (per point cloud), with the ground-truth labels preserved for the interpretation studies. The hits left by the \\\\( \\\\mu \\\\)'s from the \\\\( \\\\tau \\\\to \\\\mu\\\\mu\\\\mu \\\\) decay are labeled as ground-truth interpretation. We only use hits on the first layer of detectors to train models and make sure every sample in the dataset has at least three detector hits. Each point in the sample contains measurements of a local bending angle and a 2D coordinate in the pseudorapidity-azimuth (\\\\( \\\\eta - \\\\phi \\\\)) space. Because in the best case the model only needs to capture hits from each \\\\( \\\\mu \\\\), we report precision@3. And because 80% of positive samples have less than 7 hits labeled as ground-truth interpretation, we also report precision@7.\\n\\nFinally, we randomly split the dataset into training/validation/test sets with a ratio of 70 : 15 : 15.\\n\\n### SynMol\\n\\nWe utilize the molecules in ZINC (Irwin et al., 2012), and follow McCloskey et al. (2019) and Sanchez-Lengeling et al. (2020) to create synthetic properties based on the existence of certain functional groups. Specifically, if a molecule contains both the unbranched alkane and carbonyl, then we label it as a positive sample; otherwise it is labeled as a negative sample. So, the atoms in branched alkanes and carbonyl are labeled as ground-truth interpretation. Instead of 2D representations of molecules, we associate each atom a 3D coordinate by generating a conformer for each molecule. To do this, we first add hydrogens to the molecule and apply the ETKDG method (Riniker & Landrum, 2015). After that, the generated structures are cleaned up using the MMFF94 force field (Halgren, 1999) with a maximum iteration of 1000, and the added hydrogens are removed once it is finished. Both ETKDG and MMFF94 are implemented using RDKit. Besides a 3D coordinate, each point in a sample also has a categorical feature indicating the atom type. Even though the two functional groups may only have five atoms in total, some molecules may contain multiple such functional groups. So, we report both precision@5 and precision@8 (80% of positive samples have less than 8 atoms labeled as ground-truth interpretation). Finally, we split the dataset into training/validation/test sets in a way that the number of molecules with or without either of these functional groups is uniformly distributed following (McCloskey et al., 2019) so that the dataset bias is minimized.\\n\\n### PLBind\\n\\nWe utilize protein-ligand complexes from PDBBind (Liu et al., 2017), which annotates binding affinities for a subset of complexes in the Protein Data Bank (PDB) (Berman et al., 2000). In PDBBind, each protein-ligand pair is annotated with a dissociation constant \\\\( K_d \\\\), which measures the binding affinity between a pair of protein and ligand. We use a threshold of 10 nM on \\\\( K_d \\\\) to obtain a binary classification task, and the model interpretability is studied on the protein-ligand pairs with high affinities. To augment negative data, during training, there is a 10% change of switching the ligand of a complex to a random ligand, and the new protein-ligand pair will be labeled as a negative sample, i.e., low affinity. The ground-truth interpretation labels consist of two parts. First, as shown in previous studies (Liu et al., 2022), using the part of the protein that is within 15 \u00c5 of the ligand is enough to even learn to generate ligands that bind to a certain protein, so, we define the amino acids that are within 15 \u00c5 of the ligand to be the binding site and label them as ground-truth interpretation. Second, we retrieve all atomic contacts (hydrogen bond and hydrophobic contact) for every protein-ligand pair from PDBsum (Laskowski et al., 2018) and label the corresponding amino acids in the protein as the ground-truth interpretation. Each amino acid in a protein is associated with a 3D coordinate, the amono acid type, solvent accessible surface area (SASA), and the B-factor. Each atom in a ligand is associated with a 3D coordinate, the atom type, and Gasteiger charges. Finally, we split the dataset into training/validation/test sets according to the year the complexes are discovered following Stark et al. (2022).\\n\\n### Details on Hyperparameter Tuning\\n\\nAll hyperparameters are tuned based on validation classification AUC for a fair comparison. All settings are trained with 5 different seeds and the average performance on the 5 seeds are reported.\\n\\n**Basic Settings.** We use a batch size of 128 on all datasets, except on Tau3Mu we use a batch size of 256 due to its large dataset size. The Adam (Kingma & Ba, 2015) optimizer with a learning rate 20.\"}"}
{"id": "6u7mf9s2A9", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in its loss function, and their coefficients are both tuned from paper, i.e., $\\\\alpha$.\\n\\nPointMask\\n\\nThe Gumbel-softmax trick is set to training the classifier, the explainer module in BernMask-P will be trained with extra $\\\\beta_{LRI-Bernoulli}$. This approach generalizes PGExplainer (Luo et al., 2020) and learns a node mask BernMask-P.\\n\\nLRI-Gaussian\\n\\nThe best validation classification AUC during pretraining.\\n\\nBackbone Models\\n\\nLayers use a dropout (Srivastava et al., 2014) ratio of $0$ and ReLU activation (Nair & Hinton, 2010) are used. All MLP layers use a dropout ratio of $0$ and ReLU activation are used. All MLP layers use a dropout ratio of $0$ and ReLU activation are used.\\n\\nTable 6: Ablation studies on the effect of the differentiable graph reconstruction module in LRI-\\n\\n| Backbone Models | w/o Graph Recons. | w/ Graph Recons. |\\n|-----------------|-------------------|-----------------|\\n| SynMol          | 67 \u00b1 91          | 65 \u00b1 98         |\\n| DGCNN Point Transformer | 74 \u00b1 83     | 72 \u00b1 86         |\\n| ActsTrack       | 98 \u00b1 39         | 95 \u00b1 36         |\\n\\nPost-hoc methods, a classifier will be first pretrained with the\\n\\nw/o Graph Recons.\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\n ActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN Point Transformer EGNN\\n\\nActsTrack\\n\\nSynMol\\n\\nDGCNN"}
