{"id": "HcUf-QwZeFh", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "HcUf-QwZeFh", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"APPENDIX A DETAILS OF IMPLEMENTATION\\n\\nThe hyperparameters we used are listed in Table 3. We implement MLP and Transformer policy with Jax (Bradbury et al., 2018) and Flax (Heek et al., 2020). For the implementation of GNN policy, we use a graph neural network library, Jraph (Godwin et al., 2020).\\n\\n| Method            | Hyperparameter       | Value  |\\n|-------------------|----------------------|--------|\\n| Shared Learning rate  |                      | 3e-4   |\\n| Batch size       |                      | 256    |\\n| Gradient clipping |                      | 0.1    |\\n| Activation function |                    | ReLU   |\\n| Gradient steps   |                      | 100k   |\\n| MLP Hidden size  |                      | 1024   |\\n| # of layers      |                      | 2      |\\n| Transformer Embedding size |                | 256    |\\n| Attention hidden size |                | 512    |\\n| # of attention heads |                 | 2      |\\n| # of attention layers |               | 3      |\\n| GNN Hidden size  |                      | 256    |\\n| # of layers (per node) |                 | 3      |\\n\\nTable 3: Hyperparameters for each method.\\n\\nTransformer for Morphology-Task Graph\\n\\nTransformer first encodes morphology-task graph to the latent representation vector $z_0$ with shared single-layer MLP and learnable position embedding (PE) (in case of MTGv1, we omit $s_g$, but include it to corresponding node observation $s_i$):\\n\\n$$z_0 = [\\\\text{MLP}(s_1), \\\\ldots, \\\\text{MLP}(s_{|V_m|}), \\\\text{MLP}(s_g)] + \\\\text{PE},$$\\n\\nthen multi-head attention (MHA) (Vaswani et al., 2017) and layer normalization (LayerNorm) (Ba et al., 2016) are recursively applied to latent representation $z_l$ at $l$-th layer,\\n\\n$$z'_l = \\\\text{LayerNorm} (\\\\text{MHA}(z_{l-1}) + z_{l-1})$$\\n$$z_l = \\\\text{LayerNorm} (\\\\text{MLP}(z'_l) + z'_{l-1})$$\\n\\n($l = 1, \\\\ldots, L$).\\n\\nBefore decoding the action per module from the last-layer latent representation $z_L$, we employ the residual connection of the node features (Kurin et al., 2021),\\n\\n$$a_m = [\\\\text{MLP}([z_1^L, s_1]), \\\\ldots, \\\\text{MLP}([z_{|V_m|}^L, s_{|V_m|}], \\\\text{MLP}([z_{|V_m|}^L+1, s_g])],$$\\n\\nwhere shared MLP has a single layer and tanh activation to clip the output within the range of $[-1, 1]$.\\n\\nWe mask out the output from the goal modules or modules that have no actuators to ensure action size as $|E_m|$.\\n\\nDetails of Morphology-Task Graph v1 and v2\\n\\nOur proposed morphology-task graph expresses the agent\u2019s observations, actions, and goals or tasks in a unified graph representation, while preserving the geometric structure of the task. Morphology-task graph v1 accepts the morphological graph, encoded from the agent\u2019s geometric information, as an input-output interface, and merges positional goal information as a part of corresponding node features, and morphology-task graph v2 treats given goals as extra disjoint nodes of morphological graph. In practice, we pad the incompatible parts of goal information by 0. For both MTGv1 and MTGv2, we prepare the binary vectors that have indicators to the target (or goal) nodes. MTGv1 obtains the products of the goal information and binary vectors, then leverages them as extra node features. MTGv2 just takes such binary vectors as extra node features (and also includes goals as extra nodes).\"}"}
{"id": "HcUf-QwZeFh", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We prepare 4 base blueprints; ant, centipede, claw, and worm, for procedural scene generation (Figure 6). Base ant has 1 torso and 2 legs with 2 joints per limb, and we could procedurally generate the agents with different number of limbs. Base centipede has 2 bodies and 4 legs with 2 joints per limb, and we could procedurally generate the agents with different number of bodies. Base claw has 1 torso and 2 legs with 4 joints per limb (each leg consists of 3 modules), and we could procedurally generate the agents with different number of limbs. Base worm has 2 bodies and no legs, and we could procedurally generate the agents with different number of bodies.\\n\\nFurthermore, we develop the functionality for morphology diversification, with missing, mass, and size parameters (Figure 7). Missing randomization lacks one module at one leg. This might be an equivalent situation that one leg is broken. Mass randomization changes the default mass of each module with specified scales. The appearance does not change, but certainly, the dynamics would differ. Size randomization changes the default length and radius of each module with specified scales.\\n\\nWe prepare 4 base tasks with parameterized goal distributions; reach, touch, twisters, and push, for procedural task generation (Figure 8). Reach task requires the agents to put their one leg to the given goal position (XY). The variant, reach_hard task, represents that the goal distribution is farther than reach task. Touch task requires the agents to contact their body or torso to the movable ball (i.e. movable ball is a goal). Twisters is a multi-goal problem; the agents should satisfy given goals at the same time. There are two basic constraints; reach and handsup. Handsup requires the agents to raise their one leg to the given goal Z height. Twisters has some combinations like reach_handsup, reach_hard_handsup, reach2_handsup, or reach_handsup2. For instance, in reach_handsup2, the agents should put their one leg to the given goal position, and raise their two legs to the given goal heights simultaneously. The XY position goals are sampled from uniform distribution with a predefined donut-shape range, and Z height goals are also from uniform distribution with predefined range. Push task requires the agents to move the box object to the given goal position (XY) sampled\"}"}
{"id": "HcUf-QwZeFh", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nFrom uniform distribution with predefined range. Since it has richer interaction with the object, push might be a more difficult task than those three. We use this task in Appendix L.\\n\\nFigure 8: Examples of pre-defined task in MxT-Bench. From left to right, each figure shows the example of reach, touch, twisters, and push task.\\n\\nB.3 Custom Morphology\\n\\nMxT-Bench also supports custom morphology import used in previous work. For instance, Gupta et al. (2022) propose unimal agents that are generated via evolutional strategy and designed for MuJoCo. Since they are not manually designed, their morphologies seem more diverse than our ant, centipede, claw, and worm. We inspect unimals whether they are suitable for goal-reaching, and include 72 morphologies from there (and use 60 morphologies for the experiments). Figure 9 shows some example of unimals.\\n\\nFigure 9: Examples of unimal agents, adapted from Gupta et al. (2022)\\n\\nB.4 Comparison to Existing Benchmark\\n\\nSince the current RL community has not paid much attention to embodied control so far, there are no suitable benchmarks to quantify the generalization over various tasks beyond single locomotion tasks and morphologies at the same time. In addition, the scalability to various morphologies or tasks seems to be required for the benchmark, because we should avoid \u201coverfitting\u201d to manually-designed tasks.\\n\\nAs summarized in Figure 4, MuJoCo (Todorov et al., 2012) or DM Control (Tassa et al., 2018), the most popular benchmarks in the continuous control domain, could not evaluate task or morphology generalization; they only have a single morphology for a single task as a pre-defined environment. Yu et al. (2019) propose a robot manipulation benchmark for meta RL, but it does not care about the morphology. Furthermore, while it has quite a diverse set of tasks, the scalability of environments seems to be limited. In contrast, previous morphology-agnostic RL works (Huang et al., 2020; Kurin et al., 2021; Hong et al., 2022; Trabucco et al., 2022) have a set of different morphologies adapted from MuJoCo agents. Gupta et al. (2022) also provide a much larger set of agents that are produced via joint-optimization of morphology and task rewards by the evolutionary strategy (Gupta et al., 2021), with kinematics and dynamics randomization. However, those works only aim to solve single locomotion tasks, i.e. running forward as fast as possible.\"}"}
{"id": "HcUf-QwZeFh", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use them as test environments, and for training datasets, we leverage In-Distribution division in\\nAfter convergence, we collect the proficient behaviors. Unless otherwise specified, we use 12k\\nTable 4. \\nTable 5 provides the combinations for the experiments of compositional generalization for task and\\nTable 6 also explains the combinations for the experiments of in-distribution generalization with\\nThroughout the experiments, we test a lot of morphology-task combinations to investigate the\\nIn addition, we extensively evaluate the compositional generalization for task and out-of-distribution\\nunimals (Gupta et al., 2022) in Table 1 and Table 12.\\nFor compositional morphology evaluation, we use Morph-Train division as training dataset and\\nof nodes in Morph-Train division for convenience. We have checked our trained model could be\\n\\n\\\\[d_{\\\\psi}(s, g) \\\\leq d_{\\\\psi}(m, \\\\psi) \\\\leq d_{\\\\psi}(s, g) + \\\\epsilon_{\\\\text{trans}} \\\\]\\n\\n\\\\[d_{\\\\psi}(s, g) \\\\leq d_{\\\\psi}(m, \\\\psi) \\\\leq d_{\\\\psi}(s, g) + \\\\epsilon_{\\\\text{trans}} \\\\]\\n\\n\\\\[d_{\\\\psi}(s, g) \\\\leq d_{\\\\psi}(m, \\\\psi) \\\\leq d_{\\\\psi}(s, g) + \\\\epsilon_{\\\\text{trans}} \\\\]\\n\\n\\\\[d_{\\\\psi}(s, g) \\\\leq d_{\\\\psi}(m, \\\\psi) \\\\leq d_{\\\\psi}(s, g) + \\\\epsilon_{\\\\text{trans}} \\\\]\\n\\n\\\\[d_{\\\\psi}(s, g) \\\\leq d_{\\\\psi}(m, \\\\psi) \\\\leq d_{\\\\psi}(s, g) + \\\\epsilon_{\\\\text{trans}} \\\\]\\n\\n\\\\[d_{\\\\psi}(s, g) \\\\leq d_{\\\\psi}(m, \\\\psi) \\\\leq d_{\\\\psi}(s, g) + \\\\epsilon_{\\\\text{trans}} \\\\]\\n\\n\\\\[d_{\\\\psi}(s, g) \\\\leq d_{\\\\psi}(m, \\\\psi) \\\\leq d_{\\\\psi}(s, g) + \\\\epsilon_{\\\\text{trans}} \\\\]\\n\\n\\\\[d_{\\\\psi}(s, g) \\\\leq d_{\\\\psi}(m, \\\\psi) \\\\leq d_{\\\\psi}(s, g) + \\\\epsilon_{\\\\text{trans}} \\\\]\\n\\n\\\\[d_{\\\\psi}(s, g) \\\\leq d_{\\\\psi}(m, \\\\psi) \\\\leq d_{\\\\psi}(s, g) + \\\\epsilon_{\\\\text{trans}} \\\\]\\n\\n\\\\[d_{\\\\psi}(s, g) \\\\leq d_{\\\\psi}(m, \\\\psi) \\\\leq d_{\\\\psi}(s, g) + \\\\epsilon_{\\\\text{trans}} \\\\]\\n\\n\\\\[d_{\\\\psi}(s, g) \\\\leq d_{\\\\psi}(m, \\\\psi) \\\\leq d_{\\\\psi}(s, g) + \\\\epsilon_{\\\\text{trans}} \\\\]\"}"}
{"id": "HcUf-QwZeFh", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4: The combinations of environments used in the experiments of in-distribution generalization, and compositional generalization for morphology.\"}"}
{"id": "HcUf-QwZeFh", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Sub-domain Environment       | Task-Test | OOD-Test |\\n|------------------------------|-----------|----------|\\n| ant_reach_hard               |           |          |\\n| ant_reach_hard_3             | 0.1       | 11.0     |\\n| ant_reach_hard_4             | 0.1       | 11.0     |\\n| ant_reach_hard_5             | 0.1       | 11.0     |\\n| ant_reach_hard_6             | 0.1       | 11.0     |\\n| centipede_reach_hard         |           |          |\\n| centipede_reach_hard_3       | 0.1       | 5.5      |\\n| centipede_reach_hard_4       | 0.1       | 5.5      |\\n| centipede_reach_hard_5       | 0.1       | 5.5      |\\n| centipede_reach_hard_6       | 0.1       | 5.5      |\\n| centipede_reach_hard_7       | 0.1       | 5.5      |\\n| ant_reach_hard_diverse       |           |          |\\n| ant_reach_hard_4_b           | 0.1       | 11.0     |\\n| ant_reach_hard_5_b           | 0.1       | 11.0     |\\n| ant_reach_hard_4_mass        | 0.1       | 11.0     |\\n| ant_reach_hard_4_mass_0.5    | 0.1       | 11.0     |\\n| ant_reach_hard_4_mass_1.0    | 0.1       | 11.0     |\\n| ant_reach_hard_4_mass_3.0    | 0.1       | 11.0     |\\n| ant_reach_hard_4_mass_0.9    | 0.1       | 11.0     |\\n| ant_reach_hard_5_mass_0.5    | 0.1       | 11.0     |\\n| ant_reach_hard_5_mass_1.0    | 0.1       | 11.0     |\\n| ant_reach_hard_5_mass_3.0    | 0.1       | 11.0     |\\n| centipede_reach_hard_diverse |           |          |\\n| centipede_reach_hard_3_b_r   | 0.1       | 5.5      |\\n| centipede_reach_hard_3_b_l   | 0.1       | 5.5      |\\n| centipede_reach_hard_3_b_r_1 | 0.1       | 5.5      |\\n| centipede_reach_hard_3_b_l_1 | 0.1       | 5.5      |\\n| centipede_reach_hard_4_b_r   | 0.1       | 5.5      |\\n| centipede_reach_hard_4_b_l_2 | 0.1       | 5.5      |\\n| centipede_reach_hard_3_size  | 0.1       | 5.5      |\\n| centipede_reach_hard_3_mass  | 0.1       | 5.5      |\\n| centipede_reach_hard_4_size  | 0.1       | 5.5      |\\n| centipede_reach_hard_4_mass  | 0.1       | 5.5      |\\n\\nTable 5: The combinations of environments used in the experiments of compositional generalization for task and out-of-distribution generalization.\"}"}
{"id": "HcUf-QwZeFh", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: Unimal IDs we adapted from Gupta et al. (2022). We inspect 100 morphologies and select 72 morphologies that work healthily. In the experiment of Table 1 and Table 12, we select 20 morphologies each for 3 tasks (reach, touch, twisters) as listed above.\"}"}
{"id": "HcUf-QwZeFh", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A SYSTEM FOR MORPHOLOGY-TASK GENERALIZATION VIA UNIFIED REPRESENTATION AND BEHAVIOR DILUTION\\n\\nHiroki Furuta, Yusuke Iwasawa, Yutaka Matsuo, Shixiang Shane Gu\\n\\nABSTRACT\\n\\nThe rise of generalist large-scale models in natural language and vision has made us expect that a massive data-driven approach could achieve broader generalization in other domains such as continuous control. In this work, we explore a method for learning a single policy that manipulates various forms of agents to solve various tasks by distilling a large amount of proficient behavioral data. In order to align input-output (IO) interface among multiple tasks and diverse agent morphologies while preserving essential 3D geometric relations, we introduce morphology-task graph, which treats observations, actions and goals/task in a unified graph representation. We also develop MxT-Bench for fast large-scale behavior generation, which supports procedural generation of diverse morphology-task combinations with a minimal blueprint and hardware-accelerated simulator. Through efficient representation and architecture selection on MxT-Bench, we find out that a morphology-task graph representation coupled with Transformer architecture improves the multi-task performances compared to other baselines including recent discrete tokenization, and provides better prior knowledge for zero-shot transfer or sample efficiency in downstream multi-task imitation learning. Our work suggests large diverse offline datasets, unified IO representation, and policy representation and architecture selection through supervised learning form a promising approach for studying and advancing morphology-task generalization.\\n\\nINTRODUCTION\\n\\nThe impressive success of large language models (Devlin et al., 2019; Radford et al., 2019; Bommasani et al., 2021; Brown et al., 2020; Chowdhery et al., 2022) has encouraged the other domains, such as computer vision (Radford et al., 2021; Gu et al., 2021b; Alayrac et al., 2022; Jaegle et al., 2021) or robotics (Ahn et al., 2022; Huang et al., 2022b), to leverage the large-scale pre-trained model trained with massive data with unified input-output interface. These large-scale pre-trained models are innately multi-task learners: they surprisingly work well not only in the fine-tuning or few-shot transfer but also in the zero-shot transfer settings (Raffel et al., 2020; Chen et al., 2022a). Learning a \u201cgeneralist\u201d model seems to be an essential goal in the recent machine learning paradigm with the same key ingredients: curate massive diverse dataset, define unified IO representation, and perform efficient representation and architecture selection, altogether for best generalization.\\n\\nIn reinforcement learning (RL) for continuous control, various aspects are important for generalization. First, we care about \u201ctask\u201d generalization. For instance, in robotic manipulation, we care the policy to generalize for different objects and target goal positions (Kalashnikov et al., 2018; Andrychowicz et al., 2017; Yu et al., 2019; Lynch et al., 2019). Recent advances in vision and language models also enable task generalization through compositional natural language instructions (Jiang et al., 2019; Shridhar et al., 2022a; Ahn et al., 2022; Cui et al., 2022). However, to scale the data, equally important is \u201cmorphology\u201d generalization, where a single policy can control agents of different embodiment (Wang et al., 2018; Noguchi et al., 2021) and can thereby ingest experiences from as...\"}"}
{"id": "HcUf-QwZeFh", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Behavior distillation pipeline. We first train a single-task policy for each environment on MxT-Bench, and then collect proficient morphology-task behavior dataset (Section 4.1). To enable a single policy to learn multiple tasks and morphologies simultaneously, we convert stored transitions to the morphology-task graph representation to align with unified IO interface (Section 4.3) for multi-task distillation (Section 4.2). After behavior distillation, the learned policy can be utilized for in-distribution or zero-shot generalization (Section 5.1), downstream fine-tuning (Section 5.2), and representation and architecture selection (Section 5.3).\\n\\nMany robots in different simulators (Freeman et al., 2021; Todorov et al., 2012; Coumans & Bai, 2016) as possible. Most prior works (Mendonca et al., 2021; Gupta et al., 2022) only address either the task or morphology axis separately, and achieving broad generalization over task and morphology jointly remains a long-standing problem.\\n\\nThis paper first proposes MxT-Bench, the first multi-morphology and multi-task benchmarking environments, as a step toward building the massive diverse dataset for continuous control. MxT-Bench provides various combinations of different morphologies (ant, centipede, claw, worm, and unimal (Gupta et al., 2022)) and different tasks (reach, touch, and twisters). MxT-Bench is easily scalable to additional morphologies and tasks, and is built on top of Brax (Freeman et al., 2021) for fast behavior generation.\\n\\nNext, we define unified IO representation for an architecture to ingest all the multi-morphology multi-task data. Inspired by scene graph (Johnson et al., 2015) in computer vision that represents the 3D relational information of a scene, and by morphology graph (Wang et al., 2018; Chen et al., 2018; Huang et al., 2020; Gupta et al., 2022) that expresses an agent's geometry and actions, we introduce the notion of morphology-task graph (MTG) as a unified interface to encode observations, actions, and goals (i.e. tasks) as nodes in the shared graph representation. Goals are represented as sub-nodes, and different tasks correspond to different choices: touching is controlling a torso node, while reaching is controlling an end-effector node (Figure 3). In contrast to discretizing and tokenizing every dimension as proposed in recent work (Janner et al., 2021; Reed et al., 2022), this unified IO limits data representation it can ingest, but strongly preserves 3D geometric relationships that are crucial for any physics control problem (Wang et al., 2018; Ghasemipour et al., 2022), and we empirically show it outperforms naive tokenization in our control-focused dataset.\\n\\nLastly, while conventional multi-task or meta RL studies generalization through on-policy joint training (Yu et al., 2019; Cobbe et al., 2020), we perform efficient representation and architecture selection, over 11 combinations of unified IO representation and network architectures, and 8 local node observations, for optimal generalization through behavior distillation (Figure 1), where RL is essentially treated as a (single-task, low-dimensional) behavior generator (Gu et al., 2021a) and multi-task supervised learning (or offline RL (Fujimoto et al., 2019)) is used for imitating all the behaviors (Singh et al., 2021; Chen et al., 2021b; Reed et al., 2022). Through offline distillation, we controllably and tractably evaluate two variants of MTG representation, along with multiple network architectures (MLP, GNN (Kipf & Welling, 2017), Transformers (Vaswani et al., 2017)), and show that MTGv2 variant with Transformer improves the multi-task goal-reaching performances compared to other possible choices by 23% and provides better prior knowledge for zero-shot generalization (by 14\u201318%) and fine-tuning for downstream multi-task imitation learning (by 50\u201355%).\\n\\nAs the fields of vision and language move toward broad generalization (Chollet, 2019; Bommasani et al., 2021), we hope our work could encourage RL and continuous control communities to continue considering the models, as slightly overloaded, it may imply morphological diversity as well.\"}"}
{"id": "HcUf-QwZeFh", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"growing diverse behavior datasets, designing different IO representations, and iterating more representation and architecture selection, and eventually optimize a single policy that can be deployed on any morphology for any task. In summary, our key contributions are:\\n\\n\u2022 We develop MxT-Bench as a test bed for morphology-task generalization with fast expert behavior generator. MxT-Bench supports the scalable procedural generation of both agents and tasks with minimal blueprints.\\n\\n\u2022 We introduce morphology-task graph, a universal IO for control which treats the agent's observations, actions and goals/tasks in a unified graph representation, while preserving the task structure.\\n\\n\u2022 We study generalization through offline supervised behavior distillation, where we can efficiently try out various design choices; over 11 combinations of unified IO representation and network architectures, and 8 local node observations. As a result, we find that Transformer with MTGv2 achieves the best multi-task performances among other possible designs (MLP, GNN and Transformer with MTGv1, Tokenized-MTGv2, etc.) in both in-distribution and downstream tasks, such as zero-shot transfer and fine-tuning for multi-task imitation learning.\\n\\nMorphology Generalization\\n\\nWhile, in RL for continuous control, the policy typically learns to control only a single morphology (Tassa et al., 2018; Todorov et al., 2012), several works succeed in generalizing the control problem for morphologically different agents to solve a locomotion task by using morphology-aware Graph Neural Network (GNN) policies (Wang et al., 2018; Huang et al., 2020; Blake et al., 2021). In addition, several work (Kurin et al., 2021; Gupta et al., 2022; Hong et al., 2022; Trabucco et al., 2022) have investigated the use of Transformer (Vaswani et al., 2017). Other work jointly optimize the morphology-agnostic policy and morphology itself (Pathak et al., 2019; Gupta et al., 2021; Yuan et al., 2022; Hejna et al., 2021), or transfer a controller over different morphologies (Devin et al., 2017; Chen et al., 2018; Hejna et al., 2020; Liu et al., 2022).\\n\\nWhile substantial efforts have been investigated to realize morphology generalization, those works mainly focus on only a single task (e.g. running), and less attention is paid to multi-task settings, where the agents attempt to control different states of their bodies to desired goals. We believe that goal-directed control is a key problem for an embodied single controller. Concurrently, Feng et al. (2022) propose an RL-based single controller that is applied to different quadruped robots and target poses in the sim-to-real setting. In contrast, our work introduces the notion of morphology-task graph as a unified IO that represents observations, actions, and goals in a shared graph, and can handle more diverse morphologies to solve multiple tasks.\\n\\nTask Generalization\\n\\nIn the previous research, task generalization has been explored in multi-task or meta RL literature (Wang et al., 2016; Duan et al., 2017; Cabi et al., 2017; Teh et al., 2017; Colas et al., 2019; Li et al., 2020; Yang et al., 2020; Kurin et al., 2022). Each task might be defined by the difference in goals, reward functions, and dynamics (Ghasemipour et al., 2019; Kalashnikov et al., 2021; Eysenbach et al., 2020), under shared state and action spaces. Some works leverage graph representation to embed the compositionality of manipulation tasks (Li et al., 2019; Zhou et al., 2022; Li et al., 2021; Kumar et al., 2022; Ghasemipour et al., 2022), while others use natural language representation to specify diverse tasks (Jiang et al., 2019; Shridhar et al., 2022a; Ahn et al., 2022; Huang et al., 2022a; Cui et al., 2022). Despite the notable success of acquiring various task generalization, multi-task RL often deals with only a single morphology. We aim to extend the general behavior policy into the \u201ccartesian product\u201d of tasks and morphologies (as shown in Figure 2) to realize a more scalable and capable controller.\"}"}
{"id": "HcUf-QwZeFh", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: We propose the notion of morphology-task graph, which expresses the agent's observations, actions, and goals/tasks in a unified graph representation, while preserving the geometric structure of the task. We develop two practical implementations; morphology-task graph v1 (left) accepts the morphological graph, encoded from the agent's geometric information, as an input-output interface, and merges positional goal information as a part of corresponding node features. morphology-task graph v2 (right) treats given goals as extra disjoint nodes of the morphological graph. While most prior morphology-agnostic RL have focused on locomotion (run task), i.e. a single static goal node controlling (maximizing) the velocity of center of mass, morphology-task graph could naturally extend morphology-agnostic control to other goal-oriented tasks: single static goal node for reach, multiple static goal nodes for twisters, and single dynamic goal node tracking a movable ball for an object interaction task; touch.\\n\\nTransformer for RL\\nRecently, Chen et al. (2021a) and Janner et al. (2021) consider offline RL as supervised sequential modeling problem and following works achieve impressive success (Reed et al., 2022; Lee et al., 2022; Furuta et al., 2022; Xu et al., 2022; Shafiullah et al., 2022; Zheng et al., 2022; Paster et al., 2022). In contrast, our work leverages Transformer to handle topological and geometric information of the scene, rather than a sequential nature of the agent trajectory.\\n\\nBehavior Distillation\\nDue to the massive try-and-error and large variance, training a policy from scratch in online RL is an inefficient process, especially in multi-task setting. It is more efficient to use RL for generating single-task behaviors (often from low dimensions) (Gu et al., 2021a) and then use supervised learning to imitate all behaviors with a large single policy (Levine et al., 2016; Rusu et al., 2016; Parisotto et al., 2016; Singh et al., 2021; Ajay et al., 2021; Chen et al., 2021b). Several works tackle the large-scale behavior distillation with Transformer (Reed et al., 2022; Lee et al., 2022), or with representation that treats observations and actions in the same vision-language space (Zeng et al., 2020; Shridhar et al., 2022b). Our work utilizes similar pipeline, but focuses on finding the good representation and architecture to generalize across morphology and tasks simultaneously with proposed morphology-task graph. See Appendix M for the connection to policy distillation.\\n\\n3 Preliminaries\\nIn RL, consider a Markov Decision Process with following tuple \\\\((S, A, p, p_1, r, \\\\gamma)\\\\), which consists of state space \\\\(S\\\\), action space \\\\(A\\\\), state transition probability function \\\\(p: S \\\\times A \\\\times S \\\\rightarrow [0, \\\\infty)\\\\), initial state distribution \\\\(p_1: S \\\\rightarrow [0, \\\\infty)\\\\), reward function \\\\(r: S \\\\times A \\\\rightarrow \\\\mathbb{R}\\\\), and discount factor \\\\(\\\\gamma \\\\in [0, 1)\\\\).\\n\\nThe agent follows a Markovian policy \\\\(\\\\pi: S \\\\times A \\\\rightarrow [0, \\\\infty)\\\\), which is often parameterized in Deep RL, and seeks optimal policy \\\\(\\\\pi^*\\\\) that maximizes the discounted cumulative rewards:\\n\\n\\\\[\\n\\\\pi^* = \\\\arg \\\\max_{\\\\pi} \\\\frac{1}{1 - \\\\gamma} \\\\sum_{t=0}^{\\\\infty} \\\\gamma^t r(s_t, a_t),\\n\\\\]\\n\\nwhere \\\\(p_{\\\\pi}(s_t) = \\\\sum_{t=0}^{\\\\infty} \\\\gamma^t p(s_t | s_{t-1}, a_{t-1})\\\\pi(a_t | s_t)\\\\) and \\\\(\\\\rho_{\\\\pi}(s) = (1 - \\\\gamma) \\\\sum_{t=0}^{\\\\infty} \\\\gamma^t p_{\\\\pi}(s_t = s)\\\\) represent time-aligned and time-aggregated state marginal distributions following policy \\\\(\\\\pi\\\\).\\n\\nGraph Representation for Morphology-Agnostic Control\\nFollowing prior continuous control literature (Todorov et al., 2012), we assume the agents have bodies modeled as simplified skeletons of animals. An agent's morphology is characterized by the parameter for rigid body module (torso, limbs), such as radius, length, mass, and inertia, and by the connection among those modules (joints). In order to handle such geometric and topological information, an agent's morphology can be expressed as an acyclic tree graph representation \\\\(G = (V, E)\\\\), where \\\\(V\\\\) is a set of nodes \\\\(v_i \\\\in V\\\\) and \\\\(E\\\\) is a set of edges \\\\(e_{ij} \\\\in E\\\\) between \\\\(v_i\\\\) and \\\\(v_j\\\\). The node \\\\(v_i\\\\) corresponds to the \\\\(i\\\\)-th module of the agent, and the edge \\\\(e_{ij}\\\\) corresponds to the hinge joint between the nodes \\\\(v_i\\\\) and \\\\(v_j\\\\). Each joint may\"}"}
{"id": "HcUf-QwZeFh", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "HcUf-QwZeFh", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "HcUf-QwZeFh", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alexander C. Li, Lerrel Pinto, and Pieter Abbeel. Generalized hindsight for reinforcement learning. In *Advances in Neural Information Processing Systems*, 2020.\\n\\nRichard Li, Allan Jabri, Trevor Darrell, and Pulkit Agrawal. Towards practical multi-object manipulation using relational reinforcement learning. In *International Conference on Robotics and Automation*, 2019.\\n\\nYunfei Li, Yilin Wu, Huazhe Xu, Xiaolong Wang, and Yi Wu. Solving compositional reinforcement learning problems via task reduction. In *International Conference on Learning Representations*, 2021.\\n\\nYunfei Li, Tian Gao, Jiaqi Yang, Huazhe Xu, and Yi Wu. Phasic self-imitative reduction for sparse-reward goal-conditioned reinforcement learning. In *International Conference on Machine Learning*, 2022.\\n\\nXingyu Liu, Deepak Pathak, and Kris M. Kitani. Revolver: Continuous evolutionary models for robot-to-robot policy transfer. *arXiv preprint arXiv:2202.05244*, 2022.\\n\\nRicky Loynd, Roland Fernandez, Asli Celikyilmaz, Adith Swaminathan, and Matthew Hausknecht. Working memory graphs. In *International Conference on Machine Learning*, 2020.\\n\\nCorey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet. Learning latent plans from play. In *Conference on Robot Learning*, 2019.\\n\\nZhao Mandi, Pieter Abbeel, and Stephen James. On the effectiveness of fine-tuning versus meta-reinforcement learning. *arXiv preprint arXiv:2206.03271*, 2022.\\n\\nTatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-efficient reinforcement learning via model-based offline optimization. In *International Conference on Learning Representations*, 2021.\\n\\nRussell Mendonca, Oleh Rybkin, Kostas Daniilidis, Danijar Hafner, and Deepak Pathak. Discovering and achieving goals via world models. In *Advances in Neural Information Processing Systems*, 2021.\\n\\nYuki Noguchi, Tatsuya Matsushima, Yutaka Matsuo, and Shixiang Shane Gu. Tool as embodiment for recursive manipulation. *arXiv preprint arXiv:2112.00359*, 2021.\\n\\nAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. *arXiv preprint arXiv:1609.03499*, 2016.\\n\\nEmilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer reinforcement learning. In *International Conference on Learning Representations*, 2016.\\n\\nEmilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant M. Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia Hadsell. Stabilizing transformers for reinforcement learning. *arXiv preprint arXiv:1910.06764*, 2019.\\n\\nKeiran Paster, Sheila McIlraith, and Jimmy Ba. You can't count on luck: Why decision transformers fail in stochastic environments. *arXiv preprint arxiv.2205.15967*, 2022.\\n\\nDeepak Pathak, Chris Lu, Trevor Darrell, Phillip Isola, and Alexei A. Efros. Learning to control self-assembling morphologies: A study of generalization via modularity. In *Advances in neural information processing systems*, 2019.\\n\\nXue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. AMP: Adversarial motion priors for stylized physics-based character control. *arXiv preprint arXiv:2104.02180*, 2021.\\n\\nVitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal difference models: Model-free deep rl for model-based control. In *International Conference on Learning Representations*, 2018.\"}"}
{"id": "HcUf-QwZeFh", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "HcUf-QwZeFh", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the experiments, we evaluate in-distribution generalization, as done in Section 5.1, with MTGv2 representation. Table 2 shows that, while some additional features (id, rp, m) contribute to improving the performance, the most effective feature can be morphological information (m), which suggests base_set contains sufficient features for control, and raw morphological properties (m) serve better task specification than manually-encoded information such as limb id (id). The key observations are; (1) morphological information is critical for morphology-task generalization, and (2) extra observation might disrupt the performance, such as relative rotation between parent and child node (rr). Throughout the paper, we treat base_set-m as node features for morphology-task graph.\\n\\nNode features:\\n\\n| Feature | Average Dist. |\\n|---------|--------------|\\n| base_set | 0.4330 \u00b1 0.02 |\\n| base_set-id | 0.4090 \u00b1 0.02 |\\n| base_set-rp | 0.3820 \u00b1 0.01 |\\n| base_set-rr | 0.4543 \u00b1 0.01 |\\n| base_set-m | 0.3128 \u00b1 0.02 |\\n| base_set-rp-rr | 0.3869 \u00b1 0.01 |\\n| base_set-jv-rp-rr | 0.4000 \u00b1 0.01 |\\n| base_set-jv-rp-rr-m | 0.3323 \u00b1 0.01 |\\n\\nTable 2: Offline node feature selection. We compare the combination of Cartesian position (p), Cartesian velocity (v), quaternion (q), angular velocity (a), joint angle (ja), joint range (jr), limb id (id), joint velocity (jv), relative position (rp), relative rotation (rr), and morphological information (m). base_set is composed of {p, v, q, a, ja, jr}. These results suggest base_set contains sufficient features for control, and the most effective feature seems morphological information (m) for better task specification.\\n\\n6 DISCUSSION AND LIMITATION\\n\\nWhile the experimental evaluation on MxT-Bench implies that morphology-task graph is a simple and effective method to distill the diverse proficient behavioral data into a generalizable single policy, there are some limitations. For instance, we focus on distillation from expert policies only, but it is still unclear whether morphology-task graph works with moderate or random quality behaviors in offline RL (Fujimoto et al., 2019; Levine et al., 2020; Fujimoto & Gu, 2021). Combining distillation with iterative data collection (Ghosh et al., 2021; Matsushima et al., 2021) or online fine-tuning (Li et al., 2022) would be a promising future work. In addition, we avoided tasks where expert behaviors cannot be generated easily by single-task RL without fine-scale reward engineering or human demonstrations; incorporating such datasets or bootstrapping single-task RL from the distilled policy could be critical for scaling the pipeline to more complex tasks such as open-ended and dexterous manipulation (Lynch et al., 2019; Ghasemipour et al., 2022; Chen et al., 2022b). Since morphology-task graph only uses readily accessible features in any simulator and could be automatically defined through URDFs or MuJoCo XMLs, in future work we aim to keep training our best morphology-task graph architecture policy on additional data from more functional and realistic control behaviors from other simulators like MuJoCo (Yu et al., 2019; Tassa et al., 2018), PyBullet (Shridhar et al., 2022a), IsaacGym (Chen et al., 2022b; Peng et al., 2021), and Unity (Juliani et al., 2018), and show it achieves better scaling laws than other representations (Reed et al., 2022) on broader morphology-task families.\\n\\n7 CONCLUSION\\n\\nThe broader range of behavior generalization is a promising paradigm for RL. To achieve morphology-task generalization, we propose morphology-task graph, which expresses the agent's modular observations, actions, and goals as a unified graph representation while preserving the geometric task structure. As a test bed for morphology-task generalization, we also develop MxT-Bench, which enables the scalable procedural generation of agents and tasks with minimal blueprints. Fast-generated behavior datasets of MxT-Bench with RL allow efficient representation and architecture selection through supervised learning, and MTGv2, variant of morphology-task graph, achieves the best multi-task performances among other possible designs (MLP, GNN and Transformer with MTGv1, and tokenized-MTGv2, etc), outperforming them in in-distribution evaluation (by 23 %), zero-shot transfer among compositional or out-of-distribution evaluation (by 14 \u223c 18 %) and fine-tuning for downstream multi-task imitation (by 50 \u223c 55 %). We hope our work will encourage the community to explore scalable yet incremental approaches for building a universal controller.\"}"}
{"id": "HcUf-QwZeFh", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGMENTS\\n\\nThis work was supported by JSPS KAKENHI Grant Number JP22J21582. We thank Mitsuhiko Nakamoto and Daniel C. Freeman for converting several MuJoCo agents to Brax, So Kuroki for the support on implementations, Yujin Tang, Kamyar Ghasemipour, Yingtao Tian, and Bert Chan for helpful feedback on this work.\\n\\nREFERENCES\\n\\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arxiv:2204.01691, 2022.\\n\\nAnurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. OPAL: Offline primitive discovery for accelerating offline reinforcement learning. In International Conference on Learning Representations, 2021.\\n\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. arXiv preprint arxiv:2204.14198, 2022.\\n\\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in neural information processing systems, 2017.\\n\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n\\nPeter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.\\n\\nAditya Bhatt, Max Argus, Artemij Amiranashvili, and Thomas Brox. Crossnorm: Normalization for off-policy td reinforcement learning. arXiv preprint arXiv:1902.05605, 2019.\\n\\nCharlie Blake, Vitaly Kurin, Maximilian Igl, and Shimon Whiteson. Snowflake: Scaling gnns to high-dimensional continuous control via parameter freezing. arXiv preprint arXiv:2103.01009, 2021.\\n\\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Kohd, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Li, et al. The future of research in artificial intelligence. arXiv preprint arXiv:2202.09082, 2022.\"}"}
{"id": "HcUf-QwZeFh", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "HcUf-QwZeFh", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "HcUf-QwZeFh", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Environment   | Task-Test | OOD-Test |\\n|---------------|-----------|----------|\\n| ant_push      | ant_push_3 1.0 4.0 |          |\\n|               | ant_push_4 1.0 4.0 |          |\\n|               | ant_push_5 1.0 4.0 |          |\\n|               | ant_push_6 1.0 4.0 |          |\\n| centipede_push| centipede_push_3 1.5 3.75 |          |\\n|               | centipede_push_4 1.5 3.75 |          |\\n|               | centipede_push_5 1.5 3.75 |          |\\n|               | centipede_push_6 1.5 3.75 |          |\\n|               | centipede_push_7 1.5 3.75 |          |\\n| worm_push     | ant_push_3 1.5 3.25 |          |\\n|               | worm_push_4 1.5 3.25 |          |\\n|               | worm_push_5 1.5 3.25 |          |\\n| ant_push_diverse| ant_push_4_b 1.0 4.0 |          |\\n|               | ant_push_5_b 1.0 4.0 |          |\\n|               | ant_push_4_mass_0.5_1.0_3.0 1.0 4.0 |          |\\n|               | ant_push_4_mass_0.5_1.0_1.0 1.0 4.0 |          |\\n|               | ant_push_4_mass_1.0_3.0_3.0 1.0 4.0 |          |\\n|               | ant_push_4_size_0.9_1.0_1.1 1.0 4.0 |          |\\n|               | ant_push_5_mass_0.5_1.0_3.0 1.0 4.0 |          |\\n|               | ant_push_5_mass_0.5_1.0_1.0 1.0 4.0 |          |\\n|               | ant_push_5_mass_1.0_3.0_3.0 1.0 4.0 |          |\\n|               | ant_push_5_size_0.9_1.0_1.1 1.0 4.0 |          |\\n| centipede_push_diverse | centipede_push_3_b_r_0 1.5 3.75 |          |\\n|               | centipede_push_3_b_l_0 1.5 3.75 |          |\\n|               | centipede_push_3_b_r_1 1.5 3.75 |          |\\n|               | centipede_push_3_b_l_1 1.5 3.75 |          |\\n|               | centipede_push_4_b_r_0 1.5 3.75 |          |\\n|               | centipede_push_4_b_r_1 1.5 3.75 |          |\\n|               | centipede_push_4_b_r_2 1.5 3.75 |          |\\n|               | centipede_push_4_b_l_1 1.5 3.75 |          |\\n|               | centipede_push_4_b_l_2 1.5 3.75 |          |\\n|               | centipede_push_3_size_0.9_1.0_1.1 1.5 3.75 |          |\\n|               | centipede_push_3_mass_0.5_1.0_3.0 1.5 3.75 |          |\\n|               | centipede_push_3_mass_0.5_1.0_1.0 1.5 3.75 |          |\\n|               | centipede_push_3_mass_1.0_3.0_3.0 1.5 3.75 |          |\\n|               | centipede_push_3_mass_0.5_1.0_3.0 1.5 3.75 |          |\\n|               | centipede_push_3_mass_0.5_1.0_1.0 1.5 3.75 |          |\\n|               | centipede_push_3_mass_1.0_3.0_3.0 1.5 3.75 |          |\\n|               | centipede_push_4_size_0.9_1.0_1.1 1.5 3.75 |          |\\n|               | centipede_push_4_mass_0.5_1.0_3.0 1.5 3.75 |          |\\n|               | centipede_push_4_mass_0.5_1.0_1.0 1.5 3.75 |          |\\n|               | centipede_push_4_mass_1.0_3.0_3.0 1.5 3.75 |          |\\n\\nTable 7: The extra combinations of environments used in the experiments of compositional generalization for task and out-of-distribution generalization with push task.\"}"}
{"id": "HcUf-QwZeFh", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"In the recent literature, offline RL is considered as supervised sequential modeling problem (Chen et al., 2021a), and some works (Janner et al., 2021; Reed et al., 2022) tokenize the continuous observations and actions, like an analogy of vision transformer (Dosovitskiy et al., 2020); treating input modality like a language.\\n\\nAs a part of offline architecture selection, we examine the effectiveness of tokenization. We mainly follow the protocol in Reed et al. (2022); we first apply mu-law encoding (Oord et al., 2016) to morphology-task graph representation:\\n\\n$$\\n\\\\text{mu}_\\\\text{law}(x) := \\\\frac{\\\\text{sgn}(x) \\\\log(|x| + \\\\mu + 1)}{\\\\log(M\\\\mu + 1)}\\n$$\\n\\nwith $\\\\mu = 100$ and $M = 256$. This pre-processing could normalize the input to the range of $[0, 1]$.\\n\\nThen, we discretize pre-processed observations and actions with 1024 bins. To examine the broader range of design choice, we prepare the following 6 variants of tokenized morphology-task graph:\\n- whether layer normalization is added after embedding function (LN) (Bhatt et al., 2019; Parisotto et al., 2019; Furuta et al., 2021a; Chen et al., 2021a),\\n- or outputting discretized action (D),\\n- smoothed action by taking the average of bins (DA),\\n- and continuous values directly (C).\\n\\nAs shown in Table 8, predicting continuous value reveals the better performance than discretized action maybe due to some approximation errors. However, the performance of Token-MTGv2 (C) or Token-MTGv2 (C, LN) is still lower than MTGv2 itself. This might be because tokenization loses some morphological invariance among nodes. In Table 1, we adopt Token-MTGv2 (C) for comparison.\\n\\n| Sub-domain        | Token-MTGv2 (D, LN) | Token-MTGv2 (DA, LN) | Token-MTGv2 (C, LN) | Token-MTGv2 (D) | Token-MTGv2 (DA) | Token-MTGv2 (C) |\\n|-------------------|---------------------|----------------------|---------------------|----------------|----------------|----------------|\\n| Transformer (MTGv2) |                     |                      |                     |                |                |                |\\n| ant_reach         | 0.9256 \u00b1 0.01       | 0.9252 \u00b1 0.01        | 0.4394 \u00b1 0.01       | 0.9210 \u00b1 0.02  | 0.9215 \u00b1 0.00  | 0.3846 \u00b1 0.03  |\\n| ant_touch         | 1.0373 \u00b1 0.01       | 1.0631 \u00b1 0.01        | 0.4465 \u00b1 0.01       | 1.0528 \u00b1 0.00  | 1.0630 \u00b1 0.01  | 0.3458 \u00b1 0.03  |\\n| ant_twisters      | 0.5581 \u00b1 0.00       | 0.5655 \u00b1 0.00        | 0.2090 \u00b1 0.00       | 0.5587 \u00b1 0.01  | 0.5655 \u00b1 0.00  | 0.2487 \u00b1 0.01  |\\n| claw_reach        | 0.9568 \u00b1 0.00       | 0.9584 \u00b1 0.01        | 0.3685 \u00b1 0.01       | 0.9674 \u00b1 0.00  | 0.9583 \u00b1 0.01  | 0.2862 \u00b1 0.07  |\\n| claw_touch        | 1.0300 \u00b1 0.01       | 1.0584 \u00b1 0.01        | 0.3238 \u00b1 0.07       | 1.0392 \u00b1 0.03  | 1.0585 \u00b1 0.01  | 0.3229 \u00b1 0.07  |\\n| claw_twisters     | 0.6228 \u00b1 0.01       | 0.6205 \u00b1 0.00        | 0.4035 \u00b1 0.02       | 0.6241 \u00b1 0.00  | 0.6202 \u00b1 0.00  | 0.3810 \u00b1 0.03  |\\n| centipede_reach   | 0.5692 \u00b1 0.12       | 0.5784 \u00b1 0.13        | 0.1166 \u00b1 0.00       | 0.6373 \u00b1 0.09  | 0.5780 \u00b1 0.13  | 0.1132 \u00b1 0.03  |\\n| centipede_touch   | 0.9818 \u00b1 0.01       | 1.0088 \u00b1 0.00        | 0.1696 \u00b1 0.00       | 0.9910 \u00b1 0.01  | 1.0087 \u00b1 0.00  | 0.1823 \u00b1 0.03  |\\n| centipede_twisters| 0.6228 \u00b1 0.01       | 0.6205 \u00b1 0.00        | 0.4035 \u00b1 0.02       | 0.6241 \u00b1 0.00  | 0.6202 \u00b1 0.00  | 0.3810 \u00b1 0.03  |\\n| Average Dist.     | 0.8124 \u00b1 0.01       | 0.8232 \u00b1 0.02        | 0.3572 \u00b1 0.02       | 0.8248 \u00b1 0.01  | 0.8225 \u00b1 0.02  | 0.3402 \u00b1 0.01  |\"}"}
{"id": "HcUf-QwZeFh", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As a part of architecture selection, we investigate whether position embedding (PE) contributes to the generalization. Our empirical results in Table 9 suggest that, multi-task goal reaching performance seems comparable between those. However, in more diverse morphology domains, PE plays an important role. Therefore, we include PE into a default design.\\n\\n| Sub-domain       | Transformer (MTGv2) (w/ PE) | Transformer (MTGv2) (w/o PE) |\\n|------------------|----------------------------|------------------------------|\\n| ant_reach        | 0.3206 \u00b1 0.06             | 0.3966 \u00b1 0.08               |\\n| ant_touch        | 0.2668 \u00b1 0.08             | 0.4573 \u00b1 0.14               |\\n| ant_twisters     | 0.1039 \u00b1 0.05             | 0.1569 \u00b1 0.02               |\\n| claw_reach       | 0.3581 \u00b1 0.04             | 0.3508 \u00b1 0.03               |\\n| claw_touch       | 0.2573 \u00b1 0.08             | 0.3278 \u00b1 0.07               |\\n| claw_twisters    | 0.3442 \u00b1 0.04             | 0.3071 \u00b1 0.04               |\\n| centipede_reach  | 0.1057 \u00b1 0.04             | 0.0610 \u00b1 0.02               |\\n| centipede_touch  | 0.3869 \u00b1 0.04             | 0.1687 \u00b1 0.03               |\\n| worm_touch       | 0.8952 \u00b1 0.05             | 0.8427 \u00b1 0.03               |\\n| Average Dist.    | 0.3128 \u00b1 0.02             | 0.3142 \u00b1 0.03               |\\n\\nTable 9: The average normalized final distance on in-distribution evaluation. We compare the effect of position embedding.\\n\\nSome meta RL or multi-task RL algorithms take historical observations as inputs, or leverage recurrent neural networks to encode the temporal information of the tasks (Wang et al., 2016; Teh et al., 2017; Rakelly et al., 2019). To examine whether morphology-task graph could be augmented with historical observations, we test the morphology-task graph with history, which concatenates recent morphology-task graph as inputs while predicting one-step actions, in various types of morphology-task generalization with ant agents. In all the setting, we choose ant_{reach, touch, twisters} (in Table 4) for training. In morphology generalization setting, we hold out ant_5 agent tasks for evaluation. In task generalization setting, we use ant_reach_hard (in Table 5) as a test set. In out-of-distribution setting, we use ant_reach_hard_diverse (in Table 5) for evaluation. We also set the history length as $H = 3$.\\n\\nTable 10 implies that when the policy faces unseen tasks (i.e. in Compositional (Task) or Out-of-Distribution settings), morphology-task graph with history may help to improve the performance, which seems promising results to extend our framework to more complex tasks.\\n\\n| Sub-domain       | MLP Transformer (MTGv1) | Transformer (MTGv2) | Transformer (MTGv2-history) |\\n|------------------|-------------------------|---------------------|-----------------------------|\\n| In-Distribution  | 0.3035 \u00b1 0.03           | 0.1364 \u00b1 0.04       | 0.0961 \u00b1 0.02               |\\n| Compositional (Morphology) | 0.7480 \u00b1 0.02 | 0.1477 \u00b1 0.04 | 0.1010 \u00b1 0.01 |\\n| Compositional (Task)      | 0.4468 \u00b1 0.11           | 0.2243 \u00b1 0.03       | 0.2201 \u00b1 0.04               |\\n| Out-of-Distribution     | 0.6788 \u00b1 0.01           | 0.3663 \u00b1 0.05       | 0.3370 \u00b1 0.03               |\\n\\nTable 10: The average normalized final distance in various types of morphology-task generalization on MxT-Bench (especially, ant). We concatenate recent three frames morphology-task graph as inputs to Transformer. The results show that when the policy faces unseen tasks, MTGv2-history may help to improve the performance.\"}"}
{"id": "HcUf-QwZeFh", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The important aspect of the success in large language models is the scalability to the size of training data, the number of tasks for joint-training, and the number of parameters (Raffel et al., 2020; Brown et al., 2020). One natural question is whether a similar trend holds even in RL.\\n\\nFigure 11 suggests that the performance can get better if we increase the number of datasets and the number of parameters. The performance of Transformer with 0.4M parameters is equivalent to that of MLP with 3.1M. In contrast, when we increase the number of environments, the performance degrades while Transformer with MTGv1 and MTGv2 surpasses the degree of degradation, which seems inevitable trends in multi-task RL (Yu et al., 2020; Kurin et al., 2022) and an important future direction towards generalist controllers.\\n\\nFor clarification, we compute the percentage of improvement between two average normalized final distances (defined in Equation 3) $\\\\bar{d}_1$ and $\\\\bar{d}_2$ as follows ($\\\\bar{d}_1 < \\\\bar{d}_2$):\\n\\n$$100 \\\\times \\\\frac{\\\\bar{d}_2 - \\\\bar{d}_1}{\\\\bar{d}_2}.$$\"}"}
{"id": "HcUf-QwZeFh", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"In this section, we provide the detailed performance of in-distribution generalization (Table 11 and Table 12), compositional morphology and task generalization (Table 13 and Table 14), and out-of-distribution generalization (Table 15). For fine-tuning experiments, we summarized the detailed scores of Figure 5 in Table 16.\\n\\n| Sub-domain       | Random MLP GNN (MTGv1) | Transformer (MTGv1) | Transformer (MTGv2) |\\n|------------------|------------------------|---------------------|---------------------|\\n| ant_reach        | 0.9637 \u00b1 0.02          | 0.6426 \u00b1 0.03       | 0.6240 \u00b1 0.03       |\\n| ant_touch        | 1.0817 \u00b1 0.02          | 0.3689 \u00b1 0.02       | 0.4434 \u00b1 0.03       |\\n| ant_twisters     | 0.9113 \u00b1 0.08          | 0.3708 \u00b1 0.01       | 0.2513 \u00b1 0.01       |\\n| claw_reach       | 1.0760 \u00b1 0.20          | 0.6617 \u00b1 0.00       | 0.6214 \u00b1 0.02       |\\n| claw_touch       | 1.5240 \u00b1 0.88          | 0.6824 \u00b1 0.06       | 0.3135 \u00b1 0.06       |\\n| claw_twisters    | 1.3786 \u00b1 1.42          | 0.4907 \u00b1 0.03       | 0.6063 \u00b1 0.07       |\\n| centipede_reach  | 0.5843 \u00b1 0.35          | 0.0803 \u00b1 0.02       | 0.1088 \u00b1 0.02       |\\n| centipede_touch  | 1.0077 \u00b1 0.01          | 0.4743 \u00b1 0.03       | 0.5089 \u00b1 0.01       |\\n| worm_touch       | 2.7087 \u00b1 1.73          | 1.1034 \u00b1 0.06       | 1.0559 \u00b1 0.04       |\\n| Average Dist.    | 1.2019 \u00b1 0.41          | 0.5150 \u00b1 0.01       | 0.4776 \u00b1 0.01       |\\n\\nTable 11: The average normalized final distance for in-distribution evaluation on MxT-Bench (as shown in Table 1).\\n\\n| Sub-domain       | Random MLP Transformer (MTGv1) | Transformer (MTGv1) | Transformer (MTGv2) |\\n|------------------|-------------------------------|---------------------|---------------------|\\n| unimal_reach     | 0.9662 \u00b1 0.01                | 0.7448 \u00b1 0.02       | 0.5692 \u00b1 0.01       |\\n| unimal_touch     | 1.1302 \u00b1 0.10                | 0.7634 \u00b1 0.03       | 0.5290 \u00b1 0.04       |\\n| unimal_twisters  | 0.6305 \u00b1 0.02                | 0.5027 \u00b1 0.02       | 0.3534 \u00b1 0.03       |\\n| Average Dist.    | 0.9090 \u00b1 0.03                | 0.6703 \u00b1 0.01       | 0.4839 \u00b1 0.02       |\\n\\nTable 12: The average normalized final distance for in-distribution evaluation on MxT-Bench with challenging morphologies from Gupta et al. (2022) (as shown in Table 1).\\n\\n| Sub-domain       | Random MLP Transformer (MTGv1) | Transformer (MTGv1) | Transformer (MTGv2) |\\n|------------------|-------------------------------|---------------------|---------------------|\\n| ant_reach        | 0.9697 \u00b1 0.03                | 0.8541 \u00b1 0.03       | 0.5511 \u00b1 0.10       |\\n| ant_touch        | 1.0821 \u00b1 0.00                | 0.7742 \u00b1 0.07       | 0.4464 \u00b1 0.07       |\\n| ant_twisters     | 0.9215 \u00b1 0.95                | 0.5356 \u00b1 0.02       | 0.2569 \u00b1 0.04       |\\n| claw_reach       | 0.9915 \u00b1 0.04                | 0.9370 \u00b1 0.01       | 0.7332 \u00b1 0.03       |\\n| claw_touch       | 1.0695 \u00b1 0.02                | 1.0283 \u00b1 0.02       | 0.7074 \u00b1 0.10       |\\n| claw_twisters    | 0.7001 \u00b1 0.17                | 0.5456 \u00b1 0.02       | 0.5486 \u00b1 0.02       |\\n| centipede_reach  | 0.7929 \u00b1 0.04                | 0.4626 \u00b1 0.15       | 0.2640 \u00b1 0.07       |\\n| centipede_touch  | 1.0121 \u00b1 0.01                | 0.7632 \u00b1 0.02       | 0.4971 \u00b1 0.06       |\\n| worm_touch       | 2.7376 \u00b1 2.23                | 1.1417 \u00b1 0.10       | 0.8608 \u00b1 0.07       |\\n| Average Dist.    | 1.1419 \u00b1 0.41                | 0.7216 \u00b1 0.01       | 0.4940 \u00b1 0.01       |\\n\\nTable 13: The average normalized final distance for compositional morphology evaluation on MxT-Bench (as shown in Table 1).\"}"}
{"id": "HcUf-QwZeFh", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Sub-domain          | Random MLP Transformer (MTGv1) | Transformer (MTGv2) |\\n|---------------------|--------------------------------|---------------------|\\n| ant_reach_hard      | 0.9542 \u00b1 0.01                 | 0.8299 \u00b1 0.03       |\\n|                     | 0.6176 \u00b1 0.07                 | 0.4522 \u00b1 0.06       |\\n| centipede_reach_hard| 0.8443 \u00b1 0.02                 | 0.5689 \u00b1 0.03       |\\n|                     | 0.4770 \u00b1 0.05                 | 0.4412 \u00b1 0.05       |\\n| Average Dist.       | 0.8932 \u00b1 0.01                 | 0.6849 \u00b1 0.01       |\\n|                     | 0.5395 \u00b1 0.04                 | 0.4461 \u00b1 0.05       |\\n\\nTable 14: The average normalized final distance for compositional task evaluation on MxT-Bench (as shown in Table 1).\\n\\n| Sub-domain          | Random MLP Transformer (MTGv1) | Transformer (MTGv2) |\\n|---------------------|--------------------------------|---------------------|\\n| ant_reach_hard_diverse | 0.9520 \u00b1 0.01             | 0.8288 \u00b1 0.02       |\\n|                     | 0.6798 \u00b1 0.04             | 0.5365 \u00b1 0.05       |\\n| centipede_reach_hard_diverse | 0.8660 \u00b1 0.02 | 0.7546 \u00b1 0.02       |\\n|                     | 0.6130 \u00b1 0.03             | 0.5208 \u00b1 0.04       |\\n| Average Dist.       | 0.8979 \u00b1 0.01             | 0.7821 \u00b1 0.02       |\\n|                     | 0.6144 \u00b1 0.04             | 0.5266 \u00b1 0.04       |\\n\\nTable 15: The average normalized final distance for out-of-distribution evaluation on MxT-Bench (as shown in Table 1). The agents are diversified with missing, mass, size randomization.\\n\\n| Method (data size) | Compositional (Morphology) | Compositional (Task) | Out-of-Distribution |\\n|-------------------|-----------------------------|----------------------|---------------------|\\n| MLP (4K, randinit) | 0.8162 \u00b1 0.02              | 0.7046 \u00b1 0.01        | 0.6674 \u00b1 0.03       |\\n| MLP (4K, fine-tuning) | 0.6715 \u00b1 0.02 | 0.5582 \u00b1 0.03        | 0.5694 \u00b1 0.04       |\\n| Transformer (MTGv1) (4K, randinit) | 0.6021 \u00b1 0.03 | 0.5017 \u00b1 0.04        | 0.5222 \u00b1 0.06       |\\n| Transformer (MTGv1) (4K, fine-tuning) | 0.3499 \u00b1 0.04 | 0.3378 \u00b1 0.08        | 0.3401 \u00b1 0.08       |\\n| Transformer (MTGv2) (4K, randinit) | 0.5504 \u00b1 0.03 | 0.4894 \u00b1 0.03        | 0.4980 \u00b1 0.05       |\\n| Transformer (MTGv2) (4K, fine-tuning) | 0.2863 \u00b1 0.02 | 0.3180 \u00b1 0.07        | 0.3115 \u00b1 0.07       |\\n| MLP (8K, randinit) | 0.7044 \u00b1 0.02              | 0.4818 \u00b1 0.02        | 0.5247 \u00b1 0.04       |\\n| MLP (8K, fine-tuning) | 0.5579 \u00b1 0.06 | 0.3709 \u00b1 0.01        | 0.4238 \u00b1 0.05       |\\n| Transformer (MTGv1) (8K, randinit) | 0.4001 \u00b1 0.04 | 0.2791 \u00b1 0.06        | 0.2762 \u00b1 0.04       |\\n| Transformer (MTGv1) (8K, fine-tuning) | 0.2600 \u00b1 0.04 | 0.1926 \u00b1 0.03        | 0.2089 \u00b1 0.03       |\\n| Transformer (MTGv2) (8K, randinit) | 0.3758 \u00b1 0.03 | 0.1868 \u00b1 0.05        | 0.1868 \u00b1 0.05       |\\n| Transformer (MTGv2) (8K, fine-tuning) | 0.1871 \u00b1 0.01 | 0.1356 \u00b1 0.03        | 0.1498 \u00b1 0.03       |\\n| MLP (12K, randinit) | 0.5530 \u00b1 0.01              | 0.4256 \u00b1 0.03        | 0.4399 \u00b1 0.02       |\\n| MLP (12K, fine-tuning) | 0.4312 \u00b1 0.03 | 0.2982 \u00b1 0.02        | 0.3358 \u00b1 0.02       |\\n| Transformer (MTGv1) (12K, randinit) | 0.2914 \u00b1 0.03 | 0.1555 \u00b1 0.04        | 0.1759 \u00b1 0.03       |\\n| Transformer (MTGv1) (12K, fine-tuning) | 0.2042 \u00b1 0.01 | 0.0888 \u00b1 0.01        | 0.0894 \u00b1 0.01       |\\n| Transformer (MTGv2) (12K, randinit) | 0.2655 \u00b1 0.03 | 0.1149 \u00b1 0.02        | 0.1149 \u00b1 0.02       |\\n| Transformer (MTGv2) (12K, fine-tuning) | 0.1301 \u00b1 0.02 | 0.0562 \u00b1 0.02        | 0.0513 \u00b1 0.01       |\\n\\nTable 16: The average normalized final distance among test environments in fine-tuning settings (compositional morphology/task or out-of-distribution evaluation).\"}"}
{"id": "HcUf-QwZeFh", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we provide the attention analysis of Transformer with MTGv1 (Figure 13) and MTGv2 (Figure 12). The experimental results reveal that despite slight differences, MTGv2 generalizes various morphologies and tasks better than MTGv1. To find out the difference between those, we qualitatively analyze the attention weights in Transformer. Figure 12 shows that MTGv2 consistently focuses on goal nodes over time, and activates important nodes to solve the task; for instance, in centipede_touch (top), MTGv2 pays attention to corresponding nodes (torso0 and goal0) at the beginning of the episode, and gradually sees other relevant nodes (torso1 and torso2) to hold the movable ball. Furthermore, in ant_twisters (bottom), MTGv2 firstly tries to raise the agent's legs to satisfy goal1 and goal2, and then focus on reaching a leg (goal0). Temporally-consistent attention to goal nodes and dynamics attention to relevant nodes can contribute to generalization over goal-directed tasks and morphologies.\\n\\nFigure 13 implies that MTGv1 does not show such consistent activation to the goal-conditioned node; for instance, in centipede_touch_3 (above), the goal information is treated as an extra node feature of torso0, but there are no nodes that consistently activated with torso0. Moreover, in ant_reach_handsup2_3 (bottom), MTGv1 does not keep focusing on the agent's limbs during the episode. Rather, MTGv1 tends to demonstrate some periodic patterns during the rollout as implied in prior works (Huang et al., 2020; Kurin et al., 2021).\"}"}
{"id": "HcUf-QwZeFh", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13: Attention analysis of MTGv1 in centipede_touch_3 and ant_reach_handsup2_5. This tends to demonstrate some periodic patterns during the rollout as implied in prior works (Huang et al., 2020; Kurin et al., 2021).\"}"}
{"id": "HcUf-QwZeFh", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: The overview of MxT-Bench, which can procedurally generate both various morphologies and tasks with minimal blueprints. MxT-Bench can not only construct the agents with different number of limbs, but also randomize missing limbs and size/mass of bodies. We could design the tasks with parameterized goal distributions. It also supports to import custom complex agents such as unimals (Gupta et al., 2022). Compared to relevant RL benchmarks in terms of (1) multi-task (task coverage), (2) multi-morphology (morphology coverage), and (3) scalability, MuJoCo (Todorov et al., 2012) and DM Control (Tassa et al., 2018) only have a single morphology for a single task. While other existing works (Yu et al., 2019; Huang et al., 2020) partially cover task-/morphology-axis with some sort of scalability, they do not satisfy all criteria.\\n\\nWe assume that node $v_i$ observes local sensory input $s_i$ at time step $t$, which includes the information of limb $i$ such as position, velocity, orientation, joint angle, or morphological parameters. To process these node features and graph structure, a morphology-agnostic policy can be modeled as node-based GNN (Kipf & Welling, 2017; Battaglia et al., 2018; Cappart et al., 2021), which takes a set of local observations $\\\\{s_i|V|\\\\}$ as an input and emits the actions for actuators of each joint $\\\\{a_e|E|\\\\}$. The objective of morphology-agnostic RL is the average of Equation 1 among given morphologies.\\n\\n**Goal-conditional RL**\\n\\nIn goal-conditional RL (Kaelbling, 1993; Schaul et al., 2015), the agent aims to find an optimal policy $\\\\pi^*(a|s, s_g)$ conditioned on goal $s_g \\\\in G$, where $G$ stands for goal space that is sub-dimension of state space $S$ (e.g. XYZ coordinates, velocity, or quaternion). The desired goal $s_g$ is sampled from the given goal distribution $p_\\\\psi: G \\\\rightarrow [0, \\\\infty)$, where $\\\\psi$ stands for task in the task space $\\\\Psi$ (e.g. reaching the agent's leg or touching the torso to the ball). The reward function can include a goal-reaching term, that is often modeled as $r_\\\\psi(s_t, s_g) = -d_\\\\psi(s_t, s_g)$, where $d_\\\\psi(\\\\cdot, \\\\cdot)$ is a task-dependent distance function, such as Euclidean distance, between the sub-dimension of interest in current state $s_t$ and given goal $s_g$. Some task $\\\\psi$ give multiple goals to the agents. In that case, we overload $s_g$ to represent a set of goals; $\\\\{s_i|N_\\\\psi\\\\}_{i=1}^N$, where $N_\\\\psi$ is the number of goals that should be satisfied in the task $\\\\psi$.\\n\\n**Morphology-Task Generalization**\\n\\nThis paper aims to achieve morphology-task generalization, where the learned policy should generalize over tasks and morphologies simultaneously. The optimal policy should generalize over morphology space $M$, task $\\\\Psi$, and minimize the distance to any given goal $s_g \\\\in G$. Mathematically, this objective can be formulated as follows:\\n\\n$$\\\\pi^* = \\\\arg \\\\max_{\\\\pi} \\\\frac{1}{1 - \\\\gamma} \\\\mathbb{E}_{m, \\\\psi \\\\sim M, \\\\Psi} \\\\mathbb{E}_{s_g \\\\sim p_\\\\psi(s_g)} \\\\mathbb{E}_{s_m, a_m \\\\sim \\\\rho_\\\\pi(s_m)} \\\\pi(a_m|s_m, s_g)[-d_\\\\psi(s_m, s_g)],$$\\n\\nwhere the graph representation of morphology $m \\\\in M$ is denoted as $G_m = (V_m, E_m)$, and $s_m := \\\\{s_i|V_m\\\\}$ and $a_m := \\\\{a_e|E_m\\\\}$ stand for a set of local observations and actions of morphology $m$. While we can use multi-task online RL to maximize Equation 2 in principle, it is often sample inefficient due to the complexity of task, which requires a policy that can handle the diversity of the scene among morphology $M$, task $\\\\Psi$, and goal space $G$ simultaneously.\\n\\n4.1 MxT-Bench as a Test Bed for Morphology-Task Generalization\\n\\nTo overcome these shortcomings in the existing RL environments, we develop MxT-Bench, which has a wide coverage over both tasks and morphologies to test morphology-task generalization, with the...\"}"}
{"id": "HcUf-QwZeFh", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"functionalities for procedural generation from minimal blueprints (Figure 4). MxT-Bench is built on top of Brax (Freeman et al., 2021) and Composer (Gu et al., 2021a), for faster iteration of behavior distillation with hardware-accelerated environments. Beyond supporting multi-morphology and multi-task settings, the scalability of MxT-Bench helps to test the broader range of morphology-task generalization since we can easily generate out-of-distribution tasks and morphologies, compared to manually-designed morphology or task specifications.\\n\\nIn the morphology axis, we prepare 4 types of blueprints (ant, claw, centipede, and worm) as base morphologies, since they are good at the movement on the XY-plane. Through MxT-Bench, we can easily spawn agents that have different numbers of bodies, legs, or different sizes, lengths, and weights. Moreover, we can also import the existing complex morphology used in previous work. For instance, we include 60+ morphologies that are suitable for goal-reaching, adapted from Gupta et al. (2022) designed in MuJoCo. In the task axis, we design reach, touch, and twisters as basic tasks, which could evaluate different aspects of the agents; the simplest one is the reach task, where the agents aim to put their leg on the XY goal position. In the touch task, agents aim to create and maintain contact between a specified torso and a movable ball. The touch task requires reaching behavior, while maintaining a conservative momentum to avoid kicking the ball away from the agent. Twisters tasks are the multi-goal problems; for instance, the agents should satisfy XY-position for one leg, and Z height for another leg. We pre-define 4 variants of twisters with max 3 goals (see Appendix B.2 for the details). Furthermore, we could easily specify both initial and goal position distribution with parameterized distribution. In total, we prepare 180+ environments combining the morphology and task axis for the experiments in the later section. See Appendix B for further details.\\n\\n4.2 Behavior Distillation\\n\\nToward broader generalization over morphologies and tasks, a single policy should handle the diversity of the scene among morphology $M$, task $\\\\Psi$, and goal space $G$ simultaneously. Multi-task online RL from scratch, however, is difficult to tune, slow to iterate, and hard to reproduce. Instead, we employ behavior cloning on RL-generated expert behaviors to study morphology-task generalization. To obtain rich goal-reaching behaviors, we train a single-morphology single-task policy using PPO (Schulman et al., 2017) with a simple MLP policy, which is significantly more efficient than multi-morphology training done in prior work (Gupta et al., 2022; Kurin et al., 2021; Huang et al., 2020). Since MxT-Bench is built on top of Brax (Freeman et al., 2021), a hardware-accelerated simulator, training PPO policies can be completed in about 5 $\\\\sim$ 30 minutes per environment (on NVIDIA RTX A6000). We collect many behaviors per morphology-task combination from the expert policy rollout. We then train a single policy $\\\\pi_{\\\\theta}$ with a supervised learning objective:\\n\\n$$L_{\\\\pi} = -\\\\mathbb{E}_{m,\\\\psi \\\\sim M, \\\\Psi} \\\\mathbb{E}_{s \\\\sim D_{m,\\\\psi}} \\\\log \\\\pi_{\\\\theta}(a | \\\\{s, s_g\\\\})$$\\n\\nwhere $D_{m,\\\\psi}$ is an expert dataset of morphology $m$ and task $\\\\psi$. Importantly, offline behavior distillation protocol runs (parallelizable) single-task online RL only once, and allows us to reuse the same fixed data to try out various design choices, such as model architectures or local features of morphology-task graph, which is often intractable in multi-task online RL.\\n\\n4.3 Morphology-Task Graph\\n\\nTo learn a single policy that could solve various morphology-task problems, it is essential to unify the input-output interface among those. Inspired by the concept of scene graph (Johnson et al., 2015) and morphological graph (Wang et al., 2018), we introduce the notion of morphology-task graph (MTG) representation that incorporates goal information while preserving the geometric structure of the task. Morphology-task graph could express the agent's observations, actions, and goals/tasks in a unified graph space. Although most prior morphology-agnostic RL has focused on locomotion (running) with the reward calculated from the velocity of the center of mass, morphology-task graph could naturally extend morphology-agnostic RL to multi-task goal-oriented settings: including static single positional goals (reaching), multiple-goal problems (twister-game) and object interaction tasks (ball-touching). In practice, we develop two different ways to inform the goals and tasks (Figure 3); morphology-task graph v1 (MTGv1) accepts the morphological graph, encoded from the agent's geometric information, as an input-output interface, and merges positional goal information as a\"}"}
{"id": "HcUf-QwZeFh", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In-Distribution\\n\\n1.2019 $\\\\pm$ 0.41\\n0.5150 $\\\\pm$ 0.01\\n0.4776 $\\\\pm$ 0.01\\n0.4069 $\\\\pm$ 0.02\\n0.3128 $\\\\pm$ 0.02\\n\\nIn-Distribution (unimal)\\n\\n0.9090 $\\\\pm$ 0.03\\n0.6703 $\\\\pm$ 0.01\\n\u2013\\n0.4839 $\\\\pm$ 0.02\\n\\nCompositional (Morphology)\\n\\n1.1419 $\\\\pm$ 0.41\\n0.7216 $\\\\pm$ 0.01\\n\u2013\\n0.4940 $\\\\pm$ 0.01\\n\\nCompositional (Task)\\n\\n0.8932 $\\\\pm$ 0.01\\n0.6849 $\\\\pm$ 0.01\\n\u2013\\n0.5395 $\\\\pm$ 0.04\\n\\nOut-of-Distribution\\n\\n0.8979 $\\\\pm$ 0.01\\n0.7821 $\\\\pm$ 0.02\\n\u2013\\n0.6144 $\\\\pm$ 0.04\\n\\nTable 1: The average normalized final distance in various types of morphology-task generalization on MxT-Bench. We test (1) in-distribution, (2) compositional morphology/task, and (3) out-of-distribution generalization. (2) and (3) evaluate zero-shot transfer. We compare MLP, GNN and Transformer with MTGv1, Transformer with MTGv2, and tokenized MTGv2 (Reed et al., 2022). MTGv2 improves multi-task performance to other choices by 23 % in the in-distribution evaluation, and achieves better zero-shot transfer in the compositional and out-of-distribution settings by 14 \u223c 18 %.\\n\\npart of corresponding node features. For instance, in touch task, MTGv1 includes XY position of the movable ball as an extra node feature of the body node. Moreover, morphology-task graph v2 (MTGv2) considers given goals as additional disjoint nodes of morphological graph representation. These morphology-task graph strategies enable the policy to handle a lot of combinations of tasks and morphologies simultaneously.\\n\\nTransformer for MTG\\n\\nWhile morphology-task graph could represent the agent and goals in a unified manner, because the task structure may change over time, the policy should unravel the implicit relationship between the agent\u2019s modules and goals dynamically. We mainly employ Transformer as a policy architecture because it can process morphological graph as a fully-connected graph and achieve notable performance by leveraging the hidden relations between the nodes beyond manually-encoded geometric structure (Kurin et al., 2021). The combination of morphology-task graph and Transformer is expected to model the local relations between goals and target nodes explicitly and to embed the global relations among the modules. See Appendix A for further details.\\n\\n5 EXPERIMENTS\\n\\nWe first evaluate the multi-task performance of morphology-task graph representation (MTGv1, MTGv2) in terms of in-distribution (known morphology and task with different initialization), compositional (known task with unseen morphology or known morphology with unseen task), and out-of-distribution generalization (either morphology or task is unseen) on MxT-Bench (Section 5.1). Then, we investigate whether morphology-task graph could contribute to obtaining better control prior for multi-task fine-tuning (Section 5.2). In addition, we conduct the offline node feature selection to identify what is the most suitable node feature set for morphology-task generalization (Section 5.3). The results are averaged among 4 random seeds. See Appendix A for the hyperparameters. We also investigate the other axis of representations or architectures (Appendix E, F, G) and test the effect of dataset size, the number of morphology-task combinations, and model size (Appendix H). Lastly, we examine why morphology-task graph works well by visualizing attention weights (Appendix K).\\n\\nEvaluation Metric\\n\\nGoal-reaching tasks are evaluated by the distance to the goals at the end of episode (Pong et al., 2018; 2020; Ghosh et al., 2021; Choi et al., 2021; Eysenbach et al., 2021). However, this can be problematic in our settings, because the initial distance or the degree of goal-reaching behaviors might be different among various morphologies and tasks. We measure the performance of the policy $\\\\pi$ by using a normalized final distance metric $\\\\bar{d}(M, \\\\Psi; \\\\pi)$ over morphology $M$ and task space $\\\\Psi$ with pre-defined max/min value of each morphology $m$ and task $\\\\psi$,\\n\\n$$\\\\bar{d}(M, \\\\Psi; \\\\pi) = \\\\frac{1}{|M|} \\\\sum_{m} \\\\frac{1}{|\\\\Psi|} \\\\sum_{\\\\psi} \\\\left( \\\\sum_{s_g \\\\sim \\\\psi} P_{N_{\\\\psi,i}} \\\\left( d_{\\\\psi}(s_{m_T}, s_i) - \\\\min_{\\\\psi} d_{i,m,\\\\psi} \\\\right) - \\\\max_{\\\\psi} d_{i,m,\\\\psi} - \\\\min_{\\\\psi} d_{i,m,\\\\psi} \\\\right)$$\\n\\nwhere $s_{m_T}$ is the last state of the episode, $d_{i,m,\\\\psi}$ is a maximum, and $d_{i,m,\\\\psi}$ is a minimum distance of $i$-th goal $s_i$ with morphology $m$ and task $\\\\psi$. We use a distance threshold to train the expert PPO policy as $d_{i,m,\\\\psi}$, and the average distance from an initial position of the scene as $d_{i,m,\\\\psi}$. Equation 3 is normalized around the range of $[0, 1]$, and the smaller, the better. See Appendix B for the details.\"}"}
{"id": "HcUf-QwZeFh", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we test the fine-tuning performance for multi-task imitation learning on MxT-Bench. We adopt the\\nwith MTGv1 achieves better goal-reaching behaviors than GNN. In compositional and out-of-\\nwe also leverage 50 environments as a training set, and define 27 environments with diversified\\nmorphologies for morphology evaluation, and leverage 50 train environments and\\n(50)\\nTable 1 reveals that MTGv2 achieves the best multi-task goal-reaching performances among other\\nand others (Kurin et al., 2021; Hong et al., 2022) used {\\nHuang et al. (2020)\\nTo reveal whether the distilled policy obtains reusable inductive bias for unseen morphology or task,\\nwe used {\\nand test the combination to other observations (Gupta et al., 2022).\\nIn the agent's system, there are a lot of observable variables per module: such as Cartesian position\\n(p), Cartesian velocity (v), joint velocity (jv), joint angle (ja), relative position (rp), relative rotation (rr),\\ndof-index (id), joint range (jr), limb (la), angular velocity (ja), quaternion (rr), mass (m), inertia (i),\\nactuator's gear (g), shape (s), and morphological information\\n}. Considering the intersection of those, we define {\\nbase_set\\nand test the combination to other observations (Gupta et al., 2022) for both training and evaluation.\\nCompositional transfer results in Section 5.1, MTGv2 outperforms other baselines, and is better than the second best,\\nthat behavior-distilled policy works as a better prior knowledge for control. The same as zero-shot\\nFigure 5 shows that fine-tuning outperforms random initialization in all settings, which suggests\\nsame morphology-task division for compositional and out-of-distribution evaluation in Section 5.1.\\n8k); for instance, in compositional morphology evaluation (left in Figure 5), MTGv2 trained with 8k\\nsample efficiency (see Appendix J for the detailed scores). These results suggest MTGv2 significantly\\nMTGv1 by 50%\\nFurthermore, MTGv2 could work even with a small amount of dataset (4k,\\nin-distribution settings. These results imply MTGv2 might be the better formulation to realize the\\ncompositional zero-shot performance of MTGv2 is comparable with the performance of MTGv1 in\\nimpossible combinations in all the aspects of generalization. Comparing average normalized distance,\\ntransitions per each environment. See Appendix D for the details of environment division.\\nMTGv2 improves the multi-task performance against the second best, MTGv1, by 23% in in-\\nnearly 55%. Moreover, the\\nout-of-distribution evaluation. These results reveal that fine-tuning outperforms random initialization in all\\nsettings, and fine-tuned MTGv2 outperforms others by 50\\nout-of-distribution evaluation. These results suggest MTGv2 significantly\\nsample efficiency (see Appendix J for the detailed scores). These results suggest MTGv2 significantly\\nMTGv1 by 50%\\nMTGv2\\nMTGv1\\nMTGv1\\nMTGv2\\nMTGv2\\nMTGv1\\nMTGv1\\nMTGv2\\nMTGv1\\nMTGv1\\nMTGv1\\nMTGv2\\nMTGv2\\nMTGv1\\nMTGv1\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\\nMTGv2\"}"}
{"id": "HcUf-QwZeFh", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Although in Table 1 and Figure 5, we examine the compositional task generalization and out-of-distribution generalization with reach_hard tasks, where the goal distribution is farther than original reach tasks. While they seem more difficult unseen tasks (Furuta et al., 2021b), they also seem to have some sort of task similarities between training dataset environments and those evaluation environments.\\n\\nAnother question might be how morphology-task graph performs in more different unseen tasks. To evaluate compositional task generalization and out-of-distribution on the environments that have less similarity to the training datasets, we prepare push task, where the agents try to move the box objects to the given goal position. See Table 7 for the environment division. For training datasets, we leverage In-Distribution division in Table 4. Because this task requires the sufficient interaction with the object, the nature of tasks seem quite different from training dataset environments (reach, touch, and twisters).\\n\\nTable 17 shows the results of compositional task evaluation and Table 18 shows those of out-of-distribution evaluation. In contrast to Table 1, the zero-shot performance seems limited. Transferring pre-trained control primitives to significantly different tasks still remains as important future work. However, as shown in Figure 14, morphology-task graph works as better prior knowledge for downstream multi-task imitation learning, even with the environments that have less similarity to the pre-training datasets. As prior work suggested (Mandi et al., 2022), these results suggests that, in RL, jointly-learned multi-task model has a strong inductive bias even for unseen and significantly different environments.\\n\\n| Sub-domain      | Random MLP Transformer (MTGv1) | Transformer (MTGv2) | Transformer (MTGv2) | Transformer (MTGv2) |\\n|-----------------|--------------------------------|---------------------|---------------------|---------------------|\\n| ant_push        | 1.0808 \u00b1 0.02                 | 0.9995 \u00b1 0.00       | 0.9212 \u00b1 0.14       | 0.8836 \u00b1 0.09      |\\n| centipede_push  | 1.0816 \u00b1 0.01                 | 0.9942 \u00b1 0.00       | 0.9557 \u00b1 0.02       | 0.9377 \u00b1 0.01      |\\n| worm_push       | 1.1026 \u00b1 0.04                 | 0.6300 \u00b1 0.01       | 0.5579 \u00b1 0.10       | 0.4091 \u00b1 0.15      |\\n\\nTable 17: The average normalized final distance for compositional task evaluation on MxT-Bench with unseen push task. See Table 7 for the environment division.\\n\\n| Sub-domain      | Random MLP Transformer (MTGv1) | Transformer (MTGv2) | Transformer (MTGv2) | Transformer (MTGv2) |\\n|-----------------|--------------------------------|---------------------|---------------------|---------------------|\\n| ant_push_diverse| 1.0814 \u00b1 0.02                 | 1.0018 \u00b1 0.01       | 0.9906 \u00b1 0.01       | 0.8800 \u00b1 0.04      |\\n| centipede_push_diverse | 1.0848 \u00b1 0.02 | 1.0031 \u00b1 0.01 | 0.9997 \u00b1 0.00 | 0.9355 \u00b1 0.04 |\\n| worm_push_diverse | 1.1026 \u00b1 0.04 | 0.6300 \u00b1 0.01 | 0.5579 \u00b1 0.10 | 0.4091 \u00b1 0.15 |\\n\\nTable 18: The average normalized final distance for out-of-distribution evaluation on MxT-Bench with unseen push task. See Table 7 for the environment division.\"}"}
{"id": "HcUf-QwZeFh", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Figure 14: Comparison of multi-task goal-reaching performances on fine-tuning settings in the unseen push tasks. The results imply that morphology-task graph works as better prior knowledge for downstream multi-task imitation learning, even with the environments that have less similarity to the pre-training datasets.\\n\\n| Method (data size)           | Compositional (Task) Out-of-Distribution | MLP (4K, randinit) | MLP (4K, fine-tuning) | MLP (8K, randinit) | MLP (8K, fine-tuning) | MLP (12K, randinit) | MLP (12K, fine-tuning) | Transformer (MTGv1) (4K, randinit) | Transformer (MTGv1) (4K, fine-tuning) | Transformer (MTGv2) (4K, randinit) | Transformer (MTGv2) (4K, fine-tuning) | Transformer (MTGv1) (8K, randinit) | Transformer (MTGv1) (8K, fine-tuning) | Transformer (MTGv2) (8K, randinit) | Transformer (MTGv2) (8K, fine-tuning) | Transformer (MTGv1) (12K, randinit) | Transformer (MTGv1) (12K, fine-tuning) | Transformer (MTGv2) (12K, randinit) | Transformer (MTGv2) (12K, fine-tuning) |\\n|-----------------------------|------------------------------------------|--------------------|-----------------------|--------------------|---------------------|--------------------|---------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|\\n| Compositional (Task) Out-of-Distribution | 0.8042 \u00b1 0.05                        | 0.8332 \u00b1 0.04                  | MLP (4K, randinit)    | 0.6653 \u00b1 0.03                  | 0.7191 \u00b1 0.02                  | Transformer (MTGv1) (4K, randinit) | 0.6027 \u00b1 0.03                  | Transformer (MTGv1) (4K, fine-tuning) | 0.5448 \u00b1 0.06                  | Transformer (MTGv2) (4K, randinit) | 0.5704 \u00b1 0.03                  | Transformer (MTGv2) (4K, fine-tuning) | 0.4993 \u00b1 0.06                  | Transformer (MTGv2) (8K, randinit) | 0.5510 \u00b1 0.05                  | Transformer (MTGv2) (8K, fine-tuning) | 0.4132 \u00b1 0.04                  | Transformer (MTGv2) (12K, randinit) | 0.4993 \u00b1 0.06                  | Transformer (MTGv2) (12K, fine-tuning) |\\n| Compositional (Task) Out-of-Distribution | 0.7818 \u00b1 0.05                        | 0.8108 \u00b1 0.02                  | Transformer (MTGv1) (4K, randinit) | 0.6276 \u00b1 0.05                  | 0.6999 \u00b1 0.04                  | Transformer (MTGv1) (4K, fine-tuning) | 0.6398 \u00b1 0.05                  | Transformer (MTGv1) (8K, randinit) | 0.6027 \u00b1 0.03                  | Transformer (MTGv1) (8K, fine-tuning) | 0.5448 \u00b1 0.06                  | Transformer (MTGv2) (8K, randinit) | 0.5704 \u00b1 0.03                  | Transformer (MTGv2) (8K, fine-tuning) | 0.4993 \u00b1 0.06                  | Transformer (MTGv2) (12K, randinit) | 0.4993 \u00b1 0.06                  | Transformer (MTGv2) (12K, fine-tuning) |\\n| Compositional (Task) Out-of-Distribution | 0.7542 \u00b1 0.06                        | 0.7751 \u00b1 0.03                  | Transformer (MTGv1) (4K, randinit) | 0.6027 \u00b1 0.03                  | 0.6999 \u00b1 0.04                  | Transformer (MTGv1) (4K, fine-tuning) | 0.6398 \u00b1 0.05                  | Transformer (MTGv1) (8K, randinit) | 0.6027 \u00b1 0.03                  | Transformer (MTGv1) (8K, fine-tuning) | 0.5448 \u00b1 0.06                  | Transformer (MTGv2) (8K, randinit) | 0.5704 \u00b1 0.03                  | Transformer (MTGv2) (8K, fine-tuning) | 0.4993 \u00b1 0.06                  | Transformer (MTGv2) (12K, randinit) | 0.4993 \u00b1 0.06                  | Transformer (MTGv2) (12K, fine-tuning) |\\n| Compositional (Task) Out-of-Distribution | 0.6847 \u00b1 0.04                        | 0.7611 \u00b1 0.03                  | Transformer (MTGv1) (4K, randinit) | 0.6027 \u00b1 0.03                  | 0.6999 \u00b1 0.04                  | Transformer (MTGv1) (4K, fine-tuning) | 0.6398 \u00b1 0.05                  | Transformer (MTG1) (8K, randinit) | 0.6027 \u00b1 0.03                  | Transformer (MTG1) (8K, fine-tuning) | 0.5448 \u00b1 0.06                  | Transformer (MTG2) (8K, randinit) | 0.5704 \u00b1 0.03                  | Transformer (MTG2) (8K, fine-tuning) | 0.4993 \u00b1 0.06                  | Transformer (MTG2) (12K, randinit) | 0.4993 \u00b1 0.06                  | Transformer (MTG2) (12K, fine-tuning) |\\n| Compositional (Task) Out-of-Distribution | 0.6678 \u00b1 0.04                        | 0.7110 \u00b1 0.06                  | Transformer (MTG1) (4K, randinit) | 0.6027 \u00b1 0.03                  | 0.6999 \u00b1 0.04                  | Transformer (MTG1) (4K, fine-tuning) | 0.6398 \u00b1 0.05                  | Transformer (MTG1) (8K, randinit) | 0.6027 \u00b1 0.03                  | Transformer (MTG1) (8K, fine-tuning) | 0.5448 \u00b1 0.06                  | Transformer (MTG2) (8K, randinit) | 0.5704 \u00b1 0.03                  | Transformer (MTG2) (8K, fine-tuning) | 0.4993 \u00b1 0.06                  | Transformer (MTG2) (12K, randinit) | 0.4993 \u00b1 0.06                  | Transformer (MTG2) (12K, fine-tuning) |\\n| Compositional (Task) Out-of-Distribution | 0.7037 \u00b1 0.03                        | 0.7452 \u00b1 0.03                  | Transformer (MTG1) (4K, randinit) | 0.6027 \u00b1 0.03                  | 0.6999 \u00b1 0.04                  | Transformer (MTG1) (4K, fine-tuning) | 0.6398 \u00b1 0.05                  | Transformer (MTG1) (8K, randinit) | 0.6027 \u00b1 0.03                  | Transformer (MTG1) (8K, fine-tuning) | 0.5448 \u00b1 0.06                  | Transformer (MTG2) (8K, randinit) | 0.5704 \u00b1 0.03                  | Transformer (MTG2) (8K, fine-tuning) | 0.4993 \u00b1 0.06                  | Transformer (MTG2) (12K, randinit) | 0.4993 \u00b1 0.06                  | Transformer (MTG2) (12K, fine-tuning) |\\n\\nTable 19: The average normalized final distance among test environments in fine-tuning settings (compositional morphology/task or out-of-distribution evaluation) with unseen push task.\\n\\nMorphology-Task Generalization\\nIn the previous literature, multi-task in RL has arisen from each MDP component; from different (1) dynamics (Hallak et al., 2015; Yu et al., 2019), (2) reward (Kaelbling, 1993; Andrychowicz et al., 2017), and (3) state/action space (Wang et al., 2018; Huang et al., 2020). We think multi-morphology RL covers (1) and (3), and multi-task (or multi-goal) RL does (2) and (1). Considering morphology-task generalization, we could study the generalization in RL across all the MDP components (dynamics, reward, state space, action space). While, from a broader perspective, the notion of multi-task in RL may contain both morphological and goal diversity, in this paper, we treat them separately, and multi-task just stands for multi-goal settings.\\n\\nConnection to Policy Distillation\\nPolicy distillation has been proposed and studied for a while (Rusu et al., 2016; Parisotto et al., 2016; Levine et al., 2016; Czarnecki et al., 2019), where a single student policy distills the knowledge from multiple teacher policies to obtain better multitask learners. In contrast, we call our protocol behavior distillation, where the single student policy distills the knowledge from multi-source offline data (e.g. human teleoperation, scripted behavior (Singh et al., 2021), play data (Lynch et al., 2019)), not limited to RL policy, since recent RL or robotics research often leverage such a data-driven approach for scalability (Levine et al., 2020; Chen et al., 2021b; Gu et al., 2021a; Lee et al., 2022; Reed et al., 2022; Zeng et al., 2020; Shridhar et al., 2022b).\"}"}
