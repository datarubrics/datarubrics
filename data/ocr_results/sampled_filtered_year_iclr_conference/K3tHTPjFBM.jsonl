{"id": "K3tHTPjFBM", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Understanding and leveraging the 3D structures of proteins is central to various tasks in biology and drug discovery. While deep learning has been applied successfully for modeling protein structures, current methods usually employ distinct models for different tasks. Such a single-task strategy is not only resource-consuming when the number of tasks increases but also incapable of combining multi-source datasets for larger-scale model training, given that protein datasets are usually of small size for most structural tasks. In this paper, we propose to adopt one single model to address multiple tasks jointly, upon the input of 3D protein structures. In particular, we first construct a standard multi-task benchmark called PROMPT, consisting of 6 representative tasks integrated from 4 public datasets. The resulting benchmark contains partially labeled data for training and fully-labeled data for validation/testing. Then, we develop a novel graph neural network for multi-task learning, dubbed Heterogeneous Multichannel Equivariant Network (HeMeNet), which is equivariant to 3D rotations/translations/reflections of proteins and able to capture various relationships between different atoms owing to the heterogeneous multichannel graph construction of proteins. Besides, HeMeNet is able to achieve task-specific learning via the task-aware readout mechanism. Extensive evaluations verify the effectiveness of multi-task learning on our benchmark, and our model generally surpasses state-of-the-art models. Our study is expected to open up a new venue for structure-based protein learning.\\n\\n1 INTRODUCTION\\n\\nProteins are the workhorses of biological systems, performing a myriad of vital functions within cells and organisms. In recent years, learning-based methods have been applied widely to represent and leverage the 3D structures of proteins for various tasks including property prediction (Wang et al., 2023a), affinity prediction (Li et al., 2021), rigid docking (Ganea et al., 2022), and generation Kong et al. (2023), owing to their more efficient and reproducible implementation compared to those wet-lab experimental approaches. A major part of learning-based methods resort to Graph Neural Networks (GNNs) (Zhou et al., 2020; Xu et al., 2019), which naturally encode the 3D structures of proteins by modeling atoms or residues as nodes, and the connections in between as edges. In addition, certain GNNs are geometry-aware and they are $E(3)$ equivariant/invariant in terms of arbitrary rotation/reflection/translation of the input structures, enabling the promising generalization ability across different choices of 3D coordinate systems (Satorras et al., 2021; Huang et al., 2022; Brandstetter et al., 2022).\\n\\nIn spite of the fruitful progress, existing methods usually employ one model for one task. A clear drawback of such a single-task training strategy is that it is resource-consuming, as the model should be re-trained for a newly coming task. More importantly, it is unable to combine multi-source datasets for model training. It is well known that structural datasets are often limited in size due to the expensive cost of collecting protein 3D structures and labels via wet lab experiments. For example, in PDBbind (Wang et al., 2004), a binding affinity database, there are only 5316 complexes for the Ligand Binding Affinity (LBA) refined set and 2852 complexes for the Protein-Protein Interaction (PPI) set. Considering that deep learning models nowadays are typically equipped with thousands of hundreds of parameters, conducting model training on a single-task dataset of small size usually leads to defective generalization ability.\"}"}
{"id": "K3tHTPjFBM", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this paper, we propose protein multi-task learning: one model for multiple tasks upon 3D protein structures. We expect to increase the number of training samples by leveraging multi-task datasets and hence improve the generalizability and universality of the model. Nevertheless, accomplishing multi-task training is challenging. The first challenge is that there is even no available benchmark for multi-task training and evaluation. The ideal benchmark should cover a sufficient range of data and task types, and it should also contain a fully-labeled subset to better compare how a model performs on the same input for different task outputs. Therefore, constructing a benchmark of protein multi-task learning is far more difficult beyond a simple combination of current public datasets. The second challenge is that it is nontrivial to design a model that is generalist to serve our purpose. The model we desire should be capable of processing the complicated 3D structures of input proteins of various types, and it should also exhibit generally-promising performance across different tasks.\\n\\nTo achieve protein multi-task learning, we make the following contributions:\\n\\n\u2022 To the best of our knowledge, we are the first to propose the concept of structure-based protein multi-task learning. To fulfil our purpose, we carefully integrate the structures and labels from 4 public datasets, and construct a new benchmark named PROtein MultiPle Task (PROMPT), which consists of 6 representative tasks upon 3 different types of inputs, including single-chain proteins, protein-protein and ligand-protein complexes.\\n\\n\u2022 We propose a novel model for protein structure learning, dubbed Heterogeneous Multichannel Equivariant Network (HeMeNet), which is equivariant to 3D rotations/translations/reflections of proteins and able to capture various relationships between different atoms owing to the heterogeneous multichannel graph construction of proteins. Additionally, HeMeNet develops a task-aware readout mechanism by associating the output head of each task with a learnable task prompt.\\n\\n\u2022 For the experiments on PROMPT, our HeMeNet surpasses other state-of-the-art methods in most tasks under both the single-task and multi-task settings. Particularly on the LBA and PPI tasks, we find that the multi-task version of HeMeNet is significantly better than its single-task counterpart, verifying the benefit of multi-task learning in these cases.\\n\\nRelated Work\\n\\nProtein Interaction and Property Prediction\\n\\nPredicting the binding affinity and functional properties for proteins with computational methods is of growing interest (Wang et al., 2022; Zhao et al., 2020). Previous research learns protein representations by different forms of information, most of which take amino acid sequence (Alley et al., 2019; Rao et al., 2019), multiple sequence alignment (Rao et al., 2021) or 3D structure (Hermosilla et al., 2021; Zhang et al., 2023) as input. Many works encode the information of a protein\u2019s 3D structure by graph neural network (Gligorijevi\u0107 et al., 2021a; Zhang et al., 2023; Morehead et al., 2022). Li et al. (2021) take full-atom geometry at the interaction pocket/interface, and Zhang et al. (2023) take residue-level geometry of the whole protein for property prediction. Our method utilizes full-atom geometry on the whole protein to address interaction and property prediction tasks together.\\n\\nEquivariant Graph Neural Networks\\n\\nMany equivariant graph neural networks have emerged recently with the inductive bias of 3D symmetry, modeling various tasks including docking, molecular function prediction and sequence design (Thomas et al., 2018; Gasteiger et al., 2020; Satorras et al., 2021; Brandstetter et al., 2022). To empower the model with the ability to handle the complicated full-atom geometry, our model is closely related to dyMEAN (Kong et al., 2023), a multi-channel equivariant network that can deal with a dynamic number of channels in the protein graph. We propose a more powerful heterogeneous equivariant network that is capable of handling various incoming message types.\\n\\nProtein Multi-Task Learning\\n\\nMulti-task learning takes advantage of knowledge transfer from multiple tasks, achieving a better generalization performance. In the field of protein, several works leverage multi-task learning on the task of interaction prediction and property prediction, most of which are for sequence-based models. Lin et al. (2022) propose a pre-training method to learn protein and ligand representation, together with a multi-task dual adaptation mechanism for better LBA prediction from sequences. Shi et al. (2023) design three Enzyme Commission number related hierarchical tasks to train the model. Moon & Kim (2022) take a knowledge distillation manner to transfer knowledge from single-task models for DTA prediction. Wang et al. (2023b) introduce a\"}"}
{"id": "K3tHTPjFBM", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=SJU4ayYgl.\\n\\nXiangzhe Kong, Wenbing Huang, and Yang Liu. End-to-end full-atom antibody design. arXiv preprint arXiv:2302.00203, 2023.\\n\\nShuangli Li, Jingbo Zhou, Tong Xu, Liang Huang, Fan Wang, Haoyi Xiong, Weili Huang, Dejing Dou, and Hui Xiong. Structure-aware interactive graph neural networks for the prediction of protein-ligand binding affinity. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 975\u2013985, 2021.\\n\\nShaofu Lin, Chengyu Shi, and Jianhui Chen. Generalized dta: combining pre-training and multi-task learning to predict drug-target binding affinity for unknown drug discovery. BMC bioinformatics, 23(1):1\u201317, 2022.\\n\\nZemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. In Proceedings of the ACM Web Conference 2023, pp. 417\u2013428, 2023.\\n\\nChaeyoung Moon and Dongsup Kim. Prediction of drug\u2013target interactions through multi-task learning. Scientific Reports, 12(1):18323, 2022.\\n\\nAlex Morehead, Xiao Chen, Tianqi Wu, Jian Liu, and Jianlin Cheng. EGR: Equivariant Graph Refinement and Assessment of 3D Protein Complex Structures. arXiv e-prints, art. arXiv:2205.10390, May 2022. doi: 10.48550/arXiv.2205.10390.\\n\\nRoshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, and Yun Song. Evaluating protein transfer learning with tape. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch \u00b4e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/37f65c068b7723cd7809ee2d31d7861c-Paper.pdf.\\n\\nRoshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 8844\u20138856. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/rao21a.html.\\n\\nV\u00b4\u0131ctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9323\u20139332. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/satorras21a.html.\\n\\nKristof Sch \u00a8utt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert M \u00a8uller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf.\\n\\nZhenkun Shi, Rui Deng, Qianqian Yuan, Zhitao Mao, Ruoyu Wang, Haoran Li, Xiaoping Liao, and Hongwu Ma. Enzyme commission number prediction and benchmarking with hierarchical dual-core multitask learning framework. Research, 6:0153, 2023.\\n\\nMartin Steinegger and Johannes S\u00a8oding. Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. Nature Biotechnology, 35, 10 2017. doi: 10.1038/nbt.3988.\\n\\nNathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds. arXiv e-prints, art. arXiv:1802.08219, feb 2018. doi: 10.48550/arXiv.1802.08219.\"}"}
{"id": "K3tHTPjFBM", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Raphael Townshend, Martin Vogele, Patricia Suriana, Alex Derry, Alexander Powers, Yianni Laloudakis, Sidhika Balachandar, Bowen Jing, Brandon Anderson, Stephan Eismann, Risi Kondor, Russ Altman, and Ron Dror. Atom3d: Tasks on molecules in three dimensions. In J. Vanschoren and S. Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Curran, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/c45147dee729311ef5b5c3003946c48f-Paper-round1.pdf.\\n\\nPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f3, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJXMpikCZ.\\n\\nGuangyu Wang, Xiaohong Liu, Kai Wang, Yuanxu Gao, Gen Li, Daniel T Baptista-Hon, Xiaohong Helena Yang, Kanmin Xue, Wa Hou Tai, Zeyu Jiang, et al. Deep-learning-enabled protein-protein interaction analysis for prediction of sars-cov-2 infectivity and variant evolution. Nature Medicine, pp. 1\u201312, 2023a.\\n\\nHuiwen Wang, Haoquan Liu, Shangbo Ning, Chengwei Zeng, and Yunjie Zhao. DLSSAffinity: protein-ligand binding affinity prediction via a deep learning model. Physical Chemistry Chemical Physics (Incorporating Faraday Transactions), 24(17):10124\u201310133, May 2022. doi: 10.1039/D1CP05558E.\\n\\nRenxiao Wang, Xueliang Fang, Yipin Lu, and Shaomeng Wang. The pdbbind database: Collection of binding affinities for protein-ligand complexes with known three-dimensional structures. Journal of Medicinal Chemistry, 47(12):2977\u20132980, 2004. doi: 10.1021/jm030580l. URL https://doi.org/10.1021/jm030580l. PMID: 15163179.\\n\\nZeyuan Wang, Qiang Zhang, Shuang-Wei HU, Haoran Yu, Xurui Jin, Zhichen Gong, and Huajun Chen. Multi-level protein structure pre-training via prompt learning. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?id=XGagtiJ8XC.\\n\\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=ryGs6iA5Km.\\n\\nZuobai Zhang, Minghao Xu, Arian Rokkum Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang. Protein representation learning by geometric structure pretraining. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=to3qCB3tOh9.\\n\\nYingwen Zhao, Jun Wang, Jian Chen, Xiangliang Zhang, Maozu Guo, and Guoxian Yu. A literature review of gene function prediction by modeling gene ontology. Frontiers in genetics, 11:400, 2020.\\n\\nJie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI open, 1:57\u201381, 2020.\\n\\nZhaocheng Zhu, Chence Shi, Zuobai Zhang, Shengchao Liu, Minghao Xu, Xinyu Yuan, Yangtian Zhang, Junkun Chen, Huiyu Cai, Jiarui Lu, Chang Ma, Runcheng Liu, Louis-Pascal Xhonneux, Meng Qu, and Jian Tang. Torchdrug: A powerful and flexible machine learning platform for drug discovery, 2022.\"}"}
{"id": "K3tHTPjFBM", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Dataset Sources\\n\\nA.1.1 ENZYME COMMISSION AND GENETOLOGY\\n\\nWe adopt the data set from (Gligorijevi \u00b4c et al., 2021b). This data set contains 19201 PDB chains from 538 EC numbers, selected from the third and fourth levels of EC tree. The GO terms with at least 50 and not more than 5,000 training examples are selected. The number of classes in each branch of GO (MF, BP, CC) is 490, 1944 and 321, respectively. This data set consolidates 36641 PDB chains with their GO terms label. We obtain the structure of PDB chains from TorchDrug (Zhu et al., 2022).\\n\\nA.1.2 LBA AND PPI\\n\\nWe adopt the data set from PDBbind (Wang et al., 2004) (version 2020). This dataset contains 5316 protein-ligand complexes in the refined set and 2852 protein-protein complexes with known binding data (in the form of $K_d$, $K_i$, $IC_{50}$ values) manually collected from the original references (Wang et al., 2004). We obtain the structure of the complexes from PDBbind. The PDBbind dataset can be downloaded from http://www.pdbbind.org.cn.\\n\\nA.2 Prompt Statistics\\n\\nAs described in section 3, after the procedure of labeling and splitting, we get a new dataset named Prompt. Table 5 shows the detailed statistics of Prompt in different tasks. Note that the sample number for multi-task is slightly different from that in Table 4 since we removed samples with atom numbers greater than 15,000.\\n\\nTable 4: Dataset split. The fully-labeled data are randomly divided into the train, validation and test sets. Partially labeled samples located in the clusters different from the above test complexes are retained and added to the training set.\\n\\n| Clusters after merged | Train set size | Validation set size | Test set size |\\n|----------------------|---------------|---------------------|--------------|\\n|                      | 30034         | 31252               | 530          |\\n|                      |               |                     | 469          |\\n\\nTable 5: Dataset details for different tasks. We summarize the number of samples that contains a specific task's annotation in Prompt.\\n\\n| Task Name | # Train samples | # Validation samples | # Test samples |\\n|-----------|-----------------|----------------------|---------------|\\n| multi-task| 30904           | 516                  | 467           |\\n| LBA       | 3247            | 493                  | 452           |\\n| PPI       | 1976            | 23                   | 15            |\\n| EC        | 15025           | 516                  | 467           |\\n| GO-MF     | 22997           | 516                  | 467           |\\n| GO-BP     | 21626           | 516                  | 467           |\\n| GO-CC     | 10543           | 516                  | 467           |\"}"}
{"id": "K3tHTPjFBM", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we mainly introduce TR and TS borrowed from (Kong et al., 2023) and our modification for the heterogeneous graph.\\n\\nThe Geometric Relation Extractor TR can deal with coordinate sets with different channels. Given $X_i \\\\in \\\\mathbb{R}^{3 \\\\times c_i}$ and $X_j \\\\in \\\\mathbb{R}^{3 \\\\times c_j}$, we can compute the channel-wise distance for each coordinate pairs:\\n\\n$$D_{ij}(p, q) = ||X_i(:, p) - X_j(:, q)||_2.$$ \\n\\nDifferent from (Kong et al., 2023), we use two fixed binary vectors $w_i \\\\in \\\\mathbb{R}^{c_i \\\\times 1}$ and $w_j \\\\in \\\\mathbb{R}^{c_j \\\\times 1}$, when there is an element in the channel, its weight is set to 1, otherwise 0. We also adjusted the learnable attribute matrices $A_i \\\\in \\\\mathbb{R}^{c_i \\\\times d}$ and $A_j \\\\in \\\\mathbb{R}^{c_j \\\\times d}$ to be suitable to our input, assigning different element embedding for each channel. The final output $R_{ij} \\\\in \\\\mathbb{R}^{d \\\\times d}$ is given by:\\n\\n$$R_{ij} = A_i^T (w_i w_j^T \\\\odot D_{ij}) A_j.$$ (8)\\n\\n$R_{ij}$ keeps its shape awareness of $c_i$ and $c_j$.\\n\\nThe Geometric Message Scaler TS aims to generate geometric information of vary coordinate set $X \\\\in \\\\mathbb{R}^{3 \\\\times c}$ with the fixed length incoming message $s = \\\\phi(x(m_{ij})) \\\\in \\\\mathbb{R}^C$, where $C = 14$ is the max channel size of the common amino acids. Then, $TS(X, s)$ is calculated by:\\n\\n$$X' = X \\\\cdot \\\\text{diag}(s'),$$ (9)\\n\\nwhere $s' \\\\in \\\\mathbb{R}^c$ is the average pooling of $s$ with a sliding window of size $C - c + 1$ and stride 1, and $\\\\cdot$ is a diagonal matrix with the input vector $s$ as the diagonal elements.\\n\\nIMPLEMENTATION DETAILS AND HYPERPARAMETERS\\n\\nIn this section, we introduce the implementation details of all baselines and our model. For all models, we concatenate the hidden output for the final output. For the multi-task setting, all the models except HeMeNet take the sum readout method. The feature after the readout function will be fed into six two-layer MLPs to make predictions for different prediction tasks. The input for models are full-atom with KNN edges, except for GearNet and HeMeNet.\\n\\nGCN (Kipf & Welling, 2017), GAT (Veli\u010dkovi\u0107 et al., 2018) and SchNet (Sch\u00fctt et al., 2017). We take the implementation in PyTorch Geometric (Fey & Lenssen, 2019), with a 3-layer design. For all the models, the hidden size is set to 256.\\n\\nGearNet (Zhang et al., 2023). We re-implement GearNet with reference to its official implementation, with a six-layer design. The hidden size is set to 512, and the cutoff value is 4.5 following the original settings. For the multi-task setting, we take the sum readout method. We use the alpha-Carbon atom only graph for GearNet as the input.\\n\\nEGNN (Satorras et al., 2021). We re-implement EGNN with reference to its official implementation, with a 3-layer design. The hidden size is set to 256.\\n\\nGVP (Jing et al., 2021). We take the implementation in PyTorch Geometric (Fey & Lenssen, 2019), with a 3-layer design. The hidden size is set to 128 following the original implementation.\\n\\ndyMEAN (Kong et al., 2023). We re-implement dyMEAN with reference to its official implementation, with a 6-layer design. The hidden size is set to 256.\\n\\nHeMeNet (ours). We take a 6-layer design for our model, and the hidden size is set to 256. We take our task-aware readout module to generate features for different tasks. We use the full-atom heterogeneous graph for HeMeNet as the input.\"}"}
{"id": "K3tHTPjFBM", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"trained on samples with labels of the corresponding task. We also remove the task-aware readout\\nof our model for a fair comparison. For multi-task training, the models are trained on all partially\\nlabeled training samples. During distributed training, for each batch on each node, we use a balanced sampler to sample at least\\none complex from LBA and one complex from PPI. We include samples with up to 15,000 atoms\\nfor training and evaluation. All models are trained for 30 epochs on 4 NVIDIA A100 GPUs with a\\nbatch size of 8 for each GPU. More detail on experimental settings can be found in Appendix C\\n\\nBaselines\\nWe compared our model with seven representative baselines. GCN (Kipf & Welling, 2017) aggregates information weighted by the degree of nodes. GAT (Veli\u02c7ckovi\u00b4c et al., 2018) utilizes an attention mechanism for message passing. Schnet (Sch\u00a8utt et al., 2017) is an invariant network with continuous filter convolution on the 3D molecular graph. GearNet (Zhang et al., 2023) designs a relational message-passing network to capture information on protein function tasks. Besides the previous invariant models, we also compare our method with equivariant models. EGNN (Satorras et al., 2021) is a lightweight but effective E(n) equivariant graph neural network. GVP (Jing et al., 2021) designs an equivariant geometric vector perceptron for protein representation. dyMEAN (Kong et al., 2023) is an equivariant model for antibody design; it takes a dynamic multichannel equivariant function for full-atom coordinates. For GearNet, we take alpha-Carbon atoms as input following the original paper, and we take full-atom graphs for other models.\\n\\nEvaluation\\nFor LBA and PPI tasks, we employ the commonly used Root Mean Square Error (RMSE) and Mean Average Error (MAE) as the evaluation metrics (Townshend et al., 2021). For EC and GO tasks, we use maximum F-score (Fmax) following (Zhang et al., 2023). Each experiment is independently run three times with different random seeds.\\n\\n5.2 RESULTS ON PROMPT\\nWe conduct experiments under both single-task and multi-task settings. The mean results of three\\nruns are reported in Table 1. Results with standard deviation can be found in Appendix D. According\\nto the results, we draw conclusions summarized in the subsequent paragraphs.\\n\\nOur model outperforms the baselines on most of the tasks under both settings. Under the\\nsingle-task setting, our model surpasses other models in five of the six tasks. Under the multi-task\\nsetting, our model surpasses other models in four of the six tasks, with the remaining two tasks\\nreaching second and third place, respectively. These results demonstrate the effectiveness of our\\nheterogeneous graph construction and relational equivariant message passing for handling the full-\\natom input. Notably, under the single-task setting, only the models with a heterogeneous message\\npassing (GearNet and ours) can perform well on all of the four property prediction tasks. For the\\nsingle-task PPI prediction, our model and other full-atom models are struggling due to the small size\\nof the PPI training set and the noise introduced by processing the atoms of the whole protein. Under\\nthe multi-task setting, our full-atom model, benefiting from joint learning, shows superior results on\\ndifferent tasks, and there are two main interesting observations discussed next.\\n\\nOur model benefits from the multi-task setting, especially on LBA and PPI. We observe that\\nalmost all models improve their performance on LBA and PPI tasks under the multi-task setting.\\nIn particular, our model significantly improves the PPI RMSE from 6.031 to 1.087 by utilizing a\\ntraining set that is more than ten times larger (2587 for PPI single-task and 30904 for our multi-task\\ntraining set). We also train our model with alpha C atom (Hemenet w/ C \u03b1) as input, resulting in a\\nbest PPI RMSE of 0.861. The performance on LBA is also improved by 0.182 for RMSE. Moreover,\\nmulti-task learning also reduces the standard deviation of the regression error (see Table 6). These\\nresults demonstrate that the model can handle challenging tasks (complex-level affinity prediction)\\n better when more structural information is available (e.g., single-chain structures and their labels).\\n\\nOur model performs harmonious multi-task training on property prediction tasks. We ob-\\nserve that when switching from the single-task to multi-task setting, baseline models experience\\nperformance degradation to some extent across the four property prediction tasks. This is a common\\nphenomenon in multi-task learning due to task interference among diverse tasks, and combining\\ndifferent tasks for training without careful adaption can harm performance. With the guidance of\\nour task-aware readout module, our model is able to learn from multiple tasks in a task-harmonious\"}"}
{"id": "K3tHTPjFBM", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: The mean result for three runs on the full-label test set. We select representative invariant and equivariant models for affinity prediction and property prediction. The upper half reports the results for the single-task setting, and the lower half reports the results for the multi-task setting. The best results are marked in bold and the second best results are underlined. In the multi-task setting, we train the models with the same size compared to their corresponding single-task models.\\n\\n| Method          | LBA   | PPI   | EC   | GOD  | RMSE | MAE  | RMSE | MAE  |\\n|-----------------|-------|-------|------|------|------|------|------|------|\\n| Single-task     |       |       |      |      |      |      |      |      |\\n| GCN (Kipf & Welling, 2017) | 2.193 | 1.721 | 7.840 | 7.738 | 0.022 | 0.207 | 0.254 | 0.367 |\\n| GAT (Veli\u0107ovi\u0107 et al., 2018) | 2.301 | 1.838 | 7.820 | 7.720 | 0.018 | 0.223 | 0.249 | 0.354 |\\n| SchNet (Sch\u00fctt et al., 2017) | 2.162 | 1.692 | 7.839 | 7.729 | 0.097 | 0.311 | 0.281 | 0.431 |\\n| GearNet (Zhang et al., 2023) | 1.957 | 1.542 | 2.004* | 1.279* | 0.716 | 0.677 | 0.252 | 0.438 |\\n| EGNN (Satorras et al., 2021) | 2.282 | 1.849 | 4.854 | 4.756 | 0.039 | 0.206 | 0.253 | 0.357 |\\n| GVP (Jing et al., 2021) | 2.281 | 1.789 | 5.280 | 5.267 | 0.020 | 0.204 | 0.244 | 0.454 |\\n| dyMEAN (Kong et al., 2023) | 2.410 | 1.987 | 7.309 | 7.182 | 0.115 | 0.436 | 0.292 | 0.477 |\\n| Hemenet (Ours) | 1.912 | 1.490 | 6.031 | 5.891 | 0.863 | 0.778 | 0.404 | 0.544 |\\n\\n| Multi-task      |       |       |      |      |      |      |      |      |\\n| SchNet (Sch\u00fctt et al., 2017) | 1.763 | 1.447 | 1.216 | 1.120 | 0.093 | 0.192 | 0.264 | 0.402 |\\n| GearNet (Zhang et al., 2023) | 2.193 | 1.863 | 1.275 | 1.035 | 0.187 | 0.203 | 0.261 | 0.379 |\\n| EGNN (Satorras et al., 2021) | 1.777 | 1.441 | 0.999 | 0.821 | 0.048 | 0.169 | 0.244 | 0.352 |\\n| GVP (Jing et al., 2021) | 1.870 | 1.572 | 0.906 | 0.758 | 0.018 | 0.168 | 0.246 | 0.360 |\\n| dyMEAN (Kong et al., 2023) | 1.777 | 1.446 | 1.725 | 1.523 | 0.038 | 0.164 | 0.263 | 0.449 |\\n| Hemenet w/ $C_\\\\alpha$ (Ours) | 1.799 | 1.420 | 0.861 | 0.719 | 0.630 | 0.595 | 0.279 | 0.426 |\\n| Hemenet (Ours) | 1.730 | 1.335 | 1.087 | 0.912 | 0.810 | 0.727 | 0.379 | 0.436 |\\n\\n* In the single-task setting, only GearNet and is trained under the alpha-Carbon atom only setting and it significantly outperforms other baselines on PPI. We suppose this is because the training set for PPI is quite small, and there is too much noise when considering the full atoms of the whole protein. The alpha-only input simplifies single-task PPI learning.\"}"}
{"id": "K3tHTPjFBM", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Comparison of different readout functions for multi-task learning. $s$, $w$, and $t$ represent sum, weighted feature and task-aware readout, respectively.\\n\\nTable 3: Ablation study for different components in HeMeNet.\\n\\nWe present the results for ablation studies in Table 3, the observations are as follows: 1) Without our task-aware readout strategy, significant performance degradation are observed in all tasks, indicating that the tasks can hinder each other without appropriate guidance. 2) Without the heterogeneous graph representation and the corresponding relational message passing, our model's performance drops on property prediction tasks, especially the Enzyme Commission number prediction. 3) Removing the full-atom geometry decreases the performance in multiple tasks. However, it improves our model's performance in PPI. Similar to the explanation in Section 5.2, we suppose that the large number of atoms in the full-atom protein-protein complex introduces excessive noise to prediction compared with input with alpha-Carbon atoms only.\\n\\nConclusion\\n\\nLimitations\\n\\nIt is hard to add more tasks into PROMPT while maintaining the fully-labeled sample size under our definition of fully-labeled data. Relaxing the restriction of the test set can alleviate this issue. Meanwhile, the input is now a mixture of single chains and complexes, we can randomly augment chains from their original PDB complex to form 'complexes' and label them based on their UniProt IDs. Besides, we only consider invariant tasks in this work, we can also extend our model to more tasks in future work (e.g. equivariant tasks).\\n\\nIn this paper, we alleviate the problem of sparse data in structured protein datasets by a multi-task setting. We construct a standard multi-task benchmark PROMPT, consisting of 6 representative tasks integrated from 4 public datasets for joint learning. We propose a novel network called HeMeNet to address multiple tasks in protein 3D learning, with a novel heterogeneous equivariant full-atom encoder and a task-aware readout module. Comprehensive experiments demonstrate our model's performance on the affinity and property prediction tasks. Our work brings insights for utilizing different structural datasets to train a more powerful generalist model in future research.\"}"}
{"id": "K3tHTPjFBM", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"REPRODUCIBILITY\\nWe provide detailed information about our dataset PROMPT in Appendix A, and we will make our\\n\\nREFERENCES\\nEthan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, George M Church, and\\nGeorge M Church. Unified rational protein engineering with sequence-based deep representation\\nlearning. Nature methods, 16(12):1315\u20141322, December 2019. ISSN 1548-7091. doi: 10.1038/\\ns41592-019-0598-1.\\nHelen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady N Bhat, Helge Weissig,\\nIlya N Shindyalov, and Philip E Bourne. The protein data bank. Nucleic acids research, 28(1):\\n235\u2013242, 2000.\\nJohannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, and Max Welling. Geo-\\nmetric and physical quantities improve e(3) equivariant message passing. In International Confer-\\nence on Learning Representations, 2022. URL https://openreview.net/forum?id=_xwr8gOBeV1.\\nMatthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In\\nICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.\\nOctavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi S.\\nJaakkola, and Andreas Krause. Independent SE(3)-equivariant models for end-to-end rigid protein\\ndocking. In International Conference on Learning Representations, 2022. URL https://openreview.\\next?id=GQjaI9mLet.\\nMatt W Gardner and SR Dorling. Artificial neural networks (the multilayer perceptron)\u2014a review\\nof applications in the atmospheric sciences. Atmospheric environment, 32(14-15):2627\u20132636,\\n1998.\\nJohannes Gasteiger, Janek Gro\u00df, and Stephan G\u00fcnnemann. Directional message passing for molec-\\nular graphs. In International Conference on Learning Representations, 2020. URL https://open-\\nreview.net/forum?id=B1eWbxStPH.\\nVladimir Gligorijevi\u0107, P. Douglas Renfrew, Tomasz Kosci\u00f3\u0142ek, Julia Koehler Leman, Daniel Beren-\\nberg, Tommi Vatanen, Chris Chandler, Bryn C. Taylor, Ian Fisk, Hera Vlamakis, Ramnik J.\\nXavier, Rob Knight, Kyunghyun Cho, and Richard Bonneau. Structure-based protein func-\\ntion prediction using graph convolutional networks. Nature Communications, 12, 2021a. doi:\\n10.1038/s41467-021-23303-9.\\nVladimir Gligorijevi\u0107, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Beren-\\nberg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, Ian M Fisk, Hera Vlamakis, et al. Structure-\\nbased protein function prediction using graph convolutional networks. Nature communications,\\n12(1):3168, 2021b.\\nPedro Hermosilla, Marco Sch\u00e4fer, Matej Lang, Gloria Fackelmann, Pere-Pau V\u00e1zquez, Barbora Ko-\\nzlikova, Michael Krone, Tobias Ritschel, and Timo Ropinski. Intrinsic-extrinsic convolution and\\npooling for learning on 3d protein structures. In International Conference on Learning Represen-\\ntations, 2021. URL https://openreview.net/forum?id=l0mSUROpwY.\\nWenbing Huang, Jiaqi Han, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Equivariant\\ngraph mechanics networks with constraints. In International Conference on Learning Representations,\\n2022. URL https://openreview.net/forum?id=SHbhHHfePhP.\\nBowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror.\\nLearning from protein structure with geometric vector perceptrons. In International Confer-\\nence on Learning Representations, 2021. URL https://openreview.net/forum?id=1YLJDvSx6J4.\"}"}
{"id": "K3tHTPjFBM", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We first extract the UniProt ID for each chain by querying PDB API and construct a UniProt-Property dictionary to map the UniProt ID of each protein chain with EC and GO-MF, GO-BP, GO-CC labels annotated in the EC and GO datasets. With this dictionary, we can extract the UniProt ID for each chain, search it in the dictionary and map the chain with its labels. The complex with one affinity label and all property labels for each chain is defined as fully-labeled. We take most of the fully-labeled data for val/test, a few of the fully-labeled and most of the partially labeled data for training.\\n\\nWe are the first to combine structure-based interaction prediction and property prediction in a multi-task setting. We construct a new dataset called PROtein MultiPle Tasks (PROMPT) for protein representation learning. PROMPT is composed of different types of tasks on 3D protein structures: the prediction of Ligand Binding Affinity (LBA) and Protein-Protein Interaction (PPI) based on two-instance complexes and the prediction of Enzyme Commission (EC) number and Gene Ontology (GO) terms based on single-chain structures. Particularly, the LBA and PPI tasks originated from the PDBbind database (Wang et al., 2004) aim at regressing the affinity value of a protein-ligand complex and protein-protein complex, respectively. The EC task is initially constructed by (Gligorijevi\u0107 et al., 2021b) to describe the catalysis of biochemical reactions, and it contains 19201 protein chains with 538 binary-class labels. The GO task consisting of 36641 annotated protein chains predicts the hierarchically-related functional properties of gene products (Gligorijevi\u0107 et al., 2021b): Molecular Function (MF), Biological Process (BP) and Cellular Component (CC). We treat the prediction of MF, BP and CC as three individual tasks, resulting in six different prediction tasks in total.\\n\\nOne key difficulty in integrating these tasks from their sourced datasets is that samples from one task may lack the labels for other tasks. It is crucial to obtain samples with a complete set of labels across tasks for the training and evaluation of multi-task learning methods. To address this, we propose a matching pipeline that enables us to transfer the labels between EC and GO, and assign EC and GO labels for the chains of complexes in LBA and PPI as well (it is impossible to conduct the inverse direction since it is meaningless to assign LBA or PPI affinity for those single chains in...\"}"}
{"id": "K3tHTPjFBM", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nEC and GO). Specifically, we utilize the UniProt ID to uniquely identify a protein chain. We first obtain the UniProt IDs of all protein chains in PROMPT from Protein Data Bank (Berman et al., 2000). For each UniProt ID, we then determine the EC and GO properties based on the labels of the corresponding chains in the EC and GO datasets, resulting in a UniProt-Property dictionary. With this dictionary, for a chain missing EC or GO labels (e.g., a chain of a complex in LBA and PPI), we can supplement the missing labels by searching the UniProt-Property dictionary by its UniProt ID to retrieve any known EC and GO labels. We define a complex (from either LBA or PPI) as fully-labeled if the complex has one affinity label (LBA or PPI) and four function labels for each of its chains. After our above matching process, we yield 1327 fully-labeled complexes.\\n\\nWe formulate the train/validation/test split in terms of the chain-level sequence identity, through the alignment methods commonly used in single-chain property prediction tasks (Gligorijevi\u0107 et al., 2021b). Specifically, we employ MMSeq2 (Steinegger & S\u00f6ding, 2017) to cluster all the chains in PROMPT with an alignment coverage >30% and sequence identity of the aligned fragment >90%, leading to 33704 chain-level clusters. Then, we merge the clusters that contain the chains belonging to the same complex and finally get 30034 clusters. For the fully-sampled complexes, we randomly split them into the training, validation and test sets, with the number of complexes as 328, 530, and 469, respectively. For the partially labeled samples, we only retain those samples located in clusters different from the above test complexes and add them into the training set, resulting in an augmented training set with a total of 31252 samples. The statistics of the PROMPT dataset are listed in Table 5 and more details are presented in Appendix A.\\n\\n**4 Method**\\n\\nIn this section, we first introduce our heterogeneous graph representation and the multi-task formulation in Section 4.1. Then, we design the architecture of the proposed HeMeNet in Section 4.2, which consists of two key components: heterogeneous multi-channel equivariant message passing and task-aware readout.\\n\\n### 4.1 Heterogeneous Graph Representation and Task Formulation\\n\\nThe input of our model is of various types. It could be either a two-instance complex (protein-ligand for LBA and protein-protein for PPI) or a single chain (for EC and GO). Here, for consistency, we unify these two different kinds of input as a graph \\\\( G \\\\) composed of two sets of nodes \\\\( V_r \\\\) and \\\\( V_l \\\\). For the LBA complex input, \\\\( V_r \\\\) and \\\\( V_l \\\\) denote the receptor and the ligand, respectively, while for the PPI complex and single-chain input, \\\\( V_r \\\\) refers to the input protein and \\\\( V_l \\\\) becomes an empty set.\\n\\nEach node in \\\\( V_r \\\\) and \\\\( V_l \\\\) corresponds to a residue of the input protein or an atom of the input small molecular. We associate each node \\\\( v \\\\) with the representation \\\\((h, \\\\vec{X})\\\\), where \\\\( h \\\\in \\\\mathbb{R}^d \\\\) denotes the node feature and it is initialized as a learnable residue embedding, \\\\( \\\\vec{X} \\\\in \\\\mathbb{R}^{3 \\\\times c} \\\\) indicates the 3D coordinates of all \\\\( c \\\\) atoms within the node. As for edge construction, we borrow the ideas from GearNet (Zhang et al., 2023). In detail, for residue nodes, we allow heterogeneous types of edge connections including sequential edges of different distances (\\\\( d = \\\\{-2, -1, 1, 2\\\\} \\\\)), self-loop edges, and knn edges; for single-atom nodes, only knn edges are created. Note that different from GearNet, we expand the graph to the full-atom setting by involving all atom coordinates as input, rather than only using alpha-Carbon atoms. We present an example from the LBA task in Figure 2, where we only draw a few nodes and omit the self-loop edges except for the central node for simplicity. Overall, we obtain a full-atom heterogeneous graph representation \\\\( G \\\\) for each input.\\n\\n### Task Formulation\\n\\nGiven a full-atom heterogeneous graph \\\\( G \\\\), our goal is to design a model \\\\( p = f(G) \\\\) with multiple-dimensional output \\\\( p \\\\) that is able to predict the complex-level affinity and chain-level functional properties simultaneously. By making use of our proposed dataset PROMPT, we train the model with a partially labeled training set (containing three possible kinds of input: protein-protein graph, protein-ligand graph and single-chain graph) and test it on the fully-labeled test set (only containing protein-protein graph or protein-ligand graph). Notably, the prediction should be invariant with regard to any E(3) transformation (rotation/reflection/translation) of the input.\"}"}
{"id": "K3tHTPjFBM", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Heterogeneous Graph and Relational Equivariant Message Passing\\n\\nTask Aware Readout\\n\\nSequential Edge ($d=1$)\\nSequential Edge ($d=2$)\\nKNN Edge\\n\\n\\\\[ m_{ijr} = \\\\phi_m(h_{l}i, h_{l}j, T_R(\\\\vec{X}_{l}i, \\\\vec{X}_{l}j)) \\\\]\\n\\n\\\\[ \\\\vec{M}_{ijr} = T_S(\\\\vec{X}_{l}i - e_r, k=1, c_{j}(\\\\vec{X}_{l}j(:), k)), \\\\phi_x(m_{ijr}) \\\\]\\n\\nwhere, \\\\( m_{ijr} \\\\) and \\\\( \\\\vec{M}_{ijr} \\\\) are separately the invariant and equivariant messages from node \\\\( j \\\\) to \\\\( i \\\\) along the \\\\( r \\\\)-th edge; \\\\( e_r \\\\) is the edge embedding feature; \\\\( \\\\phi_m, \\\\phi_x \\\\) are Multi-Layer Perceptrons (MLPs) (Gardner & Dorling, 1998) with one hidden layer; \\\\( \\\\| \\\\cdot \\\\|_F \\\\) computes the Frobenius norm, \\\\( T_R \\\\) and \\\\( T_S \\\\) are the adaptive multichannel geometric relation extractor and geometric message scaler proposed in dyMEAN (Kong et al., 2023), in order to deal with the issue incurred by the varying shape of \\\\( \\\\vec{X}_{l}i \\\\) and \\\\( \\\\vec{X}_{l}j \\\\) since the number of atoms could be different for different nodes. With the calculated...\"}"}
{"id": "K3tHTPjFBM", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"messages, the node representation is updated by:\\n\\n$$h_{l+1}^i = h_l^i + \\\\sigma(BN(\\\\varphi_h(X_r \\\\in \\\\mathbb{R}^W_r X_j \\\\in N_r(i) m_{ijr})))$$\\n\\n$$\\\\vec{X}_{l+1}^i = \\\\vec{X}_l^i + \\\\text{Pr} \\\\in \\\\mathbb{R}^{|N_r(i)|} X_r \\\\in \\\\mathbb{R} X_j \\\\in N_r(i) w_{jr} \\\\vec{M}_{ijr},$$\\n\\nwhere, $W_r, w_{jr}$ are a learnable matrix and a learnable scalar, respectively, for the $r$-th kind of edge; $N_r(i)$ denotes the neighbor nodes of $i$ regarding the $r$-th kind of edges; $\\\\varphi_h$ is an MLP, $\\\\text{BN}$ is the batch normalization operation, and $\\\\sigma$ is an activation function. During the message-passing process, our model gathers information from different relations for $h_i$ and $\\\\vec{X}_i$, ensuring the $E(3)$ equivariance. For further details of the components in our model, please refer to Appendix B.\\n\\n### Task-Aware Readout\\n\\nAfter $L$ layers of the above relational message passing, we attain a set of $E(3)$ invariant node features $H(L) \\\\in \\\\mathbb{R}^{n \\\\times d_L}$, where $n$ is the number of nodes and $d_L$ is the feature dimension. To correlate the task-specific information with each node feature, we propose a task-aware readout function. We compute the attention weights between each node and each task-specific query, and then readout all weighted node features into a graph-level or chain-level representation for each task. As shown in Figure 2, the task-aware readout module is formulated as\\n\\n$$\\\\alpha_t = \\\\text{Softmax}(KQ_t \\\\sqrt{d_L})$$\\n\\n$$f_t = \\\\text{FFN}(\\\\alpha_t V + \\\\text{Linear}(Q_t)),$$\\n\\nwhere, $\\\\alpha_t \\\\in [0, 1]^n$ defines the attention values for task $t$; $Q_t \\\\in \\\\mathbb{R}^{d_L}$ is the learnable query for task $t$; $K = HW_K \\\\in \\\\mathbb{R}^{n \\\\times d_L}$ and $V = HW_V \\\\in \\\\mathbb{R}^{n \\\\times d_L}$ are the key and value matrices, respectively; $\\\\text{FFN}$ is the feed-forward network containing layer normalization and linear layers; $\\\\text{Linear}(Q_t) = W_Q Q_t + b$ is used before the shortcut addition. In our implementation, we apply the multi-head attention strategy by defining multiple query vectors for each task. Although we compute the attention by only using the invariant features $H(L)$, it indeed has involved the geometric information from the 3D coordinates during the previous $L$-layer message passing.\\n\\n### Multiple Task Heads\\n\\nWe feed the above task-specific feature $f_i$ into different task heads implemented by MLPs, resulting in a prediction list $(p_{lba}, p_{ppi}, p_{ec}, p_{mf}, p_{bp}, p_{cc})$. For regression tasks (including LBA and PPI), we use the Mean Square Error (MSE) loss $L_{MSE}$. For classification tasks (including EC, GO-MF, GO-BP and GO-CC), we use the Binary Cross Entropy (BCE) loss $L_{BCE}$.\\n\\nThe training loss is formulated as:\\n\\n$$L = L_{\\\\text{reg}} + \\\\lambda L_{\\\\text{cls}},$$\\n\\nwhere $L_{\\\\text{reg}} = \\\\frac{1}{lba} L_{MSE}(p_{lba}) + \\\\frac{1}{ppi} L_{MSE}(p_{ppi})$, $L_{\\\\text{cls}} = \\\\frac{1}{ec} L_{BCE}(p_{ec}) + \\\\frac{1}{mf} L_{BCE}(p_{mf}) + \\\\frac{1}{bp} L_{BCE}(p_{bp}) + \\\\frac{1}{cc} L_{BCE}(p_{cc})$, $\\\\lambda$ is a hyper-parameter to balance the trade-off of the losses. To allow training on the partially labeled sample, if the label of the task $*$ exists, then $\\\\frac{1}{*} = 1$, otherwise $\\\\frac{1}{*} = 0$. In addition, we adopt a balanced sampling strategy to ensure that each sampled mini-batch should contain at least one sample labeled for each task, which is shown to further accelerate the training convergence in our experiments.\\n\\n## 5 EXPERIMENTS\\n\\nIn this section, we will first introduce the experimental setup in Section 5.1. In Section 5.2, we evaluate our model on the proposed challenging dataset PROMPT for affinity and property prediction in both single-task and multi-task settings and compare it with other baseline models. In Section 5.3, we experiment with different readout strategies and compare their performance on property prediction tasks. In Section 5.4, we perform ablation experiments on different modules.\"}"}
{"id": "K3tHTPjFBM", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: The detailed results for three runs on the full-label test set. We select representative invariant and equivariant models for affinity prediction and property prediction. The upper half reports the results for the single-task setting, and the lower half reports the results for the multi-task setting. The best results are marked in bold and the second best results are underlined. In the multi-task setting, we train the models with the same size compared to their corresponding single-task models.\\n\\n| Method       | LBA | PPI | EC   |\\n|--------------|-----|-----|------|\\n|              | RMSE\u2193 | MAE\u2193 | RMSE\u2193 | MAE\u2193 | RMSE\u2193 | MAE\u2193 |\\n| Single-task  |      |     |      |      |      |      |\\n| GCN (Kipf & Welling, 2017) | 2.193 \u00b1 0.004 | 1.721 \u00b1 0.005 | 7.840 \u00b1 0.044 | 7.738 \u00b1 0.044 | 0.022 \u00b1 0.017 | 0.207 \u00b1 0.002 |\\n| GAT (Veli\u010dkovi\u0107 et al., 2018) | 2.301 \u00b1 0.004 | 1.838 \u00b1 0.013 | 7.820 \u00b1 0.116 | 7.720 \u00b1 0.112 | 0.018 \u00b1 0.001 | 0.223 \u00b1 0.009 |\\n| SchNet (Sch\u00fctt et al., 2017)   | 2.162 \u00b1 0.016 | 1.692 \u00b1 0.030 | 7.839 \u00b1 0.057 | 7.729 \u00b1 0.056 | 0.097 \u00b1 0.012 | 0.311 \u00b1 0.021 |\\n| Gearnet (Zhang et al., 2023)   | 1.957 \u00b1 0.044 | 1.542 \u00b1 0.055 | 2.004 \u00b1 0.213 | 1.279 \u00b1 0.290 | 0.716 \u00b1 0.026 | 0.677 \u00b1 0.026 |\\n| EGNN (Satorras et al., 2021)   | 2.282 \u00b1 0.082 | 1.849 \u00b1 0.083 | 4.854 \u00b1 0.679 | 4.756 \u00b1 0.672 | 0.039 \u00b1 0.025 | 0.206 \u00b1 0.011 |\\n| GVP (Jing et al., 2021)        | 2.281 \u00b1 0.005 | 1.789 \u00b1 0.007 | 5.280 \u00b1 0.041 | 5.267 \u00b1 0.040 | 0.020 \u00b1 0.006 | 0.204 \u00b1 0.010 |\\n| dyMEAN (Kong et al., 2023)     | 2.410 \u00b1 0.184 | 1.987 \u00b1 0.181 | 7.309 \u00b1 0.540 | 7.182 \u00b1 0.543 | 0.115 \u00b1 0.034 | 0.436 \u00b1 0.056 |\\n| Hemenet (Ours)                 | 1.912 \u00b1 0.420 | 1.490 \u00b1 0.050 | 6.031 \u00b1 0.380 | 5.891 \u00b1 0.380 | 0.863 \u00b1 0.016 | 0.778 \u00b1 0.003 |\\n| Multi-task                |      |     |      |      |      |      |\\n| SchNet (Sch\u00fctt et al., 2017)   | 1.763 \u00b1 0.019 | 1.447 \u00b1 0.015 | 1.216 \u00b1 0.033 | 1.120 \u00b1 0.037 | 0.093 \u00b1 0.008 | 0.192 \u00b1 0.013 |\\n| Gearnet (Zhang et al., 2023)   | 2.193 \u00b1 0.373 | 1.863 \u00b1 0.364 | 1.275 \u00b1 0.471 | 1.035 \u00b1 0.197 | 0.187 \u00b1 0.128 | 0.203 \u00b1 0.107 |\\n| EGNN (Satorras et al., 2021)   | 1.777 \u00b1 0.029 | 1.441 \u00b1 0.046 | 0.999 \u00b1 0.054 | 0.821 \u00b1 0.057 | 0.048 \u00b1 0.057 | 0.169 \u00b1 0.020 |\\n| GVP (Jing et al., 2021)        | 1.870 \u00b1 0.309 | 1.572 \u00b1 0.034 | 0.906 \u00b1 0.329 | 0.758 \u00b1 0.015 | 0.018 \u00b1 0.001 | 0.168 \u00b1 0.003 |\\n| dyMEAN (Kong et al., 2023)     | 1.777 \u00b1 0.075 | 1.446 \u00b1 0.103 | 1.725 \u00b1 0.362 | 1.523 \u00b1 0.383 | 0.038 \u00b1 0.014 | 0.164 \u00b1 0.014 |\\n| Hemenet (Ours)                 | 1.799 \u00b1 0.019 | 1.420 \u00b1 0.040 | 0.861 \u00b1 0.051 | 0.719 \u00b1 0.068 | 0.630 \u00b1 0.043 | 0.595 \u00b1 0.037 |\\n| Hemenet w/ C\u03b1 (Ours)           | 1.730 \u00b1 0.030 | 1.335 \u00b1 0.031 | 1.087 \u00b1 0.224 | 0.912 \u00b1 0.204 | 0.810 \u00b1 0.083 | 0.727 \u00b1 0.084 |\"}"}
{"id": "K3tHTPjFBM", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We show the standard deviation of three independent repeated results on PROMPT in Table 6. As represented in the table, the standard deviation of LBA and PPI tasks in the multi-task setting decreases significantly than in the single-task setting.\"}"}
