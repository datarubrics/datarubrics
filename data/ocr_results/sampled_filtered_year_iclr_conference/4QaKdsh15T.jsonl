{"id": "4QaKdsh15T", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 22: Additional qualitative results on ScanQA validation set.\\n\\n| Scene                                      | Question                                                                 | Response         | Ground Truth                                                                 | Response Type |\\n|--------------------------------------------|--------------------------------------------------------------------------|------------------|----------------------------------------------------------------------------|---------------|\\n| What is placed on the black chair out of the three chairs present in the room? | pillow                                                                   | tan jacket, guitar box like object | Wrong                                                      |               |\\n| What is next to the organizer shelf?       | mini fridge                                                              | coat rack         | Wrong but reasonable                                                       |               |\\n| Where is the dresser located?              | to right of door                                                         | underneath television next to black guitar case and green guitar case, underneath tv set next to guitar cases | Wrong but accurate                                                       |               |\\n| What kind of desk is the electric piano on? | wooden desk                                                              | wooden desk       | Correct                                                                    |               |\\n| How many monitor sits on top of the desk?  | 2                                                                       | 2                | Correct                                                                   |               |\\n| What is on the left side of the brown desk? | black computer monitor                                                   | monitor / computer monitor | Correct and more accurate                                                   |               |\\n\\n- Correct and more accurate. The response is more accurate than the ground truth annotations.\\n\\nI.4 SQA3D\\n\\nWe provide additional qualitative results on SQA3D test set in Tab. 23 and follow the aforementioned response types. The embodied reasoning in SQA3D requires the understanding of not only the scene but also the situation of embodiment. In Tab. 23, answering \u201cWhat am I sitting at?\u201d necessitates that LEO accurately identifies the objects at its current location. And the response to \u201cHow many beds are in front of me?\u201d indicates that LEO can reason based on the understanding of its orientation.\"}"}
{"id": "4QaKdsh15T", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Scene | Question | Response | Ground Truth | Response Type |\\n|-------|----------|----------|--------------|---------------|\\n|       | What is the only orange object on my left side? | backpack | jacket | Wrong |\\n|       | Is there anything on the desk? | yes | laptop | Wrong but accurate |\\n|       | What type of bed is the one on the right? | loft bed | loft bed | Correct |\\n|       | What am I sitting at? | desk | desk | Correct |\\n|       | How many beds are in front of me? | one | one | Correct |\"}"}
{"id": "4QaKdsh15T", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 24: Examples from our datasets. We provide the tasks, 2D images (optional), 3D point clouds, and instructions as the input of LEO, as well as the expected responses of LEO.\\n\\n| Dataset & Task   | 2D & 3D input | Instruction                                      | Response                                                                 |\\n|------------------|---------------|-------------------------------------------------|--------------------------------------------------------------------------|\\n| LEO-align        |               | USER: Explain the object at the pinpointed location in the 3D setting. | ASSISTANT: a yellow metal cage with a fence around it.                   |\\n|                  |               | USER: Give me a snapshot of the object at the chosen 3D location.       | ASSISTANT: find the office chair that is near the copier                  |\\n| LEO-align        |               | USER: Relay a brief summary of this scene's elements.                    | ASSISTANT: In this scene, there is a gray flat picture hanging on the wall, standing in front of a windowsill. There is also a rectangular white padded pillow lying on the bed, close to a big rectangular white padded pillow. The room has a white architectural wall and a flat overhead ceiling. The bed is tidy and white, located in front of a narrow tall commode. The curtains are hanging on the wall. The room has a calm and comfortable atmosphere, suitable for relaxation and sleep. |\"}"}
{"id": "4QaKdsh15T", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nTable 25: Examples from our datasets. Continued.\\n\\nLEO-instruct 3D captioning\\n\\nUSER: What does the object at the specified position in the 3D visualization look like?\\n\\nASSISTANT: there is a door in the middle of the northern wall next to a metal cabinet. the door is shaped like a brown rectangle and as a dark yellow colored door-knob.\\n\\nLEO-instruct 3D QA\\n\\nUSER: What is in the right corner of room by curtains?\\n\\nASSISTANT: brown cabinet with tv sitting in it\\n\\nLEO-instruct 3D dialogue\\n\\nUSER: I'm redecoring my living room. Can you give me some suggestions on how to arrange the furniture?\\n\\nASSISTANT: Of course! I'd be happy to help. What furniture do you have in your living room?\\n\\nUSER: I have a sofa, a table, a chair, and a couch table. \\n\\nASSISTANT: Great! Here's a suggestion: you can place the sofa against the wall, with the table in front of it. The chair can be placed to the right of the table, and the couch table can be placed to the right of the chair. This arrangement creates a cozy seating area.\\n\\nLEO-instruct task planning\\n\\nUSER: Develop a plan for this: Arrange and embellish the family room.\\n\\nASSISTANT: 1. Arrange and fluff the cushions and pillows on the armchair.\\n2. Place decorations, such as figurines or vases, on the cabinet or shelf.\\n3. Place hygiene products, such as wipes or lotions, on the commode.\\n4. Turn on the lights and adjust their brightness if needed.\\n5. Arrange diapers in a designated storage area.\\n6. Place a blanket or pillow on the armchair for added comfort.\\n7. Adjust the festoon or other decorative elements if desired.\"}"}
{"id": "4QaKdsh15T", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 16: Hyperparameters for the alignment stage.\\n\\n| Hyperparameter | Value |\\n|----------------|-------|\\n| Optimizer      | AdamW |\\n| Weight Decay   | 0.05  |\\n| betas          | [0.9, 0.999] |\\n| Learning Rate  | $3 \\\\times 10^{-4}$ |\\n| Warmup Steps   | 400   |\\n| Number of Workers | 4      |\\n| Parallel Strategy | DDP    |\\n| Type of GPUs   | NVIDIA A100 |\\n| Number of GPUs | 4     |\\n| Accumulate Gradient Batches | 5     |\\n| Batch Size/GPU (total) | 4 (80) |\\n| Training Precision | bfloat16 |\\n| gradient norm  | 5.0   |\\n| epochs         | 10    |\\n\\nThe hyperparameters for 3D VLA instruction tuning are presented in Tab. 17.\\n\\nTable 17: Hyperparameters for the instruction-tuning stage.\\n\\n| Hyperparameter | Value |\\n|----------------|-------|\\n| Optimizer      | AdamW |\\n| Weight Decay   | 0.05  |\\n| betas          | [0.9, 0.999] |\\n| Learning Rate  | $3 \\\\times 10^{-5}$ |\\n| Warmup Steps   | 400   |\\n| Number of Workers | 4      |\\n| Parallel Strategy | DDP    |\\n| Type of GPUs   | NVIDIA A100 |\\n| Number of GPUs | 4     |\\n| Accumulate Gradient Batches | 5     |\\n| Batch Size/GPU (total) | 4 (80) |\\n| Training Precision | bfloat16 |\\n| gradient norm  | 5.0   |\\n| epochs         | 10    |\\n\\nF I N D I N G \u2014 T U N I N G \u2014 S E T U P\\n\\nData ablation. We present the numerical results in Tab. 18 as complements to Fig. 4(a).\\n\\nModel ablation. We also make some explorations in model ablation and simply present qualitative findings here. For the point cloud encoder, we choose Point-BERT (Yu et al., 2022) as an alternative to the default PointNet++ (Qi et al., 2017). We utilize the checkpoint from PointLLM (Xu et al., 2023), which has adapted Point-BERT to 6-channel (XYZRGB) input and learned a language-aligned representation for 3D objects. Despite larger capacity, Point-BERT shows significantly worse performances than PointNet++ as the point cloud encoder. Similarly, for the Spatial Transformer and LLM, we ablate with different model scales but find no improvement. The influence of different modules remains an interesting question that deserves further exploration.\\n\\nG A B L A T I O N \u2014 D E T A I L S\\n\\nH E V A L U A T I O N \u2014 D E T A I L S\\n\\nH.1 3D Q U E S T I O N \u2014 A N S W E R I N G\\n\\nRationality of QA evaluation protocol. We argue that exact match (EM), as a conventional metric for 3D QA, is unsuitable for evaluating the open-ended answer generated by LLMs. For example,\"}"}
{"id": "4QaKdsh15T", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 18: Evaluation results of LEO with different data configurations for training. We test models on the validation set of Scan2Cap and ScanQA, and the held-out test sets of other datasets. We report exact match scores for QA tasks and SentenceSim for all other tasks. * indicates datasets that belong to LEO-align. Figures in gray indicate zero-shot transfer.\\n\\n| Data Configuration | Cap3D | SceneCap | Scan2Cap | ScanQA | SQA3D | 3RQA | 3RPlan | 3RDialog |\\n|--------------------|-------|----------|----------|--------|-------|------|--------|--------|\\n| NoAlign            | 20.5  | 23.6     | 52.9     | 35.3   | 50.9  | 46.8 | 69.2   | 76.4   |\\n| PartialData        | 23.0  | 61.5     | 62.2     | 30.2   | 45.4  | 47.3 | 69.3   | 76.4   |\\n| ScanNetOnly        | 34.5  |          |          |        |       |      |        |        |\\n| NoAct (VL)         | 35.9  |          |          |        |       |      |        |        |\\n| Full (VLA)         | 12.9  |          |          |        |       |      |        |        |\\n\\nTable 19: Examples from ScanQA validation set, showing the rationality of our refined exact match protocol.\\n\\n| Question                           | Ground-truth answer | Generated answer | Strict EM | Refined EM |\\n|------------------------------------|---------------------|------------------|-----------|------------|\\n| What color is the chair in the kitchen? | dark brown         | brown            | \u2717         | \u2713          |\\n| What is under the long kitchen counter? | kitchen cabinets   | brown rectangular kitchen cabinets | \u2717         | \u2713          |\\n| What type of refrigerator is on the right of a kitchen counter? | stainless steel refrigerator | stainless steel | \u2717         | \u2713          |\\n| Where is the beige wooden desk placed? | up against wall    | against wall     | \u2717         | \u2713          |\\n| What color does the sofa look?      | it looks black      | black            | \u2717         | \u2713          |\\n| Where is the black office chair located? | in front of desks  | in front of desk | \u2717         | \u2713          |\\n| What is in the corner by windows?   | book shelf          | bookshelf        | \u2717         | \u2713          |\\n| Where is the chair pulled into?     | table               | under table      | \u2717         | \u2713          |\\n| How many chairs are to the left of the table? | 4 chairs           | 4 chairs         | \u2717         | \u2713          |\\n| What objects are sitting on the black couch? | pillow             | pillows          | \u2717         | \u2713          |\\n| Where are the two different size tables located in room? | in center          | in center of room | \u2717         | \u2713          |\\n| Where is the laptop located?        | desk                | on desk          | \u2717         | \u2713          |\\n| Where is the soap dispenser mounted above sink on wall above sink? | bookshelf          | bookshelf        | \u2717         | \u2713          |\\n\\nGiven the question \\\"On what side of the towel is a bathroom curtain?\\\" with ground-truth answer \\\"left side of towel\\\", it is never wrong to answer \\\"left\\\". However, this will be deemed incorrect if we adopt the strict exact match protocol. By contrast, the classifier heads for QA (e.g., MCAN) are less affected because they collect all possible answers in advance to formulate the QA as a close-set classification problem. Hence, we refine the strict exact match protocol as follows.\\n\\n```python\\ndef strict_em(pred, gts):\\n    for gt in gts:\\n        if pred == gt:\\n            return True\\n    return False\\n\\ndef refined_em(pred, gts):\\n    for gt in gts:\\n        if pred == gt:\\n            return True\\n        elif ''.join(pred.split()) in ''.join(gt.split()):\\n            return True\\n        elif ''.join(gt.split()) in ''.join(pred.split()):\\n            return True\\n    return False\\n```\\n\\nIn a nutshell, we squeeze the pred and gt, and then check whether one is a subset of the other. To justify our refined exact match protocol, in Tab. 19 we provide some representative examples in the ScanQA validation set. Despite the improvements, we speculate such a simple refinement is still insufficient for a sound evaluation metric considering the flexibility of human language.\"}"}
{"id": "4QaKdsh15T", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To construct our training set, we adopt all 57 scenes in the MP3D ObjNav training split (Savva et al., 2019; Ramrakhya et al., 2022) and generate ~60K shortest-path navigation episodes. The evaluation is conducted on the original validation split of the MP3D ObjNav task and the newly introduced HM3D ObjNav task (Ramakrishnan et al., 2021).\\n\\nIn contrast to most ObjNav agents that utilize recurrence through either RNN (Ramrakhya et al., 2022) or DT-style Transformer (Suglia et al., 2021), LEO only employs a simplistic feed-forward policy, i.e., the Transformer in LEO only takes in the instruction, current state (2D and 3D observation), and past 4 actions, and predicts the next action, similar to RT-2 (Brohan et al., 2023). Therefore, the only information relayed from the past is about past actions. The absence of recurrence in LEO's acting policy is indeed the result of a trade-off between better performances and training efficiency. We will commit to exploring the possibility of looping in more sophisticated policy architectures (e.g., recurrence) in future work.\\n\\n### Additional Results\\n\\nI.1 Embodied Acting\\n\\nQuantitative results of ObjNav. We provide additional results of LEO 1) generalizing to unseen objects on MP3D, and 2) learning with 70K human demonstrations provided by Habitat-web (Ramrakhya et al., 2022) instead of shortest path. Below is a list of the objects used during training (seen) and for OOD evaluation (unseen). Evaluation results are shown in Tab. 20. Note that the baseline Habitat-web is unable to generalize to novel objects as it uses categorical embedding rather than natural language to represent object goals.\\n\\n| Objects (seen)                                | Objects (unseen) |\\n|-----------------------------------------------|------------------|\\n| \\\"gym_equipment\\\", \\\"tv_monitor\\\", \\\"picture\\\", \\\"counter\\\", \\\"chair\\\", \\\"cabinet\\\", \\\"table\\\", \\\"stool\\\", \\\"plant\\\", \\\"towel\\\", \\\"sofa\\\", \\\"cushion\\\", \\\"sink\\\", \\\"fireplace\\\", \\\"toilet\\\", \\\"seating\\\", \\\"chest_of_drawers\\\", \\\"bed\\\", \\\"shower\\\", \\\"bathtub\\\", \\\"clothes\\\" | \\\"shelf\\\", \\\"pillow\\\", \\\"lamp\\\", \\\"box\\\", \\\"desk\\\", \\\"refrigerator\\\", \\\"vase\\\", \\\"armchair\\\" |\\n\\nTable 20: Results on object navigation with OOD objects and human demonstrations. Note that the baseline Habitat-web is unable to generalize to MP3D-unseen as it uses categorical embedding rather than natural language to represent object goals.\\n\\n| MP3D-seen     | MP3D-unseen     | Habitat-web (shortest) | Habitat-web (70k demo) | LEO (shortest) | LEO (70k demo) |\\n|---------------|-----------------|-------------------------|-------------------------|----------------|----------------|\\n| Success \u2191     | SPL \u2191           | Success \u2191               | SPL \u2191                   | Success \u2191     | SPL \u2191           |\\n| 4.4           | 2.2             | 35.4                    | 10.2                    | 23.1           | 15.2           |\\n\\nThe results show that LEO can generalize to novel objects. On the other hand, human demonstrations include more explorations, compared with shortest-path data. Therefore, it will be much harder for agents without a recurrent module (e.g., LEO) to learn from human demonstrations (see Appendix H.2), leading to significantly weaker performances.\\n\\nQualitative results. We provide more qualitative results of robotic manipulation and embodied navigation in the supplementary video.\"}"}
{"id": "4QaKdsh15T", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Additional qualitative results on Scan2Cap validation set\\n\\n| Scene                                                                 | Instruction                                                                 | Response                                                                                                                                  | Ground Truth                                                                 |\\n|----------------------------------------------------------------------|------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------|\\n| Develop a description of the object at the specified position in the 3D backdrop. | This is a black keyboard. It is in front of two monitors.                    | The keyboard sits atop the workstation desk. It is located in front of the left-side monitor.                                           |                                                                            |\\n| Focus on the object at the given 3D location and describe it.        | This is a white pillow. It is on the couch.                                   | It is a small pillow located on the couch. You can notice it directly on your left when walking through the door into the room.           |                                                                            |\\n| Provide a clearer description of the object at that specific 3D spot.  | This is a white cabinet. It is to the right of a couch.                      | This is a white ottoman. It is located to the right of the black couch.                                                                    |                                                                            |\\n| What does the object at the specified position in the 3D visualization look like? | There is a rectangular brown door. It is next to a couch.                    | There is a door in the middle of the northern wall next to a metal cabinet. The door is shaped like a brown rectangle and as a dark yellow colored door-knob. |                                                                            |\\n\\n### I.2 Scan2Cap\\n\\nWe provide additional qualitative results on Scan2Cap validation set in Tab. 21. The results show that LEO can correctly refer to the queried object and provide accurate descriptions, including spatial relationships with other objects. However, LEO's responses are confined to simple formats that lack diversity. How to unlock more flexible responses while maintaining accuracy can be a direction for future research.\\n\\n### I.3 ScanQA\\n\\nWe provide additional qualitative results on ScanQA validation set in Tab. 22 and categorize the responses into several types:\\n\\n- **Wrong**: The response is inaccurate and deemed wrong.\\n- **Wrong but reasonable**: The response is deemed wrong but is reasonable to some extent, probably due to ambiguities in the scene. Consider the second case in Tab. 22. There are many objects such as a coat rack, a coat, and a mini fridge-shaped cabinet on the right side of the organizer. Though LEO's response \\\"mini fridge\\\" does not match the ground truth \\\"coat rack\\\", it is consistent with the 3D scene layout.\\n- **Wrong but accurate**: The response is accurate according to the scene but is deemed wrong due to imperfect ground truth annotations.\\n- **Correct**: The response is accurate and deemed correct.\"}"}
{"id": "4QaKdsh15T", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nFigure 16: Noun-verb pairs: 3RDialog instruction.\\n\\nFigure 17: Noun-verb pairs: 3RPlan instruction.\\n\\nFigure 18: Noun-verb pairs: 3RDialog response.\\n\\nFigure 19: Noun-verb pairs: 3RPlan response.\\n\\nYou are an AI visual assistant situated in a 3D scene. You can perceive (1) an ego-view image (accessible when necessary) and (2) the objects (including yourself) in the scene (always accessible). You should properly respond to the USER's instructions according to the given visual information.\\n\\nThe situation prompt begins with a common sentence: You are at a selected location in the 3D scene.\\n\\nFor SQA3D (Ma et al., 2023), the situation prompt is further extended with the situation description in the dataset. The situation prompt is only used jointly with the embodiment token to support tasks that require information about the embodiment. Details can be found in Appendix D.2.1.\\n\\nNext are the visual tokens, including 2D image tokens and object-centric 3D tokens. Each token sequence is interleaved within text tokens and starts with a text prefix.\\n\\nEgo-view image: {IMAGE_TOKENS}\\n\\nObjects (including you) in the scene: {OBJECT_TOKENS}\\n\\nThe last portion of prompts is a task-specific instruction. For object-level caption and object-in-the-scene caption, we randomly chose one sentence from 151 sentences to be the instruction.\"}"}
{"id": "4QaKdsh15T", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Some examples can be found in Tab. 12. For scene-level caption, we randomly choose one from 183 instructions. Examples can be found in Tab. 13. For 3D question answering task, we simply use the question as the instruction. The dialog history is used as the instruction for 3D dialogue to provide continuity across multiple rounds of interactions. A planning instruction pool consisting of 202 instructions is introduced for scene-aware task planning and we randomly choose one from it as done in the caption tasks. Examples from the pool can be found in Tab. 14. The chosen instruction is further followed by an instruction that specifies the task, e.g., set up a home office. With past action tokens \\\\{PAST\\\\}_ACTIONS appended at the end, the instruction for embodied navigation is as follows, where \\\\{GOAL\\\\} stands for the goal specified by the target object name:\\n\\nThe task is navigation. Your goal is to find \\\\{GOAL\\\\} by moving around in the scene. Past actions: \\\\{PAST\\\\}_ACTIONS.\\n\\nThe instruction for robotic manipulation is similar to the one in embodied navigation. Here \\\\{GOAL\\\\} is the task description in CLIPort:\\n\\nThe task is manipulation. Your goal is to \\\\{GOAL\\\\}. Past actions: \\\\{PAST\\\\}_ACTIONS.\\n\\nWe have several modules to encode the multi-modal features.\\n\\n- **Object-centric 3D token embedding.**\\n  \\n  The encoder for 3D object-centric point clouds is a PointNet++ (Qi et al., 2017) pre-trained on ScanNet (Dai et al., 2017) with object-classification task. We sample 1024 points for every object as in Chen et al. (2022). The architecture parameters all remain the same with Chen et al. (2022). We freeze the PointNet++ for empirically better results.\\n\\n- **Spatial Transformer (Chen et al., 2022).**\\n  \\n  Spatial Transformer is a modified transformer architecture that explicitly encodes spatial relations between object pairs. Specifically, consider the vanilla self-attention (Vaswani et al., 2017) mechanism which takes as input a feature matrix $X \\\\in \\\\mathbb{R}^{N \\\\times d}$, where $N$ stands for the number of tokens and $d$ is the feature dimension. Vanilla self-attention first computes $Q = XW_Q$, $K = XW_K$, $V = XW_V$ from $X$ using learnable projection matrices $W_Q, W_K, W_V \\\\in \\\\mathbb{R}^{d \\\\times d_H}$ where $d_H$ stands for the output feature dimension. Then the attention weight matrix is computed by $(\\\\omega_{ij})_{N \\\\times N} = \\\\Omega_o = \\\\text{softmax}(QK^T / \\\\sqrt{d_H})$ and finally used for re-weighting $\\\\Omega_o V$. The intuition of Spatial Transformer is that we can re-scale the elements $\\\\omega_{ij}$ in the weight matrix $\\\\Omega_o$. \\n\\n---\\n\\n**Table 12:** Examples from our object-level caption instruction set.\\n\\n- \u201cProduce a description for the object at the chosen spot in the 3D scene.\u201d\\n- \u201cHow would you depict the object located at the selected point in the 3D environment?\u201d\\n- \u201cFormulate a description of the item at the picked position within the 3D scene.\u201d\\n- \u201cHow would you describe the entity at the designated location in the 3D backdrop?\u201d\\n- \u201cCan you detail the object situated at the selected point in the 3D setting?\u201d\\n- \u201cCompose a narrative for the object at the chosen locale within the 3D environment.\u201d\\n- \u201cWhat does the object at the specified position in the 3D visualization look like?\u201d\\n- \u201cProvide a description for the item located at the marked site in the 3D world.\u201d\\n- \u201cHow would you illustrate the object placed at the selected spot in the 3D landscape?\u201d\\n- \u201cCraft a depiction of the object at the pinpointed location within the 3D territory.\u201d\\n- \u201cWhat kind of object is illustrated at the identified site in the 3D tableau?\u201d\\n- \u201cDevelop a description of the object at the specified position in the 3D backdrop.\u201d\\n- \u201cWhat is the entity\u2019s detail at the highlighted site in the 3D view?\u201d\\n- \u201cWrite up a description of the entity at the selected spot in the 3D realm.\u201d\\n- \u201cWhat does the object look like at the pinpointed location in the 3D space?\u201d\\n- \u201cDetail the entity located at the chosen position within the 3D scene.\u201d\\n- \u201cCan you explain the essence of the object at the selected spot in the 3D zone?\u201d\"}"}
{"id": "4QaKdsh15T", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 13: Examples from our scene-level caption instruction set.\\n\\n- \\\"Describe this scene.\\\"\\n- \\\"Generate a description of this scene.\\\"\\n- \\\"Generate a caption of this scene.\\\"\\n- \\\"Can you describe the scene?\\\"\\n- \\\"Can you generate a description of the scene?\\\"\\n- \\\"Can you generate a caption of the scene?\\\"\\n- \\\"Summarize this scene.\\\"\\n- \\\"Provide an outline of this 3D scene's characteristics.\\\"\\n- \\\"How would you describe the 3D scene?\\\"\\n- \\\"How would you summarize this scene?\\\"\\n- \\\"Convey a summary of the 3D structure of this scene.\\\"\\n- \\\"How would you interpret this 3D scene?\\\"\\n- \\\"Offer a summary of the 3D scene.\\\"\\n- \\\"Can you describe this scene in detail?\\\"\\n- \\\"I'm interested in this scene, can you explain?\\\"\\n- \\\"What is this scene made of?\\\"\\n- \\\"Could you provide more info about this scene?\\\"\\n\\nTable 14: Examples from our planning instruction pool.\\n\\n- \\\"Plan for the task\\\"\\n- \\\"Can you come up with a plan for this task\\\"\\n- \\\"How can we do this task, provide a step-by-step plan\\\"\\n- \\\"Draft a plan for completing this task\\\"\\n- \\\"Detail a strategy for the task\\\"\\n- \\\"What's the best plan for this task\\\"\\n- \\\"Draw out a procedure for the task\\\"\\n- \\\"Lay out the steps for this task\\\"\\n- \\\"Could you devise a plan for the task\\\"\\n- \\\"Show me a plan for this task\\\"\\n- \\\"I need a plan for the task\\\"\\n- \\\"Sketch a plan for the task at hand\\\"\\n- \\\"Set up a plan for this\\\"\\n- \\\"Recommend a plan for this task\\\"\\n- \\\"Offer a strategy for this task\\\"\\n- \\\"Design a blueprint for the task\\\"\\n- \\\"Outline the approach for this task\\\"\\n\\nIn the object-centric reasoning setting, the input feature matrix is \\\\( O \\\\in \\\\mathbb{R}^{N \\\\times d} \\\\). Consider an object pair \\\\((O_i, O_j)\\\\) with their geometric centers \\\\( c_i, c_j \\\\). Spatial Transformer (Chen et al., 2022) computes the Euclidean distance \\\\( d_{ij} = ||c_i - c_j||^2 \\\\) and the horizontal and vertical angles \\\\( \\\\theta_h, \\\\theta_v \\\\) of the line connecting \\\\( c_i \\\\) and \\\\( c_j \\\\). The spatial feature between the two objects \\\\((O_i, O_j)\\\\) is a 5-dimensional vector \\\\( f_{ij} = [d_{ij}, \\\\sin(\\\\theta_h), \\\\cos(\\\\theta_h), \\\\sin(\\\\theta_v), \\\\cos(\\\\theta_v)] \\\\). To combine this feature with objects, the spatial attention computes \\\\( \\\\omega_{sij} = g_i f_{ij} \\\\) where \\\\( g_i = W^T S o_i \\\\) is a 5-dimensional vector. The spatial attention further reweights the original self-attention weight matrix as \\\\( \\\\omega_{ij} = \\\\sigma(\\\\omega_{sij}) \\\\exp(\\\\omega_{oij}) \\\\). Readers are referred to Chen et al. (2022) for more details. In summary, Spatial Transformer explicitly computes pairwise spatial relations and fuses them with vanilla self-attention to provide better spatial reasoning ability. We use a three-layer Spatial Transformer with 8 heads to process.\"}"}
{"id": "4QaKdsh15T", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the object-centric features produced by PointNet++ and output object tokens for LLM. For other settings, we follow all the default hyperparameters in Chen et al. (2022).\\n\\n- **2D token embedding.** We use OpenCLIP ConvNext-base model (Liu et al., 2022) pre-trained on LAION2B (Schuhmann et al., 2022) to process the egocentric 2D image.\\n\\n- **CLIP fusion.** To enhance the alignment between visual tokens and instruction tokens, we use the text encoder from CLIP (Radford et al., 2021) to process the instruction tokens to obtain a global feature of the instruction. Next, we update the visual tokens with the element-wise product between the CLIP instruction feature and each image & object token embedding.\\n\\n**D.2.1 EMBODIMENT ENCODING**\\n\\nIn addition to the egocentric 2D input, we introduce an embodiment token to help LEO reason in an embodiment-aware fashion. We find it useful to use it together with the situation prompt and 2D egocentric input. Specifically, an embodiment token $e$ is introduced in embodied navigation, embodied reasoning, and object-in-the-scene caption tasks. Specifically, $e$ is a learnable embedding that will be inserted into the 3D object list.\\n\\nSo what does embodiment information mean in these tasks? In embodied navigation, it means the agent\u2019s position and orientation in the scene, which can be derived from a GPS and a compass sensor. The orientation of the agent is further represented by a rotation which is Fourier-embedded and mapped to a feature vector $r$ by a linear layer. It is the same in embodied reasoning task. In the object-in-the-scene caption task, we assume the agent is situated at the location of the object that is being referred to. Therefore, embodiment information also means the location of the referred object. We obtain this location by randomly choosing a spot inside the referred object bounding box. To sum up, we could simply treat the embodiment token as a special self object, where its object embedding is learnable, and its location/orientation corresponds to the actual or assumed \u201cagent\u201d.\\n\\nAfter inserting the embodiment token, we obtain a new 3D object token list: $e, s_1, s_2, \\\\ldots, s_N$, where $s_i \\\\in \\\\{1, 2, \\\\ldots, N\\\\}$ are 3D object token embeddings produced by PointNet++, along with location specified for each object (including the self-object). We can concatenate them together to get a feature matrix $O \\\\in \\\\mathbb{R}^{(N+1) \\\\times d}$ and send them to the Spatial Transformer to explicitly fuse the spatial information of all the 3D objects and the self-object.\\n\\n**D.3 LLM HYPERPARAMETERS**\\n\\nWe set the maximum output length of our Vicuna-7B to be 256. The maximum context length is also set to 256 and if the length of the input is greater than 256, we truncate it to 256 by deleting tokens from the left (i.e., only the rightmost 256 tokens are preserved). We set rank and $\\\\alpha$ in LoRA (Hu et al., 2022) to be 16 and the dropout rate to be 0. LoRA is implemented for all the projection matrices in the LLM, i.e., $(W_q, W_k, W_v, W_o)$ in attention modules and $(W_{gate}, W_{up}, W_{down})$ in MLPs.\\n\\nThe hyperparameters for beam search during inference are as follows:\\n\\n| Hyperparameter     | Value   |\\n|--------------------|---------|\\n| Number of beams    | 5       |\\n| maximum output length | 256    |\\n| minimum output length | 1      |\\n| top p              | 0.9     |\\n| repetition penalty | 3.0     |\\n| length penalty     | 1       |\\n| temperature        | 1       |\"}"}
{"id": "4QaKdsh15T", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: Examples of dialogue refinement.\\n\\n| Types              | Raw Responses                                      | Refined Responses                                      |\\n|--------------------|----------------------------------------------------|--------------------------------------------------------|\\n| Object Counting    | There are 3 chairs in the room.                    | I see there are 2 washing machines in the bathroom.    |\\n|                    | There are 4 chairs in the room.                    | I see there are 4 washing machines in the bathroom.    |\\n| Object Existence   | Yes, there is a cutting board in the kitchen.      | No, there is no cutting board in the room.             |\\n|                    | Yes, there is a computer and a monitor on the desk. | No, there is no computer in the room.                  |\\n| Object Non-existence| No, there is no stereo equipment in the room.      | Yes, there is a stereo equipment in the room.          |\\n|                    | I'm sorry, but I couldn't find a hair dryer in the bathroom. | Yes, I found a hair dryer in the room.                  |\\n| Negative Response  | No, there is nothing else mentioned in the scene graph. | I'm sorry, but there is no mention of a mirror in the scene graph for the bathroom. |\\n|                    | I'm sorry, but there is no mention of a mirror in the scene graph for the bathroom. | The negative responses will be removed.                |\\n| Response with ID   | You can place your backpack on the floor, to the left of the dining table-33. As for your bag, you can place it on the floor, to the left of the bed-10. | You can place your backpack on the floor, to the left of the dining table. As for your bag, you can place it on the floor, to the left of the bed. |\\n\\nTable 7: Examples of QA refinement.\\n\\n| Types            | Raw Responses                                      | Refined Responses                                      |\\n|------------------|----------------------------------------------------|--------------------------------------------------------|\\n| Object Counting  | Q: How many chairs are in the room? A: 3           | Q: How many chairs are in the room? A: four            |\\n| Object Existence | Q: Is there a mirror in the room? A: yes           | Q: Is there a mirror in the room? A: no                |\\n| Object Non-existence | Q: Is there an ironing board in the room? A: no  | Q: Is there an ironing board in the room? A: yes      |\\n| Negative Response | Q: What is the material of the bathtub? A: unknown | Q: Where is the shampoo dispenser? A: unknown         |\\n| Response with ID | Q: Where is the mirror located? A: attached to wall-3, behind heater-18, to the left of shelf-19 | Q: Where is the mirror located? A: attached to a wall, behind a heater, to the left of a shelf |\\n\\nTable 8: The accuracy of three types of raw responses from ChatGPT.\\n\\n| Tasks            | Object Counting | Object Existence | Object Non-existence |\\n|------------------|-----------------|------------------|----------------------|\\n| Dialogue         | 0.7824          | 0.9314           | 0.2986               |\\n| QA               | 0.7691          | 0.8999           | 0.2848               |\\n\\nTable 9: The proportion of Negative Response and Response with ID in the total set of responses.\\n\\n| Tasks            | Negative Response | Response with ID |\\n|------------------|--------------------|------------------|\\n| Dialogue         | 0.0080             | 0.0360           |\\n| QA               | 0.1045             | 0.0221           |\"}"}
{"id": "4QaKdsh15T", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: The distribution of node numbers for 3DSSG scenes. The node number represents the number of objects in a scene.\\n\\nTable 10: Sampling rates for scenes with different node numbers. The hyphen denotes a sweep of sampling rates, e.g., \\\"0.7-0.9\\\" means \\\"0.7, 0.8, 0.9\\\".\\n\\n| Node Number | Sampling Rate |\\n|-------------|---------------|\\n| 10-20       | 0.8, 0.9     |\\n| 20-30       | 0.7-0.9      |\\n| 30-40       | 0.6-0.9      |\\n| 40-50       | 0.6-0.9      |\\n| 50-60       | 0.5-0.9      |\\n| 60-70       | 0.5-0.9      |\\n| >70         | 0.4-0.9      |\\n\\ndifferent numbers of nodes in Tab. 10. For each sampling rate, we set 4 random prompt seeds to further enhance the diversity of prompted data.\\n\\nTo verify whether the subgraph sampling strategy can maintain the consistency and diversity of scene captions, we generate scene captions for the same scene using both the full graph and subgraph. We then employ GPT-4 (OpenAI, 2023) to evaluate the similarities and differences between the two captions. The results in Tab. 11 indicate that our subgraph sampling strategy can maintain both consistency and diversity.\\n\\nA.6 SCENE-GRAPH-BASED PROMPTING vs BOX-BASED PROMPTING\\n\\nIn this section, we provide a comparative analysis of scene-graph-based prompting and box-based prompting (Hong et al., 2023). We refer the readers to Figure 6 in 3D-LLM (Hong et al., 2023) for details of the box-based prompting method. Fig. 11 shows the contents of two methods. To present a fair comparison between the two methods, we prompt with 1) demonstrations that have similar content under the same scene (see Fig. 12) and 2) identical new scene queries. Since 3D-LLM does not elaborate on attribute-related prompts, we mainly compare the spatial relations in the responses. As shown in Fig. 13, we highlight some spatial relations in red. The comparison shows that our method provides more diverse and reliable spatial relations, which are important for 3D scene understanding.\\n\\nA.7 DATASET STATISTICS\\n\\nWe provide statistics on the instruction-tuning datasets. We visualize the distribution of the question types in 3RQA (Fig. 14) and 3RDialog (Fig. 15). The pie chart's inner circle represents the first word of the questions, while the outer circle accounts for the second or third word in the corresponding questions. The results show that the questions cover the attributes and spatial relations of the objects, as well as high-level topics such as room types and functionalities.\\n\\nWe also provide statistics of the root noun-verb pairs for instructions and responses in 3RDialog and 3RPlan, as shown in Fig. 16\u201319.\\n\\nB ACTION TOKENIZATION\\n\\nTo empower LEO to exert control over an embodiment or a robot, we encode all actions within the context of Object Navigation (Ramrakhya et al., 2022) and CLIPort (Shridhar et al., 2021) tasks.\"}"}
{"id": "4QaKdsh15T", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: The comparative analysis of full graph prompting and subgraph prompting.\\n\\nSettings\\n\\nGenerated Scene Captions\\n\\nFull Graph\\n\\nIn this scene, there is a colorful and flat white wall. A small TV stand is positioned in front of a low upholstered couch, to the left of a brown lamp, and to the right of a wooden table. The couch is covered with a pink padded square pillow and a patterned blanket. A light hangs on the wall and another light is supported by the table. There is a rectangular glass window attached to the wall, behind which is a yellow curtain. The room has a comfortable and cozy atmosphere, suitable for relaxation and entertainment.\\n\\nSampling Rate 0.6\\n\\nIn this scene, there is a colorful white wall attached to a brown wooden floor. The couch is upholstered and low in size, located to the right of a small TV stand and a table. A red blanket covers the couch, and there are patterned, padded square pillows on the couch. A black glass window is attached to the wall, and there are artificial lamps standing on the floor and close to the couch. The lamps are also near a table and to the right of the TV stand. The scene suggests a comfortable living room with a cozy couch, pillows, and a blanket, suitable for relaxation and entertainment.\\n\\nGPT-4 Evaluation\\n\\nSimilarities:\\n1. Both scenes describe a living room setting, emphasizing comfort and coziness suitable for relaxation and entertainment.\\n2. Both rooms have a white wall.\\n3. Each scene features a couch and a TV stand.\\n4. Both scenes describe a blanket and a padded square pillow on the couch.\\n\\nDifferences:\\n1. The first scene has a brown wooden floor, while the floor is not mentioned in the second scene.\\n2. The first scene has a red blanket on the couch; the second has a patterned blanket but doesn't specify the color.\\n3. The first scene describes the lamps as \\\"standing on the floor\\\", while the second mentions one light hanging on the wall and another supported by the table.\\n4. The second scene includes a yellow curtain behind the window, which the first scene does not mention.\\n\\nSummary:\\nOverall, both summaries provide a similar thematic view of a comfortable living room but differ in the specific arrangement and color details of the items within the room.\\n\\nBox-based Content\\n\\nFigure 11: Comparison of the content between box-based and scene-graph-based prompting.\\n\\nUsing the least frequently employed language tokens. Specifically, for the Object Navigation task, we allocate 4 tokens to represent actions of move forward, turn right, turn left, and stop. For the CLIPort task, we use a total of 516 tokens to discretize action poses, with 320 tokens dedicated to the x-axis pose bins, 160 tokens for the y-axis pose bins, and 36 tokens for the z-rotation bins.\\n\\nPlease refer to Tabs. 24\u201326 for examples of our dataset.\"}"}
{"id": "4QaKdsh15T", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Box-based Prompting\\n\\nDemonstration #1:\\n\\n**Human:** Do you think the room is suitable for me to read a book?\\n\\n**Robot:** Let me check the arrangement of the room. There is a light in the room, which can provide adequate lighting. There is a desk for you to put your book. However, there is no chair or sofa in the room. It means you cannot seat down and rest in the room.\\n\\n**Human:** Thanks a lot. Can you describe the position of the desk?\\n\\n**Robot:** Sure! The desk is in front of the window.\\n\\n**Human:** Thanks a lot.\\n\\n**Robot:** You are welcome!\\n\\nDemonstration #2:\\n\\n**Human:** Can you help me print a document?\\n\\n**Robot:** Sure! There is a printer in the room. You can use it for printing.\\n\\n**Human:** Ok. I also need a laptop to download the document.\\n\\n**Robot:** Ok. I find a laptop on the table. Do you need any other help?\\n\\n**Human:** Thanks a lot.\\n\\n**Robot:** You are welcome!\\n\\n---\\n\\nFigure 12: The demonstrations for prompting, which are designed to be similar for a fair comparison.\\n\\nFigure 13: The responses of two prompting methods. Descriptions highlighted in red show our method leads to more flexible and reliable spatial relations.\"}"}
{"id": "4QaKdsh15T", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "4QaKdsh15T", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "4QaKdsh15T", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\nXiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang.\\n\\nSqa3d: Situated question answering in 3d scenes. In International Conference on Learning Representations (ICLR), 2023.\\n\\nArjun Majumdar, Karmesh Yadav, Sergio Arnaud, Yecheng Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Pieter Abbeel, Jitendra Malik, et al. Where are we in the search for an artificial visual cortex for embodied intelligence. arXiv preprint arXiv:2303.18240, 2023.\\n\\nVernon B Mountcastle. An organizing principle for cerebral function: the unit module and the distributed system. The neurosciences. Fourth study program, 1979.\\n\\nYao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. arXiv preprint arXiv:2305.15021, 2023.\\n\\nOpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2022.\\n\\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\\n\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.\\n\\nSongyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nCharles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in Neural Information Processing Systems (NeurIPS), 2017.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR), 2020.\\n\\nSanthosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. arXiv preprint arXiv:2109.08238, 2021.\\n\\nRam Ramrakhya, Eric Undersander, Dhruv Batra, and Abhishek Das. Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. Transactions on Machine Learning Research (TMLR), 2022.\\n\\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Annual Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019.\\n\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations (ICLR), 2022.\"}"}
{"id": "4QaKdsh15T", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied AI research. In International Conference on Computer Vision (ICCV), 2019.\\n\\nJuergen Schmidhuber. One big net for everything. arXiv preprint arXiv:1802.08864, 2018.\\n\\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\nJonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3d for 3d semantic instance segmentation. arXiv preprint arXiv:2210.03105, 2022.\\n\\nMohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning (CoRL), 2021.\\n\\nAlessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, and Gaurav Sukhatme. Embodied bert: A transformer model for embodied, language-guided visual task completion. arXiv preprint arXiv:2108.04927, 2021.\\n\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\\n\\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems (NeurIPS), 2021.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 2017.\\n\\nJohanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias Nie\u00dfner. Rio: 3d object instance re-localization in changing indoor environments. In International Conference on Computer Vision (ICCV), 2019.\\n\\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.\\n\\nXinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A generalist painter for in-context visual learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. In Annual Meeting of the Association for Computational Linguistics (ACL), 2023.\\n\\nZehan Wang, Haifeng Huang, Yang Zhao, Ziang Zhang, and Zhou Zhao. Chat-3d: Data-efficiently tuning large language model for universal dialogue of 3d scenes. arXiv preprint arXiv:2308.08769, 2023.\\n\\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.\\n\\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations (ICLR), 2022.\\n\\nShun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab, and Federico Tombari. Scenegraph-fusion: Incremental 3d scene graph prediction from rgb-d sequences. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\"}"}
{"id": "4QaKdsh15T", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nLeveraging massive knowledge and learning schemes from large language models (LLMs), recent machine learning models show notable successes in building generalist agents that exhibit the capability of general-purpose task solving in diverse domains, including natural language processing, computer vision, and robotics. However, a significant challenge remains as these models exhibit limited ability in understanding and interacting with the 3D world. We argue this limitation significantly hinders the current models from performing real-world tasks and further achieving general intelligence. To this end, we introduce an embodied multi-modal and multi-task generalist agent that excels in perceiving, grounding, reasoning, planning, and acting in the 3D world. Our proposed agent, referred to as LEO, is trained with shared LLM-based model architectures, objectives, and weights in two stages: (i) 3D vision-language alignment and (ii) 3D vision-language-action instruction tuning. To facilitate the training, we meticulously curate and generate an extensive dataset comprising object-level and scene-level multi-modal tasks with exceeding scale and complexity, necessitating a deep understanding of and interaction with the 3D world. Through rigorous experiments, we demonstrate LEO\u2019s remarkable proficiency across a wide spectrum of tasks, including 3D captioning, question answering, embodied reasoning, embodied navigation, and robotic manipulation. Our ablation results further provide valuable insights for the development of future embodied generalist agents.\\n\\nIntroduction\\nBuilding one generalist model that can achieve comprehensive tasks like humans has been a long-existing pursuit in artificial intelligence and neuroscience (Lake et al., 2015; 2017; Zhu et al., 2020; Mountcastle, 1979; Schmidhuber, 2018; Huang et al., 2022a). Recent advances in large language models (LLMs) (Brown et al., 2020) and \u201cfoundation model\u201d (Bommasani et al., 2021) emerge as a promising paradigm in building such generalist models in natural language processing (OpenAI, 2022; 2023), computer vision (Kirillov et al., 2023), and robotics (Brohan et al., 2022; 2023). The keys to the success of this paradigm lie in large-scale internet-level datasets from numerous tasks and domains and scalable Transformer architectures (Vaswani et al., 2017) that can absorb generalizable and task-agnostic knowledge from the data. Such efforts are further extended to multi-modal (Alayrac et al., 2022; Lu et al., 2023; Li et al., 2023c) and generalist models (Reed et al., 2022; Driess et al., 2023) where the agents can solve versatile tasks based on the language-specified task descriptions and show certain generalizability to novel situations. Nonetheless, their abilities are primarily demonstrated within 2D domains, thereby limiting the comprehension of the 3D physical environment that envelops humans and other intelligent species. This limitation acts as an obstacle, preventing current models from successfully executing real-world tasks and the attainment of general intelligence. Therefore, we ask a fundamental question: how to equip the generalist agent with a comprehensive understanding of and the ability to interact with the real 3D world?\\n\\nThe development of such generalist agents encounters three primary challenges: the creation of suitable datasets, the design of unified models, and the design of effective learning strategies. Despite substantial progresses in scaling up image-text models (Tsimpoukelli et al., 2021; Alayrac et al., 2022) and the curation of corresponding datasets (Radford et al., 2021; Schuhmann et al., 2022), advancements in 3D scene-level understanding have significantly lagged behind. This is largely attributed to the limited scale and manual labeling of 3D datasets (Dai et al., 2017; Wald et al., 2019; Chen et al., 2020), given the higher cost associated with collecting 3D data compared to 2D data. Furthermore, previous models have often been designed with strong priors (Zhao et al., 2021; Chen et al., 2022), with limited exploration of large-scale unified pretraining and efficient fine-tuning based on LLMs. Notably, recent works (Zhu et al., 2023c; Hong et al., 2023) utilize the...\"}"}
{"id": "4QaKdsh15T", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nEmbodied Navigation\\n\\nLEO:\\n1. Move forward\\n2. Turn right\\n3. Move forward\\n4. \u2026\\n\\nHuman: Find the pillow\\n\\nRobotic Manipulation\\n\\nHuman: pack the yoshi figure in the brown box\\n\\nLEO:\\n\\nTask Planning\\n\\nHuman: Organize and tidy up the bedroom.\\n\\nLEO:\\n1. Clean the floor by sweeping to remove \u2026\\n2. Make the bed \u2026\\n3. Fold and organize \u2026\\n\\n3D Question Answering\\n\\nQuestion: Where is the Printer located?\\n\\nLEO:\\nStanding on the desk next to the wardrobe.\\n\\n3D Dialogue\\n\\nHuman: I need help organizing my bedroom. Can you assist me?\\n\\nLEO:\\nOf course! What specifically do you need help with?\\n\\nHuman: Can you check if there are any clean clothes hanging on the wall?\\n\\nLEO:\\nI'm sorry, but the clothes hanging on the wall are messy.\\n\\nHuman: Thanks! Also, can you check if there are any clean clothes hanging on the wall?\\n\\nScene Captioning\\n\\nLEO:\\nIn this room, there is a wooden floor that is clean and flat. A tall wardrobe \u2026\\n\\nEmbodied Reasoning\\n\\nSituation:\\nSitting at the edge of the bed and facing the couch.\\n\\nQuestion:\\nCan I go straight to the coffee table in front of me?\\n\\nLEO:\\nNo.\\n\\n3D Object Captioning\\n\\nLEO:\\nA 3D model of a Southern White Rhino, one of largest animal species in the world.\\n\\nLEO:\\nIt's a 3D model of a midnight green ceramic coffee cup with a plate.\\n\\nUser:\\nPlease describe the toy house over \u2026\\n\\n\u2744\ud83d\udd25\ud83d\udd25\\n\\nYou are an assistant situated in a 3D scene.\\n\\n\u2744\\n\\nTokenizer Action Response\\n\\nText Response\\n\\nThere is a sofa next to the TV.\\n\\nIt's a kitchen for cooking.\\n\\nEmbodied Generalist Agent in 3D World\\n\\n2D Encoder\\n\\n3D Encoder\\n\\nSystem\\n\\nMessage\\n\\nEgocentric Image\\n\\n3D Scene\\n\\nInstruction\\n\\nTokenizer\\n\\nP = [0, 1, -0.2, 0]\\n\\nR = [0, 0, 0, 1]\\n\\n\\\"Turn right\\\"\\n\\nDe-tokenize\\n\\nLarge Language Model\\n\\nLoRA\\n\\n\ud83d\udd25\ud835\udf39\\n\\nFigure 1: The proposed embodied generalist agent LEO. It takes egocentric 2D images, 3D point clouds, and texts as input and formulates comprehensive 3D tasks as autoregressive sequence predictions. By fine-tuning LEO, it extends the capability of LLMs to multi-modal vision-language-action tasks with a unified model.\\n\\nHowever, they still lack the agency to act within 3D environments and efforts in unleashing LLMs for 3D vision-language-action (VLA) learning. How to equip the 3D agent with a simple unified architecture and effective learning strategy to establish VLA capability remains rarely explored.\\n\\nIn this work, we introduce the generalist agent LEO, which is generically embodied, multi-modal, and multi-task. It can take egocentric 2D images, 3D point clouds, and texts as task input and achieve comprehensive tasks within the 3D environment. As shown in Fig. 1, LEO exhibits the capability of perceiving, grounding, reasoning, planning, and acting with shared model architectures and weights.\\n\\nLEO perceives through an egocentric 2D image encoder for the embodied view and an object-centric 3D point cloud encoder for the third-person global view. The 3D encoder generates an object-centric token for each observed entity. Such encoder design can be flexibly adapted to tasks with various embodiments. These output tokens are then interleaved with text tokens to form a scene-grounded instructional task sequence, which further serves as the input to a decoder-only LLM. All the tasks are re-formulated as sequence prediction problems. Therefore, LEO can be trained with task-agnostic inputs and outputs using autoregressive training objectives. To accommodate embodied tasks that predict action tokens, we employ a pool of special tokens to represent actions and replace the least used tokens (Brohan et al., 2023) in LLM. We perform LoRA (Hu et al., 2022) to fine-tune and adapt the innate knowledge of LLM to the multi-modal generalist. LEO demonstrates two essential capabilities: 3D vision-language grounding and 3D VLA. They are injected with two training stages: 3D vision-language alignment and VLA instruction-tuning. To facilitate the training, we curate a large-scale dataset with object-level and scene-level tasks by fusing existing datasets with high-quality data prompted from the LLMs. We propose scene-graph-based prompting and refinement methods, along with Object-centric Chain-of-Thought (O-CoT) for improving the quality of generated data, largely enriching the data scale and diversity, and further eliminating the hallucination of LLMs.\\n\\nWe quantitatively evaluate and ablate LEO on diverse 3D tasks, including object-level and scene-level captioning (Luo et al., 2023; Chen et al., 2021), 3D question answering (Azuma et al., 2022), situated question answering (Ma et al., 2023), embodied navigation (Ramrakhya et al., 2022), and robotic manipulation (Shridhar et al., 2021). The results indicate (i) LEO achieves state-of-the-art results on most tasks; (ii) through task-agnostic instruction tuning with a unified model, LEO outperforms most previous task-specific models on various domains; (iii) the pretraining of 3D vision-language alignment greatly elevates the performance of VLA instruction-tuning; (iv) scaling up the training data boosts the performance of generalist agent, similar to scaling laws (Kaplan et al., 2020; OpenAI, 2023) in LLMs and generalist agent (Reed et al., 2022). We also show qualitative results of chatting with LEO over diverse tasks, demonstrating its capability in scene-aware planning and dialogue.\\n\\nIn summary, our main contributions are: (i) we propose LEO, the first generalist agent with the capability to perceive, ground, reason, plan, and act in the 3D world; (ii) we demonstrate that a generalist agent can be built via fine-tuning the LLM with object-centric multi-modal representations and mixing the training data with embodied action sequences, enabling it to excel in embodied tasks; (iii) we meticulously curate a large-scale dataset to train such agent and propose several techniques\u2026\"}"}
{"id": "4QaKdsh15T", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024 aimed at enhancing the quality of prompted data from LLMs; (iv) we extensively evaluate LEO and demonstrate its proficiency on diverse tasks including embodied navigation and robotic manipulation, we also observe consistent performance gains while simply scaling up the training data; (v) we will release the data, code, and model weights to endow the future research of generalist agents.\\n\\n2. MODEL\\n\\nThe leading design principles of LEO are two-fold: 1) It should handle the input of egocentric 2D, global 3D, and textual instruction, and the output of textual response as well as embodied action commands in a unified architecture; 2) It should leverage pretrained large language models (LLMs) as a powerful prior for the downstream tasks. We therefore convert all data of different modalities into a sequence of tokens, illustrated below:\\n\\n\\\\[\\n\\\\text{system message (1)} \\\\rightarrow \\\\text{2D image tokens (optional)} \\\\rightarrow \\\\text{object-centric 3D tokens (N)} \\\\rightarrow \\\\text{instruction (1)} \\\\rightarrow \\\\text{response (T)}\\n\\\\]\\n\\nWith this representation, we formulate the learning of LEO as GPT-style autoregressive language modeling (Brown et al., 2020) given the prefix (from system message to instruction), i.e. prefix language modeling (Raffel et al., 2020). Therefore, a pretrained LLM can be used to process such sequences. In the following, we will detail the tokenization of multi-modal data, model architecture, training loss, and inference settings. An overview of our model can be found in Fig. 1.\\n\\n2.1 TOKENIZATION\\n\\nWe follow prior practices in 2D VLM (Liu et al., 2023b; Alayrac et al., 2022) and 3D VLM (Zhu et al., 2023c) to tokenize the multi-modal data in LEO. We use SentencePiece tokenizer (Kudo & Richardson, 2018) to encode text with 32k subwords; 2D image tokens for egocentric 2D images; and object-centric 3D tokens extracted over Mask3D-based (Schult et al., 2022) object proposals for 3D point cloud inputs. For embodied action commands, continuous actions (e.g. in manipulation) are discretized (details in Appendix B) to join the discrete actions (e.g. navigation) and form a unified discrete action space. We follow Brohan et al. (2023) to map these discrete actions to the least used tokens in SentencePiece. After tokenization, all tokens are ordered into the format in (1).\\n\\n2.2 TOKEN EMBEDDING & LLM\\n\\nWe apply several token embedding functions to process the tokens in the sequence before sending them to the LLM. The LLM will then align these tokens of different modalities, and produce the response. Most of the responses are text and can be decoded directly. For responses that include embodied actions, we will map the reserved SentencePiece text tokens back to action commands.\\n\\nText & 2D token embedding.\\n\\nFor text tokens (including embodied actions that have been mapped to the reserved text tokens), an embedding look-up table is used to map them into vectors. While the egocentric 2D image is encoded by a pretrained OpenCLIP ConvNext (Liu et al., 2022) for obtaining image token embeddings. We apply MLP adapters to match the dimensions of all token embeddings.\\n\\nObject-centric 3D token embedding.\\n\\nEach 3D object token (i.e., the point cloud of a 3D object) is first encoded by a pretrained point cloud encoder (e.g., PointNet++ (Qi et al., 2017)). We then adopt the Spatial Transformer introduced in Chen et al. (2022) to further process the point cloud embedding of all objects into object-centric 3D token embeddings. In a nutshell, Spatial Transformer biases the standard attention score with relative position and size for capturing 3D relations between objects. Due to space limit, the readers are referred to Chen et al. (2022) and Appendix D.2 for more details.\\n\\nPretrained LLM.\\n\\nWe choose Vicuna-7B (Chiang et al., 2023) to process the token sequence. In order to tackle the challenging alignment and grounding problem of multi-modal tokens (2D, 3D, text, embodied action) while preserving the LLM pretrained knowledge, we employ LoRA (Hu et al., 2022) to introduce additional tunable parameters to the frozen pretrained LLM.\"}"}
{"id": "4QaKdsh15T", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To the right of toilet\\nWhere is the bathtub located in the bathroom?\\n\\nThere is a rectangular brown table. It is in the center of the room.\\n\\nDescribe object.\\n\\nClosed\\nIs the door closed or open to my left side?\\n\\nOf course! I'd be happy to help. What do you need advice on?\\n\\nI'm redecorating my bedroom and I need some advice. Can you help me?\\n\\n1. Wipe down the door, including the handle and any glass panels.\\n2. Vacuum or sweep the floor to remove any dust, dirt, or debris.\\n3. Dust and clean the cabinet, including any shelves or drawers.\\n4. Arrange the pillows on the couch neatly and fluff them up.\\n5. Dust and clean the ceiling using a long-handled duster or appropriate cleaning tool.\\n6. Check and adjust the temperature or settings of the radiator if necessary.\\n\\nNeatly organize and remove dirt from the living room.\\n\\nI want to add some color to the room. Are there any colorful objects in the room?\\n\\nYes, there is a colorful picture hanging on the wall. It is in front of a shelf and behind an armchair. It adds a nice pop of color to the room.\\n\\nThat sounds great! What other decorations or furniture are in the room?\\n\\nThere is a wardrobe to the right of a stuffed animal. It is in front of a doorframe.\\n\\nAdditionally, there are shelves attached to the wall, with various items displayed on them.\\n\\nThank you for the information! It's really helpful for my redecorating plans.\\n\\nYou're welcome! I'm glad I could assist you. If you have any questions, feel free to ask.\\n\\nPack the black shoe with green stripes in the brown box.\\n\\n(0.29, 0.07, 0.06), (0., 0., 0., 1.0), (0.53, -0.18, 0.01), (0., 0., 0.38, 0.92).\\n\\nPlease navigate to the toilet in the scene.\\n\\nFigure 3: Qualitative results of interacting with LEO on unseen scenarios from a held-out test set of LEO-instruct. LEO's responses and actions can be grounded in novel scenes.\\n\\n4.3 EMBODIED ACTION IN 3D WORLD\\n\\nOverview.\\n\\nFinally, we hope to directly probe the embodied acting and interacting capacity of LEO in the 3D World. We select two canonical embodied AI tasks: embodied navigation with ObjNav on AI Habitat (Ramrakhya et al., 2022) and robotic manipulation on CLIPort (Shridhar et al., 2021).\\n\\nSpecifically, for ObjNav, although LEO is trained on a customized dataset (see Sec. 3.2), the scenes are all included in the original MP3D ObjNav training split (Savva et al., 2019). Therefore, we still evaluate LEO on the original MP3D ObjNav validation split against baselines. Additionally, we test LEO on the validation split of the newly introduced HM3D ObjNav task (Ramakrishnan et al., 2021). We report the success rate and SPL metrics following Ramrakhya et al. (2022). For CLIPort robotic manipulation, we evaluate LEO on the three training tasks listed in Tab. 3 and their corresponding unseen tasks and report the average reward across the evaluation episodes.\\n\\nResults & analysis.\\n\\nWe present the results of CLIPort manipulation and object navigation in Tabs. 3 and 4. Our findings are as follows: 1) In robotic manipulation, LEO exhibits comparable performances to many strong baselines and even achieves significantly better results on some challenging unseen tasks. Note that compared to baselines that rely on heatmap output, LEO produces motor commands directly. 2) On ObjNav, LEO attains a reasonable success rate and better SPL on MP3D-val compared with baselines, suggesting that LEO can learn to leverage the object-centric 3D scene input (potentially offering a coarse global map) and take a shorter path to the target. Further, results\"}"}
{"id": "4QaKdsh15T", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: (a) Comparison of LEO trained under different data configurations. We test models on the validation set of Scan2Cap and ScanQA, and the held-out test sets of other datasets. * indicates datasets that belong to LEO-align. The color-gradient bar of ScanNetOnly on 3RQA, 3RPlan and 3RDialog indicates zero-shot transfer. Refer to Appendix G for numerical results. (b) Instruction-tuning losses on the test set, with the growth of data and model scale. These results echo the scaling law introduced in Kaplan et al. (2020); Reed et al. (2022). on HM3D-val confirm LEO's zero-shot generalization to novel scenes. Please note that all baselines use an RNN policy while LEO can be viewed as a transformer-based feed-forward policy similar to RT-2 (Brohan et al., 2023) considering the training efficiency, which could lead to a lower success rate. More discussions on this can be found in Appendix H.2. 3) Overall, the align-then-instruct tuning scheme endows LEO with semantic-level generalization (novel objects, etc) in both manipulation and navigation tasks (we provide results on ObjNav with unseen objects in Appendix I.1).\\n\\n4.4 ABLATIVE STUDY\\n\\nSettings. We ablate LEO on different data configurations. Specifically, we compare LEO's performance under the following settings: (1) NoAlign: tuning LEO from scratch on LEO-instruct, skipping pre-training on LEO-align; (2) PartialData: uniformly sampling 10% of data in LEO-instruct during instruction-tuning; (3) ScanNetOnly: excluding data generated by LLM and embodied tasks (i.e., navigation and manipulation) during instruction tuning; (4) NoAct (VL): excluding embodied task data during instruction tuning. We provide additional results and findings in Appendix G.\\n\\nEvaluation. We provide a more comprehensive quantitative evaluation of LEO on all 3D VL tasks, including 3D object/scene captioning, 3D QA, scene-grounded dialogue and task planning. Following prior works (Achlioptas et al., 2020), we use ground-truth object proposals in our ablation to pinpoint the reasoning and planning capabilities of LEO. We report exact match scores for QA tasks and SentenceSim for all other tasks. Fig. 4 shows a holistic view of the results.\\n\\nResults & analysis. 1) The two-stage align-then-instruct pipeline is critical for LEO learning. The lack of alignment harms detailed understanding of scenes, while the decrease in instruction-tuning data affects reasoning and planning. 2) Compositional generalization poses considerable challenges. ScanNetOnly, having been exposed to 3RScan scenes or QA skills during the two stages respectively, still struggles to handle the QA task in 3RScan scenes (3RQA). 3) General vs. specific. We observe model performance drops on in-domain tasks (e.g., Scan2Cap) when adding data from other domains or new tasks (ScanNetOnly vs NoAct (VL)). Scaling up the instruction-tuning data brings significant improvements, though the embodied acting data counteracts such effects due to the domain gap (PartialData vs Full (VLA) vs NoAct (VL)).\\n\\n4.5 SCALING ANALYSIS\\n\\nSettings. Following the analysis in Sec. 4.4, we study the scaling effect (Kaplan et al., 2020; Reed et al., 2022) of data and model in LEO. We use the instruction-tuning loss (on the test set) of LEO with the growth of data and model scale as an indicator. Based on NoAct (VL) with Vicuna-7B (referred to as Vicuna-7B Aligned), we add two variants: (1) Vicuna-7B Scratch, trained without the alignment stage; and (2) Vicuna-13B Scratch, trained without the alignment stage and scaling up the LLM to 13B. The curves of test loss are visualized in Fig. 4(b).\\n\\nResults & analysis. 1) The instruction tuning of LEO conforms to the scaling law (Kaplan et al., 2020; Reed et al., 2022). For all three settings, we find the test loss of LEO decreases log-linearly as it is fed with more data. 2) The lack of alignment causes significantly higher loss. Vicuna-7B Scratch shows consistently higher loss than Vicuna-7B Aligned. This corresponds to the inferior performances of NoAlign in Sec. 4.4 and emphasizes the importance of alignment. 3) Scaling up LLM leads to degradation. Vicuna-13B Scratch shows consistently higher loss than Vicuna-7B.\"}"}
{"id": "4QaKdsh15T", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Scratch, which echoes previous findings (Dai et al., 2023; Xu et al., 2023). We conjecture there are two possible reasons: multi-modal instruction tuning data is insufficient to reveal the benefit of scaling up LLMs, or a small-scale LLM (e.g., Vicuna-7B) already suffices for connecting the visual modality.\\n\\n**Generalist agents.**\\nThe AI community has witnessed the rising generalist models in both vision (Lu et al., 2023; Wang et al., 2023b; Kirillov et al., 2023) and language (OpenAI, 2022; 2023) domains. A generalist agent requires additional embodiment knowledge to interact with the environment and complete embodied acting tasks. Existing efforts towards generalist agents include: grounded reasoning and task planning in the real world (Ahn et al., 2022; Huang et al., 2022b), skill generalization in open-world environment (Fan et al., 2022; Cai et al., 2023a; Wang et al., 2023e;a; Cai et al., 2023b; Gong et al., 2023b), general robotic manipulation (Brohan et al., 2022; Jiang et al., 2023; Gong et al., 2023a), and unified vision-language-action (VLA) models such as Gato (Reed et al., 2022), PaLM-E (Driess et al., 2023), EmbodiedGPT (Mu et al., 2023), and RT-2 (Brohan et al., 2023). LEO belongs to the VLA model, however, its goal is to build a generalist agent that can understand the real 3D world beyond 2D images, which is absent in existing works.\\n\\n**Multi-modal instruction tuning.**\\nPre-trained LLMs demonstrated practical for solving vision-language tasks (Tsimpoukelli et al., 2021; Alayrac et al., 2022; Guo et al., 2023; Li et al., 2023d; Zhao et al., 2023). Meanwhile, the instruction-tuning paradigm exhibited strong zero-shot generalization in NLP tasks (Wei et al., 2022; Sanh et al., 2022; Ouyang et al., 2022; Chung et al., 2022). The two streams merged into instruction-tuned LVLMs (Liu et al., 2023b; Zhu et al., 2023b; Ye et al., 2023; Gao et al., 2023; Li et al., 2023b; Gong et al., 2023c; Dai et al., 2023). Despite the burst, these models are confined to 2D visual modalities, e.g., image or video. Concurrent works (Yin et al., 2023; Hong et al., 2023; Wang et al., 2023d; Xu et al., 2023) extend to 3D vision tasks, but these models either lack the acting capability or unified efficient architecture.\\n\\n**Grounded 3D scene understanding.**\\nOne key obstacle to building LEO is grounding the 3D world with natural languages. There exist diverse methods of grounded scene understanding, e.g., spatial relation modeling (Zhao et al., 2021; Chen et al., 2022; Zhu et al., 2023c) and fine-grained open-scene understanding (Peng et al., 2023b; Kerr et al., 2023). However, due to data scarcity, how to utilize LLMs to ground the 3D scene is rarely explored. Recently, 3D-LLM (Hong et al., 2023) leverages multi-view images and Chat-3D (Wang et al., 2023d) uses object-centric point clouds to enable the LLMs with 3D grounding. In this work, we devise both 2D and 3D encoders for grounding various visual representations and employ LoRA (Hu et al., 2022) to efficiently fine-tune the LLMs.\\n\\n**3D data prompting from LLMs.**\\nLLMs exhibit extraordinary capabilities of text generation and serve as a source for collecting diverse instruction-following data (Wang et al., 2023c; Taori et al., 2023; Peng et al., 2023a). However, the lack of access to visual modalities makes it troublesome to collect visual instruction-tuning data. To address this issue, existing methods provide bounding boxes (Liu et al., 2023b) and add dense captions (Li et al., 2023a; Liu et al., 2023a) as image descriptions or directly use off-the-shelf large vision-language models (LVLM) (Zhu et al., 2023a; Luo et al., 2023) to help collect such data. Unlike concurrent attempts (Yin et al., 2023; Hong et al., 2023; Wang et al., 2023d) in collecting 3D instruction-tuning data, our approach features a scene-graph-based prompting and refinement method to prompt and correct the data.\\n\\n**Conclusions**\\nThe proposed agent LEO extends the current generalist ability of LLMs from text towards the 3D world and embodied tasks. It is a crucial initial step toward building embodied generalist agents. In light of this work, we identify several promising directions that hold the potential for substantial advancement: (1) enhancing the 3D vision-language grounding capability by leveraging larger-scale paired data from richer real-world 3D domains; (2) continually bridging the gap between 3D vision-language and embodied action, as our experiments reveal the feasibility of their joint learning; (3) investigating the issues of safety and alignment in the context of embodied generalist agents, particularly given that our scaling law analysis suggests that such agents can experience significant enhancements through data scaling in the near future.\"}"}
{"id": "4QaKdsh15T", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In European Conference on Computer Vision (ECCV), 2020.\\n\\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\\n\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In International Conference on Computer Vision (ICCV), 2015.\\n\\nDaichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023, 2023.\\n\\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\\n\\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.\\n\\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nDaigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong Xu. 3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nShaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13734\u201313744, 2023a.\\n\\nShaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Groot: Learning to follow instructions by watching gameplay videos. arXiv preprint arXiv:2310.08235, 2023b.\\n\\nDave Zhenyu Chen, Angel X Chang, and Matthias Nie\u00dfner. Scanrefer: 3d object localization in rgb-d scans using natural language. In European Conference on Computer Vision (ECCV), 2020.\\n\\nShizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Language conditioned spatial relation reasoning for 3d object grounding. Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\nSijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu, and Tao Chen. End-to-end 3d dense captioning with vote2cap-detr. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\"}"}
{"id": "4QaKdsh15T", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nRunsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: Empowering large language models to understand point clouds. arXiv preprint arXiv:2308.16911, 2023. 10\\n\\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 10\\n\\nZhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, et al. Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark. arXiv preprint arXiv:2306.06687, 2023. 6, 10\\n\\nXumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 28\\n\\nHaozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with multi-modal in-context learning. arXiv preprint arXiv:2309.07915, 2023. 10\\n\\nLichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvg-transformer: Relation modeling for visual grounding on point clouds. In International Conference on Computer Vision (ICCV), 2021. 1, 10\\n\\nDeyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, and Mohamed Elhoseiny. Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions. arXiv preprint arXiv:2303.06594, 2023a. 10\\n\\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023b. 10\\n\\nYixin Zhu, Tao Gao, Lifeng Fan, Siyuan Huang, Mark Edmonds, Hangxin Liu, Feng Gao, Chi Zhang, Siyuan Qi, Ying Nian Wu, et al. Dark, beyond deep: A paradigm shift to cognitive ai with humanlike common sense. Engineering, 2020. 1\\n\\nZiyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pre-trained transformer for 3d vision and text alignment. In International Conference on Computer Vision (ICCV), 2023c. 1, 3, 4, 6, 7, 10\"}"}
{"id": "4QaKdsh15T", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Fig. 5\u20139, we show the prompts for five types of LLM-assisted 3D-language data generation. We provide few-shot examples as the context. In each example, the \\\"content\\\" contains a scene graph, and the \\\"response\\\" refers to a human-labeled response. The query is a new scene graph, based on which ChatGPT (OpenAI, 2022) generates responses.\\n\\nYou are an AI visual assistant in a 3D scene. The scene contains some objects, which compose a scene graph in json format. Each entity in the scene graph denotes an object instance, with a class label and an object id. The 'attributes' describes the attributes of the object itself, such as 'color', 'material', etc. The 'relations' describes the spatial relations with other objects. For example, from the scene graph\\n\\n```\\n{'sofa-1': {'attributes': {'color': 'red'}, 'relations': ['to the right of chair-2', 'in front of table-3']}, 'chair-2': {'attributes': {'color': 'brown'}, 'relations': ['to the left of sofa-1']}, 'table-3': {'attributes': {'material': 'wood'}, 'relations': []}}\\n```\\n\\nwe can know that 1) the sofa is red, 2) the chair is brown, 3) the football table is made of wood, 4) the chair is on the left of the sofa, 5) the chair is in front of the table. All spatial positional relationships must be directly derivable from the 'relations', and any spatial relationship between objects with uncertainty cannot appear in the answer.\\n\\nYou need to generate meaningful conversations based on the scene information. The conversations include questions from human and responses from an AI assistant. Ask questions about the object types, counting the objects, object attributes, relative positions between objects. Also ask questions concerning commonsense, e.g., how the objects can be used by human and human activity in the scene. You can ask questions about the affordance of the objects in the scene. The questions should conform to the given scene information. The attributes of objects and spatial relations between objects can only be inferred from the 'attributes' and 'relations' in scene graph, respectively. The questions must be able to be answered correctly based on the scene graph. You need to provide the queried object. Note that all answers to the questions must be single words or phrases. The question answer pair should be following format:\\n\\nQ: <question>\\nT: <queried object(s)>\\nA: <Answer>. You can answer the question according to the queried object(s). If there is no information about the question, the <Answer> should be 'unknown'.\\n\\nFigure 5: The prompt for generating 3D Dialogue.\\n\\nFigure 6: The prompt for generating 3D QA.\"}"}
{"id": "4QaKdsh15T", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fig. 5 shows the prompt for generating 3D dialogue data. Red fonts outline our requirements of the dialogue content, including object attributes, spatial relations, and commonsense topics. Purple fonts formulate the template of the response. We require the response generated by the ChatGPT should include the dialogue context as well; the \u201cthought\u201d contains the involved objects in the question, which is used to enhance the reliability of the answer. These two components will be removed after the refinement procedures.\\n\\nFig. 7: The prompt for generating 3D planning.\\n\\nYou are an AI visual assistant that can analyze a 3D scene. The scene contains some objects, which compose a scene graph in json format. Each entity in the scene graph denotes an object instance, with a class label and an object id. The 'attributes' describes the attributes of the object itself, such as \u2018color\u2019, \u2018material\u2019, etc. The 'relations' describes the spatial relations with other objects.\\n\\nFor example, from the scene graph:\\n\\n```json\\n{\\n  \\\"sofa-1\\\": {\\n    \\\"attributes\\\": {\\n      \\\"color\\\": \\\"red\\\"\\n    },\\n    \\\"relations\\\": [\\n      \\\"to the right of chair-2\\\",\\n      \\\"in front of table-3\\\"\\n    ]\\n  },\\n  \\\"chair-2\\\": {\\n    \\\"attributes\\\": {\\n      \\\"color\\\": \\\"brown\\\"\\n    },\\n    \\\"relations\\\": [\\n      \\\"to the left of sofa-1\\\"\\n    ]\\n  },\\n  \\\"table-3\\\": {\\n    \\\"attributes\\\": {\\n      \\\"material\\\": \\\"wood\\\"\\n    },\\n    \\\"relations\\\": []\\n  }\\n}\\n```\\n\\nWe can know that 1) the sofa is red, 2) the chair is brown, 3) the football table is made of wood, 4) the chair is on the left of the sofa, 5) the chair is in front of the table.\\n\\nAll spatial positional relationships must be directly derivable from the 'relations', and any spatial relationship between objects with uncertainty cannot appear in the answer. Do not use the id of the object in the dialogue, use ordinal words and attributes to refer to different objects with the same label.\\n\\nYou need to provide a summary for a scene. The summary should be about the object types, object attributes, relative positions between objects. Also describe the scene concerning commonsense, e.g., how the objects can be used by human and human activity in the scene. The description should conform to the given scene information. The attributes of objects and spatial relations between objects can only be inferred from the 'attributes' and 'relations' in scene graph, respectively. You don't need to describe each object in the scene, pick some objects of the scene for summary. You can also summarize the room's function, style, and comfort level based on the arrangement and color of objects within the room. Your summary must not exceed 110 words.\\n\\nTo further investigate the impact of Object-centric Chain-of-Thought (O-CoT) on data quality, we analyze the answer accuracy for Object Counting questions. Specifically, we collect several demonstrations, and for each run, we select two of them as the prompt seed. With these seeds, we generate dialogues across all scenes in 3DSSG (Wu et al., 2021) and then assess the answer accuracy for Object Counting questions. The results are presented in Tab. 5.\"}"}
{"id": "4QaKdsh15T", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: The prompt for generating 3D object-in-the-scene caption.\\n\\nTable 5: The effect of O-CoT on the answer accuracy for Object Counting questions.\\n\\n| Settings | Seed 1 | Seed 2 | Seed 3 | Seed 4 | Average Avg. Gain |\\n|----------|--------|--------|--------|--------|------------------|\\n| w/o O-CoT | 0.5838 | 0.5349 | 0.5962 | 0.5816 | 0.5741           |\\n|          |        |        |        |        | 0.2061           |\\n| O-CoT    | 0.7647 | 0.8117 | 0.7778 | 0.7667 | 0.7802           |\\n\\nThe results in Tab. 5 indicate that O-CoT consistently improves the answer accuracy for Object Counting questions. Though there remain errors after applying O-CoT, we will conduct refinement to fix them. Examples of Object Counting questions are provided in Appendix A.3.\\n\\nA.3 R\\\\_EFINEMENT\\\\_DETAILS\\n\\nWe conduct refinement by passing raw LLM-generated responses into several human-defined filtering procedures based on the 3D scene graph. The refinement considers five raw response categories:\\n\\n- **Object Counting.** The question concerns counting the target object.\\n- **Object Existence.** The response claims the existence of objects, which can be actually either existent or non-existent.\\n- **Object Non-existence.** The response claims the non-existence of objects, which can be actually either existent or non-existent.\\n- **Negative Response.** The scene graph cannot provide a solid response to the question, which means the question cannot be answered and will be discarded.\\n- **Response with ID.** The response contains unexpected object IDs.\\n\\nSpecifically, we employ regular expression matching to detect errors in these five categories. And we also employ this method to correct the responses except for Response with ID, which will be rewritten by ChatGPT instead. The QA pair will be eliminated if multiple rounds of rewriting fail to remove the IDs. Tab. 6 and Tab. 7 show some examples of the responses subject to the above five categories as well as the effect of our refinement.\\n\\nA.4 S\\\\_TATISTICS OF R\\\\_AW\\\\_RESPONSES\\n\\nBased on the aforementioned five raw response categories, we assess their quality by statistics and clarify the refinement effect accordingly. In Tab. 8, we quantify the answer accuracy for Object Counting, Object Existence, and Object Non-existence in dialogue and QA tasks. Results of the two tasks are averaged over all 3DSSG scenes across 6 prompt seeds and 3 prompt seeds, respectively. For these three categories of responses, we can fix almost all the detected errors by referring to the scene graph. In Tab. 9, we present the proportion of Negative Response and Response with ID in the total set of responses. Negative responses will be removed, as well as the responses with remaining IDs after multiple rounds of rewriting. All results are based on the O-CoT method.\\n\\nA.5 S\\\\_UBGRAPH\\\\_S\\\\_AMPLING\\n\\nTo enhance the diversity of the 3D scene graphs used for prompting, we perform subgraph sampling on the 3DSSG according to a sampling rate, which denotes the ratio of preserved nodes. The sampled subgraphs are used for generating scene captions and planning data. We analyze the distribution of node numbers across the 3DSSG dataset in Fig. 10 and set different sampling rates for scenes with...\"}"}
{"id": "4QaKdsh15T", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Datasets statistics. Top: Datasets for 3D-language alignment; Bottom: Datasets for 3D VLA instruction tuning.\\n\\n| Dataset       | Task       | 2D required? | 3D dataset | #instance  | #token (res.) | #token (prefix + res.) |\\n|---------------|------------|--------------|------------|------------|--------------|------------------------|\\n| LEO-align     | object captioning | \u2717           | Objaverse  | 660K       | 10M          | 27M                    |\\n|               | object referring | \u2717           | ScanNet + 3RScan | 354K       | 15M          | 39M                    |\\n| LEO-instruct  | scene captioning | \u2717           | 3RScan     | 20K        | 3.3M         | 4.4M                   |\\n|               | 3D captioning | \u2717           | ScanNet    | 37K        | 821K         | 3M                     |\\n|               | 3D QA       | \u2717           | ScanNet + 3RScan | 83K        | 177K         | 4M                     |\\n|               | 3D dialogue | \u2717           | 3RScan     | 11K        | 1.1M         | 8.3M                   |\\n|               | task planning | \u2717           | 3RScan     | 14K        | 1.9M         | 2.7M                   |\\n|               | navigation  | \u2713            | MP3D       | 60K        | 11.4M        | 272M                   |\\n|               | manipulation | \u2713            | *CLIPort   | 300K       | 7.2M         | 734M                   |\\n\\n2.3 T RAINING & INFERENCE\\n\\nWe formulate the learning objective of LEO following (Brown et al., 2020; Raffel et al., 2020) in a prefix language modeling fashion. For a batch $B$ of token sequence $s$, we optimize LEO via:\\n\\n$$L(\\\\theta, B) = -\\\\frac{1}{|B|} \\\\sum_{b=1}^{T} \\\\sum_{t=1}^{X} \\\\log p_\\\\theta(s(b,t) | s(b, <t \\\\) prefix, ..., s(b,L) prefix), (2)$$\\n\\nwhere $s$ prefix denotes the prefix token (from system message to instruction) in (1). During training, we freeze the pretrained 3D point cloud encoder and the LLM and finetune the 2D image encoder, the Spatial Transformer and the LoRA parameters. In total, LEO has ~7B parameters and ~142M of them will be tuned. During inference, we use beam search to generate textual responses. For tasks that require action commands, we map the textual outputs to action commands as discussed in Sec. 2.1. More details on the model and training can be found in Appendix D.\\n\\n### 3D DATASETS\\n\\nSince LEO is a generalist agent that receives multi-modal inputs and follows instructions, we adopt the two-stage training proposed in Liu et al. (2023b) and categorize the data into two sets: (i) LEO-align that focuses on 3D vision-language alignment at object-level and scene-level, to bridge the gap between 3D scene representations and natural language; and (ii) LEO-instruct that targets at 3D VLA instruction tuning for endowing LEO with the generalist capability of accomplishing myriad tasks in the 3D world including perceiving, reasoning, and acting. We provide the statistics of the two separate splits of data in Tab. 1. Examples of both datasets can be found in Appendix C.\\n\\n#### 3.1 LEO-align: 3D Vision-Language Alignment\\n\\nIn LEO-align, we focus on 3D vision-language alignment. We follow the alignment method proposed by BLIP-2 (Li et al., 2023d) and train the model to follow the instructions for captioning given 3D input. As a result, we consider collecting the following types of 3D caption data:\\n\\n- **Object-level caption.** To facilitate object-level grounding of detailed object attributes, we leverage Cap3D (Luo et al., 2023), which contains language descriptions for objects in Objaverse (Deitke et al., 2023). Given a single 3D object as input, LEO will be asked to predict its caption.\\n\\n- **Object-in-the-scene caption.** For a better understanding of how an object can be related to others (spatial relations, etc.) when situated in a 3D scene, we collect referring expressions of objects in scenes from existing datasets, including ScanScribe (Zhu et al., 2023c) and ReferIt3D (Achlioptas et al., 2020). Further, we generate additional object-referring expressions on 3RScan (Wald et al., 2019) scenes by prompting LLMs (details in Appendix A.1). During alignment, LEO needs to predict these referring expressions given the object-centric 3D input of the scene and the referred object.\\n\\n- **Scene-level caption.** Finally, we encourage LEO to capture scene-level descriptions of a 3D scene. These scene-level captions focus on global information depicting key objects in the scene as well as their attributes and functionalities, relations among multiple objects, and room types and styles. We leverage scene graph annotations (Wald et al., 2019) and promptLLMs to produce a total of ~20K captions. To further increase caption diversity, we propose a subgraph sampling strategy to prevent...\"}"}
{"id": "4QaKdsh15T", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dialogue (O-CoT):\\nDialogue Context: high level task: organize the bedroom. low level task: check some objects.\\nHuman: I need help organizing my bedroom. Can you assist me?\\nThought: no specific objects\\nAssistant: Of course! What specifically do you need help with?\\nHuman: Can you check if there are any clean clothes hanging on the wall?\\nThought: clothes-13\\nAssistant: I'm sorry, but the clothes hanging on the wall are messy.\\n\\nHuman: Thanks for your help!\\nThought: no specific object\\nAssistant: You're welcome! If you need any further assistance, feel free to ask.\\n\\nObject Scene Caption:\\nThe showcase is supported by the wall and positioned behind, close to, and to the left of the shelf. The wall is flat, architectural, and dark in color. The shelf is made of wood and has a low size.\\n\\nScene Caption:\\nIn this room, there is a wooden floor that is clean and flat. A tall wardrobe stand on the right side of a desk, close to a basket. The wardrobe is in front of a chair, a bed, and behind a bedside table. Clothes are hanging on a white wall, to the right of a closed curtain. The bed is wide, clean, and covered with blue blanket. The room has a comfortable and organized setting with functional furniture.\\n\\nPlanning:\\nHigh-Level Task: Organize and tidy up the bedroom.\\nLow-Level Actions:\\n1. Clean the floor by sweeping to remove any dirt.\\n2. Make the bed by arranging the blanket and pillows.\\n3. Place any loose items or belongings into the basket.\\n4. Arrange items on the shelves and showcase in a tidy way.\\n\\nFigure 2:\\nAn overview of our proposed LLM-assisted 3D-language data generation and examples of LEO datasets. (Top-left) Messages with 3D scene graphs, including object attributes and relations in a phrasal form, used for providing scene context when prompting LLM. (Top-right) The human-defined refinement procedures conducted over raw LLM responses to improve data quality. (Bottom) Examples of LLM-assisted generation in LEO-align and LEO-instruct. Thoughts, colored in gray, will be removed after refinements.\\n\\n3.2 LEO-INSTRUCT: INSTRUCTION TUNING FOR TASKS IN THE 3D WORLD\\nAfter alignment, LEO will be tuned to follow instructions and accomplish various 3D VLA tasks. Below, we provide a comprehensive illustration of the data preparation process for these tasks and an overview of generated data in Fig. 2. We list the corresponding instructions in Appendix C.\\n\\n3D captioning. The task is to produce a generic caption given 3D input. We adopt the Scan2Cap dataset (Chen et al., 2021), which is based on the ScanNet (Dai et al., 2017) 3D scenes and covers various levels (object-level and scene-level) and aspects (attributes, relations, etc.) of scene details.\\n\\n3D question answering. The 3D-QA task is an extension of VQA (Antol et al., 2015) to 3D scenes with a focus on 3D knowledge, ranging from spatial relations to functionalities of objects. For this task, we first aggregate two existing 3D-QA datasets: ScanQA (Azuma et al., 2022) and SQA3D (Ma et al., 2023). To further generate questions concerning rich 3D knowledge, we prompt LLMs to generate ~35K QA pairs on 3RScanQA with our quality refinement techniques discussed in Sec. 3.3.\\n\\n3D dialogue. The goal of this task is to support natural conversations between LEO and users about a given 3D scene. This task necessitates coherence and continuity across multiple rounds of conversational interactions. We build such dialogues on 3RScan scenes by prompting LLMs with a variant of the Chain-of-Thought prompting method discussed in Sec. 3.3 to facilitate diverse dialogues about relevant and accurate details about the 3D scene. In total, ~11K dialogues are collected.\\n\\nScene-aware task planning. In this task, LEO is required to decompose high-level tasks into step-by-step low-level plans given 3D scenes. We expectLEO to generate feasible plans based on the current 3D scene and ground its inherent common sense knowledge about procedures to the scene configurations, including, objects, their attributes, relations, and functional characteristics, etc. By prompting LLMs, we end up collecting ~14K task-plan pairs on 3RScan scenes.\\n\\nEmbodied navigation. We follow imitation learning setting in Habitat-web (Ramrakhya et al., 2022) for the embodied navigation task. We choose ObjNav, where LEO needs to map navigation instructions (e.g., \\\"find bed\\\"), object-centric 3D input, and an egocentric 2D input into discrete habitat motor commands. For simplicity, we use shortest path navigation trials rather than human demonstrations for learning as they are less noisy and therefore easier to learn when provided with\"}"}
{"id": "4QaKdsh15T", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the 3D scene. In total, we generate ~60K navigation episodes out of the MP3D ObjNav training scenes (Savva et al., 2019) for this task.\\n\\nRobotic manipulation. We employ a subset of the manipulation tasks introduced in CLIPort (Shridhar et al., 2021). The input of this task includes instructions, egocentric 2D observations, and object-centric 3D information. As discussed in Sec. 2.1, we discretize the continuous action space of CLIPort into bins to unify the action decoding of navigation and manipulation (more details in Appendix B). We generate 100K demonstrations for each selected manipulation task.\\n\\n3.3 LLM-ASSISTED 3D-LANGUAGE DATA GENERATION\\n\\nAs mentioned above, at the core of producing a large proportion of LEO-align and LEO-instruct is the assistance of LLMs. We now detail the key techniques of prompting LLMs (more specifically, ChatGPT) to generate 3D-text data. An overview can be found in Fig. 2.\\n\\nScene-graph-based prompting. We use 3D scene graph from 3DSSG (Wu et al., 2021) to provide scene contexts in prompts. Compared to recent efforts that utilize object boxes (Yin et al., 2023; Hong et al., 2023; Wang et al., 2023d), we observed that our method provides high-quality object attributes and spatial relation information among objects, allowing LLMs to capture and generate more accurate and relevant 3D details (see comparisons in Appendix A.6). To further improve data quality in open-ended generation and reduce the hallucination of LLMs (Bang et al., 2023), we propose the Object-centric chain of thought (O-CoT) prompting that requires the LLM to explicitly provide the label and ID of object candidates as thoughts during question and dialogue generation. We provide examples of O-CoT in Fig. 2 and comparative experiments to verify the effectiveness of O-CoT on improving answer reliability in Appendix A.2. We also utilize subgraph sampling to further enhance the diversity of 3D scene graph (see details in Appendix A.5).\\n\\nRefinement procedures. We pass raw LLM-generated responses into several human-defined filtering procedures based on the 3D scene graph. Notably, negative responses (e.g., lacking necessary information to answer) will be removed; unnatural narratives will be rewritten. For generated text that involves logical reasoning (e.g., counting) or hallucination, we manually fix the wrong responses based on the information provided by the 3D scene graph. We provide details about these procedures in Appendix A.3 and statistics in Appendix A.4.\\n\\n4 CAPABILITIES AND ANALYSES\\n\\nWe present a comprehensive demonstration of LEO's capabilities by evaluating it on the full spectrum of embodied 3D tasks encompassing perceiving, grounding, reasoning, planning, and acting. We provide both quantitative comparisons between LEO and competitive task-specific baselines and qualitative visualizations (see in Fig. 3) to showcase the power of LEO as an embodied generalist agent. We provide additional experimental details about the model and implementation in Appendix D.\\n\\nWe further ablate LEO with various data configurations and analyze the scaling law.\\n\\n4.1 3D VISION-LANGUAGE UNDERSTANDING AND EMBODIED REASONING\\n\\nOverview. Understanding and reasoning about object attributes, object relations, and other facets of 3D scenes from an agent's egocentric perspective is a fundamental capability of an embodied generalist agent in the 3D world. We investigate how well can LEO perform 3D VL understanding and embodied reasoning tasks, especially when being compared against task-specific models and existing generalist agents. Specifically, we consider three renowned 3D tasks: 3D captioning on Scan2Cap (Chen et al., 2021), 3D QA on ScanQA (Azuma et al., 2022), and 3D embodied reasoning on SQA3D (Ma et al., 2023). By prompting LEO to follow instructions for these tasks, we follow the standard evaluation metric to report conventional captioning scores (CIDEr, BLEU, METEOR, and ROUGE) and SentenceSim (Reimers & Gurevych, 2019) for open-ended VL generation, as well as exact-match accuracies for QA tasks. Following 3D-VisTA (Zhu et al., 2023c), we use object proposals from Mask3D (Schult et al., 2022) in our object-centric 3D encoder.\\n\\nBaselines. For quantitative comparisons, we include both task-specific approaches and generalist models: 1) state-of-the-art specialists in 3D dense captioning (Chen et al., 2021; Cai et al., 2022; Chen et al., 2023); 2) state-of-the-art specialists in 3D QA (Azuma et al., 2022; Ma et al., 2023); 3) task-specific fine-tuned generalist models like 3D-VisTA (Zhu et al., 2023c) and 3D-LLM (Hong et al., 2023).\"}"}
{"id": "4QaKdsh15T", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Quantitative comparison with state-of-the-art models on 3D VL understanding and embodied reasoning tasks. \u201cC\u201d stands for \u201cCIDEr\u201d, \u201cB-4\u201d for \u201cBLEU-4\u201d, \u201cM\u201d for \u201cMETEOR\u201d, \u201cR\u201d for \u201cROUGE\u201d, \u201cSim\u201d for sentence similarity, and \u201cEM@1\u201d for top-1 exact match. The n-gram metrics for Scan2Cap are governed by IoU@0.5.\\n\\n| Model                        | C  | B-4 | M  | R  | Sim | C  | B-4 | M  | R  | Sim | EM@1 | EM@1 |\\n|------------------------------|----|-----|----|----|-----|----|-----|----|----|-----|------|------|\\n| Scan2Cap (val) (Chen et al., 2021) | 35.2 | 22.4 | 21.4 | 43.5 | - | - | - | - | - | - | 41.0 |\\n| 3DJCG (Cai et al., 2022)     | 47.7 | 31.5 | 24.3 | 51.8 | - | - | - | - | - | - | - |\\n| Vote2Cap-DETR (Chen et al., 2023) | 61.8 | 34.5 | 26.2 | 54.4 | - | - | - | - | - | - | - |\\n| ScanRefer+MCAN (Chen et al., 2020) | - | - | - | - | 55.4 | 7.9 | 11.5 | 30.0 | 18.6 | - | - |\\n| ClipBERT (Lei et al., 2021)  | - | - | - | - | - | - | - | - | - | - | 43.3 |\\n| ScanQA (Azuma et al., 2022)  | - | - | - | - | - | 64.9 | 10.1 | 13.1 | 33.3 | 21.1 | 47.2 |\\n| 3D-VisTA (Zhu et al., 2023c) | 66.9 | 34.0 | 27.1 | 54.3 | 53.8 | 69.6 | 10.4 | 13.9 | 35.7 | 22.4 | 48.5 |\\n| 3D-LLM (FlanT5) (Hong et al., 2023) | - | - | - | - | - | 69.4 | 12.0 | 14.5 | 35.7 | 20.5 | - |\\n| LEO                          | 68.4 | 36.9 | 27.7 | 57.8 | 54.7 | 80.0 | 11.5 | 16.2 | 39.3 | 36.6 | 53.7 |\\n\\nTable 3: Results on CLIPort robot manipulation. We compare with results from Shridhar et al. (2021). seen indicates in-domain tasks. unseen marks OOD tasks with novel colors or objects.\\n\\n| Task               | Transporter | CLIP-only | RN50-BERT | CLIPort (single) | CLIPort (multi) | LEO |\\n|--------------------|-------------|-----------|-----------|------------------|-----------------|-----|\\n| seen/unseen        | 48.4/52.3  | 90.2/71.0 | 94.0/56.1 | 98.0/75.2        | 90.0/62.8       | 98.8/75.2 |\\n| seen/unseen        | 46.3/37.3  | 64.7/18.7 | 94.0/56.1 | 97.7/45.8        | 84.4/70.3       | 98.8/75.2 |\\n| seen/unseen        | 64.7/18.7  | 97.7/45.8 | 91.8/23.8  | 98.0/75.2        | 84.4/70.3       | 98.8/75.2 |\\n\\nTable 4: Results on object navigation. We compare LEO with similar imitation learning agents in Habitat-web (H.w.) (Ramrakhya et al., 2022) and CortexBench (VC-1) (Majumdar et al., 2023). S: success rate; L: SPL.\\n\\n| Model               | H.w. (shortest) | H.w. (70k demo) | VC-1 (ViT-B) | LEO |\\n|---------------------|------------------|-----------------|--------------|-----|\\n| MP3D-val            | 4.4              | 35.4            | -            | -   |\\n| HM3D-val            | 2.2              | 10.2            | 57.1         | 23.1 |\\n| S                   | 4.4              | 35.4            | 10.2         | 23.1 |\\n| L                   |                  |                 | 57.1         | 15.2 |\\n\\nTo the best of our knowledge, LEO is the first model that, in stark contrast to prior models, can handle the aforementioned VL tasks in a unified architecture without additional fine-tuning. This lends greater credence to LEO\u2019s comparative superiority.\\n\\nResults & analysis. As shown in Tab. 2, LEO surpasses state-of-the-art task-specific and task-specific fine-tuned models significantly on both 3D dense captioning and 3D QA tasks. In contrast to the specialist models that utilize task-specific heads, we demonstrate our LLM-based approach not only affords the flexibility of generating open-ended responses but also can achieve excellent scores in terms of standard metrics. Notably, considering the difference between close-set classification (e.g., 3D-VisTA) and open-ended text generation, we refine the protocol of exact match (see details in Appendix H.1). On the other hand, compared with the complicated 3D feature aggregation in 3D-LLM, we suggest that object-centric 3D representation is a simple yet effective option to connect 3D scenes with LLM while harnessing the inherent knowledge of LLM.\\n\\n4.2 C HATTING AND PLANNING ABOUT A 3D SCENE\\n\\nOverview. Upon the 3D VL understanding and reasoning, we anticipate LEO to support more sophisticated and grounded interaction with human users, i.e., responding to complex multi-round user instructions in the 3D world. To verify these capabilities, we choose two tasks: 3D dialogue and scene-aware task planning. We provide qualitative examples of unseen scenarios from the held-out test sets of LEO-instruct, highlighting LEO\u2019s merits of instruction following and scene-grounded responses. We defer the quantitative results of dialogue and planning to our ablation study in Sec. 4.4. Quantitative comparison with other approaches is infeasible due to the lack of a common benchmark.\\n\\nResults & analysis. As shown in Fig. 3, LEO is capable of generating high-quality responses to complete the tasks of dialogue and planning. We highlight two features: 1) The responses of LEO are precisely grounded to the 3D scenes. In particular, the proposed plan by LEO contains concrete objects that are present in the scenes, as well as concrete actions regarding these objects. 2) The responses of LEO incorporates rich informative spatial relations. Such information is necessary to refer to specific objects in complex 3D scenes and affords considerable assistance for humans.\"}"}
{"id": "4QaKdsh15T", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 26: Examples from our datasets. Continued.\\n\\nLEO-instruct\\n\\nembodied navigation\\n\\nUSER: The task is navigation. Your goal is to find counter by moving around in the scene. Past actions: <31999> <31999> <31999> <31999>.\\n\\nASSISTANT:\\n\\nrobotic manipulation\\n\\nUSER: The task is manipulation. Your goal is to put the blue blocks in a green bowl. Past actions: <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671}\"}"}
