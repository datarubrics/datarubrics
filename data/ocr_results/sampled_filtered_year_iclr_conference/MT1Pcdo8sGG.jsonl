{"id": "MT1Pcdo8sGG", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ABSTRACT\\nCan a machine learn machine learning? We propose to answer this question using the same criteria we use to answer a similar question: can a human learn machine learning? We automatically answer final exams in MIT\u2019s, Harvard\u2019s and Cornell\u2019s large machine learning courses and generate new questions at a human level. Recently, program synthesis and few-shot learning solved university-level problem set questions in mathematics and STEM courses at a human level. In this work, we solve questions from final exams that differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We provide a new dataset and benchmark of questions from machine learning final exams and code for automatically answering these questions and generating new questions. To make our dataset a reproducible benchmark, we use automatic checkers for multiple choice questions, questions with numeric answers, and questions with expression answers, and evaluate a large free language model, Meta\u2019s OPT, and compare the results with Open AI\u2019s GPT-3 and Codex. A student survey comparing the quality, appropriateness, and difficulty of machine-generated questions with human-written questions shows that across multiple aspects, machine-generated questions are indistinguishable from human-generated questions and are suitable for final exams. We perform ablation studies comparing zero-shot learning with few-shot learning, chain-of-thought prompting, GPT-3 and OPT pre-trained on text and Codex fine-tuned on code on a range of machine learning topics and find that few-shot learning methods perform best. We make our data and code publicly available for the machine learning community.\\n\\n1 INTRODUCTION\\nCan a machine learn machine learning? This work presents a new dataset of machine learning final exams with 646 question parts and a benchmark of baselines using transformers and their respective grade performance, demonstrating that the best baseline performs at a human level. In university-level STEM courses, students complete assignments (including problem sets and labs) and exams throughout the course. Recent work has opened the door for a machine to solve course problem sets (Drori et al., 2022) using language models and few-shot learning. However, final exams remain challenging, and this work is the first to present a structured dataset of machine learning finals and a benchmark of baseline methods for answering them. Final exams differ from problem sets because they serve as a benchmark of cumulative understanding of material learned over a semester and evaluate the students\u2019 depth and breadth of expertise. Further, questions on final exams are longer, have multiple parts, span a broader set of topics, and are more complicated and nuanced. All the above holds for MIT\u2019s and Cornell\u2019s Introduction to Machine Learning classes and Harvard\u2019s Machine Learning course. These are undergraduate courses with hundreds of students each semester, making them the largest undergraduate courses offered. Introduction to Machine Learning is a core class in the computer science program. The prerequisites for the course are Python Programming and Multivariate Calculus, with Introduction to Algorithms and Linear Algebra recommended. The class typically consists of weekly exercises, labs, quizzes, homework, a midterm, and a final exam. There were no final exams in at MIT for Fall 2020 and Spring 2020 due to COVID-19. Introduction to Machine Learning final exams differ from problem sets in several ways, and the experience of solving each varies. First, finals are long, containing around nine questions with\"}"}
{"id": "MT1Pcdo8sGG", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\naround seven parts each. Final exam questions are also multifaceted and multi-stepped: different parts of a single question require applying different concepts and problem-solving skills, and parts may build upon each other. While weekly problem sets focus on a single topic, finals span topics from the entire semester. Further, final questions are often story-based problems that may require mathematical modeling. Due to the time constraint of these exams, finals are also designed to test core understanding and application of course material over rote calculations. Thus, asking a machine to answer questions from finals allows for testing whether the model is able to learn a breadth and depth of topics beyond problem sets.\\n\\nIn this work, we present a new dataset curated from final exams of MIT's and Cornell's Introduction to Machine Learning course and Harvard's Machine Learning class, totaling 646 question parts spanning the broad range of topics in machine learning: regression, classifiers, logistic regression, features, loss functions, neural networks, convolutional neural networks (CNNs), Markov decision processes (MDPs), recurrent neural networks (RNNs), reinforcement learning, clustering, decision trees, model selection, ensemble methods, Bayesian networks, hidden Markov models (HMMs), and optimization. Our dataset covers the finals given at MIT for semesters of Fall 2017, Spring 2018, Fall 2018, Spring 2019, Fall 2019, Spring 2021, Fall 2021, and Spring 2022, Harvard Spring 2015 and Spring 2021, and Cornell Spring 2017 and Fall 2018. Due to the COVID-19 pandemic, no finals were in the MIT course during 2020.\\n\\nTable 1: MIT Spring 2021, Question 10.a: question part, solution, Codex Zero-Shot answer, program output, GPT-3 Zero-Shot answer, and GPT-3 Zero-Shot with CoT prompt answer.\\n\\n| Topic | Classifiers |\\n|-------|-------------|\\n| Question Part | Given a set of data $D_{train} = \\\\{(x_i, y_i)\\\\}$, a weighted nearest neighbor regressor has the form $h(x; \\\\theta) = \\\\frac{P(x_i, y_i) \\\\in D_{train}}{f(x, x_i; \\\\theta) y_i}$ $P(x_i, y_i) \\\\in D_{train}}$ $f(x, x_i; \\\\theta) y_i}$. A typical choice for $f$ is $f(x, x_i; \\\\theta) = e^{-\\\\theta \\\\|x - x_i\\\\|^2}$ where $\\\\theta$ is a scalar and $\\\\|x - x_i\\\\|^2 = \\\\sum_{j=1}^{d} (x_j - x_i)^2$. Assume our training data $D_{train} = ((1, 1), (2, 2), (3, 6))$. What is $h(10; 0)$? That is, letting $\\\\theta = 0$, what is our prediction for $x = 10$? |\\n\\n| Solution | 3 |\\n|----------|---|\\n| Codex Zero-Shot Answer | import numpy as np\\ndef f(x, x_i, theta):\\n    return np.exp(-theta * np.linalg.norm(x - x_i)**2)\\ndef h(x, theta):\\n    D = np.array([\\\\(1, 1\\\\), \\\\(2, 2\\\\), \\\\(3, 6\\\\)])\\n    numerator = 0\\ndenominator = 0\\nfor x_i, y_i in D:\\n    numerator += f(x, x_i, theta) * y_i\\ndenominator += f(x, x_i, theta)\\nreturn numerator / denominator\\nprint(h(10, 0)) |\\n| Program Output | 3.0 |\\n| 2 |\"}"}
{"id": "MT1Pcdo8sGG", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We verify that our results are not due to overfitting by including MIT finals from 2021 and 2022 that are unavailable online. Also, the MIT Spring 2022 final exam was given after GPT-3 and Codex were last updated, which means that the models were not trained on this data. The final exam questions have many parts, each posing a new problem, and each question in the dataset corresponds to one part. The questions in the finals are varied in topics and solution types. Solutions are primarily open-ended questions with some true/false and multiple-choice questions on theory, math, and code implementations.\\n\\nWe make the dataset publicly available and welcome others to use it to aid in developing and assessing new language models and methods. Due to the diversity of Intro to ML final questions, our dataset uniquely assesses advanced problem-solving and reasoning skills in machine learning, math, and natural language processing. This dataset opens the door to achieving breakthroughs in machine learning performance in machine learning final exams. In addition to the dataset, we present a benchmark using several baseline methods. We apply zero-shot and few-shot learning to GPT-3 and Codex, adding chain-of-thought prompting for GPT-3. We find that few-shot learning methods perform best. As shown in Table 2 the best performing methods pass the final exams, and their grade is comparable with human grades of MIT students on the same machine learning finals evaluated by the same human graders. We generate new final exam questions that are indistinguishable from human-written questions.\\n\\n1.1 RELATED WORK\\n\\nThere is often thought that humans are generalists, whereas machines are specialists. However, large language models based on transformers such as GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), and PaLM (Chowdhery et al., 2022), also called foundation models, are generalist learners. Specifically, in our setting, while humans care about the number of topics in an exam and therefore find finals more difficult than problem sets, foundation models effortlessly scale to many topics without re-training. Language models may be pre-trained on text and fine-tuned on specific datasets such as code, for example OpenAI's Codex (Chen et al., 2021), which allows generating programs from text. There are several ways to improve the mathematical reasoning ability of language models: (1) using chain-of-thought (CoT) prompting (Kojima et al., 2022; Wei et al., 2022), (2) using the top-k ranking solutions (Li et al., 2022) and merging them by voting (Wang et al., 2022) or least-to-most prompting (Zhou et al., 2022), and (3) using program synthesis and few-shot learning to generate code that answers questions (Drori et al., 2022).\"}"}
{"id": "MT1Pcdo8sGG", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Generating new questions: example of a new question for each topic automatically generated and the closest question in the dataset based on cosine similarity of the questions embeddings.\\n\\n| Topic Question | Similarity |\\n|----------------|------------|\\n| Logistic Regression Generated Question: \u201cIn logistic regression, we use the sigmoid function to \\\"squish\\\" the output of our linear function to be between 0 and 1. What is the output of the sigmoid function when the input is 0?\u201d | 0.810 |\\n| Closest Question: \u201cConsider the simplest of all neural networks, consisting of a single unit with a sigmoid activation function: $h(x; w) = \\\\sigma(w_0 + w_1 x)$ where $\\\\sigma(z) = (1 + \\\\exp(-z))^{-1}$ Let's start with a classifier defined by $w_0 = -1$ and $w_1 = 1$. Which range of input values $x$ are classified as positive? Which as negative?\u201d | |\\n| Features Generated Question: \u201cYou are working on a project to predict how successful a given Kickstarter campaign will be, based on information about the campaign (such as the amount of money pledged, the number of backers, the category of the project, the country where the project is based, and so forth). One of the features you want to use is the project's title. What is a good way to encode the project's title for use in a machine-learning predictor?\u201d | |\\n| Closest Question: \u201cMac O'Larnin is considering selling an app on Frugal Play. You have a friend with inside info at Frugal, and they're able to share data on how previous apps have performed on the store. Mac decides that he will learn a neural network with no hidden layer (i.e., consisting only of the output layer). He needs help in figuring out the precise formulation for machine learning. What is the best way to encode the app characteristic 'Suitable for people ages (2\u20134, 5\u201310, 11\u201315, 16 and over)' as a feature for an input to the neural network? Choose from among the following: multiple unary features (one-hot encoding), multiple binary features (thermometer encoding), an integer or real-valued feature. Also give the exact function that maps each input to its corresponding feature(s).\u201d | 0.761 |\"}"}
{"id": "MT1Pcdo8sGG", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Generating new questions: example of a new question for each topic automatically generated and the closest question in the dataset based on cosine similarity of the questions embeddings.\\n\\n| Topic                  | Question                                                                 | Similarity |\\n|------------------------|--------------------------------------------------------------------------|------------|\\n| Loss Functions         | Generated Question: \\\"We have a data set of images, each of which is either a picture of a dog or a picture of a cat. We want to train a binary classifier to distinguish between the two classes. Which of the following is a better choice of loss function for this task: (a) The 0-1 loss, which gives a loss of 1 if the classifier predicts the wrong class and a loss of 0 otherwise. (b) The cross-entropy loss, which is given by -log(p) if the correct class is predicted and -log(1-p) otherwise, where p is the predicted probability of the correct class. | 0.790      |\\n|                        | Closest Question: \\\"Consider the following 2D dataset in (x,y) format: ((1,-1), +1), ((1,1), +1), ((1,2.5),+1), ((2,-2),-1), ((2,1),+1),((2,3),+1),((5,-1),-1),((5,-2),-1). Break ties in distance by choosing the point with smaller $x_1$ coordinate, and if still tied, by smaller $x_2$ coordinate. Compute the leave-one-out cross validation accuracy of the 3-nearest-neighbor learning algorithm on this dataset.\\\" |            |\\n| Neural Networks        | Generated Question: \\\"Consider a neural network with two hidden layers, each with two units. The input layer has two units, and the output layer has one unit. The hidden layers use the sigmoid activation function, and the output layer uses the linear activation function. The weights from the input layer to the first hidden layer are $w_{11} = 1$, $w_{12} = 1$, $w_{21} = 1$, and $w_{22} = 1$. The weights from the first hidden layer to the second hidden layer are $w_{11} = 1$, $w_{12} = 1$, $w_{21} = 1$, and $w_{22} = 1$. The weights from the second hidden layer to the output layer are $w_{11} = 1$, $w_{21} = 1$. The bias terms are all zero. What is the output of the neural network for the input $x_1 = 1$, $x_2 = 1$?\\\" | 0.880      |\\n|                        | Closest Question: \\\"A neural network is given as $Z_1 = X \\\\ast W_1$, $A_1 = f_1(Z_1)$, $Z_2 = W_2 \\\\ast A_1$, $\\\\hat{y} = f_2(Z_2)$. Specifically, the input $X$ is a $4 \\\\times 1$ column vector, $\\\\hat{y}$ is a $1 \\\\times 1$ scalar. $W_2$ is a $3 \\\\times 1$ vector. We also know that, $Z_1 = (W_1)^T X$ and $Z_2 = (W_2)^T A_1$. What are the dimensions of $Z_2$?\\\" |            |\\n| CNNs                  | Generated Question: \\\"Suppose we have a 3x3 image and we use a 2x2 filter with stride 1. What are the dimensions of the output image?\\\" |            |\\n|                        | Closest Question: \\\"A neural network is given as $Z_1 = X \\\\ast W_1$, $A_1 = f_1(Z_1)$, $Z_2 = W_2 \\\\ast A_1$, $\\\\hat{y} = f_2(Z_2)$. There is only one data point which is: $X = [1, 1, 1, 1]^T$ and $y = [1]$. If $W_1$ and $W_2$ are both matrices/vectors of all ones, what is the resulting Loss where the Loss = $(y - \\\\hat{y})^2$?\\\" | 0.895      |\"}"}
{"id": "MT1Pcdo8sGG", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 9: Generating new questions: example of a new question for each topic automatically generated and the closest question in the dataset based on cosine similarity of the questions embeddings.\\n\\n| Topic          | Question                                                                 | Similarity |\\n|----------------|--------------------------------------------------------------------------|------------|\\n| MDPs           | Generated Question: \\\"Consider an MDP with four states, called A, B, C, and D, and with two actions called Move and Stay. The discount factor $\\\\gamma = 0.9$. Here is a reminder of the Q-learning update formula, based on experience tuple $(s, a, r, s')$: $Q(s, a) := (1 - \\\\alpha)Q(s, a) + \\\\alpha r + \\\\gamma \\\\max_{a'} Q(s', a')$ |\\n|                | Closest Question: \\\"Consider an MDP with four states, called A, B, C, and D, and with two actions called Move and Stay. The discount factor $\\\\gamma = 0.9$. Here is a reminder of the Q-learning update formula, based on experience tuple $(s, a, r, s')$: $Q(s, a) := (1 - \\\\alpha)Q(s, a) + \\\\alpha r + \\\\gamma \\\\max_{a'} Q(s', a')$ |\\n| RNNs           | Generated Question: \\\"Consider the following RNN: $s_t = \\\\tanh(w_1 x_t + w_2 s_{t-1} + b_1)$, $y_t = w_3 s_t + b_2$. Assume $s_0 = 0$ and $b_2 = 0$. What values of $w_1$, $w_2$, $w_3$ and $b$ would generate output sequence $[0, 0, 0, 1, 1, 1, 1]$ given input sequence $[0, 0, 0, 1, 0, 1, 0]$ |\\n|                | Closest Question: \\\"Ronnie makes a simple RNN with state dimension 1 and a step function for $f_1$, so that $s_t = \\\\text{step}(w_1 x_t + w_2 s_{t-1} + b)$ where $\\\\text{step}(z) = 1$ if $z > 0$ and equals 0 otherwise, and where the output $y_t = s_t$. Assuming $s_0 = 1$, we want to generate output sequence $[0, 0, 0, 1, 1, 1, 1]$ given input sequence $[0, 0, 0, 1, 0, 1, 0]$. Rennie thinks this is not possible using Ronnie's architecture. Rennie makes an argument based on the relationships in the table above. Is Rennie right?\\\" |\\n| Reinforcement Learning | Generated Question: \\\"What is the tabular Q-learning update equation, based on experience tuple $(s, a, r, s')$?\\\" |\\n|                | Closest Question: \\\"b is the bias for classifier network. What are dimensions of b for multi k-class classification?\\\"\"}"}
{"id": "MT1Pcdo8sGG", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Generating new questions: example of a new question for each topic automatically generated and the closest question in the dataset based on cosine similarity of the questions embeddings.\\n\\n| Topic | Question Similarity |\\n|-------|---------------------|\\n| Clustering | Generated Question: \\\"Suppose that we have a dataset with $n$ data points, $k$ clusters, and $d$ features. After running the $k$-means algorithm, the within-cluster sum of squared errors (WCSS) is given by:\\n\\\\[\\n\\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\| x_i - \\\\mu_{y_i} \\\\|^2\\n\\\\]\\nwhere $y_i$ is the cluster label of the $i$th data point, and $\\\\mu_{y_i}$ is the cluster center associated with the $i$th data point. The within-cluster sum of squared errors (WCSS) is a measure of how well the clusters fit the data.\\nSuppose that we have two datasets, $X_1$ and $X_2$, where $X_1$ has $n_1$ data points and $X_2$ has $n_2$ data points. We run the $k$-means algorithm on both datasets. We find that the WCSS for $X_1$ is smaller than the WCSS for $X_2$. Does this imply that the clusters for $X_1$ are better than the clusters for $X_2$? Why or why not?\\\"\\n| Closest Question: \\\"Consider the following 2D dataset in (x,y) format:\\n\\\\((1,-1), +1\\\\), \\\\((1,1), +1\\\\), \\\\((1,2.5),+1\\\\), \\\\((2,-2),-1\\\\), \\\\((2,1),+1\\\\),\\\\((2,3),+1\\\\),\\\\((5,-1),-1\\\\),\\\\((5,-2),-1\\\\). We will construct a tree using a greedy algorithm that recursively minimizes weighted average entropy. Recall that the weighted average entropy of a split into subsets A and B is:\\n\\\\[\\n\\\\text{fraction of points in } A \\\\cdot H_R_{A,j,s} + \\\\text{fraction of points in } B \\\\cdot H_R_{B,j,s}\\n\\\\]\\nwhere the entropy $H(R_m)$ of data in a region $R_m$ is given by\\n$H(R_m) = -\\\\sum_{k=1}^{\\\\hat{P}_{mk}} \\\\hat{P}_{mk} \\\\log_2 \\\\hat{P}_{mk}$. The $\\\\hat{P}_{mk}$ is the empirical probability, which is in this case the fraction of items in region $m$ that are of class $k$. Some facts that might be useful to you: $H(0) = 0$, $H(3/5) = 0.97$, $H(3/8) = 0.95$, $H(3/4) = 0.81$, $H(5/6) = 0.65$, $H(1) = 0$. Draw the decision tree that would be constructed by our tree algorithm for this dataset. Clearly label the test in each node, which case (yes or no) each branch corresponds to, and the prediction that will be made at each leaf. Assume there is no pruning and that the algorithm runs until each leaf has only members of a single class.\\\" |\\n\\nDecision Trees Generated Question: \\\"The Gini score is a measure of how often a randomly chosen element would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. It ranges from 0 to 1, with 0 meaning that there are no mislabeled elements and 1 meaning that the subset is perfectly mixed. Explain whether or not it would be a good idea to use the Gini score as a scoring function for pruning decision trees.\\\"\\n\\nClosest Question: \\\"There are different strategies for pruning decision trees. We assume that we grow a decision tree until there is one or a small number of elements in each leaf. Then, we prune by deleting individual leaves of the tree until the score of the tree starts to get worse. The question is how to score each possible pruning of the tree. Here is a definition of the score: The score is the percentage correct of the tree on a separate validation set. Explain whether or not it would be a good idea and give a reason why or why not.\\\"\"}"}
{"id": "MT1Pcdo8sGG", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: The number of question parts that do not rely on images, the number of points that do not rely on images, and the number of non-open question points, in finals for each semester. MIT Spring 2020 and MIT Fall 2020 did not have final exams due to COVID-19.\\n\\nTable 8: The number of questions parts that do not rely on images, number of points that do not rely on images, and number of non-open question points in the finals for each topic of the course.\\n\\nThe creation of new, high-quality questions by course instructors and TA is often a time-consuming, high-effort process. These new questions must be varied from past questions while still testing the same core concepts. We explore the potential of using GPT-3 to write exam content efficiently by using the dataset of exam questions to generate new questions automatically. We use questions from our dataset as prompts to create new high-quality questions not present in our dataset. We create a list of various questions in our curated dataset and use the resulting list to prompt GPT-3 to create a new question. The supplementary material demonstrates the results of this process for each topic in the course. The Appendix consists of new generated questions and the closest question from our dataset as measured by the cosine similarity of the embedding of each question. These new questions are diverse and qualitatively similar to questions on previous MIT final exams. This provides an efficient way for course TAs and instructors to generate new final questions.\\n\\n3.5.1 Student Survey\\n\\nTo evaluate the machine-generated questions, we conducted an anonymous online student survey comparing them with the human-written questions in terms of quality, appropriateness relative to\"}"}
{"id": "MT1Pcdo8sGG", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The course, and question difficulty. We surveyed 15 students who have taken the Introduction to Machine Learning course or its equivalent. The survey was optional and included informed consent, with the following description: \u201cWe are conducting a survey to assess the quality and difficulty of automatically generated questions for an introductory machine learning course final. You will be presented with a series of questions, either human-written (taken from an actual course final exam) or machine generated, but you will not be told the source of a given question. For each question, you will be asked (a) whether you think the question is human-written or machine-generated, (b) whether the question is appropriate for the given course final, and finally (c) how you would rate the difficulty of the question. Please carefully read each question and answer to the best of your ability.\u201d\\n\\nWe randomly sampled one generated question and its closest (measured by cosine similarity) original, human-written question for each of the twelve machine learning topics. Students were asked to read these 24 questions in the survey, mixed and presented randomly, and then answer three questions for each: (1) \u201cIs the question human-written or machine-generated?\u201d, (2) \u201cIs the question appropriate or not appropriate for the specific course final?\u201d, and (3) \u201cWhat is the question\u2019s difficulty level on a scale between 1 (easiest) and 5 (hardest)?\u201d. We ask the students to provide ratings and not to solve the questions. The results of our survey are as follows: Out of the human-written questions, students identified 56.11% of them correctly as human-written and 43.89% incorrectly as machine-generated. Of the machine-generated questions, students identified 45% of them correctly as machine-generated and 55% of them incorrectly as human-written. The difficulty ratings were between 1 (the easiest) and 5 (the hardest). Students rated machine-generated questions with a difficulty level of 2.55 with a 1.11 standard deviation and rated human-written questions with a difficulty level of 2.85 with a 1.12 standard deviation. Students rated machine-generated questions as appropriate 82.6% of the time and human-written questions as appropriate 85.0% of the time.\\n\\nThe conclusions we draw from the survey are that (1) survey participants considered human-written questions to be as likely to be human-written or machine-generated, and similarly, machine-generated questions were considered equally likely to be machine-generated as human-written, (2) survey participants considered the machine-generated questions slightly easier than human-written questions, and (3) survey participants considered machine-generated questions as appropriate as human-written questions. Based on these results, we conclude that across multiple aspects, the machine-generated questions are highly similar to human-generated questions and can be adapted to generate questions for machine learning courses.\\n\\n### 3.6 Implementation Details\\n\\nWe use the latest OpenAI GPT-3 and Codex models and do not re-train these very large language models. We fix all the hyperparameters of the models so that the answers are deterministic and reproducible. Specifically, we set both top P, which controls diversity, and sampling temperature, which controls randomness, to 0. The frequency and presence penalties are also set to 0, and we do not halt on any stop sequences. We allow diversity for generating new questions by setting the top P and temperature to 0. We run Codex with an upper bound of generating programs with 1024 tokens. We use the OpenAI text-davinci-002 and code-davinci-002 engines for generating text and programs. For few-shot-learning and question generation, we use the text-similarity-babbage-001 engine to embed the questions and find the closest questions in the dataset by cosine similarity. The running time for answering or generating each question part is a few seconds.\\n\\n### 4 Conclusions\\n\\nWe present a dataset and benchmark for answering and generating university-level final exams in machine learning. Machine performance and human performance are evaluated by the same graders and grading instructions, as well as by automatic checkers. A comparison of baselines shows that few-shot learning methods perform best across semesters and topics. A limitation of our work is that our benchmark does not consider questions that rely on images for their solution. This work may result in improving students learning for final exams, help course staff generate questions for finals, and compare levels of difficulty of exams across semesters and schools. This work includes final exams from MIT\u2019s and Cornell\u2019s Introduction to Machine Learning classes and Harvard\u2019s Machine Learning course. We hope this dataset and benchmark serve the machine learning community and advance the state-of-the-art in the field.\"}"}
{"id": "MT1Pcdo8sGG", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 1877\u20131901, 2020.\\n\\nMark Chen et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\\n\\nIddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L. Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, and Gilbert Strang. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences, 119(32), 2022.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.\\n\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.\\n\\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814, 2022.\\n\\nFanyi Qu, Xin Jia, and Yunfang Wu. Asking questions like educational experts: Automatically generating question-answer pairs on real-world examination data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\\n\\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\\n\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\\n\\nLeonard Tang, Elizabeth Ke, Nikhil Singh, Bo Feng, Derek Austin, Nakul Verma, and Iddo Drori. Solving probability and statistics problems by probabilistic program synthesis at human level and predicting solvability. In Proceedings of the International Conference on Artificial Intelligence in Education (AIED), 2022.\\n\\nSunny Tran, Pranav Krishna, Ishan Pakuwal, Prabhakar Kafle, Nikhil Singh, Jayson Lynch, and Iddo Drori. Solving machine learning problems. In Proceedings of the Asian Conference on Machine Learning (ACML), pp. 470\u2013485, 2021.\\n\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\\n\\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.\"}"}
{"id": "MT1Pcdo8sGG", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Generating new questions: example of a new question for each topic automatically generated and the closest question in the dataset based on cosine similarity of the questions embeddings.\\n\\n| Topic   | Question                                                                 | Similarity |\\n|---------|--------------------------------------------------------------------------|------------|\\n| Regression | Given a data set $D = \\\\{x_i, y_i\\\\}_{i=1}^n$, where $x_i \\\\in \\\\mathbb{R}^d$ and $y_i \\\\in \\\\mathbb{R}$. Let $X$ be a $d \\\\times n$ matrix in which the $x_i$ are the columns and let $Y$ be a $1 \\\\times n$ vector containing the values of $y_i$. Using the ordinary least-squares formula, we can compute $W_{ols} = (XX^T)^{-1}XY^T$. Using ridge regression, we can compute $W_{ridge} = (XX^T + \\\\lambda I)^{-1}XY^T$. We decide to try to use these methods to initialize a single-unit neural network with a linear activation function. Assume that $XX^T$ is neither singular nor equal to the identity matrix, and that neither $W_{ols}$ nor $W_{ridge}$ is equal to $(0, 0, \\\\ldots, 0)$. Consider a neuron initialized with $W_{ridge}$. Provide an objective function $J(W)$ that depends on the data, such that batch gradient descent to minimize $J$ will have no effect on the weights, or argue that one does not exist. | 0.945 |\\n| Classifiers | Consider a binary classification problem with two classes, +1 and -1. Assume that the training data is linearly separable. Consider the following two models: Model 1: $g(x) = \\\\text{sgn}(w^T x)$, Model 2: $g(x) = \\\\text{sgn}(w^T x + b)$. Assume that the training data is linearly separable. Which model is more likely to overfit? | 0.782 |\"}"}
{"id": "MT1Pcdo8sGG", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: The number of questions and parts in the final for each semester and topic of Introduction to Machine Learning. MIT Spring 2020 and Fall 2020 did not have final exams due to COVID-19.\\n\\nTopics can have half-questions attributed to them if a question has some parts under one topic and the other parts under another topic.\\n\\nMuch of the prior work focuses on high school or middle school level material (Qu et al., 2021). The first work to tackle university-level machine learning course problem set questions (Tran et al., 2021) used a transformer and GNN architecture and heavily relied on data augmentation. This resulted in overfitting and did not scale up to other types of questions or courses. Probability and statistics course problem-set questions have been answered (Tang et al., 2022) by probabilistic program synthesis with human performance. Problem-set questions from the core university math courses (Drori et al., 2022) have been automatically solved using few-shot learning and program synthesis at a human level. Other work considers university-level course questions across a variety of domains (Hendrycks et al., 2021) and identifying theorems (Srivastava et al., 2022). Prior work on question generation includes question\u2013answer pair generation based on a text passage (Qu et al., 2021) and question text generation based on other questions (Drori et al., 2022).\\n\\n2 DASET\\n\\nWe present a new dataset of 646 question parts from a dozen recent final exams of MIT's and Cornell's Introduction to Machine Learning courses and Harvard's Machine Learning class. The dataset spans questions on the 17 machine learning topics covered in the courses: (1) regression, (2) classifiers, (3) logistic regression, (4) features, (5) loss functions, (6) neural networks, (7) convolutional neural networks (CNNs), (8) Markov decision processes (MDPs), (9) recurrent neural networks (RNNs), (10) reinforcement learning, (11) clustering, (12) decision trees, (13) model selection, (14) ensemble methods, (15) Bayesian networks, (16) hidden Markov models (HMMs), and (17) optimization. We make our data and code publicly available.\\n\\nThe breakdown of questions, parts, points, and non-image points by each semester and topic are shown in Table 2. Each question in a final exam consists of multiple parts. Questions are written by providing set-up and context information first, followed by the question parts (which may come with additional information). Set-up and context information may contain (1) story elements (ex., character names, and motivations), (2) relevant definitions and equations, and (3) data points. We format questions in the dataset by concatenating the question context, any context or solutions from prior parts of the question required for answering the part, and the part's context and question. We split the questions into their corresponding parts. Questions consist of English text, mathematical notation, and images. Mathematical notation is represented in the dataset by LaTeX and images by screenshots from pdf files. The types of question answers are diverse. A few are multiple-choice or...\"}"}
{"id": "MT1Pcdo8sGG", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"true/false questions. Most are open-ended, for which the evaluation requires modeling the problem, mathematical manipulation, or code writing. Many questions require providing an explanation.\\n\\nWe used twelve final exams from different semesters for data curation. We had access to the Latex version for the three most recent semesters of MIT Spring 2021, Fall 2021, and Spring 2022, and therefore did not require transcription. For the nine remaining exams, MIT Fall 2017, Spring 2018, Fall 2018, Spring 2019, Fall 2019, Harvard Spring 2015 and Spring 2021, and Cornell Spring 2018 and Fall 2018, we had access to the pdf versions. In these cases, we used mathpix.com for an initial transcription, and curators then evaluated and manually corrected the input questions and verified the correctness of each input question.\\n\\nWe extract questions and solutions for all parts of all types of questions, including those that rely on images. We curated nine exams from publicly available pdf files. MIT Spring 2020 and Fall 2020 do not have final exams due to COVID-19. The three MIT exams between 2021 and 2022 were unavailable online; therefore, the model does not overfit their solutions. The aggregate average grades were available to the students and did not contain any personally identifiable information.\\n\\nThree duplicate questions were originally on the final exam of MIT Fall 2017 (questions 1, 3, 6) and appeared again in the final exam of MIT Spring 2022.\\n\\n3 BENCHMARK\\n\\n3.1 BASELINES\\n\\nWe provide a benchmark by comparing six baselines for answering the final exam questions: (1) GPT-3 with zero-shot learning, (2) GPT-3 with few-shot learning, (3) GPT-3 with zero-shot learning and chain-of-thought (CoT) prompting, (4) GPT-3 with few-shot learning and chain-of-thought (CoT) prompting, (5) Codex with zero-shot learning, and (6) Codex with few-shot learning.\\n\\nTable 4 shows the prompt used for each approach. GPT-3 zero-shot uses the question as-is, whereas GPT-3 zero-shot with CoT uses the suffix \u201cLet\u2019s think step by step.\u201d after the question to encourage multi-step output. Codex zero-shot uses the prefix \u201cWrite a program that answers\u201d before the question within Python comments denoted by triple quotes \"\"\" to encourage Codex to write code. GPT-3 few-shot finds the closest questions in the embedding space, measured by cosine similarity, and uses them and their corresponding answers before the new question as examples in the prompt. Codex few-shot finds the closest questions in the embedding space also as measured by cosine similarity and uses these questions and their corresponding code as examples.\\n\\nFor students, a good study technique is to use previous final exams to review and practice for their upcoming final. We model this method by few-shot learning using the question\u2013answer pairs (for GPT-3) or question\u2013code (for Codex) with the closest question embeddings from previous finals. We implement this by considering all the exam questions, marking each question by its semester and year, and using only previous semesters\u2019 questions for few-shot learning. The MIT Fall 2017 and Spring 2022 exams contain three duplicate questions, and we handle these same questions the same way humans do by allowing few-shot learning in MIT Spring 2022 based on successful Fall 2017 zero-shot answers. It is reasonable that if a student studies all previous exams, there may be 8.5% of repeated question points. Since MIT Fall 2017, Harvard Spring 2015, and Cornell Spring 2017 are the first final exams in the corresponding universities, we do not perform few-shot learning on these.\\n\\n3.1.1 COMPARISON WITH OPEN LANGUAGE MODELS\\n\\nWe also evaluated our dataset on an open-source language model, Meta\u2019s OPT-175B. OPT-175B is a model consisting of 175 billion parameters. Our dataset consists of final exam questions from machine learning courses and fit to be used by OPT-175B. Tables 5 and 6 compare the results of OpenAI GPT-3, OpenAI Codex, and Meta OPT. We evaluated OPT on only 163 question parts, since OPT was limited to handling questions under 256 characters in length. We implement the inference for the OPT-175B model using Alpa. Alpa is a particular framework designed for training and inference of large models. For the hardware, we use an 8x A100 PCIE cluster. The model requires about 560 GB of VRAM in our run case and each example takes nine minutes for inference.\"}"}
{"id": "MT1Pcdo8sGG", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nMethod Prompt\\n\\nGPT-3 Zero-Shot\\n\\nGPT-3 Few-Shot\\n\\nQ: <question>\\n\\nA: <similar question's answer>\\n\\nQ: <question>\\n\\nA:\\n\\nGPT-3 Zero-Shot with CoT\\n\\nGPT-3 Few-Shot with CoT\\n\\nQ: <similar question>\\n\\nA: <similar question's answer>\\n\\nQ: <question>\\n\\nA: Let's think step by step.\\n\\nCodex Zero-Shot\\n\\n\\\"\\\"\\\"Write a program that answers the following question: <question>\\\"\\n\\nCodex Few-Shot\\n\\n\\\"\\\"\\\"Write a program that answers the following question: <similar question>\\\"\\n\\n\\\"\\\"\\\"<similar question's correct code>\\\"\\n\\n\\\"\\\"\\\"Write a program that answers the following question: <question>\\\"\\n\\n3.2.1 HUMAN GRADING\\n\\nThe questions are of different types: multiple-choice, numerical, expressions, and open-text. We grade answers and aim to keep all factors equal in grading human and machine answers. Human and machine answers are graded based on the number of points allocated to each question part, giving full, partial, or no credit for each answer. We approximate partial credit by assigning half-credit.\\n\\nThe course staff graded student final exams, which included graduate TAs and instructors. Two of the same graduate TAs and the instructor that graded the student answers also graded the machine answers. Grading instructions are the same for grading student answers as grading machine answers.\\n\\n3.2.2 AUTOMATIC GRADING\\n\\nWe label each question's answer type into one or two categories out of four options - multiple choice (MC), numerical, expression, or open. We consider answers multiple choice if the test-taker is presented with an enumerated list of choices, numerical if the answer is a number, expression if the answer includes variables or other notation, and open if the answer calls for free-response text. We categorize questions that have additional questions nested within them by the multiple relevant categories. Most often, this is the case when a question with one of MC, numerical, or expression, is followed by a follow-up question asking the student to explain their previous answer. The breakdown of the questions is: 98 are multiple-choice, 84 numerical, 81 expressions, and 204 are open. The 'Non-Open Points' column of Tables 7 and 8 show the answer type breakdown by number of points.\\n\\nTable 7 shows the number of question parts that do not rely on images, the number of points that do not rely on images, and the number of non-open question points in Introduction to Machine Learning finals for each semester. MIT Spring 2020 and Fall 2020 did not have final exams due to COVID-19.\\n\\nTable 8 shows the breakdown by topic. Our automatic grading uses string matching and regular expressions. In the case of multiple-choice results, we check that the output of the code is equal to the solution. In the case of numerical answers, we look for a matching integer or real number.\\n\\n3.3 PERFORMANCE\\n\\nTable 5 shows the machine grades by semester and Table 6 shows the machine grades by topic, excluding question parts that rely on images. We compare the average grade of GPT-3 with zero-shot\"}"}
{"id": "MT1Pcdo8sGG", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Semester      | Regression | Classifiers | Logistic Reg. | Features | Loss Functions | Neural Networks | CNNs | MDPs | RNNs | RL | Clustering | Decision Trees | Model Selection | Ensemble Methods | Bayesian Networks | HMMs | Optimization |\\n|---------------|------------|-------------|---------------|----------|---------------|-----------------|------|------|------|----|------------|----------------|-----------------|-----------------|-----------------|------|-------------|\\n| MIT Fall 2017| 31.71      | 38.18       | 50.00         | 58.65    | NA            | 48.34           | 37.50| 49.19| 61.46| 36.09| 100.00     | 54.70          | 82.93           | 27.89           | 100.00          | 100.00| 55.00       |\\n| MIT Spring 2018| 50.00      | 46.21       | 26.28         | 53.74    | NA            | 60.23           | 53.58| 52.01| 71.88| 42.99| 100.00     | 71.80          | 83.74           | 50.00           | 100.00          | 100.00| 60.00       |\\n| MIT Fall 2018| 25.61      | 42.35       | 40.85         | 40.24    | 40.00         | 44.54           | 28.36| 46.03| 57.29| 36.67| 100.00     | 32.48          | 72.76           | 22.12           | 100.00          | 100.00| 35.00       |\\n| MIT Spring 2019| 40.24      | 18.88       | 40.24         | 68.85    | NA            | 68.42           | 47.81| 54.23| 66.14| 50.11| 100.00     | 51.28          | 95.12           | 66.35           | 100.00          | 100.00| 35.00       |\\n| MIT Fall 2019| 50.00      | 53.74       | 50.00         | 55.00    | NA            | 77.50           | 36.77| 24.38| 40.63| 28.79| 100.00     | 46.15          | 67.48           | 32.69           | 100.00          | 100.00| 17.50       |\\n\\nTable 5: We benchmark different baselines for each semester, excluding question parts that rely on images. We compare the average grade of GPT-3 with zero-shot (ZS), GPT-3 with few-shot (FS) learning, GPT-3 with ZS, and chain-of-thought (CoT) prompting, GPT-3 with FS and CoT prompting, Codex with ZS, Codex with FS, and OPT with ZS. MIT Fall 2017, Cornell Spring 2017, and Harvard Spring 2015 were the first semester for each university, so few-shot learning results based on previous semesters are unavailable (NA). MIT Spring 2020 and MIT Fall 2020 did not have final exams due to COVID-19. MIT Spring 2021, MIT Fall 2021, and MIT Spring 2022 final exams were unavailable online when GPT-3 and Codex were trained, ensuring that the model is not overfitting. The result of the best-performing method for each semester is marked in bold.\\n\\nTable 6: We benchmark different baselines for each course topic, excluding question parts that rely on images. We compare the grade of GPT-3 with zero-shot (ZS), GPT-3 with few-shot (FS) learning, GPT-3 with zero-shot and chain-of-thought (CoT) prompting, GPT-3 with FS and CoT, Codex with zero-shot, Codex with few-shot learning, and OPT with ZS. The question parts on loss functions rely on image information and are therefore unavailable (marked NA). The result of the best-performing method for each semester is marked in bold.\"}"}
