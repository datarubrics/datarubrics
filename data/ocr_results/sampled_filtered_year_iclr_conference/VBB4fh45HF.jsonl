{"id": "VBB4fh45HF", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In many real-world settings, image observations of freely rotating 3D rigid bodies, such as satellites, may be available when low-dimensional measurements are not. However, the high-dimensionality of image data precludes the use of classical estimation techniques to learn the dynamics and a lack of interpretability reduces the usefulness of standard deep learning methods. In this work, we present a physics-informed neural network model to estimate and predict 3D rotational dynamics from image sequences. We achieve this using a multi-stage prediction pipeline that maps individual images to a latent representation homeomorphic to SO(3), computes angular velocities from latent pairs, and predicts future latent states using the Hamiltonian equations of motion with a learned representation of the Hamiltonian. We demonstrate the efficacy of our approach on a new rotating rigid-body dataset with sequences of rotating cubes and rectangular prisms with uniform and non-uniform density.\\n\\nImages of 3D rigid bodies in motion are available across a range of application areas and can give insight into system dynamics. Learning dynamics from images has applications to planning, navigation, prediction, and control of robotic systems. Resident space objects (RSOs) are natural or man-made objects that orbit a planet or moon and are examples of commonly studied, free-rotating rigid bodies. When planning proximity operation missions with RSOs\u2014collecting samples from an asteroid (Williams et al., 2018), servicing a malfunctioning satellite (Flores-Abad et al., 2014), or active space debris removal (Mark and Kamath, 2019)\u2014it is critical to correctly estimate the RSO dynamics in order to avoid mission failure. Space robotic systems typically have access to onboard cameras, which makes learning dynamics from images a compelling approach for vision-based navigation and control.\\n\\nPrevious work Allen-Blanchette et al. (2020); Zhong and Leonard (2020); Toth et al. (2020) has made significant progress in learning dynamics from images of planar rigid bodies. Learning dynamics of 3D rigid-body motion has also been explored with a variety of types of input data Duong and Atanasov (2021); Byravan and Fox (2017); Peretroukhin et al. (2020). Duong and Atanasov (2021) uses state measurement data (i.e. rotation matrix and angular momenta), while Peretroukhin et al. (2020) learn the underlying dynamics in an overparameterized black-box model. The combination of deep learning with physics-based models allows models to learn dynamics from high-dimensional data such as images (Allen-Blanchette et al., 2020; Zhong and Leonard, 2020; Toth et al., 2020). However, as far as we know, our method is the first to use the Hamiltonian formalism to learn 3D rigid-body dynamics from images.\\n\\nKinematics and dynamics of 3D rigid body rotation are both fundamental to accomplishing the goals of this paper. The kinematics describe the rate of change of rigid body orientation as a function of the orientation and the angular velocity. Our method integrates the kinematic equations to compute the orientation trajectory in latent space using the latent angular velocity. The dynamics describe the rate of change of the angular velocity as a function of the angular velocity and the moment-of-inertia matrix $J$, which depends on the distribution of mass over the rigid body volume. $J$ is unknown and cannot be computed from knowledge of the external geometry of the rigid body, except in the special case in which the mass is known and the mass is uniformly distributed over the rigid body volume.\"}"}
{"id": "VBB4fh45HF", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Schematic of model architecture (top). The architecture combines an auto-encoding neural network with a Hamiltonian dynamics model for 3D rigid bodies (bottom-right). The encoder maps a sequence of images to a sequence of latent states in $\\\\text{SO}(3)$. We estimate angular velocity and momentum, then predict future orientation and momentum using the learned Hamiltonian (bottom-left). Each future latent state is decoded into an image using only the predicted orientation.\\n\\nGeometry since the special case is not practical. Importantly, the difference between the dynamics of a uniformly distributed mass and a non-uniformly distributed mass inside the same external geometry is significant. We show that we can learn these very different dynamics even when the external geometry is the same. By integrating the learned dynamics, we can predict future latent angular velocity. Works such as [1,2] estimate the transformation between image pairs, which can then be used to generate image trajectories. However, this approach implicitly assumes the object evolves with a constant velocity, an assumption that is not true in the general case. Our work addresses this limitation by estimating the angular acceleration and mass distribution for the object in addition to its angular velocity.\\n\\nIn this paper we introduce a model, with architecture depicted in Figure 1, that is capable of (1) learning 3D rigid-body dynamics from images, (2) predicting future image sequences in time, and (3) providing a low-dimensional, interpretable representation of the latent state. Our model incorporates the Hamiltonian formulation of the dynamics as an inductive bias to facilitate learning the moment of inertia tensor $\\\\mathbf{J} \\\\in \\\\mathbb{R}^{3 \\\\times 3}$ and an auto-encoding map between images and $\\\\text{SO}(3)$. The efficacy of our approach is demonstrated through long-term image prediction and its additional latent space interpretability.\\n\\n2 RELATED WORK\\n\\n2.1 HAMILTONIAN AND LAGRANGIAN PRIORS IN LEARNING DYNAMICS\\n\\nPhysics-guided machine learning approaches with Lagrangian and Hamiltonian priors often use state-control (e.g. 1D position and control) trajectories (Ahmadi and Khadir, 2020; Chen et al., 2020; Cranmer et al., 2020a; Duong and Atanasov, 2021; Finzi et al., 2020; Greydanus et al., 2019; 2...\"}"}
{"id": "VBB4fh45HF", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Angel Flores-Abad, Ou Ma, Khanh Pham, and Steve Ulrich. A review of space robotics technologies for on-orbit servicing. Progress in Aerospace Sciences, 68:1\u201326, 2014. ISSN 0376-0421. doi: https://doi.org/10.1016/j.paerosci.2014.03.002. URL https://www.sciencedirect.com/science/article/pii/S0376042114000347.\\n\\nHerbert Goldstein, Charles P. Poole, and John L. Safko. Classical Mechanics. Addison Wesley, 2002, 2002. ISBN 9780201657029.\\n\\nSam Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. Conference on Neural Information Processing Systems, abs/1906.01563, 2019.\\n\\nJayesh K. Gupta, Kunal Menda, Zachary Manchester, and Mykel J. Kochenderfer. A general framework for structured learning of mechanical systems. arXiv, 2019.\\n\\nIrina Higgins, David Amos, David Pfau, S\u00e9bastien Racani\u00e8re, Lo\u00efc Matthey, Danilo Jimenez Rezende, and Alexander Lerchner. Towards a definition of disentangled representations. ArXiv, abs/1812.02230, 2018.\\n\\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.\\n\\nDu Q. Huynh. Metrics for 3d rotations: Comparison and analysis. Math Imaging Vis, 35:155\u2013164, 2009. ISSN 0921-8890. doi: 10.1007/s10851-009-0161-2. URL https://www.cs.cmu.edu/~cga/dynopt/readings/Rmetric.pdf.\\n\\nC. Kane, Jerrold E. Marsden, Michael Ortiz, and Matthew West. Variational integrators and the Newmark algorithm for conservative and dissipative mechanical systems. International Journal for Numerical Methods in Engineering, 49:1295\u20131325, 2000.\\n\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nDiederik P. Kingma and Max Welling. Auto-encoding variational Bayes. 2014.\\n\\nTaeyoung Lee. Computational geometric mechanics and control of rigid bodies. PhD thesis, University of Michigan, 2008.\\n\\nTaeyoung Lee, Melvin Leok, and N. Harris McClamroch. Global Formulations of Lagrangian and Hamiltonian Dynamics on Manifolds. 1860-6245. Springer, Cham, 2018.\\n\\nJake Levinson, Carlos Esteves, Kefan Chen, Noah Snavely, Angjoo Kanazawa, Afshin Rostamizadeh, and Ameesh Makadia. An analysis of svd for deep rotation estimation. Advances in Neural Information Processing Systems, 33:22554\u201322565, 2020.\\n\\nYunzhu Li, Toru Lin, Kexin Yi, Daniel Bear, Daniel Yamins, Jiajun Wu, Joshua Tenenbaum, and Antonio Torralba. Visual grounding of learned physical models. In International conference on machine learning, pages 5927\u20135936. PMLR, 2020.\\n\\nMichale Lutter, Christian Ritter, and Jan Petter. Deep Lagrangian networks: Using physics as model prior for deep learning. International Conference of Learning Representations, 2018.\\n\\nC. Priyant Mark and Surekha Kamath. Review of active space debris removal methods. Space Policy, 47:194\u2013206, 2019. ISSN 0265-9646. doi: https://doi.org/10.1016/j.spacepol.2018.12.005. URL https://www.sciencedirect.com/science/article/pii/S0265964618300110.\\n\\nF. Landis Markley. Unit quaternion from rotation matrix. AIAA, Journal Guidance and Control, 31(02):440\u2013442, 2008. doi: https://doi.org/10.2514/1.31730.\\n\\nJerrold E. Marsden and Tudor S. Ratiu. Introduction to Mechanics and Symmetry. Texts in Applied Mathematics. Springer New York, NY, 1999. ISBN 978-0-387-98643-2.\\n\\nJerrold E. Marsden and Matthew West. Discrete mechanics and variational integrators. Acta Numerica, 10:357 \u2013 514, 2001.\"}"}
{"id": "VBB4fh45HF", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"J\u00fcrgen Moser and Alexander P. Veselov. Discrete versions of some classical integrable systems and factorization of matrix polynomials. *Communications in Mathematical Physics*, 139(2):217\u2013243, 1991. doi: 10.1007/BF02352494. URL https://doi.org/10.1007/BF02352494.\\n\\nSamuel E. Otto and Clarence W. Rowley. Linearly-recurrent autoencoder networks for learning dynamics. *ArXiv*, abs/1712.01378, 2019.\\n\\nValentin Peretroukhin, Matthew Giamou, David M. Rosen, W. Nicholas Greene, Nicholas Roy, and Jonathan Kelly. A smooth representation of belief over SO(3) for deep rotation learning with uncertainty. *CoRR*, abs/2006.01031, 2020. URL https://arxiv.org/abs/2006.01031.\\n\\nPeter Toth, Danilo J. Rezende, Andrew Jaegle, S\u00e9bastien Racani\u00e8re, Aleksandar Botev, and Irina Higgins. Hamiltonian generative networks. In *International Conference on Learning Representations*, 2020.\\n\\nJohan Verbeke and Ronald Cools. The newton-raphson method. *International Journal of Mathematical Education in Science and Technology*, 26:177\u2013193, 1995.\\n\\nManuel Watter and Martin Riedmiller Jost Tobias Springenberg, Joschka Boedecker. Embed to control: A locally linear latent dynamics model for control from raw images. *Advances in neural information processing systems*, 27, 2015.\\n\\nJeffrey M. Wendlandt and Jerrold E. Marsden. Mechanical integrators derived from a discrete variational principle. *Physica D: Nonlinear Phenomena*, 106:223\u2013246, 1997.\\n\\nJared Willard, Xiaowei Jia, Shaoming Xu, Michael S. Steinbach, and Vipin Kumar. Integrating scientific knowledge with machine learning for engineering and environmental systems. 2020.\\n\\nB. Williams, P. Antreasian, E. Carranza, C. Jackman, J. Leonard, D. Nelson, B. Page, D. Stanbridge, D. Wibben, K. Williams, M. Moreau, K. Berry, K. Getzandanner, A. Liounis, A. Mashiku, D. Highsmith, B. Sutter, and D. S. Lauretta. OSIRIS-REx Flight Dynamics and Navigation Design. *Space Science Reviews*, 214(4):69, April 2018. ISSN 1572-9672. doi: 10.1007/s11214-018-0501-x. URL https://doi.org/10.1007/s11214-018-0501-x.\\n\\nLinhui Xiao, Jinge Wang, Xiaosong Qiu, Zheng Rong, and Xudong Zou. Dynamic-slam: Semantic monocular visual localization and mapping based on deep learning in dynamic environment. *Robotics and Autonomous Systems*, 117:1\u201316, 2019. ISSN 0921-8890. doi: https://doi.org/10.1016/j.robot.2019.03.012. URL https://www.sciencedirect.com/science/article/pii/S0921889018308029.\\n\\nYaofend Desmond Zhong and Naomi Ehrich Leonard. Unsupervised learning of Lagrangian dynamics from images for prediction and control. In *Conference on Neural Information Processing Systems*, 2020.\\n\\nYaofend Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Dissipative SymODEN: Encoding Hamiltonian dynamics with dissipation and control into deep learning. In *ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations*, 2020a.\\n\\nYaofend Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Symplectic ODE-Net: Learning Hamiltonian dynamics with control. In *International Conference on Learning Representations*, 2020b.\\n\\nYi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. In *2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 5738\u20135746, 2019.\"}"}
{"id": "VBB4fh45HF", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Appendix A.1: Autoencoder Architecture\\n\\nTables 2 and 3 give the architecture of the encoder/decoder neural networks. The encoder and decoder combine convolutional and linear layers to map from images to the desired latent space. The nonlinear activation function used in the auto-encoding neural network is the exponential linear unit (ELU). This activation function was chosen for continuity, as well as to prevent vanishing and exploding gradients as shown in Otto and Rowley (2019).\\n\\n### Table 2: Encoder architecture.\\n\\n| Layer Number | Layer Name       | Input Channel Size | Output Channel Size | Kernel/Stride Size |\\n|--------------|------------------|--------------------|---------------------|--------------------|\\n| 1            | Conv2d           | 3                  | 16                  | 3                  |\\n| 2            | ELU              | N/A                | N/A                 | N/A                |\\n| 3            | Conv2d           | 16                 | 16                  | 3                  |\\n| 4            | ELU              | N/A                | N/A                 | N/A                |\\n| 5            | MaxPOOL          | N/A                | N/A                 | 2/2                |\\n| 6            | BatchNorm2d      | 16                 | N/A                 | N/A                |\\n| 7            | Conv2d           | 3                  | 16                  | 3                  |\\n| 8            | ELU              | N/A                | N/A                 | N/A                |\\n| 9            | Conv2d           | 16                 | 16                  | 3                  |\\n| 10           | ELU              | N/A                | N/A                 | N/A                |\\n| 11           | MaxPOOL          | N/A                | N/A                 | 2/2                |\\n| 12           | BatchNorm2d      | 32                 | N/A                 | N/A                |\\n| 13           | Flatten          | N/A                | N/A                 | N/A                |\\n| 14           | Linear           | 32*4*4             | 120                 | N/A                |\\n| 15           | ELU              | N/A                | N/A                 | N/A                |\\n| 16           | BatchNorm2d      | 120                | N/A                 | N/A                |\\n| 17           | Linear           | 120                | 84                  | N/A                |\\n| 18           | ELU              | N/A                | N/A                 | N/A                |\\n| 19           | Linear           | 84                 | 6                   | N/A                |\\n\\n### Table 3: Decoder architecture.\\n\\n| Layer Number | Layer Name       | Input Channel Size | Output Channel Size | Kernel/Stride Size |\\n|--------------|------------------|--------------------|---------------------|--------------------|\\n| 1            | Linear           | 6                  | 84                  | N/A                |\\n| 2            | ELU              | N/A                | N/A                 | N/A                |\\n| 3            | Linear           | 84                 | 120                 | N/A                |\\n| 4            | BatchNorm2d      | 120                | N/A                 | N/A                |\\n| 5            | ELU              | N/A                | N/A                 | N/A                |\\n| 6            | Linear           | 120                | 32*4*4              | N/A                |\\n| 7            | Unflatten        | (32, 4, 4)         | N/A                 | N/A                |\\n| 8            | BatchNorm2d      | 32                 | N/A                 | N/A                |\\n| 9            | MaxUNPOOL        | N/A                | N/A                 | 2/2                |\\n| 10           | ELU              | N/A                | N/A                 | N/A                |\\n| 11           | ConvTranspose2d  | 16                 | 16                  | 3                  |\\n| 12           | ELU              | N/A                | N/A                 | N/A                |\\n| 13           | ConvTranspose2d  | 16                 | 3                   | 3                  |\"}"}
{"id": "VBB4fh45HF", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We compare the performance of our model against three baseline architectures: (1) an LSTM baseline, (2) a Neural ODE (Chen et al., 2018) baseline, and the Hamiltonian Generative Network (HGN) (Toth et al., 2020). The first two architectures use the same encoder-decoder backbone as our model (see Tables 2 and 3) the HGN is trained with the architecture described in their work. The baselines differ from our approach in the way the dynamics are computed.\\n\\nB.1 LSTM - BASELINE\\n\\nThe LSTM-baseline uses an LSTM network to predict the dynamics. The LSTM-baseline is a three-layer LSTM network with an input dimension of 6 and a hidden dimension of 50. The hidden state and cell state are randomly initialized and the output of the network is mapped to a 6-dimensional latent vector by a learned linear transformation. We train the LSTM-baseline to predict a single step from 9 sequential latent vectors by minimizing the sum of the autoencoder and dynamics losses, $L_{\\\\text{ae}}$ and $L_{\\\\text{dyn}}$ as defined in equations 4.4.1 and 7, and the MSE loss between the encoder generated latent vectors and the LSTM-baseline latent vectors. The baseline is trained using the hyper-parameters in Tables 5, 7, 8, and 6. At inference we use a recursive strategy to predict farther into the future. The qualitative performance for the LSTM-baseline is given in figure 4, and the quantitative performance in terms of pixel mean-square error given in Table 1. The total number of parameters in the network is 52400.\\n\\n![Figure 4: Predicted sequences for uniform/non-uniform prism and cube datasets given by the LSTM-baseline. At prediction time, the model takes the first 9 encoded latent states and predicts a sequence of 20 time steps recursively. The figure shows time steps 10-19, which are the first 10 predictions of the model. The LSTM-baseline has poorer performance than the proposed approach on all datasets.](image)\\n\\nB.2 NEURAL ODE (CHEN ET AL., 2018) - BASELINE\\n\\nThe Neural ODE-baseline uses Neural ODE (Chen et al., 2018) framework to predict dynamical updates. The Neural ODE-baseline is a three layer MLP with ELU Clevert et al. (2015) activations. The baseline has an input dimension of 6, a hidden dimension of 50, and an output dimension of 6. We train the Neural ODE-baseline to predict a sequence of latent vectors from a single latent vector input by minimizing the sum of the autoencoder and dynamics losses, $L_{\\\\text{ae}}$ and $L_{\\\\text{dyn}}$ as defined in equations 4.4.1 and 7, and the MSE loss between the encoder generated latent vectors and the Neural ODE-baseline latent vectors. We use the RK4-integrator to integrate the dynamical update and the hyper-parameters in Tables 5, 7, 8, and 6. The qualitative performance for the NeuralODE-baseline is...\"}"}
{"id": "VBB4fh45HF", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nGupta et al., 2019; Lutter et al., 2018; Zhong et al., 2020b;a) to train their models. Lagrangian-based methods (Lutter et al., 2018; Gupta et al., 2019; Cranmer et al., 2020a; Finzi et al., 2020; Allen-Blanchette et al., 2020; Zhong and Leonard, 2020) parameterize the Lagrangian function $L(q, \\\\dot{q})$ using neural networks to derive the Euler-Lagrange dynamical equations (ELEs). These methods have been used to learn inverse dynamics models to derive control laws from data (Lutter et al., 2018) and approximate the ELEs with neural networks for forward prediction (Cranmer et al., 2020a; Finzi et al., 2020). Greydanus et al. (2019) create Hamiltonian-based priors by introducing Hamiltonian Neural Networks (HNNs), a method for parameterizing the Hamiltonian, $H$, as a neural network and calculating the symplectic gradients to construct Hamilton's equations. Zhong et al. (2020b) builds upon the ideas of (Greydanus et al., 2019) by further decomposing the Hamiltonian into different coordinate-dependent functions, such as the system's potential energy, inverse mass matrix, and input matrix\u2014providing additional structure. This method was extended to more general systems by Zhong et al. (2020a), where they introduce dissipative forces into their formulation. These models focus on using a dataset of state-control trajectories whereas our model uses image sequences. Moreover, while (Zhong et al., 2020b;a) have elements of their work that address rigid-body systems, these models focus on planar systems and do not use images.\\n\\n2.2 Using Images for Learning Dynamics\\n\\nImage-based learning models focus on using a set of images that represent the state trajectories for systems as data to train their models for a variety of tasks such as dimensionality reduction and learning dynamics. Approaches that model the dynamics of the system (Li et al., 2020; Zhong and Leonard, 2020; Allen-Blanchette et al., 2020; Toth et al., 2020) learn dynamics from image-state data but only for either 2D planar systems or systems with dynamics in $\\\\mathbb{R}^n$, using pixel images. Zhong and Leonard (2020) use a Lagrangian prior and a coordinate-aware encoder and decoder in their variational auto-encoding neural network to estimate the underlying dynamics and do video prediction with control. Allen-Blanchette et al. (2020) focus on learning dynamics from images for similar systems but using a non-variational approach, encouraging an interpretable latent space for long-term video prediction. Most similar to our work, Toth et al. (2020) use a Hamiltonian-based approach to learning dynamics from images. The work of Toth et al. differs from ours in that we do not use a variational auto-encoding neural network, our model is used for systems with configuration space on $SO(3)$, and our work focuses on learning a physically interpretable latent space while Toth et al. have a much higher dimension latent space than the physical configuration space.\\n\\n2.3 Learning Dynamics for Rigid Bodies\\n\\nDuong and Atanasov (2021) investigate learning rigid-body dynamics from state-control trajectory data using a Hamiltonian approach. In their work, Duong and Atanasov focus on systems with configuration spaces on the special orthogonal groups and special Euclidean groups (e.g. $SO(2)$, $SO(3)$, and $SE(3)$). They parameterize the inverse mass properties, potential energy, and control input functions, which are estimated with neural networks similar to our method. Duong and Atanasov go on to use the estimated pose dynamics to develop control methodologies for stabilization and tracking using the learned model of a quad-rotor system. In contrast, our work focuses on learning the dynamics of a free-rotating rigid body with configuration space on $SO(3)$ using images.\\n\\n3 Background\\n\\n3.1 The $S^2 \\\\times S^2$ Parameterization of 3D Rotation Group\\n\\nThe $S^2 \\\\times S^2$ parameterization of the 3D rotation group is a surjective and differentiable mapping with a continuous right inverse Falorsi et al. (2018). Define the $n$-sphere:\\n\\n$$S^n = \\\\{ v \\\\in \\\\mathbb{R}^{n+1} | v_1^2 + v_2^2 + \\\\ldots + v_{n+1}^2 = 1 \\\\},$$\\n\\nand the 3D rotation group:\\n\\n$$SO(3) = \\\\{ R \\\\in \\\\mathbb{R}^{3 \\\\times 3} | R^T R = I_3, \\\\det(R) = +1 \\\\},$$\\n\\nwhere $I_3$ denotes the $3 \\\\times 3$ identity matrix. The $S^2 \\\\times S^2$ parameterization of $SO(3)$ is given by\\n\\n$$(u, v) \\\\mapsto (w_1, w_2, w_3)$$\\n\\nwhere\\n\\n$$w_1 = u, \\\\quad w_2 = v - v \\\\langle u, v \\\\rangle, \\\\quad w_3 = w_1 \\\\times w_2,$$\\n\\nwhere $w_i$ are renormalized to have unit norm. Intuitively, this mapping constructs an orthonormal frame from the unit vectors $u$ and $v$ by Gram-Schmidt orthogonalization. The right inverse of the parameterization is given by\\n\\n$$(w_1, w_2, w_3) \\\\mapsto (w_1, w_2).$$\\n\\nOther parameterizations of $SO(3)$ such as the exponential map ($so(3) \\\\mapsto SO(3)$) and the quaternion map ($S^3 \\\\mapsto SO(3)$) do not have continuous inverses and...\"}"}
{"id": "VBB4fh45HF", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"therefore are more difficult to use in deep manifold regression (Falorsi et al., 2018; Levinson et al., 2020; Br\u00e9gier, 2021).\\n\\n### 3.2 3D Rotating Rigid Body Kinematics\\n\\nA rotating 3D rigid body's orientation \\\\( R(t) \\\\in SO(3) \\\\) changing over time \\\\( t \\\\) can be computed from angular velocity \\\\( \\\\Omega(t) \\\\) using the kinematic equations given by the time-rate-of-change of \\\\( R(t) \\\\) as\\n\\n\\\\[\\n\\\\frac{dR(t)}{dt} = R(t) \\\\Omega(t) \\\\times (t),\\n\\\\]\\n\\nwhere \\\\( \\\\Omega \\\\times \\\\) is a 3-dimensional skew-symmetric matrix defined by\\n\\n\\\\[\\n(\\\\Omega \\\\times) y = \\\\Omega \\\\times y, \\\\quad y \\\\in \\\\mathbb{R}^3\\n\\\\]\\n\\nand \\\\( \\\\times \\\\) is the vector cross-product.\\n\\nFor computational purposes, 3D rigid-body rotational kinematics are commonly expressed in terms of the quaternion representation \\\\( q(t) \\\\in S_3 \\\\) of the rigid-body orientation \\\\( R(t) \\\\). The kinematics equation 1 written in terms of quaternions (Andrle and Crassidis, 2013) are\\n\\n\\\\[\\n\\\\frac{dq(t)}{dt} = Q(\\\\Omega(t)) q(t),\\n\\\\]\\n\\nwhere\\n\\n\\\\[\\nQ(\\\\Omega) = -\\\\Omega \\\\times -\\\\Omega^T 0\\n\\\\]\\n\\n### 3.3 3D Rigid Body Dynamics in Hamiltonian Form\\n\\nThe canonical Hamiltonian formulation derives the equations of motion for a mechanical system using only the symplectic form and a Hamiltonian function, which maps the state of the system to its total (kinetic plus potential) energy Goldstein et al. (2002). This formulation has been used by several authors to learn unknown dynamics: the Hamiltonian structure (canonical symplectic form) is used as a physics prior and the unknown dynamics are uncovered by learning the Hamiltonian (Greydanus et al., 2019; Zhong et al., 2020b; Toth et al., 2020).\\n\\nConsider a system with configuration space \\\\( \\\\mathbb{R}^n \\\\) and a choice of \\\\( n \\\\) generalized coordinates that represent configuration. Let \\\\( z(t) \\\\in \\\\mathbb{R}^{2n} \\\\) represent the vector of \\\\( n \\\\) generalized coordinates and their \\\\( n \\\\) conjugate momenta at time \\\\( t \\\\).\\n\\nDefine the Hamiltonian function \\\\( H: \\\\mathbb{R}^{2n} \\\\rightarrow \\\\mathbb{R} \\\\) such that \\\\( H(z) \\\\) is the sum of the kinetic plus potential energy. Then the equations of motion derive as\\n\\n\\\\[\\n\\\\frac{dz}{dt} = \\\\Lambda_{can} \\\\nabla z H(z),\\n\\\\]\\n\\nwhere \\\\( \\\\Lambda_{can} = 0_n I_n - I_n 0_n \\\\)\\n\\n\\\\[\\n(3)\\n\\\\]\\n\\nThe equations of motion for a freely rotating 3D rigid body evolve on \\\\( T^* SO(3) \\\\) and describe the evolution of \\\\( R \\\\) given by equation 1 and the evolution of the angular momentum \\\\( \\\\Pi \\\\) given by Euler's equations Goldstein et al. (2002):\\n\\n\\\\[\\n\\\\frac{d\\\\Pi}{dt} = \\\\Pi \\\\times J - \\\\frac{1}{\\\\Pi} \\\\Pi,\\n\\\\]\\n\\nwhere \\\\( J \\\\) denotes the moment-of-inertia matrix and \\\\( \\\\Pi = J \\\\Omega \\\\).\\n\\nImportantly, the dynamics equation 4 do not depend on \\\\( R \\\\). This is due to the rotational symmetry of a freely rotating 3D rigid body, i.e., it is arbitrary how we assign an inertial frame. Due to this symmetry, the Hamiltonian formulation can be reduced using the Lie-Poisson Reduction Theorem Marsden and Ratiu (1999) to describe the time evolution of \\\\( \\\\Pi \\\\) on \\\\( \\\\mathbb{R}^3 \\\\sim so^*(3) \\\\), the Lie coalgebra of \\\\( SO(3) \\\\), independent of equation 1. The reduced Hamiltonian \\\\( h: so^*(3) \\\\rightarrow \\\\mathbb{R} \\\\) for the freely rotating 3D rigid body is its kinetic energy:\\n\\n\\\\[\\nh(\\\\Pi) = \\\\frac{1}{2} \\\\Pi \\\\cdot J^{-1} \\\\Pi.\\n\\\\]\\n\\nThe reduced Hamiltonian formulation Marsden and Ratiu (1999) is\\n\\n\\\\[\\n\\\\frac{d\\\\Pi}{dt} = \\\\Lambda_{so^*}(3)(\\\\Pi) \\\\nabla_\\\\Pi h(\\\\Pi),\\n\\\\]\\n\\n\\\\[\\n\\\\Lambda_{so^*}(3)(\\\\Pi) = \\\\Pi \\\\times,\\n\\\\]\\n\\nwhich can be seen to be equivalent to equation 4. The equations equation 6, called Lie-Poisson equations, generalize the canonical Hamiltonian formulation. The generalization allows for different symplectic forms, i.e., \\\\( \\\\Lambda_{so^*}(3) \\\\) instead of \\\\( \\\\Lambda_{can} \\\\) in this case, each of which is only related to the latent space and symmetry. Our physics prior is the generalized symplectic form and learning the unknown dynamics means learning the reduced Hamiltonian. This is a generalization of the existing literature.\"}"}
{"id": "VBB4fh45HF", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023 where dynamics of canonical Hamiltonian systems are learned with the canonical symplectic form (Greydanus et al., 2019; Cranmer et al., 2020a; Chen et al., 2020; Toth et al., 2020). Using the generalized Hamiltonian formulation extends the approach to a much larger class of systems than those described by Hamilton's canonical equations, including rotating and translating 3D rigid bodies, rigid bodies in a gravitational field, multi-body systems, and more.\\n\\n4 LEARNING HAMILTONIAN DYNAMICS ON $\\\\text{SO}(3)$\\n\\nIn this section we outline our approach for learning and predicting rigid-body dynamics from image sequences. The multi-stage prediction pipeline maps individual images to an $\\\\text{SO}(3)$ latent space where angular velocities are computed from latent pairs. Future latent states are computed using the generalized Hamiltonian equations of motion and a learned representation of the reduced Hamiltonian. Finally, the predicted latent representations are mapped to images giving a predicted image sequence.\\n\\n4.1 NOTATION\\n\\n$N$ denotes the number of image sequences in the dataset and $T$ the length of each image sequence. Image sequences are written $I_k = \\\\{I_{k1}, \\\\ldots, I_{kT}\\\\}$ with $I_{kt} \\\\in I$, embedded sequences are written $Z_k = \\\\{z_{k1}, \\\\ldots, z_{kT}\\\\}$ with $z_{kt} \\\\in Z$, $\\\\text{SO}(3)$ latent sequences are written $R_k = \\\\{R_{k1}, \\\\ldots, R_{kT}\\\\}$ with $R_{kt} \\\\in \\\\text{SO}(3)$, and quaternion sequences are written $q_k = \\\\{q_{k1}, \\\\ldots, q_{kT}\\\\}$ with $q_{kt} \\\\in S^3$. Quantities generated with the learned dynamics are denoted with a hat (e.g., $\\\\hat{q}$). For all sequences $k \\\\in \\\\{1, \\\\ldots, N\\\\}$.\\n\\n4.2 EMBEDDING TO AN $\\\\text{SO}(3)$ LATENT SPACE\\n\\nWe embed image observations of a rotating rigid body to an $\\\\text{SO}(3)$ latent space using the composition of functions $f \\\\circ \\\\pi \\\\circ E_\\\\theta: I \\\\rightarrow \\\\text{SO}(3)$. The encoding network $E_\\\\theta: I \\\\rightarrow \\\\mathbb{R}^6$ is learned during training, the projection $\\\\pi: \\\\mathbb{R}^6 \\\\rightarrow S^2 \\\\times S^2$ is defined as $\\\\pi(z) = (u/\\\\|u\\\\|, v/\\\\|v\\\\|)$, $u, v \\\\in \\\\mathbb{R}^3$ where $z = (u, v)$, and the function $f: S^2 \\\\times S^2 \\\\rightarrow \\\\text{SO}(3)$ denotes the surjective and differentiable parameterization of $\\\\text{SO}(3)$ (see Section 3.1) which constrains embedded representations to the $\\\\text{SO}(3)$ manifold where we compute dynamics.\\n\\n4.3 HAMILTONIAN DYNAMICS ON $\\\\text{SO}(3)$\\n\\nWe predict future $\\\\text{SO}(3)$ latent states using the equations of motion for a freely rotating 3D rigid body (see equations 2 and 6) and the learned moment of inertial tensor $J_\\\\psi$. We construct the initial state $x_k^0 = (q_k^0, \\\\Pi_k^0)$ using the pair of sequential $\\\\text{SO}(3)$ latent states $(R_k^0, R_k^1)$ (see Section 4.2). The quaternion $q_k^0$ is computed using the implementation of a modified Shepperd's algorithm (Markley, 2008) proposed in (Falorsi et al., 2018) and the angular momentum $\\\\Pi_k^0$ is computed as $\\\\Pi_k^0 = J_\\\\psi \\\\Omega_k^0$ where the angular velocity $\\\\Omega_k^0$ is approximated using the algorithm proposed in (Barfoot, 2017) (see also Figure 1: Angular Velocity Estimator). The kinematic equations equation 2 are integrated forward using a Runge-Kutta fourth-order solver (RK45) and a normalization step (Andrle and Crassidis, 2013) to ensure the quaternions are valid.\\n\\nWe decode a predicted image sequence from the predicted quaternion sequence $q_k$ in three steps. We first transform each $q_{kt}$ to $R_{kt}$, then apply the right inverse of the $S^2 \\\\times S^2$ parameterization of $\\\\text{SO}(3)$, and finally, decode from $S^2 \\\\times S^2$ with the decoding neural network $D_\\\\phi$ which is learned during training.\\n\\n4.4 LOSS FUNCTIONS\\n\\nIn this section we describe each component of our loss function: the auto-encoder reconstruction loss $L_{ae}$, dynamics reconstruction loss $L_{dyn}$, and latent losses functions $L_{latent, R}$, $L_{latent, \\\\Pi}$, $L_{energy}$. The function $L_{ae}$ ensures the embedding to $\\\\text{SO}(3)$ is sufficiently expressive to represent the image state, and $L_{dyn}$ ensures the dynamics consistency between decoded latent states images and their decoded predicted states. Both $L_{latent, R}$ and $L_{latent, \\\\Pi}$ ensure the dynamics consistency of encoded images and their predicted states in the latent space. Lastly, $L_{energy}$ enforces the conservation of energy between...\"}"}
{"id": "VBB4fh45HF", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nFigure 2: Example training sequences. (Top) Uniform density cube, (bottom) Uniform density prism.\\n\\nencoded and predicted trajectories. For notational convenience we denote the embedding pipeline $E: I \\\\rightarrow \\\\mathbb{R}^3$, and the decoding pipeline $D: \\\\mathbb{R}^3 \\\\rightarrow I$.\\n\\n4.4.1 RECONSTRUCTION LOSS\\nThe auto-encoding reconstruction loss is the mean square error (MSE) between the ground-truth image sequence:\\n\\n$$L_{ae} = \\\\frac{1}{N T} \\\\sum_{k=1}^{N} \\\\sum_{t=0}^{T-1} (D \\\\circ E(I_k^t) - I_k^t)^2.$$  (1)\\n\\nThe dynamics reconstruction loss function is the MSE between the ground-truth image sequence and the predicted image sequence:\\n\\n$$L_{dyn} = \\\\frac{1}{N T} \\\\sum_{k=1}^{N} \\\\sum_{t=1}^{T} (D(\\\\hat{q}_k^t) - I_k^t)^2.$$  (7)\\n\\n4.4.2 LATENT LOSS\\nThe latent state loss function is a distance metric on $SO(3)$ (Huynh, 2009), defined as the MSE between the $3 \\\\times 3$ identity matrix and right-difference between the encoded latent states and the latent states predict using the learned dynamics:\\n\\n$$L_{latent, R} = \\\\frac{1}{N T} \\\\sum_{k=1}^{N} \\\\sum_{t=1}^{T} (I_3 - R_{k}^{enc} \\\\hat{R}_k^t)^T F.$$  (1)\\n\\nAn additional loss is computed over the angular momentum vectors estimated from the encoded latent states (see Figure 1) and the predicted angular momentum vectors using the MSE loss:\\n\\n$$L_{latent, \\\\Pi} = \\\\frac{1}{N T} \\\\sum_{k=1}^{N} \\\\sum_{t=1}^{T} (\\\\Pi_{k}^{enc} - \\\\hat{\\\\Pi}_k^t)^2.$$  (1)\\n\\n4.5 ENERGY-BASED LOSS\\nWe encourage conservation of energy in the latent representation using the energy conservation loss:\\n\\n$$L_{energy} = \\\\frac{1}{N (T+1)} \\\\sum_{k=1}^{N} \\\\sum_{t=0}^{T} (E_k^t - \\\\bar{E}_k)^2,$$\\n\\nwhere,\\n\\n$$E_k^t = h(\\\\Pi_k^{enc}; J - 1 \\\\psi)$$\\n\\nis computed using equation 5:\\n\\nrigid body datasets\\n\\nThe lack of exploration of 3D learning tasks creates a deficit of datasets for models designed for 3D dynamics. Previous contributions to the study of learning dynamics from images (Greydanus et al., 2019).\"}"}
{"id": "VBB4fh45HF", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023 given in figure 5, and the quantitative performance in terms of pixel mean-square error given in table 1. The total number of parameters in the network is 11406.\\n\\nFigure 5: Predicted sequences for uniform/non-uniform prism and cube datasets given by the Neural ODE-baseline. At prediction time, the model takes the first encoded latent states and predicts a sequence of 10 steps recursively. The Neural ODE-baseline has poorer performance than the proposed approach on all datasets.\\n\\nB.3 Hamiltonian Generative Network (HGN)\\n\\nHGN (Toth et al., 2020) uses a combination of variational auto-encoding neural networks, transformer, and Hamiltonian dynamics to do video prediction. For our comparison experiments, we use the implementation Balsells Rodas et al. (2021). During training, the same training hyperparameters are used as described in Toth et al. (2020). We train HGN on our four datasets using their loss function (i.e., reconstruction and KL divergence). We use the Leap-frog integrator as this provided their model with its best performance on their datasets. The qualitative performance for HGN on our datasets is given in figure ??, and the quantitative performance in terms of pixel mean-square error given in table 1.\\n\\nC. Ablation Study\\n\\nIn this ablative study, we explore the impacts of the dynamics loss 7 and energy loss 4.5 on the performance of the proposed model. The ablated model is trained similarly to the proposed model except the dynamics (or energy) loss is not present in the total loss. From table 4 and figure 6 it can be seen that removing the dynamics loss negatively affects the performance of our proposed model. Inspecting table 4 it can be seen that removing energy loss has a minimal degradation effect on model performance. These results are further corroborated in the literature (Allen-Blanchette et al., 2020; Watter and Jost Tobias Springenberg, 2015).\\n\\nD. Experiment Hyper-Parameters\\n\\nD.1 Uniform Mass Density Cube\\n\\n\\\\[ \\\\begin{align*}\\n  m^1_{\\\\text{cube}} &= \\\\begin{pmatrix}\\n  3.0 & 0.0 & 3.0 \\\\\\\\\\n  0.0 & 0.0 & 3.0 \\\\\\\\\\n  0.0 & 0.0 & 3.0 \\n  \\\\end{pmatrix}\\n\\\\]\"}"}
{"id": "VBB4fh45HF", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Average pixel MSE over a 30 step unroll on the train and test data on four datasets for our ablative study. All values are multiplied by 1e+3. We evaluate our model and compare to a version of our model without the dynamics loss and without the energy-base loss. Our full model outperforms the ablated models in the prediction task across all datasets.\\n\\n| Dataset      | Ours | Ours - dynamics | Ours - energy |\\n|--------------|------|-----------------|---------------|\\n| TRAIN        |      |                 |               |\\n| Uniform Prism| 2.66 \u00b10.10 | 2.71 \u00b10.08 | 6.41 \u00b12.02 |\\n| Uniform Cube | 3.54 \u00b10.17 | 3.97 \u00b10.16 | 11.30 \u00b11.47 |\\n| Non-uniform Prism | 4.27 \u00b10.18 | 6.61 \u00b10.88 | 8.80 \u00b12.51 |\\n| Non-uniform Cube | 6.24 \u00b10.29 | 4.85 \u00b10.35 | 13.84 \u00b11.62 |\\n| TEST         |      |                 |               |\\n| Uniform Prism| 3.03 \u00b11.26 | 3.05 \u00b11.21 | 3.03 \u00b11.26 |\\n| Uniform Cube | 4.13 \u00b12.14 | 4.62 \u00b12.02 | 4.13 \u00b12.14 |\\n| Non-uniform Prism | 4.98 \u00b11.26 | 7.07 \u00b11.88 | 4.98 \u00b11.26 |\\n| Non-uniform Cube | 7.27 \u00b11.06 | 5.65 \u00b11.50 | 7.27 \u00b11.06 |\\n\\nFigure 6: Predicted sequences for uniform/non-uniform prism and cube datasets given by the ablated version of our proposed model. At prediction time, the model takes the first encoded latent states and predicts a sequence of 10 steps recursively. The ablated model has poorer performance than the proposed approach on all datasets.\\n\\nThe training parameters used to run this experiment with the train_dev.py file is given in table 5.\\n\\nTable 5: Hyper-parameters used to train model for the uniform mass density cube experiment. Only values differing from default values are given in the table.\\n\\n| Uniform Cube Experiment | Parameter Name | Value |\\n|-------------------------|----------------|-------|\\n|                         | seed           | 17    |\\n|                         | single-gpu     | True  |\\n|                         | test_split     | 0.2   |\\n|                         | val_split      | 0.1   |\\n|                         | n_epoch        | 1000  |\\n|                         | batch_size     | 256   |\\n|                         | learning_rate_ae | 1E-3 |\\n|                         | learning_rate_dyn | 1E-3 |\\n|                         | seq_len        | 10    |\\n|                         | time_step      | 1E-3  |\\n|                         | loss_gamma     | 1. 1. 1. 0.1 0.1 |\\n\\n16\"}"}
{"id": "VBB4fh45HF", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D.2 Uniform Mass-Density Prism\\n\\n\\\\[ J_{\\\\text{prism}} = \\\\begin{bmatrix} 2.40 & 0.00 \\\\cdot 0.71 & 0.00 \\\\\\\\ 0.00 & 0.00 & 0.00 \\\\cdot 0.6 \\\\\\\\ \\\\end{bmatrix} \\\\]\\n\\nThe training parameters used to run this experiment with the `train_dev.py` file is given in table 6.\\n\\n**Table 6: Hyper-parameters used to train model for the uniform mass-density prism experiment. Only values differing from default values are given in table.**\\n\\n| Parameter Name     | Value                        |\\n|--------------------|------------------------------|\\n| seed               | 17                           |\\n| single-gpu         | True                         |\\n| test_split         | 0.2                          |\\n| val_split          | 0.1                          |\\n| n_epoch            | 1000                         |\\n| batch_size         | 256                          |\\n| learning_rate_ae   | $1 \\\\times 10^{-3}$          |\\n| learning_rate_dyn  | $1 \\\\times 10^{-3}$          |\\n| seq_len            | 10                           |\\n| time_step          | $1 \\\\times 10^{-3}$          |\\n| loss_gamma         | 1.0 1.0 1.0 0.1 0.1          |\\n\\nD.3 Non-Uniform Density Cube\\n\\n\\\\[ J_{\\\\text{nu-cube}} = \\\\begin{bmatrix} 4.52677669 \\\\cdot 2.61906366 \\\\cdot 0.43651061 \\\\cdot 2.61906366 \\\\cdot 1.34388683 \\\\cdot 0.77601886 \\\\cdot 0.43651061 \\\\cdot 0.77601886 \\\\cdot 0.12933648 \\\\end{bmatrix} \\\\]\\n\\nThe training parameters used to run this experiment with the `train_dev.py` file is given in table 7.\\n\\n**Table 7: Hyper-parameters used to train model for the non-uniform mass density cube experiment. Only values differing from default values are given in table.**\\n\\n| Parameter Name     | Value                        |\\n|--------------------|------------------------------|\\n| seed               | 17                           |\\n| single-gpu         | True                         |\\n| test_split         | 0.2                          |\\n| val_split          | 0.1                          |\\n| n_epoch            | 1000                         |\\n| batch_size         | 256                          |\\n| learning_rate_ae   | $1 \\\\times 10^{-3}$          |\\n| learning_rate_dyn  | $1 \\\\times 10^{-3}$          |\\n| seq_len            | 10                           |\\n| time_step          | $1 \\\\times 10^{-3}$          |\\n| loss_gamma         | 1.0 1.0 1.0 0.1 0.1          |\\n\\nD.4 Non-Uniform Density Prism\\n\\n\\\\[ J_{\\\\text{nu-prism}} = \\\\begin{bmatrix} 2.36999579 \\\\cdot 0.12136536 \\\\cdot 0.39443742 \\\\cdot 0.12136536 \\\\cdot 0.67762326 \\\\cdot 0.2022756 \\\\cdot 0.39443742 \\\\cdot 0.2022756 \\\\cdot 0.6573957 \\\\end{bmatrix} \\\\]\\n\\nThe training parameters used to run this experiment with the `train_dev.py` file is given in table 8.\"}"}
{"id": "VBB4fh45HF", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Hyper-parameters used to train model for the non-uniform mass density prism experiment. Only values differing from default values are given in table.\\n\\n| Parameter Name     | Value               |\\n|--------------------|---------------------|\\n| seed               | 17                  |\\n| single-gpu         | True                |\\n| test_split         | 0.2                 |\\n| val_split          | 0.1                 |\\n| n_epoch            | 1000                |\\n| batch_size         | 256                 |\\n| learning_rate_ae   | 1E-3                |\\n| learning_rate_dyn  | 1E-3                |\\n| seq_len            | 10                  |\\n| time_step          | 1E-3                |\\n| loss_gamma         | 1.1 1.1 1.1 0.1 0.1 |\\n\\nThe angular velocity is estimated using two sequential image frames, $I_0$ and $I_1$. The image frames are encoded into two latent states, $R_0$ and $R_1$. These latent states are the orientation matrices in the body-fixed frame. The angular velocity between frame 0 and frame 1 is calculated in two parts: (1) the unit angular velocity vector and (2) the angular velocity magnitude. The unit vector and magnitude are estimated as shown in Figure 1.\\n\\nThe models are trained on a server with 8 NVIDIA A100 SXM4 GPUs. The processor is an AMD EPYC 7763, with 64 cores, 128 threads, 2.45 GHz base, 3.50 GHz boost, 256 MB cache, PCIe 4.0.\\n\\nFor access to the code repository, go to the Github link: https://github.com/jjmason687/LearningSO3fromImages. For access to data used in this work please see link at https://www.dropbox.com/sh/menv3lu9mquu1wh/AABovQ53udtryDC24xPLGw17a?dl=0.\"}"}
{"id": "VBB4fh45HF", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Average pixel mean square error over a 30 step unroll on the train and test data on four datasets. All values are multiplied by 1e+3. We evaluate our model and compare to three baseline models: (1) recurrent model (LSTM Hochreiter and Schmidhuber (1997)), (2) NeuralODE (Chen et al. (2018)), (3) HGN (Toth et al. (2020)). Our model outperforms both baseline models in the prediction task across the majority of the datasets. The number parameters for the dynamics models of each baselines are given in the last row of the table.\\n\\n| Dataset      | Ours   | LSTM - baseline | NeuralODE - baseline | HGN - baseline |\\n|--------------|--------|-----------------|----------------------|----------------|\\n| Uniform Prism| 2.66\u00b10.10 | 2.71\u00b10.08       | 3.46\u00b10.59           | 3.96\u00b10.68      | 4.00\u00b10.68 | 4.18\u00b10.07 |\\n| Uniform Cube | 3.54\u00b10.17 | 3.97\u00b10.16       | 21.55\u00b11.98          | 21.64\u00b12.12     | 9.48\u00b11.19 | 9.43\u00b11.20 | 17.43\u00b10.00 | 18.69\u00b10.12 |\\n| Non-uniform Prism | 4.27\u00b10.18 | 6.61\u00b10.88       | 4.50\u00b11.31           | 4.52\u00b11.34      | 4.67\u00b10.58 | 4.75\u00b10.59 | 6.16\u00b10.08 | 8.33\u00b10.26 |\\n| Non-uniform Cube | 6.24\u00b10.29 | 4.85\u00b10.35       | 7.47\u00b10.51           | 7.51\u00b10.50      | 7.89\u00b11.50 | 7.94\u00b11.59 | 14.11\u00b10.13 | 18.14\u00b10.36 |\\n\\nNumber of Parameters: 6 - 52400 - 11400 - 2019; Toth et al., 2020; Allen-Blanchette et al., 2020; Zhong and Leonard, 2020. These approaches, which primarily use 2D datasets, are not applicable to our problem of learning 3D rigid-body dynamics. Therefore, we created datasets that demonstrate the rich dynamics behaviors of 3D rotational dynamics through images, capable of being used for 3D dynamics learning tasks. We empirically test the performance of our model on the following datasets:\\n\\n- Uniform density cube: Multi-colored cube of uniform mass density\\n- Uniform density prism: Multi-colored rectangular prism with uniform mass density\\n- Non-uniform density cube: Multi-colored cube with non-uniform mass density\\n- Non-uniform density prism: Multi-colored prism with non-uniform mass density\\n\\nThe uniform mass density cube and prism datasets demonstrate baseline capabilities of our approach while the non-uniform datasets are used to demonstrate capabilities of our model pertinent to applications. The uniform mass density datasets can be learned by estimating a diagonal moment of inertia, since the principle axes of rotation align with our defined body axes. These axes are more intuitive to estimate from a visual perspective. For the uniform cube dataset, inspired by Falorsi et al. (2018), the angular momentum vector is constant. For the uniform prism dataset, there exists a set of initial conditions that result in dynamics that are not locally bounded, and thus more difficult to learn. The non-uniform density datasets are used to demonstrate the capability of our model to learn dynamics when the principle axes of rotation do not align with the set of intuitive body axes of the object. The physical appearance of the objects match those of the uniform mass density datasets; however, the momentum of inertia in the body axes frame will be non-diagonal. This dataset will validate the model's capability to predict changes in mass distribution that may not be visible for failure diagnostics due to broken or shifted internal components.\\n\\nFor each dataset \\\\( N = 1000 \\\\) trajectories were created. Each trajectory consisted of an initial condition \\\\( x_0 = (R_0, \\\\Pi_0) \\\\) pair that was integrated forward in time using a Python-based Runge-Kutta solver for \\\\( T = 100 \\\\) timesteps with spacing \\\\( \\\\Delta t = 10^{-3} \\\\). Initial conditions were chosen such that \\\\( (R_0, \\\\Pi_0) \\\\sim \\\\text{Uniform} \\\\ SO(3) \\\\times S^2 \\\\) with \\\\( \\\\Pi_0 \\\\) scaled to have \\\\( \\\\| \\\\Pi_0 \\\\|_2 = 50 \\\\).\\n\\nThe orientations from the trajectories were passed to Blender Community (2018) to render 28x28 pixel images. Examples of the renderings for both the cube and the rectangular prism can be found in Figure 2. For training, each trajectory is shaped into trajectory windows of sequence length \\\\( \\\\tau = 10 \\\\), using a sliding window such that each iteration trains on a batch of image sequences of length \\\\( \\\\tau \\\\).\"}"}
{"id": "VBB4fh45HF", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Predicted sequences for uniform and non-uniform mass density datasets given by the model. At prediction time, the model takes the first two images of a sequence, encodes them into our latent space to estimate the angular momentum. The model then predicts and decodes the predicted future states into images of rotating rigid bodies. The prediction results show that the model is qualitatively capable of predicting into the future using images.\\n\\nThe model's ability to accurately predict future image states for the non-uniform density datasets is also interesting in that it demonstrates that the model is able to predict states accurately even when the density is visually ambiguous. This is particularly important because mass density (and thus rotational dynamics) is not something that can easily be inferred directly from images. The model is compared to three baseline models: (1) an LSTM-baseline, (2) a Neural ODE (Chen et al., 2018)-baseline, and (3) the HGN (Toth et al., 2020) model. Recurrent neural networks like the LSTM-baseline provide a discrete dynamics model. Neural ODE can be combined with a multi-layer perceptron to model and predict continuous dynamics. HGN is a variational model with a Hamiltonian inductive bias. Architecture and training details for each baseline is given in the appendix B. The prediction performance of our model and baselines is shown in Table 1. Our model outperforms the baseline models on most of the datasets with a more interpretable latent space, continuous dynamics, and fewer model parameters\u2014motivating principles for this work.\\n\\n6.2 LATENT SPACE ANALYSIS AND INTERPRETABILITY\\n\\nAnother contribution of this work is the interpretability of the latent space generated by our model from the image datasets. Black-box models with high-dimensional latent states make it very difficult to interpret and inspect the behavior of the latent space. Because our approach encodes all images into a latent space homeomorphic to $\\\\mathbb{SO}(3)$, we have convenient ways to interpret this low-dimensional space. The form of our latent space provides us a way of inspecting the behavior of our model that previous works lack. In this sense, our approach provides a step towards producing new frameworks for interpreting deep learning models, analyzing failure modes, and using control for dynamic systems with significant structure from prior knowledge.\"}"}
{"id": "VBB4fh45HF", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CONCLUSIONS\\n\\n7.1 SUMMARY\\n\\nIn this work, we have presented the first physics-informed deep learning framework for predicting image sequences of 3D rigid-bodies by embedding the images as measurements in the configuration space \\\\( \\\\text{SO}(3) \\\\) and propagating the Hamiltonian dynamics forward in time. We have evaluated our approach on a new dataset of free-rotating 3D bodies with different inertial properties, and have demonstrated the ability to perform long-term image predictions.\\n\\nBy enforcing the representation of the latent space to be the correct manifold, this work provides the advantage of interpretability over black-box physics-informed approaches. The extra interpretability of our approach is a step towards placing additional trust into sophisticated deep learning models. This work provides a natural path to investigating how to incorporate\u2014and evaluate the effect of\u2014classical model-based control directly to trajectories in the latent space. This interpretability is essential to deploying ML algorithms in safety-critical environments.\\n\\n7.2 LIMITATIONS\\n\\nWhile this approach has shown significant promise, it is important to highlight that this has only been tested in an idealized setting. Future work can examine the effect of dynamic scenes with variable backgrounds, lighting conditions, object geometries, and multiple bodies. Perhaps more limiting, this approach currently relies on the ability to train the model for each system being examined; however, future efforts can explore using transfer/few-shot learning between different 3D rigid-body systems.\\n\\n7.3 POTENTIAL NEGATIVE SOCIETAL IMPACTS\\n\\nWhile we do not believe this work directly facilitates injury to living beings, the inconsistency between the predicted latent representation and ground truth data may lead to unexpected results if deployed in real world environments.\\n\\n7.4 FUTURE WORK\\n\\nAlthough our approach so far has been limited to embedding RGB-images of rotating rigid-bodies with configuration spaces in \\\\( \\\\text{SO}(3) \\\\), it is important to note that there are natural extensions to a wider variety of problems. For instance, this framework can be extended to embed different high-dimensional sensor measurements\u2014such as point clouds\u2014by only modifying the feature extraction layers of the autoencoder. Likewise, depending on the rigid-body system, the latent space can be chosen to reflect the appropriate configuration space, such as generic rigid-bodies in \\\\( \\\\text{SE}(3) \\\\) or systems in more complicated spaces, such as the \\\\( n \\\\)-jointed robotic arm on a restricted subspace of \\\\( \\\\prod_{i=1}^{n} (\\\\text{SO}(3)) \\\\).\"}"}
{"id": "VBB4fh45HF", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Amir Ali Ahmadi and Bachir El Khadir. Learning dynamics systems with side information. In Learning for Dynamics and Control, June 2020.\\n\\nChristine Allen-Blanchette, Sushant Veer, Anirudha Majumdar, and Naomi Ehrich Leonard. LagNetViP: A Lagrangian neural network for video prediction. arXiv, 2020.\\n\\nMichael S. Andrle and John L. Crassidis. Geometric integration of quaternions. AIAA, Journal Guidance and Control, 36(06):1762\u20131772, 2013. doi: https://doi.org/10.2514/1.58558.\\n\\nKendall A. Atkinson. An introduction to numerical analysis. John Wiley & Sons, 1989. ISBN 9780471500230.\\n\\nCarles Balsells Rodas, Oleguer Canal Anton, and Federico Taschin. [re] hamiltonian generative networks. ReScience C, 7(2):#18, May 2021. doi: 10.5281/zenodo.4835278. URL https://doi.org/10.5281/zenodo.4835278.\\n\\nTimothy Barfoot. Robotic State Estimation. Number 1107159393. Cambridge University Press, 1st edition, 2017.\\n\\nRomain Br\u00e9gier. Deep regression on manifolds: a 3d rotation case study. In 2021 International Conference on 3D Vision (3DV), pages 166\u2013174. IEEE, 2021.\\n\\nSteven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the National Academy of Sciences, 113(15):3932\u20133937, 2016. doi: 10.1073/pnas.1517384113. URL https://www.pnas.org/doi/abs/10.1073/pnas.1517384113.\\n\\nArunkumar Byravan and D. Fox. SE3-nets: Learning rigid body motion using deep neural networks. In International Conference on Robotics and Automation, 2017.\\n\\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.\\n\\nZhengdao Chen, Jianyu Zhang, Martin Arjovsky, and Leon Bottou. Symplectic recurrent neural networks. In International Conference on Learning Representations, 2020.\\n\\nDjork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.\\n\\nBlender Online Community. Blender - a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. URL http://www.blender.org.\\n\\nM. Cranmer, Sam Greydanus, Stephan Hoyer, Peter W. Battaglia, David N. Spergel, and Shirley Ho. Lagrangian neural networks. In International Conference on Learning Representations, 2020a.\\n\\nMiles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, and Shirley Ho. Discovering symbolic models from deep learning with inductive biases, 2020b. URL https://arxiv.org/abs/2006.11287.\\n\\nThai Duong and Nikolay Atanasov. Hamiltonian-based neural ODE networks on the SE(3) manifold for dynamics learning and control. In Proceedings of Robotics: Science and Systems, July 2021.\\n\\nLuca Falorsi, Pim de Haan, Tim R. Davidson, Nicola De Cao, Maurice Weiler, Patrick Forr\u00e9, and Taco S. Cohen. Explorations in homeomorphic variational auto-encoding. International Conference of Machine Learning Workshop on Theoretical Foundations and Application of Deep Generative Models, 2018.\\n\\nMarc Finzi, Ke Alexander Wang, and Andrew G Wilson. Simplifying Hamiltonian and Lagrangian neural networks via explicit constraints. Conference on Neural Information Processing Systems, 33(13880\u201313889):13, 2020.\"}"}
