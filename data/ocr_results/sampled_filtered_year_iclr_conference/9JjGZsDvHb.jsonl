{"id": "9JjGZsDvHb", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 10: Top-$k$ exact match accuracy of Transformer in terms of depth of reaction tree.\\n\\n| Depth | Top-$k$ accuracy (%) |\\n|-------|---------------------|\\n| 1     | 2 33.2 44.4 49.1 51.6 53.0 |\\n| 2     | 22.1 30.8 35.1 38.4 40.0 |\\n| 3     | 13.4 17.9 21.3 23.3 24.9 |\\n| 4     | 6.9 10.3 12.5 13.5 14.2 |\\n| 5     | 7.3 14.5 19.0 22.3 24.0 |\\n| 6     | 0 0 0 1.4 1.4 |\\n| 7     | 0 0 0 2.9 5.9 |\\n| 8     | 0 0 0 0 0 |\\n| 9     | 0 0 0 0 0 |\\n| 10    | 0 0 0 0 0 |\\n| 11    | 0 0 0 0 0 |\\n| 12    | 0 0 0 0 0 |\\n| 13    | 0 0 0 0 0 |\\n\\nTable 11: Top-$k$ exact match accuracy of Metro in terms of depth of reaction tree.\\n\\n| Depth | Top-$k$ accuracy (%) |\\n|-------|---------------------|\\n| 1     | 2 42.5 50.2 53.6 55.9 57.0 |\\n| 2     | 37.5 45.2 48.4 50.0 50.9 |\\n| 3     | 28.5 36.5 39.4 40.8 42.0 |\\n| 4     | 28.2 35.5 39.5 40.4 41.1 |\\n| 5     | 33.0 39.1 40.2 43.6 44.7 |\\n| 6     | 28.4 32.4 35.1 39.2 39.2 |\\n| 7     | 14.7 17.6 23.5 23.5 26.5 |\\n| 8     | 7.7 7.7 7.7 7.7 7.7 |\\n| 9     | 0 0 0 0 0 |\\n| 10    | 0 0 0 0 0 |\\n| 11    | 0 0 0 0 0 |\\n| 12    | 0 0 0 0 0 |\\n| 13    | 0 0 0 0 0 |\"}"}
{"id": "9JjGZsDvHb", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Fig. 5 outlines the details of three modules: encoder, decoder, and memory module. The encoder and decoder consist of several stacked attention layers. Each attention layer comprises multi-attention heads and a feed-forward layer. The attention heads of each attention layer perform attention mechanism in parallel and then are concatenated and projected into the final embedding. Specifically, an attention head (Scaled Dot-Product Attention) consists of three matrices: the queries $Q$, the keys $K$, and the values $V$. We get the attention scores by multiplying $Q$ and $K$, and then get the output by multiplying the attention scores and $V$. The computation process can be written as $\\\\text{Attention}(Q, K, V) = \\\\text{softmax}(\\\\frac{QK^T}{\\\\sqrt{d_k}})V$, (8)\\n\\nwhere $\\\\sqrt{d_k}$ is the scaling factor. As illustrated in Fig. 5, we use the self attention layers in the encoder to transform the embeddings of input SMILES into the latent representations, e.g., encoder output. The $Q$, $K$, and $V$ are the same hidden states from the previous layer in the encoder. But for the attention layer in the decoder, $P$ corresponds the embeddings or the hidden states of the output SMILES (right shift), while $Q$ and $V$ correspond the concatenation of the outputs of the encoder and the memory module. RetroXpert Yan et al. (2020) calls this type of attention as encoder-decoder attention. This type of attention enables the decoder to combine the information of the input SMILES and output SMILES to capture the relationship between the product molecule and reactant molecules. By encoding the information of the relationship, we can model the retrosynthesis reaction and make reasonable predictions. Note that the input SMILES given to the decoder are different from the output SMILES predicted by the decoder. The former is the right shift of the output SMILES. In addition, masked attention is used to avoid information leakage during training.\\n\\nWe compare our proposed benchmark with Retro* (Chen et al., 2020) and PaRoutes (Genheden & Bjerrum, 2022). As we have discussed in the Introduction, the benchmark of Retro* only consists...\"}"}
{"id": "9JjGZsDvHb", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of 189 routes in the test set, which is too small to comprehensively evaluate the performance. The test set of our benchmark consists of 5838 reaction trees, which can comprehensively evaluate the performance. PaRoutes does not use the purchasable compounds in eMolecule/ZINC as stop condition. But in our opinion, we think using purchasable compounds in eMolecule/ZINC as stop condition is a better solution with the following reason: If we use the reactions of other patents to build routes, the stock molecules which are constructed with the method proposed in PaRoutes may be different, which is inconsistent and tricky for the evaluation of machine learning models. Using the purchasable compounds in the eMolecule/ZINC databases as the stop condition has better scalability. We can train the machine learning model in one dataset and can apply this model for inference on other datasets directly.\"}"}
{"id": "9JjGZsDvHb", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2: Model overview. Our model consists of three modules: encoder, memory module, and decoder. The three styles mean the three different retrosynthesis reactions on the reaction route.\\n\\nReaction Graph Construction. We extract all the reactions from USPTO-all dataset (Lowe, 2012) and build a directed reaction graph. This is partially inspired by the construction of the reaction knowledge graph in (Li & Chen, 2022), which is used to identify the minimum steps needed to synthesize molecules for synthesis accessibility analysis.\\n\\nLet $G = \\\\{V, E\\\\}$ denote the directed graph, where $V$ is set of nodes $\\\\{v_1, v_2, \\\\cdots, v_N\\\\}$ with $|V| = N$ and $E$ is the set of edges. On this graph $G$, $v_i$ is the molecule that exists in the reaction database and $(v_i, v_j) (v_i \\\\to v_j)$ is the directed edge which means $v_i$ and $v_j$ exists in the same retrosynthesis reaction where $v_i$ is the product molecule and $v_j$ is one of the reactants. One molecule can be a product or reactant on the graph.\\n\\nReaction Tree Traversal. We perform reaction tree traversal after reaction graph construction. We treat the nodes whose in-degree is 0 as our desired product molecules, which can be represented as $V_p$. Given one node $v \\\\in V_p$, we perform dynamic programming and backtracking to find all the reaction trees $\\\\{T_{(1)}(v), T_{(2)}(v), \\\\cdots, T_{(s)}(v)\\\\}$ where the leaf nodes are starting materials for $v$. For two trees which have different retrosynthesis reactions for the same product, we think these are different trees.\\n\\nDataset Split. We first build the reaction graph based on all the reactions in the database, in order to identify reaction trees with smaller depth (Chen et al., 2020). We then split the reaction trees into training/validation/test datasets. This procedure aligns with the scenario in real-world organic chemical production, where we have access to a huge database of reactions and we can find the reaction tree with the shortest depth with our dataset construction method. Although the reaction tree of the training and test datasets will have overlapping reactions, we only select the target molecule that does not exist in the training dataset in the testing phase and do not provide any information about the reaction tree of this target molecule to avoid information leakage. Moreover, many organic synthesis routes share basic chemical reactions. Therefore, it is reasonable that the reaction trees in the training and test datasets have overlapping reactions. Note that there may be several reactions to synthesize a target molecule, leading to multiple reaction trees. We extract the reaction trees with the minimum depth as our dataset according to the synthetic accessibility criteria (Li & Chen, 2022).\\n\\n5 METRO\\n\\nIn this section, we describe the details of our proposed Metro. Our model is based on single-step retrosynthesis transformer (Vaswani et al., 2017; Karpov et al., 2019). We further propose a memory module that can capture the dependencies between molecules on the retrosynthesis route as context information. We don't rely on a template to formulate the reaction pattern or labeling number (Lin et al., 2022) to extract the reaction center. The training and inference can be done in an end-to-end manner. Fig. 2 outlines the sketch of our model.\"}"}
{"id": "9JjGZsDvHb", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.1 OVERVIEW OF METRO\\n\\n5.1.1 ARCHITECTURE\\n\\nAs shown in Fig. 2, our model consists of three components: encoder, memory module, and decoder. The input for the encoder is the SMILES representations of the product molecules (A, B, D) on the same reaction route. The SMILES representations of molecules have been processed. We add the start symbol \u2227 and the end symbol $ for the input. We also put the output in the decoder. But there is a difference between the output SMILES fed into the decoder and the output SMILES predicted by the decoder. We add the start symbol \u2227 (right shift) on the former output SMILES and the end symbol $ (left shift) on the later output SMILES. So there is not any leakage problem with the supervised information. The encoder transforms the embedding matrices of product molecules into latent representations (matrices), which is also called the encoder output. We take the encoder output as input to obtain the context information (memory output) through the memory module. After that, we concatenate the encoder output and memory output and feed it into the decoder.\\n\\n5.1.2 SEQUENCE GENERATION\\n\\nMetro models the prediction of reaction route as a sequence of molecule generation. Given the SMILES representations of (A, B, D) from the same reaction route as input, the output of our model is the SMILES representations of (B+C, D, E+F). When we predict B+C, the input is A and we will not consider the information of (B, D). When we complete the prediction of (B, C), we add B to the reaction route and use (A, B) as input to predict D. At the same time, we also add C to another reaction route as input to predict (G, H) as shown in Fig. 1. We repeat this step until all reactants are starting materials. So during the training stage, we do not consider the information of later molecules when making the current prediction. The attention mechanism ensures that our predictions of all reactions along the same route are parallelized at the training stage. This can be implemented by masking the input of later molecules.\\n\\n5.2 MEMORY MODULE\\n\\nIn this section, we will describe the details of the memory module. We employ attention mechanism (Vaswani et al., 2017) and memory network (Weston et al., 2015; Sukhbaatar et al., 2015; Ramsauer et al., 2020) to build our memory layer to capture the context information on the reaction route. We formally describe the blocking components of one memory layer as follows. Our memory module takes a series of encoder output $X_1, X_2, \\\\ldots, X_n$ as input, where $n$ is the depth of the reaction route. Each of $X_i \\\\in \\\\mathbb{R}^{l \\\\times d}$ is the latent embedding matrix corresponding to the $i$-th molecule of the encoder input (A, B, and D in our illustrated case). We first use the Linear Projection of Flattened Patches employed in ViT (Dosovitskiy et al., 2020) to transform matrices $X_1, X_2, \\\\ldots, X_n$ into vectors $v_1, v_2, \\\\ldots, v_n$ as follows:\\n\\n$$v_i = X_i W_p,$$\\n\\nwhere $W_p \\\\in \\\\mathbb{R}^{(l \\\\cdot d) \\\\times d}$, and $l$ is the length of SMILES. We then use $v_i$ to compute the query and key and use $X_i$ to compute the value as follows:\\n\\n$$Q = [v_1, v_2, \\\\ldots, v_n]^T W_Q$$\\n$$K = [v_1, v_2, \\\\ldots, v_n]^T W_K$$\\n$$V = [X_1, X_2, \\\\ldots, X_n]^T W_V$$\\n\\nwhere $W_Q \\\\in \\\\mathbb{R}^{d \\\\times d_k}$ is the query matrix, $W_K \\\\in \\\\mathbb{R}^{d \\\\times d_k}$ is the key matrix, and $W_V \\\\in \\\\mathbb{R}^{d \\\\times d_k}$ is the value matrix. Then we compute the memory embedding matrices as follows:\\n\\n$$\\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^T}{\\\\sqrt{d_k}}\\\\right)V$$\\n\\nThe output $\\\\text{Attention}(Q, K, V)$ is computed as a weighted sum of the values $V$, where the weight assigned to each value is the attention score between two molecules.\"}"}
{"id": "9JjGZsDvHb", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the two embedding matrices instead of vectors due to the embedding matrix representation of the molecules. We also utilize multi-head attention to jointly compute the context information from different subspaces and then we use residual connection (He et al., 2016) and fully connected layer (FFN) to get the memory output $M_1, M_2, \\\\ldots, M_n$. We treat the memory output $[M_1, M_2, \\\\ldots, M_n]^T$ as context information. $M_i$ captures the dependencies between $X_i$ and $X_1, X_2, \\\\ldots, X_{i-1}$. After getting the embedding matrices of context information, we concatenate them with the encoder output to feed the decoder.\\n\\n5.3 CONTEXT INFORMATION\\n\\nThe intuition of exploiting the reaction tree (context information) instead of single-step prediction is that we can better prune the reaction search space. The single-step retrosynthesis reaction model searches the entire chemical reaction space of a molecule, but some candidates, although valid, do not align with the synthesis goal of the current synthetic route and should be discarded when taking the entire reaction tree into account. Experimental results show that our proposed Metro improves by a large margin over Transformer.\\n\\n5.4 INFERENCE\\n\\nAlgorithm 1\\n\\nInference of the reaction tree given a target molecule\\n\\n1: Input: Target molecule $T$, starting material set $S$\\n2: Initialize reactant set $R = \\\\emptyset$, reaction route set $L = \\\\emptyset$\\n3: Put the initial route $[T]$ into $L$\\n4: while $L$ is not an empty set do\\n5: Take a route $l$ from $L$\\n6: Predict the reactants $r_l$ given $l$\\n7: for reactant $r_i(l)$ in $r_l$ do\\n8: if $r_i(l) \\\\in S$ then\\n9: Put $r_i(l)$ into $R$\\n10: else\\n11: Generate a new route $l' = l + [r_i(l)]$\\n12: Put $l'$ into $L$\\nreturn predicted reactant set $R$\\n\\nIn the inference stage, we start from the target molecule $T$, and perform backward chaining to do a series of one-step retrosynthesis predictions until the reactants are all starting materials. This backward method is also adopted by Chen et al. (2020). After getting the predicted reactant molecules for the retrosynthesis reaction at each step, we refer to the set of starting materials to check whether the reactant molecules are starting materials. If they are starting materials, we add them into the prediction reactant set. Otherwise, we get a new reaction route and predict the next step's output. Once we obtain the predicted reactant set, we compare it with the ground truth reactant set and get the inference accuracy. The inference process is outlined in the Algorithm 1. Note that we perform a Depth First Search (DFS) for all models in our paper.\\n\\n6 EXPERIMENTS\\n\\nIn this section, we evaluate the performance of different base models on our proposed benchmark for the retrosynthetic planning task.\\n\\n6.1 EXPERIMENT SETUP\\n\\nDataset. We utilize the public dataset USPTO-full to construct the benchmark for the retrosynthetic planning task. The USPTO-full dataset consists of 1,808,937 reactions. After removing invalid and duplicate reactions, we obtain 906,164 reactions. Based on these reactions, we construct a reaction graph. We treat the molecules whose out-degree is 0 as our desired target molecules. And we use dynamic programming and backtracking to find all the reaction trees for each target molecule. There are 124,869 molecules for which we can find the reaction trees where the leaf nodes are all starting materials. We extract molecules with shortest depths greater than 1 and split these molecules into training, validation, and test datasets. For those molecules with shortest depths between 2 and 10, they are randomly split into training/validation/test datasets following 80%/10%/10% proportions. For those molecules with shortest depths larger than 10, we put them into the test dataset to evaluate the performance of models on the molecules which need too many steps to synthesize. The number\"}"}
{"id": "9JjGZsDvHb", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Top-k exact match accuracy.\\n\\n| Methods         | Top-k accuracy % |\\n|-----------------|------------------|\\n|                 | 1    | 2    | 3    | 4    | 5    |\\n| Template-based  |      |      |      |      |      |\\n| RETROSIM (Coley et al., 2017) | 23.2 | 27.2 | 28.9 | 30.0 | 30.5 |\\n| NEURALSYM (Segler & Waller, 2017) | 26.8 | 32.2 | 34.1 | 35.2 | 35.8 |\\n| GLN (Dai et al., 2019) | 25.9 | 32.7 | 35.0 | 36.5 | 37.2 |\\n| Semi-template-based |      |      |      |      |      |\\n| G2G S (Shi et al., 2020) | 4.0  | 6.1  | 7.2  | 8.2  | 8.8  |\\n| GGRAPH RETRO (Somnath et al., 2021) | 14.4 | -    | -    | -    | -    |\\n| Template-free   |      |      |      |      |      |\\n| TRANSFORMER (Karpov et al., 2019) | 24.3 | 33.1 | 37.2 | 39.7 | 41.2 |\\n| MEGAN (Sacha et al., 2021) | 20.1 | 29.5 | 34.9 | 38.3 | 40.4 |\\n| MRETRO (Ours)   | 37.5 | 47.6 | 48.3 | 50.2 | 51.2 |\\n\\nImplementation Details.\\n\\nBased on the vanilla TRANSFORMER (Karpov et al., 2019), we only introduce additional 3 memory layers and do not touch other components. For the hyper-parameters, we implement TRANSFORMER using Pytorch (Paszke et al., 2019).\"}"}
{"id": "9JjGZsDvHb", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3: The top-1, top-3, and top-5 test accuracy in terms of depth (the minimum steps required to synthesize a target molecule). There is no beam search for GraphRetro because of missing probabilities of each answer in their released codes. The cases whose depth is 10 have too few data points for the results to be convincing, although there is a jump at depth 10 for Neuralsym.\\n\\nwe directly followed the reported setting in their released codes, and did not perform additional hyperparameter tuning. We still achieve a dramatic improvement for the planning task, which well demonstrates the effectiveness of our proposed method. The details of hyper-parameters can also be found in Appendix B.1. Metro is trained on 2 NVIDIA Tesla V100 GPUs.\\n\\n6.2 RESULTS\\n\\nMain Results.\\n\\nTable 1 reports the main results. From the table, we observe that our proposed Metro achieves the best performance and outperforms the baselines by at least 10.7% of top-1 test accuracy. Besides, the results demonstrate that our proposed model achieves a better performance against Transformer. More specifically, we can improve upon Transformer by a margin of 13.2%, 14.5%, 11.1%, 10.5%, and 10.0% on top-1, top-2, top-3, top-4, and top-5 accuracy respectively. It demonstrates that the context information on the reaction route captured by our introduced memory module enables our model to search the reactants for single-step retrosynthesis reaction in a reasonable reaction space specified for this reaction route. Context information also preventing us searching through the whole reaction space, thus avoiding some wrong answers. Moreover, the results of baselines on our retrosynthetic planning task do not match well with single-step retrosynthesis prediction. Existing semi-template-based models outperform or match the template-based and template-free models on the single-step retrosynthesis prediction but have poor performance on our task. A reasonable reason is that approximately 95% of the reactions in the USPTO-50K dataset used for single-step retrosynthesis reactions have only one reaction center. But in our constructed dataset, approximately 30% of reactions have multiple reaction centers. G2Gs only can handle cases with one reaction center, and so perform badly on our task. Template-free models, which do not need to extract templates or find the reaction center using the labeling number, model the retrosynthesis as a sequence-to-sequence problem or a sequence of graph edits, which has better scalability and perform better on retrosynthetic planning tasks.\\n\\nSynthetic Accessibility Analysis.\\n\\nWe group by the length of the reaction tree of the target molecules and report the top1, top3, and top5 test accuracy for each group. We report the results in Fig 3. The results show that the more steps required to synthesize a target molecule, the lower the accuracy of all models' predictions. When the number of steps required is greater than 9, our model cannot predict the starting materials to synthesize the target molecule. This result is also consistent with molecule synthetic accessibility. The molecules are hard to synthesize when the number of reaction steps required is larger in Li & Chen (2022). Our results further support this view. From Fig 3, we can observe that our model still performs better than all baselines when predicting the starting materials of the target molecules which need 2-7 reaction steps to synthesize. When testing on the 49 cases which need 8-10 steps, only GLN and Neuralsym outperform or match our model. When testing on cases that need more than 10 steps, all models cannot make an accurate prediction. One future work is to add the template information to our model to improve the performance of the cases which need too many steps to synthesize.\\n\\n6.3 CASE STUDY\\n\\nIn Fig. 4, we visualize the predictions from Metro and Transformer. The top of Fig. 4 are the correct predictions from Metro, and the bottom are the wrong predictions from Transformer.\"}"}
{"id": "9JjGZsDvHb", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Case Study. We split the predicted reaction tree into retrosynthesis reactions. On the top is the correct reaction tree predicted by METRO, and on the bottom is the predicted reaction tree predicted by TRANSFORMER.\\n\\nFrom Figure 4, we can observe that TRANSFORMER makes a wrong prediction on the third-step retrosynthesis reaction. Since the whole reaction space on which TRANSFORMER searches is too large to get correct predictions. Due to the dependency of molecules on the reaction route as context information, our METRO can search the reactants on a reasonable space. So our model can make correct predictions.\\n\\n**RELATED WORK**\\n\\nRetrosynthesis Model. Existing machine learning models for retrosynthesis prediction can be classified into Template-based, Semi-template-based, and Template-free models. Template-based retrosynthetic algorithms (Chen et al., 2020; Coley et al., 2017; Dai et al., 2019; Segler & Waller, 2017; Chen & Jung, 2021; Seidl et al., 2021) extract patterns from the training data which encode how atoms and bonds change during the reaction. Semi-template-based models (Shi et al., 2020; Yan et al., 2020; Somnath et al., 2021) predict reactants via two stages: reaction center identification and reactants generation. Template-free algorithms (Liu et al., 2017; Zheng et al., 2019; Chen et al., 2019; Karpov et al., 2019) model the retrosynthesis as a sequence-to-sequence problem. Our work is closely related to the single-step retrosynthesis transformer. To capture the context information of the reaction route, we introduce the memory module (Sukhbaatar et al., 2015).\\n\\nRetrosynthetic Planning Search Algorithm. Existing Retrosynthetic Planning Search Algorithms model the retrosynthetic planning as a search problem. MCTS (Segler et al., 2018) employs Monte Carlo tree search, DFPN-E (Kishimoto et al., 2019) combines Depth-First Proof-Number (DFPN) with Heuristic Edge Initialization, Retro* (Chen et al., 2020) proposes a neural-based A*-like algorithm, and RetroGraph (Xie et al., 2022) proposes a graph-based search policy. Our solution focuses on the base model which differs greatly from search algorithms.\\n\\nSynthetic Accessibility. Some machine learning models have been proposed for the estimation of the synthetic difficulty of molecules, such as SAscore (Ertl & Schuffenhauer, 2009), PGFS (Gottipati et al., 2020), SCScore (Coley et al., 2018b), SYBA (Vor\u0161il\u00e1k et al., 2020), RAscore (Thakkar et al., 2021), and knowledge graph (Li & Chen, 2022). These approaches divide drug molecules into easy and hard-to-synthesize categories, where hard-to-synthesize molecules require longer reaction routes to synthesize.\\n\\n**CONCLUSION**\\n\\nIn this work, we build a reaction-tree-based benchmark. We also propose a new retrosynthetic planning base model by extending the single-step retrosynthesis transformer with an additional memory module. Our memory module can capture the context information of the reaction route, enabling us to make a better prediction for the retrosynthesis reaction on this route.\"}"}
{"id": "9JjGZsDvHb", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Benson Chen, Tianxiao Shen, Tommi S Jaakkola, and Regina Barzilay. Learning to make generalizable and diverse predictions for retrosynthesis. arXiv preprint arXiv:1910.09688, 2019.\\n\\nBinghong Chen, Chengtao Li, Hanjun Dai, and Le Song. Retro*: learning retrosynthetic planning with neural guided a* search. In International Conference on Machine Learning, 2020.\\n\\nShuan Chen and Yousung Jung. Deep retrosynthetic reaction prediction using local reactivity and global attention. JACS Au, 1(10):1612\u20131620, 2021.\\n\\nConnor W Coley, Luke Rogers, William H Green, and Klavs F Jensen. Computer-assisted retrosynthesis based on molecular similarity. ACS Central Science, 3(12):1237\u20131245, 2017.\\n\\nConnor W Coley, William H Green, and Klavs F Jensen. Machine learning in computer-aided synthesis planning. Accounts of Chemical Research, 51(5):1281\u20131289, 2018a.\\n\\nConnor W Coley, Luke Rogers, William H Green, and Klavs F Jensen. Scscore: synthetic complexity learned from a reaction corpus. Journal of Chemical Information and Modeling, 58(2):252\u2013261, 2018b.\\n\\nHanjun Dai, Chengtao Li, Connor Coley, Bo Dai, and Le Song. Retrosynthesis prediction with conditional graph logic network. In Advances in Neural Information Processing Systems, 2019.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n\\nPeter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of Cheminformatics, 1(1):1\u201311, 2009.\\n\\nSamuel Genheden and Esben Bjerrum. Paroutes: towards a framework for benchmarking retrosynthetic route predictions. Digital Discovery, 1(4):527\u2013539, 2022.\\n\\nSamuel Genheden, Amol Thakkar, Veronika Chadimov\u00e1, Jean-Louis Reymond, Ola Engkvist, and Esben Bjerrum. Aizynthfinder: a fast, robust and flexible open-source software for retrosynthetic planning. Journal of Cheminformatics, 12(1):1\u20139, 2020.\\n\\nSai Krishna Gottipati, Boris Sattarov, Sufeng Niu, Yashaswi Pathak, Haoran Wei, Shengchao Liu, Simon Blackburn, Karam Thomas, Connor Coley, Jian Tang, et al. Learning to navigate the synthetically accessible chemical space using reinforcement learning. In International Conference on Machine Learning, 2020.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.\\n\\nPavel Karpov, Guillaume Godin, and Igor V Tetko. A transformer model for retrosynthesis. In International Conference on Artificial Neural Networks, 2019.\\n\\nJunsu Kim, Sungsoo Ahn, Hankook Lee, and Jinwoo Shin. Self-improved retrosynthetic planning. In International Conference on Machine Learning, 2021.\\n\\nAkihiro Kishimoto, Beat Buesser, Bei Chen, and Adi Botea. Depth-first proof-number search with heuristic edge cost and application to chemical synthesis planning. In Advances in Neural Information Processing Systems, 2019.\\n\\nBaiqing Li and Hongming Chen. Prediction of compound synthesis accessibility based on reaction knowledge graph. Molecules, 27(3):1039, 2022.\"}"}
{"id": "9JjGZsDvHb", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atom-to-atom mapping: A benchmarking study of popular mapping algorithms and consensus strategies.\\n\\nMolecular Informatics, 41(4):2100138, 2022.\\n\\nBowen Liu, Bharath Ramsundar, Prasad Kawthekar, Jade Shi, Joseph Gomes, Quang Luu Nguyen, Stephen Ho, Jack Sloane, Paul Wender, and Vijay Pande. Retrosynthetic reaction prediction using neural sequence-to-sequence models.\\n\\nACS Central Science, 3(10):1103\u20131113, 2017.\\n\\nDaniel Mark Lowe. Extraction of chemical structures and reactions from the literature. PhD thesis, University of Cambridge, 2012.\\n\\nDai Hai Nguyen and Koji Tsuda. A generative model for molecule generation based on chemical reaction trees. arXiv preprint arXiv:2106.03394, 2021.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, 2019.\\n\\nHubert Ramsauer, Bernhard Sch\u00e4fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovi\u0107, Geir Kjetil Sandve, et al. Hopfield networks is all you need. arXiv preprint arXiv:2008.02217, 2020.\\n\\nMiko\u0142aj Sacha, Miko\u0142aj B\u0142az, Piotr Byrski, Pawe\u0142 Dabrowski-Tumanski, Miko\u0142aj Chrominski, Rafa\u0142 Loska, Pawe\u0142 W\u0142odarczyk-Pruszynski, and Stanis\u0142aw Jastrzebski. Molecule edit graph attention network: modeling chemical reactions as sequences of graph edits. Journal of Chemical Information and Modeling, 61(7):3273\u20133284, 2021.\\n\\nMarwin Segler, Mike Preu\u00df, and Mark P Waller. Towards\u2019 alphachem\u2019: Chemical synthesis planning with tree search and deep neural network policies. arXiv preprint arXiv:1702.00020, 2017.\\n\\nMarwin HS Segler and Mark P Waller. Neural-symbolic machine learning for retrosynthesis and reaction prediction. Chemistry\u2013A European Journal, 23(25):5966\u20135971, 2017.\\n\\nMarwin HS Segler, Mike Preuss, and Mark P Waller. Planning chemical syntheses with deep neural networks and symbolic ai. Nature, 555(7698):604, 2018.\\n\\nPhilipp Seidl, Philipp Renz, Natalia Dyubankova, Paulo Neves, Jonas Verhoeven, J\u00f6rg K Wegner, Sepp Hochreiter, and G\u00fcnter Klambauer. Modern hopfield networks for few-and zero-shot reaction prediction. arXiv preprint arXiv:2104.03279, 2021.\\n\\nChence Shi, Minkai Xu, Hongyu Guo, Ming Zhang, and Jian Tang. A graph to graphs framework for retrosynthesis prediction. In International Conference on Machine Learning, 2020.\\n\\nRyosuke Shibukawa, Shoichi Ishida, Kazuki Yoshizoe, Kunihiro Wasa, Kiyosei Takasu, Yasushi Okuno, Kei Terayama, and Koji Tsuda. Compret: a comprehensive recommendation framework for chemical synthesis planning with algorithmic enumeration. Journal of Cheminformatics, 12(1):1\u201314, 2020.\\n\\nVignesh Ram Somnath, Charlotte Bunne, Connor Coley, Andreas Krause, and Regina Barzilay. Learning graph models for retrosynthesis prediction. In Advances in Neural Information Processing Systems, 2021.\\n\\nTeague Sterling and John J Irwin. Zinc 15\u2013ligand discovery for everyone. Journal of Chemical Information and Modeling, 55(11):2324\u20132337, 2015.\\n\\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in Neural Information Processing Systems, 2015.\\n\\nAmol Thakkar, Veronika Chadimov\u00e1, Esben Jannik Bjerrum, Ola Engkvist, and Jean-Louis Reymond. Retrosynthetic accessibility score (rascore)\u2013rapid machine learned synthesizability classification from ai driven retrosynthetic planning. Chemical Science, 12(9):3339\u20133349, 2021.\"}"}
{"id": "9JjGZsDvHb", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.\\n\\nMilan Vor\u0161il\u00e1k, Michal Kol\u00e1\u0159, Ivan \u010cmelo, and Daniel Svozil. Syba: Bayesian estimation of synthetic accessibility of organic compounds. Journal of Cheminformatics, 12(1):1\u201313, 2020.\\n\\nDavid Weininger. Smiles, a chemical language and information system. Journal of Chemical Information and Computer Sciences, 1988.\\n\\nJason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In International Conference on Learning Representations, 2015.\\n\\nShufang Xie, Rui Yan, Peng Han, Yingce Xia, Lijun Wu, Chenjuan Guo, Bin Yang, and Tao Qin. Retrograph: Retrosynthetic planning with graph search. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022.\\n\\nChaochao Yan, Qianggang Ding, Peilin Zhao, Shuangjia Zheng, Jinyu Yang, Yang Yu, and Junzhou Huang. Retroxpert: Decompose retrosynthesis prediction like a chemist. In Advances in Neural Information Processing Systems, 2020.\\n\\nYemin Yu, Ying Wei, Kun Kuang, Zhengxing Huang, Huaxiu Yao, and Fei Wu. Grasp: Navigating retrosynthetic planning with goal-driven policy. In Advances in Neural Information Processing Systems, 2022.\\n\\nShuangjia Zheng, Jiahua Rao, Zhongyue Zhang, Jun Xu, and Yuedong Yang. Predicting retrosynthetic reactions using self-corrected transformer neural networks. Journal of Chemical Information and Modeling, 60(1):47\u201355, 2019.\"}"}
{"id": "9JjGZsDvHb", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: The number of target molecules in training/validation/test datasets in term of the shortest depths to synthesize the target molecules.\\n\\n| Dataset   | #Molecules | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 |\\n|-----------|------------|---|---|---|---|---|---|---|---|----|----|----|----|\\n| Training  | 22,903     |   |   |   |   |   |   |   |   | 25 |    |    |    |\\n| Validation| 2,862      |   |   |   |   |   |   |   |   |    | 32 |    | 1  |\\n| Test      | 2,862      |   |   |   |   |   |   |   |   |    |    | 0  | 0  |\\n\\nB.1 IMPLEMENTATION Details\\n\\nWe use Pytorch (Paszke et al., 2019) to implement Metro. The codes of baselines are implemented referring to the implementation of RETROSIM 2, NEURALSYM 3, GLN 4, G2G 5, GRAPHRETRO 6, TRANSFORMER 7, and MEGAN 8. All the experiments in this work are conducted on a single NVIDIA Tesla V100 with 32GB memory size. The software that we use for experiments are Python 3.6.8, pytorch 1.9.0, pytorch-scatter 2.0.9, pytorch-sparse 0.6.12, numpy 1.19.2, torchvision 0.10.0, CUDA 10.2.89, CUDNN 7.6.5, einops 0.4.1, and torchdrug 0.1.3.\\n\\nB.2 HYPERPARAMETER DETAILS\\n\\nTable 3: The hyper-parameters for Metro.\\n\\n| Parameter       | Value |\\n|-----------------|-------|\\n| max length      | 200   |\\n| embedding size  | 64    |\\n| encoder layers  | 3     |\\n| decoder layers  | 3     |\\n| memory layers   | 3     |\\n| attention heads | 10    |\\n| FFN hidden      | 512   |\\n| dropout         | 0.1   |\\n| epochs          | 4000  |\\n| batch size      | 64    |\\n| warmup          | 16000 |\\n| lr factor       | 20    |\\n\\nC DETAILS OF SYNTHETIC ACCESSIBILITY ANALYSIS\\n\\n2 https://github.com/connorcoley/retrosim\\n3 https://github.com/linminhtoo/neuralsym\\n4 https://github.com/Hanjun-Dai/GLN\\n5 https://torchdrug.ai/docs/tutorials/retrosynthesis\\n6 https://github.com/vsomnath/graphretro\\n7 https://github.com/bigchem/synthesis\\n8 https://github.com/molecule-one/megan\"}"}
{"id": "9JjGZsDvHb", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4: Top-$k$ exact match accuracy of RETROSIM in terms of depth of reaction tree.\\n\\n| Depth | Top-$k$ accuracy (%) |\\n|-------|----------------------|\\n| 1     | 28.3 33.2 35.3 36.7 37.4 |\\n| 2     | 21.0 24.5 26.1 27.1 27.7 |\\n| 3     | 18.9 22.0 22.8 23.9 23.9 |\\n| 4     | 14.5 17.2 19.1 19.1 19.1 |\\n| 5     | 12.3 14.0 14.0 14.5 14.5 |\\n| 6     | 10.8 13.5 13.5 14.9 14.9 |\\n| 7     | 11.8 ? 14.7 17.6 17.6 |\\n| 8     | 0 0 0 0 0 |\\n| 9     | 0 0 0 0 0 |\\n| 10    | 0 0 0 0 0 |\\n| 11    | 0 0 0 0 0 |\\n| 12    | 0 0 0 0 0 |\\n| 13    | 0 0 0 0 0 |\\n\\nTable 5: Top-$k$ exact match accuracy of EURALSYM in terms of depth of reaction tree.\\n\\n| Depth | Top-$k$ accuracy (%) |\\n|-------|----------------------|\\n| 1     | 32.4 39.0 41.2 42.5 43.4 |\\n| 2     | 24.7 30.1 31.7 33.0 33.7 |\\n| 3     | 20.5 24.4 26.0 26.7 27.2 |\\n| 4     | 18.9 22.5 23.8 24.0 24.0 |\\n| 5     | 14.5 15.1 16.2 16.8 16.8 |\\n| 6     | 9.5 10.8 12.2 12.2 12.2 |\\n| 7     | 23.5 ? 23.5 23.5 23.5 |\\n| 8     | 0 0 0 0 0 |\\n| 9     | 0 0 0 0 0 |\\n| 10    | 0 0 0 0 0 |\\n| 11    | 0 0 0 0 0 |\\n| 12    | 0 0 0 0 0 |\\n| 13    | 0 0 0 0 0 |\\n\\nTable 6: Top-$k$ exact match accuracy of GLN in terms of depth of reaction tree.\\n\\n| Depth | Top-$k$ accuracy (%) |\\n|-------|----------------------|\\n| 1     | 33.2 41.1 43.7 45.1 45.8 |\\n| 2     | 22.1 29.7 31.7 33.7 34.6 |\\n| 3     | 18.1 23.1 26.1 26.9 27.6 |\\n| 4     | 16.4 20.8 21.8 23.0 23.3 |\\n| 5     | 9.5 12.8 15.6 16.8 17.3 |\\n| 6     | 8.1 8.1 8.1 8.1 9.5 |\\n| 7     | 8.8 8.8 8.8 8.8 8.8 |\\n| 8     | 15.4 ? 15.4 15.4 15.4 |\\n| 9     | 0 0 0 0 0 |\\n| 10    | 0 0 0 0 0 |\\n| 11    | 0 0 0 0 0 |\\n| 12    | 0 0 0 0 0 |\\n| 13    | 0 0 0 0 0 |\"}"}
{"id": "9JjGZsDvHb", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 7: Top-$k$ exact match accuracy of G2G in terms of depth of reaction tree.\\n\\n| Depth | 1  | 2  | 3  | 4  | 5  |\\n|-------|----|----|----|----|----|\\n|       |    | 6.5| 10.1| 11.9| 13.5| 14.4|\\n| 3     | 2.7| 3.9| 4.6| 5.1| 5.8|\\n| 4     | 1.0| 1.1| 1.5| 1.8| 1.8|\\n| 5     | 0.2| 0.5| 0.5| 0.5| 0.5|\\n| 6     | 0  | 0  | 0  | 0  | 0  |\\n| 7     | 0  | 0  | 0  | 0  | 0  |\\n| 8     | 0  | 0  | 0  | 0  | 0  |\\n| 9     | 0  | 0  | 0  | 0  | 0  |\\n| 10    | 0  | 0  | 0  | 0  | 0  |\\n| 11    | 0  | 0  | 0  | 0  | 0  |\\n| 12    | 0  | 0  | 0  | 0  | 0  |\\n| 13    | 0  | 0  | 0  | 0  | 0  |\\n\\nTable 8: Top-$k$ exact match accuracy of GRAPHETRO in terms of depth of reaction tree.\\n\\n| Depth | 1  | 2  | 3  | 4  | 5  |\\n|-------|----|----|----|----|----|\\n| 2     | 21.3| -  | -  | -  | -  |\\n| 3     | 11.6| -  | -  | -  | -  |\\n| 4     | 6.0 | -  | -  | -  | -  |\\n| 5     | 3.7 | -  | -  | -  | -  |\\n| 6     | 0  | -  | -  | -  | -  |\\n| 7     | 0  | -  | -  | -  | -  |\\n| 8     | 0  | -  | -  | -  | -  |\\n| 9     | 0  | -  | -  | -  | -  |\\n| 10    | 0  | -  | -  | -  | -  |\\n| 11    | 0  | -  | -  | -  | -  |\\n| 12    | 0  | -  | -  | -  | -  |\\n| 13    | 0  | -  | -  | -  | -  |\\n\\nTable 9: Top-$k$ exact match accuracy of MEGAN in terms of depth of reaction tree.\\n\\n| Depth | 1  | 2  | 3  | 4  | 5  |\\n|-------|----|----|----|----|----|\\n| 2     | 26.2| 37.8| 44.5| 48.6| 51.0|\\n| 3     | 18.4| 27.3| 32.6| 35.3| 37.5|\\n| 4     | 13.0| 20.0| 23.1| 26.1| 27.4|\\n| 5     | 8.6 | 14.5| 18.1| 19.9| 21.1|\\n| 6     | 10.6| 14.5| 18.4| 21.8| 24.0|\\n| 7     | 1.4 | 1.4 | 2.7 | 5.4 | 5.4|\\n| 8     | 0   | 0   | 0   | 0   | 2.9|\\n| 9     | 0   | 0   | 0   | 0   | 0  |\\n| 10    | 0   | 0   | 0   | 0   | 0  |\\n| 11    | 0   | 0   | 0   | 0   | 0  |\\n| 12    | 0   | 0   | 0   | 0   | 0  |\\n| 13    | 0   | 0   | 0   | 0   | 0  |\"}"}
{"id": "9JjGZsDvHb", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"METRO: MEMORY-ENHANCED TRANSFORMER FOR RETROSYNTHETIC PLANNING VIA REACTION TREE\\n\\nAnonymous authors\\nPaper under double-blind review\\n\\nABSTRACT\\n\\nRetrosynthetic planning plays a critical role in drug discovery and organic chemistry. Starting from a target molecule as the root node, it aims to find a complete reaction tree subject to the constraint that all leaf nodes belong to a set of starting materials. The multi-step reactions are crucial because they determine the flow chart in the production of the Organic Chemical Industry. However, existing datasets lack curation of tree-structured multi-step reactions, and fail to provide such reaction trees, limiting models' understanding of organic molecule transformations. In this work, we first develop a benchmark curated for the retrosynthetic planning task, which consists of 124,869 reaction trees retrieved from the public USPTO-full dataset. On top of that, we propose Metro: Memory-enhanced Transformer for Retrosynthetic planning. Specifically, the dependency among molecules in the reaction tree is captured as context information for multi-step retrosynthesis predictions through transformers with a memory module. Extensive experiments show that Metro dramatically outperforms existing single-step retrosynthesis models by at least 10.7% in top-1 accuracy. The experiments demonstrate the superiority of exploiting context information in the retrosynthetic planning task. Moreover, the proposed model can be directly used for synthetic accessibility analysis, as it is trained on reaction trees with the shortest depths. Our work is the first step towards a brand new formulation for retrosynthetic planning in the aspects of data construction, model design, and evaluation.\\n\\nINTRODUCTION\\n\\nRetrosynthetic planning is a fundamental problem in organic chemistry (Coley et al., 2018a; Genheden et al., 2020). The goal of retrosynthetic planning is to find a series of starting molecules that go through a sequence of reactions, which can also be represented as reaction tree, to synthesize the target molecule. Retrosynthetic planning can be decomposed into multi-step retrosynthesis reactions through which we find all starting molecules that meet the requirements. The multi-step reactions outline the transformation direction of organic molecules and the transformation target. In the production of the Organic Chemical Industry, it requires us to design efficient organic synthesis routes to synthesize our desired target products at a low cost. Therefore, given a target molecule, predicting reasonable and efficient reaction routes to synthesize this molecule is a very crucial problem in both machine learning and organic chemistry (Segler et al., 2018).\\n\\nTo tackle this problem, past works, including MCTS (Segler et al., 2018), DFPN-E (Kishimoto et al., 2019), Retro* (Chen et al., 2020), self-improved retrosynthetic planning (Kim et al., 2021), RetroGraph (Xie et al., 2022), and Grasp (Yu et al., 2022), model the retrosynthetic planning as a search problem (Xie et al., 2022). Specifically, they first utilize reactions to train a template-based MLP retrosynthesis model (Segler et al., 2017) and then learn a search algorithm to perform a backward search to transform the molecules through retrosynthesis predictions until all the reactants are starting materials (Chen et al., 2020). The current benchmark for test evaluation of retrosynthetic planning models consists of 189 test routes (Chen et al., 2020).\\n\\nThese approaches have the following limitations: 1) the training dataset of single-step reactions limits the understanding of the transformation of organic molecules as a sequence of chaining chemical reactions. 2) past works use single-step retrosynthesis models, which neglect the context information in the reaction tree. 3) the test set is too small to comprehensively evaluate the performance. 4) the\"}"}
{"id": "9JjGZsDvHb", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this work, we address these limitations by first constructing a new benchmark with 124,869 reaction trees retrieved from the public USPTO dataset and leverage the retrosynthesis transformer with an additional memory module to capture reaction tree information for retrosynthetic planning.\\n\\nBenchmark.\\n\\nSCScore (Coley et al., 2018b) concludes that the number of steps required to synthesize a molecule is an accurate metric for estimating molecule synthetic accessibility. Based on this observation and inspired by the prediction of synthesis accessibility with reaction knowledge graph (Li & Chen, 2022), we construct a reaction graph from the existing reactions in the database. On the reaction graph, directed edges represent retrosynthesis reactions where the starting point denotes the product molecule and the ending point represents the reactant molecule to synthesize this product. Given a target molecule, we can search the shortest routes to form an efficient reaction tree from the reaction graph, while the ending points of these routes are the starting molecules that satisfy the requirements. By constructing the reaction trees for target molecules, we can obtain a new benchmark for our retrosynthetic planning task.\\n\\nMetro.\\n\\nIn this work, we propose Metro: Memory-enhanced Transformer for Retrosynthetic planning by extending Transformer with an additional memory module. Our proposed Metro can capture the dependency among the molecules on the reaction route as context information. By taking the context information into consideration, we can control the search within a reasonable reaction space specified for the reaction route. Extensive experimental results on retrosynthetic planning show that Metro achieves up to 13.2%, 14.5%, 11.1%, 10.5%, and 10.0% over transformer on top-1, top-2, top-3, top-4, and top-5 accuracy, which demonstrates the superiority of exploiting context information for retrosynthetic planning task.\\n\\n2 PRELIMINARIES\\n\\nIn this section, we formally define important terminologies used in the rest of the paper, including SMILES representation, starting material, and reaction tree.\\n\\nSMILES Representation.\\nThe simplified molecule-input line-entry system (SMILES) (Weininger, 1988) is a chemical specification for describing the structure of chemical compounds using strings. Organic compounds can be denoted by SMILE representations like in Fig. 1, which is well suited for machine learning models to process. We denote the SMILES representation of molecule \\\\( x \\\\) as \\\\( s(x) \\\\), where \\\\( s(x)_i \\\\) is the character at the \\\\( i \\\\)-th position of the string \\\\( s(x) \\\\). Given a reaction \\\\( r_1 + r_2 + \\\\ldots + r_n \\\\rightarrow p \\\\), the SMILES representation of this reaction is as follows:\\n\\n\\\\[\\n\\\\begin{align*}\\n  s(r_1) \\\\cdot s(r_2) \\\\cdot \\\\ldots \\\\cdot s(r_n) \\\\rightarrow s(p), \\\\\\\\\\n\\\\end{align*}\\n\\\\]\\n\\nStarting Material.\\n\\nWe denote the space of all chemical molecules as \\\\( M \\\\). The starting materials are a special set of molecules, denoted as \\\\( S \\\\subseteq M \\\\). AiZynthFinder (Genheden et al., 2020) defines the starting material as a commercially purchasable compound. ZINC (Sterling & Irwin, 2015) releases the open source databases of purchasable compounds. We define this list of compounds in these databases as our starting materials.\\n\\nReaction Tree and Reaction Routes.\\n\\nGiven the above definitions, we can denote a reaction tree (Shibukawa et al., 2020; Nguyen & Tsuda, 2021) as \\\\( T = \\\\{ T, R, I, \\\\tau \\\\} \\\\), where \\\\( T \\\\in M \\\\setminus S \\\\) is the product molecule we desire to synthesize (A in Fig. 1), \\\\( R = \\\\{ r_1, r_2, \\\\ldots, r_n \\\\} \\\\subseteq S \\\\) is the set of starting materials (E, F, G, H in Fig. 1) that go through a series of reactions \\\\( \\\\tau \\\\) to synthesize \\\\( A \\\\), and \\\\( I = \\\\{ m_1, m_2, \\\\ldots, m_u \\\\} \\\\subseteq M \\\\setminus S \\\\) is the set of intermediate products (B, C, D in Fig. 1) where intermediate products are formed from reactants or intermediate products and then react further to give the final product or produce intermediate products. A reaction tree consists of multiple reaction routes. A reaction route is a path from the target molecule to a starting material in the reaction tree. According to the definition, the number of reaction routes is equal to the number of starting materials. We denote reaction route as \\\\( l \\\\), the set of reaction routes as \\\\( L = \\\\{ l_1, l_2, \\\\ldots, l_n \\\\} \\\\), and we have\\n\\n\\\\[\\n\\\\tau = \\\\tau_{l_1} \\\\cup \\\\tau_{l_2} \\\\cup \\\\ldots \\\\cup \\\\tau_{l_n},\\n\\\\]\"}"}
{"id": "9JjGZsDvHb", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Reaction tree. Given the definition in Eq. (3), the depth of this tree is 3, which means the depth of the longest reaction route is 3. A is the desired product molecule to be synthesized. B, C, and D are the intermediate product molecules. E, F, G, and H are the starting molecules.\\n\\nwhere $\\\\tau_l$ is the set of reactions accompanying this reaction route $l$. As illustrated in Fig. 1, A->B->D->E is one of the reaction routes in this tree. We denote the depth $D_T$ of a reaction tree as the length of the longest reaction route in this tree, where $D_T = \\\\max_i D_l_i$. (3)\\n\\nThe depth of a reaction tree is also the number of steps required to synthesize a molecule from a fixed set of commercially purchasable compounds. Note that in this paper, the default order of the reaction route is the retrosynthesis order.\\n\\n3 PROBLEM FORMULATION OF RETROSYNTHETIC PLANNING\\n\\nIn this section, we formally give the problem formulation and the goal of retrosynthetic planning.\\n\\nSingle-Step Retrosynthesis. Given a target product molecule $T \\\\in M$, the goal of retrosynthesis is to predict a set of reactants $R = \\\\{r_1, r_2, \\\\ldots, r_n\\\\} \\\\subseteq M$ that can react to synthesize this product, which can be formulated as: $T \\\\rightarrow R$.\\n\\nRetrosynthetic Planning. Given a target molecule $T \\\\in M$, the goal of retrosynthetic planning is to search for the starting materials $R = \\\\{r_1, r_2, \\\\ldots, r_n\\\\} \\\\subseteq S$ that can synthesize the target molecule through a set of chemical reactions $\\\\tau = \\\\{R_1, R_2, \\\\ldots, R_m\\\\}$, which can be formulated as follows: $T \\\\rightarrow I \\\\rightarrow R$, (4) where $I \\\\subseteq M \\\\setminus S$ is the set of intermediate product molecules.\\n\\nThe Goal of Retrosynthetic Planning. Our goal of retrosynthetic planning is to find the reaction tree with minimum depth to synthesize the target molecule. The reaction routes for retrosynthetic planning we construct follow the principle of finding starting materials as early as possible. This principle guides the transformation direction of the molecules, thus enabling our machine learning models to make predictions on the best retrosynthesis direction. Moreover, what needs to be declared is that molecule synthesis accessibility is a part of our work due to the construction of the dataset. By predicting the reaction tree with minimum depth, we can also make a prediction of the number of steps needed to synthesize a target molecule.\\n\\n4 NEW BENCHMARK BASED ON REACTION TREES\\n\\nIn this section, we describe the details of how to construct the new benchmark. Current benchmark (Chen et al., 2020) has the two limitations: 1) the test size is too small to evaluate the performance of models, 2) the dataset is based on reaction route instead of reaction tree. Therefore, we build a new benchmark based on reaction trees. The benchmark construction consists of three steps: reaction graph construction, reaction tree traversal, and dataset split.\"}"}
