{"id": "nRCS3BfynGQ", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022.\\n\\nFor this dataset, we use the same splits as specified in (Dwivedi et al., 2020).\\n\\nH.3.1 Specific Implementation Details\\n\\nThe MLPs have one hidden layer containing 64 neurons, swish activation function and dropout layers with dropout rate 0.01. We used 2 graph layers for AGN and 3 for both the DGN and GN, with embeddings in the hidden layers having dimension 64. Aggregation functions and the pooling layer implement sum operations. The models are trained with Adam with learning rate $\\\\alpha = 0.001$ and no regularisation for 100 epochs. Batch-size 128 is used for MNIST and CIFAR10, and 8 for TSP.\\n\\nH.4 Molecular Property Prediction - QM9\\n\\nThe QM9 dataset (Wu et al., 2017) is comprised of small molecules (hydrogen, carbon, nitrogen, oxygen, flourine) with the target properties being 12 chemical properties. As the target properties are equivariant to Euclidean transformations of the atoms' coordinates, and also to the order in which atoms are processed, QM9 is an excellent benchmarking dataset for a GNN, especially if $E(n)$ is invariant. Indeed, state of the art results have been achieved on this dataset by EGNN, Dimenet, SE3-Transformer (Satorras et al., 2021; Klicpera et al., 2020; Fuchs et al., 2020). Here we show that, in addition to leading to better accuracy, employing equivariant networks give a significant improvement in convergence speed.\\n\\nFigure 5 show the evolution of the mean squared error on the test set on all the target properties for both architectures. We see that both the AGN and DGN outperform the standard GN in two aspects; the model trains more rapidly, and also reaches a lower loss.\"}"}
{"id": "nRCS3BfynGQ", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"activation and dropout rate of 0.01. The target chemical properties are all standardised by subtracting the mean and dividing by the standard deviation for each target. The networks are trained for 1000 epochs using ADAM with a learning rate of 0.0005 and mean squared error loss.\"}"}
{"id": "nRCS3BfynGQ", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SYMMETRY-DRIVEN GRAPH NEURAL NETWORKS\\n\\nAbstract\\nExploiting symmetries and invariance in data is a powerful, yet not fully exploited, way to achieve better generalisation with more efficiency. In this paper, we introduce two graph network architectures that are equivariant to several types of transformations affecting the node coordinates. First, we build equivariance to any transformation in the coordinate embeddings that preserves the distance between neighbouring nodes, allowing for equivariance to the Euclidean group. Then, we introduce angle attributes to build equivariance to any angle preserving transformation \u2013 thus, to the conformal group. Thanks to their equivariance properties, the proposed models can be vastly more data efficient with respect to classical graph architectures, intrinsically equipped with a better inductive bias and better at generalising. We demonstrate these capabilities on a synthetic dataset composed of \\\\( n \\\\)-dimensional geometric objects. Additionally, we provide examples of their limitations when (the right) symmetries are not present in the data.\\n\\nIntroduction\\nSymmetries exist throughout nature. All the fundamental laws of physics are built upon the framework of symmetries, from the gauge groups describing the Standard Model of particle physics, to Einstein's theories of general and special relativity. Once one understands the symmetry of a certain system, powerful predictions can be made. A notable example is that of Gell-Mann's eightfold-way (Gell-Mann, 1961), built upon the symmetries observed in hadrons, that led to his prediction of the \\\\( \\\\Omega^- \\\\) baryon, which was subsequently observed 3 years later (Barnes et al., 1964). The study of symmetries and invariance in deep learning has recently become a field of interest to the community (see, e.g., (Bronstein et al., 2021) for a comprehensive overview), and rapid progress has been made in constructing architectures with group theoretic structures embedded within. Two fundamental architectures in machine learning, the convolutional and graph neural networks, are invariant to the translation and permutation groups respectively.\\n\\nGraph networks in particular are designed to learn from graph-structured data and are by construction invariant to permutations of the input nodes. They were originally proposed in (Gori et al., 2005; Scarselli et al., 2008) and have received a lot of attention in the last years (see, e.g., (Battaglia et al., 2018; Hamilton, 2020; Wu et al., 2020) for a comprehensive overview). Due to their properties, they find application in a broad range of problems like learning the dynamics of complex physical systems (Sanchez-Gonzalez et al., 2020; Pfaff et al., 2021), particle identification in particle physics (Dreyer & Qu, 2021), learning causal and relational graphs (Kipf et al., 2018; Li et al., 2020), discovering symbolic models (Cranmer et al., 2020), as well as quantum chemistry (Gilmer et al., 2017) and drug discovery (Stokes et al., 2020).\\n\\nIn this work, we decouple node coordinates from other node attributes to obtain invariance (and possibly equivariance) to many transformations affecting the node coordinates, possibly belonging to important groups, in addition to permutation invariance. First, we define the distance preserving graph network (DGN) whose updated node, edge and global attributes are invariant to any transformation in the coordinate embeddings that preserves the distance between neighbouring nodes, while node coordinates can be updated in an equivariant way. Examples of such transformations are rotations and translations, but also transformations on the Hoberman sphere, whose 3D shape changes while preserving distances between connected nodes. Equivariance to dilations in the coordinates can be obtained via the conformal orthogonal group with the inclusion of an additional input layer. Then, we define the angle preserving graph network (AGN), whose updated node, edge and global attributes (resp. node coordinates) are invariant (resp. equivariant) to any transformation preserving...\"}"}
{"id": "nRCS3BfynGQ", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the angles between triples of neighbouring nodes in the graph (a notable example being molecular conformations) and, thus, to the $n$-dimensional conformal group on $\\\\mathbb{R}^n$.\\n\\nBy constructing such architectures, we enable a wide range of possible transformations to be performed on graph-structured data, with the only requirement being that distances or angles between coordinates of neighbouring nodes are preserved. This means that the updated attributes/coordinates of both networks are invariant/equivariant to translations, rotations, reflections (i.e., the Euclidean group, $E(n)$), with the AGN allowing also invariance/equivariance to dilations, inversions and non-orthogonal rotations. In practice, this means the networks are able to filter out many copies of the same input whose coordinate embeddings have been transformed and therefore learn more efficiently than architectures which consider the inputs as distinct. In other words, a single sample contains the same information as many copies of it obtained by appropriately transforming it.\\n\\nFinally, while the two architectures we present are partially overlapping in terms of symmetries, it is important to consider specific use cases where the use of one architecture over another would be preferred. We test the architectures on a synthetic dataset consisting of $n$-dimensional geometric shapes and other benchmark datasets. Moreover, we show examples of cases in which using our architectures is counterproductive due to the lack of symmetries in the data.\\n\\n2 BACKGROUND\\n\\n2.1 GROUP THEORY AND EQUIVARIANCE\\n\\nGroup theory provides the mathematical formulation for symmetries of systems, with symmetry operations represented by individual group elements. A group can be defined as a set $G$ equipped with a binary operation, which enables one to combine two group elements to form a third, whilst preserving the group axioms (associativity, identity, closure and inverse). A function unaffected by a group action is said to be invariant, which is a particular instance of equivariance. Formally, let $X \\\\subseteq \\\\mathbb{R}^n$ and $\\\\varphi_g : X \\\\rightarrow X$ be a transformation on $X$ for a group element $g \\\\in G$. Then, the linear map $\\\\Phi : X \\\\rightarrow Y$, $Y \\\\subseteq \\\\mathbb{R}^n$, is equivariant to $G$ if $\\\\forall g \\\\in G, \\\\exists \\\\varphi'_g : Y \\\\rightarrow Y$ such that $\\\\Phi(\\\\varphi_g(x)) = \\\\varphi'_g(\\\\Phi(x))$, $\\\\forall \\\\varphi_g : X \\\\rightarrow X$. When $\\\\varphi'_g$ is the identity, we say that $\\\\Phi$ is invariant to $G$. The functional form of $\\\\Phi$ is determined by the specific group of interest. It is worth pointing out that, whilst not often expressed in group theoretic notation, equivariance exists in common deep learning architectures; the convolution operation used in CNNs is equivariant under the translation group and approximate invariance is typically achieved via pooling operations. Graph neural networks are invariant to the permutation group.\\n\\nEuclidean group\\n\\nThe translation and orthogonal groups ($T(n)$ and $O(n)$ respectively) act through translations, orthogonal rotations and reflections. The semidirect product of these two groups $E(n) = T(n) \\\\rtimes O(n)$ is known as the Euclidean group, which consists of transformations that preserve distances. A function that is invariant under $E(n)$ is $\\\\|x_i - x_j\\\\|^2_2$, for any $x_i, x_j \\\\in E_n$ (see Appendix A for a proof).\\n\\nConformal group\\n\\nThe conformal group $\\\\text{Conf} (\\\\mathbb{R}^n, 0)$ is the group of transformations $\\\\varphi : \\\\mathbb{R}^n \\\\rightarrow \\\\mathbb{R}^n$ that preserve angles. Formally, for any triple of vectors $x_j, x_i, x_k \\\\in \\\\mathbb{R}^n$, let us denote by $\\\\angle(x_j, x_i, x_k)$ the angle centred on $x_i$ with rays given by $x_j - x_i$ and $x_k - x_i$. Then, a conformal transformation $\\\\varphi$ satisfies $\\\\angle(x_j, x_i, x_k) \\\\rightarrow \\\\angle(\\\\varphi(x_j), \\\\varphi(x_i), \\\\varphi(x_k)) = \\\\angle(x_j, x_i, x_k)$.\\n\\nBy definition, the transformations of the conformal group include translations, rotations, reflections (which collectively form the Euclidean group $E(n)$, as well as dilations, inversions and other features. An important subgroup is the conformal orthogonal group, which requires transformations to be orthogonal (and thus, inversions are not allowed).\\n\\n2.2 GRAPHS AND COORDINATE EMBEDDINGS\\n\\nA graph is defined as $G(V, E)$ where $V = \\\\{1, \\\\ldots, N\\\\}$ is the set of nodes and $E = \\\\{(j, i)\\\\} \\\\subseteq V \\\\times V$ is the set of (directed) edges connecting nodes in $V$ (where $j$ and $i$ denote the source and target nodes).\"}"}
{"id": "nRCS3BfynGQ", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We define $N_i \\\\equiv \\\\{ j | (j, i) \\\\in E \\\\}$ as the set of (in-)neighbours of node $i$. Moreover, we associate node and edge features, $v_i \\\\in \\\\mathbb{R}^{n_v}, \\\\forall i \\\\in V$, and $e_{ji} \\\\in \\\\mathbb{R}^{n_e}, \\\\forall (j, i) \\\\in E$ respectively, to each node and edge in the graph as well as a global attribute $u \\\\in \\\\mathbb{R}^{n_u}$. We assume that node features $v_i$ consist of node coordinates $x_i \\\\in \\\\mathbb{R}^{n_x}$ and additional features $h_i \\\\in \\\\mathbb{R}^{n_h}$ (unrelated to coordinates), so that $v_i = [x_i, h_i]$ and $n_v = n_x + n_h$. With a slight abuse of notation, we denote $G_X(V, E)$ and $G_Y(V, E)$ as different coordinate embeddings of the same graph, meaning that they differ only in their node coordinates $x_i$, while having the same node, edge and global attributes $h_i, e_{ji}$ and $u$. Finally, let $A \\\\subseteq V \\\\times V \\\\times V$ be the set of (ordered) triples of nodes in a graph $G$ that form an angle, i.e., $A \\\\equiv \\\\{(j, i, k) | j, k \\\\in N_u i, j \\\\neq k, \\\\forall i \\\\in V\\\\}$ with $N_u i \\\\equiv \\\\{ j | (j, i) \\\\in E \\\\lor (i, j) \\\\in E \\\\}$ being the set of in- and out-neighbors of node $i$.\\n\\nWe define relative distance and angle preserving maps as follows (see Figure 1).\\n\\n**Definition.** Let $G_X(V, E)$ and $G_Y(V, E)$ be such that $\\\\|x_i - x_j\\\\|_2 = \\\\|y_i - y_j\\\\|_2, \\\\forall (i, j) \\\\in E$. Let $x_{i}^{+} = \\\\psi(i, G_X)$ and $y_{i}^{+} = \\\\psi(i, G_Y)$ for some function $\\\\psi : \\\\mathbb{R}^K \\\\rightarrow \\\\mathbb{R}^n, K \\\\in \\\\mathbb{Z}^+$. We say that $\\\\psi$ is a relative distance preserving map if $\\\\|x_{i}^{+} - x_{j}^{+}\\\\|_2 = \\\\|y_{i}^{+} - y_{j}^{+}\\\\|_2, \\\\forall (i, j) \\\\in E$.\\n\\n**Definition.** Let $G_X(V, E)$ and $G_Y(V, E)$ be such that $\\\\angle(x_j, x_i, x_k) = \\\\angle(y_j, y_i, y_k), \\\\forall (j, i, k) \\\\in A$. Let $x_{i}^{+} = \\\\psi(i, G_X)$ and $y_{i}^{+} = \\\\psi(i, G_Y)$ for some function $\\\\psi : \\\\mathbb{R}^K \\\\rightarrow \\\\mathbb{R}^n, K \\\\in \\\\mathbb{Z}^+$. We say that $\\\\psi$ is a relative angle preserving map if $\\\\angle(x_{j}^{+}, x_{i}^{+}, x_{k}^{+}) = \\\\angle(y_{j}^{+}, y_{i}^{+}, y_{k}^{+}), \\\\forall (j, i, k) \\\\in A$.\\n\\nThe simplest map satisfying the above definitions is the identity function $x_{i}^{+} = \\\\psi(i, G_X) = x_i^{(2)}$ which keeps the coordinates unchanged during the update step. Updating all the coordinates in the same way also trivially preserves both relative distances and angles. In the case where the coordinate embedding $X$ is a Conformal orthogonal transformation of $Y$ a possible map $\\\\psi$ is defined by $x_{i}^{+} = x_i^{(2)} + \\\\sum_{j \\\\in N_i} a_{ji} (x_j - x_i)$, (3) with $a_{ji}$ possibly being a parametric function of other graph attributes. A thorough discussion and proofs are reported in Appendix C.\"}"}
{"id": "nRCS3BfynGQ", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To show that $\\\\|x_i - x_j\\\\|^2$ is invariant under $E(n)$, it is sufficient to see that, under translation, one has $x_i \\\\rightarrow x_i + z$, $z \\\\in E(n)$, while under rotation $x_i \\\\rightarrow Qx_i$, $Q \\\\in O(n)$. Now, by using the above relations and the fact that $E(n)$ is the semidirect product of $T(n)$ and $O(n)$, it is easy to show that $\\\\|x_i - x_j\\\\|^2$ is invariant under $E(n)$ since\\n\\n$$\\\\|x_i - x_j\\\\|^2 \\\\rightarrow \\\\|x_i + z - x_j + z\\\\|^2 = (Q(x_i + z) - Q(x_j + z))^\\\\top (Q(x_i + z) - Q(x_j + z)) \\\\quad (9a)$$\\n\\n$$= (Qx_i - Qx_j)^\\\\top (Qx_i - Qx_j) \\\\quad (9b)$$\\n\\n$$= (x_i - x_j)^\\\\top Q^\\\\top Q(x_i - x_j) \\\\quad (9c)$$\\n\\n$$= (x_i - x_j)^\\\\top (x_i - x_j), \\\\quad (9d)$$\\n\\nwhere we used the fact that $Q^\\\\top Q = I \\\\forall Q \\\\in O(n)$.}\\n\\nBIINVARIANCE UNDER THE CONFORMAL ORTHOGONAL GROUP\\n\\nIn this section we will derive the result from Section 2.1 in the main text, that dilations and orthogonal rotations are transformations under $CO(R^n, Q)$. The group is defined for a vector space $V$ with a quadratic form $Q$. The group contains the linear transformations $\\\\phi: T \\\\rightarrow V$. The group action is defined as $Q(Tx) = \\\\gamma^2 Q(x)$, (10)\"}"}
{"id": "nRCS3BfynGQ", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nwhere \\\\( T \\\\) is the set of linear transformations that we need to define and \\\\( \\\\gamma \\\\) is a scalar. For our purposes, we can consider a positive-definite quadratic form on \\\\( \\\\mathbb{R}^n \\\\); as we restrict our discussion to Euclidean space then the relevant quadratic form is\\n\\n\\\\[\\nQ = \\\\sum_i x_i^2, \\\\tag{11}\\n\\\\]\\n\\nand so inserting equation 11 into equation 10 we have the condition\\n\\n\\\\[\\n\\\\sum_i (Tx_i)^2 = \\\\gamma^2 \\\\sum_i x_i^2. \\\\tag{10}\\n\\\\]\\n\\nIf we consider, as above, an orthogonal transformation \\\\( Q \\\\), and in addition, a dilation \\\\( x_i \\\\rightarrow \\\\gamma x_i \\\\), then it is simple to show that, with \\\\( T = \\\\gamma Q \\\\)\\n\\n\\\\[\\n\\\\sum_i (\\\\gamma Qx_i)^2 = \\\\gamma^2 Q^\\\\top Q \\\\sum_i x_i^2 = \\\\gamma^2 \\\\sum_i x_i^2.\\n\\\\]\\n\\n### Relative Distance and Angle Preserving Maps\\n\\nMany different relative distance and/or angle preserving maps to be used in the coordinate update can be obtained in different ways, eventually requiring further assumptions on the data.\\n\\nThe simplest one is the identity function\\n\\n\\\\[\\nx_i^+ = \\\\psi(i, GX) = x_i \\\\tag{12}\\n\\\\]\\n\\nwhich keeps the coordinates unchanged during the update step. In this case both relative distances and angles are trivially preserved. Updating all the coordinates in the same way also trivially preserves both relative distances and angles. Examples include\\n\\n\\\\[\\nx_i^+ = ax_i, \\\\quad a \\\\in \\\\mathbb{R}, \\\\tag{13}\\n\\\\]\\n\\n\\\\[\\nx_i^+ = x_i + a, \\\\quad a \\\\in \\\\mathbb{R}^n \\\\tag{14}\\n\\\\]\\n\\n\\\\[\\nx_i^+ = Qx_i + a, \\\\quad Q \\\\in \\\\mathbb{R}^{n \\\\times n}, Q^\\\\top Q = I, \\\\quad a \\\\in \\\\mathbb{R}^n \\\\tag{15}\\n\\\\]\\n\\nwhere \\\\( a, a, Q \\\\) can be learnable parameters or parametric functions of other network parameters, e.g. \\\\( a = \\\\phi_x(u) \\\\).\\n\\nThe matrix \\\\( Q \\\\) can also be non-orthogonal, provided that the obtained transformation preserves distances or angles.\\n\\nDevising more complex forms for \\\\( \\\\psi \\\\) for arbitrary coordinate embeddings \\\\( X, Y \\\\) satisfying the relative distance or angle preserving property, is quite tricky and further assumptions are in general required. To see this, consider a relative distance preserving transformation. Any transformation \\\\( X \\\\rightarrow Y \\\\), such that\\n\\n\\\\[\\n\\\\|x_i - x_j\\\\|^2 = \\\\|y_i - y_j\\\\|^2,\\n\\\\]\\n\\ncan be obtained as\\n\\n\\\\[\\nx_i = \\\\gamma_i A_i y_i + q_i, \\\\quad \\\\gamma_i, A_i, q_i, i \\\\in V, \\\\tag{16}\\n\\\\]\\n\\nwhere \\\\( \\\\gamma_i \\\\in \\\\mathbb{R} \\\\), \\\\( A_i^\\\\top A_i = I \\\\), \\\\( q_i \\\\in \\\\mathbb{R}^n \\\\), \\\\( \\\\forall i \\\\in V \\\\). In this general case, defining a (non-trivial) map \\\\( \\\\psi \\\\) such that, after its application, one has\\n\\n\\\\[\\n\\\\|x_i^+ - x_j^+\\\\|^2 = \\\\|y_i^+ - y_j^+\\\\|^2 \\\\forall (i, j) \\\\in E,\\n\\\\]\\n\\nis hard without any assumption on, at least, the topology of the graph \\\\( G_X \\\\) (or, equivalently \\\\( G_Y \\\\)). A similar reasoning can be applied also to relative angle preserving maps.\\n\\nIf we restrict ourselves to the case in which \\\\( \\\\gamma_i = \\\\gamma, A_i = A, q_i = q, \\\\forall i \\\\in V \\\\), this results in \\\\( X \\\\) being a Conformal orthogonal transformation of \\\\( Y \\\\). In this case, a possible map \\\\( \\\\psi \\\\) is defined by\\n\\n\\\\[\\nx_i^+ = x_i + \\\\sum_{j \\\\in N_i} a_{ji}(x_j - x_i). \\\\tag{16}\\n\\\\]\\n\\nwith \\\\( a_{ji} \\\\) possibly being a parametric function of node/edge/angle/global attributes, e.g. \\\\( a_{ji} = \\\\phi_{x(e + ji, v + j, v + i, u)} \\\\). Notice that, under conformal orthogonal transformations (hence also \\\\( E(n) \\\\) transformations)\\n\\n\\\\[\\ny_i = \\\\phi_g(x) = Ax + b, \\\\quad AA^\\\\top = I,\\n\\\\]\\n\\none has that equation 16 is an equivariant map.\"}"}
{"id": "nRCS3BfynGQ", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In fact, \\\\( y + i = y_i + \\\\sum_{j \\\\in N_i} a_{ji} (y_j - y_i) = \\\\gamma A x_i + b + \\\\sum_{j \\\\in N_i} a_{ji} (\\\\gamma A x_i + b - \\\\gamma A x_j + b) = \\\\gamma A \\\\left( y_i + \\\\sum_{j \\\\in N_i} a_{ji} (x_j - x_i) \\\\right) + b \\\\).\\n\\nThus, when this map is used as \\\\( \\\\psi_x \\\\) in the DGN or AGN block, node coordinates can be updated in an equivariant way with respect to conformal orthogonal transformations. Using the same argument, it is easy to show that equation 13 is also equivariant, but equation 14 is not.\\n\\nNow we show that equation 16 is both relative distance and angle preserving. For the sake of notation, we assume \\\\( \\\\gamma = 1 \\\\), however the same arguments can be applied when \\\\( \\\\gamma \\\\neq 1 \\\\).\\n\\nRelative distance preservation of equation 16\\nTo show that equation 16 satisfies the definition of a distance-preserving map it is sufficient to show that, since \\\\( x = A y + q \\\\), one has\\n\\n\\\\[\\n\\\\begin{align*}\\nx + i &= x_i + \\\\sum_{j \\\\in N_i} a_{ij} (x_j - x_i) = A y_i + q + \\\\sum_{j \\\\in N_i} a_{ij} (A x_j + q - A x_i - q) = A y_i + q + \\\\sum_{j \\\\in N_i} a_{ij} (A x_j - A x_i) = A \\\\left( y_i + \\\\sum_{j \\\\in N_i} a_{ij} (x_j - x_i) \\\\right) + q\\n\\\\end{align*}\\n\\\\]\\n\\nwhich implies\\n\\n\\\\[\\n\\\\| x + i - x_j \\\\|_2 = \\\\| A y_i + q - A y_j - q \\\\|_2 = \\\\| A (y_i - y_j) \\\\|_2 = \\\\| y_i - y_j \\\\|_2\\n\\\\]\\n\\nwhere in the last line we used the fact that \\\\( A^\\\\top A = I \\\\).\\n\\nRelative angle preservation of equation 16\\nWhile equation 17 implies that angles are preserved since \\\\( x_i \\\\) is an \\\\( E(n) \\\\) transformation of \\\\( y_i \\\\), one can show this explicitly by recalling that the angle between two vectors \\\\( x_j - x_i \\\\) and \\\\( x_k - x_i \\\\) can be computed as\\n\\n\\\\[\\n\\\\cos \\\\angle(x_j, x_i, x_k) = \\\\frac{(x_j - x_i) \\\\cdot (x_k - x_i)}{\\\\| x_j - x_i \\\\| \\\\| x_k - x_i \\\\|}\\n\\\\]\"}"}
{"id": "nRCS3BfynGQ", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Then, one has\\n\\n\\\\[(x + j - x + i) \\\\top (x + k - x + i) = (Ay + j + q - Ay + i - q)\\n\\\\]\\n\\n\\\\[\\\\|x + j - x + i\\\\|\\\\|x + k - x + i\\\\|\\\\]\\n\\n\\\\[= (Ay + j - Ay + i) \\\\top (Ay + k - Ay + i)\\n\\\\]\\n\\n\\\\[\\\\sqrt{(Ay + j - Ay + i) \\\\top A \\\\top A (Ay + j - Ay + i)}\\n\\\\]\\n\\n\\\\[= (y + j - y + i) \\\\top (y + k - y + i)\\n\\\\]\\n\\nwhere in the last line we used the fact that $A \\\\top A = I$.\\n\\nLocal symmetry transformation of equation 16\\n\\nSuppose we have a local symmetry transform, $A(\\\\tilde{x})$ which only acts on a subgraph $\\\\tilde{G} \\\\in G$, such that $v = (\\\\tilde{v}_1, \\\\tilde{v}_2, ..., \\\\tilde{v}_n, v_{n+1}, ..., v_m)$, and similarly for the edge, coordinate and angle features. The action of $A(\\\\tilde{x})$ is then $A(\\\\tilde{x})x = (A(\\\\tilde{x})\\\\tilde{x}_1, A(\\\\tilde{x})\\\\tilde{x}_2, ..., A(\\\\tilde{x})\\\\tilde{x}_n, A(\\\\tilde{x})x_{n+1}, ..., A(\\\\tilde{x})x_m) = (A(\\\\tilde{x})\\\\tilde{x}_1, A(\\\\tilde{x})\\\\tilde{x}_2, ..., A(\\\\tilde{x})\\\\tilde{x}_n, x_{n+1}, ..., x_m)$.\\n\\nBy defining the 2 subgraphs as containing the coordinate features which are and are not affected by the symmetry transformation, we can therefore write a coordinate update equation 16 for both subgraphs; Eq. equation 16 for the $x_i$ and, for the $\\\\tilde{x}_i$, $\\\\tilde{x}_i + \\\\sum_{j \\\\in N(i)} a_{ji}(\\\\tilde{x}_j - \\\\tilde{x}_i)$.\\n\\nWe have shown above that this coordinate update preserves distances and angles for global group transformations; by defining the 2 subgraphs as above, we can promote the local symmetry transformation $A(\\\\tilde{x})$ to global transformations on subgraphs. Whilst here we have shown this for only 2 subgraphs, one can generalise the argument to any number of local transformations so long as the subgraphs are defined as above. As we have to subdivide the graph into subgraphs, we note that these local transformations cannot be defined arbitrarily as there must be a sense of a neighborhood of nodes within each subgraph. A local transformation which affects unrelated nodes identically (which is a valid class of local symmetry) is not valid for this reason.\\n\\n**EUCLIDEAN GROUP EQUIVARIANCE OF THE DGN BLOCK**\\n\\nTo show equivariance to $E(n)$ transformations of the input coordinates for the DGN block, we begin with the edge update in equation 5a. Since $\\\\|x_i - x_j\\\\|^2$ is invariant under an $E(n)$ transformation, $x \\\\mapsto Qx + z$, for some rotation matrix $Q \\\\in O(n)$ and translation vector $z \\\\in \\\\mathbb{R}^n$, then\\n\\n$\\\\phi_e(e_{ji}, v_i, v_j, \\\\|x_i - x_j\\\\|^2, u) \\\\rightarrow \\\\phi_e(e_{ji}, v_i, v_j, \\\\|Qx_i + z - Qx_j - z\\\\|^2, u)$\\n\\n$= \\\\phi_e(e_{ji}, v_i, v_j, \\\\|x_i - x_j\\\\|^2, u)$.\\n\\nInvariance of the node and global updates in equation 5 follows naturally as they are composed of invariant quantities. The coordinate update in equation 5c can be invariant or equivariant under $E(n)$ depending on the structure of $\\\\psi_x$.**\"}"}
{"id": "nRCS3BfynGQ", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"datasets containing symmetries. However, when represented as graphs as in (Dwivedi et al., 2020), an edge classification one. When treated as images, MNIST and CIFAR10 are usually examples of\\nprovided. In particular, we consider MNIST, CIFAR10 as graph classification problems and TSP as\\nNow we consider benchmark datasets from (Dwivedi et al., 2020) for which node coordinates are\\northogonal transformations due to the Gram\u2013Euler theorem. While this behaviour turns out to be\\npreserved (due to the non-orthogonality of $A$ only AGN and SDGN (and the SE3-Transformer for\\npossibly unexpected occurs. The AGN (and partially also the SDGN and the SE3-Transformer for\\nadding random non-orthogonal transformations ($A$ non-orthogonal $R$). In this case, one can easily envisage cases in which it may cause issues.\\n\\nThe AGN block is also combined with a scaling layer (SDGN) and possible angle attributes ($h$ attributes), and list of edges $E$.\\n\\nWe consider a $n$-dimensional polytopes classification problems for $n = 4$ square, $5$ simplexes, hypercubes and orthoplexes are considered for all values of $n$.\\n\\nWe compare graph networks built with AGN, DGN and standard (GN) blocks, as well as\\nlarger training dataset is needed in order to do so, as shown in Appendix H.\\n\\nNote that, while even a standard GN could learn to correctly classify transformed polytopes, a much\\n\\n| $n$ | Training Accuracy | Test Accuracy |\\n|-----|------------------|--------------|\\n| 3   | 99 \u00b1 0          | 99 \u00b1 0       |\\n| 4   | 98 \u00b1 0          | 98 \u00b1 0       |\\n| 5   | 96 \u00b1 0          | 96 \u00b1 0       |\\n| 6   | 59 \u00b1 5         | 61 \u00b1 5       |\\n\\nWe set to equation 3 (other cases are considered in\\n\\n$$\\\\gamma A \\\\in \\\\mathbb{R}^{n \\\\times n}$$\\n\\nfor some\\n\\n$$\\\\gamma, I \\\\in \\\\mathbb{R}^{n \\\\times n}$$\\n\\nand\\n\\n$$E \\\\in \\\\mathbb{R}^{n \\\\times n}$$\\n\\n$A$ and\\n\\n$$E$$\\n\\n$A$\\n\\n$E$\\n\\n$A$\\n\\n$E$\\n\\n$A$\\n\\n$E$\\n\\n$A$\\n\\n$E$\\n\\n$A$\\n\\n$E$\\n\\n$A$\\n\\n$E$\\n\\n$A$\\n\\n$E$\\n\\n$A$\\n\\n$E$\\n\\n$A$\\n\\n$E$\\n\\n$A$\\n\\n$E$\\n\\n$A$\\n\\n$E$\\n\\n$A$\\n\\n$E$\\n\\n$A$\"}"}
{"id": "nRCS3BfynGQ", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"those symmetries are mostly lost. This is due to, for example, the fact that both the foreground\\nand the background of the images are embedded in the graphs. Furthermore, in TSP there are no\\napparent symmetries. In these cases, as one may expect, using an equivariant network would not\\nproduce any benefits. Rather, as we show, negative effects can appear.\\n\\nWe trained our models and a standard GNN on these datasets. All models have roughly the same\\nnumber of parameters and the results are reported in Table 2. As can be seen, treating node co-\\nordinates as if they can possess some symmetric between among different samples turns out to be\\ncounterproductive both in terms of convergence speed and accuracy. In particular we see that while\\nthe standard GN has the best performance, the DGN is slightly better than the AGN. This is probably\\ndue to the fact that, while some non-orthogonal transformations mostly preserving local distances\\nmay be present in the dataset, angle preserving ones are much more rare. On TSP in particular, using\\nangle related properties results in a significant drop in performance.\\n\\nTable 2: Benchmark datasets. Train and test accuracy are reported for MNIST and CIFAR10. For\\nTSP, due to the high class unbalance, the train and test F1 score for the positive class is reported.\\n\\n|                  | MNIST | CIFAR10 | TSP          |\\n|------------------|-------|---------|--------------|\\n|                  | train | test    | train        | test        |\\n|                  | acc   | acc     | F1 (positive)| F1 (positive)|\\n| AGN              | 0.943 | 0.932   | 0.644        | 0.590        |\\n|                  | \u00b10.004| \u00b10.004  | \u00b10.004       | \u00b10.004       |\\n| DGN              | 0.957 | 0.945   | 0.657        | 0.592        |\\n|                  | \u00b10.001| \u00b10.006  | \u00b10.004       | \u00b10.002       |\\n| GN               | 0.982 | 0.977   | 0.719        | 0.657        |\\n|                  | \u00b10.001| \u00b10.002  | \u00b10.007       | \u00b10.001       |\\n\\n6.2.1 COMPUTATIONAL COMPLEXITY\\n\\nAll the experiments have been run on an NVIDIA V100 GPU. The average time required for each\\ntraining step (i.e., forward pass, back-propagation and optimiser step for a batch of data) is reported\\nin Table 3 for the architectures presented in this paper, normalised with respect to the batch-size. For\\nthe polytopes classification experiment we report the results obtained by using equation 2 and equa-\\ntion 3 (in parentheses). Using the scaling layer for the DGN drastically increase the computational\\ncomplexity. Moreover, it can be seen that while the AGN and DGN have similar complexities for\\ndatasets in which the number of edges (and hence angles) is relatively low, when graphs are (nearly)\\ncomplete (like in TSP) or highly connected, the AGN has an overhead with respect to the DGN (due\\nto the need of updated and computing angles in addition to edges).\\n\\nTable 3: Time per training step in the various experiments, normalised with respect to the batch-size.\\n\\n|                  | polytopes (n=3) | polytopes (n=4) | polytopes (n=5) | MNIST | CIFAR10 | TSP | QM9 |\\n|------------------|----------------|----------------|----------------|-------|---------|-----|-----|\\n|                  | batch-size     |                |                |       |         |     |     |\\n| AGN              | 5              | 6              | 3              |       |         |     |     |\\n|                  | 3.3 (3.8)      | 3 (3.8)        | 5.6 (6.3)      | 1.17  | 1.64    | 35  | 0.23|\\n| SDGN             | 5              | 6              | 3              |       |         |     |     |\\n|                  | 4.4 (5)        | 14.5 (15.2)    | 7 (8)          | 7.81  | 12.5    | -   | -   |\\n| DGN              | 5              | 6              | 3              |       |         |     |     |\\n|                  | 3.3 (3.8)      | 3 (3.7)        | 5.3 (6.3)      | 0.78  | 1.09    | 16  | 0.19|\\n| GN               | 5              | 6              | 3              |       |         |     |     |\\n|                  | 3.2            | 3              | 5.2            | 0.77  | 1.02    | 14  | 0.16|\\n\\n7 CONCLUSION\\n\\nIn this paper we have presented novel deep learning architectures which are equivariant to distance\\nand angle preserving transformations in graph coordinate embeddings. In particular, we have shown\\ninvariance or equivariance to the $E(n)$, CO($R^n$, $Q$) and Conf($R^n$, 0) groups in addition to permuta-\\ntion invariance. Invariance to local symmetries can also be achieved when the different transforma-\\ntions are applied to suitable subgraphs. We have applied our models to a synthetic dataset composed\\nof $n$-dimensional regular polytopes as well as to several benchmark datasets. We have shown that\\nthe architectures we propose are significantly more accurate and data efficient than a standard graph\\nnetwork on datasets where there are large numbers of symmetries in the data. We also explicitly\\nshow examples where our architecture produces unexpected results or would not be applicable, due\\nto a lack of symmetry in the data.\\n\\nREFERENCES\\n\\nGabriela Barenboim, Johannes Hirn, and Veronica Sanz. Symmetry meets AI. SciPost Phys., 11:14,\\n2021.\"}"}
{"id": "nRCS3BfynGQ", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Observation of a Hyperon with Strangeness Minus Three.\\n\\n*Physical Review Letters*, 12(8):204\u2013206, 1964.\\n\\nRelational inductive biases, deep learning, and graph networks. 2018.\\n\\nSe(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. 2021.\\n\\nB-spline CNNs on Lie groups. In *International Conference on Learning Representations*, 2020.\\n\\nLorentz group equivariant neural network for particle physics. In *International Conference on Machine Learning*, pp. 992\u20131002. PMLR, 2020.\\n\\nGeometric deep learning: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021.\\n\\nGroup equivariant convolutional networks. In *International Conference on Machine Learning*, pp. 2990\u20132999. PMLR, 2016.\\n\\nDiscovering symbolic models from deep learning with inductive biases. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), *Advances in Neural Information Processing Systems*, volume 33, pp. 17429\u201317442. Curran Associates, Inc., 2020.\\n\\nJet tagging in the Lund plane with graph networks. *JHEP*, 03:052, 2021. doi: 10.1007/JHEP03(2021)052.\\n\\nBenchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.\\n\\nData efficiency in graph networks through equivariance. arXiv preprint arXiv:2106.13786, 2021. Presented at the ICML 2021 Workshop on Subset Selection in Machine Learning: From Theory to Practice.\\n\\nGeneralizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data. In *International Conference on Machine Learning*, pp. 3165\u20133176. PMLR, 2020.\\n\\nA practical method for constructing equivariant multilayer perceptrons for arbitrary matrix groups. In Marina Meila and Tong Zhang (eds.), *Proceedings of the 38th International Conference on Machine Learning*, volume 139 of *Proceedings of Machine Learning Research*, pp. 3318\u20133328. PMLR, 18\u201324 Jul 2021.\\n\\nSe(3)-transformers: 3d roto-translation equivariant attention networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), *Advances in Neural Information Processing Systems*, volume 33, pp. 1970\u20131981. Curran Associates, Inc., 2020.\\n\\nThe eightfold way: A theory of strong interaction symmetry. 3 1961.\"}"}
{"id": "nRCS3BfynGQ", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, pp. 1263\u20131272. PMLR, 2017.\\n\\nMarco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2, pp. 729\u2013734. IEEE, 2005.\\n\\nWilliam L Hamilton. Graph representation learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 14(3):1\u2013159, 2020.\\n\\nMasanobu Horie, Naoki Morita, Toshiaki Hishinuma, Yu Ihara, and Naoto Mitsume. Isometric transformation invariant and equivariant graph convolutional networks. In International Conference on Learning Representations, 2021.\\n\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster), 2015.\\n\\nThomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In International Conference on Machine Learning, pp. 2688\u20132697. PMLR, 2018.\\n\\nJohannes Klicpera, Janek Gro\u00df, and Stephan G\u00fcnnemann. Directional message passing for molecular graphs. In International Conference on Learning Representations, 2020.\\n\\nJonas K\u00f6hler, Leon Klein, and Frank Noe. Equivariant flows: Exact likelihood generative learning for symmetric densities. In Hal Daum\u00e9 III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 5361\u20135370. PMLR, 13\u201318 Jul 2020.\\n\\nYann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems, pp. 396\u2013404, 1990.\\n\\nYunzhu Li, Antonio Torralba, Anima Anandkumar, Dieter Fox, and Animesh Garg. Causal discovery in physical systems from videos. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 9180\u20139192. Curran Associates, Inc., 2020.\\n\\nMarios Mattheakis, Pavlos Protopapas, David Sondak, Marco Di Giovanni, and Efthimios Kaxiras. Physical symmetries embedded in neural networks. arXiv preprint arXiv:1904.08991, 2019.\\n\\nSong Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random features and kernel models. arXiv preprint arXiv:2102.13219, 2021.\\n\\nTobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. Learning mesh-based simulation with graph networks. In International Conference on Learning Representations, 2021.\\n\\nDavid W. Romero and Jean-Baptiste Cordonnier. Group equivariant stand-alone self-attention for vision. In International Conference on Learning Representations, 2021.\\n\\nAlvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia. Learning to simulate complex physics with graph networks. In International Conference on Machine Learning, pp. 8459\u20138468. PMLR, 2020.\\n\\nV\u00edctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9323\u20139332. PMLR, 18\u201324 Jul 2021.\\n\\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks, 20(1):61\u201380, 2008.\"}"}
{"id": "nRCS3BfynGQ", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022 to conformal transformations it is sufficient to note that given a conformal transformation, $\\\\phi$, one has that the angle update equation 6a is invariant to it since\\n\\\\[\\n\\\\phi(\\\\alpha(v_i, v_j, v_k, \\\\alpha_{jik}, \\\\angle(x_j, x_i, x_k), u)) = \\\\phi(\\\\alpha(v_i, v_j, v_k, \\\\alpha_{jik}, \\\\angle(x_j, x_i, x_k), u)).\\n\\\\]\\nInvariance of the other updates is trivially satisfied by construction. The coordinate updates can also be constructed to be equivariant to conformal orthogonal (and hence Euclidean) transformations, with an appropriate choice of $\\\\psi_x$. We stress that equivariance to the conformal group includes by definition equivariance to the Euclidean group as we are only concerned with transformations on $\\\\mathbb{R}^n$. The orthogonal rotations and translations of the Euclidean group are therefore a subset of possible conformal transformations on $\\\\mathbb{R}^n$ as they are angle-conserving transformations. Angular information can be extremely powerful in tasks where classical (or distance-based) GNNs fail, like in graph isomorphism tests where, e.g., a hexagon is not distinguished from two triangles.\\n\\n**Additional Formulations**\\n\\n**F.1 Alternative Formulations for the Angle Preserving Graph Network**\\n\\nA number of variations can be proposed for the angle preserving graph network:\\n\\n- Edge attributes can be used in angle updates\\n  \\\\[\\n  \\\\alpha_{jik} = \\\\phi(\\\\alpha(v_i, v_j, v_k, e_{ij}, e_{ik}, \\\\alpha_{jik}, \\\\angle(x_j, x_i, x_k), u)), \\\\quad \\\\forall (j,i,k) \\\\in A.\\n  \\\\]\\n- Relative distances can be used in the angle updates\\n  \\\\[\\n  \\\\alpha_{jik} = \\\\phi(\\\\alpha(..., ||x_i - x_j||_2^2, ||x_i - x_k||_2^2, \\\\angle(x_j, x_i, x_k), u)), \\\\quad \\\\forall (j,i,k) \\\\in A.\\n  \\\\]\\n- Angle attributes can be used in edge updates\\n  \\\\[\\n  e_{ji} = \\\\phi(e_{ij}, v_j, v_i, \\\\rho(\\\\alpha_{jik} \\\\rightarrow e_{\\\\{\\\\alpha_{jik}\\\\}_{k \\\\in A_{ij}}}), \\\\rho(\\\\alpha_{jik} \\\\rightarrow e_{\\\\{\\\\alpha_{jik}\\\\}_{k \\\\in A_{ji}}}), u)), \\\\quad \\\\forall (i,j) \\\\in E.\\n  \\\\]\\n- Angle embeddings can be ignored and node attributes can be updated with the angles themselves\\n  \\\\[\\n  e_{ji} = \\\\phi(e_{ij}, v_j, v_i, \\\\rho(\\\\alpha_{jik} \\\\rightarrow e_{\\\\{\\\\alpha_{jik}\\\\}_{k \\\\in A_{ij}}}), \\\\rho(\\\\alpha_{jik} \\\\rightarrow e_{\\\\{\\\\alpha_{jik}\\\\}_{k \\\\in A_{ji}}}), u)), \\\\quad \\\\forall (i,j) \\\\in E.\\n  \\\\]\\n  \\\\[\\n  v_i = \\\\phi(v_i, \\\\rho(\\\\alpha_{jik} \\\\rightarrow v_{\\\\{\\\\alpha_{jik}\\\\}_{k \\\\in A_{i}}}), \\\\rho(\\\\alpha_{jik} \\\\rightarrow v_{\\\\{\\\\alpha_{jik}\\\\}_{k \\\\in A_{i}}}), u)), \\\\quad \\\\forall i \\\\in V.\\n  \\\\]\\n  \\\\[\\n  x_i = \\\\psi_x(i, G_X), \\\\quad \\\\forall i \\\\in V.\\n  \\\\]\\n  \\\\[\\n  u = \\\\phi(u, \\\\rho(v \\\\rightarrow u_{\\\\{v_i\\\\}_{i \\\\in V}}), \\\\rho(e \\\\rightarrow u_{\\\\{e_{ji}\\\\}_{(j,i) \\\\in E}}), u).\\n  \\\\]\\n\\nWhere the global update can contain aggregated information about angles and distances and also the other updates can be generalised as shown above. This architecture is by construction equivariant to transformations in the coordinate embeddings for which both relative distances and angles are preserved.\"}"}
{"id": "nRCS3BfynGQ", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this appendix we will represent some of the architectures discussed in the main text explicitly as instances of our architecture.\\n\\nG.1 DimeNet (Klicpera et al., 2020)\\nDimenet can be obtained from the AGN by considering edge and distance information in the angle update and using sum aggregation functions as\\n\\n\\\\[\\n\\\\alpha_{ijk} = \\\\phi_{\\\\alpha}(e_{ji}, \\\\|x_i - x_j\\\\|^2, \\\\angle(x_i, x_j, x_k)), \\\\quad \\\\forall (j,i,k) \\\\in A\\n\\\\]\\n\\n\\\\[\\ne_{ji}^+ = \\\\phi_{e}(\\\\phi_{e}(e_{ji}, \\\\sum_{i \\\\in N_j} \\\\alpha_{ijk}), v_j), \\\\quad \\\\forall (j,i) \\\\in E\\n\\\\]\\n\\n\\\\[\\nx_i^+ = x_i, \\\\quad \\\\forall i \\\\in V\\n\\\\]\\n\\n\\\\[\\nv_i^+ = \\\\phi_v(v_i, \\\\sum_{j \\\\in N_i} e_{ji}^+), \\\\quad \\\\forall i \\\\in V\\n\\\\]\\n\\nwhere \\\\(\\\\|x_i - x_j\\\\|^2\\\\) is defined represented within a set of orthogonal basis functions \\\\(e_{RBF}\\\\) and the angles \\\\(\\\\angle(x_i, x_j, x_k)\\\\) within a basis defined as \\\\(\\\\alpha_{SBF}\\\\).\\n\\nG.2 EGNN (Satorras et al., 2021)\\nThe EGNN network is obtained from the DGN by selecting a specific form for the coordinate update function \\\\(\\\\psi\\\\), using the sum aggregation function as \\\\(\\\\phi_{e \\\\to v}\\\\), and not propagating updated edges, i.e.,\\n\\n\\\\[\\ne_{ji}^+ = \\\\phi_{e}(v_j, v_i, e_{in}^+_{ji}, \\\\|x_i - x_j\\\\|^2), \\\\quad \\\\forall (j,i) \\\\in E\\n\\\\]\\n\\n\\\\[\\nx_i^+ = x_i + \\\\sum_{j \\\\neq i} (x_i - x_j) \\\\phi_{x}(e_{ji}^+), \\\\quad \\\\forall i \\\\in V\\n\\\\]\\n\\n\\\\[\\nv_i^+ = \\\\phi_v(v_i, \\\\sum_{j \\\\in N_i} e_{ji}^+), \\\\quad \\\\forall i \\\\in V\\n\\\\]\\n\\nwhere \\\\(e_{in}^+_{ji}\\\\) are the edge attributes of the input data (implying that \\\\(e_{ji}^+\\\\) is not propagated to any subsequent layer, as in message passing networks).\\n\\nG.3 IsoGNN (Horie et al., 2021)\\nThe IsoGNN architecture is defined for tensors of rank-\\\\(n\\\\); to compare with the other architectures presented here, we show below the architecture for rank-1 tensors.\\n\\n\\\\[\\ne_{ji}^+ = \\\\sum_{k,l \\\\in V, k \\\\neq l} T_{ijkl} (x_k - x_l), \\\\quad \\\\forall (j,i) \\\\in E\\n\\\\]\\n\\n\\\\[\\nx_i^+ = x_i, \\\\quad \\\\forall i \\\\in V\\n\\\\]\\n\\n\\\\[\\nv_i^+ = \\\\phi_v(v_i, \\\\sum_{j \\\\in N_i} e_{ji}^+), \\\\quad \\\\forall i \\\\in V\\n\\\\]\\n\\nwhere \\\\(T_{ijkl}\\\\) is an untrainable 2-dimensional matrix which is translation and rotation invariant, and determined offline from the data for each class of problem.\\n\\nG.4 Other Networks\\nStandard graph networks can be obtained from ours by skipping some updates or not considering equivariant information. Also other variants, including SchNet (Sch\u00fctt et al., 2017) or TFN (Thomas et al., 2018), can be cast as message passing architectures as shown in (Satorras et al., 2021).\"}"}
{"id": "nRCS3BfynGQ", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The update functions of the networks are all implemented as MLPs. After the graph layers, the produced node embeddings are passed through another MLP, a global pooling layer and a final MLP with output dimension equal to the number of classes for graph classification tasks. For edge classification tasks (TSP), the architecture after the graph layers is a single MLP taking as input source and target nodes and predicting the class of each edge. Each network is trained starting from 10 different initial conditions. The results in the tables contain the mean and standard deviation resulting from the 10 initialisations.\\n\\nH.2 Polytopes Classification\\n\\nH.2.1 Specific Implementation Details\\n\\nAll the MLPs have one hidden layer containing 64 neurons and swish activation function. We used 2 graph layers for AGN and DimeNet and 3 for the DGN, GN and EGNN with embeddings in the hidden layers having dimension 32. Aggregation functions and the pooling layer implements mean or sum operations (and are specified in the results' tables) for our architectures. As for DimeNet, we use distances and angles directly instead of their RBF embeddings since we do not consider atomistic quantities. Adam (Kingma & Ba, 2015) is used to train the all the models with a learning rate $\\\\alpha = 0.001$ and no regularisation for 1000 epochs. The batch-size is equal to the number of training samples (so 5, 6, 3 respectively, for $n = 3, 4, 5$).\\n\\nH.2.2 Additional Experiments\\n\\nIn the main paper we reported results when the identity function equation 12 was used to perform the coordinate update together with a sum aggregation function. In Tables 4, 5 and 6 we report also the results obtained when using equation 16 for the coordinate updates and possibly mean aggregation function. As can be seen, using the mean aggregation function usually causes a drop in performance. In particular, the SDGN with mean aggregation function is unable to correctly classify the polytopes even at training time. This is because, after rescaling the coordinates, all edges have the same length and, employing a mean aggregation results in always the same output for all polytopes. This is partially alleviated by using equation 16 as the node coordinate update function, which allows one to differently remap coordinates of different polytopes. The other results are qualitatively similar.\\n\\n|               | train acc | test accuracy |\\n|---------------|-----------|---------------|\\n| AGN mean 12   | 1.0        | 0.96 \u00b1 0.04   |\\n| AGN mean 16   | 1.0        | 0.97 \u00b1 0.03   |\\n| AGN sum 12    | 1.0        | 0.93 \u00b1 0.07   |\\n| AGN sum 16    | 1.0        | 0.89 \u00b1 0.16   |\\n| SDGN mean 12  | 0.2        | 0.20 \u00b1 0.22   |\\n| SDGN mean 16  | 0.8        | 0.83 \u00b1 0.40   |\\n| SDGN sum 12   | 1.0        | 0.93 \u00b1 0.07   |\\n| SDGN sum 16   | 1.0        | 0.89 \u00b1 0.16   |\\n| DGN mean 12   | 0.83 \u00b1 0.22| 0.82 \u00b1 0.03   |\\n| DGN mean 16   | 0.29 \u00b1 0.02| 0.25 \u00b1 0.03   |\\n| DGN sum 12    | 0.45 \u00b1 0.05| 0.44 \u00b1 0.12   |\\n| DGN sum 16    | 0.43 \u00b1 0.05| 0.41 \u00b1 0.04   |\\n| GN mean 12    | 0.25 \u00b1 0.04| 0.20 \u00b1 0.05   |\\n| GN mean 16    | 0.44 \u00b1 0.15| 0.41 \u00b1 0.17   |\\n| GN sum 12     | 0.44 \u00b1 0.15| 0.41 \u00b1 0.17   |\\n| GN sum 16     | 0.43 \u00b1 0.04| 0.40 \u00b1 0.03   |\\n\\nTable 4: Polytopes classification: training and test accuracy (mean \u00b1 standard deviation over 10 runs) for $n = 3$, for different transformations in the test set.\"}"}
{"id": "nRCS3BfynGQ", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Polytopes classification: training and test accuracy (mean \u00b1 standard deviation).\\n\\n| Transformation | Mean Test Accuracy | Standard Deviation |\\n|---------------|-------------------|-------------------|\\n| Orthogonal    | 0.75              | 0.05              |\\n| Orthogonal + dilation | 0.78          | 0.04              |\\n| Non-orthogonal (\u00b5 = 0.5) | 0.70         | 0.03              |\\n| Non-orthogonal (\u00b5 = 1.5) | 0.65         | 0.02              |\\n| Non-orthogonal (\u00b5 = 3.0) | 0.60         | 0.01              |\\n\\nUnder review as a conference paper at ICLR 2022.\\n\\nMNIST and CIFAR10 are classical image classification datasets converted into graphs using super-vertices.\\n\\nWe considered the following transformations in the dataset in terms of data efficiency, we study how many samples in the training set are necessary for a standard GNN to reach reasonable generalisation performance. For the set of transformations we used in the dataset, we augmented the training set with random initialisations. It can be seen that the reported in Figure 4 for each set of transformations in terms of mean and standard deviation over 1000 runs.\\n\\nWe transformed (as in the respective test set) copies of each polytope. We trained a standard GNN on these augmented datasets and observed the resulting test accuracy after training for 200 epochs. Results are reported in Figure 4 for each set of transformations in terms of mean and standard deviation over 1000 runs.\\n\\nTo emphasise the advantage of having a network that is able to exploit symmetries, we studied how many samples in the training set are necessary for a standard GNN to reach reasonable generalisation performance. For the set of transformations we used in the dataset, we augmented the training set with random initialisations. It can be seen that the reported in Figure 4 for each set of transformations in terms of mean and standard deviation over 1000 runs.\\n\\nWe transformed (as in the respective test set) copies of each polytope. We trained a standard GNN on these augmented datasets and observed the resulting test accuracy after training for 200 epochs. Results are reported in Figure 4 for each set of transformations in terms of mean and standard deviation over 1000 runs.\\n\\nTo emphasise the advantage of having a network that is able to exploit symmetries, we studied how many samples in the training set are necessary for a standard GNN to reach reasonable generalisation performance. For the set of transformations we used in the dataset, we augmented the training set with random initialisations. It can be seen that the reported in Figure 4 for each set of transformations in terms of mean and standard deviation over 1000 runs.\\n\\nWe transformed (as in the respective test set) copies of each polytope. We trained a standard GNN on these augmented datasets and observed the resulting test accuracy after training for 200 epochs. Results are reported in Figure 4 for each set of transformations in terms of mean and standard deviation over 1000 runs.\\n\\nTo emphasise the advantage of having a network that is able to exploit symmetries, we studied how many samples in the training set are necessary for a standard GNN to reach reasonable generalisation performance. For the set of transformations we used in the dataset, we augmented the training set with random initialisations. It can be seen that the reported in Figure 4 for each set of transformations in terms of mean and standard deviation over 1000 runs.\\n\\nWe transformed (as in the respective test set) copies of each polytope. We trained a standard GNN on these augmented datasets and observed the resulting test accuracy after training for 200 epochs. Results are reported in Figure 4 for each set of transformations in terms of mean and standard deviation over 1000 runs.\\n\\nTo emphasise the advantage of having a network that is able to exploit symmetries, we studied how many samples in the training set are necessary for a standard GNN to reach reasonable generalisation performance. For the set of transformations we used in the dataset, we augmented the training set with random initialisations. It can be seen that the reported in Figure 4 for each set of transformations in terms of mean and standard deviation over 1000 runs.\\n\\nWe transformed (as in the respective test set) copies of each polytope. We trained a standard GNN on these augmented datasets and observed the resulting test accuracy after training for 200 epochs. Results are reported in Figure 4 for each set of transformations in terms of mean and standard deviation over 1000 runs.\\n\\nTo emphasise the advantage of having a network that is able to exploit symmetries, we studied how many samples in the training set are necessary for a standard GNN to reach reasonable generalisation performance. For the set of transformations we used in the dataset, we augmented the training set with random initialisations. It can be seen that the reported in Figure 4 for each set of transformations in terms of mean and standard deviation over 1000 runs.\\n\\nWe transformed (as in the respective test set) copies of each polytope. We trained a standard GNN on these augmented datasets and observed the resulting test accuracy after training for 200 epochs. Results are reported in Figure 4 for each set of transformations in terms of mean and standard deviation over 1000 runs.\\n\\nTo emphasise the advantage of having a network that is able to exploit symmetries, we studied how many samples in the training set are necessary for a standard GNN to reach reasonable generalisation performance. For the set of transformations we used in the dataset, we augmented the training set with random initialisations. It can be seen that the reported in Figure 4 for each set of transformations in terms of mean and standard deviation over 1000 runs.\\n\\nWe transformed (as in the respective test set) copies of each polytope. We trained a standard GNN on these augmented datasets and observed the resulting test accuracy after training for 200 epochs. Results are reported in Figure 4 for each set of transformations in terms of mean and standard deviation over 1000 runs.\"}"}
{"id": "nRCS3BfynGQ", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nFigure 2: Graphical representation of the graph architectures\\n\\n3.1 THE STARTING POINT - GRAPH NETWORK BLOCK (GN)\\n\\nA general standard graph network block can be defined in terms of edge, node and global updates as:\\n\\n\\\\[ e_{ji}^+ = \\\\phi_e(v_j, v_i, e_{ji}, u), \\\\quad \\\\forall (j,i) \\\\in E \\\\] (4a)\\n\\n\\\\[ v_i^+ = \\\\phi_v(v_i, \\\\rho_{e \\\\rightarrow v}(\\\\{e_{ji}^+ \\\\}_{j \\\\in N_i})), u), \\\\quad \\\\forall i \\\\in V \\\\] (4b)\\n\\n\\\\[ u^+ = \\\\phi_u(\\\\rho_{v \\\\rightarrow u}(\\\\{v_i^+ \\\\}_{i \\\\in V}), \\\\rho_{e \\\\rightarrow u}(\\\\{e_{ji}^+ \\\\}_{(j,i) \\\\in E})), u) \\\\] (4c)\\n\\nwhere \\\\( \\\\phi_e : \\\\mathbb{R}^{2n_{v} + n_{e} + n_u} \\\\rightarrow \\\\mathbb{R}^{n_e} \\\\), \\\\( \\\\phi_v : \\\\mathbb{R}^{n_v + n_e + n_u} \\\\rightarrow \\\\mathbb{R}^{n_v} \\\\), \\\\( \\\\phi_u : \\\\mathbb{R}^{n_v + n_e + n_h + n_u} \\\\rightarrow \\\\mathbb{R}^{n_u} \\\\) are update functions (usually defined as neural networks whose parameters are to be learned) and \\\\( \\\\rho_{e \\\\rightarrow v}, \\\\rho_{e \\\\rightarrow u}, \\\\rho_{v \\\\rightarrow u} \\\\) are aggregation functions reducing a set of elements of variable length to a single one via an input's permutation equivariant operation like element-wise summation or mean.\\n\\n3.2 DISTANCE PRESERVING GRAPH NETWORK BLOCK (DGN)\\n\\nBy decoupling the updates of node coordinates and features (\\\\( x_i \\\\) and \\\\( h_i \\\\)), the DGN block is defined through the following updates:\\n\\n\\\\[ e_{ji}^+ = \\\\phi_e(e_{ji}, h_i, h_j, \\\\|x_i - x_j\\\\|^2), \\\\quad \\\\forall (j,i) \\\\in E \\\\] (5a)\\n\\n\\\\[ h_i^+ = \\\\phi_h(\\\\rho_{e \\\\rightarrow h}(\\\\{e_{ji}^+ \\\\}_{j \\\\in N_i}), h_i, u), \\\\quad \\\\forall i \\\\in V \\\\] (5b)\\n\\n\\\\[ x_i^+ = \\\\psi_x(i, G_X), \\\\quad \\\\forall i \\\\in V \\\\] (5c)\\n\\n\\\\[ u^+ = \\\\phi_u(\\\\rho_{v \\\\rightarrow u}(\\\\{h_i^+ \\\\}_{i \\\\in V}), \\\\rho_{e \\\\rightarrow u}(\\\\{e_{ji}^+ \\\\}_{(j,i) \\\\in E}), \\\\rho_{x \\\\rightarrow u}(\\\\{\\\\|x_i - x_j\\\\|^2 \\\\}_{(j,i) \\\\in E})), u) \\\\] (5d)\\n\\nwhere \\\\( \\\\phi_e : \\\\mathbb{R}^{n_{e} + 2n_{h} + n_{u} + 1} \\\\rightarrow \\\\mathbb{R}^{n_e} \\\\), \\\\( \\\\phi_h : \\\\mathbb{R}^{n_e + n_h + n_u} \\\\rightarrow \\\\mathbb{R}^{n_h} \\\\), \\\\( \\\\phi_u : \\\\mathbb{R}^{n_v + n_e + n_h + n_u} \\\\rightarrow \\\\mathbb{R}^{n_u} \\\\) are the update functions, \\\\( \\\\rho_{e \\\\rightarrow h}, \\\\rho_{e \\\\rightarrow u}, \\\\rho_{h \\\\rightarrow u} \\\\) are aggregation functions and \\\\( \\\\psi_x : \\\\mathbb{R}^K \\\\rightarrow \\\\mathbb{R}^{n_x}, K \\\\in \\\\mathbb{Z}^+ \\\\) is some, possibly parametric, relative distance preserving map.\\n\\nBecause of the way in which node coordinates are processed to update edge and global embeddings (i.e., only through their relative distances), it can be easily seen that \\\\( e_{ji}, h_{i}^+ \\\\) and \\\\( u^+ \\\\) are, by construction, invariant to any transformation of the input coordinates that locally maintains their relative distances along the edges defined by the graph structure (a notable example being the one of Euclidean transformations; see Appendix D for a proof). Moreover, updated node coordinates \\\\( x_i^+ \\\\) can be invariant or equivariant to (some of) those transformations, depending on the particular structure of \\\\( \\\\psi_x \\\\).\\n\\nFor example, using equation 3 or the trivial identity function (equation 2) would result in coordinates being updated in an equivariant way with respect to Euclidean transformations, with \\\\( a_{ji} \\\\) possibly being parametric functions (e.g., \\\\( a_{ji} = \\\\phi_x(e_{ji}) \\\\)).\\n\\n3.3 ANGLE PRESERVING GRAPH NETWORK BLOCK (AGN)\\n\\nGiven a graph \\\\( G(V, E) \\\\), let \\\\( \\\\alpha_{jik} \\\\in \\\\mathbb{R}^{n_\\\\alpha} \\\\) be the angle embedding associated to each angle \\\\( (j,i,k) \\\\in A \\\\) and assume that \\\\( \\\\alpha_{jik} \\\\) contain no information about the angles \\\\( \\\\angle(x_j, x_i, x_k) \\\\). Moreover, define\"}"}
{"id": "nRCS3BfynGQ", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nas the set of (ordered) couples of nodes forming an angle whose vertex is node $i$, i.e., $A_i \\\\equiv \\\\{ (j,k) | (j,y,k) \\\\in A, y = i \\\\}$. Then, the AGN block is then characterised by the following updates\\n\\n\\\\[\\n\\\\alpha^{+}_j i k = \\\\phi(\\\\alpha(h_i, h_j, h_k, \\\\alpha^{+}_j i k), \\\\angle(x_j, x_i, x_k), u), \\\\forall (j,i,k) \\\\in A \\\\tag{6a}\\n\\\\]\\n\\n\\\\[\\ne^{+}_j i = \\\\phi(e(h_j, h_i, e^{+}_j i, u), \\\\forall (j,i) \\\\in E \\\\tag{6b}\\n\\\\]\\n\\n\\\\[\\nh^{+}_i = \\\\phi(h(h_i, \\\\rho_{e \\\\rightarrow h}(\\\\{e^{+}_j i \\\\}_{j \\\\in N_i}), \\\\rho_{\\\\alpha \\\\rightarrow h}(\\\\{\\\\alpha^{+}_j i k \\\\}_{j,i,k})), u), \\\\forall i \\\\in V \\\\tag{6c}\\n\\\\]\\n\\n\\\\[\\nx^{+}_i = \\\\psi(x(i, G_X)), \\\\forall i \\\\in V \\\\tag{6d}\\n\\\\]\\n\\n\\\\[\\nu^{+} = \\\\phi(u(\\\\rho_{h \\\\rightarrow u}(\\\\{h^{+}_i \\\\}_{i \\\\in V}), \\\\rho_{e \\\\rightarrow u}(\\\\{e^{+}_j i \\\\}_{(j,i) \\\\in E}), \\\\rho_{\\\\alpha \\\\rightarrow u}(\\\\{\\\\alpha^{+}_j i k \\\\}_{j,i,k}), u), \\\\forall i \\\\in V \\\\tag{6e}\\n\\\\]\\n\\nwhere $\\\\phi(\\\\alpha), \\\\phi(e), \\\\phi(h)$ are the update functions, and $\\\\rho_{e \\\\rightarrow h}, \\\\rho_{\\\\alpha \\\\rightarrow h}, \\\\rho_{h \\\\rightarrow u}, \\\\rho_{e \\\\rightarrow u}, \\\\rho_{\\\\alpha \\\\rightarrow u}$ are the aggregation functions, with $\\\\psi(x)$ being a (possibly parametric) relative angle preserving map. Variants of this network that still preserve its properties can be easily constructed by, e.g., using edge embeddings to update the angle embeddings, using angles to update edges or not considering angle attributes at all. Some examples are reported in Appendix F.1. It can be easily seen that $\\\\alpha^{+}_j i k, e^{+}_j i, h^{+}_i, u^{+}$ are invariant to any transformation of the coordinate embeddings that preserves the angles created by neighbouring nodes in the graph, while updated coordinates $x^{+}_i$ can be invariant or equivariant to (some of) the same transformations, depending on the structure of $\\\\psi(x)$. As with the DGN, equivariance to the conformal orthogonal group can be achieved by using equation 3. A particular case of transformations the AGN can deal with is the one of conformal transformations (see Appendix E for a formal discussion).\\n\\n4 DISCUSSION\\n\\nIn the previous section we introduced two novel graph architectures. To construct them we have mainly built upon the actions on a coordinate system of two transformations that preserves distances and angles between neighbouring nodes in a graph. These transformations include as particular cases those in the Euclidean and conformal groups. Equivariance under $E(n)$ means equivariance to orthogonal rotations and translations, while by considering the full conformal group, we can further generalise our neural network architecture. Conformal $n$-dimensional transformations consist of the groups containing translations, dilations, rotations and inversions with respect to an $n-1$ sphere. A conformal transformation is therefore a powerful tool for mapping data points onto each other, and hence, building a neural network architecture invariant to the conformal group enables the architecture to be invariant to a wide selection of interesting subgroups. By introducing distance and angle preserving transformations that take into account the structure of the graph, invariance and equivariance under more general transformations have been achieved. A notable example is that the DGN architecture, whose updates are invariant to any transformation of a Hoberman sphere, while both the DGN and AGN can be invariant to transformations affecting only subsets of their nodes, as shown below.\\n\\nIn summary, by taking advantage of the powerful tools of group theory, and performing operations on a vector space which are invariant or equivariant under group transformations, we can build a neural network architecture which can take advantage of these group properties. Such an architecture will be able to deal with rotated, translated, dilated (or more generally transformed) data more efficiently than a standard graph network which does not have the above group properties built into it.\\n\\n4.1 BEYOND GLOBAL TRANSFORMATIONS\\n\\nSo far, we have talked about global transformations in the coordinate space. In general, a global symmetry $\\\\phi_{g}$, as defined in Section 2.1 acts on a function $\\\\psi(x)$ as $\\\\psi(x) \\\\rightarrow \\\\phi_{g} \\\\psi(x)$.\\n\\n\\\\[\\n(7)\\n\\\\]\\n\\nA local group symmetry $\\\\phi_{l}(x)$ is instead defined as $\\\\psi(x) \\\\rightarrow \\\\phi_{l}(x) \\\\psi(x)$.\\n\\n\\\\[\\n(8)\\n\\\\]\\n\\nThe update functions in equation 6 live in the function spaces $\\\\phi(\\\\alpha): \\\\mathbb{R}^{3n}_{h} + n^{+}_{\\\\alpha} + n^{+}_{u} \\\\rightarrow \\\\mathbb{R}^{n}_{\\\\alpha}$, $\\\\phi(e): \\\\mathbb{R}^{2n}_{h} + n^{+}_{e} + n^{+}_{u} \\\\rightarrow \\\\mathbb{R}^{n}_{e}$, $\\\\phi(h): \\\\mathbb{R}^{n}_{h} + n^{+}_{e} + n^{+}_{\\\\alpha} + n^{+}_{u} \\\\rightarrow \\\\mathbb{R}^{n}_{h}$, $\\\\psi(x): \\\\mathbb{R}^{K} \\\\rightarrow \\\\mathbb{R}^{n}_{x}$, for an appropriate $K \\\\in \\\\mathbb{Z}^{+}$ depending on the actual number of parameters, and $\\\\phi(u): \\\\mathbb{R}^{n}_{h} + n^{+}_{e} + n^{+}_{\\\\alpha} + n^{+}_{u} \\\\rightarrow \\\\mathbb{R}^{n}_{u}$. 5\"}"}
{"id": "nRCS3BfynGQ", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Equivariance of the two networks. The green/red square denotes the base graph and the blue/black ones alternative coordinate embeddings, equivariant for the DGN (red) or AGN (green).\\n\\nwhere now the group action can differ at all points. The architectures we present are also able to deal with some local symmetries. Namely, the DGN and AGN preserve local distances and angles respectively where a suitable subgraph must be defined. As an example consider a transformation that rotates only some nodes of a graph; all nodes which are in the neighbourhoods of the nodes will, in the angular architecture, have their angles preserved, and in the distance-preserving architecture, the distances between nodes will be preserved. A pertinent use-case for such a symmetry is that of molecular conformations, where the spatial arrangement of the atoms can be interconverted by rotations about formally single bonds). Both the DGN and the AGN updates would be invariant to such a transformation at the level of the two distinct subgraphs separated by the bond in question, thus enabling one to learn conformation-invariant properties.\\n\\n4.2 Comparing and extending the two architectures\\n\\nAs mentioned, the two architectures we presented are partially overlapping in terms of equivariance features. Whilst both of them contain invariance to $E(n)$, the DGN can deal with non-orthogonal transformations that preserve the distance between neighbouring nodes (such as with the Hoberman sphere) while the AGN is invariant to the conformal group which includes non-orthogonal transformations that preserve angles but not distances (see Figure 3).\\n\\nThe DGN can be equipped with a sense of scale invariance through an input scaling layer that makes it invariant to the conformal orthogonal group. Let $\\\\gamma = \\\\alpha/\\\\max_{i,j \\\\in E} \\\\|x_i - x_j\\\\|$ for some $\\\\alpha \\\\in \\\\mathbb{R}$, where we identify $\\\\gamma$ as satisfying the dilation condition required under the conformal orthogonal group. Then, conformal orthogonal invariance can simply be obtained by computing scale-normalised coordinates $\\\\tilde{x}_i = \\\\gamma x_i$ and using them as the coordinates in equation 5.\\n\\nIntroducing coordinate scaling therefore enables the DGN to be scale invariant, under the condition that all transformations remain orthogonal. With the AGN, we can relax this condition and be invariant under the full conformal group. This larger group invariance enables us to work with very powerful transformations on data, which can also be dangerous. For example, if the scale is an important property of the dataset, the AGN will not recognise it and so will learn incorrect information about the data. Similarly, one must also not use the DGN blindly and irrespective of the properties of the dataset; as the architecture is invariant to transformations that preserve distance between neighbouring nodes but not angles it is able to, for example, map a square onto a line. The two architectures we presented can be also combined together at the price of losing some features, while generalising to both distance and angle preserving transformations.\\n\\n5 Related work\\n\\nThe study and formulation of group equivariant neural networks have flourished in the last years and they have proven to be extremely powerful in many tasks. The first example is probably that of convolutional neural networks (LeCun et al., 1990), which exploits translation equivariance and invariance, thanks to the convolution and pooling operations respectively, and have led to breakthroughs in most vision tasks. CNNs have been generalised to exploit larger groups of symmetries. G-CNNs are proposed in (Cohen & Welling, 2016) to deal with more general symmetries, including rotations and translations, and extended in (Bekkers, 2020; Finzi et al., 2020) to deal with Lie groups. Equivariance to arbitrary symmetry groups can be achieved via self-attention mechanisms in (Romero & Cordonnier, 2021). Continuous convolutions are used in SchNet (Sch\u00fctz et al., 2017)\"}"}
{"id": "nRCS3BfynGQ", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to achieve $E_n$ invariance, and $\\\\text{SE}(3)$ equivariance is achieved in Thomas et al. (2018) via the use of spherical harmonics. The drawback of many of these methods is their limited applicability due to computational complexity. There has been some work on constructing general MLPs that are equivariant to different groups; a layer equivariant to general matrix groups is presented in Finzi et al. (2021), whilst equivariance to the Lorentz group for physics applications has recently been explored (Bogatskiy et al., 2020). Further applications include (Mattheakis et al., 2019), where physical symmetries are embedded in neural networks via embedding physical constraints in the structure of the network, and in (Barenboim et al., 2021) where the ability of neural networks to discover and learn symmetries in data is explored.\\n\\nGraph neural networks are, by construction, permutation invariant (Scarselli et al., 2008; Battaglia et al., 2018). Recently, there has been a lot of work on building equivariance to other interesting groups in GNNs. There has been particular interest in the Euclidean group with results for the subgroups $\\\\text{SE}(3)$ and $E(3)$ obtained in (Thomas et al., 2018; Fuchs et al., 2020; Kohler et al., 2020; Finzi et al., 2020; Batzner et al., 2021; Yang et al., 2020). Extending these architectures, a message-passing convolutional graph layer is proposed in (Satorras et al., 2021) which implements $E_n$ invariance for edge and node features and equivariance for node coordinates, leading to state of the art results in a variety of tasks, including $n$-body particle systems simulations and molecular property predictions. A similar idea has been also proposed in (Farina & Slade, 2021), where a general $E_n$ equivariant GNN is presented, and in (Horie et al., 2021), in which equivariance to isometric transformations is considered. These $E_n$ equivariant networks can be seen as special instances of our DGN architecture, where particular aggregation functions are applied and a particular choice is made for the distance preserving map $\\\\psi_x$ (see Appendix G). In (Satorras et al., 2021), for example, the map $\\\\psi_x$ is chosen so that relative distances between all possible pairs of node in the graph are preserved, thus allowing only for rigid transformations in $E_n$.\\n\\nAngular information is used in (Smith et al., 2017) but no attention is explicitly paid to equivariance. DimeNET is proposed in (Klicpera et al., 2020), where both distance and angle embeddings computed through embeddings in novel orthogonal basis functions are used in a message passing graph network to achieve equivariance. State of the art results are obtained on molecular property and dynamics prediction datasets. We note, however, that while DimeNET produces state of the art results, it is restricted to atomic data, due to the requirement of Gaussian radial and Bessel functions as the basis representations. Additionally, angular embeddings computed in the isometric invariant layer in (Horie et al., 2021) corresponds to the extraction of both relative distances and angles of each pair of vertices. Again, these approaches are special instances of our networks where particular aggregation and learnable functions or hand engineered distance and/or angle embeddings are used (see Appendix G). In this sense, our network is unconstrained and can learn what it needs. Moreover, thanks to its general form it can achieve invariance to the conformal group (and more) which, to the best of our knowledge, does not hold for known architectures.\\n\\nIn summary, most of the existing equivariant graph architectures are at most equivariant to the Euclidean group (represented as the intersection set in Figure 3), whilst the architectures we propose allow, in their general form, more general transformations.\"}"}
