{"id": "PN9uaKA1nV", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DA-Mixup (Chen et al., 2020; Zhang et al., 2020): It adds interpolation on the embedding space of the training examples to create virtual augmented examples. In this work, we use the TMix version of MixText for data augmentation on the few-shot labeled dataset. For token-level classification tasks, we employ one variant of Mixup, namely SeqMix (Zhang et al., 2020) that leverage the interpolation on the embedding space to generate new tokens as augmentations.\\n\\nDA-Transformer (MELM) (Kumar et al., 2020; Zhou et al., 2022): It introduces a conditional data augmentation technique that prepends class labels to text sequences for pre-trained transformer-based models. Specifically, it leverage the sequence to sequence transformer (BART) to perform conditional text generation based on the seed examples. For token-level classification tasks, we select MELM (Zhou et al., 2022), which first masks some entities in the few-shot examples, and use a text-to-text transformer (T5) to predict masked entity tokens by explicitly conditioning on their labels for creating new examples.\\n\\nLightNER (Chen et al., 2022a): It adopts a seq2seq framework, generating the entity span sequence and entity categories under the guidance of a self-attention-based prompting module. It is designed specifically for NER tasks.\\n\\nKGPC (Chen et al., 2023a): It injects the semantic relations of the knowledge graph to sequence to sequence text generation models to perform knowledge-guided instance generation for few-shot biomedical NER. It also only applies to NER tasks.\\n\\nLLM-based Generation Methods.\\n\\nZeroGen (Ye et al., 2022a): It generates a dataset using simple class-conditional prompts and then trains a tiny task-specific model for zero-shot inference. We follow the prompting method mentioned in their original paper as implementation, which does not consider any style information as well as domain knowledge.\\n\\nDemoGen (Meng et al., 2023; Yoo et al., 2021): It leverages LLMs to synthesize novel training data by feeding few-shot samples as demonstrations to guide the data generation process without providing additional instructions. Note that we focus on using the black-box LLM as the generator, thus we do not tune the LLM as Meng et al. (2023).\\n\\nProGen (Ye et al., 2022b): It leverages the feedback from the task-specific model to guide the generation of new training data via in-context examples. Specifically, it first identify the most important examples from the generated synthetic data using the influence function, then added these examples as demonstrations to generate new training instances. To ensure fair comparison, we also add the few-shot demonstrations for data generation.\\n\\nWe do not compare with Tang et al. (2023) in the main experiments as it leverages entities extracted from the entire training set and violates the true few-shot learning setting.\\n\\n### Prompt Format\\n\\n**F.1 The Prompts for Writing Styles Suggestion with CLINGen**\\n\\nSuppose you need to generate a synthetic clinical text dataset on [task] tasks. Here are a few examples from the original training set:\\n\\n**[demonstrations]**\\n\\nPlease write three potential sources, speakers or authors of the sentences.\\n\\n**[task]**: The task names for each specific task.\\n\\n**[demonstrations]**: The few-shot demonstrations from the original training set.\\n\\n**F.2 The Prompts for Data Generation with CLINGen**\\n\\nIn the following prompt format, [topic] and [style] are randomly sampled from the topics candidate set and styles candidate set we formulate in the knowledge extraction step, respectively.\\n\\n20\"}"}
{"id": "PN9uaKA1nV", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Named entity recognition tasks:\\n\\nListing 2: Prompt Format for NER tasks with CLINGEN.\\n\\nSuppose you need to create a dataset for [domain] recognition. Your task is to:\\n1. generate a sentence about [domain],\\n2. output a list of named entity about [domain] only,\\n3. the sentence should mimic the style of [style],\\n4. the sentence should mention the [domain] named [topic].\\n\\n[domain]: \u201cdisease\u201d for BC5CDR-Disease and NCBI-Disease; \u201cchemical\u201d for BC5CDR-Chemical and CHEMDNER.\\n\\nMedication attributes tasks:\\n\\nListing 3: Prompt Format for medication attributes tasks with CLINGEN.\\n\\nSuppose you need to create a dataset for clinical attributes recognition. Your task is to:\\n1. generate a sentence about clinical attributes, The Clinical Attributes you need to extract include \u201cMedication\u201d, \u201cDosage\u201d, \u201cRoute\u201d, \u201cFrequency\u201d, \u201cReason\u201d, \u201cDuration\u201d. For each attribute class, please return a list of attributes within the class that occurs in the Sentence.\\n2. the sentence should mimic the style of [style],\\n3. the sentence should be relevant to [topic].\\n\\nText classification tasks:\\n\\nListing 4: Prompt Format for text classification tasks with CLINGEN.\\n\\nSuppose you need to create a dataset for [domain]. Your task is to:\\n1. generate a sentence about [domain].\\n2. the sentence should mimic the style of [style].\\n3. the sentence should be relevant to the subtopic of [topic] for [class name].\\n\\n[domain]: \u201cCOVID-19 Literature\u201d for LitCovid and \u201cCancer Document\u201d for HOC.\\n[class name]: the label name for this generated sample.\\n\\nRelation extraction tasks:\\n\\nListing 5: Prompt Format for relation extraction tasks with CLINGEN.\\n\\nSuppose you need to generate synthetic data for the biomedical [domain] task. Your task is to:\\n1. give a sentence about [class name] relation between [entity0] and [entity1]\\n2. the sentence should discuss the [entity0]: [topic0] and [entity1]: [topic1] with the relation [label desc].\\n3. the sentence should mimic the style of [style].\\n\\n[domain]: \u201cDisease Gene Relation\u201d for GAD, \u201cChemical Disease Relation\u201d for CDR, and \u201cChemical Protein Relation\u201d for ChemProt.\\n[entity0] and [entity1]: \u201cdisease\u201d and \u201cgene\u201d for GAD, \u201cchemical\u201d and \u201cdisease\u201d for CDR, and \u201cchemical\u201d and \u201cprotein\u201d for ChemProt.\\n[class name]: the label name for this generated sample.\"}"}
{"id": "PN9uaKA1nV", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\n**Natural language inference tasks:**\\n\\nListing 6: Prompt Format for generating the first sentence in NLI tasks with CLINGEN.\\n\\nSuppose you need to create a set of [content]. Your task is to:\\n1. generate one sentence for a [content].\\n2. the [content] should be relevant to [topic],\\n3. The [content] should mimic the style of [style].\\n\\n[content]: \\\"health question\\\" for MEDIQA-RQE, \\\"claim\\\" for MEDIQA-NLI, MedNLI and MQP, and \\\"health news\\\" for PUBHEALTH and HealthVer.\\n\\nListing 7: Prompt Format for generating the second sentence in NLI tasks with CLINGEN.\\n\\nSuppose you need to create a pair of sentences for the [domain] task with the label '[class name]'. Given the [content]: '[first sentence]', Your task is to:\\n1. generate one short [content] about [topic] so that\\n2. The [content] should mimic the style of the first sentence.\\n\\n[domain]: \\\"Question Entailment\\\" for MEDIQA-RQE, \\\"Natural Language Entailment\\\" for MEDIQA-NLI and MedNLI, \\\"Fact Verification\\\" for PUBHEALTH and HealthVer, and \\\"Sentence Similarity Calculation\\\" for MQP.\\n\\n[content]: \\\"health question\\\" for MEDIQA-RQE, \\\"hypothesis\\\" for MEDIQA-NLI, MedNLI, \\\"evidence\\\" for PUBHEALTH and HealthVer, and \\\"sentence\\\" for MQP.\\n\\n[class name]: the label name for this generated sample.\\n\\n[label desc]: the description of the selected label.\\n\\n[first sentence]: the first sentence we generate\\n\\n**F.3 PROMPTS FOR ZEROGEN, DEMOGEN, PROGEN**\\n\\nWe use the same set of prompts for ZeroGen, DemoGen and ProGen, while DemoGen and ProGen have additional demonstrations augmented to the prompts. DemoGen uses the few-shot examples in the training set as demonstrations, and ProGen leverages feedbacks from previous rounds to iteratively guide the generation.\\n\\n**Named entity recognition tasks:**\\n\\nListing 8: Prompt Format for NER tasks with baselines.\\n\\nSuppose you need to create a dataset for [domain] recognition. Your task is to generate a sentence about [domain] and output a list of named entity about [domain] only.\\n\\n[domain]: \\\"disease\\\" for BC5CDR-Disease and NCBI-Disease; \\\"chemical\\\" for BC5CDR-Chemical and CHEMDNER.\\n\\n**Medication attributes tasks:**\\n\\nListing 9: Prompt Format for medication attributes tasks with baselines.\\n\\nSuppose you need to create a dataset for clinical attributes recognition. Your task is to generate a sentence about clinical attributes, The Clinical Attributes you need to extract include \\\"Medication\\\", \\\"Dosage\\\", \\\"Route\\\", \\\"Frequency\\\", \\\"Reason\\\", \\\"Duration\\\".\\n\\nFor each attribute class, please return a list of attributes within the class that occurs in the Sentence.\"}"}
{"id": "PN9uaKA1nV", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we present additional experimental results on every dataset in Tables 6, 7, 8. We also include the experimental results combining topic from both KG and LLM, which yields a performance improvement, though not a substantial one. However, note that in practice, it is challenging to tune the ratio in the few-shot setting.\"}"}
{"id": "PN9uaKA1nV", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13: The regularized entity frequencies of datasets generated by C\\\\textsc{LinG}, ZeroGen and DemoGen compared with the ground truth in log scale.\\n\\n| Dataset       | PubMedBERT Base | Reframe (Mishra et al., 2022) | APE (Zhou et al., 2023b) | ClinGen w/ KG | ClinGen w/ LLM |\\n|---------------|-----------------|-------------------------------|--------------------------|--------------|---------------|\\n| LitCovid      | 56.74           | 57.27                         | 61.92                    | 67.60        | 54.61         |\\n| CDR           | 56.06           | 58.75                         | 66.55                    | 68.00        | 52.10         |\\n| MEDIQA-RQE    | 58.01           | 61.75                         | 74.85                    | 72.17        | 56.94         |\\n| MQP           | 59.22           | 63.34                         | 72.40                    | 73.31        | 54.84         |\\n| CHEMDNER      | 57.07           | 64.99                         | 77.36                    | 76.21        | 55.37         |\\n\\nTable 12: Comparison between existing prompting optimization methods and C\\\\textsc{LinG}.\\n\\nWe carry out an additional analysis with two recent and representative prompt optimization techniques, namely Reframe (Mishra et al., 2022) and APE (Zhou et al., 2023b). In our setting, Reframe incorporates several principles (e.g. using low-level patterns, itemizing instructions, etc.) to produce high-quality prompts to enhance text generation, whereas APE leverages the LLM itself to automatically optimize the prompts based on the target task information. We demonstrate their performance on various clinical tasks in Table 12.\\n\\nThe results indicate that our proposed C\\\\textsc{LinG} consistently outperforms both baselines. This performance gain is attributed to the fact that the prompts generated by Reframe and APE mainly focus on incorporating and decomposing task-specific information, but do not adequately address the unique challenges for the clinical data generation task, i.e. distribution shift and lack of diversity.\"}"}
{"id": "PN9uaKA1nV", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We provide the detailed prompt templates we use for Reframe (Mishra et al., 2022) and APE (Zhou et al., 2023b) in the followings.\\n\\n**Natural Language Inference tasks:**\\n\\n**Listing 14:** Prompt Format for generating sentences in NLI tasks with Reframe.\\n\\nGenerate a pair of sentences for the [domain] task. Follow these guidelines:\\n\\n1. Formulate a medical premise in the first sentence, such as a clinical observation or a patient's medical history.\\n2. Craft a medical hypothesis or claim related to the premise in the second sentence.\\n3. Ensure that the hypothesis logically follows from the premise.\\n4. Avoid introducing any unrelated or contradictory information in either sentence.\\n5. The length should be in 50 words.\\n\\n**Listing 15:** Prompt Format for generating sentences in NLI tasks with APE.\\n\\nGenerate a pair of sentences for the [domain] task. The first sentence should be a medical premise, such as a clinical observation or a patient's medical history. The second sentence should be a medical hypothesis or claim, related to the premise. The goal is to determine whether the hypothesis logically follows from the premise, and you can use various medical scenarios, conditions, or treatments for creating these sentence pairs.\\n\\n[domain]: \u201cQuestion Entailment\u201d for MEDIQA-RQE.\\n\\n**Sentence similarity tasks:**\\n\\n**Listing 16:** Prompt Format for generating sentences in sentence similarity tasks with Reframe.\\n\\nSuppose you need to generate two sentences for the [domain] task. Your task is to give a pair of sentences with the following instructions:\\n\\n1. Generate two sentences that exhibit a clear similarity or dissimilarity in meaning without using complex or specialized terms.\\n2. Express attributes affirmatively.\\n3. Ensure that both sentences have a common attribute for comparison.\\n4. The length should be in 50 words.\\n\\n**Listing 17:** Prompt Format for generating sentences in sentence similarity tasks with APE.\\n\\nSuppose you need to generate two sentences for the [domain] task. The goal is to assess how close or similar the meaning of two sentences is, including 'equivalent' or 'not equivalent'.\\n\\n[domain]: \u201cSentence Similarity Calculation\u201d for MQP.\\n\\n**Text classification tasks:**\\n\\n**Listing 18:** Prompt Format for generating sentences in text classification tasks with Reframe.\\n\\nSuppose you are a writer for [domain]. Your task is to give a synthetic [domain] about [class name] with the following instructions:\\n\\n1. Illustrate points with everyday scenarios related to the [class name].\\n2. About 50 - 100 words.\"}"}
{"id": "PN9uaKA1nV", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Listing 19: Prompt Format for generating sentences in text classification tasks with APE.\\n\\nSuppose you are a writer for [domain]. Generate a clinical article discussing the latest advancements in [domain] with a focus on [class name]. Please include information on recent clinical trials, emerging research findings, and potential implications for healthcare practitioners and patients.\\n\\n[domain]: \u201cCOVID-19 Literature\u201d for LitCovid.\\n\\n[class name]: the label name for this generated sample.\\n\\nRelation extraction tasks:\\n\\nListing 20: Prompt Format for generating sentences in relation extraction tasks with Reframe.\\n\\nSuppose you need to generate a dataset for the biomedical [domain] task where the relationships between entities in biomedical texts need to be identified. Your task is to give a synthetic example about [class name] relation with the following instructions:\\n\\n(1) Provide the sentence or text snippet where the relationship is mentioned.\\n(2) The length should be in 50 words.\\n\\nListing 21: Prompt Format for relation extraction tasks with APE.\\n\\nGenerate a sentence that describes a [class name] [domain] between [entity0] and [entity1]. The sentence should provide information about how these terms are related, such as its potential therapeutic use, side effects, or any relevant research findings.\\n\\n[domain]: \u201cChemical Disease Relation\u201d for CDR.\\n\\n[entity0] and [entity1]: \u201cchemical\u201d and \u201cdisease: for CDR.\\n\\n[class name]: the label name for this generated sample.\\n\\nNamed entity recognition tasks:\\n\\nListing 22: Prompt Format for generating sentences in NER tasks with Reframe.\\n\\nSuppose you need to create a dataset for [domain] recognition. Your task is to generate a sentence about [domain] and also output the [domain] name with the following instructions:\\n\\n(1) Generate a sentence that contains a named entity. The named entity should be a recognizable entity type within the sentence.\\n(2) The named entity must be contextually relevant and correctly labeled with its type.\\n(3) The length should be in 50 words.\\n\\nListing 23: Prompt Format for NER tasks with APE.\\n\\nSuppose you need to create a dataset for [domain] recognition. Generate a sentence or short text passage where you mention a [domain] entity within a context. The named entity should be clearly identifiable within the text.\\n\\n[domain]: \u201cdisease\u201d for BC5CDR-Disease; \u201cchemical\u201d for CHEMDNER.\"}"}
{"id": "PN9uaKA1nV", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "PN9uaKA1nV", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\\n\\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics, 23:bbac409, 2022.\\n\\nClara H McCreery, Namit Katariya, Anitha Kannan, Manish Chablani, and Xavier Amatriain. Effective transfer learning for identifying similar questions: matching user questions to covid-19 faqs. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 3458\u20133465, 2020.\\n\\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. Generating training data with language models: Towards zero-shot language understanding. In Advances in Neural Information Processing Systems, 2022.\\n\\nYu Meng, Martin Michalski, Jiaxin Huang, Yu Zhang, Tarek Abdelzaher, and Jiawei Han. Tuning language models as training data generators for augmentation-enhanced few-shot learning. In International Conference on Machine Learning, pp. 24457\u201324477. PMLR, 2023.\\n\\nBertalan Mesk\u00f3 and Eric J Topol. The imperative for regulatory oversight of large language models (or generative ai) in healthcare. NPJ Digital Medicine, 6(1):120, 2023.\\n\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing instructional prompts to GPTk's language. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 589\u2013612, Dublin, Ireland, May 2022.\\n\\nSungrim Moon, Serguei Pakhomov, Nathan Liu, James O Ryan, and Genevieve B Melton. A sense inventory for clinical abbreviations and acronyms created using clinical notes and medical dictionary resources. Journal of the American Medical Informatics Association, 21(2):299\u2013307, 2014.\\n\\nHarsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.\\n\\nOpenAI. Introducing chatgpt, 2023a. URL https://openai.com/blog/chatgpt.\\n\\nOpenAI. Gpt-4 technical report. arXiv, 2023b.\\n\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\\n\\nYifan Peng, Shankai Yan, and Zhiyong Lu. Transfer learning in biomedical natural language processing: An evaluation of BERT and ELMo on ten benchmarking datasets. In Proceedings of the 18th BioNLP Workshop and Shared Task, pp. 58\u201365, Florence, Italy, August 2019.\\n\\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models. In Advances in Neural Information Processing Systems, 2021.\\n\\nZhaozhi Qian, Thomas Callender, Bogdan Cebere, Sam M Janes, Neal Navani, and Mihaela van der Schaar. Synthetic data for privacy-preserving clinical risk prediction. medRxiv, 2023.\\n\\nAlexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher R R\u00e9. Snorkel: rapid training data creation with weak supervision. Proceedings of the VLDB Endowment, 11(3):269\u2013282, 2017.\\n\\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3982\u20133992, 2019.\"}"}
{"id": "PN9uaKA1nV", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4902\u20134912, Online, July 2020. Association for Computational Linguistics.\\n\\nMourad Sarrouti, Asma Ben Abacha, Yassine Mrabet, and Dina Demner-Fushman. Evidence-based fact-checking of health-related claims. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 3499\u20133512, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\\n\\nChaitanya Shivade. Mednli \u2014 a natural language inference dataset for the clinical domain, 2017.\\n\\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 2023a.\\n\\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617, 2023b.\\n\\nChang Su, Yu Hou, Manqi Zhou, Suraj Rajendran, Jacqueline RMA Maasch, Zehra Abedi, Haotan Zhang, Zilong Bai, Anthony Cuturrufo, Winston Guo, et al. Biomedical discovery through the integrative biomedical knowledge hub (ibkh). Iscience, 26(4), 2023.\\n\\nOlivier Taboureau, Sonny Kim Nielsen, Karine Audouze, Nils Weinhold, Daniel Eds\u00e5rd, Francisco S Roque, Irene Kouskoumvekaki, Alina Bora, et al. Chemprot: a disease chemical biology database. Nucleic acids research, 39:D367\u2013D372, 2010.\\n\\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. Does synthetic data generation of llms help clinical text mining? arXiv preprint arXiv:2303.04360, 2023.\\n\\nBrandon Theodorou, Cao Xiao, and Jimeng Sun. Synthesize extremely high-dimensional longitudinal electronic health records via hierarchical autoregressive language model. arXiv preprint arXiv:2304.02169, 2023.\\n\\nGeorge Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, et al. An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. BMC bioinformatics, 16(1):1\u201328, 2015.\\n\\nTao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Chuck Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. arXiv preprint arXiv:2307.14334, 2023.\\n\\nYanshan Wang, Sunghwan Sohn, Sijia Liu, Feichen Shen, Liwei Wang, Elizabeth J Atkinson, Shreyasee Amin, and Hongfang Liu. A clinical text classification paradigm using weak supervision and deep representation. BMC medical informatics and decision making, 19:1\u201313, 2019.\\n\\nZifeng Wang and Jimeng Sun. PromptEHR: Conditional electronic healthcare records generation with prompt learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2873\u20132885, Abu Dhabi, United Arab Emirates, December 2022.\\n\\nChih-Hsuan Wei, Yifan Peng, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Jiao Li, Thomas C Wiegers, and Zhiyong Lu. Assessing the state of the art in biomedical relation extraction: overview of the biocreative v chemical-disease relation (cdr) task. Database, 2016, 2016.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rami Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\\n\\nCliff Wong, Sheng Zheng, Yu Gu, Christine Moung, Jacob Abel, Naoto Usuyama, Roshanthi Weerasinghe, Brian Piening, Tristan Naumann, Carlo Bifulco, et al. Scaling clinical trial matching using large language models: A case study in oncology. arXiv preprint arXiv:2308.02180, 2023.\"}"}
{"id": "PN9uaKA1nV", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Michael Wornow, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. The shaky foundations of clinical foundation models: A survey of large language models and foundation models for EMRs. arXiv preprint arXiv:2303.12961, 2023.\\n\\nChaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Further fine-tuning llama on medical papers. arXiv preprint arXiv:2304.14454, 2023.\\n\\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. Advances in Neural Information Processing Systems, 33:6256\u20136268, 2020.\\n\\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.\\n\\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong. Zerogen: Efficient zero-shot learning via dataset generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11653\u201311669, 2022a.\\n\\nJiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Progen: Progressive zero-shot dataset generation via in-context feedback. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 3671\u20133683, 2022b.\\n\\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and Woomyoung Park. GPT3Mix: Leveraging large-scale language models for text augmentation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2225\u20132239, Punta Cana, Dominican Republic, November 2021.\\n\\nYue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, and Chao Zhang. Large language model as attributed training data generator: A tale of diversity and bias. arXiv preprint arXiv:2306.15895, 2023.\\n\\nWerner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschlager, and Susanne Saminger-Platz. Central moment discrepancy (CMD) for domain-invariant representation learning. In International Conference on Learning Representations, 2017.\\n\\nRongzhi Zhang, Yue Yu, and Chao Zhang. SeqMix: Augmenting active sequence labeling via sequence mixup. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pp. 8566\u20138579, Online, November 2020.\\n\\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the AI ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023.\\n\\nJing Zhou, Yanan Zheng, Jie Tang, Jian Li, and Zhilin Yang. Flipda: Effective and robust data augmentation for few-shot learning. arXiv preprint arXiv:2108.06332, 2021.\\n\\nRan Zhou, Xin Li, Ruidan He, Lidong Bing, Erik Cambria, Luo Si, and Chunyan Miao. MELM: Data augmentation with masked entity language modeling for low-resource NER. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2251\u20132262, Dublin, Ireland, May 2022.\\n\\nWenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. Context-faithful prompting for large language models. arXiv preprint arXiv:2303.11315, 2023a.\\n\\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, 2023b.\"}"}
{"id": "PN9uaKA1nV", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limited Diversity.\\n\\nClinical datasets in real-world scenarios harbor a wealth of valuable knowledge that can be challenging to replicate within synthetically generated data by AI models. We evaluate synthetic dataset diversity by using both entity quantity and their normalized frequencies. The results are illustrated in Figures 2(b) and 2(c). Our analysis reveals that datasets generated by ZeroGen and DemoGen exhibit a limited number of clinical entities, having a substantial discrepancy with the ground truth. Furthermore, it is highlighted that only a minority of potential entities and relations are frequently referenced across instances, while the majority are generated infrequently.\\n\\nTo explicitly illustrate the aforementioned limitations of synthetic datasets created using existing methods, we present a case study in Figure 1(b). In this case study, we randomly select one sample from each class within the training set generated by ZeroGen and DemoGen. These selected samples are compared with the ground truth data from the MEDIQA-RQE dataset, which aims to predict whether a consumer health query can entail an existing Frequently Asked Question (FAQ). The comparison reveals that the samples generated by ZeroGen and DemoGen tend to be more straightforward, lacking the sufficient details and real-case nuances present in the ground truth data. Furthermore, the generated samples adhere to a more uniform style and structure, while the ground truth encompasses various situations and writing styles, including urgent and informal inquiries.\\n\\nThe revealed insights from the preliminary studies assert the necessity of domain-tailored knowledge for clinical synthetic data generation. In pursuit of efficient, effective, and scalable data generation for clinical domains, we introduce our novel framework, CLENG, a prior knowledge-informed clinical data generation. The overview of CLENG is shown in Figure 3. This innovative two-step methodology harnesses the emergent capabilities of LLMs and external knowledge from KGs to facilitate the synthesis of clinical data, even when only presented with few-shot examples.\\n\\n4.1 CLINICAL KNOWLEDGE EXTRACTION\\n\\nContrary to previous studies (Ye et al., 2022a;b; Meng et al., 2022; 2023) which employ generic queries to prompt LLMs for text generation, CLENG emphasizes refining clinically informed prompts. This approach aims to extract rich clinically relevant knowledge from parametric (e.g. LLMs) or nonparametric sources (e.g. knowledge graphs) and tailor it to clinical NLP tasks. To realize this objective, our modeling contains two dimensions including clinical topics and writing styles, which are integrated into the original prompts to infuse domain-specific knowledge. By dynamically composing different topics and writing styles together, CLENG can provide a diverse suite of prompts, resulting in a wider spectrum of text produced from LLM.\\n\\n4.1.1 CLINICAL TOPICS GENERATION\\n\\nWe provide two choices to generate clinical topics \u2013 one is to sample related entities or relations from external KG, and the other is to query relevant knowledge from LLM.\\n\\nTopics sampled from Non-Parametric KGs. Healthcare KGs offer a rich collection of medical concepts and their complex relationships, and have emerged as a promising tool for organizing medical knowledge in a structured way (Li et al., 2022). In our methodology, we employ the iBKH KG (Su et al., 2023) due to its broad coverage over clinical entities. To illustrate, for the Disease Recognition task (NCBI) (Dogan et al., 2014), we extract all medication nodes from the iBKH to...\"}"}
{"id": "PN9uaKA1nV", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Suppose you need to create a dataset for disease recognition. Your task is to: 1. Generate a sentence about disease. 2. Output a list of named entity about disease only. 3. The sentence should mention the disease named \\\\{Stroke\\\\}. 4. The sentence should mimic the style of \\\\{medical literature\\\\}.\\n\\nSome examples are:\\n\\nSentence: \\\"The development of tolerance to the muscular rigidity produced by morphine was studied in rats.\\\"\\nDisease: \\\\{muscular rigidity\\\\}\\n\\nSentence: \\\"Elevated levels of cholesterol in the blood are associated with an increased risk of cardiovascular diseases such as stroke and heart attack.\\\"\\nDisease: \\\\{cardiovascular diseases, stroke, heart attack\\\\}\\n\\n3. Language Model Fine-Tuning\\n\\nStyles suggested by LLMs.\\n\\nTo address the limitations mentioned in Sec 3.2 and introduce a diverse range of writing styles into the generated samples, we leverage the powerful LLM again by suggesting candidate writing styles for each task. Specifically, we incorporate task names into our prompts (e.g., disease entity recognition, recognizing text entailment, etc.) and integrate few-shot demonstrations. We then engage ChatGPT in suggesting several potential sources, speakers, or authors of the sentences. See Appendix F.1 for detailed prompt. Responses such as \\\"medical literature\\\" or \\\"patient-doctor dialogues\\\" are augmented into the prompts to imitate the writing styles found in real datasets.\\n\\n4. Knowledge-Infused Synthetic Data Generation\\n\\nWith the generated entities as well as styles, the key challenge becomes how to leverage them to extract rich clinical information from the LLM for improving synthetic data quality. Directly putting all the elements to enrich the prompt is often infeasible due to the massive size of entities. To balance informativeness as well as diversity, we propose a knowledge-infused strategy, where the collected clinical topics and writing styles serve as the base unit. In each step, we randomly sample a topic and a writing style from the candidate set to augment the prompts. For instance, for the Disease Recognition (NCBI) task, consider a clinical entity like \\\"stroke\\\". We enrich the prompt query for LLM by appending \\\"generate a sentence about stroke\\\" as a generation guidance.\\n\\nFor a comprehensive view of the prompt formats across various tasks, please refer to Appendix F. Despite its simplicity, this knowledge-infused strategy ensures that the clinical context is incorporated into the prompts while encouraging prompt diversity (via composing different entities and writing styles), thereby enhancing the quality and clinical relevance of the generated synthetic data.\"}"}
{"id": "PN9uaKA1nV", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nvised training objectives (denoted as $\\\\ell(\\\\cdot)$), then on the synthetic data $D_{syn}$ as\\n\\nStage I: $\\\\theta(1) = \\\\min \\\\theta E(x,y) \\\\sim D_{train} \\\\ell(f(x; \\\\theta), y)$,\\n\\nStage II: $\\\\theta(2) = \\\\min \\\\theta E(x,y) \\\\sim D_{syn} \\\\ell(f(x; \\\\theta), y)$, $\\\\theta_{init} = \\\\theta(1)$.\\n\\nIt's important to highlight that we strictly follow a standard fine-tuning process and avoid using any extra techniques: (1) for standard classification tasks, $\\\\ell(\\\\cdot)$ is the cross-entropy loss; (2) for multi-label classification tasks, $\\\\ell(\\\\cdot)$ is the binary cross-entropy loss; (3) for token-level classification tasks, we stack an additional linear layer as the classification head and $\\\\ell(\\\\cdot)$ is the token-level cross-entropy loss. The design of advanced learning objectives as well as data mixing strategies, while important, are orthogonal to the scope of this paper.\\n\\n5 EMPIRICAL EVALUATION\\n\\nGiven our focus on synthetic text generation, our primary interest lies in faithfully evaluating different synthetic text generation approaches under few-shot scenarios, rather than engaging in a \u201cstate-of-the-art\u201d race with general few-shot learning approaches (i.e. we never claim that we achieve state-of-the-art performance on these tasks). In this context, the following questions particularly intrigue us:\\n\\nRQ1: How does CLINGEN perform when compared with baselines on different down-stream tasks?\\nRQ2: How do different factors such as LLM generators and the size of synthetic data affect the performance of CLINGEN?\\nRQ3: How is the quality of the synthetic datasets generated by CLINGEN and baselines? These questions are addressed in Sec 5.2, Sec 5.3 and Sec 6, respectively.\\n\\n5.1 EXPERIMENT SETUP\\n\\nWe conduct experiments in the few-shot settings with 5 examples available for each class. We employ ChatGPT (OpenAI, 2023a) (gpt-3.5-turbo-0301) as the generator and maintain the same amount of synthetic training data for both CLINGEN and baselines for a fair comparison. The pre-trained PubMedBERT (Gu et al., 2021) is then applied to fine-tune on the generated synthetic data for both CLINGEN and baselines, where we consider both the Base and Large variants. See Appendix C for implementation details.\\n\\nDatasets and Tasks. In our exploration of few-shot synthetic data generation, we undertake a comprehensive evaluation of 16 datasets across a diverse array of tasks typically encountered in clinical NLP benchmarks (Peng et al., 2019; Fries et al., 2022). Specifically, we consider 2 text classification, 3 relation extraction (RE), 3 natural language inference (NLI), 2 fact verification, 1 sentence similarity (STS), 4 NER, and 1 attribute extraction tasks. Please see Appendix D for detailed dataset descriptions and the statistics of each dataset.\\n\\nBaselines. We compare CLINGEN with 9 baselines in total, including 6 data augmentation and 3 LLM-based data generation techniques. The data augmentation models include Word Substitution (Ribeiro et al., 2020), Back Translation (Xie et al., 2020), Mixup (Chen et al., 2020; Zhang et al., 2020), Transformer (Kumar et al., 2020; Zhou et al., 2022), LightNER (Chen et al., 2022a), and KGPC (Chen et al., 2023a). For LLM-based generation models, we consider ZeroGen (Ye et al., 2022a), DemoGen (Meng et al., 2023; Yoo et al., 2021) and ProGen (Ye et al., 2022b) as representative methods. See Appendix E for details.\\n\\n5.2 MODEL PERFORMANCE WITH THE SYNTHETIC DATA\\n\\nTable 1 summarizes the experimental results on different datasets. We also conduct supervised learning on the original training data and the extracted few-shot examples, denoted as \u201cSupervised-Full\u201d and \u201cSupervised-Few\u201d, respectively. Due to space limits, we report the average performance over all datasets for each task, but provide the detailed results for each dataset in Tables 6, 7, 8 in Appendix G. Based on the experimental results, we have the following findings:\\n\\n\u22c4 Our proposed approach, CLINGEN, consistently outperforms the baselines across all tasks. The average performance gain over all main metrics is 8.98% at Base scale and 7.27% at Large scale.\\n\\nIn addition, methods utilizing LLMs have better performance than traditional data augmentation techniques, illustrating the capacity of LLMs to extract valuable information from limited examples.\"}"}
{"id": "PN9uaKA1nV", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\n| Task                  | Text Class RE | NLI Fact Verification STS | NER MedAttr |\\n|-----------------------|--------------|----------------------------|-------------|\\n| Single-Sentence Tasks |              |                            |             |\\n| Sentence-Pair Tasks   |              |                            |             |\\n| Token Classification Tasks |          |                            |             |\\n\\n| Model          | F1 | Acc | F1-subset | P | R | F1 | Acc | F1-subset | P | R | F1 | Acc | F1-subset | P | R | F1 | Acc | F1-subset | P | R | F1 |\\n|----------------|----|-----|-----------|---|---|-----|-----|-----------|---|---|-----|-----|-----------|---|---|-----|-----|-----------|---|---|-----|\\n| PubMedBERT Base |   |     |           |   |   |     |     |           |   |   |     |     |           |   |   |     |     |           |   |   |     |\\n| Supervised-Full  | 77.01 | 77.34 | 79.20 | 67.58 | 65.49 | 75.70 |         |         |         |   |   |     |     |           |   |   |     |\\n| Supervised-Few   | 18.61 | 43.89 | 44.64 | 29.43 | 27.10 | 55.70 |         |         |         |   |   |     |     |           |   |   |     |\\n| DA-Word Sub      | 40.74 | 38.14 | 55.08 | 28.86 | 25.83 | 54.40 |         |         |         |   |   |     |     |           |   |   |     |\\n| DA-Back Trans    | 47.24 | \u2014     | 54.30 | 32.15 | 28.04 | 55.80 |         |         |         |   |   |     |     |           |   |   |     |\\n| DA-Mixup         | 45.09 | 43.37 | 53.52 | 32.78 | 29.12 | 58.20 |         |         |         |   |   |     |     |           |   |   |     |\\n| DA-Transformer   | 41.02 | 47.56 | 55.71 | 35.32 | 31.77 | 58.80 |         |         |         |   |   |     |     |           |   |   |     |\\n| LightNER         | \u2014   | \u2014    | \u2014         | \u2014    | \u2014   | \u2014    |         |         |         |   |   |     |     |           |   |   |     |\\n| KGPC             | \u2014   | \u2014    | \u2014         | \u2014    | \u2014   | \u2014    |         |         |         |   |   |     |     |           |   |   |     |\\n| ZeroGen          | 59.02 | 63.84 | 55.96 | 35.30 | 32.50 | 68.35 |         |         |         |   |   |     |     |           |   |   |     |\\n| DemoGen          | 64.09 | 67.46 | 59.80 | 40.30 | 35.95 | 70.85 |         |         |         |   |   |     |     |           |   |   |     |\\n| ProGen           | 65.16 | 67.23 | 59.57 | 37.71 | 34.54 | 69.30 |         |         |         |   |   |     |     |           |   |   |     |\\n| CLIN w/ KG       | 67.15 | 69.01 | 64.89 | 43.83 | 39.43 | 72.17 |         |         |         |   |   |     |     |           |   |   |     |\\n| CLIN w/ LLM      | 67.82 | 70.06 | 67.24 | 46.50 | 41.46 | 73.31 |         |         |         |   |   |     |     |           |   |   |     |\\n\\n| Performance Gain | 4.08% | 3.85% | 12.44% | 15.38% | 13.00% | 3.47% | 6.23% | \u2014 | \u2014 | 17.47% |\\n\\nTable 1: Experimental results aggregated by tasks. Bold and underline indicate the best and second best results for each dataset, respectively. \u2020: The models can only be applied to NER tasks, and the number is reported from the original paper. \u2217: Since the two \u2020 models only report results on two NER datasets, we report an average performance on those two datasets for a fair comparison.\\n\\nThe performance gain of DemoGen and ProGen over ZeroGen further demonstrates the positive influence of few-shot examples on overall comprehension.\\n\\n\u22c4 In token classification tasks, CLIN GEN performs better with KG compared to LLM. This improvement stems from the strong alignment between the task's target and the generated domain knowledge, where the extracted topics serve as direct labels for these datasets. The single-sentence and sentence-pair tasks, on the other hand, display an advantage for the LLM-based knowledge extraction. This can be attributed to two potential reasons: first, these tasks prioritize understanding entire sentences over specific terminologies, and some specialized terms might even impede LLM comprehension. Second, KGs may not always contain the required information. For example, in a RE task involving chemicals and proteins, some types of the relations are absent from the KG, thus the performance gain is rather limited.\\n\\n\u22c4 Some data augmentation methods are task-specific, limiting their generalizability. For example, LightNER and KGPC are designed specifically for NER tasks. It is also non-trivial to apply Back Translation to NER or RE tasks, as it requires locating related entities in the generated sentence accurately. In contrast, CLIN GEN is flexible and can be effectively applied to various tasks.\\n\\n5.3 ABLATION AND PARAMETER STUDIES\\n\\nEffect of Different LLM Generators. To investigate the impact of various LLMs on CLIN GEN, we leverage other models in the GPT-family as the text generator. Specifically, we utilize InstructGPT (text-curie-001) (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023b). Note that we only generate 500 samples in the GPT-4 setting due to budget constraints, but we provide the results of GPT-3.5 with same amount of synthetic samples for a fair comparison. From Figure 4 we observe that CLIN GEN generally outperforms the best baseline in all settings. Additionally, we observe gen...\"}"}
{"id": "PN9uaKA1nV", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nClinical natural language processing requires methods that can address domain-specific challenges, such as complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation using LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, LINGEN, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 7 clinical NLP tasks and 16 datasets reveals that LINGEN consistently enhances performance across various tasks, effectively aligning the distribution of real datasets and significantly enriching the diversity of generated training instances. We will publish our code and all the generated data upon acceptance.\\n\\nIntroduction\\nClinical natural language processing (NLP) emerges as a distinct subfield including the extraction, analysis, and interpretation of medical data from unstructured clinical text (Wornow et al., 2023). Despite its significance, unique challenges evolve for methodology development in clinical NLP. For example, clinical texts are often dense with abbreviations and specialized medical terminologies that can be perplexing to standard NLP models (Lee et al., 2023). Fortunately, recent advances in large language models (LLMs) (Brown et al., 2020; Chung et al., 2022; Ouyang et al., 2022; OpenAI, 2023a;b) provide a promising way to resolve these issues, as they contain billions of parameters and have been pretrained on massive corpora, thus inherently capturing a significant amount of clinical knowledge (Agrawal et al., 2022; Nori et al., 2023; Eric et al., 2023; Wong et al., 2023; Singhal et al., 2023a;b; Luo et al., 2022; Liu et al., 2023b). These progresses inspire the need for designing specialized approaches for adapting LLMs to clinical settings, which both address the terminology complexities and improve models through clinical data finetuning (Tu et al., 2023; Liu et al., 2023a).\\n\\nDespite the strong capacity of general LLMs, directly applying them to infer over clinical text data is often undesired in practice. Firstly, these LLMs often have billions of parameters that translate to significant computational resources even for inference, leading to increased infrastructure costs and long inference time. Furthermore, the sensitive patient information contained in the clinical text naturally raises privacy and regulatory compliance concerns (Mesk\u00f3 & Topol, 2023; Keeling, 2023).\\n\\nTo effectively combat these challenges, generating synthetic training data using LLMs stands out as a promising solution: It leverages the capability of LLMs in a way that is both resource-efficient and privacy-centric. When trained with these synthetic datasets mimicking real-world clinical data, models can achieve high performance while obeying data protection regulations.\\n\\nSynthetic data generation with foundation models is a popular research domain in general machine learning (Azizi et al., 2023; Borisov et al., 2023; Meng et al., 2022; Ye et al., 2022a;b). However, when considering producing high-quality data that conforms to the distribution of the original dataset, simply adapting LLMs trained on general texts to generate clinical data presents unique challenges. To assess the quality of data generated by current methods, we carry out an exhaustive evaluation centered on distribution and diversity, detailed in Section 3. Insights from the t-SNE...\"}"}
{"id": "PN9uaKA1nV", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"under review as a conference paper at ICLR 2024\\n\\nembeddings visualization and the Central Moment Discrepancy (CMD) score indicate a noteworthy data distribution shift. We further examine the clinically-related entity quantities and frequencies in synthetic data, where a notable decline is observed when contrasting synthetic data with ground truth data. While some research has delved into clinical data generation with language models, many of these efforts are tailored to specific tasks. Examples include medical dialogues (Chintagunta et al., 2021), clinical notes (Giorgi et al., 2023), medical text mining (Tang et al., 2023), and electronic health records (Ive et al., 2020; Wang & Sun, 2022; Theodorou et al., 2023; Qian et al., 2023).\\n\\nThese studies often directly adopt language models for text generation, and sometimes on excessive training data. Till now, a unified principle to better adapt LLMs for generating synthetic text for facilitating clinical downstream applications is still missing.\\n\\nMotivated by the above analysis, we propose C LIN GEN, a clinical knowledge-infused generic framework for high-quality clinical text generation in few-shot scenarios. Our ultimate goal is to narrow the gap between synthetic and ground-truth data and encourage the topic diversity of the generated text. Towards this end, we propose a strategy to utilize clinical knowledge extraction to contextualize the prompts. This includes generating clinical topics on entity and relation information from both KGs and LLMs and deriving writing style suggestions from LLMs. By doing this, C LIN GEN integrates both non-parametric insights from external clinical knowledge graphs with the intrinsic parametric knowledge encoded in LLMs. It is worth noting that, C LIN GEN only rely on minimal additional human efforts, and can be readily applied to a wide array of core tasks in clinical NLP.\\n\\nOur contributions can be summarized as follows:\\n\\n\u2022 We propose C LIN GEN, a generic clinical knowledge-infused framework for clinical text data generation in few-shot settings. It can be readily applied to a wide range of tasks in clinical NLP.\\n\\n\u2022 We present a simple yet effective strategy to utilize clinical knowledge extraction to customize the prompts toward target clinical NLP tasks. This includes generating clinical topics from both KGs and LLMs and deriving writing style suggestions from LLMs.\\n\\n\u2022 We conduct an exhaustive evaluation of synthetic clinical data generation across 7 clinical NLP tasks and 16 datasets. Empirical findings demonstrate that C LIN GEN not only aligns more closely with the distribution of the original data but also amplifies the diversity of the generated training samples. The empirical performance gains are consistent across various tasks with different LLMs and classifiers (8.98% for PubMedBERT Base and 7.27% for PubMedBERT Large).\\n\\n2 RELATED WORK\\n\\nGenerating additional training data enables a more precise analysis of medical text, and has gained more attention in the past years. Earlier research has employed data augmentation techniques to generate similar samples to existing instances with word substitution (Kang et al., 2021; Ribeiro et al., 2020), back translation (Xie et al., 2020), pretrained transformers (Kumar et al., 2020; Zhou et al., 2021; 2022) for enhancing model generalization. But they often yield rigid transformations and the quality of the augmented text cannot be always guaranteed. Another line of work focuses on leveraging external knowledge to create weak labels (Ratner et al., 2017; Fries et al., 2017; Wang et al., 2019; Dunnmon et al., 2020). These methods typically require domain expertise and additional task-specific corpora, which can be resource-intensive to obtain for low-resource clinical tasks.\\n\\nThe emergence of LLMs has presented new possibilities, and some studies attempt to use LLM to generate training data (Meng et al., 2022; 2023; Ye et al., 2022a; Yu et al., 2023; Chung et al., 2023), often with few demonstrations (Yoo et al., 2021). However, these methods often use generic and simple prompts that may not fully capture domain-specific knowledge, thus potentially limiting the quality of the generated data. Liu et al. (2022a); Chung et al. (2023) employ interactive learning to generate additional instances to refine the existing dataset, at the cost of additional human efforts.\\n\\nOne recent study (Tang et al., 2023) explores synthetic data generation for clinical NLP. Nevertheless, their proposed approach relies on a much larger training set to generate candidate entities, which disregards the practical low-resource setting (Perez et al., 2021). Furthermore, their study is limited to a narrow range of tasks (2 tasks and 4 datasets only), lacking breadth in terms of exploring a diverse set of clinical NLP applications.\\n\\nOn the other hand, several works aimed at optimizing prompts using LLM itself (Mishra et al., 2022; Zhou et al., 2023b; Yang et al., 2023) or knowledge graphs (Chen et al., 2022b; Hu et al., 2022; Liu et al., 2022b).\"}"}
{"id": "PN9uaKA1nV", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nSentence A: Can drinking alcohol increase the risk of liver disease?\\nSentence B: Does alcohol consumption contribute to liver disease risk?\\n\\nSentence A: What are the side effects of chemotherapy?\\nSentence B: What are the possible adverse effects of chemotherapy?\\n\\nSentence A: My 3yrs old boy found my bleach at the laundry and I suspect he swallowed a bit of it. How do I treat this pls.\\nSentence B: What the Doc will do if a child swallows bleach?\\n\\nSentence A: What are the side effects of metformin?\\nSentence B: Can I take ibuprofen for a headache?\\n\\nSentence A: What are the common symptoms of influenza?\\nSentence B: Can I take ibuprofen to manage my headache?\\n\\nSentence A: I have exercise induced asthma. Would any of these non drug devises be suitable please?\\nSentence B: Are there any treatments or cures for albinism?\\n\\nIn this paper, we study the synthetic data generation problem in the few-shot setting. The input consists of a training set \\\\( D_{train} = \\\\{ (x_i, y_i) \\\\}_{K=1}^K \\\\), where each \\\\( (x_i, y_i) \\\\) pair represents an input text and its corresponding label for the \\\\( i \\\\)-th example. \\\\( K \\\\) denotes the total number of training samples, which is intentionally kept at a very small value (5-shot per label). The primary objective is to harness the capabilities of an LLM \\\\( M \\\\) to generate a synthetic dataset, denoted as \\\\( D_{syn} = \\\\{ (e_xi, e_yi) \\\\}_{N=1}^N \\\\), where \\\\( N \\\\) is the number of generated samples (\\\\( N \\\\gg K \\\\)). For each downstream task, we fine-tune a classifier \\\\( C_\\\\theta \\\\) (a moderate-size pre-trained language model) parameterized by \\\\( \\\\theta \\\\) on the synthetic dataset \\\\( D_{syn} \\\\) for evaluating on the target task.\\n\\nHere, we take a closer look at the synthetic text data generated by two representative approaches: ZeroGen (Ye et al., 2022a), which directly instructs LLMs for data generation, and DemoGen (Yoo et al., 2021; Meng et al., 2023), which augments the prompt with few-shot demonstrations. We observe that these methods often introduce distribution shifts and exhibit limited diversity, which can be suboptimal for improving downstream performance. The illustration is as follows, and we include additional figures in Appendix B.\\n\\nDistribution Shift. An inherent challenge when adapting LLMs to specific domains for text generation is the issue of distribution shift, given that LLMs are primarily trained on vast amounts of web text in general domains. In Figure 1(a), we visualize the embeddings of both the ground truth training data and synthetic datasets generated via two representative methods. Overall, these methods use generic prompts (see Appendix F.3 for details) with minimal domain-specific constraints. This limitation remains evident even when incorporating few-shot demonstrations into the process, with a notable disparity between the embeddings of the ground truth data and synthetic data.\\n\\nTo quantify the data distribution shift, we employ Central Moment Discrepancy (CMD) (Zellinger et al., 2017) to measure the gap between synthetic and real data across six clinical NLP datasets. Particularly, a high CMD value indicates a large gap between the two given distributions. Figure 2(a) illustrates that both ZeroGen and DemoGen exhibit elevated CMD scores, indicating substantial dissimilarity between the synthetic data and those of the real dataset.\\n\\nWhile In-context Learning (Brown et al., 2020) can also be utilized, it is often hard to fit all generated instances into the context window, especially for datasets with high cardinality.\\n\\n2 We employ SentenceBERT (Reimers & Gurevych, 2019) as the text encoder.\"}"}
{"id": "PN9uaKA1nV", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Effect of Size of Synthetic Data.\\n\\nIn Figure 5 (and more in Appendix H), we study the effect of the size of synthetic data. The result shows that ClinGen consistently outperforms the best baseline, using only around 10% of the synthetic examples. This illustrates that incorporating domain knowledge and increasing the diversity of the prompts could be an effective way to improve the sample efficiency, and narrow the gap between the performance of synthetic and ground-truth dataset.\\n\\nComparison with few-shot inference via prompting ChatGPT.\\n\\nWe also evaluate the performance of few-shot in-context learning with ChatGPT and two medical LLMs, namely PMC-LLaMa (Wu et al., 2023) and MedAlpaca (Han et al., 2023). Due to budget limits, we only run experiments on datasets with few testing samples for each task. As presented in Table 2, ClinGen at PubMedBERT Large scale achieves better results on 5 out of 6 datasets than ChatGPT few-shot learning, which uses \\\\( \\\\sim 530 \\\\times \\\\) more parameters. One exception is for PUBHEALTH, as it requires complex reasoning abilities that PubMedBERT may not fully possess. The two medical LLMs, on the other hand, perform less effectively than both ClinGen and GPT-3.5 due to fewer parameters, limited reasoning capabilities, and training on a general medical corpus unsuited for the tasks.\\n\\nOverall, ClinGen offers cost-effective and time-efficient advantages. While it entails a one-time investment in both money and time for synthetic training data generation, subsequent prediction relying on a moderate-sized model is much more efficient. Besides, the continued use of ChatGPT for inference on new testing data incurs ongoing time and financial costs, while our model requires zero additional costs for querying APIs. The price information is exhibited in Appendix J.\\n\\nEffect of Topic Extraction and Style Suggestion.\\n\\nWe inspect different components of ClinGen in Table 3. It is observed that both Topics Extraction and Style Suggestion contribute to model performance as they enhance the relevance of generated samples to domain knowledge and introduce greater diversity. Different from the other datasets, MEDIQA-RQE shows more performance gain incorporating writing style than topics. It is because NLI tasks focus on capturing the relationships between two sentences while incorporating additional knowledge entities does not directly help the model improve the reasoning ability.\\n\\n| HOC | CDR | MEDIQA-RQE | NCBI-Disease | w/ KG | w/ LLM | w/ KG | w/ LLM | w/ KG | w/ LLM |\\n|-----|-----|-------------|--------------|-------|--------|-------|--------|-------|--------|\\n| ClinGen w/o Topics | 70.86 | 58.51 | 69.86 | 55.09 |\\n| ClinGen w/ Topics | 76.28 | 76.42 | 61.74 | 63.34 |\\n| ClinGen w/o Styles | 73.25 | 74.40 | 59.10 | 60.15 |\\n| ClinGen w/ Styles | 74.85 | 72.40 | 59.46 | 55.95 |\\n\\nTable 3: Ablation studies on topic extraction and style suggestion at Base scale.\"}"}
{"id": "PN9uaKA1nV", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nGround Truth\\nZeroGen\\nDemoGen\\nClinGen w/KG\\nClinGen w/LLM\\n\\n(a) t-SNE plot\\n\\nSentence A: I've been experiencing a discomfort in my stomach, what could be causing it?\\nSentence B: What are the possible causes for abdominal pain?\\n\\nSentence A: I recently started working with metal and found out about the health risks of beryllium exposure. What are the symptoms of beryllium poisoning, and how can I protect myself from it?\\nSentence B: What are the symptoms and preventive measures for berylliosis?\\n\\nSentence A: Why are my nails turning yellow? It's never happened before.\\nSentence B: What are some home remedies for acne scars?\\n\\nSentence A: I feel like food is getting stuck in my throat, is there anything I can do to make it easier to swallow?\\nSentence B: What are some home remedies for a sore throat?\\n\\nEntail Not Entail\\n\\n(b) Case study of generated examples\\n\\nFigure 6: Data distribution and diversity measures on CLINGEN. (a) is from BC5CDR-Disease and (b) is from MEDIQA-RQE using CLINGEN with LLM.\\n\\nLitCovid CDR MEDIQA-RQE MQP CHEMDNER BC5CDR-D\\n\\n0.0\\n0.4\\n0.8\\n1.2\\n1.6\\n2.0\\n\\nCMD\\nZeroGen\\nDemoGen\\nProGen\\nClinGen w/KG\\nClinGen w/LLM\\nGround Truth\\n\\n(a) CMD\\n\\nLitCovid CDR MEDIQA-RQE MQP CHEMDNER BC5CDR-D\\n\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n\\nAvg. # of Unique Entities per Instance\\nZeroGen\\nDemoGen\\nProGen\\nClinGen w/KG\\nClinGen w/LLM\\nGround Truth\\n\\n(b) Entity Coverage\\n\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\n\\nEntity ID's Sorted by Frequency\\n10 1\\n10 2\\n10 3\\n10 4\\n\\nEntity Frequency\\nZeroGen\\nDemoGen\\nClinGen w/KG\\nClinGen w/LLM\\nGround Truth\\n\\n(c) Entity Frequency\\n\\nFigure 7: Data distribution and diversity measures on CLINGEN. (c) is from BC5CDR-Disease.\\n\\ngenerated by CLINGEN and baselines compared with the ground truth. This visualization clearly demonstrates that CLINGEN exhibits a greater overlap with the ground truth, indicating a similar distribution as the original dataset. In addition, as depicted in Figure 7(a), the embedding of CLINGEN aligns more closely with the ground truth distribution than other baselines across all six datasets, further justifying the efficacy of CLINGEN for mitigating the distribution shift issue.\\n\\nDiversity Measures.\\nTable 4 calculates the average cosine similarity for sample pairs using SentenceBERT embeddings. Compared to baselines, the dataset generated with CLINGEN exhibits lower cosine similarity and the average similarity is close to that of the ground truth training data, which shows CLINGEN could render more diverse data. Moreover, Figure 7(b) highlights the ability of CLINGEN to cover a broader range of entities in comparison to the baselines. We find that CLINGEN w/ KG captures a larger variety of entities than CLINGEN w/ LLM, because KG tends to cover more extensive knowledge, including relatively uncommon information that may not be present in LLMs. Figure 7(c) reflects that the entity frequency distribution of CLINGEN is more in line with the ground truth, having a relatively balanced distribution among all entities. This ensures that CLINGEN generates synthetic data with a wide range of diverse topics.\\n\\nCase Study.\\nIn Figure 6(b), we present a case study of examples generated by CLINGEN with LLM on MEDIQA-RQE dataset, which consists of consumer health queries. The showcased examples reveal that the sentences generated by CLINGEN include more extensive contextual information compared with the baseline as shown in Figure 1(b). These sentences closely resemble the queries people might pose in real-life scenarios.\\n\\n7 CONCLUSION\\nIn this work, we propose a versatile approach to clinical text data generation using LLMs. We thoroughly assess existing methods for clinical data generation and identify issues including distribution shifts and limited diversity. To tackle these challenges, we introduce CLINGEN, a new framework that leverages clinical knowledge from non-parametric KGs and parametric LLMs. This knowledge empowers data generation by utilizing clinical topic knowledge and real-world writing styles in domain-specific prompts. Our extensive empirical evaluations across 7 clinical NLP tasks and 16 datasets, comparing to 9 baseline methods, consistently show that CLINGEN improves task performance, aligns closely with real data, and enhances data diversity. We expect this approach can be seamlessly incorporated into a broad suite of clinical text tasks to advance clinical NLP research.\"}"}
{"id": "PN9uaKA1nV", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Asma Ben Abacha and Dina Demner-Fushman. Recognizing question entailment for medical question answering. In AMIA Annual Symposium Proceedings, volume 2016, pp. 310. American Medical Informatics Association, 2016.\\n\\nMonica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. Large language models are few-shot clinical information extractors. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1998\u20132022, Abu Dhabi, United Arab Emirates, December 2022.\\n\\nAhmed Alaa, Boris Van Breugel, Evgeny S. Saveliev, and Mihaela van der Schaar. How faithful is your synthetic data? Sample-level metrics for evaluating and auditing generative models. In Proceedings of the 39th International Conference on Machine Learning, pp. 290\u2013306. PMLR, 17\u201323 Jul 2022.\\n\\nShekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J Fleet. Synthetic data from diffusion models improves imagenet classification. arXiv preprint arXiv:2304.08466, 2023.\\n\\nSimon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan H\u00f6gberg, Ulla Stenius, and Anna Korhonen. Automatic semantic classification of scientific literature according to the hallmarks of cancer. Bioinformatics, 32(3):432\u2013440, 2015.\\n\\nAsma Ben Abacha, Chaitanya Shivade, and Dina Demner-Fushman. Overview of the MEDIQA 2019 shared task on textual inference, question entailment and question answering. In Proceedings of the 18th BioNLP Workshop and Shared Task, pp. 370\u2013379, Florence, Italy, August 2019.\\n\\nVadim Borisov, Kathrin Se\u00dfler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models are realistic tabular data generators. arXiv preprint arXiv:2210.06280, 2022.\\n\\nVadim Borisov, Kathrin Sessler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models are realistic tabular data generators. In The Eleventh International Conference on Learning Representations, 2023.\\n\\nAlex Bravo, Janet Pi\u02dcnero, N\u00b4uria Queralt-Rosinach, Michael Rautschka, and Laura I Furlong. Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research. BMC Bioinformatics, 16(1), February 2015.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33, 2020.\\n\\nAnthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Evaluating question answering evaluation. In Proceedings of the 2nd workshop on machine reading for question answering, pp. 119\u2013124, 2019.\\n\\nJiaao Chen, Zichao Yang, and Diyi Yang. Mixtext: Linguistically-informed interpolation of hidden space for semi-supervised text classification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2147\u20132157, 2020.\\n\\nPeng Chen, Jian Wang, Hongfei Lin, Di Zhao, and Zhihao Yang. Few-shot biomedical named entity recognition via knowledge-guided instance generation and prompt contrastive learning. Bioinformatics, 39(8):btad496, 2023a.\\n\\nQingyu Chen, Alexis Allot, Robert Leaman, Rezarta Islamaj Do\u02d8gan, and Zhiyong Lu. Overview of the biocreative vii litcovid track: multi-label topic classification for covid-19 literature annotation. In Proceedings of the BioCreative challenge evaluation workshop, 2021.\\n\\nXiang Chen, Lei Li, Shumin Deng, Chuanqi Tan, Changliang Xu, Fei Huang, Luo Si, Huajun Chen, and Ningyu Zhang. LightNER: A lightweight tuning paradigm for low-resource NER via pluggable prompting. In Proceedings of the 29th International Conference on Computational Linguistics, pp. 2374\u20132387, Gyeongju, Republic of Korea, October 2022a.\"}"}
{"id": "PN9uaKA1nV", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nXiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction. In Proceedings of the ACM Web conference 2022, pp. 2778\u20132788, 2022.\\n\\nYi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study. arXiv preprint arXiv:2304.00723, 2023.\\n\\nBharath Chintagunta, Namit Katariya, Xavier Amatriain, and Anitha Kannan. Medically aware gpt-3 as a data generator for medical dialogue summarization. In Machine Learning for Healthcare Conference, pp. 354\u2013372. PMLR, 2021.\\n\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\\n\\nJohn Chung, Ece Kamar, and Saleema Amershi. Increasing diversity while maintaining accuracy: Text data generation with large language models and human interventions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 575\u2013593, Toronto, Canada, July 2023.\\n\\nRezarta Islamaj Dogan, Robert Leaman, and Zhiyong Lu. NCBI disease corpus: A resource for disease name recognition and concept normalization. Journal of biomedical informatics, 47:1\u201310, 2014.\\n\\nJared A Dunnmon, Alexander J Ratner, Khaled Saab, Nishith Khandwala, Matthew Markert, Hersh Sagreiya, Roger Goldman, Christopher Lee-Messer, Matthew P Lungren, Daniel L Rubin, et al. Cross-modal data programming enables rapid medical machine learning. Patterns, 1(2), 2020.\\n\\nLehman; Eric, Evan Hernandez, Diwakar Mahajan, Jonas Wulff, Micah J Smith, Zachary Ziegler, Daniel Nadler, Peter Szolovits, Alistair Johnson, and Emily Alsentzer. Do we still need clinical language models? In Proceedings of the Conference on Health, Inference, and Learning, volume 209, pp. 578\u2013597. PMLR, 2023.\\n\\nJason Fries, Sen Wu, Alex Ratner, and Christopher R\u00e9. Swellshark: A generative model for biomedical named entity recognition without labeled data. arXiv preprint arXiv:1704.06360, 2017.\\n\\nJason Alan Fries, Leon Weber, Natasha Seelam, Gabriel Altay, Debajyoti Datta, Samuele Garda, Myungsun Kang, Ruisi Su, Wojciech Kusa, Samuel Cahyawijaya, Fabio Barth, Simon Ott, et al. Bigbio: A framework for data-centric biomedical natural language processing. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.\\n\\nJohn Giorgi, Augustin Toma, Ronald Xie, Sondra Chen, Kevin R An, Grace X Zheng, and Bo Wang. Clinical note generation from doctor-patient conversations using large language models: Insights from mediqa-chat. arXiv preprint arXiv:2305.02220, 2023.\\n\\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):1\u201323, 2021.\\n\\nTianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander L\u00f6ser, Daniel Truhn, and Keno K Bressem. Medalpaca\u2013an open-source collection of medical conversational AI models and training data. arXiv preprint arXiv:2304.08247, 2023.\\n\\nStefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. Tabllm: Few-shot classification of tabular data with large language models. In International Conference on Artificial Intelligence and Statistics, pp. 5549\u20135581. PMLR, 2023.\\n\\nShengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu, and Maosong Sun. Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2225\u20132240, Dublin, Ireland, May 2022.\"}"}
{"id": "PN9uaKA1nV", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                  | PubMedBERT Base | PubMedBERT Large |\\n|------------------------|-----------------|------------------|\\n| Supervised-Full (SOTA) | 73.55 84.35 67.81 76.60 71.96 | - 84.87 - |\\n| Supervised-Full        | 71.70 82.32 67.81 76.60 71.96 82.55 85.10 83.81 76.24 | - |\\n| Supervised-Few         | 24.08 13.13 41.62 52.96 46.61 57.71 46.54 51.53 33.54 | |\\n| DA-Word Sub            | 36.49 44.98 40.50 46.20 43.16 51.15 32.10 39.45 31.82 | |\\n| DA-Back Trans          | 39.7 54.78 - - - - - - | |\\n| DA-Mixup               | 40.82 49.35 41.4 44.8 43.03 55.44 48.30 51.62 35.45 | |\\n| DA-Transformer         | 39.86 42.18 44.6 61.7 51.77 59.4 46.5 52.16 38.73 | |\\n| ZeroGen                | 50.50 67.90 38.82 91.82 54.57 84.38 80.68 82.49 54.46 | |\\n| DemoGen                | 57.65 70.52 46.9 83.3 60.01 93.14 80.19 86.18 56.18 | |\\n| ProGen                 | 58.06 - - - - - - - - | |\\n| CLINGEN w/ KG          | 58.01 76.28 56.98 67.38 61.75 93.33 83.68 88.24 57.04 | |\\n| CLINGEN w/ LLM         | 59.22 76.42 60.6 66.35 63.34 94.61 78.17 85.61 61.22 | |\\n| CLINGEN w/ KG+LLM      | 56.56 78.02 57.97 71.09 63.86 92.57 88.59 90.54 58.48 | |\\n\\nTable 6: Performance on single-sentence tasks evaluated by PubMedBERT Base and PubMedBERT Large. Bold and underline indicate the best and second best results for each dataset, respectively. Note that the performance of 'Supervised-Full (SOTA)' is copied from the existing paper. If the value in this field is missing, this means we cannot find reported results with the same-scale model on that dataset.\"}"}
{"id": "PN9uaKA1nV", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                     | BC5CDR-Disease | BC5CDR-Chemical | NCBI-Disease | CHEMDNER | CASI  |\\n|--------------------------|----------------|-----------------|--------------|----------|-------|\\n|                          | P   | R   | F1  | P   | R   | F1  | P   | R   | F1  | P   | R   | F1  |\\n| **PubMedBERT** Base      | 86.10| 86.30| 74.20| 92.20| 92.45| 79.30| 88.76| 88.60| 77.90| 75.30| 74.00| 67.20|\\n| **Supervised-Full (SOTA)** | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   |\\n| **Supervised-Full**      | 83.84| 87.92| 85.83| 91.70| 91.90| 89.25| 92.35| 92.14| 89.65| 87.30| 86.90| 78.50|\\n| **Supervised-Few**       | 24.86| 39.47| 30.51| 63.70| 46.07| 53.48| 36.16| 39.47| 37.74| 48.00| 28.70| 35.92|\\n| **DA-Word Sub**          | 35.34| 39.54| 37.32| 63.13| 52.52| 57.34| 53.40| 36.70| 43.50| 47.45| 33.15| 39.03|\\n| **DA-Mixup**             | 36.13| 42.90| 37.32| 63.13| 52.52| 57.34| 53.40| 36.70| 43.50| 47.45| 33.15| 39.03|\\n| **ZeroGen**              | 55.60| 39.10| 45.91| 73.20| 82.85| 77.73| 56.25| 45.98| 50.60| 54.34| 52.93| 53.63|\\n| **DemoGen**              | 63.10| 48.44| 54.81| 76.40| 81.65| 78.94| 57.65| 49.08| 53.02| 54.00| 53.77| 53.88|\\n| **ProGen**               | 61.60| 50.50| 55.50| 77.10| 82.02| 79.48| 56.01| 53.50| 54.73| 51.55| 53.00| 52.26|\\n\\n| Model                     | BC5CDR-Disease | BC5CDR-Chemical | NCBI-Disease | CHEMDNER | CASI  |\\n|--------------------------|----------------|-----------------|--------------|----------|-------|\\n|                          | P   | R   | F1  | P   | R   | F1  | P   | R   | F1  | P   | R   | F1  |\\n| **PubMedBERT** Large     | 86.39| 86.60| 74.96| 92.80| 92.45| 79.86| 89.18| 89.00| 77.20| 75.92| 74.50| 67.20|\\n| **Supervised-Full (SOTA)** | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   | \u2014   |\\n| **Supervised-Full**      | 86.77| 85.92| 86.34| 92.80| 92.94| 92.80| 92.48| 92.35| 89.00| 87.90| 87.50| 79.00|\\n| **Supervised-Few**       | 25.52| 45.85| 32.79| 61.40| 54.41| 57.69| 44.86| 40.12| 42.35| 43.40| 34.60| 38.50|\\n| **DA-Word Sub**          | 38.54| 38.85| 38.69| 64.85| 53.96| 58.91| 52.59| 45.35| 48.70| 44.85| 36.69| 40.36|\\n| **DA-Mixup**             | 36.27| 46.67| 41.61| 64.85| 53.96| 58.91| 52.59| 45.35| 48.70| 44.85| 36.69| 40.36|\\n| **ZeroGen**              | 57.40| 39.21| 46.59| 78.08| 80.97| 79.49| 54.52| 49.00| 51.61| 48.56| 59.44| 53.45|\\n| **DemoGen**              | 57.34| 49.48| 53.12| 78.27| 84.65| 82.90| 59.43| 56.83| 58.10| 48.03| 60.39| 53.51|\\n| **ProGen**               | 60.34| 54.13| 57.07| 78.42| 83.90| 81.49| 60.02| 55.28| 57.55| 50.40| 59.64| 54.63|\\n\\n**Table 7:** Performance on sentence-pair tasks evaluated by PubMedBERT Base and PubMedBERT Large.\"}"}
{"id": "PN9uaKA1nV", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10 and 11 show the effect of different generators and the effect of the proportion of data on two additional datasets, respectively. Overall, our method generally outperform the best baseline. One interesting finding for the NCBI-Disease dataset is that ClinGen performs worse than the best on one variant. We hypothesize that it is because this task involves more complex input and output, potentially posing a challenge for moderate-size LLMs to follow the instructions.\\n\\nBesides, as few-shot sample selection is important for the final performance, we show the performance of different random seeds (with different seed examples/training process), and observe that our method ClinGen generally outperforms the baselines with non-negligible margins, which indicates the robustness of ClinGen as it does not rely on a specific subset of few-shot training examples to perform well.\\n\\nWe present additional quality analysis of the synthetic dataset with t-SNE plots in Figure 12 and the regularized entity frequencies in Figure 13.\\n\\nWe display the monetary cost of ClinGen for calling the OpenAI APIs, with a comparison with prompting GPT-3.5 for direct inference and DemoGen. From the values shown in Figure 10, we observe that inference via GPT-3.5 generally has a higher cost, as it needs to input all the testing samples for prompting. In contrast, DemoGen has a relatively lower cost, because it does not include the topics and writing styles to the prompts as ClinGen does.\\n\\nTable 10: The average cost (in US dollars) of running ClinGen on various datasets per 1000 samples, compared with prompting GPT-3.5 for inference and DemoGen.\\n\\nWe conduct additional experiments on two QA datasets, namely BioASQ (Tsatsaronis et al., 2015) and PubMedQA (Jin et al., 2019) on ClinGen and the most relevant baselines. As presented in Table 11, ClinGen consistently outperforms the baselines on these two datasets.\"}"}
{"id": "PN9uaKA1nV", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12: The t-SNE plots of datasets generated by ClinGen, ZeroGen and DemoGen compared with the ground truth.\\n\\nHowever, when it comes to more intricate QA tasks designed to generate free-form answers, it is challenging to establish quantitative metrics that reliably correlate with human accuracy judgments (Chen et al., 2019), even with the assistance of state-of-the-art LLMs (Chen et al., 2023b). Usually, human evaluators are required to assess the answer quality, which might also introduce subjectivity and scalability issues into the evaluation process.\\n\\nTable 11: The performance of ClinGen and the most relevant baselines on two QA datasets.\"}"}
{"id": "PN9uaKA1nV", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this work, we propose CLING to better harness the LLM for synthetic text data generation. Despite the strong performance of CLING on 16 clinical NLP tasks, we mainly verify their efficacy from their empirical performance, sample diversity, and distribution gaps. However, there still exist gaps between the performance of the model $C_{\\\\theta}$ fine-tuned using our generated synthetic data and ground-truth data. To further improve CLING, there are several avenues of future works:\\n\\nUsing Clinical LLMs as Data Generator: Our method CLING relies on an LLM with instruction following ability. We mainly evaluate CLING using GPT-family models as the LLM. Recently, there are many LLMs that have been fine-tuned on additional clinical contexts as well as instructions (e.g. Med-PALM3), and achieved superior performance on challenging clinical NLP benchmarks. However, they are not open-sourced, thus we cannot run them in our experiments. An interesting future work could be how to leverage these Clinical LLMs as Data Generator to further boost the performance. Besides, it can be beneficial to inject more fine-grained clinical knowledge beyond entity and relations to further benefit data generation pipelines.\\n\\nData Evaluation: In this work, we consider the distribution gap and sample diversity as our optimization objective. However, there might be many other aspects for synthetic quality estimation (Alaa et al., 2022). We need more tools to capture, analyze, and improve this new aspect of data-centric AI.\\n\\nFactuality: One issue with LLM-based synthetic data generation is the phenomenon of hallucination, wherein the model generates information that are not grounded in reality (Zhang et al., 2023). This can lead to the propagation of misinformation, which may have negative impacts on the clinical domain. It is crucial to cross-verify the generated text with a reliable knowledge base or dataset. Furthermore, incorporating an additional layer of human review can also help in mitigating hallucinations and ensuring the faithfulness of LLM-generated synthetic outputs (Zhou et al., 2023a).\\n\\nApplication to Structured EHR datasets: On the other hand, EHR data falls within a distinct modality (i.e. tabular data) from textual data, which may require different methodologies and approaches (Ive et al., 2020; Wornow et al., 2023). Nonetheless, we are aware of the capabilities of LLMs in this context. Recent studies (Hegselmann et al., 2023; Borisov et al., 2022) have explored transforming tabular data into text to harness the power of LLMs, which yields promising results and shows the potential of LLMs for structured data generation. However, as these approaches are fundamentally different from the methods we propose in this paper, they are beyond the scope of this paper.\\n\\nPrivacy concerns: We are well aware of the patient privacy concern in clinical NLP. Specifically, we carefully curate the five few-shot demonstrations to ensure they only contain conceptual information and are fully free from any Protected Health Information (PHI) related to patients. With the five de-identified examples as the only data input for demonstrations, the synthetic training data we generate is highly unlikely to include any private information. We also acknowledge the possibility of inadvertently introducing sensitive data through the GPT model itself. To address this, we make a deliberate effort to avoid any instructions that can potentially extract sensitive patient information within the prompts. Instead, the prompts we use focus solely on obtaining conceptual information relevant to the target task. Lastly, we conduct rigorous inspections of the generated synthetic data across all covered tasks to affirm that no such private information exists in the synthetic data generated by our method.\\n\\nAdditional Preliminary Studies: We present additional preliminary studies of the t-SNE plots in Figure 8 and the regularized entity frequencies in Figure 9. These results further justify the distribution shift issue mentioned in section 3.2, demonstrating that the limited diversity as well as the distribution shift issue generally exists for a broad range of clinical NLP tasks.\"}"}
{"id": "PN9uaKA1nV", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: The t-SNE plots of datasets generated by ZeroGen and DemoGen compared with the ground truth.\\n\\nFigure 9: The regularized entity frequencies of datasets generated by ZeroGen and DemoGen compared with the ground truth in log scale.\"}"}
{"id": "PN9uaKA1nV", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For implementation, we use PyTorch (Paszke et al., 2019) and HuggingFace (Wolf et al., 2019). For each dataset, we randomly sample 5 examples from each class to provide few-shot demonstrations and keep a validation set of the same size. In the experiments, we generate 5000 synthetic training data for both CLINGEN and the baselines and report the average performance over 3 random seeds for all the results.\\n\\nDuring the data generation process when we call the ChatGPT APIs (OpenAI, 2023a), we set the parameter $\\\\text{top}_p = 1$ and $\\\\text{temperature} = 1.0$ to balance between the quality of the generated text as well as diversity (Chung et al., 2023; Yu et al., 2023). With the generated synthetic dataset, we follow the common few-shot learning setting (Perez et al., 2021) to train all the models for 6 epochs and use the model with the best performance on the validation set for evaluation.\\n\\nDuring the PubMedBERT fine-tuning, we adopt AdamW (Loshchilov & Hutter, 2017) for optimization with a linear warmup of the first 5% steps and linear learning rate decay. The learning rate is set to $2 \\\\times 10^{-5}$ for Base and $4 \\\\times 10^{-5}$ for Large, and the maximum number of tokens per sequence is 256.\\n\\n### Dataset Description\\n\\n| Corpus Tasks | #Class | #Train/#Test Metrics |\\n|--------------|--------|-----------------------|\\n| **Single-Sentence Tasks** |        |                       |\\n| LitCovid (Chen et al., 2021) Text Classification | 7 | 24960/6238 F1         |\\n| HOC (Baker et al., 2015) Text Classification | 10 | 3091/898 F1           |\\n| GAD (Bravo et al., 2015) Relation Extraction (RE) | 1 | 4750/350 P, R, F1     |\\n| CDR (Wei et al., 2016) Relation Extraction (RE) | 1 | 8431/2522 P, R, F1    |\\n| ChemProt (Taboureau et al., 2010) Relation Extraction (RE) | 5 | 8793/1087 F1          |\\n| **Sentence-Pair Tasks** |        |                       |\\n| MedNLI $^*$ (Shivade, 2017) Natural Language Inference (NLI) | 3 | 11232/1422 Acc        |\\n| MEDIQA-NLI $^\\\\dagger$ (Ben Abacha et al., 2019) Natural Language Inference (NLI) | 3 | -/405 Acc             |\\n| MEDIQA-RQE (Abacha & Demner-Fushman, 2016) Natural Language Inference (NLI) | 2 | 8588/302 Acc          |\\n| PUBHEALTH (Kotonya & Toni, 2020) Fact Verification | 4 | 9804/1231 Acc, F1     |\\n| HealthVer (Sarrouti et al., 2021) Fact Verification | 3 | 10591/1824 Acc, F1    |\\n| MQP (McCreery et al., 2020) Sentences Similarity (STS) | 2 | 10/3033 Acc           |\\n| **Token Classification Tasks** |        |                       |\\n| BC5CDR-Disease (Li et al., 2016a) Named Entity Recognition (NER) | 1 | 4882/5085 P, R, F1   |\\n| BC5CDR-Chemical (Li et al., 2016a) Named Entity Recognition (NER) | 1 | 4882/5085 P, R, F1   |\\n| NCBI-Disease (Dogan et al., 2014) Named Entity Recognition (NER) | 1 | 5336/921 P, R, F1    |\\n| CHEMDNER (Krallinger et al., 2015) Named Entity Recognition (NER) | 1 | 14522/12430 P, R, F1 |\\n| CASI (Agrawal et al., 2022; Moon et al., 2014) Attribute Extraction | 6 | 5/100 F1             |\\n\\nTable 5: Dataset statistics. We do not count the non-entity/non-relation class for relation extraction and token classification tasks to align with existing works. P and R stand for Precision and Recall. Metrics in bold are considered as the main metrics.\\n\\n$^*$ is not allowed to put into GPT and $^\\\\dagger$ does not provide training data, so we sample few-shot examples from the SciTail (Khot et al., 2018) instead.\\n\\nThe evaluation tasks and datasets are summarized in Table 5. Note that the number of training samples indicates the size of the original training set. Specifically, we consider the following datasets:\\n\\n- **Single-Sentence Tasks**\\n  - Text Classification:\\n    - The LitCovid dataset (Chen et al., 2021) consists of COVID-19-related publications from PubMed. The task is to predict the topics of the sentences, including \\\"Epidemic Forecasting\\\", \\\"Treatment\\\", \\\"Prevention\\\", \\\"Mechanism\\\", \\\"Case Report\\\", \\\"Transmission\\\", and \\\"Diagnosis\\\".\\n    - The HOC dataset (Baker et al., 2015) also extracts sentences from PubMed articles, each annotated at the sentence level. The task is to predict the topics of the sentences, including \\\"evading growth suppressors\\\", \\\"tumor promoting inflammation\\\", \\\"enabling replicative...\\\".\\n\\nWe do not further increase $t$, as previous analysis (Chung et al., 2023; Yu et al., 2023) has shown that increasing $t$ to larger value does not help with additional performance gain.\"}"}
{"id": "PN9uaKA1nV", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"immortality\u201d, \u201ccellular energetics\u201d, \u201cresisting cell death\u201d, \u201cactivating invasion and metastasis\u201d, genomic instability and mutation\u201d, \u201cinducing angiogenesis\u201d, \u201csustaining proliferative signaling\u201d, and \u201cavoiding immune destruction\u201d.\\n\\n\u25e6 Relation Extraction:\\n* The GAD (Bravo et al., 2015) dataset is to predict whether there is a relation between the given disease and gene in the sentences. Note that the original annotation for this dataset is Noisy. To remedy this issue, we relabel 350 examples from the original test set to form a clean subset for faithful evaluation.\\n* The CDR (Wei et al., 2016) dataset is to predict whether the provided chemical can induce the disease in the sentences.\\n* The ChemProt (Taboureau et al., 2010) dataset focuses on the chemical-protein relations, and the labels include \u201cUpregulator\u201d, \u201cDownregulator\u201d, \u201cAgonist\u201d, \u201cAntagonist\u201d, \u201cProduct of\u201d and \u201cNo relation\u201d.\\n\\n\u2022 Sentence-Pair Tasks\\n\u25e6 Natural Language Inference (NLI):\\n* The MedNLI (Shivade, 2017) dateset consists of sentences pairs derived from MIMIC-III, where we predict the relations between the sentences. The labels include \u201centailment\u201d, \u201cneutral\u201d and \u201ccontradiction\u201d.\\n* The MEDIQA-NLI (Ben Abacha et al., 2019) dataset comprises text-hypothesis pairs. Their relations include \u201centailment\u201d, \u201cneutral\u201d and \u201ccontradiction\u201d.\\n* The MEDIQA-RQE (Abacha & Demner-Fushman, 2016) dataset contains NIH consumer health question pairs, and the task is to recognize if the first question can entail the second one.\\n\\n\u25e6 Fact Verification:\\n* The PUBHEALTH (Kotonya & Toni, 2020) encompasses claims paired with journalist-crafted explanations. The task is to predict the relations between the claim and evidence, including \u201cRefute\u201d, \u201cUnproven\u201d, \u201cSupport\u201d, and \u201cMixture\u201d.\\n* The HealthVer (Sarrouti et al., 2021) contains evidence-claim pairs from search engine snippets regarding COVID-19 questions. The relations between claims and evidences are chosen from \u201cRefute\u201d, \u201cUnproven\u201d, and \u201cSupport\u201d.\\n\\n\u25e6 Sentence Similarity (STS):\\n* the MQP (McCreery et al., 2020) dataset comprises a collection of medical question pairs designed for identifying semantically similar questions. The task is to predict whether the two questions are equivalent or not.\\n\\n\u2022 Token Classification Tasks\\n\u25e6 Named Entity Recognition (NER):\\n* The BC5CDR-Disease (Li et al., 2016b) is to recognize diseases in the sentences.\\n* The BC5CDR-Chemical (Li et al., 2016b) is to recognize chemicals in the sentences.\\n* The NCBI-Disease (Dogan et al., 2014) is to recognize diseases in the sentences.\\n* The CHEMDNER (Krallinger et al., 2015) is to recognize chemicals in the sentences.\\n\\n\u25e6 Attribute Extraction (MedAttr):\\n* The CASI dataset (Agrawal et al., 2022; Moon et al., 2014) aims to identify interventions including medication, dosage, route, freq, reason, duration.\\n\\nIn this section, we give a detailed description for all baselines used in this study.\\n\\nData Augmentation Methods:\\n\u2022 DA-Word Sub: It performs word substitution for few-shot demonstrations to create new training sample. Specifically, we follow Checklist (Ribeiro et al., 2020) and maintain a word list to generate new examples.\\n\u2022 DA-Back Translation: It employ back translation to augment the training data Xie et al. (2020), including translating text from the target language to the source language and then back to the target language.\"}"}
