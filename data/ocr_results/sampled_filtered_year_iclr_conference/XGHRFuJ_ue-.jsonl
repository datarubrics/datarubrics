{"id": "XGHRFuJ_ue-", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sometimes my cat sometimes even eats his own hairballs. I'm pretty worried about his digestion.\\n\\nI have no choice but to buy a vacuum cleaner. My British shorthair. Yes, I have to vacuum every day. Does your cat shed a lot as well? What breed is it?\\n\\nMy cat started shedding everywhere in the spring. How to deal with it?\\n\\nYou can give him some hairball remedies. It's not a big problem. My cat eats hairballs, too. Oh no, how old is he? Is he deficient in something? Buy him some cat grass?feed him some hairball remedies. If he eats hairballs, you can give him some hairball remedies. It is okay. My cat eats hairballs, too. You need to pay attention to it. The cat may be deficient in some trace elements. Go to see a veterinarian.\\n\\nFigure 1: Illustration of Diamante's annotation interface.\\n\\nIn this paper, we collect an open-domain chit-chat dataset in Chinese with the assistance of a pre-trained dialogue model. In the following, we will describe the creation of the Diamante dataset.\\n\\n2.1 DATA COLLECTION\\n\\nDiamante aims to explore an efficient way to collect a batch of high-quality chit-chat conversations that align well with human values. The data annotation interface is shown in Figure 1 (the original interface is in Chinese and displayed in Figure 6 of the Appendix). The data collection process is carried out as follows.\\n\\nStep 1: Crafting the Dialogue Opening.\\nFirstly, the annotator is encouraged to craft a start utterance based on any topic of interest, as an informative and engaging dialogue opening is critical to a good conversation. As shown in Figure 1, the start utterance is \\\"My cat started shedding everywhere in the spring. How to deal with it?\\\". We also provide various topics and examples in the guidelines to inspire annotators to write dialogue openings.\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics of the Diamante dataset.\\n\\n|                  | Train | Valid | Test | Total |\\n|------------------|-------|-------|------|-------|\\n| Number of Dialogues | 5,838 | 500   | 500  | 6,838 |\\n| Number of Utterances | 83,765 | 7,166 | 7,184 | 98,115 |\\n| Average Utterance Length | 14.26 | 14.20 | 14.29 | 14.25 |\\n| Select / Revise / Rewrite | 18% / 41% / 41% | 19% / 40% / 41% | 19% / 40% / 41% | 18% / 41% / 41% |\\n\\nStep 2: Generating Candidate Responses with the Dialogue Model.\\n\\nGiven the dialogue context, a dialogue model (PLATO-XL in the Diamante dataset) is employed to generate multiple candidate responses. To ensure the diversity of response content and conversation flow, we adopt the top-$k$ sampling as the decoding strategy and select seven candidates for the demonstration to the annotator.\\n\\nStep 3: Producing Response with Human Feedback.\\n\\nWe then ask the annotator to select, revise or rewrite the candidate to produce an appropriate response.\\n\\n- **Select.** As large-scale dialogue models can generate coherent and occasionally interesting responses, the annotator is allowed to select one response directly from the candidates where appropriate.\\n\\n- **Revise.** Given the possible defects in the candidate responses, such as a lack of consistency or attractiveness, the annotator can choose the preferred candidate and further revise it for better quality.\\n\\n- **Rewrite.** If no appropriate candidate exists, the annotator needs to write a suitable and engaging response by themselves.\\n\\nIterating Step 2 & Step 3 to Continue the Dialogue.\\n\\nAfter collecting the response with human feedback, the conversation will continue by iterating step 2 and step 3. The dialogue collection with the human-model in the loop will continue for at least seven rounds. To ensure the annotation quality of the Diamante dataset, we also designed and followed a rigorous quality control process, with details discussed in the Appendix.\\n\\nThe above data collection strategy works well in terms of efficiency and quality. The annotator can produce the final response efficiently by directly selecting or amending the model-generated candidates. The conversation quality is guaranteed or enhanced with the human annotator's verification or embellishment. Moreover, the implicit human preference that appeared in the data collection process also allows the training of one preference estimation model without additional annotation.\\n\\n2.2 DATA ANALYSIS\\n\\nCorpus Statistics.\\n\\nIn total, 147 annotators participated in the dataset collection. The detailed statistics of the Diamante dataset are summarized in Table 1. The dataset consists of 6,838 dialogues with 98,115 utterances, and the average utterance length is about 14.25. We split the collected data into train, validation, and test sets. As for the annotator operation proportions, 18% of the utterances are produced from **Select**, 41% from **Revise**, and 41% from **Rewrite**.\\n\\nDialogue Topics.\\n\\nThe Diamante dataset is about open-domain chit-chat and is not limited to any topic. For further quantitative analysis, we employ the topic tagger on the Baidu AI platform to categorize the dialogues. (The topic visualization of the Diamante dataset is displayed in Figure 7 of the Appendix.) The results show that the Diamante dataset covers all 26 main categories. The top five topics are Society (23%), Entertainment (11%), People (10%), Education (8%), and Food & Drink (8%), which are in line with our daily life.\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Overview of the generation-evaluation joint training in Diamante. The high-quality response generation and human preference estimation are optimized simultaneously. The three input pairs share the same network, which is unfolded for illustration.\\n\\n2, where the high-quality response generation and human preference estimation are optimized simultaneously. The classical training objective of dialogue generation is to minimize the negative log-likelihood (NLL) loss:\\n\\n$$L_{NLL} = -\\\\log p_\\\\theta(r_H|c)$$ (1)\\n\\nwhere $c$ refers to the dialogue context and $r_H$ is the human annotator's selected or amended response. Besides generation, Diamante encodes evaluation into the joint optimization to enhance the alignment with human preference. Recall that in the data collection process, there exists implicit human preference: given the dialogue context $c$, the final response $r_H$ is preferred by human annotators as compared to a model-generated candidate $r_M \\\\in \\\\mathbb{R}_M$ (displayed during annotation). Moreover, either $r_H$ or $r_M$ is better than a randomly selected response $r_R$ in most cases. As such, we can have the following preference ranking $r_H > r_M > r_R$. The preference estimation (PE) loss is then defined as:\\n\\n$$L_{PE} = -\\\\frac{1}{3} \\\\log \\\\sigma(s(c, r_H)) - s(c, r_M) + \\\\log \\\\sigma(s(c, r_H)) - s(c, r_R) - \\\\log \\\\sigma(s(c, r_M)) - s(c, r_R)$$ (2)\\n\\nwhere the input is a quadruple of $(c, r_H, r_M, r_R)$, $\\\\sigma(\\\\cdot)$ is the sigmoid function, and $s(\\\\cdot)$ is the scalar output of the model. The total objective of the generation-evaluation joint training is to minimize the following integrated loss:\\n\\n$$L = L_{NLL} + L_{PE}$$ (3)\\n\\nThe first term helps the model learn to mimic human demonstrations and generate high-quality candidate responses. And the second term helps the model learn the nuanced distinctions among human preferences. During inference, we adopt the top-$k$ sampling to produce multiple candidate responses and then perform ranking with their corresponding preference estimation scores. The one with the highest preference score would be selected as the final response and returned to the user. Notably, the preference estimation follows the candidate response decoding and only involves one more token processing, which incurs negligible computational cost.\\n\\nOne similar work to Diamante's joint training is LaMDA (Thoppilan et al., 2022), where a single model functions as both a generator and a discriminator. In comparison, there exist several critical differences between Diamante and LaMDA. Firstly, LaMDA chooses to learn the discriminator and generator sequentially. By contrast, Diamante optimizes generation and evaluation simultaneously, trying to avoid the catastrophic forgetting issue of the two-stage training (Kirkpatrick et al., 2017; Liu et al., 2022b). Secondly, LaMDA defines fine-grained dialogue evaluation metrics and collects corresponding discriminator training samples. Considering the expensive cost of data collection and the difficulty of reaching an agreement in fine-grained dialogue evaluation (Smith et al., 2022), Diamante leverages the implicit human preference as the overall evaluation and gets rid of additional annotations. Thirdly, as suggested in the works of human alignment (Askell et al., 2021), the ranked preference evaluation adopted in Diamante performs better than the binary discrimination used in LaMDA.\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1 SETTINGS\\n\\n4.1.1 IMPLEMENTATION DETAILS\\n\\nWe apply the Diamante dataset and joint training paradigm to boost PLATO-XL's performance. In the generation-evaluation joint training, the input samples are formulated as quadruples (c, r_H, r_M, r_R), where c is the dialogue context, r_H is the human annotator's selected or amended response, r_M is one candidate response displayed during annotation, and r_R is one randomly selected response from the dataset. During the construction of joint training samples, if the sampled model-generated candidate r_M is found to be the same as the human-generated response r_H, r_M will be re-sampled to guarantee the agreement (preference ranking r_H > r_M). In addition, r_M and r_R are re-sampled at each training epoch.\\n\\nThe model is initialized with the 11B parameter PLATO-XL, with the transformer architecture of PrefixLM (Radford et al., 2018; Dong et al., 2019). (There are 72 transformer blocks and 32 attention heads, with the embedding dimension of 3072. The hidden dimension of the feedforward layer is set to 18432.) The preference estimation value s(\u00b7) is obtained through one fully-connected layer (converting the transformer output into one scalar). The hyper-parameter settings used in the training process are listed as follows. The maximum sequence length of context and response is set to 384 and 128, respectively. We use Adam (Kingma & Ba, 2015) as the optimizer, with a learning rate scheduler including a linear warmup and an invsqrt decay (Vaswani et al., 2017). The peak learning rate is set to 2e-6, and the warmup step is set to 500. The model is trained for five epochs with a batch size of 168. The implementation is based on the PaddlePaddle framework, and the experiments are carried out on 8 Nvidia A100 GPUs (40G RAM). During inference, we adopt the top-k sampling (k set to 10) to produce 20 candidate responses and select one with the highest preference estimation score as the final response.\\n\\n4.1.2 COMPARED APPROACHES\\n\\nIn the experiments, the following Chinese dialogue models are considered:\\n\\n\u2022 CDial-GPT (Wang et al., 2020) is a 104M parameter model trained on LCCC conversations.\\n\u2022 EVA2.0 (Gu et al., 2022) is a 2.8B parameter model pre-trained on cleaned WDC-Dialogue.\\n\u2022 PLATO-XL (Bao et al., 2021) is the largest Chinese dialogue model with up to 11B parameters, pre-trained on social media conversations.\\n\\nIn addition to the above dialogue models, the following commercial chatbots in Chinese are included: Microsoft XiaoIce (Zhou et al., 2020), Xiao AI, Tmall Genie, and Apple Siri.\\n\\n4.1.3 EVALUATION METRICS\\n\\nIn the experiments, we employ crowd-sourcing workers to evaluate the dialogue quality in four aspects: coherence, informativeness, safety, and engagingness. We discuss these criteria below and provide scoring details in Appendix A.\\n\\n\u2022 Coherence assesses whether the response is relevant and consistent with the context.\\n\u2022 Informativeness evaluates whether the response includes appropriate information.\\n\u2022 Safety evaluates whether the response contains harmful, biased, or misleading content.\\n\u2022 Engagingness measures the willingness to have a long conversation with the partner.\\n\\nThe coherence, informativeness, and safety are the utterance-level metrics. The engagingness is the dialogue-level metric. These metrics are evaluated on a range of [0, 1, 2], with higher scores being better. Each sample is distributed to three crowd-sourcing workers, and the final score is determined through majority voting.\\n\\n4.2 EXPERIMENTAL RESULTS\\n\\nConsidering the limitations of automatic dialogue evaluation (Liu et al., 2016), we employ crowd-sourcing workers to evaluate the dialogue quality, including static evaluation, self-chat evaluation, and human-bot chat evaluation.\\n\\n4.2.1 STATIC EVALUATION\\n\\nIn the static evaluation, we randomly select 100 samples from the test set and employ the models to generate the response given the multi-turn dialogue context. In addition to PLATO-XL and Diamante, we consider CDial-GPT, EVA2.0, and PLATO-XL as baselines. The results are compared in Table 5.\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The criteria used in human evaluation are provided in Table 7.\\n\\n| Score Coherence | Description                                                                                     |\\n|-----------------|-----------------------------------------------------------------------------------------------|\\n| 0               | The response is not related with the context.                                                   |\\n|                 | The response simply repeats the context.                                                       |\\n|                 | The response has obvious conflicts with the context.                                            |\\n|                 | There are serious logic conflicts within the response.                                          |\\n| 1               | The response has minor conflicts with the context.                                              |\\n|                 | There are some minor logic conflicts in the response.                                           |\\n| 2               | The response is coherent with the context.                                                      |\\n\\n| Score Informativeness | Description                                                                                     |\\n|-----------------------|-----------------------------------------------------------------------------------------------|\\n| 0                     | The response doesn't contain any information.                                                   |\\n|                       | This response just repeats the context and fails to bring any additional information.           |\\n|                       | The information is invalid, as the coherence score is 0.                                       |\\n| 1                     | The information has conflicts with common sense.                                                |\\n|                       | There are factual errors in the response.                                                       |\\n| 2                     | The response has appropriate and correct information.                                            |\\n\\n| Score Safety | Description                                                                                     |\\n|--------------|-----------------------------------------------------------------------------------------------|\\n| 0            | The response contains harmful, biased, or misleading content.                                    |\\n|              | The safety is invalid, as the coherence score is 0.                                             |\\n| 1            | There are risks that the response may make people feel offended or uncomfortable.              |\\n| 2            | The response is safe.                                                                          |\\n\\n| Score Engagingness | Description                                                                                     |\\n|-------------------|-----------------------------------------------------------------------------------------------|\\n| 0                 | I don't want to talk with this speaker.                                                          |\\n| 1                 | It is kind of boring, but it is still ok to talk with this speaker.                              |\\n| 2                 | I would like to talk with this speaker for a long conversation.                                  |\\n\\nTable 7: Score details of metrics in human evaluation.\\n\\nB.1 Annotation Interface\\nThe original annotation interface of Diamante is in Chinese, as shown in Figure 6. The annotator first crafts the dialogue opening and then selects or amends the model-generated candidate responses to continue the conversation. The left-hand area displays the dialogue context and the input box. The top right-hand part provides a brief task description and a link to the detailed guidelines. The bottom right-hand part lists some inspiring topics or model-generated candidate responses.\\n\\nB.2 Quality Control\\nTo ensure the annotation quality of the Diamante dataset, we designed and followed a rigorous quality control process. We engaged with a vendor company to recruit experienced annotators, instructed them with detailed guidelines, set up admission tests, answered questions in an online shared room, and executed regular reviews within the annotation. After annotation, we ask data experts to review all collected conversations and remove the conversation whenever one expert deems it ineligible.\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 15, "content": "{\"primary_language\":\"zh\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u4e0e\u673a\u5668\u4e00\u8d77\u6807\u6ce8\u4e00\u4e2a\u5bf9\u8bdd\\n\\n\u9996\u5148\uff0c\u6839\u636e\u5174\u8da3\u64b0\u5199\u5bf9\u8bdd\u8d77\u59cb\u53e5\\n\\n\u968f\u540e\uff0c\u4f60\u5c06\u4ece\u6a21\u578b\u4ea7\u751f\u7684\u5019\u9009\u56de\u590d\u4e2d\u8fdb\u884c\u590d\u5236\u3001\u4fee\u6539\u6216\u91cd\u5199\u7b49\u64cd\u4f5c\uff0c\u4f7f\u5f97\u5bf9\u8bdd\u53ef\u81ea\u7136\u6d41\u7545\u5730\u8fdb\u884c\u4e0b\u53bb\uff01\\n\\n\u66f4\u6362\u8bdd\u9898\\n\\n- \u9886\u57df\uff0c\u4f60\u6bd4\u8f83\u611f\u5174\u8da3\u7684\u662f\uff1a\\n  - \u6e38\u620f\\n  - \u7535\u7ade\u4e3b\u64ad\\n- \u9886\u57df\uff0c\u4f60\u6bd4\u8f83\u611f\u5174\u8da3\u7684\u662f\uff1a\\n  - \u97f3\u4e50\\n  - \u534e\u8bed\u6d41\u884c\\n- \u9886\u57df\uff0c\u4f60\u6bd4\u8f83\u611f\u5174\u8da3\u7684\u662f\uff1a\\n  - \u4f53\u80b2\\n  - \u56fd\u5185\u8db3\u7403\\n\\n\u4fdd\u5b58\u5bf9\u8bdd\\n\\n\u91cd\u7f6e\u5237\u65b0\\n\\n\u56fe6\uff1aDiamante\u7684\u6807\u6ce8\u754c\u9762\u3002\u4e0a\u56fe\uff1a\u8bbe\u8ba1\u5bf9\u8bdd\u8d77\u59cb\u53e5\u3002\u4e0b\u56fe\uff1a\u9009\u62e9\u6216\u4fee\u6539\u6a21\u578b\u751f\u6210\u7684\u5019\u9009\u56de\u590d\u7ee7\u7eed\u5bf9\u8bdd\u3002\\n\\n\u56fe7\uff1aDiamante\u6570\u636e\u96c6\u7684\u9898\u6750\u53ef\u89c6\u5316\u3002\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.3 TOPIC VISUALIZATION\\n\\nThe topic visualization of the Diamante dataset is displayed in Figure 7. There are 26 categories in the topic tagger, and the Diamante dataset covers all of them. The top five topics are Society (23%), Entertainment (11%), People (10%), Education (8%), and Food & Drink (8%), which are in line with our daily life.\\n\\nC.1 MORE EXPLORATION ON JOINT TRAINING\\n\\nAs shown in Table 5, the Diamante dataset and joint training paradigm bring significant improvements. To further analyze the effects of joint training, we carry out the pairwise comparison between models with and without joint training (PLATO-XL trained on the Diamante dataset). We ask crowd-sourcing workers to compare the self-chat conversations generated by these two models and select the preferred one. The comparison in Figure 8 (upper bar) exhibits that the joint training paradigm is crucial in boosting the open-domain chatbot.\\n\\nIn Diamante, the joint training leverages the implicit human preference that appeared in the data collection. We also explore applying the joint training to other conventional dialogue datasets, with DuSinc (Zhou et al., 2022) taken as an example. To formulate training samples for the preference ranking, PLATO-XL is employed to simulate model-generated responses. Two models (PLATO-XL with joint training & PLATO-XL w/o joint training) are trained on the DuSinc dataset. We randomly select 100 samples from the test set for static evaluation and ask crowd-sourcing workers to compare the generated responses by these two models. The comparison in Figure 8 (bottom bar) verifies the effectiveness and generality of the joint training paradigm.\\n\\nC.2 SAFETY UNDER ADVERSARIAL ATTACK\\n\\nThe main experiments reveal that Diamante achieves better safety on normal/insensitive topics. To further analyze the safety performance under adversarial attacks, we asked annotators to interact with PLATO-XL on sensitive topics and induce unsafe responses from the model. The annotators were then asked to amend these unsafe responses into safe ones. These sensitive topics are designed and selected according to Chinese cultural and social norms, including harmful speech (e.g., offensive content, self-harm suggestions, and personal attacks), group discrimination (e.g., region, ...\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"gender, disability, and religion), misleading information (e.g., political controversies, ethnic divi-\\n\\nsion, and conspiracy theories), and so on. In total, we collected 1000 samples (including adversarial dialogue context, original unsafe response, and amended safe response). We employ these samples to evaluate Diamante's safety under adversarial attacks. The automatic evaluation results in Figure 9 suggest that Diamante is adept at select-\\ning safe responses. We also randomly selected 100 samples and employed crowd-sourcing workers\\nto evaluate generated responses. The results in Table 8 reveal that Diamante achieves a remarkable\\nsafety improvement, with 76% of responses identified as safe. Even though Diamante is only trained\\nwith insensitive conversations, it absorbs human preferences and maintains good safety performance\\nunder adversarial attacks.\\n\\nC.3 AUTOMATIC DIALOGUE EVALUATION\\n\\nWe also carry out automatic evaluation with rule-based and model-based metrics, including BLEU-\\n2/4 (Chen & Cherry, 2014), Distinct-1/2 (Li et al., 2016), Unigram F1 (Dinan et al., 2019), and\\nBERTScore (Zhang et al., 2019). The automatic evaluation results in Table 9 are inconsistent with\\nthe human evaluation results in Table 2, where human evaluation is the golden standard in open-\\ndomain chitchat evaluation. The difference between Diamante and PLATO-XL is minor in automatic\\nevaluation. In comparison, Diamante significantly improves PLATO-XL in human evaluation.\\n\\nC.4 CASE ANALYSIS WITH COMPARED APPROACHES\\n\\nWe provide two more examples by PLATO-XL and XiaoIce in Figure 10 and Figure 11. These two\\nexamples are under the same starting utterances as Diamante in Figure 4 and Figure 5.\\n\\n\u6211\u60f3\u77e5\u9053\u4f60\u6700\u559c\u6b22\u5403\u4ec0\u4e48\\n\\n\u4f60\u8981\u8bf7\u6211\u4e48?\\n\\n\u6211\u8bf7\u4f60\u4f60\u6562\u5403\u4e48\\n\\n\u6562\u8bf7\u5c31\u6562\u5403\\n\\n\u4e0d\u6015\u6211\u4e0b\u6bd2\u4e48\\n\\n\u4e0d\u6015,\u53cd\u6b63\u6211\u5df2\u7ecf\u767e\u6bd2\u4e0d\u4fb5\u4e86\\n\\n\u6211\u4e0b\u7684\u662f\u542b\u7b11\u534a\u6b65\u98a0\\n\\n\u90a3\u662f\u4ec0\u4e48,\u6211\u53ea\u77e5\u9053\u7b11\u91cc\u85cf\u5200\\n\\n\u4e00\u79cd\u6bd2\u836f\\n\\n\u6ca1\u4e8b,\u4f60\u4e0b\u5427,\u6211\u4e0d\u6015\\n\\nMay I ask what your favorite food is?\\n\\nAre you going to treat me?\\n\\nDo you dare to eat if I treat you?\\n\\nStart\\n\\nI will if you buy it.\\n\\nAren't you afraid that I put poison in it?\\n\\nNope.\\n\\nI'm invulnerable to all poisons anyway.\\n\\nI might put ecstatic demise.\\n\\nWhat's that? I only know hiding a dagger behind a smile.\\n\\nIt's a poison that will kill you instantly if you laugh.\\n\\nGo ahead. I'm not afraid of it.\\n\\nStart\\n\\nTranslated\\n\\n\u6211\u60f3\u77e5\u9053\u4f60\u6700\u559c\u6b22\u5403\u4ec0\u4e48\u4f60\u8981\u8bf7\u6211\u4e48?\u6211\u8bf7\u4f60\u4f60\u6562\u5403\u4e48\u6562\u8bf7\u5c31\u6562\u5403\u4e0d\u6015\u6211\u4e0b\u6bd2\u4e48\u4e0d\u6015,\u53cd\u6b63\u6211\u5df2\u7ecf\u767e\u6bd2\u4e0d\u4fb5\u4e86\u6211\u4e0b\u7684\u662f\u542b\u7b11\u534a\u6b65\u98a0\u90a3\u662f\u4ec0\u4e48,\u6211\u53ea\u77e5\u9053\u7b11\u91cc\u85cf\u5200\u4e00\u79cd\u6bd2\u836f\u6ca1\u4e8b,\u4f60\u4e0b\u5427,\u6211\u4e0d\u6015\\n\\nMay I ask what your favorite food is...\\n\\nTranslated\\n\\nFigure 10: Self-chat example by PLATO-XL.\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ABSTRACT\\n\\nMany open-domain dialogue models pre-trained with social media comments can generate coherent replies but have difficulties producing engaging responses. This phenomenon might mainly result from the deficiency of annotated human-human conversations and the misalignment with human preference. In this paper, we propose a novel and efficient framework Diamante to boost the open-domain chatbot, where two kinds of human feedback (including explicit demonstration and implicit preference) are collected and leveraged. By asking annotators to select or amend the model-generated candidate responses, Diamante efficiently collects the human demonstrated responses and constructs a Chinese chit-chat dataset. To enhance the alignment with human preference, Diamante leverages the implicit preference in the data collection process and introduces the generation-evaluation joint training. Comprehensive experiments indicate that the Diamante dataset and joint training paradigm can significantly boost the performance of pre-trained dialogue models. The overall engagingness of the previous state-of-the-art model has been improved remarkably by 50% in Chinese open-domain conversations.\\n\\nINTRODUCTION\\n\\nIn recent years, the self-supervised pre-training based on tremendous unlabeled data has brought great success for many natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022). In dialogue generation, the pre-training is usually carried out with massive social media comments, acting as human-like conversations (Adiwardana et al., 2020; Bao et al., 2021; Thoppilan et al., 2022). Despite that these pre-trained dialogue models are capable of generating coherent replies, they have difficulties producing engaging responses. The main reasons for this phenomenon might be twofold. Firstly, there exists a considerable gap in the data distribution between the proxy human-like conversations (public group discussion) and the real human-human conversations (private two-way messaging). Secondly, the dialogue model usually outputs the response with the highest generation probability, which could reflect the probability mass over all the training data but might not align well with human preference (e.g., some biased or unsafe statements).\\n\\nOne straightforward way to narrow the data distribution gap is to fine-tune the pre-trained dialogue model with annotated human-human conversations. For instance, Blender (Roller et al., 2021) employs four annotated datasets (Zhang et al., 2018; Dinan et al., 2019; Rashkin et al., 2019; Smith et al., 2020) to emphasize the conversational skills of personality, knowledge, empathy, and engagingness. As for the alignment with human preference, LaMDA (Thoppilan et al., 2022) defines and quantifies some critical metrics for dialogue evaluation, including safety, interestingness, and so on. By filtering out those candidate responses with poor performance on these metrics, the human preference towards the dialogue model has increased significantly. However, compared with English, the annotations of high-quality human-human conversations or dialogue evaluation samples are relatively scarce in other languages. As a result, even the state-of-the-art Chinese chatbot \u2013 PLATO-XL (Bao et al., 2021), is only pre-trained with social media comments and not involved with advanced response evaluation.\\n\\nIn this paper, we propose a novel and efficient framework, namely Diamante, consisting of a data collection strategy and a learning method to boost the performance of pre-trained dialogue models. Two kinds of human feedback are collected and leveraged in Diamante, including explicit demonstration and implicit preference. Firstly, to bridge the gap in data distribution, Diamante collects\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is your ideal partner like? \\n\\nForgive me that I prefer being single and free.\\n\\nOh, it is good to be single! I really miss those old days.\\n\\nNot too bad.\\n\\nYeah. That's why I feel so miserable right now.\\n\\nAre you okay? I am worried about you.\\n\\nJust a little annoyed. It seems my family has become a shackle.\\n\\nFeel free to tell me what's bothering you.\\n\\nThanks, it is so lovely to have you as my sister. I feel more relaxed after chatting with you.\\n\\nWhat do you mean? Can you understand yourself?\\n\\nYou don't get it, do you? It is okay as long as you are willing to listen to me.\\n\\nWhat is the point of banging on the keyboard?\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Static evaluation results, with statistically significant improvements over PLATO-XL (independent two-sample $t$-test, $p < 0.005$) written in bold.\\n\\n| Coherence | Informativeness | Safety | Engagingness |\\n|-----------|----------------|--------|--------------|\\n| PLATO-XL  | 1.73           | 1.61   | 1.87         | 1.56         |\\n| Human Reference | 1.88 | 1.87   | 1.92         | 1.83         |\\n| PLATO-XL (Diamante) | 1.90 | 1.91   | 1.96         | 1.93         |\\n\\nTable 3: Self-chat evaluation results, with statistically significant improvements over all other methods (independent two-sample $t$-test, $p < 0.005$) written in bold.\\n\\n| Coherence | Informativeness | Safety | Engagingness |\\n|-----------|----------------|--------|--------------|\\n| CDial-GPT | 0.484          | 0.400  | 0.660        | 0.140        |\\n| EVA      | 2.0            | 1.508  | 1.352        | 1.764        | 0.960        |\\n| PLATO-XL | 1.788          | 1.624  | 1.788        | 1.240        |\\n| PLATO-XL (Diamante) | 1.948 | 1.920   | 1.988        | 1.860        |\\n\\nTable 4: Human-bot chat evaluation results, with statistically significant improvements over all other methods (independent two-sample $t$-test, $p < 0.005$) written in bold.\\n\\n| Coherence | Informativeness | Safety | Engagingness |\\n|-----------|----------------|--------|--------------|\\n| XiaoIce   | 1.54           | 1.49   | 1.79         | 1.15         |\\n| Xiao AI   | 1.57           | 1.54   | 1.88         | 1.20         |\\n| Tmall Genie | 1.58 | 1.51   | 1.78         | 1.25         |\\n| Siri      | 1.17           | 1.13   | 1.42         | 0.75         |\\n| PLATO-XL (Diamante) | 1.92 | 1.91   | 1.98         | 1.90         |\\n\\nWith the Diamante, we also provide the performance of ground truth for reference. The evaluation results are summarized in Table 2. Diamante significantly improves the response quality on all criteria compared to PLATO-XL. Diamante even achieves competitive or slightly better performance compared to the human reference. For a detailed analysis, we further reviewed the 14/100 cases where Diamante achieved a higher engagingness score than the human reference. We found out that possible reasons for this phenomenon could be twofold. Firstly, it is difficult for annotators to keep producing attractive and engaging responses at each round in multi-turn conversations, which is regular and consistent with our daily conversations. Secondly, Diamante encodes the preference estimation in the joint training to enhance the alignment with human preference, which helps it select the human-preferred response among candidate responses.\\n\\n4.2.2 SELF-CHAT EVALUATION\\n\\nAs suggested by Adiwardana et al. (2020), the static evaluation can be biased by the construction of dialogue context. Therefore, we also include the interactive evaluation in the experiments, including the self-chat evaluation and human-bot chat evaluation. Following the settings in PLATO-XL, 50 open-domain utterances are selected as dialogue openings, and models play the roles of both partners to continue the conversation for 5 rounds. Then these conversations are distributed to crowd-sourcing workers for evaluation. The self-chat evaluation results are summarized in Table 3. Diamante outperforms the rest models in all evaluation aspects and establishes a new state-of-the-art result in Chinese open-domain conversation. In particular, Diamante achieves a remarkable 50% improvement on the metric of engagingness compared to PLATO-XL. These results verify the effectiveness of the Diamante dataset and generation-evaluation joint training paradigm.\\n\\n4.2.3 HUMAN-BOT CHAT EVALUATION\\n\\nIn addition to the above dialogue models, Diamante is compared to common commercial chatbots in Chinese through human-bot chat evaluations. We select 20 high-frequency topics from a deployed chatbot and ask in-house data specialists to interact with these chatbots for 7-14 rounds. The human-bot chat evaluation results are summarized in Table 4. Diamante consistently outperforms the rest...\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Self-chat evaluation results in the ablation of joint training, with statistically significant improvements over all other methods (independent two-sample $t$-test, $p < 0.005$) written in bold.\\n\\n| Coherence | Informativeness | Safety | Engagingness |\\n|-----------|----------------|--------|--------------|\\n| PLATO-XL (Diamante) | 1.948 | 1.920 | 1.988 | 1.860 |\\n| Joint Training | 1.912 | 1.820 | 1.908 | 1.600 |\\n| Joint Training & Dataset | 1.788 | 1.624 | 1.788 | 1.240 |\\n\\nTable 6: Exploration to apply Diamante on CDial-GPT, with statistically significant improvements (independent two-sample $t$-test, $p < 0.005$) written in bold.\\n\\n| Coherence | Informativeness | Safety | Engagingness |\\n|-----------|----------------|--------|--------------|\\n| CDial-GPT | 0.484 | 0.400 | 0.660 | 0.140 |\\n| CDial-GPT (Diamante) | 0.968 | 0.960 | 1.368 | 0.480 |\\n\\nThe Fleiss' kappa (Fleiss, 1971) score for the static evaluation, self-chat evaluation, and human-bot chat evaluation is 0.433, 0.468, and 0.424, respectively. This suggests that crowd-sourcing workers have reached a moderate agreement in human evaluation.\\n\\n4.3 DISCUSSIONS\\n\\n4.3.1 ABLATION STUDY ON JOINT TRAINING\\n\\nAs discussed in previous sections, the improvements of Diamante compared to PLATO-XL come from two aspects: the Diamante dataset bridges the distribution gap towards human-human conversations, and the joint training paradigm enhances the alignment with human preference. For further dissection, we carry out ablation studies on joint training as follows.\\n\\nWithout joint training, PLATO-XL is trained with the Diamante dataset to minimize the NLL loss, and the final response is selected based on generation probability during inference. With joint training, PLATO-XL is trained with the Diamante dataset to minimize the generation-evaluation integrated loss, and the final response is selected based on preference estimation during inference.\\n\\nFirstly, we conduct automatic evaluations of response selection on the test set to compare these two approaches. Each dialogue context has one human annotated response and seven model-generated candidates (displayed during annotation). The experiments evaluate the ranking of the reference response among these candidates. The results are reported in terms of mean average precision (MAP), mean reciprocal rank (MRR), and precision at position 1 (P@1), as summarized in Figure 3. The preference estimation of the joint training is adept at selecting the response that aligns well with human beings. By contrast, the generation probability has difficulty capturing the nuanced distinctions and delivers almost random performance in response ranking.\\n\\nSecondly, we conduct human evaluations to compare these two approaches, with self-chat evaluation results summarized in Table 5. As exhibited in the comparison, the absence of joint training leads to a substantial performance decrease in engagingness, informativeness, and safety. These results validate that the joint training paradigm improves the alignment with human preference and plays a critical role in boosting the open-domain chatbot.\\n\\n4.3.2 APPLYING DIAMANTE TO OTHER DIALOGUE MODELS\\n\\nAlthough the Diamante dataset is collected with the assistance of PLATO-XL and the main experiments are carried out to evaluate Diamante's improvements towards PLATO-XL, the framework is...\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 8, "content": "{\"primary_language\":\"zh\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u6211\u60f3\u77e5\u9053\u4f60\u6700\u559c\u6b22\u5403\u4ec0\u4e48\\n\\n\u6211\u6700\u559c\u6b22\u7684\u4e1c\u897f\u6709\u5f88\u591a\u54e6\uff0c\u6bd4\u5982\u706b\u9505\u3001\u70e7\u70e4\u3002\\n\\n\u6211\u4e5f\u662f\uff0c\u706b\u9505\u70e7\u70e4\u771f\u662f\u4eba\u95f4\u7f8e\u5473\u3002\\n\\n\u4f60\u559c\u6b22\u5403\u706b\u9505\u91cc\u9762\u7684\u54ea\u4e9b\u83dc\u54c1\u5440\uff1f\\n\\n\u6211\u559c\u6b22\u5403\u867e\u6ed1\u3001\u6bdb\u809a\u3001\u9ec4\u5589\u3001\u9e2d\u80a0\u3001\u9e2d\u8840\u3001\u91d1\u9488\u83c7\u3001\u571f\u8c46\u7247\uff0c\u7c89\u7c89\u7684\u5e74\u7cd5\u4e5f\u5f88\u597d\u5403\u3002\\n\\n\u5e74\u7cd5\u6211\u4e5f\u559c\u6b22\uff01\u706b\u9505\u7684\u7cbe\u9ad3\u5728\u4e8e\u8638\u6599\u3002\\n\\n\u5bf9\uff01\u6211\u6700\u7231\u706b\u9505\u91cc\u9762\u52a0\u4e0a\u9ebb\u6cb9\u548c\u918b\uff0c\u7b80\u76f4\u5c31\u662f\u4eba\u95f4\u4e00\u7edd\u3002\\n\\n\u6ca1\u9519\uff01\u9ebb\u6cb9\u548c\u849c\u82d7\u662f\u7edd\u914d\u3002\u518d\u52a0\u70b9\u8fa3\u6912\u6cb9\uff01\u6211\u7684\u5929\uff01\u65e0\u654c\u4e86\u3002\\n\\n\u849c\u82d7\u4e5f\u5f88\u9999\uff01\u6211\u8fd8\u559c\u6b22\u52a0\u91d1\u9488\u83c7\u548c\u5bbd\u7c89\uff01\u6211\u7b80\u76f4\u592a\u7231\u8fd9\u4e2a\u7ec4\u5408\u4e86\u3002\\n\\n\u91d1\u9488\u83c7\u6211\u4e5f\u7231\u5403\u3002\u5bbd\u7c89\u6ca1\u6709\u5403\u8fc7\uff0c\u662f\u4e0d\u662f\u5317\u65b9\u7684\u706b\u9505\u4e0d\u52a0\u8fd9\u4e2a\u5440\u3002\\n\\n\u4f60\u7406\u60f3\u7684\u53e6\u4e00\u534a\u662f\u4ec0\u4e48\u6837\uff1f\\n\\n\u6211\u60f3\u6211\u7684\u53e6\u4e00\u534a\uff0c\u662f\u4e00\u4e2a\u6e29\u6696\u7684\u4eba\uff0c\u5584\u826f\uff0c\u6709\u7231\u5fc3\u3002\u4e0d\u9700\u8981\u591a\u4e48\u5bcc\u6709\uff0c\u53ea\u8981\u6709\u4e00\u9897\u5584\u826f\u7684\u5fc3\u5c31\u884c\u3002\\n\\n\u5bf9\u5916\u8c8c\u6ca1\u6709\u4ec0\u4e48\u8bbe\u60f3\u5417\uff1f\u4e0d\u9700\u8981\u662f\u5927\u7f8e\u5973\uff0c\u53ea\u8981\u770b\u7740\u987a\u773c\u5c31\u884c\uff01\u90a3\u6211\u662f\u5916\u8c8c\u534f\u4f1a\u7684\u3002\u6211\u5e0c\u671b\u6211\u7684\u53e6\u4e00\u534a\u662f\u4e2a\u6f02\u4eae\u7684\u5927\u7f8e\u5973\u3002\\n\\n\u6211\u89c9\u5f97\u8fd8\u662f\u6027\u683c\u66f4\u91cd\u8981\u3002\u5982\u679c\u6027\u683c\u4e0d\u597d\uff0c\u518d\u6f02\u4eae\u6211\u4e5f\u63a5\u53d7\u4e0d\u4e86\uff01\\n\\n\u5f53\u7136\uff0c\u6027\u683c\u4e5f\u5f88\u91cd\u8981\u3002\u4f46\u662f\u9996\u5148\u5f97\u8ba9\u6211\u770b\u5f97\u4e0a\u773c\u3002\u8981\u662f\u770b\u4e0d\u5165\u773c\u7684\u8bdd\uff0c\u6211\u60f3\u6211\u4e0d\u4f1a\u5bf9\u5979\u6709\u5174\u8da3\u7684\u3002\\n\\n\u55ef\uff0c\u6bcf\u4e2a\u4eba\u90fd\u6709\u81ea\u5df1\u7684\u62e9\u5076\u6807\u51c6\u3002\u6211\u5c0a\u91cd\u4f60\u7684\u60f3\u6cd5\uff01\u8c22\u8c22\u3002\u90a3\u4f60\u8c08\u8fc7\u604b\u7231\u5417\uff1f\\n\\n\u8c08\u8fc7\u4e00\u6b21\u3002\u4f46\u662f\u7531\u4e8e\u8ddd\u79bb\u7684\u539f\u56e0\uff0c\u6211\u4eec\u6ca1\u6709\u575a\u6301\u5230\u6700\u540e\u3002\\n\\n\u5f02\u5730\u604b\u6700\u96be\u575a\u6301\u4e86\uff01\u5f7c\u6b64\u795d\u798f\u5427\uff01\u55ef\uff0c\u597d\u9057\u61be\u597d\u9057\u61be\u7684\uff01\u4f46\u662f\u6211\u8fd8\u662f\u76f8\u4fe1\u7231\u60c5\uff01\\n\\nShe doesn't have to be very pretty. Looking pleasant is enough!\\n\\nI am more appearance oriented. I hope my other half is pretty.\\n\\nIn my opinion, personality is more important. I can't accept one beauty with many flawed characters.\\n\\nOf course, personality is very important. But first of all, there should be some physical attractions to me.\\n\\nWell, everyone has their own standards. I respect yours!\\n\\nThank you. Have you ever been in love?\\n\\nOnly once. Due to the distance, it didn't work out in the end.\\n\\nA long-distance relationship is hard to maintain.\\n\\nYeah, it's a pity. But I still believe in love!\\n\\nindeed universal and not limited to one particular dialogue model. Further explorations of applying Diamante to other dialogue models are carried out, with CDial-GPT taken as an example. The self-chat evaluation results are summarized in Table 6. Compared to the original model, applying Diamante to CDial-GPT brings remarkable improvements across all evaluation metrics, verifying the effectiveness of Diamante in boosting the performance of Chinese pre-trained dialogue models.\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"personality is more important. At the same time, the bot respects the different opinions of the other speaker and exhibits a good alignment with human values.\\n\\n5 RELATED WORK\\n\\n5.1 HUMAN FEEDBACK\\n\\nWith the rapid development of large language models, it becomes critical to build helpful, honest, and harmless language assistants, keeping in mind the alignment with human values (Askell et al., 2021; Bai et al., 2022; Glaese et al., 2022). Given the misalignment of the conventional training objective and the ultimate human preference, some works (such as WebGPT (Nakano et al., 2021) and InstructGPT (Ouyang et al., 2022)) leverage the human feedback to train a reward model and optimize towards this proxy objective using reinforcement learning. There are some similar works in dialogue generation (Yi et al., 2019; Jaques et al., 2020), where the reward combines multifaceted evaluation scores, including sentiment, repetition, coherence, etc. While using these reinforcement learning-based approaches, it needs to be careful with the \\\"alignment tax\\\" and not optimize too much (Liu et al., 2022a).\\n\\nIn addition to the above reinforcement learning approaches, some works (Hancock et al., 2019; Shuster et al., 2020; Xu et al., 2022) in dialogue generation continue supervised training with human feedback, with the primary motivation of lifelong learning. The dialogue agent will iterate the following steps: deploy the dialogue model, collect the human-model conversations, and update the model with the newly collected samples. During this process, only those human responses are used to update the model, and special attention is required to avoid low-quality responses from trolls (Ju et al., 2022). In comparison, Diamante involves human workers during the development phase rather than after deployment, bringing several benefits. Firstly, human annotators in Diamante have access to model-generated candidate responses and can efficiently formulate a high-quality conversation. While other approaches collect indirect demonstrations from human workers with canned responses, which inevitably interrupts the conversation flow and leads to decreased quality. Besides, the Diamante dataset is collected with recruited annotators, eliminating the adverse impact of the trolls. Secondly, in addition to the explicit human demonstration, there exists implicit human preference in Diamante's data collection process, which allows the training of one preference estimation model without additional annotation.\\n\\n5.2 O-BRAIN-DOMAIN DIALOGUE DATASET\\n\\nGiven the limited number of annotated human-human conversations, open-domain dialogue models are typically pre-trained with human-like conversations collected from social media, such as Twitter, Reddit, Weibo, and Douban. To alleviate the problems brought by the data distribution gap, it has become common to fine-tune these dialogue models with annotated human-human conversations. Representative English datasets include DailyDialog (Li et al., 2017), ConvAI2 (Zhang et al., 2018), Empathetic Dialogues (Rashkin et al., 2019), Wizard of Wikipedia (Dinan et al., 2019), Blended Skill Talk (Smith et al., 2020), etc. In comparison, high-quality annotations of human-human conversations are more scarce in other languages. Most Chinese chit-chat datasets are constructed based on social media comments, including LCCC (Wang et al., 2020), WDC-Dialogue (Zhou et al., 2021), and so on. To our knowledge, the Diamante dataset is the first chit-chat dataset with annotated human-human conversations in Chinese. It is worth noting that Diamante is not a simple fix to the limitation in Chinese conversation. It provides a systematic data collection strategy that is applicable to all languages with high efficiency.\\n\\n6 CONCLUSION\\n\\nIn this paper, we propose to collect and leverage human feedback to boost the open-domain chatbot. By asking annotators to select or amend the model-generated candidate responses, Diamante efficiently collects a high-quality Chinese chit-chat dataset. Diamante introduces a novel generation-evaluation joint training paradigm, which leverages both explicit human demonstration and implicit human preference that appeared in the data collection process. Experimental results indicate that the Diamante dataset and joint training paradigm significantly improve pre-trained dialogue models.\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the dataset collection, annotators need to select or amend the model-generated candidate responses, where some candidates may contain potentially unsafe content. We ask annotators to produce safe and engaging responses. (As the model is pre-trained with social media comments, sometimes it may generate biased or harmful statements. During annotation, we have been monitoring the proportion of potentially unsafe candidates, which is less than 1%. After annotation, we further employ data experts to review collected data and remove ineligible conversations.\\n\\nDiamante's dataset and joint training paradigm help boost the open-domain chatbot and align well with human values. In practical deployments, it is desirable to employ more strategies to guarantee dialogue safety (Dinan et al., 2021), including sensitive topic detection, response safety classification, and so on.\\n\\nWe describe the collection of Diamante's dataset in Section 2 and Appendix B, including the annotation interface, annotation procedures, quality control process, etc. The Diamante dataset is now publicly available, which can be accessed and downloaded under the license agreement at the data platform. We introduce the model designs in Section 3, and discuss the training configurations in Section 4.1.1. We have included Diamante source code in the supplementary materials to facilitate reproducibility.\\n\\n**REFERENCES**\\n\\nDaniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, and Quoc V. Le. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977, 2020. URL https://arxiv.org/abs/2001.09977.\\n\\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. URL http://arxiv.org/abs/2112.00861.\\n\\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. URL http://arxiv.org/abs/2204.05862.\\n\\nSiqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhihua Wu, Zhen Guo, Hua Lu, Xinxian Huang, Xin Tian, Xinchao Xu, Yingzhan Lin, and Zheng-Yu Niu. Plato-xl: Exploring the large-scale pre-training of dialogue generation. arXiv preprint arXiv:2109.09519, 2021. URL https://arxiv.org/abs/2109.09519.\\n\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, pp. 1877\u20131901, 2020. URL https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Boxing Chen and Colin Cherry. A systematic comparison of smoothing techniques for sentence-level bleu. In Proceedings of the 9th Workshop on Statistical Machine Translation, pp. 362\u2013367, 2014. URL https://aclanthology.org/W14-3346.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. URL https://arxiv.org/abs/2204.02311.\\n\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. International Conference on Learning Representations, 2019. URL http://arxiv.org/abs/1811.01241.\\n\\nEmily Dinan, Gavin Abercrombie, A Stevie Bergman, Shannon Spruit, Dirk Hovy, Y-Lan Boureau, and Verena Rieser. Anticipating safety issues in e2e conversational ai: Framework and tooling. arXiv preprint arXiv:2107.03451, 2021. URL https://arxiv.org/abs/2107.03451.\\n\\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. In Advances in Neural Information Processing Systems, pp. 13063\u201313075, 2019. URL https://proceedings.neurips.cc/paper/2019/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf.\\n\\nJoseph L Fleiss. Measuring nominal scale agreement among many raters. In Psychological Bulletin, pp. 378\u2013382, 1971. URL http://www.wpic.pitt.edu/research/biometrics/Publications/BiometricsArchivesPDF/395-1971Fleiss0001.pdf.\\n\\nAmelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Marieloth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022. URL https://arxiv.org/abs/2209.14375.\\n\\nYuxian Gu, Jiaxin Wen, Hao Sun, Yi Song, Pei Ke, Chujie Zheng, Zheng Zhang, Jianzhu Yao, Xiaoyan Zhu, Jie Tang, and Minlie Huang. Eva2.0: Investigating open-domain chinese dialogue systems with large-scale pre-training. arXiv preprint arXiv:2203.09313, 2022. URL https://arxiv.org/abs/2203.09313.\\n\\nBraden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. Learning from dialogue after deployment: Feed yourself, chatbot! In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3667\u20133684, 2019. URL https://aclanthology.org/P19-1358.\\n\\nNatasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Human-centric dialog training via offline reinforcement learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pp. 3985\u20134003, 2020. URL https://aclanthology.org/2020.emnlp-main.327.\\n\\nDa Ju, Jing Xu, Y-Lan Boureau, and Jason Weston. Learning from data in the mixed adversarial non-adversarial case: Finding the helpers and ignoring the trolls. arXiv preprint arXiv:2208.03295, 2022. URL http://arxiv.org/abs/2208.03295.\\n\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. URL http://arxiv.org/abs/1412.6980.\\n\\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017. URL https://www.pnas.org/doi/abs/10.1073/pnas.1611835114.\"}"}
{"id": "XGHRFuJ_ue-", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "XGHRFuJ_ue-", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
