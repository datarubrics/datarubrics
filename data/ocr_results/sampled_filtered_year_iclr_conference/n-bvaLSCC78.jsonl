{"id": "n-bvaLSCC78", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Visualization of generated B\u00e9zier Curves and original learning curves (Ground Truth).\\n\\nTable 6: Details of the machines used to collect energy consumption\\n\\n| Property Name | Value |\\n|---------------|-------|\\n| CPU           | Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz 2600 MHz |\\n| Memory-GB     | 112   |\\n| Operation system | Linux Ubuntu 20.04 LTS |\\n| Hard drive-GB | 1000  |\\n| GPU           | Nvidia Tesla V100 with 32 GB memory |\\n\\nB.4 ABLATION STUDY ON SURROGATE MODEL\\n\\nThe effect of Degree\\n\\nIn Table 7, we show the effect of different degrees of the B\u00e9zier curve on the prediction performance. The higher the degree, the better the B\u00e9zier curve fits the real learning curve, but it also leads to overfitting. As the degree increases, the prediction performance becomes worse instead. When the Degree is 4, the Quadratic B\u00e9zier curve achieves the best results.\\n\\nB\u00e9zier curves vs. Polynomial functions\\n\\nCompared to general n-order polynomial functions, the coefficients of the B\u00e9zier Curve are explainable and have real-world semantics (i.e. the control points that define the curvature). As a result, we can leverage the prior knowledge of the learning curve by adding constraints to the control points and fitting a better learning curve. For example, in our implementation, we constrained the starting and ending points of the learning curve to make the accuracy value stay within the \\\\([0, 1]\\\\) range.\\n\\nEmpirically, we conduct an ablation study in which instead of predicting the B\u00e9zier Curve control points, directly predicts the coefficients and intercept of polynomial functions. However, we observe that for polynomial functions of higher order (n=4), the model is almost impossible to fit. The possible reason is that the scales of the parameters differ too much, and the magnitude of the coefficients varies widely, making it difficult to learn the model. When we set n to 2, the results are as shown in Table 8. In contrast, regardless of the order of B\u00e9zier's curve, the size of the control points is basically in the same order of magnitude and the model can be easily fitted (as shown in Table 7).\\n\\nB.5 API ON HOW TO USE EA-HAS-BENCH\\n\\nHere is an example of how to use EA-HAS-Bench:\\n\\n```python\\ndef get_ea_has_bench_api(dataset):\\n    full_api = {}\\n    # load the ea-nas-bench surrogate models\\n    if dataset=='cifar10':\\n        ea_has_bench_model = load_ensemble('checkpoints/ea_has_bench-v0.2')\\n        train_energy_model = load_ensemble('checkpoints/ea_has_bench-trainE-v0.2')\\n        test_energy_model = load_ensemble('checkpoints/ea_has_bench-testE-v0.1')\\n```\"}"}
{"id": "n-bvaLSCC78", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: The predicted performance of different degrees of the B\u00e9zier Curve on CIFAR-10\\n\\n| Method                | Avg R2 | Final R2 | Avg KT | Final KT |\\n|-----------------------|--------|----------|--------|----------|\\n| Cubic B\u00e9zier (Degree = 3) | 0.870  | 0.859    | 0.858  | 0.855    |\\n| Quadratic B\u00e9zier (Degree = 4) | 0.892  | 0.872    | 0.860  | 0.841    |\\n| Quintic B\u00e9zier (Degree = 5) | 0.891  | 0.862    | 0.855  | 0.834    |\\n| Sextic B\u00e9zier (Degree = 6) | 0.852  | 0.796    | 0.771  | 0.736    |\\n\\nTable 8: The polynomial function model on CIFAR10\\n\\n| Degree | Avg.R2 | Avg.KT | Final.R2 | Final.KT |\\n|--------|--------|--------|----------|----------|\\n| n=2    | 0.0437 | 0.547  | -2.66    | 0.182    |\\n\\nListing 3: EA-HAS-Bench API\\n\\n```python\\n# output the learning curve, train time, TEC and IEC\\nlc = ea_api['ea_has_bench_model'][0].predict(config=arch_str)\\ntrain_time = ea_api['ea_has_bench_model'][1].predict(config=arch_str)\\ntrain_cost = ea_api['ea_has_bench_model'][2].predict(config=arch_str)\\ntest_cost = ea_api['ea_has_bench_model'][3].predict(config=arch_str)\\n```\\n\\nC.1 Core Analysis on Impact of Hyperparameter Configurations\\n\\nTable 13 compares how different hyper-parameter configurations affect the search space quality on both CIFAR10 and TinyImageNet datasets. Besides the learning rate, optimizer, and the number of total training epochs discussed in the main paper, here we further examine the influence of learning rate policy and data augmentation. For the investigation of data augmentation, we found that cutout achieves opposite effects on different datasets.\\n\\nC.2 Comparing with Original RegNet Pace\\n\\nAs shown in the figure 8, we compare the learning curves of one hundred samples actually measured from under two search spaces - RegNet and RegNet + HPO. The different architectures converge effectively under well-designed training hyperparameters. However, many architectures fail to converge under the search space of RegNet + HPO. But the latter may find a better combination to produce better performance. With a fixed training hyperparameter, the best model obtained by searching only in the network architecture space is not necessarily the true best model. At these fixed training hyperparameters, the sub-optimal model obtained by searching may get better performance under another set of training hyperparameters. In addition, the optimal hyperparameters are difficult...\"}"}
{"id": "n-bvaLSCC78", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: Comparing the learning curve under RegNet and RegNet + HPO search spaces. (a) The RegNet search space. (b) The RegNet + HPO search space.\\n\\nC.3.1 Analysis on Energy Cost\\n\\nPower Distribution\\n\\nThe correlation between energy consumption and runtime is $\\\\text{energy} = \\\\text{runtime} \\\\times \\\\text{Avg.power}$. As shown in Figure 9, since the power of different models trained on Tesla V100 is not constant, the energy consumption is not linearly related to the runtime.\\n\\nFigure 9: Visualization of distributions of power on CIFAR10 (left) and TinyImageNet (right).\"}"}
{"id": "n-bvaLSCC78", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Correlation Between Training Energy Cost and FLOPs. Although HW-NAS-Bench has discussed the relationship between FLOPs and energy in detail on six different hardware devices and concluded that the theoretical hardware consumption does not always correlate with hardware consumption in both search spaces, we still analyze the correlation between the two in our search space. The Kendall Rank Correlation Coefficient between FLOPs and TEC on TinyImageNet is 0.396 which is less than 0.5. As shown in Figure 10, we observe that the distribution of scattered points is triangular rather than linear. Based on the qualitative and quantitative results, the correlation between FLOPs and TEC is not strong.\\n\\nOverall Energy Consumption of Sampled Configurations\\n\\nThe total energy consumption to build EA-HAS-Bench in Table 9.\\n\\n| Dataset | Training & Validation | Testing | Total       |\\n|---------|-----------------------|---------|-------------|\\n| CIFAR10 | 660,313 kWh           | 46,813  | 707,126 kWh |\\n| TinyImageNet | 1,715,985 kWh     | 124,088 | 1,840,074 kWh |\\n\\n| Total   |\\n|---------|\\n| 2,547,200 kWh |\\n\\nFigure 10: FLOPs vs. training energy consumption (TEC) on TinyImageNet.\\n\\nD.1 Introduction on Compared NAS/HPO Methods\\n\\nAll algorithms we use in Section 5 are based on NASLib (Ruchte et al., 2020). For regularized evolution and local search, we modify them to suit our joint search space. The algorithm description and implementation details are as follows.\\n\\n\u2022 Random Search is a classic baseline of HPO algorithms and is the basis for some complex algorithms (Li & Talwalkar, 2019). For EA-HAS-Bench, we sample randomly both the architecture and the hyperparameter space. The sampling type is the same as the dataset that we build for training the surrogate model.\\n\\n\u2022 Local search iteratively evaluates all architectures in the neighborhood of the current best architecture found so far (White et al., 2020). For the search space of EA-HAS-Bench, we define neighborhood as having the same network architecture or the same training hyperparameters, i.e., points with the same subspace are neighboring points.\\n\\n\u2022 Regularized evolution mutates the best architecture from a sample of all architectures evaluated so far. For EA-HAS-Bench, we define a mutation as a random change of one dimension in the architecture or hyperparameters.\"}"}
{"id": "n-bvaLSCC78", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nThe energy consumption for training deep learning models is increasing at an alarming rate due to the growth of training data and model scale, resulting in a negative impact on carbon neutrality. Energy consumption is an especially pressing issue for AutoML algorithms because it usually requires repeatedly training large numbers of computationally intensive deep models to search for optimal configurations. This paper takes one of the most essential steps in developing energy-aware (EA) NAS methods, by providing a benchmark that makes EA-NAS research more reproducible and accessible. Specifically, we present the first large-scale energy-aware benchmark that allows studying AutoML methods to achieve better trade-offs between performance and search energy consumption, named EA-HAS-Bench. EA-HAS-Bench provides a large-scale architecture/hyperparameter joint search space, covering diversified configurations related to energy consumption. Furthermore, we propose a novel surrogate model specially designed for large joint search space, which proposes a B\u00e9zier curve-based model to predict learning curves with unlimited shape and length. Based on the proposed dataset, we modify existing AutoML algorithms to consider the search energy consumption, and our experiments show that the modified energy-aware AutoML methods achieve a better trade-off between energy consumption and model performance.\\n\\nIntroduction\\n\\nAs deep learning technology progresses rapidly, its alarming increased rate of energy consumption causes growing concerns (Schwartz et al., 2020; Li et al., 2021a; Strubell et al., 2019). Neural architecture search (NAS) (Elsken et al., 2019), hyperparameter optimization (HPO) (Feurer & Hutter, 2019) lifted the manual effort of neural architecture and hyperparameter tuning but require repeatedly training large numbers of computationally intensive deep models, leading to significant energy consumption and carbon emissions. For instance, training 10K models on CIFAR-10 (Krizhevsky et al., 2009) for 100 epochs consume about 500,000 kWh of energy power, which is equivalent to the annual electricity consumption of about 600 households in China.\\n\\nAs a result, it is essential to develop search energy cost aware (EA) AutoML methods, which are able to find models with good performance while minimizing the overall energy consumption throughout the search process. However, existing NAS studies mainly focus on the resource cost of the searched deep model, such as parameter size, the number of float-point operations (FLOPS), or latency (Tan et al., 2019; Wu et al., 2019; He et al., 2021). Exploiting the trade-off between model performance and energy cost during the searching process has been rarely studied (Elsken et al., 2019). In this paper, we propose to take one of the most essential steps in developing energy-aware (EA) NAS methods that make EA-NAS research more reproducible and accessible. Specifically, we provide a benchmark for EA-NAS called Energy Aware Hyperparameter and Architecture Search Benchmark (EA-HAS-Bench), where the researchers can easily obtain the training energy cost and model performance of a certain architecture and hyperparameter configuration, without actually training the model. In order to support developing energy-aware HPO and NAS methods, the proposed EA-HAS-Bench should satisfy three requirements.\\n\\n* Work done during an internship in Microsoft Research Asia. Email: dousg@tongji.edu.cn.\\n\u2020 Corresponding authors. Email: zhaocairong@tongji.edu.cn, xinyangjiang@microsoft.com\"}"}
{"id": "n-bvaLSCC78", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Overview of NAS benchmarks related to EA-HAS-Bench\\n\\n| Benchmark          | Size   | Type    | Arch. Type | Learning Controls (LCs) | Query Type                                      |\\n|--------------------|--------|---------|------------|-------------------------|------------------------------------------------|\\n| NAS-Bench-101      | 423k   | Tabel.  | Cell Accuracy | \u2713                        |\\n| NAS-Bench-201      | 6k     | Tabel.  | Cell Accuracy & Loss | \u2713                        |\\n| NAS-Bench-301      | 10     | Surr.   | Cell Accuracy | \u2713                        |\\n| NATS-Bench         | 32K    | Tabel.  | Macro Accuracy | \u2713                        |\\n| HW-NAS-Bench       | 15K/10 | Tabel./Esti. | Cell Latency & Inference Energy | \u2713            |\\n| NAS-Bench-x11      | 423k   | Surr.   | Cell Accuracy | \u2713                        |\\n|                   | 6k     | Surr.   | Cell Accuracy & Loss | \u2713                        |\\n|                   | 10     | Surr.   | Cell Accuracy | \u2713                        |\\n| NAS-HPO-Bench      | 62K    | Tabel.  | MLP Accuracy | \u2713                        |\\n| NAS-HPO-Bench-II   | 192K   | Surr.   | Cell Accuracy | \u2713                        |\\n| EA-HAS-Bench (Ours)| 3\u00d710   | Surr.   | Macro Accuracy | \u2713                        |\\n\\nSearch Energy Cost\\n\\nOur dataset needs to provide the total search energy cost of running a specific AutoML method. This can be obtained by measuring the energy cost of each particular configuration the method traverses and summing them up. As shown in Table 1, most of the existing conventional benchmarks (Ying et al., 2019; Dong & Yang, 2020; Siems et al., 2020) like NAS-Bench-101 do not directly provide training energy cost but use model training time as the training resource budget, which as verified by our experiments, is an inaccurate estimation of energy cost. HW-NAS-bench (Li et al., 2021b) provides the inference latency and inference energy consumption of different model architectures but also does not provide the search energy cost.\\n\\nEnergy Related Joint Search Space\\n\\nThe search space of EA-HAS-Bench should include the configurations that affect both the model training energy cost and performance. Since both model architectures (e.g., scales, width, depth) and training hyperparameters (e.g., number of epochs) are correlated to the training energy cost, designing a joint hyperparameter and architecture search (HAS) search space is essential. Most NAS benchmarks use a single fixed training hyperparameter configuration for all architectures. Existing HAS benchmarks (Klein & Hutter, 2019; Hirose et al., 2021) use small toy search space which does not cover enough critical factors affecting the search energy cost. As a result, EA-HAS-Bench provides the first dataset with a ten-billion-level architecture/hyperparameter joint space, covering diverse types of configurations related to search energy cost.\\n\\nSurrogate Model for Joint Search Space\\n\\nDue to the enormous size of the proposed joint search space, a surrogate model is needed to fill out the entire search space with a subset of sampled configurations. Existing surrogate methods (Zela et al., 2022; Yan et al., 2021) are not applicable to our proposed large-scale joint space. For instance, those methods do not consider the situation of maximum epoch variation and predict only a fixed-length learning curve or final performance. Thus, we propose the B\u00b4ezier Curve-based Surrogate (BCS) model to fit accuracy learning curves of unlimited shape and length.\\n\\nWe summarize the contribution of this paper as follows:\\n\\n\u2022 EA-HAS-Bench is the first HAS dataset aware of the overall search energy cost. Based on this dataset, we further propose an energy-aware AutoML method with search energy cost related penalties, showing energy-aware AutoML achieves a better trade-off between model performance and search energy cost.\\n\\n\u2022 We provide the first large-scale benchmark containing an architecture/hyperparameter joint search space with over 10 billion configurations, covering various configurations related to search energy cost.\\n\\n\u2022 We develop a novel surrogate model suitable for more general and complex joint HAS search space, which outperforms NAS-Bench-X11 and other parametric learning curve-based methods.\\n\\nThe dataset and codebase of EA-HAS-Bench are available at https://github.com/microsoft/EA-HAS-Bench.\"}"}
{"id": "n-bvaLSCC78", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unlike the search space of existing mainstream NAS-Bench that focuses only on network architectures, our EA-HAS-Bench consists of a combination of two parts: the network architecture space\u2014RegNet (Radosavovic et al., 2020) and the hyperparameter space for optimization and training, in order to cover diversified configurations that affect both performance and energy consumption. The details of the search space are shown in Table 2.\\n\\n### RegNet Search Space.\\n\\nOur benchmark applies RegNet as the architecture search space because it contains several essential factors that control the scale and shape of the deep model, which corresponds to the training energy cost. Specifically, RegNet is a macro search space with the variation in width and depth explained by a quantized linear function. Specifically, for widths of residual blocks:\\n\\n$$u_j = w_0 + w_a \\\\times j,$$\\n\\nwhere $0 \\\\leq j \\\\leq d$ and $d$ denotes the depth. An additional parameter $w_m$ is introduced to quantize $u_j$, i.e.,\\n\\n$$u_j = w_0 \\\\times w_s^m j,$$\\n\\nand the quantized per-block widths $w_j$ is computed by\\n\\n$$w_j = w_0 \\\\times w_s^m \\\\lfloor s_j \\\\rfloor.$$\\n\\nThe original search space of RegNet is for ImageNet and is non-trivial to directly apply to other datasets. As a result, we shrink down the original model size range of the RegNet space and constraint the total parameters and FLOPs of a model to a relatively small size, which achieves faster training and saves resources.\\n\\n### Training hyperparameter Search Space.\\n\\nHyperparameter space (e.g., learning rate and maximum epoch) also has a great impact on energy consumption throughout the training phase. For example, the maximum epoch is almost proportional to training time, which is also proportional to training energy cost. Different learning rate also causes different convergence rate and different total training time, resulting in different training energy cost. Thus, for hyperparameter search space, EA-HAS-Bench considers training epochs, and the most important factors in training schemes, i.e., base learning rate, decay policy of learning rate, optimizer, and data augmentation.\\n\\nAs a result, the search space of EA-HAS-Bench contains a total of $3^{26} \\\\times 10^{10}$ configurations, including $6 \\\\times 10^7$ architectures and 540 training hyperparameter settings with variant training epochs.\\n\\n### Evaluation Metrics\\n\\nThe EA-HAS-Bench dataset provides the following three types of metrics to evaluate different configurations.\"}"}
{"id": "n-bvaLSCC78", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Overview of B\u00e9zier Curve-based Surrogate Model. HC denotes Hyperparameter configuration.\\n\\nModel Complexity. Metrics related to model complexity include parameter size, FLOPs, number of network activations (the size of the output tensors of each convolutional layer), as well as the inference energy cost of the trained model.\\n\\nModel Performance. In order to support multi-fidelity NAS algorithms such as Hyperband (Li et al., 2017), EA-HAS-Bench provides the full training information including training, validation, and test accuracy learning curves for each hyperparameter and architecture configuration.\\n\\nSearch Cost. Firstly, the energy cost (in kWh) and time (in seconds) to train a model under a certain configuration for one epoch are obtained. Then, by accumulating the energy consumption and runtime at each epoch, we obtain the total search cost of a configuration, which allows NAS/HPO methods to search optimal models under a limited cost budget.\\n\\n2.3 B\u00e9zier Curve-based Surrogate Model\\n\\nDue to the large EA-HAS-Bench search space, directly training all configurations in the space is infeasible even for a small dataset like CIFAR10. As a result, some of the metrics can not be directly measured, including the model performance curve, search energy cost, and runtime. Thus, similar to other recent works (Zela et al., 2022; Yan et al., 2021), we develop a surrogate model that expands the entire search space from a sampled subset of configurations.\\n\\nAs for energy cost and training time, we follow the Surrogate NAS Benchmark (Zela et al., 2022) and train LGB (LightGBM) (Ke et al., 2017) models to predict these. However, for learning curve prediction, surrogate models proposed by the existing NAS-Bench are not applicable to EA-HAS-Bench. Since EA-HAS-Bench contains various maximum training epochs in the search space, none of the existing surrogate model can cope with dimensionally varying inputs. More importantly, it is not possible to directly constrain the learning curve after using the dimensionality reduction operation (e.g., the error rate should be between 0 and 1). In our early experiments, the implementation of the NAS algorithm on the NAS-Bench-X11 surrogate model would yield results with more than 100% accuracy. Therefore, we propose a B\u00e9zier Curved-based Surrogate (BCS) Model to solve the above problems.\\n\\nConvert Learning Curve to B\u00e9zier Curve Control Points. For each configuration of network architecture and training hyperparameters, the surrogate model outputs the learning curve containing the accuracy or error rate of each epoch. Inspired by the success of the B\u00e9zier curve in other areas (Liu et al., 2020), we choose the B\u00e9zier curve to fit learning curves of arbitrary length. The shape of a B\u00e9zier curve is entirely determined by its control points, and degree $n$ control points correspond to a B\u00e9zier curve of order $n-1$. The B\u00e9zier curve can be formulated in a recursive way as follows:\\n\\n$$P(t) = \\\\sum_{i=0}^{n} P_i B_{i,n}(t), \\\\quad t \\\\in [0, 1]$$\\n\\n(2)\"}"}
{"id": "n-bvaLSCC78", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nwhere $P_i$ denotes control points, $B_{i,n}(t) = C_{i,n}t^{i}(1-t)^{n-i}$ and $i = 0, 1, \\\\ldots, n$.\\n\\nAs a result, the task of regressing a learning curve of arbitrary length is simplified to predicting Bezier curve control points. Given a learning curve, $\\\\{(e_{x_{i}}, e_{y_{i}})\\\\}_{i=1}^{m}$ where $e_y$ is the error of the $e_{x}$th epoch and $m$ is the maximum epoch, we need to get the optimal control points to generate Quartic Bezier curves to fit the learning curve. The control points are learned with the standard least square method. Since the horizontal coordinates of the start and end points of the learning curve are fixed (i.e., $e_{x_{\\\\text{start}}}=1$ and $e_{x_{\\\\text{end}}}$=maximum epoch), the surrogate model only predicts the vertical coordinates of these two control points. An illustration of generated Bezier curves is shown in Figure 7 in Appendix B.4.\\n\\nSurrogate Model Structure. Given a RegNet architecture and hyperparameter configurations, BCS estimates the Bezier curve control points with a neural network. As shown in Figure 1, the proposed Bezier curve-based Surrogate model is composed of a hyperparameter encoder $E_h$, architecture encoder $E_a$, and learning curve prediction network $f$. The training hyperparameter configurations are represented as one-hot vector $v_h$ and then fed into $E_h$. The RegNet configuration parameters are normalized to values between 0 and 1, concatenated to a vector, and fed into $E_a$. Finally, the hyperparameter representation and architecture representation are fed into the learning curve predictor to estimate Bezier curve starting/ending points and control points:\\n\\n$$\\\\{i_{y_{\\\\text{start}}}, i_{y_{\\\\text{end}}}, c_{x_{1}}, c_{y_{1}}, \\\\ldots, c_{x_{3}}, c_{y_{3}}\\\\} = f(E_a(v_{a}), E_h(v_{h})) \\\\tag{3}$$\\n\\nThe learning curve predictor consists of two branches. One predicts the vertical coordinates of the starting and ending points of the Bezier curve, and the other branch predicts the other control points. With the control points obtained, we can generate Bezier curves with equation (2), and then obtain the accuracy of each epoch based on the horizontal coordinates $e_{x_{i}}$ of the curve.\\n\\n2.4 DATASET COLLECTION\\n\\nSome previous works (Eggensperger et al., 2015) propose to sample more training data from the high-performance regions of the search space because an effective optimizer spends most of its time in high-performance regions of the search space. However, this sampling strategy causes a distribution shift between the sampled training data and the actual search space, which hurts the prediction accuracy of the surrogate model. As discussed and verified in recent work (Zela et al., 2022), a search space containing hyper-parameters is more likely to produce dysfunctional models which are rarely covered in a sampled sub-set focusing on high-performance regions, and hence purely random sampled training data yields more satisfactory performance. In summary, for EA-HAS-Bench\u2019s search space that contains both model architectures and hyperparameters, we use random search (RS) to sample unbiased data to build a robust surrogate benchmark.\\n\\nThe sampled architecture and hyperparameter configurations are trained and evaluated on two of the most popular image classification datasets, namely CIFAR-10 (Krizhevsky et al., 2009) and MicroImageNet challenge\u2019s (Tiny ImageNet) dataset (Le & Yang, 2015).\\n\\n2.5 SURROGATE BENCHMARK EVALUATION\\n\\nComparison Methods. We compare the proposed method with six parametric learning curve based models (Domhan et al., 2015) (Exp$^3$, ilog$^2$, pow$^2$, log power, log linear, vapor pressure) and three surrogate models (SVD-XGB, SVD-LGB, SVD-MLP) from NAS-Bench-X11 Yan et al. (2021). For a fair comparison, the parametric learning curve-based model applies the same network structure as our proposed BCS. For NAS-Bench-X11, we compress learning curves of different lengths (50, 100, and 200 epochs) into the hidden space with the same dimension with three different SVDs respectively (although this is not convenient to cope with learning curves of arbitrary length). Tree-of-Parzen-Estimators (TPE) (Bergstra et al., 2011) is adopted for all surrogate models to search for the best hyperparameter configuration. The details of the experiments and ablation study are in Appendix B.\"}"}
{"id": "n-bvaLSCC78", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Methods                  | CIFAR10 Avg.R\u00b2 | CIFAR10 Final R\u00b2 | TinyImageNet Avg.KT | TinyImageNet Final KT |\\n|-------------------------|----------------|------------------|---------------------|-----------------------|\\n|                         |                |                  |                     |                       |\\n| Parametric learning curve neural network (Domhan et al., 2015) | 0.397          | 0.791            | -1.128              | 0.935                 |\\n| exp3                    | 0.799          | 0.830            | 0.297               | 0.978                 |\\n| ilog2                   | 0.212          | -0.056           | 0.321               | 0.396                 |\\n| pow2                    | 0.195          | 0.583            | -1.933              | 0.872                 |\\n| log power               | 0.808          | 0.825            | 0.779               | 0.969                 |\\n| logx linear             | 0.790          | 0.671            | 0.897               | 0.957                 |\\n| NAS-Bench-X11 (Yan et al., 2021) | 0.762          | 0.731            | 0.890               | 0.897                 |\\n| SVD-XGB                 | 0.838          | 0.850            | 0.967               | 0.976                 |\\n| SVD-LGB                 | 0.869          | 0.835            | 0.967               | 0.972                 |\\n| SVD-MLP                 | 0.892          | 0.872            | 0.968               | 0.979                 |\\n| BCS(Ours)               | 0.857          | 0.821            | 0.979               | 0.975                 |\\n| GT (1 seed)             | 0.892          | 0.872            | 0.860               | 0.841                 |\\n\\nThe sampled configurations on CIFAR10 and TinyImageNet are split into training, validation, and testing sets containing 11597, 1288, and 1000 samples respectively.\\n\\nEvaluation Metrics. Following Wen et al. (2020), White et al. (2021b) and Zela et al. (2022), we use the coefficient of determination $R^2$ and the Kendall rank correlation coefficient $\\\\tau$ as the evaluation metrics. These two metrics only evaluate the performance based on the overall statistics of the curve rather than anomalies. However, a few spike anomalies on a validation curve could significantly affect the final accuracy prediction. As a result, we further adopt spike anomalies (Yan et al., 2021) as extra metrics (detailed descriptions in appendix).\\n\\nEvaluation Results. The performance of surrogate models is shown in Table 3. First, the parametric learning curve-based models function cannot well fit the real learning curve in EA-HAS-Bench, and some of the higher order functions even fail to converge, such as $\\\\text{pow}^3(c - ax - \\\\alpha)$ and $\\\\text{pow}^4(c - (ax + b) - \\\\alpha)$. This is because the importance of the different parameters in a surrogate model varies considerably, especially the parameter which is in the exponent of an exponential function. The percentage of spike anomalies for real vs. BCS is 3.72% and 4.68% on CIFAR-10 and 0.83% and 1.31% on TinyImageNet, respectively. We further evaluate the consistency between the real measured energy cost and the predicted energy cost by the surrogate model. Specifically, on CIFAR-10, the energy cost surrogate model achieves $R^2$ of 0.787, $KT$ of 0.686, and Pearson correlation of 0.89. On TinyImageNet, it achieves $R^2$ of 0.959, $KT$ of 0.872, and Pearson correlation of 0.97.\\n\\n## Difference with Existing NAS Benchmarks\\n\\nCompared with existing NAS benchmarks such as NAS-Bench-101 (Ying et al., 2019), NAS-Bench-201 (Dong & Yang, 2020), NAS-Bench-301 (Siems et al., 2020) or Surrogate NAS Bench (Zela et al., 2022), EA-HAS-Bench has three significant differences.\\n\\n1. **Diverse and Large Scale Joint Search Space.** EA-HAS-Bench is more diverse in terms of the types of configurations in the search space, which contains both model architectures and training hyperparameters. Although NAS-HPO-Bench (Klein & Hutter, 2019) and NAS-HPO-Bench-II (Hirose et al., 2021) also consider both architectures and hyperparameters, both benchmarks are based on small and homogeneous search spaces. Specifically, NAS-HPO-Bench focuses only on 2-layer feed-forward network training on tabular data, and the hyperparameters search space of NAS-HPO-Bench-II only contains learning rate and batch size. Besides the search space, diversity is also reflected in the evaluated performance ranges. As shown in Figure 2, the performance range of DARTS (Liu et al., 2019) used by NAS-Bench-301 is the smallest for the validation performance on CIFAR-10. Although DARTS contains more architectures, the performance of the models in this space is significantly less diverse (Yang et al., 2020). Compared with NAS-Bench-101/201, the configurations in EA-HAS-Bench cover a much larger performance range.\"}"}
{"id": "n-bvaLSCC78", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To the best of our knowledge, NAS-Bench-X11 (Yan et al., 2021) is the only existing surrogate model that provides the full training status over the entire training process. However, NAS-Bench-X11 is only available for learning curves with fixed maximum epochs. The number of training epochs required for convergence is not always the same for different architectures and it also directly affects the training energy cost. As a result, for a more realistic search space like EA-HAS-Bench, we propose BSC to predict learning curves with different maximum epochs.\\n\\nFull-cycle Energy Consumption.\\n\\nMost existing benchmarks use model training time as the training resource budget, which is not an accurate estimation of energy cost. Firstly, the GPU does not function at a stable level throughout the entire training process, or it may not even be working at all for a significant time period during training, and hence longer training time does not always mean more GPU usage and more energy cost. Secondly, energy cost not only corresponds to training time but also relates to the energy cost per unit time, which is affected by the architecture and hyperparameter configurations. Our experimental analysis in the next section also verifies that training time and energy cost are not equivalent. HW-NAS-Bench (Li et al., 2021b) also considers energy consumption, but its focus is on the model inference energy cost. On the other hand, EA-HAS-Bench provides a full-cycle energy consumption, both during training and inference. The energy consumption metric allows HPO algorithms to optimize for accuracy under an energy cost limit (Section 5).\\n\\n4 A NALYSIS ON EA-HAS-BENCH\\n\\nImpact of Hyperparameter Configurations.\\n\\nSince most existing large-scale computer vision NAS benchmarks focus solely on network architectures and apply the same hyperparameters for all models, we examine how different hyperparameters affect the searched model performance. We use an empirical cumulative distribution (ECDF) (Radosavovic et al., 2020) to assess the quality of search space. Specifically, we take a set of configurations from the search space and characterize its error distribution. Figure 3 shows the empirical cumulative distribution (ECDF) of different training hyperparameters on CIFAR-10 and TinyImageNet. (The full version is in the Figure 13 of Appendix.) We observe that the learning rate and the choice of optimizer may have the most significant impact on the search space quality. The size of the maximum number of epochs is also positively correlated to the quality of the search space.\\n\\nCorrelation Between Training Energy Cost and Training Time.\\n\\nFigure 4 investigates the relationship between the training energy consumption (TEC), training time, and the test accuracy of models in TinyImageNet. Firstly, we observe that the points in the left figure of Figure 4 is rather scattered. This means the correlation between training time and energy cost is not strong. Although training the model for a longer period is likely to yield a higher energy cost, the final cost still depends on many other factors including power (i.e., consumed energy per hour). The middle and right plots of Figure 4 also verify the conclusion, where the models in the Pareto Frontier on the\"}"}
{"id": "n-bvaLSCC78", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: (left) Training time vs. training energy consumption (TEC), color-coded by test accuracy. (middle) Test Accuracy vs. TEC. (right) Test Accuracy vs. training time. TEC and training time are the per epoch training energy consumption (Kw*h) and runtime (seconds) on the Tesla V100. The orange cross in the middle plot denotes the models in the Pareto Frontier on the accuracy-runtime coordinate but are not in the Pareto Frontier on the accuracy-TEC coordinate, showing that training time and energy cost are not equivalent. Meanwhile, training a model longer (or with more energy) does not guarantee better accuracy. In the middle and right plots of Figure 4, we see that many models with high TECs, still fail to train, due to improper neural architectures or hyperparameter configurations. On the other hand, simply searching for the best accuracy might not be cost-efficient, since there are quite a few configurations with the same level of accuracy. The observation motivates finding the best trade-off between accuracy and energy consumption.\\n\\nFigure 5: Correlation coefficient between RegNet + HPO and Accuracy, Runtime, TEC, and inference energy cost (IEC) on TinyImageNet. Configurations\u2013Accuracy/Energy Correlation. Figure 5 shows the correlation between architecture/hyperparameter configurations and accuracy, runtime, TEC, and inference energy cost (IEC). We observe that hyperparameters like learning rate also have a high correlation with model performance, which further verifies the importance to consider hyperparameters in NASBench. Both network architecture like network depth and width and hyperparameters like training epoch has a relatively strong correlation with energy cost, showing the importance of considering both perspectives in our energy-aware Benchmark.\\n\\n5 EA-HAS-B\\n\\nEA-HAS-Bench saves tremendous resources to train and evaluate the configurations for real. We demonstrate how to leverage the proposed dataset to conduct energy-aware AutoML research with two use cases. Firstly, we evaluate the trade-off between search energy cost and model performance of four single-fidelity algorithms: random search (RS) (Li & Talwalkar, 2019), local search (LS) (White et al., 2020), regularized evolution (REA) (Real et al., 2019), BANANAS (White et al., 2021a), and two multi-fidelity bandit-based algorithms: Hyperband (HB) (Li et al., 2017) and Bayesian optimization Hyperband (BOHB) Falkner et al. (2018). The implementation details of the above algorithms are in Appendix D. Then, in the second sub-section, as the first step toward a long journey of energy-aware AutoML, we arm several existing AutoML algorithms with another energy-related objective and verify the effectiveness of energy-aware AutoML on our hyperparameter-architecture benchmark.\"}"}
{"id": "n-bvaLSCC78", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BANANAS is based on Bayesian optimization and samples the next point by acquisition function (White et al., 2021a). We used a modified version from NAS-Bench-X11 to construct the candidate pool by mutating the best four points 10 times each.\\n\\nHyperband is based on the random search with successive halving (Li et al., 2017). Since the space size of EA-NAS-Bench is $3 \\\\times 10^{10}$, we expect to explore more points and set the maximum budget to 512 and the minimum budget to 4.\\n\\nBayesian optimization Hyperband is based on Hyperband with Bayesian optimization (Falkner et al., 2018). We use the same parameters as Hyperband.\\n\\nFollowing Ying et al. (2019) and Yan et al. (2021), during the architecture search we keep track of the best architectures found by the algorithm after each evaluation and rank them according to their validation accuracy. When the metric we specify (e.g., total energy consumption or target accuracy) exceeds the limit we set, we stop the search. After the search, we query the corresponding best accuracy of the model. We then compute regret:\\n\\n$$\\\\text{regret}_i = \\\\text{Acc}_i - \\\\text{Acc}^*$$\\n\\nwhere $\\\\text{Acc}_i$ denotes the accuracy of the best architecture after each evaluation $i$ and $\\\\text{Acc}^*$ denotes the model with the highest average accuracy in the entire dataset. For experiments in section 5, we run 5 trials of each AutoML algorithm and compute the mean and standard deviation.\\n\\nWe also conduct experiments to evaluate the proposed energy-aware baselines on the TinyImageNet dataset. Specifically, we set $T$ to 2.5 and target performance to 57%. The result is shown in Figure 11. Same to the experiment on CIFAR-10, compared with the baseline without the energy cost penalty, our EA algorithm costs significantly lower search energy to reach the target accuracy.\\n\\nIn this section, we conduct a more comprehensive empirical evaluation of the proposed new energy-aware baselines. Specifically, an ablation study is conducted to examine how different reward functions and hyper-parameters in the proposed objective affect the performance of the energy-aware HPO.\"}"}
{"id": "n-bvaLSCC78", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Choice of the reward function.\\n\\nInspired by existing work on joint optimization of accuracy and latency (Tan et al., 2019; Bender et al., 2020), we modify the existing NAS methods (LS and BA-NAS) by using multi-objective optimization functions, including soft exponential reward function (SoftE), hard exponential reward function (HardE), absolute reward function (Absolute). The soft exponential reward function is expressed as\\n\\n$$r(\\\\alpha) = \\\\text{Acc}(\\\\alpha) \\\\times \\\\left(\\\\frac{T(\\\\alpha)}{T_0}\\\\right)^\\\\beta$$\\n\\nwhere \\\\(\\\\text{Acc}(\\\\alpha)\\\\) denotes the accuracy of a candidate architecture \\\\(\\\\alpha\\\\), \\\\(T(\\\\alpha)\\\\) is its training energy consumption, \\\\(T_0\\\\) denotes the TEC target to control the total TEC in the search process, and \\\\(\\\\beta < 0\\\\) is the cost exponent. The hard exponential reward function imposes a \\\"hard\\\" limit constraint:\\n\\n$$r(\\\\alpha) = \\\\begin{cases} \\\\text{Acc}(\\\\alpha), & \\\\text{if } T(\\\\alpha) \\\\leq T_0 \\\\\\\\ \\\\text{Q}(\\\\alpha) \\\\times \\\\left(\\\\frac{T(\\\\alpha)}{T_0}\\\\right)^\\\\beta, & \\\\text{if } T(\\\\alpha) > T_0 \\\\end{cases}$$\\n\\nThe absolute reward function aims to find a configuration whose TEC is close to \\\\(T_0\\\\):\\n\\n$$r(\\\\alpha) = \\\\text{Acc}(\\\\alpha) + \\\\beta \\\\left|\\\\frac{T(\\\\alpha)}{T_0} - 1\\\\right|$$\\n\\nFor the three functions, we set TEC target \\\\(T_0 = 0.45\\\\) and \\\\(\\\\beta = -0.07\\\\). In Table 10, we compare the total energy required by these methods to achieve the target performance. Since SoftE requires the least amount of energy, we choose SoftE to convert the current NAS algorithm to the EA-NAS algorithm.\\n\\nTable 10: The total TEC in reaching target performance on CIFAR10\\n\\n| Algorithms | Origin | Soft Exp | Hard Exp | Absolute |\\n|------------|-------|---------|----------|-----------|\\n| Local Search | 5,521 | 3,218 | 3,595 | 6,070 |\\n| BANANAS | 4,966 | 3,630 | 5,227 | 5,227 |\\n\\nChoice of the scaling.\\n\\nFollowing MnasNet, we simply set the beta to -0.07. Based on the TEC distribution of sampling points mainly between 0.4 and 0.5, we try three different parameters. The results are shown in the table. After \\\\(T_0 \\\\geq 0.45\\\\), the final Total TEC is the same, which indicates that EA-NAS is robust to the parameter \\\\(T_0\\\\).\\n\\nTable 11: The impact of different \\\\(T_0\\\\) in CIFAR10\\n\\n| Algorithms | \\\\(T_0 = 0.4\\\\) | \\\\(T_0 = 0.45\\\\) | \\\\(T_0 = 0.5\\\\) |\\n|------------|----------------|----------------|----------------|\\n| EA-BNANAS | 3,692 | 3,630 | 3,630 |\\n| EA-LS | 4,751 | 3,218 | 3,218 |\\n\\nBesides providing a large-scale proxy benchmark and the tens of thousands of sampling points used to construct it, we also provide a small real tabular benchmark. As shown in Table 12, we redefine a very small joint search space with a size of 500. As with the previous tabular benchmark, we evaluate all models within this space.\\n\\nNAS algorithms on Tabular Benchmark.\\n\\nSimilar to Section 5.1, we implement 6 NAS algorithms on the small tabular benchmark. In this experiment, the maximum search cost is set to 20,000 kWh, which is equivalent to running a single-fidelity HPO algorithm for about 200 iterations. We run 10 trials of each AutoML algorithm and compute the mean and standard deviation. The result is shown in Figure 12. Due to the different search spaces and budgets, the conclusions drawn differ slightly from the previous ones on the surrogate benchmark.\"}"}
{"id": "n-bvaLSCC78", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12: Overview of the toy search space\\n\\n| Type     | Hyperparameter | Range       | Quantize | Space |\\n|----------|----------------|-------------|----------|-------|\\n| RegNet   | Depth          | \\\\(d\\\\)       | \\\\([6,15]\\\\) | 1 10   |\\n|          | \\\\(w_0\\\\)       | \\\\([80,112]\\\\) | 8 5      |\\n|          | \\\\(w_a\\\\)       | \\\\([-20,-1]\\\\) |          |\\n|          | \\\\(w_m\\\\)       | \\\\([-2.75,1]\\\\) |          |\\n|          | Group Width    | \\\\([-16,-1]\\\\) |          |\\n|          | Total of Network Architectures | 50       |\\n\\nOptim\\n\\n| Learning rate | \\\\{0.001, 0.003, 0.005, 0.01, 0.03, 0.05, 0.1, 0.3, 0.5, 1.0\\\\} |\\n| Max epoch     | \\\\{100\\\\} |\\n| Decay policy  | \\\\{'cos'\\\\} |\\n| Optim         | sgd         |\\n\\nTraining Data augmentation None\\n\\nTotal of Hyperparameter Space 10\\n\\nFigure 12: NAS results on small tabular benchmark CIFAR10.\"}"}
{"id": "n-bvaLSCC78", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 13: The empirical cumulative distribution (ECDF) of all real measured configurations on CIFAR-10 and TinyImageNet for 5 different splits.\"}"}
{"id": "n-bvaLSCC78", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.1 Benchmarking Existing Algorithms\\n\\nExperimental setup. Since EA-HAS-Bench focuses on the trade-off between model performance and search energy cost, in this experiment we use the total search energy cost as the resource limitation, instead of training time. As a result, we set the maximum search cost to roughly 40,000 kWh for CIFAR 10 and 250,000 kWh for TinyImageNet, which is equivalent to running a single-fidelity HPO algorithm for about 1,000 iterations.\\n\\nResults. In Figure 6 left and middle, we compare single- and multi-fidelity algorithms on the search space of EA-HAS-Bench. For single-fidelity algorithms, LS is the top-performing algorithm across two datasets. This shows that similar to NAS-Bench, HAS-Bench with a joint search space also has locality, a property by which \u201cclose\u201d points in search space tend to have similar performance.\\n\\n5.2 A New Energy-Aware HPO Baseline\\n\\nEnergy Aware HPO\\n\\nMost existing HPO methods do not directly consider the search energy cost. In order to verify the importance to consider the energy cost during the search process, we propose a new energy-aware HPO baseline by modifying existing HPO methods. Following MnasNet (Tan et al., 2019), we modify the BANANAS (White et al., 2021a) and LS (White et al., 2020) by changing the optimization goal to a metric that considers both accuracy and energy cost:\\n\\n\\\\[ \\\\text{ACC} \\\\times (\\\\frac{\\\\text{TEC}}{\\\\text{T}_0})^w, \\\\]\\n\\nwhere \\\\( \\\\text{T}_0 \\\\) is the target TEC. For CIFAR10, we set the \\\\( \\\\text{T}_0 \\\\) to 0.45 and \\\\( w \\\\) to -0.07.\\n\\nExperimental Setup and Result\\n\\nWe explore another important usage scenario, where the goal is to achieve a target model performance using as little energy cost as possible. As a result, we use model performance rather than energy consumption as a resource limitation and stop the search when the model hits a target performance and compare the corresponding search energy cost. For CIFAR10, the target accuracy is set to 97%. As shown in Figure 6 right, EA algorithms that consider TEC save close to 20% of search energy consumption compared to the origin algorithms in achieving the target accuracy. The ablation study is shown in Appendix D.4.\\n\\n6 Conclusion\\n\\nEA-HAS-Bench is the first large-scale energy-aware hyperparameter and architecture search benchmark. The search space of EA-HAS-Bench consists of both network architecture scale and training hyperparameters, covering diverse configurations related to energy cost. A novel B\u00b4ezier curve-based surrogate model is proposed for the new joint search space. Furthermore, we analyze the difference between existing NAS benchmarks and EA-HAS-Bench and dataset statistics and the correlation of the collected data. Finally, we provide use cases of EA-HAS-Bench to show that energy-aware algorithms can save significant energy in the search process. We expect that our EA-HAS-Bench expedites and facilitates the EA-NAS and HAS research innovations.\"}"}
{"id": "n-bvaLSCC78", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING\\n\\nS. Dou and C. Zhao acknowledge that the work was supported by the National Natural Science Fund of China (62076184, 61976158, 61976160, 62076182, 62276190), in part by Fundamental Research Funds for the Central Universities and State Key Laboratory of Integrated Service Networks (Xidian University), in part by Shanghai Innovation Action Project of Science and Technology (20511100700) and Shanghai Natural Science Foundation (22ZR1466700). We thank Yuge Zhang (Microsoft Research Asia) for suggestions on the design of the search space and for revising the writing of the paper. We thank Bo Li (Nanyang Technological University) for help with the codes.\\n\\nREFERENCES\\n\\nArchit Bansal, Danny Stoll, Maciej Janowski, Arber Zela, and Frank Hutter. Jahs-bench-201: A foundation for research on joint architecture and hyperparameter search. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.\\n\\nGabriel Bender, Hanxiao Liu, Bo Chen, Grace Chu, Shuyang Cheng, Pieter-Jan Kindermans, and Quoc V Le. Can weight sharing outperform random architecture search? an investigation with tunas. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14323\u201314332, 2020.\\n\\nJames Bergstra, R\u00b4emi Bardenet, Yoshua Bengio, and Bal\u00b4azs K\u00b4egl. Algorithms for hyper-parameter optimization. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain, pp. 2546\u20132554, 2011.\\n\\nTobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pp. 3460\u20133468. AAAI Press, 2015.\\n\\nXuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=HJxyZkBKDr.\\n\\nXuanyi Dong, Mingxing Tan, Adams Wei Yu, Daiyi Peng, Bogdan Gabrys, and Quoc V Le. Autohas: Efficient hyperparameter and architecture search. arXiv preprint arXiv:2006.03656, 2020.\\n\\nKatharina Eggensperger, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Efficient benchmarking of hyperparameter optimizers via surrogates. In Blai Bonet and Sven Koenig (eds.), Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA, pp. 1114\u20131120. AAAI Press, 2015.\\n\\nThomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. J. Mach. Learn. Res., 20:55:1\u201355:21, 2019.\\n\\nStefan Falkner, Aaron Klein, and Frank Hutter. BOHB: robust and efficient hyperparameter optimization at scale. In Proceedings of the 35th International Conference on Machine Learning, ICML, volume 80 of Proceedings of Machine Learning Research, pp. 1436\u20131445. PMLR, 2018.\\n\\nMatthias Feurer and Frank Hutter. Hyperparameter optimization. In Automated machine learning, pp. 3\u201333. Springer, Cham, 2019.\\n\\nXin He, Kaiyong Zhao, and Xiaowen Chu. Automl: A survey of the state-of-the-art. Knowledge-Based Systems, 212:106622, 2021.\\n\\nYoichi Hirose, Nozomu Yoshinari, and Shinichi Shirakawa. Nas-hpo-bench-ii: A benchmark dataset on joint optimization of convolutional neural network architecture and training hyperparameters. In Vineeth N. Balasubramanian and Ivor W. Tsang (eds.), Asian Conference on Machine Learning, ACML 2021, 17-19 November 2021, Virtual Event, volume 157 of Proceedings of Machine Learning Research, pp. 1349\u20131364. PMLR, 2021.\"}"}
{"id": "n-bvaLSCC78", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "n-bvaLSCC78", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. Communications of the ACM, 63(12):54\u201363, 2020.\\n\\nJulien Siems, Lucas Zimmer, Arber Zela, Jovita Lukasik, Margret Keuper, and Frank Hutter. Nas-bench-301 and the case for surrogate benchmarks for neural architecture search. arXiv preprint arXiv:2008.09777, 2020.\\n\\nDavid R. So, Quoc V. Le, and Chen Liang. The evolved transformer. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 5877\u20135886. PMLR, 2019.\\n\\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243, 2019.\\n\\nMingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V. Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2820\u20132828, 2019.\\n\\nWei Wen, Hanxiao Liu, Yiran Chen, Hai Helen Li, Gabriel Bender, and Pieter-Jan Kindermans. Neural predictor for neural architecture search. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXIX, volume 12374 of Lecture Notes in Computer Science, pp. 660\u2013676. Springer, 2020.\\n\\nColin White, Sam Nolen, and Yash Savani. Local search is state of the art for NAS benchmarks. CoRR, abs/2005.02960, 2020.\\n\\nColin White, Willie Neiswanger, and Yash Savani. BANANAS: Bayesian optimization with neural architectures for neural architecture search. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pp. 10293\u201310301. AAAI Press, 2021a.\\n\\nColin White, Arber Zela, Robin Ru, Yang Liu, and Frank Hutter. How powerful are performance predictors in neural architecture search? In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 28454\u201328469, 2021b.\\n\\nBichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10734\u201310742, 2019.\\n\\nShen Yan, Colin White, Yash Savani, and Frank Hutter. Nas-bench-x11 and the power of learning curves. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems, pp. 22534\u201322549, 2021.\\n\\nAntoine Yang, Pedro M. Esperan\u00e7a, and Fabio Maria Carlucci. NAS evaluation is frustratingly hard. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\\n\\nChris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy, and Frank Hutter. Nas-bench-101: Towards reproducible neural architecture search. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 7105\u20137114. PMLR, 2019.\\n\\nArber Zela, Julien Niklas Siems, Lucas Zimmer, Jovita Lukasik, Margret Keuper, and Frank Hutter. Surrogate nas benchmarks: Going beyond the limited search spaces of tabular nas benchmarks. In International Conference on Learning Representations, 2022.\\n\\nBarret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.\"}"}
{"id": "n-bvaLSCC78", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Existing NAS Benchmarks.\\n\\nTabular NAS Benchmarks.\\n\\nWhile neural architecture search (NAS) has succeeded in various practical tasks such as image recognition (Pham et al., 2018) and sequence modeling (So et al., 2019), the non-reproducibility of NAS has been contested (Li & Talwalkar, 2019). One of the main reasons that complicate NAS reproducibility studies is the high computational cost as well as the large carbon emissions that result (Li et al., 2021a). For the reproducibility of NAS research, NAS-Bench-101 (Ying et al., 2019) and NAS-Bench-201 (Dong & Yang, 2020) have been proposed. Due to the success of these tabular benchmarks in image classification, corresponding benchmarks have been proposed in areas such as NLP and speech recognition (Mehta et al., 2022).\\n\\nSurrogate NAS Benchmarks.\\n\\nThe recent Surrogate NAS Benchmarks (Zela et al., 2022) builds surrogate benchmarks on a realistic search space and shows strong generalization performance. To extend the surrogate model, NAS-Bench-X11 (Yan et al., 2021) uses singular value decomposition and noise modeling to output the learning curve.\\n\\nNAS Benchmarks joint HPO\\n\\nMost NAS Benchmarks have fixed training hyperparameters, but the training hyperparameters strongly influence the performance of the model obtained by NAS (Dong et al., 2020). To alleviate this problem, NAS-HPO-Bench (Klein & Hutter, 2019) and NAS-HPO-Bench-II Hirose et al. (2021) are proposed. However, as shown in Table 1, the size of the two Benchmarks is small and the search spaces are simple. The architecture space of NAS-HPO-Bench is multi-layer perception (MLP) trained on the tabular datasets for regression tasks. NAS-HPO-Bench-II only really measures 12 epochs using the CIFAR-10 dataset Krizhevsky et al. (2009) and the training hyperparameters space only contains learning rate and batch size. A similar work to ours is JAHS-Bench-201 (Bansal et al.), also with a large-scale joint search space. JAHS-Bench-201 provides FLOPS, latency, and runtime in addition to performance and loss. However, JAHS-Bench-201 does not focus on energy consumption during the search.\\n\\nA.2 Resource-Aware NAS\\n\\nEarly NAS algorithms (Zoph & Le, 2016; Real et al., 2019) focused only on performance and ignored some of the associated hardware consumption. To this end, many resource-aware NAS algorithms are proposed to balance performance and resource budgets (He et al., 2021). These resource-aware NAS algorithms focus on the four types of computational costs that are included the FLOPs, parameter size, the number of Multiply-ACcumulate (MAC) operations, and real latency. Among the NAS algorithms, two classic works are MnasNet (Tan et al., 2019) and FBNet (Wu et al., 2019). MnasNet proposes a multi-objective neural architecture search approach that optimizes both accuracy and real-world latency on mobile devices. Similar to MansNet, FBNet designs a loss function to trade off the cross-entropy loss and latency of architecture. However, none of the above NAS algorithms focus on the huge energy consumption in search.\\n\\nB More Details of Section 2\\n\\nB.1 More Details on Evaluation Metrics\\n\\nDetails of Spike Anomalies.\\n\\nAlthough R2 and KT can evaluate the surrogate model by measuring the overall statistics between the surrogate benchmark and the ground truth, they are not sensitive to anomalies. Following, NAS-bench-X11 Yan et al. (2021), to evaluate the performance of surrogate models based on anomalies, we introduce the Spike Anomalies metrics. We first calculate the largest value $x$ such that there are fewer than 5% of learning curves whose maximum validation accuracy is higher than their final validation accuracy, on the true learning curves. Next, the percentage of surrogate learning curves whose maximum validation accuracy is $x$ higher than their final validation accuracy was computed.\"}"}
{"id": "n-bvaLSCC78", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Network Structure of BSC. The encoder of architecture and hyperparameters adopts a simple Multi-Layer Perceptron (MLP) structure, consisting of two linear layers with ReLU activation functions. Then, the encoded features are fed into the learning curve prediction network, which is also an MLP with an extra dropout layer, whose output is fed into two linear regressors that output the coordinates of the control points. We use the sigmoid activation function to the regressor, which directly constrains the initial final performance between 0 and 1. The control points are learned with the standard least square method as follows:\\n\\n\\\\[\\n\\\\begin{bmatrix}\\nB_{0,5}(t_0) \\\\\\\\\\nB_{1,5}(t_0) \\\\\\\\\\n\\\\vdots \\\\\\\\\\nB_{5,5}(t_0)\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\nB_{0,5}(t_1) \\\\\\\\\\nB_{1,5}(t_1) \\\\\\\\\\n\\\\vdots \\\\\\\\\\nB_{5,5}(t_1)\\n\\\\end{bmatrix}\\n\\\\cdots\\n\\\\begin{bmatrix}\\nB_{0,5}(t_m) \\\\\\\\\\nB_{1,5}(t_m) \\\\\\\\\\n\\\\vdots \\\\\\\\\\nB_{5,5}(t_m)\\n\\\\end{bmatrix}\\n= \\\\begin{bmatrix}\\nix_{\\\\text{start}} \\\\\\\\\\ny_{\\\\text{start}} \\\\\\\\\\ncx_1 \\\\\\\\\\ncy_1 \\\\\\\\\\n\\\\vdots \\\\\\\\\\nix_{\\\\text{end}} \\\\\\\\\\ny_{\\\\text{end}}\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\nex_0 \\\\\\\\\\ney_0 \\\\\\\\\\nex_1 \\\\\\\\\\ney_1 \\\\\\\\\\n\\\\vdots \\\\\\\\\\nex_m \\\\\\\\\\ney_m\\n\\\\end{bmatrix}\\n\\\\tag{4}\\n\\\\]\\n\\nDetails on the Compared Parametric Learning Curves based Methods. Several parametric learning curves-based methods are selected as the comparison methods. The detailed formulation of those parametric models is shown in Table 4. Following Domhan et al. (2015), we first try the Levenberg-Marquardt algorithm and fall back to Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) in case that fails. In the experiment, we found that the initial parameters are important in the fitting process. Some parametric models of a high order cannot be fitted to the learning curve in the training set because suitable initial parameters cannot be found. In contrast, the initial point of the BSC model is the starting and ending point of the learning curve.\\n\\nTable 4: The formula of parametric learning curve\\n\\n| Reference name | Formula |\\n|---------------|---------|\\n| exp           | \\\\(c - \\\\exp(-ax + b)\\\\) |\\n| ilog          | \\\\(c - a \\\\log x\\\\) |\\n| pow           | \\\\(ax^\\\\alpha\\\\) |\\n| log power     | \\\\(a \\\\log(1 + x^b)\\\\) |\\n| logx linear   | \\\\(a \\\\log x + b\\\\) |\\n| vapor         | \\\\(\\\\exp(a + bx^c \\\\log x)\\\\) |\\n\\nHyperparameters of Surrogate Models. Table 3 shows the optimal hyper-parameters searched by TPE for different surrogate models. Due to the page limit, here we only listed the hyperparameters of the three models that achieve the best performance in Table 5. We used a fixed budget of 500 trials for all surrogate models and average R2 as the optimal target.\\n\\nSurrogate Models of Runtime Metrics\\n\\nAlthough our benchmark focuses on energy consumption, we also provide runtimes to allow NAS methods to use runtimes as budgets. We train an LGB model with the average runtime of each epoch as a target and the model achieves 0.926 for R2 and 0.849 for KT on runtime prediction on CIFAR10.\\n\\nB.3 MORE DETAILS ON DATA COLLECTION\\n\\nHere we provide a more detailed introduction to energy consumption measurement for data collection. Intuitively, the search energy cost is the total energy consumption to complete a search algorithm. Since the majority of the energy cost comes from training each deep model the search algorithm traverses, in our dataset, the search energy cost is defined as the total energy cost (in kWh) or time (in seconds) to train the model configurations traversed by the search algorithms. Specifically, we denote a training configuration in the EA-HAS-Bench search space as \\\\(c \\\\in \\\\mathbb{N}^d\\\\), where \\\\(c\\\\) is a \\\\(d\\\\)-dimensional vector containing \\\\(d\\\\) training parameters. \\\\(e_{\\\\text{ep}}(c)\\\\) is the energy cost measure function that returns the training energy cost to train a model with training configuration \\\\(c\\\\) for one epoch. \\\\(A = \\\\{c(i)\\\\}_{i=0}^{N} \\\\) is the set of configurations a NAS/HPO search method traversed. As a\"}"}
{"id": "n-bvaLSCC78", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Hyperparameters of the surrogate models and the optimal values found via TPE.\\n\\n| Model       | Hyperparameter               | Range       | Type      | Optimal Value |\\n|-------------|------------------------------|-------------|-----------|---------------|\\n| SVD-LGB     | Num. components              | [1, 20]     | uniform   | int           |\\n|             |                              |             |           | 4             |\\n|             | Num. rounds                  | -           | constant  | 3000          |\\n|             | Early Stopping               | -           | constant  | 100           |\\n|             | Max. depth                   | [1, 24]     | uniform   | int           |\\n|             |                              |             |           | 9             |\\n|             | Num. leaves                  | [10, 100]   | uniform   | int           |\\n|             |                              |             |           | 84            |\\n|             | Min. child weight            | [0.001, 10] | log uniform | 0.4622       |\\n|             | Lambda L1                    | [0.001, 1000]| log uniform | 0.0056        |\\n|             | Lambda L2                    | [0.001, 1000]| log uniform | 0.0054        |\\n|             | Boosting type                | -           | constant  | gbdt          |\\n|             | Learning rate                | [0.001, 0.1]| log uniform | 0.5822        |\\n| SVD-MLP     | Num. components              | [1, 20]     | uniform   | int           |\\n|             |                              |             |           | 3             |\\n|             | Num. epochs                  | [5, 200]    | uniform   | int           |\\n|             |                              |             |           | 190           |\\n|             | hidden dim                   | [32, 250]   | constant  | 183           |\\n|             | Num. layers                  | -           | constant  | 4             |\\n|             | Learning rate                | [0.0001, 0.1]| log uniform | 0.0008        |\\n|             | drop out                     | [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]| uniform | int           |\\n|             |                              |             |           | 0.2           |\\n|             | batch size                   | [64, 128, 256]| uniform | int           |\\n|             |                              |             |           | 64            |\\n| BSC         | Num. epochs                  | [5, 200]    | uniform   | int           |\\n|             |                              |             |           | 180           |\\n|             | hidden dim                   | [32, 250]   | constant  | 247           |\\n|             | Num. layers                  | -           | constant  | 4             |\\n|             | Learning rate                | [0.0001, 0.1]| log uniform | 0.0003        |\\n|             | drop out                     | [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]| uniform | int           |\\n|             |                              |             |           | 0             |\\n|             | batch size                   | [64, 128, 256]| uniform | int           |\\n|             |                              |             |           | 64            |\\n| LGB-E       | Num. rounds                  | -           | constant  | 3000          |\\n|             | Early Stopping               | -           | constant  | 100           |\\n|             | Max. depth                   | [1, 100]    | uniform   | int           |\\n|             |                              |             |           | 31            |\\n|             | Num. leaves                  | [10, 1000]  | uniform   | int           |\\n|             |                              |             |           | 315           |\\n|             | Min. child weight            | [0.001, 10] | log uniform | 0.0046        |\\n|             | Lambda L1                    | [0.001, 1000]| log uniform | 20.3216       |\\n|             | Lambda L2                    | [0.001, 1000]| log uniform | 11.6866       |\\n|             | Boosting type                | -           | constant  | gbdt          |\\n|             | Learning rate                | [0.001, 0.1]| log uniform | 0.0451        |\\n\\nThe total search energy cost is defined as:\\n\\n\\\\[ e_s(A) = \\\\sum_{c(i) \\\\in A} e_{ep}(c(i)) \\\\cdot c(n), \\\\]\\n\\nwhere \\\\( n \\\\) is the index of \\\\( c \\\\) that stores the number of total training epochs to train the deep model under configuration \\\\( c \\\\).\\n\\nNext, we introduce how to measure the per epoch energy consumption for different training configurations. Following Li et al. (2021a), we collect the real-time power of the GPU during the operation through the interface of `pynvml`. In the following, we will provide an implementation of the GPU tracker to accurately describe its functionality.\\n\\n```python\\nimport re\\nimport subprocess\\nimport threading\\nimport time\\nimport pynvml\\n\\nimport torch\\nimport xmltodict\\n\\nclass Tracer(threading.Thread):\\n    def __init__(self, gpu_num=(0,), profiling_interval=0.1):\\n        super(Tracer, self).__init__()\\n        self.gpu_num = gpu_num\\n        self.profiling_interval = profiling_interval\\n        self._running = True\\n\\n    def run(self):\\n        pynvml.nvmlInit()\\n        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\\n        power_list = []\\n        while self._running:\\n            time.sleep(self.profiling_interval)\\n            for i in self.gpu_num:\\n                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\\n                power = pynvml.nvmlDeviceGetPowerUsage(handle)\\n                power_list.append(power)\\            \\n```\\n\"}"}
{"id": "n-bvaLSCC78", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"self.counters += 1\\npower_u_info = pynvml.nvmlDeviceGetPowerUsage(handle)\\npower_list.append(power_u_info/1000)\\ntime.sleep(self.profiling_interval)\\n\\nclass GPUTracer:\\n    all_mode = ['normal']\\n    def __init__(self, mode, gpu_num=(0,), profiling_interval=0.1, verbose=False):\\n        if not mode in GPUTracer.all_mode:\\n            raise ValueError(f'Invalid mode : {mode}]\\n        self.mode = mode\\n        self.gpu_num = gpu_num\\n        self.profiling_interval = profiling_interval\\n        self.verbose = verbose\\n\\n    def wrapper(self, *args, **kwargs):\\n        if not GPUTracer.is_enable:\\n            return self.func(*args, **kwargs), None\\n        tracer = Tracer(gpu_num=self.gpu_num, profiling_interval=self.profiling_interval)\\n        start = torch.cuda.Event(enable_timing=True)\\n        end = torch.cuda.Event(enable_timing=True)\\n        start.record()\\n        tracer.start()\\n        results = self.func(*args, **kwargs)\\n        tracer.terminate()\\n        end.record()\\n        torch.cuda.synchronize()\\n\\n        if tracer.counters == 0:\\n            print(\"*\" * 50)\\n            print(\"No tracing info collected, increasing sampling rate if needed.]\\n            print(\"*\" * 50)\\n            tracer.join()\\n            return results, None\\n        else:\\n            tracer.join()\\n            avg_power, avg_temperature, avg_gpu_utils, avg_mem_utils, total_power, total_gpu_utils, total_mem_utils = tracer.communicate()\\n            time_elapse = start.elapsed_time(end) / 1000\\n            energy_consumption = time_elapse * avg_power / 3600\\n            return results, None\\n\\nListing 1: GPU Tracer\\n\\nSpecifically, we implemented this tracer using Python's decorator function and then just logged the GPU information at runtime. In the following, we provide a user case for collecting energy data.\\n\\n@GPUTracer(mode='normal', verbose=True)\\ndef train_epoch(loader, model, ...):\\n    \"\"\"Performs one epoch of training.\\\"\\n    ...\\n\\n@GPUTracer(mode='normal', verbose=True)\\n@torch.no_grad()\\ndef test_epoch(loader, model, ....):\\n    \"\"\"Evaluates the model on the test set.\\\"\\n\\nListing 2: GPU information Collection by GPUTracer\\n\\nThe details of the machines used to collect energy consumption are in Table 6.\"}"}
