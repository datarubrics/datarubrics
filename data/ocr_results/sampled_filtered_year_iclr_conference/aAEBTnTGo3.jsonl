{"id": "aAEBTnTGo3", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: The 95% and 99% quantile of CMMs with standard error computed over 10 seeds.\"}"}
{"id": "aAEBTnTGo3", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we plot the learning curves of the best performing algorithm in each of the four possible settings in \\\\{disable CP, enable CP\\\\} \u00d7 \\\\{left, bushy\\\\}. Performance is measured by mean CCM (from Table 2) and we plot the average performance over 10 runs (shaded region is stderr over said runs). For each setting, we also show the CCM distribution over the training, validation and testing query sets in a complementary CDF (CCDF) plot. The CCDF shows that these distributions are long-tailed.\\n\\nG.1.1 Bushy Plans with Disable CP Heuristic\\n\\nFigure 6: The best algorithm (in terms of CCM mean) for bushy plans with disable CP heuristic is TD3 with prioritized replay with policy learning rate 0.0003 and critic learning rate 0.0001. Shaded region is stderr over 10 runs.\"}"}
{"id": "aAEBTnTGo3", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: The best algorithm (in terms of CCM mean) for left-deep plans with disable CP heuristic is TD3 with prioritized replay with policy learning rate $0.0001$ and critic learning rate $0.0003$. Shaded region is stderr over $10$ runs.\"}"}
{"id": "aAEBTnTGo3", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: The best algorithm (in terms of CCM mean) for bushy plans with CPs enabled is TD3 with prioritized replay with policy learning rate $0.0003$ and critic learning rate $0.0003$. Shaded region is stderr over 10 runs.\"}"}
{"id": "aAEBTnTGo3", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unlike other data system gym environments (Mao et al., 2019; Lim et al., 2023) or reinforcement learning (RL) on real database systems (Marcus et al., 2019; Yang et al., 2022; Krishnan et al., 2018), which execute queries online and use runtime costs as rewards, our environment is lightweight (i.e., it uses offline traces) and focuses on the fundamental challenge in query optimization: cardinality estimation. With the JoinGym environment, users can simulate millions of large joins within just a few minutes, whereas it takes a few hours (e.g., q24_20, q24_98), even a few days (e.g., q29_44, q29_80) to run some queries with the default configuration in PostgreSQL, which makes the other system slow and expensive.\\n\\nWhy IR cardinality is a good proxy for the cost of a query plan?\\n1. Lohman (2014) puts it eloquently, \u201cThe root of all evil, the Achilles Heel of query optimization, is the estimation of the size of intermediate results, known as cardinalities. In my experience, the cost model may introduce errors of at most 30% for a given cardinality, but the cardinality model can quite easily introduce errors of many orders of magnitude! Let\u2019s attack problems that really matter, those that account for optimizer disasters, and stop polishing the round ball.\u201d\\n2. Neumann & Radke (2018) adopts IR cardinalities as the key metric for benchmarking query optimization algorithms. Numerous database papers focus on enhancing cardinality estimation with sketching/statistical methods (Kipf et al., 2019) and neural models (Kipf et al., 2018). Also, many database theory works (Atserias et al., 2013; Ngo et al., 2018) focus on designing new algorithms to minimize the size of the intermediate result.\\n3. Furthermore, the IR cardinality metric provides computational advantages, which we leverage: (i) IR cardinality does not depend on specific system configurations (e.g., IO and CPU); (ii) IR cardinality is deterministic so we can pre-compute it for all our queries; (iii) with our pre-computed dataset, users can simulate millions of large joins within just a few minutes.\\n\\nIn sum, IR cardinality is the most common and important metric for query optimization algorithms. At the same time, its system-independent and deterministic nature allows us to design a realistic environment that is lightweight, enabling ML & RL researchers from diverse communities to collaboratively tackle the core problem in query optimization.\\n\\nState and context embeddings. We use the same selectivity encoding as Balsa (Yang et al., 2022) and Neo (Marcus et al., 2019). However, their query encoding is an adjacency matrix (at the table level) that preserves the tree structure, while we encode queries with a multi-hot vector (at the column level) marking which columns should be joined, similar to Dp (Krishnan et al., 2018) and REJOIN (Marcus & Papaemmanouil, 2018). To the best of our knowledge, marking the tree\u2019s component index in the partial plan encoding is novel and allows us to handle bushy plans without keeping track of the whole tree; except Dp, the aforementioned works only consider left-deep trees.\\n\\nNote that since the graph structure does not influence future IR sizes but column information does, our encoding is more compact than prior encoding schemes of Balsa and Neo. JOINGYM is designed so that one can easily change the state and context encoding schemes without needing to collect any more data, which is the costly step of building JOINGYM that we have already finished.\\n\\nImplementation details of JOINGYM. We now describe the specific implementation details of JOINGYM. This section is intended for advanced users who want to change how we encode state, actions or rewards. We appreciate any questions or feedback and welcome pull requests.\\n\\nOur code registers two gymnasium.Env classes that implement bushy and left-deep join plans:\\n1. JoinOptEnvBushy (in file join_optimization/envs/join_opt_env_bushy.py),\\n2. JoinOptEnvLeft (in file join_optimization/envs/join_opt_env_left.py).\\n\\nAs mentioned in Table 1, the main difference between these two environments lies in their action space; a bushy plan\u2019s actions are pairs of tables, while a left-deep plan\u2019s...\"}"}
{"id": "aAEBTnTGo3", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"actions are single tables. Since their state representations are nearly identical, both JoinOptEnvBushy and JoinOptEnvLeft subclass a base class called JoinOptEnvBase (in file join_optimization/envs/join_opt_env_base.py), which we describe first.\\n\\nJoinOptEnvBase This base class handles most of the __init__ initialization work of loading in the database schema, loading in the IR cardinality dataset, as well as constructing the selectivity encoding $v_{\\\\text{Sel}}(q)$ and goal encodings $v_{\\\\text{goal}}(q)$ (defined in Section 4.2) for all the queries $q$ in our dataset. Recall that $v_{\\\\text{Sel}}(q)$ and $v_{\\\\text{goal}}(q)$ are static during the trajectory, so we can pre-compute them when initializing the environment.\\n\\nJoinOptEnvBase also contains a helper function log_cardinality_to_reward that converts log IR cardinality at step $h$, i.e., $\\\\log c_h$, to this step's reward $r_h = 1$ if $C_{\\\\text{max}}(q)(C_{\\\\text{min}}(q) - \\\\exp(\\\\min\\\\{\\\\log c_h, \\\\log C_{\\\\text{max}}(q)\\\\}))$, where $C_{\\\\text{max}}(q) = 100 \\\\cdot C^{\\\\star}(q)$ is the optimal (minimum-possible) cumulative IR cardinality for query $q$, and $C_{\\\\text{min}}(q) = C^{\\\\star}(q) / \\\\text{num tables to join in } q$. To interpret this expression, note that $\\\\exp(\\\\min\\\\{\\\\log c_h, \\\\log C_{\\\\text{max}}(q)\\\\}) = \\\\min\\\\{c_h, C_{\\\\text{max}}(q)\\\\}$. We perform the clipping since IR cardinalities can get large, especially with Cartesian products enabled; this is also why we perform clipping inside the $\\\\exp$ and work in log-space. Next, we can interpret $P_{\\\\text{h}}c_h - C_{\\\\text{min}}(q)$ as essentially the regret of this trajectory, as $C_{\\\\text{min}}(q) \\\\cdot H = C^{\\\\star}(q)$. Finally, the scaling by $1 / C_{\\\\text{max}}(q)$ is for normalization. In essence, our reward is the per-step negative regret.\\n\\nJoinOptEnvLeft and JoinOptEnvBushy Each class has three main jobs: 1) maintaining the left-deep join tree, 2) a function to compute the partial plan encoding, 3) a function for computing the valid action masks. As for (1), since left-deep and bushy trees have different structures, we maintain them in different ways, though they both use the TreeNode data structure to do so. For (2), the partial plan encoding can be computed by examining the TreeNode so far, and only retaining the useful information. Finally, since the action spaces are different, each class has different functions for the valid action mask (3). It is worth highlighting that each class has two functions for computing the valid action mask: self.valid_action_mask() is used when Cartesian products are allowed, and self.valid_action_mask_with_heurstic() is used otherwise.\\n\\nEMECHANISM FOR GENERATING QUERIES\\n\\nWe use the predefined query templates of the Join Order Benchmark (JOB) (Leis et al., 2015) and introduce variations in unary predicates, i.e., filter statements, to generate new queries. To randomly generate realistic unary predicates, we begin by conducting a manual examination of all columns within each table to select a subset of columns that are typically used in real user queries. The columns we identified were aka_name(name), aka_title(title), char_name(name), comp_cast_type(kind), company_name(name, country_code), company_type(kind), info_type(info), keyword(keyword), kind_type(kind), link_type(link), movie_companies(note), movie_info(info), movie_info_idx(info), name(name), person_info(note), role_type(role), title(title, production_year).\\n\\nTo simulate searches by real IMDb users, we compiled the top 100 most common values for each column using ChatGPT. If any column has less than 100 unique values, we do not need to use ChatGPT and simply used all the possible values.\\n\\nFor example, consider the SQL template $q_1$, reproduced below.\\n\\n```sql\\nSELECT MIN(mmc. note) AS production_note, \\nMIN(t.title) AS movie_title, \\nMIN(t.production_year) AS movie_year\\nFROM company_type AS ct, info_type AS it, movie_companies AS mmc,\\nmovie_info_idx AS mi_idx, title AS t\\nWHERE ct.id = mmc.company_type_id\\nAND t.id = mmc.movie_id\\n```\\n\\n\"}"}
{"id": "aAEBTnTGo3", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We consider unary predicates from those candidate columns \\n\\ncompany_type(kind),\\ninfo_type(info), movie_companies(note),\\nmovie_info_idx(info),\\ntitle(title, production_year).\\n\\nFor each candidate column, we flip a coin and \\ndecide to add a unary predicate with probability \\n50%.\\n\\nSuppose that the coin flips for each column \\nwere respectively \\n1, 0, 0, 0, 0, so we only choose the \\ncompany_type(kind) column to create \\na unary predicate. Subsequently, we pick random number \\n\\\\( n \\\\sim \\\\text{Unif}(\\\\{1, 2, 3, 4, 5\\\\}) \\\\) and take \\n\\\\( n \\\\) random elements from the 'top-100 list' described above. Suppose that \\n\\\\( n = 2 \\\\) and we randomly \\nsampled 'production companies' and 'special effects companies' from the 'top-100 list' for the \\ncompany_type(kind) column. This process leads us to the resulting query \\n\\\\[\\n\\\\text{SELECT } \\\\min(\\\\text{mc.n}\\text{ote}) \\\\text{AS production_note,}\\n\\\\text{MIN(t.title) AS movie_title,}\\n\\\\text{MIN(t.production_year) AS movie_year}\\n\\\\text{FROM company_type AS ct, info_type AS it, movie_companies AS mc, movie_info_idx AS mi_idx, title AS t}\\n\\\\text{WHERE ct.id = mc.company_type_id}\\n\\\\text{AND t.id = mc.movie_id}\\n\\\\text{AND t.id = mi_idx.movie_id}\\n\\\\text{AND mc.movie_id = mi_idx.movie_id}\\n\\\\text{AND it.id = mi_idx.info_type_id}\\n\\\\text{AND ct.kind in ('production companies', 'special effects companies')}\\n\\\\]\\n\\nNote the key addition is the last filter statement, \\n\\\\( \\\\text{AND ct.kind in ('production}\\n\\\\text{companies', 'special effects companies')} \\\\). We repeat this procedure \\n99 more \\ntimes to produce \\n\\\\( q_1_0, \\\\ldots, q_1_{99} \\\\). We repeat the above for the \\n32 remaining templates \\n\\\\( q_2, \\\\ldots, q_{33} \\\\), which yields the \\n\\\\( 100 \\\\times 33 = 3300 \\\\) random queries that make up our new dataset.\\n\\n**FLIMITATIONS**\\n\\nMultiple base tables in a query. \\nOur current solution is to introduce duplicate tables and treat \\ntables from the same basetables differently. Given query templates, our encoding has \\n\\\\( n \\\\) positions \\nfor a basetable, where \\n\\\\( n \\\\) is the maximum number of times this basetable appears among all query \\ntemplates. We assume that query templates are fixed. We acknowledge that this solution may not be \\nelegant and can be improved in future work.\\n\\nDynamic workload. \\nIn this benchmark, we assume the RL agent is trained and evaluated on the \\nsame database, \\n\\\\( \\\\text{i.e., we assume the DB content is kept static as in prior works Yang et al. (2022).} \\\\)\\n\\nHowever, in real applications, the database may dynamically change over time. It is possible to add \\nmore queries and databases to \\\\text{JOIN} \\\\text{GYM} by simply running our data collection script to collect more \\ncardinality data.\\n\\n**ADDITIONAL RESULTS FOR ONLINE RL**\\n\\nThe following tables show the mean, \\n\\\\( p_{90}, p_{95}, p_{99} \\\\), results with standard error confidence intervals \\ncomputed over \\n10 seeds.\"}"}
{"id": "aAEBTnTGo3", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: The results of Table 2 with standard error computed over 10 seeds.\"}"}
{"id": "aAEBTnTGo3", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Join order selection (JOS) is the problem of ordering join operations to minimize total query execution cost and it is the core NP-hard combinatorial optimization problem of query optimization. In this paper, we present JOINGYM, a lightweight and easy-to-use query optimization environment for reinforcement learning (RL) that captures both the left-deep and bushy variants of the JOS problem. Compared to existing query optimization environments, the key advantages of JOINGYM are usability and significantly higher throughput which we accomplish by simulating query executions entirely offline. Under the hood, JOINGYM simulates a query plan's cost by looking up intermediate result cardinalities from a pre-computed dataset. We release a novel cardinality dataset for 3300 SQL queries based on real IMDb workloads which may be of independent interest, e.g., for cardinality estimation. Finally, we extensively benchmark four RL algorithms and find that their cost distributions are heavy-tailed, which motivates future work in risk-sensitive RL. In sum, JOINGYM enables users to rapidly prototype RL algorithms on realistic database problems without needing to setup and run live systems.\\n\\n1 Introduction\\n\\nDeep reinforcement learning (RL) has achieved many successes in games (Bellemare et al., 2013; Cobbe et al., 2020; Schrittwieser et al., 2020) and robotics simulators (Tassa et al., 2018; Freeman et al., 2021), which has driven most of the empirical research in RL. Beyond these settings, RL has the potential to greatly impact many other real-world domains such as congestion control (Tessler et al., 2022), job scheduling (Mao et al., 2016) and database query optimization (Marcus et al., 2019; Yang, 2022; Lim et al., 2023), which is the focus of this paper. Unfortunately most systems applications do not have realistic simulators and training occurs online in a live system. This makes RL research in these domains prohibitively expensive for most labs, which is not inclusive and slows down progress. It is thus crucial to develop realistic and efficient simulators to accelerate RL research for data systems problems. In this work, we present JOINGYM, the first lightweight and easy-to-use simulator for query optimization that can simulate real-world data management problems without needing to setup live systems.\\n\\nIn database query optimization, join order selection (JOS; a.k.a. join order optimization or access path selection) is the NP-hard combinatorial optimization problem of finding the query execution plan of minimum cost. Given the ubiquity of databases, the JOS problem is very practically important and this has motivated many works in search heuristics (Selinger et al., 1979; Chandra & Harel, 1980; Vardi, 1982; Cosmadakis et al., 1988) and RL (Marcus et al., 2019; Yang et al., 2022; Marcus & Papaemmanouil, 2018; Krishnan et al., 2018). The JOS problem exhibits three key challenges: (1) long-tailed return distributions, (2) generalization in discrete combinatorial problems, (3) partial observability. These challenges are common in real systems applications but are understudied since they are not captured by the popular game and robotic simulators. With JOINGYM, our aim is to provide a lightweight yet realistic simulator that can motivate methodological innovations in these three underexplored areas of RL.\\n\\nThe novel idea that makes JOINGYM so efficient is to simulate query executions completely offline by looking up the size of intermediate tables from join sequences. These intermediate result (IR) cardinalities can be pre-computed since they are deterministic and system-agnostic. Along with JOINGYM, we also release a new dataset of IR cardinalities for 3300 queries on the Internet Movie Database.\"}"}
{"id": "aAEBTnTGo3", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Albert Atserias, Martin Grohe, and D\u00e1niel Marx. Size bounds and query plans for relational joins. *SIAM Journal on Computing*, 42(4):1737\u20131767, 2013.\\n\\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. *Journal of Artificial Intelligence Research*, 47:253\u2013279, 2013.\\n\\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. *arXiv preprint arXiv:1606.01540*, 2016.\\n\\nAshok K Chandra and David Harel. Structure and complexity of relational queries. In *21st Annual Symposium on Foundations of Computer Science*, pp. 333\u2013347. IEEE, 1980.\\n\\nYinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust decision-making: a cvar optimization approach. *Advances in neural information processing systems*, 28, 2015.\\n\\nKarl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. In *International conference on machine learning*, pp. 2048\u20132056. PMLR, 2020.\\n\\nStavros Cosmadakis, Haim Gaifman, Paris Kanellakis, and Moshe Vardi. Decidable optimization problems for database logic programs. In *Proceedings of the twentieth annual ACM symposium on Theory of computing*, pp. 477\u2013490, 1988.\\n\\nFarama Foundation. Gymnasium, 2023. URL https://github.com/Farama-Foundation/Gymnasium.\\n\\nC Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. Brax\u2014a differentiable physics engine for large scale rigid body simulation. *arXiv preprint arXiv:2106.13281*, 2021.\\n\\nScott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In *International conference on machine learning*, pp. 1587\u20131596. PMLR, 2018.\\n\\nKarthick Prasad Gunasekaran, Kajal Tiwari, and Rachana Acharya. Deep learning based auto tuning for database management system. *arXiv preprint arXiv:2304.12747*, 2023.\\n\\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In *International conference on machine learning*, pp. 1861\u20131870. PMLR, 2018.\\n\\nYuxing Han, Ziniu Wu, Peizhi Wu, Rong Zhu, Jingyi Yang, Liang Wei Tan, Kai Zeng, Gao Cong, Yanzhao Qin, Andreas Pfadler, et al. Cardinality estimation in dbms: A comprehensive benchmark evaluation. *arXiv preprint arXiv:2109.05877*, 2021.\\n\\nShengyi Huang and Santiago Onta\u00f1\u00f3n. A closer look at invalid action masking in policy gradient algorithms. In *The International FLAIRS Conference Proceedings*, volume 35, 2022.\\n\\nToshihide Ibaraki and Tiko Kameda. On the optimal nesting order for computing n-relational joins. *ACM Transactions on Database Systems (TODS)*, 9(3):482\u2013502, 1984.\\n\\nAndreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter Boncz, and Alfons Kemper. Learned cardinalities: Estimating correlated joins with deep learning. *arXiv preprint arXiv:1809.00677*, 2018.\\n\\nAndreas Kipf, Dimitri Vorona, Jonas M\u00fcller, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter Boncz, Thomas Neumann, and Alfons Kemper. Estimating cardinalities with deep sketches. In *Proceedings of the 2019 International Conference on Management of Data*, pp. 1937\u20131940, 2019.\\n\\nSanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph Hellerstein, and Ion Stoica. Learning to optimize join queries with deep reinforcement learning. *arXiv preprint arXiv:1808.03196*, 2018.\"}"}
{"id": "aAEBTnTGo3", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "aAEBTnTGo3", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nJennifer Ortiz, Magdalena Balazinska, Johannes Gehrke, and S Sathiya Keerthi. Learning state representations for query optimization with deep reinforcement learning. In Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning, pp. 1\u20134, 2018.\\n\\nRaghu Ramakrishnan and Johannes Gehrke. Database management systems, volume 3. McGraw-Hill New York, 2003.\\n\\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.\\n\\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604\u2013609, 2020.\\n\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\n\\nP Griffiths Selinger, Morton M Astrahan, Donald D Chamberlin, Raymond A Lorie, and Thomas G Price. Access path selection in a relational database management system. In Proceedings of the 1979 ACM SIGMOD international conference on Management of data, pp. 23\u201334, 1979.\\n\\nYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.\\n\\nChen Tessler, Yuval Shpigelman, Gal Dalal, Amit Mandelbaum, Doron Haritan Kazakov, Benjamin Fuhrer, Gal Chechik, and Shie Mannor. Reinforcement learning for datacenter congestion control. ACM SIGMETRICS Performance Evaluation Review, 49(2):43\u201346, 2022.\\n\\nImmanuel Trummer, Junxiong Wang, Ziyun Wei, Deepak Maram, Samuel Moseley, Saehan Jo, Joseph Antonakakis, and Ankush Rayabhari. Skinnerdb: Regret-bounded query evaluation via reinforcement learning. ACM Transactions on Database Systems (TODS), 46(3):1\u201345, 2021.\\n\\nN\u00faria Armengol Urp\u00ed, Sebastian Curi, and Andreas Krause. Risk-averse offline reinforcement learning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=TBIzh9b5eaz.\\n\\nHado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.\\n\\nBennet Vance and David Maier. Rapid bushy join-order optimization with cartesian products. ACM SIGMOD Record, 25(2):35\u201346, 1996.\\n\\nMoshe Y Vardi. The complexity of relational query languages. In Proceedings of the fourteenth annual ACM symposium on Theory of computing, pp. 137\u2013146, 1982.\\n\\nKaiwen Wang, Nathan Kallus, and Wen Sun. Near-minimax-optimal risk-sensitive reinforcement learning with cvar. International Conference on Machine Learning, 2023.\\n\\nZongheng Yang. Machine Learning for Query Optimization. PhD thesis, EECS Department, University of California, Berkeley, Aug 2022. URL http://www2.eecs.berkeley.edu/Pubs/TechRpts/2022/EECS-2022-194.html.\\n\\nZongheng Yang, Wei-Lin Chiang, Sifei Luan, Gautam Mittal, Michael Luo, and Ion Stoica. Balsa: Learning a query optimizer without expert demonstrations. In Proceedings of the 2022 International Conference on Management of Data, pp. 931\u2013944, 2022.\"}"}
{"id": "aAEBTnTGo3", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: List of Abbreviations\\n\\n| Abbreviation | Description |\\n|--------------|-------------|\\n| JOS          | Join Order Selection |\\n| DB           | Database     |\\n| IR           | Intermediate result table |\\n| CP           | Cartesian product join |\\n| JOB          | Join Order Benchmark (Leis et al., 2015) |\\n| CCM          | Cumulative Cost Multiple |\\n\\nTable 4: List of Notations\\n\\n| Notation | Description |\\n|----------|-------------|\\n| $N_{tables}$ | Number of tables in the DB |\\n| $N_{cols}(T)$ | Number of columns in table $T$ |\\n| $N_{cols}$ | Total number of columns amongst all tables in DB |\\n| $\\\\bigotimes$ $\\\\bigodot$ | Binary join operator, defined in Eq. (1) |\\n\\nSearch space of Left-deep vs. Bushy plans.\\n\\nRecall that left-deep plans only allow for left-deep join trees, while bushy plans allow for arbitrary binary trees. We can compute the size of the search space for both types of plans, which is a simple exercise in combinatorics. With $|I|$ tables, there are $|I|!$ possible left-deep plans and $|I|! - C_{|I| - 1}$ bushy plans, where the $k$-th Catalan number $C_k$ is the number of unlabeled binary trees with $k + 1$ leafs. By Stirling's approximation, $n! \\\\approx \\\\Theta(\\\\sqrt{n}(ne)^n)$ and $n! - C_{n-1} \\\\approx \\\\Theta(n-1(4ne)^n)$. The middle of Fig. 5 illustrates the exponential growth of two different search spaces (i.e., left-deep search space and bushy search space) as the number of join tables increases. The bushy search space exhibits even faster growth compared to the left-deep plans. In our most challenging query template, search space exceeds $10^{15}$ for left-deep plans and surpasses $10^{23}$ for bushy plans. While left-deep plans usually suffice for fast query execution, bushy plans can sometimes yield join plans with smaller IR cardinalities and faster runtime (Leis et al., 2015).\\n\\nStatistics of queries in JOINGYM.\\n\\nThe left of Fig. 5 shows the distribution of the number of join tables in JOINGYM. More than half of the queries join at least nine tables. The right figure shows the sorted cumulative IR sizes of different join orders for the same representative query. The left-most point is the optimal plan. The sharp jump in cardinalities from the optimal illustrates the key challenge of query optimization.\"}"}
{"id": "aAEBTnTGo3", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: The best algorithm (in terms of CCM mean) for left-deep plans with CPs enabled is TD3 with policy learning rate $0.0001$ and critic learning rate $0.0003$. Shaded region is stderr over $10$ runs.\"}"}
{"id": "aAEBTnTGo3", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We also plot results for a heuristic \\\\textit{ikkbzbushy} which builds join selectivity estimates based on the training data and uses dynamic programming (DP) to compute the best bushy plan according to the estimated IR cardinalities (based on the join selectivity estimates). Similar with RL agents which learns in training queries, we build selectivity estimation of join predicates using training queries. We follow the most classic approach (Ramakrishnan & Gehrke, 2003) for estimating selectivities of each join pair $R \\\\bowtie P \\\\leftarrow S$ using $\\\\text{Sel}_{i}(R \\\\bowtie P \\\\leftarrow S) = \\\\frac{|R \\\\bowtie P \\\\leftarrow S|}{|R| |S|}$ and take the average among all queries as the final selectivities. And in the validate and test, we estimate the IR size using $\\\\text{Sel}_{i}(R \\\\bowtie P \\\\leftarrow S)\\\\frac{|R| |S|}{\\\\text{IR}}$. \\\\textit{ikkbzbushy} can guarantee that the final plan has the smallest estimation cost. So amongst all heuristics that use the same estimated IR cardinalities, this approach is the best possible heuristic since it uses the plan with the smallest estimated cost. However, since the selectivity estimation step is biased, the final performance is very poor, and worse than the RL-based approaches. It's worth mentioning that DP-based approaches can take hours on med-large size queries since the problem space grows exponentially. In contrast, RL methods are much faster to run.\\n\\nFigure 10: Distribution of cost multiples for the heuristic. The mean performance is train $7.8e+49$, val $1.1e+50$ and test $2.5e+45$. Notice that these are many orders of magnitude worse than the RL policy's performance.\"}"}
{"id": "aAEBTnTGo3", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We now benchmark offline RL algorithms on optimizing left-deep plans with CPs disallowed. In this section, we focus on the 113 queries from the JOB (Leis et al., 2015) which is a smaller and easier setting than our main dataset of 3300 queries. We focus on the JOB for offline RL because Leis et al. (2015) provided behavior policy traces for all 113 queries, based on popular search heuristics (described below).\\n\\n### JOB Data\\n\\n| Algorithm | Median | STD  |\\n|-----------|--------|------|\\n| DQN       | 3.22   | 1.9e6|\\n| DDQN      | 1471   | 3.4e10|\\n| BCQ       | 7.6e+11| 3.1e3|\\n| BCQ       | 2.7e13 | 79   |\\n| CQL       | 3.19   | 1.9e6|\\n| CQL       | 1470   | 3.3e10|\\n| CQL       | 8.0e4  | 7.3e12|\\n| CQL       | 9908   | 2.6e13|\\n| CQL       | 76     | 4.1e12|\\n\\nTable 7: CCM (lower is better) averaged over the training (trn), validation (val) or testing (tst) query sets.\\n\\n### Experimental Setup\\n\\nOur offline dataset is comprised of trajectories from the following behavior policies provided by the JOB (Leis et al., 2015): adaptive, dphyp, genetic, goo, goodp, goodp2, gooikkbz, ikkbz, ikkbzbushy, minsel, quickpick, simplification (Neumann & Radke, 2018). We highlight that these behavior policies are search heuristics, which operate given a cost model, e.g., estimated IR cardinalities, to plan over. To generate behavior trajectories, we provided these heuristics access to the ground-truth IR cardinalities. Alternatively, one could take traces from existing DBMS such as PostgreSQL. For each heuristic, we collected 1000 trajectories across different queries. We partition the dataset for training, evaluation and testing similarly as in our online experiments.\\n\\n### Discussions\\n\\nTable 7 summarizes our offline results. We find that DQN has the best performance in terms of median and the validation/testing results are even better than online. CQL also obtains reasonable performance, but all other methods seem to have relatively poor median performance even on the training set. It's worth noting that all methods seem to have a heavy tail performance distribution (over queries), as shown by the large standard deviations. In later sections of the appendix, we see this is the case for online RL as well. This heavy-tail distribution of returns motivates applying risk-sensitive RL methods to JOIN for future work.\\n\\nWe also tested on some other offline algorithms, such as SAC, and it is hard to converge hence we didn't report the results. We observe that the TD error is increasing, although the Q value functions, actors and critics are learning. Making too many TD updates to the Q-function in offline deep RL is known to sometimes lead to performance degradation and unlearning, we can use regularization to address the issue (Kumar et al., 2021).\\n\\n### Hyperparameters for Offline RL Algorithms\\n\\nWe performed hyperparameter search with grid search and Bayesian optimization. The final parameters we used for evaluation is shown below in Tables 9-12.\\n\\n#### H.1.1 Batch-Constrained Q-Learning\\n\\nTable 8: Hyperparameter of Batch-Constrained Q-learning algorithm (BCQ).\\n\\n| Hyperparameter       | Value       |\\n|----------------------|-------------|\\n| Learning rate        | 6.25\u00d710\u207b\u2075   |\\n| Optimizer            | Adam (\u03b2=(0.95, 0.999)) |\\n| Batch size           | 32          |\\n| Number of critics    | 6           |\\n| Discount factor      | 0.99        |\\n| Target network       | 0.005       |\\n| Action flexibility   | 0.3         |\\n| Gamma                | 0.99        |\"}"}
{"id": "aAEBTnTGo3", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 9: Hyperparameter of Behavior Cloning (BC).\\n\\n| Hyperparameter     | Value                        |\\n|--------------------|------------------------------|\\n| Learning rate      | 0.001                        |\\n| Optimizer          | Adam ($\\\\beta = (0.9, 0.999)$) |\\n| Batch size         | 100                          |\\n| Beta               | 0.5                          |\\n\\nTable 10: Hyperparameter of Conservative Q-Learning (CQL).\\n\\n| Hyperparameter                                | Value                        |\\n|-----------------------------------------------|------------------------------|\\n| Actor learning rate                           | $3 \\\\times 10^{-4}$          |\\n| Critic learning rate                          | $3 \\\\times 10^{-4}$          |\\n| Learning rate for temperature parameter of SAC| $1 \\\\times 10^{-4}$          |\\n| Learning rate for alpha                       | $1 \\\\times 10^{-4}$          |\\n| Batch size                                    | 256                          |\\n| N-step TD calculation                         | 1                            |\\n| Discount factor                                | 0.99                         |\\n| Target network synchronization coefficient     | 0.005                        |\\n| The number of Q functions for ensemble        | 2                            |\\n| Initial temperature value                     | 1.0                          |\\n| Initial alpha value                           | 1.0                          |\\n| Threshold value                                | 10.0                         |\\n| Constant weight to scale conservative loss    | 5.0                          |\\n| The number of sampled actions to compute      | 10                           |\\n\\nTable 11: Hyperparameter of DQN.\\n\\n| Hyperparameter   | Value   |\\n|------------------|---------|\\n| Learning rate    | $6.25e^{-4}$ |\\n| Batch size       | 32      |\\n| Target update interval | 8000   |\\n\\nTable 12: Hyperparameter of DDQN.\\n\\n| Hyperparameter   | Value   |\\n|------------------|---------|\\n| Learning rate    | $6.25e^{-4}$ |\\n| Batch size       | 32      |\\n| Target update interval | 8000   |\"}"}
{"id": "aAEBTnTGo3", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our query set contains 100 queries for each of the 33 templates that reflect diverse user interests about movies; this is 30\u00d7 larger and more diverse than the Join Order Benchmark (JOB; Leis et al., 2015). One trade-off of our offline approach is that cumulative IR cardinality is a proxy for the online runtime metrics that end users care about, e.g., query latency or resource consumption. However, it is well-established that minimizing IR cardinalities is by far the problem of query optimization that has the largest impact on execution cost (Lohman, 2014; Leis et al., 2015; Neumann & Radke, 2018; Kipf et al., 2019; Trummer et al., 2021). In particular, Lohman (2014) observed that bad cost models typically account for at most 30% degradation in runtime metrics, while high IR cardinalities can cause metrics to blow up by many orders of magnitude. Importantly, focusing on cardinality minimization affords computational advantages that we leverage: IR cardinalities are deterministic and system-agnostic so they can be pre-computed. In contrast, runtime metrics are system-dependent and can only be obtained from live query executions which are slow and costly. For example, large queries (e.g., \\\\( q_{29} \\\\sim 44 \\\\), \\\\( q_{29} \\\\sim 80 \\\\) in JOIN) can take days to process on a commercial database, especially if the RL policy selects a sub-optimal join order, while JOIN can simulate thousands of queries per second on a personal laptop. Thus, by reducing query optimization to its absolute core, we can provide a realistic lightweight simulator that is practically useful for RL research.\\n\\nWe now briefly overview the paper. In Section 3, we mathematically formulate query optimization and recall the four variants of the JOS problem that are most often used in practice. Namely, JOIN supports both left-deep and bushy plans as well as enabling or disabling Cartesian products, which are common heuristics to reduce search space at the slight cost of optimality. Then in Section 4, we formulate JOS as a Partially Observable Contextual Markov Decision Process (POCMDP) and describe our design choices for the state, action, and reward. The context is partially observable because query embeddings are lossy compressions of base table contents and hence cannot fully determine the IR cardinalities. Finally, in Section 5, we perform an extensive benchmarking of standard RL algorithms on JOIN. We observe that return distributions are long-tailed and that generalization can be improved, which motivates further methodological RL research for problems in database query optimization and beyond.\\n\\nOur main contributions are summarized as follows:\\n\\n1. We provide the first lightweight simulator for query optimization that enables inexpensive and inclusive RL research on said problem.\\n2. We release a novel dataset containing all IR cardinalities of 3300 queries on IMDb, which is used by JOIN and may be of independent interest, e.g., cardinality estimation.\\n3. We extensively benchmark RL algorithms on JOIN and find that existing algorithms can generalize well at the 90% quantile. However, we observe that their generalization sharply worsens due to the long-tailed cost distribution. This motivates future research to address the key challenges of query optimization: 1) long-tailed returns, 2) generalization in combinatorial optimization, 3) partial observability.\\n\\nIn sum, JOIN is an efficient and realistic environment that enables rapid prototyping of algorithms for query optimization. Our aim is to make query optimization accessible to the ML&RL communities and to accelerate methodological research in the intersection of data systems and ML&RL.\\n\\n2 RELATED WORKS\\n\\nEnvironments for Query Optimization.\\n\\nThe query optimization environment from Park (Mao et al., 2019) require a DBMS backend (e.g., PostgreSQL or Apache Calcite) and setting this up correctly can already be nontrivial. Their costs are computed by either querying the DBMS's cost model, which takes seconds per step, or online query execution time, which can take hours per step. Meanwhile, our costs are computed by looking up IR cardinalities and JOIN can simulate thousands of trajectories per second on a standard laptop, which is orders of magnitude faster than Park's environment. Also, Park only supports the 113 queries of JOB, while JOIN supports 3300 queries that we selected for diverse human searches based on templates from JOB. Lim et al. (2023) describes the architecture of DB-Gym, which like Park also runs in a real DBMS and thus can be very costly.\"}"}
{"id": "aAEBTnTGo3", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The IR cardinality generally depends on the selectivity of base tables and the correlation of joined columns. JOS is the problem of finding a feasible join order that achieves the minimum total size of all IRs.\\n\\nImproving Query Optimization with RL.\\n\\nAll join orders are expressible as a binary tree where the leaves are matches the first table, while the internal nodes are the IRs from joining its two children. In left-deep plans, iteratively join base tables with the IR of cumulative joins so far, which is represented by a left-deep tree. Bushy plans allow for any possible binary tree; bushy vs. left-deep plans.\\n\\nBushy plans are disallowed in IRs in left-deep plans, which usually suffice for left-deep plans. However, there are different ways to execute the same query as different sequences of binary joins. For example, if the query matches the filter predicate of rows in a table, we restrict the plans to those trees where each internal node represents the IR from joining its two children. In left-deep plans, iteratively join base tables with the IR of cumulative joins so far, which is represented by a left-deep tree. Bushy plans allow for any possible binary tree; bushy vs. left-deep plans.\\n\\nThe former involves the intermediate results of IRs, while the latter involves the IRs of IRs. Intermediate results are either (1) long-tailed (Yang, 2022) or (2) generalization (Cobbe et al., 2020). For (2), Procgen (Chow et al., 2015; Ma et al., 2021; Wang et al., 2023) provides a suite of games and navigation environments that are specifically designed for testing RL generalization by using procedural generated video games. For (1), there are no RL environments based on realistic problems with long-tailed returns, (2) generalization and (3) partial observability. For (1), there are no RL environments based on realistic problems with long-tailed returns.\"}"}
{"id": "aAEBTnTGo3", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Components\\n\\n- Left-deep JOIN\\n- Bushy JOIN\\n\\nContext\\n\\n- Query encoding described in Section 4.2.\\n\\nState\\n\\n- Partial plan encoding described in Section 4.2.\\n\\nAction\\n\\n- Table to join, from Discrete(N tables)\\n- Edge to join, from Discrete(N tables^2)\\n\\nReward\\n\\n- Negative step-wise regret: \\\\( r_h \\\\propto C^{\\\\star}_{\\\\text{plan_type}} / H - c_h \\\\) for plan_type \\\\( \\\\in \\\\{ \\\\text{bushy}, \\\\text{left-deep} \\\\} \\\\)\\n\\nTransition\\n\\n- Deterministic transition of dynamic state features, described in Section 4.2.\\n\\nHorizon\\n\\n- \\\\( |I| - 1 \\\\)\\n\\nTable 1: Components of JOIN for deciding the join order of a single query \\\\( q = (I, U, J) \\\\).\\n\\nBetween the left-deep and bushy variants, the key difference is their actions (table vs edge).\\n\\nDisabling Cartesian Products to Reduce Search Space.\\n\\n- CPs are expensive join operations where no constraints are placed on the column values, i.e., the CP between \\\\( R \\\\) and \\\\( S \\\\) is \\\\( R \\\\bowtie \\\\emptyset S = \\\\{ r \\\\cup s | r \\\\in R, s \\\\in S \\\\} \\\\), which always has cardinality equal to the product of the two source tables\u2019 cardinalities.\\n- Disabling (i.e., avoiding) Cartesian products (CPs) has been a popular heuristic to reduce search space size by trading off optimality (Ramakrishnan & Gehrke, 2003).\\n- CP is indeed the most expensive binary join, but they can actually sometimes lead to smaller cumulative costs, e.g., it may be beneficial to CP two small tables before joining a large table (Vance & Maier, 1996).\\n\\n4 JOIN: A DATABASE QUERY OPTIMIZATION ENVIRONMENT\\n\\nWe formulate JOIN as a Partially Observable Contextual Markov Decision Process (POCMDP), and then describe how we encode the context and observation that is implemented by JOIN.\\n\\n4.1 PARTIALLY OBSERVABLE CONTEXTUAL MDP FORMULATION\\n\\nSelecting the join order naturally fits into the POCMDP framework, since it is a sequence of actions (joins) and per-step costs (IR sizes) with the goal of minimizing the cumulative cost. This can be formulated as a (partially observable) contextual MDP, consisting of context space \\\\( X \\\\), state space \\\\( S \\\\), finite action space \\\\( A \\\\), horizon \\\\( H \\\\), transition kernels \\\\( P(s'|s,a) \\\\), and contextual reward functions \\\\( r(s,a;x) \\\\), where \\\\( s, s' \\\\in S \\\\), \\\\( a \\\\in A \\\\), \\\\( x \\\\in X \\\\).\\n\\nFirst, the context \\\\( x \\\\in X \\\\) is a fixed encoding of the query \\\\( q \\\\). The trajectory is a join plan for this query and is generated as follows. At time \\\\( h \\\\in [H] \\\\), the state \\\\( s_h \\\\) is the partial join plan that has been executed so far, and the action \\\\( a_h \\\\) specifies the join operation to perform now. For bushy plans, \\\\( a_h \\\\) can be any valid edge in the join graph; for left-deep plans, \\\\( a_h \\\\) is simply the \\\\( h \\\\)-th table to add to the left-deep tree. Performing the join specified by \\\\( a_h \\\\) results in an IR, whose cardinality is the cost \\\\( c_h \\\\). We define the reward as the negative step-wise regret \\\\( r_h \\\\propto C^{\\\\star}_{\\\\text{plan_type}} / H - c_h \\\\), where \\\\( C^{\\\\star}_{\\\\text{method}} \\\\) is the minimum cumulative cost, for plan_type \\\\( \\\\in \\\\{ \\\\text{bushy}, \\\\text{left-deep} \\\\} \\\\).\\n\\nFor numerical stability, we normalize our reward and clip each \\\\( c_h \\\\) by \\\\( 10^2 C^{\\\\star}_{\\\\text{plan_type}} \\\\). While \\\\( r_h \\\\) can be either negative or positive, the cumulative reward is non-positive with zero being optimal. This procedure iterates until all queried tables are joined. For bushy plans, the horizon is the number of joins, i.e., \\\\( H = |J| = |I| - 1 \\\\); for left-deep plans, the \\\\( a_1 \\\\) corresponds to staging the first table, which does not perform any joins, and so \\\\( r_1 = 0 \\\\) and \\\\( H = |J| + 1 = |I| \\\\).\\n\\nTable 1 summarizes this setup.\\n\\nRemarks.\\n\\n- First, the set of legal actions shrinks throughout the trajectory as the policy cannot choose joins that have already been selected, i.e., \\\\( A = A_1 \\\\supset A_2 \\\\supset \\\\cdots \\\\supset A_H \\\\). We handle this by action masking (Huang & Onta\u00f1\u00f3n, 2022), where we constrain the policy\u2019s action selection and update rules to only consider legal actions at each step.\\n- Second, the transition and reward dynamics are deterministic and stochasticity comes from the context since queries may be sampled from some another approach would be to give a large cost for choosing unavailable actions, but this would require the agent to learn to avoid such actions. We avoid this unnecessary learning with action masking.\"}"}
{"id": "aAEBTnTGo3", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Partial Observability.\\n\\nRecall that each query $q$ is encoded by a context $x$ which in particular encodes the base tables. Partial observability arises when this encoding of the base tables is lossy, i.e., the encoding is not fully predictive of the IR cardinalities. Note that the Markov property still holds for the non-contextual transition kernel and only the reward is affected by partial observability. In practical applications where base tables contain millions of rows, any tractable encoding will necessarily be lossy. The best representation for tables is still an open research question (Ortiz et al., 2018; Marcus et al., 2019; Yang, 2022). JOB can be easily modified to handle new table embeddings by changing a few lines of code. Our dataset of IR cardinalities remains valid for any embedding scheme as IR cardinalities are agnostic to the table representation.\"}"}
{"id": "aAEBTnTGo3", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Given a query $q$, we represent the context $x = (v_{Sel}(q), v_{goal}(q))$ with two components that are fixed for this query. The first part is the selectivity encoding, a vector $v_{Sel}(q) \\\\in [0, 1]^N_{tables}$ where the $t$-th entry is the selectivity of $u_t$ if $t \\\\in I$, and 0 otherwise:\\n\\n$$\\\\forall t \\\\in [N_{tables}] : v_{Sel}(q)[t] = \\\\begin{cases} \\\\text{Sel}_t, & \\\\text{if } t \\\\in I, \\\\\\\\ 0, & \\\\text{o/w}. \\\\end{cases}$$\\n\\nThe second part is the query encoding, a binary vector $v_{goal}(q) \\\\in \\\\{0, 1\\\\}^{N_{cols}}$ representing which columns need to be joined in this query; the $c$-th entry is 1 if column $c$ appeared in any join predicate, and 0 otherwise:\\n\\n$$\\\\forall c \\\\in [N_{cols}] : v_{goal}(q)[c] = \\\\begin{cases} 1, & \\\\exists (R, S, P) \\\\in J, \\\\exists p \\\\in P: c = p[0] \\\\lor c = p[1] \\\\\\\\ 0, & \\\\text{otherwise}. \\\\end{cases}$$\\n\\nAt time $h \\\\in [H]$, we represent the state $s_h$ as the partial plan encoding $v_{pp}h$, which represents the joins specified by prior actions $a_{1:h-1}$. Two examples are given in Fig. 1(d). The partial plan encoding is a vector $v_{pp}h \\\\in \\\\{-1, 0, 1, 2, \\\\ldots\\\\}^{N_{cols}}$ where the $c$-th entry is positive if column $c$ has already been joined, $-1$ if the table of column $c$ has been joined or selected but column $c$ has not been joined yet, and 0 otherwise. If column $c$ has already been joined, the $c$-th entry will be the index of its join-tree in the forest; in left-deep plans, the index will always be 1 since there is always only one join-tree, but in bushy plans with more than one tree, the index can be larger than 1. It is clearly important to mark joined columns (with positive numbers) since the policy needs to know which columns have been joined to choose the next action. In addition, we also mark unjoined columns belonging to joined or selected tables with the value $-1$. For example, in left-deep plans, we must be able to tell which table was selected by $a_1$ at $h = 2$, even though said table has only been \\\"staged\\\" but not joined. Beyond this special case, another use-case of the $-1$-marking is that it signals marked columns as part of potentially small IR; perhaps the rows of the table has been filtered from a prior join and it is better to join with this table rather than an unjoined base table. We highlight that the partial plan encoding only logs which columns have been joined/staged rather than the current join tree, because future costs only depend on the current IRs rather than the tree structure.\\n\\n4.3 Importance of Generalization in Query Optimization\\n\\nIn real applications such as IMDb, developers create query templates so that searches by a user instantiates a template with filter predicates that reflect the user's interests. A query's template determines its final query graph and different query templates may often share common subgraphs. Using the query graph structure, deep RL models can generalize to improve future query execution planning. For example in Fig. 2, the optimal join plan for (b) is a sub-tree of the optimal plan for (c). However, while queries with the same template have a common query graph, optimal join orders can vary significantly due to different filter conditions that are applied, e.g., Fig. 2 (a) & (b) are instances of the same template (and hence share the same graph) but have different optimal join orders. Thus, the key challenge in data-driven query optimization is to learn which correct query instances to mimic based on the context (i.e., filter predicates, query graph).\\n\\n4.4 Join Gym\\n\\nJOIN Gym adheres to the standard Gymnasium API (Farama Foundation, 2023), a maintained fork of OpenAI Gym (Brockman et al., 2016). The left-deep and bushy variants are registered under the environment-ids 'joinopt_left-v0' and 'joinopt_bushy-v0' respectively. JOIN Gym can be instantiated with env = gym.make(env_id, disable_cp, query_ids), where disable_cp is a flag for disabling Cartesian products (described in Section 3), and query_ids is a set of queries that will be loaded. JOIN Gym implements the abstract MDP described in Section 4.2 via the standard Gymnasium API:\\n\\n(i) state, info = env.reset(options={query_id=x}) : reset the trajectory to represent the query with id $x$, and observe the initial state. If no query_id is specified, then a random query is picked from the query set inputted to the constructor.\\n\\n(ii) next_state, reward, done, _, info = env.step(action) : perform join operation specified by action, and observe the next state and reward. done is True if and only if all tables of the current query have been joined. There is no step truncation, so we ignore the fourth output of the Gymnasium specification for env.step.\"}"}
{"id": "aAEBTnTGo3", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: On the left are three raw SQL queries. The middle are their query graphs where each edge represents a join between two tables. Filtered tables are denoted by blue nodes and CPs are denoted by dashed lines. On the right, we show tree representations of the query plan, where each leaf is a table and each node is a join of its two children tables. Queries (a) and (b) are derived from the same template and so share an identical query graph but their optimal query plans are different due to different filters on the base tables. Query (c) is from a different template, but it contains (b) as a subgraph and their optimal query plans share a common sub-tree (highlighted in red).\\n\\nAbove, state & next_state are vector representations of the partially observed state, described in Section 4.2. Moreover, info['action_mask'] is a multi-hot encoding of the valid actions $A_h$ at the current step. The RL algorithm should take this into account, e.g., masking out Q-values, so only valid actions are considered. We provide examples of how to do this in our experimental code.\\n\\n4.5 NEW DATASET OF IR CARDINALITIES\\n\\nRecall that JOINGYM contains 3300 IMDb queries, comprised of 100 queries for each of the 33 templates from the Join Order Benchmark (JOB; Leis et al., 2015). The $N$-th query from the $M$-th template has query id $q_{N \\\\_ M}$, with $N \\\\in \\\\{33\\\\}$, $M \\\\in \\\\{100\\\\}$. Our query set is 30\u00d7 larger and more diverse than the 113 queries of the JOB that were used in prior works (Mao et al., 2019). With JOINGYM, we release a new dataset containing all possible IR cardinalities for all 3300 queries, which was an exhaustive pre-computation we ran on hundreds of CPUs for weeks. We describe our query generation process in Appendix E. JOINGYM relies on this dataset to simulate query plan costs offline by simply looking up the IR cardinalities, instead of executing queries online. Beyond its use in JOINGYM, this dataset may also be of independent interest for research in cardinality estimation (Han et al., 2021) and representation learning for query and table embeddings (Ortiz et al., 2018).\"}"}
{"id": "aAEBTnTGo3", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We present benchmarking results on J O I N G Y M that measure the performance of popular RL methods on the JOS task. We're specifically interested in testing the generalization abilities of these algorithms on tasks with long-tailed returns as well as partial observability.\\n\\nExperiment Setup. Recall that our new dataset contains 100 queries for each of the 33 templates from the JOB (Leis et al., 2015). For each template, we randomly selected 60, 20, 20 queries for training (1980 queries), validation (660 queries) and testing (660 queries) respectively. We benchmarked four different RL algorithms: (i) an off-policy Q-learning algorithm Deep Q-Network (DQN) (Mnih et al., 2015); (ii-iii) two off-policy actor-critic algorithms, Twin Delayed Deep Deterministic policy gradient (TD3) (Fujimoto et al., 2018) and Soft Actor-Critic (SAC) (Haarnoja et al., 2018); and (iv) an on-policy actor-critic algorithm Proximal Policy Optimization (PPO) (Schulman et al., 2017). For DQN, we conducted an ablation with the Double Q-learning (Van Hasselt et al., 2016). For (i-iii), we conducted ablations with standard replay buffer (RB) vs. prioritized experience replay (PER) (Schaul et al., 2015).\\n\\nTraining and Evaluation. All algorithms were trained for one million steps on the training queries and the best performing algorithm was selected by the cumulative cost multiple (CCM) averaged over the validation queries. We define the cumulative cost multiple (CCM) as the cumulative IR cardinality of the join plan divided by the smallest possible cumulative IR cardinality for this query. This can be interpreted as a multiplicative regret and lower is better. We swepted over multiple learning rates per algorithm and also used the average CCM on the validation queries for selecting the best hyperparameter. For these best hyperparameters, we report the average CCM over the test queries in Table 2. As shown in Fig. 3, the CCM distributions are long-tailed so we also report the \\\\( p_{90} \\\\) CCM (90% quantile) over each query set. Our results are averaged over 10 seeds and we report the standard errors along additional \\\\( p_{95} \\\\) & \\\\( p_{99} \\\\) results in Appendix G. An example learning curve of the best algorithm (TD3) for left-deep plans with CPs disabled is shown in Fig. 4.\"}"}
{"id": "aAEBTnTGo3", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Learning curves of the best algorithm (TD3) for \u201cdisable CP & left-deep\u201d, where y-axis is the mean CCM and x-axis is the number of update steps. Shaded region is stderr over 10 runs.\\n\\nFigure 3: Complementary CDF of the CCM distribution over train, validation and test queries, of the best algorithm (TD3) for \u201cdisable CP & left-deep\u201d.\\n\\nDiscussion. 1) Algorithms uniformly perform much better in left-deep JOIN than bushy JOIN because the search space is exponentially smaller. For the same reason, algorithms uniformly perform better when CPs are disabled. The hardest setting is bushy with CPs, where most algorithms diverge and those that converge have CCMs orders of magnitude worse than the other settings.\\n\\n2) Regarding generalization, the gap between the validation and test performance is only 2\u00d7 for p90 and it increases to 10\u00d7 for p95. For p99, both the validation and test performance are significantly worse than training. This exponential widening of the generalization gap is likely explained by the fact that the long tail is exactly where partial observability is more pronounced, causing catastrophic failure in planning of the policy.\\n\\n3) Off-policy actor critic methods (TD3 & SAC) achieve the best results for the mean and are also near-optimal for the quantiles. They are more sample efficient than PPO due to their sample reuse and more stable than DQN for the query optimization task. We also find that prioritized replay does not always improve performance.\\n\\n6 CONCLUSION\\nIn this work, we presented JOINYM, the first efficient simulator for query optimization based on IR cardinalities which we hope can accelerate research in the intersection of RL and systems.\\n\\nFor the RL community, JOINYM is a realistic environment for prototyping data-driven combinatorial optimization algorithms. Notably, it exhibits specific challenges in long-tailed returns, generalization and partial observability, which are underexplored by existing simulators, e.g., Atari and MuJoCo.\\n\\nFor the systems community, this work also provides a new dataset of IR cardinalities, which can be useful for improving cardinality estimation algorithms (Yang, 2022; Han et al., 2021) and representation learning for query and table embeddings (Ortiz et al., 2018). Finally, we believe that risk-sensitive RL can play an important role in dealing with the long-tailed returns, which are common in systems applications. In our experiments, we found that standard RL algorithms can generalize well at the 90% quantile, but their performance sharply drops at higher quantiles, e.g., 95% or 99%. Risk-sensitive RL can optimize for the worst \u03c4-percent of outcomes, e.g., Conditional Value-at-Risk (CVaR), which would ensure better tail performance. Existing works in CVaR RL (Lim & Malik, 2022; Urp\u00ed et al., 2021; Ma et al., 2021) have focused on noisy versions of games and MuJoCo, so it would be promising future work to develop and test such algorithms for real systems applications.\"}"}
