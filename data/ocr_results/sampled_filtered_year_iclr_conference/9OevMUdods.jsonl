{"id": "9OevMUdods", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Understanding Actual Knowledge of Large Language Models\\n\\nXuming Hu 1,2*, Junzhe Chen 1*, Xiaochuan Li 1*, Yufei Guo 1, Lijie Wen 1\u2020, Philip S. Yu 3, Zhijiang Guo 4\u2020\\n\\n1 Tsinghua University\\n2 The Hong Kong University of Science and Technology (Guangzhou)\\n3 University of Illinois at Chicago\\n4 University of Cambridge\\n\\nxuminghu@hkust-gz.edu.cn, wenlj@tsinghua.edu.cn, zg283@cam.ac.uk\\n\\nABSTRACT\\n\\nLarge language models (LLMs) have recently driven striking performance improvements across a range of natural language processing tasks. The factual knowledge acquired during pretraining and instruction tuning can be useful in various downstream tasks, such as question answering, and language generation. Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowledge, LLMs implicitly store facts in their parameters. Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time. To this end, we aim to explore the extent and scope of factual knowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. Furthermore, we investigate whether LLMs can compose multiple facts, update factual knowledge temporally, reason over multiple pieces of facts, identify subtle factual differences, and resist adversarial examples. Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing trustworthy artificial intelligence. The dataset Pinocchio and our codes are publicly available at: https://github.com/THU-BPM/Pinocchio.\\n\\nINTRODUCTION\\n\\nLarge language models (LLMs) have revolutionized natural language processing (NLP) in recent years since they have significantly improved performance on various downstream tasks (Brown et al., 2020; Chowdhery et al., 2022; Ouyang et al., 2022; Touvron et al., 2023a;b; OpenAI, 2022; 2023). Prior efforts have shown that language models can store factual knowledge and act as knowledge bases (Petroni et al., 2019; Jiang et al., 2020c). Factual knowledge in language models acquired during pretraining can benefit knowledge-intensive downstream tasks such as question answering and fact checking (Roberts et al., 2020; Yu et al., 2023a; Pan et al., 2023).\\n\\nDespite advancements in LLMs, they still struggle with generating content that exhibits inaccuracies or deviations from the facts and making reasoning errors (Lin et al., 2022; Bubeck et al., 2023). These factual errors can be difficult to identify since LLMs implicitly memorize facts through their parameters rather than explicitly store factual knowledge as traditional Knowledge Bases. Accessing and interpreting the computations and memories of these models can be challenging (Ribeiro et al., 2016; Belinkov & Glass, 2019), especially when APIs are the only means of interaction and many interpretation methods rely on weights and representations (Cao et al., 2021b). The presence of errors in stored factual knowledge or the incorrect induction and obsolescence of certain facts over time may be contributing factors to this limitation, which in turn affects the performance of LLMs (Elazar et al., 2021; Cao et al., 2021a). This limitation restricts the application of LLMs in some high-stakes areas, such as healthcare, finance, and law (Dong et al., 2022). Hence, exploring the degree to which LLMs hold factual information and their ability to reason with such knowledge is vital.\"}"}
{"id": "9OevMUdods", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Pinocchio is a comprehensive dataset that tackles 7 distinct tasks related to factual knowledge and reasoning. It consists of 20,713 multiple-choice questions that have been sourced from various reliable and diverse channels.\\n\\nTo this end, we propose the Pinocchio, a testbed aimed at understanding factuality and reasoning for LLMs. It contains 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. Furthermore, we investigate whether LLMs are able to recognize the combination of multiple facts, reason over structured and unstructured evidence, realize facts change over time, identify subtle factual differences, and resist adversarial examples based on the dataset.\\n\\nWe control for problem difficulty in each distinct reasoning task to enable fine-grained analysis. With the Pinocchio benchmark, we explore whether various LLMs (Scao et al., 2022b; Zhang et al., 2022; Ouyang et al., 2022; Chung et al., 2022; Touvron et al., 2023a; Chiang et al., 2023) could store factual knowledge and perform reasoning based on it. We envision Pinocchio as a suite of benchmarks, subsets of which could be separately utilized to assess certain model abilities of interest and analyze important strengths and limitations of LLMs. For instance, in temporal tasks, we find that LLMs lack factual knowledge for up-to-date questions; in complex factual tasks that require multi-hop reasoning, LLMs still have limitations, even when various prompting strategies are employed. We hope Pinocchio can serve as the initial step towards understanding the abilities of LLMs from multiple dimensions and facilitate the development of LLMs.\\n\\n2. DATASET CONSTRUCTION\\n\\n2.1. TASKS\\n\\nAiming to systematically evaluate the factual knowledge and related reasoning abilities of LLMs, we raise seven research questions, then carefully select factual statements from different sources summarized in Table 1.\\n\\n\u2022 Task 1: Multifaceted\\n  Previous research (Petroni et al., 2019) has shown that small language models like BERT have the ability to retain relational knowledge from training data and answer \\\"fill-in-the-blank\\\" cloze statements. This raises the question of whether LLMs can also store and reason over multiple pieces of facts obtained during pretraining. It is not just important for LLMs to memorize individual facts accurately, but to also recognize and generate new combinations of facts from different sources. To investigate this issue, we have selected claims from the FEVER dataset (Thorne et al., 2018), which were written by human annotators based on information from Wikipedia articles. These claims are either supported or refuted by multiple facts from (the same or several) Wikipedia articles, or there is insufficient information available to verify them. To assess the performance of language models in handling various combinations of facts, we have sampled statements that require different numbers of evidence, ranging from one to many, enabling fine-grained analysis.\\n\\n\u2022 Task 2: Structural\\n  In addition to unstructured text, factual knowledge is also commonly stored in a structured format, such as tables, lists, or databases (Bhagavatula et al., 2013). However,\"}"}
{"id": "9OevMUdods", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Pinocchio Dataset Sources, Descriptions, and Data Distribution.\\n\\n| Domain Description       | Sources       | Distribution |\\n|--------------------------|---------------|--------------|\\n| Fact. Non-Fact. NEI ALL  |               |              |\\n| Multifaceted             | Contain multiple facts | FEVER  | 1,111  | 1,111  | 1,110  | 3,332  |\\n| Structural               | Contain structured and unstructured facts | FEVEROUS | 1,741  | 1,953  | 250    | 3,944  |\\n| Adversarial              | Contain facts edited by adversarial methods | Symmetric, FM2 | 815    | 921    | -      | 1,736  |\\n| Temporal                 | Contain facts that change over time | VitaminC | 1,898  | 1,043  | 355    | 3,296  |\\n| Real-World               | Contain factual statements spread online | PolitiFact | 986    | 1,987  | 609    | 3,582  |\\n| Domain-Specific          | Contain facts from health and science domains | PubHealth, SciFact | 1,156  | 715    | 737    | 2,608  |\\n| Multi-Lingual            | Contain facts in different languages | XFact, CHEF | 820    | 848    | 547    | 2,215  |\\n\\nCurrent LLMs are primarily trained on unstructured text using next word prediction loss (Brown et al., 2020; Touvron et al., 2023a). In order to process structured data, it is often converted into text strings using various methods, such as linearizing tables. This raises the question of whether LLMs are capable of effectively memorizing and reasoning over facts from structured sources, similar to their performance with unstructured text.\\n\\nTo investigate this question, we sample factual statements from the FEVEROUS dataset (Aly et al., 2021), which is constructed in a similar manner to FEVER but includes evidence in the form of tables, sentences, or both.\\n\\n- **Task 3: Adversarial**\\n  Language models are known to be vulnerable to adversarial examples that are strategically modified to deceive even advanced models with hardly noticeable changes (Shen et al., 2023). Given this knowledge, it is important to examine whether LLMs can withstand adversarial examples in the context of factuality.\\n  To investigate this, we utilize two datasets, namely Symmetric (Schuster et al., 2019) and FM2 (Eisenschlos et al., 2021). These datasets consist of adversarial examples that have been crafted using various strategies, including temporal inference and diverting to unrelated facts.\\n\\n- **Task 4: Temporal**\\n  Facts are not static but rather possess a dynamic nature. With the vast amount of new information constantly emerging, facts often undergo changes, additions, or alterations. It raises the question of whether LLMs are able to adapt to these factual changes over time. In particular, we wonder if LLMs are capable of discerning factual knowledge from different time periods, since the pretraining corpus may not be processed and organized chronologically.\\n  To explore this, we utilize the VitaminC (Schuster et al., 2021) dataset, which consists of claims based on modifications made to factual content in Wikipedia articles. Claims can be either refuted by outdated facts or supported by updated facts.\\n\\n- **Task 5: Real-World**\\n  In contrast to other tasks that assume Wikipedia has all the essential factual information, verifying viral claims on the internet often requires not only factual knowledge from various sources but also common sense and worldly knowledge. An important query we have is whether LLMs can effectively integrate diverse types and sources of knowledge acquired during training.\\n  To address this, we select claims from the FactCheck (Misra, 2022) dataset, which consists of claims spread over the Internet and subsequently verified by journalists.\\n\\n- **Task 6: Domain-Specific**\\n  In addition to the tasks mentioned earlier, which primarily focus on factual knowledge in general domains, we are also interested in exploring how LLMs possess the capability to access domain-specific factual knowledge. The domain-specific setting presents unique challenges. For example, the science domain requires LLMs to acquire background knowledge, handle quantitative reasoning, and comprehend specialized statistical language.\\n  To investigate this further, we sample claims from PubHealth (Kotonya & Toni, 2020) in the public health domain and SciFact (Wadden et al., 2022) in the science domain.\\n\\n- **Task 7: Multi-Lingual**\\n  Existing LLMs are mainly trained on English corpus because of their abundance and quality (Chowdhery et al., 2022; Touvron et al., 2023a). However, the scarcity of training data in other languages raises the question of whether LLMs can transfer the factual knowledge acquired in English to other languages.\\n  To investigate this, we collected claims from various languages including French, Chinese, and more, using the XFACT dataset (Gupta & Srikumar, 2021) and the CHEF dataset (Hu et al., 2022b) in a total of 27 different languages.\"}"}
{"id": "9OevMUdods", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Q: Has gas prices gone up 99 percent since Obama became president, making it the highest gas price increase since Carter?\\n\\nA: \\n\\nLet's think step by step.\\n\\nFigure 2: Illustration of prompts using different settings.\\n\\nHellaSwag tests commonsense reasoning, and ARC focuses on challenging questions. TruthfulQA measures how LLMs mimic human falsehoods. Furthermore, the evaluation of language generation brings its own set of challenges, as a universal metric for measurement is currently lacking (Sai et al., 2023), which multiple-choice questions help to mitigate by offering straightforward classification accuracy for assessment (Hendrycks et al., 2021b). Also, prior studies (Kadavath et al., 2022) underscore that LLMs demonstrate reliable calibration on multiple-choice scenarios. Therefore, we also used the multi-choice questions as a simple but good proxy to evaluate the abilities of LLMs.\\n\\nFor data annotation, we hired 10 undergraduate students, all with good English proficiency. We asked the students to rewrite the original claims into questions without distorting factuality while providing factuality labels for the questions. By transforming declarative statements into questions, using a Question-Answering approach can more effectively elicit factual knowledge from LLMs (Kadavath et al., 2022; Lin et al., 2022), and we also illustrate through experiments in Sec. 4.2. Note that claims in the original datasets are usually labeled based on given evidence, e.g. evidence supports or refutes the claim, but in Pinocchio, we only need to judge the factuality of the question. So we use unified labels: Yes, No, Not Sure Enough. The three labels correspond respectively to Factual, Non-Factual, and Not Enough Information for factual questions. Considering that all fact-checking datasets use a three-label system (Guo et al., 2022), we did not modify the number of labels to maintain consistency in labeling. When dealing with factuality questions in low-resource languages, for Chinese, the 5 undergraduate students we hired are native Chinese speakers. For other low-resource languages, we first use Google Translate to translate them into English and generate factuality questions, then translate the English questions back to the corresponding languages. The label distribution is shown in Table 1. We paid the annotators accordingly based on the quantity and quality of the annotations. We ensure the quality of the annotated factuality questions in two ways. The two authors of this paper served as meta-reviewers, sampling 10 questions from each of the three categories across the seven domains in Pinocchio. The meta-reviewers judged if the factuality labels were correct. For the 210 factuality questions, the average label accuracy was 92.4%. We divided the 10 students into two groups and had each group re-annotate a random 200 questions annotated by the other group, then calculated inter-annotator agreement (IAA). The final IAA was 85.6%. Based on meta-reviewer results and IAA, the factuality labels in Pinocchio are of good quality.\\n\\n3.1 Models\\n\\nTo give a comprehensive view of the status of LLMs in a factual context, we evaluate 10 accessible LLMs, undergone different training stages including pretraining, instruction tuning, and reinforcement learning from human feedback (Ouyang et al., 2022), covering diverse organizations and varying in size. A detailed description can be found in Appendix A.2.\\n\\n3.2 Prompt Strategy\\n\\nAs illustrated in Figure 2, we employ 4 types of prompts to elicit desired responses from LLMs, namely: Zero-shot, Zero-shot with CoT (Kojima et al., 2022), Few-shot, and Few-shot with CoT (Wei et al., 2022). Specifically, we begin by providing the model with task instruction, denoted as Z:\"You\"}"}
{"id": "9OevMUdods", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Results obtained using different forms of prompts on 10 accessible LLMs.\\n\\n| Methods          | Zero-shot w/o CoT | Zero-shot w/ CoT | Few-shot w/o CoT | Few-shot w/ CoT | Overall Performance |\\n|------------------|-------------------|------------------|------------------|-----------------|---------------------|\\n|                  | Accuracy F1       | Accuracy F1      | Accuracy F1      | Accuracy F1      | Accuracy F1         |\\n| OPT-6.7B         | \u2014                 | 36.9             | 27.9             | 37.9            | 28.5                |\\n| BLOOM-7B         | 29.7              | 29.7             | 14.8             | 18.1            | 20.2                |\\n| LLaMA-7B         | 31.8              | 36.8             | 22.3             | 24.9            | 31.6                |\\n| Alpaca-7B        | 40.2              | 37.9             | 33.7             | 24.4            | 37.8                |\\n| Vicuna-7B        | 33.2              | 35.5             | 34.2             | 32.9            | 34.9                |\\n| Vicuna-13B       | 42.6              | 47.0             | 44.0             | 36.9            | 45.2                |\\n| ChatGLM-6B       | 37.4              | 41.6             | 36.5             | 31.7            | 39.6                |\\n| Flan-T5-11B      | 24.6              | 25.9             | 29.9             | 29.3            | 29.7                |\\n| Text-Davinci-002 | 45.2              | 48.8             | 43.1             | 42.1            | 43.4                |\\n| GPT-3.5-Turbo    | 46.9              | 47.2             | 46.8             | 44.4            | 47.1                |\\n\\nWill be given a question. You should answer whether it is Yes, No, or Not Sure Enough and show your evidence.\"}\\n\\nThis instruction informs the LLMs about the expected input and output. Subsequently, for any given input \\\\( Q \\\\), we anticipate obtaining an output label \\\\( Y \\\\) from the LLMs:\\n\\n\\\\[ Y = f(Q, Z) \\\\]\\n\\n### Zero-Shot Prompt\\n\\nIn the zero-shot setting, the LLMs are expected to provide answers based on the question \\\\( Q \\\\) and the task instruction \\\\( Z \\\\). We anticipate that the LLMs can directly generate the factual answer \u201cNo\u201d when presented with \\\\( Q \\\\): \u201cHas gas prices gone up 99 percent since Obama became president, making it the highest gas price increase since Carter?\u201d The zero-shot with CoT setting extends the question \\\\( Q \\\\) by adding a two-stage prompt (Kojima et al., 2022): \u201cLet\u2019s think step by step\u201d, designed to encourage the LLMs to contemplate the process of determining the factual label \\\\( Y \\\\).\\n\\n### Few-Shot Prompt\\n\\nIn the few-shot setting, we employ three shots for model input \\\\( Q \\\\). Detailed examples of the prompts in Figure 2 are presented in Appendix A.4. In the few-shot with CoT setting, we provide potential reasoning instructions to the LLMs before presenting the factual label \\\\( Y \\\\). As shown in Figure 2, for the \\\\( Q \\\\): \u201cIs there a capital called Mogadish?\u201d Our reasoning approach entails first explaining the noun phrase in the \\\\( Q \\\\) (the subject and object), and subsequently elaborating on modifying phrases such as predicates or adjectives. Regarding the subject \u201cMogadish\u201d, we begin by furnishing a detailed definition: \u201cMogadishu is a city in East Africa, specifically in Somalia.\u201d Following this, we proceed to reason about the relation between \u201cMogadish\u201d and \u201ccapital\u201d: \u201cFurthermore, the capital of Somalia is indeed Mogadishu.\u201d Consequently, we arrive at the ultimate factual label: \u201cTherefore, the answer is Yes.\u201d\\n\\n### EXPERIMENTS\\n\\nIn an effort to take the initial step in understanding the capabilities of LLMs, we undertake a comprehensive analysis of various LLMs on Pinocchio, under different conditions and tasks.\\n\\n### 4.1 RESULTS\\n\\nIn Table 2, we present the average results of 10 accessible LLMs operating under varying settings on Pinocchio, run three times each. From Table 2, we draw the following conclusions:\\n\\n- Regarding overall performance, we observe that, on average, LLMs without instruction tuning underperform those with instruction tuning by 16.0%. GPT family LLMs undergoing RLHF exhibit superior results, indicating that instruction tuning and RLHF optimize alignment with human knowledge, thereby improving factual question response accuracy.\\n\\n- Results obtained using the Few-shot setting significantly outperform those obtained when simply asking factual questions to LLMs in the Zero-shot setting, especially for models without RLHF, exhibiting an average improvement of 7.3%. This highlights the capability of some sample prompts to better extract the inherent factual knowledge of LLMs.\\n\\n- Using the CoT method, we observed a relative boost in performance in LLMs subjected to instruction tuning and RLHF, improving by an average of 2.1%. Notably, the factual accuracy of LLMs like OPT, BLOOM, and LLaMA was mostly stable or even decreased. A review of outputs from these untuned LLMs revealed that, post-CoT application, LLMs tend to produce related...\"}"}
{"id": "9OevMUdods", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"knowledge which is inherently diverse and spread across varied formats. Relying solely on homogeneous sources may result in limited scope and inadequate coverage of information. Addressing this gap, Chen et al. (2020) proposed HybridQA, a dataset that necessitates reasoning over a blend of heterogeneous information sources. In HybridQA, each question necessitates the integration of information from a Wikipedia table and assorted text corpora tied to the entities mentioned within the table, thereby combining tabular and textual data. Parallel initiatives targeting niche fields also emerged, with Li et al. (2021) focusing on geographical data and Zhu et al. (2021) on financial information. These domain-specific endeavors highlight the growing interest in incorporating structural knowledge.\\n\\nDeparting from the provision of pre-selected tables and textual passages, OTTQA (Chen et al., 2021a) and NQ-table (Herzig et al., 2021) propel the question-answering challenge into the open-domain setting. Here, the retrieval of pertinent tables and text from comprehensive sources like Wikipedia becomes an integral part of the task. Our structural task aligns more closely with the objectives of OTTQA and NQ-table, where LLMs are tasked with performing advanced multi-hop inference. This entails navigating through a combination of both structural and unstructured factual knowledge to deduce accurate answers, reflecting a more realistic and complex information processing challenge akin to the ways humans interact with a variety of knowledge types to make informed decisions.\\n\\nAdversarial\\n\\nMachine learning models have a known susceptibility to adversarial examples\u2014inputs that have been intentionally modified to cause a model to make a mistake. A notable instance within the realm of question answering tasks is the presence of questions based on dubious assumptions, which are typically classified as unanswerable questions (Rajpurkar et al., 2018; Kwiatkowski et al., 2019; Asai & Choi, 2021). More recently, Kim et al. (2021) critiqued the practice of lumping questions with dubious assumptions into the 'unanswerable' category as inadequate. They advocated for employing presuppositions within explanations as a means to more effectively determine their unanswerability. Additionally, their work demonstrates the complexity of verifying assumptions, proving it to be a formidable challenge even in closed-book environments. Building upon this, Kim et al. (2023b) expanded the investigation into open-domain contexts, confirming the inherent difficulties associated with QA that involve problematic assumptions. They discovered that, even when the hurdle of recognizing assumptions is eliminated, the task of factual verification remains unsolved\u2014though, it should be noted, recent enhancements in LLMs have indeed contributed to some progress in verification capabilities. In a related vein, Yu et al. (2023b) presented a new open-domain QA dataset that features a natural distribution of failures due to presuppositions. Their research reveals that the challenges in handling questions with questionable assumptions are consistent, irrespective of the different sources from which the questions are derived, which include search engine prompts as well as Reddit inquiries. This body of work indicates that while strides have been made in addressing some aspects of QA tasks, the nuanced issue of dealing with questionable assumptions persists across various settings and requires further exploration.\\n\\nTemporal\\n\\nUnderstanding the temporal evolution of information is a significant area of interest in the field of question answering. Initial research, such as TempQuestions (Jia et al., 2018), investigated temporal aspects of questions that incorporated time specifiers within knowledge bases. Subsequent studies have shifted their focus toward apprehending the nuances of temporal progression in natural language texts. For example, Chen et al. (2021c) introduced TimeQA, a resource constructed by extracting and compiling evolving facts from WikiData alongside corresponding Wikipedia passages, resulting in a dataset of 20,000 timestamped question-answer pairs. Moreover, Zhang & Choi (2021) presented SituatedQA, which includes 9,000 realistically formulated questions from pre-existing open-domain QA datasets, each complemented with temporal contexts, such as specific timestamps. StreamingQA (Liska et al., 2022) is another relevant contribution that encompasses a blend of machine-generated and human-authored questions\u2014altogether totaling 146,000 entries\u2014designed to be answerable using a repository of timestamped news articles. In the same vein, the dynamic RealTimeQA benchmark (Kasai et al., 2022b) poses a challenge for models by offering 30 multiple-choice questions based on recent events curated from news websites, thereby testing their ability to handle fresh content. Adding to these advancements, FreshQA (Vu et al., 2023) brings a new dimension to the table with a static compilation of human-curated open-ended questions. The uniqueness of FreshQA lies in the evolving nature of its answers, which are subject to change in response to ongoing world developments, providing a generative assessment for time-sensitive question answering. This body of work collectively underscores the complexity and dynamism inherent in temporal question answering research.\"}"}
{"id": "9OevMUdods", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Domain-Specific\\n\\nWhile there have been successful developments in question-answering within broad domains, specialized domains such as science and biomedicine remain relatively underexplored and present unique challenges. The limited availability of domain-specific datasets, coupled with the need for an in-depth understanding of specialized knowledge to match that of human experts, marks these areas as fertile ground for ongoing research. In the scientific domain, existing datasets necessitate the use of varied reasoning methods tailored to each specific question (Clark et al., 2018a). For instance, the OpenBookQA dataset (Mihaylov et al., 2018) presents multiple-choice questions that are generated based on a core book of fundamental science facts. Similarly, the QASC dataset (Khot et al., 2020) offers multiple-choice questions on science topics appropriate for elementary and middle school levels, emphasizing the combination of facts. QASC is unique in that it intentionally includes pairs of facts that, according to evaluations by crowd workers, provide enough information to deduce the answer to each question. Shifting the focus to the biomedical field, a range of new datasets have emerged to support question-answering tasks that hinge on domain-specific expertise. These include datasets such as HealthQA (Zhu et al., 2019), MASH-QA (Zhu et al., 2020a), and MedMCQA (Pal et al., 2022), which have been introduced to bolster research in medical question-answering applications. These datasets serve as valuable resources to address the nuanced queries that arise within the complex terrain of biomedical knowledge.\\n\\nMulti-Lingual\\n\\nRecent effort has been made to create non-English QA datasets to overcome the data scarcity in non-English languages, typically including one or two languages. These include DuReader (He et al., 2018) in Chinese, French/Japanese evaluation sets for SQuAD created via translation (Asai et al., 2018), a semi-automatic Italian translation of SQuAD (Croce et al., 2019), ARCD\u2014an Arabic reading comprehension dataset (Mozannar et al., 2019), a Hindi-English parallel dataset in a SQuAD-like setting (Gupta et al., 2018), and a Chinese\u2013English dataset focused on visual QA (Gao et al., 2015). Recent datasets cover more languages, such as XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020), which are examples of SQuAD-style extractive datasets, employing human translators to create parallel examples. MLQA and XQuAD ensure that all answers are answerable, and derive answers from provided documents. Instead of extractive answers, Hardalov et al. (2020) introduced EXAMS, a multilingual multiple-choice QA from school exams. TyDiQA (Clark et al., 2020) and MKQA (Longpre et al., 2021), focus on typological diversity in its wide language selection. While TyDiQA offers a more natural distribution of questions, its annotations are based on the retrieval system used by the authors (Google search); hence their answers are actually start and end indices for spans of text within a given passage. Xor QA (Asai et al., 2021) explores cross-lingual subtasks by re-annotating TyDiQA examples, sourcing answers from English documents, and translating them back to the target language. While state-of-the-art models have matched or surpassed human performance in general-purpose monolingual benchmarks, current methods still fall short of human performance on multilingual benchmarks, despite recent gains. Multilingual question answering consequently is at the frontier of such cross-lingual generalization.\"}"}
{"id": "9OevMUdods", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset Type | Source | Answer Type | Domain |\\n|--------------|--------|-------------|--------|\\n| WikiHop (Welbl et al., 2018a) | Multifaceted WikiData | \u2717 | Multiple Choice General |\\n| HotpotQA (Yang et al., 2018) | Multifaceted Wikipedia | \u2713 | General |\\n| MultiRC (Khashabi et al., 2018) | Multifaceted Multiple | \u2717 | Multiple Choice General |\\n| StrategyQA (Geva et al., 2021) | Multifaceted Wikipedia | \u2713 | General |\\n| OpenBookQA (Mihaylov et al., 2018) | Multifaceted/Domain-Specific WorldTree | \u2713 | Multiple Choice General |\\n| QASC (Khot et al., 2020) | Multifaceted/Domain-Specific Wikipedia | \u2713 | Multiple Choice Science |\\n| NQ-tables (Herzig et al., 2021) | Structural Google Queries | \u2713 | General |\\n| TAT-QA (Zhu et al., 2021) | Structural/Domain-Specific Wikipedia | \u2717 | General |\\n| TSQA (Li et al., 2021) | Structural/Domain-Specific Exam | \u2717 | General |\\n| HybridQA (Chen et al., 2020) | Structural/Multifaceted Wikipedia | \u2717 | General |\\n| OTTQA (Chen et al., 2021b) | Structural/Multifaceted Wikipedia | \u2713 | General |\\n| QA (Kim et al., 2023b) | Adversarial Google Queries | \u2717 | General |\\n| CREPE (Yu et al., 2023b) | Adversarial/Real-World Reddit | \u2717 | General |\\n| TempQuestions (Jia et al., 2018) | Temporal Datasets | \u2717 | General |\\n| TimeQA (Chen et al., 2021c) | Temporal WikiData | \u2717 | General |\\n| SituatedQA (Zhang & Choi, 2021) | Temporal Datasets | \u2713 | General |\\n| RealTimeQA (Kasai et al., 2022a) | Temporal News | \u2713 | General |\\n| StreamingQA (Liska et al., 2022) | Temporal News | \u2713 | General |\\n| FreshQA (Vu et al., 2023) | Temporal/Multifaceted Manual | \u2713 | General |\\n| MSMarco (Nguyen et al., 2016) | Real-World Bing Queries | \u2713 | General |\\n| SearchQA (Dunn et al., 2017) | Real-World Google Queries | \u2713 | General |\\n| TriviaQA (Joshi et al., 2017) | Real-World Forum | \u2713 | General |\\n| DuReader (He et al., 2018) | Real-World Baidu Queries | \u2713 | General |\\n| NQ (Kwiatkowski et al., 2019) | Real-World Google Queries | \u2713 | General |\\n| ELI5 (Fan et al., 2019) | Real-World Reddit | \u2713 | General |\\n| QASPER (Dasigi et al., 2021) | Domain-Specific Papers | \u2713 | Science |\\n| ScienceQA (Lu et al., 2022) | Domain-Specific Exams | \u2717 | Multiple Choice Science |\\n| HealthQA (Zhu et al., 2019) | Domain-Specific Patient | \u2717 | Free-form BioMed |\\n| MedMCQA (Pal et al., 2022) | Domain-Specific Exams | \u2717 | Multiple Choice BioMed |\\n| MASH-QA (Zhu et al., 2020b) | Domain-Specific WebMD | \u2717 | Free-form BioMed |\\n| XQuAD (Artetxe et al., 2020) | Multilingual SQuAD | \u2717 | General |\\n| MLQA (Lewis et al., 2020) | Multilingual Wikipedia | \u2717 | General |\\n| EXAMS (Hardalov et al., 2020) | Multilingual Exam | \u2717 | General |\\n| TydiQA (Clark et al., 2020) | Multilingual/Real-World NQ | \u2717 | General |\\n| MKQA (Longpre et al., 2021) | Multilingual/Real-World NQ | \u2717 | General |\\n| XOR QA (Asai et al., 2021) | Multilingual/Real-World TydiQA | \u2713 | General |\"}"}
{"id": "9OevMUdods", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the number of shots in a few-shot learning setting also did not yield a substantial improvement in performance.\\n\\nA.6 ACT CHECKING DATASETS\\n\\nIn this section, we first undertook a comprehensive survey of numerous existing Fact-Checking datasets, the summary of which is presented in Table 13. Our analysis focused on identifying the various challenges these datasets aim to address regarding factuality. We found that the challenges tackled by these datasets generally align with the seven aspects we have listed in our dataset. These aspects either appear individually or in combination across the surveyed datasets, indicating their relevance and importance in the field of fact-checking. This realization led us to intentionally design our evaluation framework around these seven specific challenges, ensuring that our benchmark is not only comprehensive but also directly addresses the core difficulties encountered in current fact-checking tasks.\\n\\nTable 9: Domain Distribution of Various Fact-Checking Datasets.\\n\\n| Dataset             | Multifaceted | Structural | Adversarial | Temporal | Real-World | Domain-Specific | Multi-Lingual |\\n|---------------------|--------------|------------|-------------|----------|------------|----------------|--------------|\\n| COVID-19 Disinfo    | \u2713            | \u2713          | \u2713           |          |            |                 |               |\\n| SPICED              | \u2713            | \u2713          | \u2713           | \u2713        |            |                 |               |\\n| EMU                 | \u2713            |            |             |          |            |                 |               |\\n| NeuralNews          | \u2713            |            |             |          |            |                 |               |\\n| Propa-News          | \u2713            |            |             |          |            |                 |               |\\n| HOVER               | \u2713            |            |             |          |            |                 |               |\\n| ParsFEVER           | \u2713            |            |             |          |            |                 |               |\\n| MultiFC             | \u2713            | \u2713          |             |          |            |                 |               |\\n| Fact-KG             | \u2713            | \u2713          |             |          |            |                 |               |\\n| NewsCLIPpings       | \u2713            | \u2713          |             |          |            |                 |               |\\n| Semeval 2021 Task9  | \u2713            |            |             |          |            |                 |               |\\n| Infotabs            | \u2713            |            |             |          |            |                 |               |\\n| TabFact             | \u2713            |            |             |          |            |                 |               |\\n| InfoSurgeon         | \u2713            | \u2713          |             |          |            |                 |               |\\n| DeSePtion           | \u2713            | \u2713          |             |          |            |                 |               |\\n| RumorEval19         | \u2713            | \u2713          |             |          |            |                 |               |\\n| AdverBenc           | \u2713            | \u2713          |             |          |            |                 |               |\\n| Fakeedit            | \u2713            | \u2713          |             |          |            |                 |               |\\n| Claimde-Comp        | \u2713            | \u2713          |             |          |            |                 |               |\\n| A VeriTec           | \u2713            | \u2713          |             |          |            |                 |               |\\n| V oynaSlov          | \u2713            | \u2713          |             |          |            |                 |               |\\n| WatClaimCheck       | \u2713            | \u2713          |             |          |            |                 |               |\\n| MuMiN               | \u2713            | \u2713          | \u2713           | \u2713        |            |                 |               |\\n| MR2                 | \u2713            | \u2713          |             |          |            |                 |               |\\n| FakeSV              | \u2713            | \u2713          |             |          |            |                 |               |\\n| Weibo20             | \u2713            | \u2713          |             |          |            |                 |               |\\n| Rumor Stance        | \u2713            | \u2713          |             |          |            |                 |               |\\n| Veritas             | \u2713            |            |             |          |            |                 |               |\\n| LIAR                | \u2713            | \u2713          |             |          |            |                 |               |\\n| FakeNewsNet         | \u2713            | \u2713          |             |          |            |                 |               |\\n| ClaimBuster         | \u2713            | \u2713          |             |          |            |                 |               |\\n| CURT                | \u2713            |            |             |          |            |                 |               |\\n| Health-VER          | \u2713            | \u2713          |             |          |            |                 |               |\\n| Covid-Fact          | \u2713            | \u2713          |             |          |            |                 |               |\\n| CoVERT              | \u2713            | \u2713          |             |          |            |                 |               |\\n| Answer-Fact         | \u2713            | \u2713          |             |          |            |                 |               |\\n| SciTweets           | \u2713            | \u2713          |             |          |            |                 |               |\\n| Dial-Fact           | \u2713            |            |             |          |            |                 |               |\\n| CHEF                | \u2713            | \u2713          |             |          |            |                 |               |\\n| DanFEVER            | \u2713            | \u2713          |             |          |            |                 |               |\\n| Stanceosaurus       | \u2713            | \u2713          |             |          |            |                 |               |\\n| Climate-FEVER       | \u2713            |            |             |          |            |                 |               |\\n| FakeCovid           | \u2713            | \u2713          |             |          |            |                 |               |\"}"}
{"id": "9OevMUdods", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Q: Were there over a dozen arrests in SC following the 4th Poor People's Rally?\\nA: There were reports of several arrests following the 4th Poor People's Rally in South Carolina, but the exact number is unclear. Without further information, it is difficult to determine the exact number of arrests. Therefore, the answer is \\\"Not Enough Information\\\".\\n\\nGold Label: Factual\\nGold Evidence: Protesters with the Poor People's Campaign were back at South Carolina's Statehouse on Monday, this time blocking a major road while demanding universal health care. About a dozen people were arrested. Following that protest, about a dozen demonstrators blocked U.S. Highway 1 in front of the capitol and were arrested by Columbia police.\\n\\nQ: Has the central parity rate of the RMB against the US dollar been lowered by 27 basis points on November 30, 2020?\\nA: On November 30, 2020, the People's Bank of China (PBOC) announced a 27 basis point decrease in the midpoint of the yuan's exchange rate against the US dollar, to 6.5675. Therefore, the answer is \\\"Factual\\\".\\n\\nGold Label: Factual\\nGold Evidence: Xinhua News Agency, Shanghai, November 30. Data from the China Foreign Exchange Trading Center showed that the central parity rate of the RMB against the US dollar was reported at 6.5782 on the 30th, a decrease of 27 basis points from the previous trading day. On the previous trading day, the central parity rate of the RMB against the US dollar was reported at 6.5755.\\n\\nQ: Has the video Castle on the Hill had more than 100 million views on YouTube?\\nA: \\\"Castle on the Hill\\\" is a song by Ed Sheeran, released in 2017. As of my knowledge cutoff date of September 2021, the video had over 1.3 billion views on YouTube. However, I don't have information on whether it had over 100 million views specifically. Therefore, the answer is \\\"Not Enough Information\\\".\\n\\nGold Label: Factual\\nGold Evidence: As of 8 February 2017, the lyric video on YouTube has amassed over 101 million views in a period of 34 days.\\n\\nQ: Does a router with more antennas have a stronger signal?\\nA: In general, the more antennas a router has, the stronger its signal can be. This is because multiple antennas allow the router to transmit and receive signals from different directions and angles, which can help to improve the overall coverage and strength of the signal. Therefore, the answer is \\\"Non-Factual\\\".\\n\\nGold Label: Non-Factual\\nGold Evidence: The advantages and disadvantages of a Wi-Fi router depend on many factors. It is not that the more antennas, the better, nor does it mean that a router with more antennas has wider signal coverage. The strength of the router signal mainly depends on whether it is equipped with an independent high-power signal amplifier. The signal amplifier is more powerful than the antenna.\\n\\n2. Flawed Reasoning Leading to Incorrect Conclusions: 51% of the instances exhibited incorrect reasoning processes, which inevitably led to incorrect conclusions. As an example, consider Figure 9 (b): The question posed was whether there were over a dozen arrests in South Carolina following the 4th Poor People's Rally. The LLM responded by stating there were reports of several arrests, but it could not ascertain the exact number, leading to a conclusion of \\\"Not Enough Information.\\\" However, the 'Gold Evidence' clearly stated that about a dozen people were arrested following the demonstration, indicating that the correct label should have been \\\"Factual.\\\" This instance underscores a scenario where the LLM might not possess the specific numerical details present in the gold evidence, leading to an incomplete and therefore inaccurate conclusion.\\n\\n3. Discrepancies in Reasoning and Conclusions: Interestingly, in our analysis, we identified cases where the reasoning process did not align with the final conclusion. Specifically, there were instances, comprising 4% of our sample, where the LLMs reached a correct answer despite flawed reasoning. For example, in Figure 9 (c), the question was about whether Ed Sheeran's \\\"Castle on the Hill\\\" had more than 100 million views on YouTube. The LLM correctly concluded that it had over 1.3 billion views by September 2021, but stated \\\"Not Enough Information\\\" for the specific query of 100 million views, despite the fact being factual as per the gold evidence. Conversely, in 6% of the cases, the LLMs displayed sound reasoning but still arrived at an incorrect conclusion. A notable example of this is seen in Figure 9 (d), where the question asked whether a router with more antennas has a stronger signal. The LLM reasoned that a greater number of antennas would generally lead to a stronger signal due to enhanced multi-directional transmission and reception capabilities. Despite this logical reasoning, the LLM concluded the answer as \\\"Non-Factual\\\", which contradicts the reasoning provided.\\n\\n4. Internal Reasoning Contradictions: We also found an example where the reasoning process of the LLM was contradictory. The question is \\\"Does a deficiency of vitamin B6 decrease blood levels of homocysteine?\\\" and LLM answers: \\\"Vitamin B6 is involved in the metabolism of homocysteine, which is an amino acid that is potentially harmful to blood vessels when present in high levels. A deficiency of vitamin B6 can lead to an accumulation of homocysteine in the blood. Therefore, a deficiency of vitamin B6 can decrease blood levels of homocysteine. The answer is \\\"Factual\\\".\\\" Initially, the LLM correctly states that a deficiency in vitamin B6 can lead to an accumulation of homocysteine. However, it concludes with a contradictory statement, suggesting that a deficiency in vitamin B6 can decrease homocysteine levels.\"}"}
{"id": "9OevMUdods", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: The performance of different models across Pinocchio.\\n\\n| Datasets          | Accuracy (%) |\\n|-------------------|--------------|\\n| No Context        | 28.3         |\\n| LLaMA-7B          | 31.6         |\\n| Alpaca-7B         | 37.8         |\\n| Vicuna-13B        | 45.2         |\\n| GPT-3.5           | 47.0         |\\n\\nOur experimental findings show that our dataset does not exhibit the same level of vulnerability to the exploitation of question-label correlations as observed by Chen & Durrett (2019) in the WikiHop dataset. With performance improvements of 16.9 points by Vicuna-13B and 18.7 by GPT-3.5 over the \u201cno context\u201d baseline, our results offer compelling evidence that our dataset is more resilient to such biases, contrary to the reported susceptibilities within WikiHop.\\n\\nWe extended our analysis to include a direct comparison with several established multiple-choice question-answering benchmarks, such as the WikiHop mentioned above, as well as with other prevalent benchmarks like TruthfulQA and ARC utilized in evaluating LLMs. The performance of the \u201cno context\u201d baseline across these benchmarks is displayed in Table 11:\\n\\nTable 11: The performance of the \u201cno context\u201d baseline across these benchmarks.\\n\\n| Datasets                      | Accuracy (%) |\\n|-------------------------------|--------------|\\n| WikiHop (Welbl et al., 2018b) | 59.7         |\\n| TruthfulQA (Lin et al., 2021)  | 34.5         |\\n| ARC (Clark et al., 2018b)     | 33.2         |\\n| Ours                          | 28.3         |\\n\\nEvidently, our proposed dataset presented the most challenge to the \u201cno context\u201d baseline, marking the lowest performance compared to other datasets. The notable performance on WikiHop, with a \u201cno context\u201d baseline score of 59.7%, underscores the presence of spurious correlations that facilitate gaming that dataset. On the contrary, the lower baseline performances on TruthfulQA and ARC suggest that such issues are less prevalent. Our dataset, therefore, not only stands out as the least prone \u201cto be gamed\u201d but also underscores its robustness and the high level of rigor needed to tackle it effectively.\\n\\nA.9 PEEER ANALYSIS\\n\\nThe comparisons between LLaMA and its instruction-tuned versions, Alpaca and Vicuna, can be found in Table 2. Furthermore, we have conducted extra tests under the few-shots with CoT setting for T5-11B vs. Flan-T5-11B and BLOOM-6.7B vs. BLOOMz-6.7B as shown in Table 12. For T5, the accuracy was 18.6%, and the Macro F1 was 25.2%. In contrast, as shown in Table 2, Flan-T5 achieved an accuracy of 38.4% and a Macro F1 of 38.4%. Similarly, BLOOM\u2019s performance was at an accuracy of 6.6% and Macro F1 of 12.2%, whereas BLOOMz showed a marked improvement with an accuracy of 27.5% and a Macro F1 of 27.7%. These peer-by-peer comparisons reveal that, with few exceptions (e.g., LLaMA vs. Alpaca in terms of Macro F1), models that underwent instruction tuning generally outperform their backbone counterparts, achieving an average improvement of 11.3%.\\n\\nA.10 RELATED WORK: QUESTION ANSWERING DATASETS\\n\\nIn this section, we offer a thorough examination of existing question-answering initiatives as they pertain to the seven key dimensions that form the core of our benchmark. These dimensions are multifaceted, structural, adversarial, temporal, real-world, and multilingual.\\n\\nAs detailed in Table 13, we present a comprehensive overview of notable datasets within the realm of question-answering. We categorize these datasets based on several criteria to illuminate their\"}"}
{"id": "9OevMUdods", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12: Peer-to-peer comparison between the instruction-tuned models and their backbones.\\n\\n| Models             | Accuracy(%) | Macro F1(%) |\\n|--------------------|-------------|-------------|\\n| LLaMA-7B           | 35.3        | 31.4        |\\n| Alpaca-7B          | 39.4        | 26.2        |\\n| Vicuna-7B          | 48.5        | 40.6        |\\n| T5-11B             | 18.6        | 25.2        |\\n| Flan-T5-11B        | 38.4        | 38.4        |\\n| Bloom-6.7B         | 6.6         | 12.2        |\\n| Bloomz-6.7B        | 27.5        | 27.7        |\\n\\ndistinctive challenges and characteristics. First, we identify the \\\"Type\\\" of challenge each dataset presents. Next, \\\"Source\\\" provides the origins of the questions. \\\"Retrieval\\\" indicates the necessity of sourcing external knowledge, such as documents, to formulate an answer. When it comes to the \\\"Answer types\\\", datasets may require various forms of responses ranging from multiple-choice options (A, B, C, etc.), specific text spans (e.g., an entity or a phrase), to Boolean (yes or no) and free-form answers that allow for the generated text of any length. \\\"Domain\\\" captures the field to which the questions belong, encompassing areas like science, biography, or geography.\\n\\nInterestingly, beyond these seven axes, there exist other datasets that probe the knowledge and reasoning capabilities of large language models (LLMs) from different perspectives. For instance, research centered around knowledge updating, particularly focusing on entities, has been conducted. Onoe et al. (2022) delve into the ability of LLMs to make inferences about newly emerged entities that were not part of the LLMs' pretraining data. Building on this, Onoe et al. (2023) investigated the extent to which LLMs can integrate descriptions of new entities. On the other hand, Peng et al. (2022) have assessed LLMs' conceptual knowledge by crafting three distinct tasks that test whether LLMs are capable of categorizing entities based on conceptual similarities.\\n\\nMultifaceted\\n\\nExisting efforts in question answering that relate to the multi-faceted nature of our dataset predominantly encompass multi-hop reasoning datasets. These datasets necessitate models to synthesize multiple information snippets to formulate an answer. For instance, WikiHop Welbl et al. (2018a) constructs a bipartite graph from a knowledge base populated with relational triplets. This graph undergoes a breadth-first traversal to yield valid multi-hop reasoning chains. Similarly, HotpotQA (Yang et al., 2018) narrows its focus to 2-hop questions derived from the initial sections of English Wikipedia documents. The selection of two passages to form a reasoning chain is predicated upon one of two conditions: either a hyperlink connects the first document to the second, or the associated entities belong to an identical category. Moving onto a broader spectrum, MultiRC (Khashabi et al., 2018) introduces multi-domain multi-hop questions. It compiles documents from various domains and a multitude of datasets, where the different contexts are all embedded within the same textual passage. As opposed to the questions themselves providing explicit decompositional cues, StrategyQA (Geva et al., 2021) conceals the necessary reasoning steps within the question itself. These steps must be astutely deduced using strategic inference.\\n\\nAdditionally, several datasets intertwine multi-hop reasoning with further complexities. OpenBookQA (Mihaylov et al., 2018) offers a specialized challenge, combining question answering techniques with a compendium of scientific facts to assess knowledge in the scientific domain, supplemented by a broader base of common understanding. In a vein similar to OpenBookQA, QASC (Khot et al., 2020) also revolves around two-hop question answering with a foundation in scientific facts; its methodology for reasoning chain generation closely resembles that of OpenBookQA. Furthermore, datasets like HybridQA (Chen et al., 2020) and OTTQA (Chen et al., 2021b) venture into multi-hop reasoning across both tabular and textual data sources. Vu et al. (2023) introduces FreshQA, incorporating questions that demand multi-hop reasoning where answers may shift over time, as well as tackling premises that are fundamentally flawed.\"}"}
{"id": "9OevMUdods", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "9OevMUdods", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multi-modal reasoning via thought chains for science question answering. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/11332b6b6cf4485b84afadb1352d3a9a-Abstract-Conference.html.\\n\\nGrace Luo, Trevor Darrell, and Anna Rohrbach. Newsclippings: Automatic generation of out-of-context multimodal media. arXiv preprint arXiv:2104.05893, 2021.\\n\\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.\\n\\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. CoRR, abs/2303.08896, 2023. doi: 10.48550/ARXIV.2303.08896. URL https://doi.org/10.48550/arXiv.2303.08896.\\n\\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? A new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 2381\u20132391. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1260. URL https://doi.org/10.18653/v1/d18-1260.\\n\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. CoRR, abs/2305.14251, 2023. doi: 10.48550/ARXIV.2305.14251. URL https://doi.org/10.48550/arXiv.2305.14251.\\n\\nRishabh Misra. Kaggle politifact fact-checking dataset, 2022. URL https://www.kaggle.com/datasets/rmisra/politifact-fact-check-dataset.\\n\\nIsabelle Mohr, Amelie W\u00fchrl, and Roman Klinger. Covert: A corpus of fact-checked biomedical covid-19 tweets. arXiv preprint arXiv:2204.12164, 2022.\\n\\nHussein Mozannar, Elie Maamary, Karl El Hajal, and Hazem M. Hajj. Neural arabic question answering. In Wassim El-Hajj, Lamia Hadrich Belguith, Fethi Bougares, Walid Magdy, and Imed Zitouni (eds.), Proceedings of the Fourth Arabic Natural Language Processing Workshop, WANLP@ACL 2019, Florence, Italy, August 1, 2019, pp. 108\u2013118. Association for Computational Linguistics, 2019. doi: 10.18653/V1/W19-4612. URL https://doi.org/10.18653/v1/w19-4612.\\n\\nKai Nakamura, Sharon Levy, and William Yang Wang. r/fakeddit: A new multimodal benchmark dataset for fine-grained fake news detection. arXiv preprint arXiv:1911.03854, 2019.\\n\\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d\u2019Avila Garcez, and Greg Wayne (eds.), Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL https://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.\\n\\nDan S Nielsen and Ryan McConville. Mumin: A large-scale multilingual multimodal fact-checked misinformation social network dataset. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 3141\u20133153, 2022.\\n\\nJeppe N\u00f8rregaard and Leon Derczynski. Danfever: claim verification dataset for danish. In Proceedings of the 23rd Nordic conference on computational linguistics (NoDaLiDa), pp. 422\u2013428, 2021.\"}"}
{"id": "9OevMUdods", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "9OevMUdods", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "9OevMUdods", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGEMENT\\nThis work is supported in part by NSF under grant III-2106758. Additionally, Junzhe Chen and Xiaochuan Li are supported by Beijing Natural Science Foundation under grant number QY23115 and QY23116.\\n\\nREFERENCES\\nFiroj Alam, Shaden Shaar, Fahim Dalvi, Hassan Sajjad, Alex Nikolov, Hamdy Mubarak, Giovanni Da San Martino, Ahmed Abdelali, Nadir Durrani, Kareem Darwish, Abdulaziz Al-Homaid, Wajdi Zaghouani, Tommaso Caselli, Gijs Danoe, Friso Stolk, Britt Bruntink, and Preslav Nakov. Fighting the COVID-19 infodemic: Modeling the perspective of journalists, fact-checkers, social media platforms, policy makers, and the society. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 611\u2013649, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.56. URL https://aclanthology.org/2021.findings-emnlp.56.\\n\\nRami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull, James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. FEVEROUS: fact extraction and verification over unstructured and structured information. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/68d30a9594728bc39aa24be94b319d21-Abstract-round1.html.\\n\\nFatma Arslan, Naeemul Hassan, Chengkai Li, and Mark Tremayne. A benchmark dataset of check-worthy factual claims. In Proceedings of the International AAAI Conference on Web and Social Media, volume 14, pp. 821\u2013829, 2020.\\n\\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of monolingual representations. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 4623\u20134637. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.ACL-MAIN.421. URL https://doi.org/10.18653/v1/2020.acl-main.421.\\n\\nAkari Asai and Eunsol Choi. Challenges in information-seeking QA: unanswerable questions and paragraph retrieval. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 1492\u20131504. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.ACL-LONG.118. URL https://doi.org/10.18653/v1/2021.acl-long.118.\\n\\nAkari Asai, Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. Multilingual extractive reading comprehension by runtime machine translation. CoRR, abs/1809.03275, 2018. URL http://arxiv.org/abs/1809.03275.\\n\\nAkari Asai, Jungo Kasai, Jonathan H. Clark, Kenton Lee, Eunsol Choi, and Hannaneh Hajishirzi. XOR QA: cross-lingual open-retrieval question answering. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T\u00fcr, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 547\u2013564. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.NAACL-MAIN.46. URL https://doi.org/10.18653/v1/2021.naacl-main.46.\\n\\nIsabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Christian Hansen, and Jakob Grue Simonsen. Multifc: A real-world multi-domain dataset for evidence-based fact checking of claims. arXiv preprint arXiv:1909.03242, 2019.\"}"}
{"id": "9OevMUdods", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "9OevMUdods", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "9OevMUdods", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "9OevMUdods", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "9OevMUdods", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "9OevMUdods", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html.\\n\\nJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop reading comprehension across documents. Trans. Assoc. Comput. Linguistics, 6:287\u2013302, 2018a. doi: 10.1162/TACL_A_00021. URL https://doi.org/10.1162/tacl_a_00021.\\n\\nJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association for Computational Linguistics, 6:287\u2013302, 2018b.\\n\\nDustin Wright, Jiaxin Pei, David Jurgens, and Isabelle Augenstein. Modeling information change in science communication with semantically matched paraphrases. arXiv preprint arXiv:2210.13001, 2022.\\n\\nQiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On the tool manipulation capability of open-source large language models, 2023.\\n\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 2369\u20132380. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1259. URL https://doi.org/10.18653/v1/d18-1259.\\n\\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models are strong context generators. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023a. URL https://openreview.net/pdf?id=fB0hRu9GZUS.\\n\\nXinyan Yu, Sewon Min, Luke Zettlemoyer, and Hannaneh Hajishirzi. CREPE: Open-domain question answering with false presuppositions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10457\u201310480, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.583. URL https://aclanthology.org/2023.acl-long.583.\\n\\nMajid Zarharan, Mahsa Ghaderan, Amin Pourdabiri, Zahra Sayedi, Behrouz Minaei-Bidgoli, Sauleh Eetemadi, and Mohammad Taher Pilehvar. Parsfever: a dataset for farsi fact extraction and verification. In Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics, pp. 99\u2013104, 2021.\\n\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquez (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 4791\u20134800. Association for Computational Linguistics, 2019. doi: 10.18653/V1/P19-1472. URL https://doi.org/10.18653/v1/p19-1472.\\n\\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130B: an open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=-Aw0rrrPUF.\\n\\nMichael J. Q. Zhang and Eunsol Choi. Situatedqa: Incorporating extra-linguistic contexts into QA. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021. URL.\"}"}
{"id": "9OevMUdods", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix A.1 Ethical Statement\\n\\nPinocchio primarily serves to assess LLMs\u2019 responses to questions concerning factual knowledge. If a model performs effectively, it would be imprudent to infer that its reliability will uniformly translate to diverse task domains (even if some degree of transfer learning is anticipated). For instance, Pinocchio does not encompass long-form generation, such as news articles, or interactive settings, such as extended dialogues with adversarial entities. Furthermore, although the questions within Pinocchio parallel real-world inquiries, they originate not from a deployed system, thus posing a potential risk of over- or under-estimating the factuality of such a system.\"}"}
{"id": "9OevMUdods", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur G\u00fcney, Volkan Cirik, and Kyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search engine. CoRR, abs/1704.05179, 2017. URL http://arxiv.org/abs/1704.05179.\\n\\nJulian Eisenschlos, Bhuwan Dhingra, Jannis Bulian, Benjamin B\u00f6rschinger, and Jordan L. Boyd-Graber. Fool me twice: Entailment from wikipedia gamification. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T\u00fcr, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 352\u2013365. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.naacl-main.32. URL https://doi.org/10.18653/v1/2021.naacl-main.32.\\n\\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard H. Hovy, Hinrich Sch\u00fctze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. Trans. Assoc. Comput. Linguistics, 9:1012\u20131031, 2021. doi: 10.1162/tacl_a_00410. URL https://doi.org/10.1162/tacl_a_00410.\\n\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: long form question answering. In Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquez (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 3558\u20133567. Association for Computational Linguistics, 2019. doi: 10.18653/V1/P19-1346. URL https://doi.org/10.18653/v1/p19-1346.\\n\\nLorenzo Jaime Yu Flores and Yiding Hao. An adversarial benchmark for fake news detection models. arXiv preprint arXiv:2201.00912, 2022.\\n\\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720, 2022.\\n\\nYi Fung, Christopher Thomas, Revanth Gangi Reddy, Sandeep Polisetty, Heng Ji, Shih-Fu Chang, Kathleen McKeown, Mohit Bansal, and Avirup Sil. Infosurgeon: Cross-media fine-grained information consistency checking for fake news detection. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1683\u20131698, 2021.\\n\\nHaoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. Are you talking to a machine? dataset and methods for multilingual image question answering. CoRR, abs/1505.05612, 2015. URL http://arxiv.org/abs/1505.05612.\\n\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies. Trans. Assoc. Comput. Linguistics, 9:346\u2013361, 2021. doi: 10.1162/TACL_A_00370. URL https://doi.org/10.1162/tacl_a_00370.\\n\\nGenevieve Gorrell, Elena Kochkina, Maria Liakata, Ahmet Aker, Arkaitz Zubiaga, Kalina Bontcheva, and Leon Derczynski. Semeval-2019 task 7: Rumoureval 2019: Determining rumour veracity and support for rumours. In Proceedings of the 13th International Workshop on Semantic Evaluation: NAACL HLT 2019, pp. 845\u2013854. Association for Computational Linguistics, 2019.\\n\\nZhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos. A survey on automated fact-checking. Transactions of the Association for Computational Linguistics, 10:178\u2013206, 2022. doi: 10.1162/tacl_a_00454. URL https://aclanthology.org/2022.tacl-1.11.\\n\\nAshim Gupta and Vivek Srikumar. X-fact: A new benchmark dataset for multilingual fact checking. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 675\u2013682, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.86. URL https://aclanthology.org/2021.acl-short.86.\"}"}
{"id": "9OevMUdods", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "9OevMUdods", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Christopher Hidey, Tuhin Chakrabarty, Tariq Alhindi, Siddharth Varia, Kriste Krstovski, Mona Diab, and Smaranda Muresan. Deseption: Dual sequence prediction and adversarial examples for improved fact-checking. arXiv preprint arXiv:2004.12864, 2020.\\n\\nXuming Hu, Zhijiang Guo, GuanYu Wu, Aiwei Liu, Lijie Wen, and S Yu Philip. Chef: A pilot Chinese dataset for evidence-based fact-checking. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3362\u20133376, 2022a.\\n\\nXuming Hu, Zhijiang Guo, GuanYu Wu, Aiwei Liu, Lijie Wen, and Philip Yu. CHEF: A pilot Chinese dataset for evidence-based fact-checking. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3362\u20133376, Seattle, United States, July 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.246. URL https://aclanthology.org/2022.naacl-main.246.\\n\\nXuming Hu, Zhijiang Guo, Junzhe Chen, Lijie Wen, and Philip S Yu. Mr2: A benchmark for multimodal retrieval-augmented rumor detection in social media. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 2901\u20132912, 2023.\\n\\nKung-Hsiang Huang, Kathleen McKeown, Preslav Nakov, Yejin Choi, and Heng Ji. Faking fake news for real fake news detection: Propaganda-loaded training data generation. arXiv preprint arXiv:2203.05386, 2022.\\n\\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: A multi-level multi-discipline Chinese evaluation suite for foundation models. CoRR, abs/2305.08322, 2023. doi: 10.48550/arXiv.2305.08322. URL https://doi.org/10.48550/arXiv.2305.08322.\\n\\nZhen Jia, Abdalghani Abujabal, Rishiraj Saha Roy, Jannik Str\u00f6tgen, and Gerhard Weikum. Tempquestions: A benchmark for temporal question answering. In Pierre-Antoine Champin, Fabien Gandon, Mounia Lalmas, and Panagiotis G. Ipeirotis (eds.), Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon, France, April 23-27, 2018, pp. 1057\u20131062. ACM, 2018. doi: 10.1145/3184558.3191536. URL https://doi.org/10.1145/3184558.3191536.\\n\\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. Hover: A dataset for many-hop fact extraction and claim verification. arXiv preprint arXiv:2011.03088, 2020a.\\n\\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki, Haibo Ding, and Graham Neubig. X-FACTR: multilingual factual knowledge retrieval from pretrained language models. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 5943\u20135959. Association for Computational Linguistics, 2020b. doi: 10.18653/v1/2020.emnlp-main.479. URL https://doi.org/10.18653/v1/2020.emnlp-main.479.\\n\\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language models know. Trans. Assoc. Comput. Linguistics, 8:423\u2013438, 2020c. doi: 10.1162/tacl_a_00324. URL https://doi.org/10.1162/tacl_a_00324.\\n\\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pp. 1601\u20131611. Association for Computational Linguistics, 2017. doi: 10.18653/V1/P17-1147. URL https://doi.org/10.18653/v1/P17-1147.\"}"}
{"id": "9OevMUdods", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "9OevMUdods", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Results of different LLMs using Few-shot w/ CoT prompts across different tasks.\\n\\n| Task                          | Multifaceted | Structural | Adversarial | Temporal | Real-World | Domain-Specific | Multi-lingual |\\n|------------------------------|--------------|------------|-------------|----------|------------|----------------|--------------|\\n| Acc. F1                      |              |            |             |          |            |                |              |\\n| OPT-6.7B                     | 34.5         | 24.1       | 45.5        | 30.9     | 51.8       | 51.7           | 30.0         |\\n| 53.7                         | 27.5         | 28.2       | 28.3        | 16.2     | 17.7       |                 |              |\\n| BLOOM-7B                     | 10.7         | 13.5       | 0.8         | 3.5      | 2.0        | 3.7            | 3.7          |\\n|                             |              |            |             |          | 7.7        | 5.4            | 8.5          |\\n| LLaMA-7B                     | 38.3         | 33.9       | 44.1        | 32.1     | 43.2       | 46.1           | 41.6         |\\n|                             |              |            |             |          | 30.0       | 26.4           | 26.3         |\\n| Alpaca-7B                    | 38.6         | 28.8       | 48.0        | 23.6     | 46.4       | 35.1           | 49.6         |\\n|                             |              |            |             |          | 49.6       | 26.1           | 24.5         |\\n| Vicuna-7B                    | 44.2         | 36.0       | 49.7        | 36.3     | 59.0       | 59.2           | 50.1         |\\n|                             |              |            |             |          | 37.6       | 49.0           | 41.8         |\\n| Vicuna-13B                   | 49.9         | 45.3       | 48.1        | 37.9     | 58.9       | 60.0           | 45.4         |\\n|                             |              |            |             |          | 37.6       | 49.0           | 41.8         |\\n| ChatGLM-6B                   | 41.0         | 36.0       | 46.8        | 35.7     | 51.5       | 48.6           | 39.4         |\\n|                             |              |            |             |          | 32.4       | 48.9           | 34.8         |\\n| Flan-T5-11B                  | 49.2         | 49.4       | 43.5        | 33.7     | 54.7       | 56.6           | 31.6         |\\n|                             |              |            |             |          | 30.6       | 31.1           | 31.1         |\\n| Text-Davinci-002             | 47.7         | 47.7       | 50.8        | 38.4     | 64.2       | 64.3           | 33.9         |\\n|                             |              |            |             |          | 64.3       | 31.1           | 51.7         |\\n| Text-Davinci-003             | 51.1         | 47.8       | 44.3        | 33.7     | 64.1       | 63.7           | 41.4         |\\n|                             |              |            |             |          | 63.7       | 35.1           | 48.0         |\\n| GPT-3.5-Turbo                | 53.6         | 53.1       | 44.8        | 37.8     | 67.4       | 67.4           | 37.4         |\\n\\nContent considerations, and extensive considerations often overshadow factual discernment tasks, causing incorrect factual label outputs. In contrast, for instruction-tuned LLMs, the CoT method facilitates enhanced exploration of factual entity relations in questions, resulting in accurate factual labels. See Appendix A.5 for detailed case analyses.\\n\\n\u2022 The OPT model, without being tuned to instructions, struggles significantly to output correct factual labels under the settings of Zero-shot and Zero-shot CoT, often resulting in either a repetition of the original question or a refusal to output any content at all. This issue is somewhat alleviated under the settings of Few-shot and Few-shot CoT.\\n\\n\u2022 Additionally, we studied the hyperparameters of LLMs. Due to limited computing resources, we only explored Vicuna-7B and Vicuna-13B. We found that as model parameters increase, performance on factual questions improves correspondingly, with an average increase of 5.4%. This indicates that LLMs with more parameters can store more world knowledge and have stronger factual knowledge recognition capabilities.\\n\\nIn Table 3, we present the factual performance of LLMs in various tasks under the Few-shot CoT setting. This reveals the relative difficulty LLMs have in understanding and responding to factual questions in different tasks, providing insights for future training of factual knowledge in LLMs.\\n\\nFrom Table 3, it is observed that LLMs exhibit relatively poorer performance on factual questions related to the real-world, domain-specific knowledge, and multilingualism, being on average 6.4% lower compared to the other four tasks. This is attributed to the fact that the training data for LLMs typically come from general domains and are not up-to-date, which indirectly inspires the exploration of retrieval-augmented LLMs (Ram et al., 2023). We analyze the LLMs in different tasks in Sec. 4.2.\\n\\n4.2 ANALYSIS\\n\\nIn this section, we explore LLMs' capabilities focusing on key areas like handling of multi-hop factual questions, proficiency in diverse prompt strategies, and tackling challenges like numerical reasoning and entity ambiguity. We also examine their performance on time-sensitive factual questions, against adversarial attacks, with fine-grained labels and prompts in multiple languages.\"}"}
{"id": "9OevMUdods", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\ncounts and Macro F1 scores of GPT-3.5-Turbo for each respective subset. The figure reveals a clear pattern: as the number of \u201chops\u201d increases, the reasoning chain for deriving conclusions from existing factual knowledge extends, necessitating heightened logical reasoning capabilities from the LLMs. Consequently, the performance of the LLMs exhibits diminishing trends.\\n\\nStructural Knowledge Analysis in LLMs\\nTo investigate whether LLMs can effectively memorize factual knowledge from structured data, we divided the structural task questions into three subsets according to evidence distribution: evidence in unstructured data (Only text), structured data (Only tables), or both (Combine text and tables). Figure 3(b) shows a notable decline (Avg. -5.5%) in GPT-3.5-Turbo\u2019s performance when evidence involves structured data, indicating LLMs\u2019 limited ability in extracting knowledge from structured tables. The LLMs also perform less effectively when handling questions requiring the combination of both evidence types, reflecting their incapacity to integrate diverse structured evidence effectively.\\n\\nAnalysis of Different Factual Questions Poses Challenges\\nTo assess the capabilities of LLMs in addressing various challenges, we partitioned each factual question within the structural task into six distinct challenges: 1) Entity disambiguation, 2) Other, 3) Multi-hop reasoning, 4) Combining tables and text, 5) Search terms not in claim, 6) Numerical reasoning, each centered around the most critical difficulty encountered during verification. Figure 3(c) illustrates GPT-3.5-Turbo\u2019s performance and data distribution across challenges. The extensive training and large-scale parameters enhance LLMs\u2019 performance in handling entity ambiguity. Longer reasoning chains and various forms of evidence challenge LLMs\u2019 factual abilities. When correct inference involves unmentioned entities, LLMs may lack necessary hints from factual questions, posing significant challenges. LLMs also exhibit deficiencies in precise numerical calculations due to the inherent hallucination phenomenon, resulting in subpar performance when numerical reasoning is needed for verification.\\n\\nTemporal Analysis\\nAs time progresses, the factuality of questions may undergo changes. This task encompasses such data, and we leverage this task to explore the ability of LLMs to adapt to factual changes. Figure 4(a) illustrates that GPT-3.5-Turbo exhibits a modest yet noticeable performance difference when dealing with outdated data as compared to updated data. This discrepancy arises from the fact that LLMs are pretrained on a corpus of text prior to a specific temporal point. Consequently, LLMs lack the capability to acquire real-time, up-to-date knowledge, rendering them unable to validate questions that hinge on the most recent information for accurate assessments.\\n\\nAdversarial Analysis\\nTo evaluate the robustness of LLMs to adversarial attacks, we divide the adversarial questions into three subsets: auto-generated questions from the corpus, manually modified synthesized questions yielding adversarial ones, and artificially created adversarial questions. Figure 3(b) presents the performance of GPT-3.5-Turbo on these three subsets. It is evident that following adversarial attacks, LLMs exhibit a substantial decrease in performance. Furthermore, factual questions that have undergone manual modifications or were artificially created prove to be more challenging compared to those that are automatically generated (Shen et al., 2023). This disparity could be attributed to the fact that automatically synthesized factual questions often contain explicit positive or negative words that hint at the outcome, and the exceptional comprehension abilities of LLMs enable them to accurately discern and provide the correct response in such cases.\\n\\nLabel Granularity Analysis\\nTo assess the effect of different label granularities on LLMs\u2019 performance, we conducted a manual re-labeling of the real-world task questions. Per the settings of Misra (2022), besides labeling as \u201cFactual\u201d, \u201cNon-Factual\u201d, and \u201cNot Enough Information\u201d, we also require them to annotate the dataset with six factual labels: \u201cFactual\u201d, \u201cMostly Factual\u201d, \u201cMostly False\u201d, \u201cPants\u201d, \u201cFire\u201d, and \u201cFactual\u201d. Figure 4(b) and (c) illustrate the performance of GPT-3.5-Turbo in three different tasks under Few-shot CoT setting.\"}"}
{"id": "9OevMUdods", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"False\u201d, \u201cNon-Factual\u201d, \u201cPants-Fire\u201d, and \u201cNot Enough Information\u201d. We also modified the prompt for GPT-3.5-Turbo for more intricate factual responses to test its competency with nuanced labels.\\n\\nResults in Figure 4(c) disclosed: 1) The results show that, in general, there is a significant decrease in performance (-23.83%) when transitioning from coarse-grained justification to fine-grained justification. With finer granularity, LLMs are not only required to assess the authenticity of each question but also to judiciously employ their knowledge base to precisely gauge the credibility of each factual questions. 2) When comparing the performance of coarse-grained labels with fine-grained labels, we observe significant drops in the three categories: \u201cFactual\u201d by 13.3%, \u201cNon-Factual\u201d by 23.2%, and \u201cNot Enough Information\u201d by 22.3%. This indicates that finer-grained labels introduce additional options that can potentially disrupt the original judgment of the LLMs. A potential remedy could be the aggregation of multiple judgments through voting (Wang et al., 2023a).\\n\\nTable 4: Macro F1 over Chinese and English prompts.\\n\\n| Prompt Strategy Analysis |\\n|-------------------------|\\n| In prior research, various CoT methods have been employed to enhance the performance of LLMs. These methods include 1) augmenting the number of in-context learning examples, 2) implementing self-consistency mechanisms, which alleviates the hallucination phenomenon through majority voting after multiple judgments of LLMs (Wang et al., 2023a), 3) incorporating complex instances as demos to steer the cognitive processes of LLMs (Fu et al., 2022), and 4) employing self-refinement strategies, which refines LLMs\u2019 answers through continuous feedback of another LLM on responses to achieve better results (Madaan et al., 2023) and so forth. Additionally, we examined the influence of utilizing declarative claims as instances of in-context learning. We randomly sampled 200 factual questions from each task of the Pinocchio, totaling 1400 questions, to compose Pinocchio-Lite with the aim of speeding up the testing of different prompt strategies. The performance results of various CoT methods are presented in Table 5. To maintain fairness, three in-context learning examples are employed in the complex CoT, self-consistency, self-refinement, and declarative claim methods. Different types of CoT prompts are shown in Appendix A.4.\\n\\nTable 5: Results in different domains obtained on the Pinocchio-Lite using different prompts.\\n\\n| Task       | Complex CoT | Self-Consistency | Self-Refinement | Declarative Claim |\\n|------------|-------------|------------------|-----------------|------------------|\\n| Multifaceted | 51.0        | 55.5             | 55.0            | 52.0             |\\n| Structural | 50.2        | 51.2             | 52.1            | 51.1             |\\n| Adversarial | 38.5        | 43.0             | 44.5            | 39.0             |\\n| Temporal   | 35.0        | 42.6             | 44.0            | 35.1             |\\n| Real-World | 37.5        | 49.5             | 53.5            | 45.5             |\\n| Domain-Specific | 47.2 | 54.8             | 59.2            | 47.2             |\\n| Multi-lingual | 39.0 | 43.0             | 42.5            | 39.0             |\\n| Overall    | 38.5        | 36.8             | 37.5            | 38.5             |\\n\\nIt is worth noting that 1) when the number of in-context learning examples is limited, the incremental improvement in performance is marginal upon increasing the number of examples. However, beyond a specific threshold, the addition of more examples gains more performance improvement. This could be due to the inability of LLMs to fully encapsulate the correct reasoning with fewer examples. 2) Concurrently, a fascinating observation is that the LLM\u2019s performance substantially deteriorates as the complexity of the CoT increases. This could stem from the difficulty LLMs have in extracting a generalized reasoning pattern from complex, multi-stage thinking processes with limited examples. 3) The self-consistency method markedly boosts performance by mitigating the hallucination issue in LLMs through consistency voting, enhancing their response accuracy. 4) In the self-refinement approach, the model might initially provide an incorrect response, but it can amend its mistakes through feedback and refine its answers. In the end, when no additional refinement is needed, the model often reaches the correct conclusion, achieving optimal performance. 5) Compared to the\"}"}
{"id": "9OevMUdods", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"shots method, the declarative claims method saw a 2.3% performance drop, illustrating that using questions as inputs better elicits factual knowledge than the original claim in the datasets.\\n\\nFactual Knowledge in Language Models\\n\\nPrevious research shows that LLMs can retain and utilize factual knowledge, effectively acting as knowledge bases (Petroni et al., 2019; 2020; Heinzerling & Inui, 2021). This acquired factual knowledge in language models during pretraining can be advantageous for knowledge-intensive tasks like question answering and fact checking (Roberts et al., 2020; Yu et al., 2023a; Pan et al., 2023). To evaluate the factual knowledge stored in language models, Petroni et al. (2019) employed cloze tests consisting of triples and prompts specifically designed to simulate missing objects. Jiang et al. (2020b) explored the role of prompts in retrieving factual information from language models and devised improved prompts for probing. However, Elazar et al. (2021) demonstrated the unreliability of rank-based probing methods with paraphrased context, leading to inconsistent findings. Cao et al. (2021b) contended that biased prompts and leakage of golden answers often lead to overestimations of LLMs' knowledge storage capability. Our method is more in line with Kadavath et al. (2022) and Lin et al. (2022), employing self-evaluation by querying the models to assess response accuracy regarding factual knowledge.\\n\\nMore recent studies have directed their focus towards the detection of hallucinations\u2014factually incorrect statements\u2014in the responses generated by LLMs. For instance, the SelfCheckGPT (Manakul et al., 2023) uses a sampling method to detect inconsistencies in LLM responses, identifying hallucinated claims. Alternatively, FactScore (Min et al., 2023) approaches the challenge by deconstructing generations into atomic facts\u2014concise statements\u2014and assigning binary labels to assess their veracity. Furthermore, Chern et al. (2023) introduced a tool-enhanced framework for hallucination detection encompassing five core components: claim extraction, query formulation, tool-based querying, evidence gathering, and validation of consistency. However, these contributions primarily target the identification of factual inaccuracies in the models' output. In contrast, our benchmark is primarily designed to evaluate the breadth and depth of factual knowledge within LLMs.\\n\\nBenchmarks for Large Language Models\\n\\nThe advent of LLMs has underscored the importance of exhaustive benchmarks for effective capability assessment. Presently, there are predominantly two types of existing benchmarks. One evaluates the general knowledge and reasoning capacities of LLMs, exemplified by the MMLU (Hendrycks et al., 2021a), a multi-choice benchmark that measures tasks from real-world tests and literature, spanning diverse subjects like elementary math, US history, computer science, and law. Moreover, benchmarks also exist for non-English languages (Huang et al., 2023) or in a bilingual context (Zhong et al., 2023). BIG-bench (Srivastava et al., 2022) is a collaborative benchmark examining LLMs' capabilities across 204 diverse tasks from various fields like linguistics, childhood development, software development, and more. HELM (Liang et al., 2022) employs 7 metrics over 42 tasks to assess LLMs, focusing on aspects from accuracy to robustness. Specific benchmarks like GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021a) target mathematical problem-solving, presenting elementary to competition-level problems. In program synthesis, HumanEval (Chen et al., 2021a) and MBPP (Austin et al., 2021) evaluate functional correctness through program synthesis from docstrings. Additional benchmarks address instruction following (Dubois et al., 2023), tool usage (Xu et al., 2023), and decision making (Liu et al., 2023). Our benchmark mainly focuses on factual knowledge, differing from ones like TruthfulQA (Lin et al., 2022), which specifically tests truthfulness in LLMs' generated responses, with questions structured to provoke imitative falsehoods over truthful answers.\\n\\nIn this work, our primary focus is the development of the Pinocchio benchmark, an extensive test bed encompassing 20,713 questions across seven varying complexity tasks, as a tool to investigate whether LLMs are capable of memorizing factual knowledge and reasoning on the basis of it. Upon applying the Pinocchio benchmark, we observe that various types of LLMs using different prompting strategies such as self-refine and self-consistency still have challenges in optimal performance on factual tasks. It is our hope that this novel benchmark will shed light on this area and act as a foundation for further improvements in LLMs' factual knowledge and reasoning abilities.\"}"}
{"id": "9OevMUdods", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We postulate that Pinocchio is unlikely to prove advantageous for those intending to fabricate deceptive models with malicious intent. To effectuate deception, a model must generate erroneous responses relatively infrequently, lest humans swiftly discern its unreliability. However, acquiring a low score on Pinocchio necessitates the provision of incorrect answers to virtually all questions. To be instrumental for malevolent purposes, a model must generate highly specific false statements, such as assertions concerning a maliciously targeted victim or a particular governmental policy. Yet, Pinocchio lacks coverage of highly specific subjects, offering instead a superficial overview of general factual topics.\\n\\nWhile Wikipedia and some news websites are exemplary collaborative resources, they inherently contain inaccuracies and noise, akin to any encyclopedia or knowledge repository. Consequently, we advise users of Pinocchio against making absolute assertions about the validated claims and discourage its utilization for the development of truth-revealing models. We refrained from collecting participants' personal data in any form. Participants accessed our online tool exclusively using an identification number. Generated assertions must solely incorporate information deemed as general world knowledge or sourced from Wikipedia, thereby excluding any personally identifiable information or offensive content.\\n\\nA.2 The Detailed Introduction to the LLMs\\n\\nFor pretraining models, OPT (Zhang et al., 2022) is an open-sourced large causal language model which performs similarly in performance to GPT-3 (Brown et al., 2020). BLOOM (Scao et al., 2022a) is an open-access multilingual large language model that is suitable for non-English facts. LLaMA (Touvron et al., 2023a) is probably the best open-weight foundation model so far that achieves the highest accuracy on various English benchmarks (e.g. MMLU (Hendrycks et al., 2021a)) within open-weight models. For instruction-tuned models, Alpaca (StanfordCRFM, 2023) is fine-tuned from the LLaMA model on 52K self-instructed demonstrations (Wang et al., 2023b). Alpaca behaves qualitatively similarly to OpenAI's Text-Davinci-003 on evaluation of single-turn instruction following. Vicuna is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT (ShareGPT, 2023). Flan-T5 (Chung et al., 2022) is an enhanced version of T5 that has been instruction fine-tuned in a mixture of tasks. ChatGLM is an open bilingual language model based on the General Language Model (Zeng et al., 2023). ChatGLM is trained on Chinese and English corpus, supplemented by instruction tuning, feedback bootstrap, and reinforcement learning with human feedback (RLHF; Ouyang et al. 2022). ChatGPT (OpenAI, 2022) from OpenAI that has undergone pretraining, instruction tuning, and RLHF. ChatGPT has been observed to have impressive capabilities in various aspects favoring reasoning capabilities (Qin et al., 2023).\\n\\nA.3 Results\\n\\nIn this section, we present the results of all LLMs across different tasks under three different settings: Zero-shot w/o CoT, Zero-shot w/ CoT, and Few-shot w/o CoT.\"}"}
{"id": "9OevMUdods", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: Results of different LLMs using Zero-shot w/o CoT prompts across different domains.\\n\\n| Task          | Multifaceted | Structural | Adversarial | Temporal | Real-World | Domain Specific | Multi-lingual |\\n|---------------|--------------|------------|-------------|----------|------------|----------------|---------------|\\n|               | Acc. F1      | Acc. F1    | Acc. F1     | Acc. F1  | Acc. F1    | Acc. F1        | Acc. F1       |\\n| OPT-6.7B      | -            | -          | -           | -        | -          | -              | -             |\\n| BLOOM-7B      | 21.9         | 17.8       | 24.9        | 17.9     | 32.4       | 36.3           | 17.6          |\\n| LLaMA-7B      | 30.7         | 28.8       | 38.3        | 29.3     | 30.8       | 35.6           | 37.9          |\\n| Alpaca-7B     | 34.8         | 21.6       | 47.9        | 23.7     | 47.7       | 35.7           | 52.9          |\\n| Vicuna-7B     | 38.6         | 35.4       | 19.4        | 16.8     | 50.8       | 53.9           | 42.0          |\\n| Vicuna-13B    | 45.0         | 41.1       | 43.9        | 31.0     | 57.1       | 56.7           | 45.9          |\\n| ChatGLM-6B    | 30.6         | 30.3       | 45.6        | 30.8     | 42.9       | 46.4           | 28.0          |\\n| Flan-T5-11B   | 39.2         | 29.6       | 11.2        | 10.2     | 56.2       | 49.9           | 12.9          |\\n| Text-Davinci-002 | 44.7       | 38.4       | 49.2        | 37.8     | 57.2       | 56.1           | 36.2          |\\n| Text-Davinci-003 | 50.9       | 48.9       | 36.4        | 29.5     | 58.7       | 57.9           | 51.7          |\\n| GPT-3.5-Turbo | 53.2         | 50.1       | 43.1        | 35.8     | 62.3       | 61.8           | 43.4          |\\n\\nTable 7: Results of different LLMs using Zero-shot w/ CoT prompts across different domains.\\n\\n| Task          | Multifaceted | Structural | Adversarial | Temporal | Real-World | Domain Specific | Multi-lingual |\\n|---------------|--------------|------------|-------------|----------|------------|----------------|---------------|\\n|               | Acc. F1      | Acc. F1    | Acc. F1     | Acc. F1  | Acc. F1    | Acc. F1        | Acc. F1       |\\n| OPT-6.7B      | -            | -          | -           | -        | -          | -              | -             |\\n| BLOOM-7B      | 17.0         | 20.2       | 10.1        | 12.6     | 12.0       | 19.2           | 6.9           |\\n| LLaMA-7B      | 20.3         | 23.5       | 29.5        | 26.4     | 18.3       | 26.2           | 25.7          |\\n| Alpaca-7B     | 38.3         | 28.9       | 42.7        | 22.4     | 38.6       | 36.1           | 38.0          |\\n| Vicuna-7B     | 29.4         | 35.8       | 45.7        | 31.6     | 4.4        | 8.3            | 49.0          |\\n| Vicuna-13B    | 46.7         | 42.8       | 46.2        | 32.7     | 58.8       | 58.6           | 47.3          |\\n| ChatGLM-6B    | 34.0         | 33.0       | 40.5        | 29.8     | 46.3       | 46.6           | 27.3          |\\n| Flan-T5-11B   | 49.6         | 49.1       | 19.2        | 16.8     | 58.2       | 58.2           | 21.7          |\\n| Text-Davinci-002 | 47.2       | 40.1       | 51.7        | 38.0     | 59.9       | 58.2           | 37.2          |\\n| Text-Davinci-003 | 52.7       | 51.1       | 37.5        | 31.3     | 61.0       | 59.5           | 40.8          |\\n| GPT-3.5-Turbo | 53.3         | 52.1       | 43.1        | 35.5     | 59.8       | 61.6           | 42.2          |\\n\\nTable 8: Results of different LLMs using Few-shot w/o CoT prompts across different domains.\\n\\n| Task          | Multifaceted | Structural | Adversarial | Temporal | Real-World | Domain Specific | Multi-lingual |\\n|---------------|--------------|------------|-------------|----------|------------|----------------|---------------|\\n|               | Acc. F1      | Acc. F1    | Acc. F1     | Acc. F1  | Acc. F1    | Acc. F1        | Acc. F1       |\\n| OPT-6.7B      | 38.1         | 30.1       | 45.9        | 27.1     | 46.8       | 32.4           | 28.7          |\\n| BLOOM-7B      | 32.7         | 22.5       | 8.8         | 9.0      | 43.5       | 32.6           | 23.8          |\\n| LLaMA-7B      | 34.8         | 21.9       | 40.5        | 27.0     | 47.4       | 38.4           | 45.5          |\\n| Alpaca-7B     | 34.9         | 25.4       | 48.0        | 22.6     | 43.4       | 32.5           | 48.0          |\\n| Vicuna-7B     | 34.5         | 27.6       | 40.1        | 25.4     | 54.5       | 53.3           | 30.1          |\\n| Vicuna-13B    | 47.9         | 42.5       | 48.9        | 31.4     | 54.7       | 53.1           | 53.4          |\\n| ChatGLM-6B    | 37.9         | 32.9       | 44.6        | 35.4     | 52.2       | 46.8           | 44.9          |\\n| Flan-T5-11B   | 42.3         | 35.0       | 12.4        | 11.7     | 57.7       | 53.6           | 15.1          |\\n| Text-Davinci-002 | 45.4       | 41.2       | 51.4        | 38.4     | 61.7       | 61.8           | 37.0          |\\n| Text-Davinci-003 | 59.6       | 43.4       | 48.1        | 33.7     | 62.0       | 61.8           | 46.4          |\\n| GPT-3.5-Turbo | 52.1         | 48.4       | 42.5        | 35.4     | 61.2       | 61.1           | 43.7          |\\n\\nA.4 Prompt Strategy\\n\\nIn this section, we provide the comprehensive versions of all the prompts utilized in both the main experiments and the subsequent analysis. We engaged native Chinese annotators to rephrase the English prompts while maintaining their semantic integrity, thus yielding Chinese prompts.\"}"}
{"id": "9OevMUdods", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stonehearst Asylum has an average rating of 6.2/10 on IMDb and a 53% approval rating on Rotten Tomatoes, based on 56 households, which could potentially slow down consumer spending, thus negatively impacting economic behavior can be complex and often two-sided, the answer is \\\"Not Sure Enough\\\".\\n\\nCritics, however, worry that the FairTax would disproportionately burden lower-income families. They also contend that by eliminating the corporate income tax, the FairTax would give the largest tax cuts, both in terms of dollars and as a percentage of income, to the highest-income households, suggesting some disadvantage in some instances disadvantage middle-class families compared to the wealthy. After searching for information about Trump's tax plan, it was found that according to analyses by independent organizations such as the Committee for a Responsible Budget, Donald Trump's tax plan would blow up the debt by over $5 trillion and would disadvantage middle-class families compared to the wealthy.\\n\\nThe FairTax is a proposal to reform the federal tax code of the United States. It's a type of consumption tax that would replace all federal income taxes, payroll taxes, gift and estate taxes, and the corporate income tax. It is intended to encourage innovation and investment, level the playing field for American companies to compete globally? The answer is \\\"Yes\\\".\\n\\nThe FairTax proposal was written and co-written by Moufida Tlatli and John Hulsman. It was an American tried-and-true proposal aimed at reducing the government's role in the economy, widening the tax base, and simplifying the tax code. The FairTax was also supported by economist Art Laffer, who is known for the Laffer curve. Donald Trump did not support the FairTax.\\n\\nAccording to the U.S. Customs and Border Protection (CBP), all undocumented immigrants are required to undergo a medical examination. While immigration can theoretically contribute to the spread of communicable diseases, in this case, there's no evidence that the 2015 measles outbreak was caused by immigration policy and it was actually mainly among unvaccinated U.S. residents in California. Also, measles was not widespread in Central and South America, which is the main sources of undocumented immigration across the southern U.S. border, at that time. Based on the information above, Rush Limbaugh's 2015 claim that \\\"immigrants are bringing in measles. We now have an outbreak of it all because of our immigration policy? Is false. It is impossible to compare his rushing average to Ell Roberson's. Therefore, the answer is \\\"Not Sure Enough\\\".\\n\\nHere are some examples:\\n\\n- Don Bradman was an Australian cricketer. Since Don Bradman was the greatest batsman of all time. Since Don Bradman was a cricketer, the question is not relevant, as batsmen are not soccer players. Therefore, the answer is \\\"Not Sure Enough\\\".\\n\\n- Is the claim made by David Perdue in 2017 that according to independent experts, Donald Trump's tax plan would change the national debt by an estimated $5.3 trillion over a decade and co-written by Moufida Tlatli and John Hulsman true that the Fair Tax proposal would blow up the debt by over $5 trillion and would disadvantage middle-class families compared to the wealthy?\\n\\n- The COVID-19 pandemic is caused by the SARS-CoV-2 virus, which is thought to have originated in bats and jumped to humans through an existing pathogen. World leaders are generally motivated by the desire to protect their populations, maintain social order, and quell popular unrest. Pandemics are typically caused by the global spread of a new infectious disease, often due to the mutation or evolution of an existing pathogen. The IEA has estimated that the spread of the pandemic has caused a GDP loss of $4.5 trillion, which is around 10% of the global GDP.\\n\\n- Is Jan Moir an opinion columnist and has won several awards for her writing? She was indeed an opinion columnist and has won several awards for her writing.\\n\\n- Did Ryan Mathews of the 2003 Kansas State Wildcats post a higher rushing average than his teammate Ell Roberson? There is no record of him playing for the 2003 Kansas State Wildcats football team. Ell Roberson III is a former American football quarterback who did not play for the 2003 Kansas State Wildcats football team. Therefore, the answer is \\\"Not Sure Enough\\\".\\n\\n- Has gas prices gone up 99 percent since Obama became president, making it the highest gas price increase since Carter? This is false. Gas prices have not gone up 99 percent since Obama became president.\\n\\n- Is Savages exclusively a German film? It is a co-production between the United States and Germany, not exclusively a German film. Therefore, the answer is \\\"No\\\".\\n\\n- Is The Season of Men a film directed by Oliver Stone? Oliver Stone is an American director, producer, and writer who has directed several films, including Platoon, Born on the Fourth of July, and Money. Therefore, the answer is \\\"Yes\\\".\\n\\n- Did Nathan Phillips falsely claim to be a U.S. military veteran who served in the Vietnam War? He did not falsely claim to be a U.S. military veteran who served in the Vietnam War; he was actually a U.S. military veteran who served in the Vietnam War era, but not in Vietnam itself. Official records show that he served in the Marine Corps Reserve.\\n\\n- Is there a capital called Mogadishu? Mogadishu is a city in East Africa, specifically in Somalia. Furthermore, the capital of Somalia is exactly Mogadishu. Therefore, the answer is \\\"Yes\\\".\\n\\n- Did Bill Bowerman serve during the Vietnam War era? He did not serve during the Vietnam War era, but served in the Marine Corps Reserve. Therefore, the answer is \\\"Yes\\\".\\n\\n- Are Pope Francis and the Mexican government supportive of the plan to create a new global currency? Pope Francis and the Mexican government have both expressed support for the plan to create a new global currency, which is known as the \\\"e-Chinchilla.\\\" The e-Chinchilla is a digital currency that would be used to facilitate international trade and commerce. Therefore, the answer is \\\"Yes\\\".\\n\\n- Is the capital of Somalia the capital of Somaliland? The capital of Somalia is Mogadishu, not Somaliland. Therefore, the answer is \\\"No\\\".\\n\\n- Did the United States send troops to Somalia to occupy Mogadishu and specifically in Somalia? Before joining the Daily Mail, Jan Moir was a senior correspondent for The Sunday Times. Jan Moir was born on 9 September 1966 in London, England. Jan Moir is an opinion columnist and has won several awards for her writing. Therefore, the answer is \\\"Yes\\\".\"}"}
{"id": "9OevMUdods", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We have introduced an additional scenario for investigation, which occurs frequently in the output generated by the zero-shot prompt method. We conducted an experiment involving three models: OPT, ChatGLM, and GPT-3.5-Turbo. These models are presented with the same set of questions, and their responses are shown in Figure 8. It is noteworthy that the OPT model, in both questions, reiterated the question itself without providing the corresponding answer. It is essential to mention that the actual output of the OPT model repeats the problem until it reaches the maximum output length (controlled by the \\\"max_length\\\" parameter), and we truncated the repeated portion.\\n\\nQ: Does Google Search search for hypotheses based on information entered into a device?\\n\\nOPT: Does Google Search search for hypotheses based on information entered into a device?\\n\\nChatGLM: Google Search can provide information based on the keywords entered into the search bar, but it does not search for hypotheses based on information entered into a device. Therefore, the answer is 'yes'.\\n\\nGPT-3.5-Turbo: No, Google Search does not search for hypotheses based on information entered into a device. Google Search retrieves information based on keywords and queries entered by users but does not generate or search for hypotheses. Therefore, the answer is 'yes'.\\n\\nQ: Was the television sitcom with the same name as Clueless (film) a spin-off?\\n\\nOPT: Was the television sitcom with the same name as Clueless (film) a spin-off?\\n\\nChatGLM: I'm sorry, but I am not aware of a television sitcom with the same name as Clueless. Clueless is a film and a television sitcom, but I do not have any information about a TV show with the same name. Therefore, the answer is not sure enough.\\n\\nGPT-3.5-Turbo: Yes, the television sitcom \\\"Clueless\\\" was indeed a spin-off of the 1995 film of the same name, which itself was inspired by Jane Austen's novel \\\"Emma.\\\" The show aired from 1996 to 1999 and continued the story of the film's characters, including Cher Horowitz and her friends, as they navigated high school life and social challenges in Beverly Hills. Therefore, the answer is 'yes'.\\n\\nThe OPT model even declined to generate any content when presented with the zero-shot prompt, resulting in a significant number of empty responses in the statistical results. In the first question, both ChatGLM and GPT-3.5-Turbo provided correct answers. However, in the second question, when faced with more detailed information inquiries, ChatGLM failed to produce a correct response, while GPT-3.5-Turbo demonstrated proficient reasoning and provided accurate answers. Increasing\"}"}
