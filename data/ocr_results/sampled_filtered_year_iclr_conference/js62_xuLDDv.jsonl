{"id": "js62_xuLDDv", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "js62_xuLDDv", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "js62_xuLDDv", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "js62_xuLDDv", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "js62_xuLDDv", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep metric learning (DML) extends standard metric learning to deep neural networks, where the goal is to learn metric spaces such that embedded data sample distance is connected to actual semantic similarities (Globerson & Roweis, 2006; Weinberger et al., 2006; Hoffer & Ailon, 2018; Wang et al., 2014). The explicit optimization of similarity makes deep metric spaces well suited for usage in unseen classes, such as zero-shot image or video retrieval or facial re-identification (Milbich et al., 2021; Roth et al., 2020c; Musgrave et al., 2020; Hoffer & Ailon, 2018; Wang et al., 2014; Schroff et al., 2015; Wu et al., 2018; Roth et al., 2020c; Brattoli et al., 2020; Hu et al., 2014; Deng et al., 2019; Liu et al., 2017). However, while DML is effective in establishing notions of similarity, work describing potential fairness issues is limited to individual fairness in standard metric learning (Ilvento, 2020), disregarding embedding models.\\n\\nIndeed, the impacts and metrics of fairness are well studied in machine learning (ML) generally, and representation learning specifically (Dwork et al., 2012; Mehrabi et al., 2019; Locatello et al., 2019b). This is especially true on high-risk tasks such as facial recognition and judicial decision-making (Chouldechova, 2017; Berk, 2017), where there are known risks to minoritized subgroups (Samadi et al., 2018). Yet, relatively little work has been done in the domain of DML (Rosenberg et al., 2021). It is crucial to address this knowledge gap \u2013 if DML embeddings are used to create upstream embeddings that facilitate downstream transfer tasks, biases may propagate unknowingly.\\n\\nTo tackle this issue, this work first proposes a benchmark to characterize fairness in balanced DML\u2014finDML. finDML introduces three subgroup fairness definitions based on feature space performance metrics\u2014recall@k, alignment and group uniformity. These metrics measure clustering ability and generalization performance via feature space uniformity. Thus, we select the metrics for our definitions to enforce independence between inclusion in a particular cluster or class, and a protected attribute (given the ground-truth label). We leverage existing datasets with fairness limitations (CelebA (Liu et al., 2015) and LFW (Huang et al., 2007)) and induce imbalance in training data of standard DML benchmarks, CARS196 (Krause et al., 2013) and CUB200 (Wah et al., 2011), in order to create an effective benchmark for fairness analysis in DML.\"}"}
{"id": "js62_xuLDDv", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Making use of finDML, we then perform an evaluation of 11 state-of-the-art (SOTA) DML methods representing frequently used losses and sampling strategies, including: ranking-based losses (Wang et al., 2014; Hoffer & Ailon, 2018), proxy-based (Kim et al., 2020) losses, semi-hard sampling (Schroff et al., 2015) and distance-weighted sampling (Wu et al., 2018). Our experiments suggest that imbalanced data during upstream embedding impacts the fairness of all benchmarks methods in both upstream embeddings (subgroup gaps up to 21%) as well as downstream classifications (subgroup gaps up to 45.9%). This imbalance is significant even when downstream classifiers are given access to balanced training data, indicating that data cannot naively be used to de-bias downstream classifiers from imbalanced embeddings.\\n\\nFinally, inspired by prior work in DML on multi-feature learning (Milbich et al., 2020), we introduce PARtial Attribute DE-correlation (PARADE). PARADE addresses imbalance by de-correlating two learned embeddings: one learnt to represent similarity in class labels, and one learnt to represent similarity in the values of a sensitive attribute, which is discarded at test-time. This creates a model in which the ultimate target class embeddings have been de-correlated from the sensitive attributes of the input. We note that as opposed to previous work on variational latent spaces, PARADE de-correlates a learned similarity metric. We find that PARADE reduces gaps of SOTA DML methods by up to 2% downstream in finDML.\\n\\nIn total, our contributions can be summarized as follows:\\n\\n1. We define finDML; introducing three definitions of fairness in DML to capture multi-faceted minoritized subgroup performance in upstream embeddings through focus on feature representation characteristics across subgroups, and five datasets for benchmarking.\\n\\n2. We analyze SOTA DML methods using finDML, and find that common DML approaches are significantly impacted by imbalanced data. We show empirically that learned embedding bias cannot be overcome by naive inclusion of balanced data in downstream classifiers.\\n\\n3. We present PARADE, a novel adaptation of previous zero-shot generalization techniques to enhance fairness guarantees through de-correlation of class discriminative features with sensitive attributes.\\n\\n2 BACKGROUND\\n\\nDeep Metric Learning\\n\\nDML extends standard metric learning by fusing feature extraction and learning a parametrized metric space into one end-to-end learnable setup. In this setting, a large convolutional network $\\\\psi$ provides the mapping to a feature space $\\\\Psi$, while a small network $f$, usually a single linear layer, generates the final mapping to the metric or embedding space $\\\\Phi$. The overall mapping from the image space $X$ is thus given by $\\\\phi = f \\\\circ \\\\psi$. Generally, the embedding space is projected on the unit hypersphere $S^{D-1}$ through normalization (Weisstein, 2002; Wu et al., 2018; Roth et al., 2020c; Wang & Isola, 2020) to limit the volume of the representation space with increasing embedding dimensionality. The embedding network $\\\\phi$ is then trained to provide a metric space $\\\\Phi$ that operates well under some predefined, usually non-parametric metric such as the Euclidean or cosine distance defined over $\\\\Phi$.\\n\\nTypical objectives used to learn such metric spaces range from contrastive ranking-based training using tuples of data, such as pairwise (Hadsell et al., 2006), triplet- (Schroff et al., 2015; Wu et al., 2018) or higher-order tuple-based training (Sohn, 2016; Wang et al., 2020a), procedures to bring down the effective complexity of the tuple space (Schroff et al., 2015; Harwood et al., 2017; Wu et al., 2018) or the introduction of learnable tuple constituents (Movshovitz-Attias et al., 2017; Qian et al., 2019; Kim et al., 2020).\\n\\nMore recent work (Milbich et al., 2020; Roth et al., 2020c; Jacob et al., 2019) extends standard DML training through incorporation of objectives going beyond just sole class label discrimination: e.g., through the introduction of artificial samples (Lin et al., 2018; Duan et al., 2018), regularization of higher-order moments (Jacob et al., 2019), curriculum learning (Zheng et al., 2019; Harwood et al., 2017; Roth et al., 2020a), knowledge distillation (Roth et al., 2020b) or the inclusion of additional features (DiVA) to produce diverse and de-correlated representations (Milbich et al., 2020).\\n\\nDML Evaluation\\n\\nStandard performance measures reflect the goal of DML: namely, optimizing an embedding space $\\\\Phi$ for best transfer to new test classes via learning semantic similarities. As...\"}"}
{"id": "js62_xuLDDv", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "js62_xuLDDv", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here, we iterate through some common DML criteria and batch mining strategies more formally than in the main paper. Throughout this section, let $X$ denote the input data, $\\\\phi(X)$ the embedded data, $Y$ the class label, and let $Y(x)$ denote the value of the ground truth class label for data instance $x$. Denote the set of all positive pairs with respect to class label $Y$ as $P = \\\\{(x_1, x_2) \\\\in X \\\\times X : Y(x_1) = Y(x_2), x_1 \\\\neq x_2\\\\}$. Denote the set of all negative pairs with respect to class label $Y$ as $N = \\\\{(x_1, x_2) \\\\in X \\\\times X : Y(x_1) \\\\neq Y(x_2)\\\\}$. We use the notation $(x_a, x_p, x_n) \\\\in X \\\\times X \\\\times X$ to denote a triplet with an anchor sample $x_a$, positive sample $x_p$ where $Y(x_a) = Y(x_p)$, and negative sample $x_n$ where $Y(x_a) \\\\neq Y(x_n)$.\\n\\n**Batch Sampling and Mining**\\n\\nThe batch sampling procedure in deep metric learning methods differ from that of generic deep classifiers in that canonical loss functions require tuples or pairs of samples in order to utilise ranking objectives as training surrogates to learn an appropriate similarity metric. To ensure that tuples with positive and negative examples can be extracted from the batch, the Samples-Per-Class-$n$ (SPC-$n$) heuristic (see e.g. Roth et al. (2020c)) is generally used, where commonly $n = 2, 4, 8$. Given a batch size $b$, the SPC-$n$ technique randomly selects $b/n$ classes from which $n$ training samples are then drawn randomly to be included in each batch $B$.\\n\\nAfter feeding the batch through the network, tuples are mined from the batch to use in the loss function. We refer to mining in this paper either as batch mining or overload the both as batch sampling terminology. The naive solution to tuple mining is random mining, in which all possible tuples of the form $(x_a, x_p, x_n)$ are considered and $b$ are randomly chosen from the batch. However, this method lacks the capacity to utilize valuable information about the current embedding space, and is prone to significant redundancy in the training signal Schroff et al. (2015); Wu et al. (2018).\\n\\n**Definition 4** (Random Mining). Hu et al. (2014) For each $x_a \\\\in B$, we randomly draw a positive example from $\\\\{x_p \\\\in B : Y(x_p) = Y(x_a), x_p \\\\neq x_a\\\\}$ and a negative example from $\\\\{x_n \\\\in B : Y(x_n) \\\\neq Y(x_a)\\\\}$ to form the triplet $(x_a, x_p, x_n)$.\\n\\nIntuitively, this could be mitigated by hard mining heuristics searching for negative samples that are closer to the anchor sample in the embedding space than positive samples, thereby always ensuring a significant training signal. Unfortunately, such approaches are prone to heavy overfitting, training instability and large gradient variance, thereby commonly resulting in less-than-optimal solutions (see e.g. Schroff et al. (2015); Harwood et al. (2017); Wu et al. (2018)). Recent approaches thus establish more lenient heuristics, such as through the introduction of slack parameters to the hard mining objective (e.g. semi-hard mining Schroff et al. (2015) or softhard mining Roth & Brattoli (2019)).\\n\\n**Definition 5** (Semi-hard Mining). For each $x_a \\\\in B$, we randomly draw a positive example from $\\\\{x_p \\\\in B : Y(x_p) = Y(x_a), x_p \\\\neq x_a\\\\}$, and a negative example from the set $\\\\{x_n \\\\in B : Y(x_n) \\\\neq Y(x_a), \\\\|\\\\phi(x_a) - \\\\phi(x_n)\\\\|_2^2 \\\\leq \\\\|\\\\phi(x_a) - \\\\phi(x_p)\\\\|_2^2 + \\\\gamma \\\\|\\\\phi(x_a) - \\\\phi(x_p)\\\\|_2^2\\\\}$ where $\\\\gamma \\\\in \\\\mathbb{R}$ is a slack parameter, to form the triplet $(x_a, x_p, x_n)$.\\n\\nWhile other adaptive means (e.g. Harwood et al. (2017); Roth et al. (2020a)) have shown strong performance improvements, modern predefined heuristics such as distance-weighted tuple mining Wu et al. (2018) offer a better cost-to-performance tradeoff Roth et al. (2020a). Here, the heuristic leverages the fact that embeddings are commonly normalized to have unit $L_2$ norm for regularization purposes Wu et al. (2018). This ensures a distribution over a unit hypersphere, in which explicit pairwise distributions can be established Weisstein (2002); Wu et al. (2018). By inverting this distribution, distance-weighted mining can thus encourage a much more diverse coverage of tuple difficulties, improving generalization performance and reducing gradient variance Wu et al. (2018).\"}"}
{"id": "js62_xuLDDv", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Definition 6\\n\\n(Distance-weighted)\\n\\nFor embedding spaces normalized to the \\\\((D-1)\\\\)-dimensional hypersphere \\\\(S^{D-1}\\\\), we have the following pairwise sampling distribution \\\\(q(\\\\cdot, \\\\cdot)\\\\):\\n\\n\\\\[\\nq(d(\\\\phi(x_i), \\\\phi(x_j))) \\\\propto d(\\\\phi(x_i), \\\\phi(x_j))^{D-2} \\\\left(1 - d(\\\\phi(x_i), \\\\phi(x_j))^{D-3}/2\\\\right)\\n\\\\]\\n\\nfor embedding pairs \\\\((\\\\phi(x_i), \\\\phi(x_j)) \\\\in S^{D-1}\\\\) and Euclidean distance \\\\(d(\\\\cdot, \\\\cdot)\\\\). For each \\\\(x_a \\\\in B\\\\), we randomly draw a positive example from \\\\(\\\\{x_p \\\\in B: Y(x_p) = Y(x_a), x_p \\\\neq x_a\\\\}\\\\), and sample a negative example based on an inverse distance distribution w.r.t. \\\\(q\\\\):\\n\\n\\\\[\\nP(x_n | x_a) \\\\propto \\\\min(\\\\lambda, q^{-1}(d(\\\\phi(x_i), \\\\phi(x_j))))\\n\\\\]\\n\\nwhere \\\\(\\\\lambda \\\\in \\\\mathbb{R}\\\\) defines a clipping parameter to avoid potentially erroneous training samples.\\n\\nExamined Objectives\\n\\nThe primary goal of DML loss functions is to provide a training surrogate that implicitly optimizes for desired metric space quantities by narrowing down the expected distance between positive pairs of samples and expanding on the expected distance between negative pairs of samples in the embedding space. Most commonly employed pair Hadsell et al. (2006) and tripled-based Schroff et al. (2015); Hoffer & Ailon (2018) ranking losses penalize close negative pairs and disparate positive pairs up to a predefined margin to avoid overclustering. Using \\\\(P(x)\\\\) to denote all positive pairs containing \\\\(x\\\\):\\n\\n\\\\[\\nP(x) = \\\\{(x_1, x_2) \\\\in P: x_1 = x\\\\}\\n\\\\]\\n\\nand \\\\(N(x)\\\\) to denote all negative pairs containing \\\\(x\\\\):\\n\\n\\\\[\\nN(x) = \\\\{(x_1, x_2) \\\\in N: x_1 = x\\\\}\\n\\\\]\\n\\nwe define\\n\\nDefinition 7\\n\\n(Contrastive)\\n\\nHadsell et al. (2006) Given a batch \\\\(B\\\\), and pairs of samples \\\\(S\\\\) over \\\\(B \\\\times B\\\\), the contrastive objective is defined as:\\n\\n\\\\[\\nL_{\\\\text{contr}} = \\\\frac{1}{b} \\\\sum_{(x_i, x_j) \\\\in S} \\\\left[I_{Y(x_i) = Y(x_j)} \\\\cdot d(\\\\phi(x_i), \\\\phi(x_j)) + I_{Y(x_i) \\\\neq Y(x_j)} \\\\cdot \\\\left(\\\\gamma - d(\\\\phi(x_i), \\\\phi(x_j))\\\\right)\\\\right]\\n\\\\]\\n\\nwith margin \\\\(\\\\gamma\\\\).\\n\\nDefinition 8\\n\\n(Triplet)\\n\\nHoffer & Ailon (2018) The triplet loss extends the contrastive objective with sample triplets and can be defined as:\\n\\n\\\\[\\nL_{\\\\text{tripl}} = \\\\frac{1}{b} \\\\sum_{(x_a, x_p, x_n) \\\\in T} \\\\left[I_{Y(x_a) = Y(x_p)} \\\\cdot \\\\left(d(\\\\phi(x_a), \\\\phi(x_p)) - d(\\\\phi(x_a), \\\\phi(x_n)) + \\\\gamma\\\\right)\\\\right]\\n\\\\]\\n\\nwith margin \\\\(\\\\gamma\\\\).\\n\\nMargin loss extends the triplet objective through the inclusion of a learnable boundary \\\\(\\\\beta\\\\) between positive and negative pairs Wu et al. (2018). In our experiments, we utilise \\\\(\\\\beta = 1/2\\\\). These criteria are widely used (see e.g. Roth et al. (2020c); Musgrave et al. (2020)) and require mining to make use of the batch information.\\n\\nDefinition 9\\n\\n(Margin)\\n\\nWu et al. (2018) The margin objective integrates the learnable distance boundary \\\\(\\\\beta\\\\) between positive and negative pairs of samples for a relative ordering of pairs with respect to \\\\(\\\\beta\\\\) as:\\n\\n\\\\[\\nL_{\\\\text{margin}} = \\\\sum_{(x_i, x_j) \\\\in S} \\\\left[I_{Y(x_i) = Y(x_j)} \\\\cdot d(\\\\phi(x_i), \\\\phi(x_j)) + I_{Y(x_i) \\\\neq Y(x_j)} \\\\cdot (\\\\gamma - d(\\\\phi(x_i), \\\\phi(x_j)))\\\\right]\\n\\\\]\\n\\nGoing beyond pairs and triplets, one can also consider the case of more general n-tuples, which was investigated e.g. in the N-Pair objective Sohn (2016) and the Multisimilarity loss Wang et al. (2020a).\"}"}
{"id": "js62_xuLDDv", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Definition 10 (N-Pair).\\nSohn (2016) N-Pair loss is a simple augmentation of the triplet framework in which all negatives in the batch $B$ are incorporated in the objective function as:\\n\\n$$L_{npair} = \\\\frac{1}{b} \\\\sum_{(x_a, x_p) \\\\in B} \\\\log \\\\left( \\\\frac{1}{\\\\sum_{x_n \\\\in B \\\\setminus \\\\{y(x_a)\\\\}} \\\\exp(\\\\phi(x_a) \\\\cdot T \\\\phi(x_n) - \\\\phi(x_a) \\\\cdot T \\\\phi(x_p))} + \\\\nu b \\\\cdot \\\\sum_{x_i \\\\in B} \\\\|\\\\phi(x_i)\\\\|^2 \\\\right)$$\\n\\nwhere $\\\\nu$ denotes an embedding regularization parameter due to slow convergence for normalized embeddings stated in Sohn (2016).\\n\\nDefinition 11 (Multisimilarity).\\nWang et al. (2020a) Multisimilarity loss fits into the ranking loss category, but in addition to evaluation of cosine similarity between positive-anchor pairs and negative-anchor pairs, the objective evaluates positive-positive and negative-negative pairs with respect to the anchor:\\n\\n$$s^*_{c}(x_i, x_j) = \\\\begin{cases} s_c(\\\\phi(x_i), \\\\phi(x_j)) & \\\\text{if } s_c(\\\\phi(x_i), \\\\phi(x_j)) > \\\\min_{x_j \\\\in P(x_i)} s_c(\\\\phi(x_i), \\\\phi(x_j)) - \\\\epsilon \\\\\\\\ s_c(\\\\phi(x_i), \\\\phi(x_j)) & \\\\text{if } s_c(\\\\phi(x_i), \\\\phi(x_j)) < \\\\max_{x_k \\\\in N(x_i)} s_c(\\\\phi(x_i), \\\\phi(x_k)) + \\\\epsilon \\\\\\\\ 0 & \\\\text{otherwise} \\\\end{cases}$$\\n\\n$$L_{multisim} = \\\\frac{1}{b} \\\\sum_{x_i \\\\in B} \\\\alpha \\\\log \\\\left( \\\\frac{\\\\sum_{x_j \\\\in P(x_i)} \\\\exp(-\\\\alpha (s^*_{c}(\\\\phi(x_i), \\\\phi(x_j)) - \\\\lambda))}{\\\\prod_{x_k \\\\in N(x_i) \\\\setminus \\\\{y(x_i)\\\\}} \\\\exp(-\\\\alpha (s^*_{c}(\\\\phi(x_i), \\\\phi(x_k)) - \\\\lambda))} \\\\right) + \\\\beta \\\\log \\\\left( \\\\frac{\\\\prod_{x_j \\\\in P(x_i) \\\\setminus \\\\{y(x_i)\\\\}} \\\\exp(-\\\\beta (s^*_{c}(\\\\phi(x_i), \\\\phi(x_j)) - \\\\lambda))}{\\\\prod_{x_k \\\\in N(x_i)} \\\\exp(-\\\\beta (s^*_{c}(\\\\phi(x_i), \\\\phi(x_k)) - \\\\lambda))} \\\\right)$$\\n\\nNotably, the Multisimilarity loss employs a masking process as a stand-in for the lack of batch-mining heuristic. While this proves to be similarly successful in addressing the tuple sampling complexity issue, this can also be addressed through the usage of proxy-samples. These are dummy variables that represent various contextual properties (such as mean class representations) to serve as standing for actual samples, which is found e.g. in the ArcFace Deng et al. (2019) or ProxyNCA loss Movshovitz-Attias et al. (2017).\\n\\nDefinition 12 (Proxy-NCA).\\nKim et al. (2020) ProxyNCA learns class proxies, or class centers, which each represent a class in the set of unique classes $Y$. Then, each anchor from the batch is sampled and a positive or negative proxy $\\\\psi_c \\\\in \\\\mathbb{R}^d$ per class $c \\\\in Y$ is introduced in lieu of a positive or negative sample, respectively, giving:\\n\\n$$L_{proxy} = -\\\\frac{1}{b} \\\\sum_{x_i \\\\in B} \\\\log \\\\left( \\\\frac{\\\\exp(-d(\\\\phi(x_i), \\\\psi_{y(x_i)})/\\\\lambda)}{\\\\prod_{c \\\\in Y \\\\setminus \\\\{y(x_i)\\\\}} \\\\exp(-d(\\\\phi(x_i), \\\\psi_c)/\\\\lambda)} \\\\right)$$\\n\\nDefinition 13 (Arcface).\\nDeng et al. (2019) Arcface combines proxy and angular loss methods (e.g. in Wang et al. (2017)) to enforce an angular margin between the embeddings $\\\\phi$ and a proxy (or approximate center) $W \\\\in \\\\mathbb{R}^{c \\\\times d}$ for each class, giving the following:\\n\\n$$L_{arc} = -\\\\frac{1}{b} \\\\sum_{x_i \\\\in B} \\\\log \\\\left( \\\\frac{\\\\exp(s \\\\cdot \\\\cos(W^T y(x_i)) + \\\\gamma)}{\\\\prod_{x_j \\\\in B \\\\setminus \\\\{y(x_i)\\\\}} \\\\exp(s \\\\cdot \\\\cos(W^T y(x_j)) + \\\\gamma)} \\\\right)$$\\n\\nwhere the angular component is encoded in additive angular margin penalty $\\\\gamma$, and $s$ is a scaling parameter, which denotes the radius of the effective utilized hypersphere $S$. Standard Performance Metrics\\nPerformance metrics in deep metric learning aim to capture the quality of the similarity metric learned by the deep embedding model. Therefore, standard performance metrics in DML reflect the closeness between samples of the same class, the separability...\"}"}
{"id": "js62_xuLDDv", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the embedding space, we analyze fairness via performance gaps between minoritized groups and majoritized groups, or worst-group performance gaps (Lahoti et al., 2020). For fairness of the feature representations, we compute gaps in three metrics: recall@k and NMI for intra- and inter-class distance (Section 3.2), and the uniformity measure $\\\\text{KL}_{\\\\text{corr}}$ corresponding to Definition 3 (defined in Section 3.1).\\n\\nTraining and Evaluation on Downstream Classifiers\\n\\nTo link fairness performance in the embedding space to downstream classification (in which more extensive prior work has been completed), we train downstream classifiers and evaluate classification bias. After training the DML model with the aforementioned criteria, the network is fixed. The output embeddings from the image training datasets, in addition to the class labels, are used to train four downstream classification models: logistic regression (LR), support vector machine (SVM), K-Means (KM), and random forest (RF) (Pedregosa et al., 2011). In the manually imbalanced upstream setting, we train downstream classifiers on the original balanced image datasets to ascertain if bias incurred in the embedding can propagate downstream even if the downstream classifier is trained with real balanced data.\\n\\nWe execute class imbalanced experiments for CARS196 and CUB200 and vary the level of imbalance between minoritized and majoritized classes in the upstream training set.\\n\\nPARADE Configuration\\n\\nWe test Partial Attribute De-correlation, PARADE, by training models in the listed settings: manually color imbalanced dataset for CUB200, CelebA and LFW. The attribute used to train the sensitive attribute embedding for each dataset, and the attribute used for fairness evaluation. We compare PARADE with margin loss and distance-weighted sampling (Wu et al., 2018) to standard margin loss and distance-weighted sampling.\\n\\n6 RESULTS\\n\\n6.1 SOTA DML METHODS\\n\\nOur experiments indicate that current DML methods encounter crucial fairness limitations in the presence of imbalanced training data. Table 1 (along with a corresponding table for CARS196 in the Supplemental) demonstrate that gaps in the manually class imbalanced setting are greater than the balanced control setting. In four combinations of loss functions and sampling strategies, we do not observe a scenario in which the class imbalanced setting achieves a smaller gap than the control in the embedding space, nor the downstream classification. This is particularly significant due to the nature of sampling strategies studied (Wu et al., 2018; Schroff et al., 2015), which batch samples to force the model to correct \u201chard\u201d examples. The results validate finDML as a benchmark and framework for fairness through the lens of well-studied fairness characterization in classification.\\n\\nInterestingly, Table 1 displays non-negligible gaps in downstream performance metrics recall and precision even in the balanced control case. This could represent stenography of underlying structures in the data, such as car color or bird size. More likely, however, these gaps are due to use of macro-averaging in recall and precision calculations. Nonetheless, the manually class imbalanced settings consistently produce larger gaps.\\n\\n6.2 PROPAGATION OF BIAS TO DOWNSTREAM TASKS\\n\\nThe tabular results emphasize a significant result: naive re-balancing with real data downstream cannot overcome bias incurred in the upstream embedding in any setting studied. Indeed, Table 1 exhibits propagation of bias from upstream embeddings (trained on imbalanced data) to downstream tasks (trained on fixed upstream embeddings with a re-balanced dataset). To provide additional context for the result, we direct to increasing use of DML models as components of larger classification models. This trend is arising in literature such as supervised contrastive learning, and recent developments in pre-training and lifting DML models for classification (Khosla et al., 2020). This necessitates tackling bias in the representation space of DML as opposed to patches downstream, and emphasizes the importance of defining fairness in this setting as done in our work.\\n\\nImpact of imbalance degree on lack of fairness\\n\\nFigure 3 shows that gaps in downstream classification mimic those upstream, even as we vary the level of imbalance introduced when training the upstream embedding. Here, the random forest classifier sees greater gaps in downstream metrics.\"}"}
{"id": "js62_xuLDDv", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Gap study on CUB200-2011. Average gaps in representation space and downstream classification (logistic regressor) over 10 seeds between minoritized and majoritized classes in manually class imbalanced experiments (Imbalanced) and control experiments (Balanced) for CUB200-2011. Results for CARS196 are available in the supplementary with similar conclusions.\\n\\n| Objective                  | Balanced | Imbalanced |\\n|----------------------------|----------|------------|\\n| Margin \u00b7 Distance          | 0.212 \u00b1 0.029 | 0.187 \u00b1 0.031 |\\n| Margin \u00b7 Semi-hard         | 0.02 \u00b1 0.007  | 0.187 \u00b1 0.031 |\\n| UPSTREAM EMBEDDING        |          |            |\\n| Recall@1                  | 0.017 \u00b1 0.007  | 0.017 \u00b1 0.007  |\\n| NMI                       | 0.112 \u00b1 0.012  | 0.092 \u00b1 0.017  |\\n| U KL                      | 0.002 \u00b1 0.004  | 0.002 \u00b1 0.004  |\\n| DOWNSTREAM CLASSIFICATION |          |            |\\n| Precision                 | 0.339 \u00b1 0.007  | 0.339 \u00b1 0.007  |\\n| Recall                    | 0.36 \u00b1 0.007   | 0.351 \u00b1 0.005  |\\n| Accuracy                  | 0.014 \u00b1 0.002  | 0.014 \u00b1 0.002  |\\n\\nFigure 3: Impact of varying imbalance between the minoritized and majoritized classes on upstream embedding and downstream classifier (RF) in the manually class imbalanced CARS196 experiments. (Note: the imbalance percentage 50\u221250 is equivalent to the balanced setting). Gaps increase both upstream and downstream with more imbalance introduced to the upstream training data.\\n\\nthan the control, even when manual imbalance is set at 40\u221260 upstream, and the downstream training dataset is balanced. For results with additional downstream classifiers, see Supplemental.\\n\\nThis experiment demonstrates that the propagation of bias to downstream will occur even with lower levels of imbalance, and does not appear to depend on the downstream classifier chosen.\\n\\n6.3 REDUCED SUBGROUP GAPS THROUGH PARTIAL DE-CORRELATION WITH SENSITIVE ATTRIBUTE\\n\\nTable 2a shows results for performance gaps between relevant subgroups in both facial recognition datasets. PARADE shows strong results for CUB200 bird color dataset, primarily reducing gaps downstream and accordingly to recall@1 (Definition 1). PARADE can reliably reduce gaps for both the representation space and downstream classifiers on LFW. Interestingly, we observe that the majoritized subgroup (\u201cWhite\u201d) had worst performance of all \u201cRace\u201d subgroups (see Supplemental), contrary to previous results (Samadi et al., 2018).\\n\\nAs such, we measure gaps between the worst-performing subgroup and others. Due to the great number of singleton classes in LFW, recall@1 is discarded as a metric. Note: minoritized subgroups can still encounter notable bias across other axes more difficult to measure (Radford & Espenshade, 2014).\"}"}
{"id": "js62_xuLDDv", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Comparison between PARADE and standard losses with distance-weighted sampling of average gaps in representation space and downstream classification (logistic regressor) over 3 seeds between minoritized and majoritized groups in: (a) facial dataset studies, namely on CelebA (w.r.t. \u201cFitzpatrick Skintone\u201d) and between worst-performing subgroup and other subgroups in LFW (w.r.t. \u201cRace\u201d) with Margin loss and (b) bird color experiments for CUB200 image dataset (w.r.t. color) with Margin and Triplet loss. Bold represents smaller gap (better fairness performance).\\n\\nFacialDatasets: CelebA (skintone), LFW (race)\\n\\nRecall@1:\\n- PARADE Margin \u00b7 Distance: 0.085 \u00b1 0.009, 0.122 \u00b1 0.005\\n- UPSTREAM: 0.075 \u00b1 0.014, 0.068 \u00b1 0.013\\n\\nNMI:\\n- PARADE Margin \u00b7 Distance: -0.012 \u00b1 0.003, -0.002 \u00b1 0.003\\n- UPSTREAM: 0.041 \u00b1 0.003, 0.048 \u00b1 0.003\\n\\nU KL:\\n- PARADE Margin \u00b7 Distance: -0.04 \u00b1 0.011, -0.03 \u00b1 0.007\\n- UPSTREAM: 0.163 \u00b1 0.003, 0.165 \u00b1 0.005\\n\\nFor CelebA, we find for standard methods the minoritized subgroups to generally perform worst. PARADE excels at gap reduction upstream but encounters larger subgroup gaps downstream compared to standard methods. While PARADE does reduce downstream gaps between light skintones (I, II, and III), and the two lighter dark skintones (IV, V), gaps increase between lighter skintones and the darkest skintone (VI) (see Supplemental). Because skintone VI constitutes <1% of the CelebA dataset, PARADE is likely not able to learn similarity between faces over attributes besides skintone. And PARADE is prevented from learning similarities based on skintone due to de-correlation. In such settings, PARADE could be combined with oversampling minoritized subgroups to ensure better performance.\\n\\nIn general, the results show promising benefits of PARADE to adequately address and improve on the challenge of subgroup gaps for DML models used in facial recognition; and in the standard DML dataset CUB200, for recall@1 upstream (Definition 1) and across metrics downstream (Table 2b).\\n\\nDiscussion\\nIn this work, we introduce the FinDML benchmark, a framework for fairness in deep metric learning (\u00a73.2). We demonstrate the fairness limitations of established DML techniques, and the surprising propagation of embedding space bias to downstream classifiers. Importantly, we find that this bias cannot be addressed at the level of downstream classifiers but instead needs to be addressed at the DML stage. We investigate the limit of this propagation in manually introduced imbalance, and finally show that PARADE can reduce subgroup gaps in several settings.\\n\\nLimitations\\nPARADE suffers from pitfalls similar to other \u201cfairness with awareness\u201d methods: PARADE uses information only on pre-defined sensitive attributes and therefore can be unfair w.r.t. other sensitive attributes. PARADE does have an advantage in addressing the combinatorial number of attributes considered in multi-attribute fairness through DML, which will scale sub-combinatorially in time/space complexity. We also note that subgroup gaps are not sufficient to capturing societal understandings of fairness, and there is no consensus as to how to remedy such gaps (Chouldechova & Roth, 2018; Dwork et al., 2012; Hardt et al., 2016; Zemel et al., 2013; Zafar et al., 2017). Additionally, while PARADE intentionally optimizes Definitions 1 and 2, we provide no explicit guarantee and optimization of uniformity, Definition 3, remains an open problem. Finally, PARADE does incur slight decrease in overall performance, similar to other methods (Wick et al., 2019) (see Supplemental for per-subgroup performance and additional fairness-utility trade-off analysis for PARADE).\"}"}
{"id": "js62_xuLDDv", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The work presented here deals with fairness in deep metric learning. A portion of our studies in the paper focus on CARS196 and CUB200-2011 datasets, which have consistently been used in benchmarking novel DML frameworks (Krause et al., 2013; Wah et al., 2011). The fairness analysis considered for CUB200-2011 deals with bird color, which does not, to our knowledge, correspond with any societal problems relating to fairness. Nonetheless, as CUB200-2011 is used in a litany of papers for SOTA performance comparison, finDML includes CUB200 so that DML methods can be analyzed w.r.t. fairness on a dataset used in their original paper. We do include facial recognition datasets and tasks and analyze fairness with respect to facial attributes. Facial recognition does raise ethical concerns in practice. We note that our paper attempts to address primary social concerns in facial identity recognition. We do not encourage the task of facial attribute recognition, and solely use labeled attributes that correspond to known axes of bias for fairness analysis (e.g. Race and Skintone). As PARADE has solely been tested in two widely used public facial recognition datasets, we cannot guarantee fairness nor privacy in practical settings with private facial datasets.\\n\\nReproducibility Statement\\nAdditional experimental results discussed in the main paper and others are contained in Supplemental C. Implementation details including attribute information, generation of attributes, training parameters, metric calculation and gap computation are listed in Supplemental D. Code available here: https://github.com/ndullerud/dml-fairness.\\n\\nACKNOWLEDGEMENTS\\nWe would like to acknowledge and thank our sponsors, who support our research with financial and in-kind contributions: CIFAR through the Canada CIFAR AI Chair, Intel, NFRF through an Exploration grant, NSERC through the COHESA Strategic Alliance, International Max Planck Research School for Intelligent Systems (IMPRS-IS), European Laboratory for Learning and Intelligent Systems (ELLIS) PhD program, Mitacs Accelerate through internship program, and Microsoft Research. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute. We would like to thank members of the CleverHans Lab and HealthyML Lab for their invaluable feedback.\\n\\nREFERENCES\\nTameem Adel, Isabel Valera, Zoubin Ghahramani, and Adrian Weller. One-network adversarial fairness. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 2412\u20132420, 2019.\\nAlexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A Saurous, and Kevin Murphy. Fixing a broken elbo. In International Conference on Machine Learning, pp. 159\u2013168. PMLR, 2018.\\nAlexander Amini, Ava P Soleimany, Wilko Schwarting, Sangeeta N Bhatia, and Daniela Rus. Uncovering and mitigating algorithmic bias through learned latent structure. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp. 289\u2013295, 2019.\\nRichard Berk. An impact assessment of machine learning risk forecasts on parole board decisions and recidivism. Journal of Experimental Criminology, 13(2):193\u2013216, 2017.\\nAlex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. Data decisions and theoretical implications when adversarially learning fair representations. arXiv preprint arXiv:1707.00075, 2017.\\nBiagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking zero-shot video classification: End-to-end training for realistic applications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\\nChristopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in \\\\( \\\\beta \\\\)-vae. arXiv preprint arXiv:1804.03599, 2018.\"}"}
{"id": "js62_xuLDDv", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of samples of different classes, the clustering quality of embedding, and the uniformity over the hypersphere embedding space, which has been linked to zero-shot generalization capability Wang & Isola (2020), as discussed in Section 2. In our experiments, we utilize recall@1 Jegou et al. (2011), normalized mutual information Manning et al. (2010) between cluster labels assigned by the well-known K-Means Lloyd (1982) algorithm and ground-truth class labels, and $U_{KL}$ to measure the closeness between samples of same class, cluster quality of the embedding (and hence, the separability of distinct classes) and uniformity, respectively. Here, we define these metrics formally, but we note that there exist multitudinous performance metrics for DML that we do not define here or use explicitly for our results, including f1 score, mean average precision (mAP), and recall@k for $k > 1$ Jegou et al. (2011).\\n\\nDefinition 14 (Recall@k). Jegou et al. (2011) Given $k \\\\in \\\\{1, \\\\ldots, |X|\\\\}$, denote $NN_k$ as defined in Definition 1. Then, Recall@k is measured as:\\n\\n$$\\\\text{Recall@}_k = \\\\frac{1}{|X|} \\\\sum_{x \\\\in X} \\\\left( \\\\exists \\\\tilde{x} \\\\in NN_k(x) : Y(\\\\tilde{x}) = Y(x) \\\\right)$$\\n\\nDefinition 15 (Normalized Mutual Information Score on Clusters). Manning et al. (2010) Let $C$, a clustering algorithm, such as K-Means Lloyd (1982) with the number of clusters set to $|Y|$, such that $C(x)$ indicates the cluster label for data point $x \\\\in X$. The normalized mutual information score between the target labels $Y$ and the cluster labels $C$ is measured as:\\n\\n$$\\\\text{NMI} = 2 \\\\cdot \\\\frac{I(Y(X); C(X))}{H(Y(X)) + H(C(X))}$$\\n\\nwhere for random variables $X, Y$, $I(\\\\cdot, \\\\cdot)$ denotes the mutual information function:\\n\\n$$I(X; Y) = H(Y) - H(Y|X)$$\\n\\nand $H(\\\\cdot)$ denotes the entropy function:\\n\\n$$H(X) = -\\\\sum_{x \\\\in X} \\\\Pr(x) \\\\log(\\\\Pr(x))$$\\n\\nThe performance metric $U_{KL}$, used to measure feature uniformity for our empirical evaluations, is defined in Section 3.1.\\n\\nA.2 Classification Fairness Definitions\\n\\nFairness definitions and criteria in classification are briefly mentioned in Section 2 of the main paper. Here, we provide explicit formulas for the most common fairness definitions, including demographic parity, equalized odds, and equality of opportunity Hardt et al. (2016), and provide some additional context on fairness definition evolution.\\n\\nDefinition 16 (Demographic Parity). The predictor $\\\\hat{Y}$ satisfies demographic parity with respect to attribute $A$ and class $Y$ if the predictor is independent of $A$:\\n\\n$$\\\\Pr[\\\\hat{Y} = 1 | A = a] = \\\\Pr[\\\\hat{Y} = 1 | A = b] \\\\quad \\\\forall a, b \\\\in A$$\\n\\nSpecifically, demographic parity has largely been used over the years as a simple and intuitive definition of fairness, in which a classifier is said to satisfy demographic parity if the sensitive attribute is independent of the output of the classifier. While demographic parity provides a simple fairness definition, the measure cannot capture fairness in classification tasks where the ground-truth label is inherently related to a certain attribute value Li et al. (2017).\\n\\nDefinition 17 (Equalized Odds). The predictor $\\\\hat{Y}$ satisfies demographic parity with respect to attribute $A$ and class $Y$ if the predictor is independent of $A$ conditional on $Y$:\\n\\n$$\\\\Pr[\\\\hat{Y} = 1 | A = a, Y = y] = \\\\Pr[\\\\hat{Y} = 1 | A = b, Y = y] \\\\quad \\\\forall a, b \\\\in A, \\\\forall y \\\\in \\\\{0, 1\\\\}$$\\n\\nfrom Hardt et al. (2016).\\n\\nDefinition 18 (Equality of Opportunity). The predictor $\\\\hat{Y}$ satisfies demographic parity with respect to attribute $A$ and class $Y$ if the predictor is independent of $A$ conditional on positively labelled $Y$:\\n\\n$$\\\\Pr[\\\\hat{Y} = 1 | A = a, Y = 1] = \\\\Pr[\\\\hat{Y} = 1 | A = b, Y = 1] \\\\quad \\\\forall a, b \\\\in A$$\\n\\nfrom Hardt et al. (2016).\"}"}
{"id": "js62_xuLDDv", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This led to the introduction of other fairness definitions that capture such nuances, the most well-known of which are probably equalized odds and equality of opportunity (Hardt et al., 2016). However, fairness metrics overall have been criticized due to the choice of protected attribute over which to measure, and the inability of these metrics to capture bias with respect to certain attributes which are not known at test-time. We discuss this to a limited extent in Section 7.\"}"}
{"id": "js62_xuLDDv", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Class distribution in CelebA. Histograms visualizing the distribution over number of samples per class in the train (left) and test (right) datasets in CelebA.\\n\\nTable 4: Summary statistics for CelebA Fitzpatrick Skintone. The percentage of the dataset constituted by each Fitzpatrick Skintone in CelebA, in the train dataset and test dataset, respectively.\\n\\nFigure 7: Class distribution in LFW. Histograms visualizing the distribution over logarithm of number of samples per class in the train (left) and test (right) datasets in LFW.\\n\\nTable 5: Summary statistics for LFW Race. The percentage of the dataset constituted by each Race in LFW, in the train dataset and test dataset, respectively.\\n\\ntested as a downstream classifier but showed poor performance. The impact of varying imbalance in the manually class imbalanced CARS196 experiments with all tested downstream classifiers is displayed in Table 8. Additional results for benchmarking of further fairness improvement methods in downstream classification of \u201cimbalanced\u201d embeddings (aside from naive use of balanced datasets) are shown in Table 8.\"}"}
{"id": "js62_xuLDDv", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Additional results for all loss and batch mining strategies for the manually class imbalanced experi-\\nment study on CARS196.\\n\\nTable 7:\\n\\n| Subgroup Gap | Multisimilarity Proxy-NCA Triplet | Contrastive | Overall |\\n|--------------|----------------------------------|-------------|---------|\\n| Semi-hard | Balanced          | Imbalanced  | Balanced | Imbalanced | Balanced |\\n| Distance Margin |                     |             |         | 349 \u00b1362 | 335 \u00b1347 |\\n| SVMAccuracy |                    |             |         | 889 \u00b1876 | 859 \u00b1874 |\\n| LR Accuracy |                    |             |         | 151 \u00b1129 | 116 \u00b1115 |\\n| RF Accuracy |                    |             |         | 358 \u00b1353 | 343 \u00b1402 |\\n\\nPublished as a conference paper at ICLR 2022\"}"}
{"id": "js62_xuLDDv", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The selected sample patches are converted to CIELab-space to retrieve the $L$ (luminance) and $b$ (yellow) values. We then calculate the Mean Individual Typology Angle (ITA) value:\\n\\n$$\\n\\\\text{ITA} = \\\\arctan\\\\left(\\\\frac{L-50}{b}\\\\right) \\\\times \\\\frac{180}{\\\\pi}\\n$$\\n\\nTable 23: Fitzpatrick Skin Tone Categories corresponding to Mean ITA values, information taken from Cheng et al. (2021)\\n\\n| ITA Range | Fitzpatrick Category | Description |\\n|-----------|----------------------|-------------|\\n| $50 \\\\leq \\\\text{ITA} < 60$ | I | Extremely Light |\\n| $40 \\\\leq \\\\text{ITA} < 50$ | II | Very Light |\\n| $30 \\\\leq \\\\text{ITA} < 40$ | III | Light / Somewhat Light |\\n| $20 \\\\leq \\\\text{ITA} < 30$ | IV | Dark / Somewhat Dark |\\n| $10 \\\\leq \\\\text{ITA} < 20$ | V | Very Dark |\\n| ITA $< 10$ | VI | Extremely Dark |\\n\\nBased on the Mean ITA calculation, we classify each image into one of the 6 Fitzpatrick skintone categories, as listed in Table 23. To calculate subgroup gaps, we calculate gaps between the mean value over the 3 lightest Fitzpatrick skintones and the mean value over the 3 darkest Fitzpatrick skintones.\\n\\nD.3 TRAINING PARAMETERS\\n\\nFor CUB200 and CARS196, we did not perform hyperparameter search but followed reported hyperparameters from Roth et al. (2020c) for best performance with an ImageNet Deng et al. (2009) pretrained ResNet50 He et al. (2016) and frozen batch normalization layers. As detailed in Roth et al. (2020c), we train for 150 epochs with embedding dimension 128, learning rate 0.00001 with no scheduler, and weight decay 0.0004. We train with a batch size of 128, with the Adam optimizer Kingma & Ba (2015) over five seeds inclusive for the balance control datasets, and for CUB200 color experiments; and seeds 0\u22129 for the manually class imbalanced experiments. For training transforms, we normalize each image using color channel means (0.485, 0.456, 0.406) and standard deviations (0.229, 0.224, 0.225), randomly crop the image and re-size to 224\u00d7224 and horizontally flip with probability 0.5. For testing transforms, we normalize each image with the aforementioned color channel means and standard deviations, resize to 256\u00d7256, and center crop to 224\u00d7224.\\n\\nFor CelebA and LFW, we performed hyperparameter search over the following hyperparameters: architectures: ResNet50 He et al. (2016), and SE-Net50 (both with and without frozen batch normalization layers); number of training epochs; learning rates; last linear layer learning rate (differ from other layer learning rates); learning rate schedulers; embedding dimensions: 64, 128, 256; pre-training; image augmentations. We evaluated hyperparameter sets on a validation set we cut from the typical training set (20% of training set), and chose the set of hyperparameters with best recall@k score for CelebA and best NMI score for LFW. NMI is used for LFW due to the high number of singleton classes present in the dataset (recall@1 is meaningless for singleton classes).\\n\\nFor CelebA, we train on the ResNet50 He et al. (2016) architecture with frozen batch normalization layers, for 125 epochs with learning rate 0.00001, and no scheduler, weight decay 0.0004, Adam Kingma & Ba (2015) optimizer, and batch size of 128. For training transforms, we normalize each image using color channel means (0.5, 0.5, 0.5) and standard deviations (0.5, 0.5, 0.5), resize to 256\u00d7256, center crop to 224\u00d7224 and horizontally flip with probability 0.5. For testing transforms, we normalize each image with the aforementioned color channel means and standard deviations, resize to 256\u00d7256, and center crop to 224\u00d7224. We average over runs with seeds 0\u22122, inclusive.\\n\\nFor LFW, we train on the ResNet50 He et al. (2016) architecture with frozen batch normalization layers, for 125 epochs with initial learning rate 0.00001 for all model parameters except the last linear layer, which has initial learning rate 0.0001, and a multi-step learning rate scheduler which reduces the learning rate by a factor of 0.3 at epochs 50 and 100, weight decay 0.0004, Adam Kingma & Ba optimizer, and batch size of 128.\"}"}
{"id": "js62_xuLDDv", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2022\\n\\nFor training transforms, we normalize each image using color channel means \\\\((0.5, 0.5, 0.5)\\\\) and standard deviations \\\\((0.5, 0.5, 0.5)\\\\), resize to \\\\(256 \\\\times 256\\\\), center crop to \\\\(224 \\\\times 224\\\\) and horizontally flip with probability 0.5. For testing transforms, we normalize each image with the aforementioned color channel means and standard deviations, resize to \\\\(256 \\\\times 256\\\\), and center crop to \\\\(224 \\\\times 224\\\\). We average over runs with seeds 0\u22122, inclusive.\\n\\nFor each dataset we choose a set of loss and batch mining strategies that have historically been used for the relevant task, encompassing a broad range of methods, and/or achieved good performance. However, for n-pair loss and sampling, good performance was not achieved for the facial datasets despite use in the past for facial recognition Sohn (2016). For manually class imbalanced experiments with CARS196 and CUB200 and the associated balanced controls, we used: margin loss / distance-weighted sampling, margin loss / semi-hard sampling, triplet loss / distance-weighted sampling, triplet loss / semi-hard sampling, contrastive loss / distance-weighted sampling, multisimilarity loss, and proxy-NCA loss. For the color experiments with CUB200, we used: margin loss / distance-weighted sampling. For CelebA and LFW, we used: margin loss / distance-weighted sampling, arcface loss, and n-pair loss and sampling. For all testing and evaluation experiments with PARADE, we used margin loss and distance-weighted sampling, but PARADE can be used with any loss and mining strategy.\\n\\nDML-specific parameters\\n\\nHere we list the hyperparameters that we use for each evaluated loss function and batch mining strategy, if applicable. Refer to A for explicit formulas associated with the parameters here. We set \\\\(\\\\gamma = 0.2\\\\) in semi-hard mining. For distance-weighted mining, we set \\\\(\\\\lambda = 0.5\\\\) and clip the maximum distance to \\\\(1.4\\\\). In the triplet objective, we use \\\\(\\\\gamma = 0.2\\\\) for triplet loss. For margin loss, the learning rate of the boundary \\\\(\\\\beta\\\\) is set to \\\\(0.0005\\\\), with initial value \\\\(1.2\\\\) and triplet margin \\\\(\\\\gamma = 0.2\\\\). For N-Pair uses embedding regularization parameter \\\\(\\\\nu = 0.005\\\\). In Multisimilarity loss, we use \\\\(\\\\alpha = 2, \\\\beta = 40, \\\\lambda = 0.5\\\\) and \\\\(\\\\epsilon = 0.1\\\\). Finally, for ArcFace, additive angular margin penalty is set to \\\\(\\\\gamma = 0.5\\\\), while scaling parameter \\\\(s = 16\\\\) and class centers are optimized with learning rate \\\\(0.0005\\\\). The two PARADE parameters, \\\\(\\\\alpha\\\\) and \\\\(\\\\rho\\\\), as described in Section 4, were optimized via worst-group performance over a grid search. For CUB200, we set \\\\(\\\\alpha = 0.3, \\\\rho = 1500\\\\). For CelebA, we set \\\\(\\\\alpha = 0.1, \\\\rho = 1000\\\\). For LFW, we set \\\\(\\\\alpha = 0.3, \\\\rho = 100\\\\).\\n\\nD.4 Fairness Evaluation\\n\\nFor each dataset, we calculate subgroup gaps between the majoritized and minoritized subgroup (CARS196, CUB200 class, CelebA) or between the worst-performing subgroup and other subgroups (LFW). In CUB200 color experiments, due to the large number of subgroups, we calculate the gap between the top 6 performing subgroups and the bottom 6 performing subgroups (there are 12 total subgroups).\\n\\nUpstream\\n\\nIn the upstream embedding tasks, in which we denote \\\\(\\\\phi\\\\) as the embedding function for the learned model, and use \\\\(A(x)\\\\) to denote the value of the attribute \\\\(A\\\\) for data point \\\\(x\\\\), we calculate recall@1 for data samples in \\\\(X\\\\) with associated class label \\\\(Y\\\\) and attribute \\\\(a \\\\in A\\\\) as:\\n\\n\\\\[\\n\\\\text{Recall@k} = \\\\frac{1}{|\\\\{x \\\\in X: A(x) = a\\\\}|} \\\\sum_{x \\\\in X} \\\\begin{cases} 1 & \\\\exists \\\\bar{x} \\\\in \\\\text{NN}_k(x): Y(\\\\bar{x}) = Y(x) \\\\\\\\ 0 & \\\\text{else} \\\\end{cases}\\n\\\\]\\n\\nNote here that the nearest neighbors function is computed with respect to all \\\\(x \\\\in X\\\\), not exclusively \\\\(x \\\\in X\\\\) with attribute \\\\(a \\\\in A\\\\), but the input to the nearest neighbors function is exclusively \\\\(\\\\{x \\\\in X: A(x) = a\\\\}\\\\). To calculate NMI, let \\\\(C\\\\) be the output of a clustering algorithm \\\\(C\\\\) on the entire dataset \\\\(X\\\\), i.e. \\\\(C = C(X)\\\\) and let \\\\(C|_S\\\\) denote the output of clustering algorithm \\\\(C\\\\) restricted to some subset \\\\(S \\\\subset X\\\\). The important note here is that the clustering algorithm is run over the entire dataset, but \\\\(C|_S\\\\) expresses the cluster labels only for \\\\(S \\\\subset X\\\\). Then, we measure NMI for data samples in \\\\(X\\\\) with associated class label \\\\(Y\\\\) and attribute \\\\(a \\\\in A\\\\) as:\\n\\n\\\\[\\n\\\\text{NMI} = \\\\frac{\\\\sum_{\\\\bar{a}, \\\\bar{b} \\\\in A} \\\\sum_{x \\\\in \\\\text{NN}_k(x)} \\\\begin{cases} 1 & Y(x) = \\\\bar{a} \\\\text{ and } Y(\\\\bar{x}) = \\\\bar{b} \\\\\\\\ 0 & \\\\text{else} \\\\end{cases}}{\\\\sum_{\\\\bar{a} \\\\in A} \\\\sum_{x \\\\in \\\\text{NN}_k(x)} Y(x) = \\\\bar{a}}\\n\\\\]\\n\\nWe calculate\\\\(U_{KL}\\\\) for data samples in \\\\(X\\\\) with attribute \\\\(a \\\\in A\\\\) as:\\n\\n\\\\[\\nU_{KL}(X) = D_{KL}(U_{X|A=a} || U_{X})\\n\\\\]\\n\\nNote: \\\\(D_{KL}\\\\) denotes the Kullback-Leibler divergence.\"}"}
{"id": "js62_xuLDDv", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $S_{\\\\phi}(\\\\{x \\\\in X: A(x) = a\\\\})$ denotes the singular values over $\\\\phi(\\\\{x \\\\in X: A(x) = a\\\\})$.\\n\\nDownstream\\n\\nIn the downstream tasks, for data samples in $X$ with class label $Y$, and predictor $\\\\hat{Y}$, let $Y(x)$ express the value of the ground-truth label for data sample $x$ and let $\\\\hat{Y}(x)$ express the value of the predicted label. Then, we denote $TP(y)$ the number of true positives with attribute $a \\\\in A$:\\n\\n$TP(y) = \\\\{x \\\\in X: A(x) = a, Y(x) = y, \\\\hat{Y}(x) = y\\\\}$\\n\\nand $FP(y) a$ the number of false positives with attribute $a \\\\in A$:\\n\\n$FP(y) = \\\\{x \\\\in X: A(x) = a, Y(x) = y, \\\\hat{Y}(x) \\\\neq y\\\\}$\\n\\nand $FN(y) a$ the number of false negatives with attribute $a \\\\in A$:\\n\\n$FN(y) = \\\\{x \\\\in X: A(x) = a, Y(x) \\\\neq y, \\\\hat{Y}(x) = y\\\\}$\\n\\nfor $y \\\\in Y$.\\n\\nWe calculate macro-averaged recall for data samples in $X$ with associated class label $Y$ and attribute $a \\\\in A$ as:\\n\\n$\\\\text{Recall} = \\\\frac{TP(y) a}{|Y| \\\\times X y \\\\in Y TP(y) a + FN(y) a}$\\n\\nwhere $|Y|$ is the number of possible class labels, i.e. the size of the set of all possible values of $Y$. We calculate macro-averaged precision for data samples in $X$ with associated class label $Y$ and attribute $a \\\\in A$ as:\\n\\n$\\\\text{Precision} = \\\\frac{TP(y) a}{|Y| \\\\times X y \\\\in Y TP(y) a + FP(y) a}$\\n\\nWe calculate accuracy for data samples in $X$ with associated class label $Y$ and attribute $a \\\\in A$ as:\\n\\n$\\\\text{Accuracy} = \\\\frac{|\\\\{x \\\\in X: A(x) = a, Y(x) = \\\\hat{Y}(x)\\\\}|}{|\\\\{x \\\\in X: A(x) = a\\\\}|}$\\n\\nThe subgroup gaps are then considered to be the difference between the metric value for the majoritized subgroup and the metric value for the minoritized subgroup (CARS196, CUB200, CelebA); or between the metric value for each subgroup with better performance than the worst-performing subgroup and the metric value for the worst-performing subgroup (LFW). As stated in Section D.4, for CUB200 bird color experiments, the subgroup gaps were calculated between the top performing 50% of subgroups and bottom performing 50% of subgroups.\"}"}
{"id": "js62_xuLDDv", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Gap study on CelebA excluding Fitzpatrick Skintone VI.\\n\\n### Table 16: Gap study on CelebA.\\n\\n| Skintone 1 | Skintone 2 | Skintone 3 |\\n|------------|------------|------------|\\n| Overall    | Overall    | Overall    |\\n| Accuracy   | NMI@1      | Margin (D) |\\n| 0          | \u00b10.05      | \u00b10.12      |\\n| 0          | \u00b10.03      | \u00b10.07      |\\n| \u00b10.02      | \u00b10.01      | \u00b10.03      |\\n\\nOverall, the average gaps in representation space and downstream classification were explained in Section 3.3.\\n\\nAverage gaps in representation space and downstream classification for standard methods and PARADE in CelebA.\\n\\n### Table 17: Absolute performance for all CelebA subgroups.\\n\\n| Skintone   | Recall@1 | Precision | NMI@1 | Margin (D) |\\n|------------|----------|-----------|-------|------------|\\n| Fitzpatrick Skintone VI | 0.885\u00b10.003 | 0.907\u00b10.001 | 0.940\u00b10.002 | 0.018\u00b10.000 |\\n| Fitzpatrick Skintone V   | 0.929\u00b10.002 | 0.954\u00b10.002 | 0.950\u00b10.003 | 0.039\u00b10.003 |\\n| Fitzpatrick Skintone IV  | 0.946\u00b10.001 | 0.960\u00b10.001 | 0.958\u00b10.002 | 0.054\u00b10.004 |\\n\\nFor example, here we use Recall@1 to determine the optimal choice of hyperparameters and validate subgroup in the CelebA test dataset respectively, in representation space and downstream classification (logistic regressor) over logistic regressor.\"}"}
{"id": "js62_xuLDDv", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For CUB200 bird color experiments, we utilized the labeled bird color attributes from Wah et al.\\n\\nTable 18: Overall results on LFW.\\n\\nTable 19: Gap study on LFW.\\n\\nTable 20: Absolute performance for all LFW subgroups.\\n\\nEach image can have multiple \\\"primary color\\\" labels. Therefore, we take the mode over 3 seeds for standard methods and PARADE in LFW.\\n\\nDue to the number of singleton classes in LFW, Recall@1 is not considered a good metric of downstream classification (random forest) over LFW test dataset respectively, in representation space and downstream classification (random forest) over PARADE in LFW.\\n\\nFigure 10: A t-SNE (Maaten & Hinton, 2008) visualization of the two distinct PARADE embeddings for Fitzpatrick Skintone CelebA experiments: the sensitive attribute embedding (for Fitzpatrick Skintone CelebA experiments: the sensitive attribute embedding (right) and the class embedding (left)).\"}"}
{"id": "js62_xuLDDv", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 21: Benchmarking additional fairness improvement methods in downstream classification on LFW.\\n\\nOverall performance and subgroup gaps for Domain-Independent Training and Oversampling (Wang et al., 2020b) on LFW with Race attribute.\\n\\n(a) Domain-Independent Training\\n\\n| Metric     | ArcFace Margin (D) | N-Pair |\\n|------------|--------------------|--------|\\n| Accuracy   | 0.759 \u00b1 0.017      | 0.753 \u00b1 0.002 |\\n| Precision  | 0.793 \u00b1 0.010      | 0.786 \u00b1 0.003 |\\n| Recall     | 0.799 \u00b1 0.012      | 0.792 \u00b1 0.003 |\\n\\nGap\\n\\n| Metric     | ArcFace Margin (D) | N-Pair |\\n|------------|--------------------|--------|\\n| Accuracy   | 0.093 \u00b1 0.011      | 0.095 \u00b1 0.006 |\\n| Precision  | 0.030 \u00b1 0.011      | 0.020 \u00b1 0.009 |\\n| Recall     | 0.040 \u00b1 0.012      | 0.044 \u00b1 0.008 |\\n\\n(b) Oversampling\\n\\n| Metric     | ArcFace Margin (D) | N-Pair |\\n|------------|--------------------|--------|\\n| Accuracy   | 0.775 \u00b1 0.017      | 0.767 \u00b1 0.004 |\\n| Precision  | 0.771 \u00b1 0.014      | 0.762 \u00b1 0.002 |\\n| Recall     | 0.815 \u00b1 0.014      | 0.807 \u00b1 0.002 |\\n\\nGap\\n\\n| Metric     | ArcFace Margin (D) | N-Pair |\\n|------------|--------------------|--------|\\n| Accuracy   | 0.070 \u00b1 0.012      | 0.072 \u00b1 0.006 |\\n| Precision  | 0.020 \u00b1 0.013      | 0.024 \u00b1 0.009 |\\n| Recall     | 0.023 \u00b1 0.013      | 0.031 \u00b1 0.011 |\\n\\nFigure 11: A t-SNE (Maaten & Hinton, 2008) visualization of the two distinct PARADE embeddings for Race LFW experiments: the sensitive attribute embedding (left) and the class label embedding (right).\\n\\n| Dataset   | Protected Attribute | Values                                      |\\n|-----------|---------------------|---------------------------------------------|\\n| CUB200-2011 | Color               | Black, Blue, Brown, Buff, Green, Grey, Iridescent, Olive, Orange, Red, White, Yellow |\\n| CelebA    | Fitzpatrick Skintone | Category I, II, III, IV, V, VI              |\\n| LFW       | Race                | Asian, Black, Indian, White                 |\\n\\nTable 22: Summarizing attribute information. Protected attribute examined and associated values taken by the protected attribute in each dataset analyzed w.r.t. a sensitive attribute in the main paper (CUB200, CelebA, LFW).\\n\\nFor CUB200-2011, all bird colors labeled for each image in order to determine a single bird color associated with the image. For CelebA, we calculate the Fitzpatrick skintone based on the image pixel information for each image. The calculation is described in Section D.2. For LFW, we construct the \u201cRace\u201d attribute from labels of \u201cWhite\u201d, \u201cBlack\u201d, \u201cAsian,\u201d and \u201cIndian\u201d as labelled by Kumar et al. (2009). For each of these attributes, the labelling provided by Kumar et al. (2009) has a float value, which we map to binary values: the image is considered to have the attribute if the value is greater than 0, and the image is considered to not have the attribute if the value is less than 0. Naturally, the labelling is not necessarily correct for each image, as the confidence about the \u201cRace\u201d labelling can be quite low for some images. We remove all images without at least one of these attributes, though we note that these attributes do not encompass all races. Therefore, our analysis may not be relevant for other races not labelled by Kumar et al. (2009).\"}"}
{"id": "js62_xuLDDv", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12: Exploring fairness-utility tradeoffs in PARADE on CUB200 over grid of $\\\\alpha$ and $\\\\rho$ values. Overall performance (left column), subgroup gap (middle column) and worst-group performance (right column) over metrics Recall@1 (top row), NMI (middle row), and $U_{KL}$ (bottom row) in PARADE on CUB200. $\\\\alpha_{SA}$ and $\\\\rho$ in PARADE objective (Section 4) varied from 0.1 to 0.9, and 1 to 3000, respectively.\\n\\nD.2 FITZPATRICK SKIN TONE\\n\\nWe follow the methods from Cheng et al. (2021) for calculation of Fitzpatrick Skintone based on image pixel information. However, we calculate these values for CelebA, as opposed to CelebA-HQ. As CelebA-HQ incorporates higher resolution images, but has fewer images, our process of Fitzpatrick Skintone calculation on CelebA is slightly modified to account for lower resolution, and differing image size.\\n\\nIn Cheng et al. (2021), two sample skin patches are selected from each image of CelebA-HQ to determine the skintone. We select three sample skin patches, as we are forced to reduce the dimensions of the patches to account for the smaller image size of CelebA. Additionally, we leverage facial landmark attributes provided by CelebA Liu et al. (2015) in order to choose our sample patches. Specifically, given the $(x, y)$ landmarks for the left eye, right eye, and nose for each image, we choose to sample square patches of size $20 \\\\times 20$ (all 3 color channels are selected) with the following center points:\\n\\n- $(x_{\\\\text{left eye}}, y_{\\\\text{nose}})$\\n- $(x_{\\\\text{right eye}}, y_{\\\\text{nose}})$\\n- $(x_{\\\\text{nose}}, y_{\\\\text{nose}})$\\n\\nThe first two center points are intended to capture the likely location of the left and right cheeks, respectively, as these are likely located below each eye and adjacent to the nose. The last center point is the nose. We note that this protected attribute generation is not perfect. In some cases, such label generation can accidentally use aspects of the background, if the individual\u2019s face position in the image is not facing forward. Also, extreme lighting can lead to misclassification of skintone. Nonetheless, we believe the procedure provides a good approximation of Fitzpatrick skintone category, but do not recommend these attribute labels for use outside of fairness analysis.\"}"}
{"id": "js62_xuLDDv", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1:\\na) Visualization of the standard DML pipelines and the aspects of intra-class alignment and uniformity in the embedding space.\\nb) Infographic of the fairness issue in DML, where learned representational bias can even transfer to downstream models building on previously learned representations.\\nc) Layout of our proposed PARADE approach to better incorporate sensitive attribute context and improve representational fairness.\\n\\nImmediate applications are commonly found in zero-shot clustering or image retrieval, respectively. Retrieval and clustering metrics are predominantly utilized for evaluation. Recall@k (Jegou et al., 2011) or mean average precision measured on recall (Roth et al., 2020c; Musgrave et al., 2020) typically estimate retrieval performance. Normalized mutual information (NMI) on clustered embeddings (Manning et al., 2010) is used as a proxy for clustering quality (see Supplemental for detailed definitions). We leverage these performance metrics to inform the DML and our experiments.\\n\\nFairness in Classification\\n\\nFormalizing fairness in ML continues to be an open problem (Mehrabi et al., 2019; Chen et al., 2018a; Chouldechova, 2017; Berk, 2017; Locatello et al., 2019b; Chouldechova & Roth, 2018; Dwork et al., 2012; Hardt et al., 2016; Zafar et al., 2017). In classification, definitions for fairness such as demographic parity, equalized odds, and equality of opportunity, rely on model outputs across the random variables of protected attribute and ground-truth label (Dwork et al., 2012; Hardt et al., 2016).\\n\\nFairness in Representations\\n\\nA more relevant family of fairness definitions for DML would be those explored in fairness for general representation learning (Edwards & Storkey, 2015; Beutel et al., 2017; Louizos et al., 2015; Madras et al., 2018). Here, the goal is to learn a fair mapping from an original domain to a latent domain so that classifiers trained on these representations are more likely to be agnostic to the sensitive attribute in unknown downstream tasks. This assumption distinguishes our setting from previous fairness work in which the downstream tasks are known at train time (Madras et al., 2018; Edwards & Storkey, 2015; Moyer et al., 2018; Song et al., 2019; Jaiswal et al., 2019). DML differs from this form of representation learning as it aims to learn a mapping capturing semantic similarity, as opposed to latent space representation.\\n\\nEarlier works in fair representation learning intended to obfuscate any information about sensitive attributes to approximately satisfy demographic parity (Zemel et al., 2013) while a wealth of more recent works focus on using adversarial methods or feature disentanglement in latent spaces of VAEs (Locatello et al., 2019a; Kingma & Welling, 2013; Gretton et al., 2006; Louizos et al., 2015; Amini et al., 2019; Alemi et al., 2018; Burgess et al., 2018; Chen et al., 2018b; Kim & Mnih, 2018; Esmaeili et al., 2019; Song et al., 2019; Gitiaux & Rangwala, 2021; Rodr\u00edguez-G\u00e1lvez et al., 2020; Sarhan et al., 2020; Paul & Burlina, 2021; Chakraborty et al., 2020). In this setting, the literature has focused on optimizing on approximations of the mutual information between representations and sensitive attributes: maximum mean discrepancy (Gretton et al., 2006) for deterministic or variational (Li et al., 2014; Louizos et al., 2015) autoencoders (VAEs); cross-entropy of an adversarial network that predicts sensitive attributes from the representations (Edwards & Storkey, 2015; Xie et al., 2017; Beutel et al., 2017; Zhang et al., 2018; Madras et al., 2018; Adel et al., 2019; Zhao & ...\"}"}
{"id": "js62_xuLDDv", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"PARADE shares aspects of these previous methods in its choice of de-correlation or disentanglement. However, PARADE de-correlates the learned similarity metric as opposed to the latent space. In addition, with DML-specific criteria, PARADE learns similarities over the sensitive attribute while not directly removing all information about the sensitive attribute, as the sensitive attribute and target class embeddings share a base network.\\n\\n3 EXTENDING FAIRNESS TO DML - finDML BENCHMARK\\n\\nTo characterize fairness with finDML, this section introduces the key constituents \u2013 definitions to characterize fairness in embedding spaces and respective benchmark datasets.\\n\\n3.1 PRELIMINARIES\\n\\nOur embedding space fairness definitions rely on embedding space metrics adapted from (Wang & Isola, 2020) and (Roth et al., 2020c), namely alignment and uniformity. Both metrics we use to characterize embeddings for our definitions in the next section (intra- as well as inter-class alignment and uniformity) have been successfully linked to generalization performance in contrastive self-supervised and metric learning models (Wang & Isola, 2020; Roth et al., 2020c; Sinha et al., 2020).\\n\\nAlignment succinctly captures the similarity structure learned by the representation space with respect to the target labels through measuring distances between pairs of samples. On the other hand, notions of uniformity can differ. Uniformity of the sample distribution over the hypersphere has been studied through the radial basis function (RBF) over pairs of samples. Alternatively, uniformity of the feature space has been studied through the KL-divergence $D_{KL}$ between the discrete uniform distribution $U_D$ and the sorted singular value distribution $S_\u03d5(X)$ of the representation space $\u03d5$ on dataset $X$.\\n\\n$$U_{KL}(X) = D_{KL}(U_D, S_\u03d5(X))$$\\n\\nHere, lower scores indicate more significant directions of variance in learned representations. Both introduced notions of uniformity represent important aspects of the embedding space, but the computational overhead in computing RBF over all pairs of samples in large datasets makes it impractical for our uses and is less interpretable than $U_{KL}$. Therefore, we leave the uniformity metric utilized in finDML general, but utilize $U_{KL}$ for our experiments.\\n\\n3.2 DEFINING FAIRNESS\\n\\nBuilding on the aforementioned performance metrics, we introduce three definitions for fairness in the embedding spaces of DML models. As the recall@k and alignment metrics inform inclusion in an embedded cluster (or class), we follow fair classification literature in the motivation for our first fairness definition: inclusion in a class should be independent of a protected attribute given the ground-truth label. Thus, we examine the probability of encountering a data instance of the same class in a data point's $k$-nearest neighbors to form the first definition. The second definition relies on equal expectation of alignment across sensitive attribute values. Departing from classification literature, our third definition encapsulates fairness in a task-agnostic sense (as DML is often applied in such settings): fairness across the \u201cgoodness\u201d of the learned features via a uniformity metric.\\n\\nLet $X$ denote the input data, and $A$ a protected attribute variable. Denote $X_a$ the partition of $X$ with attribute $a \u2208 A$. To recap common DML terminology, a positive pair of samples is defined as $(x_1, x_2) \u2208 X \u00d7 X$ s.t. the class label of $x_1$ and $x_2$ are identical. A negative pair of samples is defined as $(x_1, x_2) \u2208 X \u00d7 X$ such that the class label of $x_1$ and $x_2$ differ. Let $P_a$ denote the set of all positive pairs s.t. at least one of $x_1$ or $x_2$ has attribute $a \u2208 A$, and analogously for $N_a$ and negative pairs.\\n\\n**Definition 1 (K-Close Fairness).**\\n\\nDefine $NN_k: \u03a6 \u2282 S^{D\u22121} \u2192 P(X)$ as a function that receives a point $\u03d5(x) \u2208 \u03a6$ and returns a set in the powerset of $X$, $P(X)$, containing points in $X$ that map to the $k$ nearest neighbors of $\u03d5(x)$ in $\u03a6$. Thus, $\u03d5$ is $k$-close fair with respect to attribute $A$ if:\\n\\n$$\\\\Pr_{x \u2208 X_a}(\u2203 \u02dcx \u2208 NN_k(\u03d5(x))) s.t. Y(\u02dcx) = Y(x) = \\\\Pr_{x \u2208 X_b}(\u2203 \u02dcx \u2208 NN_k(\u03d5(x))) s.t. Y(\u02dcx) = Y(x) \\\\forall a, b \u2208 A$$\\n\\nNote: the criteria weakens as $k$ increases, similar to recall@k.\"}"}
{"id": "js62_xuLDDv", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Definition 2 (Alignment). \\\\( \\\\phi \\\\) is fair, according to alignment with respect to attribute \\\\( A \\\\), if:\\n\\n\\\\[\\nE(x_1, x_2) \\\\in P_a \\\\left[ ||\\\\phi(x_1) - \\\\phi(x_2)||_2 \\\\right] = E(x_1, x_2) \\\\in P_b \\\\left[ ||\\\\phi(x_1) - \\\\phi(x_2)||_2 \\\\right] (3)\\n\\\\]\\n\\n\\\\[\\nE(x_1, x_2) \\\\in N_a \\\\left[ ||\\\\phi(x_1) - \\\\phi(x_2)||_2 \\\\right] = E(x_1, x_2) \\\\in N_b \\\\left[ ||\\\\phi(x_1) - \\\\phi(x_2)||_2 \\\\right] \\\\quad \\\\forall a, b \\\\in A (4)\\n\\\\]\\n\\ni.e. the expectation of the alignment is equal across domain of \\\\( A \\\\).\\n\\nDefinition 3 (Uniformity Across Groups). \\\\( \\\\phi \\\\) is fair, according to uniformity, and with respect to attribute \\\\( A \\\\), if the expectation of the uniformity is equal across domain of \\\\( A \\\\):\\n\\n\\\\[\\nU(\\\\phi(X_a)) = U(\\\\phi(X_b)) \\\\quad \\\\forall a, b \\\\in A (5)\\n\\\\]\\n\\nwhere \\\\( U(\\\\cdot) \\\\) denotes some measure of uniformity over a set \\\\( V \\\\in S_{D-1} \\\\).\\n\\n3.3 Constructed finDML Benchmark Datasets\\n\\nfinDML encompasses existing DML benchmark datasets, CUB200 and CARS196, and facial recognition datasets, CelebA and LFW (Wah et al., 2011; Krause et al., 2013; Liu et al., 2015; Huang et al., 2007). For fairness analysis, we investigate bird color in CUB200, Race in LFW and Skintone in CelebA (Kumar et al., 2009). A detailed description of dataset and attribute labeling is included in the Supplemental. To create additional fairness benchmarks, we induce class imbalance in CUB200 and CARS196, as both datasets are naturally balanced w.r.t. class.\\n\\nManually Introduced Class Imbalance\\n\\nWe introduce imbalance by reducing the number of training data samples of 50 randomly selected classes by 90\\\\% (Imbalanced). We run an experiment with the original datasets as a balanced control (Balanced) for comparison. In the imbalanced setting, we adjust (increase) the number of training samples of the majoritized groups to match the number of datapoints in the balanced control experiments. We average metrics over 10 sets of 50 randomly selected classes for imbalanced experiments. We use the standard ratio of 50\u221250 for train-test split of these datasets, but split over number of data points per class, as opposed to splitting over the classes themselves. The manually imbalanced datasets are used to benchmark standard DML methods, validate our framework, and analyze downstream effects.\\n\\nAlthough dataset imbalance does not constitute the sole source of bias in machine learning applications, unfairness as a result of imbalance is the most well-understood in the literature (Chen et al., 2018a). Additionally, we do not assume for our naturally imbalanced datasets, particularly the facial datasets, that attribute imbalance is the only source of bias we observe.\\n\\n4 Partial Attribute De-correlation (PARADE)\\n\\nIn this section, we present Partial Attribute De-correlation, or PARADE, in which we incorporate adversarial separation (Milbich et al., 2020) during training to de-correlate separate embeddings. We enumerate several significant changes: 1) only target embedding released at test-time; 2) triplet formation and loss term w.r.t. sensitive attribute; 3) de-correlation with sensitive attribute as opposed to de-correlation to reduce redundancy in concatenated feature space. These two representations branch off from the deep metric embedding model at the last layer. The two representations encode the similarity metrics learned over the sensitive attribute and target class, respectively. The sensitive attribute embedding layer is discarded at test time. The resulting network expresses a similarity metric with respect to the target class, de-correlated from the sensitive attribute (Figure 1). Therefore, PARADE figuratively optimizes the first two fairness definitions proposed in Section 3.2 via an objective that maximizes independence between the sensitive attribute and target class.\\n\\nObjective Term Per Embedding\\n\\nTo achieve efficient training and de-correlation of the target class and the sensitive attribute embedding layers, we simultaneously train both layers that branch from the penultimate layer of the model and de-correlate at each iteration. Because PARADE must learn one embedding w.r.t. target class (\\\\( \\\\phi_{targ} \\\\)) and one embedding w.r.t. the sensitive attribute (\\\\( \\\\phi_{SA} \\\\)), we introduce separate objectives for each embedding:\\n\\n\\\\[\\nL_{targ} = \\\\frac{1}{N} \\\\sum_{t \\\\sim T_{targ}} L(t) \\\\\\\\\\nL_{SA} = \\\\frac{1}{N} \\\\sum_{t \\\\sim T_{SA}} L(t)\\n\\\\]\\n\\nWhile bird color in CUB200 does not represent a real-world fairness setting, CUB200 is widely used as a DML benchmark. Thus, a fairness angle allows fairness analysis of previous methods benchmarked on CUB200.\"}"}
{"id": "js62_xuLDDv", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: A t-SNE (Maaten & Hinton, 2008) visualization of the two distinct PARADE embeddings for bird color CUB200 experiments: the sensitive attribute embedding (left) and the class label embedding (right). In the sensitive attribute embedding, both example images are mapped to clusters with birds of the same plumage (yellow and blue, respectively). Due to de-correlation, in the class label embedding, the images are separated from the region of space with other birds of the same plumage, but are still well-clustered, indicating that PARADE can find other attributes to distinguish these species clusters.\\n\\n\\\\[ N \\\\] is the number of training triplet samples, and \\\\( L \\\\) represents a generic loss function, such as triplet loss (Hoffer & Ailon, 2018). We use \\\\( t \\\\sim T_{targ} \\\\) to illustrate sampling over triplets of the form \\\\((x_a, x_p, x_n)\\\\) where \\\\( x_a \\\\) and \\\\( x_p \\\\) are of the same target class and \\\\( x_a \\\\) and \\\\( x_n \\\\) are of differing target classes. Similarly, \\\\( t \\\\sim T_{SA} \\\\) indicates sampling over triplets of the form \\\\((x_a, x_p, x_n)\\\\) where \\\\( x_a \\\\) and \\\\( x_p \\\\) are of the same sensitive attribute subgroup and \\\\( x_a \\\\) and \\\\( x_n \\\\) are of differing sensitive attribute subgroups.\\n\\nSee Figure 2 for a t-SNE visualization of the distinct embeddings of PARADE.\\n\\nPartial De-correlation\\n\\nIn order to minimize the correlation between \\\\( \\\\phi_{targ} \\\\) and \\\\( \\\\phi_{SA} \\\\), we use the adversarial separation (de-correlation) method from (Milbich et al., 2020), which minimizes the mutual information between a pair of embeddings. The task of mutual information minimization is accomplished through learning an MLP to maximize the pair's correlation, \\\\( c \\\\), and consequently performing a gradient reversal \\\\( R \\\\), which inverts the gradients during backpropagation. The MLP, \\\\( \\\\xi \\\\), is trained to maximize\\n\\n\\\\[\\n\\\\| R(\\\\phi_{targ}) \\\\|_2^2 - \\\\rho \\\\cdot c(\\\\phi_{targ}, \\\\phi_{SA})\\n\\\\]\\n\\nwhere \\\\( \\\\alpha_{SA} \\\\) weights the sensitive attribute loss and \\\\( \\\\rho \\\\) weights the degree of de-correlation. \\\\( \\\\rho \\\\) modulates the de-correlation term to allow \\\\( \\\\psi \\\\) to retain some attribute information (i.e. partial de-correlation). Thus, the deployed model \\\\( \\\\phi_{targ} = f_{targ} \\\\circ \\\\psi \\\\) can retain information about the sensitive attribute in its feature representations, as \\\\( \\\\alpha_{SA} L_{SA} \\\\) appears in the loss function back-propagated through the full model \\\\( \\\\psi \\\\). The extent to which the sensitive attribute affects the output features is controlled by \\\\( \\\\alpha_{SA} \\\\); we suggest optimizing \\\\( \\\\alpha_{SA} \\\\in (0, 1) \\\\) and \\\\( \\\\rho \\\\) through maximization of worst-group performance (Lahoti et al., 2020) (See Supplemental C.5 for further analysis of PARADE hyperparameters).\\n\\n5 EXPERIMENTS\\n\\nBaseline DML Methods\\n\\nFor all datasets, we use a ResNet-50 (He et al., 2016) architecture with best performing parameters on a validation set (for further implementation details, see Supplemental). To investigate a sweeping set of frequently used DML methods, we benchmark across a diverse, representative set of 11 techniques, including: three standard ranking-based losses (margin, triplet, n-pair, and contrastive) three batch mining strategies (random, semi-hard and distance-weighted sampling) and three common loss functions (multisimilarity loss, ArcFace loss for handling facial datasets, and proxy-based loss, ProxyNCA) (Hoffer & Ailon, 2018; Hadsell et al., 2006; Wu et al., 2018; Sohn, 2016; Hadsell et al., 2006; Kim et al., 2020; Wang et al., 2020a; Deng et al., 2019; Wu et al., 2018; Schroff et al., 2015). See Supplementary for more details.\"}"}
{"id": "js62_xuLDDv", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 8: Benchmarking additional fairness improvement methods in downstream classification on CARS196 (Classes).\\n\\nOverall performance and subgroup gaps for Domain-Independent Training and Oversampling (Wang et al., 2020b) on CARS196 in class imbalanced experiments with upstream embedding trained on imbalanced dataset.\\n\\n(a) Domain-Independent Training\\n\\n| Metric | Contr. (D) | Margin (D) | Margin (Sem.) | Msim. | ProxyNCA | Triplet (D) | Triplet (S) |\\n|--------|------------|------------|---------------|-------|----------|------------|------------|\\n| ACCURACY | 0.812 \u00b1 0.011 | 0.834 \u00b1 0.009 | 0.820 \u00b1 0.008 | 0.842 \u00b1 0.008 | 0.869 \u00b1 0.005 | 0.836 \u00b1 0.007 | 0.742 \u00b1 0.007 |\\n| PRECISION | 0.834 \u00b1 0.009 | 0.861 \u00b1 0.004 | 0.847 \u00b1 0.004 | 0.872 \u00b1 0.004 | 0.878 \u00b1 0.005 | 0.865 \u00b1 0.009 | 0.804 \u00b1 0.008 |\\n| RECALL | 0.811 \u00b1 0.011 | 0.833 \u00b1 0.009 | 0.818 \u00b1 0.008 | 0.840 \u00b1 0.008 | 0.869 \u00b1 0.005 | 0.834 \u00b1 0.008 | 0.740 \u00b1 0.008 |\\n\\nGap\\n\\n| ACCURACY | 0.001 \u00b1 0.027 | 0.010 \u00b1 0.018 | 0.017 \u00b1 0.021 | 0.018 \u00b1 0.018 | 0.120 \u00b1 0.018 | 0.022 \u00b1 0.017 | 0.094 \u00b1 0.021 |\\n| PRECISION | 0.304 \u00b1 0.021 | 0.275 \u00b1 0.021 | 0.313 \u00b1 0.019 | 0.247 \u00b1 0.027 | 0.398 \u00b1 0.015 | 0.236 \u00b1 0.022 | 0.289 \u00b1 0.016 |\\n| RECALL | 0.260 \u00b1 0.024 | 0.218 \u00b1 0.022 | 0.258 \u00b1 0.018 | 0.182 \u00b1 0.027 | 0.398 \u00b1 0.018 | 0.175 \u00b1 0.022 | 0.177 \u00b1 0.016 |\\n\\n(b) Oversampling\\n\\n| Metric | Contr. (D) | Margin (D) | Margin (Sem.) | Msim. | ProxyNCA | Triplet (D) | Triplet (S) |\\n|--------|------------|------------|---------------|-------|----------|------------|------------|\\n| ACCURACY | 0.851 \u00b1 0.007 | 0.862 \u00b1 0.004 | 0.853 \u00b1 0.006 | 0.875 \u00b1 0.004 | 0.878 \u00b1 0.004 | 0.875 \u00b1 0.005 | 0.820 \u00b1 0.005 |\\n| PRECISION | 0.854 \u00b1 0.006 | 0.864 \u00b1 0.004 | 0.855 \u00b1 0.006 | 0.877 \u00b1 0.004 | 0.880 \u00b1 0.005 | 0.876 \u00b1 0.004 | 0.822 \u00b1 0.005 |\\n| RECALL | 0.853 \u00b1 0.006 | 0.862 \u00b1 0.004 | 0.853 \u00b1 0.006 | 0.875 \u00b1 0.004 | 0.878 \u00b1 0.004 | 0.875 \u00b1 0.004 | 0.821 \u00b1 0.005 |\\n\\nGap\\n\\n| ACCURACY | 0.128 \u00b1 0.023 | 0.099 \u00b1 0.014 | 0.102 \u00b1 0.020 | 0.097 \u00b1 0.019 | 0.136 \u00b1 0.017 | 0.108 \u00b1 0.018 | 0.109 \u00b1 0.019 |\\n| PRECISION | 0.398 \u00b1 0.012 | 0.386 \u00b1 0.017 | 0.391 \u00b1 0.014 | 0.383 \u00b1 0.015 | 0.422 \u00b1 0.014 | 0.387 \u00b1 0.013 | 0.387 \u00b1 0.011 |\\n| RECALL | 0.423 \u00b1 0.015 | 0.403 \u00b1 0.017 | 0.414 \u00b1 0.014 | 0.396 \u00b1 0.019 | 0.436 \u00b1 0.017 | 0.406 \u00b1 0.015 | 0.413 \u00b1 0.012 |\\n\\nFigure 8: Impact of varying imbalance between the minoritized and majoritized classes on various downstream classifiers (RF, LR and SVM) in the manually class imbalanced CARS196 experiments. (Note: the imbalance percentage 50\u201350 is equivalent to the balanced setting). Gaps increase for all classifiers downstream with more imbalance introduced to the upstream training data.\\n\\nPer-subgroup and overall results for CUB200 color experiments with standard margin-distance and PARADE are displayed in Table 13. Benchmarking of fairness improvement methods in downstream classification for bird color are shown in Table 12. Additional results for all loss and batch mining strategies for the CelebA dataset are located in Tables 14 and 15. Additional PARADE results for subgroup gaps excluding Fitzpatrick Skintone VI (as mentioned in Section 6.3) are located in Table 16.\"}"}
{"id": "js62_xuLDDv", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 18: LR, RF, and SVM accuracy over 10 seeds in manually class imbalanced experiments (Imbalanced) and control experiments (Balanced) for CUB200.\\n\\n| Subgroup | LR Accuracy | RF Accuracy | SVM Accuracy |\\n|----------|-------------|-------------|-------------|\\n| White    |             |             |             |\\n| Black    |             |             |             |\\n| Other    |             |             |             |\\n\\nTable 20: Benchmarking of fairness improvement for LFW to demonstrate worst-group performance for the \\\"White\\\" subgroup.\\n\\n| Subgroup | Precision | Recall | NMI |\\n|----------|-----------|--------|-----|\\n| White    |           |        |     |\\n| Black    |           |        |     |\\n| Other    |           |        |     |\"}"}
{"id": "js62_xuLDDv", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 11: Benchmarking additional fairness improvement methods in downstream classification on CUB200 (Classes).\\n\\n| Method | **Overall Performance** | **Subgroup Gap** |\\n|--------|------------------------|-----------------|\\n|        | ACCURACY                | Overall         |\\n|        |                         | **Marginal (D)**|\\n|        |                         | **Margin (Sem.)**|\\n|        |                         | **MSim.**       |\\n|        |                         | **ProxyNCA**    |\\n|        |                         | **Triplet (D)** |\\n|        |                         | **Triplet (S)** |\\n| **A**  | 0.782 \u00b1 0.008          | 0.034 \u00b1 0.017   |\\n| **B**  | 0.809 \u00b1 0.004          | 0.003 \u00b1 0.007   |\\n| **C**  | 0.794 \u00b1 0.005          | 0.014 \u00b1 0.008   |\\n| **D**  | 0.805 \u00b1 0.004          | 0.024 \u00b1 0.009   |\\n| **E**  | 0.798 \u00b1 0.005          | 0.024 \u00b1 0.008   |\\n| **F**  | 0.749 \u00b1 0.006          | 0.140 \u00b1 0.030   |\\n\\n## Table 12: Benchmarking additional fairness improvement methods in downstream classification on CUB200 (Color).\\n\\n| Method | **Overall Performance** | **Subgroup Gap** |\\n|--------|------------------------|-----------------|\\n|        | ACCURACY                | Overall         |\\n|        |                         | **Marginal (D)**|\\n|        |                         | **Margin (Sem.)**|\\n| **A**  | 0.782 \u00b1 0.008          | 0.034 \u00b1 0.017   |\\n| **B**  | 0.809 \u00b1 0.004          | 0.003 \u00b1 0.007   |\\n| **C**  | 0.794 \u00b1 0.005          | 0.014 \u00b1 0.008   |\\n| **D**  | 0.805 \u00b1 0.004          | 0.024 \u00b1 0.009   |\\n| **E**  | 0.798 \u00b1 0.005          | 0.024 \u00b1 0.008   |\\n| **F**  | 0.749 \u00b1 0.006          | 0.140 \u00b1 0.030   |\\n\\n### C.5 Exploration of Fairness-Utility Tradeoff and Varying Hyperparameters in PARADE\\n\\nWe vary $\\\\alpha_{SA}$ and $\\\\rho$ in the PARADE objective to explore the relationship between the overall performance, subgroup gap, and worst-group performance in PARADE. As stated in the main paper, we optimize $\\\\alpha_{SA}$ and $\\\\rho$ via worst-group performance. Results of this analysis are displayed in Figure 12. We use our exploration to expound on how to optimize for $\\\\alpha_{SA}$ and $\\\\rho$. As seen in Figure 12, a clear trend that inversely relates overall performance, and fairness as measured by subgroup gap and worst-group performance is seen for the uniformity metric, $\\\\mathcal{U}_{KL}$ over the grid of $\\\\alpha_{SA}$ and $\\\\rho$ values (Note that higher values of $\\\\mathcal{U}_{KL}$ correspond to worse performance). Recall@1 and NMI demonstrate noisier relationships between overall performance and fairness; and several $\\\\alpha_{SA}$, $\\\\rho$ choices appear to select an optimal tradeoff. In Figure 12, for Recall@1, we observe that at the location $\\\\alpha_{SA}=0.1$, $\\\\rho=500$ in the optimization grid, PARADE reaches peak overall performance and fairness (measured by low subgroup gap and high performance for the worst-performing subgroup) simultaneously. Thus, we could conclude that this choice of $\\\\alpha_{SA}$ and $\\\\rho$ represents an optimal tradeoff for utility and fairness.\"}"}
{"id": "js62_xuLDDv", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where to operate within this trade-off should depend on the application that is being targeted. For\\n\\nOverall results on CelebA.\\n\\nTable 13: Absolute performance for all CUB200 subgroups.\\n\\n| Seeds | PARADE | Standard |\\n|-------|--------|----------|\\n| 1     |        |          |\\n| 2     |        |          |\\n| 3     |        |          |\\n| 4     |        |          |\\n| 5     |        |          |\\n\\nBy the other displayed metrics, we see that\\n\\ndownstream classification (logistic regressor) over\\n\\ndownstream classifiers (RF, LR and SVM) in the manually class imbalanced CUB200 experiments.\\n\\nFigure 9: Impact of varying imbalance between the\\n\\nclassifiers downstream with more imbalance introduced to the upstream training data.\\n\\nTable 14: Distance N-Pair \u00b7 Distance N-Pair for Arcface Margin, Margin (D), Parade Margin (D) and Color, olive orange red white yellow yellow.\\n\\n| Seeds | Arcface Margin | Distance N-Pair \u00b7 Distance N-Pair |\\n|-------|----------------|-----------------------------------|\\n| 1     |                |                                   |\\n| 2     |                |                                   |\\n| 3     |                |                                   |\\n| 4     |                |                                   |\\n| 5     |                |                                   |\"}"}
