{"id": "aqTipMg9CZ", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: RMSE of REMO, Graphormer-p, and Graphormer-s on MoleculeACE 17\"}"}
{"id": "aqTipMg9CZ", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: RMSE cliff of REMO, Graphormer-p, and Graphormer-s on MoleculeACE 18\"}"}
{"id": "aqTipMg9CZ", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Case 1: Reconstruction of the reaction centre involving a carbon atom and a nitrogen atom. REMO exhibits higher confidence in the reconstruction choice, while Graphormer-p shows increased uncertainty (evidenced by higher predicted logits for the masked nitrogen atom in indices 17 to 30, and for the masked carbon atom in indices 3 to 10).\\n\\nFigure 10: Case 2: Reconstruction of the reaction centre involving a carbon atom in a substituent reaction. In the absence of reaction context, Graphormer-p exhibits higher confusion, as indicated by the flatter distribution of logits observed in the heatmap.\"}"}
{"id": "aqTipMg9CZ", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: Case 3: Reconstruction of the reaction centre involving a carbon atom and a chlorine atom. Graphormer-p fails to reconstruct the chlorine atom in the absence of reaction context, whereas REMO accurately identifies the correct answer. Additionally, REMO demonstrates a more definitive choice for mask reconstruction of the carbon atom.\\n\\nFigure 12: Case 4: The reaction involves the substitution of an iodine atom by an organozinc compound. In the absence of reaction context, Graphormer-p fails to reconstruct the reaction centre, specifically a carbon atom, whereas REMO successfully identifies the correct reaction centre.\"}"}
{"id": "aqTipMg9CZ", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"REMOI-Graphormer\\nREMOM-Graphormer\\nECFP+SVM\\n\\nFigure 5: RMSE of REMOM-Graphormer, REMOI-Graphormer, and ECFP+SVM on MoleculeACE\\n\\nC.2 Detailed Results of Ablation Studies on MoleculeACE\\n\\nThe detailed results of the conducted ablation studies on MoleculeACE are depicted in Figure 7 and Figure 8, showcasing the superiority of our contextual pre-training objective over the single molecular-based masking strategy.\"}"}
{"id": "aqTipMg9CZ", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To gain deeper insights into the distinctions between REMO and Graphormer-p during the pre-training phase, we perform a detailed analysis of the output logits derived from the mask reconstruction process. The prediction head, responsible for the reconstruction task, generates a vector of length 2401, which corresponds to the 2401 possible 1-hop topological combinations of atoms in the dataset. The primary objective of this prediction head is to accurately classify the masked component using the available dictionary of 2401 tokens. Through this meticulous examination, we obtain additional evidence highlighting the divergent approaches and characteristics of REMO and Graphormer-p throughout the pre-training phase.\\n\\nAs depicted in Figures 9-12, the variations in logits for reconstructing the reaction centre between REMO and Graphormer-p are visualized as square heatmaps of size $49 \\\\times 49$, representing the 2401 possible values in the token dictionary. The target label is highlighted by red squares.\\n\\nIn Table 5 REMO IM AttrMASK and REMO IM GraphMAE undergo continuous pretraining using the updated objective function. We acknowledge that distinguishing between the performance improvement stemming from increased pretraining data and the impact of the modified objective function can be unclear. To address this concern, we have conducted comprehensive ablation studies specifically focused on REMO IM AttrMASK and REMO IM GraphMAE. The aim of these studies is to elucidate the precise influence of the objective function alteration.\\n\\nAttrMASK - USPTO and GraphMAE - USPTO involve consistent pretraining of the original model using molecules from the USPTO dataset. Although they exhibit comparable performance to the original models across numerous benchmarks, they do not achieve comparable results to REMO for certain tasks. Remarkably, when introducing USPTO data through the original method, the overall average score even experiences a slight reduction. Nonetheless, the advantage stemming from the new objective function remains conspicuous.\\n\\nThe potential challenge of imbalanced distribution in Masked Reaction Centre Reconstruction has been thoroughly examined. Specifically, we assessed the distribution of atoms and their adjacent bonds in both the ZINC dataset and the USPTO dataset, adhering to the reconstruction target construction approach outlined in Rong et al. (2020). To ensure clarity, we created a dictionary that encompasses atoms along with their neighboring bonds. This facilitated an in-depth analysis of the distribution of reconstruction targets. We conducted this analysis across three significant datasets: ZINC, the broader USPTO dataset, and the reaction centres within USPTO.\\n\\nUpon analyzing the distribution of atom and edge types, we selected the top 20 labels with the highest frequencies for closer examination, addressing the issue of label imbalance.\"}"}
{"id": "aqTipMg9CZ", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As shown in Figure 13, in the ZINC dataset, the top 20 atom labels constituted approximately 76.54% of the dataset, with the highest frequency label being carbon connected with two aromatic bonds at a rate of 22.58%.\\n\\nAs shown in Figure 14, within the USPTO dataset, the top 20 atom labels accounted for around 71.53% of the dataset, and the most frequent label was carbon connected with two aromatic bonds at a rate of 19.79%.\\n\\nAs shown in Figure 15, in the reconstruction targets of USPTO, the top 20 atom labels represented approximately 63.58% of the total reconstruction targets, with the label nitrogen connected with a single bond occurring most frequently at a rate of 8.62%.\\n\\nIt is noteworthy that the issue of imbalanced distribution exists in both the ZINC and USPTO datasets, with a prevalence of carbon atoms. However, within the context of masked reaction centre reconstruction, this concern is mitigated compared to the random masking of atoms. Indeed, the distribution of atom types within the reconstruction target is an advantageous aspect of REMO compared to other baseline models. The reconstruction target's relative balance is a notable strength when compared to the data treatment employed by other baselines.\"}"}
{"id": "aqTipMg9CZ", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 6: RMSE cliff of REMOM-Graphormer, REMOI-Graphormer, and ECFP+SVM on MoleculeACE 16\"}"}
{"id": "aqTipMg9CZ", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nFigure 13: Distribution of Atom Types in ZINC\\n\\nFigure 14: Distribution of Atom Types in USPTO\"}"}
{"id": "aqTipMg9CZ", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nFigure 15: Distribution of Atom Types in reconstruction centres in USPTO\"}"}
{"id": "aqTipMg9CZ", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In recent years, self-supervised learning has emerged as a powerful tool to harness abundant unlabelled data for representation learning, and has been broadly adopted in diverse areas. However, when applied to molecular representation learning (MRL), prevailing techniques such as masked sub-unit reconstruction often fall short, due to the high degree of freedom in the possible combinations of atoms within molecules, which brings insurmountable complexity to the masking-reconstruction paradigm. To tackle this challenge, we introduce REMO, a self-supervised learning framework that takes advantage of well-defined atom-combination rules in common chemistry. Specifically, REMO pre-trains graph/Transformer encoders on 1.7 million known chemical reactions in the literature. We propose two pre-training objectives: Masked Reaction Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI). REMO offers a novel solution to MRL by exploiting the underlying shared patterns in chemical reactions as context for pre-training, which effectively infers meaningful representations of common chemistry knowledge. Such contextual representations can then be utilized to support diverse downstream molecular tasks with minimum fine-tuning, such as molecule property prediction and affinity prediction. Extensive experimental results on MoleculeNet, MoleculeACE, ACNet, and drug-drug interaction (DDI) show that across all tested downstream tasks, REMO outperforms the standard baseline of single-molecule masked modeling used in current MRL.\"}"}
{"id": "aqTipMg9CZ", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"very different chemical properties. In such cases, traditional masked reconstruction loss is far from being sufficient as a learning objective.\\n\\nIs there an essential natural principle that guides the atomic composition within molecules? In fact, most biochemical or physiological properties of a molecule are determined and demonstrated by its reaction relations to other substances (e.g., other small molecules, protein targets, bio-issues). Further, these natural relations between different reactants (e.g., molecules) are reflected by chemical reaction equations that are public knowledge in the literature. These chemical reaction relations offer a reliable context for the underlying makeup of molecule sub-structures, especially for reactive centres. As in Figure 1a, the choice of carbon or oxygen can be easily determined by different reactants, even though the two molecules have very similar structures.\\n\\n\\\\[\\n\\\\begin{align*}\\nK_i &= 2670 \\\\text{ nM} \\\\\\\\\\nK_i &= 560 \\\\text{ nM}\\n\\\\end{align*}\\n\\\\]\\n\\nFigure 1: a: Adding either carbon or oxygen to the left molecule can lead to the formation of a valid molecule. However, these resulting molecules exhibit distinct reactions and consequently possess varying properties. b: Example of an activity cliff pair on the target Thrombin (F2), where \\\\( K_i \\\\) is to measure the equilibrium binding affinity for a ligand that reduces the activity of its binding partner.\\n\\nMotivated by these insights, we propose a new self-supervised learning method, REMO, by harnessing chemical reaction knowledge to model molecular representations. Specifically, given a chemical reaction equation known in the literature, we mask the reaction centres and train the model to reconstruct the missing centres based on the given reactants as context. The key difference between REMO and existing methods that apply masked language model to a single molecule is the usage of sub-molecule interaction context, as illustrated in Fig 2. By relying on chemical reactants as context, the degree of freedom in possible combinations of atoms within a molecule is significantly reduced, and meaningful sub-structure semantics capturing intrinsic characteristics can be learned in the reconstruction process.\\n\\nExtensive experiments demonstrate that REMO achieves new state-of-the-art results against other pre-training methods on a wide range of molecular tasks, such as activity-cliff, molecular property prediction, and drug-drug interaction. And to the best of our knowledge, REMO is the first deep learning model to outperform traditional methods on activity-cliff benchmarks such as MoleculeACE and ACNet.\"}"}
{"id": "aqTipMg9CZ", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Since this work is related to masked language model (MLM), we review recent MLM-based molecular representation learning methods. MLM is a highly effective proxy task, which has recently been applied to molecular pre-training on SMILES sequences, 2D graphs, and 3D conformations.\\n\\nFor pre-training on SMILES sequences, existing work including ChemBERTa (Chithrananda et al., 2020), MG-BERT (Zhang et al., 2021) and Molformer (Ross et al., 2022) directly apply MLM to SMILES sequence data, and conduct similar reconstruction optimization to obtain molecular representations. When applied to 2D graphs, different masked strategies have been proposed. For example, AttrMask (Hu et al., 2020) randomly masks some atoms of the input graph and trains a Graph Neural Network (GNN) model to predict the masked atom types. GROVER (Rong et al., 2020) trains the model to recover masked sub-graph, which consists of k-hop neighbouring nodes and edges of the target node. Mole-BERT (Xia et al., 2023) uses a VQ-V AE tokenizer to encode masked atoms with contextual information, thereby increasing vocabulary size and alleviating the unbalanced problem in mask prediction. By capturing topological information in a graph, these methods have the ability to outperform the aforementioned SMILES-based methods.\\n\\nRecently studies also apply MLM to 3D conformations. For example, Uni-MOL (Zhou et al., 2023) predicts the masked atom in a noisy conformation, while ChemRL-GEM (Fang et al., 2022) predicts the atom distance and bond angles from masked conformation. Yet these methods suffer from limited 3D conformation data.\\n\\nIn summary, MLM has been widely used in molecule representation over different data forms (1D, 2D, 3D). Given a single molecule, some sub-structures are masked, and a reconstruction loss is computed to recover the semantic relations between masked sub-structures and remaining contexts. However, this training objective does not fit the essence of molecule data and may lead to poor performance.\\n\\nFor example, MoleculeACE (van Tilborg et al., 2022) and ACNet (Zhang et al., 2023) demonstrate that existing pre-trained models are incomparable to SVM based on fingerprint on activity cliff.\\n\\nMost existing work on modelling chemical reactions is based on supervised learning (Jin et al., 2017; Coley et al., 2019). For example, Jin et al. (2017) and Coley et al. (2019) use a multiple-step strategy to predict products given reactants, and achieve human-expert comparable accuracy without using reaction templates.\\n\\nRecently, self-supervised learning methods have been proposed to utilize chemical reaction data for pre-training, including Wen et al. (2022), Wang et al. (2022) and Broberg et al. (2022). Wen et al. (2022) conducts a contrastive pre-training on unlabelled reaction data to improve the results of chemical reaction classification. Wang et al. (2022) focuses on introducing reaction constraints on GNN-based molecular representations, to preserve the equivalence of molecules in the embedding space. Broberg et al. (2022) pre-trains a transformer with a sequence-to-sequence architecture on chemical reaction data, to improve the performance of molecular property prediction.\\n\\nAlthough these methods use chemical reaction data for pre-training, REMO is different as the objective is to focus on reaction centre, i.e. masked reaction centre construction and identification, which better fits chemical reaction data. REMO also provides a general framework that can be used in different architectures, such as transformer and GNNs, to support diverse downstream tasks.\\n\\nThis section describes the molecule encoder and the pre-training process of REMO, specifically, its two pre-training tasks: Masked Reaction Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI).\"}"}
{"id": "aqTipMg9CZ", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Molecule Encoder + CM1 CM2 ... CMn ... C\u00b7R1 C\u00b7R2 C\u00b7R3 C\u00b7Rn\\n\\nFigure 3: Architecture of REMO.\\n\\n(a): An example chemical reaction consisting of two reactants and one product, where the primary reactant has two reaction centres.\\n\\n(b): Illustrations of two pre-training tasks in REMO, i.e., Masked Reaction Centre Reconstruction and Reaction Centre Identification.\\n\\nWe initialized embedding based on its type. Particularly, we experiment with two model architectures for pre-training: one is GNN-based, with Graph Isomorphism Network (GIN) (Xu et al., 2018) as representative; the other is transformer-based, with Graphormer as the backbone (Ying et al., 2021).\\n\\nGIN (Xu et al., 2018) learns representations for each node and the whole graph through a neighbour aggregation strategy. Specifically, each node $v \\\\in V$ updates its representation based on its neighbours and itself in the aggregation step, and the $l$-th layer of a GIN is defined as:\\n\\n$$h(l)_v = \\\\text{MLP}(l) \\\\left[ 1 + \\\\epsilon(l) \\\\cdot h(l-1)_v + \\\\sum_{u \\\\in N(v)} h(l-1)_u + \\\\sum_{u \\\\in N(v)} e_{uv} \\\\right]$$\\n\\nwhere $h(l)_v$ is the representation of node $v$ in the $l$-th layer, $e_{uv}$ is the representation of the edge between $u$ and $v$, and $N(v)$ stands for the neighbors of $v$.\\n\\nThen, we obtain the graph-level representation through a \u201cREADOUT\u201d function, with the average of node features used as the pooling layer:\\n\\n$$h_G = \\\\text{READOUT}_n h(K_v | v \\\\in G).$$\\n\\nGraphormer (Ying et al., 2021) is the first deep learning model built upon a standard Transformer that greatly outperforms all conventional GNNs on graph-level prediction tasks, and has won the first place in the KDD Cup-OGB-LSC quantum chemistry track (Hu et al., 2021). The basic idea is to encode graph structural information to positional embeddings, to facilitate Transformer architecture. Specifically, Graphormer introduces three types of positional embeddings, and we simply follow the original architecture. The attention in Graphormer is calculated by:\\n\\n$$A_{ij} = (h_i W_Q) (h_j W_K)^T \\\\sqrt{d} + b \\\\phi(v_i, v_j) + c_{ij}, c_{ij} = \\\\frac{1}{N} \\\\sum_{n=1}^{N} x_n e_n w_n E_n T,$$\\n\\nwhere $h_i$ and $h_j$ are centrality embeddings to capture the node importance in the graph, $b \\\\phi(v_i, v_j)$ is spatial encoding calculated by the shortest path (SP) to capture the structural relation between nodes.\"}"}
{"id": "aqTipMg9CZ", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As shown in Table 4, our pre-trained model outperforms all the baselines, indicating that REMO effectively captures molecule interaction information from chemical reaction data.\\n\\nIn order to further explain the advantage of REMO compared to traditional masked language model on single molecule, we conduct an ablation study to compare AttrMask and REMO, where Graphormer is utilized as backbones of both methods to ensure a fair comparison. Firstly, we show the comparison results on MoleculeACE. As shown in Figure 4 Right, even though pre-trained Graphormer (i.e. Graphormer-p) outperforms supervised Graphormer (i.e. Graphormer-s), it is inferior to REMO, demonstrating the superiority of our contextual pre-training objective. To explain why our pre-training strategy is much more effective, we propose to compare the information entropy. The main difference of our pre-trained model and the traditional one is that we utilize different context, i.e. we further use other reactants in chemical reactions as the context to determine the semantic of a substructure in a molecule. That is to say, we are modelling a conditional probability \\\\( P(z|Z,R) \\\\) as compared with the traditional one \\\\( Q = P(z|Z) \\\\), where \\\\( Z \\\\) stands for the remaining substructures in a molecule and \\\\( R \\\\) stands for other reactants in the chemical reactions. It is clear that the information entropy in our case is much lower than that in traditional case, i.e. \\\\( H(P) \\\\leq H(Q) \\\\), meaning that the choice of masked sub-units becomes more definite under the context of chemical reactions. To qualitatively evaluate their difference, we select 10,000 reactions from USPTO-full and conduct the reconstructions separately. The result in Figure 4 Left show a clear distribution shifts to left with the average information entropy dropping from 2.03 to 1.29, indicating that introducing reaction information contributes to the reaction centre reconstruction.\\n\\n|             | RMSE | RMSE | \\\\( \\\\text{cliff} \\\\) |\\n|-------------|------|------|-------------------|\\n| Graphormer-s| 0.805(0.017) | 0.848(0.003) |\\n| Graphormer-p| 0.695(0.004) | 0.821(0.004) |\\n| REMO        | 0.694(0.006) | 0.771(0.004) |\\n\\nFigure 4: Left: The information entropy of reconstructing reaction centres from reactions (blue), and reconstructing the same atoms of reaction centres without the context of reactions (red). Right: Ablation study of REMO on MoleculeACE.\\n\\nWe propose REMO, a novel self-supervised molecular representation learning method (MRL) that leverage the unique characteristics of chemical reactions as knowledge context for pre-training. By involving chemical reactants as context, we use both masked reaction centre reconstruction and reaction centre identification objectives for pre-training, which reduces the degree of freedom in possible combinations of atoms in a molecule as in traditional MLM approaches for MRL. Extensive experimental results reveal that REMO achieves state-of-the-art performance with less pre-training data on a variety of downstreaming tasks, including activity cliff prediction, molecular property prediction, and drug-drug interaction, demonstrating the superiority of contextual modelling with chemical reactions.\\n\\nReferences\\nJohan Broberg, Maria Margareta B\u00e5nestad, and Erik Ylip\u00e4\u00e4 Hellqvist. Pre-training transformers for molecular property prediction using reaction prediction. In ICML 2022 2nd AI for Science.\"}"}
{"id": "aqTipMg9CZ", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yujie Chen, Tengfei Ma, Xixi Yang, Jianmin Wang, Bosheng Song, and Xiangxiang Zeng. Muffin: multi-scale feature fusion for drug\u2013drug interaction prediction. *Bioinformatics*, 37(17):2651\u20132658, 2021.\\n\\nSeyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta: Large-scale self-supervised pretraining for molecular property prediction. *arXiv preprint arXiv:2010.09885*, 2020.\\n\\nConnor W. Coley, Wengong Jin, Luke Rogers, Timothy F. Jamison, Tommi S. Jaakkola, William H. Green, Regina Barzilay, and Klavs F. Jensen. A graph-convolutional neural network model for the prediction of chemical reactivity. *Chem. Sci.*, 10:370\u2013377, 2019. doi: 10.1039/C8SC04228D.\\n\\nXiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, and Haifeng Wang. Geometry-enhanced molecular representation learning for property prediction. *Nature Machine Intelligence*, 4(2):127\u2013134, 2022.\\n\\nJ. R. Firth. A synopsis of linguistic theory, 1930-1955. 1957.\\n\\nJiang Guo, A Santiago Ibanez-Lopez, Hanyu Gao, Victor Quach, Connor W Coley, Klavs F Jensen, and Regina Barzilay. Correction to automated chemical reaction extraction from scientific literature. *Journal of Chemical Information and Modeling*, 61(8):4124\u20134124, 2021.\\n\\nShion Honda, Shoi Shi, and Hiroki R Ueda. Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery. *arXiv preprint arXiv:1911.04738*, 2019.\\n\\nZhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, pp. 594\u2013604, 2022.\\n\\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In *International Conference on Learning Representations*, 2020. URL https://openreview.net/forum?id=HJlWWJSFDH.\\n\\nWeihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A large-scale challenge for machine learning on graphs. *arXiv preprint arXiv:2103.09430*, 2021.\\n\\nJohn J Irwin, Khanh G Tang, Jennifer Young, Chinzorig Dandarchuluun, Benjamin R Wong, Munkhzul Khurelbaatar, Yurii S Moroz, John Mayfield, and Roger A Sayle. Zinc20\u2014a free ultralarge-scale chemical database for ligand discovery. *Journal of chemical information and modeling*, 60(12):6065\u20136073, 2020.\\n\\nWengong Jin, Connor W. Coley, Regina Barzilay, and Tommi Jaakkola. Predicting organic reaction outcomes with weisfeiler-lehman network. In *Proceedings of the 31st International Conference on Neural Information Processing Systems*, NIPS'17, pp. 2604\u20132613, Red Hook, NY , USA, 2017. Curran Associates Inc. ISBN 9781510860964.\\n\\nXinyuan Lin, Chi Xu, Zhaoping Xiong, Xinfeng Zhang, Ningxi Ni, Bolin Ni, Jianlong Chang, Ruiqing Pan, Zidong Wang, Fan Yu, et al. Pangu drug model: learn a molecule like a human. *bioRxiv*, pp. 2022\u201303, 2022.\\n\\nShengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. Pre-training molecular graph representation with 3d geometry. *arXiv preprint arXiv:2110.07728*, 2021.\\n\\nDaniel Lowe. Chemical reactions from us patents (1976-sep2016). https://doi.org/10.6084/m9.figshare.5104873.v1, 2017. Dataset.\\n\\n\u0141ukasz Maziarka, Tomasz Danel, S\u0142awomir Mucha, Krzysztof Rataj, Jacek Tabor, and Stanis\u0142aw Jastrzebski. Molecule attention transformer. *arXiv preprint arXiv:2002.08264*, 2020.\\n\\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In *Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining*, pp. 701\u2013710, 2014. 10.\"}"}
{"id": "aqTipMg9CZ", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "aqTipMg9CZ", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The probability of pre-training from single molecules could be presented as\\n\\\\[ Q = P(z \\\\mid Z) = \\\\sum z' P(z \\\\mid z', Z) \\\\cdot P(z' \\\\mid Z) \\\\]\\n(6)\\nwhere \\\\( z \\\\) stands for the masked sub-units of molecules for reconstruction, \\\\( Z \\\\) stands for the remaining substructures in a molecule, \\\\( z' \\\\) stands for the embedding of \\\\( z \\\\) for prediction. \\\\( z' (Z) \\\\) stands for \\\\( z' \\\\) is solely dependent on \\\\( Z \\\\).\\n\\nThe probability of pre-training from chemical reactions, as REMO proposes could be presented as\\n\\\\[ P = P(z \\\\mid Z, R) = \\\\sum (R', z') P(z \\\\mid z', R', Z, R) \\\\cdot P(z', R' \\\\mid Z, R) \\\\]\\n(7)\\nwhere \\\\( R \\\\) stands for the reactants other than \\\\( Z \\\\) in the chemical reaction, and \\\\( R' \\\\) stands for the embedding of \\\\( R \\\\). \\\\( R' (R) \\\\) stands for the embedding \\\\( R' \\\\) is solely dependent on \\\\( R \\\\).\\n\\nB PROOF FOR THE SUPERIORITY OF REMO-TRAINING\\nIn order for a given molecule \\\\( Z \\\\) to undergo a reaction with another reactant \\\\( R \\\\), only specific sub-units \\\\( z \\\\) can enable the occurrence of the reaction. It stands for the information gain between \\\\( z \\\\) and \\\\( R \\\\) on the condition of \\\\( Z \\\\) is greater than 0.\\n\\n\\\\[ I(z, R \\\\mid Z) > 0 \\\\]\\n(8)\\n\\n\\\\[ H(z \\\\mid Z) - H(z \\\\mid Z, R) > 0 \\\\]\\n(9)\\n\\n\\\\[ H(z \\\\mid Z, R) < H(z \\\\mid Z) \\\\]\\n(10)\\n\\nComparing to the reconstruction of \\\\( z \\\\) solely conditioned on \\\\( Z \\\\), reconstructing \\\\( z \\\\) as the reaction centre conditioned on both \\\\( Z \\\\) and \\\\( R \\\\) is more definite.\\n\\nC DETAILED RESULTS ANALYSIS ON MOLECULE ACE\\nC.1 DETAILED RESULTS OF SOTA RESULTS ON MOLECULE ACE\\nWe show the detailed performance of 3 best methods including ECFP+SVM, REMO-I-Graphormer and REMO-M-Graphormer on 30 sub-tasks of MoleculeACE in Figure 5 and Figure 6, illustrating REMO's mastery in grasping the Quantitative Structure-Activity Relationship (QSAR).\"}"}
{"id": "aqTipMg9CZ", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 LEARNING OBJECTIVE\\n\\nOur design philosophy of the learning objective is as follows: reaction centres are the core of a chemical reaction, which depict meaningful structural and activity-related relations within a molecule. Specifically, chemical reaction describes a process in which one or more substances (i.e., reactants) are transformed to one or more different substances (i.e., products). This process involves exchanging electrons between or within molecules, and breaking old chemical bonds while forming new ones. It has been observed that only parts of molecules are directly involved in reactions (known as 'reaction centres') in organic chemistry. Therefore, reaction centres are the key to revealing the behaviors of molecule reactivity properties during the chemical reaction process.\\n\\nFormally, each chemical reaction is viewed as two graphs: reactant graph $G_r$ and product graph $G_p$, which are unconnected when there are multiple reactants or products. Since reaction centres are usually not provided by chemical reaction data, we utilize a widely used technique (Jin et al., 2017; Shi et al., 2020) to detect corresponding reaction centres. Specifically, the reaction centre for a chemical reaction process is represented as a set of atom pairs $(a_i, a_j)$, where the bond type between $a_i$ and $a_j$ differs between the aligned reactant graph $G_r$ and the product graph $G_p$. Intrinsically, the reaction centre is the minimum set of graph edits to transform reactants to products.\\n\\n3.3asked reaction centre reconstruction\\n\\nWe propose a new pre-training task, where each reactant in a chemical reaction is treated as the primary molecule, and the remaining reactants are treated as its contexts or conditional molecules. The reaction centre atoms in the primary molecule and their adjacent bonds are masked with a special token, denoted as $[\\\\text{MASK}]$. Then all the conditional molecules are fed into the molecule encoder to obtain a global representation. Specifically for GIN encoder, the node vector of each atom in conditional molecules are pooled to produce a global representation $c$. While for Graphormer, the process is a bit different. To adapt to the transformer architecture, we introduce a virtual node to connect all the atoms together, and the representation of the virtual node in the last layer is treated as the global representation.\\n\\nThe masked primary molecule with $k$ reaction centres is directly fed into the molecule encoder (GIN or Graphormer), and the final hidden vectors corresponding to each $[\\\\text{MASK}]$ token are produced by concatenating the vectors of atoms belonging to this reaction centre, denoted as $M_j$, $\\\\forall j = 1, \\\\ldots, k$.\\n\\nAfter that, both $c$ and each $M_j$, $j = 1, \\\\ldots, k$ are fed into a 2-layer MLP to output a softmax over the vocabulary to predict the probability of the masked reaction centre. Specifically, the vocabulary is produced by enumerating all the topological combinations of the atoms and their adjacent bonds. The model is expected to predict the exact atom type and the adjacent bonds during the pre-training stage:\\n\\n$$P(g_j) = \\\\text{softmax}(\\\\text{MLP}(c, M_j)), \\\\forall j = 1, \\\\ldots, k,$$\\n\\nwhere $g_j$ stands for the graph representation of the $j$-th reaction centre, including both atom types and adjacent bonds. Finally, the sum of negative likelihood loss is used for training.\\n\\n3.4 reaction centre identification\\n\\nAnother novel pre-training objective in REMO directly conducts classification on each atom without introducing any $[\\\\text{MASK}]$ label. For each primary molecule in a chemical reaction, all the conditional molecules are fed into the molecule encoder to obtain a global representation $c$, similar to that in Masked Reaction Centre Reconstruction. The primary molecule is directly fed into the molecule encoder (GIN or Graphormer), and the final hidden vectors corresponding to each atom are produced, denoted as $R_j$, $\\\\forall j = 1, \\\\ldots, N$, where $N$ stands for the number of atoms in the primary molecule.\\n\\nAfter that, both $c$ and each $R_j$, $j = 1, \\\\ldots, N$ are fed into a 2-layer MLP to predict whether an atom belongs to a reaction centre, as follows:\\n\\n$$P(y_j) = \\\\text{softmax}(\\\\text{MLP}(c, R_j)), \\\\forall j = 1, \\\\ldots, N,$$\\n\\nwhere $y_j = 1/0$ denotes whether the $j$-th atom is a reaction centre. During training, the sum of the negative likelihood loss is utilized as the objective function.\"}"}
{"id": "aqTipMg9CZ", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Table 1**: Comparison between REMO and baselines on MoleculeACE. The best and second best results are indicated in bold and underlined, respectively.\\n\\n### 4 EXPERIMENTS\\n\\nWe evaluate REMO against state-of-the-art (SOTA) baselines on multiple molecular tasks, including activity cliff prediction, molecular property prediction, and drug-drug interaction. Extensive experiments show that REMO achieves new SOTA on all tasks.\\n\\n#### 4.1 IMPLEMENTATION\\n\\nFirstly, we introduce the pre-training of REMO. We utilize USPTO (Lowe, 2017) as our pre-training data, which is a subset and preprocessed version of Chemical reactions from US patents (1976-Sep2016). From different versions of USPTO, we select the largest one, i.e. USPTO-full, which contains 1.9 million chemical reactions. After removing the reactions that fail to locate reaction centres, there remain 1,721,648 reactions in total, which constitutes our pre-training dataset. In statistics, this dataset covers 0.7 million distinct molecules as reactants.\\n\\nWe implement three versions of REMO, denoted as **REMO**\\\\_M, **REMO**\\\\_I and **REMO**\\\\_IM, to represent using different training objectives, i.e. masked reaction centre reconstruction, reaction centre identification, and both. As for the molecule encoders, we implement both GIN and Graphormer, with the same configuration from the original paper. Specially for Graphormer, we select the small configuration.\\n\\nWe apply Adam optimizer throughout the pre-training and fine-tuning procedure. Specifically, the learning rate is set to 3e-4, and the batch size is set to 128. We randomly split 10,000 reactions as the validation set, and the pre-training lasts for 5 epochs over the dataset.\\n\\n#### 4.2 ACTIVITY CLIFF PREDICTION\\n\\nActivity cliff is a known challenging task, because all given molecule pairs are structurally similar but exhibit significantly different activities. An accurate activity cliff predictor requires to be sensitive to the structure-activity landscape. SOTA deep learning based molecular representation learning methods could not yield better results compare to fingerprint based methods on this task (van Tilborg et al., 2022; Zhang et al., 2023). In our experiments, we use two benchmarks, MoleculeACE and ACNet.\\n\\n#### 4.2.1 EVALUATION ON MOLECULE ACE\\n\\nData MoleculeACE (van Tilborg et al., 2022) includes 30 tasks, where each task provides binding affinities of different molecules to a specific protein target. Among these molecules, there are some activity-cliff pairs, spreading to training set and test set. The size of every single benchmark ranges from 615 to 3,658, and the range of cliff ratio (the percentage of activity cliff molecules to all molecules) is from 9% to 52%. In experiments, we follow the train-test split of the default setting of MoleculeACE, and accordingly stratified split the training set into train and validation sets by ratio 4:1.\"}"}
{"id": "aqTipMg9CZ", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Comparison between REMO and baselines on the Few subset of ACNet, with AUC-ROC(%) as the evaluation metric.\\n\\n| Models               | AUC   |\\n|----------------------|-------|\\n| GROVER (Rong et al., 2020) | 0.753(0.010) |\\n| ChemBERT (Guo et al., 2021) | 0.656(0.029) |\\n| MAT (Maziarka et al., 2020) | 0.730(0.069) |\\n| Pretrain8 (Zhang et al., 2022) | 0.782(0.031) |\\n| pretrainGNNs (Hu et al., 2020) | 0.758(0.015) |\\n| S.T. (Honda et al., 2019) | 0.822(0.022) |\\n| GraphLoG (Xu et al., 2021) | 0.752(0.040) |\\n| GraphMVP (Liu et al., 2021) | 0.724(0.026) |\\n| ECFP                   | 0.813(0.024) |\\n| REMO                   | 0.828(0.020) |\\n\\n**Baselines**\\n\\nWe compare with the baselines tested by MoleculeACE (van Tilborg et al., 2022), including fingerprint-based methods, SMILES-based methods and graph-based methods. Since we also use Graphormer as our molecule encoder, we implement its supervised version for comparison. In addition, we compare with a pre-training method, AttrMask (Hu et al., 2020) with GIN as the backbone.\\n\\n**Finetuning**\\n\\nThe pre-trained model of REMO is followed by a 2-layer MLP to obtain the logits. Since binding-affinity prediction is a regression task, we utilize RMSE as the loss for finetuning. For evaluation, we follow the default setting of MoleculeACE and report two metrics, averaged RMSE over the entire test set and averaged RMSE of the cliff molecules subset, denoted as $\\\\text{RMSE}_{\\\\text{cliff}}$.\\n\\n**Results**\\n\\nTable 1 shows that:\\n\\n1. Our contextual pretraining strategy is much more suitable for activity cliff, as compared to traditional masked strategy on single molecules. For example, the AttrMask model, a pre-trained GIN (0.820/0.930), performs worse than the original GIN (0.815/0.901). While if we pre-train GIN within REMO, better results (0.767/0.878) are achieved, as shown in REMO-\\n\\n2. None of the deep learning baselines is comparable to the fingerprint-based method ECFP+SVM (0.675/0.762), while REMO has the ability to achieve comparable best results, by employing Graphormer as molecule encoder, as shown in REMO-Graphormer (0.675/0.768) and REMO-M-Graphormer (0.679/0.764).\\n\\n4.2.2 EVALUATION ON ACNet\\n\\nData\\n\\nACNet (Zhang et al., 2023) serves as another benchmark for activity cliff evaluation. Different from MoleculeACE, ACNet specifically focuses on classification task of determining whether a given molecular pair qualifies as an activity cliff pair. In our experiment, we employ a subset of tasks known as Few for evaluation, which is comprised of 13 distinct tasks with a total of 835 samples. Due to the limited size of training data, learning a model from scratch is challenging. Therefore, it is especially suitable to evaluate the transfer learning capability of pre-trained models. We adhere to the split method in ACNet, dividing each task into separate train, validation, and test sets.\\n\\n**Baselines**\\n\\nWe use the same baselines as the original ACNet, including self-supervised pre-trained methods such as Grover (Rong et al., 2020), ChemBERT (Guo et al., 2021), MAT (Maziarka et al., 2020), Pretrain8 (Zhang et al., 2022), PretrainGNNs (Hu et al., 2020), S.T. (Honda et al., 2019), GraphLoG (Xu et al., 2021), and GraphMVP (Liu et al., 2021). Additionally, we include another baseline method that utilizes ECFP fingerprint-based MLP for training. All the results here are from the original ACNet paper.\\n\\n**Finetuning**\\n\\nFollowing the practice of ACNet, we extract the embedding of each molecule in the input pair with pre-trained REMO. Since Graphormer demonstrates superior performance to GIN, we only evaluate REMO with Graphormer as the molecule encoder hereafter. We concatenate these embeddings and feed them into a 2-layer MLP model to predict whether the pair exhibits an activity cliff. Each experiment is repeated three times, and the mean and deviation values are reported. Prediction accuracy is measured by ROC-AUC(%). We run finetuning with 3 random seeds, and take the average along with standard deviation (reported in bracket).\\n\\n**Results**\\n\\nTable 2 shows that REMO clearly outperforms all other pre-trained methods, yielding the best results. This further demonstrates REMO's proficiency in capturing the Quantitative Structure-Activity Relationship (QSAR).\\n\\n4.3 MOLECULAR PROPERTY PREDICTION\\n\\nData\\n\\nWe evaluate our method on MoleculeNet (Wu et al., 2018) which is a prevalent benchmark for molecular property prediction, including physiological properties such as BBBP, Sider, ClinTox, etc.\"}"}
{"id": "aqTipMg9CZ", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Comparison between REMO and baselines on 8 classification tasks of MoleculeNet.\\n\\n| Method       | Accuracy | Precision | Recall | F1-Score |\\n|--------------|----------|-----------|--------|----------|\\n| DeepDDI      | 0.877    | 0.799     | 0.759  | 0.766    |\\n| DeepWalk     | 0.800    | 0.822     | 0.710  | 0.747    |\\n| LINE         | 0.751    | 0.687     | 0.545  | 0.580    |\\n| MUFFIN       | 0.939    |           |        |          |\\n| REMO         | 0.953    | 0.932     | 0.932  | 0.928    |\\n\\nTable 4: Evaluation of REMO on Drug-Drug Interaction task.\\n\\n| Method       | Accuracy | Precision | Recall | F1-Score |\\n|--------------|----------|-----------|--------|----------|\\n| REMO         | 0.953    | 0.932     | 0.932  | 0.928    |\\n\\nWe use the scaffold method to split each dataset into training, validation, and test sets by ratio 8:1:1.\\n\\nBaselines\\n\\nWe compare our methods with various state-of-the-art self-supervised techniques that are also based on 2D graph, including AttrMask (Hu et al., 2020), GraphMAE (Hou et al., 2022), GraphLoG (Xu et al., 2021), Grover (Rong et al., 2020), Mole-BERT (Xia et al., 2023), and GraphMVP (Liu et al., 2021).\\n\\nSince USPTO only covers 0.7 million molecules, we propose to continuously pre-train AttrMask and GraphMAE with both masked reaction center reconstruction and identification objectives, denoted as REMO IM-AttrMask and REMO IM-GraphMAE, respectively.\\n\\nFinetuning\\n\\nWe append a 2-layer MLP after the pre-trained model to get the logits of property predictions, and minimize binary cross-entropy loss for the classification tasks during fine-tuning. ROC-AUC(%) is employed as evaluation metric for these classification tasks. All methods are run 3 times with different seed and the mean and deviation values are reported.\\n\\nResults\\n\\nTable 3 shows that: (1) both pre-training objectives in REMO significantly improve the prediction performances, i.e., increasing from 67.8% (supervised Graphormer denoted as Graphormer-s) to 73.3% (REMO M) and 73.7% (REMO I), respectively. (2) By using only 0.7 million molecules for pre-training, REMO outperforms other methods such as MoleBERT and Grover which use much larger data (i.e., 2 and 10 million molecules respectively), demonstrating the effectiveness of our contextual pre-training strategy. (3) REMO\u2019s performance can be further improved when combined with other methods. As compared to the original AttrMask and GraphMAE, REMO improves their results from 71.2%/73.9% to 72.6%/75.0%, respectively.\\n\\n4.4 DRUG-Drug Interaction Data\\n\\nWe collect the multi-class drug-drug interaction (DDI) data following the work of MUFFIN (Chen et al., 2021). The dataset, which contains 172,426 DDI pairs with 81 relation types, is filtered from DrugBank (Wishart et al., 2018) that relationships of less than 10 samples are eliminated. We randomly split the dataset into training, validation, and test set by 6:2:2.\\n\\nBaselines\\n\\nTo examine the effectiveness of our model, we compare REMO with several baselines, including well known graph based methods DeepWalk (Perozzi et al., 2014), LINE (Tang et al., 2015), and MUFFIN, and a typical deep learning based method DeepDDI (Ryu et al., 2018).\\n\\nFinetuning\\n\\nWe concatenate the paired global embedding of each molecule and feed it into a 2-layer MLP. Then a cross-entropy loss is utilized for fine-tuning. Typical classification metrics such as Precision, Recall, F1, and Accuracy are used for evaluation.\"}"}
