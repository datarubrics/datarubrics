{"id": "5BCFlnfE1g", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12: Hyperparameters of OpenAI CLIP vs OpenCLIP on LAION-400M (Schuhmann et al., 2021) and DataComp 1B.\\n\\nNext, we use KL-divergence to measure alignment:\\n\\n$$D_{KL}(T || P) = -\\\\sum_{m \\\\in M} T(m) \\\\log P(m),$$\\n\\nwhere $T(m)$ represents the task distribution over $M$ and $P(m)$ represents the pre-training data distribution. We compute $T(m)$ using the benchmark data distribution over class labels of a task. Taking ImageNet as an example from Table 11, $T(m)$ ImageNet has 998 entries uniformly distributed (each has evenly 50 examples) and $P(m)$ has normalized counts over all 500k entries but only 703 entries used to compute KL-divergence. In the third and fourth row of Table 11, we can see improvements for balanced data points over unbalanced ones for each task, showing balancing improves similarity with most tasks.\\n\\nA.5 Training Setup of OpenAI CLIP vs OpenCLIP\\n\\nOur work strictly follows CLIP's setup for a controlled comparison focusing on data curation and quality. We notice differences in the training setup of OpenCLIP and list the difference (known to us). OpenCLIP varies the setup from CLIP (e.g., global batch size, learning schedule, etc.). Here we only list the difference for LAION-400M (Schuhmann et al., 2021), which is closer to the CLIP setup. We note that DataComp differs even more, e.g., by curating images close to ImageNet training data, a large batch size of 90k that is almost 3\u00d7 larger than CLIP, and using the CLIP model to filter data.\\n\\nA.6 Benchmark Data Duplication\\n\\nOur pools are deduplicated from the benchmark/ImageNet data using a 64-bit PCA hash, derived from a similarity search model's feature embeddings with PCA reduction to 64 dimensions and sign quantization. DataComp-12.8B is already deduplicated.\\n\\nA.7 Negative Results Learned From CLIP Curation\\n\\nWe briefly describe a few ideas close to CLIP curation that did not look promising in our initial attempts and were abandoned:\\n\\n1. Self-curated Metadata. We initially attempted to build metadata directly from the text in raw image-text pairs (i.e., using terms appearing in text above a certain threshold of counts). We rank entries by count and keep the top 500,000. Metadata built this way appeared worse. We notice that although the top frequent entries are similar to CLIP's metadata, the long-tailed part is very different. For example, the minimal count to be in the 500,000 budget needs at least 130 counts. In contrast, our metadata has 114K entries that have no matches. This approach results in worse quality metadata including low-quality spelling/writing (instead of high-quality entries from WordNet or Wikipedia). Further, the effect of balancing saturates earlier for such data (in a larger $t$, verified by CLIP training) since low-quality entries are also heavily in long-tail.\\n\\n2. Cased WordNet. We also notice many cased words are missing from metadata (e.g., WordNet is in lowercase). After adding cased WordNet into metadata, we notice a performance drop on ImageNet. The reason could be class names are more likely in lower case and upper case entry matching may reduce the written quality of texts.\"}"}
{"id": "5BCFlnfE1g", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Stopwords/Useless Entries Removal\\n\\nWe further study the effect of whether removing stopwords and useless words such as \\\"photo\\\" and \\\"image\\\" is beneficial. This led to almost no difference since balancing entries reduced the effects of useless entries (each entry contributes to 0.0002% (1/500k) level of the total data points). To encourage a simplified solution, we do not intend to add more artificial filters.\\n\\nA.8 MORE DETAILS ON DISTRIBUTION ON META DATA\\n\\nExtending the top-20 matches in Table 3, we further group counts of metadata entries and show 5 examples per group as in Table 13.\\n\\n| Group     | Examples (Entry:Count)                       |\\n|-----------|---------------------------------------------|\\n| 0-10k     | ivailo:12, Kunta Kinte:201, vikernes:33, peria:50, ankoku:20 |\\n| 10k-20k   | queer:19k, barry:10k, bandages:12k, The Police:15k, sigma:14k |\\n| 20k-50k   | polygonal:21k, widely:28k, however:35k, toppers:25k, executives:21k |\\n| 50k-100k  | planted:52k, olive oil:58k, yours:63k, packages:82k, Spokane:53k |\\n| 100k-500k | compact:133k, vertical:222k, underwear:111k, powder:323k, weekly:130k |\\n| 500k-1M   | Tokyo:713k, Lead:620k, Diagram:809k, Dye:858k, unnamed:512k |\\n| 1M-50M    | see:1.4M, Door:3.2M, News:2.3M, sea:1.1M, street:1M |\\n| 50M-130M  | with:67M, and:100M, to:61M, in:107M, of:121M |\\n\\nTable 13: Distribution of metadata entries with counts, similar as head shown in Table 3.\\n\\nA.9 RANDOMNESS OF ALGORITHM 1\\n\\nWe further study the randomness of algorithm 1 by running it 3 times for Pool 1(400M) and Pool 2(2.5B). This ended with a standard deviation of 4035 examples for Pool 1 and 2104 examples for Pool 2, respectively.\\n\\nA.10 QUALITATIVE DATA EXAMPLES\\n\\nIn Table 14, we illustrate data before/after sub-string matching and balancing. We also highlight class labels from ImageNet in the table. We mark a matched entry with a bigger font size indicating higher probability of sampling that entry. Intuitively, sub-string matching removes low quality text and balancing favors longer text with long-tail entities to improve data quality. In Table 15, we show more examples of matched text that include ImageNet tail entries.\"}"}
{"id": "5BCFlnfE1g", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Substr. Bal. | IN Head | IN Tail | control_14ct | \u2717 | \u2717 | \u2717 | \u2717 |\\n|-------------|---------|---------|-------------|----|----|----|----|\\n| cudd2008    | \u2717       | \u2717       | \u2717           |    |    |    |    |\\n| product-img | \u2717       | \u2717       | \u2717           |    |    |    |    |\\n| Skirmisher-Main-440x412 | \u2717 | \u2717 | \u2717 |    |    |    |\\n| A4omote     | \u2717       | \u2717       | \u2717           |    |    |    |    |\\n| How-to-Find-the-Finest-Electric-Car-Companies | \u2717 | \u2717 | \u2717 |    |    |    |\\n| hanukkah-party-facebook-event-cover-template | \u2717 | \u2717 | \u2717 |    |    |    |\\n| johnny_cash_chili_dog (2) | \u2717 | \u2717 | \u2717 |    |    |    |\\n| 8533 GOLDEN RIDGE COURT | \u2717 | \u2717 | \u2717 |    |    |    |\\n\\n| How to build a stone patio on your own | \u2713 | \u2717 | \u2713 |    |    |\\n| battery plate | \u2713 | \u2717 | \u2713 |    |    |\\n| barn wedding | \u2713 | \u2717 | \u2713 |    |    |\\n\\n| Picture sand, machine, Concept, jeep, the concept, the front, Slim, Wrangler, Jeep | \u2713 | \u2717 | \u2713 |    |    |\\n| desk | \u2713 | \u2717 | \u2713 |    |    |\\n| Adult T-shirt | \u2713 | \u2717 | \u2713 |    |    |\\n| Imix m10 stage monitor | \u2713 | \u2717 | \u2713 |    |    |\\n| google wallet md3 | \u2713 | \u2717 | \u2713 |    |    |\\n| Distressed beach bar sign - Pearly's Oyster Bar | \u2713 | \u2717 | \u2713 |    |    |\\n| Why the Kilimanjaro Trek should be top of your bucket list | \u2713 | \u2717 | \u2713 |    |    |\\n| J70 desk model | \u2713 | \u2717 | \u2713 |    |    |\\n| Whitby castle | \u2713 | \u2717 | \u2713 |    |    |\\n\\n| Inside the castle Hama Hama Oyster Saloon | \u2713 | \u2717 | \u2713 |    |    |\\n| beach | \u2713 | \u2717 | \u2713 |    |    |\\n| Caramelized onions, sauteed red bell peppers and zucchini combined create a winning egg frit-tata breakfast dish | \u2713 | \u2713 | \u2713 |    |    |\\n\\n| Vector layered paper cut craft style music composition of saxophone guitar trumpet violin music instruments, notes on abstract color background. Jazz concert festival party poster banner card template | \u2713 | \u2713 | \u2713 |    |    |\\n| Nautilus hot tub | \u2713 | \u2713 | \u2713 |    |    |\\n| night binoculars for hunting | \u2713 | \u2713 | \u2713 |    |    |\\n\\n| 2017 cat eyes women's sunglasses for women vintage sun glasses round women sun glasses oculos oculos de sol feminino | \u2713 | \u2713 | \u2713 |    |    |\\n\\nTable 14: Examples categorized by whether passing sub-string matching and balancing: Words in violet color are in metadata and their font size indicates probability of being sampled, ranging from 13pt that has probability close to 0, to 22pt that has probability 1. ImageNet labels in head entries are cyan.\"}"}
{"id": "5BCFlnfE1g", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Substr. Bal. | IN Head | IN Tail |\\n|------------|---------|---------|\\n| Antique German | \u2717 | \u2713 |\\n| Sterling silver | \u2717 | \u2713 |\\n| 800 | \u2717 | \u2713 |\\n| Cup | \u2717 | \u2713 |\\n| Julep goblet | \u2717 | \u2713 |\\n| A journey to the East Sea by high-speed train | \u2717 | \u2713 |\\n| New KTX line makes Gangneung's wonders all the more accessible | \u2717 | \u2713 |\\n| Watercolor rainbow blush shower curtain and mat | \u2717 | \u2713 |\\n| Staff | \u2713 | \u2713 |\\n| Staffordshire Bull Terrier | \u2717 | \u2713 |\\n| KI Plantation Timbers fire truck on the night of January 3, 2020 | \u2717 | \u2713 |\\n| Photography: Tim Wilson | \u2717 | \u2713 |\\n| Water buffalo bath | \u2717 | \u2713 |\\n| Basset Hound pup with Lionhead x Lop rabbit | \u2717 | \u2713 |\\n| Single serving of peach trifle | \u2717 | \u2713 |\\n| A tarantula (hairy arachnid belonging to the Theraphosidae family of spiders) in a box, standing still | \u2717 | \u2713 |\\n| Modern Staffordshire Bull Terrier LED Night Light Animal Pet Dog Puppy 3D Optical illusion Lamp Home Decor Table Lamp Desk Light | \u2717 | \u2713 |\\n| Amazon, rocking chair, CANVA, camping chairs | \u2717 | \u2713 |\\n| Displaying kukri machete fitted inside scabard | \u2717 | \u2713 |\\n| Jacksonschameleon | \u2717 | \u2713 |\\n| Wall Mural - Insects pollinating blooming rapeseed crops in field | \u2717 | \u2713 |\\n| Scottish Terrier Phone Pocket | \u2717 | \u2713 |\\n| ImageNet tail labels are in cyan. |\\n\\nTable 15: Examples passing both sub-string matching and balancing with ImageNet tail classes:\\n\\nWords in violet color are in metadata and their font size indicates probability of being sampled, ranging from 13pt that has probability close to 0, to 22pt that has probability 1. ImageNet labels in tail entries are in cyan.\"}"}
{"id": "5BCFlnfE1g", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nToneva et al., 2018), aim to select a subset of data that yields similar performance to training on the entire dataset. Post-hoc data pruning with model filters has limited utility, if the model is used as a black-box filter that forbids to control biases or improve the filter quality.\\n\\nHandling Noisy Internet Data.\\n\\nAddressing noisy data from the Internet is a significant challenge, and existing approaches often heavily rely on human-designed filter systems. Classical methods involve dataset cleaning and outlier removal (Jiang et al., 2001; Yu et al., 2002) to discard samples that may introduce undesirable biases to models.\\n\\nReplicating CLIP's Training Data.\\n\\nRecent efforts, such as LAION (Schuhmann et al., 2021; 2022) and concurrent work DataComp (Gadre et al., 2023), attempt to replicate CLIP's training data. However, they adopt fundamentally different strategies for several reasons. First, the data used in these approaches are post-hoc, filtered, by vanilla CLIP as a teacher model. Second, the curation process in these methods relies on a labor-intensive pipeline of filters, making it challenging to comprehend the resulting data distribution from the raw Internet (refer to the unknown biases of using CLIP filter in (Schuhmann et al., 2022)). Thirdly, the goal is to match the quantity of CLIP's target data size rather than the data distribution itself, which may lead to an underestimation of the data pool size needed to obtain sufficient quality data. Consequently, the performance on the 400M scale is sub-optimal, with LAION400M only achieving 72.77% (Schuhmann et al., 2021) accuracy on ViT-L/14 on ImageNet, whereas vanilla CLIP obtains 75.5%.\\n\\nImportance of Understanding CLIP's Data Curation.\\n\\nThe observations made in these studies underscore the critical importance of understanding how OpenAI CLIP curates its data in the first place. A comprehensive understanding of the curation process can shed light on the factors that contribute to its success, allowing researchers to devise more effective and efficient algorithms for future vision-language pre-training endeavors.\\n\\n3 M ETA\\n\\nThe original paper (Radford et al., 2021) only provides limited details about how CLIP curates its data. Since important design choices for a direct reproduction are missing, we will clarify our choices in this section. Our goal is to uncover CLIP's data curation process, which involves preserving signal in the data while minimizing noise. In this section, we will explain the principles we have adopted to achieve this, which may differ from CLIP's as these are not known publicly.\\n\\nCLIP's WIT400M is curated with an information retrieval method, quoting (Radford et al., 2021):\\n\\n\\\"To address this, we constructed a new dataset of 400 million (image, text) pairs collected from a variety of publicly available sources on the Internet. To attempt to cover as broad a set of visual concepts as possible, we search for (image, text) pairs as part of the construction process whose text includes one of a set of 500,000 queries. We approximately class balance the results by including up to 20,000 (image, text) pairs per query.\\\"\\n\\nWe rigorously adhere to this description and provide detailed insights into the construction process of CLIP's metadata (in \u00a73.1), sub-string matching (in \u00a73.2), inverted indexing (in \u00a73.3), as well as query and balancing (in \u00a73.4).\"}"}
{"id": "5BCFlnfE1g", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The base query list is all words occurring at least 100 times in the English version of Wikipedia. This is augmented with bi-grams with high pointwise mutual information as well as the names of all Wikipedia articles above a certain search volume. Finally all WordNet synsets not already in the query list are added.\\n\\nThe metadata ('queries' or 'entries') consists of four components: (1) all synsets of WordNet, (2) uni-grams from the English version of Wikipedia occurring at least 100 times, (3) bi-grams with high pointwise mutual information, and (4) titles of Wikipedia articles above a certain search volume. We rebuild these components from WordNet and Wikipedia and summarize the statistics in Table 1. We estimate the thresholds for components (3) and (4) as in the 3rd column of Table 1, by first choosing a point-wise mutual information threshold of 30 that meets the budget of 100k entries for bi-grams and then fill the rest of the entries with Wikipedia titles.\\n\\n| Source                  | # of Entries | Desc. of Threshold | Threshold |\\n|-------------------------|--------------|--------------------|-----------|\\n| WordNet synsets         | 86,654       | N/A                | ALL (follow CLIP) |\\n| Wiki uni-gram           | 251,465      | Count              | 100 (follow CLIP) |\\n| Wiki bi-gram            | 100,646      | Pointwise Mutual Info. (PMI) | 30 (estimated) |\\n| Wiki titles             | 61,235       | View Frequency     | 70 (estimated) |\\n\\nTable 1: Composition of MetaCLIP Metadata.\\n\\n3.2 SUB-STRING MATCHING: text \\\\rightarrow entry\\n\\nAfter constructing the metadata, CLIP's curation aligns a pool of image-text pairs with metadata entries through sub-string matching. This process identifies texts that contain any of the metadata entries, effectively associating unstructured texts with structured metadata entries. The sub-string matching step retains only high-quality matching texts, automatically filtering out various types of noises that a typical filter system would consider on a case-by-case basis.\\n\\nSuch alignment is referred to as sub-string matching in Radford et al. (2021): \u201cWe also restrict this step in CLIP to text-only querying for sub-string matches while most webly supervised work uses standard image search engines...\u201d\\n\\nImage-Text Pair Pool\\n\\nWe start by estimating the pool size used by CLIP's curation. CLIP's data source is unknown to us (\\\"a variety of publicly available sources\\\" in Radford et al. (2021)). We adopt CommonCrawl (CC) as the source to build such a pool and re-apply sub-string matching to this source. We ended with a pool of 1.6B image-text pairs (5.6B counts of sub-string matches).\\n\\nNote that one text can have multiple matches of entries and we have 3.5 matches per text on average. As a result, sub-string matching builds the mapping txt \\\\rightarrow entry. This step has two outcomes: (1) low-quality text is dropped; (2) unstructured text now has a structured association with metadata. For all English text, \\\\sim 50% image-text pairs are kept in this stage. Similar to CiT (Xu et al., 2023), this approach looks for quality matches and automatically gets rid of some type of noise (such as date strings) that a typical filter system would remove consider case-by-case (e.g., regular expression on dates, ids etc.).\\n\\n3 Note that we cannot find Wikipedia's search volume for titles of Wikipedia (4). Instead, we use volumes of Pageviews on Wiki articles. We randomly selected 26 days' Pageviews from Apr. 2018 to Sep. 2022.\\n\\n4 https://commoncrawl.org\"}"}
{"id": "5BCFlnfE1g", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Following sub-string matching, CLIP builds an inverted index of the data pool. All texts associated with each metadata entry are aggregated into lists, creating a mapping from each entry to the corresponding texts, \\\\( \\\\text{entry} \\\\rightarrow \\\\text{text} \\\\). As an analysis, we count the number of matches for each entry and summarize that in Table 2. The counts exhibit a long-tailed distribution. Out of the 500k entries, 114k entries have no matches. This signifies the importance of knowing the training data distribution since it is very likely the training data does not have certain visual concepts. We observed that only 16k entries had counts higher than 20k, accounting for only 3.2% (16k/500k) of the entries, but their counts made up 94.5% (5.35B/5.6B) of the total counts of all entries.\\n\\n### Top Entries\\n\\nWe show the top entries of the matching in Table 3. Interestingly, many of these are stopwords, which don't carry specific meaning but can enhance the overall text quality (e.g., by generating grammatically correct sentences rather than just keyword lists). It's important to note that although sub-string matching aims to select only high-quality texts, there are instances where common entries may still include irrelevant texts. For instance, the entry \u201cphoto\u201d could match with the popular but unhelpful term \u201cuntitled photo\u201d. These noise-related issues can be addressed in the subsequent stage of processing.\\n\\n### Query and Balancing\\n\\nThe key secret behind OpenAI CLIP\u2019s curation is to balance the counts of matched entries. For each metadata entry, the associated list of texts (or image-text pairs) is sub-sampled, ensuring that the resulting data distribution is more balanced. This step aims to mitigate noise and diversify the distribution of data points, making the data more task-agnostic as foundation data for pre-training.\\n\\nThe magic number \\\\( t = 20k \\\\) is a threshold used to limit the number of texts/pairs for each entry. Entries with fewer than \\\\( t \\\\) pairs (tail entries) retain all associated pairs, while entries with more than \\\\( t \\\\) pairs (head entries) are sub-sampled to \\\\( t \\\\) pairs. The selection is based on the density of information in texts; texts with more matched entries have a higher chance of being curated (recall that the average is 3.5 matches per text).\\n\\nTo study the effect of the magic number \\\\( t = 20k \\\\), we plot the cumulative sum of counts for entries sorted by counts from tail to head in Fig. 2. Interestingly, the value of \\\\( t = 20k \\\\) seemingly represents the transition from tail to head entries, when the head entries start exhibiting an exponential growth rate. By applying a max count of \\\\( t \\\\), the growth rate of total counts (i.e., the scale of resulting data points) is reduced to linear. This significantly flattens (and balances) the training data distribution.\\n\\nWe further study the optimality of \\\\( t = 20k \\\\) for the 400M data scale in our experiments. In summary, balancing yields three interesting outcomes:\"}"}
{"id": "5BCFlnfE1g", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Cumulative sum of counts on entries from tail to head on a data pool with 1.6B image-text pairs (5.6B match counts). (1) raw/unbalanced cumulative counts, $t=\\\\infty$; (2) balanced cumulative counts after applying $t=20k$. The limit $t$ defines the transition of tail/head entries.\\n\\n(i) It reduces dominance and noise from head entries, like common web terms. E.g., out of 400M pairs, only 20k texts containing \\\"photo\\\" are kept (while there are 54M \\\"photo\\\" instances in the pool).\\n\\n(ii) It diversifies the data distribution and balances tail/head entries, leading to a more task-agnostic foundation.\\n\\n(iii) Sampling for each entry ensures that data points with more matched entries or denser information are prioritized for curation.\\n\\nDiscussion.\\n\\nCLIP employs a pure NLP-based approach, requiring no access to ML models and minimizing explicit/implicit priors from humans. The metadata plays a central role in mitigating noise and preserving signal in the data distribution. The balancing step effectively flattens the data distribution, diversifying the data and making it more suitable as foundation data for pre-training tasks. We analyze the effects of balancing in Appendix A.3.\\n\\n3.5 A SIMPLE ALGORITHM FOR CURATION\\n\\nThis section presents an algorithm that formalizes the curation process described earlier. The algorithm aims to improve scalability and reduce space complexity for operations across data points, such as inverted indexing and sub-sampling. Instead of building inverted indexes, the algorithm only maintains total counts for each entry.\\n\\nWe assume that CLIP curation constructs an inverted index that maps entries to documents (image-text pairs) to enable efficient search for each entry (\u201cwe search for (image-text) pairs\u201d in Radford et al. (2021)). In contrast, our algorithm approaches the balancing process through independent sampling. This avoids the need to build an inverted index that could potentially store hundreds of millions of concrete pairs for popular entries, thereby improving efficiency and scalability.\\n\\nOur algorithm takes three inputs: metadata $M$, a data pool $D$, and a hyper-parameter $t$. It aims to find a subset $D^*$ with a balanced distribution over $M$, denoted as $D^* \\\\leftarrow f(D; M, t)$. The algorithm consists of two parts, each corresponding to a specific stage of the curation process.\\n\\nWe provide the Python pseudo-code in Algorithm 1.\"}"}
{"id": "5BCFlnfE1g", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Amro Abbas, Kushal Tirumala, D\u00e1niel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023.\\n\\nOlivier Bachem, Mario Lucic, and Andreas Krause. Coresets for nonparametric estimation\u2014the case of dp-means. In International Conference on Machine Learning, pp. 209\u2013217. PMLR, 2015.\\n\\nMehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gorndon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. arXiv preprint arXiv:2212.07143, 2022.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.\\n\\nDan Feldman, Matthew Faulkner, and Andreas Krause. Scalable training of mixture models via coresets. Advances in neural information processing systems, 24, 2011.\\n\\nSamir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets, 2023.\\n\\nSariel Har-Peled and Soham Mazumdar. On coresets for k-means and k-median clustering. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pp. 291\u2013300, 2004.\\n\\nMon-Fong Jiang, Shian-Shyong Tseng, and Chih-Ming Su. Two-phase clustering process for outliers detection. Pattern recognition letters, 22(6-7):691\u2013700, 2001.\\n\\nBaharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of machine learning models. In International Conference on Machine Learning, pp. 6950\u20136960. PMLR, 2020.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR, 2021.\\n\\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\\n\\nChristoph Schuhmann, Romain Beaumont, Cade W Gordon, Ross Wightman, Theo Coombes, Aarush Katta, Clayton Mullis, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. 2022.\\n\\nBen Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:19523\u201319536, 2022.\\n\\nMariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018.\\n\\nHu Xu, Saining Xie, Po-Yao Huang, Licheng Yu, Russell Howes, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Cit: Curation in training for effective vision-language data. arXiv preprint arXiv:2301.02241, 2023.\"}"}
{"id": "5BCFlnfE1g", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "5BCFlnfE1g", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ABSTRACT\\n\\nContrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP's data by filtering with its model parameters. In this work, we intend to reveal CLIP's data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outperforms CLIP's data on multiple standard benchmarks. In zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy, surpassing CLIP's 68.3% on ViT-B models. Scaling to 1B data, while maintaining the same training budget, attains 72.4%. Our observations hold across various model sizes, exemplified by ViT-bigG producing 82.1%. Curation code and training data distribution over metadata is available at https://github.com/facebookresearch/MetaCLIP.\\n\\nINTRODUCTION\\n\\nDeep learning has revolutionized the field of artificial intelligence, and pre-trained models have played a pivotal role in democratizing access to cutting-edge AI capabilities. However, the training data used to create these models is often concealed from the public eye, shrouded in secrecy. The increasing availability of pre-trained models for public use contrasts sharply with the lack of transparency regarding their training data. Further, proprietary concerns, such as copyright issues, often limit access to the original data sources. Consequently, the need to explore novel approaches for curating high-quality training data that can be shared openly arises.\\n\\nIn the vision-language domain, the dominant model and learning approach is Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021), a simple technique to learn from image-text pairs. We believe that the secret to the dominance of CLIP models is attributed to its high-quality WIT400M dataset which is curated from the web. Despite its popularity, the specifics of CLIP's curation process have remained a mystery, captivating the research community for years. Follow-up works (Schuhmann et al., 2022; 2021) have attempted to replicate CLIP's data, but with a notable difference in their curation method. While CLIP generates data based on its unknown data source and curation methodology, these approaches remove noise by applying the CLIP model as a hard blackbox filter which in turn is a form of distilling WIT400M information captured in CLIP.\\n\\nThe advantages of CLIP's curation are apparent. First, it starts from scratch, avoiding the introduction of biases through filters. Second, CLIP's curation process balances the data distribution over metadata, maximizing signal preservation while mitigating, rather than removing, noise in the data.\\n\\n1 For example, a filter on digits can remove noise from date or id strings but remove signal for tasks that involve OCR (e.g., MNIST), or a filter removing text with less than 5 characters can remove signal \\\"dog\\\".\\n\\n1\"}"}
{"id": "5BCFlnfE1g", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this paper, we attempt to reveal CLIP's method around training data curation. We present an empirical study on data curation, with frozen model architecture and training schedule. We focus solely on the impact of training data, excluding other factors that could confound the results. We make several observations for good data quality and present a simple algorithm to make CLIP's curation more transparent.\\n\\nConsequently, we shed light on both the curation process and the resulting training data distribution. Our algorithm enables easy adaptation to different data pools, allowing parties to fully own their data pipeline without relying on blackbox filters from external providers.\\n\\nOur algorithm takes a raw data pool $D$ and metadata $M$ (derived from CLIP's queries or visual concepts) and yields a balanced subset $D^*$ over $M$:\\n\\n$$D^* \\\\leftarrow f(D; M)$$\\n\\nOur approach, named Metadata-Curated Language-Image Pre-training (MetaCLIP), marks a significant step towards making the curation process more transparent and accessible.\\n\\nMetaCLIP applied to CommonCrawl (CC) with 400M data points outperforms CLIP on multiple standard benchmarks. In terms of zero-shot ImageNet classification, using ViT (Dosovitskiy et al., 2020) models of various sizes. Our MetaCLIP achieves 70.8% vs CLIP's 68.3% on ViT-B and 76.2% vs 75.5% on ViT-L. Scaling to 2.5B data, with the same training budget and similar distribution boosts this to unprecedented accuracy of 79.2% for ViT-L, 80.5% for ViT-H and 82.1% for ViT-bigG in the vanilla training setting (not using any external data, models, or longer training).\\n\\nIn Fig.1, we show the impact of metadata curation on ImageNet validation plotted over training steps. First, we are training on Raw English data from the web (400M image-text pairs, 57.4% accuracy), after applying Language IDentification (LID) to the random Raw set ($\\\\sim 1.1B$ pairs, 54.1%).\\n\\nUsing metadata to curate the training set (MetaCLIP 400M w/o bal, 60.8%) performs significantly better than these baselines, and using balancing significantly increases accuracy further (MetaCLIP, 65.5%), outperforming similar datasets, WIT400M from CLIP, 63.4% and LAION 400M, 60.0% (Schuhmann et al., 2021).\\n\\n2 RELATED WORK\\n\\nThe training data of CLIP differs significantly from a traditional supervised dataset (Gadre et al., 2023) in various aspects. Firstly, it involves large-scale training with mixed-quality image-text pairs rather than categorized images with human annotated labels, as commonly seen in classification datasets. Secondly, CLIP's pre-training is the initial stage of training, assuming no access to previously trained models.\\n\\nData Pruning on Established Datasets. Current research on data algorithms primarily revolves around data pruning techniques applied to well-established datasets using pre-trained models (Sorscher et al., 2022; Abbas et al., 2023). These approaches, such as coreset selection techniques (Har-Peled & Mazumdar, 2004; Feldman et al., 2011; Bachem et al., 2015; Mirzasoleiman et al., 2020), form the basis of our approach.\"}"}
{"id": "5BCFlnfE1g", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1: Pseudo-code of Curation Algorithm in Python style (see Sec. A.10 for samples).\\n\\n# D: raw image-text pairs;\\n# M: metadata;\\n# t: max matches per entry in metadata;\\n# D_star: curated image-text pairs;\\n\\nD_star = []\\n\\n# Part 1: sub-string matching: store entry indexes in text.matched_entry_ids and output counts per entry in entry_count.\\nentry_count = substr_matching(D, M)\\n\\n# Part 2: balancing via independent sampling\\nentry_prob = t / entry_count\\nfor image, text in D:\\n    for entry_id in text.matched_entry_ids:\\n        if random.random() < entry_prob[entry_id]:\\n            D_star.append((image, text))\\n            break\\n\\nPart 1: Entry Counts from Sub-string Matching.\\nThis corresponds to Sec. 3.2. The `substr_matching` function outputs the total counts of matches per entry, `entry_count`, represented as a NumPy array indexed by `entry_id`. Each text is associated with `matched_entry_ids` that contains a list of matched entries.\\n\\nPart 2: Balancing via Independent Sampling.\\nThis part corresponds to Sec. 3.3 and Sec. 3.4 and focuses on balancing counts on entries. Instead of building an expensive inverted index with associated lists of texts for each entry, we sample each data point independently.\\n\\nWe first compute the probability of sampling each entry, `entry_prob`, where tail entries (entry_count < t) have a probability equal to 1, and head entries have a probability less than 1. We iterate through all image-text pairs and sample/curate each pair. When an image-text pair has a matched entry sampled/selected, we include that pair in D_star.\\n\\nThis procedure is equivalent to CLIP's curation, because if one image-text pair has one or more matched entries, the chance of that pair being selected is determined by the probability of sampling for each individual entry: t / entry_count[entry_id]. As long as one entry selects that pair, it will be kept in D_star. Our independent sampling approach allows us to scale balancing for each data point independently and reduces the global operation to counting the total matches for each entry.\\n\\nWe demonstrate case studies in experiments on (1) scaling curation in a data pipeline and (2) online balancing in data loader.\\n\\n4 EXPERIMENTS\\n\\nData Pools.\\nWe collect two pools of data:\\n\\nPool 1 contains 1.6 billion image-text pairs with a total of 5.6 billion counts of matches. This pool was used to estimate a target of 400M image-text pairs, collected from 15 snapshots of Common-Crawl (CC) from January 2021 to January 2023.\\n\\nPool 2 aims to scale curation in our data pipeline. We parsed all 90 CC snapshots from 2013 to April 2023, using our algorithm (see \u00a7A.2 for details on the curation pipeline) to curate from a pool of 10.7B matched image-text pairs that are originally from a large set of URL-text pairs, which have undergone de-duplication, English Language Identification (LID) and sub-string matching.\\n\\nHowever, we only perform (expensive) image downloading, storing, and transferring for data points that are distribution-calibrated and selected by our algorithm.\\n\\nFor balancing we consider 2 scenarios on this data: (i) \\\\( t = 170000 \\\\), which results in 2.5B image-text pairs. This \\\\( t = 170000 \\\\) configuration has tail counts amounting to 6% of the total counts, the same tail/head ratio that the 400M Pool 1 data has, produced by applying \\\\( t = 20000 \\\\) on the 1.6B Pool 1 data. (ii) The \\\\( t = 20000 \\\\) threshold applied to Pool 2 which results in 1B image-text pairs and compared to the 400M set from Pool 1 only increases tail metadata matches (head counts are capped at 20k).\"}"}
{"id": "5BCFlnfE1g", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: MetaCLIP-400M vs. CLIP (WIT400M data) and OpenCLIP (LAION-400M data (Schuhmann et al., 2021)). We use 3 different model scales (ViT-B/32 and -B/16 and -L/14) and an identical training setup as CLIP.\\n\\n4.1 Results\\n\\nZero-shot Image Classification. We follow the standard evaluation benchmark and made sure all prompts and class names were the same as those used by CLIP Radford et al. (2021). We also re-evaluated OpenAI/OpenCLIP\u2019s checkpoints to avoid differences caused by benchmark data copies. The results are shown in Tab 4. The standard deviation of training multiple seeds is relatively small with \u00b10.1% for ImageNet on ViT-B/32.\\n\\nIn Table 4, we observe that MetaCLIP outperforms OpenAI CLIP on ImageNet and average accuracy across 26 tasks, for 3 model scales. With 400 million training data points on ViT-B/32, MetaCLIP outperforms CLIP by +2.1% on ImageNet and by +1.6% on average. On ViT-B/16, MetaCLIP outperforms CLIP by +2.5% on ImageNet and by +1.5% on average. On ViT-L/14, MetaCLIP outperforms CLIP by +0.7% on ImageNet and by +1.4% on average across the 26 tasks.\\n\\nWe next turn to Pool 2 which is a larger set of image-text pairs and study the effect of scaling data. In Table 5, we scale data to 1B and 2.5B and observe a large gain over 400M, with similar performance for both 1B and 2.5B scales. Note that the number of training iterations (and therefore compute) is the same for all rows. The main difference between 1B and 2.5B is the threshold $t$, where 1B is a more balanced set by adding more data points (compared to the 400M set) to tail entries (up to $t = 20k$), instead the 2.5B set adds (up to $t = 170k$) data points to all, head and tail, entries. The extra data in the tail entries (1B set), seems to benefit downstream accuracy for tasks on specific data such as CUB fine-grained bird classification, Flowers, KITTI, PCAM, while the larger 2.5B data that has more head entries increases broadly over more datasets, but each at a smaller amount. The overall average accuracies are similar for 1B and 2.5B (e.g., 70.2% vs. 69.8% for ViT-L model).\"}"}
{"id": "5BCFlnfE1g", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Cumulative sum of counts on entries from tail to head on a data Pool 2. We again show (1) raw/unbalanced cumulative counts), $t = \\\\infty$; (2) balanced cumulative counts after applying $t = 20k$ and $t = 170k$. $t$ defines maximum number of counts per entry and the transition of tail/head entries.\\n\\nWe show the Pool 1 configuration from Fig. 2 as dashed lines for reference.\\n\\nOn ImageNet, the 2.5B training data achieves 67.6% on ViT-B/32 that breaks the previous believed saturated B/32 models (Cherti et al., 2022), 79.2% on ViT-L/14, 80.5% on ViT-H/14 and 82.1% on ViT-bigG/14.\\n\\nWe plot the cumulative sum of counts for entries sorted by counts from tail to head in Fig. 3 for all these cases, similar to Fig. 2 for Pool 1 (and the Pool 1 configuration as dashed lines). The plot shows that the 2.5B data is still relatively long-tail, while the 1B data is more balanced, explaining it\u2019s better performance on specific data such as bird and flower types observed above.\\n\\n4.2 ABLATION STUDY\\n\\nWe show ablations for MetaCLIP for the 400M scale and ViT-B/32 in Table 6. We first ablate different balancing thresholds $t$. We observe that the choice of $t = 20k$ by CLIP yields the best performance for ImageNet and averaged accuracy and $t = 15k$ or $t = 35k$ are slightly worse.\\n\\nTo understand the key effect of balancing, we use the whole matched pool (1.6B image-text pairs) to train CLIP. Surprisingly, training on 4\u00d7 more data (on head entries) significantly hurts the accuracy on ImageNet (61.9 vs 65.5) and averaged accuracy across 26 tasks (56.6 vs 58.2).\\n\\nBalancing can also be applied online in the data loader with head entries down-sampled leading to slightly better performance (58.5 vs 58.2); see appendix for details. This is useful if head data has already been collected and one wants to train on a different distribution. The better accuracy for online balancing is explained by the larger diversity in head data.\\n\\n| Average | ImageNet | Food-101 | CIFAR10 | CIFAR100 | CUB | SUN397 | Cars | Aircraft | DTD | Pets | Caltech-101 | Flowers | MNIST | FER-2013 | STL-10 | EuroSAT | RESISC45 | GTSRB | KITTI | Country211 | PCAM | UCF101 | Kinetics700 | CLEVR | HatefulMemes | SST2 |\\n|---------|----------|----------|---------|----------|-----|-------|------|----------|-----|-------|-------------|---------|-------|----------|--------|--------|----------|--------|---------|---------|--------|--------|----------|------|------------|-------|\\n| MetaCLIP | $t = 20k$ | 58.2 | 65.5 | 80.6 | 91.3 | 70.2 | 63.4 | 63.0 | 70.7 | 26.8 | 52.8 | 88.7 | 91.9 | 68.5 | 41.5 | 35.9 | 95.4 | 52.6 | 64.2 | 35.8 | 30.7 | 17.2 | 55.5 | 66.1 | 45.4 |\\n|         | $t = 15k$ | 57.5 | 65.5 | 79.9 | 90.4 | 68.8 | 65.7 | 64.6 | 69.4 | 25.6 | 52.1 | 88.8 | 91.9 | 69.5 | 35.8 | 39.7 | 96.5 | 54.0 | 64.1 | 34.8 | 30.6 | 16.1 | 52.3 | 67.1 | 45.4 |\\n|         | $t = 35k$ | 57.8 | 65.4 | 79.3 | 91.2 | 69.0 | 63.0 | 65.0 | 72.0 | 28.5 | 52.7 | 88.5 | 91.8 | 68.0 | 42.0 | 23.0 | 96.2 | 50.0 | 63.8 | 40.2 | 32.4 | 17.7 | 56.1 | 64.2 | 44.8 |\\n|         | - unbalanced (1.6B) | 56.6 | 61.9 | 76.9 | 90.0 | 67.6 | 50.8 | 65.8 | 77.0 | 19.9 | 51.0 | 83.1 | 91.5 | 64.5 | 58.2 | 37.0 | 95.1 | 55.2 | 58.2 | 41.4 | 32.2 | 15.1 | 51.0 | 59.2 | 42.6 |\\n|         | online balancing | 58.5 | 66.1 | 80.8 | 89.9 | 68.8 | 65.7 | 65.4 | 71.6 | 27.9 | 55.1 | 88.2 | 92.7 | 68.8 | 38.3 | 42.1 | 96.5 | 54.5 | 64.8 | 36.2 | 29.1 | 17.6 | 58.8 | 66.0 | 45.8 |\\n\\nTable 6: Ablation studies on balancing in MetaCLIP. Default: $t = 20k$, 400M. Model: ViT-B/32.\\n\\n5 CONCLUSION\\n\\nIn this paper, we attempt to reveal CLIP's data curation. Our MetaCLIP builds upon metadata for curation and balancing of raw data sourced from the web. Curating with metadata and balancing are essential for good data quality, significantly outperforming the use of raw data. Our experiments show that MetaCLIP performs well for different scales sourced from CommonCrawl data and outperforms CLIP's proprietary data source, without reliance on any external model. We make our pipeline for generating the data publicly available.\"}"}
{"id": "5BCFlnfE1g", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGMENTS\\n\\nWe thank Zeyuan Allen-Zhu, and Chunting Zhou for the insightful discussion and Brighid Meredith for suggestions on scaling the pipeline.\\n\\nAPPENDIX\\n\\nA.1 ADDITIONAL RESULTS\\n\\nFigure 4: ViT-B/32 on our Pool 1, Pool 2 and DataComp's unfiltered 12.8B pool. We show ImageNet zero-shot classification with a fixed 12.8B seen pair training budget. MetaCLIP's curation is effective for all pools. However, with the same curation method, the unfiltered DataComp-12.8B pool lacks quality (we suspect it is caused by implicit filters in the parser of DataComp).\\n\\nCuration from DataComp-12.8B.\\n\\nThe concurrent work Gadre et al. (2023) released a collection of 12.8B image-text pairs from CommonCrawl from 2014-2022. We further investigate whether we can apply the algorithm on its 12.8 unfiltered pool. Although the unfiltered pool seemingly offers an opportunity to apply our algorithm on a publicly available source, our initial studies show that, implicit biases may still be present in this pool. For example, we notice that all image URLs are collected as a string starting with http. This excludes relative URLs that could be frequently used by quality websites (with potentially good image-text pairs). We curate from DataComp's 12.8B unfiltered pool with t = 60k, which has 6% of tail counts that is the same as t = 20k for 400M from our 1.6B pool. When using 1B image-text pairs curated from DataComp's pool we notice a quality drop during training, compared to data curated from our pools, see Fig. 4. Our smaller 400M set is slightly better than using DataComp-1B and our larger sets (1B, 2.5B) are significantly better. In Table 7, we show our 400M data vs our curated DataComp-1B data at various model scales, where the same observation holds, suggesting our raw data pool is more effective.\\n\\n| Average | ImageNet | Food-101 | CIFAR10 | CIFAR100 | CUB | SUN397 | Cars | Aircraft | DTD | Pets | Caltech-101 | Flowers | MNIST | FER-2013 | STL-10 | EuroSAT | RESISC45 | GTSRB | KITTI | Country211 | PCAM | UCF101 | Kinetics700 | CLEVR | HatefulMemes | SST2 |\\n|---------|----------|----------|---------|----------|-----|--------|------|----------|-----|------|-------------|---------|-------|----------|--------|---------|---------|--------|---------|--------|--------|--------|----------|--------|\\n| ViT-B/32 | MetaCLIP (400M) | 58.2 | 65.5 | 80.6 | 91.3 | 70.2 | 63.4 | 63.0 | 70.7 | 26.8 | 52.8 | 88.7 | 91.9 | 68.5 | 41.5 | 35.9 | 95.4 | 52.6 | 64.2 | 35.8 | 30.7 | 17.2 | 56.4 | 53.4 |\\n| MetaCLIP (1B,DataComp) | 57.5 | 65.4 | 81.5 | 88.7 | 68.5 | 59.4 | 64.7 | 76.6 | 18.1 | 57.5 | 89.9 | 92.4 | 67.3 | 40.7 | 38.2 | 96.5 | 41.2 | 62.3 | 43.4 | 35.1 | 17.6 | 53.5 | 61.5 | 44.8 | 19.2 |\\n| ViT-B/16 | MetaCLIP (400M) | 61.1 | 70.8 | 86.8 | 90.1 | 66.5 | 70.8 | 66.6 | 74.1 | 27.9 | 55.9 | 90.4 | 93.8 | 72.3 | 47.8 | 44.6 | 97.2 | 55.4 | 68.8 | 43.8 | 33.4 | 22.6 | 52.9 | 68.0 | 49.5 | 22.8 | 54.8 | 60.6 |\\n| MetaCLIP (1B,DataComp) | 61.2 | 70.7 | 88.1 | 91.3 | 71.6 | 64.5 | 67.6 | 81.2 | 21.6 | 62.3 | 92.7 | 93.2 | 70.7 | 55.6 | 39.6 | 97.7 | 52.9 | 66.3 | 45.7 | 36.0 | 22.3 | 50.1 | 68.1 | 49.2 | 17.1 | 53.5 | 61.5 | 44.8 | 19.2 |\\n| ViT-L/14 | MetaCLIP (400M) | 67.1 | 76.2 | 90.7 | 95.5 | 77.4 | 75.9 | 70.5 | 84.7 | 40.4 | 62.0 | 93.7 | 94.4 | 76.4 | 61.7 | 46.5 | 99.3 | 59.7 | 71.9 | 47.5 | 29.9 | 30.9 | 70.1 | 75.5 | 57.1 | 35.1 | 56.6 | 65.6 |\\n| MetaCLIP (1B,DataComp) | 66.3 | 76.7 | 92.6 | 95.6 | 78.9 | 72.1 | 71.6 | 87.1 | 31.6 | 67.5 | 93.4 | 95.3 | 76.0 | 65.1 | 42.4 | 99.1 | 61.7 | 69.8 | 45.9 | 36.8 | 31.8 | 51.0 | 76.6 | 57.5 | 29.3 | 57.1 | 60.8 |\\n\\nTable 7: MetaCLIP curating our 400M data vs curating 1B data from DataComp-12.8B: The pool of DataComp leads to a quality drop with performance closer to our 400M set.\\n\\nDataComp Benchmark\\n\\nWe also evaluate MetaCLIP on the benchmark used by (Gadre et al., 2023) that contains 38 tasks including variants of ImageNet, retrieval, VTAB, etc. For simplicity, we average the scores over each category.\\n\\n| DataComp Benchmark | MetaCLIP (400M) | 58.2 | 65.5 | 80.6 | 91.3 | 70.2 | 63.4 | 63.0 | 70.7 | 26.8 | 52.8 | 88.7 | 91.9 | 68.5 | 41.5 | 35.9 | 95.4 | 52.6 | 64.2 | 35.8 | 30.7 | 17.2 | 56.4 | 53.4 | 22.6 | 52.9 | 68.0 | 49.5 | 22.8 | 54.8 | 60.6 |\\n| MetaCLIP (1B,DataComp) | 57.5 | 65.4 | 81.5 | 88.7 | 68.5 | 59.4 | 64.7 | 76.6 | 18.1 | 57.5 | 89.9 | 92.4 | 67.3 | 40.7 | 38.2 | 96.5 | 41.2 | 62.3 | 43.4 | 35.1 | 17.6 | 53.5 | 61.5 | 44.8 | 19.2 | 56.4 | 53.4 | 22.6 | 52.9 | 68.0 | 49.5 | 22.8 | 54.8 | 60.6 |\\n\\nTable 7: MetaCLIP curating our 400M data vs curating 1B data from DataComp-12.8B: The pool of DataComp leads to a quality drop with performance closer to our 400M set.\"}"}
{"id": "5BCFlnfE1g", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 8: Zero-shot classification and retrieval on tasks from (Gadre et al., 2023).\\n\\n| Model          | CLIP (400M) | OpenCLIP (407M) | MetaCLIP (400M) | MetaCLIP (1B) | MetaCLIP (2.5B) |\\n|----------------|-------------|------------------|-----------------|---------------|-----------------|\\n| ViT-B/32       | 51.5        | 63.4             | 48.2            | 53.0          | 54.2            |\\n| OpenCLIP       | 52.7        | 62.9             | 48.5            | 53.0          | 50.7            |\\n| MetaCLIP 400M | 53.5        | 65.5             | 50.4            | 54.1          | 50.6            |\\n| MetaCLIP 1B   | 54.2        | 67.3             | 51.9            | 53.6          | 51.1            |\\n| MetaCLIP 2.5B | 55.4        | 67.6             | 52.3            | 55.3          | 52.6            |\\n| ViT-B/16       | 55.5        | 68.3             | 54.1            | 54.4          | 50.2            |\\n| OpenCLIP       | 56.1        | 67.0             | 52.6            | 54.9          | 53.9            |\\n| MetaCLIP 400M | 57.5        | 70.8             | 55.5            | 56.7          | 53.9            |\\n| MetaCLIP 1B   | 58.4        | 72.4             | 57.8            | 56.3          | 54.3            |\\n| MetaCLIP 2.5B | 60.0        | 72.1             | 57.7            | 59.0          | 54.0            |\\n| ViT-L/14       | 61.4        | 75.5             | 61.6            | 59.5          | 53.6            |\\n| OpenCLIP       | 59.7        | 72.7             | 57.3            | 58.6          | 55.9            |\\n| MetaCLIP 400M | 62.2        | 76.2             | 61.3            | 59.8          | 57.3            |\\n| MetaCLIP 1B   | 65.0        | 79.0             | 64.5            | 62.5          | 58.3            |\\n| MetaCLIP 2.5B | 65.6        | 79.2             | 64.6            | 64.1          | 60.1            |\\n| ViT-bigG/14    |             |                  |                 |               |                 |\\n\\nNote that prompts and class names used by (Gadre et al., 2023) could be different from prompts and classnames used by OpenAI CLIP.\\n\\nFrom Table 8, we can see that MetaCLIP outperforms CLIP and OpenCLIP across various model sizes. First, for the same data scale (400M), MetaCLIP outperforms OpenCLIP, which is better than CLIP on this benchmark, by +1.4% for ViT-B/16 and +2.5% for ViT-L/14, when comparing average accuracy across the 38 tasks. Second, for increasing our MetaCLIP data size to 1B we see a significant gain, especially for the larger model, from 62.2% to 65.0% average accuracy. Using our larger dataset with 2.5B and more head entries leads to a further gain to 65.5%.\\n\\nWe further detail the breakdown of each dataset in Table 9 (please view in landscape orientation).\"}"}
{"id": "5BCFlnfE1g", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | CLIP (400M) | OpenCLIP (407M) | MetaCLIP (400M) | MetaCLIP (1B) | MetaCLIP (2.5B) |\\n|---------------|-------------|-----------------|-----------------|--------------|-----------------|\\n| ViT-B/32      | 63.3 42.3 56.0 | 31.5 47.8 69.4  | 87.6 64.2 81.8  | 23.3 23.4 44.3 50.5 27.4 66.6 87.0 53.6 62.5 14.9 51.6 68.8 40.3 34.9 | 89.8 17.2 19.5 | 84.0 32.5 48.3 44.3 76.4 62.2 | 58.6 59.6 97.1 6.7 13.3 53.9 82.2 |\\n| ViT-B/16      | 68.3 48.3 61.9  | 49.9 42.3 77.7  | 89.0 66.9 21.2 22.3 44.9 56.0 26.3 69.1 88.9 58.2 64.4 40.2 60.2 72.2 42.8 35.7 | 90.8 22.8 24.3 88.7 43.3 51.3 55.3 78.3 50.7 60.5 64.7 98.3 11.1 16.7 58.8 86.1 |\\n| ViT-L/14      | 75.6 59.6 69.8 70.7 32.3 87.9 92.5 75.8 19.5 20.2 55.3 62.6 21.8 79.2 93.2 63.3 67.6 52.8 69.6 75.1 46.4 39.2 95.6 31.9 31.7 93.1 50.6 76.3 68.9 78.3 52.0 68.9 77.9 | 92.8 59.6 65.4 46.5 41.9 84.7 92.6 77.4 24.3 24.5 60.5 62.3 20.1 73.1 91.7 67.4 72.6 49.5 45.5 78.9 51.3 37.4 94.6 23.0 24.9 90.1 49.9 76.1 59.7 75.6 49.7 56.0 99.4 98.1 12.3 15.2 63.0 88.4 13.2 15.8 63.3 87.9 64.0 88.7 69.4 91.1 91.4 60.5 65.1 49.2 41.9 82.7 93.5 77.9 19.3 24.4 58.2 47.6 24.6 74.8 92.1 65.1 66.8 27.8 60.1 76.4 68.8 39.1 67.4 68.8 39.1 | 67.4 68.8 39.1 67.4 68.8 39.1 | 69.8 69.8 39.1 67.4 68.8 39.1 | 69.8 69.8 39.1 67.4 68.8 39.1 | 69.8 69.8 39.1 67.4 68.8 39.1 | 69.8 69.8 39.1 67.4 68.8 39.1 |\\n| ViT-H/14      | 75.6 59.6 69.8 70.7 32.3 87.9 92.5 75.8 19.5 20.2 55.3 62.6 21.8 79.2 93.2 63.3 67.6 52.8 69.6 75.1 46.4 39.2 95.6 31.9 31.7 93.1 50.6 76.3 68.9 78.3 52.0 68.9 77.9 | 92.8 59.6 65.4 46.5 41.9 84.7 92.6 77.4 24.3 24.5 60.5 62.3 20.1 73.1 91.7 67.4 72.6 49.5 45.5 78.9 51.3 37.4 94.6 23.0 24.9 90.1 49.9 76.1 59.7 75.6 49.7 56.0 99.4 98.1 12.3 15.2 63.0 88.4 13.2 15.8 63.3 87.9 64.0 88.7 69.4 91.1 91.4 60.5 65.1 49.2 41.9 82.7 93.5 77.9 19.3 24.4 58.2 47.6 24.6 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8 27.8 60.1 74.8 92.1 65.1 66.8"}
{"id": "5BCFlnfE1g", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Curation in Data Pipeline.\\n\\nOur data collection pipeline contains 5 major components: HTML parsing/Language-Identification, URLs & Text deduplication, Image Download, NSFW image filter & deduplication and packaging, which are described next.\\n\\nOur HTML Parser is applied to all HTML files of CommonCrawl by keeping all `<img>` tags with both relative and absolute URLs and one or multiple text keys.\\n\\nLanguage-Identification (LID): we use an internal language identification system that can detect 191 languages with different dialects and keep all texts that are tagged as English (or its dialects).\\n\\nURLs / Text deduplication: We use 24-bit sha224 hashing to encode an URL and into tables to deduplicate columns with the same hash, avoiding downloading the same image URL multiple times. URLs with illegal domains are removed; if there are multiple texts associated with the same image, these are further deduplicated; texts with NSFW keywords are removed;\\n\\nNSFW filter: We use an internal system that can classify inappropriate content in images into 96 types of dangerous content and discard such image/text pairs. Images are further deduplicated by 64-bit PCA hash, derived from a similarity search model's feature embeddings with PCA reduction to 64 dimensions and sign quantization.\\n\\nOur curation algorithm does not require access to images, making it suitable for integration into a pipeline to reduce the scale of data points after parsing and before image downloading. We designed the algorithm to be modular, allowing different parts to be placed at different stages in the pipeline, as shown in Figure 5.\\n\\nSpecifically, sub-string matching can be placed immediately after HTML parsing to reduce data points for English-only pairs (e.g. by $\\\\sim 50\\\\%$). Balancing can be applied earlier, before image downloading, to further reduce data points by $\\\\sim 77\\\\%$. This approach led to a total reduction of $\\\\sim 90\\\\%$, allowing for curation of the entire CommonCrawl data without the need to store and transfer all data points, which allows us to curate the whole CommonCrawl since 2013 with $300\\\\text{B}+\\\\text{scale URL-text pairs without spending the storage/transfer on all } \\\\sim 10^8\\\\text{ data points, where the rate of keeping a data point with MetaCLIP curation is } \\\\sim 0.1\\\\text{ (}0.5\\\\times 0.23\\\\text{).}$\\n\\nCuration in Data Loading\\n\\nWe applied the balancing/sampling part of the algorithm to the data loader to adjust data distribution on-the-fly. Although data points for tail entries are always sampled in Algorithm 1, the diverse pairs from the head entries are sub-sampled from a larger pool while maintaining a similar distribution as offline curation. This diversification of pairs that match the head entries improved the performance, as shown in Table 6.\\n\\nHuman Study on the Effects of Curation\\n\\nIn this section, we study the impact of MetaCLIP curation on data distribution by using human evaluation. We approach this exploration from three distinct angles: noise reduction, alignment of visual content, and task-agnostic attributes. In the pursuit of comprehending the first two aspects, we undertake a human study aimed at comprehending the data quality implications of implementing the balancing technique (outlined in Part 2 of Algorithm 1). This evaluation encompasses three dimensions: image-only, text-only, and image-text alignment. We collect an evaluation set of 100 random image-text pairs for balanced and unbalanced data, respectively, and ask annotators to score on the image, text, and pair quality metrics, separately, on a scale of 1 to 5.\"}"}
{"id": "5BCFlnfE1g", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Annotation Guidelines. Annotators follow guidelines to assess both images and texts, evaluating informativeness (how well information is conveyed) and aesthetics. For images, aesthetics considers visual elements like composition, color, lighting, balance, contrast, texture, and subject matter. For texts, aesthetics gauges factors like delimiters, sentence structure, capitalization, prefixes/suffixes, recognized words, generic words, and overall text quality. The alignment metric for image-text pairs measures the relevance between the two modalities, assessing how well the text describes the image content. Ratings are averaged across annotators for each dimension.\\n\\nWe show the study results in Table 10 and discuss the different criteria next.\\n\\n|                | Balanced Data | Unbalanced Data | p-value  |\\n|----------------|---------------|-----------------|----------|\\n| **Image**      | 4.60          | [4.50, 4.70]    | < 0.001  |\\n| **Text**       | 4.67          | [4.56, 4.78]    | < 0.001  |\\n| **Alignment**  | 4.41          | [4.23, 4.59]    | < 0.001  |\\n\\nTable 10: Average human rating on the effect of balancing on data quality, with confidence intervals shown in parentheses. Higher rating is better. Balanced data is rated of higher quality.\\n\\nNoise Mitigation in Image and Text. As shown in Table 10, significant quality improvement for all the three evaluation dimensions is observed after applying balancing. MetaCLIP curation has no specific hard filters such as removing shorter text, removing dates, etc. However, curation by sub-string matching and balancing has a different filtering effect. For example, a sub-string itself can never curate a date-only text. Further, balancing allows signal and noise to co-exist when they are difficult to be separated by human designed filters. For example, if one entry such as \u201cimage\u201d or \u201cphoto\u201d is capped to \\\\( t = 20 \\\\)k, it can only contribute 0.005% of 400M data.\\n\\nVisual Content Alignment. Although MetaCLIP curation does not directly involve images, it has a positive effect on aligning visual content by controlling the quality and distribution of text. First, sub-string matching increases the chance of having (visual) entities mentioned in the text, thereby improving the likelihood of finding corresponding visual content. Second, balancing favors long-tailed entries that could have more diverse visual content than a head entry (such as the text \u201c1\u201d). In Table 10, we observe significant improvement on pair quality from unbalanced data to balanced data.\\n\\nA.4 MEASURING TASK-ALIGNMENT\\n\\nWith metadata, one can measure the alignment of pre-training data distribution with the data distribution in downstream tasks. First we can do a simple measure of counting the number of metadata substring matches that are directly corresponding to class names in the downstream tasks. In the first row of Table 11, we show the accuracy of MetaCLIP (400M), ViT-L for reference. The second row shows the number of classes that are present in the metadata, for each downstream dataset. For example, for ImageNet 703 of the 998 unique class names are present in the metadata. Interestingly, there seems to be a correlation with the accuracy and the number of classes matched in the metadata.\\n\\n| Dataset        | MetaCLIP (400M) | ViT-L |\\n|----------------|------------------|-------|\\n|                | 76.2             | 90.7  |\\n|                | 95.5             | 77.4  |\\n|                | 70.5             | 62.0  |\\n|                | 84.7             | 93.7  |\\n| # of cls. w/ non-zero counts | 703/998         | 52/101 |\\n\\nTable 11: Measuring task-alignment. First row: MetaCLIP (400M) ViT-L accuracy, second row: number of classes matched in metadata, Third and fourth row: We use KL-divergence (lower is better) to measure the similarity between pre-training distribution and benchmark task distribution. Note that, for ImageNet, two classes have duplicated class names and therefore there are only 998 unique classes (out of 1000).\"}"}
