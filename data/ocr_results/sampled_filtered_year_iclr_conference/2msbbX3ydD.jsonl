{"id": "2msbbX3ydD", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Phrase Grounding on Flickr30k Entities and Referring Expression Comprehension on RefCOCO/RefCOCO+/RefCOCOg. The tasks aim to localize specific object(s) in an image described by a referring expression/given entity.\\n\\nFigure 7: Grounded Captioning on Flickr30k. The task aims to generate a caption about the image and ground all generated noun phrases to image regions.\"}"}
{"id": "2msbbX3ydD", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Is there a microwave in the image /questionmark\\nYes , there is a microwave [box0] in the image.\\n\\nIs there a person in the image /questionmark\\nYes , there is a person [box0] in the image.\\n\\nIs there a tv in the image /questionmark\\nNo , there is no tv , but there is a toaster [box0] in the image.\\n\\nIs there a chair in the image /questionmark\\nNo , there is no chair , but there is a car [box0] in the image.\\n\\nIs there a tennis racket in the image /questionmark\\nYes , there is a tennis racket [box0] in the image.\\n\\nIs there a baseball glove in the image /questionmark\\nNo , there is no baseball glove , but there is a tennis ball [box0] in the image.\\n\\nIs there a skis in the image /questionmark\\nYes , there is a skis [box0] in the image.\\n\\nIs there a dining table in the image /questionmark\\nNo , there is no dining table , but there is a skateboard [box0] in the image.\\n\\nIs there a broccoli in the image /questionmark\\nYes , there is a broccoli [box0] in the image.\\n\\nIs there a bowl in the image /questionmark\\nNo , there is no bowl , but there is a plate [box0] in the image.\\n\\nIs there a truck in the image /questionmark\\nYes , there is a truck [box0] in the image.\\n\\nIs there a person in the image /questionmark\\nNo , there is no person , but there is a truck [box0] in the image.\\n\\nFigure 8: Object Hallucination Evaluation (POPE) on COCO.\\n\\nThe task aims to input a query inquiring about the existence of an object, and the model is expected to generate a response in the form of either \\\"yes/no\\\".\"}"}
{"id": "2msbbX3ydD", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 17: Referring Description in Ferret-Bench.\\n\\nQualitative examples to illustrate the difference between various models (LLaV A vs. Kosmos-2 vs. Shikra vs. Ferret (Ours)). For clarity, we have omitted the bounding box outputs in the textual representations.\\n\\n**Question:** What is the interaction between the object [region0] and its surrounding objects?\\n\\n**Ground Truth:** In the region, there is a metal support column. This column is providing support for a bridge above it, which a train is passing over. The column also features a red line on it. This region is part of a larger scene that includes a train track on an elevated bridge.\\n\\n**LLaV A:** The object is a bridge which appears to be a part of a train station, as there are traffic lights present in the scene. The traffic lights are likely used to control the train's movement and ensure safe passage over the bridge.\\n\\n**Kosmos-2:** The interaction between the object and its surroundings is that of a train passing over a bridge.\\n\\n**Shikra:** The metal pole is standing tall and straight, supporting the train that is traveling on the elevated track.\\n\\n**Ferret (Ours):** The object is a metal post that is part of a bridge. This post is essential because it supports the bridge structure, ensuring its stability for the train to safely traverse the bridge. Other elements such as the train, and the overhead guard rail, depend on this post for support and stability. Without this post, the bridge wouldn't be able to withstand the train's weight and motion, potentially leading to a structural collapse. Therefore, this post plays a critical role in maintaining the bridge's integrity and stability.\\n\\n**Question:** What is the relationship between the object [region0] and object [region1] and its surrounding objects?\\n\\n**Ground Truth:** The object is a vase, and the object is a flower. The flower is in the vase, suggesting it is a decorative element within the room.\\n\\n**LLaV A:** A wooden table with a green bowl placed on it.\\n\\n**Kosmos-2:** The object is a vase with a green base.\\n\\n**Shikra:** The answer is flowers.\\n\\n**Ferret (Ours):** The object and object are a vase and a flower, respectively. The vase is holding the flower, suggesting it's displaying or supporting it.\"}"}
{"id": "2msbbX3ydD", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 18: Referring Reasoning in Ferret-Bench. Qualitative examples to illustrate the difference between various models (LLaVA vs. Kosmos-2 vs. Shikra vs. Ferret (Ours)). For clarity, we have omitted the bounding box outputs in the textual representations.\\n\\n**Question:** What is the purpose of the object [region0]?\\n\\n**Ground Truth:** The object is a bottle. Bottles are typically used to hold and store different types of liquids. In this context, it might be used to store a beverage for the meal.\\n\\n**LLaVA:** A knife and a fork, indicating that it is ready for a meal.\\n\\n**Kosmos-2:** The purpose of the object is to attract the birds to the table.\\n\\n**Shikra:** The answer is keep birds away.\\n\\n**Ferret (Ours):** The object is a bottle, generally used to store and dispense liquids like water, juice, or other beverages.\\n\\n**Question:** What is the purpose of the object [region0] on the bike?\\n\\n**Ground Truth:** The object is a shock absorber on the bike. Its purpose is to absorb or dampen shock impulses. It does this by converting the kinetic energy of the shock into another form of energy which is then dissipated. In the context of the motorcycle, it is particularly useful in providing comfort and stability for the rider, especially when travelling over uneven or rough terrains.\\n\\n**LLaVA:** A black and silver Kawasaki motorcycle which is likely used for transportation, commuting, leisure rides, or long-distance travel.\\n\\n**Kosmos-2:** The purpose of the object on the motorcycle is to store and protect the motorcycle\u2019s fuel tank.\\n\\n**Shikra:** The answer is leg.\\n\\n**Ferret (Ours):** The object is a shock absorber. This component is essential for the bike\u2019s stability, providing a smoother ride by soaking up the impact from bumps and uneven surfaces. It significantly improves the overall performance and comfort of the bike, enabling the rider to maintain control and stability even on rough terrains.\"}"}
{"id": "2msbbX3ydD", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 13: In this example, we provide the prompt used to generate the reasoning response for refer-and-ground instruction tuning, following the practice of LLaVA (Liu et al., 2023b).\\n\\nmessages = [\\n    {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"'You are an AI visual assistant that can analyze a single image. You receive five global captions, each describing the same image you are observing. In addition, specific object locations within the image are given, along with detailed coordinates. These coordinates are in the form of bounding boxes, represented as (x1, y1, x2, y2) with floating numbers ranging from 0 to 1. These values correspond to the top left x, top left y, bottom right x, and bottom right y. Also, the relationships between pairs of objects are provided, in the format of object \u2192 relationship \u2192 subject, where the object/subject are indexed by object id from previous object lists as well as the object names. Also, several region descriptions are given, each describing a box region of the image, with detailed coordinates.\\n\\nThe task is to use the provided image information (objects, attribute, relationship, region description, captions), create a plausible and challenging question about the image, and provide the answer in detail.\\n\\nCreate complex questions that mention specific regions of the image, but the question should require some knowledge-aware or high-level commonsense reasoning beyond describing the scene.\\n\\nTo answer such questions, one should first understand the visual content, then based on the background knowledge or reasoning, either explain why the things are happening that way or provide guides and help to the user's request. Make the question challenging by not including the visual content details in the question so that the user needs to reason about that first.\\n\\nHere are some additional requirements about generated questions and answers:\\n\\n1. In question or answer, you must mention bounding box coordinates to refer to the object or regions, instead of directly say the object name or describing the regions in text. In answers, explain the region in the context of scene. Include details like object counts, position of the objects, relative position between the objects.\\n\\n2. Don't ask the question you are not confident to answer. Only include question that have definite answer.\\n\\n3. Do not mention that the information source is provided in text/catpion/region description. Always answer as if you are directly looking at the image.\\n\\n4. Make the question as diverse as possible and as complex-reasoning required as possible.'\\n]\\n\\nfor sample in fewshot samples:\\n    messages.append({\\\"role\\\": \\\"user\\\", \\\"content\\\": sample['context']})\\n    messages.append({\\\"role\\\": \\\"assistant\\\", \\\"content\\\": sample['response']})\\n\\nmessages.append({\\\"role\\\": \\\"user\\\", \\\"content\\\": 'n'.join(query)})\"}"}
{"id": "2msbbX3ydD", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 14: One example used in in-context learning to construct GPT-Assisted Refer-and-Ground Instruction-Tuning. We encourage readers to refer to the codebase for the prompts for other types.\\n\\nObjects\\n\\nObject 0: badge at [0.802, 0.505, 0.872, 0.581].\\n\\nRelationships\\n\\nObject 5: fringe $\\\\rightarrow$ on $\\\\rightarrow$ Object 4: flag.\\n\\nRegion Descriptions\\n\\nRegion Description at [0.038, 0.029, 0.508, 0.995]: A Grinning gentlemen about to get his tie out.\\n\\nGlobal Caption\\n\\nA female in military uniform cutting a businessman's neck tie.\\n\\nConversation\\n\\nQuestion: What is unusual about the region [0.102, 0.230, 0.578, 0.761]?\\n\\nAnswer: In the specified region, there is an intriguing sight: a pair of oversized scissors positioned near a man's tie, accompanied by a woman in military attire. The two of them share a hearty laugh, creating an unexpected contrast. The atmosphere appears celebratory, resembling a ribbon-cutting ceremony with flags in the background. However, rather than cutting a ribbon, the scissors are directed toward the man's tie, adding an unexpected twist to the situation.\\n\\nDEEXAMPLES AND PROMPTS FOR FERRET-BENCH\\n\\nWe leverage GPT-4 to generate three kinds of region-based questions evaluating referring and grounding capability: (i) Referring Description, (ii) Referring Reasoning, and (iii) Grounding in Conversation. Here, we only provide the prompt in Table 15 used to generate the referring description response. One example of GPT-4 answers is shown in Table 16. We recommend readers check out more examples in Appendix E.\"}"}
{"id": "2msbbX3ydD", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 15: In this example, we provide the prompt used to generate the referring description response.\\n\\nmessages = [\\n  {\\n    \\\"role\\\": \\\"system\\\",\\n    \\\"content\\\": \\\"'You are an AI visual assistant that can analyze a single image. You receive five global captions, each describing the same image you are observing. In addition, specific object locations within the image are given, along with detailed coordinates. These coordinates are in the form of bounding boxes, represented as (x1, y1, x2, y2) with floating numbers ranging from 0 to 1. These values correspond to the top left x, top left y, bottom right x, and bottom right y. Also, the relationships between pairs of objects are provided, in the format of object \u2192 relationship \u2192 subject, where the object/subject are indexed by object id from previous object lists as well as the object names. Also, several region description are given, each describing a box region of image, with detailed coordinates.\\n\\nThe task is to use the provided image information (objects, attribute, relationship, region description, captions), create a plausible and challenging question about the image, and provide the answer in detail.\\n\\nCreate questions that refer to coordinates of some objects or regions without describing it, and ask about its interaction with surrounding/nearby objects.\\n\\nTo answer such questions, one should require first understanding the visual content, then based on the spatial information provided.\\n\\nHere are some additional requirements about generated questions and answers:\\n\\n1. In question, you must mention bounding box coordinates to refer to the object or regions, instead of directly say the object name or describing the regions in text. In answers, explain the region in the context of scene. Include details like object counts, position of the objects, relative position between the objects.\\n\\n2. Don't ask the question you are not confident to answer. Only include question that have definite answer.\\n\\n3. Do not mention that the information source is provided in text/catpion/region description. Always answer as if you are directly looking at the image.\\n\\n4. Don't mention additional coordinates in the answer.\\n\\n5. Question should be explicitly ask about context/surrounding/nearby information/interaction.\\\"\\n}\\n]\\n\\nfor sample in fewshot samples:\\n  messages.append(\\n    {\\n      \\\"role\\\": \\\"user\\\",\\n      \\\"content\\\": sample['context']\\n    }\\n  )\\n\\n  messages.append(\\n    {\\n      \\\"role\\\": \\\"assistant\\\",\\n      \\\"content\\\": sample['response']\\n    }\\n  )\\n\\nmessages.append(\\n  {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\": '\\n'.join(query)\\n  }\\n)\"}"}
{"id": "2msbbX3ydD", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 16: One example of GPT4\u2019s answers about referring description.\\n\\nConversation\\n\\nQuestion:\\nDescribe the region [0.102, 0.230, 0.578, 0.761] in the context of surrounding objects.\\n\\nAnswer:\\nIt\u2019s a man in the region, who is standing on top of a skateboard, performing a rail grind over a metal hand rail. There is a bench and a tree near him. Also, he is in front of some buildings.\\n\\nWe provide more quantitative results of the predictions under various tasks from Ferret to indicate the model\u2019s strength and capability.\\n\\n\u2022 Please refer to Figure 5 for Referring Object Classification on LVIS with different referring formats (point/box/).\\n\u2022 Please refer to Figure 6 for Visual Grounding on Flickr30k Entities and Referring Expression Comprehension on RefCOCO/RefCOCO+/RefCOCOg.\\n\u2022 Please refer to Figure 7 for Grounded Captioning on Flickr30k Karpathy split.\\n\u2022 Please refer to Figure 8 for Evaluating Object Hallucination (POPE) on COCO val split.\\n\u2022 Please refer to Table 17 for Referring Description in Ferret-Bench.\\n\u2022 Please refer to Table 18 for Referring Resoning in Ferret-Bench.\\n\u2022 Please refer to Table 19 for Grounding in Conversation in Ferret-Bench.\\n\\nIs the object [region0] a refrigerator or a chair?\\nIt is a refrigerator [box0].\\n\\nIs the object [region0] a headband or a necklace?\\nIt is a headband [box0].\\n\\nIs the object [region0] a pipe or a clock tower?\\nIt is a clock tower [box0].\\n\\nIs the object [region0] a lamp or a lampshade?\\nIt is a lampshade [box0].\"}"}
{"id": "2msbbX3ydD", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MLLMs for referring and grounding.\\n\\nIn the realm of existing literature, works such as Kosmos-2 (Peng et al., 2023) and Shikra (Chen et al., 2023b), closely resemble ours as they also enable MLLMs for fine-grained image comprehension and open-world referring and grounding. Additional works in this direction include GPT4ROI (Zhang et al., 2023), PVIT (Chen et al., 2023a), BuboGPT (Zhao et al., 2023), VisionLLM (Wang et al., 2023), and ContextDET (Zang et al., 2023).\\n\\nNevertheless, pivotal distinctions set our model apart. First, prior endeavors supported only bounding boxes (and points in Shikra) as input. Conversely, due to Ferret\u2019s innovative hybrid region representation, we accommodate a broader range of free-form shapes for referring, encompassing points, boxes, sketches, scribbles, polygons, and more. Second, we meticulously curate an extensive refer-and-ground instruction tuning dataset. Third, we introduce Ferret-Bench to facilitate forthcoming research and enhance evaluation benchmarks in this direction. Lastly, our model exhibits superior performance compared to previous works, notably mitigating object hallucination to a significant extent. A more straightforward side-by-side comparison is shown in Tab. 8.\\n\\nUnifying grounding and VL understanding.\\n\\nOur work is also related to previous work that aims to unify text and bounding box output for vision-language (VL) models, such as UniTAB (Yang et al., 2022), OFA (Wang et al., 2022b), and Unified-IO (Lu et al., 2022), which also represent bounding boxes using a set of additional discrete tokens as proposed in Pix2Seq (Chen et al., 2021; 2022a). Ferret is unique in that (i) our model is built upon LLMs, marrying the power of LLMs and grounding, thus unlocking new capabilities such as grounded instruction tuning, and (ii) we handle bounding box coordinates as regular text tokens, avoiding the need for extra specialized tokens dedicated to representing boxes.\\n\\nDiscussion on Limitation and Failure Cases\\n\\nWe acknowledge certain specific failure scenarios and limitations for our models, which are detailed as follows:\\n\\nFailure Scenarios: (1). Referring to too many objects (more than 3) in one question might not be as accurate as referring to each of them in separate conversations. This is likely due to a relative scarcity of training data that mentions too many objects. (2). The referring and grounding of very small objects is less accurate than large or medium objects. It\u2019s a common challenge in object detection. However, we think further improving input image resolution is able to help.\\n\\nLimitations: (1). Not good at other languages because the training dataset is curated only in English. Although Ferret shows some emergent referring and grounding capability in other languages, its performance in other languages is still worse than in English. Future incorporation of multilingual training data could potentially mitigate this. (2). Similar to many large language models, Ferret has the potential to generate harmful or factually incorrect responses. (3). Ferret is not designed for segmentation tasks requiring mask outputs.\\n\\nDetails of Dataset\\n\\nC.1 Task Template for Public Datasets\\n\\nIn Section 3.1, we mentioned using carefully designed task templates to convert public datasets such as Visual Genome into instruction-following format. The task templates we used are provided in Table 9. For simplicity, we only list three examples for each task.\\n\\nC.2 Details on Spatial Negative Mining\\n\\nIn Section 3.3, we conducted negative sample mining for two aspects: (i) Image-conditioned Category Localization, and (ii) Semantics-conditioned Category Localization. They use the same template to convert the original data, which falls into the task of object hallucination in Table 9. Specifically, for the negative categories in (ii), we prompt ChatGPT/GPT-4 to generate entities that are most analogous to the original class, attribute, or quantity, e.g., 'man' vs. 'woman', 'blue' vs. 'yellow', 'two' vs. 'three'. The prompt feed into ChatGPT/GPT-4 encompasses all the entities extracted from 5 captions associated with one single image. We show the exact prompt template in Table 10.\"}"}
{"id": "2msbbX3ydD", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 9: Examples of task templates Ferret used to transfer different public data types into the instruction-following format.\\n\\n| Task                  | Examples                                                                 |\\n|-----------------------|--------------------------------------------------------------------------|\\n| Referring-Object      | What is the class of the object within the image?                        |\\n|                       | Classify object in the image.                                           |\\n|                       | Identify the object in the image.                                       |\\n| Referring-Relation    | What does object1 <location1> do to object2 <location2> of the image?    |\\n|                       | What is the physical relation between object1 <location1> and object2 <location2>? |\\n|                       | Can you figure out the geometric relation of the object1 <location1> and object2 <location2>? |\\n| Referring-Region      | Describe the region <location> in a short phrase.                        |\\n|                       | What is in the region <location>? Describe in a phrase.                 |\\n|                       | Capture in a phrase: what's near region <location> in the picture?       |\\n| REC.                  | Where is object in the image?                                           |\\n|                       | What are the coordinates for the given object in the image?             |\\n|                       | Given the image, could you please tell me where object is?              |\\n| Phrase Grounding      | What are the locations of objects?                                      |\\n|                       | Could you provide me with the exact locations of objects?               |\\n|                       | Please indicate the positions of objects in the image?                  |\\n| Object Detection (O365)| Detect all objects among class in the image.                             |\\n|                       | Perform object detection given the image within class.                  |\\n|                       | Given the image and set class, identify all the objects that belong to the set. |\\n| Grounded Captioning   | What is this photo about? Use concise language.                          |\\n|                       | Describe the overall picture in just a few words.                       |\\n|                       | What do you see happening in this image? Provide the answer in short.    |\\n| Object Hallucination  | Is there a object in the image?                                         |\\n|                       | Are there object in the image?                                          |\\n|                       | Please tell me whether object exists in the image?                      |\\n\\nTable 10: In this example, we provide the prompt to generate the spatial negative sets.\\n\\nmessages = [\\\"You are an AI visual assistant that can analyze a single image. You receive several entities given by a list, each describing the objects in the image you are observing. For each entity mentioned, change them with the most misleading entity name (may belong to the same category but are actually different) (nonexistent objects: man \u2192 woman, nonexistent attributes: brown \u2192 yellow, nonexistent quantities: two \u2192 three, etc.). The instructions should contain interrogative and declarative sentences. The output format needs to be a list only which contains the misleading entity names. Please follow the instructions carefully.\\n\\n1. The length of the output list needs to be exactly equal to the input list.\\n2. Do not explain the reasons.\\n3. Do not mention the input entities, at least the output name and input name needs to be different.\\n4. Do not mention something abstract, like \\\"alien\\\".\\n5. When dealing with quantities, focus solely on increasing the numbers during revision.\\n6. When dealing with words like \\\"a few\\\", \\\"a group\\\", \\\"several\\\", \\\"some\\\", etc., try changing the objects (A few men \u2192 A few women).\\n7. Ensure that inclusive words are not substituted with their specific subsets. For example, if the word is \\\"people,\\\" avoid replacing it with genders like \\\"man\\\" or \\\"woman.\\\" Instead, consider modifying them to different categories, such as \\\"people\\\" \u2192 \\\"animals.\\\"'\"}"}
{"id": "2msbbX3ydD", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We provide some example prompts to generate refer-and-ground from ChatGPT/GPT-4. Prompt and the in-context example of multiple-round visual conversation data are shown in Table 11 and Table 12. Prompt and the in-context example of one-round reasoning data are shown in Table 13 and Table 14.\\n\\nTable 11: In this example, we provide the prompt used to generate the conversation response for refer-and-ground instruction tuning, following the practice of LLaVA (Liu et al., 2023b).\\n\\n```json\\nmessages = [\\n\\n    {\\n        \\\"role\\\": \\\"system\\\",\\n        \\\"content\\\": \\\"'You are an AI visual assistant that can analyze a single image. You receive five global captions, each describing the same image you are observing. In addition, specific object locations within the image are given, along with detailed coordinates. These coordinates are in the form of bounding boxes, represented as (x1, y1, x2, y2) with floating numbers ranging from 0 to 1. These values correspond to the top left x, top left y, bottom right x, and bottom right y. Also, the relationships between pairs of objects are provided in the format of object \u2192 relationship \u2192 subject, where the object/subject are indexed by object id from previous object lists as well as the object names. Also, several region descriptions are given, each describing a box region of the image, with detailed coordinates.\\n\\nDesign a conversation between you and a person asking about this photo. Ask diverse questions and give corresponding answers. The answers should be in a tone that a visual AI assistant is seeing the image and answering the question.\\n\\nHere are some additional requirements about generated questions and answers:\\n\\n1. Only include questions that have definite answers:\\n   (1) one can see the content in the image that the question asks about and can answer confidently;\\n   (2) one can determine confidently from the image that it is not in the image. Do not ask any questions that cannot be answered confidently.\\n\\n2. Also include complex questions that are relevant to the content in the image, for example, asking about background knowledge of the objects in the image, asking to discuss events happening in the image, asking about object actions in the context of entire images, etc. Again, do not ask about uncertain details.\\n\\n3. Provide detailed answers when answering complex questions. For example, give detailed examples or reasoning steps to make the content more convincing and well-organized. You can include multiple paragraphs if necessary.\\n\\n4. In all samples, either in question or answer, you must mention bounding box coordinates to refer to the object or regions instead of directly saying the object name or describing the regions in text. In answer, explain the region in the context of the scene.\\n\\n5. Do not mention that the information source is provided in the text/caption/region description. Always answer as if you are directly looking at the image.\\n\\n6. Make the question as diverse as possible. Include questions asking about the visual content of the image, including the object types, counting the objects, object actions, object locations, relative positions between objects, object selection, object functions, etc. Make the question challenging by less including the visual content details in the question.'\\n    }\\n]\\n```\"}"}
{"id": "2msbbX3ydD", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 12: One example used in in-context learning to construct GPT-Assisted Refer-and-Ground Instruction-Tuning. We encourage readers to refer to the codebase for the prompts for other types.\\n\\n| Objects |\\n|---------|\\n| Object 0: chair at [0.596, 0.637, 0.698, 0.997] |\\n\\n| Relationships |\\n|--------------|\\n| Object 11 : frame \u2192 with \u2192 Object 9 : photo |\\n\\n| Region Descriptions |\\n|---------------------|\\n| Region Description at [0.560, 0.466, 0.600, 0.529]: a white picture frame with a black and white photo on it. |\\n\\n| Global Caption |\\n|----------------|\\n| White chairs sit around a polished wood dining table while a sectional soft sits in the background. |\\n\\n| Conversation |\\n|-------------|\\n| Question: What is the object [0.212, 0.539, 0.718, 0.820] made of? |\\n| Answer: The dining table over there is crafted from a tree trunk. |\\n\\n| Question: Where would be a good place to have my dinner? Point out the locations in image. |\\n| Answer: I suggest having dinner at the table [0.214, 0.541, 0.720, 0.997], possibly sitting on the white chair [0.264, 0.717, 0.524, 0.995]. Also, enjoy the wine [0.242, 0.644, 0.288, 0.682]. |\\n\\n| Question: In terms of comfort for resting, which object is better - the one [0.830, 0.431, 0.978, 0.596] or the one [0.264, 0.717, 0.524, 0.995]? |\\n| Answer: Among the two, the couch [0.830, 0.431, 0.978, 0.596] would be more comfortable for resting. The object [0.264, 0.717, 0.524, 0.995] is a dining chair, which is typically less comfortable for extended relaxation. |\\n\\n| Question: Is the item [0.182, 0.238, 0.342, 0.461] real or is it a mirror reflection? |\\n| Answer: The television and the fireplace [0.182, 0.238, 0.342, 0.461] are not real but are actually reflections in the mirror. |\\n\\n| Question: Can you describe the decorating style of the living room shown in this image? |\\n| Answer: The living room is decorated in a modern chic style. The image shows white chairs and lamps, a gray couch, and a wooden table and floor, all contributing to a modern aesthetic. The large mirror and built-in wine rack are also characteristic of modern design. Thus, the overall decor creates an atmosphere of elegance and sophistication, indicating a modern chic style. |\"}"}
{"id": "2msbbX3ydD", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 19: Grounding in Conversation in Ferret-Bench. Qualitative examples to illustrate the difference between various models (Kosmos-2 vs. Shikra vs. Ferret (Ours)). For clarity, we only display the bounding box outputs in Ferret\u2019s responses.\\n\\nQuestion: What is happening in the field and provide the coordinates of mentioned objects?\\n\\nGround Truth: In the image, there is a group of sheep [box0] [box1] grazing in a field [box2]. The field is filled with plant life [box3]. There is also a bush [box4] and some trees [box5] present in the field.\\n\\nKosmos-2: In the field, a herd of sheep [box0] is grazing on the lush green grass [box1].\\n\\nShikra: A group of sheep [box0] [box1] [box2] [box3] [box4] [box5] grazing on lush green grass.\\n\\nFerret (Ours): In the field, there are sheep [box0] [box1] [box2] [box3] [box4] [box5] grazing on the grass [box6].\\n\\nQuestion: What is the man doing and what objects are in front of him? Please provide the coordinates of these objects.\\n\\nGround Truth: The man [box0] is sitting at a table [box1] and looking at a plate of food [box2]. There is a glass [box3] and a bottle [box4] on the table in front of him.\\n\\nKosmos-2: The man is sitting at a dining table [box0] smiling and looking at the plate of food in front of him [box1].\\n\\nShikra: The man [box0] is sitting at a table [box1] with a plate of food [box2] in front of him. He is looking up and smiling, possibly staring at a person [box0] in the restaurant.\\n\\nFerret (Ours): The man [box0] is having a meal at a table [box1]. In front of him, there is a plate [box2] with a variety of food items such as fruit and eggs. There is also a glass [box3] filled with water and a bottle [box4].\"}"}
{"id": "2msbbX3ydD", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Spatial-aware Visual Sampler.\\n\\nThe shape of the referred regions can be quite varied, not limited to just points or rectangle boxes. Grid-based processing like convolution or patch attention cannot handle irregular shapes. Similar to our cases, 3D point clouds are also in irregular shape and show varied sparsity in the 3D space. Inspired by existing works in 3D point cloud learning (Qi et al., 2017a; Ma et al., 2022; Wang et al., 2019), we propose a spatial-aware visual sampler.\\n\\nGiven extracted image feature map $Z \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C}$ and the binary region mask $M$, we first randomly sample $N$ positive points inside $M$. For each point, its feature is obtained by bilinear interpolation. The $N$ points are fed into a cascade of blocks, where each of them includes three steps: sampling, gathering, pooling. (1) Sampling: $N_r$ points are sampled from $N$ points via farthest point sampling (FPS) algorithm (Qi et al., 2017b), which can guarantee sufficient coverage. (2) Gathering: For each of the sampled points $x_i$, we search its $k$ nearest neighbors from the pool of previous $N$ points, and obtain a group of points $\\\\{x_{i1}, x_{i2}, ..., x_{ik}\\\\}$. Then, inspired by PointMLP (Ma et al., 2022), for each group, we fuse the features of sampled point $x_i$ and its neighbor points by:\\n\\n$$h_{ik} = \\\\sigma(\\\\theta([Z(x_{ik}) - Z(x_i]); C(x_{ik}) - C(x_i)])),$$\\n\\nwhere $x_{ik}$ is one of the neighbors of $x_i$, $Z(x)$ denotes the point $x$'s feature (in the first block, it is interpolated from feature map $Z$; in the succeeding blocks, it is the output feature from the previous block), $C(x)$ denotes the 2D coordinates of point $x$, $; \\\\cdot$ means channel-wise concatenation of multiple vectors, $\\\\theta$ is implemented by a linear layer to adapt the relative local features, and $\\\\sigma$ is also a linear layer to fuse each local feature from neighbors with sampled point feature. (3) Pooling: A max pooling is conducted to fuse $k$ neighbor features into one feature as the representation of the sampled point:\\n\\n$$h_i = \\\\max_{k: (x_{ik}) \\\\in KNNs of x_i} h_{ik}.$$\\n\\nAfter the three steps, we obtain fewer points but a more dense feature space since it incorporates the local neighbor features as well as their relative positions. In experiments, we set $N = 512$, $r = 4$ and $k = 24$, and cascade two such blocks, which in the end outputs 32 points with their features. Similar to ROIAlign (He et al., 2017), we flatten the point features into a single vector and project it to the dimension of LLM embeddings. The final feature is used to replace the $\\\\langle$SPE$\\\\rangle$ token in the input.\\n\\nOutput. The above region denotations are used in Ferret input to refer to specific regions. In Ferret output, to achieve grounding, we generate the box coordinates right after the corresponding regions/nouns in the text response. For instance, \\\"There is a dog [100, 150, 300, 200] in the figure.\\\" With this data format, our model is expected to implicitly learn what is groundable in the current image and what their locations are.\\n\\nLLM. We consider Vicuna (Chiang et al., 2023) as our language model, a decoder-only LLM (Brown et al., 2020) that is instruction-tuned on top of LLaMA (Touvron et al., 2023a). Prior to being fed into the LLM, the image embeddings undergo transformation via an additional linear layer to match the embedding dimension of the text tokens.\\n\\n4 FPS starts from a random single point sampled from $N$ points. In each iteration, it samples one point from the rest points such that it is the farthest from the set of already sampled points. See detail in Qi et al. (2017b).\"}"}
{"id": "2msbbX3ydD", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we present GRIT, a Ground-and-Refer Instruction-Tuning dataset containing around 1.1M multimodal dialogues for model training. GRIT consists of three types of data: (i) public datasets that are converted into an instruction-following format (Section 3.1); (ii) instruction-tuning data generated via ChatGPT and GPT-4 (Section 3.2); and (iii) additional data from spatial negative mining for enhancing model robustness (Section 3.3).\\n\\n3.1 HIERARCHY\\n\\nSpatial understanding can be characterized by varying levels of granularity and task formats. During our dataset creation, we look into the following categories based on two dimensions:\\n\\n\u2022 In terms of **granularity**, we identify four main categories: (i) individual objects, (ii) relationships among objects, (iii) descriptions of specific regions, and (iv) region-based complex reasoning.\\n\\n\u2022 In terms of **task format**, we further divide the data into three distinct types: (i) Region-in Text-out data, (ii) Text-in Region-out data, and (iii) Text-Region combined data.\\n\\nWe compiled an extensive set of public data focusing on the aforementioned dimensions and converted them into an instruction-following format using carefully designed templates. A more in-depth view of these templates is available in Appendix C.1.\\n\\n**Individual objects.** To achieve visual understanding at the object level, we select object detection datasets such as Visual Genome (Krishna et al., 2017), Object365 (Shao et al., 2019), and visual grounding datasets including RefCOCOs (Yu et al., 2016; Lin et al., 2014; Nagaraja et al., 2016) and Flickr30k-Entities (Plummer et al., 2015). The converted Visual Genome object data follow a Region-in Text-out format. Additionally, to enable Ferret to understand free-form shapes, we apply SAM (Kirillov et al., 2023) to Visual Genome object data to obtain a segmentation mask for each object, which is fed into the spatial-aware visual sampler to extract continuous region feature during training. The visual grounding datasets and Object365 data adhere to a Text-in Region-out format.\\n\\nThis section has in total 678k data.\\n\\n**Relationships among objects & descriptions of regions.** We selected data pertaining to object relationships and region captions from Visual Genome (Krishna et al., 2017) to address these two facets, respectively. Both datasets employ a Region-in Text-out format and 177k data are obtained. Similar to Visual Genome object data, we also extract segmentation masks of objects in Visual Genome relationship data via SAM.\\n\\nFor Region-in Text-out data, the input highlights a specific region, prompting queries about it. For Text-in Region-out data, the input comprises textual descriptions, and the task is to pinpoint or ground the relevant region in its response. The combined Text-Region data integrates both text and region within a single sequence, which can be present in the input, output, or both.\"}"}
{"id": "2msbbX3ydD", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Regarding complex reasoning centered on specific regions, we constructed a novel dataset with the help of ChatGPT/GPT-4. It adopts a combined Text-Region format, and is detailed in the subsequent section.\\n\\n### 3.2 GPT-ASSISTED VISUAL INSTRUCTION DATA GENERATION\\n\\nBesides converting existing datasets by templates, dialogue instruction tuning data is proved to be critical for MLLM to understand human intention and generate fluent, natural, and long-form responses (Liu et al., 2023b; Zhu et al., 2023a; Li et al., 2023d). Few-shot prompting is widely used to obtain visual instruction tuning data, where textual scene descriptions of images and human-annotated dialogues are provided as few-shot demonstrations, and ChatGPT/GPT4 are prompted to generate new dialogue based on the new image's textual scene descriptions.\\n\\nHowever, previous instruction tuning data mainly focus on describing the entire image without explicitly specifying spatial-related information. To collect refer-and-ground instruction tuning data, we emphasize region-based spatial knowledge in the following three steps. (i) Besides objects and global captions usually used as before, our symbolic scene description additionally includes physical relationships between objects and region captions along with coordinates of them. (ii) In human-annotated dialogues, we add coordinates after the groundable regions or objects either in input or output or both, and the dialogues are typically focused on specific regions. It helps to implicitly prompt ChatGPT/GPT4 to follow similar patterns when generating new dialogues. (iii) The generated dialogues sometimes cannot follow the rules and patterns we wrote in system prompts and few-shot examples, which might be due to that the context of LLM input is too long to handle all the details. To alleviate it, we propose to use ChatGPT/GPT-4 again to refine the initially generated dialogues, whose context length is only 10% of the data generated from the first round on average. To save cost, we use ChatGPT in the first round of generation and GPT-4 for refining. 34k dialogues in total are collected.\\n\\nAdditionally, to exploit existing instruction-tuning data such as those in LLaV A (Liu et al., 2023b), we apply an open-vocabulary object detector, GLIPv2 (Zhang et al., 2022), on LLaV A-158k data to localize groundable nouns in the text. Then, we append the bounding boxes after the corresponding nouns, forming a pseudo-grounded LLaV A instruction data that are also used for training Ferret.\\n\\n### 3.3 SPATIAL NEGATIVE MINING\\n\\nAs highlighted in prior studies (Li et al., 2023e; Liu et al., 2023a), MLLM exhibits a propensity to hallucinate in response to yes/no questions. We observed a similar occurrence when inquiring about detailed regions. To address this, we also conduct negative sample mining by following two ways: (i) Image-conditioned Category Localization, and (ii) Semantics-conditioned Category Localization. They both ask the model to localize specific object categories, thereby enabling the model's ability to discern and potentially recognize the absence of certain objects. They differ in how to select the negative category. For (i), Object365 data are employed and we randomly select the object class from the vocabulary that is not shown in the given image. For (ii), Flickr30k data are used and negative categories are sourced by utilizing ChatGPT/GPT4 to find entities that are most analogous to the original class, attribute, or quantity, e.g., 'man' vs. 'woman', 'blue' vs. 'yellow', 'two' vs. 'three'. We curate the data to maintain an equilibrium between positive and negative samples for each of the two types. 695k data are collected. A more comprehensive elaboration is provided in Appendix C.2.\\n\\n### 4 EXPERIMENTS\\n\\nFirst of all, we illustrate the training details of Ferret. Then in evaluation, we start with evaluating Ferret on conventional referring and grounding benchmarks (Sec. 4.1 and 4.2). Then, we demonstrate the power of Ferret in more complex multimodal chatting with refer-and-ground capability in Sec. 4.3. For a detailed visualization of each, kindly check Appendix E. We further ablate key components in Ferret (Sec. 4.4), analyze the object hallucination of Ferret (Sec. 4.5) and discuss Ferret v.s. GPT-4 (Sec. ??)\\n\\nWe observed that even though we don't collect other data specifically for training, Ferret demonstrates the capability to generalize robustness across diverse categories like relationships, events, etc. We attribute this versatility to the potent compositional capabilities inherent to LLM.\"}"}
{"id": "2msbbX3ydD", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Results of referring object classification on three different referring types, including point, box, and free-form shape. \u2018\u2715\u2019 means no such capability.\\n\\n| Models          | Point | Box | Free-form | Random Guess |\\n|-----------------|-------|-----|-----------|---------------|\\n| LVIS (Acc %)    |       |     |           | 50             |\\n| Kosmos-2 (Peng et al., 2023) |       |     |           | 60.25          |\\n| Shikra-7B (Chen et al., 2023b) | 57.82 | 67.71 | \u2713         |\\n| GPT4-ROI (Zhang et al., 2023) |       |     |           | 61.76          |\\n| Ferret-7B       | 67.94 | 79.42| 69.77     |\\n| Ferret-13B      | 68.35 | 80.46| 70.98     |\\n\\nTable 2: Results of grounded image captioning on the test set of Flickr30k Entities. BLEU@4, METEOR, CIDEr, and SPICE are used for the caption evaluation. \u2018F\u2019 is used for grounding evaluation. \u2018\u2013\u2019 means not reported.\\n\\n| Models          | Caption Eval. | Grounding Eval. |\\n|-----------------|---------------|-----------------|\\n|                 | B@4 M C S    | F1 all F1 loc |\\n| GVD (Zhou et al., 2019) | 27.3 22.5 62.3 | 16.5 7.55 22.2 |\\n| Cyclical (Ma et al., 2020) | 26.8 22.4 61.1 | 16.8 8.44 22.78 |\\n| POS-SCAN (Zhou et al., 2020) | 30.1 22.6 69.3 | 16.8 7.17 17.49 |\\n| UniTAB (Yang et al., 2022) | 30.1 23.7 69.7 | 17.4 12.95 34.79 |\\n| Shikra-13B (Chen et al., 2023b) | \u2013 \u2013 73.9 | \u2013 \u2013 |\\n| Ferret-7B       | 35.1 24.6 74.8 | 18.0 15.02 37.62 |\\n| Ferret-13B      | 37.0 25.5 76.1 | 18.3 15.12 38.03 |\\n\\nTraining Details.\\nWe initialize the image encoder with CLIP-ViT-L/14@336p, the LLM with Vincuna, and the projection layer with LLaVA\u2019s first-stage weights, leaving the visual sampler randomly initialized. After the initialization, Ferret is trained on the aforementioned GRIT data for three epochs, optimized by Loshchilov & Hutter (2017) with a learning rate of $2 \\\\times 10^{-5}$ and a batch size of 128. The training takes $\\\\sim 5/2.5$ days on 8 A100 GPU for a Ferret-13B/7B. During training, when input refers to regions, we randomly choose either the center points or the bounding boxes (or segmentation masks if available) to represent the regions. We perform de-duplication in training data to remove the samples that are in downstream evaluations.\\n\\n4.1 INPUT REFERRING\\nThe model\u2019s capability of understanding referring is reflected in that, given a referred region in the question, how accurately the model can understand the semantics of the referred region. To measure it, we start with the most basic semantics, object, as it is fundamental and clear to define. To be more specific, the task we evaluate on is Referring Object Classification: the question refers to a specific region in the image, and the model needs to classify the object in the region. Since Ferret and MLLMs usually generate free-form text responses, it is inaccurate to match the predicted class with the ground-truth class if directly asking the model to classify without constraints. Alternatively, we make it a binary-choice question in the format of \\\"Is the object \u27e8location\u27e9 a \u27e8class A\u27e9 or a \u27e8class B\u27e9?\\\". We feed the binary-choice question and image into the MLLMs to obtain the response, and then detect if the response matches the ground-truth (GT) class by some rule.\\n\\nTo prepare the data, we used the validation split of LVIS dataset (Gupta et al., 2019) covering over 1000 object categories, and sampled 2667 objects as the GT objects. Then, we randomly choose a different object category in the same image whose central point is close to the GT object as the negative object, and replace \u27e8class A\u27e9 and \u27e8class B\u27e9 with those two randomly to form 2667 questions. Additionally, to mimic the versatility of referring in human life, we replace the \u27e8location\u27e9 with three different types: point, box, and free-form shape. For point, we randomly sample a point inside the GT object that is also near the GT object\u2019s boundary. For box, we use the GT bounding box provided by LVIS. For the free-form shape, we randomly generate some strokes inside the GT object to simulate that. Results on all three types of referring are summarized in Table 1. Ferret can significantly outperform previous models (Peng et al., 2023; Chen et al., 2023b) and handle all types of referring, a capability notably absent in previous works.\\n\\n4.2 OUTPUT GROUNDING\\nFerret performs well in referential dialogue, allowing for its integration into various VL tasks, notably those with grounding outputs. To rigorously assess the grounding capability, we first subject Ferret to benchmark visual grounding tasks in a generative paradigm. Then, to measure the alignments between words and regions, we further evaluate Ferret on grounded captioning task.\\n\\nVisual grounding. Visual grounding aims to ground language queries into aligned image regions. We experiment on the sub-tasks of referring expression comprehension (REC) with three renowned benchmarks: RefCOCO (Lin et al., 2014), RefCOCO+ (Yu et al., 2016), and RefCOCOg (Mao et al., 2016), and phrase grounding with Flickr30k Entities dataset (Plummer et al., 2015). REC task involves a question or description about a specific area in an image, with the model expected to predict just one bounding box. Phrase grounding, conversely, seeks to associate all the noun phrases in a sentence with the visual content.\\n\\nSometimes both GT class and negative class appear in the answer, e.g., \u201cThe object is \u27e8class GT\u27e9, not \u27e8class Neg\u27e9\u201d. Our rule removes the substring in-between \u201cnot\u201d and comma/period, and then detects GT class.\"}"}
{"id": "2msbbX3ydD", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740\u2013755. Springer, 2014.\\n\\nFuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023.\\n\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.\\n\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\\n\\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916, 2022.\\n\\nRuotian Luo and Gregory Shakhnarovich. Comprehension-guided referring expressions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7102\u20137111, 2017.\\n\\nChih-Yao Ma, Yannis Kalantidis, Ghassan AlRegib, Peter Vajda, Marcus Rohrbach, and Zsolt Kira. Learning to generate grounded visual captions without localization supervision. In ECCV, 2020.\\n\\nXu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. Rethinking network design and local geometry in point cloud: A simple residual mlp framework. arXiv preprint arXiv:2202.07123, 2022.\\n\\nJunhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 11\u201320, 2016.\\n\\nVarun K Nagaraja, Vlad I Morariu, and Larry S Davis. Modeling context between objects for referring expression understanding. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pp. 792\u2013807. Springer, 2016.\\n\\nOpenAI. GPT-4 technical report. https://arxiv.org/abs/2303.08774, 2023.\\n\\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023.\\n\\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pp. 2641\u20132649, 2015.\\n\\nCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652\u2013660, 2017.\\n\\nCharles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems 30, 2017.\"}"}
{"id": "2msbbX3ydD", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In *International conference on machine learning*, pp. 8748\u20138763. PMLR, 2021.\\n\\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. *arXiv preprint arXiv:2211.05100*, 2022.\\n\\nShuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In *Proceedings of the IEEE/CVF international conference on computer vision*, pp. 8430\u20138439, 2019.\\n\\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. *arXiv preprint arXiv:2307.05222*, 2023.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*, 2023a.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko\u00adlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*, 2023b.\\n\\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. *arXiv preprint arXiv:2205.14100*, 2022a.\\n\\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In *ICML*, 2022b.\\n\\nWenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. *arXiv preprint arXiv:2305.11175*, 2023.\\n\\nYue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. *ACM Transactions on Graphics (tog)*, 38(5):1\u201312, 2019.\\n\\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. In *ICLR*, 2022c.\\n\\nJialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding. *arXiv preprint arXiv:2212.00280*, 2022.\\n\\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In *European Conference on Computer Vision*, pp. 521\u2013539. Springer, 2022.\\n\\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. *arXiv preprint arXiv:2304.14178*, 2023.\\n\\nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In *Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14*, pp. 69\u201385. Springer, 2016.\\n\\nLicheng Yu, Hao Tan, Mohit Bansal, and Tamara L Berg. A joint speaker-listener-reinforcer model for referring expressions. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pp. 7282\u20137290, 2017.\"}"}
{"id": "2msbbX3ydD", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular attention network for referring expression comprehension. In CVPR, 2018.\\n\\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning. arXiv preprint arXiv:2309.02591, 2023.\\n\\nYuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection with multimodal large language models. arXiv preprint arXiv:2305.18279, 2023.\\n\\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6720\u20136731, 2019.\\n\\nHaotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. Advances in Neural Information Processing Systems, 35:36067\u201336080, 2022.\\n\\nShilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601, 2023.\\n\\nYang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. Bubogpt: Enabling visual grounding in multi-modal llms. arXiv preprint arXiv:2307.08581, 2023.\\n\\nLuowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason J Corso, and Marcus Rohrbach. Grounded video description. In CVPR, 2019.\\n\\nYuanen Zhou, Meng Wang, Daqing Liu, Zhenzhen Hu, and Hanwang Zhang. More grounded image captioning by distilling image-text matching model. In CVPR, 2020.\\n\\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\\n\\nWanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939, 2023.\\n\\nXueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\"}"}
{"id": "2msbbX3ydD", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Results on the object hallucination benchmark using the POPE evaluation pipeline (Li et al., 2023).\\n\\n| Datasets     | Metrics    | Ferret | Shikra | InstructBLIP | MiniGPT4 | LLaV A | MM-GPT | mPLUG-Owl |\\n|--------------|------------|--------|--------|--------------|----------|--------|--------|-----------|\\n|              |            | Accuracy (\u2191) | 90.24  | 86.90  | 88.57  | 79.67  | 50.37  | 50.10  |\\n|              |            | Precision (\u2191) | 97.72  | 94.40  | 84.09  | 78.24  | 50.19  | 50.05  |\\n|              |            | Recall (\u2191)   | 83.00  | 79.26  | 95.13  | 82.20  | 99.13  | 100.00 |\\n|              |            | F1 Score (\u2191) | 89.76  | 86.19  | 89.27  | 80.17  | 66.64  | 66.71  |\\n|              | Yes        | Yes    | 43.78  | 43.26  | 56.57  | 52.53  | 98.77  | 99.90  |\\n\\nTable 8: Comparison of Ferret v.s. recent MLLMs integrating spatial awareness. 'Convention' refers to a comprehensive collection of publicly available data that has been transformed using templates, 'GPT-Generate' signifies the generated refer/ground datasets employing GPT, and 'Robustness' denotes datasets aimed at mitigating hallucination and improving robustness. Section 3 explains more details about each.\\n\\n| Model Input Types | Output Grounding | Data Construction | Quantitative Evaluation of Refer/Ground | w. Chat Point Box | Free-form | Convention | GPT-Generate | Robustness |\\n|-------------------|-------------------|-------------------|----------------------------------------|------------------|-----------|------------|-------------|------------|\\n| BuboGPT           | \u2717                 | \u2717                 | \u2717                                      | \u2714                | \u2714         | \u2717          | \u2717           | \u2717          |\\n| Vision-LLM        | \u2717                 | \u2717                 | \u2717                                      | \u2714                | \u2714         | \u2717          | \u2717           | \u2717          |\\n| Kosmos-2          | \u2717                 | \u2714                 | \u2717                                      | \u2714                | \u2714         | \u2717          | \u2717           | \u2717          |\\n| Shikra            | \u2714                 | \u2714                 | \u2717                                      | \u2714                | \u2714         | \u2714          | \u2717           | \u2717          |\\n| GPT4-ROI          | \u2717                 | \u2714                 | \u2717                                      | \u2717                | \u2714         | \u2717          | \u2717           | \u2717          |\\n| PVIT              | \u2717                 | \u2714                 | \u2717                                      | \u2714                | \u2714         | \u2717          | \u2717           | \u2717          |\\n| Ferret            | \u2714                 | \u2714                 | \u2714                                      | \u2714                | \u2714         | \u2714          | \u2714           | \u2714          |\\n\\nRelated work: Multimodal large language models (MLLMs).\\n\\nLarge Language Models (LLMs), including GPTs (Brown et al., 2020; OpenAI, 2023a), PaLM (Chowdhery et al., 2022), BLOOM (Scao et al., 2022), and LLaMA (Touvron et al., 2023a;b), have revolutionized research in NLP, spurring significant advances in multimodal language models as well. Early models primarily focused on large-scale image-text pre-training. Notable examples include SimVLM (Wang et al., 2022c), GIT (Wang et al., 2022a), PaLI (Chen et al., 2022b), PaLI-X (Chen et al., 2023c), BLIP-2 (Li et al., 2023c), Flamingo (Alayrac et al., 2022), PaLM-E (Driess et al., 2023), CM3 (Aghajanyan et al., 2022), and CM3Leon (Yu et al., 2023). Flamingo, in particular, pioneered the integration of a pre-trained CLIP image encoder with LLMs through gated cross-attention blocks, showcasing emergent multimodal in-context few-shot learning capabilities. Its open-sourced variants, such as OpenFlamingo (Awadalla et al., 2023) and IDEFICS (Laurenc\u00b8on et al., 2023), have garnered significant attention. Typically, these models undergo pre-training using millions or even billions of image-text pairs and interleaved image-text datasets (Zhu et al., 2023b).\\n\\nOn the other hand, recent research has increasingly focused on using pre-trained LLMs for visual instruction tuning. Prominent examples include LLaV A (Liu et al., 2023b), MiniGPT-4 (Zhu et al., 2023a), mPLUG-Owl (Ye et al., 2023), Otter (Li et al., 2023a), InstructBLIP (Dai et al., 2023), to name a few. In addition to text generation, recent models like FROMAGe (Koh et al., 2023b), GILL (Koh et al., 2023a), Emu (Sun et al., 2023), have also enabled MLLMs for image retrieval and image generation. Please refer to Chapter 5 of Li et al. (2023b) for a detailed review.\"}"}
{"id": "2msbbX3ydD", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Ferret enables referring and grounding capabilities for multimodal large language model (LLM). In terms of referring, a user can refer to a region or an object in point, box, or any free-form shape. The region\\\\textsubscript{N} in the input will be replaced by the proposed hybrid representation before being fed into the LLM. In terms of grounding, Ferret is able to accurately ground any open-vocabulary descriptions. The box\\\\textsubscript{N} in the output denotes the predicted bounding box coordinates.\\n\\nABSTRACT\\n\\nWe introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive referring-and-grounding instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with 95K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination. Code and data are available at https://github.com/apple/ml-ferret.\\n\\nWork done during an internship at Apple.\\n\\n\u2020 Equal contribution.\"}"}
{"id": "2msbbX3ydD", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"INTRODUCTION\\n\\nIn vision-language learning, how to enable spatial understanding in models is a fundamental research problem. Two desired capabilities stem from this problem: referring and grounding. Referring demands that the model can accurately comprehend the semantics of specific given regions (Krahmer & Van Deemter, 2012; Kazemzadeh et al., 2014; Mao et al., 2016; Yu et al., 2016; Zellers et al., 2019), whereas grounding necessitates that the model to localize the region in accordance with the given semantic description (Luo & Shakhnarovich, 2017; Nagaraja et al., 2016; Yu et al., 2017; Kamath et al., 2021).\\n\\nEssentially, referring and grounding demand the same type of knowledge: alignment of spatial information and semantics. Despite this, existing works mostly learn referring and grounding individually (Li et al., 2022; Wu et al., 2022; Yu et al., 2017). In comparison, humans can learn from one task and generalize the shared knowledge to the other task effortlessly, and are able to seamlessly integrate referring/grounding capabilities with daily dialogue and reasoning (Zellers et al., 2019).\\n\\nInspired by the above gap, in this paper, we study three main questions: (i) How to unify referring and grounding in one framework, and will they benefit each other? (ii) How to represent versatile types of regions that humans usually use for referring, such as point, box, scribble, and even free-form shapes? (iii) How to make referring and grounding open-vocabulary, instruction-following, and robust, which are crucial for practical applications?\\n\\nTargeting these three questions, we introduce Ferret, a novel refer-and-ground Multimodal Large Language Model (MLLM). First of all, we choose MLLM as the bedrock of Ferret to leverage their powerful vision-language global understanding capability (Zhu et al., 2023a; Liu et al., 2023b; Li et al., 2023c). To unify referring and grounding, Ferret first represents the coordinates of regions in natural language numerical form, as illustrated in Figure 3. However, it is inefficient to use single point or box coordinates to represent versatile shapes of regions, such as strokes, scribbles, or complex polygons. These shapes are essential for more universal and precise human-model interaction.\\n\\nTo solve this problem, we further propose a spatial-aware visual sampler to acquire the visual features for regions in any shape, taking care of the varying sparsity in those shapes. Then, the discrete coordinates and the continuous visual features are combined together to represent the visual regions in the input, composing a hybrid region representation in Ferret. Equipped with above methods, Ferret can deal with input that mixes referred regions with free-form text, and is able to ground the mentioned objects in its output by seamlessly generating the coordinates for each groundable object along with generating text. To our best knowledge, Ferret is the first work that is able to process free-formed region inputs in MLLMs.\\n\\nIn order to make the refer-and-ground capability in Ferret open-vocabulary, instruction-following, and robust, we collect GRIT, a Ground-and-Refer Instruction-Tuning dataset with 1.1M samples. GRIT contains multiple levels of spatial knowledge, covering objects, relationships, region descriptions, and complex reasoning. It includes both text-in location-out (grounding) and location-in text-out (referring) data, as well as data that mixes location and text in both input and output. The majority of the dataset is converted from existing vision(-language) tasks like object detection (Krishna et al., 2017) and phrase grounding (Yu et al., 2016; Plummer et al., 2015) with carefully designed templates to make it instruction-following. Additionally, 34K refer-and-ground instruction-tuning conversations are collected via the help of ChatGPT/GPT-4 (OpenAI, 2023b) to facilitate training an instruction-following and open-vocabulary refer-and-ground generalist. Moreover, we conduct spatial-aware negative data mining, which further promotes model robustness.\\n\\nFerret subsumes strong open-vocabulary capabilities of spatial understanding and localization. When evaluated on conventional referring and grounding tasks, it achieves superior performance. More than that, we believe refer-and-ground capabilities should be integrated into daily conversations of humans, e.g., people refer to something they don't know and ask what it is used for (like Figure 1). To evaluate this new capability, we introduce Ferret-Bench, covering three new types of tasks: Referring Description, Referring Reasoning, and Grounding in Conversation. We benchmark existing MLLMs and observe that Ferret can outperform the best of them by 20.4% on average. Moreover, Ferret demonstrates an intriguing property of alleviating object hallucinations.\\n\\nIn summary, our contributions are threefold. (i) We propose Ferret, that uses a hybrid region representation equipped with a novel spatial-aware visual sampler, to enable fine-grained and open-vocabulary spatial understanding. Note that there is no additional vocabulary or position encoders introduced in Ferret model. 2\"}"}
{"id": "2msbbX3ydD", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We construct GRIT, a large-scale ground-and-refer instruction tuning dataset, for model training. It also contains additional spatial negative samples to enhance model robustness. We introduce Ferret-Bench, to evaluate tasks jointly requiring referring/grounding, semantics, knowledge, and reasoning. Our model exhibits superior performance in a wide range of tasks and reduces object hallucination.\\n\\n2. Method\\n\\nWe start with detailing the proposed hybrid region representation to depict regions of various shapes and formats. Then, we present the model architecture of Ferret.\\n\\n2.1 Hybrid Region Representation\\n\\nThe object \\\\[\\\\text{obj0}\\\\] is a pistol, and the object \\\\[\\\\text{obj1}\\\\] is a knife. What is the object \\\\[\\\\text{region0}\\\\] and \\\\[\\\\text{region1}\\\\]?\\n\\nFigure 2: Bounding box v.s. Free-form Shape. These two objects have almost the same bounding box, causing ambiguity when relying on the box to refer to. Equipped with hybrid region representation, Ferret can separate them.\\n\\nWhen referring to specific regions, three primary formats are generally used: point, box, and free-form shapes. While the point and box formats can be succinctly represented by coordinates (e.g., \\\\([x, y]\\\\) for a point, \\\\([x_{\\\\text{min}}, y_{\\\\text{min}}, x_{\\\\text{max}}, y_{\\\\text{max}}]\\\\) for a box) as in Peng et al. (2023); Chen et al. (2023b), the free-form shape is more versatile, encompassing a variety of region types such as scribbles, polygons, and masks. The advantage of free-form shape is straightforwardly illustrated in Figure 2. Depicting free-form shapes through coordinates is computationally expensive and obscure, and its complexity hinders the model learning to establish a clear correlation between the provided coordinates and the corresponding regions.\\n\\nTo generalize across all three distinct formats, we propose a hybrid region representation that synergizes discrete coordinates with continuous visual features to refer to a particular region, which is shown in the top-left of Figure 3. For coordinates, following Chen et al. (2021); Yang et al. (2022), we quantize each coordinate into one of the \\\\(n\\\\) bins by default. The value is input invariant, which means for any input image size, the original coordinate will be mapped to the new coordinates. This makes the model robust to different input resolutions.\\n\\nRegarding continuous visual features, for a given region \\\\(R\\\\), we first construct a 2D binary mask \\\\(M\\\\) of the same size as the image, marking a value of 1 inside the targeted region and 0 outside of the region. Then, the binary mask \\\\(M\\\\), jointly with the extracted image feature map \\\\(Z\\\\), is sent into our proposed spatial-aware visual sampler \\\\(s(\\\\cdot)\\\\), which will be detailed in Section 2.2, to extract the visual continuous feature \\\\(f = s(M, Z)\\\\). Finally, we represent a point with \\\\(\\\\{x, y, f_R^p\\\\}\\\\), where the region \\\\(R^p\\\\) is a circle centered in \\\\(\\\\{x, y\\\\}\\\\) with a fixed radius.\\n\\nA box or a free-form shape can both be represented by \\\\(\\\\{x_{\\\\text{min}}, y_{\\\\text{min}}, x_{\\\\text{max}}, y_{\\\\text{max}}, f_R^{\\\\text{box}}\\\\}\\\\), where \\\\(x_{\\\\text{min}}/x_{\\\\text{max}}\\\\) denotes the minimum/maximum x-axis coordinate of the region, and so forth for y-axis. \\\\(R^{\\\\text{box}}\\\\) denotes the input region.\\n\\n2.2 Model Architecture\\n\\nAs illustrated in Figure 3, Ferret is mainly composed of (i) an image encoder to extract image embeddings, (ii) the proposed spatial-aware visual sampler to extract regional continuous features, and (iii) an LLM to jointly model image, text, and region features.\\n\\nInput. We feed the image into a pre-trained visual encoder, CLIP-ViT-L/14 (Radford et al., 2021), to extract the image embeddings \\\\(Z \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C}\\\\). For text, we tokenize the text sequence using the pre-trained LLM\u2019s tokenizer and project them into text embeddings \\\\(T \\\\in \\\\mathbb{R}^{L \\\\times D}\\\\). As for referred regions, we append the coordinates and a special token as a placeholder for continuous features after the name of the region: \\\\(\\\\langle\\\\text{name}\\\\rangle \\\\langle\\\\text{coordinates}\\\\rangle \\\\langle\\\\text{SPE}\\\\rangle\\\\). For example, \\\\(\"\\\\text{a cat [100, 50, 200, 300] \\\\langle\\\\text{SPE}\\\\rangle\"}\\\\). If the name is unknown or hard to describe because multiple objects are included, we just use \\\\(\\\\langle\\\\text{region}\\\\rangle\\\\) or \\\\(\\\\langle\\\\text{area}\\\\rangle\\\\) as the \\\\(\\\\langle\\\\text{name}\\\\rangle\\\\). In this way, referred regions can be well mixed with ordinary texts to form complete sentences.\\n\\n\\\\(n\\\\) bins = 1000 by default. The value is input invariant, which means for any input image size, the original coordinate will be mapped to the new coordinates. This makes the model robust to different input resolutions.\\n\\n\\\\(3\\\\) Radius is set to 5 by default.\"}"}
{"id": "2msbbX3ydD", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Performance comparison (Acc@0.5) on the referring expression comprehension (RefCOCO, RefCOCO+, RefCOCOg) and phrase grounding (Flickr30k Entities) tasks. \u2217 indicates that the method is specifically fine-tuned in the second stage.\\n\\n| Models          | RefCOCO  | RefCOCO+ | RefCOCOg | Flickr30k Entities |\\n|-----------------|----------|----------|----------|-------------------|\\n|                 | val      | testA    | testB    | val               | test   |\\n| MAttNet (Yu et al., 2018) | 76.40    | 80.43    | 69.28    | 64.93             | 70.26  |\\n|                  | 66.67    |          |          |                   | 67.01  |\\n| OFA-L (Wang et al., 2022b) | 79.96    | 83.67    | 76.39    | 68.29             | 76.00  |\\n|                  | 67.57    |          |          |                   | 67.58  |\\n| TransVG (Deng et al., 2021) | 81.02    | 82.72    | 78.35    | 64.82             | 70.70  |\\n|                  | 68.67    |          |          |                   | 67.73  |\\n| UNITER (Chen et al., 2020) | 81.41    | 87.04    | 74.17    | 75.90             | 81.45  |\\n|                  |          |          |          |                   | 66.70  |\\n| VILLA (Gan et al., 2020) | 82.39    | 87.48    | 74.84    | 76.17             | 81.54  |\\n|                  |          |          |          |                   | 66.84  |\\n| UniTAB (Yang et al., 2022) | 86.32    | 88.84    | 80.61    | 78.70             | 83.22  |\\n|                  |          |          |          |                   | 79.10  |\\n| MDETR (Kamath et al., 2021) | 86.75    | 89.58    | 81.41    | 79.52             | 84.09  |\\n|                  |          |          |          |                   | 81.64  |\\n| Shikra-7B (Chen et al., 2023b) | 87.01    | 90.61    | 80.24    | 81.60             | 87.36  |\\n|                  |          |          |          |                   | 72.12  |\\n| Ferret-7B        |          |          |          |                   |        |\\n| Shikra-13B (Chen et al., 2023b) | 87.83    | 91.11    | 81.81    | 82.89             | 87.79  |\\n|                  |          |          |          |                   | 74.41  |\\n| Ferret-13B       |          |          |          |                   |        |\\n\\nPhrases in the input sentence with corresponding boxes, requiring the model to predict these boxes and the word-box connections. For both tasks, we utilize uniform prompts, represented as \\\"What are the locations of \\\\(<query>\\\\) / \\\\(<phrases>\\\\)\\\\)?\\\", where \\\\(<query>\\\\) denotes the textual referring expression, while \\\\(<phrases>\\\\) stands for a \\\"comma-delimited\\\" aggregation of the given phrases. The model is trained to output in \\\"\\\\(<query>\\\\) [box].\\\" format. The generated bounding box is considered correct if its intersection over union (IoU) with the GT box is greater than 0.5. As shown in Table 3, Ferret achieves an outstanding performance on all metrics, and is comparable to specialized fine-tuning approaches (Kamath et al., 2021).\\n\\nGrounded captioning. The grounded captioning task requires the model to generate a caption and ground all generated noun phrases to image regions. The final predictions generally consist of three parts, i.e., the text caption, visual regions as boxes, and the grounding alignments between words and boxes. Following the established benchmarks on the Flickr30k Entities dataset, we evaluate captioning and grounding separately with the captioning metrics and grounding F1 scores, respectively. F1_all evaluates grounding as a multi-label classification problem. We also report F1_loc that only computes the grounding score on correctly predicted object words. Results are summarized in Table 2, and Ferret achieves state-of-the-art.\\n\\n4.3 Ferret-Bench: Multimodal Chatting with Referring and Grounding\\n\\nMultimodal chatting has been an emergent ability of MLLMs. Previous benchmarks (Liu et al., 2023b) mainly evaluate conversation, detailed description, and complex reasoning via GPT-4 as a judge. Yet, a gap exists as no dataset currently evaluates multimodal chatting that necessitates referring or grounding actions, e.g., instances where individuals reference an unfamiliar object and inquire about its purpose. To benchmark this intriguing and practical capability, we introduce Ferret-Bench that covers three kinds of region-based questions evaluating referring and grounding capability: (i) Referring Description: models are asked to describe a referred region based on its interaction with surrounding objects. (ii) Referring Reasoning: models need to reason on top of one or more referred regions correctly. (iii) Grounding in Conversation: models are required to reason correctly and accurately ground/localize the objects/regions necessary for the reasoning. For the ease of benchmarking other methods, we represent the regions with boxes instead of points or free-form shapes.\\n\\nSpecifically, we randomly sample 40 images from the COCO validation set for each type of question, and generate the questions and GPT-4's answers following the instruction generation pipeline in Sec. 3.2. Following Liu et al. (2023b), we feed the question and image into MLLMs to obtain the predicted answer, and then prompt GPT-4 to rate the predicted answer and pseudo answer from GPT-4 based on the ground-truth textual scene description (object, relationship, region caption, global caption). GPT-4 evaluates both the precision of referring understanding, object grounding, and correctness of semantics. The rating score ranges from 1 to 10, in which higher means better. We calculate the ratio of the predicted answer's score and the GPT-4 answer's score, which is then presented as a percentage to measure the performance of MLLMs. We also asked GPT-4 to give a comprehensive review for the rating and found that GPT-4 is good at measuring the degree of spatial precision, such as how much the predicted bounding box diverges from the GT box coordinate. We refer the readers to Appendix D for further elaboration.\"}"}
{"id": "2msbbX3ydD", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Results on LLaV A-Bench and the proposed Ferret-Bench via GPT4-as-a-Judge evaluation.\\n\\n|                      | LLaV A-Bench | Ferret-Bench |\\n|----------------------|--------------|--------------|\\n|                      | Conversation | Referring    | In Avg. | Reasoning | Conversation |\\n|                      | Detail       | Description  | Reasoning |\\n| LLaV A               | 8            | 85.4         | 68.3     | 92.1      | 81.9         |\\n| Kosmos-2             | 71.7         | 51.8         | 33.7     | 48.4      | 44.6         |\\n| Shikra-7B            | 80.6         | 46.0         | 41.6     | 50.1      | 45.9         |\\n| Ferret-7B            | 84.4         | 68.7         | 67.3     | 57.5      | 64.5         |\\n| Ferret-13B           | 85.2         | 70.6         | 68.7     | 59.7      | 66.3         |\\n\\nTable 5: Ablation study on the mutual benefit of grounding data and referring data. We evaluate Accuracy for LVIS referring and R@1 for grounding.\\n\\n| Model                  | Referring (LVIS) | Grounding |\\n|------------------------|------------------|-----------|\\n|                        | Point Box        |           |\\n| Flickr30k              | 67.9             | 79.4      |\\n| Ferret                 | 80.4             |           |\\n| w/o Grounding data     | 65.4             | \u2715         |\\n| w/o Referring data     | \u2715                | 79.8      |\\n\\nTable 6: Ablation study on the effectiveness of the proposed spatial-aware visual sampler. Accuracy is used to evaluate LVIS referring.\\n\\n| Module                  | Referring (LVIS) |\\n|-------------------------|------------------|\\n|                        | Point Box        |\\n|                        | Free-form        |\\n|                        | Spatial-aware Visual Sampler |\\n|                        | 67.9             | 69.8       |\\n|                        | 67.1             | 68.9       |\\n\\nWe use LLaV A-Bench (Liu et al., 2023b) and the proposed Ferret-Bench to compare Ferret with previous models, including LLaV A (Liu et al., 2023b), Shikra (Chen et al., 2023b), and Kosmos-2 (Peng et al., 2023). Results are summarized in Table 4. Ferret achieves superior performance in all types of tasks, boosting the score for the detailed description category from 68.3 to 80.9, and especially excels at the three new tasks demanding referring and grounding abilities.\\n\\n4.4 ABLATION\\n\\nIn the ablation studies below, in default, we ablate Ferret-7B and mainly evaluate in referring object classification and grounding tasks on Flickr30k Entities validation set.\\n\\nMutual benefits of grounding and referring.\\n\\nAs shown in Table 5, grounding and referring, as two main capabilities emphasized in this paper, can actually benefit each other. Particularly, when adding grounding data into training, the referring performance gets improved, and vice versa.\\n\\nSpatial-aware Visual Sampler.\\n\\nWe ablate the effectiveness of the spatial-aware visual sampler by replacing it with the visual sampler in SEEM (Zou et al., 2023), where they average the features of all the sampled points as the region feature. As we can see in Table 6, ours can outperform the previous visual sampler in all three referring tasks.\\n\\nLLM model size.\\n\\nWe study how much LLM model size influences the performance of referring and grounding. As seen in Table 1-4, having a larger LM backbone can generally help.\\n\\n4.5 OBJECT HALLUCINATION\\n\\nAttribute to the incorporation of fine-grained spatial knowledge and negative mining, Ferret also exhibits strong power against the hallucination problem. We evaluate object hallucinations on the POPE benchmark (Li et al., 2023e). Results are summarized in Table 7. Ferret has exhibited performance comparable to Shikra (Chen et al., 2023b), and far surpasses recent popular MLLMs.\\n\\nCONCLUSION\\n\\nWe present Ferret, a new multimodal large language model adept at referring and grounding. Ferret can refer image regions in any free-form shape, and automatically establish grounding for text deemed groundable by the model. We have curated the GRIT dataset for model training, and the Ferret-Bench dataset for evaluation. Ferret, like most MLLMs, may produce harmful and counterfactual responses. For future work, inspired by LISA (Lai et al., 2023), we plan to enhance Ferret to be able to output segmentation masks in addition to bounding boxes.\"}"}
{"id": "2msbbX3ydD", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multi-modal model of the internet.\\narXiv preprint arXiv:2201.07520, 2022.\\n\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning.\\narXiv preprint arXiv:2204.14198, 2022.\\n\\nAnas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. URL https://doi.org/10.5281/zenodo.7733589.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\nChi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, and Yang Liu. Position-enhanced visual instruction tuning for multimodal large language models.\\narXiv preprint arXiv:2308.13437, 2023a.\\n\\nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic.\\narXiv preprint arXiv:2306.15195, 2023b.\\n\\nTing Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language modeling framework for object detection.\\narXiv preprint arXiv:2109.10852, 2021.\\n\\nTing Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey Hinton. A unified sequence interface for vision tasks.\\narXiv preprint arXiv:2206.07669, 2022a.\\n\\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model.\\narXiv preprint arXiv:2209.06794, 2022b.\\n\\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a multilingual vision and language model.\\narXiv preprint arXiv:2305.18565, 2023c.\\n\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In ECCV, 2020.\\n\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna.lmsys.org (accessed 14 April 2023), 2023.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways.\\narXiv preprint arXiv:2204.02311, 2022.\\n\\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning.\\narXiv preprint arXiv:2305.06500, 2023.\\n\\nJiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, and Houqiang Li. Transvg: End-to-end visual grounding with transformers. In ICCV, 2021.\\n\\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied multi-modal language model.\\narXiv preprint arXiv:2303.03378, 2023.\"}"}
{"id": "2msbbX3ydD", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. NeurIPS, 2020.\\n\\nAgrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5356\u20135364, 2019.\\n\\nKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 2961\u20132969, 2017.\\n\\nAishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1780\u20131790, 2021.\\n\\nSahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 787\u2013798, 2014.\\n\\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.\\n\\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language models. arXiv preprint arXiv:2305.17216, 2023a.\\n\\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal generation. arXiv preprint arXiv:2301.13823, 2023b.\\n\\nEmiel Krahmer and Kees Van Deemter. Computational generation of referring expressions: A survey. Computational Linguistics, 38(1):173\u2013218, 2012.\\n\\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32\u201373, 2017.\\n\\nXin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. arXiv preprint arXiv:2308.00692, 2023.\\n\\nHugo Laurenc\u00b8on, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. Obelisc: An open web-scale filtered dataset of interleaved image-text documents. arXiv preprint arXiv:2306.16527, 2023.\\n\\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023a.\\n\\nChunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020, 2023b.\\n\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023c.\\n\\nLei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M"}
