{"id": "Wc5bmZZU9cy", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 13 shows the EM accuracy of SOTA models on pre-perturbation and post-perturbation data in Dr.Spider. EM accuracy does not evaluate value predictions in SQLs. Therefore, EM is not effective to measure model robustness against NonDB-number, DB-text, and DB-text. Table 14, 15 contains the relative robustness accuracy on EX and EM of SOTA text-to-SQL models. Relative robustness accuracy ignores the examples that the model predicts wrong before perturbations for each model.\\n\\nTable 11 contains the subcategories and examples in DBcontent-equivalence. DBcontent-equivalence simulate a scenario where a column content is represented in different formats by replacing a column with one or multiple semantic-equivalent columns. Table 12 contains prediction examples of PICARD on DBcontent-equivalence perturbation data. PICARD predicts SQLs correctly for all four examples before perturbations. The first two examples both replace column age with birthyear, however, PICARD defenses against one perturbation example but fails on the other. The third example replaces a fix-value text column pettype with two boolean columns is dog and is cat. PICARD fails to predict the right value for the boolean columns is dog. The last example replaces a compound column name with two columns firstname and lastname. PICARD refer both firstname and lastname when the NLQ asks for \u201cnames\u201d. We believe that DBcontent-equivalence requires models to have additional common-sense reasoning to understand the relation between a database column and how it is represented in NLQ. This phenomenon is common in reality when the actual users are not aware of DB schemas.\\n\\n| Category                | Column Example                  |\\n|-------------------------|---------------------------------|\\n| Single text to multiple texts | name = firstname, lastname     |\\n| Boolean to text         | is male => gender               |\\n| Text to boolean         | hand => is right hand           |\\n| Single text to multiple booleans | degree summary => is bachelor, is master, is PhD |\\n| Number to number        | age => birthyear                |\\n\\nTable 11: Categories and examples in the DBcontent-equivalence perturbation.\\n\\nThe effectiveness of entity linking between NLQ and DB content Figure 7 shows the relative robustness accuracy on EX and EM of T5-3B models with and without the entity linking feature. Using the entity linking significantly improves model robustness against column-value and value-synonym perturbations in terms of EX while it gives less robustness on column-attribute and column-carrier perturbations (both EX and EM). The EX and EM accuracy on the value-synonym perturbation are different. The entity linking benefits the SQL value prediction by giving how the value is presented in the DB especially when the value appears in a different format in NLQ (value-synonym). However, this feature slightly hurts the SQL prediction in terms of EM accuracy (ignoring values) on value-synonym (66.2 vs 67.2). We believe that it is because the model overfits the string-matching feature to indicate the mentioned column and does not generalize well when the matching fails due to perturbation. Therefore, it is beneficial to provide the formats of mentioned values in the DB via the linking. And a better linking mechanism should be adapted for developing robust text-to-SQL models.\\n\\nRobustness Evaluation on in-context learning models\\nBesides the models that are trained on the Spider dataset, we also evaluate the robustness of a large language model, CODEX Chen et al. (2021), for in-context learning. We use CODEX DAVINCI which finetunes GPT-3 Brown et al. (2020) on GitHub codes. We choose the Create Table + Select 3 prompt from Rajkumar et al., 2022), as it achieves the best performance with in-context learning (74.0 execution accuracy on the original Spider development set based on our implementation), which is even comparable with the SOTA finetuned model, PICARD (79.3 execution accuracy). Table 16 shows the comparison between PICARD and CODEX on their pre-perturbation, post-perturbation accuracy, and relative robustness accuracy. We have a few findings: (1) In-context CODEX is more robust than finetuned PICARD.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nExample NLQ:\\n\\nWhat are the names and release years for all the songs of the youngest singer?\\n\\nGold SQL:\\n\\n...ORDER BY age LIMIT 1\\n\\nColumn Substitution:\\n\\nage -> birthyear\\n\\nGold SQL after perturbation:\\n\\n...ORDER BY birthyear DESC LIMIT 1\\n\\nPredicted SQL after perturbation:\\n\\n...ORDER BY age LIMIT 1\\n\\nIncorrect NLQ:\\n\\nFind the type and weight of the youngest pet.\\n\\nGold SQL:\\n\\n...ORDER BY pet\\n\\nColumn Substitution:\\n\\nage -> birthyear\\n\\nGold SQL after perturbation:\\n\\n...ORDER BY pet birthyear DESC LIMIT 1\\n\\nPredicted SQL after perturbation:\\n\\n...ORDER BY pet birthyear DESC LIMIT 1\\n\\nCorrect NLQ:\\n\\nHow many dog pets are raised by female students?\\n\\nGold SQL:\\n\\n...WHERE Pets.pettype = 'dog'...\\n\\nColumn Substitution:\\n\\npettype -> is dog\\n\\nGold SQL after perturbation:\\n\\n...WHERE Pets.is dog = True...\\n\\nPredicted SQL after perturbation:\\n\\n...WHERE Pets.is dog = 'D'...\\n\\nIncorrect NLQ:\\n\\nList all singer names in concerts in year 2014.\\n\\nGold SQL:\\n\\nSELECT name FROM...\\n\\nColumn Substitution:\\n\\nname -> first name, last name\\n\\nGold SQL after perturbation:\\n\\nSELECT first name, last name FROM...\\n\\nPredicted SQL after perturbation:\\n\\nSELECT first name, last name FROM...\\n\\nCorrect\\n\\nTable 12: Prediction examples of PICARD on column-equivalence perturbation data.\\n\\nto database perturbations even though it still suffers a performance drop from 72.6 to 60.7, and (2) in-context CODEX does not show better robustness against NLQ perturbations than finetuned PICARD.\\n\\nWe hypothesize that the robustness of CODEX is largely related to its training data. As the training data are obtained from GitHub codes, CODEX has seen a diverse distribution of databases. Specifically, we find that CODEX has a limited performance drop to Schema-abbreviation perturbation (from 70.2 to 68.6), compared to PICARD (from 74.9 to 64.7). We believe that CODEX is robust to the Schema-abbreviation perturbation, as it has seen a large number of abbreviation schemas in its training data.\\n\\nIn NLQ perturbations, we find two categories that are particularly challenging to CODEX: Column-carrier (from 80.8 to 51.1), and Column-attribute (from 68.9 to 46.2). Those perturbations implicitly mention a column with the question carrier or the attribute of the column. For example, In Table 1, the column \u201cname\u201d in \u201cShow the name of teacher\u201d is implied by \u201cWhich teachers\u201d and the column \u201cyear\u201d in \u201cworked the greatest number of year\u201d is implied by \u201cworked the longest\u201d. CODEX was trained with the goal of generating code from docstrings. The language style of common docstrings is slightly different from the style of spoken language therefore CodeX may not be familiar with such implicit mentions. As the CodeX details are not fully published, the analysis is subjective.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Category                        | Pre-Perturbation | Post-Perturbation | Absolute Robustness |\\n|--------------------------------|------------------|-------------------|---------------------|\\n| Spider-dev                     | 69.5             | 73.4              | 74.7                |\\n| DB-synonym                     | 63.2             | 41.1              | 22.1                |\\n| Schema-synonym                 | 67.1             | 49.4              | 17.7                |\\n| Schema-abbreviation            | 69.3             | 54.1              | 15.2                |\\n| DB-content-equivalence          | 81.2             | 5.8               | 75.4                |\\n| Average                        | 69.9             | 29.0              | 40.9                |\\n| Keyword-synonym                | 63.4             | 49.3              | 14.1                |\\n| Keyword-carrier                | 79.2             | 76.2              | 13.0                |\\n| Column-synonym                 | 67.9             | 47.8              | 20.1                |\\n| Column-carrier                 | 74.8             | 55.3              | 19.5                |\\n| Column-attribute               | 51.3             | 34.5              | 16.8                |\\n| Column-value                   | 78.0             | 55.3              | 22.7                |\\n| Value-synonym                  | 60.3             | 38.1              | 22.2                |\\n| Sort-order                     | 64.6             | 65.6              | 1.0                 |\\n| NonDB-number                   | 66.4             | 67.2              | 0.8                 |\\n| DB-text                        | 63.6             | 65.0              | 1.4                 |\\n| Average                        | 64.7             | 64.9              | 0.2                 |\\n| All                            | 67.5             | 51.4              | 16.1                |\\n\\nTable 13: The exact set match (EM) accuracy of SOTA text-to-SQL models on the original Spider development set (Spider-dev) and Dr.Spider. * represents the model we trained with official codes as their models are not public. x\u2014y represents the pre-perturbation accuracy and post-perturbation accuracy (i.e. absolute robustness accuracy). Bold number is most highest absolute robustness accuracy in each category and underscores stand for the second best results. Note that the EM accuracy does not evaluate value predictions. Therefore, it is not effective to measure model robustness against NonDB-number, DB-text, and DB-text.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 14: The relative robustness accuracy on EX of SOTA text-to-SQL models on Dr.Spider. Bold number is most highest absolute robustness accuracy in each category and underscores stand for the second best results.\\n\\n|                  | Keyword-synonym | Keyword-carrier | Column-synonym | Column-carrier | Column-attribute | Column-value | Value-synonym | Multitype | Others |\\n|------------------|-----------------|-----------------|----------------|----------------|-----------------|--------------|---------------|----------|--------|\\n| **T5-3B**        | 64.6            | 73.5            | 46.1           | 58.5           | 62.8            | 67.2         | 74.6          |          |        |\\n| **T5-3B LK**     | 62.2            | 79.3            | 41.4           | 59.4           | 72.0            | 72.7         | 83.0          |          |        |\\n| **DBcontent-equivalence** | 11.8       | 42.6            | 27.1           | 44.0           | 44.1            | 46.1         | 49.3          |          |        |\\n| **Average**      | 46.2            | 65.1            | 38.2           | 54.0           | 59.6            | 62.0         | 69.0          |          |        |\\n| **NLQ**          |                 |                 |                |                |                 |              |               |          |        |\\n| **Keyword-synonym** |             |                 |                |                |                 |              |               |          |        |\\n| **Keyword-carrier** |             |                 |                |                |                 |              |               |          |        |\\n| **Column-synonym** |             |                 |                |                |                 |              |               |          |        |\\n| **Column-carrier** |             |                 |                |                |                 |              |               |          |        |\\n| **Column-attribute** |             |                 |                |                |                 |              |               |          |        |\\n| **Column-value**  |                 |                 |                |                |                 |              |               |          |        |\\n| **Value-synonym** |                 |                 |                |                |                 |              |               |          |        |\\n| **Multitype**    |                 |                 |                |                |                 |              |               |          |        |\\n| **Others**       |                 |                 |                |                |                 |              |               |          |        |\\n| **Average**      | 66.3            | 71.5            | 67.1           | 74.8           | 76.0            | 79.0         | 82.0          |          |        |\\n\\nFigure 7: Relative robustness accuracy on EX and EM of T5-3B models with and without question and DB content linking.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\\n\\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112\u20131122. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-1101.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\\n\\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In EMNLP, 2018.\\n\\nTao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin Wang, Yi Chern Tan, Xinyi Yang, Dragomir R Radev, Richard Socher, and Caiming Xiong. Grappa: Grammar-augmented pre-training for table semantic parsing. In ICLR, 2021.\\n\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\\n\\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To understand the linguistic phenomena that NLQs could have in a realistic scenario, we recruit annotators on Amazon Mechanical Turk to paraphrase 100 sampled questions in Spider and collect five paraphrases per question. Figure 4, 5 contain the instruction and interface on Amazon Mechanical Turk for crowdsourcing paraphrase collection. Annotators are required to rephrase a question in their own words which do not change the meaning of the question given the context. We provide a short description of the corresponding database as the context of a question.\\n\\nEach NLQ paraphrase category contains an individual prompt prefix which comprises 5 paraphrase examples and a question to be paraphrased, except for Multitype which includes examples that involve multiple phenomena selected from other categories. Each paraphrase example contains 4 parts: (1) an original sentence, (2) the mentioned SQL tokens that need to be rephrased or implied, (3) a paraphrased question, and (4) an explanation of how the question is paraphrased. We use OPT-66B to generate paraphrases given an original question. Inspired by Wei et al. (2022); Paranjape et al. (2021), we employ the OPT model to generate an explanation along with the paraphrase given an original question and the mentioned SQL tokens related to the category. Table 4 is the prompt prefix of the perturbation column-value. The prompt prefixes for other categories are available at https://github.com/awslabs/diagnostic-robustness-text-to-sql.\\n\\nAs Bandel et al. (2022) discovered, there is a trade-off between paraphrase diversity and factuality. We use multiple combinations of hyper-parameters in paraphrase generation to take both into account. For each question, we run the OPT model 4 times with the hyperparameters top_p in \\\\{0.9,1.0\\\\}. \\n\\n---\\n\\nPublished as a conference paper at ICLR 2023\\n\\nA.1 Crowdsourcing Paraphrase Study Interface\\n\\nFigure 4: The instructions for crowdsourcing paraphrase collection on Amazon Mechanical Turk.\\n\\nFigure 5: The interface for crowdsourcing paraphrase collection on Amazon Mechanical Turk. Annotators are given a free text box to input their paraphrased question.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.3 PARAPHRASE FILTERING AND QUALITY CONTROL\\n\\nWe use a simple and high-recall rule-based classifier to remove out-of-category paraphrases based on the alignment between SQL tokens and their indicators in NLQ. In particular, we use the annotations from Lei et al. (2020) to build the column and value indicators. And to obtain SQL keyword indicators, we collect the common indicators (covering over 80% examples) of each SQL keyword in the NLQs and programmatically label NLQs with those indicators. Table 5 contains the common indicators of SQL keywords in Spider. We filter out the paraphrases where all the indicators for SQL keywords, columns and values remain the same as the original NLQ except category others.\\n\\nTo ensure the data quality in Dr.Spider, we request three annotators with SQL expertise to carefully review paraphrases with gold SQL labels and select the final data. Table 6 contains the randomly sampled NLQ paraphrases in each category and the annotator's decision.\\n\\nTo measure the naturalness of our generated NLQ perturbations, we sample 500 questions from each of the original Spider development set, human paraphrases (we use all 270 factual paraphrases), and our perturbed questions. We hire crowdsourcing annotators on Amazon Mechanical Turk to score the clarity and fluency of sampled questions from 1 to 3. Clarity=1 represents that the annotator couldn't understand the question, clarity=2 indicates that the question is ambiguous to the annotator, and clarity=3 means that the question is clear. Fluency=1 represents that the question is full of grammar errors or typos, fluency=2 indicates the question has minor errors, and fluency=3 means the question is fluent without grammar errors or typos. Figure 6 is the interface for labeling the clarity and fluency of questions in the original Spider dataset, collected human paraphrases, and the questions in Dr. Spider.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SQL Keyword Indicator\\n\\n> more than, larger than, bigger than, higher than, above, after, older than, heavier than\\n\\n< less than, smaller than, lower than, below, before, younger than, lighter than\\n\\n>= at most, or more\\n\\n<= at least, or less\\n\\nbetween and\\n\\ncount() number, amount, count\\n\\nsum() total, amount\\n\\navg() average, mean\\n\\nmax() maximum, most, highest\\n\\nmin() minimum, least, lowest\\n\\nASC ascending, in alphabetical order, in lexicographical order, from youngest to oldest, from young to old, from low to high\\n\\nASC LIMIT least, lowest, smallest, youngest, earliest, shortest, minimum, fewest number, fewest amount\\n\\nDESC descending, in reverse alphabetical order, in reversed lexicographical order, from the oldest to the youngest, from old to young, from high to low\\n\\nDESC LIMIT most, highest, largest, oldest, latest, longest, maximum, greatest number, greatest amount\\n\\nTable 5: The common NLQ indicator tokens for SQL keywords.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neural text-to-SQL models have achieved remarkable performance in translating natural language questions into SQL queries. However, recent studies reveal that text-to-SQL models are vulnerable to task-specific perturbations. Previous curated robustness test sets usually focus on individual phenomena. In this paper, we propose a comprehensive robustness benchmark based on Spider, a cross-domain text-to-SQL benchmark, to diagnose the model robustness. We design 17 perturbations on databases, natural language questions, and SQL queries to measure the robustness from different angles. In order to collect more diversified natural question perturbations, we utilize large pretrained language models (PLMs) to simulate human behaviors in creating natural questions. We conduct a diagnostic study of the state-of-the-art models on the robustness set. Experimental results reveal that even the most robust model suffers from a 14.0% performance drop overall and a 50.7% performance drop on the most challenging perturbation. We also present a breakdown analysis regarding text-to-SQL model designs and provide insights for improving model robustness.\\n\\nINTRODUCTION\\n\\nLarge-scale cross-domain text-to-SQL datasets facilitate the study of machine learning models for generating a SQL query given a natural language question (NLQ) and corresponding database (DB) as input. Neural text-to-SQL models encode an NLQ and DB schema and decode the corresponding SQL (Wang et al., 2019; Lin et al., 2020; Scholak et al., 2021), which have achieved remarkable results on existing benchmarks (Zhong et al., 2017; Yu et al., 2018; Shi et al., 2020). However, those results are obtained in the setting where test data are created with the same distribution as training data. This setting prevents the evaluation of model robustness, especially when the data contain spurious patterns that do not exist in the wild. For example, previous studies (Suhr et al., 2020; Gan et al., 2021a; Deng et al., 2021) have found spurious patterns in the Spider (Yu et al., 2018), a widely used cross-domain text-to-SQL benchmark, such as NLQ tokens closely matching DB schemas, leading models to rely on lexical matching between NLQs and DB schemas for prediction instead of capturing the semantics that the task is intended to test.\\n\\nFigure 1 shows examples where the state-of-the-art (SOTA) text-to-SQL models are vulnerable to perturbations. (1) DB perturbation: replacing the column name \\\\textit{winner} with \\\\textit{champ} leads the model to miss the intent of \u201c3 youngest winners\u201d; (2) NLQ perturbation: the model confuses the selected column \\\\textit{winner} with \\\\textit{winner age} given a paraphrased NLQ which uses \u201cWho\u201d to imply the selected column; (3) SQL perturbation: a simple change to the number of returned items (from \\\\texttt{LIMIT 3} to \\\\texttt{LIMIT 8}) fails the model to detect the right intent. Recent studies created data to reveal the robustness problem of text-to-SQL models via perturbing DBs or NLQs (Ma & Wang, 2021; Pi et al., 2022; Gan et al., 2021a; Deng et al., 2021). However, they usually focus on individual linguistic phenomena and rely on rule-based methods or a few\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Find the name and rank of the 3 youngest winners across all matches.\\n\\nSELECT winner_name, winner_rank FROM matches ORDER BY winner_age LIMIT 3\\n\\nCorrect\\n\\nIncorrect: select winner_age instead of winner_name\\n\\nDB Perturbation\\n\\nFind the name and rank of the 3 youngest winners across all matches.\\n\\nSELECT T2.champ_name, T2.winner_rank_points\\nfrom matches as T1 join players as T3 on T1.winner_id = T3.player_id\\n\\nIncorrect: miss the constraints about 3 youngest winners\\n\\nSQL Perturbation\\n\\nFind the name and rank of the 8 youngest winners across all matches.\\n\\nSELECT winner_name, winner_rank FROM matches WHERE winner_age = 8\\n\\nIncorrect: predict age=8 instead of LIMIT 8\\n\\nFigure 1: An example of the SOTA model Picard (Scholak et al., 2021) against DB, NLQ, SQL perturbations on the database WTA. Picard predicts a correct SQL on pre-perturbation data but fails on post-perturbation data. The blue and gray areas highlight the modification on input and the errors of predicted SQLs respectively.\\n\\nAnnotators, which cannot cover the richness and diversity of human language. For example, the NLQ perturbation example in Figure 1 requires sentence-level paraphrasing, which cannot be generated by previous perturbation methods.\\n\\nIn this paper, we curate a comprehensive Dr.Spider robustness evaluation benchmark, based on Spider (Yu et al., 2018) via 17 perturbations to cover all three types of robustness phenomena on DBs, NLQs, and SQLs. Dr. Spider contains 15K perturbed examples. We perturb DBs with a set of predefined rules by taking advantage of their structural nature to represent data in different ways. For NLQ perturbations, we propose a collaborative expert-crowdsourcer-AI framework by prompting the pretrained OPT model (Zhang et al., 2022) to simulate various and task-specific linguistic phenomena. For SQL perturbations, we programmatically modify the local semantics in SQLs and their corresponding tokens in NLQs while minimizing the surface-level changes to measure model robustness to local semantic changes.\\n\\nWe evaluate the SOTA text-to-SQL models on our benchmark (Wang et al., 2019; Rubin & Berant, 2021; Yu et al., 2021; Scholak et al., 2021). The experiments demonstrate that although the SOTA models achieve good performance on the original data, they struggle to have consistently correct predictions in our robustness sets. Even the most robust model suffers from a 14.0% performance drop overall and a 50.7% performance drop against the most challenging perturbation. We also present a breakdown analysis of model robustness in terms of model architectures, including model size, decoder architecture, and the entity-linking component. We analyze the advantage of different model designs, which provides insights for developing robust text-to-SQL models in the future.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"et al., 2021; Pi et al., 2022). Our benchmark not only contains semantic-preserving perturbations (on DB and NLQ) but also semantic-changing perturbations (on SQL) to evaluate the robustness of models in distinguishing closely related but different semantics.\\n\\nData Generation with Pre-trained Language Models\\n\\nLarge pre-trained language models (PLM), such as GPT-3 (Brown et al., 2020), OPT (Zhang et al., 2022), PaLM (Chowdhery et al., 2022), have achieved human-level performance on many NLP tasks. Recently, they have been used to generate data with prompts that consist of a few similar examples (Schick & Sch\u00fctze, 2021; Lee et al., 2021b; 2022; Liu et al., 2022). Liu et al. (2022) has motivations that most parallel ours, which uses GPT-3 to generate a new data example with several existing data examples that share a similar pattern. However, instead of generating a new example, we leverage PLMs to paraphrase existing NLQs to create task-specific perturbations.\\n\\nDr.Spider aims to comprehensively evaluate the robustness of models with perturbations on each component of the text-to-SQL task, i.e. DB, NLQ, and SQL. We apply task-specific perturbations to create our benchmark based on the Spider development set, as the Spider test set is not public. Dr.Spider contains 3 DB perturbation test sets, 9 NLQ perturbation test sets, and 5 SQL perturbation test sets to simulate various task-specific phenomena. Each test set contains parallel pre-perturbation and post-perturbation data to measure model robustness against the perturbation. In total, Dr.Spider contains 15K pairs of pre-perturbation and post-perturbation examples.\\n\\nData Creation Principles\\n\\nWe create our data following three principles.\\n\\nTask Specificity:\\nHuang et al. (2021); Ma & Wang (2021) uses general perturbations in NLQs, such as word substitution and back translation, making it difficult to provide fine-grained analysis in addition to the general robustness problems found in other NLP tasks (Nie et al., 2019; Ribeiro et al., 2020; Goel et al., 2021). We curate our NLQ perturbation data within 9 task-specific categories to reveal unique robustness challenges in the text-to-SQL task.\\n\\nLinguistic Richness:\\nPrevious studies rely on rule-based word-level perturbations (Gan et al., 2021a; Ma & Wang, 2021) or manual perturbations provided by a few experts (Gan et al., 2021b; Deng et al., 2021) which cannot provide a high degree of linguistic richness. For example, \u201cteachers whose hometown are\u201d is perturbed to \u201cteachers whose birthplace are\u201d with word-level perturbation (Gan et al., 2021a), however, other common linguistic phenomena are not covered in previous data such as the sentence-level perturbation \u201cteachers who were born in\u201d from Dr.Spider. We leverage large pre-trained language models (PLMs) for NLQ perturbations instead of relying on rule-based methods or a few human annotators.\\n\\nDiagnostic Comprehensive-ness:\\nFor a systematic evaluation, a benchmark should provide various diagnostic angles. Previous work only considers semantic-preserving perturbations on either DBs or NLQs with a few perturbation methods, such as replacing the database schema with synonym (Pi et al., 2022) and replacing schema indicator in NLQs with synonyms Gan et al. (2021a). We apply various perturbations, including semantic-preserving perturbations to DBs and NLQs and semantic-changing perturbations to SQLs, to curate 17 perturbation test sets focusing on different robustness phenomena.\\n\\n3.1 DATABASE PERTURBATION\\n\\nDatabase perturbation aims to simulate the scenario where data can be represented in different ways in a DB. We consider three DB perturbations: schema-synonym, schema-abbreviation, and column-equivalence.\\n\\nSchema-synonym replaces the name of a column with its synonym. For instance, the column named country can be replaced with nation.\\n\\nSchema-abbreviation replaces the name of a column with its abbreviation. For example, the column named ranking points can be substituted with rank pts. First, we create a schema synonym and an abbreviation dictionary for each database. We use the synonym dictionary in Pi et al. (2022) and obtain an abbreviation dictionary from a public abbreviation website 2. Unlike Pi et al. (2022) who replace all possible columns with their synonyms, we sample a subset from the synonym and abbreviation dictionary to replace the sampled columns with their synonyms or abbreviations. The samplings are repeated five times to create diverse perturbations with duplicate samples removed. Finally, we programmatically modify the database and affected SQLs to generate\\n\\n2 https://www.allacronyms.com/\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"new data based on sampled perturbations each time. As Ma & Wang (2021) discovered that models are generally robust to DB perturbations that do not affect gold SQLs, we only select SQLs that mention any replaced columns in perturbations.\\n\\nDBcontent-equivalence simulates the scenario that data can be represented in different formats in a database by replacing a column (column name and content) with one or multiple semantic-equivalent columns. We first ask task experts to provide possible semantically equivalent substitutions for each column. (1) For text column content, we consider to split a compound column into multiple columns, such as replacing `fullname` with `firstname` and `lastname`. (2) We convert columns with fixed string values into boolean column(s) or vice versa. For instance, a boolean column `male` is equivalent to a text column `sex` with all gender options, and a text column `degree` is equivalent to three boolean columns `is bachelor`, `is master`, and `is PhD` when there are only three degrees in the database. (3) For numerical columns, we replace them with other numerical columns with equivalent semantics. For example, the column `birthyear` is semantically equivalent to the column `age` since one can be inferred from the other. Similar to the other DB perturbations, we sample a subset from the substitutions at most 5 times to ensure perturbation diversity and programmatically modify SQLs according to the changes in DBs. Note that we remove SQLs that require unseen grammar to modify according to the database perturbation. To the best of our knowledge, we are the first to consider representing database content in different ways to create database perturbations.\\n\\n### 3.2 Natural Language Question Perturbation\\n\\nWe propose a collaborative expert-crowdsourcer-AI framework to collect task-specific perturbations by leveraging the advantage of each: experts (we) provide task-specific analysis, crowdsourcers have natural and diverse language patterns, and AI models are scalable. Our framework involves the following phases: (1) Crowdsourcing annotators from Amazon Mechanical Turk to paraphrase sampled questions from the Spider dataset. (2) Task experts review the paraphrases and categorize them from the text-to-SQL task perspectives. (3) A pretrained language model (PLM) generates categorized paraphrase perturbations based on the collected human paraphrases in each category. (4) A pretrained natural language inference (NLI) model and rule-based filters are used to select factual paraphrases in each category. (5) Task experts review the paraphrased questions in the end to ensure the high quality of our dataset.\\n\\n#### Crowdsourcing Paraphrase Study\\n\\nNLQs in Spider were collected in a setting that users (questioners) are familiar with SQL and have the access to the DB schema and content. However, in reality, the general users are not familiar with DBs or SQLs in most applications (Lee et al., 2021a). To understand the linguistic phenomena that NLQs could have in a realistic scenario, we recruit annotators on Amazon Mechanical Turk to paraphrase 100 sampled questions in Spider and collect five paraphrases per question. We encourage annotators to create diverse paraphrases by not directly providing the database schema or content to the annotators. Instead, we provide a short description of each database, which reveals the topic of the database and the key tables and columns in the database. Note that we omit the description of naturally occurring tables and columns (e.g. a student table contains columns `name`, `grade`) because they do not provide the necessary information for understanding the database context and also make annotators tend to follow the structure of schema during paraphrasing.\\n\\n#### Expert Analysis\\n\\nWe find only 270 out of 500 paraphrases are factual as a small change to a question could alter its meaning in the text-to-SQL task. We remove nonfactual paraphrases and analyze linguistic phenomena in crowdsourcing paraphrases regarding how the paraphrase is different from the original question. We focus on three types of tokens in NLQ that can be aligned with SQL: SQL keyword indicators, column name indicators, and value indicators, where SQL keywords include sort keywords, comparison, and aggregation operators; column names refer to the mentioned column in SQL except for `JOIN` clause, and value tokens include database content and other numbers.\\n\\nWe find 9 categories based on whether the paraphrasing contains a task-specific linguistic phenomenon, including 8 categories that modify the aforementioned indicators and one category that does not. Table 1 shows the categorized examples. Note that a paraphrase may fall into multiple categories if it contains multiple phenomena. The category `Multitype` also include such examples.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Annotation interface for labeling the clarity and fluency for a given question.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Paraphrase Example\\n\\nKeyword-synonym\\n\\nQuestion: find the code of the country where has the greatest number of players.\\nParaphrase: Display the code of the country with the largest player population.\\n\\nSelected in Dr.Spider\\n\\nKeyword-carrier\\n\\nQuestion: Count the number of dogs that went through a treatment.\\nParaphrase: How many dogs have been treated?\\n\\nSelected in Dr.Spider\\n\\nColumn-synonym\\n\\nQuestion: What is the average weight for each type of pet?\\nParaphrase: For each category of pets, what is the average weight?\\n\\nSelected in Dr.Spider\\n\\nColumn-synonym\\n\\nQuestion: What is the money rank of the poker player with the highest earnings?\\nParaphrase: The poker player at rank n made the most income during his or her career.\\n\\nNot Selected in Dr.Spider\\n\\nColumn-carrier\\n\\nQuestion: Find the name of tourney that has more than 10 matches.\\nParaphrase: Which tournament has more than 10 games?\\n\\nSelected in Dr.Spider\\n\\nColumn-carrier\\n\\nQuestion: What are the song titles and singer names?\\nParaphrase: Which songs have titles and singers?\\n\\nNot selected in Dr.Spider\\n\\nColumn-attribute\\n\\nQuestion: What is the car model with the highest mpg?\\nParaphrase: what is the car model that is most gas efficient?\\n\\nSelected in Dr.Spider\\n\\nColumn-value\\n\\nQuestion: What is the phone number of the man with the first name Timmothy and the last name Ward?\\nParaphrase: Find the phone number for Timmothy Ward?\\n\\nSelected in Dr.Spider\\n\\nValue-synonym\\n\\nQuestion:What are the names of conductors whose nationalities are not \\\"USA\\\"?\\nParaphrase: Names of conductors whose nationalities are not US.\\n\\nSelected in Dr.Spider\\n\\nMultitype\\n\\nQuestion: What are names of countries with the top 3 largest population?\\nParaphrase: Which three countries have the highest population?\\n\\nSelected in Dr.Spider\\n\\nMultitype\\n\\nQuestion: What are the airline names and abbreviations for airlines in the USA?\\nParaphrase: list all the airlines and identify their respective codes in the United States.\\n\\nNot selected in Dr.Spider\\n\\nOthers\\n\\nQuestion: What is the average, minimum, and maximum age of all singers from France?\\nParaphrase: Show me your data on the average, minimum, and maximum ages of all singers from France.\\n\\nSelected in Dr.Spider\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| SQL Perturbation | Pre-perturbation | Post-perturbation | Comparison |\\n|------------------|------------------|------------------|------------|\\n| SQL: HAVING count(*) > 1 | SQL: HAVING count(*) >= 1 | NLQ: more than one car maker? | NLQ: at least one car maker? |\\n| SQL: ORDER BY age DESC | SQL: ORDER BY age ASC | NLQ: from the oldest to the youngest. | NLQ: from the youngest to the oldest. |\\n| SQL: ORDER BY winner age LIMIT 3 | SQL: ORDER BY winner age LIMIT 5 | NLQ: 3 youngest winners | NLQ: 5 youngest winners |\\n| SQL: country = 'France' | SQL: country = 'Netherlands' | NLQ: singers from France | NLQ: singers from Netherlands |\\n| SQL: horsepower > 150 | SQL: horsepower > 145 | NLQ: horsepower more than 150 | NLQ: horsepower more than 145 |\\n\\nTable 7: SQL perturbations and examples. Since SQL perturbations involve modifications on both SQL tokens and their indicators in NLQs, we want to minimize the surface-level changes introduced to NLQs to disentangle SQL perturbations from NLQ perturbations. For comparison and sort-order perturbations, we programmatically modify the keywords in SQL and their indicators in NLQ following the rules in Tables 8 - 10. We replace only one indicator with another that contains the same meaning in the context. For example, when replacing > with <, \\\"older than\\\" is replaced by \\\"younger than\\\" instead of \\\"lighter than\\\".\\n\\nTable 8: The common NLQ indicator tokens for comparison operations in Spider.\\n\\n| Common Token            |\\n|-------------------------|\\n| >                       |\\n| <                       |\\n| >=                      |\\n| <=                      |\\n| above                   |\\n| below                   |\\n| after                   |\\n| before                  |\\n| older than              |\\n| younger than            |\\n| heavier than            |\\n| lighter than            |\\n| more than               |\\n| less than               |\\n| at most                 |\\n| at least                |\\n| larger/bigger than      |\\n| smaller than            |\\n| higher than             |\\n| lower than              |\\n| - or more               |\\n| - or less               |\\n\\nTable 9: The common NLQ indicator tokens for ascending sort and descending sort in Spider.\\n\\n| Common Token            |\\n|-------------------------|\\n| ascending               |\\n| descending              |\\n| in alphabetical order   |\\n| in reverse alphabetical order |\\n| in lexicographical order |\\n| in reversed lexicographical order |\\n| from the youngest to the oldest |\\n| from the oldest to the youngest |\\n| from young to old       |\\n| from old to young       |\\n| from low to high        |\\n| from high to low        |\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: The common NLQ indicator tokens for ascending sort and descending sort followed by a LIMIT clause in Spider.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"models have both a higher post-perturbation accuracy and a relative robustness accuracy, which indicates that using large pre-trained models is one solution for improving the robustness of models.\\n\\nWhat are the advantages of top-down and bottom-up decoders? Figure 3 shows the robustness of GraPPa and SmBoP which have the same encoder but with a top-down decoder and a bottom-up decoder. We find that the bottom-up decoder model SmBoP is more robust to DB perturbations, and the top-down decoder model GraPPa is more robust to NLQ perturbations. We believe it is because the DB perturbation modifies local inputs of the database schema and the bottom-up decoder decodes the leaf nodes first, which learns a better alignment between NLQ and databases. However, NLQ perturbations are via sentence-level paraphrasing, where the top-down decoder decodes the structure of SQL first, which is more robust than sentence-level paraphrases. This implies that a model combining the top-down and bottom-up decoders could be a solution to improve models\u2019 robustness on text-to-SQL tasks.\\n\\nDoes question and DB content linking benefit model robustness? The SOTA models leverage an entity linking feature between question tokens and DB content based on string matching to improve value prediction performance, which concatenates column names with mentioned DB contents in input (Lin et al., 2020; Scholak et al., 2021). This feature provides two benefits: (1) indicating which column should be predicted in SQL by its mentioned DB content and (2) providing the format of a mentioned value in its DB. We compare T5-3B and T5-3B LK against NLQ perturbations and report their relative robustness accuracy. Using the entity linking significantly improves model robustness against column-value (68.7 vs 78.1) and value-synonym (35.8 vs 46.1) perturbations in terms of EX accuracy. The entity linking benefits the SQL value prediction by giving how the value is presented in the DB especially when the value appears in a different format in NLQ (value-synonym). However, this feature slightly hurts the SQL prediction in terms of EM accuracy (ignoring values) on value-synonym (66.2 vs 67.2). We believe that it is because the model overfits the string-matching feature to indicate the mentioned column and does not generalize well when the matching fails due to perturbation. Therefore, it is beneficial to provide the formats of mentioned values in the DB via the linking. And a better linking mechanism should be adapted for developing robust text-to-SQL models. A detailed analysis can be found in Appendix B.\\n\\nConclusion and future work\\nWe curate a diagnostic text-to-SQL evaluation benchmark Dr.Spider, which contains various perturbations on DB, NLQ, and SQL to simulate diverse task-specific robustness challenges. We diagnose the robustness of SOTA text-to-SQL models regarding their fine-gained components with our benchmark. Our experiments reveal existing models are vulnerable to perturbations, while some model architecture designs are more robust against certain perturbations. We envision our findings could benefit future model design for robust text-to-SQL models. In the future, we want to explore different strategies of prompt examples selection besides handpicking for paraphrasing NLQ. We also plan to improve the robustness of text-to-SQL models with the insight from this work, such as combining top-down and bottom-up decoder and using our perturbation methods for data augmentation.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We acknowledge the importance of the ICLR Code of Ethics and agree with it. An unrobust text-to-SQL system may cause harm by misinforming its users. Our work aims at addressing it by providing a diagnostic robustness evaluation platform and insights to improve the robustness of models. We construct our benchmark Dr.Spider based on Spider, a public and free text-to-SQL benchmark. We recruited annotators from Amazon Mechanical Turk (MTurk) for the crowdsourcing paraphrase study as well as the clarity and fluency annotation for our data quality control. We took great care to pay fair wages. Crowdsourcing annotators are required to paraphrase a question or label the clarity and fluency of a question. No sensitive information about annotators was asked and the only personal information we collected was the worker IDs on Mturk, which will not be released. We leverage large pre-trained language models (PLM) to paraphrase questions as our NLQ perturbations. We acknowledge that PLM may generate toxic language (Sheng et al., 2019; Gehman et al., 2020). Three expert annotators carefully review all generated paraphrases to ensure no offensive language in our dataset.\\n\\nACKNOWLEDGEMENT\\n\\nWe would like to thank Eric Fosler-Lussier, Michael White, Micha Elsner, Srinivasan Parthasarathy, Huan Sun, Yu Su, Amad Hussain, Pengfei Liu, and others at the OSU SLaTe lab and the Clippers seminar, as well as the anonymous reviewers for their valuable feedback on this work.\\n\\nREFERENCES\\n\\nElron Bandel, Ranit Aharonov, Michal Shmueli-Scheuer, Ilya Shnayderman, Noam Slonim, and Liat Ein-Dor. Quality controlled paraphrase generation. arXiv preprint arXiv:2203.10940, 2022.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\\n\\nXiang Deng, Ahmed Hassan, Christopher Meek, Oleksandr Polozov, Huan Sun, and Matthew Richardson. Structure-grounded pretraining for text-to-sql. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1337\u20131350, 2021.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\nYujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R Woodward, Jinxia Xie, and Pengsheng Huang. Towards robustness of text-to-sql models against synonym substitution. arXiv preprint arXiv:2106.01065, 2021a.\\n\\nYujian Gan, Xinyun Chen, and Matthew Purver. Exploring underexplored limitations of cross-domain text-to-sql generalization. arXiv preprint arXiv:2109.05157, 2021b.\\n\\nMatt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, et al. Evaluating models\u2019 local decision boundaries via contrast sets. arXiv preprint arXiv:2004.02709, 2020.\\n\\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Realtoxicprompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 3356\u20133369, 2020.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Karan Goel, Nazneen Rajani, Jesse Vig, Samson Tan, Jason Wu, Stephan Zheng, Caiming Xiong, Mohit Bansal, and Christopher R\u00e9. Robustness gym: Unifying the nlp evaluation landscape. arXiv preprint arXiv:2101.04840, 2021.\\n\\nShuo Huang, Zhuang Li, Lizhen Qu, and Lei Pan. On robustness of neural semantic parsers. arXiv preprint arXiv:2102.01563, 2021.\\n\\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. Adversarial example generation with syntactically controlled paraphrase networks. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1875\u20131885, 2018.\\n\\nRobin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2021\u20132031, 2017.\\n\\nDivyansh Kaushik, Eduard Hovy, and Zachary C Lipton. Learning the difference that makes a difference with counterfactually-augmented data. arXiv preprint arXiv:1909.12434, 2019.\\n\\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. Ctrl: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858, 2019.\\n\\nChia-Hsuan Lee, Oleksandr Polozov, and Matthew Richardson. Kaggledbqa: Realistic evaluation of text-to-sql parsers. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 2261\u20132273, 2021a.\\n\\nKenton Lee, Kelvin Guu, Luheng He, Tim Dozat, and Hyung Won Chung. Neural data augmentation via example extrapolation. arXiv preprint arXiv:2102.01335, 2021b.\\n\\nMina Lee, Percy Liang, and Qian Yang. Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities. In CHI Conference on Human Factors in Computing Systems, pp. 1\u201319, 2022.\\n\\nWenqiang Lei, Weixin Wang, Zhixin Ma, Tian Gan, Wei Lu, Min-Yen Kan, and Tat-Seng Chua. Re-examining the role of schema linking in text-to-sql. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6943\u20136954, 2020.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\\n\\nXi Victoria Lin, Richard Socher, and Caiming Xiong. Bridging textual and tabular data for cross-domain text-to-sql semantic parsing. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 4870\u20134888, 2020.\\n\\nAlisa Liu, Swabha Swayamdipta, Noah A Smith, and Yejin Choi. Wanli: Worker and ai collaboration for natural language inference dataset creation. arXiv preprint arXiv:2201.05955, 2022.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\\n\\nPingchuan Ma and Shuai Wang. Mt-teql: evaluating and augmenting neural nlidb on real-world linguistic and schema variations. Proceedings of the VLDB Endowment, 15(3):569\u2013582, 2021.\\n\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599, 2019.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Wc5bmZZU9cy", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We believe that the first 8 categories can diagnose models against each task-specific NLQ perturbation, and we regard the last category as a control group to evaluate the robustness of models against general syntactic perturbations in sentence-level paraphrasing.\\n\\n**Paraphrase Category Example**\\n\\n**Keyword-synonym**  \\n(Replace SQL keyword indicators with synonyms)\\n\\nQuestion: What is the code of airport that has the highest number of flights?  \\nParaphrases: Show me the code for the airport that currently has the most flights.\\n\\n**Keyword-carrier**  \\n(Imply SQL keyword indicators by carrier phrases)\\n\\nQuestion: Show the name and theme for all concerts and the number of singers in each concert.  \\nParaphrase: List the names and themes for all concerts and how many singers are in each.\\n\\n**Column-synonym**  \\n(Replace column indicators with synonyms)\\n\\nQuestion: List the name of teachers whose hometown is not Little Lever Urban District.  \\nParaphrases: Find the name of teachers who were not born in Little Lever Urban District.\\n\\n**Column-carrier**  \\n(Imply column indicators by carrier phrases)\\n\\nQuestion: Show the name of teachers aged either 32 or 33?  \\nParaphrases: Which teachers are aged either 32 or 33.\\n\\n**Column-attribute**  \\n(Imply column indicators by aggregated attributes)\\n\\nQuestion: What is the name of the conductor who has worked the greatest number of year?  \\nParaphrases: Who has worked the longest as conductor?\\n\\n**Column-value**  \\n(Imply column indicators by values)\\n\\nQuestion: What are the ids of the students who do not own cats as pets?  \\nParaphrases: Find the IDs of students who don't own cats.\\n\\n**Value-synonym**  \\n(Replace value indicators with synonyms)\\n\\nQuestion: Find all airlines that have at least 10 flights.  \\nParaphrases: Show the number of airlines that have at least ten flights.\\n\\n**Multitype**  \\n(Contain multiple phenomena above)\\n\\nQuestion: Find number of pets owned by students who are older than 20?  \\nParaphrases: How many pets are owned by students over 20?\\n\\n**Others**\\n\\nQuestion: what is the name and nation of the singer who have a song having 'Hey' in its name?  \\nParaphrases: Which singers have 'Hey' in their song\u2019s name? List their name and nation.\\n\\nTable 1: Crowdsouring paraphrase categories and examples. The red tokens in the original questions highlight the keyword/column/value indicators that are paraphrased, and the blue tokens in the paraphrases represent how the indicators are replaced or implicitly mentioned.\\n\\n## Categorized Paraphrase Generation\\n\\nAlthough crowdsourcing paraphrases can be used as NLQ perturbations, it is costly to scale and categorize them for curating a large-scale dataset. To tackle this issue, we generate paraphrases with large PLM and prompt it with crowdsourcing paraphrases in each category. We choose the OPT (Zhang et al., 2022) with 66B parameters as the PLM model. We manually select five examples from the crowdsourcing paraphrases in each category to form the in-context examples as the prompt prefix.\\n\\nFor each category, the input of OPT consists of a category-specific instruction, 5 question-paraphrase pairs in the category, an NLQ that needs to be paraphrased, and its SQL tokens (keywords/column-values) whose indicators are required to be modified in the category. Inspired by Wei et al. (2022); Paranjape et al. (2021), we employ the OPT model to generate an explanation along with the paraphrase. We overgenerate 20 paraphrases per question to create diverse paraphrases (however, at most 5 examples will be kept after the filtering stage). Our prompt prefixes and implementation details can be found in Appendix A.2.\\n\\n## Paraphrase Filtering and Quality Control\\n\\nAfter generating paraphrases in each category, we first remove the duplicate questions and use a BART-large model (Lewis et al., 2019) trained on Multi-NLI (Williams et al., 2018) to filter out nonfactual paraphrases. Then, a simple and high-recall rule-based classifier is built to remove out-of-category paraphrases based on the alignment between SQL tokens and their indicators in NLQ. In particular, we use the annotations from Lei...\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"et al. (2020) to build the column and value indicators. And to obtain SQL keyword indicators, we collect the common indicators (covering over 80% examples) of each SQL keyword in the NLQs and programmatically label NLQs with those indicators. The common indicators of SQL keywords can be found in Appendix A.3. Finally, we filter out the paraphrases where all the indicators for SQL keywords, columns and values remain the same as the original NLQ except category others.\\n\\nTo ensure the data quality in Dr.Spider, we request three annotators with SQL expertise to carefully review paraphrases with gold SQL labels and select the final data. We split the filtered data into small chunks and each chunk is reviewed by at least one annotator. 63% generated paraphrases are labeled factual and belong to the correct category, while only 54% of human paraphrases from the previous crowdsourcing are actually factual. This shows the effectiveness of our proposed framework for generating categorized paraphrases. 5% data (evenly from 9 categories) are reviewed by all three annotators. The Fleiss Kappa score between the three annotators is 0.61. Randomly sampled generated paraphrases in each category can be found in Appendix A.3.\\n\\n### 3.3 SQL Perturbation\\n\\nSQL perturbations evaluate models' robustness against local semantic changes. We consider five perturbations regarding SQL tokens: (1) Comparison replaces a comparison operation from \\\\{<, >, <, =, >, =\\\\} to another; (2) Sort-order switches ascending sort and descending sort; (3) NonDB-number replaces a non-DB number \\\\(n\\\\) with another number in \\\\([n - 10, n + 10]\\\\), including the number in the LIMIT clause (e.g. Limit \\\\(n\\\\)) and a criteria about counts (e.g. HAVING \\\\(count(*) > n\\\\)); (4) DB-text replaces a mentioned DB content text to another in the same column; (5) DB-number replaces a mentioned DB content number \\\\(m\\\\) to another number in \\\\([m - 10, m + 10]\\\\). Similar to DB and NLQ perturbations, we perturb an example at most five times to increase the diversity of perturbations. Examples for each category can be found in Appendix A.4.\\n\\nSince SQL perturbations involve modifications on both SQL tokens and their indicators in NLQs, we want to minimize the surface-level changes introduced to NLQs to disentangle SQL perturbations from NLQ perturbations. For Comparison and Sort-order, we replace an indicator in NLQ only with common indicators in the Spider training data. For example, an NLQ \\\"...more than one car\\\" corresponds to the SQL \\\"...count(*)>1\\\". We perturb the SQL by replacing operation \\\">\\\" with \\\"\\\\(\\\\geq\\\\)\\\" as well as the NLQ indicator \\\"more than\\\" with \\\"at least\\\". We choose \\\"at least\\\" rather than uncommon indicator phrases (e.g. \\\"no less than\\\") because SQL perturbations focus on evaluating models against local semantic changes on SQLs and replacing the SQL keyword indicator \\\"at least\\\" with its synonym \\\"no less than\\\" is covered in NLQ perturbation keyword-synonym.\\n\\nMore details about the replacement rules can be found in Appendix A.4. For number perturbations, a mentioned number in NLQ will only be replaced with another number in the same format, e.g., \\\\(3=4\\\\), \\\\(3rd=4th\\\\), and three \\\\(=\\\\) four, and we skip numbers 0 and 1 since they usually have non-number tokens as indicators in NLQ. We only consider the DB content text that is mentioned in the exact same format in NLQ to disentangle it from NLQ perturbation value-synonym.\\n\\n### 3.4 Data Statistics\\n\\n|                  | Perturbation Data | TS | LR | DC | # DB Perturbations | # NLQ Perturbations | # SQL Perturbations |\\n|------------------|-------------------|----|----|----|-------------------|---------------------|--------------------|\\n| Huang et al. (2021) | \u00d7\u2713               | \u2713  | \u2713  | \u2713  | 6                 | -                   | -                  |\\n| Ma & Wang (2021)   | \u2713\u00d7               | \u2713  | \u2713  | \u2713  | 8                 | 4                   | -                  |\\n| Pi et al. (2022)   | \u2713\u00d7\u00d7              | \u2713  | \u2713  | \u00d7  | 2                 | -                   | -                  |\\n| Gan et al. (2021a) | \u2713\u00d7\u00d7              | \u2713  | \u00d7  | \u00d7  | -                 | -                   | -                  |\\n| Gan et al. (2021b) | \u2713\u00d7\u00d7              | \u2713  | \u00d7  | \u00d7  | -                 | -                   | -                  |\\n| Deng et al. (2021) | \u2713\u00d7\u00d7              | \u2713  | \u00d7  | \u00d7  | -                 | -                   | -                  |\\n| Dr.Spider         | \u2713\u2713\u2713              | \u2713  | \u2713  | \u2713  | 3                 | 9                   | 5                  |\\n\\nTable 2: Perturbation attribute and the number of perturbation types for DB, NLQ, and SQL in existing studies and Dr.Spider. TS, LR, and DC represent task-specificity, linguistic richness, and diagnostic comprehensiveness, respectively.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"all DB, NLQ, and SQL. Dr.Spider also contains various task-specific paraphrases for NLQ perturbations which contain a higher degree of linguistic richness than word-level perturbation methods. Similar to ours, Ma & Wang (2021) consider various formats to represent data in a DB. However, their perturbations require NLQ and SQL not to mention the perturbed DB columns, which fail to detect the lack of robustness of models (2.5 points performance drops on average). We report the number of examples in Dr.Spider and the original Spider development set in the column # in Table 3. The number represents the size of each post-perturbation test data in Dr.Spider. To measure the naturalness of our generated NLQ perturbations, we sample 500 questions from each of the original Spider development set, human paraphrases (we use all 270 factual paraphrases), and our perturbed questions. Crowdsourcing annotators are hired to score the clarity and fluency of sampled questions from 1 to 3. Each question is evaluated by five annotators. The clarity scores of Spider questions, human paraphrases, and generated paraphrases are 2.7, 2.6, 2.6, and fluency scores are 2.7, 2.6, 2.7, which shows generated paraphrases have similar fluency and clarity to the original questions and human paraphrases. The annotation interface can be found in Appendix A.3.\\n\\n4 EXPERIMENTS\\n\\nModels\\n\\nThe SOTA text-to-SQL models follow an encoder-decoder framework that encodes an NLQ and a DB schema jointly and decodes the corresponding SQL. We evaluate multiple representative text-to-SQL models on Dr.Spider, which are trained on the Spider training set: (1) RATSQL (Wang et al., 2019): The encoder is pretrained with BERT-large (Devlin et al., 2018) and augmented with relation-aware attention (Shaw et al., 2018), and the decoder is a top-down decoding with abstract syntax tree (AST). We use the implementation from Scholak et al. (2020). (2) GRAPPA (Yu et al., 2021): RatSQL with GraPPa embedding as pretrained encoder. GraPPa finetunes RoBERTa-large (Liu et al., 2019) on synthetic table-text pair data. (3) SMOP (Rubin & Berant, 2021): An encoder-decoder model with the same encoder as GRAPPA and a bottom-up decoder with AST. (4) T5-FAMILY (Raffel et al., 2020): We use T5-BASE, T5-LARGE, and T5-3B which are fine-tuned on Spider. (5) T5-3B LK (Shaw et al., 2021): A T5-3B model with an entity linking between question and DB content. (6) PICARD Scholak et al. (2021): T5-3B LK with a constraint decoding. It is the SOTA model on the Spider benchmark. (7) GPT-3 CODEX Chen et al. (2021): A large language model without finetuning on Spider. We present the evaluation of CODEX in Appendix B.\\n\\nRobustness Evaluation Metrics\\n\\nWe follow the Spider Benchmark to use the two SQL evaluation metrics to evaluate the correctness of a predicted SQL with a gold SQL: exact set match (EM) measures a predicted SQL with a gold SQL on each SQL clause ignoring values; execution accuracy (EX) compares the denotation answers from a predicted SQL and a gold SQL. We use EX as our main evaluation metric as it evaluates the correctness of SQL values, which is an important part of model robustness, while we only report EM for GRAPPA which omits value predictions.\\n\\nTo evaluate robustness, we report 3 metrics: (1) pre-perturbation accuracy: the accuracy on pre-perturbation data, (2) post-perturbation accuracy (or absolute robustness accuracy): the accuracy on post-perturbation data, (3) relative robustness accuracy: The ratio of the number of correct predictions on both parallel pre-perturbation and post-perturbation data over the number of correct predictions on pre-perturbation data. While the post-perturbation accuracy measures the absolute performance of a model on perturbation data, relative robustness accuracy is to evaluate the regression rate of models against perturbation, which is more precise to compare the robustness of models by considering their performances on the pre-perturbation data.\\n\\n5 RESULTS\\n\\n5.1 DIAGNOSTIC RESULTS\\n\\nTable 3 shows the execution accuracy changes of the SOTA models against perturbations, where we report the macro-average scores for all perturbations. The EM accuracy can be found in Appendix B. In general, all models suffer from significant performance drops in DB and NLQ perturbations. Picard is the most robust model but still has 30.0% and 14.5% relative performance drops. The most challenging perturbation for DB is DBcontent-equivalence. Even PICARD faces a 50.7% performance drop. We believe it is because the way that schemas and contents are mentioned in NLQs is different from that in DB, which requires additional common-sense reasoning to align.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Data statistics and the execution (EX) accuracy of SOTA text-to-SQL models on the original Spider development set (Spider-dev) and Dr.Spider. The column # contains the size of Spider-dev and post-perturbation data in each perturbation test set in Dr.Spider. * represents the model we trained with official codes as their models are not public. x\u2014y represents the pre-perturbation accuracy and post-perturbation accuracy (i.e. absolute robustness accuracy). We report macro-average scores over multiple perturbations. Bold number is the highest accuracy in each category and the underscore stands for the second best result.\\n\\n| Perturbation                  | Spider-dev | Dr.Spider |\\n|-------------------------------|------------|-----------|\\n| Snake-case                    | 1,034      | 79.3      |\\n| DBSchema-synonym              | 2,619      | 73.0      |\\n| DBSchema-abbreviation         | 2,853      | 74.9      |\\n| DBContent-equivalence         | 382        | 88.7      |\\n| NLQKeyword-synonym            | 953        | 66.3      |\\n| NLQKeyword-carrier            | 399        | 85.0      |\\n| DColsynonym                   | 563        | 57.2      |\\n| DColexpression                | 579        | 86.9      |\\n| DColl-value                   | 119        | 56.3      |\\n| Value-synonym                 | 506        | 72.5      |\\n| Multitype                     | 1,351      | 74.4      |\\n| Others                        | 2,819      | 79.6      |\\n| Average                       | 70.8       | 78.9      |\\n| SQLComparison                 | 178        | 68.0      |\\n| Sort-order                    | 192        | 79.2      |\\n| NonDB-number                  | 131        | 83.2      |\\n| DBtext                        | 911        | 65.1      |\\n| DB-number                     | 410        | 86.3      |\\n| Average                       | 70.2       | 76.7      |\\n\\nSurprisingly, although most SOTA models are relatively robust to local semantic changes (SQL perturbations), except for R\\\\textsubscript{AT}SQL and T5\\\\textsubscript{BASE}, \\\\textsubscript{P\\\\textsubscript{ICARD}} still has a 7.3% performance drop in NonDB-number. Figure 1 contains an example that \\\\textsubscript{P\\\\textsubscript{ICARD}} understands \\\"3 youngest winners\\\", but confuses \\\"8 youngest winners\\\" with \\\"age is 8\\\". A robust model should recognize that the number in the question corresponds to a logical meaning LIMIT BY and be robust to the changes in the number value.\\n\\n5.2 \\\\textsubscript{DIAGNOSTIC INSIGHT FOR MODEL DESIGNS}\\n\\nThis section presents experiments to answer some questions about the robustness in terms of model designs and provide insights for developing more robust models. Are larger models more robust? Figure 2 shows the results of T5\\\\textsubscript{FAMILY} models. Not surprisingly, the larger models have a better pre-perturbation accuracy. In addition, we find that the larger...\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"|                  | Schema-synonym | Schema-abbreviation | DBcontent-equivalence | Average |\\n|------------------|----------------|---------------------|-----------------------|---------|\\n| **Keyword-synonym** | 72.2           | 75.3                | 77.2                  | 73.5    |\\n| **Keyword-carrier** | 93.0           | 93.0                | 94.0                  | 91.2    |\\n| **Column-synonym**    | 66.5           | 73.1                | 69.8                  | 62.0    |\\n| **Column-carrier**     | 67.0           | 74.2                | 68.6                  | 65.5    |\\n| **Column-attribute**   | 65.6           | 79.7                | 73.3                  | 78.4    |\\n| **Column-value**       | 66.7           | 60.2                | 75.8                  | 54.1    |\\n| **value-synonym**      | 57.7           | 79.0                | 57.8                  | 68.3    |\\n| **Multitype**          | 59.4           | 68.9                | 60.5                  | 62.8    |\\n| **Others**             | 83.5           | 85.2                | 86.8                  | 81.6    |\\n| **Average**            | 70.2           | 76.5                | 73.8                  | 70.8    |\\n\\n**Table 15:** The relative robustness accuracy on EM of SOTA text-to-SQL models on Dr.Spider. Bold number is most highest absolute robustness accuracy in each category and underscores stand for the second best results. Note that the EM accuracy does not evaluate value predictions. Therefore, it is not effective to measure model robustness against NonDB-number, DB-text, and DB-number.\"}"}
{"id": "Wc5bmZZU9cy", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 16: The execution (EX) accuracy of SOTA finetuned and in-context-learning text-to-SQL models on the original Spider development set (Spider-dev) and Dr.Spider. The first two result columns are pre-perturbation and pose-perturbation accuracy of P\\\\textsc{ICARD} and C\\\\textsc{ODE}X, where x\u2014y represents the pre-perturbation accuracy and post-perturbation accuracy (i.e. absolute robustness accuracy). The last two result columns are the relative robustness accuracy of P\\\\textsc{ICARD} and C\\\\textsc{ODE}X.\\n\\nWe report macro-average scores over multiple perturbations. Bold number is the highest accuracy in each category.\"}"}
