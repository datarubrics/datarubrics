{"id": "ZumkmSpY9G4", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nContinual learning requires the model to maintain the learned knowledge while learning from a non-i.i.d data stream continually. Due to the single-pass training setting, online continual learning is very challenging, but it is closer to the real-world scenarios where quick adaptation to new data is appealing. In this paper, we focus on online class-incremental learning setting in which new classes emerge over time. Almost all existing methods are replay-based with a softmax classifier. However, the inherent logits bias problem in the softmax classifier is a main cause of catastrophic forgetting while existing solutions are not applicable for online settings. To bypass this problem, we abandon the softmax classifier and propose a novel generative framework based on the feature space. In our framework, a generative classifier which utilizes replay memory is used for inference, and the training objective is a pair-based metric learning loss which is proven theoretically to optimize the feature space in a generative way. In order to improve the ability to learn new data, we further propose a hybrid of generative and discriminative loss to train the model. Extensive experiments on several benchmarks, including newly introduced task-free datasets, show that our method beats a series of state-of-the-art replay-based methods with discriminative classifiers, and reduces catastrophic forgetting consistently with a remarkable margin.\\n\\nIntroduction\\n\\nHumans excel at continually learning new skills and accumulating knowledge throughout their life span. However, when learning a sequential of tasks emerging over time, neural networks notoriously suffer from catastrophic forgetting (McCloskey & Cohen, 1989) on old knowledge. This problem results from non-i.i.d distribution of data streams in such a scenario. To this end, continual learning (CL) (Parisi et al., 2019; Lange et al., 2019) has been proposed to bridge the above gap between intelligent agents and humans.\\n\\nIn common CL settings, there are clear boundaries between distinct tasks which are known during training. Within each task, a batch of data are accumulated and the model can be trained offline with the i.i.d data. Recently, online CL (Aljundi et al., 2019c,a) setting has received growing attention in which the model needs to learn from a non-i.i.d data stream in online settings. At each iteration, new data are fed into the model only once and then discarded. In this manner, task boundary is not informed, and thus online CL is compatible with task-free (Aljundi et al., 2019b; Lee et al., 2020) scenario. In real-world scenarios, the distribution of data stream changes over time gradually instead of switching between tasks suddenly. Moreover, the model is expected to quickly adapt to large amount of new data, e.g. user-generated content. Online CL meets these requirements, so it is more meaningful for practical applications. Many existing CL works deal with task-incremental learning (TIL) setting (Kirkpatrick et al., 2017; Li & Hoiem, 2018), in which task identity is informed during test and the model only needs to classify within a particular task. However, for online CL problem, TIL is not realistic because of the dependence on task boundary as discussed above and reduces the difficulty of online CL. In contrast, class-incremental learning (CIL) setting (Rebuffi et al., 2017) requires the model to learn new classes continually over time and classify samples over all seen classes during test. Thus, online CIL setting is more suitable for online data streams in real-world CL scenarios (Mai et al., 2021).\"}"}
{"id": "ZumkmSpY9G4", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Most existing online CIL methods are based on experience replay (ER) (Robins, 1995; Riemer et al., 2019) strategy which stores a subset of learned data in a replay memory and uses the data in memory to retrain model thus alleviating forgetting. Recently, in CIL setting, logits bias problem in the last fully connected (FC) layer, i.e. softmax classifier, is revealed (Wu et al., 2019), which is a main cause of catastrophic forgetting. In Figure 1a, we show in online CIL, even if ER is used, logits bias towards newly learned classes in the softmax classifier is still serious and the forgetting on old tasks is dramatic (See Figure 1b). Although some works (Wu et al., 2019; Belouadah & Popescu, 2019; Zhao et al., 2020) propose different methods to reduce logits bias, they all depend on task boundaries and extra offline phases during training so that not applicable for online CIL setting.\\n\\nIn this paper, we propose to tackle the online CIL problem without the softmax classifier to avoid logits bias problem. Instead, we propose a new framework where training and inference are both in a generative way. We are motivated by the insight that generative classifier is more effective in low data regime than discriminative classifier which is demonstrated by Ng & Jordan (2001). Although the conclusion is drawn on simple linear models (Ng & Jordan, 2001), similar results are also observed on deep neural networks (DNNs) (Yogatama et al., 2017; Ding et al., 2020) recently. It should be noticed that in online CIL setting the data is seen only once, not fully trained, so it is analogous to the low data regime in which the generative classifier is preferable. In contrast, the commonly used softmax classifier is a discriminative model.\\n\\nConcretely, we abandon the softmax FC layer and introduce nearest-class-mean (NCM) classifier (Mensink et al., 2013) for inference, which can be interpreted as classifying in a generative way. The NCM classifier is built on the feature space on the top of previous network layers. Thanks to ER strategy, NCM classifier can utilize the replay memory for inference. As for training, inspired by a recent work (Boudiaf et al., 2020), which shows pair-based deep metric learning (DML) losses can be interpreted as optimizing the feature space from a generative perspective, we introduce Multi-Similarity (MS) loss (Wang et al., 2019) to obtain a good feature space for NCM classifier. Meanwhile, we prove theoretically that MS loss is an alternative to a training objective of the generative classifier. In this way, we can bypass logits bias.\\n\\nTo strengthen the model's capable of learning from new data in complex data streams, we further introduce an auxiliary proxy-based DML loss (Movshovitz-Attias et al., 2017). Therefore, our whole training objective is a hybrid of generative and discriminative losses. During inference, we ignore the discriminative objective and classify with the generative NCM classifier. By tuning weight of the auxiliary loss, our method can work well in different data streams.\\n\\nIn summary, our contributions are as follows:\\n\\n1. We make the first attempt to avoid logits bias problem in online CIL setting. In our generative framework, a generative classifier is introduced to replace softmax classifier for inference and for training, we introduce MS loss which is proven theoretically to optimize the model in a generative way.\"}"}
{"id": "ZumkmSpY9G4", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In order to improve the ability of MS loss to learn from new data, we further introduce an auxiliary loss to achieve a good balance between retaining old knowledge and learning new knowledge.\\n\\nWe conduct extensive experiments on four benchmarks in multiple online CIL settings, including a new task-free setting we design for simulating more realistic scenarios. Empirical results demonstrate our method outperforms a variety of state-of-the-art replay-based methods substantially, especially alleviating catastrophic forgetting significantly.\\n\\n**Related Work**\\n\\nCurrent CL methods can be roughly divided into three categories: which are regularization, parameter isolation and replay-based respectively (Lange et al., 2019).\\n\\n- **Regularization** methods retain the learned knowledge by imposing penalty constraints on model's parameters (Kirkpatrick et al., 2017) or outputs (Li & Hoiem, 2018) when learning new data. They work well in TIL setting but poor in CIL setting (van de Ven & Tolias, 2018).\\n\\n- **Parameter isolation** methods assign a specific subset of model parameters, such as network weights (Mallya & Lazebnik, 2018) and sub-networks (Fernando et al., 2017) to each task to avoid knowledge interference and thus the network may keep growing. This type of method is mainly designed for TIL as task identity is usually necessary during test.\\n\\n- The mainstream of **Replay-based** methods is ER-like (Rebuffi et al., 2017), which stores a subset of old data and retrains it when learning new data to prevent forgetting of old knowledge. In addition, generative replay method trains a generator to replay old data approximately (Shin et al., 2017).\\n\\nIn the field of online CL, most of methods are on the basis of ER. Chaudhry et al. (2019b) first explored ER in online CL settings with different memory update strategies. Authors suggested ER method should be regarded as an important baseline as in this setting it is more effective than several existing CL methods, such as A-GEM (Chaudhry et al., 2019a). GSS (Aljundi et al., 2019c) designs a new memory update strategy by encouraging the divergence of gradients of samples in memory. MIR (Aljundi et al., 2019a) is proposed to select the maximally interfered samples from memory for replay. GMED (Jin et al., 2020) edits the replay samples with gradient information to obtain samples likely to be forgotten, which can benefit the replay in the future. Mai et al. (2021) focus on online CIL setting and adopt the notion of Shapley Value to improve the replay memory update and sampling. All of the above methods are replay-based with softmax classifier. A contemporary work (Lange & Tuytelaars, 2020) proposes CoPE, which is somewhat similar to our method. CoPE replaces softmax classifier with a prototype-based classifier which is non-parametric and updated using features of data samples. However, the loss function of CoPE is still discriminative and the way to classify is analogous to softmax classifier.\\n\\nApart from the above ER based methods, Zeno et al. (2018) propose an online regularization method, however it performs very badly in online CL settings (Jin et al., 2020). Lee et al. (2020) propose a parameter isolation method in which the network is dynamically expanded and a memory for storing data is still required. Therefore, the memory usage is not fixed and potentially unbounded.\\n\\nA recent work (Yu et al., 2020) proposes SDC, a CIL method based on DML and NCM classifier. However, SDC requires an extra phase to correct semantic drift after training each task. This phase depends on task boundaries and the accumulated data of a task, which is not applicable for online CIL. In contrast, our method is based on ER and classifies with replay memory and thus need not correct the drift.\\n\\n**Online CIL \u2013 Incremental Learning With A Generative Framework**\\n\\n3.1 Preliminaries and Motivations\\n\\n3.1.1 Online CIL \u2013 Incremental Learning\\n\\nCIL setting has been widely used in online CL literature, e.g. (Aljundi et al., 2019a; Mai et al., 2021), and a softmax classifier is commonly used. A neural network $f(\\\\cdot; \\\\theta): X \\\\rightarrow \\\\mathbb{R}^d$ parameterized by $\\\\theta$ encodes data samples $x \\\\in X$ into a $d$-dimension feature $f(x)$ on which an FC layer $g$ outputs logits for classification: $o = Wf(x) + b$. At each iteration, a minibatch of data $B_n$ from a data stream $S$...\"}"}
{"id": "ZumkmSpY9G4", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"under review as a conference paper at ICLR 2022.\\n\\narrives and the whole model \\\\((f, g)\\\\) is trained on \\\\(B\\\\) only once. The training objective is cross-entropy (CE) loss:\\n\\n\\\\[\\nL_{CE} = -\\\\tilde{C} \\\\sum_{c=1}^{t} c \\\\log \\\\hat{y}_{c}, \\\\quad \\\\hat{y}_{c} = e^{o_c} \\\\sum_{\\\\tilde{C}} e^{o_c}(1)\\n\\\\]\\n\\nwhere \\\\(\\\\tilde{C}\\\\) is the number of classes seen so far. \\\\(t\\\\) is one-hot label of \\\\(x\\\\) and the subscript \\\\(c\\\\) denotes the \\\\(c\\\\)-th component. The new classes from \\\\(S\\\\) emerge over time. The output space of \\\\(g\\\\) is the number of seen classes and thus keeps growing. At test time, the model should classify over all \\\\(C\\\\) classes seen.\\n\\n3.1.2 EXPERIENCE REPLAY FOR ONLINE CONTINUOUS LEARNING\\n\\nER makes two modifications during online training: (1) It maintains a replay memory \\\\(M\\\\) with limited size which stores a subset of previously learned samples. (2) When a minibatch of new data \\\\(B_n\\\\) is coming, it samples a minibatch \\\\(B_r\\\\) from \\\\(M\\\\) and uses \\\\(B_n \\\\cup B_r\\\\) to optimize the model with one SGD-like step. Then it updates \\\\(M\\\\) with \\\\(B_n\\\\). Recent works, e.g. (Aljundi et al., 2019a; Mai et al., 2021) regard ER-reservoir as a strong baseline, which combines ER with reservoir sampling (Vitter, 1985) for memory update and random sampling for \\\\(B_r\\\\). See Chaudhry et al. (2019b) for more details about it.\\n\\n3.1.3 LOGITS BIAS IN SOFTMAX CLASSIFIER\\n\\nSome recent works (Wu et al., 2019; Belouadah & Popescu, 2019; Zhao et al., 2020) show in CIL scenarios, even with replay-based mechanism the logits outputted by model always have a strong bias towards the newly learned classes, which leads to catastrophic forgetting actually. In preliminary experiments, we also observe this phenomenon in online CIL setting. We run ER-reservoir baseline on 5-task Split CIFAR10 (each task has two disjoint classes) online CIL benchmark. In Figure 1a, we display the average logits of each already learned tasks over samples in test data after learning each task. The model outputs much higher logits on the new classes (of the task just learned) than old classes.\\n\\nFollowing (Ahn & Moon, 2020), we examine the CE loss in Eq (1), the gradient of \\\\(L_{CE}\\\\) w.r.t logit \\\\(o_c\\\\) of class \\\\(c\\\\) is \\\\(\\\\hat{y}_c - I[t_c = 1]\\\\). Thus, if \\\\(c\\\\) is the real label \\\\(y\\\\), i.e. \\\\(t_c = 1\\\\), the gradient is non-positive and model is trained to increase \\\\(o_c\\\\), otherwise the gradient is non-negative and model is trained to decrease \\\\(o_c\\\\). Therefore, logits bias problem is caused by the imbalance between the number of samples of the new classes and that of the old classes with a limited size of \\\\(M\\\\). As mentioned in Section 1, existing solutions (Wu et al., 2019; Belouadah & Popescu, 2019; Zhao et al., 2020) designed for conventional CIL need task boundaries to conduct extra offline training phases and even depend on the accumulated data of one task. They are not applicable for online CIL setting where task boundaries are not informed or even do not exist in task-free scenario.\\n\\n3.2 INFERENCE WITH A GENERATIVE CLASSIFIER\\n\\nProposed generative framework is based on ER strategy, and aims to avoid the intrinsic logits bias problem by removing the softmax FC layer \\\\(g\\\\) and build a generative classifier on the feature space \\\\(Z\\\\):\\n\\n\\\\[z = f(x)\\\\]\\n\\nIf the feature \\\\(z\\\\) is well discriminative, we can conduct inference with samples in \\\\(M\\\\) instead of a parametric classifier which is prone to catastrophic forgetting (Rebuffi et al., 2017).\\n\\nWe use NCM classifier firstly suggested by Rebuffi et al. (2017) for CL and show it is a generative model. We use \\\\(M_c\\\\) to denote the subset of class \\\\(c\\\\) of \\\\(M\\\\). The class mean \\\\(\\\\mu_c\\\\) is computed by\\n\\n\\\\[\\n\\\\mu_{M_c} = \\\\frac{1}{|M_c|} \\\\sum_{x \\\\in M_c} f(x)\\n\\\\]\\n\\nDuring inference, the prediction for \\\\(x^*\\\\) is made by:\\n\\n\\\\[\\ny^* = \\\\arg \\\\min_c \\\\|f(x^*) - \\\\mu_{M_c}\\\\|_2 (2)\\n\\\\]\\n\\nIn fact, the principle of prediction in Eq (2) is to find a Gaussian distribution \\\\(N(f(x^*)|\\\\mu_{M_c}, I)\\\\) with the maximal probability for \\\\(x^*\\\\). Therefore, assuming the conditional distribution \\\\(p(z|y=c) = N(z|\\\\mu_{M_c}, I)\\\\) and the prior distribution \\\\(p(y)\\\\) is uniform, NCM classifier virtually deals with \\\\(p(y|z)\\\\) by modeling \\\\(p(z|y)\\\\) in a generative way. The inference way is according to Bayes rule:\\n\\n\\\\[\\n\\\\arg \\\\max_y p(y|x^*) = \\\\arg \\\\max_y p(f(x^*)|y) p(y).\\n\\\\]\\n\\nThe assumption about \\\\(p(y)\\\\) simplifies the analysis.\"}"}
{"id": "ZumkmSpY9G4", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"In Table 6, we show the effect of Proxy-NCA loss weight $\\\\gamma$ on split datasets. We can find on MNIST and CIFAR10 where the number of classes is relatively small, although the best results are obtained with $\\\\gamma = 0$. However, on CIFAR100 and miniImageNet, as discussed in main text, the expected number of classes in each minibatch is larger which reduces the probability of occurrence of positive pair so that limits the learning ability of pair-based MS loss. At this time, it is necessary to introduce auxiliary PNCA loss. When $\\\\gamma$ is larger than 1, the performance begins to degrade, which implies that excessively focusing on discriminative PNCA loss will affect the performance of the generative NCM classifier.\\n\\nFigure 6: Average Accuracy on valid set of our method with different Proxy-NCA loss weight $\\\\gamma$ in the 6 Split datasets as reported in Table 1 of main text. We report the mean of 15 runs.\\n\\nFigure 7: Average Accuracy on valid set of ER-reservoir and our method with different size of $M$. \\n\\nNote: The figures show the trend of accuracy with varying parameters, but the specific values are not provided in the text.\"}"}
{"id": "ZumkmSpY9G4", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Table 9 and Table 10, we report the performance of our method and all baselines compared in main text on Split CIFAR100 and miniImageNet with 2k memory size. Our method performs best in all settings and the improvements are obvious, which is similar with the results in 1k memory size settings reported in Table 1-2 of main text. We also report Average Accuracy of ER-reservoir and our method on valid set with a various of memory sizes. Our method outperforms ER-reservoir consistently which shows broad applicability of our method. The improvements are relatively small when memory size is 500 on CIFAR100 and miniImageNet, which is due to the fact that when size of replay memory $M$ is too small (5 samples per class on average), class mean $\\\\mu_M^c$ cannot approximate the real class mean $\\\\mu_D^c$ well and thus proposed NCM classifier degrades.\\n\\nC.3 COMPARISON WITH TRIPLET LOSS\\n\\nYu et al. (2020) proposes SDC method for conventional CIL based on NCM classifier and another pair-based DML loss, triplet loss (Hoffer & Ailon, 2015). As discussed in Related Work of main text, SDC is not applicable for online CIL. To further clarify our contribution given this work, we evaluate the performance of NCM classifier after training the model with the hybrid objective $L_{\\\\text{Triplet}} + \\\\gamma L_{\\\\text{PNCA}}$, where $L_{\\\\text{Triplet}}$ represents triplet loss. Please see Hoffer & Ailon (2015) and (Yu et al., 2020) for details about $L_{\\\\text{Triplet}}$.\\n\\nWe report the results of the hybrid objective involving triplet loss and compare it with our hybrid loss ($L_{\\\\text{MS}} + \\\\gamma L_{\\\\text{PNCA}}$) in Table 11. We can find our proposed loss is superior to $L_{\\\\text{Triplet}} + \\\\gamma L_{\\\\text{PNCA}}$ consistently. In addition to replace softmax classifier with NCM classifier for online CIL problem, our contributions mainly reflect in introducing a hybrid of MS loss and PNCA loss in the view of training the generative NCM classifier. We believe the above results show MS loss is critical for online CIL, and introducing MS loss is not a trivial contribution, even if given the SDC work (Yu et al., 2020).\\n\\n| Methods | CIFAR100 (10-task) | CIFAR100 (20-task) | miniImageNet (10-task) | miniImageNet (20-task) |\\n|---------|-------------------|-------------------|-----------------------|-----------------------|\\n| fine-tune | 6.26 \u00b1 0.30 | 3.61 \u00b1 0.24 | 4.43 \u00b1 0.19 | 3.12 \u00b1 0.15 |\\n| ER-reservoir | 15.15 \u00b1 0.37 | 12.76 \u00b1 0.69 | 13.88 \u00b1 0.68 | 11.76 \u00b1 0.88 |\\n| A-GEM (Chaudhry et al., 2019a) | 6.50 \u00b1 0.16 | 3.61 \u00b1 0.08 | 4.41 \u00b1 0.14 | 3.23 \u00b1 0.13 |\\n| GSS-Greedy (Aljundi et al., 2019c) | 12.66 \u00b1 0.67 | 11.62 \u00b1 0.61 | 13.95 \u00b1 0.40 | 11.70 \u00b1 0.55 |\\n| MIR (Aljundi et al., 2019a) | 15.11 \u00b1 0.63 | 12.45 \u00b1 0.54 | 14.22 \u00b1 0.93 | 12.35 \u00b1 1.08 |\\n| GMED-ER (Jin et al., 2020) | 14.93 \u00b1 0.39 | 11.97 \u00b1 0.60 | 12.10 \u00b1 1.29 | 9.90 \u00b1 0.95 |\\n| GMED-MIR (Jin et al., 2020) | 15.11 \u00b1 0.50 | 12.01 \u00b1 0.81 | 13.90 \u00b1 0.58 | 12.25 \u00b1 0.59 |\\n| ASER $\\\\mu$ (Mai et al., 2021) $^*$ | 17.20 \u00b1 0.50 | \u2013 | 14.80 \u00b1 1.10 | \u2013 |\\n| Ours | 18.96 \u00b1 0.42 | 16.69 \u00b1 0.76 | 19.17 \u00b1 0.38 | 17.10 \u00b1 0.58 |\\n| i.i.d. online | 20.62 \u00b1 0.48 | 20.62 \u00b1 0.48 | 18.02 \u00b1 0.63 | 18.02 \u00b1 0.63 |\\n| i.i.d. offline | 45.59 \u00b1 0.29 | 45.59 \u00b1 0.29 | 38.63 \u00b1 0.59 | 38.63 \u00b1 0.59 |\\n\\nTable 9: Average Accuracy of 15 runs on Split datasets. Higher is better. $^*$ indicates the results are from the original paper. The size of memory $M$ is 2k.\"}"}
{"id": "ZumkmSpY9G4", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Methods\\n\\n| Dataset         | Fine-tune | ER-reservoir | A-GEM (Chaudhry et al., 2019a) | GSS-Greedy (Aljundi et al., 2019c) | MIR (Aljundi et al., 2019a) | GMED-ER (Jin et al., 2020) | GMED-MIR (Jin et al., 2020) | ASER \\\\(\\\\mu\\\\) (Mai et al., 2021) | Ours |\\n|-----------------|-----------|--------------|-------------------------------|-----------------------------------|---------------------------|---------------------------|---------------------------|--------------------------------|-------|\\n| CIFAR100 (10-task) | 51.60 \u00b1 0.77 | 40.19 \u00b1 0.71 | 51.45 \u00b1 0.68 | 40.02 \u00b1 0.81 | 39.82 \u00b1 0.79 | 42.99 \u00b1 0.75 | 43.78 \u00b1 0.93 | 38.60 \u00b1 0.60 | 26.59 \u00b1 0.58 |\\n| CIFAR100 (20-task) | 65.51 \u00b1 0.78 | 52.45 \u00b1 0.85 | 67.13 \u00b1 0.55 | 45.90 \u00b1 2.09 | 48.54 \u00b1 0.60 | 55.45 \u00b1 0.68 | 55.15 \u00b1 0.71 | \u2013 | \u2013 |\\n\\nTable 10: Average Forgetting of 15 runs on Split datasets. Lower is better. \\n\\n\\\\* indicates the results are from the original paper. The size of memory \\\\(M\\\\) is 2k.\\n\\n### Loss\\n\\n| Dataset         | \\\\(L_{\\\\text{Triplet}} + \\\\gamma L_{\\\\text{PNCA}}\\\\) | \\\\(L_{\\\\text{MS}} + \\\\gamma L_{\\\\text{PNCA}}\\\\) (Ours) |\\n|-----------------|-----------------------------------------------|-----------------------------------------------|\\n| MNIST (5-task)  | 87.06 \u00b1 0.52 | 88.79 \u00b1 0.26 |\\n| CIFAR10 (5-task) | 43.56 \u00b1 1.63 | 51.84 \u00b1 0.91 |\\n| CIFAR100 (10-task) | 13.21 \u00b1 0.28 | 15.56 \u00b1 0.39 |\\n| CIFAR100 (20-task) | 10.93 \u00b1 0.34 | 13.65 \u00b1 0.35 |\\n| miniImageNet (10-task) | 14.31 \u00b1 0.33 | 16.05 \u00b1 0.38 |\\n| miniImageNet (20-task) | 13.55 \u00b1 0.22 | 15.15 \u00b1 0.36 |\\n\\nTable 11: Comparison between MS loss and Triplet Loss. We report Average Accuracy of 15 runs on Split datasets. The size of memory \\\\(M\\\\) is 1k.\"}"}
{"id": "ZumkmSpY9G4", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Methods\\n\\nMNIST (5-task)\\nCIFAR10 (5-task)\\nCIFAR100 (20-task)\\nminiImageNet (20-task)\\n\\nmodified iCaRL  \\n34.58 \u00b1 1.18  \\n29.77 \u00b1 0.91  \\n5.38 \u00b1 0.26  \\n7.16 \u00b1 0.33\\n\\nmodified BiC  \\n83.33 \u00b1 1.35  \\n43.65 \u00b1 2.50  \\n8.72 \u00b1 0.30  \\n7.91 \u00b1 0.55\\n\\nOurs  \\n88.79 \u00b1 0.26  \\n51.84 \u00b1 0.91  \\n13.65 \u00b1 0.35  \\n15.15 \u00b1 0.36\\n\\nTable 3: The comparison between modified iCaRL, modified BiC and our method (Ours) on Split datasets.\\n\\nCIFAR-100 miniImageNet  \\n0  \\n250  \\n500  \\n750  \\n1000  \\n1250  \\n1500  \\n1750  \\n2000  \\n\\nTraining time (seconds)  \\nER-reservoir  \\nA-GEM  \\nGSS-Greedy  \\nMIR  \\nGMED-ER  \\nGMED-MIR  \\nCoPE  \\nOurs  \\n\\n(a)\\n\\nCIFAR-100 miniImageNet  \\n0  \\n1  \\n2  \\n3  \\n4  \\n5  \\n6  \\n7  \\n\\nTest time (seconds)  \\nSoftmax Classifier  \\nGenerative Classifier  \\n\\n(b)\\n\\nFigure 4: Comparison of training time (a) and test time (b) on Split CIFAR100 and miniImageNet. The number of tasks is 10.\\n\\nTraining  \\nInference  \\nCIFAR10 CIFAR100\\nCE loss  \\nSoftmax (Dis)  \\n39.88 \u00b1 1.52  \\n8.95 \u00b1 0.26\\n\\nCE loss  \\nNCM (Gen)  \\n44.46 \u00b1 0.95  \\n8.96 \u00b1 0.38\\n\\nMS loss  \\nNCM (Gen)  \\n51.72 \u00b1 1.02  \\n9.99 \u00b1 0.32\\n\\nPNCA loss  \\nNCM (Gen)  \\n41.91 \u00b1 1.78  \\n9.31 \u00b1 0.58\\n\\nHybrid loss  \\nProxy (Dis)  \\n48.16 \u00b1 1.21  \\n7.02 \u00b1 0.75\\n\\nHybrid loss  \\nNCM (Gen)  \\n51.84 \u00b1 0.91  \\n13.65 \u00b1 0.35\\n\\nTable 4: Ablation study on Split CIFAR10 and 20-task Split CIFAR100. We show the performances of different combinations of losses and inference ways. Dis: Discriminative, Gen: Generative.\\n\\nMethod  \\nCIFAR10 CIFAR100 miniImageNet  \\nfine-tune  \\n10.02 \u00b1 0.03  \\n1.02 \u00b1 0.03  \\n1.02 \u00b1 0.04\\n\\nER-reservoir  \\n20.89 \u00b1 2.07  \\n3.84 \u00b1 0.42  \\n6.85 \u00b1 0.70\\n\\nMIR  \\n18.75 \u00b1 2.53  \\n4.35 \u00b1 0.53  \\n6.09 \u00b1 1.04\\n\\nGMED-MIR  \\n18.78 \u00b1 2.31  \\n3.68 \u00b1 0.48  \\n7.22 \u00b1 0.81\\n\\nOurs  \\n34.18 \u00b1 0.81  \\n10.54 \u00b1 0.38  \\n12.24 \u00b1 0.19\\n\\ni.i.d online  \\n31.23 \u00b1 2.11  \\n18.08 \u00b1 0.62  \\n17.23 \u00b1 0.42\\n\\ni.i.d offline  \\n48.37 \u00b1 1.23  \\n42.68 \u00b1 0.37  \\n39.82 \u00b1 0.46\\n\\nTable 5: Final accuracy of 15 runs on Smooth datasets.\\n\\nold model after the last iteration. We use reservoir sampling for memory update. NCM classifier is used for inference. For modified BiC, we use the linear bias correction layer of BiC to correct the logits for all learned classes only before test as task boundaries are unavailable in online CIL setting.\\n\\nAs shown in Table 3, modified iCaRL performs very badly and the performances of modified BiC are much worse than our method. These results imply adapting existing methods to online setting can not alleviate logits bias effectively for online CIL. Therefore, our method makes a substantial contribution for online CIL.\\n\\nTime Comparison\\n\\nIn Figure 4a, we report the training time of different methods. The training time of our method is only a bit higher than ER-reservoir. Most baselines, such as GSS, MIR, and GMED, improve ER-reservoir by designing new memory update and sampling strategies which depend on extra gradient computations and thus are time-consuming. The inference costs of softmax classifier and our NCM classifier are displayed in Figure 4b. We can find the extra time of NCM to compute the class means is (about 3%) slight, as the size of memory is limited.\\n\\n4.3 RESULTS ON TASK-FREE Smooth DATASETS\\n\\nIn newly designed task-free Smooth datasets, the new classes emerge irregularly and the distribution on class changes at each time step. In Table 5, we compare our method with several baselines on three smooth datasets. The metric is final accuracy after learning the whole data stream. We can find these datasets are indeed more complex as fine-tune can only classify correctly on the last class, which is due to the higher imbalance of data streams. For this reason, baselines such as ER-reservoir and MIR degrade obviously compared with split datasets. However, our method performs best consistently.\\n\\n5 CONCLUSION\\n\\nIn this work, we tackle with online CIL problem from a generative perspective to bypass logits bias problem in commonly used softmax classifier. We first propose to replace softmax classifier with a generative classifier. Then we introduce MS loss for training and prove theoretically that it optimizes the feature space in a generative way. We further propose a hybrid loss to boost the model's ability to learn from new data. Experimental results show the significant and consistent superiority of our method compared to existing state-of-the-art methods.\"}"}
{"id": "ZumkmSpY9G4", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hongjoon Ahn and Taesup Moon. A simple class decision balancing for incremental learning. CoRR, abs/2003.13947, 2020. URL https://arxiv.org/abs/2003.13947.\\n\\nRahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In NeurIPS, 2019a. URL http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval.\\n\\nRahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In CVPR, 2019b. doi: 10.1109/CVPR.2019.01151. URL http://openaccess.thecvf.com/content_CVPR_2019/html/Aljundi_Task-Free_Continual_Learning_CVPR_2019_paper.html.\\n\\nRahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. In NeurIPS, 2019c. URL http://papers.nips.cc/paper/9354-gradient-based-sample-selection-for-online-continual-learning.\\n\\nEden Belouadah and Adrian Popescu. IL2M: class incremental learning with dual memory. In ICCV, 2019.\\n\\nGuillaume Bouchard and Bill Triggs. The tradeoff between generative and discriminative classifiers. 2004.\\n\\nMalik Boudiaf, J\u00e9r\u00f4me Rony, Imtiaz Masud Ziko, Eric Granger, Marco Pedersoli, Pablo Piantanida, and Ismail Ben Ayed. A unifying mutual information view of metric learning: Cross-entropy vs. pairwise losses. In ECCV, 2020. doi: 10.1007/978-3-030-58539-6_33. URL https://doi.org/10.1007/978-3-030-58539-6_33.\\n\\nArslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with A-GEM. In ICLR, 2019a. URL https://openreview.net/forum?id=Hkf2_sC5FX.\\n\\nArslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc'Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019b.\\n\\nXiaoan Ding, Tianyu Liu, Baobao Chang, Zhifang Sui, and Kevin Gimpel. Discriminatively-tuned generative classifiers for robust natural language inference. In EMNLP, 2020. URL https://www.aclweb.org/anthology/2020.emnlp-main.657/.\\n\\nChrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. CoRR, abs/1701.08734, 2017.\\n\\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In CVPR, 2006.\\n\\nElad Hoffer and Nir Ailon. Deep metric learning using triplet network. In ICLR Workshop, 2015. URL http://arxiv.org/abs/1412.6622.\\n\\nXisen Jin, Junyi Du, and Xiang Ren. Gradient based memory editing for task-free continual learning. CoRR, abs/2006.15294, 2020. URL https://arxiv.org/abs/2006.15294.\\n\\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 2017.\\n\\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\\n\\nMatthias De Lange and Tinne Tuytelaars. Continual prototype evolution: Learning online from non-stationary data streams. CoRR, abs/2009.00919, 2020. URL https://arxiv.org/abs/2009.00919.\\n\\nMatthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and Tinne Tuytelaars. Continual learning: A comparative study on how to defy forgetting in classification tasks. CoRR, abs/1909.08383, 2019. URL http://arxiv.org/abs/1909.08383.\"}"}
{"id": "ZumkmSpY9G4", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\\n\\nSoochan Lee, Junsoo Ha, Dongsu Zhang, and Gunhee Kim. A neural dirichlet process mixture model for task-free continual learning. In ICLR, 2020. URL https://openreview.net/forum?id=SJxSOJStPr.\\n\\nZhizhong Li and Derek Hoiem. Learning without forgetting. TPAMI, 2018.\\n\\nZheda Mai, Dongsub Shim, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, and Jongseong Jang. Online class-incremental continual learning with adversarial shapley value. AAAI, abs/2009.00093, 2021.\\n\\nArun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In CVPR, 2018.\\n\\nMichael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation. Elsevier, 1989.\\n\\nThomas Mensink, Jakob J. Verbeek, Florent Perronnin, and Gabriela Csurka. Distance-based image classification: Generalizing to new classes at near-zero cost. TPAMI, 2013. doi: 10.1109/TPAMI.2013.83. URL https://doi.org/10.1109/TPAMI.2013.83.\\n\\nYair Movshovitz-Attias, Alexander Toshev, Thomas K. Leung, Sergey Ioffe, and Saurabh Singh. No fuss distance metric learning using proxies. In ICCV, 2017.\\n\\nAndrew Y. Ng and Michael I. Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In NIPS, 2001. URL http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.\\n\\nGerman Ignacio Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 2019.\\n\\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In CVPR, 2017.\\n\\nMatthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In ICLR, 2019. URL https://openreview.net/forum?id=B1gTShAct7.\\n\\nAnthony V. Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connect. Sci., 7(2), 1995. doi: 10.1080/09540099550039318. URL https://doi.org/10.1080/09540099550039318.\\n\\nHanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In NIPS, 2017.\\n\\nGido M. van de Ven and Andreas S. Tolias. Three scenarios for continual learning. In NeurIPS Continual Learning workshop, 2018.\\n\\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 3630\u20133638, 2016. URL http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.\\n\\nJeffrey S. Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software (TOMS), 1985.\\n\\nXun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R. Scott. Multi-similarity loss with general pair weighting for deep metric learning. In CVPR, 2019. doi: 10.1109/CVPR.2019.00516. URL http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Multi-Similarity_Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.html.\\n\\nYue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In CVPR, 2019.\"}"}
{"id": "ZumkmSpY9G4", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proof. For simplicity, we ignore the coefficient $\\\\frac{1}{n}$ both in $L_{MS}$ and $L_{Gen} - Bin$ in the proof. We denote the two parts in the summation of $L_{MS}$ as $L^{+}_{MS}$ and $L^{-}_{MS}$ respectively, i.e.:\\n\\n$$L^{+}_{MS} = \\\\sum_{i=1}^{n} \\\\frac{1}{\\\\alpha} \\\\log\\\\left[1 + \\\\sum_{j \\\\in P_i} e^{-\\\\alpha (S_{ij} - \\\\lambda)}\\\\right]$$\\n\\n$$L^{-}_{MS} = \\\\sum_{i=1}^{n} \\\\frac{1}{\\\\beta} \\\\log\\\\left[1 + \\\\sum_{j \\\\in N_i} e^{\\\\beta (S_{ij} - \\\\lambda)}\\\\right]$$\\n\\nWithout hard mining, for $L^{+}_{MS}$ we have:\\n\\n$$L^{+}_{MS} = \\\\sum_{i=1}^{n} \\\\frac{1}{\\\\alpha} \\\\log\\\\left[1 + \\\\sum_{j \\\\in P_i} e^{-\\\\alpha (S_{ij} - \\\\lambda)}\\\\right] \\\\geq \\\\sum_{i=1}^{n} \\\\frac{1}{\\\\alpha} \\\\log\\\\left[\\\\sum_{j \\\\in P_i} e^{-\\\\alpha (S_{ij} - \\\\lambda)}\\\\right]$$\\n\\n$\\\\geq \\\\sum_{i=1}^{n} \\\\frac{1}{\\\\alpha} \\\\left[ \\\\frac{1}{n} \\\\sum_{j \\\\in P_i} e^{-\\\\alpha (S_{ij} - \\\\lambda)} \\\\right] = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\sum_{j \\\\in P_i} e^{-\\\\alpha (S_{ij} - \\\\lambda)}$\\n\\n$\\\\geq \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\sum_{j \\\\in P_i} e^{-\\\\alpha (S_{ij} - \\\\lambda)}$\\n\\n$\\\\geq \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\log\\\\left[\\\\sum_{j \\\\in P_i} e^{-\\\\alpha (S_{ij} - \\\\lambda)}\\\\right]$\"}"}
{"id": "ZumkmSpY9G4", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022 and works well in practice. In contrast, softmax classifier models $p(y|x^*)$ in a typical discriminative way.\\n\\nAs discussed above, online CIL is in a low data setting where generative classifiers are preferable compared to discriminative classifiers (Ng & Jordan, 2001). Moreover, generative classifiers are more robust to continual learning (Yogatama et al., 2017) and imbalanced data settings (Ding et al., 2020). At each iteration, $B_n \\\\cup B_r$ is also highly imbalanced. Considering these results, we hypothesis generative classifiers are promising for online CIL problem. It should be noted our method only models a simple generative classifier $p(z|y)$ on the feature space, instead of modeling $p(x|y)$ on the input space using DNNs (Yogatama et al., 2017), which is time-consuming and thus is not suitable for online training.\\n\\n3.3 Training with a Pair-based Metric Learning Loss from a Generative Perspective\\n\\nTo train the feature extractor $f(\\\\theta)$ we resort to DML losses which aim to learn a feature space where the distances represent semantic dissimilarities between data samples. From the perspective of mutual information (MI), Boudiaf et al. (2020) theoretically show the equivalence between CE loss and several pair-based DML losses, such as contrast loss (Hadsell et al., 2006) and Multi-Similarity (MS) loss (Wang et al., 2019). The DML losses maximize MI between feature $z$ and label $y$ in a generative way while CE loss in a discriminative way, which motivates us to train $f(\\\\theta)$ with a pair-based DML loss to obtain a good feature space $Z$ for the generative classifier.\\n\\nEspecially, we choose the MS loss as a training objective. MS loss is one of the state-of-the-art methods in the field of DML. Wang et al. (2019) point out pair-based DML losses can be seen as weighting each feature pair in the general pair weighting framework. As MS loss requires the feature $f(x)$ to be $\\\\ell_2$-normalized first, from now on, we use $z_i$ to denote the $\\\\ell_2$-normalized feature of $x_i$ and a feature pair is represented in the form of inner product $S_{ij} := z_i^T z_j$.\\n\\nTo weight feature pairs better, MS loss is proposed to consider multiple types of similarity. MS loss on a dataset $D$ is formulated as follows:\\n\\n$$L_{MS}(D) = \\\\frac{1}{|D|^2} \\\\sum_{i=1}^{|D|} \\\\left\\\\{ \\\\alpha \\\\log[1 + \\\\sum_{j \\\\in P_i} e^{-\\\\alpha(S_{ij} - \\\\lambda)}] + \\\\beta \\\\log[1 + \\\\sum_{j \\\\in N_i} e^{\\\\beta(S_{ij} - \\\\lambda)}] \\\\right\\\\}$$\\n\\n(3)\\n\\nwhere $\\\\alpha$, $\\\\beta$ and $\\\\lambda$ are hyperparameters and $P_i$ and $N_i$ represent the index set of positive and negative samples of $x_i$ respectively. MS loss also utilizes the hard mining strategy to filter out too uninformative feature pairs, i.e. too similar positive pairs and too dissimilar negative pairs:\\n\\n$$P_i = \\\\{ j | S_{ij} < \\\\max_{y_k \\\\neq y_i} S_{ik} + \\\\epsilon \\\\}$$\\n\\n$$N_i = \\\\{ j | S_{ij} > \\\\min_{y_k = y_i} S_{ik} - \\\\epsilon \\\\}$$\\n\\n(4)\\n\\nwhere $\\\\epsilon$ is another hyperparameter in MS loss. At each iteration we use MS loss on the union of new samples and replay samples $L_{MS}(B_n \\\\cup B_r)$ to train the model. The sampling of $B_r$ and update of $M$ are the same as ER-reservoir.\\n\\nTo show the connection between $L_{MS}$ and the generative classifier in Eq (2), we conduct some theoretical analyses. Proposition 1. Assume dataset $D=\\\\{(x_i,y_i)\\\\}_{i=1}^n$ is class-balanced and has $C$ classes each of which has $n_0$ samples. For a generative model $p(z,y)$, assume $p(y)$ actually obeys the uniform distribution and $p(z|y=c) = \\\\mathcal{N}(z|\\\\mu_Dc, I)$ where $\\\\mu_Dc = \\\\frac{1}{n_0} \\\\sum_{i=1}^{n_0} z_i I[y_i = c]$. For MS loss assume hard mining in Eq (4) is not employed. Then we have:\\n\\n$$L_{MS} - Bin \\\\geq L_{Gen} - Bin$$\\n\\n(5)\\n\\nwhere $c \\\\geq \\\\cdot$ stands for upper than, up to an additive constant $c$ and $L_{Gen} - Bin$ is defined in the following:\\n\\n$$L_{Gen} - Bin = -\\\\frac{1}{n} \\\\sum_{i=1}^n \\\\log p(z_i|y_i = y_i) + \\\\frac{1}{Cn} \\\\sum_{i=1}^n C \\\\sum_{c=1}^C \\\\log p(z_i|y_i = c)$$\\n\\n(6)\"}"}
{"id": "ZumkmSpY9G4", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The proof of Proposition 1 is in Appendix. Proposition 1 shows $L_{MS}$ is an upper bound of $L_{Gen - Bin}$ and thus is an alternative to minimizing $L_{Gen - Bin}$. The first term of $L_{Gen - Bin}$ aims to minimize the negative log-likelihood of the class-conditional generative classifier, while the second term maximizes the conditional entropy $H(Z|Y)$ of labels $Y$ and features $Z$. It should be noticed $L_{Gen - Bin}$ depends on modeling $p(z|y)$. With uniform $p(y)$, classifying using $p(z|y)$ equals to classifying using $p(z,y)$, and $H(Z|Y)$ is equivalent to $H(Z,Y)$, which can be regarded as a regularizer against features collapsing. Thus, $L_{Gen - Bin}$ actually optimizes the model in a generative way. The assumptions in Proposition 1 are similar with those in Section 3.2 about NCM classifier. The difference lies in that NCM classifier uses $\\\\{\\\\mu_M^c\\\\}$ computed on replay memory $M$ to approximate $\\\\{\\\\mu_D^c\\\\}$. Therefore, Proposition 1 reveals that MS loss optimizes the feature space in a generative way and it models $p(z|y)$ for classification which is consistent with the NCM classifier.\\n\\nThe real class means $\\\\{\\\\mu_D^c\\\\}$ depend on all training data and change with the update of $f(\\\\theta)$ so that are intractable in online settings. MS loss can be efficiently computed as it does not depend on $\\\\{\\\\mu_D^c\\\\}$ thus the model can be trained efficiently. During inference, we use approximate class means $\\\\mu_M^c$ to classify. In Figure 1b, on 5-task Split CIFAR10 benchmark, we empirically show compared to softmax classifier, on old tasks, our method achieves much higher accuracy and much lower forgetting, which implies MS loss is an effective objective to train the model $f$ and class mean $\\\\mu_M^c$ of replay memory is a good approximation of $\\\\mu_D^c$.\\n\\nWith discriminative loss like CE loss, the classifier models a discriminative model $p(y|z)$. Therefore, if training with discriminative loss and inference with NCM classifier based on the generative model $p(z|y)$, we can not expect to obtain good results. In the next section, experiments will verify this conjecture. In contrast, the way to train and inference are coincided in proposed generative framework.\\n\\n### 3.4 A Hybrid Generative/Discriminative Loss\\n\\nHowever, when addressing classification tasks, generative classifier has natural weakness, since modeling joint distribution $p(x,y)$ is much tougher than modeling conditional distribution $p(y|x)$ for NNs. Moreover, in preliminary experiments, we found if only trained with MS loss, the NCM classifier's performance degenerates as the expected number of classes in $B_n$ at each iteration increases. This phenomenon is attributed to the inadequate ability to learn from new data, instead of catastrophic forgetting. We speculate because the size of $B_n$ is always fixed to a small value (e.g. 10) in online CIL settings, the number of positive pairs in $B_n$ decreases as the expected number of classes increases.\\n\\nTo remedy this problem, we take advantage of discriminative losses for fast adaptation in online setting. To this end, we introduce Proxy-NCA (PNCA) (Movshovitz-Attias et al., 2017), a proxy-based DML loss, as an auxiliary loss. For each class, PNCA loss maintains \u201cproxies\u201d as the real feature to utilize the limited data in a minibatch better, which leads to convergence speed-up compared to pair-based DML losses. Concretely, when a new class $c$ emerges, we assign one trainable proxy $p_c \\\\in \\\\mathbb{R}^d$ to it. PNCA loss is computed as:\\n\\n$$L_{PNCA}(D) = -\\\\frac{1}{|D|} \\\\sum_{(x,y) \\\\in D} \\\\log e^{-\\\\|f(x) - p_y\\\\|^2_2} \\\\sum_{\\\\tilde{C} = 1} e^{-\\\\|f(x) - p_c\\\\|^2_2}$$\\n\\n(M7)\\n\\nMovshovitz-Attias et al. (2017) suggest all proxies have the same norm $N_P$ and all features have the norm $N_F$. The latter satisfies as in MS loss the feature is $\\\\ell_2$-normalized, i.e. $N_F = 1$. We also set $N_P = 1$ by normalizing all proxies after each SGD-like update. In this way, $L_{PNCA}$ is equivalent to a CE loss with $\\\\ell_2$-normalized row vectors of $W$ and without bias $b$, and thus we use PNCA instead of CE loss to keep utilizing the normalized features of MS loss. Our full training objective is a hybrid of generative and discriminative losses:\\n\\n$$L_{Hybrid} = L_{MS} + \\\\gamma L_{PNCA}$$\\n\\n(8)\\n\\nwhere $\\\\gamma$ is a hyperparameter to control the weight of $L_{PNCA}$. In general, generative classifiers have a smaller variance but higher bias than discriminative classifiers, and using such a hybrid loss can achieve a better bias-variance tradeoff (Bouchard & Triggs, 2004). Thus we think introducing the discriminative loss $L_{PNCA}$ can reduce the bias of model so that boost the ability to learn from new data.\\n\\nIt should be noticed we train the model with $L_{Hybrid}(B_n \\\\cup B_r)$, while we only use NCM classifier in Eq (2) for inference. In all experiments, we set $\\\\alpha = 2$, $\\\\beta = 50$, $\\\\epsilon = 0$ following Wang et al. (2019).\"}"}
{"id": "ZumkmSpY9G4", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The probability distribution on class of the sample at each time step on Split CIFAR10 (left) and Smooth CIFAR10 (right).\\n\\nFigure 3: Average Accuracy on already learned task during training.\\n\\n4 EXPERIMENTS\\n\\n4.1 EXPERIMENT SETUP\\n\\nDatasets\\nFirst, we conduct experiments on Split datasets which are commonly used in CIL and online CIL literature. On Split MNIST and CIFAR10, the datasets are split into 5 tasks each of which comprises 2 classes. On CIFAR100 and miniImageNet with 100 classes, we split them into 10 or 20 tasks. The number of classes in each task is 10 or 5 respectively. For MNIST we select 5k samples for training following Aljundi et al. (2019a) and we use full training data for other datasets. To simulate a task-free scenario, task boundaries are not informed during training (Jin et al., 2020).\\n\\nTo conduct a thorough evaluation in task-free scenarios, we design a new type of data streams. For a data stream with C classes, we assume the length of stream is n and \\\\( n_0 = \\\\frac{n}{C} \\\\). We denote \\\\( p_c(t) \\\\) as the occurrence probability of class c at time step t and assume \\\\( p_c(t) \\\\sim N(t | (2c - 1)n_0/2, n_0/2) \\\\). At each time step t, we calculate \\\\( p(t) = (p_1(t), ..., p_C(t)) \\\\) and normalize \\\\( p(t) \\\\) as the parameters of a Categorical distribution from which a class index \\\\( c_t \\\\) is sampled. Then we sample one data of class \\\\( c_t \\\\) without replacement. In this setting, data distribution changes smoothly and there is no notion of task. We call such data streams as Smooth datasets. To build Smooth datasets, we set \\\\( n = 5k \\\\) on CIFAR10 and \\\\( n = 40k \\\\) on CIFAR100 and miniImageNet, using all classes in each dataset. For all datasets, the size of minibatch \\\\( B_n \\\\) is 10. In Figure 2 we plot the probability distribution on class at each time step in the data stream generation process for Split CIFAR10 and Smooth CIFAR10. For better comparison, we set the length of data stream of two datasets both 5k. Split CIFAR10 has clear task boundaries and within one task the distribution on class is unchanged and uniform. However, On Smooth CIFAR10, the distribution on class keeps changing and there is no notion of task.\\n\\nBaselines\\nWe compare our method against a series of state-of-the-art online CIL methods, including: ER-reservoir, A-GEM, GSS, MIR, GMED, CoPE and ASER. We have briefly introduced them in Section 2. Specially, we use GSS-greedy and ASER \\\\( \\\\mu \\\\) which are the best variants in the corresponding paper. For GMED, we evaluate both GMED-ER and GMED-MIR. We also evaluate fine-tune baseline without any CL strategy. For all baselines and our method, the model is trained with 1 epoch, i.e. online CL setting. In addition, the performances of i.i.d online and i.i.d offline are also provided, by training the model 1 and 5 epochs respectively on i.i.d data streams. We reimplement all baselines except ASER \\\\( \\\\mu \\\\), whose results are from the original paper.\\n\\nModel\\nFollowing Aljundi et al. (2019a), the model \\\\( f \\\\) is a 2-layer MLP with 400 hidden units for MNIST and a reduced ResNet18 for other datasets. For baselines with ER strategy, the size of replay minibatch \\\\( B_r \\\\) is always 10. The budget of memory \\\\( M \\\\) is 500 on MNIST and 1000 on others. We use a relatively small budget \\\\( |M| \\\\) to mimic a practical setting. All models are optimized by SGD. The single-head evaluation is always used for CIL. More details about datasets, hyperparameter selection and evaluation metrics are in Appendix.\\n\\n4.2 RESULTS ON Split DATASETS\\n\\nOn Split datasets, we use Average Accuracy and Average Forgetting after training all tasks (Chaudhry et al., 2019b) for evaluation, which are reported in Table 1 and Table 2 respectively. For each metric, we report the mean of 15 runs and the 95% confidence interval.\"}"}
{"id": "ZumkmSpY9G4", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Table 1, we can find our method outperforms all baselines on all 6 settings. The improvement of our method is significant except on MNIST, where GMED-MIR is competitive with our method. An interesting phenomenon is all existing methods do not have a substantial improvement over ER-reservoir on CIFAR100 and miniImageNet, except ASER. We argue for online CIL problem, we should pay more attention to complex settings. Nevertheless, our method is superior to ASER obviously, especially on CIFAR10 and miniImageNet. Table 2 shows the forgetting of our method is far lower than other methods based on the softmax classifier, except on MNIST. Figure 3 shows our method is almost consistently better than all baselines during the whole learning processes. More results with various memory sizes can be found in Appendix.\\n\\nAblation Study\\n\\nWe also conduct ablation study about training objective and classifier in Table 4. The ER-reservoir corresponds to the first row and our method corresponds to the last row. Firstly, we find for ER-reservoir, replacing softmax classifier with NCM classifier makes a substantial improvement on CIFAR10. However, it has no effect on more complex CIFAR100 (row 1&2). Secondly, only using MS loss works very well on CIFAR10 while on CIFAR100 poor ability to learn from new data limits its performance (row 3&6). Lastly, when hybrid loss is used, the NCM classifier is much better than proxy-based classifier (row 5&6), and MS loss is critical for NCM classifier (row 4&6). Note that hybrid loss does not outperform MS loss much on Split-CIFAR10. This is because in Split-CIFAR10, a minibatch of new data contains a maximum of two classes, and thus the positive pairs are enough for MS loss to learn new knowledge well. These results verify our statement in Section 3.2.\\n\\nComparison with Logits Bias Solutions for Conventional CIL Setting\\n\\nAlthough existing CIL methods to alleviate logits bias are not applicable for online CIL settings as task boundaries are necessary, after being modified in some ways they can be adapted to online CIL. To better reflect the contribution of our method, we adapt iCaRL (Rebuffi et al., 2017) and BiC (Wu et al., 2019) to online CIL and compare modified iCaRL and modified BiC with our method in Table 3. iCaRL replaces softmax classifier with NCM classifier and BiC uses a linear bias correction layer to reduce logits bias. and iCaRL is modified in the following way: at each iteration, we minimize binary CE loss used by iCaRL which encourages the model to mimic the outputs for all learned classes of the current task. This modification helps iCaRL adapt to the online setting better, allowing it to achieve comparable performance to our method.\"}"}
{"id": "ZumkmSpY9G4", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nwhere c = stands for equal to, up to an additive constant. For $L_{\\\\text{MS}}$ we can write:\\n\\n$$L_{\\\\text{MS}} = n \\\\sum_{i=1}^{1} \\\\beta \\\\log \\\\left[ 1 + \\\\sum_{j:y_j \\\\neq y_i} e^{\\\\beta (S_{ij} - \\\\lambda)} \\\\right]$$\\n\\n$$c \\\\geq n \\\\sum_{i=1}^{1} \\\\beta \\\\log \\\\left[ \\\\frac{1}{n} \\\\left( C - 1 \\\\right) \\\\sum_{j:y_j \\\\neq y_i} S_{ij} \\\\right]$$\\n\\n$\\\\Delta$ Jensen's inequality\\n\\nAccording to Eq (10) and Eq (11), we have:\\n\\n$$L_{\\\\text{MS}} \\\\geq \\\\frac{1}{n} \\\\sum_{i=1}^{1} \\\\left( - \\\\log p(z_i | y = y_i) + \\\\frac{1}{C-1} \\\\sum_{c \\\\neq y_i} \\\\log p(z_i | y = c) \\\\right)$$\\n\\nNow we consider $L_{\\\\text{Gen}} - L_{\\\\text{Bin}}$. Firstly, it can be written:\\n\\n$$L_{\\\\text{Gen}} - L_{\\\\text{Bin}} \\\\geq \\\\frac{1}{C} \\\\sum_{c} \\\\left( - \\\\log p(z_i | y = y_i) + 1 \\\\right)$$\\n\\nFor convenience, we denote the features of data samples whose labels are $c$ as $\\\\{z_{ci}\\\\}_{i=1}^{n0}$. For the first part in the right hand of Eq (13), we have:\\n\\n$$\\\\sum_{i=1}^{n0} \\\\frac{1}{n} \\\\sum_{j:y_j \\\\neq y_i} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\geq \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( \\\\sum_{j:y_j \\\\neq y_i} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{1}{n} \\\\sum_{i=1}^{n0} \\\\sum_{j} \\\\|z_{ci} - \\\\mu_D_{c}\\\\|_2^2 \\\\right)$$\\n\\n$$= 1 \\\\sum_{c} \\\\frac{2}{C} \\\\sum_{i=1}^{n0} \\\\left( - \\\\frac{2}{n} \\\\sum_{i=1}^{n0} \\\\|\\\\mu_D_{c}\\\\|_2^2 + \\\\frac{"}
{"id": "ZumkmSpY9G4", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nKeep in mind that the mean of class $c$\\n\\n$$\\\\mu_D^c = \\\\frac{1}{n_0} \\\\sum_{i=1}^{n_0} z_{ci}.$$  \\n\\nFor the second part in the right hand of Eq (13), we have:\\n\\n$$n_0 \\\\sum_{i=1}^{C} \\\\sum_{c=1, c \\\\neq y_i} \\\\log p(z_i | y_i = c) = \\\\frac{1}{2} n_0 \\\\sum_{i=1}^{C} \\\\sum_{c=1, c \\\\neq y_i} -\\\\|z_i - \\\\mu_D^c\\\\|^2_2$$\\n\\n$$= \\\\frac{1}{2} n_0 \\\\sum_{i=1}^{C} \\\\sum_{c=1, c \\\\neq y_i} -\\\\|z_i\\\\|^2_2 + 2z_i^T \\\\mu_D^c - \\\\|\\\\mu_D^c\\\\|^2_2$$\\n\\n$$- \\\\left( \\\\sum_{j:y_j \\\\neq y_i} n_0 \\\\right) \\\\frac{1}{C - 1} \\\\sum_{c=1}^{C} \\\\|\\\\mu_D^c\\\\|^2_2$$\\n\\nAccording to Eq (14) and Eq (15), we have:\\n\\n$$L_{Gen} - Bin_c = n_0 \\\\sum_{i=1}^{\\\\frac{1}{2} \\\\sum_{j:y_j \\\\neq y_i} n_0 S_{ij} - \\\\sum_{j:y_j = y_i} (C - 1) n_0 S_{ij}}$$\\n\\n$(16)$\\n\\nAccording to Eq (12) and Eq (16), we can obtain:\\n\\n$$L_{MS_c} \\\\geq L_{Gen} - Bin_c$$\\n\\n$(17)$\\n\\nB.1 Split Datasets\\n\\nIn Table 6 we show the detailed statistics about Split datasets. On MNIST (LeCun et al., 1998), we randomly select 500 samples of the original training data for each class as training data stream, following previous works (Aljundi et al., 2019a;c; Jin et al., 2020). On CIFAR10 and CIFAR100 (Krizhevsky et al., 2009), we use the full training data from which 5% samples are regarded as validation set. The original miniImageNet dataset is used for meta learning (Vinyals et al., 2016) and 100 classes are divided into 64 classes, 16 classes, 20 classes respectively for meta-training, meta-validation and meta-test respectively. We merge all 100 classes to conduct class-incremental learning. There are 600 samples per class in miniImageNet. We divide 600 samples into 456 samples, 24 samples and 120 samples for training, validation and test respectively. We do not adopt any data augmentation strategy.\\n\\nB.2 Smooth Datasets\\n\\nFor a data stream with $C$ classes, we assume the length of stream is $N$ and $n_0 = N/c$. We denote $p_c(t)$ as the occurrence probability of class $c$ at time step $t$ and assume $p_c(t) \\\\sim N(\\\\frac{t}{2c - 1}, \\\\frac{n_0}{c})$. At each time step $t$, we calculate $p(t) = (p_1(t), ..., p_C(t))$ and normalize $p(t)$ as the parameters of a Categorical distribution from which a class index $c_t$ is sampled. Then we sample one data of class $c_t$ without replacement. In this setting, data distribution changes smoothly and there is no notion of task. We call such data streams as Smooth datasets.\\n\\nHere we show the characteristics of Smooth datasets in detail. In Table 7, we list the detailed statistics of Smooth datasets. Due to the randomness in the data stream generation process, the number of\"}"}
{"id": "ZumkmSpY9G4", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Details about Split datasets.\\n\\n| Dataset          | Image Size | #Classes | #Train Samples | #Valid Samples | #Test Samples | #Task | #Classes per Task |\\n|------------------|------------|----------|---------------|---------------|--------------|-------|------------------|\\n| MNIST            | (1,32,32)  | 10       | 5000          | 10000         | 10000        | 5     | 2                |\\n| CIFAR10          | (3,32,32)  | 10       | 47500         | 2500          | 10000        | 5     | 2                |\\n| CIFAR100         | (3,32,32)  | 100      | 47500         | 2500          | 10000        | 5     | 10 or 5          |\\n| miniImageNet     | (3,84,84)  | 100      | 45600         | 2400          | 12000        | 5     | 10 or 5          |\\n\\nSamples of each class is slightly imbalanced. On CIFAR100 and miniImageNet we set the mean number of samples of each class $n_0 = 400$ to avoid invalid sampling when exceeding the maximum number of sample of one class in the original training set. The last rows of Table 7 show the range of number of samples of each classes in our experiments. For example, in 15 repeated runs, 15 different Smooth CIFAR10 data streams are established. The number of samples of each class is in the interval $[464, 536]$. It should be emphasized that in all experiments we use the same random seed to obtain the identical 15 data streams for fair comparison.\\n\\nSome existing works (Zeno et al., 2018; Lee et al., 2020) also experiment on task-free datasets where a data stream is still divided into different tasks but the switch of task is gradual instead of abrupt. In fact, in their settings, task switching only occurs in a part of time, so that in the left time the distribution of data streams is still i.i.d. In contrast, in our Smooth datasets the distribution changes at the class level which simulates task-free class-incremental setting. In addition, the distribution of data streams is never i.i.d, which can reflect real-world CL scenarios better.\\n\\nIn Figure 5, we plot the curves of training loss of ER-reservoir on Split CIFAR10 and Smooth CIFAR10. When a new task emerges (each task consists of 100 iterations), the loss on Split CIFAR10 increases dramatically. Thus some methods can detect the change of loss to inference the task boundaries although they are not informed and conduct additional offline training phase (Aljundi et al., 2019b). However, such a trick is not applicable on Smooth datasets therefore only the real online CL methods can work.\\n\\nTable 7: Details about Smooth datasets.\\n\\n| Dataset          | #Classes ($C$) | #Train Samples (length of data stream) ($n$) | #Valid Samples | #Test Samples | Mean number of samples of one class ($n_0$) | Minimum number of samples of one class | Maximum number of samples of one class |\\n|------------------|----------------|---------------------------------------------|---------------|--------------|--------------------------------------------|---------------------------------------|---------------------------------------|\\n| CIFAR10          | 10             | 5000                                        | 2500          | 10000        | 500                                       | 464                                   | 536                                   |\\n| CIFAR100         | 100            | 40000                                       | 2500          | 10000        | 400                                        | 357                                   | 450                                   |\\n| miniImageNet     | 100            | 40000                                       | 2400          | 12000        | 400                                        | 354                                   | 446                                   |\"}"}
{"id": "ZumkmSpY9G4", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nFigure 5: The curves of training loss on Split CIFAR10 (left) and Smooth CIFAR10 (right). Split CIFAR10 comprises of 5 tasks, 100 iterations per task.\\n\\nmethod, we select $\\\\eta$ from $[0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]$. The hyperparameters of our method are displayed in Table 8.\\n\\n|                  | Split Datasets | Smooth Datasets |\\n|------------------|----------------|-----------------|\\n|                  | MNIST          | CIFAR10         | CIFAR100        | miniImageNet |\\n| $\\\\gamma$         | 0.1            | 0.5             | 1.0             | 0.5          |\\n| $\\\\eta$           | 0.05           | 0.2             | 0.35            | 0.2          |\\n\\nTable 8: Hyperparamters of PNCA loss weight $\\\\gamma$ and learning rate $\\\\eta$ for our method on different datasets.\\n\\nB.4 EVALUATION METRICS\\n\\nFollowing previous online CL works (Aljundi et al., 2019a; Mai et al., 2021), we use Average Accuracy and Average Forgetting (Chaudhry et al., 2019b) on Split datasets after training all $T$ tasks. Average Accuracy evaluates the overall performance and Average Forgetting measures how much learned knowledge has been forgetten. Let $a_{k,j}$ denote the accuracy on the held-out set of the $j$-th task after training the model on the first $k$ tasks. For a dataset comprised of $T$ tasks, the Average Accuracy is defined as follows:\\n\\n$$A_T = \\\\frac{1}{T} \\\\sum_{j=1}^{T} a_{T,j}$$  \\\\hspace{1cm} (18)\\n\\nand Average Forgetting can be written as:\\n\\n$$f_{T,j} = \\\\max_{l \\\\in \\\\{1, \\\\ldots, T-1\\\\}} a_{l,j} - a_{T,j}, \\\\quad \\\\forall j < T$$  \\\\hspace{1cm} (19)\\n\\n$$F_T = \\\\frac{1}{T-1} \\\\sum_{j=1}^{T-1} f_{T,j}$$  \\\\hspace{1cm} (20)\\n\\nWe report Average Accuracy and Average Forgetting in the form of percentage. It should be noted that a higher Average Accuracy is better while a lower Average Forgetting is better.\\n\\nOn Smooth datasets, as there is no notion of task, we evaluate the performance with the accuracy on the whole held-out set after finishing training the whole training set.\\n\\nB.5 CODE DEPENDENCIES AND HARDWARE\\n\\nThe Python version is 3.7.6. We use the PyTorch deep learning library to implement our method and baselines. The version of PyTorch is 1.7.0. Other dependent libraries include Numpy (1.17.3), torchvision (0.4.1), matplotlib (3.1.2) and scikit-learn (0.22). The CUDA version is 10.2. We run all experiments on 1 NVIDIA RTX 2080ti GPU. We will publish our codes once the paper is accepted.\"}"}
