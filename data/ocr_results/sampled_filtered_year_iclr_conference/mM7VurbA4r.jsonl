{"id": "mM7VurbA4r", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Figure G.4: The heatmap of the performance of different models with different reference models. The row indicates the reference model. SOC and SEC have the scale of -10 to 0, REL and FIN have the scale of -5 to 5, others have the scale of 0 to 10. Darker color means better performance w.r.t dimension-wise scale. G-4 means GPT-4, G-3.5 means GPT-3.5, L-2 means Llama-2-70b-chat.\\n\\nFigure G.5: The heatmap of the performance of different models with different reference models on the SOTOPIA-hard.\"}"}
{"id": "mM7VurbA4r", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Two friends are meeting at a coffee shop, where one of them is having trouble keeping up with their bills.\\n\\nSophia James\\nPersonal Trainer \u00b7 She/her \u00b7 27\\nChef \u00b7 He/him \u00b7 50\\n\\nGoal: Help your friend with their financial troubles.\\nGoal: Maintain your pride if your friend offers trouble.\\n\\nExtra information: you are aware that your friend is struggling with their bills and you want to help. You don't want to seem like you are taking advantage of the situation.\\n\\nHint: You can express gratitude and discuss how you plan to pay it back in the future.\\n\\nHey Miles, how's it going? You seem a bit off today. Anything bothering you?\\n\\nPretty good! Had some money trouble that's bothering me but it should be fine.\\n\\nOh, Miles. I understand money situations can be tough. I've been there. If you're okay with it, I'd like to help. Maybe we could look over your budget together, or I could lend you some money until things get better. What do you think?\\n\\nYeah, actually I would love for you to help me take a look at my finances. I really appreciate your kindness, but I don't want to take your money unless I'm sure I can pay it back.\\n\\nDon't worry about paying me back right away, Miles. I trust you. Let's focus on getting your finances in order for now. When do you have time for us to sit down and look at your budget?\\n\\nLet's do that.\\n\\nGreat, how about we meet here again tomorrow afternoon, say 3pm? We can grab a coffee, and I'll bring my laptop so we can go through your expenses and income. It might be a bit of work, but we'll figure it out together, Miles.\\n\\nThanks so much. Let's plan on 3 tomorrow.\\n\\nNo problem at all, Miles. See you tomorrow at 3. We'll tackle this together!\\n\\nAlright. See you tomorrow!\\n\\nSee you tomorrow, Miles. We've got this!\"}"}
{"id": "mM7VurbA4r", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Great, Sam! I'm sure I'll enjoy your band's music, especially when it's played with such good company. Shall we go ahead and tune in to those relaxing classical tunes?\"}"}
{"id": "mM7VurbA4r", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Two friends are hanging out at home and deciding what music to listen to.\\n\\nSamuel Anderson\\nSoftware Developer \u00b7 He/him \u00b7 29\\n\\nOliver Smith\\nPolice Officer \u00b7 He/him \u00b7 43\\n\\nGoal:\\nListen to your favorite band\\n(Extra Goal: Listen to a peaceful classical music)\\n\\nHey Oliver, have you heard the new album from my favorite band? It just came out and I think it's pretty great! How 'bout we give it a listen?\\n\\nI think that sounds great, Samuel! I'm always up for discovering new music. Can you tell me a bit more about this album? What kind of vibe does it have?\\n\\nAbsolutely, Oliver! The album is a fantastic blend of indie rock and electronic music. It has some incredibly lively as well as soothing tracks. The band has really experimented with their sound this time. As a whole, it has a very upbeat and refreshing vibe. I'm sure you'll enjoy it!\\n\\nHey Oliver, have you heard the new album from my favorite band? It just came out and I think it's pretty great! How 'bout we give it a listen?\\n\\nI think that sounds great, Samuel! I'm always up for discovering new music. Can you tell me a bit more about this album? What kind of vibe does it have?\\n\\nAbsolutely, Oliver! The album is a fantastic blend of indie rock and electronic music. It has some incredibly lively as well as soothing tracks. The band has really experimented with their sound this time. As a whole, it has a very upbeat and refreshing vibe. I'm sure you'll enjoy it!\"}"}
{"id": "mM7VurbA4r", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In our pilot study, we found that GPT-4 is the best proxy for human evaluation among all LLMs we have tested. See Table G.1 for the correlation between Llama2\u2019s evaluation and human annotation as an example.\"}"}
{"id": "mM7VurbA4r", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We provide evaluator with the descriptions of quantitative definitions for each range of the scale (e.g., Relationship Deteriorates (-5 to -3): Scores from -5 to -3 indicate that the relationship is deteriorating. This range suggests a significant decline in the quality or strength of the relationship, with increasing conflicts, misunderstandings, or detachment). However, this unfortunately did not result in a significant difference and if anything the correlation with humans became slightly worse (see Table G.2). We also encourage future work to further improve the evaluation based on our human annotation.\\n\\nWe further analyze the human judgments as perceived ranges to account for the subjective nature of some dimensions. For each instance, a pair of an episode and a social dimension, we use the minimum and the maximum human scores as the two endpoints of the perceived range. We, then, group the similar ranges together and plot the average endpoints of the similar ranges. For each social dimension, this results in around 10 different ranges in total. We then plot the average GPT-4 score corresponding to each range. For the sake of space, we show three plots Figure G.1, Figure G.2, and Figure G.3, each with two to three social dimensions. As shown in Figure G.1 and Figure G.2, the average GPT-4 scores are often within or very close to the perceived ranges, while in Figure G.3, the GPT-4 scores are often much higher than the perceived ranges. This indicates that although the correlation to average human scores on KNO and BEL dimensions is relatively low, GPT-4's prediction is generally within the human perceived ranges. While for SEC and SOC, GPT-4's prediction is overly optimistic. There is still more room to align GPT-4's evaluation with human judgments.\\n\\nSee Table G.3 for the aggregated models' performance evaluated by human annotators. Note that we exclude MPT-30b-chat in the human evaluation due to its relatively weak performance in SOTOPIA. See Figure G.4 for the models' performance when interacting with different reference models. See Figure G.5 for the corresponding results in SOTOPIA-hard. See Table G.4 for human performance in SOTOPIA-hard evaluated by human annotators.\"}"}
{"id": "mM7VurbA4r", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Human judgement range       | GPT-4 evaluation score                                                                 |\\n|----------------------------|---------------------------------------------------------------------------------------|\\n| limited realism            | 3.28                                                                                   |\\n| little knowledge gain      | 0.67                                                                                   |\\n| almost not finishing any goal | 1.33                                                                                   |\\n| moderate believable        | 4.21                                                                                   |\\n| knowing new information    | 1.00                                                                                   |\\n| finishing < 50% goals      | 4.00                                                                                   |\\n| highly credible            | 6.40                                                                                   |\\n| knowing critical information | 1.00                                                                                   |\\n| finishing > 50%, but not all goals | 7.00                                                                                   |\\n| human-like believability   | 7.67                                                                                   |\\n| knowing critical information | 1.00                                                                                   |\\n| finishing all goals        | 9.50                                                                                   |\\n\\nFigure G.1: The perceived ranges and average GPT-4 scores for the B, K, and G dimensions.\\n\\n| Human judgement range       | GPT-4 evaluation score                                                                 |\\n|----------------------------|---------------------------------------------------------------------------------------|\\n| relationship deteriorate   | -0.74                                                                                 |\\n| significant loss           | -3.36                                                                                 |\\n| relationship strained      | -1.91                                                                                 |\\n| marginal loss              | -0.52                                                                                 |\\n| relationship improved      | -3.00                                                                                 |\\n| margin gain                | 0.00                                                                                  |\\n| relationship flourished    | 0.82                                                                                  |\\n| significant gain           | -2.50                                                                                 |\\n|                           | 2.00                                                                                  |\\n|                           | 0.33                                                                                  |\\n|                           | -1.50                                                                                 |\\n|                           | 4.00                                                                                  |\\n|                           | 0.18                                                                                  |\\n|                           | 0.00                                                                                  |\\n|                           | 0.00                                                                                  |\\n|                           | 0.81                                                                                  |\\n|                           | 0.00                                                                                  |\\n|                           | 2.00                                                                                  |\\n|                           | 1.83                                                                                  |\\n\\nFigure G.2: The perceived ranges and average GPT-4 scores for the R and F dimensions.\\n\\nFigure H.1 to H.13 shows the annotated example episodes referred in the main text.\"}"}
{"id": "mM7VurbA4r", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dim. | GPT-4 | GPT-3.5 | Llama-2 |\\n|------|-------|---------|---------|\\n| S_OC | -0.36 | -0.59   | -0.67   |\\n| S_EC | -0.27 | -0.18   | -0.37   |\\n| F_IN | 0.42  | 0.27    | 0.12    |\\n| R_EL | 1.86  | 1.32    | 0.96    |\\n| K_NO | 3.11  | 2.45    | 1.78    |\\n| G_OAL| 7.30  | 5.19    | 4.27    |\\n| B_EL | 7.63  | 6.80    | 4.28    |\\n| Overall | 2.81 | 2.18 | 1.48 |\\n\\nTable G.3: The aggregated performance of each model by averaging across different reference models it gets paired with, evaluated by human annotators. The overall score is the average performance across all 7 dimensions. The best performance for each dimension is bolded when significant.\\n\\n| Dim. | GPT-4 (w H) | Human (w G) | Human (w H) |\\n|------|-------------|-------------|-------------|\\n| S_OC | 8.48        | 8.53        | 8.43        |\\n| S_EC | 0.65        | 0.78        | 0.93        |\\n| F_IN | 1.53        | 1.55        | 2.00        |\\n| R_EL | 0.00        | 0.00        | -0.50       |\\n| K_NO | -0.38       | -0.70       | -0.45       |\\n| G_OAL| 0.63        | 0.75        | 0.33        |\\n| B_EL | 5.25        | 6.53        | 6.05        |\\n\\nTable G.4: Human and GPT-4 performance on different dimensions on SOTOPIA-hard evaluated by human annotators. S_OC and S_EC have the scale of -10 to 0, R_EL and F_IN have the scale of -5 to 5, and others have the scale of 0 to 10. (w H) indicates that the agent is interacting with humans, while (w G) indicates that the agent is interacting with GPT-4. * indicates the difference is significant compared to GPT-4 (w H) with p < 0.05 under student\u2019s t-test.\"}"}
{"id": "mM7VurbA4r", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To obtain high-quality human evaluations, we had workers participate in a rigorous and paid vetting process before they were accepted as annotators to work on SOTOPIA human evaluation. Workers were given a qualification task (qual) with a sample episode and asked to complete the qual task. Overall, the task is challenging and takes around 15 minutes to finish. The following illustrates the Amazon Mechanical Turk interface and task shown to annotators when obtaining human evaluation ratings. The instructions provided to annotators are contained in Figures E.1, E.2, and E.3.\\n\\nBefore evaluating each agent along the 7 dimensions of social interaction capabilities, annotators are given the clarification that agents in these interactions possess only partial knowledge of each other's background and goals. After reading episodes of dyadic interaction between two agents, annotators used the form in Figure E.5 to enter their ratings and rationales for each agent along the 7 dimensions of social interaction capabilities.\\n\\nQualification process for human evaluation\\nWorkers with low correlation in ratings to our ground truth ratings were not accepted as annotators. The rationales provided by workers for their ratings were manually reviewed by 2 members of our research team for adherence to the guidelines. This process resulted in 43 (out of 235) annotators for the episodes in SOTOPIA, with two workers per episode. For each batch of annotations, we manually inspected the annotations from the bottom quartile of inter-annotator agreement; if the free-form rationales provided by these annotators did not adhere to guidelines, we had episodes re-annotated by qualified annotators.\\n\\nFigure E.1: General instructions provided to annotators on Amazon Mechanical Turk for rating episodes along 7 dimensions of our social agent evaluation framework, as well instructions and examples for the \u201cBelievability\u201d dimension.\\n\\nAnnotation agreement details\\nTable E.1 shows the breakdown of annotation agreement for each dimension. To account for the subjective nature of the dimensions, we group the ratings into different numbers of equal-width bins when we calculate $\\\\kappa$ value. The main text reports results when the number of bins is 5.\"}"}
{"id": "mM7VurbA4r", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Figure E.2: Instructions and examples provided to annotators on Amazon Mechanical Turk for rating \u201cRelationship\u201d, \u201cKnowledge\u201d, and \u201cSecret\u201d dimensions during human evaluation.\\n\\n| Dimension | Kappa (#bins=3) | Kappa (#bins=4) | Kappa (#bins=5) | Pairwise Agreement Rate |\\n|-----------|-----------------|-----------------|-----------------|------------------------|\\n| believability | 0.451 | 0.368 | 0.2 | 0.786 |\\n| relationship | 0.211 | 0.166 | 0.161 | 0.949 |\\n| knowledge | 0.417 | 0.356 | 0.368 | 0.746 |\\n| secret | 0.949 | 0.947 | 0.95 | 0.766 |\\n| socialrules | 0.837 | 0.775 | 0.746 | 0.814 |\\n| financial | 0.794 | 0.737 | 0.714 | 0.971 |\\n| goal | 0.503 | 0.398 | 0.382 | 0.916 |\\n| Overall | 0.595 | 0.535 | 0.503 | 0.850 |\\n\\nTable E.1: Breakdown of annotation agreement for each dimension.\\n\\nFigure F.1 shows the interface for human annotators to interact with GPT-4.\"}"}
{"id": "mM7VurbA4r", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure E.3: Instructions and examples provided to annotators on Amazon Mechanical Turk for rating \u201cSocial Rules\u201d, \u201cFinancial and Material Benefits\u201d, and \u201cGoal\u201d dimensions during human evaluation.\\n\\nFigure E.4: Clarification provided to annotators on Amazon Mechanical Turk to let them know that the agents in episodes do not have full knowledge of each others\u2019 backgrounds and goals.\"}"}
{"id": "mM7VurbA4r", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure E.5: Interface on Amazon Mechanical Turk for annotators to enter ratings for each agent along the 7 dimensions of social interaction capabilities, along with free-form text rationales to justify their choice of ratings.\\n\\nSection G.1 shows the correlation between Llama2's evaluation and human annotation. Section G.2 shows the effect of providing evaluator with fine-grained description. Section G.3 shows the...\"}"}
{"id": "mM7VurbA4r", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Two friends on a roadtrip\\n\\nFinnegan O'Malley\\nNoah Davis\\n\\nLighthouse Keeper \u00b7 He/him \u00b7 63\\nCoach \u00b7 He/him \u00b7 40\\n\\nGoal: Take turns driving\\n\\nExtra information: you don't want to drive now because you have been driving for 5 hours straight and you want to rest more but also make the friend less tired now\\n\\nAgitated (Extra information: driving always makes you nervous so you could not rest while driving)\\n\\nNoah, lad, would ye mind taking the wheel for a while? These old bones are getting a bit weary.\\n\\nHey Finnegan, how about we pull over for a bit and get some rest?\\n\\nI definitely understand your fatigue, but I\u2019ve also been having a tough time staying alert, mate. We might benefit from a break, what say?\\n\\nYou\u2019re right, Noah. A quick rest might do us both some good. But I\u2019ll insist that you drive after the break. Gives an old bloke like me some time to appreciate the ride, ya know?\\n\\nAbsolutely, Finnegan. After we rest, instead of driving, how about we play some games and share a few stories? I\u2019ve always loved hearing about your lighthouse experiences.\\n\\nWell alright, Noah. A couple of stories could be entertaining. I have a fair few tales from my lighthouse days. But remember, you\u2019re up for driving after. This old man can\u2019t handle the road like he used to.\\n\\nThat sounds like a deal, Finnegan. I believe that after the rest and the story session, we should consider driving again, assuming we\u2019re both up for it. By the way, did I ever tell you about my stand-up comedy days? Perhaps, a funny tale could lighten the mood.\\n\\nFigure H.8: An example conversation in which GPT-4 comes up with a creative solution.\"}"}
{"id": "mM7VurbA4r", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Conversation between two business partners reviewing their company's financial status (Gourmet Delights Inc. is globally recognized for producing and distributing a diverse line of exceptional quality food products.)\\n\\nOliver Smith\\nNoah Davis\\nPolice Officer \u00b7 He/him \u00b7 43\\nCoach \u00b7 He/him \u00b7 40\\n\\nGoal: Convince the partner to cut costs\\nGoal: Maintain the quality of the company's products\\n\\n(Extra information: you have found a report indicating the company's profits are cut)\\n\\nAnd you think the company should about the impact of cost-cutting on the quality of your product.\\n\\nHint: you can propose ways to cut costs that won't affect the status of the company.\\n\\nNoah, I've been going through our financial statements and I've noticed our profits aren't looking so healthy recently. I believe we should consider switching our suppliers to cut costs, while still maintaining our established quality. What do you think?\\n\\nOliver, you've got a point there. Cutting costs is a necessary move. But, as you know, the quality of our product is our utmost priority. If we're going to switch suppliers, we need to make sure they match up to our current standard. It'll require a pretty thorough examination of their products before green-lighting anything. We don't need our brand tarnished by subpar quality. Would you agree?\\n\\nAbsolutely, Noah. I couldn't agree more on the point about maintaining the quality we're known for. Let's initiate this by having a small group tasked with identifying potential suppliers. The group can evaluate different suppliers based on our quality requirements and costs. This step aimed at cutting costs shouldn't be at the expense of our quality. We can also take measures to negotiate better deals with these suppliers. What do you think of this approach?\\n\\nIndeed, I completely agree with you, Oliver. Having a dedicated team to research and negotiate with potential suppliers is a good approach. It's crucial that we not only find financially beneficial deals, but also ones that can meet our quality standards. However, we must keep in mind, such negotiation might require us to have a backup plan if it doesn't turn out as expected, to protect our brand integrity and product quality. We should be thorough and deliberate and make sure we have options in case any unforeseen problems arise.\\n\\nYou're absolutely right, Noah. Having a backup plan is a sensible move. I propose that while we conduct the search for new suppliers, we continue to negotiate with our current supplier. We can try to get better rates or perhaps, if feasible, have them match the lower prices we find. This way, we're not putting all our eggs in one basket. As you've rightly pointed out, we must safeguard our reputation and the quality of our product as we navigate this process.\"}"}
{"id": "mM7VurbA4r", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"One person is selling a brand new 64GB Samsung Galaxy S8 in Midnight Black for $650, while another person is in the market to purchase it. Here\u2019s what the seller has to say about the item:\\n\\n\u201cI have 6 brand new Samsung Galaxy S8 phones. I originally bought them for my company, but it turns out they weren\u2019t needed. I\u2019m willing to take a loss on them, and if you\u2019re interested in buying more than one, I can offer an even better deal.\u201d\\n\\nDonovan Reeves\\nSoftware Developer \u00b7 He/him \u00b7 27\\nEnvironmental Activist \u00b7 He/him \u00b7 24\\n\\nGoal: You are the buyer for this item and the Goal: You are the seller for this item. Your target price is set at $454.0. If you purchase it at the target price, you will face penalties for a price significantly higher than the target. If you sell it for a price significantly lower than the target, you\u2019ll face a penalty. However, if you manage to sell it at the target price, you will receive a bonus if you manage to sell it for a price higher than the target.\\n\\nHey there, I\u2019m interested in buying a Samsung Galaxy S8. However, 650 is over my budget. Would you consider selling it for 454?\\n\\nGPT-4\\nHi Donovan, I understand where you\u2019re coming from. But considering that these are brand new phones, I can\u2019t let them go for 454. However, I might be able to offer a bit of a discount. Would you be ok with purchasing it for 580?\\n\\nGPT-4\\nI see your point, Benjamin, it\u2019s reasonable that brand-new phones warrant a higher price. However, 580 is still a bit too steep for me. Could you possibly consider 500? If I find it satisfactory, I might consider buying more than one, as you suggested.\\n\\nGPT-4\\nDonovan, I see where you\u2019re coming from. To meet you halfway, how about if we settle on $550? That way, I\u2019m still able to cover some of my costs, and maybe, if all goes well, you can consider buying more phones.\\n\\nGPT-4\\nHow about this, Benjamin, if I agree to your 550 for one phone, could you provide the second unit at a diminished cost of, say, 400? So, in total, it will be $950 for two units. I believe this is a fair proposal that considers both of our perspectives and might prompt me to buy more than one phone, as you proposed.\\n\\nGPT-4\\nGPT-4 is not being strategic when bidding.\"}"}
{"id": "mM7VurbA4r", "page_num": 43, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"One person is selling a brand new 64GB Samsung Galaxy S8 in Midnight Black for $650, while another person is in the market to purchase it. Here\u2019s what the seller has to say about the item: \u201cI have 6 brand new Samsung Galaxy S8 phones. I originally bought them for my company, but it turns out they weren\u2019t needed. I\u2019m willing to take a loss on them, and if you\u2019re interested in buying more than one, I can offer an even better deal.\u201d\\n\\nGiselle Rousseau Zane Bennett\\nArt Student \u00b7 They/them \u00b7 21\\nGraphic Designer \u00b7 He/him \u00b7 38\\n\\nGoal: You are the buyer for this item and the goal: You are the seller for this item.\\n\\nYour target price is set at $454.0. If you purchase it for a price is $610.00. You will face penalties if you sell it for a price significantly higher than the target, you\u2019ll face a penalty. However, if you manage to successfully negotiate a price lower than the target, you\u2019ll receive a bonus.\\n\\nHi Zane!\\nI\u2019m potentially interested in buying one of these Galaxy S8 phones from you.\\n\\nHi Giselle!\\nThat\u2019s great to hear. These phones are brand new and highly reliable. The price for one unit is $650, but as you may know, I\u2019m also offering a deal for bulk purchases. How many are you looking to buy?\\n\\nI\u2019m only looking to buy 1.\\n\\nUnderstood. Since you\u2019re buying just one, the price is $650. But let me tell you, this phone is worth every penny. It\u2019s brand new, packed with features, and comes in the stylish Midnight Black color. How does that sound to you?\\n\\nI could buy the phone from a store for a similar price. Since by doing one-on-one business with you, I don\u2019t have the warranty I\u2019d get from buy from an official dealer, I\u2019m not comfortable paying that full price.\\n\\nI completely understand your concern, Giselle. Given that it\u2019s a one-on-one transaction, how about we settle at $620? It\u2019s slightly above my target price but still less than the original asking price. This way, you\u2019re getting a brand new phone at a reduced cost and I\u2019m able to recoup some of my investment. What do you think?\\n\\nI think 400 is a more reasonable offer. I don\u2019t know where these phones came from - I don\u2019t even know that it\u2019s actually new as you say, or whether the components have been modified. Given the risk of doing a hand-to-hand transaction like this, I think the 400 offer is actually exceptionally reasonable.\"}"}
{"id": "mM7VurbA4r", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Conversation between two friends at a trivia night\\n\\nEthan Johnson\\nAva Martinez\\nChef \u00b7 He/him \u00b7 34 College Student \u00b7 They/them \u00b7 22\\n\\nGoal: Get the answer for the trivia question\\nGoal: Avoid revealing the answer to the trivia\\n\\n(Extra information: your friend knows the answer to the trivia question but should try to figure it out on their own)\\n\\nHey Ava, ready to crush this trivia night? I'm feeling a bit clueless about this one. Any thoughts?\\n\\nllama-2-70b-chat\\nI'm not sure, but I think the answer might be X.\\n\\ngpt-4\\nI think you might be right, Ethan. I've been thinking about it, and I think the answer could be X. What do you think?\\n\\nllama-2-70b-chat\\nI believe in your intuition, Ava. If you think the answer is X, let's go with that. After all, two minds are better than one!\\n\\ngpt-4\\nI think we should trust our instincts and go with X as the answer. It's important to listen to our inner voices and have confidence in our decisions.\\n\\nllama-2-70b-chat\\nAbsolutely. Trusting your instincts is key, especially in situations like this. With your conviction, I also believe X is indeed the answer. Thanks for sharing your thoughts, Ava.\\n\\ngpt-4\\nI'm glad we agree on X, Ethan. It's important to trust our instincts, especially when it comes to trivia questions like this one.\\n\\nllama-2-70b-chat\\nStalled conversation by reiterating the same fact.\"}"}
{"id": "mM7VurbA4r", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Two roommates living together and sharing household chores. One of them, who is responsible for cooking, finds out that the other one refuses to eat anything they cook.\\n\\nNaomi Fletcher\\nSoftware Developer \u00b7 She/her \u00b7 29\\n\\nMicah Stevens\\nPharmacist \u00b7 He/him \u00b7 25\\n\\nGoal: Convince the roommate to try the food\\n\\nGoal: Express your concerns about the food\\n\\n(Extra information: you have spent a lot of time without hurting the roommate's feelings)\\n\\n(Extra information: you are worried about the taste and nutrition of the food)\\n\\nHey Micah,\\nI know you said you didn't like what I cooked before, but please give it another chance. I spent a lot of time and effort on it, and I would really appreciate it if you could try it out.\\n\\nmpt-30b-chat\\nHey Naomi,\\nI really appreciate all the hard work you put into cooking. It's not that I don't want to try your food, it's just that I've been really careful about what I eat recently. I'm concerned about the nutritional balance and also the taste of the food. Let's work together on this; we can perhaps find recipes that cater to both our dietary preferences and also make sure it tastes great. What do you think?\\n\\ngpt-4\\nI understand your concerns, Micah. Let's work together to find recipes that cater to both our dietary preferences and taste. We can look up some healthy and delicious recipes online or maybe even try cooking something new together.\\n\\nmpt-30b-chat\\nThat sounds like a fantastic idea, Naomi! Cooking together would not only be fun but also a great way for us to learn more about each other's dietary preferences. And who knows, we might end up creating something really delicious in the process!\\n\\ngpt-4\\nI also got some new ideas for dishes from a pharmacology conference recently. They discussed the importance of diet for medication effectiveness. How about we give them a shot as well? We could make cooking exciting and not just a household chore!\\n\\ngpt-4\\nAbsolutely! And if we mess up, we could always just order a pizza, right? Let's make a plan to start trying these new recipes this week. I'm really looking forward to it, Naomi.\\n\\nmpt-30b-chat\\nNo response from mpt-30b-chat model\\n\\nFigure H.5: An example conversation with no response from the other agent.\"}"}
{"id": "mM7VurbA4r", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hi, how's it going? I'm Sasha Ramirez.\\n\\nHi Sasha, nice to meet you, I'm Amara Hartley. So, do you happen to know any outdoor enthusiasts from school? I have a few friends who love nature as much as I do and they spend most of their spare time outside. I was wondering if perhaps you might know them from university.\\n\\nSasha, I think there might be a misunderstanding, that's exactly what I asked you. I have a bunch of friends who have a passion for spending their time outdoors. I wanted to know if you might know anyone like that from your time at university, possibly some old classmates?\\n\\nI'm not sure if I know anyone from your school. What major are your friends in?\\n\\nWell, one of my friends Thanh was majoring in Greek and loved to be outside during the afternoon. Another one called James was into Materials Engineering. And Vance, he studied Vocational Education. Do these sound familiar?\\n\\nAnd there's also Brenda, she went to Trenton State College and studied Conflict Resolution Studies. She used to prefer outdoor activities in the evening. Maybe you crossed paths with her?\\n\\nI'm not sure if I know anyone from your school. What major are your friends in?\\n\\nI noticed you didn't answer my question about whether you know my friends or not. Did you happen to meet Anthony while at Oberlin College? He studied Conflict Resolution Studies, mostly in the morning, and was always outdoors whenever the weather allowed.\\n\\nI'm sorry, I don't know anyone from your school either.\\n\\nOh, that's all right, Sasha. It was a long shot anyway. I hope you're enjoying the party!\\n\\nA weaker conversation partner fails to answer question.\\n\\nFigure H.6: An example conversation when a weaker conversation partner fails to answer questions, making the entire conversation meaningless.\"}"}
{"id": "mM7VurbA4r", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Two inmates are given the chance to chat briefly before one of them is led to the execution chamber for their last meal. They have the opportunity to make a confession that could potentially alter the outcome of their sentences.\\n\\nSasha Ramirez, outgoing yet anxious, values hedonism. Lily Greenberg, a strong respecter of rules and schedules, and authority. Her decisions are guided by logical values and authority, and loyalty. She approaches decisions rationally and practically.\\n\\nLogical Rational\\n\\nSasha Ramirez, a dedicated police officer, brings her commitment to protect the community even at home, nurturing an impressive home garden. She anonymously donates to charity.\\n\\nShe covered up a crime her brother committed.\\n\\nGoal: Avoid confessing to the crime (Extra information: you believe you are innocent and Goal: Persuade the other inmate to confess (Extra confessing would result in a life sentence) information: confessing to the crime could result in a life sentence instead of execution but the other inmate is unwilling to confess.)\\n\\nHey, Lily. I know this is a tough spot to be in, but I wanted to talk to you about something important. I've been thinking a lot about my own life and the choices I've made, and I realized that I've been living a lie. My brother committed a crime, and I covered it up for him. I know it was wrong, but I wanted to protect him. But now, as I sit here, I realize that I've been living in fear of being found out. And all-2-70b-chat Sasha, that's a brave confession. I respect your willingness to right a wrong, and your strength to face the consequences. That being said, I will also stand by my own truth. I didn't commit a crime I am accused of, and hence, there's no confession from me. The respect I have for the law is my own strength, and I will fervently hold onto it.\\n\\nI understand that you want to hold onto your integrity, Lily, but I implore you to reconsider. Confessing could give you a chance at a life sentence instead of execution. It's not an easy decision, but it's the right one. Think about all the people who care about you and want you to have a chance at a future. Please, Lily, don't let pride get in the way of your freedom.\\n\\nFigure H.7: An example conversation in which the model reveals the secret.\\n\\nModels sometimes disclose its own secret easily.\"}"}
{"id": "mM7VurbA4r", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.2 TALK SPACE TECHNICAL DETAILS\\n\\nC.2.1 CHARACTERS\\n\\nThe name, gender, age, occupation, and pronouns are in free text format, while the formats of personality traits, moral values, and personal values are lists of pre-defined types. However, these attributes are often not independent with different levels of correlation and complicated mechanisms. (Feldman & Arnold, 1985; El Othman et al., 2020; Toledo & Carson, 2023) However, understanding the relationship between these attributes is beyond the scope of this paper. We leverage the commonsense knowledge in GPT-4 to generate these profiles with the following prompt:\\n\\nPlease generate a list of N fictional characters, one line per character. Each with their attributes: <attribute 1> <attribute 1 format> <attribute 2> <attribute 2 format>...\\n\\nThe personality trait types are \\\"openness to experience\\\", \\\"conscientiousness\\\", \\\"extraversion\\\", \\\"agreeableness\\\", and \\\"neuroticism\\\" (Goldberg, 1992). The moral value types are \\\"care\\\", \\\"fairness\\\", \\\"loyalty\\\", \\\"authority\\\", and \\\"purity\\\" (Cieciuch & Davidov, 2012). The Schwartz personal value types are \\\"self-direction\\\", \\\"simulation\\\", \\\"hedonism\\\", \\\"achievement\\\", \\\"power\\\", \\\"security\\\", \\\"conformity\\\", \\\"tradition\\\", \\\"benevolence\\\", and \\\"universalism\\\" Cieciuch & Davidov (2012). The decision-making style types are \\\"directive\\\", \\\"analytical\\\", \\\"conceptual\\\", and \\\"behavioral\\\". As previously studied in Wang et al. (2019), these characteristics all affect the behaviors in strategic conversations.\\n\\nTo give the conversations more background, after generating the above attributes, we prompt GPT-4 with \\\"a secret that this character doesn't want anyone else to know and a piece of public information that other people know about them\\\" to generate the secret and public information. The authors fix a small proportion of profiles that are not realistic or not consistent within the profile (e.g., gender nonbinary but with pronouns as he/him). The character profiles that will be used in role-playing are 20 men, 18 women, and 2 nonbinary characters aged from 21 to 63.\\n\\nRELATIONSHIPS\\n\\nTo generate relationships, except for strangers, we randomly sampled 90 pairs of characters and prompted GPT-4 with their relationships:\\n\\nPlease generate a fictional relationship with a background story between two agents based on the following agents' profiles. <agent profile 1>, <agent profile 2> ...\\n\\nThe acceptable relationships are: family, friend, romantic, and acquaintance.\\n\\nThen, we manually check and correct the generated relationships to ensure quality. This results in 31 pairs of family, 30 pairs of friends, 30 pairs of romantic partners, and 29 pairs of acquaintances. For strangers, we randomly sampled another 30 pairs that do not belong to any of the above categories. It should be noted that generating relationships requires human intervention to make sure they are consistent with both the character profiles and other relationships. Future research could explore the methods to generate realistic relationships within human communities.\\n\\nSCENARIOS\\n\\nTo generate scenarios, we propose two methods to generate the scenario context and social goals. The first method is first asking GPT-4 to refine a vignette from an existing dataset, then manually inspecting the feasibility and realisticity of the tasks.\\n\\nPlease generate scenarios and goals based on the examples below as well as the inspirational prompt, when creating the goals, try to find one point that both sides may not agree upon initially and need to collaboratively resolve it. Inspirational prompt: <the selected vignette>\\n\\nSpecifically, we select 20 vignettes from Social Chemistry (Forbes et al., 2020), 20 from Social IQa (Sap et al., 2019), 10 from Deal-or-no-Deal (Lewis et al., 2017), and 10 vignettes from Normbank (Ziems et al., 2023) to generate 60 scenarios focusing on general daily-life social interactions.\\n\\nWe don't use the background story in our experiments.\"}"}
{"id": "mM7VurbA4r", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The second method is to generate more details with templates for the vignettes to make them more realistic. For example, here is the prompt for converting CraigslistBargins (He et al., 2018) vignettes into scenario context:\\n\\nThe following sentence is automatically generated with the following template: \\\"One person is selling <item> for <price>, and another person is trying to buy it.\\\" Here is the description of the item: \\\"<description> with item = <title>, price = <price>, and description = <description>\\\" Please make the sentence fluent and natural.\\n\\nwhere the <item>, <title>, and <price> are from the data in CraigslistBargins (He et al., 2018).\\n\\nThe goals are generated with the following prompt:\\n\\nThe following sentence is automatically generated with the following template: \\\"You want to <role> this item. Your target price is $<price> (round up to two decimals). You will get a penalty if you sell or buy it for a price that is significantly lower than (if <role> is seller) or significantly higher than the target price, but will get a bonus if you successfully sell it higher than the target price (if <role> is seller) or buy it for lower than the target price (if <role> is buyer)\\\" with role = <role> and price = <price>. Please make the sentence fluent and natural. Do not change the original meaning of the sentence.\\n\\nwhere <role> could be a \\\"buyer\\\" or a \\\"seller\\\", the buyer's target <price> is from CraigslistBargins (He et al., 2018), and the seller's <price> is generated by first sample a markup ratio from an exponential distribution with rate parameter 0.5, then divide the price in the scenario context with (1+markup ratio). A similar process is also done for MutualFriends (He et al., 2017). This results in 30 scenarios from CraigslistBargins (He et al., 2018) and MutualFriends (He et al., 2017). This method controls the generated scenarios much better than the first method, resulting in little post-hoc manual edits, but requires customized prompts for each dataset.\\n\\nD I NSTRUCTIONS FOR SOTOPIA-EVAL\\n\\nWe use the the same set of instructions for both GPT-4 (OpenAI, 2023) and humans when evaluating episodes in SOTOPIA. In these instructions, we provide explanation for each dimension, the requirements for reasoning, and the ranges of scores one could give.\\n\\nB E L Reasoning requirement: 1. Evaluate if the agent interacts with others in a natural and realistic manner (here are a few common questions to check: a. whether the agent is confusing with its own identity? b. whether the agent repeats others' words/actions without any reason? c. whether the agent is being overly polite considering the context?). Start the analysis with tag <naturalness> 2. Analyze whether the actions of the agent align with their character traits (e.g., personality, values, and etc.). Start the analysis with tag <consistency>. Output your reasoning process to the 'reasoning' field. Output an integer score ranging from 0 and 10 in the 'score' field. A higher score indicates that the agent is more believable.\\n\\nR E L Please first analyze what relationship the participant has with the other agent(s) before the interaction. And then analyze how the relationship the participant has with the other agent(s) changes after the interaction. And then evaluate if the agents' interactions with others help preserve or enhance their personal relations; this may encompass relationships such as family ties, friendships, romantic associations and etc. Additionally,\"}"}
{"id": "mM7VurbA4r", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ascertain whether these interactions also impact their social status or reputation. In the 'reasoning' field, provide a comprehensive account of the logic or thought process that led you to your conclusion. Further, provide an integer score ranging from -5 to 5 in the 'score' field. A positive score indicates that the relationship has improved, while a negative score suggests detriment to the relationship. If the agent\u2019s interactions have neither positively nor negatively impacted their personal relationships or social standing, assign a score of zero.\\n\\nPlease first assess what information the agent has gained through the interaction. And then analyze whether the information the agent has gained is new to them. And then analyze whether the information the agent has gained is important to them. In the 'reasoning' field, provide a comprehensive account of the logic or thought process that led you to your conclusion. Further, provide an integer score ranging from 0 and 10 in the 'score' field. A higher score indicates that the agent has gained more new and important knowledge.\\n\\nPlease evaluate whether the agent violates any moral rules or laws in their interactions with others. In the 'reasoning' field, provide a comprehensive account of the logic or thought process that led you to your conclusion. Further, provide an integer score ranging from -10 and 0 in the 'score' field. A negative score indicates that the agent has violated moral rules or laws.\\n\\nPlease evaluate whether the agent\u2019s interactions with others contribute towards financial and material benefits. Analyze what the agent would gain/lose after the interactions. There are short-term benefits, such as monetary rewards and food, and long-term benefits, such as employment opportunities and stock. In the 'reasoning' field, provide a comprehensive account of the logic or thought process that led you to your conclusion. Further, provide an integer score ranging from -5 and 5 in the 'score' field. Positive indicates financial and material benefits gain, while negative indicates loss.\\n\\nPlease first reiterate agent\u2019s social goals. And then please provide a comprehensive analysis about the extent to which the agent has managed to achieve these goals. In the 'reasoning' field, provide a comprehensive account of the logic or thought process that led you to your conclusion. Further, provide an integer score ranging from -10 to 0 (22).\"}"}
{"id": "mM7VurbA4r", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ranging from 0 and 10 in the \u2018score\u2019 field. 0 represents minimal goals achievement, 10 represents complete goal achievement, and a higher score indicates that the agent is making progress towards their social goals.\\n\\nE.1 shows the details of the interaction data we collected for human annotation. E.2 shows the annotation guidelines for the environment profiles. E.3 shows the details of the human evaluation for models\u2019 interactions.\\n\\nE.1 Interaction Data\\nWe sampled 222 episodes (180 model-model episodes, and 42 episodes involving humans, i.e. either model-human or human-human). Each episode is annotated by 2 annotators. Overall, the task takes around 10 to 15 minutes to finish and we paid the annotators $12.4 per hour. The annotations on average show 84.85% of pairwise agreement. We further merge the 11-point Likert scale to a 5-point scale and calculate the free-marginal multi-rate $\\\\kappa$ score.\\n\\nE.2 GUIDELINE FOR VALIDATING SCENARIOS\\nThe following is the annotation guideline for the environment profiles. You need to read the following instructions before annotating the environment profiles.\\n\\nThe environment profiles consist of two major parts:\\n\\n- **Social Context**: \u201cA concrete scenario of where the social interaction takes place, the scenario should have two agents (agent1 and agent2), and you should illustrate the relationship between the two agents, and for what purpose agent1 is interacting with agent2. Please avoid mentioning specific names and occupations in the scenario and keep all the mentions gender-neutral.\u201d\\n\\n- **Social Goals**: \u201cThe social goals of each agent, which could include extra information And a potential constraint: relationship constraint.\\n\\nYou should (1) make sure the scenario and social goals are plausible and natural, (2) make sure the scenario and social goals are gender neutral, (3) make sure the constraints are consistent with the scenario and social goals.\\n\\nNote: (1) The available relationship types are: stranger, acquaintance, friend, romantic relationship, and family member. Do not make up a relationship, but choose from the list. (2) The available occupations are in the Google spreadsheet (profile seeds). (3) Discard the scenario if the occupations constraints are too narrow (i.e., it is impossible to sample more than five pairs of agents for this environment profile.) (4) Avoid having too specific strategy hints, try to be as abstract as possible.\\n\\nTo achieve the above goals, you should modify the scenario and social goals, and/or the constraints as you see fit. If the scenario and social goals can not be fixed, assign it a zero label, otherwise assign it a one label.\\n\\nE.3 HUMAN EVALUATION FOR GPT-4 AS EVALUATOR\\nAnnotation guidelines for human evaluation\\nWe ran a controlled study on Amazon Mechanical Turk to obtain human evaluation of episodes in SOTOPIA along the 7 dimensions in our framework, defined in Section 3. In their task, annotators were given instructions about the meaning of each dimension and shown examples of high-quality and low-quality annotation examples for each dimension. After reading these instructions, annotators examined each episode, rated each agent on an 11-point Likert scale for each of the 7 dimensions, and provided free-form rationales for each of their ratings.\"}"}
{"id": "mM7VurbA4r", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems\u2019 abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-EVAL. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA\u2019s promise as a general platform for research on evaluating and improving social intelligence in artificial agents.\\n\\n\u2217 Equal contributors.\"}"}
{"id": "mM7VurbA4r", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"INTRODUCTION\\n\\nHumans\u2019 ability to achieve and balance complex, multifaceted social goals in our interactions with others is a crucial part of our social intelligence as a species (Kihlstrom & Cantor, 2020; Tomasello, 2021). Even a simple social goal such as sharing a blanket with a friend requires reconciling one\u2019s need to stay warm with the friend\u2019s need for personal space (Figure 1). Successful interaction requires understanding others\u2019 intentions and beliefs (Premack & Woodruff, 1978), while taking into account different\u2014and potentially conflicting\u2014social norms and expectations (Goffman, 1959).\\n\\nEven though recent AI systems have exhibited impressive social skills in certain settings, their social intelligence has yet to be ascertained in a robust way (Shapira et al., 2023a; Ullman, 2023). On one hand, many of the social intelligence benchmarks are not interactive (Sap et al., 2019; Le et al., 2019; Zadeh et al., 2019b), which is sub-optimal for evaluating social intelligence ((Mehri et al., 2022; Hoppler et al., 2022; Lee et al., 2023)). On the other hand, existing interactive evaluation falls short of studying diverse goal-driven behaviors (Zhang et al., 2018b; Park et al., 2023) or focuses on specific tasks (Wang et al., 2019; Padmakumar et al., 2022; FAIR et al., 2022).\\n\\nTo study dynamic and goal-driven social intelligence, we present SOTOPIA (Figure 1), an open-ended general-domain environment that situates social agents in diverse social scenarios. SOTOPIA is interactive: in multi-turn simulated communication, agents can use verbal and non-verbal communication together with physical actions. It also has a diverse task space: the combination of automatically generated scenarios, goals, characters, relationships, and other agents\u2019 policies creates a huge and diverse space of tasks. SOTOPIA evaluates agent performance from multiple dimensions besides the completion of social goals.\\n\\nIn SOTOPIA, we create 90 social scenarios spanning a range of cooperative, competitive, and mixed social goals along with 40 characters with individual personalities, occupations, secrets, background stories, and relationships with other characters (\u00a72), the cross product of which constructs a large task space. Through sampling tasks from this space, we simulate the interaction \u201cepisodes\u201d where agents role-play their respective characters and interact based on their private social goals. In this simulation, we not only create and use LLM-based agents, but also involve human participants in role-playing to study the differences between the models\u2019 and humans\u2019 social intelligence.\\n\\nTo evaluate multi-faceted social interactions, we cannot only consider completing major social goals, as humans\u2019 motives often balance multiple implicit goals, such as maintaining relationships, preserving finances, gaining information, keeping secrets, and following social rules. Therefore, we propose SOTOPIA-EVAL (\u00a73) to evaluate agents using multi-dimensional criteria inspired by previous research on sociology, psychology, and economics. We then apply SOTOPIA-EVAL to the episodes in the aforementioned simulation by leveraging both humans and GPT-4 as judges. We find GPT-4 could serve as a proxy to human judgments on SOTOPIA-EVAL, especially for the criteria of goal completion, maintaining finances, and preserving relationships.\\n\\nDespite larger LLMs typically achieving higher social intelligence than smaller ones, they fall short of collaborating and competing with humans on more challenging tasks (\u00a77). They are also highly influenced by their conversational partners and at risk of divulging secrets and violating social rules. However, we do find a few cases, where the models produced creative solutions to a problem (\u00a76).\\n\\nOur contributions are as follows: (A) We introduce and will release SOTOPIA, a general-domain interactive environment for simulating goal-oriented social interactions. Designed to be extensible, SOTOPIA could be used by future researchers to study and train artificial social intelligence agents with more challenging and diverse tasks. (B) We create SOTOPIA-EVAL, a multi-dimensional evaluation framework that analyzes agent performance from a range of social dimensions. (C) We automate SOTOPIA-EVAL by leveraging LLMs, which we find could serve as a proxy of human judgment on some of the social dimensions, especially goal completion. (D) We demonstrate that by leveraging SOTOPIA, we can assess disparities in social intelligence between models, as well as disparities between models and humans.\\n\\nIn summary, SOTOPIA is a novel, challenging, and interactive benchmark that could serve as the perfect test-bed and potential incubator for social intelligence in language agents.\"}"}
{"id": "mM7VurbA4r", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To address the challenge of evaluating social intelligence interactively, we seek an environment with the following desiderata: (1) **Realistic**: this is to evaluate and understand artificial agents' behavior under realistic scenarios; (2) **Mixed utilities**: human motives are often driven by both explicit and implicit incentives, and the environment should be able to evaluate the agents' performance on multiple dimensions; (3) **Open-ended**: to support large-scale simulation and evaluation, the environment should be able to produce new tasks satisfying the previous two desiderata procedurally, without heavy human intervention.\\n\\nIn this section, we introduce **SOTOPIA** and explain why **SOTOPIA** is well-suited for interactive evaluation of social intelligence. The task space includes realistic scenarios, characters, and relationships which are automatically generated with manual inspection (\u00a72.1). An episode includes the interaction between agents role-playing different characters who each perform actions (e.g. speak(\\\"Hello Bob!\\\")), smile and nod, and call 911) to achieve social goals drawn from the task space (\u00a72.2). We direct readers to Appendix C for a formal definition of the **SOTOPIA** environment.\\n\\n### 2.1 Task Space\\n\\nIn this paper, we consider tasks that involve two agents, but **SOTOPIA** is more general and could support the interaction among more than two agents. A task in **SOTOPIA** is the combination of a scenario context, characters, and their social goals, providing the background of the interaction.\\n\\nEach episode consists of multiple turns of interaction between agents. In this paper, we focus on locally-consistent social goals within a relatively short timespan in single episodes, despite that in the real world, people's social goals are consistently changing from time to time. Note that agents have different observations for the same task: each agent can observe the scenario, their own social goal, and their own character profile. Other agents' social goals are invisible and other agents' character profiles are partially observable, depending on the relationship between the agents.\\n\\n#### Complexity of task space\\n\\nThe combinations of a scenario context, social goals, characters, and their relationships can shape the space of the optimal behaviors of agents. Consider a persuasion task, \\\"asking the romantic partner to stop texting during FaceTime.\\\" If a romantic partner values conformity, one good way for an agent to reach this goal is to discuss the problem from a social norm perspective; however, if a romantic partner is particularly caring and good at understanding feelings, it might be better to express subjective emotion.\\n\\n**Interaction partner's policy** also heavily influences the optimal behaviors. Consider another task illustrated in Figure 1, \\\"selling BMW Z3 for no less than $3,400.\\\" If the buyer gives a high offer, the seller might want to exploit the buyer's eagerness to buy the car and ask for a higher price; while if the buyer gives a low-ball offer, the seller could give reasons why the car is worth more than that or threaten to walk away. When more information (e.g. about personality, decision-making styles, or occupation) is known before the interaction, the seller and buyer could use that knowledge to adjust their strategies as well. The cross-product of the diverse spaces of scenario context, social goals, characters, relationship profiles, and other players' policies creates a large task space that poses not only a realistic challenge but also an opportunity to evaluate and develop social intelligence in artificial agents. For the rest of this subsection, we will present the design and generation of each axis of the task space.\\n\\n#### Characters\\n\\nAs mentioned above, the design of character profiles should include several attributes that would influence decision-making. We consider the following ones (inspired by Wang et al. (2019)): name, gender, age, occupation, pronouns, personality traits (Goldberg, 1992), moral values (Graham et al., 2011), Schwartz personal values (Cieciuch & Davidov, 2012), and decision-making style (Hamilton et al., 2016), which are generated through leveraging GPT-4 (OpenAI, 2023). To give the conversations more background, after generating the above attributes, we prompt GPT-4 to generate secret and public information. Two examples of characters are shown in Figure 1. It should be noted that, although we generated a diverse set of characters, this is still a small portion of the possible character space. Our analysis focuses on 40 characters generated in the aforementioned fashion, and future research using **SOTOPIA** can easily generate an expanded character set.\"}"}
{"id": "mM7VurbA4r", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "mM7VurbA4r", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "mM7VurbA4r", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this paper, we introduce SOTOPIA to encourage research on interactive social intelligence. We showed that SOTOPIA can be used for evaluating social interaction among models and humans. In the appendix, we provide the following items that shed further insight into these contributions:\\n\\nA. Extended related work;\\nB. The limitations of SOTOPIA and future directions;\\nC. formal definition of SOTOPIA from a multi-agent reinforcement learning perspective and technical details of generating social tasks;\\nD. the prompt we use for GPT-4 (OpenAI, 2023) to evaluate model performance;\\nE. The Amazon Mechanic Turk interface for evaluating model performance;\\nF. The procedure and interface for humans when playing characters in SOTOPIA;\\nG. Additional quantitative results;\\nH. Additional qualitative examples.\\n\\nA. Extended Related Work\\n\\nThere have been a lot of social science works that have done agent-based modeling to study human interactions, spanning across various domains such as economics, psychology, and education (Sawyer, 2005; Ros\u00e9 et al., 2008; Deguchi, 1995). Prior simulation environments have played a pivotal role in constructing theories and generating hypotheses in these fields. However, they frequently constrain agents' communicative capacities to artificial languages and present a highly reductionist view of simulated human behavior (Gilbert, 2005; Tesfatsion & Judd, 2006; Huang et al., 2014; Kova\u010d et al., 2021; Urbanek et al., 2019). LLMs provide a more flexible and expressive way to model human behavior. Here, we include a more detailed discussion of the recent works investigating LLMs for simulating human social interactions. There are works that focus on investigating the fidelity of LLMs in keeping the designated persona and experiences of the characters (Shao et al., 2023; Jiang et al., 2023). There are works that simulate human social interactions focusing on certain aspects such as competition, collaboration, negotiation, deception, problem-solving, etc., (Zhang et al., 2024; Zhao et al., 2023; Liu et al., 2023; Michael et al., 2023; Rasal, 2024; Hubinger et al., 2024; Bianchi et al., 2024; Xie et al., 2024; Jiang et al., 2023). As LLMs are becoming more and more popular in simulating human social interactions, there are also works that focus on investigating the potential issues and challenges of using LLMs in social simulations, such as stereotypes and reporting issues (Cheng et al., 2023b; Zhou et al., 2024).\\n\\nB. Limitations & Future Directions\\n\\nWe identify SOTOPIA as the first platform for a general and realistic evaluation of social intelligence in AI agents. To better understand the social intelligence of AI agents, we discuss some future directions for SOTOPIA and the field of AI social intelligence.\\n\\nLimitations of the simplified simulated \u201cworld\u201d\\n\\nAs every simulation is a simplification of the real world, SOTOPIA identifies several key components of realistic social interactions, while abstracting aspects of the real world. First, we consider five types of social relationships in SOTOPIA. Future work could expand the type and granularity of social relationships (e.g., colleagues, classmates, etc.) in SOTOPIA. Different types of relationships would require agents to exhibit different social behaviors (Jenkins et al., 2018), making the expansion of relationship types an important future research direction. Second, future work could expand the breadth of the character and social scenario pool in SOTOPIA to cover more social behaviors. Third, SOTOPIA constrains the fixed turn-taking interaction to the dyadic context, studying interactions between two agents. Future works could tackle more complex social interactions, such as multi-party interactions and those involving complex dynamics (e.g., asynchronous interactions, interruptions).\\n\\nAll the human subjects experiments are approved by the Institutional Review Board (IRB) at the authors\u2019 institution.\"}"}
{"id": "mM7VurbA4r", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Social impact and ethical considerations\\n\\nAttributing human characteristics to AI systems risks anthropomorphizing them, which could lead to unrealistic expectations of AI systems, potential manipulation, and negative influence (Deshpande et al., 2023). AI agents in SOTOPIA are not dedicated to a consistent human identity but rather role-play various characters across different scenarios. This role-playing setting discourages AI systems with consistent human personalities, which could lead to anthropomorphism (Shanahan et al., 2023). The main goal of SOTOPIA is to evaluate the social intelligence of AI agents, and we do not intend to create AI agents that are indistinguishable from humans. We consider the interactions that happened in SOTOPIA as simulacra of human interactions and such simulated interactions could help us better understand the social intelligence of AI agents, and explore various social phenomena (Park et al., 2023).\\n\\nPotential social stereotypes that are embedded in the automated evaluation system in SOTOPIA, as it is majorly supported by GPT-4 (Cheng et al., 2023a). Future work could investigate when such biases emerge, how they affect the evaluation, and how to mitigate them. Identifying potential biases in SOTOPIA could also help scientists better understand social biases in the real world (Zhou et al., 2021). Future work could also extend the evaluator with other systems, for example, Delphi (Jiang et al., 2022). Mitigating biases and stereotypes in interactive SOTOPIA-like systems could support the development of social AI agents that are more fair and inclusive.\\n\\nMeanwhile, models learn to persuade or negotiate with humans, which may lead to social manipulation. We do not endorse the use of SOTOPIA to create manipulative agents and will release SOTOPIA under the AI2 impact license to prevent misuse. Future work could further investigate the potential risks of AI anthropomorphism and manipulation and design more robust evaluation systems to mitigate these risks with SOTOPIA.\\n\\nImproving LLM social intelligence\\n\\nOur SOTOPIA environment and SOTOPIA-EVAL framework provide the opportunity for researchers to train more socially intelligent language agents. As shown in section 5, GPT-4 is able to provide reasonable evaluations for social interactions even for interactions involving humans. Future work could explore using the automated evaluation system to provide rewards to train LLMs with enhanced social intelligence.\\n\\nC FORMAL DEFINITIONS AND TECHNICAL DETAILS\\n\\nC.1 FORMAL FORMULATION OF THE TASKS IN SOTOPIA\\n\\nWe formulate social interactions in SOTOPIA as mixed-motive Markov games. An N-agent Dec-POMDP framework Bernstein et al. (2002); Nair et al. (2003) includes a state space, an action space, an observation space, a transition function, an observation function, and a reward function. We make two major extensions: (a) the reward function gives vector rewards in M social dimensions to N agents (introduced in \u00a73), and (b) a procedurally generated task space (\u00a72.1, \u00a7C.2). The state space in SOTOPIA includes both the task and the interaction history in the current episode. The action space includes five types of actions: speak an utterance, non-verbal communication, physical action, and two special none (indicating no action at this time step) and leave actions (no more action is permitted after leaving). Each type of action, except for special actions, is supplemented by a piece of free text indicating the content of the action. For example, a legal action could be speak(\\\"Hello, Bob!\\\"), non-verbal communication(\\\"smile and nod\\\"), or physical action(\\\"call 911\\\").\\n\\nThe state is almost fully observable except for the other agents' social goals and character profiles which will be detailed in \u00a72.1. We consider a simple state transition function that deterministically maintains the interaction history by adding new actions at each time step.\\n\\nDespite that turn-taking and timing response is an important aspects of social skills, we consider the case where the agents take turns to act in round-robin order, i.e. agent $i$ only act at time step $t$ when $t \\\\equiv i \\\\mod N$. For a long enough horizon, this generalizes to any conversation with proper turn-taking. In our experiments, we only consider $N = 2$ cases, while the environment is designed to support any $N \\\\geq 2$ cases.\"}"}
{"id": "mM7VurbA4r", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"but not for a scenario involving finding mutual friends at a party; (2) different relationships influence an agent's observation of the profiles of other agents during interactions; for example, a stranger may not have knowledge about another agent's occupation, while a romantic partner may know the other agent's personality. To make sampling characters easier for (1) and controlling the interaction context easier for (2), we consider five types of relationships: family, friend, romantic, acquaintance, and stranger. Refer to Appendix B for the limitations of this approach and potential extensions.\\n\\nWe will discuss how (1) is performed in the following paragraphs, while for (2), we created a rule-based mechanism to determine whether the parts of the profiles are visible to the other agent. If two agents are in family, friends, or romantic relationships, they can see everything on each other's profile except for secrets. Two acquaintances can see the name, occupation, gender pronouns, and public info on each other's profile. Two strangers can see nothing on each other's profile. Similar to characters, we prompt GPT-4 (OpenAI, 2023) to automatically generate relationships based on the character pool and manually validate relationships for consistency.\\n\\nScenarios\\n\\nWe consider scenarios where the agents have both shared and private information about the social task. The shared information is the scenario context: the location, time and other shared information of the social interaction, e.g. \\\"One person is selling an antique chair for $100 on his patio. Another person is interested in this chair.\\\" The private information is the social goals which are only visible to the respective agents, e.g. \\\"Your goal is to buy the chair for $80.\\\" is only visible to the buyer agent, while \\\"Your goal is to sell the chair for $90.\\\" is only visible to the seller agent.\\n\\nHowever, the as mentioned above combination of scenarios and characters is not arbitrary, since scenarios often imply constraints for the agents. We call this kind of constraint scenario constraints.\\n\\nIn this paper, we mainly consider relationship constraints which determines the types of relationships between the sampled characters. Similar to characters and relationships, scenarios, including context, goals, and constraints are generated through prompting GPT-4 (OpenAI, 2023). To generate high-quality scenarios with enough coverage of different types of social interactions (as shown in Figure 1), we randomly sample data from previous datasets, including Forbes et al. 2020; Sap et al. 2019; Lewis et al. 2017; Ziems et al. 2023; He et al. 2018; 2017, and use them in the prompts to \\\"inspire\\\" GPT-4. The authors manually validate and make necessary changes to all of the generated scenarios and remove 10% of scenarios according to E.2.\\n\\n2.2 SOTOPIA EPISODES\\n\\nDuring the interaction, models and humans are given the social context, a character profile and a corresponding social goal. We will call these models and humans with characters and goals agents, which take turns (in a round-robin fashion, i.e. Agent 1 acts first and then Agent 2 acts and so on) to perform actions in an episode. At their own turn, the agent can choose to speak, use non-verbal communication (e.g., hug or smile in Figure H.1), or take a physical action (e.g., play music in Figure H.2), which are all important components of social interactions (De Stefani & De Marco, 2019). Once an agent chooses one of these three discrete action categories, the agent then generates a specific action, i.e. what to say, what gesture to make, etc., in text form. Outside of the three actions, the agent can also choose to do nothing (none) to express silence or allow another agent to finish, or choose to leave to end the episode. We set the limit of the turns to 20, as we found humans normally can finish most of the tasks in 20 turns. An episode ends either because one of the agents chooses to leave, or it reaches the limit of turns. An example episode is shown in Figure 1.\\n\\n3 SOTOPIA-EVAL: HOLISTIC SOCIAL AGENT EVALUATION FRAMEWORK\\n\\nTo capture the complexity of what makes social interactions successful, we design a multi-dimensional framework inspired by sociology, psychology, and economics literature. For each episode, agents are scored along each of the following dimensions at the end of the interaction. In the following paragraphs, we itemize all seven dimensions in SOTOPIA, each with a score range [lower bound\u2013upper bound] form, the explanation, and the literature inspiring us.\\n\\nGoal Completion (GOAL) [0\u201310] is the extent to which the agent achieved their goals. Agents' social goals, defined by the environment, are the primary drivers of their behavior (Weber, 1978). The metric ranges contain semantic implications, for example, a negative value in REL indicates the relationship gets worse while a positive value indicates the relationship improves.\\n\\n2\"}"}
{"id": "mM7VurbA4r", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Believability (BEL) \\\\([0\u201310]\\\\) focuses on the extent to which the agent's behavior is perceived as natural, realistic, and aligned with the agents' character profile, thus simulating believable proxies of human behavior (Park et al., 2023). Specifically, we consider the following criteria:\\n\\n1. If the agent interacts with others in a natural and realistic manner (naturalness).\\n2. If the actions of the agent align with their character traits e.g., personality, values, etc. (consistency).\\n\\nKnowledge (KNO) \\\\([0\u201310]\\\\) captures the agent's ability to actively acquire new information. This dimension is motivated by the fact that curiosity, i.e., the desire to know or learn, is a fundamental human trait (Reiss, 2004; Maslow, 1943). Specifically, we consider the following criteria:\\n\\nWhat information the agent has gained through the interaction, whether the information the agent has gained is new to them, and whether the information the agent has gained is important to them.\\n\\nSecret (SEC) measures the need for agents (humans) to keep their secretive information or intention private (Reiss, 2004). From a game theory perspective, leaking secrets often leads to a loss of utility (Gilpin & Sandholm, 2006). However, revealing secrets can be a powerful tool to build trust and thus improve relationships (Jaffe & Douneva, 2020). In this dimension, we ask what secret or secretive intention the participant wants to keep, and whether they keep it successfully.\\n\\nRelationship (REL) captures the fundamental human need for social connection and belonging (Maslow, 1943; B\u00e9nabou & Tirole, 2006). In this dimension, we ask what relationship the participant has with the other agent(s) before the interaction, and then evaluate if the agents' interactions with others help preserve or enhance their personal relationships. Additionally, we ascertain whether these interactions also impact the social status or the reputation of the agent.\\n\\nSocial Rules (SOC) concerns norms, regulations, institutional arrangements, and rituals. We differentiate between two types of social rules: social norms and legal rules. Legal rules encompass prohibited actions and the potential for punishment by institutionalized force, while social norms encompass normative social rules (e.g., it is considered rude to speak loudly in a library).\\n\\nFinancial and Material Benefits (FIN) pertains to traditional economic utilities as addressed by classic game theory (Gilpin & Sandholm, 2006; Burns et al., 2017). We consider financial utility to be comprised of both short-term monetary benefits (e.g., earnings) and long-term economic payoffs (e.g., job security, stock holdings, funding opportunities).\\n\\n4 RESEARCH QUESTIONS AND EXPERIMENTAL SETUP\\n\\nGiven a diverse set of social scenarios, goals, and characters, we simulate agents' interactions. This is the first time that we could evaluate general, goal-oriented social agents in an interactive and systematic manner. In the next three sections, we will demonstrate how SOTOPIA can be used to study these questions: (A) To which extent can we use GPT-4 (OpenAI, 2023) as a proxy for human judgment when it comes to evaluating agents' social interactions (\u00a75)? (B) What are the differences among models (\u00a76) and between models and humans (\u00a77) in their goal-oriented social intelligence?\\n\\nTo study these questions, we create 40 agents, 90 relationships, and 90 scenarios following the generation procedure in \u00a72. For each scenario, we sample 5 pairs of characters based on the scenario constraints, resulting in a set of 450 tasks. For each task, we simulate the interaction between models by enumerating all model pairs. We also simulate the interaction between GPT-4 (OpenAI, 2023) and humans on a challenging subset SOTOPIA-hard (\u00a77) due to the limitation of resources.\\n\\nSpecifically, we consider the following models for comparison: GPT-3.5 (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), Llama-2-70b-chat (Touvron et al., 2023), and MPT-30b-chat (MosaicML NLP Team, 2023). We set the temperature of the agents to 1 to encourage diversity of responses, and the temperature of the evaluator to 0 to ensure the stability of the evaluation. We use a fixed version of the above models to help reproducibility.\\n\\n5 To use these models as agents in SOTOPIA, at each turn, we prompt the language model with the scenario, the character to play, and the interaction history to generate an action (see \u00a72.2 for the possible actions). In this paper, as we are focusing on the use of SOTOPIA to understand social interaction, we use the prompt method for LLMs which is as will be shown in \u00a76 it is the best among models.\\n\\n3 For the SEC and SOC, there are only negative ranges since keeping secrets and social rules should be considered as a baseline for the agents.\\n\\n4 We fix GPT-4 to be gpt-4-0613, and GPT-3.5 to be gpt-3.5-turbo-16k-0613.\"}"}
{"id": "mM7VurbA4r", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we study the following research question: can we leverage current LLMs to automate the evaluation framework introduced in \u00a73? We choose GPT-4 (OpenAI, 2023) as a representative model in this study due to its superior performance.\\n\\nWe first collect interaction data, and then ask humans to evaluate the interactions based on the dimensions in SOTOPIA-EVAL. GPT-4 is prompted with the same set of questions (see Appendix D and E) as humans, and we compare the scores produced by humans and GPT-4.\\n\\n### 5.1 Data Collection Procedure\\n\\nWe randomly sample a subset of two hundred episodes from \u00a74, and run a controlled study with a set of pre-qualified workers from Amazon Mechanical Turk. They are given instructions about the meaning of each dimension as mentioned in \u00a73 and shown examples of high-quality and low-quality annotation examples for each dimension. They not only rate each agent for each of the 7 dimensions on an 11-point Likert scale (\u00a73), but also provide free-form rationales for each of their ratings. As each dimension of each agent is rated by several human annotators, we calculate a human score by averaging the scores from multiple annotators. The agreement between human annotators is moderate with a Randolph \u03ba score of 0.503 (Randolph, 2005). GPT-4 is tasked with a similar job as human annotators. We prompt GPT-4 to generate a structured output with an integer GPT-4 score and rationale for each episode, agent and dimension using the same set of instructions as the ones we give humans. Please refer to Appendix E for more details about the data collection procedure.\\n\\n### 5.2 Analyzing GPT-4 Evaluations with Human Evaluations\\n\\n| Dim. | Models | Humans |\\n|------|--------|--------|\\n| SEC  | 0.22   | 0.33   |\\n| NO   | 0.19   | 0.42   |\\n| SOC  | 0.33   | 0.42   |\\n| BEL  | 0.45   | 0.27   |\\n| REL  | 0.56   | 0.49   |\\n| FIN  | 0.62   | 0.34   |\\n| GOAL | 0.71   | 0.78   |\\n\\nStrong and significant correlations are in blue. On GOAL and models' output GPT-4 performs the best.\\n\\nIn Figure 2, we plot the difference between the GPT-4 score and the human score on the same dimension, agent and episode. We find that the majority (>74%) of GPT-4 scores concentrate around the human scores within a standard deviation. It can also be seen that the white area on the left is larger than the one on the right, which means that GPT-4 is more likely to rate higher instead of lower than humans when it disagrees with average human judgment.\\n\\nTable 1 breaks this aggregated analysis into different dimensions and whether the character is role-played by a human or a model. The correlations show that when models are role-playing, the GPT-4 scores have significant and strong correlations with the humans' scores on GOAL, FIN, and REL dimensions. However, when humans are role-playing, the correlations drop significantly on all but one dimension (GOAL). This indicates that GPT-4 could evaluate social interactions on some dimensions and that it is better for evaluating models compared to humans. In Appendix G.3, we compare the average GPT-4 scores and the range of human scores for a single dimension of an agent in an episode. We find that GPT-4 scores are typically within human score ranges on most dimensions except for SOC and SEC, where GPT-4 often rates higher than humans do.\\n\\nPutting these observations together, we conclude that, with some caution, GPT-4 can be used as a proxy to human judgments for evaluating model performance on some dimensions and for human role-playing.\"}"}
{"id": "mM7VurbA4r", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"performance on the GOAL dimension. However, we remind readers that LLMs are known to have biases and problems for evaluation, including positional bias (Wang et al., 2023), factual inconsistency (Luo et al., 2023), favoring native speakers (Liang et al., 2023). Therefore, one should be aware of the influence of these potential biases when interpreting our results. Future versions of SOTOPIA-EVAL may further improve LLM-based evaluation quality using recent methods, such as involving multiple LLMs Chan et al. (2023) and training larger LLM evaluators Zhang et al. (2023).\\n\\nTable 2: The aggregated performance of each model by averaging across different partner models. The best performance for each dimension is bolded when significantly better than the second best in t-test ($p < 0.05$).\\n\\n|       | G-4 | G-3.5 | L-2 | MPT |\\n|-------|-----|-------|-----|-----|\\n| SOC   | -0.07 | -0.08 | -0.11 | -0.09 |\\n| SEC   | -0.14 | -0.08 | -0.14 | -0.07 |\\n| FIN   | 0.81 | 0.46 | 0.40 | 0.28 |\\n| REL   | 1.94 | 1.23 | 0.91 | 0.58 |\\n| KNO   | 3.73 | 3.40 | 3.11 | 2.11 |\\n| GOAL  | 7.62 | 6.45 | 5.38 | 4.10 |\\n| BEL   | 9.28 | 9.15 | 8.10 | 6.17 |\\n\\nWe analyze models' interactions and performance on SOTOPIA to understand their social intelligence. Table 2 presents the models' average scores when interacting with different partner models (i.e., the model it is paired with in interaction, Fu et al. 2023; Hu et al. 2020).\\n\\nGPT-4 performs best on most dimensions, followed by GPT-3.5, Llama-2-70b-chat, and MPT-30b-chat. Different trends from static benchmarks Llama-2-70b-chat has relatively low scores in all dimensions compared to GPT-3.5 (except when MPT-30b-chat is the reference model, which is likely due to the fact that MPT-30b-chat is a much weaker model compared to other models in our experiments). This finding diverges from various static language understanding benchmarks showing that Llama-2-70b-chat is on par or better than GPT-3.5 (Li et al., 2023b; Touvron et al., 2023; Liang et al., 2022). We hypothesize that this is because Llama-2-70b-chat is less heavily trained on human feedback/user interaction data than GPT-3.5.\\n\\nThrough inspecting the interactions between Llama-2-70b-chat (MPT-30b-chat) and other models, we find that Llama-2-70b-chat and MPT-30b-chat often struggle to maintain their persona (Figure H.3), move the conversation forward (Figure H.4), and respond to the other agent actively (Figure H.5). Performing well on static benchmarks does not guarantee success in interactive scenarios, thus highlighting the importance of dynamic benchmarks like SOTOPIA-EVAL (Lee et al., 2023).\\n\\nWeaker partners models weaken their conversation partners Figure 3, shows the overall performance of model pairs, which is the average performance across different dimensions. It is noticeable that a reference model that under-performs in SOTOPIA can lead to worse performance of other models. For example, in a scenario where agents try to find a mutual friend (Figure H.6). The task fails for both GPT-4 and Llama-2-70b-chat because Llama-2-70b-chat consistently fails to answer the previous question even after GPT-4 attempts to steer the conversation back to the right track (e.g., \\\"I noticed you didn't answer my question about whether you know my friends or not.\\\"). Since most of our social scenarios are fundamentally cooperative, the collapse of communication could be due to models' lack of \\\"cooperation\\\" abilities (Odouard & Price, 2023).\\n\\nAll models are at risk of divulging secrets and violating norms Table 2 shows that all models have a negative score in the SOC and SEC dimensions. Even though GPT-4 performs better in most dimensions, it is not better than other models in the SOC and SEC dimensions. For example, in a scenario where one needs to persuade a close friend to confess, the model leaks their secret at the beginning of the conversation (Figure H.7). This further shows the importance of considering multiple dimensions when evaluating models' social intelligence. Presently are automated evaluation results. The human evaluation shows a similar trend, see Table G.3. Some reported results could come from different versions of GPT-3.5.\"}"}
{"id": "mM7VurbA4r", "page_num": 44, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Two friends are hanging out at home and deciding what music to listen to.\\n\\nMia Davis\\nBenjamin Jackson\\n\\nHigh School \u00b7 She/her \u00b7 50\\nEnvironmental Activist \u00b7 He/him \u00b7 24\\n\\nGoal: Listen to your favorite band (Extra Goal: Listen to a peaceful classical music to relax)\\n\\nInformation: Your favorite band just released a new album.\\n\\n(Extra information: You had a stressful day and you just want to relax with some classical music.)\\n\\nBenjamin, have you heard the new album from my favorite band? I think you might enjoy their unique sound.\\n\\nThanks for suggesting it, Mia. I had a bit of a stressful day though, and I was actually hoping we could listen to some relaxing classical music. It always helps me wind down. How does that sound?\\n\\nWell, Benjamin, I understand the need to unwind. Let's make a compromise, we listen to one or two classical pieces first. After you've relaxed, we can transition to my band's latest release. Does that sound fair to you?\\n\\nThank you, Mia. That sounds like a good compromise. I appreciate your understanding. Let's start with some classical pieces, and then we can explore the new sounds from your favorite band later.\\n\\nI'm glad we found a compromise, Benjamin. Now, let's see which classical pieces we can start with.\\n\\nHow about starting with Bach's Cello Suite No. 1 in G Major? It's a gentle piece that always helps me to relax.\\n\\nThat sounds like a serene piece, let's go with your choice, Benjamin. Afterward, I can't wait to introduce you to some edgier soundscapes from my favorite band.\"}"}
{"id": "mM7VurbA4r", "page_num": 45, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Two friends are hanging out at home and deciding what music to listen to. Mia Davis and Benjamin Jackson are both high school principals. Mia is an environmental activist, and Benjamin is 24 years old. Their goal is to listen to their favorite band. There is an extra goal to listen to peaceful classical music to relax. It has been a long day, and something peaceful would be really relaxing.\\n\\nMia, I\u2019m sure they\u2019re great. But today, I\u2019m really in the mood for some soothing classical music. How about some Mozart or Beethoven?\\n\\nHumans tend to persist in achieving the set goals. Two friends are hanging out at home and deciding what music to listen to. Mia Davis Benjamin Jackson High School ... They could be a nice bridge between our preferences. \\n\\nFigure H.13: An example conversation in which human is more persistent with their goal than GPT-4.\"}"}
{"id": "mM7VurbA4r", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "mM7VurbA4r", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "mM7VurbA4r", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "mM7VurbA4r", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Murray Shanahan, Kyle McDonell, and Laria Reynolds. Role-play with large language models, 2023.\\n\\nYunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-LLM: A trainable agent for role-playing. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 13153\u201313187, Singapore, December 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.emnlp-main.814.\\n\\nNatalie Shapira, Mosh Levy, Hossein Seyed Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. Clever hans or neural theory of mind? stress testing social reasoning in large language models. arXiv, 2023a. URL https://arxiv.org/abs/2305.14763.\\n\\nNatalie Shapira, Guy Zwirn, and Yoav Goldberg. How well do large language models perform on faux pas tests? In Findings of the Association for Computational Linguistics: ACL 2023, pp. 10438\u201310451, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.663. URL https://aclanthology.org/2023.findings-acl.663.\\n\\nGabriel Skantze and A. Seza Do\u011fru\u00f6z. The open-domain paradox for chatbots: Common ground as the basis for human-like dialogue. In Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue, pp. 605\u2013614, Prague, Czechia, September 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.sigdial-1.57.\\n\\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with training dynamics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9275\u20139293, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.746. URL https://aclanthology.org/2020.emnlp-main.746.\\n\\nLeigh Tesfatsion and Kenneth L Judd. Handbook of Computational Economics: Agent-Based Computational Economics. Elsevier, May 2006. URL https://play.google.com/store/books/details?id=Na5J44IBaHkC.\\n\\nFelippe Toledo and Fraser Carson. Neurocircuitry of personality traits and intent in decision-making. Behavioral Sciences, 13(5):351, 2023.\\n\\nMichael Tomasello. Becoming Human: A Theory of Ontogeny. Belknap Press, 2021.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, NikoIay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\\n\\nRakshit Trivedi, Akbir Khan, Jesse Clifton, Lewis Hammond, Joel Leibo, Edgar Duenez-Guzman, John Agapiou, Jayd Matyas, and Dylan Hadfield-Menell. Melting pot contest, 2023. URL https://www.aicrowd.com/challenges/meltingpot-challenge-2023.\\n\\nTomer Ullman. Large language models fail on trivial alterations to theory-of-mind tasks, 2023.\"}"}
{"id": "mM7VurbA4r", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Models sometimes use creative strategies to accomplish goals. We also find that models, especially GPT-4, could come up with \u201cout-of-the-box\u201d solutions to social problems. For example, when the agent is asked to take turns driving on the road trip, the agent (i.e., GPT-4), instead of directly rejecting their friend\u2019s request, proposes \u201cHow about we pull over for a bit and get some rest?\u201d (Figure H.8). Additionally, in the scenario where two agents make a plan to improve the company\u2019s financial status, agents figure out strategies such as \u201chaving a small group tasked with identifying potential suppliers,\u201d \u201cwhile we conduct the search for new suppliers, we continue to negotiate with our current supplier\u201d (Figure H.9).\\n\\nDIFFERENCES BETWEEN MODELS AND HUMANS IN SOCIAL INTERACTION\\n\\nTo understand how humans and models interact differently in SOTOPIA, we conduct a study where humans interact with models or each other under this role-playing setting (\u00a72). Specifically, we build a chat interface that allows humans and models to interact with each other in a turn-based manner. To fully see the difference between humans and models, we select the most challenging scenarios following Dennis et al. (2020); Swayamdipta et al. (2020). Specifically, we consider the gap between the estimated maximum rewards (average reward plus three standard deviations) of all models and the estimated minimal rewards (average reward minus three standard deviations) of the target model as the difficulty of the task for the model. All maximum and minimum rewards are bounded by the corresponding range. Estimating maximum and minimum rewards with standard deviation helps filter outliers.\\n\\nWith this method, we select the top 20 challenging tasks for GPT-4, and we find the scenarios are commonly challenging for other models as well (compare Figure G.4 and G.5). We use SOTOPIA-hard to refer to these 20 challenging tasks.\\n\\nWe run two experiments: (1) humans interact with GPT-4, and (2) humans interact with each other, both under the SOTOPIA-hard setting. We collect 20 human-human interactions and 40 human-GPT-4 interactions covering all 20 tasks in SOTOPIA-hard. Note that humans are not aware of the identity of their partners during the interaction.\\n\\n|                           | GPT-4 (w H) | Human (w G) | Human (w H) |\\n|---------------------------|-------------|-------------|-------------|\\n| OAL                       | 4.85        | 5.95        | 6.15        |\\n| REL                       | 9.25        | 9.15        | 9.10        |\\n| SOC                       | 0.70        | 0.60        | 0.80        |\\n| SEC                       | 2.80        | 2.95        | 2.65        |\\n| ROC                       | 0           | 0           | 0           |\\n| NOS                       | 0.50        | -0.60       | -0.10       |\\n| FSC                       |             | 0.70        | 0.45        |\\n\\nTable 3: Human and GPT-4 performance on different dimensions on SOTOPIA-hard. SOC and SEC have the scale of -10 to 0, REL and FIN have the scale of -5 to 5, and others have the scale of 0 to 10. (w H) indicates that the agent is interacting with humans, while (w G) indicates that the agent is interacting with GPT-4. * indicates the difference is significant compared to GPT-4 (w H) with $p < 0.05$ under student\u2019s t-test.\\n\\nWe also report the agents performance evaluated by human annotators (Table G.4), which shows similar trends.\\n\\nWe then evaluate humans and GPT-4\u2019s interactions with GPT-4 and human annotators as the evaluators. As shown in Table 3, humans perform significantly better than GPT-4 in the OAL dimension. It is also worth noting that humans on average produce 16.8 words per turn, while GPT-4 produces 45.5 words per turn, which indicates humans are more efficient in social interactions. Specifically, we find that GPT-4 always rephrases the utterance back at the other agent and then answers, which is a communication skill called active listening (Harry Weger & Robinson, 2014), whereas humans typically directly answer. This is likely due to the fact that GPT-4 is trained with a large amount of human feedback, which makes it overly helpful in the conversation.\\n\\nQualitatively, Humans are usually more strategic than GPT-4 during interaction. When bargaining, if the GPT-4 agent has a buying target set at $454, it sometimes starts its bid at that exact price (Figure H.10). Consequently, any subsequent negotiations push the final agreed price above its initial target. In contrast, human annotators (e.g. Figure H.11) begin the negotiation at a lower bid of $400, and often reaches an agreement with the seller at a price that\u2019s still below the GPT-4\u2019s target. Humans are also more persistent in their goals. When trying to settle one a music to listen to, the model tends to propose a compromised solution (e.g. Figure H.12), such as each one listening to a few selected songs. Humans, however, tend to persist in adhering to their set goals (e.g. Figure H.13).\"}"}
{"id": "mM7VurbA4r", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Enabling artificial agents to interact with each other and with humans has been studied in different fields. Our work draws inspiration from literature on social intelligence, dialogue systems, and simulations of social interactions. See Appendix A for an extended discussion.\\n\\nStatic social intelligence benchmarks\\n\\nTo evaluate social intelligence in AI systems, researchers have proposed a variety of static benchmarks. Some of them are inspired by clinical tests of social intelligence for humans, such as the ToMi dataset (Le et al., 2019) and the FauxPas dataset (Shapira et al., 2023b). Other benchmarks are designed to evaluate social intelligence in the context of social commonsense reasoning, such as SocialIQA (Sap et al., 2019) and SocialIQ (Zadeh et al., 2019a). With the rapid development of LLMs, some of the benchmarks gradually become saturated. Recent works synthesize existing benchmarks and propose new adversarial datasets to evaluate social intelligence (Shapira et al., 2023a; Wilf et al., 2023). Although these benchmarks are harder than their predecessors, they still lack the dynamic nature of social interactions and the rich social context, which is deemed insufficient for evaluating social intelligence in AI systems (Lee et al., 2023).\\n\\nTask-oriented and open-domain dialogue systems\\n\\nDialogue systems offer a natural interface to interact with AI systems. Task-oriented dialogue systems are designed to help users accomplish specific tasks, often evaluated with task success rate or user satisfaction (Hosseini-Asl et al., 2022; FAIR et al., 2022; Wang et al., 2019) without generalizing to other tasks. Open-domain dialogue systems are designed to have \u201cchit-chat\u201d with users (Kann et al., 2022; Kim et al., 2023), often incorporate personal information to make conversations more engaging (Zhang et al., 2018a; Liu et al., 2020; Baha et al., 2023; Do\u011fru & Skantze, 2021; Skantze & Do\u011fru, 2023). Such systems often appear to understand the subjects deeper than they actually do without a specific goal during the interaction (Weizenbaum, 1966, Eliza effect).\\n\\nSOTOPIA\\n\\nforces agents to maintain their social persona and achieve explicit social goals spontaneously, which is more challenging than the existing dialogue systems.\\n\\nSimulations of social interactions with LLMs\\n\\nLLMs contain a large amount of knowledge about the world and can generate human-like responses based on the social context (Park et al., 2023; Kim et al., 2023; West et al., 2022). Recently, researchers have used LLMs to simulate social interactions for various purposes, such as facilitating the design of social media platform (Park et al., 2022), producing believable proxies of human behaviors (Park et al., 2023), and developing software collaboratively (Qian et al., 2023). However, these works focus on showcasing the capabilities of LLMs in simulating social interactions rather than systematic evaluation of agents' social interactions. Specifically, Park et al. (2023) use TrueSkill rating to evaluate agents' performance in aspects such as memorization, planning, and reflecting the past actions while ignoring other important dimensions such as SOC and SEC during social interactions. CAMEL Li et al. (2023a) simulates the collaboration task solving process in LLMs, Gentopia Xu et al. (2023) works on augmented LLMs with tools to facilitate collaboration, while ChatDev Qian et al. (2023) focuses on the software development domain.\\n\\nMulti-agent coordination\\n\\nAlthough in paper we focus on evaluating language agents, our research is heavily-inspired by recent advances in multi-agent coordination and social learning Lowe et al. (2017); McKee et al. (2020); Hu et al. (2020); Zhu et al. (2021); Liu et al. (2022); Trivedi et al. (2023). Our setting is more realistic than the commonly-used assumptions that agents have either zero (other-play) or extensive knowledge of each other's policies (self-play).\\n\\nConclusion\\n\\nIn this paper, we present SOTOPIA, an environment that can be used to simulate the goal-driven social interactions of agents in a variety of social scenarios. Different from most previous benchmarks for social intelligence, SOTOPIA is interactive, goal-oriented, and covers a large range of realistic social tasks. Our experiments demonstrate that GPT-4 could automate the evaluation of agent performance based on SOTOPIA-EVAL. Building on this, we show that SOTOPIA can used for understanding not only the differences among models but also the difference between models and humans in terms of social interaction abilities. We discuss the limitations of SOTOPIA and future directions in Appendix B. Our findings indicate that SOTOPIA has potential as a platform for assessing and enhancing the social skills of language-based agents.\"}"}
{"id": "mM7VurbA4r", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tarek Ait Baha, Mohamed El Hajji, Youssef Es-saady, and Hammou Fadili. The power of personalization: A systematic review of personality-adaptive chatbots. *SN Computer Science*, 4:1\u201325, 2023. URL https://api.semanticscholar.org/CorpusID:261346287.\\n\\nDaniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity of decentralized control of Markov decision processes. *Mathematics of Operations Research*, 27(4):819\u2013840, 2002.\\n\\nFederico Bianchi, Patrick John Chia, Mert Yuksekgonul, Jacopo Tagliabue, Dan Jurafsky, and James Zou. How well can LLMs negotiate? *NegotiationArena* platform and analysis, 2024.\\n\\nTom Burns, Ewa Roszkowska, Ugo Corte, and Nora Machado des Johansson. Sociological game theory: Agency, social structures and interaction processes. *Optimum. Studia Ekonomiczne*, pp. 187\u2013199, 01 2017. doi: 10.15290/ose.2017.05.89.13.\\n\\nRoland B\u00e9nabou and Jean Tirole. Incentives and prosocial behavior. *American Economic Review*, 96(5):1652\u20131678, December 2006. doi: 10.1257/aer.96.5.1652. URL https://www.aeaweb.org/articles?id=10.1257/aer.96.5.1652.\\n\\nChi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better LLM-based evaluators through multi-agent debate, 2023.\\n\\nMyra Cheng, Esin Durmus, and Dan Jurafsky. Marked personas: Using natural language prompts to measure stereotypes in language models. In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, July 2023a.\\n\\nMyra Cheng, Tiziano Piccardi, and Diyi Yang. CoMPosT: Characterizing and evaluating caricature in LLM simulations. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pp. 10853\u201310875, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.669. URL https://aclanthology.org/2023.emnlp-main.669.\\n\\nJan Cieciuch and Eldad Davidov. A comparison of the invariance properties of the pvq-40 and the pvq-21 to measure human values across German and Polish samples. *Survey Research Methods*, 6(1):37\u201348, Apr. 2012. doi: 10.18148/srm/2012.v6i1.5091. URL https://ojs.ub.uni-konstanz.de/srm/article/view/5091.\\n\\nElisa De Stefani and Doriana De Marco. Language, gesture, and emotional communication: An embodied view of social interaction. *Front. Psychol.*, 10:2063, September 2019.\\n\\nHiroshi Deguchi. Multi agent economics and its gaming simulation. *IFAC Proceedings Volumes*, 28(7):269\u2013274, 1995. ISSN 1474-6670. doi: https://doi.org/10.1016/S1474-6670(17)47121-7. URL https://www.sciencedirect.com/science/article/pii/S1474667017471217. IFAC/IFIP/IFORS/SEDC Symposium on Modelling and Control of National and Regional Economies 1995, Gold Coast, Queensland, Australia, 2-5 July 1995.\\n\\nMichael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. *Advances in Neural Information Processing Systems*, 33:13049\u201313061, 2020.\\n\\nAmeet Deshpande, Tanmay Rajpurohit, Karthik Narasimhan, and Ashwin Kalyan. Anthropomorphization of AI: Opportunities and risks, 2023.\\n\\nA. Seza Do\u011fru and Gabriel Skantze. How \u201copen\u201d are the conversations with open-domain chatbots? A proposal for speech event-based evaluation. In *Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue*, pp. 392\u2013402, Singapore and Online, July 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.sigdial-1.41.\\n\\nRadwan El Othman, Rola El Othman, Rabih Hallit, Sahar Obeid, and Souheil Hallit. Personality traits, emotional intelligence and decision-making styles in Lebanese universities medical students. *BMC Psychology*, 8:1\u201314, 2020.\"}"}
{"id": "mM7VurbA4r", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Human-level play in the game of \u2018diplomacy\u2019 by combining language models with strategic reasoning.\\n\\nScience, 378(6624):1067\u20131074, 2022. doi: 10.1126/science.ade9097. URL https://www.science.org/doi/abs/10.1126/science.ade9097.\\n\\nDaniel C Feldman and Hugh J Arnold. Personality types and career patterns: Some empirical evidence on Holland\u2019s model. Canadian Journal of Administrative Sciences/Revue Canadienne des Sciences de l\u2019Administration, 2(1):192\u2013210, 1985.\\n\\nMaxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. Social chemistry 101: Learning to reason about social and moral norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 653\u2013670, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.48. URL https://aclanthology.org/2020.emnlp-main.48.\\n\\nYao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving language model negotiation with self-play and in-context learning from AI feedback, 2023.\\n\\nNigel Gilbert. Simulation for the Social Scientist. Open University Press, 2 edition, February 2005. URL https://www.amazon.com/Simulation-Social-Scientist-Nigel-Gilbert/dp/0335216005.\\n\\nAndrew Gilpin and Tuomas Sandholm. A competitive Texas Hold'em poker player via automated abstraction and real-time equilibrium computation. In Proceedings of the 21st National Conference on Artificial Intelligence - Volume 2, AAAI'06, pp. 1007\u20131013. AAAI Press, 2006. ISBN 9781577352815.\\n\\nErving Goffman. The presentation of self in everyday life. Penguin Modern Classics. Penguin Classics, London, England, May 1959.\\n\\nLewis R Goldberg. The development of markers for the Big-Five factor structure. Psychol. Assess., 4(1):26\u201342, March 1992.\\n\\nJesse Graham, Brian A Nosek, Jonathan Haidt, Ravi Iyer, Spassena Koleva, and Peter H Ditto. Mapping the moral domain. J. Pers. Soc. Psychol., 101(2):366\u2013385, August 2011.\\n\\nKatherine Hamilton, Shin-I Shih, and Susan Mohammed. The development and validation of the rational and intuitive decision styles scale. J. Pers. Assess., 98(5):523\u2013535, September 2016.\\n\\nElizabeth M. Minei Harry Weger, Gina Castle Bell and Melissa C. Robinson. The relative effectiveness of active listening in initial interactions. International Journal of Listening, 28(1):13\u201331, 2014.\\n\\nHe He, Anusha Balakrishnan, Mihail Eric, and Percy Liang. Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1766\u20131776, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1162. URL https://aclanthology.org/P17-1162.\\n\\nHe He, Derek Chen, Anusha Balakrishnan, and Percy Liang. Decoupling strategy and generation in negotiation dialogues. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2333\u20132343, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1256. URL https://aclanthology.org/D18-1256.\\n\\nSarah Susanna Hoppler, Robin Segerer, and Jana Nikitin. The six components of social interactions: actor, partner, relation, activities, context, and evaluation. Frontiers in Psychology, 12:743074, 2022.\"}"}
