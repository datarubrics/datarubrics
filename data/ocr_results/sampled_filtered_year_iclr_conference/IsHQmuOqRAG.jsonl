{"id": "IsHQmuOqRAG", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As the object attribution of each pixel is not known but is inferred by $f_{obj}(I(t))$, it is represented for every pixel as a probability of belonging to each object and the background $\\\\pi(t)_k, k = 1, 2, \\\\cdots, K + 1$. Therefore, the predicted motion of each pixel should be described as a probability distribution over $K + 1$ discrete target locations $p(x'(t+1)_{i,j}) = \\\\sum_{K+1}^{k=1} \\\\pi(t)_{kij} \\\\cdot \\\\delta(x'(t+1)_{k}, (i,j))$, i.e., pixel $(i,j)$ has a probability of $\\\\pi(t)_{kij}$ to move to location $x'(t+1)_{k}$ at the next time point, for $k = 1, 2, \\\\cdots, K + 1$. With such probabilistic prediction of pixel movement for all visible pixel $(i,j)_t$, we can partially predict the colors of the next image at the pixel grids where some original pixels from the current view will land nearby by weighting their contribution:\\n\\n$$I'(t+1)_{Warp(p,q)} = \\\\begin{cases} \\\\sum_{k,i,j} w_k(i,j,p,q) I(t)_{i,j} & \\\\text{if } \\\\sum_{k,i,j} w_k(i,j,p,q) > 0 \\\\\\\\ 0 & \\\\text{otherwise} \\\\end{cases}$$\\n\\nWe define the weight of the contribution from any source pixel $(i,j)$ to a target pixel $(p,q)$ as $w_k(i,j,p,q) = \\\\pi(t)_{kij} \\\\cdot e^{-\\\\beta \\\\cdot D'(t+1)_{k}(i,j) \\\\cdot \\\\max\\\\{1 - |i'(t+1)_k - p|, 0\\\\} \\\\cdot \\\\max\\\\{1 - |j'(t+1)_k - q|, 0\\\\}}$.\\n\\nThe first term incorporates the uncertainty of which object a pixel belongs to. The second term resolves the issue of occlusion when multiple pixels are predicted to move close to the same pixel grid by down-weighting the pixels predicted to land farther from the camera. These last two terms mean that only the source pixels predicted to land within a square of $2 \\\\times 2$ pixels centered at any target location $(p,q)$ will contribute to the color $I'(t+1)_{Warp(p,q)}$. The depth map $D'(t+1)_{Warp}$ can be predicted by the same weighting scheme after replacing $I(t)_{i,j}$ with each predicted depth $D'(t+1)_{k}(i,j)$ assuming the pixel belongs to object $k$. \\n\\nA.5 DEPENDENCY OF SEGMENTATION PERFORMANCE ON OBJECT SIZE AND DISTANCE ACROSS MODELS\\n\\nOur model (OPPLE)\\nMONet-128\\nSlot-attention-128\\nGenesis *\\n\\n| Model      | IoU |\\n|------------|-----|\\n| OPPLE      | 1   |\\n| MONet-128  | 1   |\\n| Slot-attention-128 | 1   |\\n| Genesis *  | 1   |\\n\\nA.6 ILLUSTRATION OF PREDICTION QUALITY\\n\\nIn the figure below, we display the first, second image, one of the masked objects, the predicted third image, and the ground truth of the third image, both for our dataset, and for a richer version of the Traffic dataset used by (Henderson & Lampert, 2020). We provide reference lines and some circles to aid the comparison between images and evaluate the warping quality. Imagination quality can be inspected usually at one side of the predicted images (the camera motion is typically larger in our dataset).\"}"}
{"id": "IsHQmuOqRAG", "page_num": 15, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "IsHQmuOqRAG", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The representation of objects is the building block of higher-level concepts. Infants develop the notion of objects without supervision. The prediction error of future sensory input is likely the major teaching signal for infants. Inspired by this, we propose a new framework to extract object-centric representation from single 2D images by learning to predict future scenes in the presence of moving objects. We treat objects as latent causes whose function to an agent is to facilitate efficient prediction of the coherent motion of their parts in visual input. Distinct from previous object-centric models, our model learns to explicitly infer objects' locations in 3D environment in addition to segmenting objects. Further, the network learns a latent code space where objects with the same geometric shape and texture/color frequently group together. The model requires no supervision or pre-training of any part of the network. We provide a new synthetic dataset with more complex textures on objects and background and found several previous models not based on predictive learning overly rely on clustering colors and lose specificity in object segmentation. Our work demonstrates a new approach for learning symbolic representation grounded in sensation and action.\\n\\nINTRODUCTION\\n\\nVisual scenes are composed of various objects in front of backgrounds. Discovering objects from 2D images and inferring their 3D locations is crucial for planning actions in robotics (Devin et al., 2018; Wang et al., 2019) and this can potentially provide better abstraction of the environment for reinforcement learning (RL), e.g. Veerapaneni et al. (2020). The appearance and spatial arrangement of objects, together with the lighting and the viewing angle, determine the 2D images formed on the retina or a camera. Therefore, objects are latent causes of 2D images, and discovering object is a process of inferring latent causes (Kersten et al., 2004). The predominant approach in computer vision for identifying and localizing objects rely on supervised learning to infer bounding boxes (Ren et al., 2015; Redmon et al., 2016) or pixel-level segmentation of objects (Chen et al., 2017). However, the supervised approach requires expensive human labeling. It is also difficult to label every possible category of objects. Therefore, an increasing interest has developed recently to build unsupervised or self-supervised models to infer objects from images, such as MONet (Burgess et al., 2019), IODINE (Greff et al., 2019) slot-attention (Locatello et al., 2020), GENESIS (Engelcke et al., 2019; 2021), C-SWM (Kipf et al., 2019) and mulMON (Nanbo et al., 2020). Our work also focuses on the same unsupervised object-centric representation learning (OCRL) problem, but offers a new learning objective and architecture to overcome the limitation of existing works in segmenting more complex scenes and explicitly represents objects' 3D locations.\\n\\nThe majority of the existing OCRL works are demonstrated on relatively simple scenes with objects of pure colors and background lacking complex textures. As recently pointed out, the success of several recent models based on a variational auto-encoder (VAE) architecture (Kingma & Welling, 2013; Rezende et al., 2014) depends on a reconstruction bottleneck that needs to be intricately balanced (Engelcke et al., 2020). To evaluate how such models perform on scenes with more complex surface textures, we created a new dataset of indoor scenes with diverse texture patterns on the objects and background. We found that several existing unsupervised OCRL models overly rely on clustering pixels based on their colors. A challenge in our dataset and in real-world perception is that sharp boundaries between different colors exist both at contours of objects and within the surface of the same object. A model essentially has to implicitly learn a prior knowledge of which types of boundaries are more likely to be real object contours. The reconstruction loss in existing works appears to be insufficient for learning this prior.\"}"}
{"id": "IsHQmuOqRAG", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To tackle this challenge, we draw our inspiration from development psychology and neuroscience. Infants understand the concept of object as early as 8 months, before they can associate objects with names (Piaget & Cook, 1952; Flavell, 1963). The fact that infants are surprised when object are hidden indicates that they have already learned to segment discrete objects from the scene and that their brains constantly make prediction and check for deviation from expected outcome. As the brain lacks direct external supervision for object segmentation, the most likely learning signal is from the error of this prediction. In the brain, a copy of the motor command (efference copy) is sent from the motor cortex simultaneously to the sensory cortex, which is hypothesized to facilitate the prediction of changes in sensory input due to self-generated motion (Feinberg, 1978). What remains to be predicted are changes in visual input due to the motion of external objects. Therefore, we believe that the functional purpose of grouping pixels into object is to allow the prediction of the motion of the constituting pixels in a coherent way by tracking very few parameters (e.g., the location, pose, and speed of an object). Driven by this hypothesis, our contribution in this paper is: (1) we combine predictive learning and explicitly 3D motion prediction to learn 3D aware object-centric representation, which we call Object Perception by Predictive LEarning (OPPLE); (2) we provide a new dataset with complex surface texture and motion by both the camera and objects to evaluate object-centric representation models; (3) we found several previous models overly rely on clustering colors to segment objects; (4) although our model leverages image prediction as learning objective, the architecture generalize the ability of object segmentation and spatial localization to single-frame images.\\n\\nMETHODOLOGY\\n\\n2.1 PROBLEM FORMULATION\\n\\nWe denote a scene as a set of distinct objects and a background $S = \\\\{O_1, O_2, \\\\ldots, O_K, B\\\\}$, where $K$ is the number of objects in scene. At any moment $t$, we denote two state variables, the location and pose of each object relative to the perspective of an observer (camera), as $x(t)_k$ and $\\\\phi(t)_k$, where $x(t)_k$ is the 3-d coordinate of the $k$-th object and $\\\\phi(t)_k$ is its yaw angle from a canonical pose, as viewed from the reference frame of the camera (for simplicity, we do not consider pitch and roll here and leave it for future work to extend to 3D pose). At time $t$, given the location of the camera $o(t) \\\\in \\\\mathbb{R}^3$ and its facing direction $\\\\alpha(t)$, $S$ renders a 2D image on the camera as $I(t) \\\\in \\\\mathbb{R}^{w \\\\times h \\\\times 3}$, where $w \\\\times h$ is the size of the image. Our goal is to train a neural network that infers properties of objects given only a single image $I(t)$ as the sole input without external supervision and with only the information of the intrinsics and egomotion of the camera without explicit knowledge of its extrinsic matrix: $\\\\{z(t)_1:K, \\\\pi(t)_1:K+1, \\\\hat{x}(t)_1:K, p(t)_\\\\phi 1:K\\\\} = f_{\\\\text{obj}}(I(t))$ (1)\\n\\nHere, $z(t)_1:K$ is a set of view-invariant vectors representing the identity of each object $k$. \u201cView-invariant\u201d is loosely defined as $|z(t)_k - z(t+\\\\Delta t)_k| < |z(t)_k - z(t)_l|$ for $k \\\\neq l$ and $\\\\Delta t > 0$ in most cases, i.e., the vector codes are more similar for the same object across views than they are different across objects. $\\\\pi(t)_1:K+1 \\\\in \\\\mathbb{R}^{(K+1) \\\\times w \\\\times h}$ are the probabilities that each pixel belongs to any of the objects or the background ($\\\\sum_k \\\\pi_k_{ij} = 1$ for any pixel at $i,j$), which achieve object segmentation. To localize objects, $\\\\hat{x}(t)_1:K$ are the estimated locations of each object relative to the observer and $p(t)_\\\\phi 1:K$ are estimated probability distribution of the poses of each object. Each $p(t)_\\\\phi_k \\\\in \\\\mathbb{R}^b$ is a probability distribution over $b$ equally-spaced bins of yaw angles in $(0, 2\\\\pi)$.\"}"}
{"id": "IsHQmuOqRAG", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"flow of each pixel can be predicted based on the object's speed and the position of each pixel relative to the object's center. The pixel-segmentation of an object essentially prescribes which pixels should move together with the object. With the predicted optical flow, one can further predict part of the next image by warping the current image. The parts of the next image unpredictable by warping include surfaces of objects or the background that are currently occluded but will become visible, and the region of a scene newly entering the view due to self- or object-motion. These portions can only be predicted based on the learned statistics of the appearance of objects and background, which we call \\\"imagination.\\\" In this work, we will show that with the information of self-motion, knowledge of geometry (rule of rigid-body movement) and the assumption of smooth object movement, the object representations captured by function $f_{\\\\text{obj}}$ and depth perception can be learned without supervision.\\n\\n2.3 NETWORK ARCHITECTURE\\n\\nTo demonstrate the hypothesized principle above, we build our OPPLE networks as illustrated in Figure 1, which process two consecutive images individually and make prediction for the next image with the information extracted from them.\\n\\n---\\n\\nFigure 1: Architecture for the Object Perception by Predictive LEarning (OPPLE) network. Images at each time point are processed by the Object Extraction and Depth Perception Networks independently. All networks use U-Net structure. The dotted boxes indicate the entire OPPLE network acting at each time step (details omitted for $t-1$). Motion information of each object is estimated from the spatial information extracted for each object between $t-1$ and $t$. Objects between frames are soft-matched by a score depending on the distance between their latent codes. Self- and object motion information are used together with object segmentation and depth map to predict the next image by warping the current image. The segmented object images and depth, together with their motion information and the observer's motion, are used by the imagination network to imagine the next scene and fill the gap not predictable by warping. The error between the final combined prediction and the ground truth of the next image provides teaching signals for all three networks.\\n\\nObject extraction network. We build an object extraction network $f_{\\\\theta_{\\\\text{obj}}}$ by modification of a U-Net (Ronneberger et al., 2015) to extract representation for each object, the pixels it occupies and its spatial location and pose. A basic U-Net is composed of a convolutional encoder and a transposed convolutional decoder, while each encoder layer sends a skip connection to the corresponding decoder layer, so that the decoder can combine both global and local information. Inside our $f_{\\\\theta_{\\\\text{obj}}}$, an image $I(t+1)$ first passes through the encoder. Additional Atrous spatial pyramid pooling layer (Chen et al., 2017) is inserted between the middle two convolutional layers of the encoder to expand the receptive field. The top layer of the encoder outputs a feature vector $e_t$ capturing the global information of the scene. A Long-Short Term Memory (LSTM) network further repeatedly reads in $e_t$. $3$\"}"}
{"id": "IsHQmuOqRAG", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022.\\n\\nand sequentially outputs one code for an object at a time. Each object code is then mapped through a one-layer fully connected network to predict object code $z(t)_k$, object location $\\\\hat{x}(t)_k$ and object pose probability $p_\\\\phi(t)_k$, $k = 1, 2, \\\\ldots, K$. The inferred location is restricted to the visible range of the camera with an upper limit of distance. The pose prediction is represented as $\\\\log p_\\\\phi(t)_k$ for numerical stability. Each object code vector $z(t)_k$ is then independently fed through the decoder with shared skip connection from the encoder. The decoder outputs one channel for each pixel, representing an un-normalized log likelihood that the pixel belongs to the object $k$. The unnormalized logit maps for all objects are concatenated with a map of all zero for the background, and compete through a softmax function to output the probabilistic segmentation map $\\\\pi(t)_k$.\\n\\nDepth perception network. We use a standard U-Net for depth perception function $h_\\\\theta$ depth that processes images $I(t)$ and output a single-channel depth map $D(t)$.\\n\\nObject-based imagination network. We build the imagination network $g_{\\\\theta}$ imag also with a modified U-Net. The input is concatenated image $I(t)$ and log of depth $\\\\log(D(t))$ inferred by the depth perception network, both multiplied element-wise by one probabilistic mask $\\\\pi(t)_k$. The output of the encoder network is concatenated with a vector composed of the observer's moving velocity $v_{\\\\text{obs}}(t)$ and rotational speed $\\\\omega_{\\\\text{obs}}(t)$, and the estimated object location $\\\\hat{x}(t)_k$, velocity $\\\\hat{v}(t)_k$ and rotational speed $\\\\hat{\\\\omega}(t)_k$ before entering the decoder. The decoder outputs five channels for each pixel: three for predicting RGB colors, one for depth and one for the probability of the pixel belonging to any object $k$ or background and is used to weight the predicted color and depth for the final \\\"imagination.\\\"\"}"}
{"id": "IsHQmuOqRAG", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Considering that object attribution of each pixel has to be inferred, we can write object attribution as a probability of belonging to each object, across all pixels, \\\\( \\\\pi(t) \\\\), \\\\( k = 1, 2, \\\\ldots, K + 1 \\\\). The predicted motion of each pixel should be described as a probability distribution over \\\\( K + 1 \\\\) discrete target locations \\n\\\\[\\n\\\\text{predicted motion} = \\\\sum_{k=1}^{K+1} \\\\pi(t)_{kij} \\\\delta(x'(t+1)_k, (i,j))\\n\\\\]\\n\\nwhere \\\\( \\\\pi(t)_{kij} \\\\) is the probability of pixel \\\\((i,j)\\\\) moving to location \\\\(x'(t+1)_k\\\\). Using probabilistic prediction of pixel movements, we partially predict the color of the next image at the pixel grids where some original pixels from the current frame will land by weighting their contribution \\n\\\\[\\nI'(t+1)_k \\\\text{Warp}(p,q)\\n\\\\]\\n\\nThis contribution term is used to calculate a weight for each pixel \\\\((i,j)\\\\) on position \\\\((p,q)\\\\) on the grid, and is written as \\\\(w_k(i,j,p,q)\\\\). More details in the appendix.\\n\\n2.4.2 Imagination\\n\\nFor the regions not fully predictable by warping the current image with the appendix equation (7), i.e., for \\\\((p,q)\\\\) where \\\\(\\\\sum_{k,i,j} w_k(i,j,p,q) < 1\\\\), we learn a function \\\\(g\\\\) that \\\"imagines\\\" the appearance \\\\(I'(t+1)_k\\\\text{Imag} \\\\in \\\\mathbb{R}^{w \\\\times h \\\\times 3}\\\\) and the pixel-wise depth \\\\(D'(t+1)_k\\\\text{Imag} \\\\in \\\\mathbb{R}^{w \\\\times h}\\\\) of the object or background \\\\(k\\\\) in the next frame, and the predicted probabilities that each pixel in the next frame belongs to each object or the background \\\\(\\\\pi'(t+1)_k\\\\text{Imag} \\\\in \\\\mathbb{R}^{w \\\\times h}\\\\). The function takes as input portion of the current image corresponding to each object \\\\(I(t) \\\\odot \\\\pi(t)_k\\\\) and the inferred depth \\\\(D(t) \\\\odot \\\\pi(t)_k\\\\), both extracted by element-wise multiplying with the probabilistic segmentation mask \\\\(\\\\pi(t)_k\\\\), the information of the camera's self-motion, and the location and motion of that object:\\n\\n\\\\[\\n\\\\{I'(t+1)_k\\\\text{Imag}, D'(t+1)_k\\\\text{Imag}, \\\\pi'(t+1)_k\\\\text{Imag}\\\\} = g(I(t) \\\\odot \\\\pi(t)_k, D(t) \\\\odot \\\\pi(t)_k, \\\\hat{x}(t)_k, \\\\hat{v}(t)_k, \\\\hat{\\\\omega}(t)_k, \\\\omega_{\\\\text{obs}}(2))\\n\\\\]\\n\\nThe \\\"imagination\\\" specific for each object and the background can then be merged using the weights prescribed by \\\\(\\\\pi'(t+1)_k\\\\) \\n\\\\[\\nI'(t+1)_k\\\\text{Imag} = \\\\sum_{k} I'(t+1)_k\\\\text{Imag} \\\\odot \\\\pi'(t+1)_k\\\\text{Imag}, \\\\quad D'(t+1)_k\\\\text{Imag} = \\\\sum_{k} D'(t+1)_k\\\\text{Imag} \\\\odot \\\\pi'(t+1)_k\\\\text{Imag}\\n\\\\]\\n\\n2.4.3 Combining Warping and Imagination\\n\\nThe final predicted image or depth map are weighted average of the prediction made by warping the current image or predicted depth map and the corresponding predictions by imagination:\\n\\n\\\\[\\nI'(t+1) = I'(t+1)\\\\text{Warp} \\\\odot W_{\\\\text{Warp}} + I'(t+1)\\\\text{Imag} \\\\odot (1 - W_{\\\\text{Warp}})\\n\\\\]\\n\\nHere, \\\\(W_{\\\\text{Warp}} \\\\in \\\\mathbb{R}^{w \\\\times h}\\\\) with each element \\n\\\\[\\nW_{\\\\text{Warp}}(p,q) = \\\\max\\\\{\\\\sum_{k,i,j} w_k(i,j,p,q), 1\\\\}\\n\\\\]\\n\\nThe intuition is that imagination is only needed when there is not sufficient contribution for predicting a pixel by warping. The same weighting applies for generating the final predicted depth \\\\(D'(t+1)\\\\).\\n\\nIn addition to the image, the states of the objects can also be predicted. The location of object \\\\(k\\\\) at \\\\(t+1\\\\) can be predicted as \\n\\\\[\\nx'(t+1)_k = M(t) - \\\\omega_{\\\\text{obs}}(\\\\hat{x}(t)_k + \\\\hat{v}(t)_k - v_{\\\\text{obs}})\\n\\\\]\\n\\nIts new pose probability can be predicted by \\n\\\\[\\np'(t+1)_{\\\\phi_k + \\\\omega_{\\\\text{obs}}} = \\\\gamma_2 = \\\\sum_{\\\\gamma_1, \\\\omega, \\\\gamma_2 - \\\\gamma_1 \\\\in \\\\{\\\\omega - 2\\\\pi, \\\\omega, \\\\omega + 2\\\\pi\\\\}} p(\\\\hat{\\\\omega}(t)_k = \\\\omega)p(\\\\phi(t)_k = \\\\gamma_1)\\n\\\\]\\n\\nfor \\\\(\\\\gamma_2\\\\) equal to each fixed yaw angle bin. To obtain \\\\(p'(t+1)_{\\\\phi_k + \\\\omega_{\\\\text{obs}}}\\\\) instead of \\\\(p'(t+1)_{\\\\phi_k + \\\\omega_{\\\\text{obs}}}\\\\) at the same set of bins, the vector \\n\\\\[\\np'(t+1)_{\\\\phi_k + \\\\omega_{\\\\text{obs}}}\\\\] can be shifted by multiplication with a pre-computed matrix composed of a bank of shifted V on Mises distributions to \\\"move\\\" the probability mass on the angle bins.\\n\\nThere is one important issue of object-centric representation when making prediction for future images: in order to predict the spatial state of each object at \\\\(t+1\\\\) based on the views at \\\\(t\\\\) and \\\\(t-1\\\\), the network needs to match the representation of an object at \\\\(t\\\\) from the representation of the same object at \\\\(t-1\\\\). As the dimensions of features (e.g., shape, surface texture, size, etc.) grows, the number of possible objects grows exponentially. Therefore, we cannot simply match object representations based on the order by which an LSTM extracts objects, as this requires learning a consistent order over enormous amount of objects. Instead, we take a soft-matching approach: we take a subset of the features in \\\\(z(t)_k\\\\) extracted by \\\\(f\\\\) as an identity code for each object. For object \\\\(k\\\\) at time \\\\(t\\\\), we calculate the distance between its identity code and those of all objects at \\\\(t-1\\\\), and pass the distances through a radial basis function to serve as a matching score \\n\\\\(r_{kl}\\\\) indicating how closely the object \\\\(k\\\\) matches each of the previous objects. The scores are used weight all the\"}"}
{"id": "IsHQmuOqRAG", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nestimated translational and rotational speeds for object\\n\\nwere the true object at \\\\( t - 1 \\\\). We additionally introduce a fixed identity code \\\\( z_{K+1} = 0 \\\\) for the background and set the predicted motion of background to zero.\\n\\n2.4.4 LEARNING OBJECTIVE\\n\\nAbove, we have explained how the next image input \\\\( I'(t+1) \\\\), the depth map \\\\( D'(t+1) \\\\) and the spatial states of each object, \\\\( x'(t+1)_k \\\\) and \\\\( p'(t+1)_\\\\phi_k \\\\) can be predicted based on object-centric representation extracted by a function \\\\( f \\\\) from the current and previous images \\\\( I(t) \\\\) and \\\\( I(t-1) \\\\), the depth \\\\( D(t) \\\\) extracted by a function \\\\( h \\\\), combined with the prediction from object-based imagination function \\\\( g \\\\) that are all to be learned. Among the three prediction targets, only the ground truth of visual input \\\\( I(t+1) \\\\) is available, while the other can only be inferred by \\\\( f \\\\) and \\\\( h \\\\) from \\\\( I(t+1) \\\\). Therefore, for the prediction targets other than \\\\( I(t+1) \\\\), we use the self-consistent loss between the predicted value based on \\\\( t \\\\) and \\\\( t - 1 \\\\) and the inferred value based on \\\\( t + 1 \\\\) as additional regularization terms to learn the functions \\\\( f \\\\) and \\\\( g \\\\).\\n\\nTo learn the functions \\\\( f \\\\), \\\\( g \\\\) and \\\\( h \\\\), we approximate them with deep neural networks with parameters \\\\( \\\\theta \\\\) and optimize \\\\( \\\\theta \\\\) to minimize the following loss function:\\n\\n\\\\[\\nL = L_{image} + \\\\lambda_{depth} L_{depth} + \\\\lambda_{spatial} L_{spatial} + \\\\lambda_{map} L_{map} (4)\\n\\\\]\\n\\nHere, \\\\( L_{image} = \\\\text{MSE}(I'(t+1), I(t+1)) \\\\) is the image prediction error. \\\\( L_{depth} = \\\\text{MSE}(\\\\log(D'(t+1)), \\\\log(\\\\hat{D}(t+1))) \\\\) is the error between the predicted and inferred depth. These provide major teaching signals. \\\\( L_{spatial} = \\\\sum_{K} \\\\sum_{k=1}^{K} \\\\left| \\\\sum_{l=1}^{K+1} r_{kl} x'(t+1)_l - \\\\hat{x}(t+1)_k \\\\right|^2 - \\\\sum_{K} \\\\min_{\\\\hat{x}(t+1)_{\\\\text{rand}}} \\\\left| \\\\hat{x}(t+1)_{\\\\text{rand}} - \\\\hat{x}(t+1)_k \\\\right| + \\\\sum_{K} \\\\left| \\\\hat{x}(t+1)_k - \\\\sum_{i,j} \\\\hat{m}(t+1)_{i,j} \\\\pi_{kij} \\\\right|^2 + \\\\sum_{K} D_{KL}(\\\\hat{p}(t+1)_\\\\phi_k \\\\| \\\\sum_{l=1}^{K+1} r_{kl} p'(t+1)_\\\\phi_l) \\\\) is the self-consistent loss on spatial information prediction. The first term is the error between inferred and predicted location of each object, while the calculation of the predicted location incorporates soft matching between objects in consecutive frames. The second term is the negative term of contrastive loss, which we found empirically prevents the network from reaching a local minimum where all objects are inferred at the same location relative to the camera (and covering minimal regions of the picture). \\\\( \\\\hat{x}_{\\\\text{rand}} \\\\) is the inferred object location from a random sample within the same batch. The third term penalizes the discrepancy between the inferred object location and the average location of pixels in its segmentation mask. The last term is the KL-divergence between the predicted and inferred pose for each object at \\\\( t + 1 \\\\).\\n\\n\\\\( L_{map} = \\\\text{ReLU}(10 - 4 - \\\\pi_{1:K}) + \\\\pi_k \\\\cdot \\\\pi_l \\\\), for \\\\( k \\\\neq l \\\\) avoids loss of gradient due to zero probability of object belonging and discourages overlap between maps of different objects.\\n\\n2.5 DATASET\\n\\nWe procedurally generated a dataset composed of 306445 triplets of images captured by a virtual camera with field of view of 90 degrees in a square room. The camera translates horizontally and pans with random small steps between consecutive frames to facilitate the learning of depth perception. 3 objects with random shape, size, surface color or textures are spawned at random locations in the room and each move with a randomly selected constant velocity and panning speed. The translation and panning of the camera is known to the networks. No other ground truth information is provided. The first two frames serve as data and the last frame serve as the prediction target at \\\\( t + 1 \\\\). An important difference between this dataset and other commonly used synthetic datasets for OCRL is that more complex and diverse textures are used on both the objects and the background. We further evaluated on a richer version of the Traffic dataset (Henderson & Lampert, 2020) in A.6.\\n\\n2.6 COMPARISON WITH OTHER WORKS\\n\\nTo compare our work with the states-of-the-art models of unsupervised object-centric representation learning, we trained MONet (Burgess et al., 2019), slot-attention (Locatello et al., 2020) and GENESIS \\\\(^2\\\\) (Engelcke et al., 2021) on the same dataset. Although these models are trained on single images, all images of each triplets are used for training.\\n\\nWe failed to obtain reasonable result by training GENESIS V2 on our dataset, thus we adopted a GENESIS network pre-trained on GQN dataset and retrained on our dataset with \\\\( K = 7 \\\\).\"}"}
{"id": "IsHQmuOqRAG", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nTable 1: Performance of different models on object segmentation.\\n\\n| Model                      | ARI   | IoU  |\\n|----------------------------|-------|------|\\n| MONet                      | 0.31  | 0.08 |\\n| MONet-128                  | 0.33  | 0.22 |\\n| MONet-128-bigger           | 0.33  | 0.15 |\\n| slot-attention             | 0.41  | 0.31 |\\n| slot-attention-128         | 0.39  | 0.54 |\\n| GENESIS                    | 0.17  | 0.03 |\\n| Our model (OPPLE)          | 0.46  | 0.35 |\\n\\nTo address the concern that the original configurations of the models are not optimized for more difficult dataset, we trained variants of some of the models with large network size. For MONet, we tested channel numbers of [32, 64, 128, 128] (MONet-128) and [32, 64, 128, 256, 256] (MONet-128-bigger) for the hidden layers of encoder of the component V AE instead of [32, 32, 64, 64] and adjusted decoder layers sizes accordingly, and increased the base channel from 64 to 128 for the attention network. For slot attention, we tested a variant which increased the number of features in the attention component from 64 to 128 (slot-attention-128). Slot numbers were chosen as 4 except for GENESIS.\\n\\n3 RESULTS\\n\\nAfter training the networks, we evaluate them on 4000 test images unused during training but generated randomly with the same procedure, thus coming from the same distribution. We compare the performance of different models mainly on their segmentation performance. Additionally, we demonstrate the ability unique to our model: inferring locations of objects in 3D space and the depth of the scene. The performance of depth perception is illustrated in the appendix.\\n\\n3.1 OBJECT SEGMENTATION\\n\\nFollowing prior works (Greff et al., 2019; Engelcke et al., 2019; 2021), we evaluated segmentation with the Adjusted Rand Index of foreground objects (ARI). In addition, for each image, we matched ground-true objects and background with each of the segmented class by ranking their Intersection over Union (IoU) and quantified the average IoU over all foreground objects. The performance is summarized in table 2.6.\\n\\nOur model outperforms all compared models on ARI and is second to a slot-attention-128 in IoU. As shown in Figure 2, MONet and GENESIS appear to heavily rely on color to group pixels into the same masks. Even though some of these models almost fully designate pixels of an object to a mask, the masks lacks specificity in that they often include pixels with similar colors from other objects or background. Patterns on the backgrounds are often treated as objects as well. The reason of such drawbacks awaits further investigation but we postulate there may be fundamental limitation in the approach that learns purely from static discrete images. Patches in the background with coherent color offer room to compress information similarly as objects with coherent colors do, and their shapes re-occur across images just as other objects.\\n\\nOur model is able to learn object-specific masks because these masks are used to predict optical flow specific to each object. A wrong segmentation would generate large prediction error even if the motion of an object is estimated correctly. Such prediction error forces the masks to be concentrated on object surface. They emerge first at object boundaries where the prediction error is the largest and gradually grow inwards during training. Figure 3A-D further compares the distribution.\"}"}
{"id": "IsHQmuOqRAG", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our model (OPPLE)\\nMONet-128 slot-attention-128\\n\\nFigure 3: A-D: distribution of IoU. All models have IoU < 0.01 for about 1/4 of objects. Only OPPLE shows a bi-modal distribution while other models' IoU are more skewed towards 0.\\n\\nE-F: object localization accuracy of OPPLE for object's polar angle and distance relative to the camera. Each dot is a valid object with color representing its segmentation IoU. Angle estimation is highly accurate for well segmented objects (red dots). Distance is under-estimated for farther objects.\\n\\nG: objects with failed segmentation (blue dots) are mostly far away and occupying few pixels.\\n\\nH: The numbers of objects sharing the same shape or texture with their nearest neighbour objects in latent space are significantly above chance.\\n\\n3.2 OBJECT LOCALIZATION\\nThe Object Extraction Network infers object location relative to the camera. We convert the inferred locations to angles and distance in polar coordinate relative to the camera. Figure 3E-F plot the true and inferred angles and distance, color coded by objects' IoUs. For objects well segmented (red dots), their angles are estimated high accurately (concentrated on the diagonal in E). Distance estimation is negatively biased for farther objects, potentially because the regularization term on the distance between the predicted and inferred object location at frame $t + 1$ favors shorter distance when estimation is noisy. Note that the ability to explicitly infer object's location is not available in other models compared.\\n\\n3.3 MEANINGFUL LATENT CODE\\nBecause a subset of the latent code (10 dimensions) was used to calculate object matching scores between frames in order to soft-match objects, this should force the object embedding $z$ to be similar for the same objects. We explored the geometry of the latent code by examining whether the nearest neighbours of each of the object in the test data with IoU $> 0.5$ are more likely to have the same property as themselves. 772 out of 3244 objects' nearest neighbour had the same shape (out of 11 shapes) and 660 objects' nearest neighbour had the same color or texture (out of 15). These numbers are 28 to 29 times the standard deviation away from the means of the distribution expected if the nearest neighbour were random (Figure 3H). This suggests the latent code reflects meaningful features of objects. However, texture and shape are not the only factors determining latent code, as we found the variance of code of all objects with the same shape and texture to still be big.\\n\\n4 RELATED WORK\\nOur work is on the same tracks as two recent trends in machine learning: object-centric representation (Locatello et al., 2020) and self-supervised learning (Chen et al., 2020). We take the same\"}"}
{"id": "IsHQmuOqRAG", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"logic as self-supervised learning that learning to predict part of the data based on another part forces a neural network to learn important structures in the data. However, most of the existing works in self-supervised learning do not focus on object-based representation, but instead encode the entire scene as one vector. Other works on object-centric representations overcome this by assigning one representation to each object, as we do. Although works such as MONet (Burgess et al., 2019), IO-DINE (Greff et al., 2019), slot-attention (Locatello et al., 2020), GENESIS (Engelcke et al., 2019) and PSGNet (Bear et al., 2020) can also segment objects and some of them can \u201cimagine\u201d complete objects based on codes extracted from occluded objects or draw objects in the correct order consistent with occlusion, few works explicitly infer an object\u2019s location in 3D space together with segmentation purely by self-supervised learning, with the exception of a closely related work O3V (Henderson & Lampert, 2020). Both our works learn from videos. One major distinction is that O3V interleaves spatial and temporal convolution, thus it still require video as input at test time. In contrast, our three major networks process each image independently. Therefore, once trained, our network can generalize to single images. Another distinction from Henderson & Lampert (2020) and many other works is that our model learns from prediction instead of reconstruction. Contrastive-learning of structured world model (Kipf et al., 2019) also learns object masks and predict their future states by linking each object mask with a node in a Graphic Neural Network (GNN). The order of mapping object slot to nodes of GNN is fixed through time, and the actions to objects are coded with specific associated nodes. This arrangement may become infeasible with combinatorial number of different possible objects as the order of assigning different objects to a limited number of nodes may not be consistent across scenes. We solve this by a soft matching of object representation between different time points, which does not require the RNN in the Object Extraction Network to learn a fixed order of extracting different types of objects. On the neuroscience side, our work is highly motivated by recent works on predictive learning (O\u2019Reilly et al., 2021) which also yields view-invariance representation while self-motion signal is available. O\u2019Reilly et al. (2021) used biologically plausible but less efficient learning and applied their model to an easier dataset with objects without background, and did not learn object localization. We should note that explicit spatial localization and depth perception were not pursued in previous works on self-supervised object-centric learning, and the images in our dataset have significantly richer texture information than those demonstrated in previous works (Burgess et al., 2019; Kipf et al., 2019), making the task more challenging. Although view synthesis is not our central goal, the principle illustrated here can be combined with recent advancement in 3D-aware image synthesis (Wiles et al., 2020).\\n\\n5 DISCUSSION\\n\\nWe provide a new approach to learn object-centric representation that includes explicit spatial localization of objects, object segmentation from image, automatic matching the same objects across scene based on a learned latent code and depth perception as a by-product. All of the information extracted by our networks are learned without supervision and no pre-training on other tasks is involved. The only additional information required is that of observer\u2019s self-motion, which is available in the brain as efference copy. This demonstrate the possibility of learning rich embodied information of object, one step toward linking neural networks with symbolic representation in general. We expect future works to develop self-supervised learning model for natural categories beyond simple object identity, building on our work.\\n\\nThe work demonstrates that the notion of object can emerge as a necessary common latent cause of the pixels belonging to the object for the purpose of efficiently explaining away the pixels\u2019 coherent movement across frames. In our experiment, object spatial location is inferred more easily than object pose (which we have not fully investigated), thus the predicted warping relies more on object translation than rotation. As a limitation, almost all existing object-centric representation works, including ours, focus on rigid bodies and simple environment. Future works need to explore how to learn object representation for deformable objects, objects with more complex shapes and lighting conditions, and more cluttered environment, towards more realistic application. There is important implication for learning 3D-aware object-based representation, because for such representation to be useful eventually for robotics and RL, agents need to understand an object\u2019s spatial relation to itself based on vision. Learning object-centric representation with explicit 3D information is an important step toward embodied representation of the environment.\"}"}
{"id": "IsHQmuOqRAG", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Daniel M Bear, Chaofei Fan, Damian Mrowca, Yunzhu Li, Seth Alter, Aran Nayebi, Jeremy Schwartz, Li Fei-Fei, Jiajun Wu, Joshua B Tenenbaum, et al. Learning physical graph representations from visual scenes. arXiv preprint arXiv:2006.12373, 2020.\\n\\nChristopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390, 2019.\\n\\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834\u2013848, 2017.\\n\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Hal Daum\u00e9 III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 1597\u20131607. PMLR, 13\u201318 Jul 2020.\\n\\nColine Devin, Pieter Abbeel, Trevor Darrell, and Sergey Levine. Deep object-centric representations for generalizable robot learning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 7111\u20137118. IEEE, 2018.\\n\\nMartin Engelcke, Adam R Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative scene inference and sampling with object-centric latent representations. arXiv preprint arXiv:1907.13052, 2019.\\n\\nMartin Engelcke, Oiwi Parker Jones, and Ingmar Posner. Reconstruction bottlenecks in object-centric generative models. arXiv preprint arXiv:2007.06245, 2020.\\n\\nIrwin Feinberg. Efference copy and corollary discharge: implications for thinking and its disorders. Schizophrenia bulletin, 4(4):636, 1978.\\n\\nJohn H Flavell. The developmental psychology of jean piaget. 1963.\\n\\nKlaus Greff, Rapha\u00ebl Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning with iterative variational inference. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 2424\u20132433. PMLR, 09\u201315 Jun 2019.\\n\\nPaul Henderson and Christoph H Lampert. Unsupervised object-centric video generation and decomposition in 3d. arXiv preprint arXiv:2007.06705, 2020.\\n\\nDaniel Kersten, Pascal Mamassian, and Alan Yuille. Object perception as bayesian inference. Annu. Rev. Psychol., 55:271\u2013304, 2004.\\n\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\\n\\nThomas Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world models. arXiv preprint arXiv:1911.12247, 2019.\\n\\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. arXiv preprint arXiv:2006.15055, 2020.\"}"}
{"id": "IsHQmuOqRAG", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Li Nanbo, Cian Eastwood, and Robert B Fisher. Learning object-centric representations of multi-object scenes from multiple views. In *34th Conference on Neural Information Processing Systems*, 2020.\\n\\nRandall C O\u2019Reilly, Jacob L Russin, Maryam Zolfaghar, and John Rohrlich. Deep predictive learning in neocortex and pulvinar. *Journal of Cognitive Neuroscience*, 33(6):1158\u20131196, 2021.\\n\\nJean Piaget and Margaret Trans Cook. The origins of intelligence in children. 1952.\\n\\nJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pp. 779\u2013788, 2016.\\n\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In *Advances in neural information processing systems*, 28:91\u201399, 2015.\\n\\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In *International conference on machine learning*, pp. 1278\u20131286. PMLR, 2014.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In *International Conference on Medical image computing and computer-assisted intervention*, pp. 234\u2013241. Springer, 2015.\\n\\nRishi Veerapaneni, John D Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu, Joshua Tenenbaum, and Sergey Levine. Entity abstraction in visual model-based reinforcement learning. In *Conference on Robot Learning*, pp. 1439\u20131456. PMLR, 2020.\\n\\nDequan Wang, Coline Devin, Qi-Zhi Cai, Fisher Yu, and Trevor Darrell. Deep object-centric policies for autonomous driving. In *2019 International Conference on Robotics and Automation (ICRA)*, pp. 8853\u20138859. IEEE, 2019.\\n\\nOlivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. Synsin: End-to-end view synthesis from a single image. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 7467\u20137477, 2020.\"}"}
{"id": "IsHQmuOqRAG", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Algorithm 1**\\n\\nDeveloping object-centric representation by predicting future scene\\n\\nInit\\n\\nInitialize:\\n\\nNetwork parameters \\\\( \\\\theta \\\\)\\n\\nInput:\\n\\nimages \\\\( I(t-1), I(t) \\\\) \u2208 \\\\( \\\\mathbb{R}^{w \\\\times h \\\\times 3} \\\\), self-motion \\\\( v(t-1) \\\\), \\\\( \\\\omega(t-1) \\\\), \\\\( v(t) \\\\), \\\\( \\\\omega(t) \\\\)\\n\\nOutput:\\n\\nprediction \\\\( I'(t+1) \\\\), segmentation \\\\( \\\\pi(t-1) \\\\)\\n\\n1: \\\\( K+1 \\\\), \\\\( \\\\pi(t) \\\\)\\n\\n1: \\\\( K+1 \\\\), objects' codes \\\\( z(t-1) \\\\)\\n\\n1: \\\\( K \\\\), \\\\( z(t) \\\\)\\n\\n1: \\\\( K \\\\), objects' locations and poses \\\\( \\\\hat{x}(t-1) \\\\)\\n\\n1: \\\\( K \\\\), \\\\( p(\\\\phi(t-1)) \\\\)\\n\\n1: \\\\( K \\\\), \\\\( p(\\\\phi(t)) \\\\)\\n\\n1: \\\\( K \\\\), \\\\( p(\\\\phi(t+1)) \\\\)\\n\\nfor \\\\( \\\\tau = \\\\{t-1, t\\\\} \\\\) do\\n\\nscene code \\\\( e(\\\\tau) \\\\) \u2190 U-NetEncoder \\\\( f(\\\\theta(I(\\\\tau))) \\\\)\\n\\nobject code \\\\( z(\\\\tau) \\\\)\\n\\n1: \\\\( K \\\\), location \\\\( \\\\hat{x}(\\\\tau) \\\\)\\n\\n1: \\\\( K \\\\), pose \\\\( p(\\\\phi(\\\\tau)) \\\\)\\n\\nbackground code \\\\( z_{K+1} = 0 \\\\)\\n\\ndepth \\\\( D(\\\\tau) \\\\) \u2190 \\\\( h(\\\\theta(I(\\\\tau))) \\\\)\\n\\nsegmentation mask \\\\( \\\\pi(\\\\tau) \\\\)\\n\\n1: \\\\( K+1 \\\\) \u2190 Softmax \\\\( U-\\\\text{NetDecoder} f(\\\\theta(I(\\\\tau)), z(\\\\tau)) \\\\), \\\\( 0 \\\\)\\n\\nend for\\n\\nobject matching scores \\\\( r_{kl} \\\\) \u2190 RBF \\\\( z(t)_{k}, z(t-1)_{l} \\\\), \\\\( k, l \\\\in 1:K+1 \\\\)\\n\\nfor \\\\( k \\\\leftarrow 1 \\\\) to \\\\( K \\\\) do\\n\\nobject motion \\\\( \\\\hat{v}(1:K), \\\\omega(1:K) \\\\) \u2190 \\\\( r_{k,l}, \\\\hat{x}(t)_{k}, \\\\hat{x}(t-1)_{l}, p(\\\\phi(t))_{k}, p(\\\\phi(t-1))_{l}, l = 1:K+1 \\\\)\\n\\nend for\\n\\n\\\\( I'(t+1) \\\\) warp \u2190 \\\\( Warp(I(t), \\\\text{optical flow} 1:K+1) \\\\)\\n\\n\\\\( I'(t+1) \\\\) imagine \u2190 \\\\( g(\\\\theta(I(t) \\\\odot \\\\pi(t-1) 1:K+1, \\\\log(D(t) \\\\odot \\\\pi(t-1) 1:K+1))) \\\\), \\\\( v_{\\\\text{obs}}, \\\\omega_{\\\\text{obs}}, \\\\hat{v}(1:K+1), \\\\hat{x}(1:K) \\\\)\\n\\nfinal image prediction:\\n\\n\\\\( I'(t+1) \\\\) \u2190 \\\\( I'(t+1) \\\\) warp, \\\\( I'(t+1) \\\\) imagine, warping weights\\n\\nupdate parameters:\\n\\n\\\\( \\\\theta \\\\) \u2190 \\\\( \\\\theta - \\\\gamma \\\\nabla_{\\\\theta} \\\\| I'(t+1) - I(t+1) \\\\|_{2} + \\\\text{regularization loss} \\\\)\\n\\n**A.2 PERFORMANCE ON DEPTH PERCEPTION**\\n\\nWe demonstrate a few example images and the inferred depth. Our network can capture the global 3D structure of the scene, although details on object surfaces are still missing. Because background occurs in every training sample, the network appears to bias the depth estimation on objects towards the depth of the walls behind, as is also shown in the scatter plot.\\n\\n![Figure 4: Comparison between ground truth depth and inferred depth](image-url)\"}"}
{"id": "IsHQmuOqRAG", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We trained the three networks jointly using ADAM optimization Kingma & Ba (2014) with a learning rate of $3 \\\\times 10^{-4}$, $\\\\epsilon = 1 \\\\times 10^{-6}$ and other default settings in PyTorch, with a batch size of 24. 40 epochs were trained on the dataset. We set $\\\\lambda_{\\\\text{spatial}} = 1.0$, $\\\\lambda_{\\\\text{depth}} = 0.1$ and $L_{\\\\text{map}} = 0.005$. Images in the dataset are rendered in Unity environment and downsampled to $128 \\\\times 128$ resolution for training. Images are rendered in sequence of 7 steps each time in a room with newly selected texture and object. Camera moves with random steps and random rotation between consecutive frames. All possible triplets equally spaced by 1, 2, and 3 frames form training samples.\\n\\nThe model was implemented in PyTorch and trained on NVidia RTX 6000. We will release the code and dataset upon publication of the manuscript.\\n\\nA.4 Method\\n\\nA.4.1 Prediction by Warping\\n\\nWe first describe the prediction of part of the next image by warping the current image. Here we consider only rigid objects and the fates of all visible pixels belonging to an object. With depth $D(t) \\\\in \\\\mathbb{R}^{w \\\\times h}$ of all pixels in a view inferred by the Depth Perception network based on visual features in the image $I(t)$, the 3D location of a pixel at any coordinate $(i,j)$ in the image, where $|i| \\\\leq w - 1$, $|j| \\\\leq h - 1$, can be determined given the focal length $d$ of the camera as\\n\\n$$\\\\hat{m}(t)(i,j) = D(t)(i,j) \\\\sqrt{i^2 + j^2 + d^2} \\\\cdot \\\\begin{bmatrix} i \\\\\\\\ d \\\\\\\\ j \\\\end{bmatrix}.$$ Here, we take the coordinate of the center of an image as $(0,0)$. On the other hand, with the inferred $\\\\hat{x}(t)_{k}$ and $\\\\hat{x}(t-1)_{k}$, the current and previous locations of the object $k$ that the pixel $(i,j)$ belongs to, from $I(t)$ and $I(t-1)$ respectively, we can estimate the instantaneous velocity of the object $\\\\hat{v}(t)_{k} = \\\\hat{x}(t)_{k} - \\\\hat{x}(t-1)_{k}$. Similarly, with the inferred the current and previous pose probabilities of the object, $p(\\\\phi(t)_{k})$ and $p(\\\\phi(t-1)_{k})$, we can obtain the likelihood of its angular velocity\\n\\n$$p(\\\\phi(t)_{k}, \\\\phi(t-1)_{k} | \\\\omega(t)_{k}) \\\\propto \\\\sum_{k_{1}, k_{2}} \\\\gamma_{1}, \\\\gamma_{2} \\\\cdot p(\\\\phi(t)_{k} = \\\\gamma_{1}) \\\\cdot p(\\\\phi(t-1)_{k} = \\\\gamma_{2}) \\\\cdot (\\\\omega(t)_{k} - 2\\\\pi, \\\\omega(t)_{k}, \\\\omega(t)_{k} + 2\\\\pi).$$\\n\\nBy additionally imposing a prior distribution (we use V on $\\\\mathbb{M}$) distribution over $\\\\omega(t)_{k}$ that favors slow rotation, we can obtain the posterior distribution of the object\u2019s angular velocity $p(\\\\omega(t+1)_{k} | \\\\phi(t)_{k}, \\\\phi(t-1)_{k})$, and eventually the posterior distribution of the object\u2019s next pose.\\n\\nAssuming a pixel $(i,j)$ belongs to object $k$, using the estimated motion information $\\\\hat{v}(t)_{k}$ and $p(\\\\omega(t)_{k} | \\\\phi(t)_{k}, \\\\phi(t-1)_{k})$ of the object, together with the current location and pose of the object and the current 3D location $\\\\hat{m}(t)(i,j)$ of the pixel, we can predict the 3D location $m'(t+1)_{k},(i,j)$ of the pixel at the next moment as\\n\\n$$m'(t+1)_{k},(i,j) = M(t) - \\\\omega_{\\\\text{obs}} \\\\left[ M(t) \\\\hat{\\\\omega}_{k} (\\\\hat{m}(t)(i,j) - \\\\hat{x}(t)_{k}) + \\\\hat{x}(t)_{k} + \\\\hat{v}(t)_{k} \\\\right] - v_{\\\\text{obs}} \\\\right].$$\\n\\nwhere $M(t) - \\\\omega_{\\\\text{obs}}$ and $M(t) \\\\hat{\\\\omega}_{k}$ are rotational matrices due to the rotation of the observer and the object, respectively, and $v_{\\\\text{obs}}$ is the velocity of the observer (relative to its own reference frame at $t$). In this way, assuming objects move smoothly most of the time, if the self motion information is known, the 3D location of each visible pixel can be predicted. If a pixel belongs to the background, $\\\\omega_{K+1} = 0$ and $v_{K+1} = 0$. Given the predicted 3D location, the target coordinate $(i',j')_{k}$ and its new depth $D'(t+1)(i,j)_{k}$ can be calculated. This prediction of pixel movement allows predicting the image $I'(t+1)$ and depth $D'(t+1)$ by weighting the colors and depth of pixels predicted to land near each pixel at the discrete grid of the next frame, as explained in Sec 2.4.2.\"}"}
