{"id": "djhu4DIZZHR", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluate on $\\\\mathcal{N}AIL$-E $\\\\mathcal{N}AIL$-I $\\\\mathcal{N}AIL$\\nTrain on $\\\\emptyset$\\n\\n| Dataset  | MathQA  | NAIL | LogiQA  | ReClor | RACE | NAIL |\\n|----------|---------|------|---------|--------|------|------|\\n| Accuracy%| 39.8    | 41.2 | 37.9    | 35.2   | 35.4 | 34.6 |\\n\\nTable 7: Transfer learning results when evaluating on the validation split of $\\\\mathcal{N}AIL$, $\\\\mathcal{N}AIL$-E, $\\\\mathcal{N}AIL$-I.\\n\\nTable 8, 9. We adopted MathQA (Amini et al., 2019), and HotpotQA (Yang et al., 2018). For MathQA, we include a prediction head for multiple choice based on the RoBERTa-large model, and for HotpotQA, we include a prediction head for question answering. For MathQA, we concat the problem and annotated formula as input.\\n\\nEvaluate on $\\\\mathcal{N}AIL$ $\\\\mathcal{N}AIL$ $\\\\mathcal{N}AIL$ $\\\\mathcal{N}AIL$ $\\\\mathcal{N}AIL$\\nTrain on $\\\\emptyset$\\n\\n| Dataset  | MathQA  | NAIL | LogiQA  | ReClor | RACE | NAIL |\\n|----------|---------|------|---------|--------|------|------|\\n| Ans F1   | 69.8    | 72.6 |         |        |      |      |\\n\\nTable 8: Evaluating on MathQA after RoBERTa-large pretrained on MathQA/NAIL training set.\\n\\nTable 9: Evaluating on HotpotQA after RoBERTa-large pretrained on HotpotQA/NAIL training set.\\n\\nExperimental results show that, if we use $\\\\mathcal{N}AIL$ as extra training resource, the supervised-learning results on all of these six datasets: LogiQA, ReClor, RACE, MathQA, HotpotQA will improve. This indicates that equipping models with na\u00efve logical reasoning ability can help solve math word problems (MathQA), improve multi-hop factual reasoning skills (HotpotQA), solve various logical reasoning problems (LogiQA and ReClor), and enhance general understanding and reading comprehension capability (RACE). In the future, we will test on more known datasets, to verify that na\u00efve logical reasoning is a basic capability and is helpful for other tasks.\\n\\nAnalysis of Fine-Grained Types\\nIn Section 4, we analyze the model performance with respect to different types of na\u00efve logical reasoning (Figure 5). Figure 8 shows the accuracy against fine-grained reasoning types on the test set of $\\\\mathcal{N}AIL$, $\\\\mathcal{N}AIL$-E, $\\\\mathcal{N}AIL$-I when training on $\\\\mathcal{N}AIL$, $\\\\mathcal{N}AIL$-E, $\\\\mathcal{N}AIL$-I respectively. And Figure 7 shows the corresponding results on the validation set.\\n\\nFrom above figures we can find that language models perform well on set operation problems, while struggle on matching and ordering. We think that language models can provide good representation of set object, even if models do not really reason derived from the context.\\n\\nDiscussion About Future Directions\\nWe have some ideas for future directions for models to solve $\\\\mathcal{N}AIL$.\\n\\nThe first point is to identify deterministic information or information with low uncertainty. Generally, we need to detect the atomic information that can be used directly without reasoning, which is the key to solving the problem. Different conditions should not be considered with equal priority.\\n\\nDue to computational resource limitations, we only trained 3 epochs for experiments involved HotpotQA.\"}"}
{"id": "djhu4DIZZHR", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1: Accuracy against the reasoning types based on different training set and development set.\\n\\n| Reasoning Type | Training Set | Development Set |\\n|----------------|--------------|-----------------|\\n| Real/Fake | NAIL | NAIL-E |\\n| Ordering | 26.4 | 24.1 | 24.9 |\\n| Matching | 25.9 | 24.4 | 25.2 |\\n| Set operation | 22.4 | 23.3 | 23.6 |\\n\\n- **Trained on NAIL, Evaluated on NAIL**\\n- **Trained on NAIL, Evaluated on NAIL-E**\\n- **Trained on NAIL-E, Evaluated on NAIL**\\n- **Trained on NAIL-E, Evaluated on NAIL-E**\\n- **Trained on NAIL-I, Evaluated on NAIL**\\n- **Trained on NAIL-I, Evaluated on NAIL-E**\\n- **Trained on NAIL-I, Evaluated on NAIL-I**\\n\\n**Figure 7**: Accuracy against the reasoning types based on different training set and development set.\\n\\n- i.e., the more certain a condition we start from, the fewer possible worlds that the condition yields.\\n- Specifically, for real/fake mixture problems, we also need to promptly identify two atomic statements that yield a contradictory, that is, the two statements cannot be both true (there must be one false) or both false (there must be one true), which is often the breakthrough in solving real/fake mixture problems.\\n- The second is to detect informative subjects/objects. Generally, the more frequently a subject/object is mentioned in context, the more relevant information it carries. Tables and bi/tri-partite diagrams can help to clarify the correspondence between given subjects and objects. For example, in the case of Figure 1, tables play an effective role in solving the problems. And as for the third case in Figure 3, which is categorized as a matching problem, a tri-partite diagram could help, that is, three disjoint and independent sets $U, V$ and $W$ represent $\\{A, B, C\\}$, $\\{Beijing, Nanjing, Chengdu\\}$, and $\\{a doctor, an actor, and a teacher\\}$ respectively, an edge connecting a vertex in one set to one in another set denotes the \u201cis a\u201d predicate.\\n- The third direction is allowing models to better utilize elimination. On the one hand, models can perform forward elimination, i.e., according to the context, each time we draw a definite conclusion, we can retain options that are logically consistent and exclude those that do not fit the derived conclusion. On the other hand, models can also perform backward elimination. For example, when sometimes it is not easy to draw exact inferences directly from the context, then we can substitute the options into the context. If substituting an option creates a contradiction within the context, then the option should be excluded. Forward and backward elimination facilitate the model in arriving at the correct answer.\"}"}
{"id": "djhu4DIZZHR", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Accuracy against the reasoning types based on different training set and testing set.\"}"}
{"id": "djhu4DIZZHR", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022 between triples. The naive logical reasoning is the process of reasoning from these statements to reach a logical conclusion.\\n\\nIn this work, we explore na\u00efve logical reasoning in the form of reading comprehension. A typical example and detailed reasoning process is shown in Figure 1. Similar to the format of multiple-choice reading comprehension, it contains a context, a query and four options with only one correct answer. To solve the problem, the model needs to understand the logical connections between the subjects, predicates and objects, and then derive a valid option.\\n\\n3 NAIL: DATA COLLECTION AND ANALYSIS\\n\\nNAIL is a carefully designed benchmark for na\u00efve logical reasoning similar to the format of multiple-choice reading comprehension. Inspired by the datasets extracted from standardized examinations (Lai et al., 2017; Clark et al., 2018; Liu et al., 2020), we first collect a small amount of examples from examinations, denoted as NAIL-E (Section 3.1). Then we propose to imitate examples of NAIL-E to collect more data, denoted as NAIL-I (Section 3.2). Finally we provide a detailed analysis of the proposed NAIL (Section 3.3).\\n\\n3.1 COLLECTION FROM EXAMINATION\\n\\nWe searched for such examples widely from two different types of public examinations: Chinese National Civil Servants Examination (CNCSE) and Law School Admission Test (LSAT). CNCSE is a once-a-year competitive examination in China, and there are overall 120-140 examples per exam per year. But only 1-4 examples belong to the scope of na\u00efve logical reasoning, which is also the most difficult type of problems for candidates within the given 120 minutes. And the LSAT is a standardized test for prospective law school candidates in the United States, Canada, and a growing number of other countries. Logical Reasoning is a multiple-choice section of LSAT, containing 24-26 questions under the limitation of 35 minutes, where about 2-4 problems fall into na\u00efve logical reasoning category.\\n\\nWe artificially selected examples that belong to the na\u00efve logical reasoning category from the above two examinations. Finally we obtained 488 examples from the last 25 years of CNCSE and LSAT in the last 30 years, denoted as NAIL-E. These two exams are from countries with different native languages. The former is expressed in Chinese, and the latter is expressed in English. And note that there are slight difference in language style between them. And for diversity and fairness, in later step we get all examples in both Chinese and English through translation.\\n\\n3.2 COLLECTION FROM IMITATION\\n\\nAfter collecting a small amount of NAIL-E, we expect to expand the number of the dataset. Designing examples from scratch requires a huge effort from human experts. One simple solution is data augmentation, which artificially scales up data by creating modified data from existing data, such as word/sentence shuffling, word replacement and syntactic variation. However, the data augmented in this way is highly correlated with the original data, and the model easily captures these semantic surface correlations, which makes it limited to train and evaluate logical reasoning capability. To alleviate this limitation, we propose to imitate examples of NAIL-E, to create more examples with a diverse semantic surface while keeping the underlying logic of the original example. Furthermore, in the process of human imitation, we design strict strategies to control the quality of imitation.\\n\\nImitation Example\\n\\nSee Figure 2, the context of an example from NAIL-E consists of 6 sentences and a query (split by blank lines), each of which can be represented as a logical template (see \\\"backbone template\\\"). The example focus on the description of a scenario: picking Prince Charming ($m_1$), which involves five entities: Li Na ($s_1$), Wang Wei ($p_1$), Wu Gang ($p_2$), Li Qiang ($p_3$), Liu Dawei ($p_4$), and three noun/adjective properties: tall ($a_1$), handsome ($a_2$), a PhD ($a_3$). An accepted imitation needs to keep original underlying logic but have semantically different groundings, i.e., to describe a completely different scenario. Imitation 1 and 2\\n\\n1 The term naive refers to the fact that this logical reasoning process of human in this task is spontaneous, intuitive and unsystematic.\\n\\n2 https://www.lsac.org/lsat/taking-lsat/test-format/logical-reasoning\"}"}
{"id": "djhu4DIZZHR", "page_num": 4, "content": "{\"primary_language\":\"zh\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u5fc3\u4e2d\u7684\u7279\u5f81: \u2460 \u2461 \u2462\u3002\\n\u674e\u5a1c\u5fc3\u4e2d\u7684\u767d\u9a6c\u738b\u5b50\u6709\u5982\u4e0b\u7279\u5f81:\u9ad8\u4e2a\u5b50\u3001\u76f8\u8c8c\u82f1\u4fca\u3001\u535a\u58eb\u3002\\n\\n\u8981\u6c42\u7684\u5168\u90e8\u6761\u4ef6\u3002\"}"}
{"id": "djhu4DIZZHR", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"problem with those scenario-related while keeping invariant logic, and finally smoothing the new sentence with some transition words if necessary. For each original example in NAIL-E, we expect at least 20 imitations (except for extremely difficult cases). Overall we use 813 paid work hours in total to build the NAIL-I.\\n\\nImitation Quality Control\\nWe adopt following strategies to ensure the quality of imitations:\\n\\n1. As mentioned above, we conducted a trial phase before the official imitation phase. In this phase, we asked them to imitate a small number of problems. Although we do not necessarily need them to write the backbone template, we will check the logic and give feedback to help them understand the task. This process was iterated for three rounds. Only those who passed the trial phase could participate in the official imitation.\\n\\n2. During the official imitation, we set up an online chat room to communicate with employees and answer their questions timely.\\n\\n3. To embrace semantic diversity, each original example is shown to at least 5 people, that is, one can only conduct 4 imitations based on one original example. People who are assigned to the same example imitate independently without interference from each other, to ensure varied inspiration for imitation.\\n\\n4. To ensure logic invariance, we adopt a double-checking strategy:\\n   - Cross Checking: Everyday, for each employee, we sample 5 imitations from all of his/her daily imitations. And the sampled imitation is assigned to other 2 employees for cross-checking. The imitation will be qualified only if they both approved, and the criterion for approval is that the imitation share the same logics with the original example. If any one of the 5 imitations produced by one employee fail, then all imitations of that employee for that day will be returned to re-check and repair.\\n   - Post Checking: To further ensure that the underlying logic do not deviate during imitation, we introduce another team of experts to solve the imitative examples. The team consists of 20 experienced experts. 10 of them speaks Chinese as native language and have passed CNCSE, while the other 10 speaks English and have passed LSAT.\\n\\n5. Each imitative example was presented to 3 experts randomly, who are allowed to select one and only choice from \\\"A\\\", \\\"B\\\", \\\"C\\\", \\\"D\\\", otherwise, \\\"UNABLE TO ANSWER\\\" if bugs exist in the example, causing no correct choice or multiple correct choices. As long as one of the 3 experts pointed out \\\"UNABLE TO ANSWER\\\", then the imitator of this problem should recheck the logic, until each of these 3 experts could give a choice from \\\"A\\\", \\\"B\\\", \\\"C\\\", \\\"D\\\". Note that in the post checking process, we broke up the imitations together and shuffle randomly, otherwise if a person is faced with imitations from same original example, he/she is prone to give a shortcut option with speculation.\\n\\nTranslation Quality Control\\nAfter collecting high-quality mono-collections, we first adopted Google Translation to translate Chinese/English collections into another language, and then employed 10 professional bilingual experts in Chinese and English for manual correction. Bilingual experts were asked to pay attention to logic-invariance and faithfulness during translation. Next, to ensure translation quality, we also adopted the post checking strategy. That is, we asked the 20 human experts mentioned above to solve the translated examples. Each translated example was presented to 3 experts randomly. Since human experts excel in solving na\u00a8\u0131ve logical reasoning problems, (i.e. achieve 100% accuracy on mono-collections), if any expert made a mistake on a translated sample or pointed out \\\"UNABLE TO ANSWER\\\", the translated instance is sent back to the bilingual experts for revision. Finally, we asked 50 native speakers to read through all paragraphs of the translation parts in NAIL and mark \\\"0\\\"/\\\"1\\\" for each, where \\\"1\\\" stands for a translated sample is idiomatic, and \\\"0\\\" otherwise. Then for all samples marked with \\\"0\\\" (about 20%), the bilingual experts and native speakers will work together to polish them and conform to the target language norms.\\n\\nHuman Evaluation\\nAs mentioned above, an imitation is finally regarded as qualified only if the sampled 3 experts could all solve the example. For any original example in NAIL-E, we also ask 3 experts in the team to solve it. Since the gold answers to examples in NAIL-E are provided in public by the examination committee, and corresponding imitations in NAIL-I share the same answer with the original example. Therefore, we calculated the mean accuracy of these three submissions on the overall NAIL, denoted as the performance of human experts. To better demonstrate the 5\\n\\nExperts who are native English speakers will be assigned English problems and experts who are native Chinese speakers will be assigned Chinese problems.\"}"}
{"id": "djhu4DIZZHR", "page_num": 6, "content": "{\"primary_language\":\"zh\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u67d0\u7701\u4e3e\u884c\u201c\u6587\u660e\u57ce\u5e02\u201d\u8bc4\u6bd4\u30024\u4f4d\u8bc4\u59d4\u5bf9\u5927\u5bb6\u666e\u904d\u770b\u597d\u7684A\u3001B\u3001C\u7b493\u57ce\u5e02\u83b7\u5f97\u201c\u6587\u660e\u57ce\u5e02\u201d\u79f0\u53f7\u7684\u53ef\u80fd\u6027\u8fdb\u884c\u4e86\u5206\u6790\u9884\u6d4b\u3002\u8bc4\u59d4\u7532\u8bf4\uff1a\u201c\u8981\u4e48A\u5e02\u80fd\u83b7\u5f97\uff0c\u8981\u4e48C\u5e02\u80fd\u83b7\u5f97\u3002\u201d\u8bc4\u59d4\u4e59\u8bf4\uff1a\u201c\u5982\u679cA\u5e02\u4e0eC\u5e02\u80fd\u83b7\u5f97\uff0c\u5219B\u5e02\u4e5f\u80fd\u83b7\u5f97\u3002\u201d\u8bc4\u59d4\u4e19\u8bf4\uff1a\u201c\u53ea\u6709\u4e00\u4f4d\u8bc4\u59d4\u9884\u6d4b\u6210\u7acb\uff0c\u800c\u6709\u4e00\u4f4d\u8bc4\u59d4\u9884\u6d4b\u4e0d\u53ef\u80fd\u6210\u7acb\u3002\u6240\u4ee5\uff0cB\u5e02\u4e0d\u53ef\u80fd\u83b7\u5f97\u6216A\u5e02C\u5e02\u80fd\u83b7\u5f97\u3002\u201d\u8bc4\u59d4\u4e01\u8bf4\uff1a\u201cB\u5e02\u80fd\u83b7\u5f97\u65f6\uff0cA\u5e02\u80fd\u83b7\u5f97\uff0cC\u5e02\u80fd\u83b7\u5f97\u3002\u201d\u7531\u6b64\u53ef\u4ee5\u63a8\u77e5\uff1a\\n\\nA. \u5982\u679cB\u5e02\u80fd\u83b7\u5f97\uff0c\u90a3\u4e48A\u5e02\u80fd\u83b7\u5f97\uff0cC\u5e02\u80fd\u83b7\u5f97\u3002\\nB. A\u5e02\u80fd\u83b7\u5f97\uff0cC\u5e02\u80fd\u83b7\u5f97\u3002\\nC. B\u5e02\u80fd\u83b7\u5f97\uff0cA\u5e02\u53ef\u80fd\u80fd\u83b7\u5f97\uff0cC\u5e02\u80fd\u83b7\u5f97\u3002\\nD. A\u5e02\u80fd\u83b7\u5f97\uff0cC\u5e02\u80fd\u83b7\u5f97\uff0cB\u5e02\u53ef\u80fd\u80fd\u83b7\u5f97\u3002\\n\\n\u6839\u636e\u4ee5\u4e0a\u4fe1\u606f\uff0c\u53ef\u4ee5\u63a8\u77e5\uff1a\\n\\nB. A\u5e02\u80fd\u83b7\u5f97\uff0cC\u5e02\u80fd\u83b7\u5f97\u3002\"}"}
{"id": "djhu4DIZZHR", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning, 2020.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\\n\\nJohn McCarthy et al. Programs with common sense. RLE and MIT computation center, 1960.\\n\\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.\\n\\nAllen Newell and Herbert Simon. The logic theory machine\u2013a complex information processing system. IRE Transactions on information theory, 2(3):61\u201379, 1956.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\\n\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383\u20132392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.org/anthology/D16-1264.\\n\\nMatthew Richardson, Christopher JC Burges, and Erin Renshaw. Mctest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 193\u2013203, 2013.\\n\\nRachel Rudinger, Vered Shwartz, Jena D Hwang, Chandra Bhagavatula, Maxwell Forbes, Ronan Le Bras, Noah A Smith, and Yejin Choi. Thinking like a skeptic: Defeasible inference in natural language. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pp. 4661\u20134675, 2020.\\n\\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie. Dream: A challenge data set and models for dialogue-based reading comprehension. Transactions of the Association for Computational Linguistics, 7:217\u2013231, 2019.\\n\\nAlon Talmor and Jonathan Berant. The web as a knowledge-base for answering complex questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 641\u2013651, 2018.\\n\\nSiyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, and Nan Duan. From lsat: The progress and challenges of complex reasoning. arXiv preprint arXiv:2108.00648, 2021.\\n\\nJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association for Computational Linguistics, 6:287\u2013302, 2018.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38\u201345, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6.\\n\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369\u20132380, 2018.\"}"}
{"id": "djhu4DIZZHR", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. Reclor: A reading comprehension dataset requiring logical reasoning. In International Conference on Learning Representations (ICLR), April 2020.\\n\\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885, 2018.\"}"}
{"id": "djhu4DIZZHR", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rule-based Methods\\nFollowing LogiQA (Liu et al., 2020), we adopt two simple rule-based methods based on text matching. Specifically, word matching is to measure the degree of unigram overlap between the candidate answer and the given paragraph-query pair; sliding window takes into account the n-gram when calculating the matching score.\\n\\nNeural-based Methods\\nPre-trained neural models, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), have achieved impressive performance on reading comprehension. The input to the pre-trained model is the concatenation of the paragraph, the query and the candidate answer, separated by [SEP] tokens, denoted as: [CLS], paragraph, [SEP], query, [SEP], answer. After the encoding of the pre-trained model, the representation of [CLS] token followed by an multiple layer perceptron (MLP) is used for scoring.\\n\\nWe re-implement the rule-based methods following LogiQA (Liu et al., 2020). For pre-trained models, we modify the code of Transformers of HuggingFace (Wolf et al., 2020) to implement them on NAIL. We take the off-the-shelf model BERT and RoBERTa for NAIL, and Chinese BERT and Chinese RoBERTa (Cui et al., 2019) for Chinese NAIL. All hyper-parameters are selected by the model performance on the development sets.\\n\\nWe also try seq2seq-based pre-trained models (i.e. T5 (Raffel et al., 2019)) and well-designed models for other related tasks (i.e. DAGN (Huang et al., 2021) for LogiQA and ReClor, HGN (Fang et al., 2020) for HotpotQA (Yang et al., 2018), and QDGAT (Chen et al., 2020) for DROP (Dua et al., 2019)) on our English version of NAIL.\\n\\nA problem is regarded as four samples to T5 during training. The input of a sample is: context+query+one of the four choices. And we let the model generate the label for the sample: if the choice is the correct answer, then generate \\\"True\\\", otherwise generate \\\"False\\\". And during inference, for a problem, we choose the option with the highest probability of generating \\\"True\\\" among the four options as the prediction. We set input max len = 512 and finetuned the off-the-shelf T5-base model on the training set of NAIL, results are shown in Table 4.\\n\\nLogiQA and ReClor are two logical benchmarks introduced in the text of the paper to test models\u2019 various reasoning abilities. HotpotQA is a multiple-choice QA dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to test models\u2019 multi-hop factual reasoning ability. DROP is a 96k-question benchmark to test models\u2019 numerical discrete reasoning ability over paragraphs (such as addition, counting, or sorting).\\n\\nWe choose three state-of-the-art and open-sourced models designed for the above benchmarks: DAGN for LogiQA and Reclor, HGN for HotpotQA, and QDGAT for DROP respectively. We reproduced these three models with the code released by authors and NAIL as training data. Results on NAIL are shown as Table 4.\\n\\n| Model Encoder | NAIL Dev | NAIL Test |\\n|---------------|---------|----------|\\n| RoBERTa LARGE | 34.6    | 30.1     |\\n| RoBERTa BASE  | 27.6    | 26.4     |\\n| T5 BASE       | 27.1    | 26.3     |\\n| DAGN RoBERTa LARGE | 36.0 | 32.3 |\\n| HGN RoBERTa LARGE | 30.1 | 28.7 |\\n| QDGAT RoBERTa LARGE | 28.3 | 28.7 |\\n\\nTable 4: Results on NAIL (accuracy%).\\n\\n10 DAGN: https://github.com/Eleanor-H/DAGN, QDGAT: https://github.com/emnlp2020qdgat/QDGAT, HGN: https://github.com/yuwfan/HGN\"}"}
{"id": "djhu4DIZZHR", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We observe that DAGN achieves the best results on NAIL, since ReClor and LogiQA are much closer to NAIL, the well-designed discourse-aware graph network DAGN is more applicable to NAIL. However there is still very much space for models' improvement compared to humans. HGN is a hierarchical graph network for multi-hop question answering, which is designed to aggregate heterogeneous information, and QDGAT is a question directed graph attention network, which is dedicated to identifying numbers and the computation between them, which are both of minimal help to NAIL. T5 is a very powerful generative model, but it seems that T5 lacks the na\u00efve reasoning capability as well. Due to computational resource limitation, we did not try a T5 model with larger number of parameters.\\n\\nTo investigate the effect of NAIL, NAIL-E and NAIL-I, we train a RoBERTa LARGE model on NAIL, NAIL-E and NAIL-I separately and evaluate on the corresponding set of all tests (Table 5). When trained on NAIL-I and evaluated on NAIL-E, the model achieves the best accuracy 38.5%, showing that human imitations can significantly help models learn logic from natural text. When trained on NAIL-E and evaluated on NAIL-E, the model achieves the worst accuracy 22.9%. The possible reason is that the amount of data of NAIL-E is too small to train a deep model. Corresponding results on the validation set are shown in Table 6.\\n\\n| Trained on | Evaluated on | NAIL | NAIL-E | NAIL-I |\\n|------------|--------------|------|--------|--------|\\n| NAIL       | 30.1         | 26.4 | 33.9   |\\n| NAIL-E     | 37.4         | 22.9 | 38.5   |\\n| NAIL-I     | 29.7         | 26.6 | 33.7   |\\n\\nTable 5: Performance of RoBERTa on different training sets and test sets.\\n\\n| Trained on | Evaluated on | NAIL | NAIL-E | NAIL-I |\\n|------------|--------------|------|--------|--------|\\n| NAIL       | 34.6         | 24.9 | 32.0   |\\n| NAIL-E     | 43.3         | 37.4 | 33.0   |\\n| NAIL-I     | 34.2         | 24.3 | 31.9   |\\n\\nTable 6: Performance of RoBERTa on different training sets and validation sets.\\n\\nWe can also consider the following scenario: What are the results if pre-trained model is first trained on existing reading comprehension datasets, and then fine-tuned on NAIL?\\n\\nTable 3 shows the results on the test set of NAIL, NAIL-E, NAIL-I where LogiQA, ReClor and RACE are adopted. And here in Table 7 we give the results on the validation splits of NAIL, NAIL-E, NAIL-I.\\n\\n| Trained on | Evaluated on | NAIL | NAIL-E | NAIL-I |\\n|------------|--------------|------|--------|--------|\\n| NAIL       | 34.6         | 24.9 | 32.0   |\\n| NAIL-E     | 43.3         | 37.4 | 33.0   |\\n| NAIL-I     | 34.2         | 24.3 | 31.9   |\\n\\nTable 7: Performance of models fine-tuned on NAIL on the test set of LogiQA, ReClor and RACE.\"}"}
{"id": "djhu4DIZZHR", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We adopt the accuracy as the evaluation metric. Simple rule-based methods and strong neural-based methods are included as our baseline. Rule-based methods involve text matching and sliding window. Neural-based methods include BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019).\\n\\nDetailed descriptions (e.g., hyper-parameters) of the baseline models and the results on the validation set are listed in the Appendix A.\\n\\n### 4.1 Results and Analysis\\n\\n| Model         | Dev Test | Dev Test | Dev Test | Dev Test |\\n|---------------|----------|----------|----------|----------|\\n| Random (C, Q, A) | 25.0     | 25.0     | 25.0     | 25.0     |\\n| Word Matching (Q, A) | 23.1     | 24.6     | 23.3     | 24.9     |\\n| (C, Q, A)      | 21.6     | 25.8     | 22.2     | 24.2     |\\n| Sliding Window (Q, A) | 23.8     | 24.2     | 24.1     | 24.7     |\\n| (C, Q, A)      | 21.9     | 22.1     | 21.6     | 22.5     |\\n| BERT LARGE (A) | 26.8     | 26.7     | 27.2     | 26.7     |\\n| (Q, A)         | 27.3     | 27.1     | 27.4     | 26.8     |\\n| (C, Q, A)      | 29.0     | 29.4     | 29.3     | 27.7     |\\n| RoBERTa LARGE (A) | 27.3     | 26.5     | 29.4     | 27.7     |\\n| (Q, A)         | 27.8     | 27.9     | 32.6     | 30.5     |\\n| (C, Q, A)      | 34.6     | 30.1     | 37.3     | 36.2     |\\n| Human baseline (C, Q, A) | 70.1     | 71.3     | 75.5     | 76.4     |\\n| Human expert   | 100.0    | 100.0    | 100.0    | 100.0    |\\n\\nTable 2: Main results on NAIL (accuracy%). The column Input means whether to input context (C), query (Q) and answer options (A).\\n\\nThe performance of all baselines on the NAIL is presented in Table 2. In particular, the human baseline is 71.3 and 76.4 on the test set of NAIL and Chinese NAIL separately, while the human expert is 100.0 since ambiguous examples are not included in the dataset. In comparison, all of the baselines perform much worse than humans, demonstrating that these methods are very limited in naive logical reasoning. In addition, the results are relatively similar on the English and Chinese datasets. The rule-based approaches achieve an accuracy of 25.8 and 24.2, which is similar to random guess, indicating that word correlation cannot be used to help improve performance. Pre-trained models have relatively good performance with about 4-10 points improvement compared to the rule-based approaches, showing that pre-trained models have a certain degree of commonsense and logical reasoning capabilities (Huang et al., 2019b). However, the best result by RoBERTa LARGE is 36.2 on the testing data of Chinese NAIL, which is still far below the human performance. This indicates that the knowledge in the pre-trained model is quite weak in logical reasoning.\\n\\n### Different Input Settings\\n\\nWe conduct experiments with different input settings. The setting of questions and answer options (Q, A) does not lead to significant improvements compared to the input setting of only answer options (A). One likely reason is that the queries usually do not provide much information, e.g., According to this, it can be deduced that? Further adding context yields a noticeable boost, showing that the context is highly informative.\\n\\n### Transfer Learning\\n\\nWe conduct a set of transfer learning experiments to understand the degree of overlap in terms of necessary knowledge for solving problems in our dataset and existing datasets.\\n\\nWhat are the results if pre-trained model is first trained on existing reading comprehension datasets, and then fine-tuned on NAIL? Table 3 shows the results where LogiQA, ReClor and RACE are adopted.\\n\\nOverall, we observe that when LogiQA, ReClor or RACE is regarded as extra training resource for RoBERTa, the performance on NAIL will increase (30.1 \u2192 34.5/33.4/31.0), since models can learn some degree of general reasoning ability when learning other comprehension tasks. While interestingly, if models are trained only on LogiQA, ReClor, or RACE, and then zero-shot to directly test on NAIL, the performance is poor and close to random guess. And as for RACE, the zero-shot performance on NAIL (22.0) is even far below that of RoBERTa (23.4). We attribute this to the...\"}"}
{"id": "djhu4DIZZHR", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Transfer learning results when evaluating on the test split of LogiQA, ReClor and RACE. \u2205 is the zero-shot performance of RoBERTa. * is a placeholder for these three datasets.\\n\\nFigure 5: Accuracy against reasoning types evaluated on different testing sets.\\n\\nTable 3: Transfer learning results when evaluating on the test split of NAIL, NAIL-E, NAIL-I. RACE \u2192 NAIL denotes finetuned on RACE first and then finetuned on NAIL.\\n\\nWhat are the results if NAIL is used as extra training resource for existing reading comprehension tasks? Figure 4 shows answers to this question. Generally, we can draw the conclusion that using NAIL as a pre-training step can significantly improve the supervised-learning performance for other tasks, such as LogiQA (35.3 \u2192 36.9 for test), ReClor (62.6 \u2192 64.4), and RACE (83.2 \u2192 85.2). This indicates that NAIL can bring na\u00efve logical reasoning ability to the model, which is a basic reasoning ability and can be reflected into other comprehension tasks. An interesting thing is that for zero-shot evaluating on RACE, NAIL seems to have a side effect in the pretraining process (22.6 < 27.8). This is because of a similar reason as mentioned above, that NAIL and RACE consist of completely different categories of examples. Results of more datasets are shown in Appendix D.2.\\n\\nFine-grained Types\\n\\nWe further analyze the model performance with respect to different types of na\u00efve logical reasoning (Figure 5). We find that language models perform well on set operation problems, while struggle on matching and ordering. We think that language models can provide good representation of set object, even if models do not really reason derived from the context.\\n\\nError Analysis\\n\\nWe further perform detailed analysis of human-baseline errors and model errors, while from Table 2, we can observe that human baseline on NAIL (around 70%) is much lower than that of human expert (100%), and RoBERTa performs much worse (above 30%). We measure the accuracy against several factors on the test set of NAIL: number of sentences in the backbone template of the context, number of possible worlds, and context length. See Figure 6(a), as the number of sentences in the backbone template decreases, the accuracy rate of human baseline increases significantly, and when reduced to 2 and below, human baseline does...\"}"}
{"id": "djhu4DIZZHR", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Number of sentences in the backbone template\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n\\nNumber of samples in test set\\n100.0\\n98.8\\n92.3\\n78.3\\n59.8\\n55.4\\n23.8\\n60.3\\n36.9\\n30.1\\n27.4\\n25.2\\n31.2\\n\\nhuman baseline: wrong\\nhuman baseline: correct\\nRoBERTa-large: wrong\\nRoBERTa-large: correct\\n\\n#917\\n[0,10)\\n#564\\n[10,20)\\n#357\\n[20,30)\\n#21\\n[30,40)\\n#42\\n[40,50)\\n#42\\n[50,60)\\n#103\\n[60,70)\\n#8\\n\\nNumber of possible worlds in context\\n0\\n20\\n40\\n60\\n80\\n100\\n\\nAccuracy\\nhuman baseline\\nRoBERTa-large model\\n\\n0 50 100 150 200 250 300\\nContext length\\n0\\n5\\n10\\n15\\n20\\n\\nNumber of mistakes\\nhuman baseline\\nRoBERTa-large model\\n\\nFigure 6: (a). X-axis: the number of sentences in the backbone template. The height of bars: Number of samples in the test set grouped by X-axis. The number in bars: The accuracy against X-axis. (b). Accuracy of human baseline and RoBERTa against the number of possible worlds in the context. (c). Number of mistakes made by human baseline and RoBERTa model against various context length.\\n\\nNot make any mistakes, while performance of RoBERTa does not show any trends, even when the number equals to 1, the model still makes mistakes in a large percentage of cases. See Figure 6(b), as the number of possible worlds in the context increases, the accuracy of human baseline tends to decrease significantly. This is because the more possible worlds that need to be considered, the more judgments humans need to perform, and humans tend to overlook certain conflicts, which leads to wrong decisions for problems. However there is no significant correlation between the performance of the model and number of possible worlds. This indicates that the model does not really filter and judge the possible worlds. See Figure 6(c), the model makes mistakes at a variety of context lengths, while humans perform perfectly for problems with context length \\\\( \\\\leq 73 \\\\).\\n\\nRelated Work\\nThere are abundant datasets in reading comprehension, which could facilitate the development of the field. MCTest (Richardson et al., 2013) is a multiple-choice reading comprehension dataset that contains 500 fictional stories and 2k questions. Rajpurkar et al. (2016) proposes the first large-scale reading comprehension dataset SQuAD (100k+ questions), where the answer to each question is a span of text from the passage. Recently more datasets requiring more complicated reasoning types are introduced, such as multi-document (Joshi et al., 2017; Dunn et al., 2017), multi-hop reasoning (Yang et al., 2018; Welbl et al., 2018; Talmor & Berant, 2018), numerical discrete reasoning (Dua et al., 2019) and commonsense reasoning (Mihaylov et al., 2018; Zhang et al., 2018; Huang et al., 2019a). However, these datasets cannot test the logical reasoning ability of the models. To fill this gap, LogiQA (Liu et al., 2020), ReClor (Yu et al., 2020) and LR-LSAT (Wang et al., 2021) was proposed. LogiQA is collected from National Civil Servants Examination. ReClor and LR-LSAT are extracted from Law School Admission Test. These datasets require logical reasoning to answer the questions. However, from human experience, there are different forms of reasoning strategies in answering different logical questions. We believe that completely different categories of logical reasoning deserve to be explored separately (Rudinger et al., 2020). Different from these datasets, we inductively define a typical class of logical reasoning, named na\u00efve logical reasoning, and then create a new benchmark targeting the task, named NAIL. Compared with LogiQA, Reclor and LR-LSAT, NAIL focuses on the more fine-grained logical reasoning type (na\u00efve logical reasoning).\\n\\nThere have been many datasets extracted from human examinations, such as LogiQA and ReClor mentioned above. Besides, RACE dataset (Lai et al., 2017) is collected from English exams for middle and high school Chinese students. ARC dataset (Clark et al., 2018) consists of 7,787 science exam questions drawn from a variety of sources. DREAM (Sun et al., 2019) is dialogue-based multiple-choice reading comprehension dataset collected from English as a Foreign Language examinations which contains 10,197 questions for 6,444 dialogues.\\n\\nConclusion\\nWe introduce a more fine-grained logical reasoning, na\u00efve logical reasoning, and we propose a new large-scale benchmark, NAIL, aiming to help models learn and evaluate na\u00efve logical reasoning capability. NAIL is sourced from standardized exams and human imitation. Preliminary results show that there is still a long way to go to equip deep models with true logical reasoning capability.\"}"}
{"id": "djhu4DIZZHR", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2357\u20132367, 2019.\\n\\nKunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan Qi, and Wei Chu. Question directed graph attention network for numerical reasoning over text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6759\u20136768, 2020.\\n\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.\\n\\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. Pre-training with whole word masking for chinese bert. arXiv preprint arXiv:1906.08101, 2019.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, 2019.\\n\\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2368\u20132378, 2019.\\n\\nMatthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179, 2017.\\n\\nYuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, and Jingjing Liu. Hierarchical graph network for multi-hop question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8823\u20138838, 2020.\\n\\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2391\u20132401, Hong Kong, China, November 2019a. Association for Computational Linguistics. doi: 10.18653/v1/D19-1243. URL https://www.aclweb.org/anthology/D19-1243.\\n\\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2391\u20132401, 2019b.\\n\\nYinya Huang, Meng Fang, Yu Cao, Liwei Wang, and Xiaodan Liang. Dagn: Discourse-aware graph network for logical reasoning. arXiv preprint arXiv:2103.14349, 2021.\\n\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601\u20131611, 2017.\\n\\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 785\u2013794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https://www.aclweb.org/anthology/D17-1082.\"}"}
{"id": "djhu4DIZZHR", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\n\\\\textbf{AIL}: A CHALLENGING BENCHMARK FOR NAIVE LOGICAL REASONING\\n\\nAnonymous authors\\n\\nPaper under double-blind review\\n\\n\\\\textbf{ABSTRACT}\\n\\nLogical reasoning over natural text is an important capability towards human level intelligence. Existing datasets are either limited and inadequate to train and evaluate logical reasoning capability (e.g., LogiQA and ReClor), or not oriented for logical reasoning (e.g., SQuAD and HotpotQA). In this paper, we focus on a specific category of logical reasoning, named na\u00efve logical reasoning, and propose a new large-scale benchmark, named NAIL, aiming to help models learn and evaluate na\u00efve logical reasoning capability. NAIL is sourced from standardized exams such as Chinese National Civil Servants Examination and Law School Admission Test. Furthermore, to collect more data, we propose to imitate examples of standardized exams rather than designing them from scratch. NAIL is available in both Chinese and English containing a total of 10,296 instances. Empirical results show that state-of-the-art neural models struggle on NAIL with very poor accuracy (the best result is 30.1\\\\% for NAIL and 36.2\\\\% for Chinese NAIL), while human experts can perform 100\\\\% accuracy. Further results indicate that human imitations can significantly help models learn na\u00efve logical reasoning ability.\\n\\n\\\\textbf{1 INTRODUCTION}\\n\\nCurrent deep models have achieved near human-level performance on many tasks in NLP (Devlin et al., 2019; Liu et al., 2019), and more often than not, superficial knowledge suffices to solve the problems. To move towards human intelligence, we need to equip the models with logical reasoning capabilities (e.g., ability to draw logical conclusions from given statements), which is also a long sought-after goal of AI (Newell & Simon, 1956; McCarthy et al., 1960). One related task is natural language inference whose goal is to assign the logical relationships (contradicted, neutral and entailment) to sentence pairs. To push the development of models in logical reasoning, the researchers have focused on more challenging reading comprehension tasks, which often require more complex reasoning as well as longer input. However, most existing reading comprehension datasets are not oriented for the logical reasoning (e.g., SQuAD and HotpotQA), with the exception of LogiQA (Liu et al., 2020), ReClor (Yu et al., 2020) and LR-LSAT (Wang et al., 2021).\\n\\nAbove datasets (LogiQA, ReClor and LR-LSAT) are limited and inadequate to train and evaluate logical reasoning capability. The reason is that all of the three datasets involve diverse types of logical reasoning, such as drawing an alternate conclusion to the argument, finding necessary/sufficient assumptions, whether statements strengthen/weaken the argument or explain/resolve the situation. Mixing multiple types of logical reasoning may pose the following challenges. a) From the perspective of human cognition, different types of logical reasoning correspond to different problem-solving ideas. But in practice we usually train a model on the whole dataset with the same idea, which makes it more limited for models to learn different logical reasoning capability. b) From the perspective of machine learning, if there are many reasoning types mixed in a dataset, then there will be less data for each reasoning type, which is inadequate to train and evaluate logical reasoning capability (demonstrated in our experiments). Furthermore, when the model does not work, it is difficult to determine which reasoning type is the bottleneck (no reasoning type annotation in the dataset), which may hinder the design of better models.\\n\\nTo tackle the challenges, we focus on a more fine-grained type of logical reasoning, named na\u00efve logical reasoning (Section 2), which is to infer the logical conclusion from statements that describe triples (subject, predicate, object) and the relationships among them. A typical example of na\u00efve logical reasoning is: Given two statements, A and B, if A entails B, then B entails A; if A contradicts B, then B contradicts A. For instance, consider the following two statements:\\n\\n- Statement 1: All dogs are mammals.\\n- Statement 2: Pluto is a dog.\\n\\nFrom these two statements, we can infer that Pluto is a mammal. This is a typical example of na\u00efve logical reasoning.\\n\\nIn this paper, we propose a new large-scale benchmark, named NAIL, to help models learn and evaluate na\u00efve logical reasoning capability. NAIL is sourced from standardized exams such as Chinese National Civil Servants Examination and Law School Admission Test. Furthermore, to collect more data, we propose to imitate examples of standardized exams rather than designing them from scratch. NAIL is available in both Chinese and English containing a total of 10,296 instances. Empirical results show that state-of-the-art neural models struggle on NAIL with very poor accuracy (the best result is 30.1\\\\% for NAIL and 36.2\\\\% for Chinese NAIL), while human experts can perform 100\\\\% accuracy. Further results indicate that human imitations can significantly help models learn na\u00efve logical reasoning ability.\\n\\nIn the next section, we define a more fine-grained type of logical reasoning, named na\u00efve logical reasoning, and propose a new large-scale benchmark, named NAIL, to help models learn and evaluate na\u00efve logical reasoning capability. NAIL is sourced from standardized exams such as Chinese National Civil Servants Examination and Law School Admission Test. Furthermore, to collect more data, we propose to imitate examples of standardized exams rather than designing them from scratch. NAIL is available in both Chinese and English containing a total of 10,296 instances. Empirical results show that state-of-the-art neural models struggle on NAIL with very poor accuracy (the best result is 30.1\\\\% for NAIL and 36.2\\\\% for Chinese NAIL), while human experts can perform 100\\\\% accuracy. Further results indicate that human imitations can significantly help models learn na\u00efve logical reasoning ability.\\n\\nIn this paper, we focus on a more fine-grained type of logical reasoning, named na\u00efve logical reasoning (Section 2), which is to infer the logical conclusion from statements that describe triples (subject, predicate, object) and the relationships among them. A typical example of na\u00efve logical reasoning is: Given two statements, A and B, if A entails B, then B entails A; if A contradicts B, then B contradicts A. For instance, consider the following two statements:\\n\\n- Statement 1: All dogs are mammals.\\n- Statement 2: Pluto is a dog.\\n\\nFrom these two statements, we can infer that Pluto is a mammal. This is a typical example of na\u00efve logical reasoning.\\n\\nIn this paper, we propose a new large-scale benchmark, named NAIL, to help models learn and evaluate na\u00efve logical reasoning capability. NAIL is sourced from standardized exams such as Chinese National Civil Servants Examination and Law School Admission Test. Furthermore, to collect more data, we propose to imitate examples of standardized exams rather than designing them from scratch. NAIL is available in both Chinese and English containing a total of 10,296 instances. Empirical results show that state-of-the-art neural models struggle on NAIL with very poor accuracy (the best result is 30.1\\\\% for NAIL and 36.2\\\\% for Chinese NAIL), while human experts can perform 100\\\\% accuracy. Further results indicate that human imitations can significantly help models learn na\u00efve logical reasoning ability.\\n\\nIn this paper, we focus on a more fine-grained type of logical reasoning, named na\u00efve logical reasoning (Section 2), which is to infer the logical conclusion from statements that describe triples (subject, predicate, object) and the relationships among them. A typical example of na\u00efve logical reasoning is: Given two statements, A and B, if A entails B, then B entails A; if A contradicts B, then B contradicts A. For instance, consider the following two statements:\\n\\n- Statement 1: All dogs are mammals.\\n- Statement 2: Pluto is a dog.\\n\\nFrom these two statements, we can infer that Pluto is a mammal. This is a typical example of na\u00efve logical reasoning.\\n\\nIn this paper, we propose a new large-scale benchmark, named NAIL, to help models learn and evaluate na\u00efve logical reasoning capability. NAIL is sourced from standardized exams such as Chinese National Civil Servants Examination and Law School Admission Test. Furthermore, to collect more data, we propose to imitate examples of standardized exams rather than designing them from scratch. NAIL is available in both Chinese and English containing a total of 10,296 instances. Empirical results show that state-of-the-art neural models struggle on NAIL with very poor accuracy (the best result is 30.1\\\\% for NAIL and 36.2\\\\% for Chinese NAIL), while human experts can perform 100\\\\% accuracy. Further results indicate that human imitations can significantly help models learn na\u00efve logical reasoning ability.\\n\\nIn this paper, we focus on a more fine-grained type of logical reasoning, named na\u00efve logical reasoning (Section 2), which is to infer the logical conclusion from statements that describe triples (subject, predicate, object) and the relationships among them. A typical example of na\u00efve logical reasoning is: Given two statements, A and B, if A entails B, then B entails A; if A contradicts B, then B contradicts A. For instance, consider the following two statements:\\n\\n- Statement 1: All dogs are mammals.\\n- Statement 2: Pluto is a dog.\\n\\nFrom these two statements, we can infer that Pluto is a mammal. This is a typical example of na\u00efve logical reasoning.\\n\\nIn this paper, we propose a new large-scale benchmark, named NAIL, to help models learn and evaluate na\u00efve logical reasoning capability. NAIL is sourced from standardized exams such as Chinese National Civil Servants Examination and Law School Admission Test. Furthermore, to collect more data, we propose to imitate examples of standardized exams rather than designing them from scratch. NAIL is available in both Chinese and English containing a total of 10,296 instances. Empirical results show that state-of-the-art neural models struggle on NAIL with very poor accuracy (the best result is 30.1\\\\% for NAIL and 36.2\\\\% for Chinese NAIL), while human experts can perform 100\\\\% accuracy. Further results indicate that human imitations can significantly help models learn na\u00efve logical reasoning ability.\\n\\nIn this paper, we focus on a more fine-grained type of logical reasoning, named na\u00efve logical reasoning (Section 2), which is to infer the logical conclusion from statements that describe triples (subject, predicate, object) and the relationships among them. A typical example of na\u00efve logical reasoning is: Given two statements, A and B, if A entails B, then B entails A; if A contradicts B, then B contradicts A. For instance, consider the following two statements:\\n\\n- Statement 1: All dogs are mammals.\\n- Statement 2: Pluto is a dog.\\n\\nFrom these two statements, we can infer that Pluto is a mammal. This is a typical example of na\u00efve logical reasoning.\\n\\nIn this paper, we propose a new large-scale benchmark, named NAIL, to help models learn and evaluate na\u00efve logical reasoning capability. NAIL is sourced from standardized exams such as Chinese National Civil Servants Examination and Law School Admission Test. Furthermore, to collect more data, we propose to imitate examples of standardized exams rather than designing them from scratch. NAIL is available in both Chinese and English containing a total of 10,296 instances. Empirical results show that state-of-the-art neural models struggle on NAIL with very poor accuracy (the best result is 30.1\\\\% for NAIL and 36.2\\\\% for Chinese NAIL), while human experts can perform 100\\\\% accuracy. Further results indicate that human imitations can significantly help models learn na\u00efve logical reasoning ability.\\n\\nIn this paper, we focus on a more fine-grained type of logical reasoning, named na\u00efve logical reasoning (Section 2), which is to infer the logical conclusion from statements that describe triples (subject, predicate, object) and the relationships among them. A typical example of na\u00efve logical reasoning is: Given two statements, A and B, if A entails B, then B entails A; if A contradicts B, then B contradicts A. For instance, consider the following two statements:\\n\\n- Statement 1: All dogs are mammals.\\n- Statement 2: Pluto is a dog.\\n\\nFrom these two statements, we can infer that Pluto is a mammal. This is a typical example of na\u00efve logical reasoning.\\n\\nIn this paper, we propose a new large-scale benchmark, named NAIL, to help models learn and evaluate na\u00efve logical reasoning capability. NAIL is sourced from standardized exams such as Chinese National Civil Servants Examination and Law School Admission Test. Furthermore, to collect more data, we propose to imitate examples of standardized exams rather than designing them from scratch. NAIL is available in both Chinese and English containing a total of 10,296 instances. Empirical results show that state-of-the-art neural models struggle on NAIL with very poor accuracy (the best result is 30.1\\\\% for NAIL and 36.2\\\\% for Chinese NAIL), while human experts can perform 100\\\\% accuracy. Further results indicate that human imitations can significantly help models learn na\u00efve logical reasoning ability.\\n\\nIn this paper, we focus on a more fine-grained type of logical reasoning, named na\u00efve logical reasoning (Section 2), which is to infer the logical conclusion from statements that describe triples (subject, predicate, object) and the relationships among them. A typical example of na\u00efve logical reasoning is: Given two statements, A and B, if A entails B, then B entails A; if A contradicts B, then B contradicts A. For instance, consider the following two statements:\\n\\n- Statement 1: All dogs are mammals.\\n- Statement 2: Pluto is a dog.\\n\\nFrom these two statements, we can infer that Pluto is a mammal. This is a typical example of na\u00efve logical reasoning.\\n\\nIn this paper, we propose a new large-scale benchmark, named NAIL, to help models learn and evaluate na\u00efve logical reasoning capability. NAIL is sourced from standardized exams such as Chinese National Civil Servants Examination and Law School Admission Test. Furthermore, to collect more data, we propose to imitate examples of standardized exams rather than designing them from scratch. NAIL is available in both Chinese and English containing a total of 10,296 instances. Empirical results show that state-of-the-art neural models struggle on NAIL with very poor accuracy (the best result is 30.1\\\\% for NAIL and 36.2\\\\% for Chinese NAIL), while human experts can perform 100\\\\% accuracy. Further results indicate that human imitations can significantly help models learn na\u00efve logical reasoning ability.\\n\\nIn this paper, we focus on a more fine-grained type of logical reasoning, named na\u00efve logical reasoning (Section 2), which is to infer the logical conclusion from statements that describe triples (subject, predicate, object) and the relationships among them. A typical example of na\u00efve logical reasoning is: Given two statements, A and B, if A entails B, then B entails A; if A contradicts B, then B contradicts A. For instance, consider the following two statements:\\n\\n- Statement 1: All dogs are mammals.\\n- Statement 2: Pluto is a dog.\\n\\nFrom these two statements, we can infer that Pluto is a mammal. This is a typical example of na\u00efve logical reasoning.\\n\\nIn this paper, we propose a new large-scale benchmark, named NAIL, to help models learn and evaluate na\u00efve logical reasoning capability. NAIL is sourced from standardized exams such as Chinese National Civil Servants Examination and Law School Admission Test. Furthermore, to collect more data, we propose to imitate examples of standardized exams rather than designing them from scratch. NAIL is available in both Chinese and English containing a total of 10,296 instances. Empirical results show that state-of-the-art neural models struggle on NAIL with very poor accuracy (the best result is 30.1\\\\% for NAIL and 36.2\\\\% for Chinese NAIL), while human experts can perform 100\\\\% accuracy. Further results indicate that human imitations can significantly help models learn na\u00efve logical reasoning ability.\\n\\nIn this paper, we focus on a more fine-grained type of logical reasoning, named na\u00efve logical reasoning (Section 2), which is to infer the logical conclusion from statements that describe triples (subject, predicate, object) and the relationships among them. A typical example of na\u00efve logical reasoning is: Given two statements, A and B, if A entails B, then B entails A; if A contradicts B, then B contradicts A. For instance, consider the following two statements:\\n\\n- Statement 1: All dogs are mammals.\\n- Statement 2: Pluto is a dog.\\n\\nFrom these two statements, we can infer that Pluto is a mammal. This is a typical example of na\u00efve logical reasoning.\\n\\nIn this paper, we propose a new large-scale benchmark, named NAIL, to help models learn and evaluate na\u00efve logical reasoning capability. NAIL is sourced from standardized exams such as Chinese National Civil Servants Examination and Law School Admission Test. Furthermore, to collect more data, we propose to imitate examples of standardized exams rather than designing them from scratch. NAIL is available in both Chinese and English containing a total of 10,296 instances. Empirical results show that state-of-the-art neural models struggle on NAIL with very poor accuracy (the best result is 30.1\\\\% for NAIL and 36.2\\\\% for Chinese NAIL), while human experts can perform 100\\\\% accuracy. Further results indicate that human imitations can significantly help models learn na\u00efve logical reasoning ability.\\n\\nIn this paper, we focus on a more fine-grained type of logical reasoning, named na\u00efve logical reasoning (Section 2), which is to infer the logical conclusion from statements that describe triples (subject, predicate, object) and the relationships among them. A typical example of na\u00efve logical reasoning is: Given two statements, A and B, if A entails B, then B entails A; if A contradicts B, then B contradicts A. For instance, consider the following two statements:\\n\\n- Statement 1: All dogs are mammals.\\n- Statement 2: Pluto is"}
{"id": "djhu4DIZZHR", "page_num": 2, "content": "{\"primary_language\":\"zh\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A, B, C, D\u4e3a\u56db\u4f4d\u6f02\u4eae\u5973\u751f\u3002\u5979\u4eec\u559c\u6b22\u7a7f\u6f02\u4eae\u8863\u670d\u3002\u67d0\u5929\uff0c\u5979\u4eec\u7a7f\u7684\u8863\u670d\u989c\u8272\u5404\u4e0d\u76f8\u540c\uff0c\u6709\u9ec4\u8272\uff0c\u7eff\u8272\uff0c\u84dd\u8272\u548c\u7ea2\u8272\u56db\u79cd\uff0c\u5728\u95ee\u5230\u5979\u4eec\u5404\u81ea\u8863\u670d\u7684\u989c\u8272\u65f6\uff0cA\u8bf4\uff1a\u201cB\u7684\u8863\u670d\u4e0d\u662f\u9ec4\u8272\u7684\u3002\u201dB\u8bf4\uff1a\u201cC\u7684\u8863\u670d\u4e0d\u662f\u7eff\u8272\u7684\u3002\u201dC\u8bf4\uff1a\u201cD\u7684\u8863\u670d\u4e0d\u662f\u84dd\u8272\u7684\u3002\u201dD\u8bf4\uff1a\u201cA\u3001B\u3001C\u4e09\u4eba\u4e2d\u6709\u4e00\u4e2a\u4eba\u7684\u8863\u670d\u662f\u7eff\u8272\u7684\uff0c\u800c\u4e14\u53ea\u6709\u8fd9\u4e2a\u4eba\u8bf4\u7684\u662f\u5b9e\u8bdd\u3002\u201d\\n\\n**Q:** \u5982\u679cD\u8bf4\u7684\u662f\u5b9e\u8bdd\uff0c\u90a3\u4e48\u4ee5\u4e0b\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\uff1f\\n\\nA. C\u7684\u8863\u670d\u662f\u7eff\u8272\u7684\uff0cD\u7684\u8863\u670d\u662f\u7ea2\u8272\u7684\\nB. A\u7684\u8863\u670d\u662f\u7eff\u8272\u7684\uff0cB\u7684\u8863\u670d\u662f\u7ea2\u8272\u7684\\nC. B\u7684\u8863\u670d\u662f\u84dd\u8272\u7684\uff0cC\u7684\u8863\u670d\u662f\u7ea2\u8272\u7684\\nD. D\u7684\u8863\u670d\u662f\u7eff\u8272\u7684\uff0cA\u7684\u8863\u670d\u662f\u7ea2\u8272\u7684\"}"}
