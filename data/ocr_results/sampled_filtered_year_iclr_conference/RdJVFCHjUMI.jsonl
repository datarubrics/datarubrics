{"id": "RdJVFCHjUMI", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A N Explanation of In-context Learning as Implicit Bayesian Inference\\n\\nSang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma\\nStanford University\\n{xie,aditir,pliang,tengyuma}@cs.stanford.edu\\n\\nAbstract\\n\\nLarge language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning.\\n\\nBeyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.\\n\\n1 Introduction\\n\\nLarge language models (LMs) such as GPT-3 (Brown et al., 2020; Lieber et al., 2021; Wang & Komatsuzaki, 2021; Radford et al., 2019) are pretrained on massive text corpora to predict the next word given previous words. They demonstrate the surprising ability to do in-context learning, where an LM \u201clearns\u201d to do a task simply by conditioning on a prompt containing input-output pairs, achieving SOTA results on LAMBADA (Paperno et al., 2016) and TriviaQA (Joshi et al., 2017) tasks (18% and 3% over previous SOTA (Brown et al., 2020)). For example, consider the task of predicting nationalities from names. A prompt (Figure 1) is constructed by concatenating independent \u201ctraining\u201d examples (e.g., \u201cAlbert Einstein was German\u201d) followed by a \u201ctest example\u201d (\u201cMarie Curie was\u201d). Conditioning on this prompt, GPT-3 places the largest probability on the correct output by inferring the task from examples. Intriguingly, GPT-3 was not explicitly pretrained to learn from examples, and the distribution of prompts (which concatenate independent examples) is quite different from natural language. Our understanding of in-context learning is limited since (i) real pretraining data is messy and (ii) in-context learning has so far required large-scale datasets and models.\\n\\nIn this paper, we introduce a simple pretraining distribution where in-context learning emerges. To generate a document, we first draw a latent concept $\\\\theta$, which parameterizes the transitions of a Hidden Markov Model (HMM) (Baum & Petrie, 1966), then sample a sequence of tokens from the HMM (Figure 9). This latent variable structure is common in topic models such as LDA (Blei et al., 2003; Gruber et al., 2007). During pretraining, the LM must infer the latent concept across multiple sentences to generate coherent continuations. When conditioning on a prompt, in-context learning occurs when the LM also infers a shared prompt concept across examples to make a prediction. We assume the LM fits the pretraining distribution exactly with enough data and expressivity, so that the question of in-context learning becomes characterizing the conditional distribution of completions.\\n\\nThe code, data, and experiments are located on GitHub and CodaLab.\"}"}
{"id": "RdJVFCHjUMI", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Published as a conference paper at ICLR 2022\\n\\nFigure 10: In-context accuracy curve of the 4 layer Transformer on the GINC dataset when the entity transition matrix does not have an additional identity component, for vocabulary sizes 50 (left), 100 (middle), and 150 (right). In-context learning is still generally successful.\\n\\n\u2013 see Figure 8, designated to be index 1. The vocabulary is generated as combinations of letters starting from a to z, then aa to az, and so on. All sequences are tokenized by splitting on whitespaces.\\n\\nMemory matrix. The shared memory matrix has 10 entities and 10 properties, totaling 100 entries (corresponding to 100 hidden states). The first column of the memory matrix is fixed to be the delimiter token, while each remaining entry of the shared memory matrix is populated with a token sampled uniformly from the vocabulary.\\n\\nTransition matrix for properties. We generate 5 property transition matrices, one for each component of the HMM mixture. We generate each transition matrix via a convex combination of 100 random permutation matrices. The weights of the convex combination are randomly generated as $\\text{softmax}(\\\\frac{u - 0.5}{t})$ (117) where $u \\\\in \\\\mathbb{R}^{100}$ has uniform random entries in $[0,1]$ and $t$ is a temperature parameter, set to 0.1.\\n\\nTransition matrix for entities. The entity transition matrix is shared between all the HMMs that constitute the mixture. The entity transition matrix is generated in the same way as the property transition matrices, except with one additional step. Letting $T$ be a transition matrix sampled in the same way as a property transition matrix, $\\\\text{In pretraining documents, we expect that the entities (e.g., Einstein) change slowly over time while the properties of the entity (e.g., their occupation) change quickly with some pattern to generate natural sentences. We implement this by ensuring that the probability of transitioning to the same entity index in the next step is at least 0.9. The final entity transition matrix is then $0.1T + 0.9I$ where $I$ is the identity matrix. Although we add the diagonal component for added realism, we also consider not adding this component. Figure 10 shows in-context learning curves for a small (4 layer) Transformer trained on data that does not add the diagonal component (we check this for vocabulary sizes 50, 100, and 150). In-context learning still works in this case, although not as well for the 50 vocab size case.\\n\\nStart distribution. The starting distribution for the hidden states in all HMMs in the mixture are close to uniform. We generate the start distribution as $\\\\text{softmax}(\\\\frac{u - 0.5}{t})$ for random vector $u$ with entries uniformly from $[0,1]$ and temperature $t = 10$. In the pretraining documents, we only sample from the start distribution in the beginning of the document.\\n\\nPrompt distribution. To generate the prompts, we first sample a concept $\\\\theta$ uniformly at random from $\\\\Theta$ (well-specification, Assumption 4), then use it to generate all the prompt examples. The prompt start distribution is chosen to be uniform over entities but with a fixed starting property that is chosen randomly for each prompt, for consistency in the task. This may not satisfy Assumption 3, but we found this to still work empirically and is simpler. Given the starting property, we sample $k$ tokens from the HMM defined by the concept $\\\\theta$. Finally, we append the delimiter token for the example. We repeat this process for each example in the prompt, concatenating all examples. The label is generated as $\\\\text{argmax}_{y \\\\mid x_{\\\\text{test}}} y_{\\\\text{prompt}}$ (118) under the prompt concept $\\\\theta^*$. This differs from the theory, which samples $y_{\\\\text{test}}$ instead of taking it to be the most likely token. However, there can be a large amount of intrinsic error that sampling introduces. We define the label this way in the simulations to remove the intrinsic error from sampling.\"}"}
{"id": "RdJVFCHjUMI", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Example of prompt generation.\\n\\nIn the example in Figure 8 (right), the starting property is fixed to be 5 (for example). The first token (l) is generated by sampling a random entity index (3), and indexing into the memory matrix returns l. Running the hidden state chain of the HMM forward gives the next pair of property and entity. Since the entity Markov chain changes slowly, the entity is still 3 in the next step \u2013 however, the property has changed to 4, and indexing into the memory matrix outputs the next token (aw). Following this same process to generate the third token (the output for the first example), we finish generating one example. To end the example, we append a delimiter (backslash). We repeat this example generation process for all the examples, except for the test example at the end, where we do not generate the last token. We condition the HMM on the generated prompt to compute the posterior distribution $p_{\\\\text{prompt}}(y|\\\\mathbf{x}_{\\\\text{test}})$. We take the argmax of this distribution to be the ground truth label.\\n\\nDataset details.\\nThe dataset contains 1000 training documents and 100 validation documents, where training documents have 10240 tokens and validation documents have 1024 tokens. Each document is generated by first selecting one of the HMMs from the mixture uniformly at random, then generating 10240 tokens from the HMM.\\n\\nWe also generate 2500 in-context prompts for each (example length, number of examples) pair, for example lengths $k = [3, 5, 8, 10]$ and number of examples $n = [0, 1, 2, 4, 8, 16, 32, 64]$. Each prompt is generated using a random HMM in the mixture.\\n\\nF.2 TRANSFORMER DETAILS\\nOur Transformer models are based on the GPT-2 architectures with 4, 12, and 16 layers respectively, with 12 attention heads, 768 dimensional embeddings, residual/embedding/attention dropout set to 0.1, and a context window of 1024. Other than the number of layers, the other parameters are the default settings from the HuggingFace library (Wolf et al., 2019). We train for 5 epochs using the AdamW optimizer (Loshchilov & Hutter, 2019; Kingma & Ba, 2015) with a batch size of 8 and a linear learning rate schedule (with 1000 step warmup) up to a learning rate of 8e-4 for the 4 layer and 12 layer model, while for the 16 layer model we start with a constant learning rate of 8e-4 and reduce by a factor of 0.25 whenever the best validation loss does not improve. We tried both learning rate strategies for all models and take the most consistent. We tuned these models so that the training loss curves between seeds have smaller variability between the runs in terms of the curve shape and when the loss decreases \u2013 we found that this is an important indication of stable results. The models took 50 minutes, 2 hours, 3 hours to train respectively. The hardware was mainly Titan Xp GPUs, trained and evaluated using 16-bit precision. All the results are reported with 5 pretraining runs (5 different seeds).\\n\\nF.3 LSTM DETAILS\\nWe train an LSTM language model with embedding size 768, hidden layer size 768, and 6 layers. We use dropout 0.2 and weight decay 1e-5. The optimizer is AdamW starting with a learning rate of 1e-3, then reducing by a factor of 0.25 whenever the best validation loss does not go down. We train for a total of 10 epochs, with gradient clipping at norm 1.0. We use a batch size of 8 and backpropagate through time for 1024 steps (each pretraining data segment is also 1024 tokens). Each model takes roughly 2 hours to train on Titan Xp GPUs.\\n\\nF.4 VARYING THE VOCABULARY SIZE\\nTo do well on the in-context learning task, the model must both infer the prompt concept and the last HMM hidden state. In general, increasing the number of observable symbols makes the in-context task easier by making the inference of the HMM hidden state easier. With more symbols, each hidden state is more likely to output a different symbol, making the inference problem easier. This improvement comes despite the number of output classes in the problem (same as the vocabulary size) increasing. Figures 11, 12, 13, 14 show in-context learning curves for vocabulary sizes 50, 100, and 150, keeping other hyperparmeters of the dataset the same.\\n\\nF.5 EXPERIMENT ON GPT-3\\nWe conduct an additional experiment which shows that longer examples improve in-context learning in GPT-3 on the LAMBADA (Paperno et al., 2016) completion task.\\n\\nData.\\nIn this experiment, we define a short version of the LAMBADA test dataset (LAMBADA test-short) which contains only test examples with up to 200\u2013300 characters in length. We also define two \u201ctraining\u201d datasets from which to sample examples for the in-context prompts from. The short training dataset (LAMBADA train-short) contains examples from the training set that are\"}"}
{"id": "RdJVFCHjUMI", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: In-context accuracy of the 4 layer Transformer on the GINC dataset for vocabulary sizes 50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases.\\n\\nFigure 12: In-context accuracy of the 12 layer Transformer on the GINC dataset for vocabulary sizes 50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases.\\n\\nFigure 13: In-context accuracy of the 16 layer Transformer on the GINC dataset for vocabulary sizes 50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases.\\n\\nFigure 14: In-context accuracy of the LSTM on the GINC dataset for vocabulary sizes 50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases.\\n\\n200\u2013300 characters in length, which matches the distribution of test-short. The long training dataset (LAMBADA train-long) contains training examples that are 500\u2013600 characters long. We cut the number of examples in the larger of the two training datasets so that the two training datasets are equally sized (47 examples). For each test example, we sample 5 random training examples (5-shot learning).\\n\\nWe also consider equalizing the total length of the prompts in two ways. First, we consider duplicating the 5 short examples (if the examples are [1,2,3,4,5], duplicating refers to [1,2,3,4,5,1,2,3,4,5]). This allows for equalizing the total length without increasing the number of examples. As a skyline comparison, we also consider sampling 10 independent short examples, which contains more input-output pairs for the task.\\n\\nResult. Table 1 shows that when evaluating only on LAMBADA test-short, 5-shot in-context learning using LAMBADA train-long improves the test accuracy by almost 1% compared to LAMBADA train-short, despite the long/short distribution mismatch between train and test. This supports intuitions from our theory.\"}"}
{"id": "RdJVFCHjUMI", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prompt example length Test Acc (200\u2013300 chars)\\n\\n5 examples\\n\\nShort (200\u2013300 chars) 69.8\\nLong (500\u2013600 chars) 70.7\\n\\n10 examples\\n\\nShort, duplicated examples 69.6\\nShort, independent examples 71.4\\n\\nTable 1: Accuracies for 5-shot in-context learning of GPT-3 on a filtered LAMBADA test set with short examples (200\u2013300 characters). Even though there is distribution mismatch with the test set, having longer examples improves the accuracy, supporting theoretical intuitions. The first two rows use 5 training examples in the prompt, while the last two rows use 10 training examples to equalize the total length.\\n\\nIn comparison, simply increasing the total prompt length by duplicating the short examples does not improve the accuracy. Intuitively, the longer examples have additional information that is not directly related to mapping between the input and output, but can be leveraged to improve in-context learning by helping the model infer the latent concept. Using 5 long examples (as opposed to 5 short examples) closes about 56% of the gap between using 5 short examples and 10 independent short examples despite not adding additional examples or task-related information.\"}"}
{"id": "RdJVFCHjUMI", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: When the signal about the prompt concept within each example (green) is greater than the error from low-probability transitions between examples, in-context learning succeeds in our latent concept setting (Theorem 1). Increasing the example length $k$ increases the signal. The signal for in-context learning comes from tokens in both the inputs and the input-output mapping.\\n\\n**Distribution mismatch, the distinguishability condition depends on the KL divergence between $p_{\\\\text{prompt}}$ (which represents $\\\\theta^*$) and $p_{\\\\text{in-context}}$ as well as error terms $\\\\epsilon_{\\\\text{start}}$ and $\\\\epsilon_{\\\\text{delim}}$ coming from the distribution mismatch between the prompt and pretraining distributions at the start and delimiter token for each example:**\\n\\n$$\\\\text{KL}_j \\\\left( \\\\theta^* \\\\parallel \\\\theta \\\\right) := \\\\mathbb{E}_{O[1:j-1] \\\\sim p_{\\\\text{prompt}}} \\\\left[ \\\\text{KL} \\\\left( p_{\\\\text{prompt}} \\\\parallel p_{\\\\text{in-context}} \\\\right) \\\\right]$$\\n\\n$$\\\\epsilon_{\\\\text{start}} := \\\\log \\\\frac{1}{c_8}.$$  \\n\\n**Condition 1 (Distinguishability).**\\n\\nWe define $\\\\theta^*$ to be distinguishable if for all $\\\\theta \\\\in \\\\Theta, \\\\theta \\\\neq \\\\theta^*$,\\n\\n$$k \\\\sum_{j=2}^{k} \\\\text{KL}_j \\\\left( \\\\theta^* \\\\parallel \\\\theta \\\\right) > \\\\epsilon_{\\\\text{start}} + \\\\epsilon_{\\\\text{delim}}.$$  \\n\\nWhen the signal from KL divergence (LHS) is larger than the error terms, Equation 14 is satisfied (Figure 2). For larger example lengths $k$, the LHS increases, improving distinguishability. Intuitively, larger example lengths increase the proportion of the prompt sampled from the pretraining distribution by providing more evidence for Bayesian inference. Under Condition 1, the in-context predictor asymptotically achieves the optimal expected error.\\n\\n**Theorem 1.** Assume the assumptions in Section 2.1 hold. If Condition 1 holds, then as $n \\\\to \\\\infty$ the prediction according to the pretraining distribution is\\n\\n$$\\\\arg\\\\max_y p(y | S_n, x_{\\\\text{test}}) \\\\to \\\\arg\\\\max_y p_{\\\\text{prompt}}(y | x_{\\\\text{test}}).$$\\n\\nThus, the in-context predictor $f_n$ achieves the optimal 0-1 risk:\\n\\n$$\\\\lim_{n \\\\to \\\\infty} L_{0-1}(f_n) = \\\\inf_f L_{0-1}(f).$$\\n\\n### 3.3.2 Non-Distinguishable Case\\n\\nThe distinguishability condition (Condition 1) fails when there is some $\\\\theta \\\\neq \\\\theta^*$ for which the KL divergence between $\\\\theta$ and $\\\\theta^*$ is less than the error terms. However, this also means that the output distributions of $\\\\theta$ and $\\\\theta^*$ are close in KL. We leverage this to prove that the expected 0-1 error decreases with the example length $k$ under two different settings where distinguishability does not hold.\\n\\n**Continuity.** Our first result relies on a continuity assumption between the concept parameter and its corresponding output distribution. Our assumption is based on prior works (Kleijn & van der Vaart, 2012), where the KL divergence is assumed to have a 2nd-order Taylor expansion.\\n\\n**Theorem 2.** Let the set of $\\\\theta$ which does not satisfy Equation 14 in Condition 1 to be $B$. Assume that KL divergences have a 2nd-order Taylor expansion around $\\\\theta^*$:\\n\\n$$\\\\forall j > 1, \\\\text{KL}_j \\\\left( \\\\theta^* \\\\parallel \\\\theta \\\\right) = \\\\frac{1}{2} (\\\\theta - \\\\theta^*)^\\\\top I_{j, \\\\theta^*} (\\\\theta - \\\\theta^*) + O(\\\\|\\\\theta - \\\\theta^*\\\\|^3)$$\\n\\nwhere $I_{j, \\\\theta^*}$ is the Fisher information matrix of the $j$-th token distribution with respect to $\\\\theta^*$. Let $\\\\gamma_{\\\\theta^*} = \\\\max_j \\\\lambda_{\\\\max}(I_{j, \\\\theta^*}) \\\\min_j \\\\lambda_{\\\\min}(I_{j, \\\\theta^*})$ where $\\\\lambda_{\\\\max}, \\\\lambda_{\\\\min}$ return the largest and smallest eigenvalues. Then for $k > 1$ and as $n \\\\to \\\\infty$, the 0-1 risk of the in-context learning predictor $f_n$ is bounded as\\n\\n$$\\\\lim_{n \\\\to \\\\infty} L_{0-1}(f_n) \\\\leq \\\\inf_f L_{0-1}(f) + \\\\frac{g - 1}{\\\\epsilon_{\\\\text{start}} + \\\\epsilon_{\\\\text{delim}}} (O(\\\\gamma_{\\\\theta^*} \\\\sup_{\\\\theta \\\\in B} (\\\\epsilon_{\\\\text{start}} + \\\\epsilon_{\\\\text{delim}})))$$\\n\\nwhere $g(\\\\delta) = \\\\frac{1}{2} ((1 - \\\\delta) \\\\log(1 - \\\\delta) + (1 + \\\\delta) \\\\log(1 + \\\\delta))$ is a calibration function (Steinwart, 2007; \u00b4Avila Pires & Szepesv\u00e1ri, 2016) for the multiclass logistic loss for $\\\\delta \\\\in [0, 1)$. \\n\\n6\"}"}
{"id": "RdJVFCHjUMI", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: In-context accuracy (95% intervals) of Transformers (left) and LSTMs (right) on the GINC dataset. Accuracy increases with number of examples $n$ and length of each example $k$.\\n\\nFigure 4: Ablation studies for 4 layer Transformers on the GINC dataset with vocab size 50. (Left) When pretrained with only one concept, in-context learning fails. (Middle) When the pretraining data has random transitions, the model sees all token transitions but in-context learning fails. (Right) When prompts are from random unseen concepts, in-context learning fails to extrapolate.\\n\\nSince the inverse calibration function $g^{-1}$ is roughly linear in $\\\\epsilon$ for $\\\\epsilon \\\\leq 0.7$, the excess risk roughly decreases as $O(1/k)$. When the \\\"worst-case condition number\\\" $\\\\gamma_\\\\theta^*$ of the Fisher information matrices is smaller (well-conditioned), the error decreases. Intuitively, this means that there is no direction to vary $\\\\theta^*$ in which the output distribution will sharply change. As a consequence, the concepts $\\\\theta$ that are not distinguishable from the prompt concept $\\\\theta^*$ parameterize distributions that produce similar outputs to the prompt concept and thus achieve a small error.\\n\\nVarying-length test examples. In the setting where the length of $x_{test}$ is random (uniformly from 2 to $k$), we can give a similar error guarantee without continuity.\\n\\nTheorem 3. Let the set of $\\\\theta$ which does not satisfy Equation 14 in Condition 1 to be $B$. Let the length of the test example $x_{test}$ be uniformly distributed between 2 and $k$, for $k \\\\geq 2$. Then for $k \\\\geq 2$ and as $n \\\\to \\\\infty$, the 0-1 risk of the in-context learning predictor $f_n$ is bounded as:\\n\\n$$\\\\lim_{n \\\\to \\\\infty} L_{0-1}(f_n) \\\\leq \\\\inf_{f} L_{0-1}(f) + g^{-1}(O(\\\\sup_{\\\\theta \\\\in B}(\\\\epsilon_{\\\\theta_{start}} + \\\\epsilon_{\\\\theta_{delim}})k^{-1})).$$\\n\\nInstead of measuring only the error at the $k$-th token, we average the prediction error on the 2nd to $k$-th tokens. However, we leave bridging the mismatch between training examples, which are consistently length $k$, and test examples, which have random length, to future work.\\n\\n4 SIMULATIONS\\n\\nWe generate the GINC dataset and show that Transformers (Vaswani et al., 2017) and LSTMs (Hochreiter & Schmidhuber, 1997) trained on GINC exhibit in-context learning. In the theory, we assumed that the pretrained LM fits the pretraining distribution exactly. Here, we pretrain LMs to approximate the pretraining distribution and find that the in-context learning properties transfer to the LM.\\n\\nGINC dataset. We construct the GINC dataset according to our theory (see Appendix F.1). For pretraining, we define a uniform mixture of HMMs over a family $\\\\Theta$ of 5 concepts to generate 1000 pretraining documents with $\\\\sim 10$ million tokens total. For prompting, we generate prompts with 0 to 64 training examples and example lengths $k \\\\in \\\\{3, 5, 8, 10\\\\}$ (2500 prompts for each setting). The target token $y_{test}$ is taken to be the most likely output instead of sampling so that the intrinsic error is 0.\\n\\nMain result. We train GPT-2-based Transformers (Radford et al., 2019) and LSTMs on three versions of the GINC dataset with vocabulary sizes 50, 100, and 150, then evaluate the in-context\"}"}
{"id": "RdJVFCHjUMI", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Figure 5: In-context accuracy (95% intervals) of Transformers improves as model size increases on the GINC dataset for vocabulary sizes 50, 100, and 150.\\n\\n| Vocab size | Transformer (4 layer) | Transformer (12 layer) | Transformer (16 layer) | LSTM |\\n|------------|----------------------|------------------------|------------------------|------|\\n| 50, k=10, n=64 | 29M 1.49 1.50 60.2\u00b15.7 | 85M 1.31 1.33 81.2\u00b17.1 | 115M 1.31 1.33 84.7\u00b13.4 | 28M 1.31 1.35 95.8\u00b11.1 |\\n| 100, k=10, n=64 | 29M 1.58 1.59 67.4\u00b14.7 | 85M 1.40 1.42 84.6\u00b13.0 | 115M 1.41 1.43 88.7\u00b11.6 | 28M 1.43 1.44 95.8\u00b11.54 |\\n| 150, k=10, n=64 | 29M 1.44 1.45 92.8\u00b11.9 | 85M 1.27 1.28 98.4\u00b10.4 | 115M 1.27 1.28 98.1\u00b10.5 | 28M 1.26 1.31 99.2\u00b11.06 |\\n\\nFigure 6: In-context accuracies (95% intervals) on GINC with vocab sizes (50, 100, 150) for Transformers and LSTMs. Accuracy improves with scale even though the pretraining loss may be the same.\\n\\nSensitivity to example ordering. In Figure 7 (left), we test the sensitivity of in-context accuracy on GINC to the ordering of the prompt examples, following Zhao et al. (2021). For this experiment, we consider prompts generated from a single concept and prompt start distribution. We sample 10 different sets (leading to 10 training set IDs) of 4 examples and generate all 24 possible permutations for each example set. We consider the in-context accuracy of the 4 layer Transformer trained on GINC with vocabulary size 50. Similarly to the behavior of GPT-3 (Zhao et al., 2021), there is a significant variation (10\u201340% difference) between permutations of the same set of examples.\\n\\nZero-shot is sometimes better than few-shot. In some settings in GINC, we find that zero-shot performance can be better than few-shot performance. This mirrors GPT-3 on some datasets (e.g., LAM-BADA, HellaSwag, PhysicalQA, RACE-m, CoQA/SAT analogies for smaller models (Brown et al., 2020)). This occurs especially when the transition probabilities in GINC are lower entropy (controlled via a temperature parameter). For this experiment, we consider GINC with transition matrix temperature parameter 0.01 (instead of 0.1), 12 concepts, and vocabulary size 100. Figure 7 (right) shows that here, few-shot accuracy is initially worse than zero-shot accuracy, but can recover with more examples. We hypothesize that the distracting prompt structure initially decreases the accuracy in this setting.\"}"}
{"id": "RdJVFCHjUMI", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: (Left) In-context accuracy varies widely with example ordering. Each training ID refers to a set of training examples. Each dot refers to the in-context learning accuracy of one permutation of the training examples for that particular training ID. (Right) Zero-shot performance can be higher than one/few-shot performance in some settings in GINC, mirroring the behavior of GPT-3 on some datasets such as LAMBADA (Brown et al., 2020). The few-shot setting introduces the distracting prompt structure, which can initially lower accuracy.\\n\\n**DISCUSSION AND RELATED WORK**\\n\\nLearning via Bayesian inference and extrapolation. The canonical Bernstein-von Mises theorem (van der Vaart, 1998) does not apply for in-context learning since the prompt examples are not independent under the pretraining distribution. Gunst & Shcherbakova (2008) show a Bernstein-von Mises-type result for observations from an HMM, but do not handle observations from a different distribution. Future directions include more precise asymptotic results about the posterior distribution and results under misspecification/extrapolation (Kleijn & van der Vaart, 2012). A possible avenue for extrapolation to some types of unseen concepts is to factorize the latent concept into semantics and syntax. While the pretraining data may contain only some semantics-syntax pairs, the language model could generalize to unseen pairs if it learns generalizable syntactical operations such as copying or reordering.\\n\\n**Topic models and HMMs.** Topic models such as LDA (Blei et al., 2003) also have document-level latent variables, but learning is typically relies on algorithms such as EM (Dempster et al., 1977), variational inference (Jordan et al., 1999), or MCMC (Metropolis et al., 1953; Hastings, 1970). We focus on learning as a natural result of Bayesian inference without an explicit inference algorithm. Wei et al. (2021a) also use an HMM model in their pretraining analysis. However, they analyze how pre-trained representations learned with masked LMs (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020; Clark et al., 2020) can improve optimization-based downstream learning (Li & Liang, 2021; Lester et al., 2021) rather than in-context learning.\\n\\n**Bridging the mismatch between pretraining and prompting.** Prior works support our theoretical intuitions that reducing the prompt distribution mismatch would improve in-context learning. Finetuning LMs on text with a prompting format improves its zero-shot performance (Wei et al., 2021b; Sanh et al., 2021) and optimizing prompt templates improves few-shot finetuning (Jiang et al., 2020; Schick & Sch\u00fctze, 2021; Shin et al., 2020; Gao et al., 2021). Zhao et al. (2021); Holtzman et al. (2021) improve in-context accuracy via calibration or renormalization, a form of adaptation to the prompt distribution.\\n\\n**Meta-learning.** Meta-learning methods can also train a sequence model to learn from examples (Ravi & Larochelle, 2017). However, meta-learning models are trained to learn, while in-context learning emerges from LM pretraining.\\n\\n**Studying large-scale phenomena at a small scale.** We can study in-context learning, a large scale phenomenon, at a small scale in GINC because the complexity of the pretraining distribution (HMM hidden state size, number of latent concepts) is small, such that the data and models are relatively larger. Since GINC is synthetic, we can also control the latent data properties (e.g., unseen concepts) to make predictions about large LMs while working at a small scale.\\n\\n**CONCLUSION**\\n\\nWe cast in-context learning as implicit Bayesian inference, where the pretrained LM implicitly infers a concept when making a prediction. We show that in-context learning occurs when the pre-training distribution is a mixture of HMMs. Our work provides a first step towards understanding in-context learning, which we hope will provide insight for improving pretraining and prompting.\"}"}
{"id": "RdJVFCHjUMI", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGEMENTS\\n\\nWe thank Tianyi Zhang, Frieda Rong, Lisa Li, Colin Wei, Shibani Santurkar, Tri Dao, Ananya Kumar, and Shivam Garg for helpful discussions and feedback. SMX is supported by an NDSEG Fellowship. The work is partially supported by an Open Philanthropy Project Award, SDSI, and SAIL at Stanford University. TM acknowledges support of Google Faculty Award, NSF IIS 2045685, the Sloan Fellowship, and JD.com. Toyota Research Institute provided funds to support this work.\\n\\nREFERENCES\\n\\nLeonard E Baum and Ted Petrie. Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics, 37(6):1554\u20131563, 1966.\\n\\nD. Blei, Andrew Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research (JMLR), 3:993\u20131022, 2003.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\n\\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations (ICLR), 2020.\\n\\nA. P. Dempster, Laird N. M., and Rubin D. B. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B, 39(1):1\u201338, 1977.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Association for Computational Linguistics (ACL), pp. 4171\u20134186, 2019.\\n\\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. arXiv, 2021.\\n\\nZoubin Ghahramani and Michael Jordan. Factorial hidden Markov models. Machine Learning, 29:245\u2013273, 1997.\\n\\nAmit Gruber, Yair Weiss, and Michal Rosen-Zvi. Hidden topic Markov models. In Artificial Intelligence and Statistics (AISTATS), 2007.\\n\\nM. Gunst and O. Shcherbakova. Asymptotic behavior of Bayes estimators for hidden Markov models with application to ion channels. Mathematical Methods of Statistics, 17, 2008.\\n\\nKeith W. Hastings. Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57(1):97\u2013109, 1970.\\n\\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u20131780, 1997.\\n\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations (ICLR), 2020.\\n\\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form competition: Why the highest probability answer isn\u2019t always right, 2021.\\n\\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? In Association for Computational Linguistics (ACL), 2020.\\n\\nMichael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. An introduction to variational methods for graphical models. Machine Learning, 37:183\u2013233, 1999.\"}"}
{"id": "RdJVFCHjUMI", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Association for Computational Linguistics (ACL), 2017.\\n\\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.\\n\\nB.J.K. Kleijn and A.W. van der Vaart. The Bernstein-von mises theorem under misspecification. Electronic Journal of Statistics, 6, 2012.\\n\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Association for Computational Linguistics (ACL), 2020.\\n\\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Association for Computational Linguistics (ACL), 2021.\\n\\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. Technical report, AI21 Labs, August 2021.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\\n\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019.\\n\\nNicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward Teller. Equation of state calculations by fast computing machines. The journal of chemical physics, 21(6):1087\u20131092, 1953.\\n\\nDenis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Association for Computational Linguistics (ACL), 2016.\\n\\nAlethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. In ICLR MATH AI Workshop, 2021.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.\\n\\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International Conference on Learning Representations (ICLR), 2017.\\n\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization, 2021.\\n\\nTimo Schick and Hinrich Sch\u00fctze. Exploiting cloze questions for few shot text classification and natural language inference. In European Association for Computational Linguistics (EACL), 2021.\\n\\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Eliciting knowledge from language models using automatically generated prompts. In Empirical Methods in Natural Language Processing (EMNLP), 2020.\"}"}
{"id": "RdJVFCHjUMI", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2022\\n\\nIngo Steinwart. How to compare different loss functions and their risks. Constructive Approximation, 26, 2007.\\n\\nA. W. van der Vaart. Asymptotic statistics. Cambridge University Press, 1998.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\\n\\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\\n\\nColin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning. arXiv, 2021a.\\n\\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. arXiv, 2021b.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u2019emi Louf, Morgan Funtowicz, and Jamie Brew. HuggingFace\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\\n\\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations (ICLR), 2017.\\n\\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning (ICML), 2021.\\n\\nBernardo \u00b4Avila Pires and Csaba Szepesv\u00b4ari. Multiclass classification calibration functions. arXiv, 2016.\"}"}
{"id": "RdJVFCHjUMI", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Framework Details\\n\\nPrompt distribution details.\\n\\nFor in-context learning, we sample a prompt from a new distribution \\\\( p_{\\\\text{prompt}} \\\\), which consists of \\\\( n \\\\) independent training examples and 1 test example. We first sample \\\\( n \\\\) hidden segments \\\\( H \\\\) of length \\\\( k \\\\) by sampling the first element \\\\( h_{\\\\text{start}} = H[1] \\\\) from a prompt start distribution \\\\( p_{\\\\text{prompt}} \\\\). Then, we sample the rest of the segment \\\\( H_{\\\\text{seg}} = H[2:k] \\\\) from the hidden transition distribution of the pretraining distribution corresponding to a particular concept \\\\( \\\\theta^* \\\\):\\n\\n\\\\[\\nH_1,\\\\ldots,H_n, H_i = [h_i,1,\\\\ldots,h_i,k]\\n\\\\]\\n\\n\\\\[\\nh_{\\\\text{start}} i = H_i[1] \\\\sim p_{\\\\text{prompt}}, H_{\\\\text{seg}} i = H_i[2:k] \\\\sim p(H_{\\\\text{seg}} i | h_{\\\\text{start}} i, \\\\theta^*).\\n\\\\]\\n\\nTo end each example (except the test example), we sample \\\\( n \\\\) delimiters \\\\( h_{\\\\text{delim}} \\\\in D \\\\) from \\\\( p_{\\\\text{delim}} \\\\):\\n\\n\\\\[\\nh_{\\\\text{delim}} 1,\\\\ldots,h_{\\\\text{delim}} n, h_{\\\\text{delim}} i \\\\sim p_{\\\\text{delim}}.\\n\\\\]\\n\\nConditioned on hidden variables \\\\( H_i \\\\) and \\\\( h_{\\\\text{delim}} i \\\\), we sample the observed tokens \\\\( O_i = [o_i,1,\\\\ldots,o_i,k] \\\\) and \\\\( o_{\\\\text{delim}} i \\\\) respectively from the pre-training distribution:\\n\\n\\\\[\\nO_1,\\\\ldots,O_n, O_i \\\\sim p(O_i | H_i)\\n\\\\]\\n\\n\\\\[\\no_{\\\\text{delim}} 1,\\\\ldots,o_{\\\\text{delim}} n, o_{\\\\text{delim}} i \\\\sim p(o_{\\\\text{delim}} i | h_{\\\\text{delim}} i, \\\\theta^*).\\n\\\\]\\n\\nThe \u201cinput\u201d for each example is \\\\( x_i = O_i[1:k-1] \\\\) and the \u201coutput\u201d is \\\\( y_i = O_i[k] \\\\). Taking \\\\( S \\\\) to be the sequence of training examples (without the test example), the resulting prompt sequence is\\n\\n\\\\[\\n[S_n, x_{\\\\text{test}}] = [O_1, o_{\\\\text{delim}} 1,\\\\ldots,O_n, o_{\\\\text{delim}} n, x_{\\\\text{test}}] = [x_1, y_1, o_{\\\\text{delim}} 1, x_2, y_2, o_{\\\\text{delim}} 2,\\\\ldots,x_n, y_n, o_{\\\\text{delim}} n, x_{\\\\text{test}}] \\\\sim p_{\\\\text{prompt}}\\n\\\\]\\n\\nwhere \\\\( x_{\\\\text{test}} = x_{n+1} = O_{n+1}[1:k-1] \\\\) is sampled via the same process but with \\\\( k-1 \\\\) elements.\\n\\nPropositions for Theorem 1\\n\\nThe following propositions, which lower bound the probability of a delimiter token and probability of an example under \\\\( \\\\theta^* \\\\), are direct corollaries of the assumptions.\\n\\nProposition 1. For all \\\\( i \\\\), we have\\n\\n\\\\[\\np(h_{\\\\text{delim}} i | O_1, o_{\\\\text{delim}} 1,\\\\ldots,O_i, \\\\theta^*) > c_1 \\\\quad \\\\text{and} \\\\quad p(h_{\\\\text{delim}} i | O_1, o_{\\\\text{delim}} 1,\\\\ldots,O_i, \\\\theta^*) < c_2.\\n\\\\]\\n\\nProof. By Assumption 2,\\n\\n\\\\[\\np(h_{\\\\text{delim}} i | O_1, o_{\\\\text{delim}} 1,\\\\ldots,O_i, \\\\theta^*) = \\\\sum_{h_{i,k}} p(h_{\\\\text{delim}} i | h_{i,k}) p(h_{i,k} | O_1, o_{\\\\text{delim}} 1,\\\\ldots,O_i, \\\\theta^*) > \\\\sum_{h_{i,k}} c_2 p(h_{i,k} | O_1, o_{\\\\text{delim}} 1,\\\\ldots,O_i, \\\\theta^*) = c_2.\\n\\\\]\\n\\nSimilarly,\\n\\n\\\\[\\np(h_{\\\\text{delim}} i | O_1, o_{\\\\text{delim}} 1,\\\\ldots,O_i, \\\\theta^*) = \\\\sum_{h_{i,k}} p(h_{\\\\text{delim}} i | h_{i,k}) p(h_{i,k} | O_1, o_{\\\\text{delim}} 1,\\\\ldots,O_i, \\\\theta^*) > \\\\sum_{h_{i,k}} c_1 p(h_{i,k} | O_1, o_{\\\\text{delim}} 1,\\\\ldots,O_i, \\\\theta^*) = c_1.\\n\\\\]\\n\\nProposition 2. The probability of an example is lower bounded for \\\\( \\\\theta^* \\\\) there is some \\\\( c_7 > 0 \\\\) such that\\n\\n\\\\[\\np(O_i | h_{\\\\text{start}} i, h_{j,l}, \\\\theta^*) > c_7 \\\\quad \\\\text{for all} \\\\quad i \\\\quad \\\\text{and future hidden states} \\\\quad h_{j,l}, \\\\quad \\\\text{for any} \\\\quad l \\\\quad \\\\text{and} \\\\quad j > i.\\n\\\\]\\n\\nProof. By Assumption 5, we have\\n\\n\\\\[\\np(O_i | h_{\\\\text{start}} i, h_{j,l}, \\\\theta^*) = \\\\sum_{H_i} p(O_i | H_i) p(H_i | h_{\\\\text{start}} i, h_{j,l}, \\\\theta^*) > (c_6)^k\\n\\\\]\\n\\nfor some \\\\( H_i \\\\). We have\\n\\n\\\\[\\np(H_i | h_{\\\\text{start}} i, h_{j,l}, \\\\theta^*) = p(h_{j,l} | H, h_{\\\\text{start}} i, \\\\theta^*) p(H | h_{\\\\text{start}} i, \\\\theta^*) p(h_{j,l} | h_{\\\\text{start}} i, \\\\theta^*) > c_2^5\\n\\\\]\\n\\nwhich lower bounds the terms in the numerator by \\\\( c_5 \\\\) (marginalizing over previous hidden states), and upper bounding the denominator by 1. Setting \\\\( c_7 = (c_6)^k c_2^5 \\\\) finishes the proof.\"}"}
{"id": "RdJVFCHjUMI", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: In-context learning can emerge from modeling long-range coherence in the pretraining data. During pretraining, the language model (LM) implicitly learns to infer a latent concept (e.g., wiki bios, which typically transition between name (Albert Einstein) \u2192 nationality (German) \u2192 occupation (physicist) ...) shared across sentences in a document. Although prompts are unnatural sequences that concatenate independent examples, in-context learning occurs if the LM can still infer the shared concept across examples to do the task (name \u2192 nationality, which is part of wiki bios).\\n\\ngiven prompts $p(output|prompt)$ under the pretraining distribution, where the prompt is generated from a different distribution $p(prompt)$. This conditional distribution, which is the posterior predictive distribution, marginalizes out the latent concepts:\\n\\n$$p(output|prompt) = \\\\int_{concept} p(output|concept, prompt) p(concept|prompt) \\\\, d(concept).$$  \\n\\nIf $p(concept|prompt)$ concentrates on the prompt concept with more examples, then the LM learns via marginalization by \\\"selecting\\\" the prompt concept. Thus, in-context learning can be viewed as the LM implicitly performing Bayesian inference.\\n\\nThe main challenge is that prompts are sampled from a different distribution than the pretraining distribution. The canonical Bayesian asymptotic tool is the Bernstein-von Mises theorem (van der Vaart, 1998; Kleijn & van der Vaart, 2012; Gunst & Shcherbakova, 2008), which asserts (under regularity conditions) that the posterior distribution of a latent variable concentrates on the maximum likelihood estimate. However, Bernstein-von Mises typically assumes observations are independent and/or drawn from the same distribution as the model, both of which are not satisfied. We prove that despite the distribution mismatch, the asymptotic prediction error of in-context learning is optimal when the signal about the latent concept in each prompt example is larger than the error due to the distribution mismatch. Additionally, we prove that the in-context learning error decreases with the length of each example\u2014thus, information in the inputs, not just the input-output mapping, can be useful for in-context learning.\\n\\nAs a companion to this theory, we created the Generative IN-context learning dataset (GINC), which is a small-scale synthetic dataset for studying in-context learning. We find that both Transformers (Vaswani et al., 2017) and LSTMs (Hochreiter & Schmidhuber, 1997) trained on GINC exhibit in-context learning. We verify intuitions from the theory, showing that the accuracy of in-context learning improves with the number of examples and example length. Ablations of the GINC dataset show that the latent concept structure in the pretraining distribution is crucial to the emergence of in-context learning. The experiments also bring up open questions which go beyond our theory, which only studies the pretraining distribution. We find that scaling up the number of model parameters steadily improves the in-context accuracy despite achieving the same pretraining loss, showing that larger models may improve in-context learning beyond increasing the capacity for memorizing the training data better.\\n\\nPreviously observed in-context learning phenomena such as sensitivity to example ordering (Zhao et al., 2021) and the existence of settings where zero-shot is better than one/few-shot learning (Brown et al., 2020) are also mirrored in GINC.\\n\\n2 IN-CONTEXT LEARNING SETTING\\n\\nPretraining distribution. In our framework, a latent concept $\\\\theta$ from a family of concepts $\\\\Theta$ defines a distribution over observed tokens $o$ from a vocabulary $O$. To generate a document, we first sample a\"}"}
{"id": "RdJVFCHjUMI", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"concept from a prior $p(\\\\theta)$ and then sample the document given the concept. Each pretraining document is a length $T$ sequence:\\n\\n$$p(o_1, \\\\ldots, o_T) = \\\\int_{\\\\theta \\\\in \\\\Theta} p(o_1, \\\\ldots, o_T | \\\\theta) p(\\\\theta) \\\\, d\\\\theta.$$  \\n\\n(2)\\n\\nWe assume $p(o_1, \\\\ldots, o_T | \\\\theta)$ is defined by a Hidden Markov Model (HMM). The concept $\\\\theta$ determines the transition probability matrix of the HMM hidden states $h_1, \\\\ldots, h_T$ from a hidden state set $H$.\\n\\nPrompt distribution. The prompt distribution $p_{\\\\text{prompt}}$ generates prompts for in-context learning. A prompt is a concatenation of $n$ independent training examples and 1 test input $x_{\\\\text{test}}$, which are all conditioned on a shared prompt concept $\\\\theta^*$. The goal is to predict the test output $y_{\\\\text{test}}$ by predicting the next token conditioned on the prompt.\\n\\nA prompt example is composed of an input token sequence (e.g., Albert Einstein was) followed by an output token (e.g., German). In particular, the $i$-th training example $O_i$ consists of an input $x_i = O_i[1:k-1]$ (the first $k-1$ tokens) followed by an output token $y_i = O_i[k]$ at the end.\\n\\nThe $i$-th training example is independently generated as follows:\\n\\n1. Generate a start hidden state $h_{\\\\text{start}}^i$ from a prompt start distribution $p_{\\\\text{prompt}}$.\\n2. Given $h_{\\\\text{start}}^i$, generate the example sequence $O_i = [x_i, y_i]$ from $p(O_i | h_{\\\\text{start}}^i, \\\\theta^*)$, the pretraining distribution conditioned on a prompt concept $\\\\theta^*$.\\n\\nThe test input $x_{\\\\text{test}} = x_{n+1}$ is sampled similarly. Between each example, there is a special delimiter token $o_{\\\\text{delim}}$. The prompt consists of a sequence of training examples ($S_n$) followed by the test example $x_{\\\\text{test}}$:\\n\\n$$[S_n, x_{\\\\text{test}}] = [x_1, y_1, o_{\\\\text{delim}}, x_2, y_2, o_{\\\\text{delim}}, \\\\ldots, x_n, y_n, o_{\\\\text{delim}}, x_{\\\\text{test}}] \\\\sim p_{\\\\text{prompt}}.$$  \\n\\n(3)\\n\\nMismatch between prompt and pretraining distributions. Since transitions between independent examples can be unnatural, the prompts are low probability sequences under the pretraining distribution.\\n\\nWe provide a simple illustration using the names to nationalities example. Suppose that wiki bio documents in the pretraining data typically transition between name $\\\\rightarrow$ nationality $\\\\rightarrow$ occupation $\\\\rightarrow$ ...\\n\\nIn the prompt, the examples transition between name $\\\\rightarrow$ nationality $\\\\rightarrow$ name $\\\\rightarrow$ nationality $\\\\rightarrow$ ...\\n\\nwhich contains low-probability transitions such as \\\"German\\\" $\\\\rightarrow$ \\\"Mahatma Gandhi\\\". The prompt formatting (e.g., choice of delimiter) can also be a source of mismatch. We aim to show that despite this mismatch, large LMs can infer the prompt concept from examples.\\n\\nIn-context predictor and task. For in-context learning, the output target $y$ for each example $x$ is sampled according to $p_{\\\\text{prompt}}(y | x)$:\\n\\n$$y_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}(y | x_{\\\\text{test}}) = \\\\mathbb{E}_{h_{\\\\text{start}}^\\\\text{test} \\\\sim p_{\\\\text{prompt}}(h_{\\\\text{start}}^\\\\text{test} | x_{\\\\text{test}})} [p(y | x_{\\\\text{test}}, h_{\\\\text{start}}^\\\\text{test}, \\\\theta^*)].$$  \\n\\n(4)\\n\\nwhere $h_{\\\\text{start}}^\\\\text{test}$ denotes the hidden state corresponding to the first token of $x_{\\\\text{test}}$. We analyze the in-context predictor $f_n(x_{\\\\text{test}}) = \\\\arg\\\\max_y p(y | S_n, x_{\\\\text{test}})$, which outputs the most likely prediction over the pretraining distribution conditioned on the prompt from the prompt distribution $p_{\\\\text{prompt}}$.\\n\\n$$L_{0-1}(f_n) = \\\\mathbb{E}_{x_{\\\\text{test}}, y_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}} [\\\\mathbb{1}[f_n(x_{\\\\text{test}}) \\\\neq y_{\\\\text{test}}]].$$\\n\\n2.1 Assumptions\\n\\nWe detail the assumptions in our framework, including the structure of delimiters and regularity assumptions. We first assume that there exists a subset of delimiter hidden states $D$ which generates the special delimiter token $o_{\\\\text{delim}}$ deterministically.\\n\\nAssumption 1 (Delimiter hidden states).\\n\\nLet the delimiter hidden states $D$ be a subset of $H$. For any $h_{\\\\text{delim}} \\\\in D$ and $\\\\theta \\\\in \\\\Theta$, $p(o_{\\\\text{delim}} | h_{\\\\text{delim}}, \\\\theta) = 1$ and for any $h/ \\\\in D$, $p(o_{\\\\text{delim}} | h, \\\\theta) = 0$.\\n\\nThus, observing the delimiter $o_{\\\\text{delim}}$ reveals that the corresponding hidden state is in $D$, but does not reveal which element of $D$ it is. The delimiter is usually a token that can appear in a broad range of contexts (e.g., newline). The delimiter ideally does not distract from the examples \u2014 for example, an adversarial delimiter could look like part of the input $x$. To mitigate these scenarios, we assume that no delimiter (e.g., newline) is significantly more likely under one concept rather than another.\\n\\nThe example length $k$ is fixed for simplicity \u2014 we leave extending our analysis to variable $k$ as future work.\\n\\n3 In practice, greedy decoding or nucleus sampling (Holtzman et al., 2020) are used for likely completions.\"}"}
{"id": "RdJVFCHjUMI", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2022\\n\\nAssumption 2\\n\\nFor any delimiter state \\\\( h_{\\\\text{delim}} \\\\in D \\\\) and any hidden state \\\\( h \\\\in H \\\\), the probability of transitioning to a delimiter hidden state under \\\\( \\\\theta \\\\) is upper bounded \\\\( p(h_{\\\\text{delim}} | h, \\\\theta) < c_2 \\\\) for any \\\\( \\\\theta \\\\in \\\\Theta \\\\{ \\\\theta^* \\\\} \\\\), and is lower bounded \\\\( p(h_{\\\\text{delim}} | h, \\\\theta^*) > c_1 > 0 \\\\) for \\\\( \\\\theta^* \\\\). Additionally, the start hidden state distribution for delimiter hidden states is bounded as \\\\( p(h_{\\\\text{delim}} | \\\\theta) \\\\in [c_3, c_4] \\\\).\\n\\nThe prompt start distribution is a source of distribution shift that is separate from the shift from concatenating independent examples. We make an assumption that limits how much distribution shift is introduced by the prompt start distribution.\\n\\nAssumption 3\\n\\nWe assume that the prompt start distribution \\\\( p_{\\\\text{prompt}} \\\\) is close in TV distance to all hidden transition distributions (under \\\\( \\\\theta^* \\\\)) starting from a delimiter hidden state:\\n\\n\\\\[\\n\\\\max_{h_{\\\\text{delim}} \\\\in D} \\\\text{TV}(p_{\\\\text{prompt}}(h) \\\\parallel p(h | h_{\\\\text{delim}}, \\\\theta^*)) < \\\\Delta / 4.\\n\\\\]\\n\\nHere, \\\\( \\\\Delta = p_{\\\\text{prompt}}(y_{\\\\text{max}} | x_{\\\\text{test}}) - \\\\max_{y \\\\neq y_{\\\\text{max}}} p_{\\\\text{prompt}}(y | x_{\\\\text{test}}) \\\\) is the margin between the most likely label \\\\( y_{\\\\text{max}} = \\\\arg\\\\max_y p_{\\\\text{prompt}}(y | x_{\\\\text{test}}) \\\\) and the second most likely label.\\n\\nEven if the maximum TV distance is 0, there is still distribution shift from concatenating independent examples. We also assume the prompt concept \\\\( \\\\theta^* \\\\) is in the family \\\\( \\\\Theta \\\\), a broad set of concepts.\\n\\nAssumption 4\\n\\nThe prompt concept \\\\( \\\\theta^* \\\\) is in \\\\( \\\\Theta \\\\).\\n\\nEven though the pretraining distribution is broad, the prompt is still low probability under the pretraining distribution since it concatenates independent examples. Finally, if the prompt has zero probability under the prompt concept \\\\( \\\\theta^* \\\\), then Bayesian inference will not be able to infer the prompt concept as in Section 3.1. The following are regularity assumptions which mainly ensure that the prompt is not zero probability under \\\\( \\\\theta^* \\\\).\\n\\nAssumption 5\\n\\nThe pretraining distribution \\\\( p \\\\) satisfies:\\n\\n1) Lower bound on transition probability for the prompt concept \\\\( \\\\theta^* \\\\): for any pair of hidden states \\\\( h, h' \\\\in H \\\\), \\\\( p(h | h', \\\\theta^*) > c_5 > 0 \\\\).\\n2) Start hidden state is lower bounded: for any \\\\( h \\\\in H \\\\), \\\\( p(h | \\\\theta^*) \\\\geq c_8 > 0 \\\\).\\n3) All tokens can be emitted: for every symbol \\\\( o \\\\), there is some hidden state \\\\( h \\\\in H \\\\) such that \\\\( p(o | h, \\\\theta^*) > c_6 > 0 \\\\).\\n4) The prior \\\\( p(\\\\theta) \\\\) has support over the entire concept family \\\\( \\\\Theta \\\\) and is bounded above everywhere.\\n\\nTheoretical Analysis\\n\\nWe prove that in the limit of infinite examples, the error of the in-context predictor is optimal if a distinguishability condition holds \u2014 the prompt concept \\\\( \\\\theta^* \\\\) is distinct enough from the other concepts in \\\\( \\\\Theta \\\\) (e.g., when \\\\( \\\\Theta \\\\) is a discrete set). When distinguishability does not hold (e.g., \\\\( \\\\Theta \\\\) is continuous-valued), we show that the expected error still decreases with the length of each example, showing that information in both the inputs and the input-output mapping contribute to in-context learning.\\n\\n3.1 High-Level Approach\\n\\nOur goal is to show that \\\\( \\\\arg\\\\max_y p(y | S^n, x_{\\\\text{test}}) \\\\to \\\\arg\\\\max_y p_{\\\\text{prompt}}(y | x_{\\\\text{test}}) \\\\) as the number of examples \\\\( n \\\\) grows. In the following, assume that the prompt has non-zero probability under the pretraining distribution \\\\( \\\\theta_{\\\\text{prompt}} \\\\), meaning that \\\\( p(S^n, x_{\\\\text{test}} | \\\\theta^*) > 0 \\\\). We expand \\\\( p(y | S^n, x_{\\\\text{test}}) \\\\) to analyze its limit:\\n\\n\\\\[\\np(y | S^n, x_{\\\\text{test}}) = \\\\int_{\\\\theta} p(y | S^n, x_{\\\\text{test}}, \\\\theta) p(\\\\theta | S^n, x_{\\\\text{test}}) d\\\\theta \\\\propto \\\\int_{\\\\theta} p(y | S^n, x_{\\\\text{test}}, \\\\theta) p(S^n, x_{\\\\text{test}} | \\\\theta) p(\\\\theta) d\\\\theta \\\\tag{Bayes' rule, drop the constant}\\n\\\\]\\n\\n\\\\[\\n= \\\\int_{\\\\theta} \\\\sum_{h_{\\\\text{start}}} p(y | x_{\\\\text{test}}, h_{\\\\text{start}}, \\\\theta) p(h_{\\\\text{start}} | S^n, x_{\\\\text{test}}, \\\\theta) p(S^n, x_{\\\\text{test}} | \\\\theta) p(\\\\theta) d\\\\theta \\\\tag{Law of total prob, Markov property, divide by \\\\( p(S^n, x_{\\\\text{test}} | \\\\theta^*) \\\\) (a constant)}\\n\\\\]\\n\\n\\\\[\\n= \\\\int_{\\\\theta} \\\\sum_{h_{\\\\text{start}}} p(y | x_{\\\\text{test}}, h_{\\\\text{start}}, \\\\theta) p(h_{\\\\text{start}} | S^n, x_{\\\\text{test}}, \\\\theta) \\\\exp(n \\\\cdot r_n(\\\\theta)) p(\\\\theta) d\\\\theta \\\\tag{6}\\n\\\\]\\n\\nwhere \\\\( r_n(\\\\theta) = \\\\frac{1}{n} \\\\log p(S^n, x_{\\\\text{test}} | \\\\theta) / p(S^n, x_{\\\\text{test}} | \\\\theta^*) \\\\). In Theorem 1, we prove that under a distinguishability condition, \\\\( \\\\exp(n \\\\cdot r_n(\\\\theta)) \\\\to 0 \\\\) for all concepts \\\\( \\\\theta \\\\) except the prompt concept \\\\( \\\\theta^* \\\\), where \\\\( \\\\exp(n \\\\cdot r_n(\\\\theta^*)) = 1 \\\\). The only nonzero term in the integral is when \\\\( \\\\theta = \\\\theta^* \\\\), and thus the prompt concept is \\\"selected\\\" as a consequence of Bayesian inference.\\n\\nWe can exchange limits and integrals since the probabilities are bounded (dominated convergence).\"}"}
{"id": "RdJVFCHjUMI", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"same as the most likely label under \\\\( p_{\\\\text{prompt}}(y|x_{\\\\text{test}}) \\\\) (using Assumption 3). Putting these together with Equation 6, the in-context predictor infers the prompt concept \\\\( \\\\theta^* \\\\):\\n\\n\\\\[\\n\\\\arg\\\\max_y p(y|S_n, x_{\\\\text{test}}) \\\\rightarrow \\\\arg\\\\max_y p_{\\\\text{prompt}}(y|x_{\\\\text{test}}) \\\\tag{7}\\n\\\\]\\n\\nThus, the in-context predictor is optimal as the number of in-context examples increases.\\n\\n### 3.2 EURISTIC DERIVATION\\n\\nRecall from Section 3.1 that if \\\\( \\\\exp(n \\\\cdot r_n(\\\\theta)) \\\\rightarrow 0 \\\\) for all \\\\( \\\\theta \\\\neq \\\\theta^* \\\\), then Bayesian inference \\\"selects\\\" the prompt concept through marginalization. To do this, we focus on showing that \\\\( r_n(\\\\theta) \\\\), the average log-likelihood ratio between \\\\( \\\\theta \\\\) and \\\\( \\\\theta^* \\\\), converges to a negative constant, and thus \\\\( nr_n \\\\) goes to \\\\(-\\\\infty\\\\).\\n\\nThe main technical challenge is to handle the sequence-of-examples structure of the prompt, which makes all the examples dependent with respect to the pretraining distribution. Our approach uses properties of delimiter tokens to approximately factorize the examples, with constant error per example.\\n\\nWe let \\\\( O_{\\\\text{ex}}^i = [o_{\\\\text{delim}}^{i-1}, O_i] \\\\) be the \\\\( i \\\\)-th input-output pair and the previous delimiter together for \\\\( i > 1 \\\\) and define \\\\( O_{\\\\text{ex}}^1 = O_1 \\\\). Expanding the likelihood term inside \\\\( r_n(\\\\theta) \\\\), our goal is to show\\n\\n\\\\[\\np(S_n, x_{\\\\text{test}}|\\\\theta) = p(x_{\\\\text{test}}|S_n, \\\\theta)p(S_n|\\\\theta) \\\\approx n \\\\prod_{i=1}^{O_1} p(O_i|\\\\theta) \\\\tag{8}\\n\\\\]\\n\\nTo show this, we expand \\\\( p(S_n|\\\\theta) \\\\) with the chain rule, and with Assumption 5 (to bound \\\\( p(x_{\\\\text{test}}|S_n, \\\\theta) \\\\) by \\\\( O_1 \\\\)) it can be shown that\\n\\n\\\\[\\np(x_{\\\\text{test}}|S_n, \\\\theta)p(S_n|\\\\theta) \\\\approx n \\\\prod_{i=1}^{O_1} p(O_{\\\\text{ex}}^i|O_{\\\\text{ex}}^1:i-1, \\\\theta) \\\\tag{9}\\n\\\\]\\n\\nWe then marginalize \\\\( p(O_{\\\\text{ex}}^i|O_{\\\\text{ex}}^1:i-1, \\\\theta) \\\\) over the hidden state \\\\( h_{\\\\text{delim}}^{i-1} \\\\) corresponding to the delimiter in \\\\( O_{\\\\text{ex}}^i = [o_{\\\\text{delim}}^{i-1}, O_i] \\\\):\\n\\n\\\\[\\nn \\\\prod_{i=1}^{O_1} p(O_{\\\\text{ex}}^i|O_{\\\\text{ex}}^1:i-1, \\\\theta) = \\\\sum_{h_{\\\\text{delim}}^{i-1} \\\\in D} p(O_i|h_{\\\\text{delim}}^{i-1}, \\\\theta)p(h_{\\\\text{delim}}^{i-1}|O_{\\\\text{ex}}^1:i-1, \\\\theta) \\\\approx n \\\\prod_{i=1}^{O_1} p(O_i|\\\\theta) \\\\tag{10}\\n\\\\]\\n\\nWhile summing over \\\\( H \\\\) above would be a trivial equality, we can replace \\\\( H \\\\) with the set of delimiter hidden states \\\\( D \\\\) since \\\\( p(h_{\\\\text{delim}}^{i-1}|O_{\\\\text{ex}}^1:i-1, \\\\theta) = 0 \\\\) for non-delimiter hidden states \\\\( h/\\\\in D \\\\) (Assumption 1). We used in the first equality that \\\\( O_{\\\\text{ex}}^1:i-1 \\\\rightarrow h_{\\\\text{delim}}^{i-1} \\\\rightarrow O_i \\\\) forms a Markov chain and \\\\( p(o_{\\\\text{delim}}^{i-1}|h_{\\\\text{delim}}^{i-1}) = 1 \\\\) (Assumption 1) to change \\\\( O_i \\\\) to \\\\( O_i \\\\).\\n\\nFinally, we can show using properties of delimiter hidden states (Assumption 2) that\\n\\n\\\\[\\np(h_{\\\\text{delim}}^{i-1}|O_{\\\\text{ex}}^1:i-1, \\\\theta) = O_1 \\\\text{ and } \\\\sum_{h_{\\\\text{delim}}^{i-1} \\\\in D} p(O_i|h_{\\\\text{delim}}^{i-1}, \\\\theta) \\\\approx O_1 p(O_i|\\\\theta)\\n\\\\]\\n\\nin the second step. Therefore, we can upper bound \\\\( r_n(\\\\theta) \\\\) as\\n\\n\\\\[\\nr_n(\\\\theta) \\\\leq \\\\frac{1}{n} (O_1 + n \\\\sum_{i=1}^{O_1} \\\\log p(O_i|\\\\theta)p(O_i|\\\\theta^*)) \\\\rightarrow O_1 + E_{O \\\\sim p_{\\\\text{prompt}}}[\\\\log p(O|\\\\theta)p(O|\\\\theta^*)]. \\\\tag{11}\\n\\\\]\\n\\nThe expectation term can be written as the difference of two KL divergences, \\\\( \\\\text{KL}(p_{\\\\text{prompt}}(O)|p(O|\\\\theta^*)) - \\\\text{KL}(p_{\\\\text{prompt}}(O)|p(O|\\\\theta)) \\\\). We bound the first KL term by a constant using Assumption 5 \u2014 intuitively for one example, \\\\( p_{\\\\text{prompt}} \\\\) and \\\\( p(\\\\cdot|\\\\theta^*) \\\\) are close. We break the second term into a sum of negative KL divergences over \\\\( k \\\\) tokens. There are \\\\( O(k) \\\\) KL terms and only \\\\( O(1) \\\\) other error terms, which come from the distribution mismatch between the prompt and pretraining distributions. If the KL terms are larger than the error terms, then \\\\( r_n(\\\\theta) \\\\) has a negative limit.\\n\\nIf this holds for all \\\\( \\\\theta \\\\neq \\\\theta^* \\\\), then we have\\n\\n\\\\[\\n\\\\exp(n \\\\cdot r_n(\\\\theta)) \\\\rightarrow 0 \\\\text{ for all } \\\\theta \\\\neq \\\\theta^*,\\n\\\\]\\n\\nenabling in-context learning.\\n\\n### 3.3 FORMAL RESULTS\\n\\n#### 3.3.1 IN-CONTEXT LEARNING UNDER DISTINGUISHABILITY\\n\\nWe define a distinguishability condition which formalizes when in-context learning occurs. Letting \\\\( p_j(\\\\theta)(o) := p(O[j] = o|O[1:j-1], \\\\theta) \\\\) be the output distribution of the \\\\( j \\\\)-th token given the previous tokens and \\\\( p_j(\\\\text{prompt})(o) := p_{\\\\text{prompt}}(O[j] = o|O[1:j-1]) \\\\) be the analogous distribution under the prompt.\"}"}
{"id": "RdJVFCHjUMI", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under Assumption 3, we show that the in-context predictor $f_{\\\\text{test}}(x) = \\\\text{argmax}_y p(y | S_n, x_{\\\\text{test}})$ converges when abstracting away the Bayesian inference component (the selection of $\\\\theta^* \\\\in \\\\Theta$) of the in-context predictor. We will complete the argument for the convergence of the in-context predictor in the proof of Theorem 1.\\n\\nLemma 1. Suppose the prompt $S_n$ and the test input $x_{\\\\text{test}}$ are given. Under Assumption 3, we show that the argmax of the averaged predictive distribution conditioned on $\\\\theta^*$ and a prompt $S_n$ is the same as the argmax of the prompt predictive distribution:\\n\\n$$\\\\text{argmax}_y \\\\sum_{h_{\\\\text{start test}} \\\\in H} p(y | x_{\\\\text{test}}, h_{\\\\text{start test}}, \\\\theta^*) p(h_{\\\\text{start test}} | S_n, x_{\\\\text{test}}, \\\\theta^*) = \\\\text{argmax}_y p_{\\\\text{prompt}}(y | x_{\\\\text{test}}).$$\\n\\nProof. First, we note by definition that $p_{\\\\text{prompt}}(y | x_{\\\\text{test}}) = \\\\sum_{h_{\\\\text{start test}} \\\\in H} p(y | x_{\\\\text{test}}, h_{\\\\text{start test}}, \\\\theta^*) p(h_{\\\\text{start test}} | x_{\\\\text{test}}, \\\\theta^*)$.\\n\\nExpanding the last term, we have $p(h_{\\\\text{start test}} | x_{\\\\text{test}}) \\\\propto p(x_{\\\\text{test}} | h_{\\\\text{start test}}, \\\\theta^*) p(h_{\\\\text{start test}} | S_n, \\\\theta^*)$.\\n\\nwhich is proportional to a constant in $x_{\\\\text{test}}$.\\n\\nOn the other hand, analyzing one term inside the LHS of the lemma statement, we have $p(h_{\\\\text{start test}} | S_n, x_{\\\\text{test}}, \\\\theta^*) \\\\propto p(x_{\\\\text{test}} | h_{\\\\text{start test}}, \\\\theta^*) p(h_{\\\\text{start test}} | S_n, \\\\theta^*)$.\\n\\nwhich is proportional to a constant in $x_{\\\\text{test}}$ and $S_n$. The quantities differ in the last term, which we expand below and put in matrix form. Let $T \\\\in \\\\mathbb{R}^{|H| \\\\times |D|}$ be the matrix that represents the transition probabilities starting from a delimiter state: $p(h_{\\\\text{start test}} | h_{\\\\text{delim}})$ for $h_{\\\\text{start test}} \\\\in H$ and $h_{\\\\text{delim}} \\\\in D$.\\n\\nAs a result, $p(h_{\\\\text{start test}} | S_n, \\\\theta^*) = \\\\sum_{h_{\\\\text{delim}} n} p(h_{\\\\text{start test}} | h_{\\\\text{delim}} n, \\\\theta^*) p(h_{\\\\text{delim}} n | S_n, \\\\theta^*)$.\\n\\nLet $W \\\\in \\\\mathbb{R}^{|Y| \\\\times |H|}$ be the matrix that represents the probabilities $p(y | x_{\\\\text{test}}, h_{\\\\text{start test}}, \\\\theta^*) p(x_{\\\\text{test}} | h_{\\\\text{start test}}, \\\\theta^*)$ for all the possible $y \\\\in Y$ and $h_{\\\\text{start test}} \\\\in H$. Overall, we can write\\n\\n$$\\\\sum_{h_{\\\\text{start test}} \\\\in H} p(\\\\cdot | x_{\\\\text{test}}, h_{\\\\text{start test}}, \\\\theta^*) p(h_{\\\\text{start test}} | S_n, x_{\\\\text{test}}, \\\\theta^*) = W^T v$$\\n\\n$p_{\\\\text{prompt}}(\\\\cdot | x_{\\\\text{test}}) = W u$.\\n\\nBounding the difference between the two predictive distributions,\\n\\n$$\\\\|W^T v - W u\\\\|_\\\\infty \\\\leq \\\\|W^T v - W u\\\\|_1 = |Y| \\\\sum_{i=1}^{|Y|} |W^T_i (v - u)|_i \\\\leq \\\\|W^T v - W u\\\\|_1 = |Y| \\\\sum_{i=1}^{|Y|} |H| \\\\sum_{j=1}^{|H|} W_{ij} (v_j - u_j) \\\\leq |Y| \\\\sum_{i=1}^{|Y|} |H| \\\\sum_{j=1}^{|H|} |W_{ij}| (v_j - u_j) \\\\leq \\\\|W^T v - W u\\\\|_1.$$\"}"}
{"id": "RdJVFCHjUMI", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Using Assumption 3, we can further bound this by\\n\\\\[ \\\\frac{\\\\Delta}{2}. \\\\]\\n\\n\\\\[ \\\\| T_v - u \\\\|_1 = 2 \\\\text{TV} \\\\left( p \\\\left( \\\\cdot | D \\\\right) \\\\right) \\\\sum_{i=1}^{|D|} v_i p \\\\left( \\\\cdot | h_{\\\\text{delim}=i, \\\\theta^*} \\\\right) \\\\] (45)\\n\\n\\\\[ \\\\leq 2 \\\\lambda \\\\sum_{i=1}^{|D|} v_i \\\\text{TV} \\\\left( p \\\\left( \\\\cdot | D \\\\right) \\\\right) \\\\] (convexity of TV distance) (46)\\n\\n\\\\[ \\\\leq 2 \\\\max_{h_{\\\\text{delim}} \\\\in D} \\\\text{TV} \\\\left( p \\\\left( \\\\cdot | h_{\\\\text{delim}} \\\\right) \\\\left| \\\\theta^* \\\\right. \\\\right) \\\\frac{\\\\Delta}{2}. \\\\] (47)\\n\\nSince the probability of any output does not change by more than \\\\( \\\\frac{\\\\Delta}{2} \\\\) and the margin between the most likely label and the second most likely label is \\\\( \\\\Delta \\\\), the argmax's are the same, showing the result.\\n\\n**Proof.**\\n\\nWe analyze the most likely prediction over the pretraining distribution conditioned on the prompt\\n\\n\\\\[ p \\\\left( y | S^n, x_{\\\\text{test}} \\\\right) = \\\\int \\\\theta p \\\\left( y | S^n, x_{\\\\text{test}}, \\\\theta \\\\right) p \\\\left( \\\\theta | S^n, x_{\\\\text{test}} \\\\right) d\\\\theta \\\\] (48)\\n\\n\\\\[ \\\\propto \\\\int \\\\theta p \\\\left( y | S^n, x_{\\\\text{test}}, \\\\theta \\\\right) p \\\\left( S^n, x_{\\\\text{test}} | \\\\theta \\\\right) p \\\\left( \\\\theta \\\\right) d\\\\theta \\\\] (49)\\n\\n\\\\[ \\\\propto \\\\int \\\\theta p \\\\left( y | S^n, x_{\\\\text{test}}, \\\\theta \\\\right) p \\\\left( S^n, x_{\\\\text{test}} | \\\\theta \\\\right) p \\\\left( S^n, x_{\\\\text{test}} | \\\\theta^* \\\\right) p \\\\left( \\\\theta \\\\right) d\\\\theta \\\\] (50)\\n\\n\\\\[ = \\\\int \\\\sum_{h_{\\\\text{start}} \\\\in H} p \\\\left( y | x_{\\\\text{test}}, h_{\\\\text{start}}, \\\\theta \\\\right) p \\\\left( h_{\\\\text{start}} \\\\left| S^n, x_{\\\\text{test}} \\\\right. \\\\theta \\\\right) p \\\\left( S^n, x_{\\\\text{test}} \\\\right| \\\\theta \\\\right) p \\\\left( S^n, x_{\\\\text{test}} | \\\\theta^* \\\\right) p \\\\left( \\\\theta \\\\right) d\\\\theta \\\\] (51)\\n\\nDefining the following quantity,\\n\\n\\\\[ r_n (\\\\theta) = \\\\frac{1}{n} \\\\log p \\\\left( S^n, x_{\\\\text{test}} \\\\right| \\\\theta \\\\right) p \\\\left( S^n, x_{\\\\text{test}} | \\\\theta^* \\\\right). \\\\] (52)\\n\\nwe will show that under distinguishability for all \\\\( \\\\theta \\\\neq \\\\theta^* \\\\), \\\\( r_n (\\\\theta) \\\\) converges to a negative constant such that\\n\\n\\\\[ p \\\\left( S^n, x_{\\\\text{test}} \\\\right| \\\\theta \\\\right) p \\\\left( S^n, x_{\\\\text{test}} | \\\\theta^* \\\\right) = \\\\exp \\\\left( n \\\\cdot r_n (\\\\theta) \\\\right) \\\\rightarrow 0 \\\\] (53)\\n\\nfor \\\\( \\\\theta \\\\neq \\\\theta^* \\\\), whereas this ratio is always 1 for \\\\( \\\\theta = \\\\theta^* \\\\). This will then \\\"select\\\" the desired prompt concept through marginalization.\\n\\nSupposing that Equation 53 holds, we show that the theorem statement holds. Let\\n\\n\\\\[ \\\\Delta' = \\\\max_{h_{\\\\text{delim}} \\\\in D} \\\\text{TV} \\\\left( p \\\\left( \\\\cdot | h_{\\\\text{delim}} \\\\right) \\\\left| \\\\theta^* \\\\right. \\\\right), \\\\] (54)\\n\\nand let \\\\( \\\\epsilon < \\\\left( \\\\frac{\\\\Delta}{2} - \\\\Delta' \\\\right) p \\\\left( \\\\theta^* \\\\right) \\\\). Then for \\\\( n \\\\) large enough (due to Equation 53),\\n\\n\\\\[ \\\\int \\\\sum_{h_{\\\\text{start}} \\\\in H} p \\\\left( y | x_{\\\\text{test}}, h_{\\\\text{start}}, \\\\theta \\\\right) p \\\\left( h_{\\\\text{start}} \\\\left| S^n, x_{\\\\text{test}} \\\\right. \\\\theta \\\\right) p \\\\left( S^n, x_{\\\\text{test}} \\\\right| \\\\theta \\\\right) p \\\\left( S^n, x_{\\\\text{test}} | \\\\theta^* \\\\right) p \\\\left( \\\\theta \\\\right) d\\\\theta \\\\] (55)\\n\\n\\\\[ \\\\propto \\\\sum_{h_{\\\\text{start}} \\\\in H} p \\\\left( y | x_{\\\\text{test}}, h_{\\\\text{start}}, \\\\theta^* \\\\right) p \\\\left( h_{\\\\text{start}} \\\\left| S^n, x_{\\\\text{test}} \\\\right. \\\\theta^* \\\\right) p \\\\left( \\\\theta^* \\\\right) + \\\\int \\\\frac{\\\\epsilon}{2} p \\\\left( \\\\theta \\\\right) d\\\\theta \\\\] (56)\\n\\n\\\\[ \\\\propto \\\\sum_{h_{\\\\text{start}} \\\\in H} p \\\\left( y | x_{\\\\text{test}}, h_{\\\\text{start}}, \\\\theta^* \\\\right) p \\\\left( h_{\\\\text{start}} \\\\left| S^n, x_{\\\\text{test}} \\\\right. \\\\theta^* \\\\right) p \\\\left( \\\\theta^* \\\\right) \\\\]\\n\\nwhere \\\\( \\\\epsilon \\\\theta (y) \\\\leq \\\\epsilon / 2 \\\\) for all \\\\( y \\\\in Y \\\\).\\n\\nBy Lemma 1, the argmax of the first term of Equation 57 is the same as\\n\\n\\\\[ \\\\text{argmax} \\\\left( y \\\\left| p \\\\text{prompt} \\\\left( y | x_{\\\\text{test}} \\\\right) \\\\right. \\\\right), \\\\]\\n\\nwhere the margin between the most likely label and the second most likely is at least \\\\( \\\\Delta / 2 - \\\\Delta' \\\\). Since\\n\\n\\\\[ p \\\\left( \\\\theta^* \\\\right) \\\\int \\\\frac{\\\\epsilon}{2} p \\\\left( \\\\theta \\\\right) d\\\\theta \\\\leq \\\\epsilon \\\\frac{\\\\Delta}{2} / 2 \\\\] (58)\"}"}
{"id": "RdJVFCHjUMI", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We restrict our attention to a delimiter line, the sum can must be over the set of delimiter states $h$ probability of $c$ in the above steps, we used both Propositions 1 and 2 in the terms involving $r$. Expanding the numerator of the ratio in $O$ also see that that observing a delimiter $\\\\text{argmax}$ of Equation 57 is also the same as $y$ for all $\\\\in Y$. Published as a conference paper at ICLR 2022...\"}"}
{"id": "RdJVFCHjUMI", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Focusing on the numerator of the ratio term and summing over the start hidden state for the $i$-th example,\\n\\n$$\\\\sum_{h_{\\\\text{delim}}^{i-1} \\\\in D} p(O_{i} | h_{\\\\text{delim}}^{i-1}, \\\\theta) = \\\\sum_{h_{\\\\text{delim}}^{i-1} \\\\in D} \\\\sum_{h_{\\\\text{start}}^{i}} p(O_{i} | h_{\\\\text{start}}^{i}, \\\\theta) p(h_{\\\\text{start}}^{i} | h_{\\\\text{delim}}^{i-1}, \\\\theta) \\\\quad (70)$$\\n\\n$$= \\\\sum_{h_{\\\\text{start}}^{i}} p(O_{i} | h_{\\\\text{start}}^{i}, \\\\theta) p(h_{\\\\text{start}}^{i} | \\\\theta) \\\\sum_{h_{\\\\text{delim}}^{i-1} \\\\in D} p(h_{\\\\text{start}}^{i} | h_{\\\\text{delim}}^{i-1}, \\\\theta) p(h_{\\\\text{start}}^{i} | \\\\theta) \\\\quad (71)$$\\n\\n$$= \\\\sum_{h_{\\\\text{start}}^{i}} p(O_{i} | h_{\\\\text{start}}^{i}, \\\\theta) p(h_{\\\\text{start}}^{i} | \\\\theta) \\\\sum_{h_{\\\\text{delim}}^{i-1} \\\\in D} p(h_{\\\\text{start}}^{i} | h_{\\\\text{delim}}^{i-1}, \\\\theta) p(h_{\\\\text{start}}^{i} | \\\\theta) \\\\quad (72)$$\\n\\nwhere the last step applies Bayes' rule. We can lower and upper bound the following quantity for any $\\\\theta$ using Assumption 2:\\n\\n$$p(h_{\\\\text{delim}}^{i-1} | h_{\\\\text{start}}^{i}, \\\\theta) p(h_{\\\\text{delim}}^{i-1} | \\\\theta) \\\\leq p(h_{\\\\text{delim}}^{i-1} | h_{\\\\text{start}}^{i}, \\\\theta) c_{3} \\\\quad (73)$$\\n\\n$$p(h_{\\\\text{delim}}^{i-1} | h_{\\\\text{start}}^{i}, \\\\theta) p(h_{\\\\text{delim}}^{i-1} | \\\\theta) \\\\geq p(h_{\\\\text{delim}}^{i-1} | h_{\\\\text{start}}^{i}, \\\\theta) c_{4} \\\\quad (74)$$\\n\\nThis implies that\\n\\n$$\\\\sum_{h_{\\\\text{delim}}^{i-1} \\\\in D} p(h_{\\\\text{delim}}^{i-1} | h_{\\\\text{start}}^{i}, \\\\theta) p(h_{\\\\text{delim}}^{i-1} | \\\\theta) \\\\leq 1 c_{3} \\\\quad (75)$$\\n\\n$$\\\\sum_{h_{\\\\text{delim}}^{i-1} \\\\in D} p(h_{\\\\text{delim}}^{i-1} | h_{\\\\text{start}}^{i}, \\\\theta) p(h_{\\\\text{delim}}^{i-1} | \\\\theta) \\\\geq 1 c_{4} \\\\quad (76)$$\\n\\nPlugging in these bounds, we have\\n\\n$$r_{n}(\\\\theta) \\\\leq \\\\frac{1}{n} \\\\left( -\\\\log(c_{7}) + 2 n \\\\left( \\\\log(c_{2}) - \\\\log(c_{1}) \\\\right) + n \\\\left( \\\\log(c_{4}) - \\\\log(c_{3}) \\\\right) + n \\\\sum_{i=1}^{n} \\\\log \\\\frac{1}{c_{3}} \\\\right) \\\\quad (77)$$\\n\\n$$= \\\\frac{1}{n} \\\\left( -\\\\log(c_{7}) + 2 n \\\\left( \\\\log(c_{2}) - \\\\log(c_{1}) \\\\right) + n \\\\left( \\\\log(c_{4}) - \\\\log(c_{3}) \\\\right) + n \\\\sum_{i=1}^{n} \\\\log \\\\frac{1}{c_{3}} \\\\right) \\\\quad (78)$$\\n\\n$$\\\\Rightarrow n \\\\to \\\\infty \\\\quad E_{O \\\\sim \\\\text{prompt}} \\\\left[ \\\\log p(O | \\\\theta) p(O | \\\\theta^{*}) \\\\right] + \\\\epsilon_{\\\\theta_{\\\\text{delim}}} \\\\quad (79)$$\\n\\nwhere we set\\n\\n$$\\\\epsilon_{\\\\theta_{\\\\text{delim}}} = 2 \\\\left( \\\\log(c_{2}) - \\\\log(c_{1}) \\\\right) + \\\\log(c_{4}) - \\\\log(c_{3}) \\\\quad (80)$$\\n\\nNext, we convert the expectation in the bound into a KL divergence. We have\\n\\n$$E_{O \\\\sim \\\\text{prompt}} \\\\left[ \\\\log p(O | \\\\theta) p(O | \\\\theta^{*}) \\\\right] = E_{O \\\\sim \\\\text{prompt}} \\\\left[ \\\\log p(O | \\\\theta) p(O | \\\\theta^{*}) + \\\\log p(O | \\\\theta^{*}) \\\\right] \\\\quad (81)$$\\n\\n$$= \\\\text{KL}(p_{\\\\text{prompt}} \\\\parallel p(\\\\cdot | \\\\theta^{*})) - \\\\text{KL}(p_{\\\\text{prompt}} \\\\parallel p(\\\\cdot | \\\\theta)) \\\\quad (82)$$\\n\\nWe will upper bound the first KL term:\\n\\n$$\\\\text{KL}(p_{\\\\text{prompt}} \\\\parallel p(\\\\cdot | \\\\theta^{*})) = E_{O \\\\sim \\\\text{prompt}} \\\\left[ \\\\log p_{\\\\text{prompt}}(O) \\\\right] \\\\quad (83)$$\\n\\nExpanding the numerator and denominator of the ratio inside, we have\\n\\n$$p_{\\\\text{prompt}}(O) = \\\\sum_{H} p_{\\\\text{prompt}}(H[1]) p(O[1] | H[1], \\\\theta^{*}) k \\\\prod_{j=2}^{H} p(O[j] | H[j], \\\\theta^{*}) p(H[j] | H[j-1], \\\\theta^{*}) \\\\quad (84)$$\\n\\n$$p(O | \\\\theta^{*}) = \\\\sum_{H} p(H[1] | \\\\theta^{*}) p(O[1] | H[1], \\\\theta^{*}) k \\\\prod_{j=2}^{H} p(O[j] | H[j], \\\\theta^{*}) p(H[j] | H[j-1], \\\\theta^{*}) \\\\quad (85)$$\\n\\n17\"}"}
{"id": "RdJVFCHjUMI", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Using Assumption 5, we have that $p(h|\\\\theta^*) \\\\geq c_8$ for any $h \\\\in H$, which implies that $p_{\\\\text{prompt}}(h) p(h|\\\\theta^*) \\\\leq 1/c_8$ (86).\\n\\nFinally, this implies that the KL term is bounded as $\\\\text{KL}(p_{\\\\text{prompt}} \\\\parallel p(\\\\cdot|\\\\theta^*)) \\\\leq -\\\\log(c_8)$ (88).\\n\\nAiming to decompose the second KL term into a sum over the $k$ tokens, we write $p_j^{\\\\theta}(o) = p(O[j]=o|O[1:j-1],\\\\theta)$ and $p_j^{\\\\text{prompt}}(o) = p_{\\\\text{prompt}}(O[j]=o|O[1:j-1])$. We have\\n\\n$$-\\\\text{KL}(p_{\\\\text{prompt}} \\\\parallel p(\\\\cdot|\\\\theta^*)) = -\\\\sum_O p_{\\\\text{prompt}}(O) \\\\log p_{\\\\text{prompt}}(O)/p(O|\\\\theta^*) (89)$$\\n\\n$$= -\\\\sum_O p_{\\\\text{prompt}}(O) \\\\sum_j = 1^k p_{\\\\text{prompt}}(O[j]|O[1:j-1]) \\\\log p_{\\\\text{prompt}}(O[j]|O[1:j-1])/p(O[j]|O[1:j-1],\\\\theta) \\\\quad \\\\text{(90)}$$\\n\\n$$= -k \\\\sum_j = 1^k \\\\mathbb{E}_{O[1:j-1] \\\\sim p_{\\\\text{prompt}}[1:j-1]} \\\\text{KL}(p_j^{\\\\text{prompt}} \\\\parallel p_j^{\\\\theta}) \\\\quad \\\\text{(92)}$$\\n\\nThen we have that $\\\\lim_{n \\\\to \\\\infty} r_n(\\\\theta) < -k \\\\sum_j = 1^k \\\\mathbb{E}_{O[1:j-1] \\\\sim p_{\\\\text{prompt}}[1:j-1]} \\\\text{KL}(p_j^{\\\\text{prompt}} \\\\parallel p_j^{\\\\theta}) + \\\\epsilon_{\\\\theta_{\\\\text{start}}} + \\\\epsilon_{\\\\theta_{\\\\text{delim}}}$ (93).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\cdot r_n(\\\\theta)) = 0$ (94).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\cdot r_n(\\\\theta)) = 0$ (94).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\cdot r_n(\\\\theta)) = 0$ (94).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\cdot r_n(\\\\theta)) = 0$ (94).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\cdot r_n(\\\\theta)) = 0$ (94).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\cdot r_n(\\\\theta)) = 0$ (94).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\cdot r_n(\\\\theta)) = 0$ (94).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\cdot r_n(\\\\theta)) = 0$ (94).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\cdot r_n(\\\\theta)) = 0$ (94).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\cdot r_n(\\\\theta)) = 0$ (94).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\cdot r_n(\\\\theta)) = 0$ (94).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\cdot r_n(\\\\theta)) = 0$ (94).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\cdot r_n(\\\\theta)) = 0$ (94).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\cdot r_n(\\\\theta)) = 0$ (94).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\cdot r_n(\\\\theta)) = 0$ (94).\\n\\nThe second term (set $\\\\epsilon_{\\\\theta_{\\\\text{start}}} = \\\\log(1/c_8)$) is an error term that depends on how different the starting prompt distribution (which is part of $p_{\\\\text{prompt}}$) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as $k$ becomes larger, the number of observations of $\\\\theta^*$ \u201coverpowers\u201d the distracting transitions in the prompt distribution. This condition is equivalent to the distinguishability condition (Condition 1).\\n\\nBy assumption, for $\\\\theta \\\\neq \\\\theta^*$ the Condition 1 holds, and thus $\\\\lim_{n \\\\to \\\\infty} p(S_n,x_{\\\\text{test}}|\\\\theta) = \\\\lim_{n \\\\to \\\\infty} \\\\exp(n \\\\"}
{"id": "RdJVFCHjUMI", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma 2. Let the set of $\\\\theta$ which does not satisfy Condition 1 to be $B$. Assume that $KL(p_{\\\\text{prompt}}(y_{\\\\text{test}}|x_{\\\\text{test}}) \\\\parallel p(y_{\\\\text{test}}|x_{\\\\text{test}}, \\\\theta)$ is bounded above for all $\\\\theta$ and that $\\\\theta^*$ minimizes the multiclass logistic risk $L_{CE}(\\\\theta) = -E_{x_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}}[p_{\\\\text{prompt}}(y_{\\\\text{test}}|x_{\\\\text{test}}) \\\\log p(y_{\\\\text{test}}|x_{\\\\text{test}}, \\\\theta)]$. If $E_{x_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}}[KL(p_{\\\\text{prompt}}(y_{\\\\text{test}}|x_{\\\\text{test}}) \\\\parallel p(y_{\\\\text{test}}|x_{\\\\text{test}}, \\\\theta)] \\\\leq \\\\epsilon_{\\\\theta}$ for all $\\\\theta \\\\in B$, then\\n\\n$$\\\\lim_{n \\\\to \\\\infty} L_{0-1}(f_n) \\\\leq \\\\inf_{f} L_{0-1}(f) + g - 1 \\\\sup_{\\\\theta \\\\in B} \\\\epsilon_{\\\\theta}$$\\n\\nwhere $g(\\\\delta) = \\\\frac{1}{2}((1 - \\\\delta) \\\\log(1 - \\\\delta) + (1 + \\\\delta) \\\\log(1 + \\\\delta))$ is a calibration function for the multiclass logistic loss for $\\\\delta \\\\in [0, 1]$.\\n\\nProof. First, we note that we can study the 0-1 risk of the limiting predictor:\\n\\n$$\\\\lim_{n \\\\to \\\\infty} L_{0-1}(f_n) = \\\\lim_{n \\\\to \\\\infty} E_{x_{\\\\text{test}}, y_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}}[1[f_n(x_{\\\\text{test}}) \\\\neq y_{\\\\text{test}}]]$$\\n\\n$$= E_{x_{\\\\text{test}}, y_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}}[1[\\\\lim_{n \\\\to \\\\infty} f_n(x_{\\\\text{test}}) \\\\neq y_{\\\\text{test}}]]$$\\n\\n(dominated convergence, boundedness of indicator)\\n\\n$$= E_{x_{\\\\text{test}}, y_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}}[1[\\\\lim_{n \\\\to \\\\infty} f_n(x_{\\\\text{test}}) \\\\neq y_{\\\\text{test}}]]$$\\n\\n(100)\\n\\nwhere in the last step we use that since the output space of $f_n$ is discrete and the probabilities that the in-context predictor takes an argmax over converges, then for $N$ large enough, $f_N(x_{\\\\text{test}}) = \\\\lim_{n \\\\to \\\\infty} f_n(x_{\\\\text{test}})$.\\n\\nNote that for every input $x_{\\\\text{test}}$, the limiting in-context learning predictor outputs the argmax of a predictive distribution which can be a mixture of predictive distributions over $B$:\\n\\n$$\\\\lim_{n \\\\to \\\\infty} f_n(x_{\\\\text{test}}) = \\\\arg\\\\max_y E_{\\\\theta \\\\sim q}[p(y|x_{\\\\text{test}}, \\\\theta)]$$\\n\\nfor some distribution $q$ over $B$. The KL divergence between this mixture and the prompt concept is bounded by the KL divergence of any one $\\\\theta \\\\in B$, due to the convexity of KL:\\n\\n$$E_{x_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}}[KL(p_{\\\\text{prompt}}(y_{\\\\text{test}}|x_{\\\\text{test}}) \\\\parallel E_{\\\\theta \\\\sim q}[p(y|x_{\\\\text{test}}, \\\\theta)])] \\\\leq E_{x_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}}[KL(p_{\\\\text{prompt}}(y_{\\\\text{test}}|x_{\\\\text{test}}) \\\\parallel p(y|x_{\\\\text{test}}, \\\\theta))]$$\\n\\n(102)\\n\\n$$= E_{\\\\theta \\\\sim q}[E_{x_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}}[KL(p_{\\\\text{prompt}}(y_{\\\\text{test}}|x_{\\\\text{test}}) \\\\parallel p(y|x_{\\\\text{test}}, \\\\theta))]]$$\\n\\n(103)\\n\\n$$\\\\leq \\\\sup_{\\\\theta \\\\in B} E_{x_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}}[KL(p_{\\\\text{prompt}}(y_{\\\\text{test}}|x_{\\\\text{test}}) \\\\parallel p(y|x_{\\\\text{test}}, \\\\theta))]]$$\\n\\n(104)\\n\\nwhere we can exchange the order of expectations since the KL is bounded (dominated convergence).\\n\\nFrom the KL bound $KL(p_{\\\\text{prompt}}(y_{\\\\text{test}}|x_{\\\\text{test}}) \\\\parallel p(y_{\\\\text{test}}|x_{\\\\text{test}}, \\\\theta)$, we thus have\\n\\n$$E_{x_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}}[KL(p_{\\\\text{prompt}}(y_{\\\\text{test}}|x_{\\\\text{test}}) \\\\parallel p(y_{\\\\text{test}}|x_{\\\\text{test}}, \\\\theta))]] = L_{CE}(\\\\theta) - L_{CE}(\\\\theta^*) \\\\leq \\\\sup_{\\\\theta \\\\in B} \\\\epsilon_{\\\\theta}$$\\n\\n(106)\\n\\nwhere $L_{CE}(\\\\theta) = -E_{x_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}}[p_{\\\\text{prompt}}(y_{\\\\text{test}}|x_{\\\\text{test}}) \\\\log p(y_{\\\\text{test}}|x_{\\\\text{test}}, \\\\theta)]$ is the multiclass logistic risk, and $L_{CE}(\\\\theta^*)$ is the optimal risk over $\\\\theta \\\\in \\\\Theta$ by assumption. Applying Theorem 2.2 and 5.11 of \u00b4Avila Pires & Szepesv\u00b4ari (2016), $g$ is a calibration function for the multiclass logistic loss, and allows us to convert the surrogate risk bound to a bound on the 0-1 loss, giving the result. Note that we have zero approximation error here, since $\\\\theta^* \\\\in \\\\Theta$.\\n\\nNote that $g - 1$ is roughly linear in $\\\\epsilon$ for $\\\\epsilon$ smaller than 0.7, where the bound is non-vacuous.\"}"}
{"id": "RdJVFCHjUMI", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proof. By the continuity assumption, we have for any $\\\\theta$ in $B$ that\\n\\\\[\\n\\\\sum_{j=2}^{k} KL_{j}(\\\\theta^*) \\\\geq \\\\frac{1}{2} \\\\sum_{j=2}^{k} (\\\\theta - \\\\theta^*)^\\\\top I_{j,\\\\theta^*} (\\\\theta - \\\\theta^*) + O(\\\\|\\\\theta - \\\\theta^*\\\\|_3)\\n\\\\]\\n(107)\\n\\\\[\\n\\\\geq \\\\frac{1}{2} (k-1) \\\\lambda_{\\\\text{min}} (I_{j,\\\\theta^*}) \\\\|\\\\theta - \\\\theta^*\\\\|_2\\n\\\\]\\n(108)\\n\\\\[\\n\\\\Rightarrow \\\\|\\\\theta - \\\\theta^*\\\\|_2 \\\\leq \\\\epsilon_{\\\\theta^*} + \\\\epsilon_{\\\\theta^*}\\\\text{delim}.\\n\\\\]\\n(109)\\nWe use this to bound the last KL term by plugging it in below:\\n\\\\[\\nKL_{k}(\\\\theta^*) = \\\\frac{1}{2} (\\\\theta - \\\\theta^*)^\\\\top I_{j,\\\\theta^*} (\\\\theta - \\\\theta^*) + O(\\\\|\\\\theta - \\\\theta^*\\\\|_3)\\n\\\\]\\n(110)\\n\\\\[\\n\\\\leq \\\\frac{1}{2} (\\\\epsilon_{\\\\theta^*} + \\\\epsilon_{\\\\theta^*}\\\\text{delim}) (\\\\lambda_{\\\\text{max}} (I_{j,\\\\theta^*}) + O(1))\\n\\\\]\\n(111)\\n\\\\[\\n\\\\leq (\\\\epsilon_{\\\\theta^*} + \\\\epsilon_{\\\\theta^*}\\\\text{delim}) (\\\\lambda_{\\\\text{max}} (I_{j,\\\\theta^*}) + O(1))\\n\\\\]\\n(112)\\nRearranging and noting that $KL_{k}(\\\\theta^*) = E_{x_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}} [KL(p_{\\\\text{prompt}}(y_{\\\\text{test}}|x_{\\\\text{test}}) \\\\| p(y_{\\\\text{test}}|x_{\\\\text{test}},\\\\theta))]$, we have\\n\\\\[\\nE_{x_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}} [KL(p_{\\\\text{prompt}}(y_{\\\\text{test}}|x_{\\\\text{test}}) \\\\| p(y_{\\\\text{test}}|x_{\\\\text{test}},\\\\theta))] \\\\leq (\\\\epsilon_{\\\\theta^*} + \\\\epsilon_{\\\\theta^*}\\\\text{delim}) (\\\\lambda_{\\\\text{max}} (I_{j,\\\\theta^*}) + O(1))\\n\\\\]\\n(113)\\nPlugging into Lemma 2 gives the result.\\n\\nE.2 PROOF OF THEOREM 3\\nNote that Condition 1 ensures that the sum of KL divergences between positions within a $k$-length input is bounded. This means that we have a bound over not only the last-position KL divergence, but also for all the intermediate tokens. Intuitively, the random length test example allows the in-context predictor to \u201ctake credit\u201d for fitting the intermediate tokens. The proof is immediate given the KL bound and Lemma 2, given that the length of $x_{\\\\text{test}}$ is uniformly random between 2 to $k$.\\n\\nProof. Let the set of $\\\\theta$ that does not satisfy Condition 1 to be $B$. We have for any $\\\\theta$ in $B$ that\\n\\\\[\\nE_{x_{\\\\text{test}} \\\\sim p_{\\\\text{prompt}}} [KL(p_{\\\\text{prompt}}(y_{\\\\text{test}}|x_{\\\\text{test}}) \\\\| p(y_{\\\\text{test}}|x_{\\\\text{test}},\\\\theta))] \\\\leq \\\\frac{1}{k-1} \\\\sum_{j=1}^{k-1} E_{O[j] \\\\sim p_{\\\\text{prompt}}} KL(O[j] \\\\| O[1:j-1],\\\\theta)\\n\\\\]\\n(114)\\n\\\\[\\n\\\\leq \\\\sup_{\\\\theta} (\\\\epsilon_{\\\\theta^*} + \\\\epsilon_{\\\\theta^*}\\\\text{delim}) (k-1)\\n\\\\]\\n(115)\\nby Theorem 1 and Condition 1. Plugging this into Lemma 2 gives the result.\\n\\nF. EXPERIMENTAL DETAILS\\nF.1 GINC DATASET\\nPretraining distribution. We consider a pretraining distribution from a mixture of HMMs with an interpretable hidden state structure and emission distribution. The HMM hidden state $h_t = [s_t, v_t]$ at time $t$ is composed of an entity $v_t \\\\in \\\\{1, \\\\ldots, |V|\\\\}$ (e.g., Einstein) and a property $s_t \\\\in \\\\{1, \\\\ldots, |S|\\\\}$ (e.g., nationality, first name, last name, other grammatical tokens). We model the entities and properties as independent Markov chains (i.e., a factorial HMM (Ghahramani & Jordan, 1997)), while the emissions depend on both. In pretraining documents, we expect that the entities (e.g., Einstein) change slowly over time while the properties of the entity (e.g., their nationality) change quickly with some pattern to generate\"}"}
{"id": "RdJVFCHjUMI", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Example pretraining document snippet (Left) and example prompt with 3 training examples, 1 test example, and example length 3 (Right). The delimiter token is the backslash.\\n\\nFigure 9: The GINC dataset generates sequences from a mixture of HMMs. The HMM hidden states consist of entities ($v$) and properties ($s$), which index into a memory matrix to produce the observed token. The entity and property sequences are sampled from independent Markov chains. The concept parameter $\\\\theta$ is the transition matrix for properties, which defines relations between properties. In this example, the sequence of properties $[2,3,5,4]$ relates names to nationalities, defining the in-context task. The blue color represents hidden states/observations sampled from the prompt distribution, and the purple color represents hidden states/observations sampled from the pretraining distribution.\\n\\nNatural sentences. We implement this by ensuring that the probability of transitioning to the same entity index in the next step is at least 0.9. The emission distribution depends on a memory matrix $M$ with $|V|$ rows and $|S|$ columns (Figure 9). At step $t$, we use the entity $v_t$ and property $s_t$ to index into the memory matrix. In particular, the observed tokens are deterministic with $p(o|h) = 1$ if $o = M[v_t, s_t]$. This construction satisfies the structure on delimiter states (Assumption 1). We ensure that all the transitions have nonzero probability and use a uniform prior over concepts, satisfying Assumptions 2 and 5.\\n\\nConcept parameter. The concept parameter is the property transition matrix, while the entity transition matrix is fixed for all concepts. The prompt start distribution and the concept together determine the in-context task. We define a uniform mixture of HMMs over a family $\\\\Theta$ of 5 concepts to generate 1000 documents with $\\\\sim 10$ million tokens total.\\n\\nVocabulary. The GINC dataset is generated from a mixture of HMMs. These HMMs output tokens from a vocabulary of size in $\\\\{50, 100, 150\\\\}$. The vocabulary contains a special delimiter token (backslash).\"}"}
