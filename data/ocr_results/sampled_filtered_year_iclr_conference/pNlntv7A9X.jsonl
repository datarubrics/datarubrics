{"id": "pNlntv7A9X", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "pNlntv7A9X", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This appendix aims to provide more details and qualitative examples to support our claim in the main paper.\\n\\nA.1 DATASET DETAILS\\n\\nFor each simulation trial, we produce two primary sets of data: sensor output and semantic annotation. The sensor output provides a comprehensive 4D state description of objects at various levels. In contrast, the semantic annotation contains pre-processed data designed to facilitate the question generation phase.\\n\\nA.1.1 SENSOR DATA STRUCTURE\\n\\nWithin the simulation pipeline, we produce sensor data across multiple modalities listed in Fig. 4, including RGB-rendered images in Full HD (1920x1080) resolution, object-level data (encompassing bounding boxes, segmentations, positions, rotations, and scales), point-level data (comprising meshes and particles), and event-level data (detailing collision or touch events). The generated meshes illustrate the sampled surface shapes of both rigid and soft objects, prepared for subsequent voxelization. Unlike the other two scenarios, the fluid and rope scenarios necessitate the re-sampling of meshes in every individual frame. This results in temporal independence for the vertices. Yet, within this context, particle outputs signify tracked points on the objects, preserving correlations between successive frames. Given that voxel data (which is temporally invariant) is derived from the voxelization of meshes, the dataset offers both temporally correlated and independent 4D data.\\n\\nFigure 4: Sensor data outputs are multimodal, depicting the 4D states of objects across various levels, ranging from object-level, point-level to event-level.\\n\\nA.1.2 ANNOTATION DATA STRUCTURE\\n\\nFor each scenario, we produce comprehensive annotation data that includes camera extrinsics/intrinsics, sampled parameters, and properties of the sampled objects and layouts. Additionally,\"}"}
{"id": "pNlntv7A9X", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"post-processed simulation data from both the pre-simulation and post-simulation stages are documented. To be specific:\\n\\nFluid. Object details such as name, color, and transforms are stored. For fluid objects, properties like densities, viscosity, surface tension, and emitted positions are added. Particle statistics in each container, collision statistics on each stick, and collision paths for each particle are recorded for both pre- and post-simulation stages, and are meticulously categorized by fluid types.\\n\\nRope. Fundamental elements of each pulley group, such as pulley, rope, fixed endpoint, cube, and sphere, are outlined at both the individual rope and group levels. A group refers to a collection of objects with interdependent mechanics, like two sets of objects on ropes connected to a specific movable pulley. Initial properties such as mass, color, shape, mobility, pose, and subsequent simulation results like motion direction of movable objects and tension in rope segments are annotated.\\n\\nCloth. Sampled cloth properties\u2014stretching compliance, bending compliance, and friction level\u2014are provided. Basic properties of each rigid object and their simulation results, which include object-cloth and object-object collision events, contact relationships, and tension values in the cloth\u2019s final frame, are stored.\\n\\nBall. The framework documents sampled properties of all rigid bodies and soft balls. For plastoelectric balls, simulation results, including the pits they settle into, are captured.\\n\\nIn this section, we visualize the distribution of question types in fluid, cloth, and ball scenarios in Figure 5, 6, and 7. Rope scenarios results are in Figure 3. We balance the number of each question type.\\n\\nA.2.2 QUESTION TEMPLATES\\n\\nIn this section, we show all of our question templates from four scenarios in Table 4, 5, 6, and 7. We define all the symbols we use in Table 8. When generating questions using templates and symbols, we balance the distribution and frequency of each symbol and answer to avoid language bias.\"}"}
{"id": "pNlntv7A9X", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We introduce Continnuum Physical Reasoning Oracle Model, ContPRO, which marries physical-based dynamics models with the recent large language models which enjoy the advantages of both models, precise dynamic predictions, and interpretable reasoning. We discuss our model framework in A.3.1, as well as the large language model backbone, the visual perception model, and the physical simulation model. We also present the representative results and analysis in A.3.2.\\n\\nA.3.1 MODELS\\n\\nInspired by prior works (Yi et al., 2018; Chen et al., 2022; Yi et al., 2020), we propose an oracle neural-symbolic framework named ContPRO. As the rapid development of large language models and their applications, we replace previous rule-based neural executor with the newest neuro-symbolic large language models. Furthermore, our oracle model is equipped with a state-of-the-art differentiable physical simulator and the popular visual perception model, MASK R-CNN (He et al., 2017). This physical simulator uses a point cloud as initial input, which is hard to perceive directly from videos. That accounts for the reason that we call it oracle model.\\n\\nLarge Language Model Backbone.\\n\\nRecent years, we have witnessed a significant breakthrough in large language models. Works (Sur\u00eds et al., 2023; Gupta & Kembhavi, 2023) introduce neural-\"}"}
{"id": "pNlntv7A9X", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The figure presents samples from the four puzzle blocks of our Continuum Physical Dataset (ContPhy). ContPhy offers rendered outputs from the simulation of randomly sampled scenarios, accompanied by their respective question-answer pairs. These pairs span from understanding soft-body physical properties, concepts and interactions with rigid objects through comparative analysis, to temporal and spatial dynamic predictions, counterfactual considerations, and goal-oriented problem-solving. It aims to provide a comprehensive resource for AI models to interpret the physical world of various deformable bodies.\\n\\n3.1 P HYSICAL DYNAMIC SIMULATION\\n\\nWe used the Unity engine (Haas, 2014) to simulate and render our physical scenes due to its effectiveness and efficiency in simulating physical effects. We design four physical scenes to study different physical phenomena across different object materials with varying physical properties, masses, friction, elasticity, density, deformability, and stretchiness.\"}"}
{"id": "pNlntv7A9X", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"behaviors. Using the Unity engine, we are able to combine various physical properties compositionally to generate scenes that require a deeper understanding of the physical scenes. For example, as in Fig 1 (c), with the coupling of deformable ropes and pulley systems, we can lift up the cube on the right with less pulling force on the rope.\\n\\nVarious Physical Properties.\\n\\nOne key feature that distinguishes the proposed ContPhy dataset from existing benchmarks like CLEVRER (Yi et al., 2020) and Physion (Bear et al.) is that our proposed benchmark varies the values used for physical parameters like masses, density, elasticity, and stretchiness. Such variation in the values will lead the dynamics to different future states. As such we can generate a richer dataset that covers a larger variance in object physical parameters. For example, the density of the liquid in Fig. 1 (a) will decide where the liquid stays, and the stretchiness of the cloth in Fig. 1 (b) will determine how the objects will contact with the cloth.\\n\\nSimulation Setup.\\n\\nIn the context of video and annotation generation, we follow a bottom-up approach, involving the following sequential steps: a) Sampling: we begin by randomly selecting scene layouts, camera parameters, and initial conditions to create a diverse set of scenarios. b) Initialization: objects are placed within the scene based on the sampled layout. c) Pre-simulation: a preliminary simulation is conducted to evaluate whether the obtained simulation results align with the expected data distribution. d) Rendering: high-quality videos are generated by configuring camera settings. e) Post-simulation: multiple simulations are carried out under varying conditions, and the simulation outputs are recorded. f) Output: sensor data and annotation information are produced, encompassing not only the original video but also semantic segmentation, bounding boxes, point clouds, mesh surfaces, collision detection data, camera parameters, raw sampling data, and other simulation results required for question generation. We provide more details on this procedure in the appendix. In terms of detailed settings, four diverse scenarios were simulated.\\n\\nLiquid Dynamics.\\n\\nAs shown in Fig. 2 A, we have designed a device that bears a resemblance to a liquid hourglass. In this device, various liquids of different densities, each represented by distinct colors, are released from corresponding emitters situated at the uppermost part of the apparatus. Under the influence of gravity, these liquids descend and traverse a series of fixed ramps (resembling sticks) positioned in the central region of the device. This arrangement causes alterations in their flow direction. Ultimately, liquids are funneled into containers at the bottom, each container designated by a unique color. This process highlights distinctive behaviors arising from the interaction of multiple fluids, attributable to their significantly varied densities. Our research is oriented towards formulating inquiries pertaining to physical properties of these liquids and dynamic trajectories they exhibit.\\n\\nCloths Manipulation.\\n\\nAs depicted in Fig. 2 B, a small table hosts an assortment of objects, including pillars and plates of varying sizes, colors, and masses. Two square pieces of cloth, each possessing distinct stretching, bending characteristics, and frictional properties, are gripped at one edge and moved forward to cover these objects, causing possible collision events. Cloths are then promptly released. The fabric obstructs the view of the objects but also delineates their shapes through its deformable surface. Objects may topple over if they exceed a certain height or have low mass, resulting in observable changes in the fabric's dynamic 3D surface geometry. This scenario serves as a test for a model's capacity to discern the physical attributes of the fabrics and to predict the spatial behavior of the concealed objects in dynamic situations.\\n\\nRope Pulley System.\\n\\nAs illustrated in Fig. 2 C, an array of pulleys, including both movable and fixed types, along with anchor points, is arranged on a wall. Ropes are configured with their ends connected to pulleys, loads, or anchor points, and can be wound around the pulleys. These loads possess varying masses, interacting with other forces in the system, leading to the emergence of distinct motion patterns. The primary objective of the model is to identify the tension distributions within this elementary rope system. Additionally, it is tasked with recognizing potential correlations or constraints among objects in motion, such as the coordinated movement of loads and the rotation of pulleys on a single rope. Moreover, the model is expected to infer numerical relationships between the loads' masses and predict whether the ropes will detach from or remain attached to certain pulleys.\\n\\nSoft Ball Dynamics.\\n\\nAs depicted in Fig. 2 D, a playground contains obstacles of different color, and pose, along with pits randomly arranged within. Soft balls with varying deformation resistance or plasticity yield are launched randomly within the space, with varying initial positions. These balls undergo a sequence of dynamic movements, including bouncing and permanent deformation. Ultimately, some may collide with obstacles and fall into pits. This experimental scenario serves as a\"}"}
{"id": "pNlntv7A9X", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\ntest to determine whether the model can accurately discern the elasticity and plasticity properties of\\nthe soft bodies. It also assesses the model's ability to make dynamic predictions and inferences based\\non these attributes during interactions between objects.\\n\\n3.2 Question Generator\\n\\nGeneration Steps.\\n\\nWe develop a question generator to generate question-answering pairs associated\\nwith the videos through the following steps: a) Template Design: create a range of question and\\noption templates for generation (See table A.2 in the appendix). b) Sampling: retrieve the simulation\\nresults, combine the properties of the objects in the scene with predefined templates, and sample\\nquestions and options accordingly. Correct answers are determined based on the simulation outcomes.\\nUnique identification and description of target objects are accomplished using visual attributes such\\nas color, shape, orientation, and mobility. c) Re-Sampling: ensure a balanced distribution of answers\\namong the options to prevent answer bias toward any particular choice.\\n\\nOverview.\\n\\nWe have categorized our questions into two major groups: Physical Property Questions\\nand Dynamics Questions. Figure 2 shows all of the question types present in ContPhy for each of\\nthe four scenarios. Sample templates are provided in Appendix A.2.\\n\\nPhysical Property Questions.\\n\\nWe formulated a set of physical property questions across four distinct\\nscenarios. These questions inquire about visible physical properties of objects, such as colors, shapes,\\nand existences, which can be discerned from static video frames. Additionally, we pose questions\\nabout physical attributes, including mass, density, and deformability, which can only be answered by\\nobserving various object dynamics and interactions. These questions primarily revolve around factual\\nfeatures that can be perceptive and answered with a brief phrase response. Models are expected\\ndeduce these physical properties based on input video data, which requires models to possess a\\nfoundational understanding of fundamental physical principles.\\n\\nDynamics Questions.\\n\\nRegarding dynamic questions, we explored various scenarios involving\\nthe behavior of objects. Dynamic questions can be further categorized into three distinct types:\\ncounterfactual, goal-driven, and predictive. These questions encompass potential outcomes when\\ninitial conditions change, strategies for achieving specific objectives and inquiries about future\\nevents. In the cloth scenario, we only designed predictive questions encouraging the model to\\nanticipate outcomes not directly visible under the cloth cover. In the rope scenario, we only have\\nexclusively formulated counterfactual and goal-driven questions, aimed at motivating models to\\nsimulate hypotheses regarding the soft-body-driven mechanical transmission scenario. For the\\nremaining two scenarios, fluid and ball, we have designed questions encompassing all three types,\\nwith the expectation that models will acquire a comprehensive understanding of these scenarios\\nthrough the diverse nature of the question templates. To enhance the cognitive challenge, we have\\nstructured these questions as multiple-choice, featuring more than two but fewer than five answer\\nchoices. Models are tasked with providing binary true-false predictions by concatenating their\\nresponse with the corresponding question-choice pair.\\n\\n3.3 Statistics and Post-Processing\\n\\nIn our dataset, we have generated a substantial volume of videos, questions, physical parameters, and\\nobjects. To provide a more detailed breakdown:\\n\\nVideo Data.\\n\\nWe categorize videos by scenario. Each scenario contains 500 videos of fixed lengths:\\n250 frames for fluids, 150 for ropes, 145 for cloths, and 120 for balls. Given the diverse responses\\nin the VQA generation phase, we employed randomization for several configuration parameters\\nduring the simulation initialization. Beyond general scene arrangements like camera, lighting, and\\nbackgrounds, unique configurations pertain to each scenario:\\na) Fluids: Fluid density factors into multi-fluid interactions. Striving for diverse results, the number\\nof fluid emitters and containers, the positions, poses, scales of obstructive sticks, and object colors\\nare randomized. Fluid densities, chosen from a preset pool, should ensure discernible stratification in\\nfluid interactions.\\nb) Ropes: The rope-pulley system layout, rope link lists, and entanglement methods are pre-set to\\nallow varied connections between adjacent objects. Filtering steps identify simulations that provide\"}"}
{"id": "pNlntv7A9X", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"diverse and aesthetically pleasing configurations. Attributes such as color, shape, load mass, load movability for loads, ropes, fixed endpoints, and pulleys are randomized prior to simulation.\\n\\nc) Cloths: Parameters like stretching compliance, bending compliance, and friction rate are drawn from a predetermined pool, ensuring cloth dynamic differences discernible to humans. Other items, such as pillars and plates, undergo random scaling and positioning. Cloth movement speeds and paths vary, aiming for diverse collision outcomes. Rigid object masses are also randomized to diversify collision event predictability.\\n\\nd) Balls: Deformation resistance and plasticity yields are sourced from a set value range to highlight differing properties. Floating wall positions and poses are constrained to specific zones to intensify collision events in videos, leading to varied outcomes during and post-video.\\n\\nIn accordance with the video content, we have formulated a substantial quantity of questions. Each video has one property question and two dynamics questions, except for rope scenario which has two property-related questions and two dynamics questions. We generated a total of 2,000 questions related to the rope scenario and 1,500 questions related to other scenarios, respectively.\\n\\nConsequently, our dataset comprises a total of 6,500 questions drawn from 2,000 videos. We have partitioned the dataset into three subsets: 50% for training, 20% for validation, and 30% for testing. More precisely, the training set consists of 3,250 questions, the validation set comprises 1,300 questions, and the testing set encompasses 1,950 questions. Through the whole dataset, 20% are counterfactual, 11% are goal-driven, 22% are predictive, and the remaining 46% pertain to physical property questions of various kinds. The detailed distribution of each question type within the rope scenario is visualized in Figure 3, while Table 4 provides templates for each rope-related question type. Further information about question types of other scenarios is available in Appendix A.2.1.\\n\\n4 EXPERIMENTS\\n\\nIn this section, we perform evaluation and analysis on various baseline models for our ContPhy dataset for video reasoning. In accordance with the standard evaluation protocols that have been adopted in previous works such as CLEVR-E, we consider each physical property question as a classification task among all possible answers. Each dynamic question is treated as a binary classification task for each question-choice pair, as dynamic questions are always multiple-choice questions. For dynamic questions, we report the accuracy for each option and per question. A question is correct only if all choices in this multiple choice question are correctly answered.\\n\\n4.1 EXPERIMENTAL SETUP\\n\\nVisual-Blind Models. This family of models include several baselines that only rely on question-only input, to help us analyze language biases in ContPhy.\\n\\nRandom chooses at random a possible answer, or randomly selects between true-false binary answer pairs for every multiple choice questions.\\n\\nFrequent selects the most frequent answer based on the question type.\\n\\nBlind-LSTM with language-only input utilizes an LSTM network (Hochreiter & Schmidhuber, 1997) to encode the question and predict the answer.\\n\\nVisual Models. These models incorporate both visual and language representations for answering questions about physical events in videos.\\n\\nCNN-LSTM extracts video features via ResNet-50 convolutional neural network (CNN) (lec, 1998; He et al., 2016) on 25 sampled frames of videos and average them over time as the visual input. We concatenate this visual input with the question embedding from the last hidden state of LSTM to predict answers.\\n\\nHCRN (Le et al., 2020) uses...\"}"}
{"id": "pNlntv7A9X", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We introduce the Continuum Physical Dataset (ContPhy), a novel benchmark for evaluating machine models in physical reasoning across diverse scenarios for the continuum. The ContPhy is specifically designed to be complementary with existing physical reasoning benchmarks by encompassing diverse physical property inferences for the physical parameters of the continuum such as mass and density across dynamic situations and predicting corresponding dynamics. This comprehensive dataset enables the development and assessment of AI models with human-like visual reasoning abilities in understanding visual attributes, physical properties, and dynamics of both rigid objects and soft objects while devising goal-oriented solutions. We evaluated a range of AI models and found that they still struggle to achieve satisfactory performance, which shows that current AI models still lack physical commonsense for the continuum, especially soft-bodies, and illustrates the value of the proposed dataset. We hope the ContPhy fosters advancements in AI perception and reasoning in diverse physical environments, bridging the gap between human and machine intelligence in the physical world.\\n\\nHumans are capable of comprehending the physical properties of various substances, including rigid objects and soft objects, understanding their dynamic interactions in complex environments, and predicting their corresponding dynamic changes. In fact, this innate ability to understand and reason about the physical world plays a crucial role in shaping our understanding of nature and the development of scientific knowledge.\\n\\nAs depicted in Fig. 1, objects like solids and liquids in nature often exhibit different properties, and these objects of different properties couple together to build our complex physical world. As humans, we are able to distinguish objects' physical properties by observing their interactions. We are able to know that the clear liquid in Fig. 1 (a) at the bottom has a higher density than the yellow liquid on the top; we know that the dynamic pulley in Fig. 1 (c) could help us to pull the cargo up more easily. These innate human skills raise an intriguing question: can current AI models have the physical commonsense to infer physical properties of the continuum and predict corresponding dynamics?\\n\\nRecently, a series of benchmarks (Riochet et al., 2018; Rajani et al., 2020; Bear et al.), have been developed to study machine models' effectiveness at physical reasoning. However, there have been limitations that make them non-ideal benchmarks for the development and assessment of whether machine models have human-like visual reasoning abilities in understanding objects' physical properties and dynamics. Firstly, most of the benchmarks mainly deal with simple visual primitives like spheres, cubes and collision events of rigid objects only. It remains doubtful whether the conclusions based on these simple scenes will still hold in more comprehensive visual scenarios with the coupling of soft objects and their interaction with rigid objects. There have also been benchmarks like Physion (Bear et al.) that were developed to evaluate machine models' physical reasoning abilities. However, objects in Physion are of the same physical parameters without any variance (e.g., solids with the same mass and water with the same density). Moreover, the Physion dataset encompasses various bodies like liquids, soft materials (e.g., soft balls, cloth, and ropes), rigid bodies (e.g., cubes, pillar, plate and spheres), and articulated bodies (e.g., pulleys).\"}"}
{"id": "pNlntv7A9X", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: The motivation is derived from a range of everyday soft materials and their interaction with rigid objects, whose physical behaviors or functions vary by their diverse physical properties.\\n\\na) Gasoline flows more fluently than glue due to lower viscosity, while oil with lower density tends to float above water. b) Poplin and canvas exhibit surface wrinkles with varying granularity due to their distinct bending compliance. c) The lifting approach requires less force due to the re-distributed tensile forces facilitated by the movable pulley. d) Trajectories of tennis ball and doughball demonstrate their differing elasticity and plasticity.\\n\\nOnly requires models to predict whether two objects will come into contact after the observed video ends. It has not incorporated natural language to answer other challenging questions like predicting dynamics in counterfactual scenes and selecting actions to achieve a goal.\\n\\nTo this end, we aim to build a Continuum Physical Dataset (ContPhy) to thoroughly evaluate and diagnose machine models\u2019 physical reasoning performance in comprehensive physical environments. The design of ContPhy aims to achieve two goals: 1) Cover diverse physical scenarios and 2) support comprehensive natural language tasks. To achieve the first goal to build diverse physical scenes, we adopt the physical engine Unity (Haas, 2014) to simulate diverse videos with dense supervision signals. As shown in Fig. 2, the simulated physical scenes include scenes with the coupling of different liquids, scenes with deformable cloths and rigid objects, and scenes with pulley systems.\\n\\nAnother goal of the built dataset is to propose diverse physical reasoning tasks in the form of video question answering. We achieve this goal with a carefully-designed question generator. The question engine takes the dense simulated video annotation as input and generates different questions based on pre-defined textual templates. Sample questions can be found at Fig. 2. It asks challenging questions such as \\\"If the red stick were removed, would most orange fluid flow into the cyan container?\\\" and \\\"Is the mass of the sphere greater than half that of the red cube?\\\", which requires the model to have a deep understanding of physical scenes and reason about their dynamics.\\n\\nWe also evaluate a series of existing machine models (Le et al., 2020; Hudson & Manning, 2018) on the new proposed ContPhy benchmark. We found that the performance of these models is far from satisfactory, demonstrating the proposed ContPhy benchmark's value and indicating the necessity of more advanced machine models with better physical common sense.\\n\\nTo summarize, the contribution of the paper lies in three aspects. First, we propose a new comprehensive physical reasoning benchmark that includes the coupling of different diverse physical properties like mass, density, elasticity, and deformability, and the interaction between soft objects and rigid objects. Second, we build a carefully-designed question generator that is able to synthesize different kinds of challenging physical reasoning question-answer pairs and provides multi-step rationales that lead to the answers. Finally, we extensively evaluate the proposed benchmark with multiple machine models to study the characteristics and show insights into physical reasoning model development.\\n\\n2 RELATED WORK\\n\\nPhysical Reasoning. Our work is closely related to Physical Reasoning benchmarks (Riochet et al., 2018; Rajani et al., 2020; Girdhar & Ramanan, 2020; Baradel et al., 2020; Bear et al.; Chen 2020).\"}"}
{"id": "pNlntv7A9X", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison between ContPhy and other physical reasoning benchmarks. ContPhy is a dataset that covers a wide variety of tasks including reasoning about the continuum's physical properties, counterfactual dynamics, and goal planning in diverse physical scenarios.\\n\\nEarly benchmarks (Riochet et al., 2018; Rajani et al., 2020) simulate physical scenes with visual primitives and test models' physical intuition. Later, CLEVER (Yi et al., 2020), ComPhy (Chen et al., 2022), and CRIPP-VQA (Patel et al., 2022) extend the simple visual primitives with natural language and asked questions about rigid bodies' collisions. Recently, Physion (Bear et al.) provides more complex visual scenes and requires models to predict whether two objects will come into contact in future frames. As summarized in table 1, the proposed ContPhy is the only benchmark that contains soft objects with different physical parameters and asks diverse language-based questions about dynamics in counterfactual and goal-planning scenarios.\\n\\nRecently, there is a paper (Li et al., 2022c) raises concerns that dynamics-based models may struggle to make accurate predictions. While several methods (Wu et al., 2022; Ding et al., 2020; 2021; Lu et al., 2023) have successfully tackled previous benchmarks like Physion (Bear et al.) and CLEVERER (Yi et al., 2020). These papers have successfully validated the performance of state-of-the-art AI models. Motivated by their significant contribution, we aim to extend this success further by evaluating dynamics and soft-body objects, and interaction between rigid and soft bodies.\\n\\nVisual Question Answering.\\n\\nOur paper is also related to Visual Question Answering (VQA) (Zadeh et al., 2019; Lei et al., 2019; Wu et al., 2021), which mainly requires machine models to answer questions about a given image or video's content like visual attributes, actions, activity, and social events. VQA was first developed for single images, which mainly asks for objects' categories and visual attributes like color and shapes (Andreas et al., 2016; Hudson & Manning, 2019). Subsequently, it was extended to the video domain. However, these datasets still typically assess abilities in visual perception, recognizing objects, shapes, and colors, and understanding human-centric actions. In this paper, we aim to build a benchmark that could evaluate machine models' comprehensive physical reasoning abilities in scenarios with the continuum, including rigid objects, soft objects and liquids.\\n\\nPhysical Benchmarks for Soft Bodies.\\n\\nIn recent years, there has been a growing interest in the properties and dynamics of soft-bodied objects within the research community (Xiang et al., 2020; Gan et al., 2020; Macklin et al., 2014; Xian et al., 2023; Haas, 2014). Much of this research has concentrated on creating simulations of deformable objects and fluids using physical engines, thus advancing the field of robotic manipulation and cognitive experimentation. Leveraging these robust tools, we can simulate deformable objects and fluids with varying physical parameters, enabling collaboration with natural language for the purpose of physical commonsense learning. This allows us to investigate the extent to which current AI models comprehend such physical phenomena.\\n\\n3 D ATASET\\n\\nThe proposed ContPhy dataset is designed to evaluate machine models' reasoning abilities on comprehensive physical scenes with different objects like rigids, liquids, ropes, and cloths and massive physical properties associated with them. In this section, we describe how we built the dataset. For the various scenarios we propose, a unified data generation pipeline is summarized into 2 key stages, physics simulation and VQA generation. In section 3.1, we introduce how we leverage the simulation engine to build diverse scenes. In section 3.2, we describe how we develop a question\"}"}
{"id": "pNlntv7A9X", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nTable 7: Question templates and examples in Ball.\\n\\n| Symbol | Explanation |\\n|--------|-------------|\\n| CLR    | blue, black, brown, cyan, gray, green, orange, pink, purple, red, yellow, light blue, white |\\n| SHP    | solid, hollow |\\n| OBJ    | plate, pillar, cube, sphere, pulley, rope |\\n| CMP    | greater than, less than, harder, easier, equal to |\\n| FAC    | twice of, half of |\\n| POS    | left, right |\\n| ENT    | move up, move down, rotate clockwise, rotate anti-clockwise |\\n\\nTable 8: Detailed explanation of symbols that we use in question generation with question templates.\"}"}
{"id": "pNlntv7A9X", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"symbolic style into large language models, proposing a powerful method for solving complex visual queries and semantic information within languages. ChatGPT and its family (Ouyang et al., 2022; OpenAI, 2023; Brown et al., 2020) have absolute advantages in understanding complex semantic information in our generated questions, saving us a lot of trouble and maintaining great parse accuracy simultaneously. With the aid of large language models, semantic information and physical concept can be precisely parsed out without manually developing a rule-based question parser and question programs.\\n\\nSimilar to ViperGPT (Sur\u00eds et al., 2023), we utilize large language models, specifically ChatGPT, as the backbone of our proposed ContPRO. We further develop a set of dynamics modules and visual perception modules serving as APIs for code generation. With the provided API access and a pre-defined physical reasoning prompt, we leverage ChatGPT to generate Python code that can be directly executed and interpreted. This method bridges the gap between language comprehension and physical concept understanding, enabling us to directly evaluate the continuum physical reasoning abilities of AI models, without concerns related to different expressions of question sentences.\\n\\nVideo Perception.\\n\\nWe adopt MASK R-CNN (He et al., 2017) as our video perception module. This module aids ContPRO to detect and ground objects in videos, bridging the gap between linguistic descriptions of objects and their physical properties. Given an input video, this module will detect objects in each frame, addressing simple property questions directly, such as object counting, object existence and color recognition, with very high accuracy. Also, its ability to locate objects through bounding boxes indirectly plays a role in other questions. For example, in the Density question of the fluid scenario, fluids' densities are comparable only if they are in the same container. This location ability enables us to discern whether they are in the same container, i.e., their densities are comparable.\\n\\nPhysical Simulation.\\n\\nPrevious methods (Chen et al., 2022; Bear et al.; Tung et al., 2023), mainly adopt DPI-Net as physical simulator, which is a 3D particle-based graph neural network (Li et al., 2018). DPI-Net performs well in mass (Tung et al., 2023), while falling short in fluid dynamics prediction. Therefore, we utilize a differentiable MPM physics simulation module DiffMPM (Hu et al., 2019) to provide a primary solution to the system identification of the fluid scenario. We also borrowed the simulation framework partly from PAC-NeRF (Li et al., 2023).\\n\\nTo show the details, we illustrate the pipeline solving fluid system identification as Fig. 8. In the physical system identification phase, we firstly initialize the density of each kind of fluid with a fixed value. We use the fluid point cloud as the initial input. For each iteration, we repeatedly feed initial points forward in the differential physics simulation pipeline, which is rewritten to fit in 2D vector computation for better performance. At the meantime, the loss is constructed as the chamfer loss between the groups of fluid points and the predicted points. Then the gradients are fed back to the optimization of each physical parameter, i.e., densities here. In the prediction phase, the fluids' trajectories are predicted, with the simulation including the fluid-rigidbody collision and fluid-fluid interaction (i.e., liquid stratification). Collision events and layer information are recorded and output for the downstream QA tasks. Likewise, in the counterfactual prediction phase, we repeatedly change the initial conditions and rerun the simulation.\\n\\nA.3.2 Representative Results Analysis\\n\\nThe physical simulation method (Hu et al., 2019; Li et al., 2023) in oracle model exhibits powerful performance that can precisely simulate the continuum dynamics, especially soft bodies, while it requires scenario-specific manipulation of hyperparameters. Therefore, we present representative results in the fluid scenarios below, which encompasses all four question types, and compare them with multiple baseline models. We report the representative results in Table 3, which reveals that our ContPRO successfully outperforms all questions in the fluid scenario. These results indicate that ContPRO is capable of understanding both semantic information in questions and the continuum physical concept well.\"}"}
{"id": "pNlntv7A9X", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Methods\\n\\nProperty Counterfactual Goal-Driven Predictive\\n\\n|                | opt. | ques. | opt. | ques. | opt. | ques. | opt. |\\n|----------------|------|-------|------|-------|------|-------|------|\\n| Random         | 33.3 | 52.9  | 6.0  | 59.9  | 7.5  | 53.8  | 4.8  |\\n| Frequent       | 52.7 | 57.9  | 17.2 | 63.1  | 36.3 | 50.1  | 12.5 |\\n| Blind-LSTM     | 49.3 | 56.1  | 7.8  | 57.3  | 22.5 | 51.4  | 12.5 |\\n| CNN-LSTM       | 54.0 | 55.0  | 8.6  | 57.3  | 22.5 | 51.4  | 12.5 |\\n| MAC            | 30.0 | 56.5  | 6.9  | 51.2  | 17.5 | 53.5  | 12.5 |\\n| HCRN           | 52.7 | 52.6  | 4.3  | 67.7  | 41.3 | 50.6  | 1.9  |\\n| ALPRO          | 48.0 | 56.8  | 6.0  | 62.7  | 32.5 | 53.8  | 12.7 |\\n| ContPRO-Oracle | 78.0 | 75.7  | 36.2 | 77.3  | 60.0 | 90.1  | 68.3 |\\n\\nTable 3: Physical reasoning in the fluid scenario on ContPhy. We list all question families, including Property, Counterfactual, Goal-driven and Predictive questions. Accuracy is reported with per option and per question.\\n\\nachieves 63.5% accuracy, which is a more intricate challenge involving density comparisons of two fluids, requiring a linkage of multiple modules. This task presents a more intricate challenge and requires a linkage of multiple modules. Firstly, the process begins with language models parsing the question and generating Python code. Subsequently, the visual perception module recognizes and locates these fluids in the video by their names, and outputs bounding boxes of two fluids. A specialized detection function then determines if the fluids share the same container by analyzing their location and overlap, which is a prerequisite for comparability of two fluids. Finally, if the fluids are comparable, the physical simulator steps in to infer the relative densities. In general, the high accuracy achieved in the Stick Number task significantly boosts the overall property question accuracy, positioning ContPRO well above other models in this question.\\n\\nDynamics.\\n\\nIn the dynamics question set, ContPRO notably excels, surpassing other baseline models across all categories. This superior performance indicates that our proposed model can accurately simulate the continuum physical scenarios via a differentiable physical dynamic simulator. This accurate simulation enhances the model's ability to reason and predict dynamics in subsequent stages. Among the three types of dynamics questions, our oracle model particularly outperforms others in predictive questions, where it achieves 68.3% accuracy per question. This discrepancy may be attributed to the training objectives of MPM physical simulator in ContPRO. In MPM prediction phase, simulator focuses on predicting trajectories and dynamics. Notably, this simulator-based method requires 3D point cloud information which cannot be perceived from 2D videos. Thus we consider it as an oracle model baseline.\\n\\nA.4 The CONTINUUM: Liquids, Soft Bodies, Rigid Bodies AND ARTICULATED BODIES\\n\\nIn this section, we consider the physical concept of the continuum. Previously, physical datasets mainly focused on simple visual primitives of rigid bodies, such as cubes and spheres. In our ContPhy, we extend this success to a broader concept, the continuum. The continuum encompasses various bodies such as liquids, soft materials (e.g., soft balls, cloth, and ropes), rigid bodies (e.g., cubes, pillar, plate and spheres), and articulated bodies (e.g., pulleys). We consciously include both physical dynamics reasoning (e.g., interactions between fluids, soft bodies and rigid bodies), and physical parameter or concept reasoning (e.g., density for fluids; tension, elasticity for soft bodies; mass for rigid bodies).\\n\\nFor instance, our rope and pulley scenario involves elements of rope, rigid bodies, and articulated bodies; the fluid scenario includes liquids; the cloth scenario covers both cloth and rigid bodies; and the ball scenario focuses on soft balls. This extensive coverage ensures that our dataset provides a comprehensive understanding of the interactions and couplings within these various types of continua, capturing the complexity and diversity of real-world physical phenomena.\\n\\nIn our paper, we focus predominantly on fluids and soft bodies, which is often overlooked in previous works. However, our dataset comprehensively encompasses rigid bodies in all scenarios and articulated bodies (e.g., in the rope scenario). This inclusion leads to our utilization of the continuum concept, enhancing the breadth and relevance of our study.\"}"}
{"id": "pNlntv7A9X", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Class Type Template and Example\\n\\nShape Factual\\n\\nQ: How many _SHP_ _OBJ_s are there in the video?\\nE.g. How many solid pulleys are there in the video?\\n\\nColor Factual\\n\\nQ: How many _CLR_ objects are there in the video?\\nE.g. How many blue objects are there in the video?\\n\\nExistence Factual\\n\\nQ: Is there any _CLR_ _OBJ_ in the video?\\nE.g. Is there any blue cube in the video?\\n\\nMass Factual\\n\\nQ: Is the mass of the _CLR_ _OBJ_ _CMP_ _FAC_ that of the _CLR_ _OBJ_?\\nE.g. Is the mass of the blue sphere greater than half that of the green cube?\\n\\nTension Factual\\n\\nQ: Is the tension of the _CLR_ rope _CMP_ _FAC_ that of the _CLR_ rope?\\nE.g. Is the tension of the blue rope greater than half that of the green rope?\\n\\nMass Change Counterfactual\\n\\nQ: If the _CLR_ _OBJ_ were heavier, what would happen?\\nE.g. If the blue sphere were heavier, what would happen?\\n\\nDirection Counterfactual\\n\\nQ: If the _CLR_ _OBJ_ were much heavier, which direction would the _CLR_ _OBJ_ move?\\nE.g. If the blue cube were much heavier, which direction would the brown sphere move?\\n\\nMass Goal Goal-Driven\\n\\nQ: If we want the _CLR_ _OBJ_ to _ENT_, what can we do?\\nE.g. If we want the yellow cube to move up, what can we do?\\n\\nTable 4: Question templates and examples in Rope.\"}"}
{"id": "pNlntv7A9X", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nClass Type Template and Example\\nDensity Factual\\nQ Is the fluid density of the _CLR_ fluid _CMP_ that of the _CLR_ fluid?\\nE.g. Is the fluid density of the pink fluid greater than that of the light blue fluid?\\n\\nStick Number Factual\\nQ How many sticks are there in the video?\\n\\nPass Predictive\\nQ Which stick will the fluid from the other _CLR_ emitter pass?\\nE.g. Which stick will the fluid from the other blue emitter pass?\\n\\nContainer Predictive\\nQ Which container will fluid from the other _CLR_ emitter flow into?\\nE.g. Which container will fluid from the other blue emitter flow into?\\n\\nPass Counterfactual\\nQ If _CLR_ stick were removed, which stick would _CLR_ fluid pass?\\nE.g. If brown stick were removed, which stick would pink fluid pass?\\n\\nContainer Counterfactual\\nQ If _CLR_ stick were removed, which container would _CLR_ fluid flow into?\\nE.g. If brown stick were removed, which container would pink fluid flow into?\\n\\nContainer Goal-Driven\\nQ What can we do to let most of the _CLR_ fluid enter _CLR_ container?\\nE.g. What can we do to let most of the pink fluid enter gray container?\\n\\nTable 5: Question templates and examples in Fluid.\\n\\nClass Type Template and Example\\nElasticity Factual\\nQ Is the elasticity of the _POS_ cloth much _CMP_ that of the other?\\nE.g. Is the elasticity of the left cloth much greater than that of the other?\\n\\nBending Factual\\nQ Is the _POS_ cloth much _CMP_ to bend or have wrinkles than the other?\\nE.g. Is the right cloth much harder to bend or have wrinkles than the other?\\n\\nFall Over Predictive\\nQ Does the _CLR_ _OBJ_ fall over?\\nE.g. Does the green plate fall over?\\n\\nCollision Predictive\\nQ Does the _CLR_ _OBJ_ collide with the _CLR_ _OBJ_?\\nE.g. Does the green plate collide with the gray pillar?\\n\\nTouch Predictive\\nQ Is the _CLR_ _OBJ_ finally in touch with the _CLR_ _OBJ_?\\nE.g. Is the green plate finally in touch with the gray pillar?\\n\\nPose Predictive\\nQ Which phrase below can best describe the final pose of the _CLR_ _OBJ_?\\nE.g. Which phrase below can best describe the final pose of the green plate?\\n\\nTable 6: Question templates and examples in Cloth.\"}"}
{"id": "pNlntv7A9X", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Methods   | Physical Property | Dynamics |\\n|-----------|-------------------|----------|\\n|           | Prop. Count. Goal. Pred. | opt. ques. opt. ques. opt. ques. |  \\n| Random    | 30.0 | 51.3 | 14.7 | 55.2 | 4.5 | 33.3 | 52.9 | 6.0 | 59.9 | 7.5 | 53.8 | 4.8 |\\n| Frequent  | 53.3 | 51.6 | 19.0 | 49.7 | 11.2 | 52.7 | 57.9 | 17.2 | 63.1 | 36.3 | 50.1 | 12.5 |\\n| Blind-LSTM | 54.7 | 74.0 | 46.0 | 47.4 | 7.9 | 49.3 | 56.1 | 7.8 | 57.3 | 22.5 | 51.4 | 12.5 |\\n| CNN-LSTM  | 52.7 | 74.0 | 45.0 | 51.2 | 6.7 | 54.0 | 55.0 | 8.6 | 57.3 | 22.5 | 51.4 | 12.5 |\\n| MAC       | 53.3 | 74.2 | 39.8 | 50.3 | 6.7 | 30.0 | 56.5 | 6.9 | 51.2 | 17.5 | 53.5 | 12.5 |\\n| HCRN      | 51.7 | 74.3 | 48.1 | 56.0 | 2.3 | 52.7 | 52.6 | 4.3 | 67.7 | 41.3 | 50.6 | 1.9 |\\n| ALPRO     | 60.7 | 76.2 | 50.7 | 46.2 | 1.1 | 48.0 | 56.8 | 6.0 | 62.7 | 32.5 | 53.8 | 12.7 |\\n| Human     | 84.7 | 90.2 | 75.0 | 91.9 | 84.0 | 75.8 | 82.5 | 60.6 | 75.0 | 64.3 | 73.9 | 42.9 |\\n\\nTable 2: Physical reasoning on ContPhy. We list all question families, Property, Counterfactual, Goal-driven and Predictive questions. Accuracy is reported with per option and per question.\\n\\nConditional relational networks to learn relations hierarchically in the video, as well as the questions. MAC (Hudson & Manning, 2018) has competitive results on previous datasets, which uses co-attention mechanism to model both textual and visual information. ALPRO (Li et al., 2022a) is a popular videoQA model pre-trained on video-text feature alignment. After stages of contrastive learning at instance level, and prompting entity fine-grained modeling between patch and text entity, ALPRO achieved state-of-the-art results on several video multi-modal datasets. We fine-tune on our dataset based on the official pre-trained checkpoint.\\n\\n4.2 EVALUATION OF PHYSICAL INFERENCE\\n\\nWe summarize the performance of all baselines in Table 2. The results show that different models exhibit distinct performances across different scenarios, even on different question types within a single scene. This indicates that our ContPhy benchmark encompasses a wide range of reasoning tasks, making it a valuable tool for evaluating the limitations of visual models.\\n\\nPhysical Property.\\n\\nPhysical property questions in ContPhy focus on both the fundamental content in the video, as well as properties governed by physical rules. This implies that models should not only recognize content simply but also understand the video and possess physical knowledge, putting forward a new challenge to models. None of the models successfully addressed all of these types of questions. Some baselines perform worse than language-only models in certain scenarios, indicating a failure to correctly understand physical properties and the importance of our dataset. Most of the baseline frameworks are not specifically designed for physical properties, which accounts for the poor performance. All the baseline models struggle to achieve decent performance on physical property questions, except ALPRO, which is in the rope scenario and maintains competitive results in other scenarios, showing the advantages of large-scale video-text pre-training and alignment.\\n\\nDynamics.\\n\\nDynamics questions including counterfactual, goal-driven, and predictive, pose another challenge that further enhanced reasoning from dynamic video sequences. These questions require models to focus not only on visual perception but also on predicting unseen information. Models are required to fully comprehend the video and make inferences or predictions based on the questions.\"}"}
{"id": "pNlntv7A9X", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"All dynamics questions are multiple-choice, although the number of choices varies among different datasets. This also explains the different accuracy per question on Random and Frequent. We find that ALPRO achieves better performance on counterfactual reasoning, proving the superiority of its pre-training framework again. HCRN has advantages in goal-driven question reasoning, but fails in predictive questions. We postulate that a possible reason could be that its hierarchical network mechanism equips the model with the ability to explore unseen scenarios under goal-driven conditions but not for predictive questions. Other baselines based on traditional neural networks have difficulty understanding the physical scenarios and in capturing the physical laws from videos and question-answer pairs. Thus, they perform worse than their previous performance on our benchmark.\\n\\nScenario Analysis.\\n\\nWe observe that different models exhibit significant variability in their performance across different scenarios. Regarding physical properties, CNN-LSTM performs well on fluid and ball, MAC excels with cloth and ALPRO demonstrates strong performance in the rope scene. For dynamics questions, ALPRO answers well in both cloth and rope scenario. A possible explanation is that both cloth and rope scenarios share some similarities as they both exhibit mechanical event such as collision, motion, and rotation led by soft objects as opposed to the fluid and ball scenario. Another reason is the fewer question types in the rope and cloth scenario than those in the fluid and ball scenarios. Specifically, the rope scenario has counterfactual and goal-driven, and the cloth scenario has predictive. Conversely, in the fluid and ball scenarios, we incorporated all four problem types, thereby making situations much more complicated. To effectively address these scenarios, models must tackle four distinct question types, each focusing different aspects of physical dynamics. Consequently, no baseline models can gain an absolute advantage in these scenarios. This indicates that our four proposed question types well evaluate different dimensions of physical reasoning, making the fluid and ball scenarios particularly challenging for AI models. In addition, visual models marginally outperform language-only models, suggesting that existing models struggle to comprehend complex soft-body visual information and interaction with other rigid bodies.\\n\\nHuman Performance.\\n\\nWe randomly sampled some video-question pairs from the test set in order to assess human ability to comprehend the physical properties and dynamics events presented in both video and textual descriptions. To evaluate human performance on ContPhy, 16 people participated in the study. Participants were required to have fundamental English reading skills and basic physical knowledge background. First, each participant was asked to select a scenario randomly, after which they presented with distinct video-question pairs. Participants were instructed to answer with a phrase when presented with physical property questions, while for dynamics questions they were required to provide a binary true-false response from available choices. We obtained 460 valid human answers encompassing all scenarios and question types within ContPhy. Human Performance is presented in Table 1 and we can observe that it beats machine models in all scenarios. This shows the fundamental ability and strength of humans to perform visual reasoning and inference from videos.\\n\\nEvaluation Conclusion.\\n\\nThe strong human results demonstrate that humans maintain a strong capacity to comprehend both videos and questions, make physical property inference from given videos, and predict and reason counterfactual hypothesis concerning unseen information. Machine models results show that even state-of-the-art models struggle with answering these physical questions. This indicates that our dataset poses a significant challenge for vision-language models to achieve similar basic physical video understanding ability with human beings. Our dataset is crucial for assessing and advancing video-text understanding as well as physical inference capacity.\\n\\nConclusion\\n\\nWe introduced the Continuum Physical Dataset (ContPhy), a pioneering benchmark for assessing machine models in physical reasoning of the continuum, especially for soft bodies and fluids in the continuum. This benchmark broadens the scope by covering various physical property inferences for soft bodies across dynamic contexts and predicting their dynamics. Our dataset has enabled the development of AI models with human-like reasoning abilities, comprehending both visual attributes and complex physical properties of objects while solving problems. Despite progress, our evaluation of AI models revealed an ongoing challenge: they struggle to perform well on our benchmark, highlighting their limited physical commonsense for the continuum, especially soft bodies and fluids. We foresee the ContPhy driving progress in AI perception and reasoning, bridging the gap between human and machine intelligence in the physical world.\"}"}
{"id": "pNlntv7A9X", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\\n\\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In CVPR, pp. 39\u201348, 2016.\\n\\nTayfun Ates, Muhammed Samil Atesoglu, Cagatay Yigit, Ilker Kesen, Mert Kobas, Erkut Erdem, Aykut Erdem, Tilbe Goksun, and Deniz Yuret. Craft: A benchmark for causal reasoning about forces and interactions. arXiv preprint arXiv:2012.04293, 2020.\\n\\nFabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, and Christian Wolf. Cophy: Counterfactual learning of physical dynamics. In International Conference on Learning Representations, 2020.\\n\\nDaniel Bear, Elias Wang, Damian Mrowca, Felix Jedidja Binder, Hsiao-Yu Tung, RT Pramod, Cameron Holdaway, Sirui Tao, Kevin A Smith, Fan-Yun Sun, et al. Physion: Evaluating physical prediction from vision in humans and machines. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1).\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\nZhenfang Chen, Kexin Yi, Antonio Torralba, Josh Tenenbaum, and Chuang Gan. Comphy: Compositional physical reasoning of objects and events from videos. In International Conference on Learning Representations, 2022.\\n\\nDavid Ding, Felix Hill, Adam Santoro, and Matt Botvinick. Attention over learned object embeddings enables complex visual reasoning. arXiv, 2020.\\n\\nMingyu Ding, Zhenfang Chen, Tao Du, Ping Luo, Joshua B Tenenbaum, and Chuang Gan. Dynamic visual reasoning by learning differentiable physics models from video and language. In NeurIPS, 2021.\\n\\nChuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, et al. Threedworld: A platform for interactive multi-modal physical simulation. arXiv preprint arXiv:2007.04954, 2020.\\n\\nRohit Girdhar and Deva Ramanan. Cater: A diagnostic dataset for compositional actions and temporal reasoning. In ICLR, 2020.\\n\\nTanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14953\u201314962, 2023.\\n\\nJohn K Haas. A history of the unity game engine. Diss. Worcester Polytechnic Institute, 2014.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\nKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In CVPR, 2017.\\n\\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.\\n\\nYuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and Fr\u00e9do Durand. Difftaichi: Differentiable programming for physical simulation. arXiv preprint arXiv:1910.00935, 2019.\\n\\nDrew A Hudson and Christopher D Manning. Compositional attention networks for machine reasoning. In ICLR, 2018.\"}"}
{"id": "pNlntv7A9X", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019.\\n\\nThao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. Hierarchical conditional relation networks for video question answering. In CVPR, 2020.\\n\\nJie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. Tvqa+: Spatio-temporal grounding for video question answering. In Tech Report, arXiv, 2019.\\n\\nDongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven CH Hoi. Align and prompt: Video-and-language pre-training with entity prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4953\u20134963, 2022a.\\n\\nShiqian Li, Kewen Wu, Chi Zhang, and Yixin Zhu. On the learning mechanisms in physical reasoning. Advances in Neural Information Processing Systems, 35:28252\u201328265, 2022b.\\n\\nShiqian Li, Kewen Wu, Chi Zhang, and Yixin Zhu. On the learning mechanisms in physical reasoning. In NeurIPS, 2022c.\\n\\nXuan Li, Yi-Ling Qiao, Peter Yichen Chen, Krishna Murthy Jatavallabhula, Ming Lin, Chenfanfu Jiang, and Chuang Gan. Pac-nerf: Physics augmented continuum neural radiance fields for geometry-agnostic system identification. arXiv preprint arXiv:2303.05512, 2023.\\n\\nYunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B Tenenbaum, and Antonio Torralba. Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids. arXiv preprint arXiv:1810.01566, 2018.\\n\\nHaoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding. Vdt: General-purpose video diffusion transformers via mask modeling, 2023.\\n\\nMiles Macklin, Matthias M\u00fcller, Nuttapong Chentanez, and Tae-Yong Kim. Unified particle physics for real-time applications. ACM Transactions on Graphics (TOG), 33(4):1\u201312, 2014.\\n\\nOpenAI. Gpt-4 technical report, 2023.\\n\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\\n\\nMaitreya Patel, Tejas Gokhale, Chitta Baral, and Yezhou Yang. CRIPP-VQA: Counterfactual reasoning about implicit physical properties via video question answering. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022.\\n\\nNazneen Fatema Rajani, Rui Zhang, Yi Chern Tan, Stephan Zheng, Jeremy Weiss, Aadit Vyas, Abhijit Gupta, Caiming Xiong, Richard Socher, and Dragomir Radev. Esprit: explaining solutions to physical reasoning tasks. In ACL, 2020.\\n\\nRonan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, V\u00e9ronique Izard, and Emmanuel Dupoux. Intphys: A framework and benchmark for visual intuitive physics reasoning. arXiv preprint arXiv:1803.07616, 2018.\\n\\nD\u00eddac Sur\u00eds, Sachit Menon, and Carl vonodrick. Vipergpt: Visual inference via python execution for reasoning. Proceedings of IEEE International Conference on Computer Vision (ICCV), 2023.\\n\\nHsiao-Yu Tung, Mingyu Ding, Zhenfang Chen, Daniel Bear, Chuang Gan, Joshua B Tenenbaum, Daniel LK Yamins, Judith E Fan, and Kevin A Smith. Physion++: Evaluating physical scene understanding that requires online inference of different physical properties. arXiv preprint arXiv:2306.15668, 2023.\\n\\nBo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan. Star: A benchmark for situated reasoning in real-world videos. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\"}"}
