{"id": "isiQ5KIXbjj", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"QUBO: Quantum Annealing with Learned Encodings\\n\\nMarcel Seelbach Benkner\\nUniversit\u00e4t Siegen\\n\\nMaximilian Krahn\\nMPI for Informatics, SIC\\nAalto University\\n\\nEdith Tretschk\\nMPI for Informatics, SIC\\n\\nZorah L\u00e4hner & Michael Moeller\\nUniversit\u00e4t Siegen\\n\\nVladislav Golyanik\\nMPI for Informatics, SIC\\n\\nABSTRACT\\n\\nModern quantum annealers can find high-quality solutions to combinatorial optimisation objectives given as quadratic unconstrained binary optimisation (QUBO) problems. Unfortunately, obtaining suitable QUBO forms in computer vision remains challenging and currently requires problem-specific analytical derivations. Moreover, such explicit formulations impose tangible constraints on solution encodings. In stark contrast to prior work, this paper proposes to learn QUBO forms from data through gradient backpropagation instead of deriving them. As a result, the solution encodings can be chosen flexibly and compactly. Furthermore, our methodology is general and virtually independent of the specifics of the target problem type. We demonstrate the advantages of learnt QUBOs on the diverse problem types of graph matching, 2D point cloud alignment and 3D rotation estimation. Our results are competitive with the previous quantum state of the art while requiring much fewer logical and physical qubits, enabling our method to scale to larger problems. The code and the new dataset are available at https://4dqv.mpi-inf.mpg.de/QuAnt/.\\n\\n1 INTRODUCTION\\n\\nHybrid computer vision methods that can be executed partially on a quantum computer (QC) are an emerging research area (Boyda et al., 2017; Cavallaro et al., 2020; Seelbach Benkner et al., 2021; Yurtsever et al., 2022). Compared to classical methods, they promise to solve computationally demanding (e.g., combinatorial) sub-problems faster, with improved scaling, and without relaxations that often lead to approximate solutions. Although quantum primacy has not yet been demonstrated in remotely practical usages of quantum computing, all existing quantum computer vision (QCV) methods fundamentally assume that it will be achieved in the future. Thus, solving these suitable algorithmic parts on a QC has the potential to reshape the field. However, reformulating them for execution on a QC is often non-trivial.\\n\\nQCV continues building up momentum, fuelled by accessible experimental quantum annealers (QA) allowing to solve practical (NP-hard) optimisation problems. Existing QCV methods using QAs rely on analytically deriving QUBOs (both QUBO matrices and solution encodings) for a specific problem type, which is challenging, especially since solutions need to be encoded as binary vectors (Li & Ghosh, 2020; Seelbach Benkner et al., 2020; 2021; Birdal et al., 2021). This often leads to larger encodings than necessary, severely impacting scalability. Alternatively, QUBO derivations with neural networks are conceivable but have not yet been scrutinised in the QA literature.\\n\\nIn stark contrast to the state of the art, this paper proposes, for the first time, to learn QUBO forms from data for any problem type using backpropagation (see Fig. 1). Our framework captures, in the weights of a neural network, the entire subset of QUBOs belonging to a problem type; a single forward pass yields the QUBO form for a given problem instance. It is thus a meta-learning approach in the context of hybrid (quantum-classical) neural network training, in which the superordinate network instantiates the parameters of the QUBO form. We find that sampling instantiated QUBOs can be a reasonable alternative to non-quantum neural baselines that regress the solution directly.\\n\\n1\"}"}
{"id": "isiQ5KIXbjj", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In 2D point set registration, we are given two point sets with potentially different numbers of points and no correspondences, and we seek to find a rotation angle that best aligns them. We follow Golyanik et al. (Golyanik & Theobalt, 2020) and use the vectorised form of their input matrix to represent a problem instance. We parametrise the solution space of $x \\\\in \\\\{0, 1\\\\}^9$ by splitting the output space $[0, 13\\\\pi]$ into $2^9$ equally sized bins and consecutively indexing them with a 9-bit integer.\\n\\nIn 3D rotation estimation, we are given two 3D point clouds with known matches and seek to estimate the 3D rotation aligning them. We represent a problem instance by the vectorised covariance matrix of the two point clouds; 3D rotation is parametrised by Euler angles $\\\\alpha, \\\\beta, \\\\gamma$. We discretise each angle into $2^5$ bins, such that $x \\\\in \\\\{0, 1\\\\}^{15}$.\\n\\n4 EXPERIMENTAL EVALUATION\\n\\nWe next experimentally evaluate QuAnt. Our goal is to show that it outperforms the previous quantum state of the art. For reference, we also report comparisons against specialised classical methods.\\n\\nData. We evaluate graph matching on the Willow object dataset (Cho et al., 2013), which contains labelled key points. We use $k=4$ randomly chosen key point pairs per image. We use 5640 images for training, and test on 846 images. Both of the sets are obtained via pygmtools (ThinkLab, 2021). We also evaluate on our synthetic dataset RandGraph (see Sec. 3.4), with both $k=4$ and $k=5$. We evaluate 2D point set registration on the 2D Shape Structure dataset (Carlier et al., 2016) providing 2D silhouette images of real-world objects. We treat the silhouette outlines as 2D points. We use 500 shapes from various classes for training, and test on 50 shapes. For each shape, we apply 1000 (for train) or 100 (for test) different rotations of up to $60^\\\\circ$ and pick random pairs to generate problem instances. For 3D rotation estimation, we evaluate on ModelNet10 (Wu et al., 2015), which contains CAD models of ten object categories. We proceed with point cloud representations of each shape. We use 300 shapes from various classes for training, and test on 30 shapes from various classes. For each shape, we apply 1000 different 3D rotations with angle ranges $\\\\alpha, \\\\gamma \\\\in [-19\\\\pi, 19\\\\pi]$ and $\\\\beta \\\\in [-118\\\\pi, 118\\\\pi]$ and pick random pairs to generate problem instances.\\n\\nComparisons. We compare QuAnt to two baselines and specialised methods, depending on the problem type. For all problem types, we demonstrate the power of using QUBOs compared to the Diag baseline that regresses a diagonal QUBO matrix (which is trivially solvable). While this baseline ablates the QUBO itself, we also consider a more natural neural network baseline, i.e., Pure, that regresses the binary solution directly (there is no activation after the last layer) and uses an $\\\\ell_1$-loss between the output and $\\\\hat{x}$ instead of $L_{\\\\text{gap}}$ and $L_{\\\\text{unique}}$. At test time, we threshold the network output of Pure at 0 to obtain binary vectors. For QuAnt, Diag and Pure variants, we experiment with all combinations of the numbers of layers $L \\\\in \\\\{3, 5\\\\}$ and hidden dimensions $H \\\\in \\\\{32, 78\\\\}$.\\n\\nWe compare our graph matching results with the Direct baseline on Willow (Cho et al., 2013); we directly solve the quadratic assignment problem given by $W$ with exhaustive search, which provides an upper bound for our method. We also compare against Quantum Graph Matching (QGM) (Seelbach Benkner et al., 2020), to which we pass our input matrices $W$. For 2D point set registration, we compare against the analytic quantum method (AQM) (Golyanik & Theobalt, 2020), which is an upper bound for our technique since we take its vectorised QUBO as input, and against the classical, specialised ICP algorithm (Lu & Milios, 1997). For 3D rotation estimation, we use Procrustes as a classical specialised method, which is thus an upper bound for our (general) method.\\n\\nMetrics. We measure accuracy of the graph matching solutions as the percentage of correctly recovered permutation matrices. For 2D point set registration and 3D rotation estimation, we quantify the difference between the known ground-truth rotations and the estimated rotations by their geodesic distances (angles) in the rotation groups $SO(2)$ and $SO(3)$, respectively.\\n\\nQUBO Solvers. For graph matching, we follow Sec. 3.3 to make our regressed QUBOs compatible with the QA. Due to a restricted QA compute budget, we train and test with simulated annealing unless stated otherwise. For the point cloud experiments, we regress dense $A$ and use our exhaustive search implementation at train and test time unless stated otherwise. When evaluating on the QA, we rely on minor embeddings to make the regressed $A$ compatible with the QA. Please refer to the Appendix for the details.\"}"}
{"id": "isiQ5KIXbjj", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1 Results\\n\\nGeneral Baselines.\\n\\nThe quantitative results for graph matching, point set registration, and rotation estimation are reported in Tables 1, 2, and 3, respectively. Across network sizes and all three problem types, the results show that having a full, $\\\\text{NP}$-hard QUBO (ours) instead of only a diagonal QUBO (Diag) is advantageous. We also find that the proposed method yields better results than Pure on both point set registration and rotation estimation, although Pure yields better results for graph matching.\\n\\nTable 1: Comparison to general baselines on graph matching. We report the accuracy (in %).\\n\\n| $L$ | $H$ | Ours | Diag | Pure |\\n|-----|-----|------|------|------|\\n| 3   | 32  | 9    | 8    | 91   |\\n| 3   | 78  | 30   | 18   | 96   |\\n| 5   | 32  | 11   | 11   | 89   |\\n| 5   | 78  | 49   | 43   | 96   |\\n\\nTable 2: Comparison to general baselines on point set registration. We report averages of the mean errors and their standard deviations over three runs.\\n\\n| $L$ | $H$ | Ours | Diag | Pure |\\n|-----|-----|------|------|------|\\n| 3   | 32  | 8.4  | 11.1 | 8.2  |\\n| 3   | 78  | 7.2  | 5.0  | 7.1  |\\n| 5   | 32  | 8.6  | 10.9 | 9.3  |\\n| 5   | 78  | 6.8  | 7.7  | 11.3 |\\n\\nTable 3: Comparison to general baselines on rotation estimation. We report averages of the mean errors and their standard deviations over three runs.\\n\\n| $L$ | $H$ | Ours | Diag | Pure |\\n|-----|-----|------|------|------|\\n| 3   | 32  | 5.9  | 5.4  | 7.9  |\\n| 3   | 78  | 4.1  | 5.0  | 7.1  |\\n| 5   | 32  | 3.7  | 5.0  | 16.2 |\\n| 5   | 78  | 3.4  | 4.7  | 10.1 |\\n\\nSpecialised Methods.\\n\\nFor reference, we compare QuAnt to methods specialised to a certain problem type. Since our approach is general, they mostly provide an upper bound for our performance. We evaluate QGM (Seelbach Benkner et al., 2020) on several RandGraph instances. We confirm their finding that strongly enforcing permutation constraints eventually retrieves the right permutation as the sample with the lowest energy. However, using the analytical bound for the penalty term leads to a success probability (i.e., the probability of getting the best solution across anneals) smaller than random guessing due to experimental errors in the couplings. Next, we find that the QUBOs of QuAnt are much smaller and better suited to be solved with a quantum annealer than QGM's. For RandGraph with $k = 5$, our method needs 15 physical qubits while their baseline and row-wise methods need 89 qubits on average and a chain length of four, and their Inserted method needs, on average, 39 qubits and a chain length of three on D-Wave Advantage. Thus, our success probability of 26% when evaluating on test data is orders of magnitude higher than Inserted's 0.22% (best in QGM). This shows how QuAnt improves over the quantum state of the art even though we merely focus on the solution with the lowest energy across anneals, while they focus on the success probabilities. We refer to Appendix D for a detailed evaluation. Table 1b confirms that Direct is an upper bound to our approach.\\n\\nFigure 3: Test-time example inputs and outputs of QuAnt trained for 2D point set registration. Table 8 shows quantitative results for 2D point set matching without noise. AQM slightly outperforms QuAnt, which is expected as we take AQM's QUBO as input; hence, its performance is an upper bound for our method. Fig. 3 shows a qualitative example. As expected, quantitative results in Table 7 (with no incorrect correspondences) show that classical, specialised Procrustes performs better than our general method on 3D rotation estimation. Note that our technique yields better results than Procrustes under test-time noise, as we discuss later in detail.\"}"}
{"id": "isiQ5KIXbjj", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 ABLATION\\n\\nWe ablate $L_{\\\\text{unique}}$ and $L_{\\\\text{mlp}}$ in Table 4. For graph matching, we use RandGraph with $k=4$. We find that removing either $L_{\\\\text{unique}}$ or $L_{\\\\text{mlp}}$ leads to mixed results on graph matching and worse results in almost all cases for point set registration and rotation estimation.\\n\\nTable 4: Loss ablations. We report accuracy for graph matching (in %) and mean/median error otherwise.\\n\\n|               | Graph Matching | Point Set Registration | Rotation Estimation |\\n|---------------|----------------|------------------------|---------------------|\\n|               |                |                        |                     |\\n| w/o $L_{\\\\text{unique}}$ |                |                        |                     |\\n| w/o $L_{\\\\text{MLP}}$  |                |                        |                     |\\n| Ours          |                |                        |                     |\\n| $L_{\\\\text{unique}}=3$, $H_{\\\\text{unique}}=32$ | 8               | 30                      | 30                   |\\n|               | 9               | 15.5                    | 15.5                 |\\n|               |                 | 8.0                     | 8.0                  |\\n|               |                 | 3.4                     | 3.4                  |\\n|               |                 | 2.5                     | 2.5                  |\\n|               |                 | 2.3                     | 2.3                  |\\n| $L_{\\\\text{unique}}=5$, $H_{\\\\text{unique}}=32$ | 14              | 30                      | 30                   |\\n|               | 6               | 18.1                    | 18.1                 |\\n|               | 11              | 19.0                    | 19.0                 |\\n|               |                 | 11.7                    | 11.7                 |\\n|               |                 | 3.4                     | 3.4                  |\\n|               |                 | 2.5                     | 2.5                  |\\n|               |                 | 2.3                     | 2.3                  |\\n| $L_{\\\\text{unique}}=5$, $H_{\\\\text{unique}}=78$ | 46              | 54                      | 54                   |\\n|               | 54              | 18.3                    | 18.3                 |\\n|               |                 | 11.7                    | 11.7                 |\\n|               |                 | 4.1                     | 4.1                  |\\n|               |                 | 3.3                     | 3.3                  |\\n\\n4.3 EVALUATION ON D-WAVE\\n\\nBy design, our QuAnt is agnostic to the type of QUBO solver used. After training with exhaustive search, we compare how the performance on the test set differs under exhaustive search, SA, or QA. The results in Fig. 4a show that the exact solutions of exhaustive search only slightly outperform the less computationally expensive QA and SA. Moreover, SA yields results very similar to QA.\\n\\n![Error in degree](image URL)\\n\\n(a) Errors on rotation estimation.\\n\\n![Evolution of result](image URL)\\n\\n(b) Evolution over different epochs of the Hamming distance between predicted solutions and the ground truth (top), and coupling matrix when training our approach for graph matching (bottom). (Top): The x-axis shows the Hamming distance. Blue indicates unprojected results, and red means after projection to a permutation. We only project after training.\\n\\nTable 5: QuAnt ($L_{\\\\text{unique}}=5$, $H_{\\\\text{unique}}=78$) on RandGraph ($k=5$) trained for 450 epochs (QA or SA).\\n\\n|               | SA       | QA       |\\n|---------------|----------|----------|\\n| Before projection | 10       | 18       |\\n| After projection | 24       | 36       |\\n\\nNext, we compare the test-time results of QA and SA (after training the method with the same technique, QA and SA, respectively). See Table 5 for the results, both with and without projecting the final binary solution to a valid permutation encoding during post-processing. Training with QA delivers better results than training with SA. We attribute this to a better second-best solution $x^+$ used by $L_{\\\\text{unique}}$. While SA yields solutions $x^*$ that are comparable to QA, its second-best solutions are worse than QA's. We refer to the appendix for details. Unfortunately, real-world compute resources for training with QA remain limited, as of this writing. We, therefore, fall back on SA for larger-scale experiments in this work. However, Table 5 suggests that our results could improve noticeably on QA.\\n\\n4.4 FURTHER ANALYSIS\\n\\nTraining. Fig. 4b visualises how the instantaneous solution and $A$ matrix evolve for graph matching.\\n\\nVarying Problem Difficulty. We provide a more detailed analysis of the performance of our method on point set registration for varying difficulty levels. Table 6 shows that a larger input misalignment between the two point clouds worsens the results, as expected.\"}"}
{"id": "isiQ5KIXbjj", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Interval analysis for point set registration with $L=5$, $H=78$. We evaluate on point cloud pairs with ground-truth angles uniformly sampled within the given intervals. We report the mean/median error in degrees.\\n\\n| angle interval | mean / median |\\n|----------------|---------------|\\n| $0 - \\\\frac{\\\\pi}{18}$ | 4.0 / 1.9 |\\n| $\\\\frac{\\\\pi}{18} - \\\\frac{2\\\\pi}{18}$ | 5.1 / 2.6 |\\n| $\\\\frac{2\\\\pi}{18} - \\\\frac{3\\\\pi}{18}$ | 6.4 / 3.0 |\\n| $\\\\frac{3\\\\pi}{18} - \\\\frac{4\\\\pi}{18}$ | 7.9 / 3.8 |\\n| $\\\\frac{4\\\\pi}{18} - \\\\frac{5\\\\pi}{18}$ | 7.9 / 3.8 |\\n| $\\\\frac{5\\\\pi}{18} - \\\\frac{6\\\\pi}{18}$ | 8.4 / 4.0 |\\n\\nTable 7: Robustness to varying amounts of incorrect test-time correspondences in rotation estimation. We report mean/median error for $L=3$, $H=32$. The first column specifies the percentage of incorrect correspondences at test time.\\n\\n| % | Ours | Procrustes | Diag | Pure |\\n|---|-----|----------|-----|------|\\n| 0 | 3.9 / 4.0 | 0.0 / 0.0 | 5.6 / 6.0 | 8.1 / 8.0 |\\n| 1 | 3.4 / 3.0 | 5.8 / 3.0 | 5.7 / 6.0 | 8.2 / 8.0 |\\n| 5 | 3.4 / 3.0 | 25.7 / 13.0 | 6.0 / 6.0 | 8.2 / 8.0 |\\n| 10 | 3.2 / 3.0 | 43.8 / 21.0 | 6.2 / 6.0 | 8.2 / 8.0 |\\n| 15 | 3.5 / 3.0 | 64.7 / 58.0 | 6.2 / 6.0 | 8.2 / 8.0 |\\n| 20 | 3.7 / 3.0 | 75.3 / 79.0 | 5.8 / 6.0 | 8.2 / 8.0 |\\n\\nRobustness to Noise. We investigate the robustness of our method and other approaches against input noise at test time after training without noisy data. We look at rotation estimation, where we randomly pick a fixed percentage of points and randomly permute their correspondences (among themselves).\\n\\nTable 8: Robustness to varying amounts of uniform noise in point set registration. We report mean/median error for $L=5$, $H=78$ and the number of logical/physical qubits. The first column states the range of the noise in % of the maximum extent of the point cloud. \u2020: uncoupled qubits.\\n\\n| % | Ours | Diag | Pure | AQM |\\n|---|-----|-----|-----|-----|\\n| 0 | 5.8 / 3.5 | 7.3 / 4.7 | 6.8 / 5.9 | 4.3 / 2.6 |\\n| 5 | 6.4 / 3.3 | 7.0 / 5.2 | 7.0 / 6.1 | 4.5 / 2.9 |\\n| 10 | 6.5 / 3.3 | 8.4 / 5.2 | 7.1 / 6.5 | 5.6 / 3.8 |\\n| 15 | 7.2 / 3.5 | 9.5 / 5.9 | 7.9 / 6.7 | 5.6 / 3.8 |\\n| 20 | 10.3 / 5.4 | 11.6 / 6.6 | 8.2 / 6.8 | 5.9 / 3.3 |\\n\\nQubits 9/14 (9/9)\u2020 n/a 21/55\\n\\nWe next look at point set registration under input noise at test time and after training without noise. Here, we add uniform noise to one point cloud, where the range of the noise is a percentage of the maximum extent of the point cloud. Table 8 contains the results. ICP, an iterative approach, is robust to the noise, gives highly accurate results and, thus, outperforms the competing non-iterative approaches. Since QuAnt takes as input the vectorised QUBO that AQM solves, AQM constitutes an upper bound for the performance of our approach. However, QuAnt could, in principle, scale to 3D point set matching while AQM's solution parametrisation severely inhibits scaling to larger problems. Finally, QuAnt performs better than the general baselines.\\n\\n5 DISCUSSION\\n\\nLimitations and Future Work. As all learning-based approaches, QuAnt can perform worse on problem instances that fall significantly outside the training distribution. While our general method does not outperform classical methods specialised on certain problem types, we achieve performance on par with hand-crafted QUBO designs used in state-of-the-art QCV methods. We achieve this while greatly reducing the effort required for new problem types. For our point cloud experiments, we rely on minor embeddings to transfer the regressed dense QUBOs to the QA. On existing hardware, large minor embeddings can worsen the resulting quality noticeably. However, we only need to embed a QUBO with nine logical qubits into 14 physical qubits. Although our focus is on a general design, our core idea of learning QUBOs can be specialised to any given problem type by designing a more specific network architecture and losses that capture priors for the problem type.\\n\\nConclusion. We showed that learning to regress QUBO forms for different problems instead of deriving them analytically can be a reasonable alternative to existing methods. We showed the generality of QuAnt on diverse problem types. Our experiments demonstrated that learning QUBO forms and solving them either on a quantum annealer or with simulated annealing, in most cases, leads to better results than directly regressing solutions. Moreover, QuAnt significantly outperformed the previous quantum state of the art in graph matching and rotation estimation in the setting with noise. We believe our work considerably broadens the available toolbox for development and analysis of quantum computer vision methods and opens up numerous avenues for future research.\"}"}
{"id": "isiQ5KIXbjj", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"APPENDIX\\n\\nThis appendix provides more details on adiabatic quantum computing in Sec. A. We provide training and implementation settings in Sec. B. Further details on the problem description for graph matching and a failure case are in Sec. C. Sec. D contains a deeper comparison with QGM (Seelbach Benkner et al., 2020) and Sec. E compares SA and QA. In Sec. F, we provide further quantitative and qualitative results on rotation estimation. Finally, Sec. G contains more details and experiments on point set registration.\\n\\nQUANTUM COMPUTING BACKGROUND\\n\\nA.1 Quantum Annealing in Detail\\n\\nAs we have seen, quantum annealing is a metaheuristic to solve the NP-hard Ising problem:\\n\\n$$\\\\arg \\\\min_{s \\\\in \\\\{-1, 1\\\\}^n} s^\\\\top J s + b^\\\\top s,$$\\n\\nwhere $s$ is a binary vector, $J \\\\in \\\\mathbb{R}^{n \\\\times n}$ is a matrix of couplings, and $b \\\\in \\\\mathbb{R}^n$ contains biases (McGeoch, 2014). Here, we give a brief overview of how this fits in the framework of quantum mechanics. D-Wave quantum annealers rely on magnetic fluxes in superconducting Niobium loops (Orlando et al., 1999). The direction of the current flowing through them can be modelled as a qubit, i.e., as a two-dimensional, complex, normalised vector $|\\\\psi\\\\rangle \\\\in \\\\mathbb{C}^2$ in the Dirac notation. In the so-called computational basis, the basis vectors correspond to the current flowing clockwise or anti-clockwise. After measuring the state, the system will collapse to either basis state. The absolute value of the complex-valued coefficients of the linear combination (probability amplitudes) is the probability of each outcome after measurement (e.g., clockwise or anti-clockwise current). The state space of $n \\\\in \\\\mathbb{N}$ qubits can be expressed with the tensor product $\\\\bigotimes_{i=1}^n \\\\mathbb{C}^2$ and is thus a $2^n$-dimensional complex vector space. We need that many parameters because entangled states cannot be described separately. If the two states $|\\\\psi\\\\rangle, |\\\\phi\\\\rangle$ corresponding to different physical systems (e.g., two niobium loops or two atoms) can be described independent from each other, the whole system is described by $|\\\\psi\\\\rangle \\\\otimes |\\\\phi\\\\rangle$. Note that if every state could be decomposed this way, one would only need $2^n$ parameters.\\n\\nThe evolution of a quantum state $|\\\\psi\\\\rangle$ over time can be described with the time-dependent Schr\u00f6dinger equation:\\n\\n$$H |\\\\psi\\\\rangle = i \\\\hbar \\\\frac{\\\\partial}{\\\\partial t} |\\\\psi\\\\rangle,$$\\n\\nwhere the Hamilton operator $H$ is a Hermitian Matrix describing the possible energies of the system, $i$ is the imaginary unit, $\\\\hbar$ is a constant, and $t$ denotes time. For adiabatic quantum computing, one needs a problem Hamiltonian $H_P$, where the eigenvector corresponding to the lowest eigenvalue is...\"}"}
{"id": "isiQ5KIXbjj", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a solution to the particular Ising problem, and an initial Hamiltonian \\\\( H_I \\\\) with an easy-to-prepare ground state. The Adiabatic Theorem (Born & Fock, 1928) states that if we start with the ground state of \\\\( H_I \\\\) and take a sufficiently long time \\\\( \\\\tau \\\\) to gradually change from \\\\( H_I \\\\) to \\\\( H_P \\\\), e.g.,\\n\\n\\\\[\\nH(t) = (1 - t\\\\tau)H_I + t\\\\tau H_P,\\n\\\\]\\n\\nthen we end up in the ground state of \\\\( H_P \\\\). From the latter, we can deduce the solution of the particular Ising problem. Simulating this whole process classically can be difficult (or even intractable) because we are dealing with \\\\( 2^n \\\\times 2^n \\\\) matrices, where \\\\( n \\\\) is the number of qubits. How difficult the classical simulation is, depends on the exact form of the Hamiltonians. (Particularly promising for speed-ups are, e.g., so-called non-stoquastic Hamiltonians (Albash & Lidar, 2018).)\\n\\n### A.2 Logical and Physical Qubits\\n\\nThe QUBO defines the couplings between two logical qubits \\\\( i \\\\) and \\\\( j \\\\). Such a QUBO can contain couplings between any two qubits. However, in contemporary hardware realisations, each physical qubit is only connected to a few others (see Fig. 5a and Fig. 5b). In the main paper, we show how the QUBO matrix \\\\( A \\\\) can account for this sparse connectivity pattern by setting entries between logical qubits \\\\( i \\\\) and \\\\( j \\\\) to 0 if the physical qubits \\\\( i \\\\) and \\\\( j \\\\) have no connection. Still, D-Wave supports denser connectivity patterns than what is implied by the hardware: Multiple physical qubits can be chained together to represent a single logical qubit of \\\\( x \\\\) that has many connections. The physical qubits in the chain will then have strong couplings along the chain to encourage them to all end up in the same final state (either all 0 or all 1), representing the final state of the corresponding logical qubit.\\n\\nThis is formalised as a minor embedding (of the connectivity graph of the logical qubits) into the connectivity graph of the physical qubits. Using the heuristic method of Cai and colleagues (Cai et al., 2014) is popular to determine the minor embeddings in practice.\\n\\n![Figure 5: Visualisation of qubit connectivities. (a) The connectivity pattern of the physical qubits in the Chimera architecture. The unit cells (green boxes) have fewer interconnections than on Pegasus. (b) The connectivity pattern of the physical qubits in the Pegasus architecture. Green, red, and yellow correspond to one problem instance each. Images due to D-Wave (D-Wave Systems, Inc.).](image)\\n\\n### B Implementation Details\\n\\nOur code, which we will release, is implemented in Pytorch (Paszke et al., 2019). We use Adam (Kingma & Ba, 2014) with a learning rate of \\\\( 10^{-3} \\\\) for training. For graph matching on \\\\( \\\\text{RandGraph} \\\\) with \\\\( k = 4 \\\\), we use a batch size of 141 and train for 150 epochs, which takes about seven hours. For \\\\( \\\\text{RandGraph} \\\\) with \\\\( k = 5 \\\\), we use 450 epochs, which takes about 23 hours. For \\\\( \\\\text{Willow} \\\\), we train for 300 epochs, which takes about 14 hours. The baselines are trained for the same number of epochs. While Diag takes a comparable amount of time, the Pure baseline takes about three minutes to train. For the experiments with \\\\( k = 5 \\\\), we use \\\\( 10^{-5} \\\\) as the learning rate.\"}"}
{"id": "isiQ5KIXbjj", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nFor the point cloud experiments, we use a batch size of 32 and train for 20 epochs, which takes about four hours. We train Pure and Diag with the same batch sizes and epochs. The training of the Diag baseline takes four hours as well, and Pure is trained within three hours. When solving a QUBO on a QA, we anneal 100 times and pick the lowest-energy solution. We access the QA via Leap 2 (D-Wave Systems, 2022) using the Ocean SDK (D-Wave Systems, Inc., 2022c). When solving with SA, we use 100 iterations from the default neal SA sampler.\\n\\nC.1 Problem Description\\n\\nHere, we describe the design of the problem description $p$ for graph matching. We use $p = \\\\text{vec}(W)$, where the diagonal of $W$ contains cosine similarities between the feature vectors extracted with AlexNet (Krizhevsky et al., 2012) pre-trained on ImageNet (Deng et al., 2009) of all pairs of key points. The off-diagonal follows the geometric term described in (Eq. (7)) from Torresani et al. (Torresani et al., 2008). In particular, we use the term $W_{\\\\text{geom}}$ from Eq. (7) from Torresani et al. (Torresani et al., 2008) with minus signs in the beginning and in the exponential, and set $\\\\eta = 0$. The convex combination with $W_{\\\\text{Alex}}$, where the cosine similarities of the feature vectors are on the diagonal, is then:\\n\\n$$W_{\\\\text{Alex}} = \\\\tau W_{\\\\text{Alex}} + (1 - \\\\tau) W_{\\\\text{geom}}.$$  (9)\\n\\nWe choose $\\\\tau = 0.81$ and $\\\\eta = 0.98$ such that the QAP often coincides with the ground-truth correspondences (see Table 1b, \u201cDirect\u201d from the main paper).\\n\\nC.2 Failure Case\\n\\nWe show a failure case of our method when applied to graph matching in Fig. 6. It occurs due to large differences in the observed appearance.\\n\\nFigure 6: Failure case for graph matching. We visualise the ground truth of an image pair. Here, our method does not find the correct matching: Only the beak and neck are matched correctly, while the geometric information for the other key points differs too strongly.\\n\\nD Detailed Comparison with QGM (Seelbach Benkner et al., 2020)\\n\\nHere, we compare our method with QGM (Seelbach Benkner et al., 2020) in detail. Note that the focus of both works differs. Their work focuses more on the probability distribution of the retrieved solutions. Our work is more concerned with incorporating the quantum annealer into the training pipeline. When training the neural network, $L_{\\\\text{gap}}$ equation 1 uses the retrieved solution with the smallest energy across anneals, while they are also interested in the success probabilities, i.e., the probability to get the best solution across anneals.\\n\\nThe individual QUBOs occurring in our QuAnt framework are much easier to solve by the QA than QUBOs that would arise in QGM (Seelbach Benkner et al., 2020). To show this, we compute the average success probabilities of the various methods from QGM (Seelbach Benkner et al., 2020).\"}"}
{"id": "isiQ5KIXbjj", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Average success probabilities of different QGM variants (Seelbach Benkner et al., 2020) over 141 problem instances on RandGraph compared to QuAnt with $k = 5$, in %.\\n\\n| Method          | Success Probability |\\n|-----------------|---------------------|\\n| Inserted Baseline Row-wise Ours | 0.22 0.07 0.07 |\\n\\nWe solve the resulting QUBO with QA and find the average probability to be 26% with a standard deviation of 18%, better than any method from the QGM paper (Seelbach Benkner et al., 2020). This difference is not surprising since we construct our method such that we only use trivial embeddings and do not need to apply the minorminer heuristic (Cai et al., 2014). Because of that, for RandGraph with $k = 5$, our method needs only 15 physical qubits while their baseline and row-wise methods need 89 qubits, on average, and a chain length of 4; their Inserted method needs, on average, 39 qubits and a chain length of 3 on D-Wave Advantage. Note that a heuristic search for better penalty parameters, as in Q-Sync (Birdal et al., 2021), could give rise to better results for the methods from QGM (Seelbach Benkner et al., 2020) in Table 9. However, the corresponding embeddings would still be problematic. Directly using the binary encoding for permutations (Gaitan & Clark, 2014) requires additional qubits because the problem would a priori not be quadratic.\\n\\nIn the main paper, we show that training with QA yields better performance than training with SA. Here, we analyse the quality of the solutions found by both techniques further. Fig. 7 contains histograms that depict the output of the quantum annealer and the two different simulated annealing solvers from neal (D-Wave Systems, Inc., 2022b) and from dimod (D-Wave Systems, Inc., 2022a). In contrast to the solver from dimod, neal is highly optimised for performance, so we used it for our experiments.\\n\\nWe focus our analysis on the number of sweeps in SA, i.e., the number of steps in the \u2018cooling\u2019 schedule. We observe that it strongly influences the quality of the second-best solution.\\n\\nTable 10 illustrates this by averaging the fraction of the second-best energies over 141 instances and analysing 1000 samples from different solvers. We see that the quantum annealer produces the second-best samples with the lowest energies. Note, however, that we do not claim that this is an intrinsic general advantage of QA over SA, but merely that in our setting, QA outperforms SA. Still, prior work (Willsch et al., 2020) also reaches the conclusion that quantum annealing has much potential for finding reasonable near-optimal solutions.\\n\\nThe dimod sampler also produces second-best solutions with low energies but is computationally expensive (D-Wave Systems, Inc., 2022a). This is, perhaps, because many non-optimal solutions are produced compared to the implementation from neal.\"}"}
{"id": "isiQ5KIXbjj", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Second-best energies of SA relative (in %) to the second-best energies of QA. We report the mean and std. deviation over 141 instances. The higher the better.\\n\\n| Method            | Mean Energy | Std. Dev. |\\n|-------------------|-------------|-----------|\\n| SA (neal)         | 94.2 \u00b1 10.3 |           |\\n| 10 sweeps SA (neal) | 86.0 \u00b1 19  |           |\\n| 99.8 \u00b1 0.9        |             |           |\\n| 10 sweeps SA (dimod) | 99.8 \u00b1 0.9 |           |\\n\\nTable 11: Comparison to general baselines on rotation estimation. We report the mean of the per-experiment median and std. deviation across experiments for three different random seeds.\\n\\n| Method    | Mean Energy | Std. Dev. |\\n|-----------|-------------|-----------|\\n| Ours      | 6.0 \u00b1 3.6   |           |\\n| Diag Pure | 5.3 \u00b1 1.1   |           |\\n| Diag Pure | 7.0 \u00b1 1.0   |           |\\n| L=3, H=3  | 4.0 \u00b1 1.0   |           |\\n| L=3, H=78 | 5.0 \u00b1 0.0   |           |\\n| L=5, H=32 | 3.7 \u00b1 1.2   |           |\\n| L=5, H=78 | 3.7 \u00b1 0.6   |           |\\n\\nTo better judge the stability under different random seeds, we repeat the main experiment from the paper three times for our QuAnt method and each baseline. In Table 11, we report the mean and std. deviation of the median. Here, similar to the results from the main paper, we outperform the Diag and Pure baselines in all but one setting.\\n\\nTable 12 shows how our method performs on noise-free and noisy test data after training on noisy data. We observe that the noisy training data appears to negatively affect the training and its performance drops, while Diag improves and Pure remains unchanged.\\n\\nFig. 9 visualises differences between our solution after training on noise-free data and Procrustes alignment on a problem with noisy data (unknown correspondences).\"}"}
{"id": "isiQ5KIXbjj", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12: Robustness to varying amounts of incorrect test-time correspondences in rotation estimation. We report the mean/median error for $L = 3$, $H = 32$. The first column specifies the percentage of incorrect correspondences at test time.\\n\\n(a) Training without noise\\n\\n|          | Ours | Procrustes | Diag | Pure |\\n|----------|------|------------|------|------|\\n| 0%       | 3.9 / 4.0 | 0.0 / 0.0 | 5.6 / 6.0 | 8.1 / 8.0 |\\n| 1%       | 3.4    | 5.8 / 3.0  | 5.7 / 6.0 | 8.2 / 8.0 |\\n| 5%       | 3.4    | 25.7 / 13.0 | 6.0 / 6.0 | 8.2 / 8.0 |\\n| 10%      | 3.2    | 43.8 / 21.0 | 5.2 / 5.0 | 8.2 / 8.0 |\\n| 15%      | 3.5    | 64.7 / 58.0 | 5.0 / 5.0 | 8.2 / 8.0 |\\n| 20%      | 3.7    | 75.3 / 79.0 | 4.9 / 5.0 | 8.2 / 8.0 |\\n\\n(b) Training with 10% incorrect correspondences\\n\\n|          | Ours | Procrustes | Diag | Pure |\\n|----------|------|------------|------|------|\\n| 0%       | 4.4 / 4.0 | 0.0 / 0.0 | 4.6 / 5.0 | 8.1 / 8.0 |\\n| 1%       | 4.3    | 5.8 / 3.0  | 5.1 / 5.0 | 8.3 / 8.0 |\\n| 5%       | 5.0    | 25.7 / 13.0 | 5.1 / 5.0 | 8.2 / 8.0 |\\n| 10%      | 5.2    | 43.8 / 21.0 | 5.2 / 5.0 | 8.2 / 8.0 |\\n| 15%      | 5.2 / 5.0 | 64.7 / 58.0 | 5.0 / 5.0 | 8.2 / 8.0 |\\n| 20%      | 5.6 / 6.0 | 75.3 / 79.0 | 4.9 / 5.0 | 8.2 / 8.0 |\\n\\nFigure 9: Comparison of initial input rotation, our method and Procrustes. The initial 3D point cloud is blue and the rotated one is red, where the unknown correspondences are displayed in green. Here, 10% of the correspondences are unknown.\\n\\nF.5 COMPARISON TO AQM (GOLYANIK & THEOBALT, 2020)\\n\\nQuAnt can estimate 3D rotations with known point matches. However, AQM (Golyanik & Theobalt, 2020) would require 81 densely connected logical qubits, which is not supported by the current quantum-hardware generations. Hence, we cannot compare against AQM for this problem setting and instead only compare to classical methods such as Procrustes.\\n\\nG.2 VARIANCE\\n\\nTable 13: Comparison of QuAnt to general baselines on point set registration. We report the mean of the per-experiment median and std. deviation across experiments for three different random seeds.\\n\\n|          | Ours | Diag | Pure |\\n|----------|------|------|------|\\n| $L = 3$, $H = 32$ | 4.8 \u00b1 0.6 | 6.8 \u00b1 0.2 | 5.8 \u00b1 0.9 |\\n| $L = 3$, $H = 78$ | 3.7 \u00b1 0.3 | 5.1 \u00b1 0.4 | 4.8 \u00b1 0.3 |\\n| $L = 5$, $H = 32$ | 4.6 \u00b1 0.1 | 7.1 \u00b1 1.0 | 7.1 \u00b1 1.6 |\\n| $L = 5$, $H = 78$ | 3.4 \u00b1 0.1 | 4.9 \u00b1 0.0 | 7.9 \u00b1 2.7 |\\n\\nIn Table 13, we report the mean median. Here, we outperform the baselines in all cases. In nearly all setups, we outperform the baselines, and only for one case, we are on par with the Pure baseline. Still, even in that case, QuAnt performs more consistently, as evidenced by its lower std. deviation. These experiments show that we consistently outperform the baselines and the performance is not dependent on the random seed.\"}"}
{"id": "isiQ5KIXbjj", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 14: Robustness to varying amounts of uniform noise. We report the mean/median error for $L = 3$, $H = 32$.\\n\\n|                | Ours | Diag | Pure | AQM |\\n|----------------|------|------|------|------|\\n| 0%             | 7.4  | 4.2  | 11.3 | 7.0  |\\n| 5%             | 6.7  | 3.8  | 11.7 | 7.3  |\\n| 10%            | 7.2  | 4.5  | 12.2 | 6.8  |\\n| 15%            | 8.2  | 4.9  | 12.6 | 6.8  |\\n| 20%            | 11.0 | 6.0  | 13.9 | 8.2  |\\n\\nIn addition to the experiments in the main paper that uses the largest architecture, we also test the noise resistance on the smallest network setup with $L = 3$, $H = 32$; see Table 14. Here, while QuAnt is better than the baselines, we do not outperform AQM. However, by construction, AQM is an upper bound for our method as the matrix introduced by Golyanik et al. (Golyanik & Theobalt, 2020) is the same as our input into the network, but it gets directly solved by the QA.\\n\\nG.4 Qualitative Results\\n\\nIn addition to the quantitative loss ablation in the paper, we visualise the effect of the losses here.\\n\\nIn Figure 10, the full loss results in a nearly ground-truth rotation. However, if we leave out $L_{\\\\text{gap}}$ or $L_{\\\\text{MLP}}$ during training, a significant reduction in rotational accuracy is visible.\\n\\n![Initial problem instance](image1)\\n\\n![Ours with the full loss](image2)\\n\\n![Ours without $L_{\\\\text{MLP}}$](image3)\\n\\n![Ours without $L_{\\\\text{unique}}$](image4)\\n\\nFigure 10: Qualitative loss ablations. We show the original point cloud (blue) and rotated point cloud (red). Removing either $L_{\\\\text{unique}}$ or $L_{\\\\text{MLP}}$ leads to significantly worse results.\\n\\nG.5 Failure Case\\n\\nWe continue our analysis with failure cases. Results in the main paper show that an increasing input angle leads to a reduction in the accuracy of our regressed angle. This can be traced back to our problem-instance encoding as Golyanik et al. (Golyanik & Theobalt, 2020) mention that an increasing angle makes finding the correspondences more error-prone. Therefore, an imperfect input encoding makes it more likely for us to regress wrong angles.\\n\\nSimilar to most prior work on point set registration, nearly symmetric shapes can be difficult, as most points can be nearly perfectly aligned even with wrong rotations. Fig. 11 contains such a failure case. The initial angle of this problem instance, $50.8^\\\\circ$, is relatively large for our setup, and the shape (which looks like the silhouette of a fish) is nearly rotationally symmetric. In cases like this, our method has difficulties regressing the correct rotation after a single QUBO sampling.\"}"}
{"id": "isiQ5KIXbjj", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 11: Example of a failure case in point set registration. We show the initial image (blue) as well as the rotated image (red).\"}"}
{"id": "isiQ5KIXbjj", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"REFERENCES\\n\\nSource code for dwave.system.composites.tiling. https://docs.ocean.dwavesys.com/projects/system/en/stable/_modules/dwave/system/composites/tiling.html. Accessed: 2022-02-09.\\n\\nTameem Albash and Daniel A Lidar. Adiabatic quantum computation. Reviews of Modern Physics, 90(1):015002, 2018.\\n\\nFederica Arrigoni, Willi Menapace, Marcel Seelbach Benkner, Elisa Ricci, and Vladislav Golyanik. Quantum motion segmentation. In European Conference on Computer Vision (ECCV), 2022.\\n\\nKerstin Beer, Dmytro Bondarenko, Terry Farrelly, Tobias J. Osborne, Robert Salzmann, Daniel Scheiermann, and Ramona Wolf. Training deep quantum neural networks. Nature Communications, 11(808), 2020.\\n\\nJacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd. Quantum machine learning. Nature, 549, 11 2016.\\n\\nTolga Birdal, Vladislav Golyanik, Christian Theobalt, and Leonidas Guibas. Quantum permutation synchronization. In Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nKelly Boothby, Paul Bunyk, Jack Raymond, and Aidan Roy. Next-Generation Topology of D-Wave Quantum Processors. arXiv e-prints, 2020.\\n\\nMax Born and Vladimir Fock. Beweis des adiabatensatzes. Zeitschrift f\u00fcr Physik, 51(3):165\u2013180, 1928.\\n\\nEdward Boyda, Saikat Basu, Sangram Ganguly, Andrew Michaelis, Supratik Mukhopadhyay, and Ramakrishna R. Nemani. Deploying a quantum annealing processor to detect tree cover in aerial imagery of California. PLoS ONE, 12, 2017.\\n\\nJun Cai, William G. Macready, and Aidan Roy. A practical heuristic for finding graph minors. arXiv e-prints, 2014.\\n\\nAxel Carlier, Kathryn Leonard, Stefanie Hahmann, Geraldine Morin, and Misha Collins. The 2d shape structure dataset: A user annotated open access database. Computers & Graphics, 58:23\u201330, 2016.\\n\\nGabriele Cavallaro, Dennis Willsch, Madita Willsch, Kristel Michielsen, and Morris Riedel. Approaching remote sensing image classification with ensembles of support vector machines on the d-wave quantum annealer. In IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2020.\\n\\nTat-Jun Chin, David Suter, Shin-Fang Ch\u2019ng, and James Quach. Quantum robust fitting. In Proceedings of the Asian Conference on Computer Vision (ACCV), November 2020.\\n\\nMinsu Cho, Karteek Alahari, and Jean Ponce. Learning graphs to match. In International Conference on Computer Vision, pp. 25\u201332, 2013.\\n\\nD-Wave Systems. Leap. https://docs.dwavesys.com/docs/latest/leap.html, 2022. online; accessed on the 24.01.2022.\\n\\nD-Wave Systems, Inc. D-wave system documentation. https://docs.dwavesys.com/en/stable, 2022c. online.\\n\\nD-Wave Systems, Inc. dimod simulated annealing sampler. https://test-projecttemplate-dimod.readthedocs.io/en/latest/reference/sampler_composites/samplers.html, 2022a. online.\\n\\nD-Wave Systems, Inc. dwave-neal. https://github.com/dwavesystems/dwave-neal, 2022b. online.\"}"}
{"id": "isiQ5KIXbjj", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prasanna Date and Thomas Potok. Adiabatic quantum linear regression.\\n\\nScientific Reports, 11, 2021.\\n\\nNike Dattani, Szilard Szalay, and Nick Chancellor. Pegasus: The second connectivity graph for large-scale quantum annealing hardware.\\narXiv e-prints, 2019.\\n\\nB Dema, Junya ARAI, and Keitarou HORIKAW A. Support vector machine for multiclass classification using quantum annealers. 2020.\\n\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, pp. 248\u2013255. IEEE, 2009.\\n\\nAnh-Dzung Doan, Michele Sasdelli, David Suter, and Tat-Jun Chin. A hybrid quantum-classical algorithm for robust fitting. In Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nYuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, and Dacheng Tao. Expressive power of parametrized quantum circuits.\\n\\nPhys. Rev. Research, 2:033125, 2020.\\n\\nVedran Dunjko and Hans J Briegel. Machine learning & artificial intelligence in the quantum domain: a review of recent progress.\\n\\nReports on Progress in Physics, 81(7):074001, 2018.\\n\\nVedran Dunjko, Jacob M. Taylor, and Hans J. Briegel. Quantum-enhanced machine learning.\\n\\nPhys. Rev. Lett., 117:130501, 2016.\\n\\nEdward Farhi, Jeffrey Goldstone, Sam Gutmann, Joshua Lapan, Andrew Lundgren, and Daniel Preda. A quantum adiabatic evolution algorithm applied to random instances of an np-complete problem.\\n\\nScience, 292(5516):472\u2013475, 2001.\\n\\nAaron Ferber, Bryan Wilder, Bistra Dilkina, and Milind Tambe. Mipaal: Mixed integer program as a layer.\\n\\nAAAI Conference on Artificial Intelligence, 34:1504\u20131511, 2020.\\n\\nThomas Gabor, Sebastian Feld, Hila Safi, Thomy Phan, and Claudia Linnhoff-Popien. Insights on training neural networks for qubo tasks. In Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops, pp. 436\u2013441, 2020.\\n\\nFrank Gaitan and Lane Clark. Graph isomorphism and adiabatic quantum computing.\\n\\nPhys. Rev. A, 89:022342, Feb 2014. doi: 10.1103/PhysRevA.89.022342. URL https://link.aps.org/doi/10.1103/PhysRevA.89.022342.\\n\\nVladislav Golyanik and Christian Theobalt. A quantum computational approach to correspondence problems on point sets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9182\u20139191, 2020.\\n\\nVojt\u02c7ech Havl \u00b4\u0131\u02c7cek, Antonio D. C \u00b4orcoles, Kristan Temme, Aram W. Harrow, Abhinav Kandala, Jerry M. Chow, and Jay M. Gambetta. Supervised learning with quantum-enhanced feature spaces.\\n\\nNature, 567:209\u2013212, 2019.\\n\\nSami Khairy, Ruslan Shaydulin, Lukasz Cincio, Yuri Alexeev, and Prasanna Balaprakash. Learning to optimize variational quantum circuits to solve combinatorial problems. In AAAI Conference on Artificial Intelligence, 2020.\\n\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\\n\\narXiv preprint arXiv:1412.6980, 2014.\\n\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 25, 2012.\\n\\nJonas M. K\u00a8ubler, Simon Buchholz, and Bernhard Sch\u00a8olkopf. The inductive bias of quantum kernels. In Neural Information Processing Systems (NIPS), 2021.\\n\\nYann LeCun, Sumit Chopra, Raia Hadsell, Fu Jie Huang, and et al. A tutorial on energy-based learning. In Predicting Structured Data. MIT Press, 2006.\"}"}
{"id": "isiQ5KIXbjj", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "isiQ5KIXbjj", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "isiQ5KIXbjj", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: We propose QuAnt for QUBO learning, i.e., a quantum-classical meta-learning algorithm that avoids analytical QUBO derivations by learning to regress QUBOs to solve problems of a given type. We first represent a problem instance as a vector $p$ and then feed it into an MLP that regresses the entries of the QUBO matrix $A$. We then initialise a quantum annealer with $A$ and use quantum annealing to find a QUBO minimiser and extract it as the solution $x^*$ to the problem instance. We define losses involving $x^*$ that avoid backpropagation through the annealing and backpropagate gradients through the MLP to train it. We demonstrate the generalisability of QuAnt on graph matching, point set registration, and rotation estimation. In particular, we show how a (combinatorial) quantum annealing solver can be integrated into a vanilla neural network as a custom layer and be used in the forward and backward passes, which may be useful in other contexts. To that end, we introduce a contrastive loss that circumvents the inherently discontinuous and non-differentiable nature of QUBO solvers. Our method is compatible with any QUBO solver at training and test time\u2014we consider parallelised exhaustive search, simulated annealing, and quantum annealing. QUBO learning, i.e., determining a function returning QUBO forms given a problem instance of some problem type as input, is a non-trivial and challenging task. In summary, this paper makes several technical contributions to enable QUBO learning:\\n\\n1. QuAnt, i.e., a new meta-learning approach to obtain QUBO forms executable on modern QAs for computer vision problems. While prior methods rely on analytical derivations, we learn QUBOs from data (Sec. 3.1).\\n\\n2. A new training strategy for neural methods with backpropagation involving finding low-energy solutions to instantaneous (optimised) QUBO forms, independent of the solver (Secs. 3.2 and 3.3).\\n\\n3. Application of the new framework to several problems with solutions encoded by permutations and discretised rigid transformations (Secs. 3.4 and 3.5).\\n\\nWe show that our methodology is a standardised way of obtaining QUBOs independent of the target problem type. This paper focuses on three problem types already tackled by QCV methods relying on analytical QUBO derivations: graph matching and point set alignment (with and without known prior point matches in the 3D and 2D cases, respectively). We emphasise that we do not claim to outperform existing specialised methods for these problem types or that QA is particularly well-suited for them. Rather, we show that this wide variety of problems can be tackled successfully and competitively by our general quantum approach already now, before quantum primacy. Thus, in the future, computer vision methods may readily benefit from the (widely expected) speed-up of QC through an easy and flexible re-formulation of algorithmic parts as QUBOs, thanks to our proposed method. We run our experiments on D-Wave Advantage5.1 (Dattani et al., 2019), an experimental realisation of AQC with remote access. This paper assumes familiarity with the basics of quantum computing. For convenience, we summarise several relevant definitions in the Appendix.\"}"}
{"id": "isiQ5KIXbjj", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"variational layer, i.e., a sequence of parametrised unitary transformations meant for execution on quantum hardware (Mitarai et al., 2018). QML methods are often optimised using variants of the backpropagation algorithm (McClean et al., 2018; Verdon et al., 2019; Beer et al., 2020). Quantum variational models were recently applied to combinatorial optimisation (Khairy et al., 2020) and reinforcement learning (Dunjko et al., 2016; Lockwood & Si, 2020). Instead of learning to regress unitary transformation parameters for gate-based QC, we learn to regress QUBO forms for QA. In contrast to gate-based machines, QAs can already solve real problems formulated as QUBOs (Neukart et al., 2017; Teplukhin et al., 2019; Stollenwerk et al., 2019; Orus et al., 2019; Mato et al., 2021; Speziali et al., 2021). Recently, QCV has rapidly transitioned from theoretical considerations (Venegas-Andraca & Bose, 2003; Chin et al., 2020; Neven et al., 2008b;a) to practical algorithms leveraging quantum-mechanical effects of quantum computers, ranging from image retrieval and processing (Venegas-Andraca & Bose, 2003; Yan et al., 2016), classification (Boyda et al., 2017; Nguyen & Kenyon, 2019; Cavallaro et al., 2020; Willsch et al., 2020; Dema et al., 2020) and tracking (Li & Ghosh, 2020; Zaech et al., 2022), to problems on graphs (Zick et al., 2015; Seelbach Benkner et al., 2020; Mariella & Simonetto, 2021), consensus maximisation (Doan et al., 2022), shape alignment (Noormandipour & Wang, 2021; Seelbach Benkner et al., 2021), segmentation (Arrigoni et al., 2022) and ensuring cycle-consistency (Birdal et al., 2021; Yurtsever et al., 2022). Many of these methods are evaluated on real quantum hardware, as both gate-based and QA machines can be accessed remotely (D-Wave Systems, 2022; Rigetti Computing, 2022).\\n\\nWe demonstrate the efficacy of our QuAnt approach on the applications of graph matching and point set alignment where we compare against recent quantum state-of-the-art methods (Seelbach Benkner et al., 2020; Golyanik & Theobalt, 2020), respectively.\\n\\nAnother line of work in different domains concerns learning the best adiabatic quantum algorithm. While some works (Pastorello et al., 2021; Pastorello & Blanzieri, 2019) develop an algorithm inspired by tabu search, our method uses a neural network to output a coupling matrix. Orthogonal to our work, others train neural networks to solve problem-specific QUBOs (Gabor et al., 2020). Nu\u00dflein et al. (2022) optimize a blackbox function by finding a QUBO as surrogate model. QML on gate-based QC has been studied at length, but machine learning with QA remains largely underexplored, with only a few exceptions (e.g., linear regression (Date & Potok, 2021) and binary neural networks (Sasdelli & Chin, 2021)). In stark contrast to existing QCV methods with analytically derived QUBOs (Li & Ghosh, 2020; Seelbach Benkner et al., 2020; 2021; Birdal et al., 2021), our approach enables more flexible and compact solution encodings.\\n\\nQuAnt is also related to recent non-quantum approaches that aim to improve combinatorial optimisation by seamlessly integrating deep learning and combinatorial building blocks as custom layers and backpropagating through them (Ferber et al., 2020; Rol\u00b4\u0131nek et al., 2020; Vlastelica et al., 2020). In this respect, ours is the first work that uses a quantum QUBO solver in neural architectures.\\n\\n### 3 QUBO LEARNING APPROACH\\n\\nWe present a new meta-learning approach for regressing quadratic unconstrained binary optimisation problems (QUBOs) suitable for modern quantum annealers (QA); see Fig. 1. While existing works analytically derive QUBOs for different problems (Birdal et al., 2021; Seelbach Benkner et al., 2020; 2021; Zaech et al., 2022), we propose to instead learn a function that turns a problem instance into a QUBO to be solved by a QA. Specifically, we train a multi-layer perceptron (MLP) that takes a vectorised problem instance and regresses the QUBO weights such that the QUBO minimiser is the solution to the problem. Note that we only specify the bit encoding of the solution but let the network learn to derive QUBOs. Crucially, we show how training the MLP is possible despite quantum annealing (like any QUBO solver) being discontinuous and non-differentiable.\\n\\n### 3.1 QUBO AND QUANTUM ANNEALING\\n\\nQuantum annealing is a metaheuristic to solve NP-hard problems of the form\\n\\\\[\\n\\\\arg\\\\min_{s} \\\\left( s^\\\\top J s + b^\\\\top s \\\\right),\\n\\\\]\\nwhere \\\\( s \\\\) is a binary vector, \\\\( J \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) is a matrix of couplings, and \\\\( b \\\\in \\\\mathbb{R}^{n} \\\\) contains biases (McGeoch, 2014). We can rewrite this as a QUBO:\\n\\\\[\\n\\\\arg\\\\min_{x} \\\\left( x^\\\\top A x \\\\right),\\n\\\\]\\nby substituting \\\\( x = \\\\frac{1}{2} (s + 1/n) \\\\) and \\\\( A = \\\\frac{1}{4} J + \\\\frac{1}{2} (J^\\\\top + J) + \\\\frac{1}{2} \\\\text{diag}(b) \\\\).\"}"}
{"id": "isiQ5KIXbjj", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In quantum annealing, the binary $n$-dimensional vector $x$ describes the measurement outcomes of $n$ qubits. Annealing starts out with an equal superposition quantum state of the qubits that assigns an equal probability to all possible binary states $\\\\{0, 1\\\\}^n$. During the anneal, the couplings and biases of $A$ are gradually imposed on the qubits. The adiabatic theorem (Born & Fock, 1928) implies that doing so sufficiently slow forces the qubits into a quantum state that assigns nonvanishing probability only to binary states that minimise the QUBO (Farhi et al., 2001). We then only need to measure the qubits to determine their binary state, which is our solution $x$. For a more detailed description of quantum annealing, we refer to prior computer-vision works (Seelbach Benkner et al., 2020; 2021; Golyanik & Theobalt, 2020; Li & Ghosh, 2020) and Appendix A.\\n\\n3.2 Network Architecture and Losses\\n\\nIn this section, we describe the network architecture that takes as input a problem instance and regresses a QUBO whose solution (e.g., obtained via quantum annealing) solves the problem instance. For a given problem type, we require a problem description that is amenable to QUBO learning: a parametrisation of problem instances as real-valued vectors $p \\\\in \\\\mathbb{R}^m$, and a parametrisation of solutions as binary vectors $x \\\\in \\\\{0, 1\\\\}^n$. Since we use supervised training, we additionally need a training set $D = \\\\{(p_d, \\\\hat{x}_d)\\\\}_{d=1}^D$ containing $D$ problem instances $p_d$ with ground-truth solutions $\\\\hat{x}_d$.\\n\\nWe use a multilayer perceptron (MLP) with $L$ layers and $H$ hidden dimensions, ReLU activations (except for the last layer, which uses $\\\\sin$ (Sitzmann et al., 2020)), and concatenating skip connections from the input into odd-numbered layers (except for the first and last layers). The input to the network is a problem instance $p$, and the output is a QUBO matrix $A$:\\n\\n$$A = \\\\text{MLP}(p).$$\\n\\nWe could now use a dataset of problem instances and corresponding $A$ to supervise the MLP directly. However, this requires specifying how $A$ is to be derived for a certain instance, which comes with two downsides: (1) A problem-type-specific algorithm for analytically deriving instance-specific $A$ needs to be designed to generate enough training data $\\\\{(p_d, A_d)\\\\}_{d=1}^D$, which is non-trivial, and (2) The binary parametrisation ($x$) of the solution space depends on the algorithm, which can lead to more variables than intrinsically needed by the problem (e.g., if $x$ needs to represent one of $k$ numbers, a one-hot parametrisation would have length $n = k$, while a binary-encoding parametrisation would have length $n = \\\\log k$). This is particularly problematic as contemporary quantum hardware only provides a limited number of qubits. We thus choose to supervise $A$ not directly and, instead, supervise the solutions of the QUBO. This strategy tackles both issues as it lets the network learn an algorithm compatible with the (potentially shorter) solution parametrisation. Therefore, our method is easily applicable to new problem types, as we show in Secs. 3.4 and 3.5.\\n\\nThe regressed $A$ defines a QUBO, which can be solved by any QUBO solver. But how can we, during training, backpropagate gradients from the solution binary vector $x$ through the QUBO solver despite these solvers having zero gradients almost everywhere (Vlastelica et al., 2020)? We circumvent this issue by exploiting a contrastive loss (cf. LeCun et al. (2006, Eq. (10))) as follows:\\n\\nWe know the energy of the ground-truth solution $\\\\hat{x}$ of the problem instance, namely $\\\\hat{x}^\\\\top A \\\\hat{x}$. If the energy of the minimiser $x^* = x^*(A)$ of the current QUBO is lower, then $A$ does not yet describe a QUBO that outputs the right solution. We, therefore, seek to push the energy of $\\\\hat{x}$ lower while pulling the energy of the minimiser $x^*$ up:\\n\\n$$L_{\\\\text{gap}} = \\\\hat{x}^\\\\top A \\\\hat{x} - x^*\\\\text{sign}(A) x^*, \\\\quad (1)$$\\n\\nwhich has a zero gradient if $x^* = \\\\hat{x}$, as desired. $L_{\\\\text{gap}}$ avoids backpropagation through $x^*$ and is even compatible with automatic differentiation. It yields the following update for an entry of $A$, which can then be further backpropagated into the MLP via the chain rule:\\n\\n$$\\\\frac{\\\\partial L_{\\\\text{gap}}}{\\\\partial A_{i,j}} = 2\\\\hat{x}_i \\\\hat{x}_j - 2 x^*_i x^*_j - 2 \\\\frac{\\\\partial x^*}{\\\\partial A} \\\\frac{\\\\partial A}{\\\\partial A_{i,j}} x^*, \\\\quad (2)$$\\n\\nwhere the last term comes from the dependency of $x^*$ on $A$ via the QUBO solver. While intuitively useful, we note that this term is zero \\\"almost everywhere\\\" (in the mathematical sense), and we hence ignore it as it provides almost no information. Note that this is a common approximation (e.g., auto-differentiation frameworks backpropagate through $\\\\max(\\\\cdot)$ pooling in the same manner).\\n\\nUnfortunately, $L_{\\\\text{gap}}$ alone would not prevent degenerate $A$, which have multiple solutions, including undesirable ones. We, therefore, discourage such $A$ that have more than one solution $x^*$:\\n\\n$$L_{\\\\text{unique}} = -|\\\\hat{x}^\\\\top A \\\\hat{x} - x^*\\\\text{sign}(A) x^*|, \\\\quad (3)$$\"}"}
{"id": "isiQ5KIXbjj", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $x$ is the $x$ that minimises $x^\\\\top Ax$ over $\\\\{0,1\\\\}^n \\\\setminus \\\\{x^*\\\\}$, and $|\\\\cdot|$ is the absolute value operator.\\n\\nIn addition to these data terms, we found it helpful to regularise the network by encouraging sparsity on all intermediate features of the MLP (Liu et al., 2015):\\n\\n$$L_{mlp} = \\\\sum_{f \\\\in F} |f| \\\\| f \\\\|_1,$$\\n\\n(4)\\n\\nwhere $F$ is the set of all network layer outputs (except for the last layer). The total loss then reads:\\n\\n$$L = L_{gap} + \\\\lambda_{unique} L_{unique} + \\\\lambda_{mlp} L_{mlp},$$\\n\\n(5)\\n\\nwhere we set $\\\\lambda_{mlp} = 10^{-4}$ and $\\\\lambda_{unique} = 10^{-3}$ regardless of problem type.\\n\\n### 3.3 QUBO on D-Wave Quantum Annealers\\n\\nWe can use D-Wave to solve any generated QUBO $A_{i,j} \\\\in \\\\mathbb{R}$ describes the direction and coupling strength between logical qubits $i$ and $j$. However, each physical qubit in the annealer is only connected to a small subset of other physical qubits, which makes the regressed $A$ not directly compatible with the annealer. We tackle this issue by manually pre-determining a sparse connectivity pattern of the physical qubits and then masking out the other entries of $A$ before solving, such that the MLP focuses on only these sparse entries. For example, when $x \\\\in \\\\mathbb{R}^n$ with $n = 8$, we can use D-Wave's Chimera architecture, which is made up of interconnected $K_4 \\\\times K_4$ unit cells (Dattani et al., 2019). ($K_4 \\\\times K_4$ is a complete bipartite graph with two sets of four qubits each.) Since $n = 8$, we can fit one problem instance into one unit cell, which allows us to anneal many problem instances in parallel by putting them in different unit cells. This speeds up training and saves expensive time on the quantum annealer. For larger problems with $n = 32$, we can use D-Wave's Pegasus architecture (Boothby et al., 2020), which has more interconnections (qubit couplings) between its $K_4 \\\\times K_4$ unit cells than Chimera. We use four such unit cells per problem instance, following D-Wave's pattern (Til).\\n\\nSee Fig. 4b for an exemplary colour-coded qubit connectivity pattern of $A$.\\n\\nGiven our full method, we next show how it can be applied to three problem types, i.e., graph matching, point set registration, and rotation estimation; see Appendix for further details.\\n\\n### 3.4 Graph Matching\\n\\nThe goal of graph matching is to determine correspondences from $k$ key points in two images or graphs; see Fig. 2 for an example. This can be formalised as a quadratic assignment problem with a permutation matrix representing the matching (Seelbach Benkner et al., 2020):\\n\\n$$\\\\arg \\\\min_{X \\\\in P_k} x^\\\\top W x,$$\\n\\nwhere $P_k$ is the set of permutation matrices, $x = \\\\text{vec}(X) \\\\in \\\\{0,1\\\\}^{k^2}$ is the vectorised permutation matrix, and $W \\\\in \\\\mathbb{R}^{k^2 \\\\times k^2}$ contains pairwise weights. Unfortunately, the permutation constraint cannot be directly realised on the quantum annealer. Instead, note that a permutation $P : [k] \\\\to [k]$ is fully defined by the sequence $P(1), P(2), \\\\ldots, P(k)$. Our method can use an efficient binary encoding for each entry of this sequence, using only $k \\\\log k$ binary variables in total.\\n\\nNote that not all vectors $x \\\\in \\\\{0,1\\\\}^{k \\\\log k}$ are valid permutations. As an optional post-processing step, we can perform a projection to the nearest permutation with respect to the Hamming distance in our binary encoding. Unless stated otherwise, we do not apply this post-processing.\\n\\nIn addition to the solution parametrisation, we also need to design the problem description $p$. For real data, we use $p = \\\\text{vec}(W)$, where the diagonal contains cosine similarities between the feature vectors extracted with AlexNet (Krizhevsky et al., 2012) pre-trained on ImageNet (Deng et al., 2009) of all key point pairs, and the off-diagonal follows the geometric term from (Torresani et al., 2008, Eq. (6)). For evaluation, we also introduce the synthetic RandGraph dataset; it uses matrices of random distances $D \\\\in \\\\mathbb{R}^{k \\\\times k}$ with entries $D_{i,j} \\\\in U(0,1)$ to define $W = k \\\\cdot i + P(i) \\\\cdot j + P(j) - D_{P(i),P(j)}$. The MLP thus learns to compress the input matrix into a much smaller QUBO.\"}"}
