{"id": "ck4SG9lnrQ", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CMMLU: Measuring Massive Multitask Language Understanding in Chinese\\n\\nAnonymous authors\\n\\nPaper under double-blind review\\n\\nABSTRACT\\n\\nAs the capabilities of large language models (LLMs) continue to advance, evaluating their performance is becoming simultaneously more important and more challenging. This paper aims to address this issue for Mandarin Chinese in the form of CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural sciences, social sciences, engineering, and the humanities. We conduct a thorough evaluation of more than 20 contemporary multilingual and Chinese LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an accuracy of 60%, which is the pass mark for Chinese exams. This highlights that there is significant room for improvement in the capabilities of LLMs. Additionally, we conduct extensive experiments to identify factors impacting the models' performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models in the Chinese context.\\n\\nINTRODUCTION\\n\\nLarge language models (LLMs) have driven remarkable advancements in natural language processing and artificial intelligence, revolutionizing the field (Zhang et al., 2022; Scao et al., 2022; Zeng et al., 2023; Touvron et al., 2023a; OpenAI, 2023; Wu et al., 2023; Taori et al., 2023; Li et al., 2023a). However, assessing the knowledge and reasoning abilities of these models has become increasingly challenging, especially with the proliferation of LLMs that generate fluent and plausible responses. To this end, researchers have created various benchmarks intended to evaluate different model capabilities (Wang et al., 2019b,a; Lin et al., 2022; Zellers et al., 2019; Hendrycks et al., 2021b; Chen et al., 2021). Specifically, Hendrycks et al. (2021a) proposed MMLU, a benchmark that encompasses various tasks ranging from elementary mathematics and computer science to management and law, which can be used to comprehensively measure LLM capabilities in terms of the knowledge embedded in them. Due to its multiple-choice question format, which facilitates easy evaluation, and the breadth of subject areas it encompasses, it has become widely used as a fundamental assessment tool of the knowledge encoded by LLMs. However, this benchmark is in English, which limits its ability to assess LLMs in other languages. Although some researchers (OpenAI, 2023) have attempted to automatically translate it to evaluate LLMs in other languages, the inherent bias towards Western (and specifically US) culture in the dataset renders it unsuitable and even inappropriate for assessing LLMs across diverse cultures and languages.\\n\\nIn this paper, we propose CMMLU (Figure 1), a comprehensive Chinese assessment suite specifically designed to evaluate the advanced knowledge and reasoning abilities of LLMs in a Chinese linguistic and cultural context. CMMLU covers a wide range of subjects, comprising 67 topics from elementary to advanced professional levels. It includes subjects that require computational expertise, such as physics and mathematics, as well as disciplines within the humanities and social sciences. Many of these tasks are not easily translatable from other languages due to their specific contextual nuances and wording. Furthermore, numerous tasks within CMMLU have answers specific to China, which may not be universally applicable or considered correct in other regions or languages.\\n\\nWe assess GPT4, ChatGPT, and more than 20 advanced open-source multilingual and Chinese LLMs on CMMLU. The results reveal that the majority of these models struggle to achieve an accuracy score of 60%, relative to random accuracy of 25%. Notably, GPT4 achieves an average accuracy of 1\"}"}
{"id": "ck4SG9lnrQ", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"These findings highlight the considerable room for improvement in LLMs in terms of Chinese knowledge and language understanding. To gain a deeper understanding of the proficiency of the models in handling Chinese knowledge, we conduct a comprehensive analysis. We first focus on examining model performance across various subjects and find that all models exhibit uneven performance across different subjects, with comparatively higher scores in humanities and social sciences, but lower scores in China-specific and STEM subjects.\\n\\nFurthermore, through extensive experiments, we find that: (1) most existing models do not benefit from chain-of-thought prompts in CMMLU; (2) few-shot examples help foundation models in the comprehension of tasks and enhance their reasoning abilities but do not help models that have undergone supervised fine-tuning (SFT) or reinforcement learning from human feedback (RLHF); (3) LLMs perform worse on questions with negation words compared to those without negation words, but recently-released models mitigate this disparity either through better pre-training data or fine-tuning; and (4) questions with sub-options (Section 4.2) are difficult for all existing LLMs, with even GPT4 dropping 20% in accuracy over such questions.\\n\\nBenchmarking plays a crucial role in measuring AI development, particularly in the domain of LLMs. While benchmarks such as GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a) have played an important role in tracking progress in natural language understanding (NLU) tasks, they primarily focus on specific language skills. With an increasing move to generative models which are highly adept at generating fluent outputs, the value of these benchmarks has diminished, and new datasets have been proposed to evaluate LLM abilities over more general tasks, such as reading comprehension (Rajpurkar et al., 2018; Kwiatkowski et al., 2019; Li et al., 2022), summarization (Hermann et al., 2015), commonsense reasoning (Clark et al., 2018; Talmor et al., 2019; Sakaguchi et al., 2020), mathematical reasoning (Hendrycks et al., 2021b; Cobbe et al., 2021), and code generation (Chen et al., 2021; Austin et al., 2021).\\n\\nIn order to comprehensively assess the capabilities of LLMs, some benchmarks have incorporated massive multi-task evaluations into their frameworks (Hendrycks et al., 2021a; Liang et al., 2022; Srivastava et al., 2023). An example is MMLU (Hendrycks et al., 2021a), which includes multiple domains and tasks based on real-world exams. It has become very popular for LLM evaluation due to...\"}"}
{"id": "ck4SG9lnrQ", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"its standardized and simplified format, comprehensive nature, and real-world relevance. However, all aforementioned benchmarks are primarily focused on English. Given that Chinese is the language with the largest number of speakers worldwide, several benchmarks have been proposed for Chinese LLM evaluation. Following in the footsteps of GLUE and SuperGLUE, Xu et al. (2020) introduced CLUE, a benchmark for Chinese NLU that is widely used today. They also recently proposed SuperCLUE (Xu et al., 2023), which specifically focuses on LLMs. Recently, several Chinese benchmarks have emerged that follow the MMLU style, all of which are concurrent work with ours. In detail, Zhang & Li (2023) proposed ACLUE, focusing on ancient Chinese language understanding. Zeng (2023) presented MMCU, which covers four major domains (medicine, law, psychology, and education), with a particular focus on medicine and education. AGIEval (Zhong et al., 2023) provides problems from both Chinese and English standardized exams. C-Eval (Huang et al., 2023) and M3KE (Liu et al., 2023) collect more than 50 tasks from standard exams in China, while C-Eval covers various professions, and M3KE focuses on education examinations.\\n\\nCompared to these benchmarks, CMMLU has several distinct features. Firstly, it includes more than 10 subjects that are not typically found in standard exams but are relevant to daily life, such as Chinese food culture and Chinese driving rules. Secondly, it covers not only China-specific knowledge but also general world knowledge, such as world religion, world history, and global facts. Lastly, we have made our data completely public, enabling the community to evaluate their models freely and conveniently. A detailed comparison between CMMLU and other concurrent benchmarks is provided in Appendix A.\\n\\n3 CMMLU\\n\\nTask Overview\\n\\nWe created an extensive multitask test for Mandarin Chinese, which covers diverse areas of knowledge, including the humanities, social sciences, STEM (science, technology, engineering, and mathematics), and other areas that are important in daily life. It includes common test questions in subjects like mathematics, physics, and chemistry with answers that are not language or region specific, but also several tasks that are very region-specific, such as Chinese driving rules, Chinese food culture, and Chinese teacher qualifications. The questions in these tasks involve lots of China-related knowledge and can test a model\u2019s understanding and adaptability to Chinese. In addition, CMMLU also contains tasks that can only be expressed in Chinese, such as ancient Chinese language and Chinese literature. The terms and concepts involved in these tasks heavily rely on Chinese expression and are almost impossible to be obtained from translation. The full list of subjects, the concepts tested in each subject, the number of questions, and the statistics of question and answer lengths are provided in Appendix B.\\n\\nData collection\\n\\nWe hired four annotators with undergraduate or higher education levels to manually collect the questions and answers from freely available resources, at a rate of 50 CNY per hour. To prevent our questions from appearing in the training set of LLMs, we invested specific effort in identifying non-publicly available materials, mock exam questions, and questions from quiz shows. More than 80% of our data was crawled from PDFs (after OCR), which further reduces the possibility of it occurring in LLM training data. The entire collection process took around 250 hours.\\n\\nFormat\\n\\nEach question in the dataset is a multiple-choice question with 4 choices, only one of which is correct; see Figure 2 for an example. The questions are expressed as fill\u2013in\u2013the-blank (by choosing the correct option), or direct-answer questions. For chemical formulae and mathematical expressions, we use a 50:50 mixture of LaTeX and plain text, where plain text was only allowed if an expression is commonly used and not prone to ambiguity (as judged by the annotators). For instance, the chemical expression for water can be written in plain text as $H_2O$, or in LaTeX format as $H_2O$.\\n\\nQuality Check\\n\\nTo further check data quality, we randomly sampled 5% questions with answers for each subject, and conduct detailed verification through online resources. We estimate that there is around 2% of noise in the data, in terms of the correct answer not being present or being incorrectly labeled. Based on the results in Section 4 that most models struggle to achieve an average accuracy of 60%, we believe such an error rate does not compromise the overall results.\"}"}
{"id": "ck4SG9lnrQ", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u4ee5\u4e0b\u662f\u4e00\u4e9b\u5355\u9009\u9898\uff0c\u8bf7\u76f4\u63a5\u7ed9\u51fa\u6b63\u786e\u7b54\u6848\u7684\u9009\u9879\u3002\\n\\n\uff08Here are some single-choice questions about [subject], please provide the correct answer choice directly.\uff09\\n\\n\u9898\u76ee\uff1a\u540c\u4e00\u7269\u79cd\u7684\u4e24\u7c7b\u7ec6\u80de\u5404\u4ea7\u751f\u4e00\u79cd\u5206\u6ccc\u86cb\u767d\uff0c\u7ec4\u6210\u8fd9\u4e24\u79cd\u86cb\u767d\u7684\u5404\u79cd\u6c28\u57fa\u9178\u542b\u91cf\u76f8\u540c\uff0c\u4f46\u6392\u5217\u987a\u5e8f\u4e0d\u540c\u3002\u5176\u539f\u56e0\u5728\u4e8e\u53c2\u4e0e\u8fd9\u4e24\u79cd\u86cb\u767d\u5408\u6210\u7684\uff1a\\n\\nA. tRNA\u79cd\u7c7b\u4e0d\u540c\uff08Different types of tRNA\uff09\\nB. \u540c\u4e00\u5bc6\u7801\u5b50\u6240\u51b3\u5b9a\u7684\u6c28\u57fa\u9178\u4e0d\u540c\uff08Different amino acids determined by the same codon\uff09\\nC. mRNA\u78b1\u57fa\u5e8f\u5217\u4e0d\u540c\uff08Different mRNA base sequences\uff09\\nD. \u6838\u7cd6\u4f53\u6210\u5206\u4e0d\u540c\uff08Different ribosome components\uff09\\n\\n\u7b54\u6848\u662f\uff1aC (Answer: C)\\n\\n... [other examples]\\n\\n\u9898\u76ee\uff1a\u67d0\u79cd\u690d\u7269\u75c5\u6bd2V\u662f\u901a\u8fc7\u7a3b\u98de\u8671\u5438\u98df\u6c34\u7a3b\u6c41\u6db2\u5728\u6c34\u7a3b\u95f4\u4f20\u64ad\u7684\u3002\u6c34\u7a3b\u7530\u4e2d\u9752\u86d9\u6570\u91cf\u7684\u589e\u52a0\u53ef\u51cf\u5c11\u8be5\u75c5\u6bd2\u5728\u6c34\u7a3b\u95f4\u7684\u4f20\u64ad\u3002\u4e0b\u5217\u53d9\u8ff0\u6b63\u786e\u7684\u6709\uff1a\\n\\nA. \u9752\u86d9\u4e0e\u7a3b\u98de\u8671\u662f\u6355\u98df\u5173\u7cfb\uff08Frogs and rice planthoppers have a predatory relationship\uff09\\nB. \u6c34\u7a3b\u548c\u75c5\u6bd2V\u662f\u4e92\u5229\u5171\u751f\u5173\u7cfb\uff08Rice plants and virus V have a mutualistic symbiotic relationship\uff09\\nC. \u75c5\u6bd2V\u4e0e\u9752\u86d9\u662f\u5bc4\u751f\u5173\u7cfb\uff08Virus V and frogs have a parasitic relationship\uff09\\nD. \u6c34\u7a3b\u4e0e\u9752\u86d9\u662f\u7ade\u4e89\u5173\u7cfb\uff08Rice plants and frogs have a competitive relationship\uff09\\n\\n\u7b54\u6848\u662f\uff1aA (Answer: A)\"}"}
{"id": "ck4SG9lnrQ", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chinese-LLaMA is part of the Chinese-LLaMA-Alpaca project, an open-source initiative that extends the vocabulary of LLaMA and Alpaca to include more Chinese tokens. The models are then further trained on a larger Chinese corpus to enhance their performance.\\n\\nBaichuan and Baichuan2 are large language model families publicly released by Baichuan Intelligent Technology. Both include versions with 7B and 13B parameters, as well as base and chat variants. Baichuan models are trained on high-quality corpora totaling 1.4 trillion tokens, which surpasses LLaMA-13B by 40%. The models offer support for both Chinese and English languages, and have an extensive context window of 4096. Baichuan2 series is trained on nearly twice the amount of high-quality data, resulting in additional performance enhancements.\\n\\nXverse is a 13B multilingual large language model developed by Shenzhen Yuanxiang Technology. It is trained on 1.4 trillion tokens from diverse sources and supports an extensive 8k context length, efficient tokenization, and advanced training technologies, making it both versatile and efficient.\\n\\nInternLM is an open-source, lightweight training framework developed collaboratively by Shanghai AI Laboratory in partnership with researchers from various universities and companies. Its primary objective is to facilitate model pre-training without the need for extensive dependencies. Utilizing a unified codebase, it supports both large-scale cluster pre-training on thousands of GPUs and fine-tuning on a single GPU, achieving remarkable performance enhancements. Notably, InternLM achieves nearly 90% acceleration efficiency when training on 1024 GPUs. Based on the InternLM framework, a model family including 7B and 20B versions as well as base and chat variants was released.\\n\\n**G S T R A T E G I E S F O R E S T I M A T I N G M O D E L C H O I C E S**\\n\\nIn this section, we compare three strategies for multiple-choice question evaluation. We introduce the mechanism of each strategy, explain its rationale, and compare their efficiency, strengths, and weaknesses. For convenience, we assume the question is \\\"textQ\\\", and the four choices are: \\\"textA\\\", \\\"textB\\\", \\\"textC\\\", \\\"textD\\\".\\n\\n**Strategy 1 \u2013 Next Token Prediction**\\n\\nThe idea is to input the question along with all candidate choices and prompt the model with a direct answer text, such as \\\"The answer is: \\\". We then retrieve the probabilities of the next predicted token and compare these probabilities over the four choice indicator tokens, typically \\\\[A, B, C, D\\\\]. The token with the highest probability is treated as the model's choice.\\n\\n- **Example input:**\\n  - Question: textQ\\n  - A. textA\\n  - B. textB\\n  - C. textC\\n  - D. textD\\n\\n  **Answer:**\\n\\n  - Efficiency: High\\n  - Pro: Most efficient method.\\n  - Con: The model may not tend to generate a token from these choice letters.\\n  - How to mitigate the cons: Provide few-shot examples with their expected answers.\\n  - Works or frameworks use this strategy: MMLU (Hendrycks et al., 2021a), HELM (Liang et al., 2022).\"}"}
{"id": "ck4SG9lnrQ", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 Example input (4 inputs):\\n\\n- Question: textQ\\n  A. textA\\n  B. textB\\n  C. textC\\n  D. textD\\n  Answer: A. textA\\n\\n- Question: textQ\\n  A. textA\\n  B. textB\\n  C. textC\\n  D. textD\\n  Answer: B. textB\\n\\n- Question: textQ\\n  A. textA\\n  B. textB\\n  C. textC\\n  D. textD\\n  Answer: C. textC\\n\\n- Question: textQ\\n  A. textA\\n  B. textB\\n  C. textC\\n  D. textD\\n  Answer: D. textD\\n\\n\u2022 Efficiency: Low\\n\\n\u2022 Pro: Aligns with the objective of language model optimization as perplexity reflects the true probability of a model generating the given text.\\n\\n\u2022 Con: Low efficiency. Usually take 4x time (for a 4-choice question) compared to Next Token Prediction.\\n\\n\u2022 How to mitigate the cons: Efficient implementation that only computes the same prefix once.\\n\\n\u2022 Works or frameworks use this strategy: LM-Evaluation-Harness (Gao et al., 2021), OpenCompass.\\n\\nStrategy 3 \u2013 Free Generation\\n\\nWe input the question and candidate choices to the model and prompt it by asking for the correct choices. We allow the model to continue generating text, and then use the auxiliary method to match the patterns and extract the model's choices.\\n\\n\u2022 Example input:\\n\\n- Question: textQ\\n  A: textA\\n  B: textB\\n  C: textC\\n  D: textD\\n\\n\u2022 Efficiency: Medium/Low\\n\\n\u2022 Pro: Allow various prompting,\\n\\n\u2022 Con: Need answer extraction via human/model/regular expression. This process can be costly and error-prone. The generation can be very long, resulting in significant time consumption.\\n\\n\u2022 How to mitigate the cons: Train a robust answer extraction model, or design robust regular expressions. Use a small temperature when doing generation.\\n\\nhttps://github.com/open-compass/opencompass\"}"}
{"id": "ck4SG9lnrQ", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Comparison of different evaluation strategies. We compare next token prediction (i.e. \u201cNext\u201d), and free generation (\u201cGen\u201d). We also list the proportion of responses that cannot matched by our regex (% E). Note that our regex is designed based on the observation of ChatGPT and ChatGLM responses.\\n\\n| Model         | Next  | Gen   | % E |\\n|--------------|-------|-------|-----|\\n| 0-shot       |       |       |     |\\n| Baichuan2-13B-Chat | 59.79 | 58.77 | 0.71|\\n| BatGPT-15B-sirius | 49.81 | 45.26 | 2.35|\\n| ChatGLM-6B   | 40.56 | 40.43 | 1.15|\\n| ChatGLM2-6B  | 51.48 | 49.61 | 1.51|\\n| InternLM-Chat-20B | 55.06 | 53.52 | 0.01|\\n| Xverse-13B-Chat | 55.59 | 52.96 | 0.88|\\n| 5-shot       |       |       |     |\\n| Baichuan2-13B-Chat | 59.89 | 54.44 | 6.44|\\n| BatGPT-15B-sirius | 47.88 | 40.13 | 4.58|\\n| ChatGLM-6B   | 37.17 | 36.83 | 1.65|\\n| ChatGLM2-6B  | 49.69 | 48.80 | 0.56|\\n| InternLM-Chat-20B | 54.52 | 51.51 | 0.42|\\n| Xverse-13B-Chat | 56.12 | 51.64 | 5.55|\\n\\nWorks or frameworks use this strategy: OpenCompass, C-Eval (Huang et al., 2023).\\n\\nTable 9 compares models performance using strategy 1 and strategy 3. Since strategy 2 is time-consuming, we didn\u2019t conduct results on it. From the table, we find that using next token prediction achieves a higher score than using the free generation strategy for all models, but the gap is less than 3% for most of the models under the zero-shot setting (with the exception of BatGPT which is about 5%). For both zero-shot and five-shot settings, the gap between strategy 1 and 2 is positively correlated to the proportion of the instances that cannot match any choice using regex. Hence, we believe using the next token prediction to force the model to make a choice among the given choices can effectively reflect its knowledge capacity.\\n\\nOur approach to regular expression matching involves the following steps:\\n\\n1. **Identification of Choices**:\\n   - The function first examines whether the first character of the string corresponds to a valid choice and returns that choice if true.\\n2. **Pattern Matching**:\\n   - To accommodate the complex responses of different LLMs, we adopt a four-step matching mechanism.\\n   - **First Step**: Identify and extract choices by seeking patterns of some choice statements, such as the term \u201canswer\\\" (answer) followed by valid options.\\n   - **Second Step**: Employ a pattern to recursively identify and extract the choices mentioned in the string, iterating until they finally appear.\\n   - **Third Step**: Use weak single matching patterns.\\n   - **Fourth Step**: Check for responses that mention a single choice.\\n\\nIf there is no matching pattern or unique selection, \u201cE\u201d is returned by default, indicating that no selection was confidently extracted.\"}"}
{"id": "ck4SG9lnrQ", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1: Algorithm for Extracting Choices from Response Strings\\n\\n1: procedure EXTRACTCHOICE(response)\\n2:     response \u2190 convert to string(response)\\n3:     choices \u2190 ['A', 'B', 'C', 'D']\\n4:     if first character of response \u2208 choices then\\n5:         return first character of response\\n6:     end if\\n7:     patterns 1 \u2190 [(r'\u7b54(\u9009\u9879)?(\u662f|\u4e3a):?[ABCD]', 3), (r'\u7b54\u662f|\u4e3a\u9009\u9879?:?[ABCD]', 2), (r'\u6545\u9009\u62e9?:?[ABCD]', 1), (r'(ABCD)?\u9009?\u9879(\u662f|\u4e3a)?\u6b63\u786e?', 1), (r'\u6b63?\u786e\u7684?\u9009?\u9879(\u662f|\u4e3a)?', 2), (r'\u7b54\u5e94|\u8be5?(\u662f|\u4e3a)?[ABCD]', 3), (r'\u9009\u62e9\u7b54?\u9879?:?[ABCD]', 1), (r'\u7b54\u9009\u62e9?:?[ABCD]', 1), (r'\u7b54\u9009\u62e9?:?[ABCD]', 1), (r'(ABCD)?\u9009?\u9879(\u662f|\u4e3a)?\u7b26\u5408\u9898\u610f?', 1), (r'\u7b54\u9009\u62e9?:?[ABCD]', 1), (r'\u7b54\u9009\u62e9?:?[ABCD]', 1), (r'(ABCD)?\u5f53\u9009?\u65f6?', 1), (r'(ABCD)?\u6b63?\u786e?', 1)]\\n8:     patterns 2 \u2190 [(r'(ABCD)?\u5f53\u9009?', 1), (r'(ABCD)?\u6b63?\u786e?', 1)]\\n9:     patterns 3 \u2190 [(r'\u02c6[\u4e0d\u662f]:?[ABCD]', 1), (r'\u02c6\u9009\u9879([ABCD])', 1)]\\n10:    for each patterns in [patterns 1, patterns 2, patterns 3]\\n11:       for each (pattern, idx) in patterns\\n12:          if pattern is found in response then\\n13:              answer \u2190 matched group(idx)\\n14:              if answer \u2208 choices then\\n15:                  return answer\\n16:              end if\\n17:          end if\\n18:       end for\\n19:    end for\\n20:    pattern 4 \u2190 r'\u02c6[\u02c6ABCD]*([ABCD][\u02c6ABCD]*)$'\\n21:    if pattern 4 is matched in response then\\n22:       answer \u2190 matched group(1)\\n23:       if answer \u2208 choices then\\n24:           return answer\\n25:       end if\\n26:    end if\\n27:    return \\\"E\\\" \u22bf Return E as default if no match is found\"}"}
{"id": "ck4SG9lnrQ", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Subject | Question | Choices                                                                 |\\n|---------|----------|------------------------------------------------------------------------|\\n| STEM    | \u6cb9\u7f50\u8f66\u540e\u9762\u90fd\u6709\u4e00\u6761\u62d6\u5730\u7684\u94c1\u94fe\uff0c\u5176\u4f5c\u7528\u662f? | A. \u4f5c\u4e3a\u6cb9\u7f50\u8f66\u7684\u6807\u5fd7 | B. \u5411\u5916\u754c\u6563\u70ed | C. \u53d1\u51fa\u54cd\u58f0\uff0c\u63d0\u793a\u5176\u4ed6\u8f66\u8f86\u548c\u884c\u4eba | D. \u628a\u7535\u8377\u5bfc\u5165\u5927\u5730\uff0c\u907f\u514d\u7531\u9759\u7535\u9020\u6210\u7684\u5371\u5bb3 |\\n| Humanities | \u957f\u7bc7\u5c0f\u8bf4\u300a\u4eac\u534e\u70df\u4e91\u300b\u7684\u4f5c\u8005\u662f? | A. \u4e01\u73b2 | B. \u67d4\u77f3 | C. \u6797\u8bed\u5802 | D. \u8001\u820d |\\n| Social Science | \u201c\u6293\u996d\u201d\u662f()\u7684\u7279\u8272\u996e\u98df | A. \u85cf\u65cf | B. \u7ef4\u543e\u5c14\u65cf | C. \u82d7\u65cf | D. \u671d\u9c9c\u65cf |\\n| Other | \u5168\u8eab\u9ec4\u67d3\u662f\u98df\u7528()\u8fc7\u91cf | A. \u7ef4\u751fA | B. \u7ef4\u751fD | C. \u7ef4\u751fB | D. \u7ef4\u751fC |\\n| China specific | \u5b54\u5b50\u5f1f\u5b50\u4e2d\u64c5\u957f\u7ecf\u5546\u7684\u662f\u8c01? | A. \u5b50\u8d21 | B. \u5b50\u8def | C. \u989c\u56de | D. \u5b50\u5f20 |\\n\\nTable 8 provides examples from CMMLU in each category.\"}"}
{"id": "ck4SG9lnrQ", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We analyze the difficulty distribution of CMMLU from two perspectives. Firstly, the CMMLU benchmark encompasses a diverse range of difficulty levels: 5 subjects at primary school level, 10 at middle/high school level, 23 at college level, and 29 at professional level, ensuring a comprehensive difficulty spectrum.\\n\\nSecondly, to estimate the difficulty distribution within each subject, we evaluated the top 20 models from our main results table. Each question was treated as a data point, and we recorded the number of models correctly answering each question. This approach allowed us to map out the difficulty distribution across subjects.\\n\\nFigure 8: Difficulty distribution estimation of each subject. We use violin plot for visualization, where the x-axis represents the number of models that correctly answer a question, and the y-axis indicates the quantity of such questions. A peak on the left side of the plot (e.g., college actuarial science at position [3,3]) suggests that the subject is generally challenging, as most questions are correctly answered by only a few models. Conversely, a peak on the right (e.g., arts at position [1,4]) indicates a relatively simpler subject, where most questions are correctly answered by many models. Subjects exhibiting multi-peak distributions reveal a varied difficulty range within that subset. For instance, a hypothetical scenario with a dataset comprising basic arithmetic problems and complex calculus questions would result in a distribution with two distinct peaks separated by a notable gap, resembling a horizontal funnel. This indicates a wide spectrum of difficulty levels, from very easy to highly challenging.\\n\\nFigure 8 reveals that the majority of subjects exhibit a single peak in their difficulty distribution. This single-peak pattern indicates a uniform level of difficulty within these subjects, suggesting a consistent challenge for models across the range of questions. However, certain subjects, such as machine learning (located at position [9,1]) and professional law (at position [10,3]), display dual\"}"}
{"id": "ck4SG9lnrQ", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"We assessed the concept of emergent ability using the LLaMA-2 model family. Figure 9 illustrates the performance of the LLaMA-2 pre-trained models (7B, 13B, and 70B) across various subjects. s, m, l means 7B, 13B and 70B models, respectively. The figure indicates that, for most subjects, there is a correlation between increased model size and enhanced performance. Notably, in subjects like college education and enhanced performance. We cannot simply attribute it to emergent ability. Given these complexities, we further investigated the role of subject domains. The peak performance indicates a notable presence of both relatively easy and challenging questions, with fewer intermediate-level questions. Despite the presence of two peaks, the transition between these peaks is gradual rather than abrupt, indicating a smooth progression in difficulty levels.\"}"}
{"id": "ck4SG9lnrQ", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ChatGPT/GPT4 are GPT models developed by OpenAI and fine-tuned using reinforcement learning from human feedback (RLHF). As commercial products, specific details about the model size, training data, and training process remain undisclosed.\\n\\nFalcon is a decoder-only model created by TII and trained on 1,000B tokens of RefinedWeb (Penedo et al., 2023) data. Due to the high quality of its training data, Falcon-40B performs competitively with LLaMA-65B on various benchmarks.\\n\\nLLaMA is an auto-regressive language model proposed by Meta. It incorporates several structural improvements over the vanilla transformer and is trained on a mixture of publicly available data sources. LLaMA has demonstrated performance that is comparable to or even superior to models that are ten times its size.\\n\\nLLaMA2 is an upgraded version of LLaMA developed by Meta. The preprocessing stage involves more robust data cleaning and updating data mixes, and the model employs a 40% increase in the total token count during training. Additionally, it up-samples the most factual sources to enhance knowledge and reduce hallucinations. Grouped-query attention (GQA) has been employed to reduce GPU memory usage.\\n\\nBLOOM is a multi-lingual targeted LLM developed by BigScience. It is trained on 46 natural languages and 13 programming languages. The largest BLOOM model consists of 176B parameters, but deploying such a large model can be challenging. In this paper, we evaluate the performance of the 7B BLOOM model.\\n\\nBLOOMZ is derived from BLOOM through fine-tuning on a cross-lingual task mixture (xP3), which is an instruction-following dataset. BLOOMZ exhibits competitive performance with models that have a larger number of parameters across various non-generation tasks.\\n\\nBactrian-X is a series of LLMs (LLaMA, BLOOM, mT5) proposed by MBZUAI. These models are fine-tuned on a multilingual instruction-following dataset that encompasses 52 languages. All the fine-tuned Bactrian-X models demonstrate performance improvements compared to their corresponding base models in multilingual generation settings.\\n\\nChatGLM and ChatGLM2 are bidirectional dense models pre-trained using the General Language Model (GLM) algorithm developed by Tsinghua University. They support bilingual (Chinese and English) language processing. ChatGLM is a version of GLM that is enhanced with supervised fine-tuning, feedback bootstrap, and reinforcement learning with human feedback, specifically optimized for Chinese question answering (QA) and dialogue tasks. In this paper, we evaluate the performance of 10B and 6B models of GLM.\\n\\nBatGPT jointly developed by Wuhan University and Shanghai Jiaotong University, is a bilingual (Chinese and English) and bidirectional language model. BatGPT is initialized with a novel parameter expansion method, which enables it to absorb knowledge from the pre-training of other LLMs. With a bidirectional autoregressive architecture and further enhancement through Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human and AI Feedback (RLHAF), BatGPT is able to handle long-range, multi-turn question-answering tasks effectively and alleviate concerns regarding memory limitations. The evaluation of the 15B version is presented in this work.\\n\\nMOSS-SFT is an open-source Chinese language model proposed by Fudan University. It is comparable to ChatGPT in terms of training scale and alignment techniques. MOSS-SFT is initialized with CodeGen and further pre-trained on 100B Chinese tokens and 20B English tokens. The Supervised Fine-Tuned (SFT) version of MOSS-SFT enables the model to follow instructions in multi-turn dialogues.\"}"}
{"id": "ck4SG9lnrQ", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The correct option for the statement about the horizontal pressure gradient force is 1. It is the direct cause of the wind; 2. It is the pressure produced by the atmosphere on the sea level; 3. The direction is perpendicular to the isobar; 4. From high pressure to low pressure.\\n\\nA. 1234 B. 234 C. 134 D. 123\\n\\nAnswer: C\\n\\nIn Table 3, we present 4 model families, from the table we find that most models (with the exception of GPT4 and ChatGLM2) perform less effectively on questions containing negative words compared to those without, aligning with the findings of previous studies, and highlights this common limitation of large language models.\\n\\nInterestingly, developers have successfully mitigated this problem in different stages of development. For example, LLaMA2 demonstrates the enhancement of model\u2019s negation process ability using SFT/RLHF. The accuracy gap between question w/ and w/o negations decrease by about 5% after applying SFT/RLHF. Baichuan shows that better pre-training can also effectively alleviate this issue. Specifically, Baichuan2 reduces such a gap to 1-2% compared to Baichuan\u2019s 8-10% by using improved pre-training data. ChatGLM2 almost shows the same performance when answering questions with and without negations. We think the researcher has noticed the negation problem, and found that compared to complex reasoning ability, enhancing negative processing is relatively easy.\\n\\nAre questions with sub-options more challenging?\\n\\nThere is a typical question type in all kinds of Chinese exams called sub-option questions. These questions include a main statement along with multiple sub-options, and inquire about the count, order, or selection of the sub-options, which requiring the model to have deeper reasoning and inference skills (see example in Figure 6). The sub-options in CMMLU can appear in different formats, such as \u201ca, b, c...; \u2460, \u2461, \u2462...\u201d, and account for about 10.8% of the dataset. We classified the data into two subsets based on sub-option presence, and put the evaluation results in Table 4. We observed that all these models performed weaker on sub-options questions compared to those without sub-options, with a decline ranging from 10% to 20%. Intuitively, the COT prompt should alleviate such a problem by guiding the model to analyze the sub-options one by one. However, the observation is that ChatGLM2 and BatGPT benefit from COT prompt while Baichuan doesn\u2019t.\\n\\nCONCLUSION\\n\\nWe introduce CMMLU, a groundbreaking benchmark designed to assess the multi-task language understanding capabilities in Chinese. Our experimental findings reveal substantial opportunities for improvement within existing large language models. Through extensive analysis, we identify several factors that impact model performance and propose actionable directions for enhancing LLMs. We are confident that our benchmark dataset and analytical insights will empower researchers to effectively evaluate and design Chinese LLMs.\\n\\nREFERENCES\\n\\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cociocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance. 2023.\\n\\nJacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. CoRR, abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.07732.\"}"}
{"id": "ck4SG9lnrQ", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond\u00e9 de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code.\\n\\nCoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.\\n\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge.\\n\\nCoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457.\\n\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems.\\n\\nCoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.\\n\\nYiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese llama and alpaca.\\n\\narXiv preprint arXiv:2304.08177, 2023. URL https://arxiv.org/abs/2304.08177.\\n\\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 320\u2013335, 2022.\\n\\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ.\\n\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html.\\n\\nKarl Moritz Hermann, Tom\u00e1s Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 1693\u20131701, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html.\\n\\nArian Hosseini, Siva Reddy, Dzmitry Bahdanau, R. Devon Hjelm, Alessandro Sordoni, and Aaron C. Courville. Understanding by understanding not: Modeling negation in language models. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T\u00fcr, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 1301\u20131312.\"}"}
{"id": "ck4SG9lnrQ", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "ck4SG9lnrQ", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "ck4SG9lnrQ", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. URL https://arxiv.org/abs/2307.09288.\\n\\nSuperglue: A stickier benchmark for general-purpose language understanding systems. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 3261\u20133275, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html.\\n\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=rJ4km2R5t7.\\n\\nLamini-lm: A diverse herd of distilled models from large-scale instructions. CoRR, abs/2304.14402, 2023. URL https://arxiv.org/abs/2304.14402.\\n\\nCLUE: A chinese language understanding evaluation benchmark. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pp. 4762\u20134772. International Committee on Computational Linguistics, 2020. doi: 10.18653/v1/2020.coling-main.419. URL https://doi.org/10.18653/v1/2020.coling-main.419.\\n\\nSuperclue: A comprehensive chinese large language model benchmark. arXiv preprint arXiv:2307.15020, 2023. URL https://arxiv.org/abs/2307.15020.\\n\\nBaichuan 2: Open large-scale language models. 2023. URL https://api.semanticscholar.org/CorpusID:261951743.\\n\\nHellaswag: Can a machine really finish your sentence? In Anna Korhonen, David R. Traum, and Llu\u00eds M. Marquez (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 3261\u20133275, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html.\"}"}
{"id": "ck4SG9lnrQ", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Comparison to Concurrent Benchmarks\\n\\nC-Eval (Huang et al., 2023) and M3KE (Liu et al., 2023) are two similar benchmarks concurrent with our work. We compare the task distribution of these benchmarks in Table 5, and demonstrate that CMMLU contains more culture-related and region-related tasks. While there are differences in task distribution, we acknowledge that these datasets exhibit similarities in the task types and can, therefore, be jointly used as assessment criteria for evaluating the Chinese language capabilities of large models.\\n\\nWe further assess the overlap between CMMLU and both of these benchmarks. For this purpose, we first sort four choices for each question to eliminate the influence of choice order. Subsequently, we concatenate the question string with the sorted choice strings. Then, we remove all punctuation marks, including underscores and brackets, from the resulting strings. The final overlap, computed using exact string matching, yields a total of 74 for CEval and 158 for M3KE. This overlap accounts for approximately 1% of our dataset.\\n\\nTable 5: Task distributions of contemporary similar datasets. CMMLU contains more subjects in humanities, social science, and others (usually country- or culture-specific) compared to CEval and M3KE, while fewer subjects in STEM. This indicates that our dataset is more inclined toward examining knowledge related to social, cultural, and regional factors.\\n\\n|        | Model | STEM | Humanities | Social Science | Other | China-specific | Total |\\n|--------|-------|------|------------|----------------|-------|----------------|-------|\\n| CEval  | 20    | 11   | 10         | 11             | \u2013     | 52             |       |\\n| M3KE   | 31    | 12   | 21         | 7              | \u2013     | 71             |       |\\n| CMMLU  | 17    | 13   | 22         | 15             | 15    | 67             |       |\\n\\nTable 6 lists all subjects of CMMLU. The table also provides details for each subject test, including the concepts covered, the supercategory to which each subject belongs, and the total number of questions.\"}"}
{"id": "ck4SG9lnrQ", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 7 presents the breakdown of statistical results of the CMMLU test set for each supercategory,\\n\\n| Subject | Tested Concepts | Supercategory | # Q |\\n|---------|-----------------|---------------|-----|\\n| Anatomy | Crop physiology, agroecology, soil science, breeding, ... | STEM | 169 |\\n| Ancient Chinese | Ancient history, modern history, ancient culture, ... | Humanities | 323 |\\n| Arts | Drama, poetry, ink painting, literature, movie, ... | Humanities | 160 |\\n| Astronomy | Astronautics, planets, galaxies, asteroids, constellations, ... | STEM | 165 |\\n| Business Ethics | Audit, financing, assets, profit distribution, ... | Social Science | 175 |\\n| Chinese Literature | Chinese Philosophy, Western Philosophy, Book of Changes, ... | Humanities | 105 |\\n| Chinese History | Islam, Judaism, Buddhism, Christianity, ... | Humanities | 160 |\\n| Chinese Civil Service Exam | Basic principles, Practical significance, contemporary value, ... | Humanities | 189 |\\n| Chinese Foreign Policy | China's foreign policy's principles, goals, history, ... | Social Science | 107 |\\n| Chinese Teacher Qualification | Marxist philosophy, political economy, scientific socialism, ... | Social Science | 143 |\\n| Clinical Knowledge | Reproductive health, contraceptive methods, mental health, ... | Other | 126 |\\n| College Actuarial Science | Finance, accounting, insurance, ... | Social Science | 175 |\\n| College Education | Educational theory, pedagogy, psychology, language, ... | Social Science | 179 |\\n| College Engineering Hydrology | Air pressure, altitude, precipitation, ... | STEM | 106 |\\n| College Law | Legal ethics, moral views and values, social ethics, history, ... | Other | 214 |\\n| College Medical Statistics | Statistics, probability, data analysis, ... | STEM | 106 |\\n| College Medicine | Anatomy, physiology, healthcare, diagnose, pathology, ... | STEM | 237 |\\n| Computer Science | Windows, word, powerpoint, ... | Other | 238 |\\n| Conceptual Physics | Atomic, synthesis, chemical equilibrium, acid-base reactions, ... | STEM | 132 |\\n| Construction Project Management | Negotiations, Organizational Image, Etiquette, ... | Social Science | 174 |\\n| Economics | International economics, organizations, global events, ... | Humanities | 149 |\\n| Education | Educational psychology, policies, technology, management ... | Social Science | 163 |\\n| Electrical Engineering | Mechanics, heat, optics, electricity, acoustics, nuclear physics, ... | STEM | 110 |\\n| Elementary Chinese | Classical Chinese, poems, words, songs,... | Social Science | 164 |\\n| Elementary Commonsense | Grammar, semantic, literature, ... | Social Science | 116 |\\n| Elementary Information and Technology | Planning, contracts, safety, budgeting, management, ... | Other | 139 |\\n| Elementary Mathematics | Equations, trigonometry, analytic geometry, probability, ... | STEM | 164 |\\n| Ethnology | Minority cultures, policies, religion, beliefs, history, ... | Social Science | 135 |\\n| Genetics | Mendelian Genetics, chromosomes, DNA, genetic disorders, ... | STEM | 176 |\\n| Global Facts | Emergency procedures, signs, signals, traffic laws, ... | Other | 131 |\\n| High School Biology | Swimming, Chinese martial arts, heart rate, ... | Other | 165 |\\n| High School Chemistry | Chemistry, microbiology, processing, preservation, nutrition, ... | Other | 143 |\\n| High School Geography | Physical geography, human geography, environmental geography, ... | Social Science | 118 |\\n| High School Mathematics | Matrices, derivatives, random variables, ... | STEM | 105 |\\n| High School Physics | Mechanics, waves, power, energy, light, electricity, ... | STEM | 147 |\\n| High School Politics | National security, terrorism, ... | Social Science | 135 |\\n| Human Sexuality | Reproductive health, contraceptive methods, mental health, ... | Other | 126 |\\n| International Law | Legal ethics, moral views and values, social ethics, history, ... | Other | 214 |\\n| Journalism | Media effects theory, communication models, journalism law, ... | Social Science | 172 |\\n| Jurisprudence | Legal ethics, moral views and values, social ethics, history, ... | Other | 214 |\\n| Legal And Moral Basis | Fairness and justice, transparency and accountability, ... | Social Science | 209 |\\n| Logical | Propositional logic, inductive reasoning, critical thinking, ... | Humanities | 123 |\\n| Machine Learning | Data structures, algorithms, programming, operating systems, ... | STEM | 122 |\\n| Management | Organizational theory, leadership, international management, ... | Social Science | 210 |\\n| Marketing | Marketing Concepts, Pricing Strategies, Consumer Behavior, ... | Social Science | 180 |\\n| Marxist Theory | Marxist philosophy, political economy, scientific socialism, ... | Social Science | 143 |\\n| Modern Chinese | Ancient poems, classics, pronunciation, meaning, ... | Social Science | 252 |\\n| Nutrition | Dietary fiber, trace elements, fatty acids, ... | STEM | 145 |\\n| Philosophy | Marxist philosophy, political economy, scientific socialism, ... | Social Science | 143 |\\n| Professional Accounting | Audit, financing, assets, profit distribution, ... | Social Science | 175 |\\n| Professional Law | Patent Law, Criminal Law, Contract Law, ... | Humanities | 211 |\\n| Professional Psychology | Emotions, thought patterns, perception, ... | Social Science | 232 |\\n| Sociology | Socialization, cities and community, ... | Social Science | 226 |\\n| Security Study | Negotiations, Organizational Image, Etiquette, ... | Social Science | 174 |\\n| Public Relations | Negotiations, Organizational Image, Etiquette, ... | Social Science | 174 |\\n| Traditional Chinese Medicine | Human meridians, yin and yang, ... | Other | 185 |\\n| Virology | Pathogen, viral gene mutation, infection STEM | STEM | 169 |\\n| World History | Ancient civilizations, the Industrial Revolution, world wars, ... | Humanities | 161 |\\n| World Religions | Islam, Judaism, Buddhism, Christianity, ... | Humanities | 160 |\"}"}
{"id": "ck4SG9lnrQ", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"Table 7: The statistics of the CMMLU test set, where Q represents the question and C indicates the answer choices.\\n\\n| Subject        | Tasks | #Q  | Avg. #Q | Max. #Q | Min. #Q | Avg. Q Tokens | Avg. C Tokens |\\n|----------------|-------|-----|---------|---------|---------|---------------|---------------|\\n| STEM           | 17    | 2531| 148.88  | 230     | 105     | 38.53         | 11.62         |\\n| Humanities     | 13    | 2489| 191.46  | 411     | 105     | 41.65         | 10.10         |\\n| Social Science | 22    | 3652| 166.00  | 252     | 107     | 36.84         | 7.25          |\\n| Other          | 15    | 2910| 194.00  | 376     | 126     | 31.31         | 7.02          |\\n| China-specific | 15    | 2572| 171.46  | 323     | 107     | 44.54         | 8.20          |\\n| All            | 67    | 11582| 172.87  | 411     | 105     | 36.85         | 8.76          |\\n\\nFigure 7: Question and answer lengths of each subject.\"}"}
{"id": "ck4SG9lnrQ", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Five-shot accuracy of models. We report macro average accuracy over subjects within each category. \\\"Overall\\\" = macro average score over all subjects. \\\"State\\\" indicates whether the model is pre-trained (Base) or Fine-tuned to follow instructions (Chat). '*' indicate there are both Base and Chat model released, we choose the one with better overall accuracy. The first block is multilingual- or English-oriented models, and the second block is Chinese-oriented models. To save space, we didn't present models with an overall score lower than 30.\\n\\n| Model          | State | STEM   | Humanities | Social Science | Other   | China-specific | Average |\\n|----------------|-------|--------|------------|----------------|---------|----------------|---------|\\n| GPT4           | Chat  | 65.23  | 72.11      | 72.06          | 74.79   | 66.12          | 70.95   |\\n| ChatGPT        | Chat  | 47.81  | 55.68      | 56.50          | 62.66   | 50.69          | 55.51   |\\n| LLaMA2-70B*    | Base  | 44.11  | 57.05      | 55.63          | 56.65   | 48.01          | 53.21   |\\n| Falcon-40B     | Base  | 33.33  | 43.46      | 44.28          | 44.75   | 39.46          | 41.45   |\\n| LLaMA-65B      | Base  | 34.47  | 40.24      | 41.55          | 42.88   | 37.00          | 39.80   |\\n| LLaMA2-13B*    | Base  | 33.04  | 39.73      | 38.45          | 42.54   | 35.67          | 38.24   |\\n| BLOOMZ-7B Chat| Chat  | 30.56  | 39.10      | 38.59          | 40.32   | 37.15          | 37.04   |\\n| LLaMA-30B      | Base  | 29.69  | 33.68      | 34.08          | 37.40   | 30.68          | 33.63   |\\n| LLaMA2-7B*     | Base  | 30.03  | 34.76      | 33.72          | 33.62   | 30.12          | 32.96   |\\n| ZH LLaMA-13B   | Chat  | 27.12  | 33.18      | 34.87          | 35.10   | 32.97          | 32.63   |\\n| BX LLaMA-13B   | Chat  | 27.50  | 32.47      | 32.33          | 35.77   | 31.64          | 31.90   |\\n| LLaMA-13B      | Base  | 29.21  | 30.96      | 31.74          | 33.07   | 30.86          | 31.24   |\\n| Baichuan2-13B* | Base  | 48.36  | 67.44      | 66.40          | 65.94   | 63.48          | 61.92   |\\n| Baichuan-13B*  | Base  | 42.38  | 61.61      | 60.44          | 59.26   | 56.62          | 55.82   |\\n| InternLM-20B*  | Chat  | 42.70  | 60.51      | 58.00          | 57.62   | 54.72          | 54.52   |\\n| Xverse-13B     | Chat  | 41.65  | 55.72      | 57.47          | 57.32   | 52.32          | 53.08   |\\n| InternLM-7B*   | Base  | 41.71  | 54.43      | 56.42          | 55.38   | 53.11          | 52.07   |\\n| ChatGLM2-6B    | Chat  | 42.65  | 50.88      | 51.22          | 50.72   | 48.66          | 48.87   |\\n| BatGPT-15B     | Chat  | 41.68  | 50.14      | 50.78          | 48.68   | 46.93          | 47.88   |\\n| Baichuan-7B*   | Base  | 35.25  | 48.07      | 47.88          | 46.61   | 44.14          | 44.43   |\\n| ChatGLM-6B     | Chat  | 32.35  | 39.22      | 39.65          | 38.62   | 37.70          | 37.48   |\\n| Random         |       | 25.00  | 25.00      | 25.00          | 25.00   | 25.00          | 25.00   |\\n\\nFor zero-shot evaluation, we present a question with choices directly after the prompt. For few-shot evaluation, we provide up to 5 demonstration examples with answers before the question. The prompt concludes with the phrase \\\"\u7b54\uff1a\\\" as shown in the example in Figure 2. If the context exceeds the model's maximum length with few-shot examples, we dynamically remove the longest examples by counting sub-tokens.\\n\\nModels we assessed more than 20 models in different sizes from 12 model families. For commercial models, we evaluated ChatGPT and GPT4, which are two of the strongest LLMs. For open-sourced models, we selected (1) English and multilingual-oriented models: BLOOM-7.1B (Scao et al., 2022), BLOOMZ-7.1B (Muennighoff et al., 2022), LLaMA-7B/13B/30B/65B (Touvron et al., 2023a), Bactrian-X-LLaMA (BX LLaMA)-7B/13B (Li et al., 2023a), Falcon-7B/40B (Almazrouei et al., 2023), LLaMA2-7B/13B/70B (Touvron et al., 2023b), Chinese-LLaMA (ZH LLaMA)-7B/13B (Cui et al., 2023); (2) Chinese-oriented models: Baichuan-7B/13B and Baichuan2-7B/13B (Yang et al., 2023), ChatGLM-6B and ChatGLM2-6B (Zeng et al., 2023), Xverse-13B, InternLM-7B/20B (Team, 2023), MOSS-SFT-16B (OpenLMLab, 2023), Chinese-GLM-10B (Du et al., 2022), BatGPT-15B (Li et al., 2023b). The details about these models are provided in Appendix F.\"}"}
{"id": "ck4SG9lnrQ", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ChatGPT performance at 55.51%. However, there is still a significant gap between LLaMA2-70B and GPT4 (70.95%); (2) 7B pre-trained multilingual models (except LLaMA2-7B) achieve nearly random results of 25% (since it\u2019s lower than 30%, they are not displayed in the table); (3) For those multilingual models, fine-tuning using Chinese resources consistently improves their performance (BX LLaMA and ZH LLaMA vs. LLaMA, BLOOMZ vs. BLOOM).\\n\\nFrom the second block, we find that: (1) Among the Chinese LLMs, Baichuan2-13B demonstrates the best overall performance (beats ChatGPT) with only 13B parameters. We attribute it to the high quality of the training data; (2) Several Chinese LLMs achieve competitive results compared to LLaMA2-70B with less than 20B parameters. This demonstrates that when focusing on a single language, high-quality monolingual (or bilingual) training data can empower small models (7B or 13B) with good capability compared to multilingual training data. An overall observation is that models from the same family always improve as the model size increases.\\n\\nBy subject type, all models exhibit relatively high performance in humanities, social sciences, and other subjects, and medium performance in China-specific subjects, while low performance in STEM subjects. We attribute this to the nature of each subject type, and the capability of LLMs: (a) humanities, social sciences assess more on memorization which is relatively easy for LLMs; (b) China-specific topics encompass information that is either absent from the training data or inconsistent in multilingual training data; (c) STEM topics usually require complex reasoning, which has been proven to be difficult for existing LLMs. As expected, Chinese LLMs exhibit smaller gaps between China-specific subjects and other categories.\\n\\nWe compare the performance of the best-performing Chinese model, Baichuan2-13B, with the best-performing multilingual model, GPT4, for each subject. We categorize the subjects and present the results in Figure 3. The numerical results can be found in Appendix J.2. From the figure, we note that the model\u2019s performance appears to be unbalanced, excelling in certain subjects but struggling in others. Specifically, ancient Chinese and college actuarial science are the most challenging subjects for both Baichuan2 and GPT4, yielding slightly better results than random, while the legal and moral basis is one of the easiest subjects for both models. When comparing the two models, we find that for most subjects, GPT4 outperforms Baichuan2 by a significant margin, while Baichuan2 surpasses GPT4 in 8 subjects, 6 of these are China-specific subjects, and the other 2 (arts and philosophy) contain a large amount of Chinese elements.\\n\\nThese findings suggest that including region- and culture-specific data in training is essential to accommodate users with different language backgrounds.\"}"}
{"id": "ck4SG9lnrQ", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Zero-shot accuracy on CMMLU STEM subset, and full set, with direct answer (DA) prompt and chain-of-thought (COT) prompt. To ensure a fair comparison, we use the free generation strategy.\\n\\n| Model          | STEM Overall | E changes (%) |\\n|----------------|--------------|---------------|\\n|               | DA | COT | DA | COT |               |\\n| ChatGPT       | 45.22 | 46.58 | 53.14 | 52.73 | +0.55          |\\n| ChatGLM2-6B   | 42.42 | 42.56 | 49.61 | 49.34 | -0.21          |\\n| Baichuan2-13B-Chat | 45.18 | 42.70 | 58.77 | 52.82 | +3.85          |\\n| BatGPT-15B-sirius | 38.13 | 34.66 | 45.26 | 42.87 | +1.35          |\\n| InternLM-Chat-20B | 42.09 | 32.31 | 53.52 | 43.29 | +3.87          |\\n| Xverse-13B-Chat | 40.13 | 30.53 | 52.96 | 39.27 | +19.77         |\\n\\n4.2 A\\n\\nAnalysis\\n\\nIn order to gain a comprehensive understanding of the LLM's performance on CMMLU, we explored three factors that may enhance the model's performance and two factors that could potentially diminish its performance. Specifically, we investigated whether the following factors can improve the model's performance: (1) utilizing chain-of-thought prompts, (2) increasing the number of input examples, and (3) employing larger-sized models within the same family. Conversely, we explored whether the following factors make the task more challenging for LLMs: (4) questions containing negation words, and (5) questions with sub-options within them. For different analyses, we choose different models in different stages according to the relevance and result availability.\\n\\nCan chain-of-thought prompt improve model performance?\\n\\nTo investigate the potential benefits of chain-of-thought (COT) prompt in generating better results, we modified the prompt from \\\"\u8bf7\u76f4\u63a5\u7ed9\u51fa\u6b63\u786e\u7b54\u6848\u7684\u9009\u62e9 (please provide the correct answer choice directly)\\\" to \\\"\u9010\u6b65\u5206\u6790\u5e76\u9009\u51fa\u6b63\u786e\u7b54\u6848 (Analyze step by step and select the correct answer).\\\" Since our dataset does not contain answer analysis, we adopt zero-shot setting for this experiment. The results are presented in Table 2, the breakdown of all sub-categories is provided in Appendix J.3.\\n\\nFrom the table, we see that for most models, the use of chain-of-thought prompt does not lead to improvement. ChatGPT and ChatGLM2 slightly gain improvement after using COT prompt for STEM subject, despite that the overall accuracy still decreases. We manually checked the outputs and found that models either fail to explicitly generate the answer option after the analysis (instead generating the content of the answer), or generate complex context to wrap the choice, which leads to the failure of regex match. An obvious case is Xverse, compare to the direct answer prompt, the use of COT prompt results in an increase of 19.77% responses that cannot be matched by our regex.\\n\\nFigure 4: Overall accuracy of models with varying number of few-shot examples.\"}"}
{"id": "ck4SG9lnrQ", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As illustrated in Figure 4, we present the overall accuracy of models utilizing varying numbers of in-context examples. There is a clear discrepancy that, when provided with only one example, foundation models exhibit an overall boost, whereas fine-tuned models experience a decline in performance. We conjecture this is because foundation models are primarily optimized for natural text and may struggle to follow instructions. Providing examples helps these models better understand the task. In contrast, SFT/RLHF models are optimized to follow instructions, and the introduction of examples introduces a certain degree of mismatch with the data distribution during their fine-tuning, thus leading to a decline in performance.\\n\\nWhen provided with more examples, while there may be fluctuations, the overall trend for foundation models indicates an improvement in performance with an increase in the number of examples. However, for fine-tuned models, there is no consistent trend.\\n\\nImpact of model size on performance\\nWe explored how the model\u2019s performance improves with an increase in the number of parameters. To this end, we examine several model families and present their five-shot accuracy in relation to model size in Figure 5.\\n\\nFrom the figure, we see that both LLaMA and LLaMA2 gain 5-point increase in scores as the model size changes from 7B to 13B, while Baichuan shows a remarkable 10-point improvement despite Baichuan-13B has 0.2T more training tokens than Baichuan-7B. We believe that have 7 billion parameters limit the model\u2019s capability in numerous tasks, while doubling the parameters to about 13 billion significantly enhances certain capabilities and improves memorization.\\n\\nAs the model size continues to increase (as seen with LLaMA and LLaMA2), the efficiency of performance improvement decreases, with a 5x increase in model size resulting in a 7% improvement for LLaMA and a 15% improvement for LLaMA2. Comparing LLaMA2 and Baichuan, it becomes evident that a smaller model equipped with higher-quality monolingual training data not only can achieve but also surpass the performance of a larger model with insufficient monolingual training data in terms of monolingual performance.\\n\\nTable 3: Average accuracy classified by questions w/ and w/o negation expressions, models are organized by model family. We use the free generation evaluation strategy.\\n\\n| Model          | 0-shot w/ | 0-shot w/o | 5-shot w/ | 5-shot w/o |\\n|----------------|-----------|------------|-----------|------------|\\n| ChatGPT        | 52.28     | 54.76      | 53.60     | 56.07      |\\n| GPT4           | 70.72     | 72.08      | 69.13     | 71.21      |\\n| LLaMA-65B      | 22.94     | 37.09      | 36.54     | 40.18      |\\n| LLaMA2-13B     | 24.16     | 30.32      | 37.27     | 39.49      |\\n| LLaMA2-13B-Chat| 28.24     | 34.40      | 37.90     | 38.73      |\\n| Baichuan-13B-Base | 47.84   | 51.20      | 55.47     | 56.03      |\\n| Baichuan2-13B-Base | 59.52  | 61.60      | 61.96     | 62.61      |\\n| Baichuan2-13B-Chat | 58.64  | 56.96      | 60.60     | 60.89      |\\n| ChatGLM-6B     | 34.00     | 31.12      | 41.62     | 38.00      |\\n| ChatGLM2-6B    | 51.20     | 50.08      | 51.88     | 50.04      |\\n\\nTable 4: Average accuracy classified by questions w/ and w/o sub-options. We use the free generation strategy, except for the models with *, which are foundation models without instruction-following ability.\\n\\n| Model          | 0-shot w/ | 0-shot w/o | 5-shot w/ | 5-shot w/o |\\n|----------------|-----------|------------|-----------|------------|\\n| GPT4           | 51.14     | 53.41      | 69.74     | 71.72      |\\n| ChatGPT        | 34.85     | 33.33      | 53.90     | 56.47      |\\n| LLaMA2-70B*    | 25.38     | 28.03      | 49.85     | 54.04      |\\n| Falcon-40B*    | 23.11     | 28.41      | 38.72     | 42.14      |\\n| Baichuan2-13B-Chat | 47.73   | 34.09      | 59.78     | 57.41      |\\n| +COT BatGPT-15B-sirius | 30.68 | 31.06      | 46.51     | 41.78      |\\n| +COT ChatGLM2-6B | 28.79     | 27.65      | 50.84     | 49.82      |\"}"}
{"id": "ck4SG9lnrQ", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To investigate the correlation between models performance on CMMLU and other benchmarks, we choose 6 popular English LLMs and 5 benchmarks to conduct correlation analysis.\\n\\nFrom Figure 10 we find that CMMLU demonstrates a strong correlation with four of these benchmarks, which span areas such as mathematics, commonsense reasoning, and coding. The exception is the PIQA task, where the relevance is somewhat diminished due to most models achieving high scores (>80%) on this task. However, 0.88 still shows strong positive correlation.\\n\\n![Figure 10: Correlation between the performance on CMMLU and that of other benchmarks. We choose RACE dataset for general language understanding, CommonSenseQA for commonsense reasoning, PIQA for general reasoning, GSM8K for mathematics, and HumanEval for code ability.](image)\\n\\nTable 11 displays zero-shot results of the LLMs on CMMLU by 5 sub-categories.\\n\\nWe compared the 0-shot and 5-shot results of selected LLMs that showed higher performance on each subject in Table 10. We further analyze the performance distribution of multiple LLMs across all subjects in Figure 11. It is evident from the figure that LLMs with higher performance exhibit diverse abilities across various tasks, while those with lower performance face challenges in most subjects. Furthermore, the scatter plot distribution indicates comparable performance levels among LLMs across different subjects.\\n\\nTable 12 shows the breakdown of the models performance after using chain-of-thought prompt.\"}"}
{"id": "ck4SG9lnrQ", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: The results of 0-shot and 5-shot accuracy per subject. The number on the left of 0-shot and the number on the right of 5-shot. The models are LLaMA2-70B, Falcon-40B, Baichuan2-13B-Chat, ChatGLM2-6B, InternLM-Chat-20B, BatGPT-15B-sirius.\\n\\n| Subject                        | GPT4 | LLaMA2 | Falcon | Baichuan2 | ChatGLM2 | InternLM | BatGPT |\\n|--------------------------------|------|--------|--------|-----------|----------|----------|--------|\\n| Ancient Chinese               | 37.2 | 27.4   | 26.8   | 40.9      | 26.8     | 33.5     | 29.9   |\\n| Chinese Civil Service Exam    | 63.7 | 50.0   | 33.8   | 61.9      | 51.2     | 49.4     | 52.5   |\\n| Chinese Driving Rule         | 82.4 | 66.4   | 55.0   | 77.1      | 60.3     | 67.2     | 62.6   |\\n| Chinese Food Culture         | 65.4 | 35.3   | 33.1   | 60.3      | 50.0     | 52.2     | 55.9   |\\n| Chinese Foreign Policy       | 81.3 | 62.6   | 48.6   | 74.8      | 60.7     | 71.0     | 52.3   |\\n| Chinese History              | 76.5 | 61.9   | 46.1   | 72.8      | 61.0     | 77.1     | 61.6   |\\n| Chinese Literature           | 49.5 | 37.7   | 27.5   | 57.4      | 36.3     | 48.0     | 39.2   |\\n| Chinese Teacher Qualification| 78.2 | 59.2   | 45.8   | 79.3      | 61.5     | 75.4     | 60.3   |\\n| Construction Project Management| 51.1 | 41.7   | 30.2   | 43.2      | 36.7     | 44.6     | 41.7   |\\n| Elementary Chinese           | 53.2 | 29.4   | 28.5   | 57.9      | 45.6     | 48.0     | 44.8   |\\n| Elementary Commonsense       | 68.2 | 46.5   | 35.6   | 62.6      | 52.5     | 55.6     | 50.5   |\\n| Ethnology                     | 63.7 | 42.2   | 36.3   | 65.9      | 48.1     | 63.0     | 47.4   |\\n| High School Politics         | 67.1 | 44.1   | 35.7   | 76.9      | 49.0     | 53.8     | 49.0   |\\n| Modern Chinese               | 56.0 | 34.5   | 28.4   | 45.7      | 44.0     | 41.4     | 40.5   |\\n| Traditional Chinese Medicine | 58.4 | 38.4   | 31.9   | 55.1      | 48.1     | 48.6     | 48.1   |\\n| Agronomy                      | 66.3 | 46.2   | 35.5   | 58.0      | 46.7     | 56.2     | 47.3   |\\n| Clinical Knowledge           | 68.8 | 42.2   | 36.7   | 51.5      | 44.3     | 45.1     | 40.5   |\\n| College Medicine             | 72.2 | 39.6   | 26.7   | 56.4      | 42.9     | 40.3     | 44.7   |\\n| Computer Security            | 87.7 | 63.7   | 40.4   | 66.1      | 56.1     | 71.3     | 63.2   |\\n| Elementary IT                | 93.7 | 76.9   | 54.6   | 79.0      | 68.1     | 73.5     | 66.0   |\\n| Food Science                  | 74.1 | 53.1   | 39.2   | 60.1      | 49.7     | 55.2     | 47.6   |\\n| Human Sexuality               | 72.2 | 60.3   | 45.2   | 61.1      | 48.4     | 61.1     | 52.4   |\\n| Legal And Moral Basis        | 91.1 | 82.7   | 67.3   | 92.1      | 83.6     | 90.2     | 84.6   |\\n| Nutrition                     | 73.8 | 49.7   | 42.1   | 57.9      | 53.1     | 52.4     | 51.0   |\\n| Professional Medicine        | 66.5 | 34.8   | 26.6   | 50.5      | 37.5     | 41.0     | 33.0   |\\n| Sports Science                | 70.9 | 51.5   | 43.6   | 60.0      | 49.7     | 60.6     | 50.3   |\\n| Business Ethics               | 70.8 | 56.9   | 40.2   | 59.8      | 46.4     | 56.5     | 52.6   |\\n| College Education             | 79.4 | 62.6   | 55.1   | 72.9      | 64.5     | 72.9     | 66.4   |\\n| Economics                     | 84.9 | 55.3   | 48.4   | 62.3      | 46.5     | 55.3     | 52.8   |\\n| Education                     | 63.8 | 51.5   | 41.7   | 69.9      | 60.1     | 60.1     | 58.9   |\\n| High School Geography         | 78.0 | 42.4   | 44.1   | 66.1      | 47.5     | 56.8     | 47.5   |\\n| Journalism                    | 68.0 | 54.1   | 43.0   | 59.3      | 52.9     | 55.8     | 52.9   |\\n| Management                    | 82.9 | 56.7   | 49.5   | 68.6      | 62.9     | 65.2     | 62.4   |\\n| Marketing                     | 81.7 | 65.6   | 43.9   | 67.8      | 57.2     | 67.2     | 55.0   |\\n| Professional Accounting       | 72.6 | 51.4   | 41.1   | 70.3      | 56.6     | 55.4     | 57.7   |\\n| Professional Psychology       | 81.9 | 50.0   | 42.2   | 70.3      | 55.6     | 68.5     | 58.2   |\\n| Public Relations              | 63.8 | 56.9   | 46.0   | 64.4      | 51.1     | 55.2     | 51.7   |\\n| Security Study                | 80.0 | 54.8   | 48.1   | 70.4      | 58.5     | 64.4     | 60.7   |\\n| Sociology                     | 72.1 | 59.3   | 41.2   | 64.2      | 51.3     | 58.8     | 49.1   |\\n| Arts                           | 74.4 | 58.8   | 50.6   | 83.1      | 66.2     | 75.6     | 69.4   |\\n| College Law                   | 59.3 | 39.8   | 31.3   | 55.6      | 45.4     | 47.2     | 42.6   |\\n| Global Facts                  | 71.8 | 49.0   | 39.5   | 71.1      | 57.0     | 64.4     | 51.7   |\\n| International Law             | 61.1 | 49.7   | 40.0   | 56.2      | 38.4     | 47.6     | 41.1   |\\n| Jurisprudence                 | 71.0 | 58.4   | 39.4   | 63.0      | 53.0     | 59.4     | 53.0   |\\n| Logical                       | 70.7 | 54.5   | 35.8   | 59.3      | 48.0     | 54.5     | 41.5   |\\n| Marxist Theory                | 78.8 | 60.8   | 50.3   | 76.2      | 56.6     | 69.8     | 56.6   |\\n| Philosophy                    | 69.5 | 61.0   | 52.4   | 68.6      | 59.0     | 70.5     | 53.3   |\\n| Professional Law              | 53.6 | 37.4   | 29.4   | 50.2      | 41.7     | 48.8     | 40.3   |\\n| World History                 | 84.5 | 64.0   | 45.3   | 64.6      | 55.3     | 76.4     | 56.5   |\\n| World Religions               | 78.8 | 61.3   | 49.4   | 72.5      | 58.8     | 63.7     | 55.0   |\\n| Anatomy                       | 69.6 | 33.8   | 25.3   | 48.6      | 34.5     | 34.5     | 35.1   |\\n| Astronomy                     | 55.8 | 37.6   | 26.7   | 41.2      | 31.5     | 37.0     | 36.4   |\\n| College Actuarial Science     | 43.4 | 28.3   | 32.1   | 30.2      | 23.6     | 27.4     | 25.5   |\\n| College Engineering Hydrology | 66.0 | 50.0   | 40.6   | 51.9      | 36.8     | 50.0     | 39.6   |\\n| College Mathematics           | 45.7 | 23.8   | 24.8   | 24.8      | 21.9     | 36.2     | 28.6   |\\n| College Medical Statistics    | 73.6 | 47.2   | 32.1   | 51.9      | 46.2     | 53.8     | 44.3   |\\n| Computer Science              | 77.9 | 52.9   | 34.3   | 58.3      | 47.1     | 55.9     | 48.0   |\\n| Conceptual Physics            | 73.5 | 47.6   | 38.8   | 60.5      | 63.3     | 51.0     | 63.9   |\\n| Electrical Engineering        | 65.1 | 47.1   | 40.1   | 54.1      | 37.8     | 55.2     | 45.9   |\\n| Elementary Mathematics        | 51.7 | 33.5   | 28.3   | 41.3      | 45.7     | 28.7     | 40.4   |\\n| Genetics                      | 68.8 | 45.5   | 32.4   | 46.0      | 40.3     | 44.9     | 41.5   |\\n| High School Biology           | 64.5 | 38.5   | 26.0   | 59.2      | 60.9     | 52.1     | 62.7   |\\n| High School Chemistry         | 44.7 | 25.0   | 28.0   | 44.7      | 55.3     | 34.8     | 52.3   |\\n| High School Mathematics       | 45.7 | 28.0   | 21.3   | 25.6      | 34.8     | 34.8     | 35.4   |\\n| High School Physics           | 70.0 | 38.2   | 28.2   | 41.8      | 47.3     | 37.3     | 45.5   |\\n| Machine Learning              | 77.9 | 48.4   | 31.1   | 51.6      | 45.1     | 54.1     | 41.0   |\\n| Virology                      | 79.3 | 58.6   | 34.9   | 63.3      | 49.1     | 55.0     | 47.3   |\"}"}
{"id": "ck4SG9lnrQ", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 11: Zero-shot accuracy of models. We report macro average accuracy over subjects within each category. \\\"Overall\\\" = macro average score over all subjects. \\\"State\\\" indicates whether the model is pre-trained (Base) or Fine-tuned to follow instructions (Chat). '*' indicate there are both Base and Chat model released, we choose the one with better overall accuracy. The first block is multilingual-or English-oriented models, and the second block is Chinese-oriented models. To save space, we didn\u2019t present models with an overall score lower than 30.\\n\\n| Model          | State | STEM     | Humanities | Social Science | Other | China-specific | Overall |\\n|----------------|-------|----------|------------|---------------|-------|----------------|---------|\\n| GPT4           | Chat  | 63.13    | 69.19      | 70.26         | 73.16 | 63.47          | 68.89   |\\n| ChatGPT        | Chat  | 44.80    | 53.61      | 54.22         | 59.95 | 49.74          | 53.22   |\\n| LLaMA2-70B*    | Base  | 40.23    | 53.41      | 50.10         | 52.91 | 45.16          | 48.87   |\\n| BLOOMZ-7B Chat| Chat  | 33.03    | 45.74      | 45.74         | 46.25 | 41.58          | 42.80   |\\n| Falcon-40B     | Base  | 31.11    | 41.30      | 40.87         | 40.61 | 36.05          | 38.50   |\\n| LLaMA2-13B*    | Chat  | 31.57    | 37.89      | 38.10         | 39.00 | 35.44          | 36.60   |\\n| LLaMA-65B      | Base  | 31.09    | 34.45      | 36.05         | 37.94 | 32.89          | 34.88   |\\n| BX LLaMA-30B   | Chat  | 28.79    | 32.61      | 31.65         | 34.22 | 31.47          | 31.69   |\\n| LLaMA-30B      | Base  | 30.02    | 31.87      | 31.51         | 32.90 | 29.64          | 31.54   |\\n| BX LLaMA-13B   | Chat  | 26.46    | 29.36      | 31.81         | 31.55 | 29.17          | 30.06   |\\n| Baichuan2-13B*| Base  | 47.59    | 65.57      | 65.24         | 65.47 | 62.10          | 60.88   |\\n| Xverse-13B*    | Base  | 43.42    | 60.51      | 60.65         | 64.20 | 56.69          | 57.04   |\\n| InternLM-20B*  | Chat  | 43.68    | 61.78      | 58.19         | 57.54 | 55.26          | 55.06   |\\n| Baichuan-13B*  | Base  | 41.63    | 60.26      | 59.62         | 56.15 | 56.03          | 54.40   |\\n| InternLM-7B*   | Base  | 43.04    | 56.72      | 56.96         | 54.50 | 54.55          | 52.83   |\\n| ChatGLM2-6B    | Chat  | 42.98    | 52.42      | 52.56         | 52.15 | 49.38          | 50.01   |\\n| BatGPT-15B     | Chat  | 43.15    | 50.91      | 52.66         | 52.23 | 49.09          | 49.81   |\\n| Baichuan-7B    | Base  | 32.79    | 44.43      | 46.83         | 44.79 | 43.19          | 42.35   |\\n| ChatGLM-6B     | Chat  | 32.54    | 42.91      | 44.91         | 42.29 | 42.08          | 40.80   |\\n| Random         | \u2013      | 25.00    | 25.00      | 25.00         | 25.00 | 25.00          | 25.00   |\\n\\nTable 12: The Impact of Chain of Thoughts (COT) on the performance of several LLMs on CMMLU. The numbers on the left represent the values after incorporating COT, with the values in parentheses indicating the change relative to the model\u2019s performance in the 0-shot scenario.\\n\\n| Model          | State | STEM (-) | Humanities (-) | Social Science (-) | Other (-) | China-specific (-) | Overall (-) |\\n|----------------|-------|----------|---------------|-------------------|-----------|----------------------|-------------|\\n| Baichuan2-13B-Chat | Chat  | 42.7    | 57.7          | 56.0              | 55.4      | 53.8                 | 52.8        |\\n| BatGPT-15B-sirius   | Chat  | 34.7    | 44.2          | 45.8              | 46.6      | 43.6                 | 42.9        |\\n| ChatGLM-6B        | Chat  | 29.9    | 37.9          | 39.6              | 36.2      | 38.3                 | 36.0        |\\n| ChatGLM2-6B       | Chat  | 42.6    | 52.3          | 51.3              | 51.6      | 49.0                 | 49.3        |\\n| ChatGPT          | Chat  | 46.6    | 52.5          | 54.0              | 58.0      | 47.7                 | 52.7        |\\n| InternLM-Chat-20B | Chat  | 32.3    | 48.1          | 48.1              | 44.6      | 44.9                 | 43.3        |\\n| Xverse-13B-Chat   | Chat  | 30.5    | 40.2          | 43.0              | 42.8      | 38.7                 | 39.3        |\\n\\n27\"}"}
