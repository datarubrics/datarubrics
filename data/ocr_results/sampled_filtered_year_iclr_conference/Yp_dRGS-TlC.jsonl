{"id": "Yp_dRGS-TlC", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1: Augmented Sentences Sampling\\n\\ninput: A pair of parallel sentences \\\\( \\\\langle x, y \\\\rangle \\\\), alignment of the \\\\( t \\\\)-th iteration \\\\( A_t \\\\), alignment of the \\\\((t + 1)\\\\)-th iteration \\\\( A_{t+1} \\\\), sampling probability \\\\( P_{old} \\\\) for old pairs, sampling probability \\\\( P_{new} \\\\) for new pairs, the sampling rounds \\\\( \\\\text{rounds} \\\\).\\n\\noutput: Augmented sentences \\\\( S_{aug} \\\\)\\n\\n1. \\\\( S_{aug} \\\\leftarrow \\\\emptyset \\\\)\\n2. \\\\( A_{old} \\\\leftarrow A_t \\\\cap A_{t+1} \\\\)\\n3. \\\\( A_{new} \\\\leftarrow A_{t+1} \\\\setminus (A_t \\\\cap A_{t+1}) \\\\)\\n4. \\\\( \\\\text{SET}AP \\\\leftarrow \\\\{(A_{old}, P_{old}), (A_{new}, P_{new})\\\\} \\\\)\\n5. for \\\\( i \\\\leftarrow 1 \\\\) to \\\\( \\\\text{rounds} \\\\) do\\n6.   for \\\\((A, P) \\\\in \\\\text{SET}AP\\\\) do\\n7.     for \\\\( \\\\text{pair} \\\\in A \\\\) do\\n8.       if \\\\( P \\\\geq \\\\text{random.random}() \\\\) then\\n9.         \\\\( \\\\langle x, y \\\\rangle = \\\\text{Code-switch}(\\\\langle x, y \\\\rangle, \\\\text{pair}) \\\\)\\n10.        \\\\( S_{aug} \\\\leftarrow S_{aug} \\\\cup \\\\langle x, y \\\\rangle \\\\)\\n\\n3.2 Code-switched Augmentation\\n\\nCode-switching is a prevalent phenomenon in multilingual communities where the words, morphemes and phrases from two or more languages are switched in speech or writing. The switched ones usually are semantically similar.\\n\\nSuppose we get \\\\( p \\\\) different pairs of source and target words for sentence \\\\( x = \\\\langle x_1, \\\\cdots, x_n \\\\rangle \\\\) and \\\\( y = \\\\langle y_1, \\\\cdots, y_m \\\\rangle \\\\), it is easy to augment sentences by code-switching. The source words and target words in one pair can be considered as synonyms and can be switched. If we augmented sentences by code-switch sentences with one token at a time, then we have \\\\( C_1^p \\\\) kinds choices, which means we can get \\\\( C_1^p \\\\) different sentence pairs at most. If the number of switched pairs range from 0 to \\\\( p \\\\), then the maximum of different sentence pairs without considering special cases:\\n\\n\\\\[\\nC_0^p + C_1^p + C_2^p + \\\\cdots + C_p^p = 2^p \\\\tag{3}\\n\\\\]\\n\\nObviously this is an exponential and impressive data augmentation method. In practice, we do not exhaust all code-switched sentences and the sampling method is illustrated in Algorithm 1. We set \\\\( A_0 = \\\\emptyset \\\\) and \\\\( A_1 = \\\\emptyset \\\\) so that when \\\\( t = 0 \\\\), Algorithm 1 still works and in this setting, the 0-th iteration is exactly the standard task-adaptive pretraining. When \\\\( t > 1 \\\\), alignment results of two successive iterations will be used so that we can assign different sampling distributions to the intersection and new aligned pairs. In general, we give a higher probability for new aligned pairs. In addition, we will also add origin parallel sentences to augmented dataset which is prepared for masked language modeling.\\n\\n3.3 MLM on Code-switched Dataset\\n\\nFor masked language modeling (MLM), the input sequence \\\\( x \\\\) consists of multiple individual tokens. A fraction of the input tokens are chosen randomly and replaced with \\\\(< \\\\text{MASK} >\\\\) tokens. Assume that these masked indices are collected together in a set \\\\( \\\\text{mask}(x) \\\\) and we use \\\\( \\\\hat{x} \\\\) to denote a masked token. Then model with parameters \\\\( \\\\theta \\\\) learns to predict \\\\( \\\\text{mask}(x) \\\\) by the surrounding unmasked tokens \\\\( x \\\\setminus \\\\text{mask}(x) \\\\).\\n\\n\\\\[\\nL_{\\\\text{MLM}} = -\\\\sum_{\\\\hat{x} \\\\in \\\\text{mask}(x)} \\\\log P_{\\\\theta}(\\\\hat{x} | x \\\\setminus \\\\text{mask}(x)) \\\\tag{4}\\n\\\\]\\n\\nUsing code-switched and origin parallel sentences to train model will encourage them to align representations from source and target languages by mixing their context information. More importantly, self-labeled word pairs have the same surrounding tokens. So training model on these sentences will obviously align code-switched tokens in implicit manner and the predicted pairs will also move towards each other in the embedding space. For example, in Figure 2, \u201cDas\u201d and \u201cThis\u201d is aligned.\"}"}
{"id": "Yp_dRGS-TlC", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nPair. When we train model on the code-switched sentence \u201cDas is quite a good idea.\u201d and origin sentence \u201cThis is quite a good idea.\u201d, the word \u201cDas\u201d and English word \u201cThis\u201d will move towards each other in the embedding space because they have same surrounding tokens. We try to dig deeper and give an approximate formal explanations for the connection between word alignment and masked language modeling, which can also serve as a potential explanation why code-switching can be used to improve machine translation and cross-lingual tasks.\\n\\n**Proposition:** Training model with standard masked language modeling on source-language, target-language and source-target code-switched sentences is approximately optimizing:\\n\\n\\\\[\\n\\\\begin{align*}\\n&c_{\\\\text{src}} x_i \\\\sim e x_i \\\\sim c_{\\\\text{tgt}} y_i \\\\\\\\\\n&\\\\text{where } c_{\\\\text{src}} x_i \\\\text{ represents the contextualized embedding of token } x_i \\\\text{ in source sentence and } c_{\\\\text{src}} y_i \\\\text{ represents the contextualized embedding of token } y_i \\\\text{ in target sentence. And } e x_i \\\\text{ and } e y_i \\\\text{ are word embeddings of token } x_i \\\\text{ and } y_i \\\\text{ in vocabulary.}\\n\\\\end{align*}\\n\\\\]\\n\\n\\\\[\\n\\\\begin{align*}\\n&c_{\\\\text{src}} x_i \\\\sim e x_i \\\\text{ represents that } c_{\\\\text{src}} x_i \\\\text{ is similar to } e x_i .\\n\\\\end{align*}\\n\\\\]\\n\\nSee appendix for details.\\n\\nWe assume that this approximate similarity is sufficient to induce word alignment. Then we have:\\n\\n**Corollary:** The performance based on the last layer of optimized model (with iterative task-adaptive pretraining) is better than the best results (usually the eighth layer) of baseline model (without iterative task-adaptive pretraining).\\n\\nIn section 4, we will test whether this corollary holds in experiments.\\n\\n### 4 EXPERIMENTS\\n\\nOur experiments focus on three questions: (1) To what extent can our paradigm improve word alignment across methods, models, languages and layers. (2) The effect of the number of augmented sentences and sampling probability. (3) The detailed ablation study.\\n\\n#### 4.1 DATASET\\n\\nOur test data are a diverse set of 6 language pairs: Persian, Czech, German, French, Hindi and Romanian, always paired with English. All of them are public dataset: En-Fa (Tavakoli & Faili), En-Cs (Marecek), En-De, En-Fr (Och & Ney, 2000), En-Hi and En-Ro. See Table 1 for detailed statistics of datasets.\\n\\n| Langs | En-Hi | En-Fa | En-Cs | En-De | En-Fr | En-Ro |\\n|-------|-------|-------|-------|-------|-------|-------|\\n| Size  | 90    | 400   | 2500  | 508   | 447   | 203   |\\n| | S    | 1409  | 11606 | 44292 | 9612  | 4038  | 5033  |\\n| | P \\\\(\\\\setminus S\\\\) | 0  | 23132 | 921   | 13400 | 0  |\\n\\nTable 1: Statistics of Datasets. Test sentences of the six gold word alignment datasets used in our experiments: English-Hindi (En-Hi), English-Persian (En-Fa), English-Czech (En-Cs), English-German (En-De), English-French (En-Fr), English-Romanian (En-Ro). \u201cSize\u201d refers to the number of sentences. S is sure alignments and P is possible alignments (\\\\(S \\\\subset P\\\\)).\\n\\n#### 4.2 EVALUATION MEASURES\\n\\nWe use Alignment Error Rate (Och & Ney, 2003) as the standard evaluation:\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{AER} = 1 - \\\\frac{|A \\\\cap S| + |A \\\\cap P|}{|\\\\Lambda| + |S|}\\n\\\\end{align*}\\n\\\\]\\n\\nwhere A is a set of predicted alignment edges, S (sure) is (sure) unambiguous alignments and P (possible) is ambiguous alignments (\\\\(S \\\\subset P\\\\)). We report the percentage.\\n\\n1. [http://www-i6.informatik.rwth-aachen.de/goldAlignment/](http://www-i6.informatik.rwth-aachen.de/goldAlignment/)\\n2. [http://web.eecs.umich.edu/mihalcea/wpt05/](http://web.eecs.umich.edu/mihalcea/wpt05/)\"}"}
{"id": "Yp_dRGS-TlC", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our experiments focus on three alignment methods based on multilingual pretrained models: Argmax, Itermax and Match, proposed by SimAlign (Sabet et al., 2020) and we follow the default setting of repository without any modification. We use the contextualized embeddings from 8-th layer. We employ two multilingual pretrained models: the multilingual BERT model (mBERT), which is pretrained on the 104 largest Wikipedia languages and XLM-RoBERTa base (Conneau et al., 2020), which is pretrained on 100 languages on cleaned CommonCrawl data (Wenzek et al., 2020). The pretrained LMs often use subword segmentation techniques (Kudo & Richardson, 2018; Sennrich et al., 2016) and the above alignment extraction methods can only produce alignments on the subword level, we follow previous work (Sabet et al., 2020; Zenkel et al., 2020b; Dou & Neubig, 2021) and consider two words to be aligned if any of their subwords is aligned. For masked language modeling, we following Devlin et al. (2019a) and use a special \\\\[\\\\text{[MASK]}\\\\] token 80\\\\% of the time, a random token 10\\\\% of the time and the original token 10\\\\% of the time to perform masking. The batch size is set to 32. Max epoch is set to 10. We use the Adam optimizer with a learning rate of $2 \\\\times 10^{-5}$. Weight decay is 0.03. We set the filtering threshold to 0.9. When the number of iterations is 1, it only one alignment set sampling probability to 0.7. When the number of iterations is more than 1, we set the sampling probability $P_{\\\\text{old}}$ to 0.7 and $P_{\\\\text{new}}$ to 1.0. The sampling rounds is defaults to 5.\\n\\n### Table 2: Evaluation results on six datasets.\\n\\n| Methods and Models | En-De | En-Fa | En-Hi | En-Ro |\\n|-------------------|-------|-------|-------|-------|\\n| Argmax | F1 | AER |\\n| Itermax | F1 | AER |\\n| Match | F1 | AER |\\n| IBM2 fast-align | 48 | 52 | 63 | 37 |\\n| IBM4 Giza++ | 56 | 48 | 57 | 43 |\\n| eflomal | 55 | 45 | 67 | 33 |\\n| mBERT Argmax | 57 | 42 | 70 | 29 |\\n| mBERT Iter 0 | 57 | 42 | 70 | 29 |\\n| mBERT Iter 1 | 57 | 42 | 70 | 29 |\\n| mBERT Iter 2 | 57 | 42 | 70 | 29 |\\n| XLM-R Argmax | 61 | 39 | 71 | 29 |\\n| XLM-R Iter 0 | 62 | 37 | 72 | 27 |\\n| XLM-R Iter 1 | 65 | 34 | 73 | 26 |\\n| XLM-R Iter 2 | 66 | 33 | 74 | 26 |\\n| XLM-R Itermax | 63 | 36 | 72 | 28 |\\n\\n### Table 3: AER with different sampling probabilities.\\n\\n| Rounds | En-De | En-Fa | En-Hi | En-Ro |\\n|--------|-------|-------|-------|-------|\\n| 2      | 17.4  | 26.2  | 35.1  | 26.5  |\\n| 5      | 17.3  | 25.8  | 34.5  | 25.8  |\\n| 8      | 17.3  | 25.7  | 34.3  | 25.8  |\\n\\n### Table 4: The effect of the number of augmented sentences.\\n\\n| Rounds | En-De | En-Fa | En-Hi | En-Ro |\\n|--------|-------|-------|-------|-------|\\n| 2      | 17.4  | 26.2  | 35.1  | 26.5  |\\n| 5      | 17.3  | 25.8  | 34.5  | 25.8  |\\n| 8      | 17.3  | 25.7  | 34.3  | 25.8  |\"}"}
{"id": "Yp_dRGS-TlC", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\n| Layer | En-Hi Baseline | Ours |\\n|-------|---------------|------|\\n| 0     | 53.4          | 53.5 |\\n| 1     | 58.5          | 55.1 |\\n| 2     | 61.9          | 59.0 |\\n| 3     | 57.4          | 52.9 |\\n| 4     | 49.7          | 45.8 |\\n| 5     | 45.9          | 41.2 |\\n| 6     | 40.4          | 36.0 |\\n| 7     | 40.2          | 35.7 |\\n| 8     | 39.4          | 34.5 |\\n| 9     | 42.2          | 39.1 |\\n| 10    | 44.4          | 37.0 |\\n| 11    | 46.7          | 35.9 |\\n\\n| Layer | En-Fa Baseline | Ours |\\n|-------|---------------|------|\\n| 0     | 47.7          | 47.6 |\\n| 1     | 50.4          | 48.2 |\\n| 2     | 54.2          | 52.0 |\\n| 3     | 47.2          | 45.8 |\\n| 4     | 39.2          | 37.1 |\\n| 5     | 34.0          | 30.7 |\\n| 6     | 29.9          | 27.2 |\\n| 7     | 29.2          | 26.6 |\\n| 8     | 28.9          | 26.1 |\\n| 9     | 30.7          | 25.8 |\\n| 10    | 33.6          | 26.1 |\\n| 11    | 37.0          | 28.0 |\\n\\nTable 5: Evaluation results of different layers.\\n\\nIt can be observed that our method improves the performance of three alignment methods on generating high-quality alignment pairs. And from section 3.2, the standard task-adaptive pretraining is equivalent to Iter 0. With the increase of iteration rounds, our method can constantly improve model performance. This proves that iterative task-adaptive pretraining is effective. In addition, there is an impressive trend: the worse the initial performance, the more the improvement. For En-Cs, En-De, and En-Fr, baseline methods perform well and the AER scores are generally lower than 20 percent. After three iterations, the AER scores are reduced by about 1.5 points on average. For En-Ro, En-Fa, and En-Hi, which are relatively low-resource or different morphological language-pairs, baseline methods perform poorly and AER scores are generally greater than 30 percent.\\n\\nFigure 3: The comparison between ours and baseline cross layers. Lower is better.\\n\\nAfter three iterations, the AER scores are reduced by about 4\u20135 points on average. This is an exciting result, which doesn't need any additional parallel sentences and gold labels. In the following experiments, we employ the alignment method Argmax for further analysis and explore the word alignment across different layers. Figure 3 and table 5 show that our paradigm consistently improves baseline performance across all layers. For both En-Hi and En-Fa, the performance of the last layer based on our methods consistently outperforms the best result of baseline models. So the Corollary in section 3.3 is valid and our assumption \\\"this approximate similarity is sufficient to induce word alignment\\\" is supported by experiments.\\n\\n4.5 ANALYSIS\\n\\nWe further explore the effect of sampling probability and the number of augmented sentences on model performance. We choose XLM-RoBERTa base for multilingual pretrained models and Argmax as the alignment method. Given the large amount of possible experiments when considering 6 language pairs, we do not present all scores for all languages and we will pick up four of them in most cases: En-De, an established and well-known dataset, En-Fa, and En-Hi, two low-resource languages written in a different script, and En-Ro. And only two rounds of iterative training are performed (t=0,1) and we only list the final results.\\n\\nTable 3 shows the Alignment Error Rate in the setting of different sampling probabilities. When the sampling probability of code-switching is too high or too low, the diversity of augmented sentences will decline, which may hurt model performance. Table 3 proves this point. Although the final scores of different sampling probabilities are close, the middle probability 0.7 achieves the best scores across four languages. Table 4 shows the effect of the number of augmented sentences. The sampling rounds are proportional to the number of augmented sentences. From this table, we can infer that the scores will increase in general when more augmented sentences become available. But gains continue to decay. At the same time, the cost of MLM training will increase with more augmented sentences. So we set the sampling rounds to 5 without special statements.\"}"}
{"id": "Yp_dRGS-TlC", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How to establish a closer relationship between pre-training and downstream task is a valuable question. We argue that task-adaptive pretraining should not be just performed before task. For word alignment task, we propose an iterative self-supervised task-adaptive pretraining paradigm, tying together word alignment and self-supervised pretraining by code-switching data augmentation. When we get the aligned pairs predicted by the multilingual contextualized word embeddings, we employ these pairs and origin parallel sentences to synthesize code-switched sentences. Then multilingual models will be continuously finetuned on the augmented code-switched dataset. Finally, finetuned models will be used to produce new aligned pairs. This process will be executed iteratively. Our paradigm is suitable for almost all unsupervised word alignment methods based on multilingual pre-trained LMs and doesn't need gold labeled data, extra parallel data or any other external resources. Experimental results on six language pairs demonstrate that our paradigm can consistently improve baseline method. Compared to resource-rich languages, the improvements on relatively low-resource or different morphological languages are more significant. For example, the AER scores of three different alignment methods based on XLM-R are reduced by about 4~5 percentage points on language pair En-Hi.\"}"}
{"id": "Yp_dRGS-TlC", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nFigure 1: Continued pretraining of a LM on the unlabeled data of a given task (task-adaptive pretraining) (Gururangan et al., 2020) has been show to be beneficial for task performance. However, it only occurs before tasks and is obviously not closely integrated with tasks. For word alignment task, we propose a new paradigm, in which task-adaptive pretraining and word alignment be executed iteratively.\\n\\nWe augment sentences with self-labeled pairs and code-switching strategy. When we get the aligned pairs predicted by the multilingual contextualized word embeddings, we employ these pairs and origin parallel sentences to synthesize code-switched sentences. Then multilingual models will be continuously finetuned on the augmented code-switched dataset. Finally, finetuned models will be used to produce new aligned pairs. This process will be executed iteratively. On the one hand, data augmentation with self-labeled pairs and code-switching strategy can alleviates data scarcity. On the other hand, training LMs with MLM on code-switched sentences can bring the different languages closer together in the embedding space and if we training LMs on both code-switched sentence and corresponding origin sentences, the code-switched tokens (predicted pairs) will also move towards each other in the embedding space.\\n\\nOur contribution can be listed as follows:\\n\\n\u2022 We design an iterative task-adaptive pretraining paradigm for word alignment, in which task-adaptive pretraining will be performed not only before task but also after task. In other words, task-adaptive pretraining and word alignment will be executed iteratively.\\n\\n\u2022 Our paradigm is suitable for almost all unsupervised word alignment methods based on multilingual pre-trained LMs and doesn't need gold labeled data, extra parallel data or any other external resources. We also don't need to introduce carefully designed loss function and the results can be easily reproduced.\\n\\n\u2022 In-depth analysis reveals that training model with standard masked language modeling on source-language, target-language and code-switched sentences is approximately optimizing the similarity of switched tokens. This can serve as a potential explanation why code-switching can be used to improve machine translation and cross-lingual tasks.\\n\\n\u2022 We perform experiments on six language pairs and demonstrate that our paradigm can consistently improve baseline methods. For example, the AER scores of three different alignment methods based on XLM-R are reduced by about $\\\\frac{4}{\\\\sqrt{2}} \\\\sim \\\\frac{5}{\\\\sqrt{2}}$ percentage points on language pair En-Hi.\\n\\n2.1 Task Adaptive Pretraining\\n\\nLanguage models pretrained on text from a wide variety of sources form the foundation of today's NLP. Gururangan et al. (2020) propose domain-adaptive pretraining and task-adaptive pretraining. They explore the benefits of continued pretraining on data from the task distribution and the domain distribution. Gu et al. (2020) add a task-guided pre-training stage with selective masking between general pre-training and fine-tuning. Jin et al. (2022) study a lifelong language model pretraining challenge where a PTLM is continually updated so as to adapt to emerging data. Karouzos et al. (2021) simultaneously minimize a task-specific loss on the source data and a language modeling...\"}"}
{"id": "Yp_dRGS-TlC", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This is quite a good idea.\\n\\nStatistical alignment models directly build on the lexical translation models such as the IBM models (Brown et al., 1993) and their implementations Giza++ (Och & Ney, 2003), fast-align (Dyer et al., 2013) and eflomal (\u00d6stling & Tiedemann, 2016) are widely used for alignment. Based on NMT models (Bahdanau et al., 2015) trained on parallel corpora, researchers have proposed several methods to extract alignments from them (Cohn et al., 2016; Zenkel et al., 2019; Garg et al., 2019; Chen et al., 2021; Zenkel et al., 2020a; Chatterjee et al., 2022). Cohn et al. (2016) and Zenkel et al. (2019) create alignments from attention matrices. Garg et al. (2019) extract discrete alignments from the attention probabilities learnt during regular neural machine translation model training and leverage them in a multi-task framework to optimize towards translation and alignment objectives. Zenkel et al. (2019; 2020a) both extend the network with an additional alignment layer. Chen et al. (2021) propose a self-supervised word alignment model that takes advantage of the full context on the target side. Chatterjee et al. (2022) propose a simple architectural modification to modern NMT systems to obtain accurate online alignments. However, most NMT-based methods require sufficient amount parallel data to train high quality NMT systems. which limits their application in the low-resource languages and in domain-specific scenarios without extra parallel data. Sabet et al. (2020) propose effective methods to extract alignments from multilingual contextualized embeddings (Devlin et al., 2019b; Conneau et al., 2020) for word alignments without explicit training on parallel data. Dou & Neubig (2021) indicate that finetuning pre-trained LMs on extra parallel corpus can improve alignment quality. There are also work on supervised neural word alignment (Stengel-Eskin et al., 2019; Nagata et al., 2020). For example, (Nagata et al., 2020) present a novel supervised word alignment method based on cross-language span prediction and formalize a word alignment problem as question answering task. However, supervised data are not always accessible, making their methods inapplicable in many scenarios.\"}"}
{"id": "Yp_dRGS-TlC", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Code-switching is a prevalent phenomenon in multilingual communities where the words, morphemes and phrases from two or more languages are switched in speech or writing. And it has been employed to improve NMT tasks (Yang et al., 2020a; Liu et al., 2021; Yang et al., 2020b) and cross lingual tasks (Qin et al., 2020; Zhang et al., 2021; Lee et al., 2021). Most of related work attribute the improvement of model performance to an intuitive assumption: Using code-switching data to train models will encourages them to align representations from source and target languages by mixing their context information. In this work, we dig deeper and give an approximate formal explanations for the connection between word alignment and masked language modeling.\\n\\nAlthough multilingual contextualized embeddings of pre-trained LMs can be employed to achieve reasonable performance even in the absence of explicit training on parallel data, there is still a clear gap: the model is trained with language modeling loss function and tested on word alignments task. Dou & Neubig (2021) leverage pre-trained LMs and fine-tune them on parallel texts with new objectives designed to improve alignment quality. However, they need extra large amounts of parallel sentences, which is at least thousands of times larger than test sentences and limit their applications on low-resource languages and settings without parallel text. So here is a practical and valuable setting: if we only have a few parallel sentences which need to be aligned and a pre-trained LM, can we further improve the performance of alignment methods? Figure 2 illustrates an overview of our paradigm. More accurate alignment results in higher-quality code-switched sentences. And finetuning on higher-quality code-switched sentences will encourage pretrained LMs to align representations from source and target languages by mixing their context information. A better pretrained LMs will obviously improve the accuracy of alignment. This iterative process will promote each other. In the following paragraphs, we will elaborate on each part.\\n\\n### 3.1 Word Alignment\\n\\nThe task of word alignment can be defined as: Given a source-language sentence \\\\( x = \\\\langle x_1, \\\\ldots, x_n \\\\rangle \\\\) of length \\\\( n \\\\) and its target-language translation \\\\( y = \\\\langle y_1, \\\\ldots, y_m \\\\rangle \\\\) of length \\\\( m \\\\), the method of word alignment needs to find a set of pairs of source and target words:\\n\\n\\\\[\\nA = \\\\{ \\\\langle x_i, y_j \\\\rangle : x_i \\\\in x, y_j \\\\in y \\\\} \\\\quad (1)\\n\\\\]\\n\\nAligned words are assumed to correspond to each other, i.e. for each word pair \\\\( \\\\langle x_i, y_j \\\\rangle \\\\), \\\\( x_i \\\\) and \\\\( y_j \\\\) are semantically similar to each other within the context of the sentence. We focus on improving the methods which can leverage multilingual pre-trained LMs (Devlin et al., 2019b; Conneau et al., 2020) for word alignments by extracting alignments from similarity matrices induced from their contextualized embeddings without relying on parallel data. For each pair of parallel sentences \\\\( x \\\\) and \\\\( y \\\\), these methods extract the hidden states of the \\\\( k \\\\)-th layer of the multilingual model:\\n\\n\\\\[\\nh_k^x = h_k^x_1, \\\\ldots, h_k^x_n \\\\quad \\\\text{and} \\\\quad h_k^y = h_k^y_1, \\\\ldots, h_k^y_m.\\n\\\\]\\n\\nGiven these contextualized word embeddings, there are many methods to obtain alignments. For example, a simple and effective method Argmax (Sabet et al., 2020) is to align \\\\( x_i \\\\) and \\\\( x_j \\\\) when \\\\( h_k^x_i \\\\) is the most similar to \\\\( h_k^y_j \\\\) and vice-versa. That is, we set\\n\\n\\\\[\\nA_{ij} = 1 \\\\quad \\\\text{if} \\\\quad i = \\\\arg \\\\max_l S_{k l, j} \\\\quad \\\\text{and} \\\\quad j = \\\\arg \\\\max_l S_{k i, l}\\n\\\\]\\n\\n\\\\[\\nS_{k i, j} = \\\\text{sim}(h_k^x_i, h_k^y_j)\\n\\\\]\\n\\nis some normalized measure of similarity, e.g., cosine-similarity. Some other methods frame alignment as an assignment problem (Sabet et al., 2020) or regularized optimal transport problem (Dou & Neubig, 2021; Chi et al., 2021) and is defined by\\n\\n\\\\[\\nA = \\\\arg \\\\max_{A \\\\in \\\\{0, 1\\\\}^{ue \\\\times uf}} \\\\sum_{i=1}^{ue} \\\\sum_{j=1}^{uf} A_{ij} S_{ij}.\\n\\\\]\\n\\nIn our setting, these methods will self-label parallel sentences. In order to get high quality aligned pairs, we filter the pairs with a particular threshold \\\\( \\\\epsilon \\\\):\\n\\n\\\\[\\nA_{\\\\text{filter}} = \\\\{ \\\\langle x_i, y_j \\\\rangle : S_{k i, j} > \\\\epsilon \\\\} \\\\quad (2)\\n\\\\]\"}"}
{"id": "Yp_dRGS-TlC", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For ablation study, we choose XLM-RoBERTa base for multilingual pretrained models and Argmax as the alignment method. We consider three kinds of ablation studies. Table 6 lists the final iterative results. \u201cNO-CS\u201d means only origin monolingual parallel sentences and there are no code-switched sentences. \u201cRandom\u201d means the pairs used for code-switching are randomly generated and they are not gold-pairs in most cases. Note that the dataset \u201cRandom\u201d also includes origin monolingual parallel sentences. \u201cNO-Filter\u201d means that we don\u2019t use a threshold to filter the pairs and all aligned pairs will be employed to augment code-switched sentences. The result of \u201cNO-CS\u201d indicates that without code-switched sentences, method can improve performance. In fact, it is almost equivalent to standard task-adaptive pretraining and Iter 0 (section 3.2). The comparison of \u201cRandom\u201d and \u201cNO-CS\u201d shows that the improvement of \u201cRandom\u201d mainly comes from origin monolingual parallel sentences. And the randomly code-switched sentences only bring a very slight boost. The comparison of \u201cNO-Filter\u201d and \u201cOurs\u201d indicates the filtering the pairs with a threshold is beneficial to model performance.\\n\\nInspired by the fact that continued pretraining of pretrained models on the unlabeled data of a given task has been show to be beneficial for task performance, we further design an iterative task-adaptive pretraining paradigm for word alignment, in which task-adaptive pretraining will be performed not only before task but also after task. The multilingual models will be continuously finetuned on the augmented code-switched dataset. The iterative process will promote each other. More accurate alignment results in higher-quality code-switched sentences. And finetuning on higher-quality code-switched sentences will encourage pretrained LMs to align representations from source and target languages by mixing their context information. A better pretrained LMs will obviously improve the accuracy of alignment. Experimental results on six language pairs and demonstrate that our paradigm can consistently improve baseline methods.\\n\\nWe are considering a more general paradigm about iterative task-adaptive pretraining and will apply the paradigm to other token-level tasks such as such as Named Entity Recognition and Parts-of-speech tagging. And how to establish a connection between the downstream tasks and self-supervised tasks of pretraining stage is key point, especially for low-resources tasks and languages. In fact, the prompt-based methods in which downstream tasks are reformulated to language modeling are alternative solutions. And we are trying to combine our iterative task-adaptive pretraining with prompt-based methods.\\n\\nREFERENCES\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.0473.\\n\\nPeter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263\u2013311, 1993. URL https://aclanthology.org/J93-2003.\\n\\nSoumya Chatterjee, Sunita Sarawagi, and Preethi Jyothi. Accurate online posterior alignments for principled lexically-constrained decoding. In Smaranda Muresan, Preslav Nakov, and Aline...\"}"}
{"id": "Yp_dRGS-TlC", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chi Chen, Maosong Sun, and Yang Liu. Mask-align: Self-supervised neural word alignment. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 4781\u20134791. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.369. URL https://doi.org/10.18653/v1/2021.acl-long.369.\\n\\nZewen Chi, Li Dong, Bo Zheng, Shaohan Huang, Xian-Ling Mao, Heyan Huang, and Furu Wei. Improving pretrained cross-lingual language models via self-labeled word alignment. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 3418\u20133430. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.265. URL https://doi.org/10.18653/v1/2021.acl-long.265.\\n\\nTrevor Cohn, Cong Duy Vu Hoang, Ekaterina Vymolova, Kaisheng Yao, Chris Dyer, and Gholamreza Haffari. Incorporating structural alignment biases into an attentional neural translation model. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 876\u2013885, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1102. URL https://aclanthology.org/N16-1102.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8440\u20138451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https://aclanthology.org/2020.acl-main.747.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171\u20134186. Association for Computational Linguistics, 2019a. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, June 2019b. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.\\n\\nZi-Yi Dou and Graham Neubig. Word alignment by fine-tuning embeddings on parallel corpora. In Paola Merlo, J\u00f6rg Tiedemann, and Reut Tsarfaty (eds.), Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pp. 2112\u20132128. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.eacl-main.181. URL https://doi.org/10.18653/v1/2021.eacl-main.181.\\n\\nChris Dyer, Victor Chahuneau, and Noah A. Smith. A simple, fast, and effective reparameterization of IBM model 2. In Lucy Vanderwende, Hal Daum\u00e9 III, and Katrin Kirchhoff (eds.), Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 9-14, 2013, Westin Peachtree Plaza Hotel, Atlanta, Georgia, USA, pp. 644\u2013648. The Association for Computational Linguistics, 2013. URL https://aclanthology.org/N13-1073/.\"}"}
{"id": "Yp_dRGS-TlC", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Yp_dRGS-TlC", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nZihan Liu, Genta Indra Winata, and Pascale Fung. Continual mixed-language pre-training for extremely low-resource neural machine translation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pp. 2706\u20132718. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-acl.239. URL https://doi.org/10.18653/v1/2021.findings-acl.239.\\n\\nDavid Marecek. Automatic alignment of tectogrammatical trees from czech-english parallel corpus. Masaaki Nagata, Katsuki Chousa, and Masaaki Nishino. A supervised word alignment method based on cross-language span prediction using multilingual BERT. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 555\u2013565. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.41. URL https://doi.org/10.18653/v1/2020.emnlp-main.41.\\n\\nKosuke Nishida, Kyosuke Nishida, and Sen Yoshida. Task-adaptive pre-training of language models with word embedding regularization. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pp. 4546\u20134553. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-acl.398. URL https://doi.org/10.18653/v1/2021.findings-acl.398.\\n\\nFranz Josef Och and Hermann Ney. Improved statistical alignment models. In 38th Annual Meeting of the Association for Computational Linguistics, Hong Kong, China, October 1-8, 2000, pp. 440\u2013447. ACL, 2000. doi: 10.3115/1075218.1075274. URL https://aclanthology.org/P00-1056/.\\n\\nFranz Josef Och and Hermann Ney. A systematic comparison of various statistical alignment models. Comput. Linguistics 29(1):19\u201351, 2003. doi: 10.1162/089120103321337421. URL https://doi.org/10.1162/089120103321337421.\\n\\nRobert \u00d6stling and J\u00f6rg Tiedemann. Efficient word alignment with markov chain monte carlo. Prague Bull. Math. Linguistics 106:125\u2013146, 2016. URL http://ufal.mff.cuni.cz/pbml/106/art-ostling-tiedemann.pdf.\\n\\nLibo Qin, Minheng Ni, Yue Zhang, and Wanxiang Che. Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual NLP. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, pp. 3853\u20133860. ijcai.org, 2020. doi: 10.24963/ijcai.2020/533. URL https://doi.org/10.24963/ijcai.2020/533.\\n\\nMasoud Jalili Sabet, Philipp Dufter, Fran\u00e7ois Yvon, and Hinrich Sch\u00fctze. Simalign: High quality word alignments without parallel training data using static and contextualized embeddings. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pp. 1627\u20131643. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.findings-emnlp.147. URL https://doi.org/10.18653/v1/2020.findings-emnlp.147.\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/v1/p16-1162. URL https://doi.org/10.18653/v1/p16-1162.\\n\\nElias Stengel-Eskin, Tzu-Ray Su, Matt Post, and Benjamin Van Durme. A discriminative neural model for cross-lingual word alignment. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 910\u2013920. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1084. URL https://doi.org/10.18653/v1/D19-1084.\\n\\nLeila Tavakoli and Heshaam Faili. Phrase alignments in parallel corpus using bootstrapping approach.\"}"}
{"id": "Yp_dRGS-TlC", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Yp_dRGS-TlC", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"APPENDIX\\n\\nA.1 A PROOF OF MLM ON CODE- SWITCHED DATASET\\n\\nProposition\\n\\nTraining model with standard masked language modeling on source-language, target-language and source-target code-switched sentences is approximately optimizing:\\n\\n\\\\[\\n\\\\begin{align*}\\n    c_{src} & \\\\sim e_{x_i} \\\\\\\\\\n    x_i & \\\\sim c_{tgt} \\\\\\\\\\n    y_i & \\\\sim e_{y_i}\\n\\\\end{align*}\\n\\\\]\\n\\nwhere \\\\( c_{src} \\\\) represents the contextualized embedding of token \\\\( x_i \\\\) in source sentence and \\\\( c_{src} y_i \\\\) represents the contextualized embedding of token \\\\( y_i \\\\) in target sentence. And \\\\( e_{x_i} \\\\) and \\\\( e_{y_i} \\\\) are word embeddings of token \\\\( x_i \\\\) and \\\\( y_i \\\\) in vocabulary.\\n\\nNote\\n\\nUnder the existing conditions, we can not derive a strict bound but an approximate conclusion.\\n\\nGiven \\\\( a \\\\in \\\\mathbb{R}^n \\\\), \\\\( b \\\\in \\\\mathbb{R}^n \\\\), \\\\( c \\\\in \\\\mathbb{R}^n \\\\) we assume \\\\( a \\\\sim b \\\\), if the projection components of \\\\( a \\\\) and \\\\( b \\\\) onto another vector \\\\( c \\\\) are the same:\\n\\n\\\\[\\n    a \\\\cdot c = b \\\\cdot c\\n\\\\]\\n\\nWe think this is approximately reasonable for word alignment task, because for word alignments method two words are aligned as long as their similarity is higher than other words in two parallel sentences and doesn't need to exceed a fixed number. And in section 3.3, we give a corollary. Subsequent experiments prove that our assumption is reasonable.\\n\\nProof\\n\\nWe denote the embeddings of the corresponding original tokens as \\\\( e_1, e_2, \\\\ldots, e_L \\\\). The MLM objective \\\\( L_{MLM}(x) \\\\) can be formulated as:\\n\\n\\\\[\\n    -\\\\frac{1}{|M|} \\\\sum_{i \\\\in M} \\\\log \\\\exp (m_i \\\\cdot e_i)\\n\\\\]\\n\\n\\\\[\\n    \\\\sum_{k=1}^{|\\n    V|} \\\\exp (m_i \\\\cdot e_k)\\n\\\\]\\n\\n\\\\[\\n    = -\\\\frac{1}{|M|} \\\\sum_{i \\\\in M} \\\\log \\\\sum_{k=1}^{|\\n    V|} \\\\exp (m_i \\\\cdot e_k - m_i \\\\cdot e_i)\\n\\\\]\\n\\n(6)\\n\\nwhere \\\\( M \\\\) denotes the set of masked tokens and \\\\( |V| \\\\) is the size of vocabulary \\\\( V \\\\).\\n\\n\\\\( m_i \\\\) is hidden state of the last layer at the masked position, and can be regarded as a fusion of contextualized representations of surrounding tokens. Given two sentences: one source-language sentence \\\\( x = \\\\langle x_1, \\\\ldots, x_{i-1}, x_i, x_{i+1}, \\\\ldots, x_n \\\\rangle \\\\) of length \\\\( n \\\\) and its code-switched sentence \\\\( x' = \\\\langle x_1, \\\\ldots, x_{i-1}, y_i, x_{i+1}, \\\\ldots, x_n \\\\rangle \\\\), where \\\\( \\\\langle x_i, y_i \\\\rangle \\\\) is aligned pair. If we only mask \\\\( x_i \\\\) in the \\\\( x \\\\) and \\\\( y_i \\\\) in the \\\\( x' \\\\), then \\\\( x\\\\text{mask} = \\\\langle x_1, \\\\ldots, x_{i-1}, < \\\\text{mask} >, x_{i+1}, \\\\ldots, x_n \\\\rangle = \\\\langle x_1, \\\\ldots, x_{i-1}, < \\\\text{mask} >, x_{i+1}, \\\\ldots, x_n \\\\rangle = x'\\\\text{mask} \\\\), the loss function can be written as\\n\\n\\\\[\\n    L_{MLM} = L_x + L_{x'}\\n\\\\]\\n\\n\\\\[\\n    = -\\\\frac{1}{2} \\\\log \\\\sum_{k=1}^{|\\n    V|} \\\\exp (m_i \\\\cdot e_k - m_i \\\\cdot e_x) + \\\\log \\\\sum_{k=1}^{|\\n    V|} \\\\exp (m_i \\\\cdot e_k - m_i \\\\cdot e_y)\\n\\\\]\\n\\n(7)\\n\\nThis inequality below is easily proved.\\n\\n\\\\[\\n    \\\\max \\\\{x_1, \\\\ldots, x_n\\\\} \\\\leq \\\\log \\\\sum_{i=0}^{n} e_{x_i} \\\\leq \\\\max \\\\{x_1, \\\\ldots, x_n\\\\} + \\\\log n\\n\\\\]\\n\\n(8)\\n\\nSo for \\\\( L_x = -\\\\log \\\\sum_{k=1}^{|\\n    V|} \\\\exp (m_i \\\\cdot e_k - m_i \\\\cdot e_x) \\\\)\\n\\n\\\\[\\n    \\\\max \\\\left\\\\{ \\\\begin{array}{c}\\n    m_i \\\\cdot e_0 - m_i \\\\cdot e_{x_i} \\\\\\\\\\n    \\\\ldots \\\\\\\\\\n    m_i \\\\cdot e_{x_i-1} - m_i \\\\cdot e_{x_i} - 1 \\\\\\\\\\n    m_i \\\\cdot e_{x_i} - m_i \\\\cdot e_{x_i} \\\\\\\\\\n    \\\\ldots \\\\\\\\\\n    m_i \\\\cdot e_{x_i} + 1 - m_i \\\\cdot e_{x_i} \\\\\\\\\\n    \\\\ldots \\\\\\\\\\n    m_i \\\\cdot e_{|V|} - m_i \\\\cdot e_{x_i} \\\\end{array} \\\\right\\\\} \\\\leq \\\\log \\\\sum_{k=1}^{|\\n    V|} \\\\exp (m_i \\\\cdot e_k - m_i \\\\cdot e_{x_i})\\n\\\\]\\n\\n(9)\\n\\n\\\\[\\n    \\\\max \\\\left\\\\{ \\\\begin{array}{c}\\n    m_i \\\\cdot e_0 - m_i \\\\cdot e_{x_i} \\\\\\\\\\n    \\\\ldots \\\\\\\\\\n    m_i \\\\cdot e_{x_i-1} - m_i \\\\cdot e_{x_i} - 1 \\\\\\\\\\n    m_i \\\\cdot e_{x_i} - m_i \\\\cdot e_{x_i} \\\\\\\\\\n    \\\\ldots \\\\\\\\\\n    m_i \\\\cdot e_{x_i} + 1 - m_i \\\\cdot e_{x_i} \\\\\\\\\\n    \\\\ldots \\\\\\\\\\n    m_i \\\\cdot e_{|V|} - m_i \\\\cdot e_{x_i} \\\\end{array} \\\\right\\\\} \\\\leq \\\\log \\\\sum_{k=1}^{|\\n    V|} \\\\exp (m_i \\\\cdot e_k - m_i \\\\cdot e_{x_i}) + \\\\log n\\n\\\\]\\n\\n(10)\\n\\nIn Ineq. 10, \\\\( 0 \\\\) is fixed value. So when training model with this loss function, model is optimized to\\n\\n\\\\[\\n    m_i \\\\cdot e_k - m_i \\\\cdot e_{x_i} \\\\leq 0, \\\\quad \\\\forall k \\\\in |V|\\n\\\\]\\n\\nIn other words,\\n\\n\\\\[\\n    m_i \\\\cdot e_k \\\\leq m_i \\\\cdot e_{x_i}, \\\\quad \\\\forall k \\\\in |V|\\n\\\\]\\n\\nWhen \\\\( k = y_i \\\\),\"}"}
{"id": "Yp_dRGS-TlC", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we have \\\\( m \\\\cdot e_y \\\\leq m \\\\cdot e_x \\\\). Similarly, for \\\\( L_x' \\\\), we have \\\\( m \\\\cdot e_x \\\\leq m \\\\cdot e_y \\\\). So when training a model with loss function \\\\( L_{MLM} = L_x + L_x' \\\\), model will be optimized to learn \\\\( m \\\\cdot e_x = m \\\\cdot e_y \\\\). This equation can't ensure \\\\( e_x = e_y \\\\) but \\\\( e_x \\\\sim e_y \\\\) to some extent. For standard masked language modeling, there is a probability that the original token will not be masked and we use \\\\( c_{src}x_i \\\\) to represent the hidden state of the last layer, which is the contextualized embedding of token \\\\( x_i \\\\). So we have \\\\( c_{src}x_i \\\\cdot e_k \\\\leq c_{src}x_i \\\\cdot e_x, \\\\forall k \\\\in |V| \\\\). Obviously, \\\\( e_x \\\\sim c_{src}x_i \\\\). Similarly, if we consider target language sentence \\\\( y = \\\\langle y_1, \\\\ldots, y_{i-1}, y_i, y_{i+1}, \\\\ldots, y_n \\\\rangle \\\\), we have \\\\( e_y \\\\sim c_{tgt}y_i \\\\). So training model with masked language modeling on source-language, target-language and source-target code-switched sentences is approximately optimizing:\\n\\n\\\\[\\n\\\\begin{align*}\\n&c_{src}x_i \\\\sim e_x \\\\sim e_y \\\\sim c_{tgt}y_i\\n\\\\end{align*}\\n\\\\]\"}"}
