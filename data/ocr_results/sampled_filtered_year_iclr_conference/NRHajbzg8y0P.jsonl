{"id": "NRHajbzg8y0P", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "NRHajbzg8y0P", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "NRHajbzg8y0P", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "NRHajbzg8y0P", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The proposed work still has some limitations. We try to simulate the real-world multimodal analogy reasoning setting; however, it still cannot predict analogical entities that are not existing in the multimodal knowledge graph. Such an issue is also known as inductive knowledge graph completion.\"}"}
{"id": "NRHajbzg8y0P", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Step 1: Collect Analogy Entities and Relations.\\n\\nSince E-KAR and BATs are widely used text analogy datasets with high-quality and semantically specific entities, we collect the analogy seed entities $E_a$ and relations from them according to the following criteria: (1) The relations and entities that have the same meanings will be merged. For example, we merge the relation is a of E-KAR and the relation Hypernyms of BATs since they both represent the hypernym relationship of entities. We obtain 38 relations after this step. (2) The relation must imply analogical knowledge reasoning rather than simple word linear analogy. For example, we discard the analogy relations that only reflect simple word changes of BATs dataset such as Inflections (Nouns, Verbs, etc.) and Derivation (Stem change, etc.). After this step, we filter 11 relations and retain 27 analogy relations. (3) The entity must be visualizable and realistic. We filter those entities that cannot be linked into Wikidata and drop out the extremely abstract entities such as virtue by hand (some entities that have no image after Step 3 are also filtered). We discard a total of 463 entities after filtering. Finally, we obtained 2,063 seed entities and 27 relations.\\n\\nStep 2: Link to Wikidata and Retrieve Neighbors.\\n\\nConsider that complex analogical reasoning is difficult through individual information (descriptions or images) of entities. We link the analogy seed entities to Wikidata by Mediawiki API and retrieve the one-hop neighbors of seed entities as well as the possible relationships between the seed entities to obtain their neighbor structure information. In this step, we also take the retrieved descriptions from Wikidata as the textual information of entities and relations.\\n\\nStep 3: Acquire and Validate Images.\\n\\nWe collect images from two sources: Google Engine and Laion-5B query service. We search from Google Engine with the descriptions of entities and crawl 5 images per entity. Laion-5B service depends on Clip retrieval and query by $k$nn index; we leverage the clip text embedding of the description and also query 5 images for each entity. Then we apply four filters to the above images: (1) we check the format of the images and filter invalid files, (2) we remove corrupted (the images are damaged and cannot be opened), low-quality (image size less than $50 \\\\times 50$ or non-panchromatic images) and duplicate images, (3) we use CLIP (Radford et al., 2021) to remove the images with outlier visual embeddings, (4) we delete unreasonable images manually.\\n\\nStep 4: Sample Analogical Reasoning Data.\\n\\nFrom Step 1 to Step 3, we obtain the MarKG, which includes 2,063 analogy entities, 8,881 neighbor entities, 27 analogy relations and 165 other relations. To construct the MARS dataset, we sample analogy example $(e_h, r, e_t)$ and analogy question-answer pair $(e_q, r, e_a)$ with the same relation $r$ from 2,063 analogy entities, but we do not explicitly provide\\n\\n5 https://www.wikidata.org/w/api.php\\n6 https://knn5.laion.ai/\"}"}
{"id": "NRHajbzg8y0P", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"then we split the data into different task settings evenly. More details about the sample strategy of MARS can be seen in Section B.2.\\n\\nIn Section B.1, we obtain the analogy seed entities $E_a$ and the analogy relations between $E_a$. Then we sample analogy example $(e_h, e_t)$ and analogy question-answer pair $(e_q, e_a)$ from $E_a$. Guided by SMT, we make sure that $(e_h, e_t)$ and $(e_q, e_a)$ have the same relation $r$. Specifically, we divide the entity pairs that share the same relation into two categories to avoid overlap issues. Then we randomly sample the analogy examples from one category and the analogy question-answer pairs from another to construct analogy input instances. Last, we split the instances into different task settings evenly.\\n\\n| Dataset  | # entity | # relation | # triple | # image | Source  |\\n|----------|----------|------------|---------|---------|---------|\\n| WN9-IMG  | 6,555    | 9          | 14,319  | 65,550  | WordNet |\\n| FB15k-IMG| 11,757   | 1,231      | 350,293 | 107,570 | Freebase |\\n| MarKG    | 11,292   | 192        | 34,420  | 76,424  | Wikidata |\\n\\nTable 5: Data statistics of MarKG. # refers to the number of.\\n\\nThe statistical comparison of MarKG with two multimodal knowledge graph datasets WN9-IMG (Xie et al., 2017) and FB15k-IMG (Liu et al., 2019) as shown in Table 5, we report the number of entity, relation, triple, image and the data source. Note that WN9-IMG and FB15k-IMG aim for knowledge completion and triple classification tasks while our MarKG aims to support MARS to do multimodal analogical reasoning. We also show the complete relations of our MARS in Table 6 and the distribution of relation categories in Figure 7.\\n\\n| Relations | Definition | Example |\\n|-----------|------------|---------|\\n| part of   | Object of which the subject is a part. | mouse : computer |\\n| corresponds to | Terms generally correspond to each other. | entrepreneur : laborer |\\n| juxtaposition to | Two terms belong to the same hypernym or have the same properties or functions. | child : minor |\\n| synonym | Sense of another lexeme with the same meaning as this sense. | tired : exhausted |\\n| made from material | Material the subject or the object is made of or derived from. | building : cement |\\n| antonym | Sense of a lexeme with the opposite meaning to this sense. | warm : cool |\\n| has cause | Underlying cause, thing that ultimately resulted in this effect. | cleaning : tidy |\\n| opposite of | Item that is the opposite of this item. | black : white |\\n| follow | The terms have a chronological or other sequential relationship, but one term does not cause the other. | implement : evaluate |\\n| intersection to | The extension of the two terms intersects. | odd : integer |\\n| takes place in | A term takes place in the other. | doctor : hospital |\\n| prerequisite | Prior event or achievement that a person or team needs to complete before joining or obtaining the item topic. | aim : shoot |\\n| subject-object | The originator and receiver of an action. | school : education |\\n| contradictory to | Two term are contradictory to each other. | english : chinese |\\n| identical to | The meanings of two terms are identical. | highway : road |\\n| head-modifier | The preceding term modifies the other. | affluence : living |\\n| different from | Item that is different from another item, with which it may be confused. | apple : nuts |\\n| probabilistic attribute | One term is probably the attribute of the other. | liquid : fluidity |\\n| instance of | That class of which this subject is a particular example and member. | coffee : drink |\\n| has use | Main use of the subject. | ballot : election |\\n| location | Location of the object, structure or event. | student : classroom |\\n| verb-object | The action and the object on which the action acts. | drilling : petroleum |\\n| has quality | The entity has an inherent or distinguishing non-material characteristic. | knife : sharp |\\n| tool of | One term is the tool of the other. | piano : play |\\n| subject-predicate | The originator of the action and the action itself. | stone : throwing |\\n| target of | One term is the target of the other. | harvest : sow |\\n| metaphor | A term is the metaphor of the other, reflecting something abstract indirectly. | pigeon : peace |\\n\\nTable 6: The complete relations with definitions, examples of MARS. Some relations and definitions refer to (Chen et al., 2022a) and Wikidata Properties.\"}"}
{"id": "NRHajbzg8y0P", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Quality Control of Datasets. We devise some quality control strategies while constructing our MarKG and MARS datasets: (1) Entity and relation formalization and normalization. We link the analogy entities collected from E-KAR and SAT to Wikidata and filter non-link items. Since Wikidata is a knowledge base with quality-assured, some rare or worthless entities are excluded. (2) Image validation mechanism. We devise complex image filter strategies to control the robustness of image data, as mentioned in Section B.1. (3) Control of text description. We take the description in Wikidata as the textual information of entities.\\n\\n| Method       | Hit@1 | Hit@3 | Hit@5 | Accuracy |\\n|--------------|-------|-------|-------|----------|\\n| TransAE      | 0.15  | 0.37  | 0.50  | -        |\\n| MarT         | 0.15  | 0.28  | 0.53  | -        |\\n| VisualBERT   | 0.16  | 0.36  | 0.59  | -        |\\n| Human        | -     | -     | -     | 0.64     |\\n\\nTable 7: Human evaluation on MARS.\\n\\nHuman evaluation on MARS. To evaluate the complexity and difficulty of the multimodal analogical reasoning task, we build a human evaluation in this section. However, humans encounter the following problems in this entity prediction task: (1) The candidate entity set is too huge for humans to select one entity. (2) Hit@k metric is not available since human hard rank predictions. Therefore, we utilize the multiple-choice format for human beings and apply the Accuracy metric to evaluate. Specifically, we randomly sample 100 instances from the test set to construct the evaluation set, and we use the top 10 ranking entities in TransAE prediction as candidate choices for each instance. If the golden answer entity is not in the top 10 entities, we will randomly replace one candidate with the golden entity. Then humans must select one entity from the candidate choices as the answer entity. The results can be seen in Table 7. We limit the prediction space of baseline models in candidate choices for a fair comparison. We find that the performance of the baselines in the Hit@1 metric has a large gap with human, which indicates the difficulty of the multimodal analogical reasoning task.\\n\\nB.4 Detailed Evaluation Metrics\\n\\nThe evaluation method of (Chen et al., 2022a) cannot reflect one-to-more entities and does not fully explore the internal knowledge in the models due to the limited search space. Thus, we follow the link prediction task and choose Hits@k and MRR as our evaluation metrics. Both metrics are in the range $[0, 1]$. The bigger, the better performance. The Hits at k metric (Hits@k) is acquired by counting the number of times the golden entity appears at the first k positions in the predictions. Given the prediction score of each entity in the candidate entity set, we sort the score and obtain the ranking of each entity. Denote the rank of the gold entity of $i$ triple as $\\\\text{rank}_i$, and the reciprocal rank is $1/\\\\text{rank}_i$. The Mean Reciprocal Rank (MRR) is the average of the reciprocal ranks across all triples in the knowledge graph:\\n\\n$$\\\\text{MRR} = \\\\frac{1}{|S|} \\\\sum_{i=1}^{|S|} \\\\frac{1}{\\\\text{rank}_i}$$\\n\\nwhere $|S|$ is the total number of the training set.\\n\\nC.1 Additional Experiments\\n\\nThis section details the training procedures and hyper-parameters for various models. For multimodal knowledge representation methods, we first use MarKG to do knowledge representation learning and obtain the entity and relation matrix embeddings. Then we apply abduction and induction processes to continue training the models on the MARS dataset. Note that these processes are serial and share models. For multimodal pre-trained Transformer models, we also use MarKG to pre-train the models and then fine-tune on MARS end-to-end with our analogy prompt tuning strategy. We utilize Pytorch to conduct all experiments with 1 Nvidia 3090 GPU. The details of hyper-parameters can be seen in Table 8.\"}"}
{"id": "NRHajbzg8y0P", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Hyper-parameter settings. We use the same parameter settings of MKGE baseline methods as the original paper except for the learning rate.\\n\\nC.2 RESULTS OF PRE-TRAINING ON MARKG.\\n\\n| Method     | Baselines | Entity Prediction | Relation Prediction |\\n|------------|-----------|-------------------|---------------------|\\n|            |           | Hits@1  Hits@3  Hits@10 | MRR  | Hits@1  Hits@3  Hits@10 | MRR  |\\n| MKGE       | IKRL      | 0.157  0.257  0.338  0.272 | -  -  -  - |\\n|            | TransAE   | 0.307  0.361  0.442  0.353 | -  -  -  - |\\n|            | RSME      | 0.417  0.460  0.520  0.452 | -  -  -  - |\\n| MPT        | MarT      | 0.466  0.598  0.692  0.546  | 0.758  0.873  0.927  0.822 |\\n|            | VisualBERT| 0.466  0.586  0.675  0.539  | 0.737  0.847  0.902  0.799 |\\n|            | MarT      | 0.489  0.621  0.711  0.569  | 0.764  0.876  0.930  |\\n|            | ViLBERT*  | 0.506  0.634  0.716  0.582  | 0.771  0.877  |\\n|            | FLAVA*    | 0.527  0.670  0.779  0.616  | 0.762  0.870  0.923  0.823 |\\n\\nTable 9: Pre-training results on MarKG. Note that these results are from the training process as we do not divide MarKG. Since we follow the link prediction task to pre-train the models for MKGE baselines, we only report the entity prediction results.\\n\\nFigure 8: The results of pre-training on MarKG and fine-tuning on MARS. * refers to the baseline model applied MarT.\\n\\nWe report the pre-train results on MarKG in Table 9. We find that MPT baselines perform better than MKGE baselines consistently. It reveals the strong fit ability of Transformer-based models. As shown in Figure 8, we can observe that pre-training and fine-tuning stages trends are roughly the same, especially in the same type of baselines, which indicates that pre-train on MarKG benefits analogical reasoning on MARS.\\n\\nC.3 RESULTS OF IMPLICIT RELATION INFERENCE OF MPT.\\n\\nWe conduct an evaluation experiment on the relation inference of MKGE and MPT methods. For MKGE methods, we evaluate the relation predicted of Abduction process with hit@k metrics. Since MPT methods solve the analogical reasoning task end-to-end without any explicit relation prediction process, we use two ways to evaluate their relation-aware abilities. The first one is that we predict the relation via the special relation token [R], which is similar to mask entities prediction and evaluate the predictions with Hit@k metrics. However, this evaluation method does not precisely reflect the relation-aware abilities of models since [R] is an abstract virtual token that may aggregate multiple\"}"}
{"id": "NRHajbzg8y0P", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Relation evaluation of MPT baselines.\\n\\n| Method     | Hits@3 | Hits@5 | Hits@10 | MRR  |\\n|------------|--------|--------|---------|------|\\n| MKGE       | 0.160  | 0.234  | 0.405   | -    |\\n| TransAE    | 0.179  | 0.242  | 0.491   | -    |\\n| VisualBERT | 0.107  | 0.181  | 0.340   | 1.418|\\n| ViLT       | 0.126  | 0.181  | 0.332   | 1.419|\\n| ViLBERT    | 0.078  | 0.189  | 0.333   | 1.412|\\n| FLAVIA     | 0.078  | 0.587  | 0.709   | 1.380|\\n| MKGformer  | 0.049  | 0.209  | 0.512   | 1.405|\\n\\nThe evaluation results are shown in Table 6, we find that MKGE methods perform better than most MPT methods on Hit@k metrics, especially on Hit@3 metric, which may benefit from the explicit relation perception in the pipeline process. Moreover, MarT FLAVIA achieves the best relation-aware performance on Hit@k and Euclidean distance metrics, but MarT MKGformer performs worse than MarT MKGformer in answer entity prediction as shown in Table 2. We speculate that the special token \\\\([R]\\\\) contains not only the golden relation but also other related relation information.\\n\\nIn this section, we detail the size of MPT baseline models and compare them with their performance. In MPT models, the single-stream models (VisualBERT, ViLT) are the smallest, the dual-stream models (ViLBERT) are the middle and the mixed-stream models (FLAVIA, MKGformer) are the biggest. The performance of models is roughly proportional to their sizes, as shown in Figure 9. MKGformer outperforms all other models, including the biggest FLAVIA model.\\n\\nD.4 ERROR CASE ANALYSIS\\n\\nIn this section, we conduct an error case study on MARS in Figure 10. From the error cases, we can see the hardship of the multimodal analogical reasoning task: 1) Imbalance of multimodal. The semantic scales of images and text are inconsistent, which leads to incorrect matching (Zhu et al., 2022). Although we filter some hard-to-visualize entities in data collection in Section B.1,\"}"}
{"id": "NRHajbzg8y0P", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nwatermelon fruit instance of Memba snake instance of laborer? farmer control management correspond to MKGformer MKGformer* TransAE TransAE*\\n\\ncorrespond to MKGformer MKGformer* TransAE TransAE*\\n\\nrural body officer rural people peasant rural insect seaport rural keep alphabet MKGformer MKGformer* TransAE TransAE*\\n\\nAnalogical Example Question-Answer Pair Task Setting\\n\\nFigure 10: Error case examples.\\n\\nthe high semantic entities exist. As shown in example (a), \\\"management\\\" and \\\"control\\\" are abstract entities that are difficult to find equivalent images. Moreover, the uncoordinated convergence problem in multimodal learning further exacerbates the difficulty of the multimodal analogical reasoning task (Peng et al., 2022; Wang et al., 2020).\\n\\n2) One-to-more problem. It is challenging for the models to solve one-to-more entities. In example (b), \\\"Memba\\\" is an instance of both \\\"snake\\\" and \\\"animal\\\", which is confusing to MKGformer.\\n\\n3) Unintuitive relations. In our MARS dataset, some relations are not intuitive, requiring models to have strong relation reasoning ability. As shown in example (c), the relation \\\"intersection to\\\" means the extension of the head and tail entity intersects. All four models are struggling and far away from the golden answer entity.\"}"}
{"id": "NRHajbzg8y0P", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Analogical reasoning is fundamental to human cognition and holds an important place in various fields. However, previous studies mainly focus on single-modal analogical reasoning and ignore taking advantage of structure knowledge. Notably, the research in cognitive psychology has demonstrated that information from multimodal sources always brings more powerful cognitive transfer than single modality sources. To this end, we introduce the new task of multimodal analogical reasoning over knowledge graphs, which requires multimodal reasoning ability with the help of background knowledge. Specifically, we construct a Multimodal Analogical Reasoning data Set (MARS) and a multimodal knowledge graph \\\\( \\\\text{MarKG} \\\\). We evaluate with multimodal knowledge graph embedding and pre-trained Transformer baselines, illustrating the potential challenges of the proposed task. We further propose a novel model-agnostic Multimodal analogical reasoning framework with Transformer (MarT) motivated by the structure mapping theory, which can obtain better performance. We hope our work can deliver benefits and inspire future research.\\n\\n### Introduction\\n\\nAnalogical reasoning \u2013 the ability to perceive and use relational similarity between two situations or events \u2013 holds an important place in human cognition (Johnson-Laird, 2006; Wu et al., 2020; Bengio et al., 2021; Chen et al., 2022a) and can provide back-end support for various fields such as education (Thagard, 1992), creativity (Goel, 1997), thus appealing to the AI community. Early, Mikolov et al. (2013b); Gladkova et al. (2016a); Ethayarajh et al. (2019a) propose visual analogical reasoning aiming at lifting machine intelligence in Computer Vision (CV) by associating vision with relational, structural, and analogical reasoning. Meanwhile, researchers of Natural Language Processing (NLP) hold the connectionist assumption (Gentner, 1983) of linear analogy (Ethayarajh et al., 2019b); for example, the relation between two words can be inferred through vector arithmetic of word embeddings. However, it is still an open question whether artificial neural networks are also capable of recognizing analogies among different modalities.\\n\\nNote that humans can quickly acquire new abilities based on finding a common relational system between two exemplars, situations, or domains. Based on Mayer's Cognitive Theory of multimedia learning (Hegarty & Just, 1993; Mayer, 2002), human learners often perform better on tests with analogy when they have learned from multimodal sources than single-modal sources. Evolving from recognizing single-modal analogies to exploring multimodal reasoning for neural models, we emphasize the importance of a new kind of analogical reasoning task with Knowledge Graphs (KGs). In this paper, we introduce the task of multimodal analogical reasoning over knowledge graphs to fill this blank. Unlike the previous multiple-choice QA setting, we directly predict the analogical target and formulate the task as link prediction without explicitly providing relations. Specifically, the task can be formalized as \\\\((e_h, e_t) : (e_q, ?)\\\\) with the help of background multimodal knowledge graph.\\n\\n\\\\[ \\\\text{https://github.com/zjunlp/MKG_Analogy} \\\\]\"}"}
{"id": "NRHajbzg8y0P", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"G, in which $e$ or $h$ or $q$ have different modalities. We collect a Multimodal Analogical Reasoning data Set (MARS) and a multimodal knowledge graph MarKG to support this task. These data are collected and annotated from seed entities and relations in E-KAR (Chen et al., 2022a) and BATs (Gladkova et al., 2016a), with linked external entities in Wikidata and images from Laion-5B (Schuhmann et al., 2021).\\n\\nTo evaluate the multimodal analogical reasoning process, we follow the guidelines from psychological theories and conduct comprehensive experiments on MARS with multimodal knowledge graph embedding baselines and multimodal pre-trained Transformer baselines. We further propose a novel Multimodal Analogical Reasoning framework with Transformer, namely MarT, which is readily pluggable into any multimodal pre-trained Transformer models and can yield better performance.\\n\\nTo summarize, our contributions are three-fold: (1) We advance the traditional setting of analogy learning by introducing a new multimodal analogical reasoning task. Our work may open up new avenues for improving analogical reasoning through multimodal resources. (2) We collect and build a dataset MARS with a multimodal knowledge graph MarKG, which can be served as a scaffold for investigating the multimodal analogy reasoning ability of neural networks. (3) We report the performance of various multimodal knowledge graph embedding, multimodal pre-trained Transformer baselines, and our proposed framework MarT. We further discuss the potential of this task and hope it facilitates future research on zero-shot learning and domain generalization in both CV and NLP.\\n\\n2 BACKGROUND\\n\\n2.1 ANALOGICAL REASONING IN PSYCHOLOGICAL TO better understand analogical reasoning, we introduce some crucial theories from cognitive psychology, which we take as guidelines for designing the multimodal analogical reasoning task.\\n\\nStructure Mapping Theory (SMT) (Gentner, 1983). SMT is a theory that takes a fundamental position in analogical reasoning. Specifically, SMT emphasizes that humans conduct analogical reasoning depending on the shared relations structure rather than the superficial attributes of domains and distinguishes analogical reasoning with literal similarity. Minnameier (2010) further develops the inferential process of analogy into three steps: abduction, mapping and induction, which inspires us to design benchmark baselines for multimodal analogical reasoning.\\n\\nMayer's Cognitive Theory (Hegarty & Just, 1993; Mayer, 2002). Humans live in a multi-source heterogeneous world and spontaneously engage in analogical reasoning to make sense of unfamiliar situations in everyday life (Vamvakoussi, 2019). Mayer's Cognitive Theory shows that human learners often perform better on tests of recall and transfer when they have learned from multimodal sources than single-modal sources. However, relatively little attention has been paid to multimodal analogical reasoning, and it is still unknown whether neural network models have the ability of multimodal analogical reasoning.\\n\\n2.2 ANALOGICAL REASONING IN CV AND NLP\\n\\nVisual Analogical Reasoning. Analogical reasoning in CV aims at lifting machine intelligence by associating vision with relational, structural, and analogical reasoning (Johnson et al., 2017; Prade & Richard, 2021; Hu et al., 2021; Malkinski & Mandziuk, 2022). Some datasets built in the context of Raven's Progressive Matrices (RPM) are constructed, including PGM (Santoro et al., 2018) and RA VEN (Zhang et al., 2019). Meanwhile, Hill et al. (2019) demonstrates that incorporating structural differences with structure mapping in analogical visual reasoning benefits the machine learning models. Hayes & Kanan (2021) investigates online continual analogical reasoning and demonstrates the importance of the selective replay strategy. However, these aforementioned works still focus on analogy reasoning among visual objects while ignoring the role of complex texts.\\n\\nNatural Language Analogical Reasoning. In the NLP area, early attempts devote to word analogy recognition (Mikolov et al., 2013b; Gladkova et al., 2016a; Jurgens et al., 2012; Ethayarajh et al., 2019a; Gladkova et al., 2016b) which can often be effectively solved by vector arithmetic for neural word embeddings Word2Vec (Mikolov et al., 2013a) and Glove (Pennington et al., 2014). Recent studies have also evaluated on the pre-trained language models (Devlin et al., 2019; Brown et al., 2020).\"}"}
{"id": "NRHajbzg8y0P", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3 MUltimodal Analogical Reasoning\\n\\n3.1 TASK DEFINITION\\nIn this section, we introduce the task of Multimodal Analogical Reasoning that can be formulated as link prediction without explicitly providing relations. As shown in Figure 1, given an analogy example \\\\((e_h, e_t)\\\\) and a question-answer entity pair \\\\((e_q, ?)\\\\) where \\\\(e_h, e_t, e_q \\\\in E\\\\) and \\\\(E_a \\\\in E\\\\), the goal of analogical reasoning is to predict the missing entity \\\\(e_a \\\\in E_a\\\\). Moreover, multimodal analogical reasoning is based on background multimodal knowledge graph \\\\(G = (E, R, I, T)\\\\), where \\\\(E\\\\) and \\\\(R\\\\) are sets of entities and relations, \\\\(I\\\\) and \\\\(T\\\\) represent images and textual descriptions of entities. Note that the relations of \\\\((e_h, e_t)\\\\) and \\\\((e_q, e_a)\\\\) are identical but unavailable, and the relation structure can be analogized implicitly from source domain to target domain without knowing the relations. Specifically, the task can be formalized as \\\\((e_h, e_t) : (e_q, ?)\\\\), further divided into Single Analogical Reasoning and Blended Analogical Reasoning according to different modalities of \\\\(e_h, e_t, e_q\\\\) and \\\\(e_a\\\\).\\n\\nSingle Analogical Reasoning. In this setting, the analogy example and the question-answer entity pair involve only one modality. As shown in the middle column of Figure 1, the modalities of the analogy example \\\\((e_h, e_t)\\\\) are identical and opposite to the analogy question-answer pair \\\\((e_q, e_a)\\\\).\\n\\nBased on both visual and textual modalities, this setting can be further divided into \\\\((I_h, I_t) : (T_q, ?)\\\\) and \\\\((T_h, T_t) : (I_q, ?)\\\\) where \\\\(I_h, T_h\\\\) represent the modality of \\\\(e_h\\\\) is visual or textual respectively.\\n\\nBlended Analogical Reasoning. In the setting, the modality of analogy example \\\\((e_h, e_t)\\\\) are unidentical, which is similar to real-world human cognition and perception. Note that Mayer's theory indicates that humans can have powerful transfer and knowledge recall abilities in multimodal scenarios. Inspired by this, we propose the blended analogical reasoning that can be formalized as \\\\((I_h, T_t) : (I_q, ?)\\\\), which means the modalities between \\\\(e_h(e_q)\\\\) and \\\\(e_t(e_a)\\\\) are different.\\n\\n3.2 DATASET COLLECTION AND PREPROCESSING\\nWe briefly introduce the construction process of the dataset in Figure 2. Firstly, we collect a multimodal knowledge graph dataset MarKG and a multimodal analogical reasoning dataset MARS.\"}"}
{"id": "NRHajbzg8y0P", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Step 1: Collect Analogical Entities and Relations\\n\\nStep 2: Link to Wikidata and Retrieve Neighbors\\n\\nStep 3: Acquire and Validate Images\\n\\nStep 4: Sample MarKG\\n\\nFigure 2: An illustration of data collection and processing steps to create MARS and MarKG.\\n\\n| Dataset Size | Modality | # Entity | # Relation | # Images |\\n|--------------|----------|----------|------------|----------|\\n| RA VEN       | Vision   | 1,120,000| \u2714          |          |\\n| SAT          | Text     | 50       | \u2714          |          |\\n| Linear Word Analogy | Google | 500     | \u2714          |          |\\n| E-KAR        | Text     | 2,032    | \u2714          | 2,063    |\\n| BATs         | Text     | 6,218    | \u2714          | 13,398   |\\n| MARS         | Vision+Text | 2,063    | 27        | 13,398   |\\n\\nTable 1: Comparison between MARS and previous analogical reasoning datasets. \u201cKB\u201d refers to the knowledge base, # denotes the number. \u201cKnowledge Intensive\u201d means reasoning requires external knowledge. Our MarKG focuses on knowledge-intensive reasoning across multiple modalities.\\n\\nwhich are developed from seed entities and relations in E-KAR (Chen et al., 2022a) and BATs (Gladkova et al., 2016a). Secondly, we link these seed entities into the free and open knowledge base Wikidata for formalization and normalization. Thirdly, to acquire the image data, we further search from the Google engine and query from the multimodal data Laion-5B (Schuhmann et al., 2021) by the text descriptions of entities. Then, an image validation strategy is applied to filter low-quality images. Lastly, we sample high-quality analogy data to construct MARS. A detailed description of the data collection and processing to create our datasets are in Appendix B.1 and B.2.\\n\\n3.3 DATASET STATISTICS\\n\\nMARS is the evaluation dataset of the multimodal analogical reasoning task that contains analogy instances, while MarKG can provide the relative structure information of those analogy entities retrieved from Wikidata. The statistics of MARS and MarKG are shown in Table 1 and Table 5. MarKG has 11,292 entities, 192 relations and 76,424 images, include 2,063 analogy entities and 27 analogy relations. MARS has 10,685 training, 1,228 validation and 1,415 test instances, which are more significant than previous language analogy datasets. The original intention of MarKG is to provide prior knowledge of analogy entities and relations for better reasoning. We release the dataset with a leaderboard at [https://zjunlp.github.io/project/MKG_Analogy/](https://zjunlp.github.io/project/MKG_Analogy/). More details including quality control can be found in Appendix B.3.\\n\\n3.4 EVALUATION METRICS\\n\\nPrevious study (Chen et al., 2022a) adopts the multiple-choice QA to conduct analogical reasoning and leverage the accuracy metric for evaluation. However, the multiple-choice QA setting may struggle to handle the one-to-more entities, which is very common in real-world analogy scenarios. Thus, we formulate the task as link prediction that directly predicts the answer entity $e_a \\\\in E_a$. Our evaluation metrics include Hits@k scores (proportion of valid entities ranked in top k) and MRR (reciprocal value of the mean rank of correct entities). More details can be found in Appendix B.4.\\n\\n4 BENCHMARK METHODS\\n\\nIn this section, we introduce some baselines to establish the initial benchmark results on MARS, including multimodal knowledge graph embedding baselines and multimodal pre-trained Transformer baselines. We further propose MarT: a multimodal analogical reasoning framework with Transformer...\"}"}
{"id": "NRHajbzg8y0P", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Learnable Analogy Entity and Relation Embeddings\\n\\nAnalogy Example\\n\\n\u2462 Induction identical to IKRL TransAE RSME (Tesla) IKRL TransAE RSME (Young Tesla)\\n\\n\u2460 Abduction Analogy Question Analogy Answer\\n\\nb. Pre-train over MarKG\\n\\nc. Prompt-based Analogical Reasoning\\n\\nFigure 3: Overview of baseline methods. (a) Pipeline of MKGE methods for multimodal analogical reasoning. (b) and (c) are two stages of multimodal pre-trained Transformer (MPT) baselines.\\n\\n4.1 MULTIMODAL KNOWLEDGE EMBEDDING BASELINES\\n\\nWe consider three multimodal knowledge embedding (MKGE) approaches as our baselines, including IKRL (Xie et al., 2017), TransAE (Wang et al., 2019), and RSME (Wang et al., 2021). These methods are typically based on TransE (Bordes et al., 2013) or ComplEx (Trouillon et al., 2016) and combine with visual encoders to encode images for multimodal knowledge representation learning. They cannot be directly applied to the multimodal analogical reasoning task. To successfully utilize MKGE methods, we first pre-train them on MarKG to obtain entity embeddings and then follow the structure-mapping theory (Minnameier, 2010) to leverage the Abduction-Mapping-Induction as explicit pipeline steps for MKGE methods. As shown in Figure 3.a, Abduction aims to predict the relation \\\\( r \\\\) of \\\\((e_h, e_t)\\\\) similar to the relation classification task, Mapping represents that the structural relation is mapped onto entity candidates, analogous to template-filling, and Induction utilizes the relation \\\\( r \\\\) to predict the tail entity of \\\\((e_q, r, ?)\\\\) similar to the link prediction task.\\n\\nDespite the previous MKGE methods achieving excellent performance for KG-related tasks, the backbone, such as TransE, is not designed for analogy reasoning, which may hinder performance. Thus, we replace the backbone of MKGE methods with ANALOGY Liu et al. (2017) that models analogical structure explicitly as baselines.\\n\\n4.2 MULTIMODAL PRE-TRAINED TRANSFORMER BASELINES\\n\\nWe select multimodal pre-trained Transformer (MPT) approaches including the single-stream models VisualBERT (Li et al., 2019), ViLT (Kim et al., 2021), the dual-stream model ViLBERT (Lu et al., 2019), and the mixed-stream model FLAVA (Singh et al., 2022) and MKGformer Chen et al. (2022b) as the strong baselines. However, the current multimodal pre-trained Transformer cannot directly deal with analogical reasoning. To address the bottleneck above, we devise an end-to-end approach to empower the MPT with analogical reasoning ability. As shown in Figure 3, we first leverage MarKG to pre-train the model over sparse MarKG to obtain the representation of entities and relations. We then present the prompt-based analogical reasoning over MARS.\\n\\n4.2.1 PRE-TRAIN OVER MARKG\\n\\nWe represent the entities \\\\( e \\\\in E \\\\) and relations \\\\( r \\\\in R \\\\) as special tokens and denote \\\\( E \\\\) as the learnable embedding of these special tokens in the word vocabulary of language models. In the pre-train stage, we design masked entity and relation prediction like the Masked Language Modeling (MLM) task to learn the embeddings of the special tokens over the MarKG dataset. As shown in Figure 3.b, we devise a prompt template to convert the input as predicting the missing entity and relation via [MASK] token. In addition, we mix missing relation and entity prediction in the pre-train stage and consider different modalities of input entities. Specifically, we represent the visual entity \\\\( e_h \\\\) by...\"}"}
{"id": "NRHajbzg8y0P", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"its image\\n\\nand special entity embedding $E_e$, and the text entity $e_t$ by its text description $T_t$ and special entity embedding $E_e$, respectively. Benefiting from the mixed entity and relation prediction with the multimodal entity in the pre-train stage, we can obtain KG embedding with multimodal semantics over the current knowledge graph MarKG.\\n\\n4.2.2 PROMPT-BASED ANALOGICAL REASONING\\n\\nBased on the above-pre-trained entity and relation embeddings over MarKG, we propose prompt-based analogical reasoning with implicit structure mapping on downstream MARS.\\n\\nTaking the blended analogical reasoning as an example, we feed the analogy example $(I_h, T_t)$ and analogy question-answer pair $(I_q, ?)$ as input, and the goal is to predict the missing answer entity $e_a \\\\in E_a$. We leverage an analogical prompt template to convert the input as follows:\\n\\n$$T(E) \\\\parallel T(A) = I_h \\\\parallel I_q \\\\[CLS\\\\] e_h \\\\[R\\\\] T_t \\\\[SEP\\\\] \\\\parallel e_q \\\\[R\\\\] [MASK] \\\\[SEP\\\\],$$\\n\\nwhere $\\\\parallel$ represents concatenate operation in the template input, $I_h$ and $I_q$ represent the images of the entity $e_h$ and $e_q$, $T_t$ is the text description of the entity $e_t$. Moreover, $e_h, e_t, e_q$ are entity ids and will be encoded to special entity tokens $E_e, E_e, E_e$ in word embedding layer. Since the relations are not explicitly provided in the actual analogical reasoning task, we assign $[R]$ as a special token to denote the explicit relation between $(I_h, T_t)$, which is initialized with the average relation embeddings. Finally, we train the model to predict the $[MASK]$ over the special token embedding $E$ via cross-entropy loss, which likes the MLM task.\\n\\nRemark 1 We summarize the two parts of $T(E)$ and $T(A)$ in the template as the implicit Abduction and Induction respectively, which are unified in an end-to-end learning manner with prompt tuning. In addition, the analogical reasoning is reformulated as predicting the $[MASK]$ over the multimodal analogy entity embeddings to obtain $e_a$.\\n\\n4.3 MAR: A MULTIMODAL ANALOGICAL FRAMEWORK WITH TRANSFORMER\\n\\nAdaptive Interaction Across Analogy\\n\\nRelation-Oriented Structure Mapping\\nclose relations alienate entities\\nSelf-Attention Layer\\nattention score\\n$[R]$ $[R]$ $[R]$ $[R]$\\n\\nFigure 4: The MarT framework.\\n\\nAlthough the approach above-mentioned can enable multimodal pre-trained Transformer models to multimodal analogical reasoning, they only superficially consider implicit Abduction and Induction, ignoring the fine-grained associations between the analogy example and analogy question-answer pair. Adaptive Interaction Across Analogy. Since the analogy question may interfere with the representation of the analogy example and the inevitable noisy data issue, we propose adaptive interaction across analogy in encoding process to interact between the analogy example and question-answer pair adaptively, as shown in Figure 4. Denote the input to a Transformer layer as $X = [X_E \\\\parallel X_A]$ where $X_E$ and $X_A$ denote the hidden representation of analogy example $T_E$ and question-answer pair $T_A$ respectively. In each attention head of layer, the query and key representation can be formalized as:\\n\\n$$Q = X_{W_Q} = X_E \\\\parallel X_A \\\\quad K = X_{W_K} = X_E \\\\parallel X_A,$$\\n\\nwhere $W_Q, W_K$ are project matrices. A similar expression also holds for values $V$. Then the attention probability matrix $P$ can be defined in terms of four sub-matrices:\\n\\n$$P = QK^\\\\top = Q_E \\\\parallel Q_A \\\\quad (K_E \\\\parallel K_A) = P_{EE} \\\\quad P_{EA} \\\\quad P_{AE} \\\\quad P_{AA},$$\\n\\nwhere $P_{EE}$, $P_{AA}$ (diagonal of $P$) are intra-analogy attentions and $P_{EA}$, $P_{AE}$ (anti-diagonal of $P$) are inter-analogy attentions. We use the gate $G$ to regulate the inter-analogy interactions adaptively:\\n\\n$$P' = G \\\\odot P = \\\\begin{bmatrix} 1 & g_{EA} \\\\\\\\ g_{AE} & 1 \\\\end{bmatrix} \\\\odot P_{EE} \\\\quad P_{EA} \\\\quad P_{AE} \\\\quad P_{AA} = P_{EE} \\\\quad g_{EA} \\\\quad P_{EA} \\\\quad g_{AE} \\\\quad P_{AA},$$\\n\\nwhere $G \\\\in \\\\mathbb{R}^{2 \\\\times 2}$ is adaptive association gate which has two learnable variables $g_{EA}, g_{AE} \\\\in [0, 1]$. \"}"}
{"id": "NRHajbzg8y0P", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method | Baselines | Backbone | Hits@1 | Hits@3 | Hits@5 | Hits@10 | MRR  |\\n|-------|-----------|----------|--------|--------|--------|---------|------|\\n| MKGE  | IKRL      | TransE   | 0.254  | 0.285  | 0.290  | 0.304   | 0.274 |\\n|       | TransAE   | TransE   | 0.203  | 0.233  | 0.241  | 0.253   | 0.223 |\\n|       | RSME      | ComplEx  | 0.255  | 0.274  | 0.282  | 0.291   | 0.268 |\\n|       | ANALOGY   | ANALOGY  | 0.266  | 0.294  | 0.301  | 0.310   | 0.283 |\\n|       | ANALOGY   | ANALOGY  | 0.261  | 0.285  | 0.289  | 0.293   | 0.276 |\\n|       | ANALOGY   | ANALOGY  | 0.266  | 0.298  | 0.307  | 0.311   | 0.285 |\\n| MPT   | VisualBERT| Single-Stream | 0.247  | 0.281  | 0.289  | 0.303   | 0.269 |\\n|       | ViLT      | Single-Stream | 0.235  | 0.266  | 0.274  | 0.286   | 0.257 |\\n|       | ViLBERT   | Dual-Stream | 0.252  | 0.308  | 0.320  | 0.338   | 0.287 |\\n|       | FLAVA     | Mixed-Stream | 0.257  | 0.299  | 0.312  | 0.325   | 0.284 |\\n|       | MKGformer | Mixed-Stream | 0.293  | 0.335  | 0.344  | 0.367   | 0.321 |\\n|       | MarT      | VisualBERT| 0.261  | 0.292  | 0.308  | 0.321   | 0.284 |\\n|       | MarT      | ViLT     | 0.245  | 0.275  | 0.287  | 0.303   | 0.266 |\\n|       | MarT      | ViLBERT  | 0.256  | 0.312  | 0.327  | 0.347   | 0.292 |\\n|       | MarT      | FLAVA    | 0.264  | 0.303  | 0.309  | 0.319   | 0.288 |\\n|       | MarT      | MKGformer| 0.301  | 0.367  | 0.380  | 0.408   | 0.341 |\\n\\nTable 2: The main performance results on MARS. We report pipeline baselines with multimodal knowledge graph embedding (MKGE) methods and replace their backbone models with analogy-aware model ANALOGY. We also utilize our MarT on end-to-end baselines with multimodal pre-trained Transformer (MPT) methods and obtain the best performance in MarT.\\n\\n**Remark 2**\\n\\nOn the one hand, the query from $T_A$ may interfere with the example from $T_E$. On the other hand, $T_E$ may have a weaker impact on $T_A$ in noisy data. Adaptive association gates can increase and decrease inter-analogy interaction automatically based on the intimacy of $T_E$ and $T_A$.\\n\\n**Relation-Oriented Structure Mapping.**\\n\\nThe structure mapping theory emphasizes the relation transfer rather than object similarity in analogical reasoning, it is relations between objects, rather than attributes of objects, are mopped from base to target. For example, battery can make an analogy to reservoir because they both store potential, rather than their shapes being cylindrical. Motivated by this, we propose the relaxation loss to bring the relations closer and alienate the entities:\\n\\n$$L_{rel} = \\\\frac{1}{|S|} \\\\sum\\\\limits_{(h_E[R], h_A[R])} (1 - \\\\text{sim}(h_E[R], h_A[R])) + \\\\max(0, \\\\text{sim}(h_E[h], h_E[q]))$$\\n\\nwhere $|S|$ is the total number of the training set $S$, $h_E[R]$ is the hidden feature of $[R]$ in analogy example $T_E$ output from the MLM head, $\\\\text{sim}(\\cdot)$ is the cosine similarity. We leverage the masked entity prediction task to obtain the answer entity $e_a$ with a cross-entropy loss:\\n\\n$$L_{mem} = -\\\\frac{1}{|S|} \\\\sum\\\\limits_{(e_h, e_t, e_q, e_a) \\\\in S} \\\\log(p([\\\\text{MASK}] = e_a) | T(e_h, e_t, e_q))$$\\n\\nAfterwards, we interpolate the relaxation loss $L_{rel}$ and the masked entity prediction loss $L_{mem}$ using parameter $\\\\lambda$ to produce the final loss $L$:\\n\\n$$L = \\\\lambda L_{rel} + (1 - \\\\lambda) L_{mem}$$\\n\\n**Remark 3**\\n\\nThe relaxation loss is composed of pull-in and pull-away that correspond to the close relation and alienate entity terms, respectively, which can constrain the model's focus on relation structure transfer and implicitly realize the Structure Mapping process.\\n\\n**5 RESULTS AND ANALYSIS**\\n\\n**5.1 MAIN RESULTS**\\n\\nThe main performance results of all benchmark methods can be seen in Table 2. In general, we find the performance of multimodal knowledge graph embedding (MKGE) baselines and multimodal pre-trained Transformer (MPT) baselines is comparable except MKGformer, which establishes a\"}"}
{"id": "NRHajbzg8y0P", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Ablation experiments on MARS. w/o MarKG refers to the model without pre-training on MarKG dataset. w/o MarT refers to ablate all components of MarT that equivalents to MKGformer. \\n\\n| Model          | Hits@1 | Hits@3 | Hits@5 | Hits@10 | MRR  |\\n|----------------|--------|--------|--------|---------|------|\\n| TransAE        | 0.203  | 0.233  | 0.241  | 0.253   | 0.223|\\n| w/o MarKG      | 0.191  | 0.224  | 0.235  | 0.245   | 0.214|\\n| MarT ViLBERT   | 0.256  | 0.312  | 0.327  | 0.347   | 0.292|\\n| w/o MarKG      | 0.253  | 0.292  | 0.297  | 0.310   | 0.270|\\n| w/o Analogy example | 0.113  | 0.143  | 0.162  | 0.179   | 0.138|\\n| MarT MKGformer | 0.301  | 0.367  | 0.380  | 0.408   | 0.341|\\n| w/o MarKG      | 0.270  | 0.305  | 0.309  | 0.315   | 0.289|\\n| w/o Relaxation loss | 0.295  | 0.349  | 0.373  | 0.399   | 0.332|\\n| w/o Adaptive interaction | 0.285  | 0.345  | 0.365  | 0.395   | 0.324|\\n| w/o MarT       | 0.293  | 0.335  | 0.344  | 0.367   | 0.321|\\n| w/o Analogy example | 0.101  | 0.123  | 0.132  | 0.149   | 0.120|\\n\\nTable 3: Results of MKGformer on novel relation generalization. \u201cw/ Full MARS\u201d is the result trained with full data (upper bound).\\n\\n| Model          | Hits@1 | Hits@3 | Hits@5 | Hits@10 | MRR  |\\n|----------------|--------|--------|--------|---------|------|\\n| MarT MKGformer | 0.254  | 0.285  | 0.292  | 0.273   |      |\\n| w/o MarKG      | 0.217  | 0.228  | 0.231  | 0.224   |      |\\n| w/ Full MARS   | 0.365  | 0.419  | 0.433  | 0.395   |      |\\n\\n5.2 Generalize to Novel Relation\\n\\n| Model          | Hits@1 | Hits@3 | Hits@5 | Hits@10 | MRR  |\\n|----------------|--------|--------|--------|---------|------|\\n| MarT MKGformer | 0.254  | 0.285  | 0.292  | 0.273   |      |\\n| w/o MarKG      | 0.217  | 0.228  | 0.231  | 0.224   |      |\\n| w/ Full MARS   | 0.365  | 0.419  | 0.433  | 0.395   |      |\\n\\nMaking analogies from one domain to another novel domain is a fundamental ingredient for human creativity. In this section, we conduct a novel relation transfer experiment (including both task settings) to measure how well the models generalize by analogy to unfamiliar relations. Specifically, we randomly split the 27 analogy relations into the source and target relations. The models are then trained on the source and tested on the novel target relations. As shown in Table 3, we observe that MarT MKGformer can indeed learn to make sense of unfamiliar relations, respectively. We further evaluate the model without pre-training on MarKG and find the performance decreased, which indicates that the structure knowledge provided by MarKG is critical for generalization. Note that the novel relation transfer setting is somewhat similar to zero-shot or domain generalization, and we hope our work can benefit other communities.\\n\\n5.3 Ablation Study\\n\\nTo validate the effectiveness of MarKG and MarT, we conduct an ablation study as shown in Table 4. We observe that discarding pre-training on MarKG results in worse performance for both MKGE and MPT baselines. It indicates that the knowledge structure information provided by MarKG helps learn the representation of entities and relations, further benefiting analogical reasoning. We also find that the performance clearly drops when ablating each component of MarT and reaches the valley when ablating all, proving the effectiveness of each analogical component of our MarT. Moreover, we ablate the analogy example in the input and find the performance drops a lot, which reveals the importance of analogical prompts.\\n\\n5.4 Analysis\\n\\nAnalysis across Different Sub-Tasks. In previous Table 2, we are amazed by ANALOGY significantly improving the performance of MKGE baselines. Therefore, we further compare the performance of different sub-tasks: ANALOGY, TRANSFORMER, and TRANSFORMER + MarKG. We find that ANALOGY achieves the best performance, followed by TRANSFORMER + MarKG and TRANSFORMER. This suggests that the analogical component of MarT is particularly effective in improving the performance of the Transformer-based model.\"}"}
{"id": "NRHajbzg8y0P", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Panax notoginseng\\n? \\nTraditional Chinese\\nMedicine\\ninstance of\\nApple fruit\\ninstance of\\nbattle\\n? \\ncampaign\\nInland Lake\\nQinghai Lake\\ncorrespond to\\nMKGformer\\nMKGformer*\\nTransAE\\nTransAE*\\nfilm\\nincr ement\\nscheme\\nbattle\\nwar\\ncourt\\naircraft\\nocean\\nbattle\\nreaction\\nlife\\nAnalogical Example\\nTask Setting\\n\\nFigure 6: Case examples of MARS. We show the analogy example and analogy question-answer pair with their implicit relations. \u201cTop-3 Entity\u201d means top-3 ranking entities in the prediction. \u201cGold Rank\u201d refers to the rank of the gold answer entity in the prediction. * denotes the baseline model with analogical components (MarT or ANALOGY).\\n\\nAs shown in Figure 5, we observe that vanilla TransAE performs poorly in the blended task setting. However, when replacing the backbone TransE with ANALOGY, TransAE is competent in blended analogical reasoning setting and even outperforms the single setting. On the other side, RSME with ComplEx as backbone can handle the blended setting reluctantly but perform worse than the single setting. ANALOGY improves the performance of RSME in this situation. Meanwhile, MarT further explores the potential of MKGformer and improves its performance in various tasks. All in all, the analogical components consistently improve the multimodal analogical reasoning ability of all baseline methods, especially in blended analogical reasoning, which supports Mayer\u2019s theory (Mayer, 2002) that analogical reasoning is more affinity for multimodal scenarios.\\n\\nCase Analysis. As shown in Figure 6, we provide case analysis and observe that the top ranking entities (film, life, etc.) of the baselines without analogical components are usually irrelevant to the question entity \u201ccampaign\u201d. Analogical components make the predictions more reasonable and successfully predict the answer entity \u201cbattle\u201d. In the difficult blended analogical reasoning setting, the blended modal input of visual and text is challenging. We find that vanilla MKGformer and TransAE fail to understand the visual semantic of \u201capple\u201d and incorrectly linked with \u201ccapital, phone, shipping\u201d that related to \u201cApple Company\u201d. We also notice that TransAE with ANALOGY as backbone significantly decreases the prediction error but incorrectly predicts \u201cplant\u201d as the top-1 entity due to the interference of \u201cPanax notoginseng\u201d. On the contrary, MarT_MKGformer with relaxation loss can alienate the entities and focus on relation structures transfer and obtain reasonable predictions. These observations reveal that multimodal analogical reasoning is a highly challenging task, and analogy-aware components could enhance the analogical ability of models. Besides, we discuss limitations in Appendix A and provide a comprehensive error analysis in Appendix D.\\n\\n6 DISCUSSION AND CONCLUSION\\nIn this work, we introduce the new task of multimodal analogical reasoning over knowledge graphs. Preliminary experiments show that this task brings a rather difficult challenge and is worth further exploration. Besides evaluating the analogical reasoning ability of models, there are some potential applications to explore: (1) knowledge graph completion with analogies, (2) transfer learning and zero-shot learning by analogy and (3) analogical question answering. We hope our work inspires future research on analogical reasoning and applications, especially in the multimodal world.\\n\\nA Huggingface Demo at https://huggingface.co/spaces/zjunlp/MKG_Analogy.\"}"}
{"id": "NRHajbzg8y0P", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The source MARS and MarKG datasets will be released on Github soon. In order to provide support to reproduce our experiments in Section 5, we provide the detailed source code of all pipeline baselines (IKRL, TransAE, RSME) and end-to-end baselines (VisualBERT, ViLBERT, ViLT, FLAVA, MKGformer) in the supplementary materials with all scripts and hyper-parameters. We also provide a README script to instruct how to run the codes.\\n\\nACKNOWLEDGMENT\\n\\nWe would like to express gratitude to the anonymous reviewers for their kind comments. This work was supported by the National Natural Science Foundation of China (No.62206246 and U19B2027), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Ningbo Natural Science Foundation (2021J190), and Yongjiang Talent Introduction Programme (2021A-156-G), CAAI-Huawei MindSpore Open Fund, and NUS-NCS Joint Laboratory (A-0008542-00-00).\\n\\nREFERENCES\\n\\nYoshua Bengio, Yann LeCun, and Geoffrey E. Hinton. Deep learning for AI. Commun. ACM, 64(7):58\u201365, 2021. doi: 10.1145/3448250. URL https://doi.org/10.1145/3448250.\\n\\nAntoine Bordes, Nicolas Usunier, Alberto Garc\u00eda-Dur\u00e1n, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In Christopher J. C. Burges, L\u00e9on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pp. 2787\u20132795, 2013. URL https://proceedings.neurips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McDermish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\\n\\nJiangjie Chen, Rui Xu, Ziquan Fu, Wei Shi, Zhongqiao Li, Xinbo Zhang, Changzhi Sun, Lei Li, Yanghua Xiao, and Hao Zhou. E-KAR: A benchmark for rationalizing natural language analogical reasoning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 3941\u20133955. Association for Computational Linguistics, 2022a. doi: 10.18653/v1/2022.findings-acl.311. URL https://doi.org/10.18653/v1/2022.findings-acl.311.\\n\\nXiang Chen, Ningyu Zhang, Lei Li, Shumin Deng, Chuanqi Tan, Changliang Xu, Fei Huang, Luo Si, and Huajun Chen. Hybrid transformer with multi-level fusion for multimodal knowledge graph completion. In Enrique Amig\u00f3, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai (eds.), SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022, pp. 904\u2013915. ACM, 2022b. doi: 10.1145/3477495.3531992. URL https://doi.org/10.1145/3477495.3531992.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Paul McFedries (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing: Poster Sessions, Hong Kong, China, December 7-9, 2019, pp. 4171\u20134182. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1416. URL https://doi.org/10.18653/v1/D19-1416.\"}"}
