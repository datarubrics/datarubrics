{"id": "53yQBJNQVJu", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification. arXiv preprint arXiv:1803.00676, 2018.\\n\\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.\\n\\nShiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameterization exacerbates spurious correlations. In Proc. of ICML, 2020.\\n\\nShiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, et al. Extending the wilds benchmark for unsupervised adaptation. arXiv preprint arXiv:2112.05090, 2021.\\n\\nTimo Schick and Hinrich Sch\u00fctze. It's not just size that matters: Small language models are also few-shot learners. In Proc. of NAACL, 2021.\\n\\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\\n\\nAman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi. Certifying some distributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.\\n\\nLeslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using large learning rates. In Artificial intelligence and machine learning for multi-domain operations applications, 2019.\\n\\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Proc. of NeurIPS, 2017.\\n\\nColin Studholme, Derek LG Hill, and David J Hawkes. An overlap invariant entropy measure of 3d medical image alignment. Pattern recognition, 1999.\\n\\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\\n\\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proc. of CVPR, 2015.\\n\\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proc. of CVPR, 2016.\\n\\nChristian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Proc. of AAAI, 2017.\\n\\nMingxing Tan and Quoc V. Le. Efficientnetv2: Smaller models and faster training. In Proc. of ICML, 2021.\\n\\nKaihua Tang, Mingyuan Tao, Jiaxin Qi, Zhenguang Liu, and Hanwang Zhang. Invariant feature learning for generalized long-tailed classification. arXiv preprint arXiv:2207.09504, 2022.\\n\\nRohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. In Proc. of NeurIPS, 2020a.\\n\\nRohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. Proc. of NeurIPS, 2020b.\\n\\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Proc. of NeurIPS, 2021.\\n\\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In Proc. of NeurIPS, 2016.\"}"}
{"id": "53yQBJNQVJu", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "53yQBJNQVJu", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma 1. \\\\( \\\\epsilon_S(h, f) := E_{x \\\\sim S} |\\\\delta(h(x)) - \\\\delta(f(x))| \\\\). For any hypothesis \\\\( h, h' \\\\in H \\\\), there exists \\\\( \\\\epsilon_H > 0 \\\\) which satisfies,\\n\\n\\\\[ |\\\\epsilon_P(I_k)(h, h') - \\\\epsilon_P(h, h')| \\\\leq \\\\text{MMD}(H, P^I_k, P) + \\\\epsilon_H^2 \\\\]\\n\\n\\\\( \\\\epsilon_H \\\\) is a constant for the complexity of hypothesis space.\\n\\nProof. \\\\( \\\\epsilon_P(I_k)(h, h') - \\\\epsilon_P(h, h') \\\\leq \\\\sup_{h, h' \\\\in H} |\\\\epsilon_P(I_k)(h, h') - \\\\epsilon_P(h, h')| = \\\\sup_{h, h' \\\\in H} P_x \\\\sim P^I_k [\\\\delta(h(x)) \\\\neq \\\\delta(h'(x))] - P_x \\\\sim P [\\\\delta(h(x)) \\\\neq \\\\delta(h'(x))] = \\\\sup_{h, h' \\\\in H} P^I_x \\\\mathbb{Z}_{X} 1_{h(x) \\\\neq h'(x)} d\\\\mu_P^I_k - P^I_x \\\\mathbb{Z}_{X} 1_{h(x) \\\\neq h'(x)} d\\\\mu_P \\\\leq \\\\mathbb{Z}_X f(x) d\\\\mu_P^I_k - \\\\mathbb{Z}_X f(x) d\\\\mu_P + \\\\epsilon_H^2 \\\\quad (12) \\\\)\\n\\n\\\\( \\\\epsilon_P(I_k)(h, f) \\\\rightarrow 0 \\\\) and \\\\( \\\\epsilon_P(h, f) \\\\rightarrow 0 \\\\),\\n\\n\\\\( \\\\epsilon_P(I_k)(h, f) \\\\leq \\\\epsilon_H \\\\), where \\\\( \\\\epsilon_H \\\\) is a constant approaching zero.\\n\\nFollowing Ben-David et al. (2010), we use Lemma 1 and 2 to prove Theorem 5.1.\\n\\nProof \\\\( \\\\epsilon_Q(f_{I_k}) \\\\leq \\\\epsilon_Q(f) + \\\\epsilon_P(I_k)(f_{I_k}, f) \\\\leq \\\\epsilon_Q(f) + \\\\epsilon_P(I_k)(f_{I_k}, f) + (\\\\epsilon_P(I_k)(f_{I_k}, f) - \\\\epsilon_Q(f_{I_k}, f)) + (\\\\epsilon_P(f_{I_k}, f) - \\\\epsilon_P(I_k)(f_{I_k}, f)) \\\\leq \\\\epsilon_Q(f) + \\\\epsilon_P(I_k)(f_{I_k}, f) + |\\\\epsilon_P(I_k)(f_{I_k}, f) - \\\\epsilon_Q(f_{I_k}, f)| + |\\\\epsilon_P(f_{I_k}, f) - \\\\epsilon_P(I_k)(f_{I_k}, f)| \\\\leq \\\\epsilon_Q(f) + \\\\epsilon_P(I_k)(f_{I_k}, f) + |\\\\epsilon_P(I_k)(f_{I_k}, f) - \\\\epsilon_Q(f_{I_k}, f)| + MMD(P^I_k, P) + \\\\epsilon_H^2 (14) \\\\)\\n\\nIn which,\"}"}
{"id": "53yQBJNQVJu", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nHere suppose test set $Q$ matches the distribution of data for this classification task, and $P$ is constructed by sampling $n$ i.i.d. samples from the distribution $Q$. Using Hoeffding inequality we have,\\n\\n$$P(|\\\\epsilon_P(f_{I_k}, f) - \\\\epsilon_Q(f_{I_k}, f)| > t) \\\\leq 2e^{-2nt^2}$$\\n\\n(16)\\n\\nTherefore, with a probability over $1 - 2e^{-2n\\\\epsilon^2t}$ (in practice $n$ is very large and the probability approaches 1), we have\\n\\n$$\\\\epsilon_Q(f_{I_k}) \\\\leq \\\\epsilon_Q(f) + \\\\text{MMD}(P_{I_k}, P) + \\\\epsilon_\\\\alpha + t + \\\\epsilon_H$$\\n\\n(17)\\n\\nA.2 PROPERTY OF NTK\\n\\nProposition A.1. (Neural Tangent Kernel, Arora et al. (2019)) Consider minimizing the loss $l(\\\\theta)$ by gradient descent with infinitesimally small learning rate:\\n\\n$$\\\\frac{d\\\\theta}{dt} = -\\\\nabla l(\\\\theta(t))$$\\n\\nLet $u(t) = (f(\\\\theta(t), x_i))_{i \\\\in [n]} \\\\in \\\\mathbb{R}^{nd}$ be the network outputs on all $x_i$ at time $t$, and $Y = (y_i)_{i \\\\in [n]}$ be the ground truth outputs, loss is $l(f(\\\\theta, X), Y)$. Then $u(t)$ follows the following evolution, where $K_t$ is the NTK matrix,\\n\\n$$u(t) - u(t-1) = K_t \\\\frac{\\\\partial l(\\\\theta(t))}{\\\\partial u(t)} + O((\\\\theta(t) - \\\\theta(t-1))^2)$$\\n\\n(18)\\n\\nThe proof follows Arora et al. (2019).\\n\\nA.3 PROPOSITION 5.1\\n\\nThis proposition is adopted from Lemma.4 in Gretton et al. (2012).\\n\\nLemma 3. (Restatement of Lemma.4 in Gretton et al. (2012)) Let $H$ be the Reproducing Kernel Hilbert Space for kernel $K(\\\\cdot, \\\\cdot)$. If $K(\\\\cdot, \\\\cdot)$ is measurable and $E_{x \\\\sim P} K(x, x) > 0$, $\\\\forall x \\\\in X$, $E_{x \\\\sim Q} K(x, x) > 0$, $\\\\forall x \\\\in X'$, then mean embeddings $\\\\mu_P, \\\\mu_Q$ exists such that $E x f = \\\\langle f, \\\\mu_P \\\\rangle$ and satisfies,\\n\\n$$\\\\text{MMD}(P, Q) = ||\\\\mu_P - \\\\mu_Q||_H$$\\n\\n(19)\\n\\nThe NTK kernel is a positive definite matrix and satisfies constraints in this Lemma. Also based on the definition of distance in RKHS, we can rewrite Equation. 19 into the form in Proposition. 5.1.\\n\\nA.4 PROOF FOR THEOREM 5.2\\n\\nProof Using the MMD expression in integral form (Cheng & Xie, 2021), we can simplify the expression in Definition 5.1:\\n\\n$$\\\\text{MMD}(P, Q) = \\\\sup \\\\int_X f(\\\\theta, x) |\\\\hat{p} - \\\\hat{q}| (x) dx$$\\n\\n(20)\\n\\nwhere $\\\\hat{p} = \\\\frac{1}{n} \\\\sum_{i} \\\\delta_{x_i}, x_i \\\\sim P$, and $\\\\hat{q} = \\\\frac{1}{m} \\\\sum_{i} \\\\delta_{y_i}, y_i \\\\sim Q$. $\\\\delta_x$ is the indicator function that equals to 1 when input equals to $x$, otherwise the output is 0.\\n\\nSimilarly we can write NTK-MMD initialized with $f(0)$ as,\\n\\n$$\\\\text{MMD}_2(K_0(P, Q)) = \\\\int_X \\\\int_X K_0(x, x') (\\\\hat{p} - \\\\hat{q})(x)(\\\\hat{p} - \\\\hat{q})(x') dx dx'$$\\n\\n(21)\"}"}
{"id": "53yQBJNQVJu", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nFigure 5: Spearman correlation of gradient norms calculated by different models. All results have $r < 0.005$ except for the VGG correlation, whose $r > 0.05$.\\n\\nFor different models, the gradient norms are different and thus various models can generate different adversarial few-shot sets. In this section, we explore whether there is an intrinsic gradient ranking correlation across different models. We report the correlation analysis between different models in Figure 6. As can be seen from the graph, the VGG is not correlating with any of the other models' rankings. The correlation result also aligns with our main results that the VGG attack is almost invalid. Beyond that, we can see that ResNet, DenseNet, and ResNeXt, which have similar residual structures, maintain some correlation. Apart from that, the FFN, which also has a strong attack capability, only maintains a high correlation with ResNeXt, showing that influential ranking is model-independent.\\n\\nD.1 WORST-CASE EVALUATION ON FEW-SHOT LEARNING MODELS\\n\\nWe also explored the ability of the NMMD-Attack on the FEW-SHOT learning models. We selected the ProtoNet (Snell et al., 2017) for experiments on CIFAR-100 with FFN-attack. From the results, we can see that even though the model is designed for few-shot learning, it still suffers from our attack.\\n\\nTable 5: The attack results on few-shot models on CIFAR-100. The abbreviations follow Table 1.\\n\\n| Models          | Average-case NMMD-attack Test Acc. | Gap          | Train Acc. | Test Acc. | Train Acc. | Test Acc. | Abs. | Rel. |\\n|-----------------|-----------------------------------|--------------|------------|-----------|------------|-----------|------|------|\\n| ProtoNet        | 99.97                             | 54.88        | 99.96      | 52.46     | 2.42       | 4.41      |      |      |\"}"}
{"id": "53yQBJNQVJu", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: The comparison between average performance and attack performance of different size on CIFAR-10. The abbreviations follow Table 1.\\n\\n| k-shot Models | Average-case | NMMD-attack | Test Acc. | Gap |\\n|---------------|--------------|-------------|-----------|-----|\\n| Train Acc.    | Test Acc.    | abs. rel.   |\\n| 50-shot       |              |             |\\n| FFN 52.16 \u00b1 10.93 | 30.13 \u00b1 1.60 | 84.52 \u00b1 9.82 | 24.90 \u00b1 1.00 | 5.23 17.36 |\\n| VGG-16 100.00 \u00b1 0.00 | 40.00 \u00b1 0.41 | 100.00 \u00b1 0.00 | 22.14 \u00b1 2.02 | 17.89 44.65 |\\n| ResNet-18 100.00 \u00b1 0.00 | 35.06 \u00b1 0.46 | 100.00 \u00b1 0.00 | 19.01 \u00b1 1.17 | 10.83 45.78 |\\n| ResNeXt-29 100.00 \u00b1 0.00 | 34.95 \u00b1 1.15 | 100.00 \u00b1 0.00 | 24.12 \u00b1 0.59 | 10.83 30.99 |\\n| DenseNet-121 100.00 \u00b1 0.00 | 40.57 \u00b1 1.75 | 100.00 \u00b1 0.00 | 24.29 \u00b1 1.88 | 16.28 40.13 |\\n| 200-shot      |              |             |\\n| FFN 100.00 \u00b1 0.00 | 43.08 \u00b1 1.29 | 99.99 \u00b1 0.01 | 36.12 \u00b1 1.68 | 6.06 16.16 |\\n| VGG-16 100.00 \u00b1 0.00 | 56.93 \u00b1 0.41 | 100.00 \u00b1 0.00 | 34.93 \u00b1 1.24 | 22.00 38.64 |\\n| ResNet-18 100.00 \u00b1 0.00 | 53.65 \u00b1 0.88 | 100.00 \u00b1 0.00 | 34.42 \u00b1 1.69 | 19.23 35.84 |\\n| ResNeXt-29 100.00 \u00b1 0.00 | 52.07 \u00b1 1.10 | 100.00 \u00b1 0.00 | 34.66 \u00b1 1.19 | 17.41 33.44 |\\n| DenseNet-121 100.00 \u00b1 0.00 | 60.85 \u00b1 0.82 | 100.00 \u00b1 0.00 | 36.18 \u00b1 1.42 | 24.67 40.54 |\\n| 2000-shot     |              |             |\\n| FFN 100.00 \u00b1 0.00 | 59.10 \u00b1 0.69 | 99.99 \u00b1 0.01 | 55.84 \u00b1 0.90 | 3.26 5.52 |\\n| VGG-16 100.00 \u00b1 0.00 | 82.66 \u00b1 0.15 | 100.00 \u00b1 0.00 | 78.22 \u00b1 0.29 | 4.44 5.37 |\\n| ResNet-18 100.00 \u00b1 0.00 | 80.68 \u00b1 0.25 | 100.00 \u00b1 0.00 | 76.49 \u00b1 0.77 | 4.19 5.19 |\\n| ResNeXt-29 100.00 \u00b1 0.00 | 79.10 \u00b1 0.35 | 100.00 \u00b1 0.00 | 76.00 \u00b1 0.37 | 3.10 3.92 |\\n| DenseNet-121 100.00 \u00b1 0.00 | 85.96 \u00b1 0.45 | 100.00 \u00b1 0.00 | 82.64 \u00b1 0.34 | 3.32 3.86 |\\n\\nWe conducted attack experiments on few-shot sets with different sizes. The results show that with the increase of training data, the robustness gradually becomes better. There is considerable overlap between the attack and random datasets at larger data sizes. The performance drop on 2000-shot cases also can indicate the effectiveness of our methods.\"}"}
{"id": "53yQBJNQVJu", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We case study the correlation between several spurious attributes and labels on the Imagenet-1K. The results in Figures 6, 7 & 8 show that spurious correlations are common. When trained on a randomly selected few-shot set, all three cases suffer from spurious correlations at different levels. However, the spurious correlations problem becomes less severe after training on our NMMD-attack set in the Bamboo Leaves VS. Small Panda and Water Background VS. Drake cases. Compared to results on CIFAR-10, the spurious correlation alleviation is less precise, possibly because the model has not learned the spurious attributes on Imagenet well. Furthermore, the spurious attributes on complicated figures may also be more complicated and hard to gain.\"}"}
{"id": "53yQBJNQVJu", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 10: Visualization of the few-shot set searched by NMMD-attack for CIFAR-10. We randomly choose 50 examples for each label.\"}"}
{"id": "53yQBJNQVJu", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We can see that MMD\\\\(_f\\\\) involves solving an optimization problem. Here we design a loss to achieve the maximum at the last time step during fine-tuning. At the same time, we require the model \\\\(f\\\\) to learn the distribution of the task.\\n\\n\\\\[\\nL(f(\\\\theta_t, x)) = -|E_{Q(f(\\\\theta_t, x), x)} - E_{P(f(\\\\theta_t, x), x)}|\\n\\\\]\\n\\nUnalignment loss\\n\\n\\\\[\\n|E_{P(f(x)) - p(y|x)}|\\n\\\\]\\n\\nClassification loss\\n\\nDuring training, we take a two step approach, first aligning the distributions and then minimizing the classification error (in practice, this can be done by training the representation encoder and then the classification head). Here we suppose at initialization, the loss is quite large such that the two distributions are aligned.\\n\\nThe NTK at time step \\\\(t\\\\) is \\\\(K_t(x, x') = \\\\langle \\\\partial f(x, \\\\theta_t) \\\\theta_t \\\\partial f(x', \\\\theta_t) \\\\rangle\\\\). Our \\\\(P\\\\) is the training set distribution, \\\\(P_I\\\\) is the distribution \\\\(Q\\\\) (here we write \\\\(Q\\\\) for convenience). Using the L1 loss, combined with Proposition 3.1, we have:\\n\\n\\\\[\\nu(x, t) - u(x, t-1) = \\\\nabla_\\\\theta f(x; \\\\theta_t)^T (\\\\theta_t - \\\\theta_{t-1}) + O(\\\\|\\\\theta_t - \\\\theta_{t-1}\\\\|^2) = Z_X \\\\nabla_\\\\theta f(x; \\\\theta_0)^T \\\\nabla_\\\\theta f(x'; \\\\theta_0) (-|\\\\hat{p} - \\\\hat{q}| + \\\\hat{p})(x') dx' + O(\\\\|\\\\theta_t - \\\\theta_{t-1}\\\\|^2)\\n\\\\]\\n\\nRecalling that, the NTK matrix remains stable during training, thus we can write it as,\\n\\n\\\\[\\nu(x, t) - u(x, t-1) = \\\\eta Z_X \\\\nabla_\\\\theta f(x; \\\\theta_0)^T \\\\nabla_\\\\theta f(x'; \\\\theta_0) (-|\\\\hat{p} - \\\\hat{q}| + \\\\hat{p})(x') dx' + O(\\\\|\\\\theta_t - \\\\theta_{t-1}\\\\|^2)\\n\\\\]\\n\\nHere \\\\(X\\\\) is the support set of \\\\(P\\\\) and \\\\(Q\\\\). Also,\\n\\n\\\\[\\n|\\\\|\\\\theta_t - \\\\theta_{t-1}\\\\|| = \\\\eta \\\\|\\\\nabla_\\\\theta L(f(\\\\theta_t), x)\\\\| \\\\leq \\\\eta L f n\\n\\\\]\\n\\nTherefore, combining \\\\(t = cn\\\\) steps (c epochs) of training we have:\\n\\n\\\\[\\nu(x, t) - u(x, 0) = \\\\eta Z_X \\\\nabla_\\\\theta f(x; \\\\theta_0)^T \\\\nabla_\\\\theta f(x'; \\\\theta_0) (\\\\hat{p})(x') dx' dt + t X_s = 0 O(\\\\eta^2 L^2 f^2 n^2) = \\\\eta Z_X \\\\nabla_\\\\theta f(x; \\\\theta_0)^T \\\\nabla_\\\\theta f(x'; \\\\theta_0) (\\\\hat{p})(x') dx' dt + O(\\\\eta^2 L^2 f^2 + O(\\\\eta^2 L^2 f^2))\\n\\\\]\\n\\nThen combined with the definition of MMD, if \\\\(n \\\\geq 2k\\\\):\\n\\n\\\\[\\nMMD_f(P, Q) = Z_X u(x, t) |\\\\hat{p} - \\\\hat{q}| (x) dx = Z_X [u(x, 0) + \\\\eta Z_X Z_X K_0(x, x')(-|\\\\hat{p} - \\\\hat{q}| + \\\\hat{p})(x') dx'] |\\\\hat{p} - \\\\hat{q}| (x) dx + O(t\\\\eta^2 L^2 f)\\n\\\\]\\n\\n\\\\[\\n\\\\leq 2\\\\eta t MMD^2 K_0(P, Q) + O(t\\\\eta^2 L^2 f)\\n\\\\]\"}"}
{"id": "53yQBJNQVJu", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nAlso,\\n\\n\\\\[ \\\\text{MMD} \\\\left( P, Q \\\\right) = \\\\int_{\\\\mathbb{X}} u \\\\left( x, t \\\\right) \\\\left| \\\\hat{p} - \\\\hat{q} \\\\right| \\\\left( x \\\\right) \\\\, dx = \\\\int_{\\\\mathbb{X}} \\\\left[ u \\\\left( x, 0 \\\\right) + \\\\eta \\\\int_{t} \\\\int_{\\\\mathbb{X}} \\\\int_{\\\\mathbb{X}} K_{0} \\\\left( x, x' \\\\right) \\\\left( -\\\\left| \\\\hat{p} - \\\\hat{q} \\\\right| + \\\\hat{p} \\\\right) \\\\left( x' \\\\right) \\\\, dx \\\\right] \\\\left| \\\\hat{p} - \\\\hat{q} \\\\right| \\\\left( x \\\\right) \\\\, dx + O \\\\left( t \\\\eta^{2} L_{2}^{2} f \\\\right) > \\\\eta \\\\int_{t} \\\\int_{\\\\mathbb{X}} \\\\int_{\\\\mathbb{X}} K_{0} \\\\left( x, x' \\\\right) \\\\left| \\\\hat{p} - \\\\hat{q} \\\\right| \\\\left( x' \\\\right) \\\\, dx \\\\right| \\\\left| \\\\hat{p} - \\\\hat{q} \\\\right| \\\\left( x \\\\right) \\\\, dx + O \\\\left( t \\\\eta^{2} L_{2}^{2} f \\\\right) \\\\]\\n\\n(28)\"}"}
{"id": "53yQBJNQVJu", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To evaluate the robustness of over-parameterized neural networks, we consider the following models. 1) **FFN**, a feed-forward neural network with two convolution and pooling layers and three feed-forward layers. 2) **VGG** (Simonyan & Zisserman, 2014), a classical convolutional neural network. We use the VGG-16 with 13 convolution layers and three fully connected layers as implementation. 3) **ResNet** (He et al., 2016), a residual neural network. We use the ResNet-18 with 16 residual blocks, one convolution layer, and one fully connected layer as implementation; 4) **ResNeXt** (Xie et al., 2017) incorporating the advantages of ResNet and Inception (Szegedy et al., 2015; 2016; 2017; Ioffe & Szegedy, 2015). We use the ResNeXt-29 (2x64d) for MNIST, CIFAR-10, and CIFAR-100, and ResNeXt-50 (32x4d) for ImageNet. 5) **DenseNet** (Huang et al., 2017). We use DenseNet-121 with 121 layers, one convolution layer, and one fully connected layer as re-implementation. Besides, to verify the attack ability NMMD-attack on the pre-trained models, we also re-implement two pre-trained models: 1) **Transformer-based ViT** (Dosovitskiy et al., 2021) and 2) **Convolutional-based EfficientNetV2** (Tan & Le, 2021). For FFN, VGG, ResNet, ResNeXt, and DenseNet on ImageNet, we resize all the images into $256 \\\\times 256$ and then center-crop them into $224 \\\\times 224$. For ViT on CIFAR, we resize all the images into $224 \\\\times 224$, while $384 \\\\times 384$ for EfficientNetV2. We list hyper-parameter settings in Table 4. All the SGD optimizers are with a momentum of 0.9. For Adam/AdamW, we set $\\\\beta = (0.9, 0.999)$. For the learning rate in selected Imagenet, the milestones are [15, 30], while [30, 60] for the full-sized. We conduct all the experiments on a single A100 GPU.\"}"}
{"id": "53yQBJNQVJu", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DETAILS OF SIMILARITY MEASURE METHODS\\n\\n1) Structural SIMilarity (SSIM), which is a widely recognized method for measuring the similarity between two images (Wang et al., 2004). The SSIM index considers three measurements on the two images $x$ and $y$, including luminance, contrast, and structure. Following the settings of Wang et al. (2004), we formulate SSIM by:\\n\\n$$SSIM(x, y) = \\\\alpha \\\\cdot c \\\\cdot s$$ (29)\\n\\nWe set the weights $\\\\alpha$, $\\\\beta$, and $\\\\gamma$ all to 1.\\n\\n2) Mutual Information (MI), which is a measure of the mutual dependence between the two variables in information theory. Following (Studholme et al., 1999), we calculate MI for image matching. Given the signal intensity in an image, it evaluates how well the signal in the other image will be predicted. Compared with other measurements, MI has fewer restrictions on image modality and alignment. We implement MI by applying the normalized mutual information function in scikit-learn toolkit and normalizing MI to the interval 0 to 1.\\n\\n3) Peak Signal-to-Noise Ratio (PSNR) is a frequently used metric for image quality comparison between two images, especially in the area of image compression. PSNR computes the mean-square error of the compressed and the original image and further calculates the peak error by:\\n\\n$$PSNR = 10 \\\\log_{10} \\\\frac{R^2}{MSE(x, y)}$$ (30)\\n\\nwhere $R$ is the maximum fluctuation in the image data type. In our experiment, we normalized the images, took the MSE of the images in the worst-case group one-to-one, and set the $R$ to 1.\"}"}
{"id": "53yQBJNQVJu", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Results of NMMD-attack variants. Since NTK requires gradient norm during few-shot set search, we implement different networks. FNN-attack and ResNeXt-attack show good attack performance. Due to its simplicity, we adopt FNN-attack by default in the following experiments.\\n\\nTable 2: The attack results (FFN-attack) on pre-trained models. The abbreviations follow Table 1. As we can see, pre-trained models have better defense performance on 500-shot cases but also suffer from poor robustness on 50-shot cases.\\n\\n| Datasets Models     | Average-case NMMD-attack | Test Acc. | Gap | Train Acc. | Test Acc. | Train Acc. | Test Acc. | Abs. Rel. |\\n|--------------------|--------------------------|-----------|-----|------------|-----------|------------|-----------|-----------|\\n| CIFAR-10 (500-shot)| EffientNetV2-S           | 99.72 \u00b1 0.03 | 91.71 \u00b1 0.29 | 99.75 \u00b1 0.04 | 91.50 \u00b1 0.43 | 0.21 | 0.23 |\\n| ViT-B/16            | 100.00 \u00b1 0.00            | 97.14 \u00b1 0.14 | 100.00 \u00b1 0.00 | 96.97 \u00b1 0.13 | 80.57 \u00b1 1.72 | 8.40 | 9.44 |\\n| CIFAR-10 (50-shot) | EffientNetV2-S           | 99.88 \u00b1 0.10 | 68.79 \u00b1 1.48 | 100.00 \u00b1 0.00 | 65.36 \u00b1 2.66 | 3.43 | 4.97 |\\n| ViT-B/16            | 100.00 \u00b1 0.00            | 88.97 \u00b1 0.41 | 100.00 \u00b1 0.00 | 80.57 \u00b1 1.72 | 8.40 | 9.44 |\\n\\nPre-trained networks outperform randomly-initialized networks under the same worst-case few-shot. The first column in Table 2, CIFAR-10 (500-shot) shows performance of pre-trained networks on the few-shot set searched by FNN-attack. As we can see, pre-trained networks achieve much better robustness than randomly-initialized networks on 500-shot cases where the FFN-based NMMD-attack slightly deduces the few-shot learning ability of the pre-trained models. ViT-B/16 has 0.73% relative accuracy drop on CIFAR-10. It proves that large-scale pre-training can reduce the over-fitting to biased attributes and help robustness.\\n\\nPre-trained networks suffer from poor robustness on smaller few-shot sets. In particular, pre-trained models have solid performance on 500-shot cases. We are wondering if high performance can be generalized to smaller few-shot sets. We reduce the number of few-shot sets to 50-shot, where each label has 50 samples. As Table 2 illustrates, the FFN-based NMMD-attack consistently deduces few-shot performance on smaller few-shot sets. EfficientNetV2 has 4.97% relative accuracy drop and ViT-B/16 has 9.44% relative accuracy drop, respectively. In summary, NMMD-attack leads to performance deduction for various models, not only over-parameterized networks but also large pre-trained models. Moreover, we report our experiments for one-shot models in Appendix D.1. Our worst-case evaluation poses a higher challenge to defense such attacks for few-shot learning.\\n\\n7 DISCUSSION\\n\\n7.1 HOW IS THE QUALITY OF THE ADVERSARIAL FEW-SHOT SETS?\\n\\nWe first explore the instance similarity in the worst-case few-shot sets generated by NMMD-attack. To this end, we analyze the instance similarity via three metrics: 1) Structural Similarity (SSIM), a widely recognized method for measuring the similarity between two images (Wang et al., 2004); 2) Mutual Information (MI), a measure between two variables in information theory; 3) Peak signal-to-noise ratio (PSNR), a measure between super resolved image and the original one. Details of similarity measures can be found at Appendix C.\\n\\nThe searched few-shot sets are natural and diverse. Table 3 shows the instance similarity in the worst-case few-shot sets and random few-shot sets. As we can see, the searched sets are natural and diverse, and close to the instance similarity in the randomly sampled few-shot set. This indicates that our approach cannot be replaced by trivial implementations, such as aggregating a group of similar instances or picking out noise instances. Appendix G shows sampled instances from the searched few-shot set.\"}"}
{"id": "53yQBJNQVJu", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: The similarity analysis on the searched few-shot sets and randomly-sampled few-shot sets.\\n\\nFor SSIM and MI, lower scores represent better diversity. For PSNR, higher scores represent better diversity. The searched few-shot sets show competitive diversity scores compared with randomly-sampled sets.\\n\\n|                      | FFN-attack | VGG-attack | ResNet-attack | ResNeXt-attack | DenseNet-attack | Average-case |\\n|----------------------|------------|------------|---------------|----------------|-----------------|---------------|\\n| SSIM                 | \u2193 0.06     | \u2193 0.12     | \u2193 0.18        | \u2193 0.20         | \u2193 0.12          | \u2193 0.12        |\\n| MI                   | \u2193 0.44     | \u2193 0.44     | \u2193 0.37        | \u2193 0.35         | \u2193 0.46          | \u2193 0.44        |\\n| PSNR                 | \u2191 54.57    | \u2191 57.25    | \u2191 56.18       | \u2191 58.04        | \u2191 56.03         | \u2191 57.43       |\\n\\nFigure 4: Case study of spurious correlation: Sky Background $\\\\rightarrow$ airplane. Specifically, $P(S|A)$ refers to the number of airplane images with a sky background correctly judged, and so on. This spurious correlation is witnessed in the classification results of the model trained on a random-chosen few-shot set. The NMMD-attack results barely show this correlation.\\n\\n7.2 EXPLAINING THE EFFECTIVENESS OF NMMD-ATTACK WITH SPURIOUS CORRELATION\\n\\nSpurious correlation occurs as a statistical phenomenon, whereas confounders in data can be used to perform inference. These confounding attributes are often superficial features like background or lightning and are easy to capture. Here we compute the conditional probability of the spurious attribute to labels. All probabilities are statistically estimated by counting the figures with or without the hand-annotated spurious attribute. As shown in Figure 4, airplanes in the CIFAR-10 dataset appear more common in a sky background than other classes, and thus models are likely to learn the spurious correlation \\\"figure of airplanes $\\\\rightarrow$ sky background $\\\\rightarrow$ planes\\\". These spurious correlations between training and test sets thus bring a high performance that is yet hard to generalize. However, our worst-case few-shot set on CIFAR-10 picks several samples for each class as a few-shot set, all with an almost plain white background. This forbids models from learning to classify airplanes based on the sky background. The essential features are much harder to learn, resulting in much lower accuracy. Similar cases are common in various datasets. Other cases on Imagenet-1K include Bamboo Leaves $\\\\rightarrow$ Small Panda, Water Background $\\\\rightarrow$ Drake, and Human Activity $\\\\rightarrow$ Paddle, as shown in Appendix F. All spurious correlation experiments are based on ResNeXt-attack set and finetuned with VGG model.\\n\\n8 CONCLUSION\\n\\nThis paper proposes a worst-case evaluation to re-examine neural networks as few-shot learners by constructing a label-balanced subset from a full-size training set that results in the largest expected risks. An efficient method NMMD-attack is proposed to optimize the target in this work. Experiments show that NMMD-attack can find natural and diverse few-shot sets that successfully attack various architectures, even pre-trained models. The quantitative analysis gives several case studies to understand how spurious correlations between training and test sets affect few-shot evaluation. We find that the searched few-shot sets have fewer spurious attributes than randomly-sampled few-shot sets, which can explain why the searched few-shot set is more challenging. The worst-case evaluation re-examines the actual ability of neural networks on few-shot cases and also brings new problems to defense such attack for better robustness. Our work is still limited in the computer vision domain and we intend to apply to natural language tasks in the future. We also intend to further explore the negative impact of spurious attribute on general benchmarking and extend our theoretical analysis to the impact of distribution shift on spurious correlation.\"}"}
{"id": "53yQBJNQVJu", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 Experiment Reproducibility: We have included the code, data, and instructions to reproduce the experimental results. The codes and searched few-shot sets are available in the supplementary materials. We also record the gradient norms in the materials, which enables a quick re-implementation. Models and hyper-parameter settings are described in 6. For each result, we run experiments with different random seeds and report the average accuracy with standard deviation.\\n\\n\u2022 Theory Reproducibility: The proofs of all propositions, lemmas, and theorems are included in A. A few theorems and lemmas referenced from other works are provided with careful citation.\\n\\nREFERENCES\\n\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. CoRR, 2022a.\\n\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022b.\\n\\nSanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. Proc. of NeurIPS, 2019.\\n\\nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 2010.\\n\\nLuca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differentiable closed-form solvers. arXiv preprint arXiv:1805.08136, 2018.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proc. of NeurIPS, 2020.\\n\\nMinghao Chen, Shuai Zhao, Haifeng Liu, and Deng Cai. Adversarial-learned loss for domain adaptation. In Proc. of AAAI, 2020.\\n\\nXiuyuan Cheng and Yao Xie. Neural tangent kernel maximum mean discrepancy. Proc. of NeurIPS, 2021.\\n\\nJeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In Proc. of ICML, 2019.\\n\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proc. of CVPR, 2009.\\n\\nGuneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classification. arXiv preprint arXiv:1909.02729, 2019.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In Proc. of ICLR, 2021.\\n\\nSimon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In Proc. of ICML, 2019.\"}"}
{"id": "53yQBJNQVJu", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020.\\n\\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.\\n\\nArthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A kernel two-sample test. The Journal of Machine Learning Research, 2012.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. of CVPR, 2016.\\n\\nKaiming He, Ross Girshick, and Piotr Doll\u00e1r. Rethinking imagenet pre-training. In Proc. of ICCV, 2019.\\n\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: decoding-enhanced bert with disentangled attention. In Proc. of ICLR, 2021.\\n\\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proc. of CVPR, 2017.\\n\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proc. of ICML, 2015.\\n\\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran S. Haque, Sara M. Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In Proc. of ICML, 2021.\\n\\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Master's thesis, Department of Computer Science, University of Toronto, 2009.\\n\\nDaniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shafieezadeh-Abadeh. Wasserstein distributionally robust optimization: Theory and applications in machine learning. In Operations research and management science in the age of analytics. 2019.\\n\\nYann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proc. of the IEEE, 1998.\\n\\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.\\n\\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\\n\\nSubhabrata Mukherjee, Xiaodong Liu, Guoqing Zheng, Saghar Hosseini, Hao Cheng, Greg Yang, Christopher Meek, Ahmed Hassan Awadallah, and Jianfeng Gao. CLUES: few-shot learning evaluation in natural language understanding. CoRR, 2021.\\n\\nAndrew Mutton, Mark Dras, Stephen Wan, and Robert Dale. GLEU: automatic evaluation of sentence-level fluency. In Proc. of ACL, 2007.\\n\\nMateusz Ochal, Massimiliano Patacchiola, Amos Storkey, Jose Vazquez, and Sen Wang. Few-shot learning with class imbalance. arXiv preprint arXiv:2101.02523, 2021.\\n\\nBoris N. Oreshkin, Pau Rodr\u00edguez L\u00f3pez, and Alexandre Lacoste. TADAM: task dependent adaptive metric for improved few-shot learning. In Proc. of NeurIPS, 2018.\\n\\nArchit Parnami and Minwoo Lee. Learning from few examples: A summary of approaches to few-shot learning. arXiv preprint arXiv:2203.04291, 2022.\\n\\nXingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proc. of ICCV, 2019.\"}"}
{"id": "53yQBJNQVJu", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neural networks have achieved remarkable performance on various few-shot tasks. However, recent studies reveal that existing few-shot models often exploit the spurious correlations between training and test sets, achieving a high performance that is hard to generalize. Motivated by a fact that a robust few-shot learner should accurately classify data given any valid training set, we consider a worst-case few-shot evaluation that computes worst-case generalization errors by constructing a challenging few-shot set. Specifically, we search for the label-balanced subset of a full-size training set that results in the largest expected risks. Since the search space is enormous, we propose an efficient method NMMD-attack to optimize the target by maximizing NMMD distance (maximum mean discrepancy based on neural tangent kernel). Experiments show that NMMD-attack can successfully attack various architectures. The large gap between average performance and worst-case performance shows that neural networks still suffer from poor robustness. We appeal to more worst-case benchmarks for better robust few-shot evaluation.\"}"}
{"id": "53yQBJNQVJu", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: An illustration of worst-case evaluation. Spurious correlations affect few-shot evaluation. Since training and test data usually come from the same distribution, over-fitting to spurious features brings performance increase but does not bring better robustness. For example, models learn to classify pig based on superficial features (e.g., in a fence), rather than the shape. In this work, we consider a worst case evaluation for few-shot robustness evaluation by extracting a challenging and label-balanced subset from a full-size training set with the largest expected risk.\\n\\nTherefore, we adopt a distribution divergence maximization approach NMMD-attack to find the most challenging few-shot set. This approach is also theoretically guaranteed as the generalization error of a few-shot set is bounded by the maximum mean discrepancy (MMD) distance (Gretton et al., 2012) between the few-shot distribution and the original training distribution. The goal of maximizing generalization error can then be simplified to maximizing the MMD distance between the few-shot set and the full-size training set. Following the MMD maximization principle, we use the MMD distance in the hypothesis space to define group (or set) distance. Since the MMD distance is still intractable, we borrow neural tangent kernel, an approximation to over-parameterized neural networks, to estimate MMD distance without optimization. Given the searched subset, we train models and report test accuracy.\\n\\nExperiments show that NMMD-attack challenges high few-shot performance in randomly-sampled cases. It can successfully attack various model architectures with large performance drops. For example, the performance of DenseNet-121 drops by 10.02% on the generated few-shot set. Also, our case study on ImageNet-1K and CIFAR-10 demonstrates that the generated few-shot sets show much fewer spurious attributes than randomly-sampled few-shot training sets.\\n\\nThis work re-examine the actual ability of representative neural networks on few-shot cases. The large performance drop indicates large improvement space in future work. Actually, the bias to spurious attributes can be a severe system bug and loophole for all few-shot learners. The attacker can manipulate sets with unseen correlations to destroy a model, which is hard to detect. How to avoid learning spurious features is a hopeful direction to improve few-shot robustness. Furthermore, compared with existing few-shot benchmarks, worst-case evaluation can provide a new view demonstrating how worse a model can be such that we can prepare backup plans in case of accidents in real-word applications. In this work, we provide a feasible solution to estimate generalization error bounds for few-shot cases, we appeal to more worst-case benchmarks for better few-shot evaluation in future.\\n\\n2 RELATED WORK\\n\\nIn this work, we review related topics, including adversarial attack, distribution shift, and distribution robustness optimization. Our work comes as a form of attack inspired by adversarial attack literature (Szegedy et al., 2013; Goodfellow et al., 2014). Adversarial attacks aim to fool neural networks while keeping innocuous to humans. This form of attack, though effective, alters each sample independently and ignores group correlations. Performing attacks mainly concerns making slight variations, e.g. adding noise to the sample. Since few-shot cases usually have serious spurious correlations between groups, the target of worst-case few-shot evaluation is to evaluate robustness to distribution shifts.\"}"}
{"id": "53yQBJNQVJu", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nDistribution shift naturally arises from real-world applications and has been widely used in robustness (Sagawa et al., 2019; 2021). Also, distribution discrepancy metrics, similar to the NMMD metric in our work, have been extensively used (Peng et al., 2019; Chen et al., 2020; Zhao et al., 2021) in predicting generalization performance under data shifts. Distributional robustness (Sinha et al., 2017) , though inspired by distribution shift, is a distinct direction. Distribution robustness requires models to minimize empirical error under any valid training set. Unlike attack methods (Madry et al., 2017; Cohen et al., 2019) that define their perturbation set with an $L^p$-ball, distributional robustness analyzes set robustness.\\n\\nOther works have also explored distribution shifts and adopted samples with spurious correlations as benchmarks. However, most studies (Sagawa et al., 2019; Taori et al., 2020b; Sagawa et al., 2021) rely on prior knowledge of spurious correlations, which limits its application to real-world tasks with complicated and unseen correlations. Existing benchmarks for few-shot learning, like Mini-Imagenet (Dhillon et al., 2019), CIFAR-FS (Bertinetto et al., 2018), Tiered-ImageNet (Ren et al., 2018), mainly focus on cross-class query and label shifts. A more practical setting is based on the idea of \u201cin context\u201d learning few-shot examples for large pre-trained models (Alayrac et al., 2022b; Yang et al., 2022; Tsimpoukelli et al., 2021) and is the field that suffers over-optimistic performances. Our few-shot robustness evaluation also follows this setting. We are the first to appeal to worst-case benchmarks.\\n\\n### 3 NOTATIONS\\n\\nFor a better description, we introduce the notations used in this paper. Let $X$ be the data space, $Y$ be the label space, and $H$ be the hypothesis space in our investigation $H: X \\\\rightarrow Y$. We consider classification tasks for simplification. The training set $X$ consists of $n$ samples $(x_i, y_i)_{i=1}^n \\\\sim P(X,Y)$. The model $f_\\\\theta \\\\in H$ maps input to prediction $Y \\\\in \\\\mathbb{R}^d$, where $d$ denotes the dimensions of the output and $\\\\theta$ represents the parameters of the model. To give a probabilistic view of the problem, the training distribution is denoted as $P(X,Y) = \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\delta(x_i, y_i)$, where $\\\\delta$ denotes the Dirac delta distribution concentrated on $(x_i, y_i)$. Consequently, $P(X) = \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\delta(x_i)$ and $P(Y) = \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\delta(y_i)$.\\n\\n### 4 FEW-SHOT ROBUSTNESS: THE WORST-CASE GENERALIZATION ERROR\\n\\nIn this work, we discuss the problem of few-shot robustness. In traditional few-shot learning, we construct a few-shot set and measure the performance given the fixed training set and test set. Here we are interested in if different few-shot training sets lead to large model performance degradation.\\n\\nInspired by the notion of robustness in distributional robust optimization (Sagawa et al., 2019; Kuhn et al., 2019), we aim to find a challenging few-shot training set $P_{\\\\text{few}}$ that maximizes the expected risk on the test set. For convenience, we focus on a data-driven few-shot distribution, where $P_{\\\\text{few}}$ is centered around empirical examples extracted from the original training set distribution.\\n\\nWe consider a few shot set with $k$ examples. The example index is defined as $I_k := \\\\{i_1, i_2, ...i_k\\\\} \\\\subset [n]$, and therefore $P_{\\\\text{few}}(X) = \\\\frac{1}{k} \\\\sum_{m=1}^k \\\\delta(X_m)$. For short, we also write $P_{\\\\text{few}}(X)$ as $P_{I_k}(X)$.\\n\\nLet $f$ be the model hypothesis and $Q$ as the test distribution, the empirical risk of $f$ w.r.t training set $P$ is $\\\\epsilon_P(f) = \\\\mathbb{E}_{x \\\\in P(X)}|\\\\delta(f(x)) - P(y|x)|$, and the expected risk of $f$ w.r.t. $Q$ is denoted as $\\\\epsilon_Q(f) = \\\\mathbb{E}_{x \\\\in Q(X)}|\\\\delta(f(x)) - Q(y|x)|$. We assume the original training sets and test sets are sampled i.i.d from a natural distribution. Then we can safely make Assumption. 4.1 that the full-size training set can teach a model with acceptable generalization errors.\\n\\n**Assumption 4.1.** (Generalization) Let $H: X \\\\rightarrow Y$ be the hypothesis space, then there exists a model $f$ satisfies $f \\\\in H$ and $\\\\epsilon_P(f) \\\\leq \\\\alpha$, $\\\\epsilon_Q(f) \\\\leq \\\\beta$, where $\\\\alpha$, $\\\\beta$ are constants approaching zero.\\n\\nIn a real-world setting, a few-shot dataset is usually constructed by choosing a few representative samples (Parnami & Lee, 2022; Gao et al., 2020). Data imbalance is preferably avoided so as not to inject bias (Ochal et al., 2021; Wang et al., 2020). It is thus logical to assume that the few-shot subset should match the balanced label distribution of the original training set, as stated in Assumption. 4.2.\\n\\n**Assumption 4.2.** (Label Alignment) Let $P_{I_k}(X,Y)$ be the few-shot set selected from $P(X,Y)$, then $I_k$ should satisfy $P(Y) = P_{\\\\text{few}}(Y)$, i.e, $\\\\frac{1}{n} \\\\sum_{i=1}^n \\\\delta(y_i) = \\\\frac{1}{k} \\\\sum_{m=1}^k \\\\delta(y_m)$. \\n\\n\"}"}
{"id": "53yQBJNQVJu", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Few-shot Robustness\\n\\nUnder these assumptions, the few-shot robustness problem we address asks to solve the worst-case problem denoted as,\\n\\n$$\\\\max_{I_k \\\\in \\\\mathcal{Q}} (f_{I_k})$$\\n\\nwhere $f_{I_k}$ is a hypothesis that satisfies $\\\\epsilon_{P_{I_k}}(f_{I_k}) \\\\leq \\\\alpha$, and $I_k$ satisfies\\n\\n$$\\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\delta_{Y_i} = \\\\frac{1}{k} \\\\sum_{m \\\\in I_k} \\\\delta_{Y_i,m}, i \\\\in [n], m \\\\in I_k, I_k \\\\subset [n].$$\\n\\n$\\mathcal{Q}$ is the uncertainty set for constraining this distributional robustness problem. Different few-shot training sets $X_{I_k}$ lead to varied generalization errors.\\n\\nTraditional few-shot evaluation usually adopts a fixed training set. Correlations between the training set and the test set largely decide the generalization errors on the test set. Higher performance does not represent better robustness due to the missing evaluation of the worst-case performance. To evaluate few-shot robustness in Eq. 1, the target is to find a subset $X_{I_k}$ to prevent models from learning biased features and increase the difficulty for generalization performance.\\n\\n5. NMMD-ATTACK FOR WORST-CASE FEW-SHOT EVALUATION\\n\\nThis section introduces our method for evaluating the worst-case generalization error by constructing a challenging few-shot set based on Eq. 1. The intuition of NMMD-ATTACK is based on eliminating the spurious correlation between few-shot training set and test set. Since most samples hold spurious attributes (Sagawa et al., 2019) and samples with the same spurious attributes tend to be more similar, it is more likely to find less biased subsets based on distribution discrepancy. Therefore, we propose a NMMD-attack approach to find the worst-case few-shot subset with the largest MMD discrepancy from the original training distribution, while maintaining the balance of labels.\\n\\nHere, we first prove the intuition that the worst-case can be achieved by maximizing the MMD distance between few-shot set and the original training set, and then introduce how to quantitatively evaluate the MMD distance using neural tangent kernel. Lastly, a practical solution is provided to generate the worst few-shot set. All proofs are available in the Appendix A.\\n\\n5.1 FEW-SHOT ROBUSTNESS EVALUATION WITH MMD MAXIMIZATION\\n\\nTo find the worst-case few-shot set, our goal is to solve the optimization problem stated in Eq. 1. Intuitively, the generalization errors increase when there are fewer correlations and lower distribution similarity between the few-shot set and test set. In many benchmarks and contests, the test set is not directly available, but we can use a well-rounded training set as a resource of natural data distribution. In this case, our optimization goal becomes maximizing the distribution distance between the few-shot set and original training set. Here we use MMD distance following Gretton et al. (2012).\\n\\n**Definition 5.1. (Distribution Distance)**\\n\\nLet $H$ be the family of mappings from $X \\\\rightarrow \\\\mathbb{R}^d$, and let $P$ and $Q$ be two distributions, then the MMD distance can be defined as,\\n\\n$$\\\\text{MMD}(H, P, Q) := \\\\sup_{f \\\\in H} (\\\\mathbb{E}_{x \\\\sim P}[f(x)] - \\\\mathbb{E}_{y \\\\sim Q}[f(y)]).$$\\n\\n(2)\\n\\nWe theoretically show that the MMD distance is an upperbound for the few-shot robustness metric.\\n\\n**Theorem 5.1. (Few-shot Robustness Measured by MMD Discrepancy)**\\n\\nLet $H$ be the hypothesis space $X \\\\rightarrow \\\\mathbb{R}^d$. $f_{I_k}$ is the empirical risk minimizer, and $f$ is the hypothesis that minimizes expected risk $\\\\epsilon_{Q}(f)$. Then\\n\\n$$\\\\epsilon_{Q}(f_{I_k}) \\\\leq \\\\epsilon_{Q}(f) + \\\\text{MMD}(P_{I_k}, P) + \\\\epsilon_{\\\\alpha} + t + \\\\epsilon_{H},$$\\n\\n(3)\\n\\nwhere $\\\\epsilon_{\\\\alpha}$, $t$ and $\\\\epsilon_{H}$ are small constants describing the error occurred in training, the sampling behavior of training distribution and the hypothesis space complexity. Details are shown in Appendix A.1\\n\\nThis theorem gives the intuition that a high discrepancy between the few-shot set and a training set results in high expected risk. Following statistics literature (Cheng & Xie, 2021), a biased empirical estimate of the MMD is obtained by replacing the population expectations with the empirical average computed on the samples. Then we have,\\n\\n$$\\\\text{MMD}(H, P, P_{I_k}) := \\\\sup_{f \\\\in H} \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} f(x_i) - \\\\frac{1}{m} \\\\sum_{m \\\\in I_k} f(x_{i,m}) \\\\right)$$\\n\\n(4)\\n\\n4\"}"}
{"id": "53yQBJNQVJu", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For over-parameterized non-linear models, calculating MMD in the hypothesis space is intractable. \\n\\nNeural Tangent Kernel (NTK), however, is a simple approximation to understand neural networks as a kernel regression that can be optimized in its Reproducing Kernel Hilbert Space. Following the convention in NTK literature, we consider the NTK kernel function defined for optimization step $t \\\\geq 0$ as:\\n\\n$$K_t(x, x') := \\\\langle \\\\nabla_\\\\theta f(\\\\theta(t), x), \\\\nabla_\\\\theta f(\\\\theta(t), x') \\\\rangle.$$  \\n\\n(5)\\n\\nThen the MMD distance could be deducted following kernel conventions (Gretton et al., 2012).\\n\\n**Proposition 5.1.** *(NMMD Distance)* The MMD static with NTK at time $t$ is,\\n\\n$$MMD_2(K_t(P, Q)) = \\\\int_X \\\\int_X K_t(x, x')(\\\\hat{p} - \\\\hat{p}')^2(x)(\\\\hat{p} - \\\\hat{p}')^2(x') dxdx'.$$\\n\\n(6)\\n\\nwhere $X$ and $X'$ denotes the support set for $P$ and $Q$.\\n\\nWhen the neural network approaches its infinite width limit, the NTK matrix stays constant during training (Arora et al., 2019; Du et al., 2019), i.e., $K_t$ equals to $K_0$. This property enables us to accurately estimate the MMD distance without costly training. Together with the approximate ability of NTK for neural network training (shown in Proposition A.1), we show that NMMD can be used as an approximation to the MMD distance.\\n\\n**Theorem 5.2.** *(NMMD Approximation)* Let $MMD_f$ be the MMD distance calculated with model $f \\\\in H$ and let $MMD_{K_0}$ be the MMD static w.r.t. the NTK kernel $K$. Assume $f$ has taken $t$ steps of training at a learning rate of $\\\\eta$, and $f$ is $L_f$-Lipschitz Continuous. Denote NTK at initialization as $K(x, x') = \\\\langle \\\\nabla_\\\\theta f(\\\\theta(0), x), \\\\nabla_\\\\theta f(\\\\theta(0), x) \\\\rangle$. If $k \\\\leq n^2$, then the two MMD statistics satisfies:\\n\\n$$MMD_f(P, P_{I_k}) = O(t\\\\eta \\\\cdot MMD_{K_0}(P, P_{I_k})).$$\\n\\n(7)\\n\\nThis theorem formally justifies using NTK kernel to calculate MMD distances. Compared with estimating MMD distance with neural networks, this method requires no training, and its calculation can be easily decomposed into kernel similarity $K(x_i, x_j)$ with the help of the kernel trick. The simplified distance would be,\\n\\n$$MMD_{K_0}(P, P_{I_k}) = \\\\frac{1}{n^2} \\\\sum_{i,k} K(x_i, x_k) + \\\\left(1 - \\\\frac{k^2}{4nk} \\\\right) \\\\sum_{s,t} K(x_i_s, x_{i_t}),$$\\n\\n(8)\\n\\nThis distance would achieve its maximum while maximizing the intra-set kernel similarity of the few-shot set and minimizing the inter-set kernel similarity.\\n\\n### 5.2 IMPLEMENTATION\\n\\nWe have simplified the task of finding the worst few-shot set to maximizing the NMMD distance between a few-shot distribution and the full-size training distribution, as shown in Eq. 8. However, there are several challenges in implementation: (1) The computational complexity for the NTK matrix on all training examples is $O(n^2)$, making it hard to scale to large-scale sets; (2) Optimizing the MMD distance requires solving a combinatorial problem of $O(n^k)$. In our work, we address these problems by taking advantage of the sparsity of NTK matrix.\\n\\n**Efficient Optimization of Eq. 8**\\n\\nIn experiments, we discovered the NTK is usually a sparse, diagonally dominant matrix. As shown in Figure 2, the diagonal terms $K(x_i, x_i)$ are more than 10x larger than cross terms $K(x_i, x_j)$. In this way, we can discard cross terms for simplification in Eq. 8.\\n\\n$$K(x_i, x_i)$$ is actually gradient norm $\\\\langle \\\\nabla_\\\\theta f(\\\\theta, x_i), \\\\nabla_\\\\theta f(\\\\theta, x_i) \\\\rangle_{i \\\\in [n]}$. Note that the gradient of model $f : X \\\\rightarrow \\\\mathbb{R}^d$ is $\\\\nabla_\\\\theta f(\\\\theta(t), x) \\\\in \\\\mathbb{R}^{p \\\\times d}$, $p$ is the number of parameters in $f$. The calculation of $K(x_i, x_i)$ can be written as,\\n\\n$$K(x_i, x_i) = \\\\langle \\\\nabla_\\\\theta f(\\\\theta, x)_i, \\\\nabla_\\\\theta f(\\\\theta, x)_i \\\\rangle = \\\\sum_{j=1}^{d} \\\\nabla^2 f(\\\\theta, x)_{ij}^2.$$  \\n\\n(9)\"}"}
{"id": "53yQBJNQVJu", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\nDetails of NMMD-Attack.\\n\\nInput: Full-size training set $X = \\\\{(x_i, y_i)\\\\}_{i=1}^n$, few-shot set size $k$, model $f$ with random parameters $\\\\theta$, initial parameters $\\\\theta_0$.\\n\\nOutput: The few-shot attack set.\\n\\n1: initialize $f_{\\\\theta}$ with $\\\\theta_0$\\n2: for $i = 1 : n$ do\\n3: $G_i \\\\leftarrow \\\\langle \\\\nabla_{\\\\theta} f_{\\\\theta}(x_i), \\\\nabla_{\\\\theta} f_{\\\\theta}(x_i) \\\\rangle$\\n4: end for\\n5: Sort examples in $X$ based on $G_i$\\n6: For each label, select top $k$ examples into the final few-shot set $X_{\\\\text{few}}$ to optimize Eq. 10\\n7: return $X_{\\\\text{few}}$\\n\\nFigure 2: Visualization of NTK matrix with 50 examples randomly sampled from CIFAR-10.\\n\\nAfter reducing the sparse terms in the NTK matrix, our new goal is to maximize the intra-set kernel similarity using the gradient norm terms:\\n\\n$$\\\\max_{I_k} \\\\sum_{s=1}^{k} K(x_s, x_s), \\\\ s \\\\in I_k$$ (10)\\n\\nImplementation Algorithm\\nGiven Eq. 10, the target of finding the worst-case few-shot set is simplified into finding the few-shot set with the maximum sum gradient norms. The algorithm details are shown in Algorithm 1. First, we rank all training samples w.r.t. their gradient norms, i.e., $K(x_{j_1}, x_{j_1}) > K(x_{j_2}, x_{j_2}) > \\\\ldots > K(x_{j_n}, x_{j_n})$, then we choose the top-k samples $(x_{j_s}, y_{j_s}) \\\\ s \\\\in [k]$ as our attack few-shot set. Last, we train models on the searched few-shot set and report its test accuracy.\\n\\n6 EXPERIMENTS\\nDatasets\\nWe explore the few-shot robustness on 4 image classification datasets, namely the CIFAR-10 dataset (Krizhevsky et al., 2009), the CIFAR-100 dataset (Krizhevsky et al., 2009), the MNIST dataset (LeCun et al., 1998) and the ILSVRC-2012 ImageNet dataset with 1K classes (Deng et al., 2009). We select a subset of the full-size training set as attack set. To build natural few-shot sets and guarantee the label alignment constraints, each label keeps $k$ corresponding instances in few-shot training set.\\n\\nModels and Hyper-parameters\\nTo evaluate the robustness of over-parameterized neural networks, we consider the following models: 1) FFN, a feed-forward neural networks; 2) VGG (Simonyan & Zisserman, 2014), a classical convolutional neural network; 3) ResNet (He et al., 2016), a residual neural network; 4) ResNeXt (Xie et al., 2017); 5) DenseNet (Huang et al., 2017). Besides, to verify the attack ability on pre-trained models, we also re-implement two pre-trained models: 1) Transformer-based ViT (Dosovitskiy et al., 2021) and 2) Convolutional-based EfficientNetV2 (Tan & Le, 2021). Appendix B provides detailed model description and hyper-parameter settings. For each result, we conduct $m$ experiments and report the mean and variance. $m = 5$ for MNIST, CIFAR-10, and CIFAR-100, and $m = 3$ for ImageNet.\\n\\nBaseline and Comparison\\nNMMD-attack finds the subset from the full-size training set to attack models. We implement an \u201caverage-case\u201d baseline that randomly samples few-shot sets from the full-size training set as a comparison. After training models on the few-shot group, we report the accuracy of the test sets for both methods. To make a fair comparison, NMMD-attack and the baseline have the same dataset size and label distribution. To be specific, we extract 10% data from the training set as a few-shot set. Each label has 500 examples in MNIST, 500 examples in CIFAR-10, 50 examples in CIFAR-100, 100 examples in ImageNet-1K. Since NMMD-attack requires gradient norms, we implement various networks to calculate gradient norms. It needs to notice that such gradients do not require a trained network. Only architecture and initialization are required.\"}"}
{"id": "53yQBJNQVJu", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: The comparison between average performance and attack (FFN-attack) performance. \\\"Acc.\\\" represents accuracy. \\\"Test Acc. Gap\\\" represents the gap between average performance and attack performance. \\\"Abs.\\\" represents absolute gap and \\\"Rel.\\\" represents relative gap. As we can see, NMMD-attack can successfully attack models with large performance drop.\\n\\n| Datasets | Models | Average-case | NMMD-attack | Test Acc. Gap | Abs. | Rel. |\\n|----------|--------|--------------|-------------|---------------|------|------|\\n|          | Train Acc. | Test Acc. | Train Acc. | Test Acc. | Abs. | Rel. |\\n| MNIST    | FFN 100.00 \u00b1 0.00 | 97.63 \u00b1 0.12 | 99.99 \u00b1 0.01 | 93.87 \u00b1 0.73 | 3.76 | 3.84 |\\n|          | VGG-16 100.00 \u00b1 0.00 | 98.73 \u00b1 0.13 | 100.00 \u00b1 0.00 | 76.47 \u00b1 0.65 | 22.26 | 22.55 |\\n|          | ResNet-18 100.00 \u00b1 0.00 | 98.65 \u00b1 0.05 | 100.00 \u00b1 0.00 | 75.41 \u00b1 0.54 | 23.24 | 23.55 |\\n|          | ResNeXt-29 100.00 \u00b1 0.00 | 98.42 \u00b1 0.10 | 100.00 \u00b1 0.00 | 70.64 \u00b1 0.77 | 27.78 | 28.23 |\\n| CIFAR-10 | FFN 100.00 \u00b1 0.00 | 49.38 \u00b1 0.47 | 100.00 \u00b1 0.00 | 42.29 \u00b1 0.73 | 7.09 | 14.36 |\\n|          | VGG-16 98.85 \u00b1 0.31 | 66.20 \u00b1 0.91 | 98.82 \u00b1 0.84 | 51.43 \u00b1 1.07 | 14.77 | 22.31 |\\n|          | ResNet-18 99.97 \u00b1 0.02 | 63.11 \u00b1 1.44 | 99.98 \u00b1 0.02 | 48.72 \u00b1 0.89 | 14.52 | 23.01 |\\n|          | ResNeXt-29 99.52 \u00b1 0.48 | 61.75 \u00b1 0.58 | 99.66 \u00b1 0.16 | 48.59 \u00b1 0.62 | 13.16 | 21.31 |\\n|          | DenseNet-121 100.00 \u00b1 0.00 | 71.19 \u00b1 0.87 | 99.64 \u00b1 0.18 | 54.76 \u00b1 1.56 | 16.43 | 23.08 |\\n| CIFAR-100| FFN 99.99 \u00b1 0.00 | 14.46 \u00b1 0.51 | 99.98 \u00b1 0.00 | 11.91 \u00b1 0.51 | 2.55 | 17.63 |\\n|          | VGG-16 100.00 \u00b1 0.00 | 27.15 \u00b1 0.65 | 100.00 \u00b1 0.00 | 14.35 \u00b1 0.12 | 12.80 | 47.15 |\\n|          | ResNet-18 100.00 \u00b1 0.00 | 24.91 \u00b1 0.20 | 100.00 \u00b1 0.00 | 14.27 \u00b1 0.29 | 10.64 | 42.71 |\\n|          | ResNeXt-29 100.00 \u00b1 0.00 | 23.88 \u00b1 0.42 | 100.00 \u00b1 0.00 | 14.54 \u00b1 0.40 | 9.34 | 39.11 |\\n|          | DenseNet-121 100.00 \u00b1 0.00 | 32.74 \u00b1 0.68 | 100.00 \u00b1 0.00 | 17.18 \u00b1 0.17 | 15.56 | 47.53 |\\n| ImageNet-1K| FFN 99.94 \u00b1 0.01 | 5.06 \u00b1 0.31 | 99.77 \u00b1 0.00 | 3.14 \u00b1 0.24 | 2.08 | 51.49 |\\n|          | VGG-16 98.11 \u00b1 0.02 | 14.22 \u00b1 0.24 | 97.97 \u00b1 0.05 | 9.74 \u00b1 0.50 | 2.54 | 40.71 |\\n|          | ResNet-18 99.96 \u00b1 0.00 | 30.06 \u00b1 0.29 | 99.72 \u00b1 0.00 | 23.66 \u00b1 0.42 | 6.93 | 38.01 |\\n|          | ResNeXt-50 99.94 \u00b1 0.01 | 37.97 \u00b1 0.09 | 99.66 \u00b1 0.05 | 26.51 \u00b1 0.50 | 9.72 | 37.28 |\\n|          | DenseNet-121 99.84 \u00b1 0.01 | 37.76 \u00b1 0.73 | 99.61 \u00b1 0.02 | 28.49 \u00b1 0.36 | 10.02 | 37.63 |\"}"}
