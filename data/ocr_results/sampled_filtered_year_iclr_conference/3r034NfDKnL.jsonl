{"id": "3r034NfDKnL", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The role of learning regime, architecture, and dataset structure on systematic generalization in neural networks\\n\\nAnonymous authors\\n\\nPaper under double-blind review\\n\\nAbstract\\n\\nHumans often systematically generalize in situations where standard deep neural networks do not. Empirical studies have shown that the learning procedure and network architecture can influence systematicity in deep networks, but the underlying reasons for this influence remain unclear. Here we theoretically study the acquisition of systematic knowledge by simple neural networks. We introduce a minimal space of datasets with systematic and non-systematic features in both the input and output. For shallow and deep linear networks, we derive learning trajectories for all datasets in this space. The solutions reveal that both shallow and deep networks rely on non-systematic inputs to the same extent throughout learning, such that even with early stopping, no networks learn a fully systematic mapping. Turning to the impact of architecture, we show that modularity improves extraction of systematic structure, but only achieves perfect systematicity in the trivial setting where systematic mappings are fully segregated from non-systematic information. Finally, we analyze iterated learning, a procedure in which generations of networks learn from languages generated by earlier learners. Here we find that networks with output modularity successfully converge over generations to a fully systematic 'language' starting from any dataset in our space. Our results contribute to clarifying the role of learning regime, architecture, and dataset structure in promoting systematic generalization, and provide theoretical support for empirical observations that iterated learning can improve systematicity.\\n\\nIntroduction\\n\\nHumans frequently display the ability to systematically generalize, that is, to leverage specific learning experiences in diverse new settings (Lake et al., 2019). For instance, exploiting the approximate compositionality of natural language, humans can combine a finite set of words or phonemes into a near-infinite set of sentences, words, and meanings. Someone who understands \\\"brown dog\\\" and \\\"black cat\\\" also likely understands \\\"brown cat,\\\" to take one example from Szab\u00f3 (2012). The result is that a human's ability to reason about situations or phenomena extends far beyond their ability to directly experience and learn from all such situations or phenomena.\\n\\nDeep learning techniques have made great strides in tasks like machine translation and language prediction, providing proof of principle that they can succeed in quasi-compositional domains. However, these methods are typically data hungry and the same networks often fail to generalize in even simple settings when training data are scarce (Lake & Baroni, 2018; Lake et al., 2019). Empirically, the degree of systematicity in deep networks is influenced by many factors. One possibility is that the learning dynamics in a deep network could impart an implicit inductive bias toward systematic structure (Hupkes et al., 2020); however, a number of studies have identified situations where depth alone is insufficient for structured generalization (Lake & Baroni, 2018; Niklasson & Sharkey, 1992; Pollack, 1990; Phillips & Wiles, 1993). Another significant factor is architectural modularity, which can enable a system to generalize when modules are appropriately configured (Vani et al., 2021; Phillips, 1995). However, identifying the right modularity through learning remains challenging (Bahdanau et al., 2019). A third possibility builds on Iterated Learning (IL), a method in which generations of agents train briefly on a 'language' produced by their parent, and then generate a new language for their child (Kalish et al., 2007). If systematic components are easier to learn\"}"}
{"id": "3r034NfDKnL", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"than non-systematic ones, this process can successively refine a language toward a systematic structure, a process which has been hypothesized to be the cause of the compositional nature of natural language (Kirby, 2001; Kirby et al., 2008). In spite of these (and many other) possibilities for improving systematicity (Hupkes et al., 2020), it remains unclear when standard deep neural networks will exhibit systematic generalization (Dankers et al., 2021), reflecting a long-standing theoretical debate stretching back to the first wave of connectionist deep networks (Rumelhart & McClelland, 1986; Fodor & Pylyshyn, 1988; Smolensky, 1991; Hadley, 1994).\\n\\nIn this work we theoretically investigate the acquisition of systematic knowledge by simple neural networks (NNs). We introduce a simple space of datasets that contain systematic and non-systematic features, and examine the impact of implicit biases, architectural modularity, and iterated learning on the learned input-output mappings of shallow and deep linear networks. In particular,\\n\\n- We derive exact training dynamics for shallow and deep linear networks as a function of the dataset parameters.\\n- We show that for all datasets in the space, despite the possibility of learning a fully systematic mapping, neither shallow nor deep networks do so under gradient descent dynamics.\\n- We show that modular network architectures can learn fully systematic network mappings, but only when the modularity segregates systematic and non-systematic features.\\n- We show that iterated learning can converge to a fully systematic 'language' when combined with the weaker architectural constraint of output modularity.\\n\\nIn Section 7 we consider how our findings, which rely on a simplified setting for mathematical tractability, generalize to more complicated datasets and non-linear architectures by training a convolutional neural network to label handwritten digits between 0 and 999. Overall, our results help clarify the diverse factors impacting systematic behavior in neural networks, and suggest that iterated learning can improve systematicity.\\n\\n2 BACKGROUND\\n\\nSystematic generalization has been proposed as a key feature of intelligent learning agents which can generalize to novel stimuli in their environment (Hockett & Hockett, 1960; Fodor & Pylyshyn, 1988; Hadley, 1993; Kirby et al., 2015; Lake et al., 2017). In particular, the closely related concept of compositional structure has been shown to have benefits for both learning speed (Ren et al., 2019) and generalizability (Lazaridou et al., 2018). There are, however, counter-examples which find only a weak correlation between compositionality and generalization (Andreas, 2018) or learning speed (Kharitonov & Baroni, 2020). In most cases neural networks do not manage to generalize systematically (Ruis et al., 2020), or systematic generalization occurs only with the addition of modular architectures, explicit regularizers or a degree of supervision of the learned features.\\n\\nNeural Module Networks (NMNs) (Andreas et al., 2016; Hu et al., 2017; 2018) have become one successful method of creating network architectures which generalize systematically. By (jointly) training individual neural modules on particular subsets of data or to perform particular subtasks, the modules will specialize. These modules can then be combined in new ways when an unseen datapoint is input to the model. Thus, through the composition of the modules, the model will systematically generalize, assuming that the correct modules can be structured together. Bahdanau et al. (2019) show, however, that strong regularizers are required for the correct module structures to be learned. Thus, without regularizers, compositional mappings do not emerge with NMNs.\\n\\nIterated learning (IL) approaches suggest that systematicity can emerge over generations of agents that learn from each other. In some works, based primarily on the topological structure of language (Brighton & Kirby, 2006), compositional structure does emerge from IL (Ren et al., 2019). When paired with NMNs, IL has previously been used to refine the module structures to be compositional (Vani et al., 2021) by treating the module structure as an output \\\"language\\\". Then each \\\"word\\\" in the language represents a particular module composed to form a \\\"sentence\\\" which describes the model architecture (the relative arrangement of the modules in Polish notation). Thus, by phrasing the problem as a language learning task, IL is able to refine this language of architectures to become modular. This work also highlights the fact that IL is applicable outside of traditional language learning tasks.\"}"}
{"id": "3r034NfDKnL", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\n(a) Example of a systematicity preference in language.\\n\\n(b) Dataset with the settings $n_x = 3$, $n_y = 2$, $k_x = 1$, $k_y = 1$ and $r = 2$.\\n\\nFigure 1: Problem setting and dataset space. (a) Upon viewing an object, an agent might extract various input features, mapping these to output features or \\\"words\\\" in order to refer to the item (Brighton & Kirby, 2006). (b) We schematize this setting with a space of datasets containing systematic ($\\\\Omega$) and non-systematic ($\\\\Gamma$) features in the input (left panel) and output (middle panel). Rows contain examples and columns contain features. In this case cars are named with both a systematic component (based on features: presence of stripe, colour and number of doors) and non-systematic component (based on brand). In iterated learning, the names of objects change from the initial labels over generations until they stabilize at the refined final labels (right panel, $\\\\hat{\\\\Omega}^y$, $\\\\hat{\\\\Gamma}^y$).\\n\\nIL has also been successfully used with NNs as part of a larger pipeline. In particular Seeded Iterated Learning (SIL) (Lu et al., 2020a) uses IL to counter language drift. Language drift occurs when a pretrained language model is then used to complete a language based task. While training on the task, the model begins to lose structure in the learned language that is not useful for performing the task. By training a \\\"teacher\\\" model on task completion and then using the teacher to supervise a pretrained \\\"student\\\", Lu et al. (2020a) show that the student network is able to learn to complete the task while maintaining the initially learned language. SIL is extended by Lu et al. (2020b) to include supervised selfplay in the teacher task-learning step. This is known as Supervised SIL (SSIL). Thus, parts of the data used to pretrain the teacher are also replayed while the teacher learns to perform the task. This also aids in avoiding language drift. While these previous works reflect the influence of certain initial data properties, regularizers or complex architectures on the emergence of compositional structure, the necessity of structured architectures with modular designs remains unclear.\\n\\n3 A SPACE OF DATASETS WITH COMBINATORIAL SYSTEMATICITY\\n\\nThe notion of systematic generalization is broad, and has been assessed using a variety of datasets and paradigms (Hupkes et al., 2020). Here we introduce a simple setting reminiscent of learning to refer to objects in an environment, as depicted in Fig. 1. For instance, we might see a particular car (Fig. 1a), extracting various perceptual input features such as whether it has a stripe, is red, or has a particular hood ornament. This scenario affords several ways in which we could refer to the car. We could systematically map individual input features to individual outputs by saying a sentence like \\\"My car is the red one with the white stripe;\\\" or we could choose a single word to refer to some combination of properties, saying something like \\\"My car is the Ferrari.\\\" This task has a type\"}"}
{"id": "3r034NfDKnL", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nof combinatorial systematic structure, because individual input features can link independently to individual output features. More generally, an agent first observes an object and forms an internal \\\"logical form\\\". The task is then to map from this logical form to a \\\"phonetic form\\\" by providing a name for each object (Brighton & Kirby, 2006). As illustrated in this example, real world settings often allow redundant expressions and mappings, such that the same object could be individuated with a variety of expressions that vary in systematicity.\\n\\nTo formalize this setting, we define a parametric space of datasets with input and output matrices $X = [\\\\Omega x \\\\Gamma x]_T$ and $Y = [\\\\Omega y \\\\Gamma y]_T$ respectively, where $n_x, n_y, k_x, k_y, r \\\\in \\\\mathbb{R}^+$ are the parameters that define a specific dataset. Figure 1b visualizes one dataset in the space. The systematic input feature matrix $\\\\Omega_x \\\\in \\\\{-1, 1\\\\}^{n_x \\\\times 2n_x}$ consists of all binary patterns with $n_x$ bits. Here $n_x$ is a key parameter determining the number of bits in the systematic input structure. Overall, the dataset contains $2^{n_x}$ examples. The systematic output feature matrix $\\\\Omega_y \\\\in \\\\{-1, 1\\\\}^{n_y \\\\times 2n_x}$ is a sampling of $n_y$ features (rows) from $\\\\Omega_x$, and is the systematic component of the output matrix. Intuitively, the binary combinatorial structure in $\\\\Omega_x$ and $\\\\Omega_y$ is meant to reflect systematic mappings from input features (the color red or blue) to output features (the words \\\"red\\\" or \\\"blue\\\"). Next, the non-systematic input feature matrix $\\\\Gamma_x = [rI_1 \\\\ldots rI_{k_x}]$ consists of $k_x$ scaled identity matrices, $I_i \\\\in \\\\{0, 1\\\\}^{2n_x \\\\times 2n_x}$. Similarly, the non-systematic output matrix $\\\\Gamma_y = [rI_1 \\\\ldots rI_{k_y}]$ has $k_y$ scaled identity matrices, with scale factor $r$. These identity matrices provide a single feature for each pattern which is only on for that pattern. Intuitively, these features are meant to reflect idiosyncratic features (like hood ornament, a distinctive scratch, or a proper noun name) or features reflecting specific nonlinear combinations of systematic features (red-striped-2-door). Together $k_x, k_y$ and $r$ control the frequency and intensity (for example the size or perceptual prominence of the brand's logo) of the non-systematic features, which are both factors that can promote non-compositional language being used by humans (Rogers et al., 2004).\\n\\nThis space of datasets is consistent with numerical notions of compositionality in previous works (Andreas, 2018), which define compositionality as a homomorphism between the observation space and the naming space (since the input-output mappings are linear they are homomorphic). The amount of systematic structure can be titrated by adjusting $n_x$ and $n_y$; and the prevalence of non-systematic structure by adjusting $k_x, k_y$, and $r$. Importantly, and novel to our analysis, datasets in this space allow redundant solutions: the systematic output features can be generated based on systematic input features alone, but they can equally be generated using non-systematic features alone, or some mixture of the two. Exploiting this fact, we now ask how reliance on systematic or non-systematic structure is influenced by implicit biases, architecture, and learning regime.\\n\\n**4 LEARNING DYNAMICS IN SHALLOW AND DEEP LINEAR NETWORKS**\\n\\nThe generalization abilities of deep networks depend on a complex interplay of learning dynamics, architecture, initialization, and dataset statistics. In this section we ask how the implicit inductive bias in gradient descent influences systematicity. We consider training both shallow and deep linear networks on datasets in our space. While deep linear networks can only represent linear input-output mappings, the dynamics of learning change dramatically with the introduction of one or more hidden layers (Fukumizu, 1998; Saxe et al., 2014; 2019; Arora et al., 2018; Lampinen & Ganguli, 2019), and the learning problem becomes non-convex (Baldi & Hornik, 1989). They therefore serve as a tractable model of the influence of depth specifically on learning dynamics, which prior work has shown to impart a low-rank inductive bias on the linear mapping (Huh et al., 2021).\\n\\nWe leverage known exact solutions to the dynamics of learning from small random weights in deep linear networks (Saxe et al., 2014; 2019) to describe the full learning trajectory analytically for every dataset in our space. In particular, consider a single hidden layer network computing output $\\\\hat{y} = W_2 W_1 x$ in response to an input $x$, trained to minimize the mean squared error loss using full batch gradient descent with small learning rate $\\\\epsilon$ (full details and technical assumptions given in Appendix A). The network's total input-output map after $t$ epochs of training is\\n\\n$$W_2(t) W_1(t) = UA(t) V_T,$$\\n\\nwhere $A(t)$ is a diagonal matrix of singular values. The dynamics of $A(t)$, as well as the orthogonal matrices $U$ and $V$, depend on the singular value decomposition of the input- and input-output correlations in the dataset. If the input- and input-output correlations can be expressed as\\n\\n$$\\\\Sigma_x = \\\\mathbb{E}[XX^T] = V D V_T,$$\\n\\n$$\\\\Sigma_{yx} = \\\\mathbb{E}[YX^T] = U S V_T$$\\n\\n(2)\"}"}
{"id": "3r034NfDKnL", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nwhere $U$ and $V$ are orthogonal matrices of singular vectors and $S,D$ are diagonal matrices of singular values/eigenvalues, then the diagonal elements of $A(t) = \\\\pi(\\\\alpha(t))$ evolve through time as\\n\\n$$\\\\pi(\\\\alpha(t)) = \\\\lambda(\\\\alpha)/\\\\delta(\\\\alpha) \\\\left( 1 - \\\\delta(\\\\alpha) \\\\pi(0) \\\\exp\\\\left(-2\\\\lambda(\\\\alpha) \\\\tau t\\\\right) \\\\right),$$\\n\\nwhere $\\\\lambda$ and $\\\\delta$ are the associated input-output singular value and input eigenvalue ($S_{\\\\alpha}$ and $D_{\\\\alpha}$ respectively), $\\\\pi(0)$ denotes the singular value at initialization, and $\\\\tau = 1/2nx$ is the learning time constant. These dynamics describe a trajectory which begins at the initial value $\\\\pi(0)$ when $t = 0$ and increases to $\\\\lambda(\\\\alpha)/\\\\delta(\\\\alpha)$ as $t \\\\to \\\\infty$.\\n\\nIn essence, the network's total input-output mapping at all times in training is a function of the singular value decomposition of the dataset statistics. We therefore analytically obtained this decomposition in terms of the dataset parameters for our space of datasets. We find that there are three distinct input-output singular values which we denote $\\\\lambda_1, \\\\lambda_2, \\\\lambda_3$; two distinct input singular values $\\\\delta_1, \\\\delta_2$; and therefore three asymptotes $\\\\pi_{ss_1}, \\\\pi_{ss_2}, \\\\pi_{ss_3}$.\\n\\nSubstituting these expressions into the dynamics yields the full learning trajectories for all datasets in the space. We note that each distinct singular value occurs multiple times in the dataset: $\\\\lambda_1$ has multiplicity $n_x - n_y$, $\\\\lambda_2$ has multiplicity $n_y$, and $\\\\lambda_3$ has multiplicity $2n_x - n_x$. Full derivations, including explicit expressions for singular vectors, are deferred to Appendix B.\\n\\nA similar derivation for a shallow network (no hidden layer) shows that the singular values of the model's mapping follow the trajectory\\n\\n$$\\\\pi(\\\\alpha(t)) = \\\\lambda(\\\\alpha)/\\\\delta(\\\\alpha) \\\\left( 1 - \\\\exp\\\\left(-\\\\delta(\\\\alpha) t/\\\\tau\\\\right) \\\\right) + \\\\pi(0) \\\\exp\\\\left(-\\\\delta(\\\\alpha) t/\\\\tau\\\\right),$$\\n\\nsuch that the time course depends on the singular values of the input covariance matrix, $\\\\Sigma_x$. The unique singular values are\\n\\n$$\\\\delta_1 = (k_x r^2 + 2n_x)^{1/2},$$\\n\\n$$\\\\delta_2 = k_x r^2/2n_x.$$\\n\\nWe empirically verify these equations by simulating the full training dynamics for deep and shallow linear networks trained using gradient descent on an instantiation from the space of datasets in Figure 2. While training, we compute the singular values of the network after each epoch of training. These simulations of the training dynamics for each unique singular value are then compared to the predicted dynamics. We see close agreement between the predicted and simulated trajectories.\\n\\n4.1 THE EVOLUTION OF SYSTEMATICITY OVER LEARNING\\n\\nTo understand the extent to which a network comes to rely on systematic or non-systematic input features, and the timing with which it produces systematic or non-systematic output features, we calculate the Frobenius norm of the input-output mapping between different subsets of features. In particular we partition the input-output mapping into four components: systematic inputs to systematic outputs; non-systematic inputs to systematic outputs; systematic inputs to non-systematic outputs; and non-systematic outputs to non-systematic outputs. Analytical expressions for these norms over training (which rely on both singular value dynamics and the structure of the singular...\"}"}
{"id": "3r034NfDKnL", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Analytical learning dynamics for deep (panels a-b) and shallow (panels c-d) linear networks. (a,c) Comparison of predicted and actual singular value trajectories over learning, for the three unique dataset singular values. (b,d) Comparison of predicted and actual Frobenius norms of the input-output mapping to/from systematic (\\\\(\\\\Omega_x, \\\\Omega_y\\\\)) and non-systematic (\\\\(\\\\Gamma_x, \\\\Gamma_y\\\\)) features. Deep networks show distinct stages of improvement over learning. However, at no point is a purely systematic mapping learned.\\n\\nParameters: \\\\(n_x = 3, n_y = 1, k_x = 3, k_y = 1, r = 1\\\\).\\n\\nVectors) are given in Appendix D Eqns. 17-20 due to space constraints. Figure 2b,c depicts these dynamics for one specific dataset. Crucially, we find that, first, systematic outputs are generated using substantial contributions from non-systematic inputs, regardless of network depth (see \\\\(\\\\Gamma_x \\\\Omega_y\\\\) curve); and second, contributions to non-systematic outputs \\\\(\\\\Gamma_y\\\\) arise simultaneously with contributions to systematic outputs \\\\(\\\\Omega_y\\\\) (all curves initially rise together). Hence perfectly systematic mappings never arise at any point in the learning process, either for shallow or deep networks. For these datasets in which a given mapping can be implemented using either systematic or non-systematic features, the implicit bias in learning dynamics settles on a mixed solution.\\n\\nWhile the results in Figure 2 are for one specific dataset, our analytical results offer more general insight throughout our space of datasets. We note that in the deep network, the timescale of learning each unique singular value is roughly \\\\(O(1/\\\\lambda)\\\\), such that larger singular values are learned faster (Saxe et al., 2019). In Appendix A we show that \\\\(\\\\lambda_1 > \\\\lambda_2 > \\\\lambda_3\\\\) for any setting of the dataset parameters, and \\\\(\\\\pi_{ss1} \\\\geq \\\\pi_{ss3} > \\\\pi_{ss2}\\\\) assuming that \\\\(k_x \\\\geq k_y\\\\) (i.e., the non-systematic output dimension is not larger than the non-systematic input dimension). Further, the three classes of singular vectors associated with each singular value occur due to specific properties of the dataset. \\\\(\\\\lambda_1\\\\), and by extension \\\\(\\\\pi_1\\\\), arises from the systematic input and output features, and it makes contributions to all four norms. \\\\(\\\\lambda_2\\\\) occurs due to the left-out systematic output features, and contributes only to the norms of the mappings to non-systematic outputs. Lastly, \\\\(\\\\lambda_3\\\\) occurs because of the non-systematic features, and appears only in the norm from non-systematic inputs to non-systematic outputs. Thus, for all datasets in the space, the mixed systematic/non-systematic singular value mode \\\\(\\\\pi_1\\\\) is the fastest singular value to learn, while the purely non-systematic singular value \\\\(\\\\pi_3\\\\) is the second largest but slowest to learn by the network.\\n\\nIn the shallow network, \\\\(\\\\delta_1\\\\) drives the dynamics of the mixed systematic/non-systematic mode and \\\\(\\\\delta_2\\\\) contributes to the non-systematic mappings. We can show that for any setting of the dataset parameters \\\\(\\\\delta_1 > \\\\delta_2\\\\) and so the mixed systematic/non-systematic mapping is again learned faster.\"}"}
{"id": "3r034NfDKnL", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Impact of architectural biases. Architectures that partition systematic and non-systematic features in different ways with the corresponding graphical representation of the resulting network mappings. The dynamical modes $\\\\pi_1$, $\\\\pi_2$, and $\\\\pi_3$ contain contributions from systematic and non-systematic input components, depicted as the color of the bottom half of each mode; and they make contributions to systematic and non-systematic output features, depicted as the color of the top half of each mode (red for systematic, green for non-systematic, and orange for mixed). To learn a systematic mapping, the fastest mode $\\\\pi_1$ must be systematic. The output partitioned network is able to obtain output systematicity but not input systematicity. Only the fully partitioned network obtains complete systematicity. However, because the shallow dynamics exhibit exponential approach to their asymptote, both modes are learned at overlapping times. In sum, a part of the non-systematic mapping will be learned at the same speed as the systematic mapping, for any setting of our dataset. Thus, there is no epoch at which training could be stopped that would completely remove mappings from the non-systematic component of the input and output. In our setting, the implicit bias arising from depth, small random initialization, and gradient descent is insufficient to learn fully systematic mappings for any datasets in this space.\\n\\n5. MODULARITY AND NETWORK ARCHITECTURE\\n\\nWe now turn to modularity and network architecture, another prominent approach for promoting systematicity in a network's mapping (Vani et al., 2021; Bahdanau et al., 2019). Architectures such as Neural Module Networks (Andreas et al., 2016; Hu et al., 2017; 2018) learn reconfigurable modules that implement specific aspects of a larger problem. By rearranging existing modules to process a novel input, they can generalize far beyond their training set. Here we investigate whether simple forms of additional architectural structure can yield strong enough inductive biases to learn fully systematic mappings.\\n\\nIn particular, instead of a dense network, we consider architectures in which systematic and non-systematic features are processed in separate processing streams, as depicted in Figure 3. The learning dynamics for several of these different settings can be obtained by combining the dynamics of other points in the space of datasets. To illustrate the approach, consider the output-partitioned network. The pathway leading to the systematic outputs effectively learns from a dataset with $k_y = 0$, and the pathway leading to the non-systematic outputs effectively learns from a dataset with $n_y = 0$. By combining these dynamics, we can reconstruct the full solution, as shown in Figure 8 of Appendix E. We note that this approach cannot be taken for the input partitioned network, which has interactions in the output layer. We summarize our findings with the graphical representations of the network mappings in Figure 3. The same three modes drive learning dynamics, but they contain subtly different mixtures of contributions from systematic and non-systematic inputs and outputs. Notably, the output-partitioned network exhibits output systematicity but not input systematicity, a significant point that we will return to in the iterated learning section. However, only the fully partitioned network achieves full systematicity, such that early stopping could completely prune non-systematic features. Hence we find that architectural biases can enforce systematicity, but only in the heavy-handed and relatively trivial case where the bias perfectly segregates systematic and non-systematic information.\"}"}
{"id": "3r034NfDKnL", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Iterated learning dynamics. (a) Generations of agents learn from languages generated by their parent, and pass on their acquired language to their children. (b) Norm of non-systematic output mapping over 20 generations of the IL procedure using deep, shallow and output-split networks. Dashed line shows the best early-stopping can achieve for the starting dataset (stopping at epoch 30 with the output-split network). Even though the networks are run for 40 epochs per generation, IL is still able to out-perform early stopping with the output-split network. It is important to note that the dense and split networks maintain the original systematic mapping by epoch 40, and these results show the closest each network can get to removing the non-systematic component under this restriction. The shallow network is unable to fully maintain the systematic mapping with any form of IL, thus the decrease in its norm is at the expense of the systematic mapping. Parameters: \\\\( n_x = 3, n_y = 2, k_x = 1, k_y = 1 \\\\) and \\\\( r = 2 \\\\).\\n\\nFinally, we consider the impact of iterated learning on extracting systematic structure, alongside architectural partitioning. In iterated learning, the focus is on refining the output 'language' toward systematicity over generations of learners (Kirby, 2001; Kalish et al., 2007), as illustrated in Figure 1b middle and right panel. Each generation learns from the language acquired by the previous generation (Figure 4a). To instantiate this setting, we start from a particular dataset in our space, but halt training before full convergence after a pre-defined number of training steps. We then use the network's output (logits) as the target outputs for the next generation. Throughout learning, the network's input-output mapping takes the same form as the dataset's singular value decomposition, but with different singular values. Therefore, the generated language always has the same singular vectors, but with different singular values corresponding to the amount of learning progress made by a given agent. This fact permits straightforward analysis of iterated learning dynamics throughout our space of datasets.\\n\\nFigure 4b depicts the refinement of the output language over generations in networks with different architectures. In particular, iterated learning is effective in reducing or eliminating the non-systematic output features, and can do so more effectively than carefully chosen early-stopping on the original dataset. We make several observations. First, while deep and shallow networks do not learn perfectly systematic mappings in the first generation, the deep network does learn a relatively more systematic mapping, such that IL yields more rapid benefits. Second, iterated learning in dense networks can reduce non-systematic structure relative to early stopping, but does not eliminate it. Third, and most notably, iterated learning can exploit weak architectural priors: with optimal early stopping, the output partitioned network does not fully remove non-systematic output features, but over generations of iterated learning it completely eliminates non-systematic output structure. This effect arises because the systematic and non-systematic mappings depend on entirely different modes. Since \\\\( \\\\pi_1 \\\\) is used by the systematic mapping and is learned the fastest for the space of datasets, in fact IL in the output-partitioned network completely removes the non-systematic mapping for any dataset in the space of datasets. Whereas early-stopping has varying performance over the space of datasets, IL with the output-split network will always be able to converge to a systematic language.\"}"}
{"id": "3r034NfDKnL", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OMPOSITIONAL MNIST (CMNIST)\\n\\nTo evaluate how well our results generalize to non-linear networks and more complex datasets, in this section we train a deep Convolutional Neural Network (CNN) to learn a compositional variant of MNIST (CMNIST). In this dataset, three digits from MNIST are stacked horizontally, resulting in a value between 0 and 999. The systematic output encodes each digit in a 10-way one-hot vector, resulting in a 30-dimensional vector. The non-systematic output encodes the number as a whole with a 1000-dimensional one-hot vector. This task is similar to the SVHN dataset (Netzer et al., 2011) with systematic and non-systematic output labels.\\n\\nWe compare results of using a single dense CNN and a split CNN, in which two parallel sets of convolutional layers with half the convolutional filters of the dense network each that connect separately to systematic and non-systematic labels. Full details of the two network architectures and hyper-parameters used for the CMNIST experiments are given in Appendix G. The effect predicted by our theoretical work is that the redundancy in the labels will interfere with the network\u2019s learned hidden representations and decrease the systematic generalizability of the network. This prediction is shown to be true in Figure 5, which shows the mean-squared error for the systematic and non-systematic network predictions over the course of training, normalized so that the initial error is at 1.0.\\n\\nFirstly, in dense networks the error of the systematic mapping is tied to the error of the non-systematic mapping. This is seen in Figure 5(a) where the blue curve cannot converge until the orange curve has become sufficiently low. This effect is not observed with the split network. Secondly, comparing Figure 5(a) and 5(b) we demonstrate the lack of generalization which occurs when using non-systematic features. This is seen as the orange and red curves achieving a lower training error while the test error increases. Lastly, again by comparing Figure 5(a) and 5(b), we see that the systematic mapping of the split architecture is the only mapping which generalizes well (near 0 training and test error). Thus, even in this more complex setting, we see the negative effect a shared hidden layer has on the generalization of the network (comparing the blue and green curves).\\n\\nDISCUSSION\\n\\nIn this work we have theoretically and empirically studied the ability of simple NNs to acquire systematic knowledge. We found that this ability is challenging even in our simple setting. Neither implicit biases in learning dynamics, nor all but the most stringent modularity, caused networks to ignore non-systematic inputs. However, iterated learning\u2014when combined with weaker architectural priors\u2014can converge to strongly systematic output features. Our results complement recent empirical studies, helping to highlight the complex factors influencing systematic generalization. We hope greater understanding of these phenomena will ultimately aid the design of improved learning systems that can leverage specific learning in diverse new ways.\"}"}
{"id": "3r034NfDKnL", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nFigure 9: Dense network architecture trained to perform the CMNIST classification task.\\n\\nFigure 10: Split network architecture trained to perform the CMNIST classification task.\"}"}
{"id": "3r034NfDKnL", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the network\u2019s hidden layer will become worse for generalization, even for the learned systematic mapping. In contrast, the split network architecture sees no generalization gap and maintains a near-zero test error from early on in the training. Thus, it is clear that the benefit of using the split network architecture is that it avoids the conflict in its hidden layers from learning to accommodate both the systematic and non-systematic mappings.\"}"}
{"id": "3r034NfDKnL", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\n$n \\\\times k \\\\times r > 0$ is true by definition since $n, k, r \\\\in \\\\mathbb{R}^+$ and, thus, $\\\\lambda_2 > \\\\lambda_3$ for all points in our space of datasets. Thus, using the transitivity of inequality: $\\\\lambda_1 > \\\\lambda_2 > \\\\lambda_3$ for all points in the space of datasets.\\n\\n**ingular Value Decomposition Equations**\\n\\nIn this section we provide the general formulas for the singular value decomposition of the $\\\\Sigma_x$ and $\\\\Sigma_{yx}$ covariance matrices for any dataset in the space of datasets. As stated in Section 4 the right singular vectors of $\\\\Sigma_{yx}$ must match the singular vectors of $\\\\Sigma_x$ which is the $V$ matrix below. Thus, $\\\\Sigma_x = VD\\\\Sigma$ and $\\\\Sigma_{yx} = USV\\\\Sigma^T$.\\n\\nLet:\\n\\n\\\\[\\nA = (\\\\Omega_y \\\\Omega_x^T)\\\\Sigma_x \\\\Omega_y \\\\Omega_x^T\\n\\\\]\\n\\n\\\\[\\nB = \\\\Omega_y^T \\\\Omega_y \\\\Omega_x \\\\Omega_x^T\\n\\\\]\\n\\n\\\\[\\nC = \\\\Omega_x^T \\\\Omega_x \\\\Omega_x \\\\Omega_x^T\\n\\\\]\\n\\n\\\\[\\nTHP \\\\Sigma^T = \\\\left( \\\\begin{array}{cc} k_x & k_y \\\\\\\\ \\\\end{array} \\\\right)^{1/2} I_{2nx \\\\times 2nx} - \\\\left( \\\\begin{array}{cc} \\\\frac{1}{2} k_x & \\\\frac{1}{2} k_y \\\\\\\\ \\\\end{array} \\\\right) \\\\Omega_x \\\\Omega_x^T\\n\\\\]\\n\\nWhere $THP \\\\Sigma^T$ is the SVD of $\\\\left( \\\\begin{array}{cc} k_x & k_y \\\\\\\\ \\\\end{array} \\\\right)^{1/2} I_{2nx \\\\times 2nx} - \\\\left( \\\\begin{array}{cc} \\\\frac{1}{2} k_x & \\\\frac{1}{2} k_y \\\\\\\\ \\\\end{array} \\\\right) \\\\Omega_x \\\\Omega_x^T$. Then the following are the matrix formulas for the components of the SVD for $\\\\Sigma_{yx}$ and $\\\\Sigma_x$.\\n\\n\\\\[\\nU = \\\\begin{bmatrix}\\n\\\\left( \\\\begin{array}{cc} \\\\frac{1}{2} k_x & \\\\frac{1}{2} k_y \\\\\\\\ \\\\end{array} \\\\right) \\\\\\\\\\n\\\\Omega_x \\\\Omega_x^T\\n\\\\end{bmatrix}\\n\\\\]\\n\\n\\\\[\\nV^T = \\\\begin{bmatrix}\\n\\\\left( \\\\begin{array}{cc} \\\\frac{1}{2} k_x & \\\\frac{1}{2} k_y \\\\\\\\ \\\\end{array} \\\\right) \\\\\\\\\\n\\\\Omega_x \\\\Omega_x^T\\n\\\\end{bmatrix}\\n\\\\]\\n\\n\\\\[\\nS = \\\\begin{bmatrix}\\n\\\\left( \\\\begin{array}{cc} k_x & k_y \\\\\\\\ \\\\end{array} \\\\right)^{1/2} I_{2nx \\\\times 2nx} - \\\\left( \\\\begin{array}{cc} \\\\frac{1}{2} k_x & \\\\frac{1}{2} k_y \\\\\\\\ \\\\end{array} \\\\right) \\\\Omega_x \\\\Omega_x^T\\n\\\\end{bmatrix}\\n\\\\]\\n\\n\\\\[\\nD = \\\\begin{bmatrix}\\n\\\\left( \\\\begin{array}{cc} k_x & k_y \\\\\\\\ \\\\end{array} \\\\right)^{1/2} I_{2nx \\\\times 2nx} - \\\\left( \\\\begin{array}{cc} \\\\frac{1}{2} k_x & \\\\frac{1}{2} k_y \\\\\\\\ \\\\end{array} \\\\right) \\\\Omega_x \\\\Omega_x^T\\n\\\\end{bmatrix}\\n\\\\]\\n\\n**S** **HALLOW** **NETWORK** **SIMULATIONS**\\n\\nIn this section we show the simulations of a shallow network trained on an instance from the space of datasets. We again see strong agreement between the predicted and simulated time-courses for the training dynamics. Importantly for our findings in Section 4, the modes of variation for a shallow network are all learned at roughly the same time. This can be seen in Figure 6 where the time-courses, and by extension norms, are all learned simultaneously. Thus, adding depth to a network is necessary for IL to be even partially effective at removing non-systematic output labels while maintaining the systematic outputs.\\n\\n**INPUT AND OUTPUT PARTITIONED FROBENIUS NORMS**\\n\\nWe partition the input-output mapping along the systematic and non-systematic input and output components. The time-courses for these norms can be seen in Equations 17 to 20. From these equations we see that the mappings to both components of the output rely on all inputs. Thus, the non-systematic inputs still offer some benefit to the systematic output and are used in the systematic mapping. Likewise the systematic inputs are used for the non-systematic mapping.\"}"}
{"id": "3r034NfDKnL", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Under review as a conference paper at ICLR 2022.\\n\\n- **Learning trajectories of the three unique Singular Values learned by the shallow network.**\\n- **Time paths of the shallow network mappings.**\\n\\nFigure 6: Singular Values and Frobenius norms of the shallow network mappings.\\n\\n- **Systematic output norm partitioned by the systematic and non-systematic input components.**\\n- **Non-systematic output norm partitioned by systematic and non-systematic input components.**\\n\\nFigure 7: Frobenius Norm of the systematic and non-systematic deep network mapping partitioned by the systematic and non-systematic inputs on a dense linear network.\\n\\n\\\\[\\n\\\\Gamma_x - \\\\Omega_y - \\\\text{Norm} = \\\\left(2n_x n_y + 2n_x\\\\right)\\\\left(k_x r^2 + 2n_x\\\\pi^2 t\\\\right)^{1/2}\\n\\\\]\\n\\n\\\\[\\n\\\\Omega_x - \\\\Gamma_y - \\\\text{Norm} = \\\\left(2n_x k_y n_y + 2n_x\\\\right)\\\\left(k_x r^2 + 2n_x\\\\pi^2 t\\\\right)^{1/2} + 2n_x\\\\left(n_x - n_y\\\\right)\\n\\\\]\\n\\n\\\\[\\n\\\\Gamma_x - \\\\Gamma_y - \\\\text{Norm} = \\\\left(k_x k_y n_y + 2n_x\\\\right)\\\\left(k_x r^2 + 2n_x\\\\pi^2 t\\\\right)^{1/2} + \\\\left(2n_x - n_x\\\\right)\\\\pi^2 t^{3/2}\\n\\\\]\\n\\nFigure 7 shows the simulated and predicted training dynamics for these norms on a deep linear network. Figure 7a reflects that the mapping to the systematic output relies evenly on both the systematic and non-systematic inputs, with a slight preference to the systematic inputs. Likewise, Figure 7b reflects that the mapping to the non-systematic outputs also uses the systematic and non-systematic inputs evenly, with a preference towards the non-systematic inputs towards the end of training.\"}"}
{"id": "3r034NfDKnL", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\n(a) Learning trajectories of the three unique Singular Values learned by the output-split network.\\n(b) Time paths of the split network mappings\\n(c) Phase diagram of the split network vs shallow network Frobenius norms\\n\\nFigure 8: Singular Values and Frobenius norms of the split network mappings\\n\\nWe can also consider the Frobenius norm from the full input to either the systematic or non-systematic components of the output (obtained by summing over input features). This takes a simpler form,\\n\\n$$\\\\Omega_y \\\\text{-Norm} = \\\\left( \\\\pi_1^2 t + \\\\frac{n_x \\\\pi_2^2}{1 + \\\\frac{n_x}{n_y} \\\\pi_2^2} \\\\right)^{1/2}$$  \\\\hspace{1cm} (21)\\n\\n$$\\\\Gamma_y \\\\text{-Norm} = \\\\left( \\\\pi_1^2 t + \\\\frac{n_x \\\\pi_2^2}{1 + \\\\frac{n_x}{n_y} \\\\pi_2^2} \\\\right)^{1/2}$$  \\\\hspace{1cm} (22)\\n\\nIn Figure 8 we depict the Frobenius norm of the mapping from the entire input to the systematic and non-systematic output, for the output-partitioned network. The dynamics of this case can equally be seen as using one neural module responsible for the systematic mapping and another for the non-systematic mapping. Now, from the perspective of the non-systematic mapping, none of the systematic features in the input are present in the output, and the mapping will no longer be dependent on $\\\\pi_1$. The systematic mapping is still only dependent on the $\\\\pi_1$ effective singular value. The time-courses for the resultant Frobenius norms are given in Equations 23 and 24.\\n\\n$$\\\\Omega_y \\\\text{-Norm} = \\\\left( \\\\pi_1^2 t + \\\\frac{n_x \\\\pi_2^2}{1 + \\\\frac{n_x}{n_y} \\\\pi_2^2} \\\\right)^{1/2}$$  \\\\hspace{1cm} (23)\\n\\n$$\\\\Gamma_y \\\\text{-Norm} = \\\\left( \\\\pi_1^2 t + \\\\frac{n_x \\\\pi_2^2}{1 + \\\\frac{n_x}{n_y} \\\\pi_2^2} \\\\right)^{1/2}$$  \\\\hspace{1cm} (24)\\n\\nWe now investigate the case where the split network architecture does not perfectly partition the output into the systematic and non-systematic components. In this case some of the non-systematic identity output blocks are grouped with the systematic outputs (we only consider partitioning along the systematic and non-systematic blocks to keep the closed form solutions tractable). Thus, we separate the number of non-systematic outputs $k_y$ into the number of non-systematic outputs of the left network branch $k_{\\\\text{left}}_y$ and right network branch $k_{\\\\text{right}}_y$, such that $k_{\\\\text{left}}_y + k_{\\\\text{right}}_y = k_y$. We also examine the mapping from the full input to aid in readability.\\n\\nWith this setup three distinct Frobenius norms emerge, as shown in Equations 25, 26, and 27. In the extreme cases when $k_{\\\\text{left}}_y = k_y$ we recover the dense network equations shown in Equations 21 and 22, since $\\\\pi_2 = \\\\pi_3 = 0$ for Equation 27. This is apparent from the singular value equations for $\\\\pi_2$ and $\\\\pi_3$ shown in Equations 7 and 9 with $k_y$ in the numerator which is replaced by $k_{\\\\text{right}}_y$ in this case. Thus,\"}"}
{"id": "3r034NfDKnL", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Equation 27 falls away completely. In the other extreme case of $k_{\\\\text{right}} y = k_{\\\\text{left}} y$ we recover the split network equations shown in Equations 23 and 24. This is again because $\\\\pi_2 = \\\\pi_3 = 0$ but this time from the left network branch\u2019s perspective. By definition $k_{\\\\text{left}} y = 0$ in this case and so all components of Equation 26 fall away, leaving just Equations 25 and 27.\\n\\n$$\\\\Omega_y - \\\\text{Norm} = \\\\left( 2 n_x n_y \\\\pi_1 (t) + 2 n_x \\\\right)^{1/2}$$ (25)\\n\\n$$\\\\Gamma_{y-\\\\text{left}} - \\\\text{Norm} = \\\\left( n_x \\\\pi_1 (t) + (2 n_x - n_x) \\\\pi_2 \\\\right)^{1/2}$$ (26)\\n\\n$$\\\\Gamma_{y-\\\\text{right}} - \\\\text{Norm} = \\\\left( 2 n_x \\\\pi_3 (t) + (2 n_x - n_x) \\\\pi_3 \\\\right)^{1/2}$$ (27)\\n\\nIn this section we provide the network architectures for the CMNIST experiments as well as other details of the experimental setup. We scale the non-systematic output labels to help the network learn these labels. For the results of this section a scale of 10 was applied, however, the results are consistent for a wide range of scale values. Increasing or decreasing the scale merely changes the time taken for the same effects to occur. Both the dense and split networks are trained using stochastic gradient descent from random initial weights sampled from an isotropic normal distribution. No regularization, learning rate decay or momentum is used. We aim to keep the setup as simple as possible while reducing the effects of other implicit or explicit regularizers on the results, since we are comparing the systematic generalization of the networks. The simplicity also aids the comparison between the dense and split network architectures which is the goal of the experiment. Table 1 shows the hyper-parameters used to train both networks, which have the same hyper-parameters, and the two architectures are shown in Figures 9 (dense architecture) and 10 (split architecture).\\n\\nTable 1: Table showing the hyper-parameters used for the CMNIST experiments.\\n\\n| Hyper-parameter      | Value |\\n|----------------------|-------|\\n| Step Size            | $2 \\\\times 10^{-3}$ |\\n| Batch Size           | 16    |\\n| Initialization Variance | 0.1  |\"}"}
{"id": "3r034NfDKnL", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To aid the reproducibility of this work, we have included all source code used to obtain the results in the supplementary material as well as instructions on using the code. In this work we introduced the space of datasets used in Sections 4 and 5, for our theoretical analysis with linear networks, as well as the Compositional MNIST dataset. Included in the source code are methods of constructing the space of datasets and for loading the CMNIST dataset. No pre-processing steps are used on either dataset before being used to train the networks. In Appendix G we have also included all hyper-parameters and network architectures used for the CMNIST experiments. Finally, for our theoretical results, the full details and technical assumptions can be found in Appendix A.\\n\\nREFERENCES\\n\\nJacob Andreas. Measuring compositionality in representation learning. In International Conference on Learning Representations, 2018.\\n\\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 39\u201348, 2016.\\n\\nS. Arora, N. Cohen, and E. Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. 35th International Conference on Machine Learning, ICML 2018, 1:372\u2013389, 2018. arXiv: 1802.06509 ISBN: 9781510867963.\\n\\nDzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron Courville. Systematic generalization: What is required and can it be learned? In International Conference on Learning Representations, 2019.\\n\\nP. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural Networks, 2(1):53\u201358, January 1989. ISSN 0893-6080. doi: 10.1016/0893-6080(89)90014-2. URL http://linkinghub.elsevier.com/retrieve/pii/0893608089900142.\\n\\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\\n\\nHenry Brighton and Simon Kirby. Understanding linguistic evolution by visualizing the emergence of topographic mappings. Artificial life, 12(2):229\u2013242, 2006.\\n\\nVerna Dankers, Elia Bruni, and Dieuwke Hupkes. The paradox of the compositionality of natural language: a neural machine translation case study. arXiv preprint arXiv:2108.05885, 2021. URL https://arxiv.org/abs/2108.05885.\\n\\nJerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3\u201371, 1988.\\n\\nK. Fukumizu. Effect of Batch Learning In Multilayer Neural Networks. In Proceedings of the 5th International Conference on Neural Information Processing, pp. 67\u201370, 1998.\\n\\nRobert F Hadley. Connectionism, explicit rules, and symbolic manipulation. Minds and machines, 3(2):183\u2013200, 1993.\\n\\nRobert F Hadley. Systematicity in connectionist language learning. Mind & Language, 9(3):247\u2013272, 1994.\\n\\nCharles F Hockett and Charles D Hockett. The origin of speech. Scientific American, 203(3):88\u201397, 1960.\\n\\nRonghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason: End-to-end module networks for visual question answering. In Proceedings of the IEEE International Conference on Computer Vision, pp. 804\u2013813, 2017.\"}"}
{"id": "3r034NfDKnL", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nRonghang Hu, Jacob Andreas, Trevor Darrell, and Kate Saenko. Explainable neural computation via stack neural module networks. In Proceedings of the European conference on computer vision (ECCV), pp. 53\u201369, 2018.\\n\\nMinyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The low-rank simplicity bias in deep networks. arXiv preprint arXiv:2103.10427, 2021.\\n\\nDieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: How do neural networks generalise? Journal of Artificial Intelligence Research, 67:757\u2013795, 2020.\\n\\nMichael L Kalish, Thomas L Griffiths, and Stephan Lewandowsky. Iterated learning: Intergenerational knowledge transmission reveals inductive biases. Psychonomic Bulletin & Review, 14(2):288\u2013294, 2007.\\n\\nEugene Kharitonov and Marco Baroni. Emergent language generalization and acquisition speed are not tied to compositionality. arXiv preprint arXiv:2004.03420, 2020.\\n\\nSimon Kirby. Spontaneous evolution of linguistic structure\u2014an iterated learning model of the emergence of regularity and irregularity. IEEE Transactions on Evolutionary Computation, 5(2):102\u2013110, 2001.\\n\\nSimon Kirby, Hannah Cornish, and Kenny Smith. Cumulative cultural evolution in the laboratory: An experimental approach to the origins of structure in human language. Proceedings of the National Academy of Sciences, 105(31):10681\u201310686, 2008.\\n\\nSimon Kirby, Monica Tamariz, Hannah Cornish, and Kenny Smith. Compression and communication in the cultural evolution of linguistic structure. Cognition, 141:87\u2013102, 2015.\\n\\nBrenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In Proceedings of the 35th International Conference on Machine Learning, pp. 4487\u20134499. International Machine Learning Society (IMLS), 2018.\\n\\nBrenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40, 2017.\\n\\nBrenden M Lake, Tal Linzen, and Marco Baroni. Human few-shot learning of compositional instructions. In Proceedings of the 41st Annual Conference of the Cognitive Science Society, 2019.\\n\\nA.K. Lampinen and S. Ganguli. An analytic theory of generalization dynamics and transfer learning in deep linear networks. In T. Sainath (ed.), International Conference on Learning Representations, 2019. ISBN 0311-5518. doi: 10.1080/03115519808619195. URL http://arxiv.org/abs/1809.10374. arXiv: 1809.10374.\\n\\nAngeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. Emergence of linguistic communication from referential games with symbolic and pixel input. In International Conference on Learning Representations, 2018.\\n\\nYuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering language drift with seeded iterated learning. In International Conference on Machine Learning, pp. 6437\u20136447. PMLR, 2020a.\\n\\nYuchen Lu, Soumye Singhal, Florian Strub, Olivier Pietquin, and Aaron Courville. Supervised seeded iterated learning for interactive language learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 3962\u20133970, 2020b.\\n\\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.\\n\\nLars Niklasson and Noel Sharkey. Systematicity and generalisation in connectionist compositional representations. Citeseer, 1992.\"}"}
{"id": "3r034NfDKnL", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nSteven Phillips and Janet Wiles. Exponential generalizations from a polynomial number of examples in a combinatorial domain. In Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan), volume 1, pp. 505\u2013508. IEEE, 1993.\\n\\nSteven Andrew Phillips. Connectionism and the problem of systematicity. PhD thesis, University of Queensland, 1995.\\n\\nJordan B Pollack. Recursive distributed representations. Artificial Intelligence, 46(1-2):77\u2013105, 1990.\\n\\nYi Ren, Shangmin Guo, Matthieu Labeau, Shay B Cohen, and Simon Kirby. Compositional languages emerge in a neural iterated learning model. In International Conference on Learning Representations, 2019.\\n\\nTimothy T Rogers, James L McClelland, et al. Semantic cognition: A parallel distributed processing approach. MIT press, 2004.\\n\\nLaura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, and Brenden M Lake. A benchmark for systematic generalization in grounded language understanding. Advances in Neural Information Processing Systems, 33, 2020.\\n\\nD. E. Rumelhart and J. L. McClelland. On Learning the Past Tenses of English Verbs, volume II, pp. 216\u2013271. MIT Press, Cambridge, MA, USA, 1986. ISBN 0262132184.\\n\\nA.M. Saxe, J.L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In Y. Bengio and Y. LeCun (eds.), International Conference on Learning Representations, Banff, Canada, 2014. Oral presentation. arXiv: 1312.6120v3.\\n\\nAndrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic development in deep neural networks. Proceedings of the National Academy of Sciences, 116(23):11537\u201311546, 2019.\\n\\nPaul Smolensky. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2):159\u2013216, 1990.\\n\\nPaul Smolensky. Connectionism, constituency, and the language of thought. In Barry M. Loewer and Georges Rey (eds.), Meaning in Mind: Fodor and His Critics. Blackwell, 1991.\\n\\nZoltan Szabo. The case for compositionality. The Oxford handbook of compositionality, 64:80, 2012.\\n\\nAnkit Vani, Max Schwarzer, Yuchen Lu, Eeshan Dhekane, and Aaron Courville. Iterated learning for emergent systematicity in vqa. In International Conference on Learning Representations, 2021.\"}"}
{"id": "3r034NfDKnL", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The learning dynamics for shallow and deep linear networks are derived in Saxe et al. (2019). We state full details of our specific setting here. We train a linear network with one hidden layer to minimize the quadratic loss $L(W_1, W_2) = \\\\frac{1}{2n} ||Y - W_2 W_1 X||^2_2$ using gradient descent. This gives the learning rules $E[\\\\Delta W_1] = \\\\epsilon W_2^T (Y - W_2 W_1 X) X^T$ and $E[\\\\Delta W_2] = \\\\epsilon n_x (Y - W_2 W_1 X)(W_1 X)^T$. By using a small learning rate $\\\\epsilon$ and taking the continuous time limit, the mean change in weights is given by $\\\\tau \\\\frac{d}{dt} W_1 = W_2^T (\\\\Sigma_{yx} - W_2 W_1 \\\\Sigma_x)$ and $\\\\tau \\\\frac{d}{dt} W_2 = (\\\\Sigma_{yx} - W_2 W_1 \\\\Sigma_x) W_1^T$ where $\\\\Sigma_x = E[XX^T]$ is the input correlation matrix, $\\\\Sigma_{yx} = E[YX^T]$ is the input-output correlation matrix and $\\\\tau = \\\\frac{1}{2n} \\\\epsilon$. Here, $t$ measures units of learning epochs.\\n\\nIt is helpful to note that since we are using a small learning rate the full batch gradient descent and stochastic gradient descent dynamics will be the same. Saxe et al. (2019) has shown that the learning dynamics depend on the singular value decomposition of $\\\\Sigma_{yx} = USV^T = \\\\sum_{\\\\alpha=1}^{\\\\min(n_x+k_x^2n_x, n_y+k_y^2n_x)} \\\\alpha = 1 \\\\lambda \\\\alpha u \\\\alpha v \\\\alpha^T$ and $\\\\Sigma_x = VDV^T = \\\\sum_{\\\\alpha=1}^{n_x+k_x^2n_x} \\\\delta \\\\alpha u \\\\alpha v \\\\alpha^T$. To solve for the dynamics we require that the right singular vectors $V$ of $\\\\Sigma_{yx}$ are also the singular vectors of $\\\\Sigma_x$. This is the case for any dataset in our space, as shown in Appendix B. Note, we assume that the network has at least $2n_x$ hidden neurons (the number of singular values in the input-output covariance matrix) so that it can learn the desired mapping perfectly. If this is not the case then the model will learn the top $n_h$ singular values of the input-output mapping where $n_h$ is the number of hidden neurons (Saxe et al., 2014). Given the SVDs of the two correlation matrices the learning dynamics can be described explicitly as $W_2(t) W_1(t) = UA(t) V^T = \\\\sum_{\\\\alpha=1}^{\\\\min(n_x+k_x^2n_x, n_y+k_y^2n_x)} \\\\pi \\\\alpha(t) u \\\\alpha v \\\\alpha^T$ where $A(t)$ is the effective singular value matrix of the network's mapping. The trajectory of each singular value in $A(t)$ is described as $\\\\pi \\\\alpha(t) = \\\\lambda \\\\alpha / \\\\delta \\\\alpha 1 - (1 - \\\\lambda \\\\alpha \\\\delta \\\\alpha \\\\pi_0) \\\\exp(-2 \\\\lambda \\\\alpha \\\\tau t)$.\\n\\nFrom these dynamics it is helpful to note that the time-course of the trajectory is only dependent on the $\\\\Sigma_{yx}$ singular values. Thus, $\\\\Sigma_x$ affects the stable point of the network singular values but not the time-course of learning. In addition for the $\\\\Sigma_{yx}$ singular values we have Theorem 1:\\n\\n**Theorem 1**\\n\\nFor all points in the space of datasets: $n_x, n_y, k_x, k_y, r \\\\in \\\\mathbb{R}^+$ the input-output covariance matrix $\\\\Sigma_{yx}$ singular values will be ordered as $\\\\lambda_1 > \\\\lambda_2 > \\\\lambda_3$.\\n\\n**Proof:**\\n\\nFirstly we prove that $\\\\lambda_1 > \\\\lambda_2$:\\n\\n$$\\\\lambda_1 > \\\\lambda_2 \\\\left( (k_x r^2 + 2n_x)(k_y r^2 + 2n_x) \\\\right)^{1/2} > (k_x r^2 + 2n_x)(k_y r^2) \\\\right)^{1/2} > k_y r^4 k_x k_y r^4 + 2n_x k_y r^2 > 0$$\\n\\nis true by definition since $n_x \\\\in \\\\mathbb{R}^+$ and, thus, $\\\\lambda_1 > \\\\lambda_2$ for all points in our space of datasets.\\n\\nNow we prove that $\\\\lambda_2 > \\\\lambda_3$:\\n\\n$$\\\\lambda_2 > \\\\lambda_3 \\\\left( (k_x r^2 + 2n_x)(k_y r^2) \\\\right)^{1/2} > (k_x r^2 + 2n_x)(k_y r^2) \\\\right)^{1/2} > k_x k_y r^4 > 0$$\\n\\nis true by definition since $n_x \\\\in \\\\mathbb{R}^+$ and, thus, $\\\\lambda_2 > \\\\lambda_3$ for all points in our space of datasets.\"}"}
