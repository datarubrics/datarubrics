{"id": "wPLEzBcSC7p", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table S4: Selected hyperparameters for all baselines.\\n\\n| Dataset       | Method       | Hyperparameters                      |\\n|---------------|--------------|--------------------------------------|\\n| Seq-CIFAR100  | ER           | $\\\\text{lr} = 0.1$                    |\\n|               | DER++        | $\\\\text{lr} = 0.03$, $\\\\alpha = 0.1$, $\\\\beta = 0.5$ |\\n|               | CO           | $\\\\text{lr} = 0.5$, $\\\\tau = 0.5$, $\\\\kappa = 0.2$, $\\\\kappa^* = 0.01$, $e = 100$ |\\n|               | ER-ACE       | $\\\\text{lr} = 0.01$                   |\\n|               | CLS-ER       | $\\\\text{lr} = 0.1$, $\\\\lambda = 0.15$, $r_p = 0.1$, $r_s = 0.05$, $\\\\alpha_p = 0.999$, $\\\\alpha_s = 0.999$ |\\n| GCIL-CIFAR100 | ER           | $\\\\text{lr} = 0.1$                    |\\n|               | DER++        | $\\\\text{lr} = 0.03$, $\\\\alpha = 0.1$, $\\\\beta = 0.5$ |\\n|               | CO           | $\\\\text{lr} = 0.5$, $\\\\tau = 0.5$, $\\\\kappa = 0.2$, $\\\\kappa^* = 0.01$, $e = 100$ |\\n|               | ER-ACE       | $\\\\text{lr} = 0.1$                   |\\n|               | CLS-ER       | $\\\\text{lr} = 0.1$, $\\\\lambda = 0.1$, $r_p = 0.7$, $r_s = 0.6$, $\\\\alpha_p = 0.999$, $\\\\alpha_s = 0.999$ |\\n\\nTable S5: Selected hyperparameters for CCL across different settings.\\n\\n| B          | lr    | batch size | #epochs | $r$ | $d$ | $\\\\lambda$ | $\\\\gamma$ | $\\\\lambda'$ | $\\\\gamma'$ |\\n|------------|-------|------------|---------|-----|-----|------------|----------|------------|-----------|\\n| Seq-CIFAR10 | 0.03  | 32         | 50      | 0.2 | 0.999| 0.1        | 0.1      | 0.1        | 0.1       |\\n| GCIL-CIFAR100 | 0.03  | 32         | 50      | 0.06| 0.999| 0.1        | 0.01     | 0.1        | 0.01      |\\n| DN4IL      | 0.03  | 32         | 50      | 0.06| 0.999| 0.1        | 0.01     | 0.1        | 0.01      |\\n\\nTable S6: Fifteen different natural corruptions\\n\\n- Gaussian Noise, Impulse Noise, Shot noise, Speckle noise\\n- Defocus blur, Glass blur, Motion blur, Zoom blur, Gaussian blur\\n- Brightness, Contrast, Fog, Frost, Snow\\n- Elastic Transformation, JPEG compression, Pixelate, Spatter, Saturate\\n\\nGeirhos et al. (2018) increases shape bias by adding multiple stylized images along with the original images used for training. Styles of artistic paintings are transferred...\"}"}
{"id": "wPLEzBcSC7p", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table S7: Backward transfer metric on Seq-CIFAR10 dataset\\n\\n| Method | ER  | DER++  | CLS-ER | CCL  |\\n|--------|-----|--------|--------|------|\\n|        | -61.75 | -33.45 | -23.47 | -6.07 |\\n\\nTable S8: Accuracy on the proposed DN4IL dataset for the Domain-IL setting. CCL shows a significant improvement in all disparate and challenging domains.\\n\\n| Method | real | clipart | infograph | painting | quickdraw | sketch |\\n|--------|------|---------|-----------|----------|-----------|--------|\\n| JOINT | 59.93 \u00b1 1.07 | 9.98 \u00b1 0.54 | 19.97 \u00b1 0.31 | 2.32 \u00b1 0.20 | 6.58 \u00b1 0.34 | 14.91 \u00b1 0.04 |\\n| SGD   | 20.83 \u00b1 0.24 | 20.08 \u00b1 0.45 | 26.37 \u00b1 0.35 | 5.56 \u00b1 0.39 | 13.92 \u00b1 0.91 | 23.69 \u00b1 1.54 |\\n| ER    | 47.52 \u00b1 0.25 | 47.52 \u00b1 0.25 | 54.69 \u00b1 0.10 | 15.70 \u00b1 0.33 | 37.54 \u00b1 0.30 | 51.98 \u00b1 0.96 |\\n\\nWe introduce a new dataset for the Domain-IL setting. It is a subset of the standard DomainNet dataset (Peng et al., 2019) used in domain adaptation. It consists of six different domains - real, clipart, infograph, painting, quickdraw, and sketch. The shift in distribution between domains is challenging. Few examples can be seen in Figure S4.\\n\\nEach domain includes 345 classes, and the overall dataset consists of \u223c59000 samples. The classes have redundancy, and also evaluating on the whole dataset can be computationally expensive for CL settings. Therefore, we create a subset by grouping semantically similar classes into 20 super categories (considering class overlap between other standard datasets can also facilitate OOD analysis). Each super category has five classes each, which results in a total of 100 classes. The specifications of the classes are given in Table S9. The dataset consists of 67080 training images and 19464 test images. The image size for all experiments is chosen as 64 \u00d7 64 (the normalize transform is not applied in the augmentations).\"}"}
{"id": "wPLEzBcSC7p", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure S4: Visual examples of DN4IL dataset.\\n\\nTable S9: Details on supercategory and classes in DN4IL dataset.\\n\\n| Supercategory    | Classes                                                                 |\\n|------------------|-------------------------------------------------------------------------|\\n| 1 small animals  | mouse, squirrel, rabbit, dog, raccoon                                   |\\n| 2 medium animals | tiger, bear, lion, panda, zebra                                         |\\n| 3 large animals  | camel, horse, kangaroo, elephant, cow                                   |\\n| 4 aquatic mammals| whale, shark, fish, dolphin, octopus                                   |\\n| 5 non-insect invertebrates | snail, scorpion, spider, lobster, crab                           |\\n| 6 insects        | bee, butterfly, mosquito, bird, bat                                    |\\n| 7 vehicle        | bus, bicycle, motorbike, train, pickup, truck                          |\\n| 8 sky-vehicle    | airplane, flying saucer, aircraft, carrier, helicopter, hot air balloon|\\n| 9 fruits         | strawberry, banana, pear, apple, watermelon                           |\\n| 10 vegetables    | carrot, asparagus, mushroom, onion, broccoli                           |\\n| 11 music         | trombone, violin, cello, guitar, clarinet                             |\\n| 12 furniture     | chair, dresser, table, couch, bed                                      |\\n| 13 household electrical devices | clock, floor lamp, telephone, television, keyboard |\\n| 14 tools         | saw, axe, hammer, screwdriver, scissors                                |\\n| 15 clothes & accessories | bowtie, pants, jacket, sock, shorts                                     |\\n| 16 man-made outdoor | skyscraper, windmill, house, castle, bridge                           |\\n| 17 nature        | cloud, bush, ocean, river, mountain                                    |\\n| 18 food          | birthday cake, hamburger, ice cream, sandwich, pizza                   |\\n| 19 stationary    | calendar, marker, map, eraser, pencil                                  |\\n| 20 household items | wine bottle, cup, teapot, frying pan, wine glass                      |\"}"}
{"id": "wPLEzBcSC7p", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure S5: Number of samples per domain in DN4IL dataset.\\n\\nFigure S6: Number of samples per super category in DN4IL dataset.\\n\\nFigure S7: Number of overall samples per class in DN4IL dataset.\"}"}
{"id": "wPLEzBcSC7p", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of different methods on standard CL benchmarks (Class-IL, Task-IL and GCIL settings). CCL shows a consistent improvement over all methods for both buffer sizes.\\n\\n| Buffer Size | Method      | Seq-CIFAR10 | Seq-CIFAR100 | GCIL-CIFAR100 |\\n|------------|-------------|-------------|--------------|---------------|\\n| 500        | SGD         | 19.62 \u00b1 0.05 | 61.02 \u00b1 3.33 | 17.58 \u00b1 0.04  |\\n| 200        | ER          | 44.79 \u00b1 1.86 | 91.19 \u00b1 0.94 | 21.40 \u00b1 0.22  |\\n| 500        | DER++       | 64.88 \u00b1 1.17 | 91.92 \u00b1 0.60 | 29.60 \u00b1 1.14  |\\n| 500        | Co2L        | 65.57 \u00b1 1.37 | 93.43 \u00b1 0.78 | 31.90 \u00b1 0.38  |\\n| 500        | ER-ACE      | 62.08 \u00b1 1.44 | 92.20 \u00b1 0.57 | 32.49 \u00b1 0.95  |\\n| 500        | CLS-ER\u2020     | 66.19 \u00b1 0.75 | 93.90 \u00b1 0.60 | 43.80 \u00b1 1.89  |\\n| 500        | CCL         | 70.04 \u00b1 1.07 | 94.49 \u00b1 0.38 | 46.55 \u00b1 1.51  |\\n\\nResults\\n\\nWe provide a comparison of our method with standard baselines and multiple other SOTA CL methods. The lower and upper bounds are reported as SGD (standard training) and JOINT (training all tasks together), respectively. We compare with other rehearsal-based methods in the literature, namely ER, DER (Buzzega et al., 2020), Co2L (Cha et al., 2021), ER-ACE (Caccia et al., 2021) and CLS-ER (Arani et al., 2021). Table S2 shows the average performance in different settings over three seeds. Co2L utilizes task boundary information, and therefore the GCIL setting is not applicable. The results are taken from the original papers and, if not available, using the original codes, we conducted a hyperparameter search for the new settings.\\n\\nCCL achieves the best performance across all datasets in all settings. In the challenging Class-IL setting, we observe a gain of \u223c50% over DER++, thus showing the efficacy of adding multiple modules to CL. Furthermore, we report improvements of \u223c6% on both the Seq-CIFAR10 and Seq-CIFAR100 datasets, over CLS-ER, which utilizes two memories in its design. CCL has a single semantic memory, and the additional boost is procured by prior knowledge from the inductive bias learner. Improvement is prominent even when the memory budget is low (200 buffer size). GCIL represents a more realistic setting, as the task boundaries are blurry and classes can reappear and overlap in any task. GCIL-Longtail version also introduces an imbalance in the sample distribution. CCL shows a significant improvement on both versions of GCIL-CIFAR100. Shape information from the inductive bias learner offers the global high-level context, which helps in producing generic representations that are not biased towards learning only the current task at hand. Furthermore, sharing of the knowledge that has been assimilated through the appearance of overlapping classes through the training scheme, further facilities learning in this general setting. The overall results indicate that the dual knowledge sharing between the explicit working module and the implicit inductive bias and semantic memory modules enables both better adaptation to new tasks and information retention.\\n\\nDomain Incremental Learning\\n\\nIntelligent agents deployed in real-world applications need to maintain consistent performance through changes in the data and environment. Domain-IL aims to assess the robustness of the CL methods to the distribution shift. In Domain-IL, the classes in each task remain the same, but the input distribution changes, and this makes for a more plausible use case for evaluation. However, the datasets used in the literature do not fully reflect this setting. For instance, the most common\"}"}
{"id": "wPLEzBcSC7p", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"datasets used in the literature are different variations (Rotated and Permuted) of the MNIST dataset (LeCun et al., 1998). MNIST is a simple dataset, usually evaluated on MLP networks, and its variations do not reflect the real-world distribution shift challenges that a CL method faces. As is evident from the different CL methods in the literature, the improvement in performance has been saturated on all variants of MNIST. Farquhar & Gal (2018) propose fundamental desiderata for CL evaluations and datasets based on real-world use cases. One of the criteria is to possess cross-task resemblances, which Permuted-MNIST clearly violates. Thus, a different dataset is needed to test the overall capability of a CL method to handle the distributional shift.\\n\\n5.1 DN4IL D ATASET\\n\\nTo this end, we propose DN4IL (DomainNet for Domain-IL), which is a well-crafted subset of the standard DomainNet dataset (Peng et al., 2019), used in domain adaptation. DomainNet consists of common objects in six different domains - real, clipart, infograph, painting, quickdraw, and sketch. The original DomainNet consists of 59,000 samples with 345 classes in each domain. The classes have redundancy, and moreover, evaluating the whole dataset can be computationally expensive in a CL setting. Considering different criteria such as the relevance of classes, uniform sample distribution, computational complexity, and ease of benchmarking for CL, we create the version DN4IL, which is tailor-made for continual learning.\\n\\nAll classes were grouped into semantically similar supercategories. Out of these, a subset of classes was selected that had relevance to domain shift while also having maximum overlap with other standard datasets such as CIFAR-10 and CIFAR-100, as this can facilitate in performing out-of-distribution analyses. 20 supercategories were chosen with 5 classes each (resulting in a total of 100 classes). In addition, to provide a balanced dataset, we performed a class-wise sampling. First, we sample images per class in each supercategory and maintain class balance. Second, we choose samples per domain, so that it results in a dataset that has a near-uniform distribution across all classes and domains. The final dataset DN4IL is succinct, more balanced, and more computationally efficient for benchmarking, thus facilitating research in CL. Additionally, the new dataset deems more plausible for real-world settings and also adheres to all the evaluation desiderata by (Farquhar & Gal, 2018). The challenging distribution shift between domains provides an apt dataset to test the capability of CL methods in the Domain-IL setting. More details, statistics, and visual examples of this crafted dataset are provided in Section I.\\n\\n5.2 DN4IL PERFORMANCE\\n\\nFigure 2 (left) reports the results on DN4IL for two different buffer sizes (Values are provided in Table S8). CCL shows a considerable performance gain in the average accuracy across all domains and can be primarily attributed to the supervision from the shape data. Standard networks tend to exhibit texture bias and learn background or spurious cues (Geirhos et al., 2018) that result in performance degradation when the distribution changes. Learning global shape information of objects,\"}"}
{"id": "wPLEzBcSC7p", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Task-wise performance on DN4IL ($|B| = 500$), where each task represents a domain. CCL shows more retention of old information without compromising much on current accuracy. On the other hand, helps in learning generic features that can translate well to other distributions. Semantic memory further helps to consolidate information across domains. Maintaining consistent performance to such difficult distribution shift proves beneficial in real-world applications, and the proficiency of CCL in this setting can thus open up new avenues for research in cognition-inspired multi-module architectures.\\n\\n6.1 Plasticity and Stability\\n\\nPlasticity refers to the capability of a model to learn new tasks, while stability shows how well it can retain old information. The plasticity-stability dilemma is a long-standing problem in CL, which requires an optimal balance between the two. We measure each of these to assess the competence of the CL methods. Plasticity is computed as the average performance of each task when it is first learned (e.g., the accuracy of the network trained on task $T_2$, evaluated on the test set of $T_2$). Stability is computed as the average performance of all tasks $1:T^{-1}$, after learning the final task $T$.\\n\\nFigure 2 (right) reports these numbers for the DN4IL dataset. As seen, the ER and DER methods exhibit forgetting and show low stability and concentrate only on the newer tasks. CLS-ER shows greater stability, but at the cost of reduced plasticity. However, CCL shows the highest stability while maintaining comparable plasticity. The shape knowledge in CCL helps in learning generic solutions that can translate to new tasks, while the semantic consolidation update at stochastic rates acts as a regularization to maintain stable parameter updates. Thus, CCL strikes a better balance between plasticity and stability.\\n\\n6.2 Task-wise Performance\\n\\nThe average accuracy across all tasks does not provide a complete measure of the ability of a network to retain old information while learning new tasks. To better represent the plasticity-stability measure, we report the task-wise performance at the end of each task. After training each task, we measure the accuracy on the test set of each of the previous tasks. Figure 3 reports this for all tasks of DN4IL. The last row represents the performance of each task after the training is complete. ER and DER++ show performance degradation on earlier tasks, as the model continues training on newer tasks. Both perform well on the last task and display the lowest stability. CCL reports the highest information retention on older tasks, while also maintaining plasticity. For example, the accuracy on the first task (real) reduces to 27.6 on ER after training the 6 tasks (domains), while the CCL maintains the accuracy of 54.9. CLS-ER shows better retention of old information but at the cost of plasticity. The last task on CLS-ER shows lower performance compared to CCL (52.7 vs. 61.0). Similar trend (with more gains) is seen on Seq-CIFAR10 dataset in Appendix Figure S2. The performance of the current task in CCL is relatively lesser and can be attributed to the stochastic update rate of this model.\\n\\nTo shed more light on the performance of each of the modules in CCL, we also provide the performance of the working model and the inductive bias learner, in Appendix Figure S1. The working model shows better plasticity, while CCL (semantic memory) displays better stability. Overall, all\"}"}
{"id": "wPLEzBcSC7p", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: CCL shows reduced task recency bias (left), as well as higher robustness against natural corruption (right) on Seq-CIFAR10 ($|B|$=200) dataset.\\n\\n6.3 Recency Bias Analysis\\n\\nRecency bias is a behavior in which the model predictions tend to be biased toward the current or the most recent task (Wu et al., 2019). This is undesirable in a CL model, as it results in a biased solution that forgets the old tasks. To this end, after the end of the training, we evaluate the models on the test set (of all tasks) and calculate the probability of predicting each task. The output distribution for each test sample is computed for all classes, and the probabilities are averaged per task.\\n\\nFigure 4 (left) shows the probabilities for each task on Seq-CIFAR10 dataset. As shown, the ER and DER++ methods tend to incline most of their predictions towards the classes seen in the last task, thus creating a misguided bias. CCL shows a lesser bias compared to both of these baselines. CLS-ER exhibits reduced bias due to the presence of multiple memories, but the distribution is still relatively skewed (w.r.t. probability of 0.2). CCL shows more of a uniform distribution across all tasks. The dual information from the shape data and the consolidated knowledge across tasks helps in breaking away from the Occam's razor pattern of neural networks to default to the easiest solution.\\n\\n6.4 Robustness\\n\\nLifelong agents, when deployed in real-world settings, must be resistant to various factors, such as lighting conditions, changes in weather, and other effects of digital imaging. Inconsistency in predictions under different conditions might result in undesirable outcomes, especially in safety-critical applications such as autonomous driving. To measure the robustness of the CL method against such natural corruptions, we created a dataset by applying fifteen different corruptions (Table S6), at varying levels of severity (1 - least severe to 5 - most severe corruption).\\n\\nThe performances on the fifteen corruptions are averaged at each severity level and are shown in Figure 4 (right). CCL outperforms all other techniques at all severity levels. ER, DER++, and CLS-ER show a fast decline in accuracy as severity increases, while CCL maintains stable performance throughout. Implicit shape information provides a different perspective of the same data to the model, which helps to generate high-level, robust representations. CCL, along with improved continual learning performance, also exhibits improved robustness to corruptions, thus proving to be a better candidate for deployment in real-world applications.\\n\\n6.5 Ablation Study\\n\\nCCL architecture comprises multiple components, each contributing to the efficacy of the method. The explicit module has the working model, and the implicit module has semantic memory (SM) and inductive bias learner (IBL). Disentangling different components in the CCL, can provide more insight into the contribution of each of them to the overall performance.\"}"}
{"id": "wPLEzBcSC7p", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A COGNITIVE-INSPIRED MULTI-MODULE ARCHITECTURE FOR CONTINUAL LEARNING\\n\\nAbstract\\n\\nArtificial neural networks (ANNs) exhibit a narrow scope of expertise on stationary independent data. However, data in the real world is continuous and dynamic, and ANNs must adapt to novel scenarios while also retaining the learned knowledge to become lifelong learners. The ability of humans to excel at these tasks can be attributed to multiple factors ranging from cognitive computational structures, cognitive biases, and the multi-memory systems in the brain. We incorporate key concepts from each of these to design a cognitive-inspired continual learning method. Cognitive Continual Learner (CCL) includes multiple modules, implicit and explicit knowledge representation dichotomy, inductive bias, and a multi-memory system. CCL shows improvement across different settings and also shows a reduced task recency bias. To test versatility of continual learning methods on a challenging distribution shift, we introduce a novel domain-incremental dataset DN4IL. In addition to improved performance on existing benchmarks, CCL also demonstrates superior performance on this dataset.\\n\\nIntroduction\\n\\nDeep learning has seen rapid progress in recent years, and supervised learning agents have achieved superior performance in perception tasks. However, unlike a supervised setting, where data is static, and independent and identically distributed, real-world data is changing dynamically. Continual learning (CL) aims at learning multiple tasks when data is streamed sequentially (Parisi et al., 2019). This is crucial in real-world deployment settings, as the model needs to adapt quickly to novel data (plasticity), while also retaining previously learned knowledge (stability). Artificial neural networks (ANN), however, are still not effective continual learners as they often fail to generalize to small changes in distribution and also suffer from forgetting old information when presented with new data (catastrophic forgetting)(McCloskey & Cohen, 1989).\\n\\nHumans, on the other hand, show a better ability to acquire new skills while also retaining previously learned skills to a greater extent. This intelligence can be attributed to different factors in human cognition. Multiple theories have been proposed to formulate an overall cognitive architecture, which is a broad domain-generic cognitive computation model that captures the essential structure and process of the mind. Some of these theories hypothesize that, instead of a single standalone module, multiple modules in the brain share information to excel at a particular task. CLARION (Connectionist learning with rule induction online) (Sun & Franklin, 2007) is one such theory that postulates an integrative cognitive architecture, consisting of a number of distinct subsystems. It predicates a dual representational structure (Chaiken & Trope, 1999), where the top level encodes conscious explicit knowledge, while the other encodes indirect implicit information. The two systems interact, share knowledge, and cooperate in solving tasks. Delving into these underlying architectures and formulating a new design can help in the quest of building intelligent agents.\\n\\nMultiple modules can be instituted instead of a single feedforward network. An explicit module that learns from the standard visual input and an implicit module that shares indirect contextual knowledge. The implicit module can be further divided into more sub-modules, each providing different information. Inductive biases and semantic memories can act as different kinds of implicit knowledge. Inductive biases are pre-stored templates or knowledge that provide some meaningful disposition toward adapting to the continuously evolving world (Chollet, 2019). Furthermore, the code and the DN4IL dataset will be made accessible upon acceptance.\"}"}
{"id": "wPLEzBcSC7p", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"theories (Kumaran et al., 2016) postulate that after rapidly learning information, a gradual consolidation of knowledge transpires in the brain for slow learning of structured information. Thus, the new design incorporates multiple concepts of cognition architectures, the dichotomy of implicit and explicit representations, inductive biases, and multi-memory systems theory.\\n\\nTo this end, we propose **Cognitive Continual Learner** (CCL), a multi-module architecture for CL. The explicit working module processes the standard input data. Two different sub-modules are introduced for the implicit module. The inductive bias learner embeds relevant prior information, and as networks are shown to be biased toward textural information (unlike humans that are more biased toward global semantics) (Geirhos et al., 2018), we propose to utilize the global shape information as the prior. Shape is already present in the visual data but in an indirect way, and extracting this implicit information and sharing with the explicit module will help to learn more generic and high-level representations. Further, to emulate the consolidation of information in the slow-fast multi-memory system, a gradual accumulation of knowledge from the explicit working module is embedded in the second semantic memory sub-module. We show that interacting and leveraging information between these modules can help alleviate catastrophic forgetting while also increasing the robustness to distribution shift.\\n\\nCCL achieves superior performance across all CL settings on various datasets. CCL outperforms the SOTA CL methods on Seq-CIFAR10, Seq-CIFAR100 in the class incremental settings. Furthermore, in more realistic general class incremental settings where the task boundary is blurry and classes are not disjoint, CCL shows significant gains. The addition of inductive bias and semantic memory helps to achieve a better balance between the plasticity-stability trade-off. The prior in the form of shape helps produce generic representations, and this results in CCL exhibiting a reduced task-recency bias. Furthermore, CCL also shows higher robustness against natural corruptions. Finally, to test the capability of the CL methods against distribution shift, we introduce a domain incremental learning dataset, **DN4IL**, which is a carefully designed subset of the DomainNet dataset (Peng et al., 2019). CCL shows considerable robustness across all domains on these challenging data, thus establishing the efficacy of our cognitive-inspired CL architecture. Our contributions are as follows:\\n\\n1. **Cognitive Continual Learner** (CCL), a novel method that incorporates aspects of cognitive architectures, multi-memory systems, and inductive bias into the CL framework.\\n2. Introducing **DN4IL**, a challenging domain incremental learning dataset for CL.\\n3. Benchmarks across different CL settings: class incremental, task incremental, generalized class incremental, and domain incremental learning.\\n4. Analyses on the plasticity-stability trade-off, task recency bias, and robustness to natural corruptions.\\n\\n### Methodology\\n\\n#### 2.1 Cognitive Architectures\\n\\nCognitive architectures refer to computational models that encapsulate the overall structure of the cognitive process in the brain. The underlying infrastructure of such a model can be leveraged to develop better intelligent systems. Global workspace theory (GWT) (Juliani et al., 2022) postulates that human cognition is composed of a multitude of special-purpose processors and is not a single standalone module. Different sub-modules might encode different contextual information which, when activated, can transfer knowledge to the conscious central workspace to influence and help make better decisions. Furthermore, CLARION (Sun & Franklin, 2007) posits a dual-system cognitive architecture with two levels of knowledge representation. The explicit module encodes direct knowledge that is externally accessible. The implicit module encodes indirect knowledge that is not directly accessible, but can be obtained through some intermediate interpretive or transformational steps. These two modules interact with each other by transferring knowledge between each other. Inspired by these theories, we formulate a method that incorporates some of the key aspects of cognitive architecture into the CL method. A working module, which encodes the direct sensory data, forms the explicit module. A second module that encodes indirect and interpretive information forms the implicit module. The implicit module further includes multiple sub-modules to encode different types of knowledge.\"}"}
{"id": "wPLEzBcSC7p", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\n2.2 INDUCIVE BIAS\\n\\nThe sub-modules in the implicit module need to encapsulate implicit information that can provide more contextual and high-level supervision. One of such knowledge can be prior knowledge or inductive bias. Inductive biases are prestored templates that exist implicitly even in earlier stages of the human brain (Pearl & Mackenzie, 2018). For instance, cognitive inductive bias may be one of the reasons why humans can focus on the global semantics of objects to make predictions. ANNs, on the other hand, are more prone to rely on local cues and textures (Geirhos et al., 2018). Global semantics or shape information already exists in the visual data, but in an indirect way. Hence, we utilize shape as indirect information in the implicit module. The sub-module uses a transformation step to extract the shape and share this inductive bias with the working module. As the standard (RGB) image and its shape counterpart can be viewed as different perspectives/modalities of the same data, ensuring that the representation of one modality is consistent with the other increases robustness to spurious correlations that might exist in only one of them.\\n\\n2.3 MULTI-MEMORY SYSTEM\\n\\nMoreover, many theories have postulated that an intelligent agent must possess deferentially specialized learning memory systems (Kumaran et al., 2016). While one system rapidly learns the individual experience, the other gradually assimilates the knowledge. To emulate this behavior, we establish a second sub-module that slowly consolidates the knowledge from the working module.\\n\\n2.4 FORMULATION\\n\\nTo this end, we propose a novel method Cognitive Continual Learner (CCL), which incorporates all these concepts into the CL paradigm. CCL consists of two modules, the explicit module and the implicit module. The explicit module has a single working model and processes the incoming sensory data. Within the implicit module, the inductive bias learner encodes the prior shape knowledge and the semantic memory consolidates information from the explicit module.\"}"}
{"id": "wPLEzBcSC7p", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The implicit module further consists of two sub-modules, namely the inductive bias learner and the semantic memory. They share relevant contextual information and assimilated knowledge with the explicit module, respectively. Figure 1 shows the overall architecture.\\n\\nIn the implicit module, semantic memory $\\\\text{SM}$, consolidates knowledge at stochastic intervals from the working model $\\\\text{WM}$, in the explicit module. The other sub-module, the inductive bias learner $\\\\text{IBL}$, processes the data and extracts the shape information (Section B).\\n\\nThe consolidated structural information in semantic memory is transferred to both the working model and the inductive bias learner by aligning the output space on the buffer samples, which further helps in information retention. The loss functions $L_{\\\\text{KS}}^{\\\\text{WM}}$ and $L_{\\\\text{KS}}^{\\\\text{IBL}}$ are as follows:\\n\\n$$L_{\\\\text{KS}}^{\\\\text{WM}} = \\\\mathbb{E}_{x_b \\\\sim B} \\\\| f(x_b; \\\\theta_{\\\\text{SM}}) - f(x_b; \\\\theta_{\\\\text{WM}}) \\\\|_2^2$$\\n\\n$$L_{\\\\text{KS}}^{\\\\text{IBL}} = \\\\mathbb{E}_{x_b \\\\sim B} \\\\| f(x_b; \\\\theta_{\\\\text{SM}}) - f(x_b; \\\\theta_{\\\\text{IBL}}) \\\\|_2^2$$\\n\\nThus, the overall loss functions for the working model and the inductive bias learner are as follows:\\n\\n$$L_{\\\\text{WM}} = L_{\\\\text{Sup}}^{\\\\text{WM}} + \\\\lambda L_{\\\\text{biKS}} + \\\\lambda' L_{\\\\text{KS}}^{\\\\text{WM}}$$\\n\\n$$L_{\\\\text{IBL}} = L_{\\\\text{Sup}}^{\\\\text{IBL}} + \\\\gamma L_{\\\\text{biKS}} + \\\\gamma' L_{\\\\text{KS}}^{\\\\text{IBL}}$$\\n\\nThe semantic memory of the implicit module is updated with a stochastic momentum update (SMU) of the weights of the working model at rate $r$ with a decay factor of $d$,\\n\\n$$\\\\theta_{\\\\text{SM}} = d \\\\cdot \\\\theta_{\\\\text{SM}} + (1 - d) \\\\cdot \\\\theta_{\\\\text{WM}}$$\\n\\nMore details are provided in Algorithm 2. Note that we use semantic memory ($\\\\theta_{\\\\text{SM}}$) for inference, as it contains consolidated knowledge across all tasks.\\n\\n3 EXPERIMENTAL SETTINGS\\n\\nResNet-18 (He et al., 2016) architecture is used for all experiments. All networks are trained using the SGD optimizer with standard augmentations of random crop and random horizontal flip. The different hyperparameters, tuned per dataset, are provided in E. The different CL settings are explained in detail in Section D. We consider CLass-IL, Domain-IL and also report the Task-IL settings. Seq-CIFAR10 and Seq-CIFAR100 (Krizhevsky et al., 2009) for the class incremental learning (Class-IL) settings, which are divided into 5 tasks each. As an addition to Class-IL, we also consider and evaluate General Class-IL (GCIL) (Mi et al., 2020) on CIFAR100 dataset. For the domain incremental learning (Domain-IL), we propose a novel dataset, $\\\\text{DN4IL}$. \\n\\n4\"}"}
{"id": "wPLEzBcSC7p", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Ablation to analyse the effect of each component of CCL on Seq-CIFAR10 and DN4IL.\\n\\n| SM  | IBL | KS    | Seq-CIFAR10 | DN4IL  |\\n|-----|-----|-------|-------------|--------|\\n| \u2713   | \u2713   | \u2713     | 70.04 \u00b1 1.07 | 44.23 \u00b1 0.05 |\\n| \u2713   | \u2713   | \u2717     | 69.28 \u00b1 1.34 | 40.35 \u00b1 0.34 |\\n| \u2713   | \u2717   |       | 69.21 \u00b1 1.46 | 39.76 \u00b1 0.56 |\\n| \u2717   | \u2717   | \u2717     | 64.61 \u00b1 1.22 | 37.33 \u00b1 0.01 |\\n| \u2717   | \u2717   |        | 44.79 \u00b1 1.86 | 26.59 \u00b1 0.31 |\\n\\nTable 2 reports the ablation study w.r.t to each of these components on both Seq-CIFAR10 and DN4IL datasets. Considering the more complex DN4IL dataset, the ER accuracy without any of our components is 26.59. Adding cognitive bias (IBL) improves performance by 40%. Shape information plays a prominent role, as the networks need to learn the global semantics of the objects, rather than background or spurious textural information to translate performance across domains. Adding the dual-memory component (SM) shows an increase of approximately 49% over the vanilla baseline. Furthermore, KS between explicit and implicit modules on current experiences also plays a key role in performance gain. Combining both of these cognitive components and, in general, following the multi-module design shows a gain of 66%. A similar trend is seen on Seq-CIFAR10.\\n\\n**8 RELATED WORKS**\\n\\nRehearsal-based approaches, which revisit examples from the past to alleviate catastrophic forgetting, have been effective in challenging CL scenarios (Farquhar & Gal, 2018). Experience Replay (ER) (Riemer et al., 2018) methods use episodic memory to retain previously seen samples for replay purposes. DER++ (Buzzega et al., 2020) adds a consistency loss on logits, in addition to the ER strategy. CO2L (Cha et al., 2021) uses contrastive learning from the self-supervised learning domain to generate transferable representations. ER-ACE (Caccia et al., 2021) targets the representation drift problem in online CL and develops a technique to use separate losses for current and buffer samples. All of these methods limit the architecture to a single stand-alone network, contrary to the biological workings of the brain. CLS-ER (Arani et al., 2021) proposed a multi-network approach that emulates fast and slow learning systems by using two semantic memories, each aggregating weights at different times. Though CLS-ER utilizes the multi-memory design, sharing of different kinds of knowledge is not leveraged, and hence presents a method with limited scope. CCL diverges from the standard architectures and proposed a multi-module design that is inspired by the cognitive computational architectures. It incorporates multiple sub-modules, each sharing different knowledge to develop an effective continual learner that has better generalization and robustness.\\n\\n**9 CONCLUSION**\\n\\nWe introduced a novel framework for continual learning which incorporates concepts inspired by cognitive architectures, high-level cognitive biases, and the multi-memory system. Our method, Cognitive Continual Learner (CCL), includes multiple subsystems with dual knowledge representation. CCL designed a dichotomy of explicit and implicit modules in which information is selected, maintained, and shared with each other, to enable better generalization and robustness. CCL outperformed on Seq-CIFAR10 and Seq-CIFAR100 on the Class-IL setting. In addition, it also showed significant gain in the more realistic and challenging GCIL setting. Through different analyses, we showed the better plasticity-stability balance achieved by CCL. Furthermore, shape prior and knowledge consolidation helps to learn more generic solutions, indicated by the reduced task recency bias problem and higher robustness against natural corruptions. Furthermore, we introduced a challenging domain-IL dataset, DN4IL, with six disparate domains. The significant improvement of CCL on this complex distribution shift demonstrates the benefits of shape context, which helps the network to converge on a generic solution, rather than a simple texture-biased one. In general, incorporating a design inspired by the cognitive model and sharing information between explicit and implicit inductive bias and implicit semantic memory modules, instead of a standalone network, helps enhance lifelong learning, while also improving generalization and robustness.\"}"}
{"id": "wPLEzBcSC7p", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Elahe Arani, Fahad Sarfraz, and Bahram Zonooz. Learning fast, learning slow: A general continual learning method based on complementary learning system. In International Conference on Learning Representations, 2021.\\n\\nPietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. Advances in neural information processing systems, 33:15920\u201315930, 2020.\\n\\nLucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuytelaars, Joelle Pineau, and Eugene Belilovsky. New insights on reducing abrupt representation change in online continual learning. In International Conference on Learning Representations, 2021.\\n\\nHyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9516\u20139525, 2021.\\n\\nShelly Chaiken and Yaacov Trope. Dual-process theories in social psychology. Guilford Press, 1999.\\n\\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Info-gan: Interpretable representation learning by information maximizing generative adversarial nets. Advances in neural information processing systems, 29, 2016.\\n\\nFran\u00e7ois Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019.\\n\\nLijun Ding and Ardeshir Goshtasby. On the canny edge detector. Pattern Recognition, 34(3):721\u2013725, 2001.\\n\\nSebastian Farquhar and Yarin Gal. Towards robust evaluations of continual learning. 2018.\\n\\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2018.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.\\n\\nArthur Juliani, Kai Arulkumaran, Shuntaro Sasai, and Ryota Kanai. On the link between conscious function and general intelligence in humans and machines. arXiv preprint arXiv:2204.05133, 2022.\\n\\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\\n\\nDharshan Kumaran, Demis Hassabis, and James L McClelland. What learning systems do intelligent agents need? complementary learning systems theory updated. Trends in cognitive sciences, 20(7):512\u2013534, 2016.\\n\\nYann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\\n\\nYingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Yuille, et al. Shape-texture debiased neural network training. In International Conference on Learning Representations, 2020.\\n\\nDavid Lopez-Paz and Marc'Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017.\\n\\nMichael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109\u2013165. Elsevier, 1989.\"}"}
{"id": "wPLEzBcSC7p", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fei Mi, Lingjing Kong, Tao Lin, Kaicheng Yu, and Boi Faltings. Generalized class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 240\u2013241, 2020.\\n\\nGerman I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54\u201371, 2019.\\n\\nJudea Pearl and Dana Mackenzie. The book of why: the new science of cause and effect. Basic books, 2018.\\n\\nXingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1406\u20131415, 2019.\\n\\nMatthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In International Conference on Learning Representations, 2018.\\n\\nIrwin Sobel and Gary Feldman. A 3x3 isotropic gradient operator for image processing. a talk at the Stanford Artificial Project, pp. 271\u2013272, 1968.\\n\\nRon Sun and Stan Franklin. Computational models of consciousness: A taxonomy and some examples., 2007.\\n\\nGido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734, 2019.\\n\\nJeffrey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software (TOMS), 11(1):37\u201357, 1985.\\n\\nYue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 374\u2013382, 2019.\"}"}
{"id": "wPLEzBcSC7p", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Appendix A.1 CCL\\n\\nFigure S1 presents the task-wise performance of all the three networks in the CCL architecture, on DN4IL dataset. Semantic memory helps in information retention by maintaining high accuracy on older tasks and is more stable. The performance of the current task is relatively lower than that of the working model and could be due to the stochastic update rate of this model. The working model has better performance on new tasks and is more plastic. Inductive bias leaner is evaluated on the transformed data (shape) and also achieves a balance between plasticity and stability. In general, all modules in our proposed method present unique attributes that improve the learning process by improving performance and reducing catastrophic forgetting.\\n\\n|          | T1  | T2  | T3  | T4  | T5  | T6  |\\n|----------|-----|-----|-----|-----|-----|-----|\\n| **Semantic Memory (Default)** | 66.5 | 57.0 | 71.2 | 50.8 | 54.3 | 30.4 |\\n| **Working Model** | 54.5 | 46.5 | 65.4 | 39.7 | 47.1 | 22.3 |\\n| **Inductive Bias Learner** | 60.5 | 62.9 | 61.4 | 59.0 | 62.1 | 26.9 |\\n\\nFigure S2 presents a similar analysis on the Seq-CIFAR10 dataset. The trend is similar, but the performance gain is much higher on this dataset.\\n\\n|          | T1  | T2  | T3  | T4  | T5  | T6  |\\n|----------|-----|-----|-----|-----|-----|-----|\\n| **Semantic Memory (Default)** | 98.8 | 67.0 | 92.1 | 54.0 | 16.9 | 95.8 |\\n| **Working Model** | 97.0 | 88.3 | 77.2 | 83.5 | 43.0 | 78.5 |\\n| **Inductive Bias Learner** | 98.2 | 89.2 | 87.3 | 82.0 | 50.0 | 90.0 |\\n\\nCCL (SM - Default)\\n\\n|          | T1  | T2  | T3  | T4  | T5  | T6  |\\n|----------|-----|-----|-----|-----|-----|-----|\\n| **Semantic Memory (Default)** | 98.3 | 92.0 | 79.7 | 92.6 | 57.2 | 77.0 |\\n| **Working Model** | 97.0 | 88.3 | 77.2 | 83.5 | 43.0 | 78.5 |\\n| **Inductive Bias Learner** | 98.7 | 89.0 | 89.5 | 78.2 | 53.5 | 89.0 |\\n\\nCCL (WM)\\n\\n|          | T1  | T2  | T3  | T4  | T5  | T6  |\\n|----------|-----|-----|-----|-----|-----|-----|\\n| **Semantic Memory (Default)** | 98.3 | 92.0 | 79.7 | 92.6 | 57.2 | 77.0 |\\n| **Working Model** | 97.0 | 88.3 | 77.2 | 83.5 | 43.0 | 78.5 |\\n| **Inductive Bias Learner** | 97.0 | 88.3 | 77.2 | 83.5 | 43.0 | 78.5 |\\n\\nCCL (IBL)\"}"}
{"id": "wPLEzBcSC7p", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table S1: Comparison of different methods on standard CL benchmarks (Class-IL, Task-IL settings), including non-ER based methods on Seq-CIFAR10 dataset\\n\\n| Method     | Class-IL | Task-IL |\\n|------------|----------|---------|\\n| JOINT      | 92.20 \u00b1 0.15 | 98.31 \u00b1 0.12 |\\n| SGD        | 19.62 \u00b1 0.05  | 61.02 \u00b1 3.33 |\\n| oEWC       | 19.49 \u00b1 0.12  | 68.29 \u00b1 3.92 |\\n| SI         | 19.48 \u00b1 0.17  | 68.05 \u00b1 5.91 |\\n| LwF        | 19.61 \u00b1 0.05  | 63.29 \u00b1 2.35 |\\n| PNN        | -95.13 \u00b1 0.72 |         |\\n| 200 ER     | 44.79 \u00b1 1.86  | 91.19 \u00b1 0.94 |\\n| DER++      | 64.88 \u00b1 1.17  | 91.92 \u00b1 0.60 |\\n| Co\u00b2L       | 65.57 \u00b1 1.37  | 93.43 \u00b1 0.78 |\\n| ER-ACE     | 62.08 \u00b1 1.44  | 92.20 \u00b1 0.57 |\\n| CLS-ER     | -         | 93.90 \u00b1 0.60 |\\n| CCL        | 70.04 \u00b1 1.07  | 94.49 \u00b1 0.38 |\\n\\nTable S2: Comparison on Seq-CIFAR100 dataset for different tasks on 500 buffer size\\n\\n| Method     | 5-Tasks | 10-Tasks | 20-Tasks |\\n|------------|----------|----------|----------|\\n| 500 ER     | 28.02 \u00b1 0.31 | 21.49 \u00b1 0.47 | 16.52 \u00b1 0.86 |\\n| DER++      | 41.40 \u00b1 0.96 | 36.20 \u00b1 0.52 | 22.25 \u00b1 5.87 |\\n| CCL        | 53.23 \u00b1 1.62 | 41.09 \u00b1 0.72 | 33.60 \u00b1 0.25 |\\n\\nThe shape extraction is performed by applying a filter on the input image. Multiple filters were considered (such as Canny (Ding & Goshtasby, 2001), Prewitt), but the Sobel filter (Sobel & Feldman, 1968) was chosen because it produces a more realistic output by being precise and also smoothing the edges. The overall algorithm is explained in the following.\\n\\nAlgorithm 1\\n\\nSobel Algorithm - Shape Extraction\\n\\nInput:\\n- Input data: x\\\\_{rgb}\\n\\n1: Up-sample the images to twice the original size:\\n\\n\\\\[ x_{rgb} = us(x_{rgb}) \\\\]\\n\\n2: Apply Gaussian smoothing to reduce noisy edges:\\n\\n\\\\[ x_g = \\\\text{Gaussian Blur}(x_{rgb}, \\\\text{kernel size} = 3) \\\\]\\n\\n3: Get Sobel kernels:\\n\\n\\\\[ S_x = \\\\begin{bmatrix} -1 & 0 & +1 \\\\\\\\ -2 & 0 & +2 \\\\\\\\ -1 & 0 & +1 \\\\end{bmatrix} \\\\]\\n\\n\\\\[ S_y = \\\\begin{bmatrix} -1 & -2 & -1 \\\\\\\\ 0 & 0 & 0 \\\\\\\\ +1 & +2 & +1 \\\\end{bmatrix} \\\\]\\n\\n4: Apply Sobel kernels:\\n\\n\\\\[ x_{dx} = x_g \\\\ast S_x \\\\]\\n\\n\\\\[ x_{dy} = x_g \\\\ast S_y \\\\]\\n\\n\\\\( \\\\ast \\\\): the 2-dimensional convolution operation\\n\\n5: The edge magnitude:\\n\\n\\\\[ x_{shape} = x_{dx}^2 + x_{dy}^2 \\\\]\\n\\n6: Down-sample to original image size:\\n\\n\\\\[ x_{shape} = ds(x_{shape}) \\\\]\\n\\nFigure S3 displays few examples of applying the Sobel operator on the original RGB images. The Sobel output is fed to the IBL model.\"}"}
{"id": "wPLEzBcSC7p", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure S3: Visual examples of the shape images using Sobel operator\\n\\nC CCL A\\n\\nAlgorithm 2\\nCognitive Continual Learner (CCL)\\n\\nInput:\\nDataset $D_t$, Buffer $B$\\n\\nInitialize:\\nThree networks: Encoder and classifier $f$ parameterized by $\\\\theta_{WM}$, $\\\\theta_{SM}$, and $\\\\theta_{IBL}$\\n\\n1: for all tasks $t \\\\in 1, 2, ..., T$ do\\n2: Sample mini-batch: $(x_c, y_c) \\\\sim D_t$\\n3: Extract shape images: $x_{cs} = IB(x_c)$ where $IB$ is a Sobel filter\\n4: $L_{Sup\\\\ WM} = L_{CE}(f(x_c; \\\\theta_{WM}), y_c)$\\n5: $L_{Sup\\\\ IBL} = L_{CE}(f(x_{cs}; \\\\theta_{IBL}), y_c)$\\n6: if $B \\\\neq \\\\emptyset$ then\\n7: Sample mini-batch: $(x_b, y_b) \\\\sim B$\\n8: Extract shape images: $x_{bs} = IB(x_b)$\\n9: Calculate the supervised loss:\\n10: $L_{Sup\\\\ WM} += L_{CE}(f(x_b; \\\\theta_{WM}), y_b)$\\n11: $L_{Sup\\\\ IBL} += L_{CE}(f(x_{bs}; \\\\theta_{IBL}), y_b)$\\n12: Knowledge sharing from semantic memory to working model and inductive bias learner:\\n13: $L_{KS\\\\ WM} = E \\\\| f(x_b; \\\\theta_{SM}) - f(x_b; \\\\theta_{WM}) \\\\|^2$\\n14: $L_{KS\\\\ IBL} = E \\\\| f(x_b; \\\\theta_{SM}) - f(x_{bs}; \\\\theta_{IBL}) \\\\|^2$\\n15: Bidirectional knowledge sharing between working model and inductive bias learner:\\n16: $L_{biKS} = E_{x \\\\sim D_t \\\\cup B} \\\\| f(x; \\\\theta_{WM}) - f(x_{s}; \\\\theta_{IBL}) \\\\|^2$\\n17: Calculate total loss:\\n18: $L_{WM} = L_{Sup\\\\ WM} + \\\\lambda L_{biKS} + \\\\lambda' L_{KS\\\\ WM}$\\n19: $L_{IBL} = L_{Sup\\\\ IBL} + \\\\gamma L_{biKS} + \\\\gamma' L_{KS\\\\ IBL}$\\n20: Update both working model and inductive bias learner:\\n21: $\\\\theta_{WM}, \\\\theta_{IBL}$\\n22: Stochastically update semantic memory:\\n23: Sample $s \\\\sim U(0, 1)$;\\n24: if $s < r$ then\\n25: $\\\\theta_{SM} = d \\\\cdot \\\\theta_{SM} + (1 - d) \\\\cdot \\\\theta_{WM}$\\n26: Update memory buffer\\n27: return model $\\\\theta_{SM}$\"}"}
{"id": "wPLEzBcSC7p", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We evaluate all methods in different CL settings. Van de Ven & Tolias (2019) describes three different settings based on increasing difficulty: task incremental learning (Task-IL), domain incremental learning (Domain-IL), and class incremental learning (Class-IL). In Class-IL, each new task consists of novel classes, and the network must learn both new classes while retaining information about the old ones. Task-IL is similar to Class-IL but assumes that the task labels are accessible at both training and inference. In Domain-IL, the classes remain the same for each task, but the distribution varies for each task. We report the results for all three settings on the relevant datasets. Class-IL is relatively the most complex setting of the three and is widely studied; however, there are some assumptions that simplify this setting to be realistic. Mi et al. (2020) highlighted some of the limitations of Class-IL, such as the assumption of the same number of classes across different tasks, no reappearance of classes, and the sample distribution per class is well balanced. Hence, Generalized Class-IL (GCIL) was suggested to overcome these limitations and introduce a more realistic setting. GCIL is a more generalized CL setting, where the number of classes in each task is not fixed, and the classes can reappear with varying sample sizes. GCIL samples the number of classes and samples from a probabilistic distribution. The two variations are Uniform (fixed uniform sample distribution over all classes) and Longtail (with class imbalance).\\n\\nWe report results on all three settings: Task-IL, Domain-IL, and Class-IL. Furthermore, we also consider the GCIL setting for one of the datasets as an additional evaluation setting. All reported results are averaged over three random seeds.\\n\\n**Hyperparameters**\\n\\nWe utilize a small validation set to tune the hyperparameters for all methods. For Seq-CIFAR10, we report the results of the original articles (Buzzega et al., 2020; Cha et al., 2021; Caccia et al., 2021; Arani et al., 2021). For the other datasets, we ran a grid search over the hyperparameters reported in the paper for a similar dataset. For Seq-CIFAR100 and GCIL-CIFAR100, we formed the search range using the Seq-CIFAR10 hyperparameters as a reference point. Search ranges are shown in Table S3. Domain dataset is more complex compared to the CIFAR versions and includes images of larger sizes. Hence, we consider the Seq-TinyImagenet hyperparameters in the respective paper as the reference point for further tuning. The learning rate $lr$, the number of epochs, and the batch size are similar across the datasets. The ema update rate $r$ is lower for more complex datasets, as shown in CLS-ER. $r$ is chosen in the range of $[0.01, 0.1]$ with a step size of $0.02$ for CLS-ER and CCL. The different hyperparameters chosen for the baselines, after tuning, are shown in Table S4.\\n\\nThe different hyperparameters chosen for CCL are shown in Table S5. The parameters: $lr$, batch size, number of epochs are uniform across all datasets. The stochastic update rate and decay parameter are similar to CLS-ER. The hyperparameters and are stable across settings and datasets and also compliment each other. The loss balancing weights are reported as four different parameters for clarity, however, they show similar pattern. Therefore, CCL does not require extensive fine-tuning across different datasets and settings.\\n\\n**Complexity**\\n\\nWe discuss the computational complexity aspect of our proposed method. CCL involves three networks during training; however, in inference, only a single network is used (SM module). Therefore, for inference purposes, the MAC count, the number of parameters, and computational capacity remain the same as the other single-network methods.\\n\\nThe training cost requires three forward passes, as it consists of three different modules. ER, DER++, CO$^2$L and ER-ACE have a single network. CLS-ER also has three networks and therefore requires 3 forward passes. CCL has training complexity similar to CLS-ER; however, it outperforms CLS-ER in all provided metrics.\"}"}
{"id": "wPLEzBcSC7p", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table S3: Search ranges for tuning hyperparameters\\n\\n| Method     | Hyperparameters | Search Range     |\\n|------------|-----------------|------------------|\\n| ER         | lr              | [0.01, 0.03, 0.1, 0.5] |\\n| DER++      | lr              | [0.01, 0.03, 0.1] |\\n|            | \u03b1               | [0.1, 0.2, 0.5]  |\\n|            | \u03b2               | [0.5, 1.0]       |\\n| CO2        | lr              | [0.01, 0.03, 0.1] |\\n|            | \u03c4               | [0.01, 0.1, 0.5] |\\n|            | k               | [0.2, 0.5]       |\\n|            | k*              | [0.01, 0.05]     |\\n|            | e               | [100, 150]       |\\n| ER-ACE     | lr              | [0.01, 0.03, 0.1, 0.5] |\\n| CLS-ER     | lr              | [0.01, 0.03, 0.1] |\\n|            | \u03bb               | [0.1, 0.2, 0.3]  |\\n|            | r               | [0.1:0.1]        |\\n|            | r's             | [0.1:0.1]        |\\n|            | \u03b1p              | [0.99, 0.999]    |\\n|            | \u03b1s              | [0.99, 0.999]    |\\n| CCL        | lr              | [0.01, 0.03, 0.1] |\\n|            | r              | [0.1:0.1]        |\\n|            | d               | [0.99, 0.999]    |\\n|            | \u03bb               | [0.01, 0.1]      |\\n|            | \u03b3               | [0.01, 0.1]      |\\n|            | \u03bb'              | [0.01, 0.1]      |\\n|            | \u03b3'              | [0.01, 0.1]      |\\n\\nOn the memory front, similar to all methods, we save memory samples based on the memory budget allotted (200 and 500 in the experiments). There are no additional memory requirements, as we do not save any extra information (such as logits) to be used later in our objectives.\\n\\n**GOTERMICS**\\nForward transfer, backward transfer, and forgetting are other metrics (Lopez-Paz & Ranzato, 2017) used in CL literature. These metrics are estimated from the model checkpoint after a task is completed, as this checkpoint has the highest accuracy for that particular task. However, this does not hold true for our method, which utilizes the stochastically updated model for inference and evaluation purposes. The SM module assimilates knowledge from the working model and is updated stochastically by the exponential moving average. It achieves highest accuracy on previous tasks while also learning the new tasks. Therefore, the results may be misleading.\\n\\nHowever, we evaluate backward transfer ($-1 \\\\times$ forgetting) by considering the best accuracy of the SM module after a particular task and then finding the difference between this and the final accuracy. Taking into account Figure S2, if we use the backward metric formula directly, we get a positive backward transfer for CCL in Tasks 3 and 4 (due to the SM model achieving high accuracy in the previous task while also learning the new task at that stochastic update frequency); therefore, we pick the maximum one and subtract it from the last row. We report the values for the metrics in Table S7. CCL fares better in backward transfer (or forgetting) compared to other techniques.\\n\\n**EXTENDED RELATED WORKS**\\nOne of the modules (IBL) in CCL utilizes the inductive bias in terms of shape to produce more generic representations. There are several works Geirhos et al. (2018) that showcase the texture bias problem of neural networks. Several techniques have been introduced to reduce texture bias.\"}"}
{"id": "wPLEzBcSC7p", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure S8: Number of samples per supercategory for each domain in the DN4IL dataset.\"}"}
