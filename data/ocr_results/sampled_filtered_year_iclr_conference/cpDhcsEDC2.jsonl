{"id": "cpDhcsEDC2", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Visualizations of word-patch alignment for 4 classes of the ImageNet dataset and \\\"a photo of a \\\\{label\\\\}\\\" is the prompt. Numbers in the parentheses after the class label indicate the location indices of the class label in the tokenized textual sequence. The correct predictions are highlighted by opaque patches with the class label indices in red.\\n\\nVisualization Method.\\nThe word-patch alignment is performed based on the token-wise similarity between the image patches and textual tokens. Specifically, for the $k$-th image patch, the location index of textual token with the largest similarity with it ($m^k_I$ in Equation (4)) is considered as its predicted label, and is placed at the center of it. Take class \\\"balloon\\\" as an example. There are 8 tokens in the tokenized textual sequence \\\"$[BOS] a photo of a balloon. [EOS] \\\\\\\"$, and the location index of the class label \\\"balloon\\\" is \\\"5\\\". Note that one class label may be tokenized to more than one token. Location indices of textual tokens corresponding to the class label are highlighted in red, while the others are marked in white. A desired model that learns fine-grained representations would predict image patches of the target object to red indices.\\n\\nObservations.\\nFigure 2 shows the word-patch alignment results for FILIP and CLIP on 4 classes from the ImageNet dataset. As can be seen, FILIP exhibits the finer-grained understanding of an image in the following aspects. (i) A single object: From the visualization of class \\\"small white butterfly\\\", the image patches covering the object are all classified correctly; (ii) Same object in different shapes: From the visualizations of class \\\"balloon\\\" and \\\"lifeboat\\\", image patches corresponding to all target objects with different shapes and locations are correctly classified; (iii) Key Components of an object: For class \\\"electric locomotive\\\", there are two key components crucial to correctly classifying the image, i.e., \\\"electric\\\" and \\\"locomotive\\\", whose corresponding textual token indices are \\\"5\\\" and \\\"6\\\", respectively. As can be seen, image patches matching these two key components are respectively correctly classified. On the other hand, CLIP can not correctly align image patches with corresponding textual tokens. Compared with Kim et al. (2021) which uses an extra optimal transport to align the textual word and image patch distributions, the word-patch alignment can be simply automatically learned by our method.\\n\\nCONCLUSION AND FUTURE WORK\\nThis paper introduces FILIP, a simple yet generic framework towards fine-grained vision-language pre-training. By using a token-wise maximum similarity, our method learns fine-grained representation for patches in the images and words in the sentences. While it achieves competitive results against several large-scale multi-modal pre-training on various downstream tasks, both its architecture and training procedure can still be optimized to improve its performance. In the future, a more advanced image encoder as well as a well-designed interaction layer can be used to boost the performance. Furthermore, we can further add more masked language/image loss to support more generation tasks. To this end, we hope to extend FILIP as a generic and unified interface for solving a large variety of vision-language tasks.\"}"}
{"id": "cpDhcsEDC2", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in neural information processing systems, 2020.\\n\\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\\n\\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. preprint arXiv:1604.06174, 2016.\\n\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European conference on computer vision, pp. 104\u2013120. Springer, 2020.\\n\\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 113\u2013123, 2019.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2019.\\n\\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. arXiv preprint arXiv:2105.13290, 2021.\\n\\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE international conference on computer vision, pp. 1422\u20131430, 2015.\\n\\nXiao Dong, Xunlin Zhan, Yangxin Wu, Yunchao Wei, Xiaoyong Wei, Minlong Lu, and Xiaodan Liang. M5product: A multi-modal pretraining benchmark for e-commercial product downstream tasks. Preprint arXiv:2109.04275, 2021.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.\\n\\nZhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. arXiv preprint arXiv:2006.06195, 2020.\\n\\nAndreas Griewank and Andrea Walther. Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation. ACM Transactions on Mathematical Software (TOMS), 26(1):19\u201345, 2000.\\n\\nElad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment your batch: Improving generalization through instance repetition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8129\u20138138, 2020.\\n\\nZhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. Seeing out of the box: End-to-end pre-training for vision-language representation learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12976\u201312985, 2021.\\n\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448\u2013456. PMLR, 2015.\\n\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, 2021.\"}"}
{"id": "cpDhcsEDC2", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "cpDhcsEDC2", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "cpDhcsEDC2", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 16: Top-1 accuracy(%) on image classification on 12 datasets.\\n\\n| Dataset          | CLIP | CLIP rep | FILIP base |\\n|------------------|------|----------|------------|\\n| CIFAR10          | 91.3 | 82.0     | 86.9       |\\n| CIFAR100         | 65.1 | 57.5     | 65.5       |\\n| Caltech101       | 87.9 | 89.9     | 91.9       |\\n| StanfordCars     | 59.4 | 45.1     | 55.4       |\\n| Flowers102       | 66.7 | 80.7     | 85.3       |\\n| Food101          | 84.4 | 75.1     | 82.8       |\\n| SUN397           | 63.2 | 63.6     | 69.1       |\\n| DTD              | 44.5 | 46.7     | 49.3       |\\n| Aircrafts        | 21.2 | 33.7     | 57.2       |\\n| OxfordPets       | 87.0 | 82.7     | 88.1       |\\n| EuroSAT          | 49.4 | 49.0     | 49.9       |\\n| ImageNet         | 63.2 | 64.2     | 68.8       |\\n| Average          | 65.3 | 64.2     | 70.9       |\\n\\nTable 17: Comparison on performance and inference time of image-text retrieval on Flickr30K and MSCOCO datasets.\\n\\n| Dataset  | Recall R@1 | Recall R@5 | Recall R@10 | Inference time |\\n|----------|------------|------------|-------------|----------------|\\n| Flickr30K| 67.4       | 90.3       | 95.8        | 4.47s          |\\n| MSCOCO   | 88.0       | 98.7       | 99.4        | 24ms           |\\n\\nFor image retrieval, we precompute the text features and report the inference time for one image query, which contains (i) the time to extract the feature of one image query, and (ii) the time of similarity calculation with all texts and ranking. The test set of Flickr30k contains 1000 images and 5000 texts, while the test set of COCO contains 5000 images and 25000 texts. The time is averaged over 1000 runs.\\n\\nResults. The inference time of retrieval is shown in Table 17. Benefitting from the efficiency optimizations (i.e., FP16 quantization and reduced feature dimension) in Section 3.1, the inference time of FILIP is close to CLIP. In image retrieval, SCAN is slightly faster than FILIP on Flickr30K with 1000 images, because SCAN uses a lightweight GRU as the text encoder. However, SCAN is much slower than FILIP (i.e., about 17ms slower per query) on MSCOCO with more (i.e., 5000) images because of the slower computation involved in the two-stage stacked cross-attention when computing the similarity. For text retrieval, SCAN is much slower than FILIP and its own image retrieval, mainly due to three reasons: (i) the image encoder is a Faster RCNN which is more expensive than the lightweight GRU text encoder; (ii) the text candidates are 5 times more than the image candidates in image retrieval; and (ii) the similarity computation of SCAN relies on the cross-attention computation, which is not straightforward to be paralleled, even in their official code; while our FILIP\u2019s similarity computation is simply a matrix multiplication and is readily optimized on most modern hardwares.\"}"}
{"id": "cpDhcsEDC2", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020.\\n\\nFei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil: Knowledge enhanced vision-language representations through scene graphs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3208\u20133216, 2021.\\n\\nXunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu, and Xiaodan Liang. Product1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining. In International Conference on Computer Vision, 2021.\\n\\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5579\u20135588, 2021.\"}"}
{"id": "cpDhcsEDC2", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Ethical Issues in Data Collection\\n\\nWhen collecting the large-scale image-text pairs from the Internet, we perform person-name substitutions as in Changpinyo et al. (2021), in order to protect the privacy of the individuals appearing in the text. Specifically, we replace each person name appeared in the text with a special <person> token. Besides, we also discard image-text pairs whose text contains sensitive words.\\n\\nA.2 Datasets Summary\\n\\nTable 6 shows the number of image-text pairs of each dataset used in different pre-training methods.\\n\\n| Dataset  | FILIP300M | CLIP | ALIGN |\\n|----------|-----------|------|-------|\\n| CC3M     | 3M        | 10M  | 26M   |\\n| CC12M    |           |      |       |\\n| YFCC100M |           |      |       |\\n\\nA.3 Detailed Experimental Settings\\n\\nTable 7: The architecture parameters for FILIP models.\\n\\n| Model   | Embedding Dimension | Image Resolution | Image Encoder Layers | Text Encoder Layers | Text Encoder Width | Text Encoder # Heads |\\n|---------|---------------------|------------------|----------------------|--------------------|-------------------|---------------------|\\n| FILIP   | 256                 | 224\u00d7224          | 12                   | 12                 | 768               | 12                  |\\n| FILIP large | 256                | 224\u00d7224          | 24                   | 16                 | 1024              | 12                  |\\n\\nModel Architectures. We follow the same architecture design as CLIP, for both FILIP base and FILIP large, except that we reduce the embedding dimension from 512/768 to 256 for the efficiency of loss computation. Table 7 describes the details of architectures.\\n\\nDetails for Pre-training and Hyperparameters.\\n\\nFor the implementation of the contrastive loss, following CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), we also set the temperature in the softmax function to be a learnable parameter and initialize it as 0.07. For the pre-training, we use the LAMB optimizer implemented by the cybertronai's open-source repository (https://github.com/cybertronai/pytorch-lamb). For the learning rate scheduler, we first assign a base learning rate and then linearly warm it up to the peak learning rate according to the effective total batch size by a square root strategy, peak lr = base lr \u00d7 \u221atotal bs. We note that a large weight decay is crucial to stabilize training and improve generalization. Specifically, we found that the training stability is a challenging issue when applying mix-precision training to large-scale models, i.e., the training is extremely unstable and the NaN loss easily happens. Recent works...\"}"}
{"id": "cpDhcsEDC2", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Model- and dataset-specific hyperparameters used for FILIP pre-training. Numbers in batch size represent the total batch size across all workers and are calculated as: batch size per GPU \u00d7 #GPUs. FILIP340M is the combination of FILIP300M, YFCC100M, CC12M and CC3M.\\n\\n| Model     | Dataset | Batch size | Base LR | Weight decay |\\n|-----------|---------|------------|---------|--------------|\\n| FILIP base | YFCC100M | 1024 \u00d7 8 6, 10^-3 | 3e-2     |\\n| FILIP base | FILIP340M | 320 \u00d7 128 2, 10^-3 | 3e-3     |\\n| FILIP large | FILIP340M | 160 \u00d7 192 1.5, 10^-3 | 3e-3     |\\n\\nDALL-E (Ramesh et al., 2021) and Cogview (Ding et al., 2021) also notice this issue and provide their solutions. However, we found that simply increasing the weight decay and applying the trick of removing the weight decay of specific parameters as described in Section 4.1 work for our case. The base learning rate and weight decay are selected manually via observing the performance at the early training stage. Table 8 summarizes the common hyperparameters and Table 9 shows the model- and dataset-specific hyperparameters for FILIP pre-training.\\n\\nDetails for Image-text Retrieval.\\n\\nFollowing previous works (Jia et al., 2021; Li et al., 2021a), for Flickr30K, we test on the 1K test set with or without fine-tuning on the 30K training set, while for MSCOCO, we test on the 5K test set with or without fine-tuning on the 113K training set. We use the similarity between image and text for ranking and use the contrastive loss for fine-tuning. Since there are multiple texts for each image in these two datasets, we change the ground-truth label of contrastive loss to consider multiple positives, by assigning a probability of 1/#positive to each positive following ALBEF (Li et al., 2021a). Besides, we also use prompts during evaluation for both datasets, see Appendix A.5 for details. Table 10 shows the hyperparameters for image-text retrieval fine-tuning.\\n\\nTable 10: Hyperparameters used for image-text retrieval fine-tuning.\\n\\n| Hyperparameter | Value |\\n|----------------|-------|\\n| Image size     | 392 \u00d7 392 |\\n| Training epochs | 3     |\\n| Optimizer      | LAMB  |\\n| Batch size     | 5120  |\\n| Base LR        | 2 \u00d7 10^{-4} |\\n| Weight decay   | 3 \u00d7 10^{-4} |\\n\\nA.4 MORE VISUALIZATIONS OF WORD-PATCH ALIGNMENT AND GRAD-CAM HEATMAPS\\n\\nIn Figure 3, we visualize the cross-modal alignment of the proposed method for more images, in terms of both word-patch alignment as described in Section 4.5 and Grad-CAM heatmaps (Selvaraju et al., 2017). We compute the Grad-CAM heatmaps based on the average self-attention maps over the image patches classified to targeted textual tokens (i.e., the textual token(s) corresponding to the class label in the ImageNet dataset) in the last layer of the image encoder. We average the heatmaps over all attention heads. As can be seen, our proposed model learns meaningful alignment between image patches and textual tokens.\\n\\nA.5 PROMPT TEMPLATES FOR DOWNSTREAM TASKS\\n\\nImage Classification.\\n\\nTable 11 shows the prompt templates for different image classification datasets in the form of \\\"[prefix] {label}, [category description]. [suffix].\\\" in Equation (6). There are three components to be determined in the template, i.e., the prefix, the category description and the suffix. For each component, we select several well-performed ones for each dataset. Then we use the full combinations of all three components as the set of prompt templates for ensemble. For instance, we use 5 prefixes, no category descriptions, and 6 suffixes for dataset ImageNet. Then the total number of prompt templates for this dataset is: 5 \u00d7 1 \u00d7 6 = 30.\"}"}
{"id": "cpDhcsEDC2", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3: More visualizations on different classes of ImageNet dataset. Numbers in the parentheses after the class label indicate the location indices of class label in the tokenized textual sequence.\"}"}
{"id": "cpDhcsEDC2", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 11: Prompt templates used for 12 downstream image classification tasks.\\n\\n| Dataset        | Prefix                          | Category description | Suffix                                      |\\n|----------------|---------------------------------|----------------------|---------------------------------------------|\\n| CIFAR10        | \\\"a photo of a\\\", \\\"a jpeg photo of a\\\", \\\"a painting of a\\\", \\\"itap of a\\\", \\\"graffiti of a\\\", \\\"a cartoon\\\", \\\"a doodle\\\" | None                 | None, \\\"It's common in daily life\\\", \\\"It's cute\\\", \\\"It's ugly\\\", \\\"It's weird\\\", \\\"Hope you like it\\\" |\\n| CIFAR100       | \\\"a jpeg photo of a\\\", \\\"a painting of a\\\", \\\"a good photo of a\\\", \\\"a bad photo of a\\\", \\\"a photo of a\\\", \\\"itap of a\\\", \\\"a rendering of a\\\" | None                 | None, \\\"It's common in daily life\\\", \\\"It's beautiful\\\", \\\"It's ugly\\\", \\\"I like it\\\", \\\"I take it today\\\" |\\n| Caltech101     | \\\"a photo of a\\\", \\\"a cropped photo of a\\\", \\\"a good photo of a\\\", \\\"a bad photo of a\\\" | None                 | None, \\\"I like it\\\", \\\"I hate it\\\", \\\"It's ugly\\\", \\\"It's cute\\\" |\\n| Stanford-Car   | \\\"a photo of a\\\", \\\"a close-up photo of a\\\", \\\"a good photo of a\\\", \\\"a bad photo of a\\\" | \\\"a type of car\\\", \\\"a type of automobile\\\" | \\\"I like it\\\", \\\"It belongs to my friend\\\", \\\"It's brand new\\\", \\\"It's popular recently\\\", \\\"It's important to me\\\", \\\"I take it today\\\" |\\n| Flowers102     | \\\"a photo of a (many)\\\", \\\"a rendering of a (many)\\\", \\\"itap of a (many)\\\" | \\\"a type of flower\\\", \\\"a type of bloom\\\" | \\\"It's beautiful\\\", \\\"It's from my best friend\\\", \\\"It gives out a sweet perfume/fragrance\\\" |\\n| ImageNet       | \\\"a photo of a\\\", \\\"a good photo of a\\\", \\\"a bad photo of a\\\", \\\"a close-up photo of a\\\", \\\"itap of a\\\" | None                 | \\\"I like it\\\", \\\"It's common in daily life\\\", \\\"It's not common in daily life\\\", \\\"It's ugly\\\", \\\"It's cute\\\", \\\"It's beautiful\\\" |\\n| Food101        | \\\"a photo of my\\\", \\\"a close-up photo of my\\\", \\\"itap of my\\\" | \\\"a type of food\\\", \\\"a type of nourishment\\\" | \\\"I made it today\\\", \\\"I like it\\\", \\\"I hate it\\\", \\\"It's delicious\\\", \\\"It's with nice flavour\\\", \\\"It's with terrible flavour\\\", \\\"It's popular recently\\\" |\\n| SUN397         | \\\"a photo of a\\\", \\\"a good photo of a\\\", \\\"a bad photo of a\\\", \\\"a bright photo of a\\\", \\\"a dark photo of a\\\", \\\"a black and white photo of a\\\", \\\"a nice scene of a\\\", \\\"a terrible scene of a\\\" | None                 | None, \\\"I like it\\\", \\\"I hate it\\\", \\\"It's beautiful\\\", \\\"It's common in daily life\\\", \\\"It's important to me\\\" |\\n| DTD            | \\\"itap of a\\\", \\\"a close-up photo of a\\\" | \\\"texture\\\", \\\"surface\\\", \\\"material\\\" | None, \\\"It's out of style\\\", \\\"It's popular in old days\\\", \\\"It's ugly\\\", \\\"It's beautiful\\\" |\\n| Aircrafts      | \\\"a photo of the\\\", \\\"a close-up photo of the\\\", \\\"a good photo of the\\\", \\\"a pixelated photo of the\\\" | \\\"a type of plane\\\", \\\"a type of aircraft\\\", \\\"a type of airliner\\\" | None, \\\"I like it\\\", \\\"It's important to me\\\", \\\"I take it today\\\", \\\"Hope you like it\\\" |\\n| Oxford Pet     | \\\"a photo of my\\\", \\\"a low resolution photo of my\\\", \\\"a good photo of my\\\" | \\\"a type of pet\\\", \\\"a type of dog or cat\\\" | None, \\\"It's cute\\\", \\\"It's important to me\\\", \\\"I like it\\\", \\\"It's beautiful\\\" |\\n| EuroSAT        | \\\"a photo of a\\\", \\\"a painting of a\\\", \\\"a cropped photo of a\\\", \\\"a good photo of a\\\", \\\"a blurry photo of a\\\" | None, \\\"an example of aerial or satellite images\\\" | None, \\\"I like it\\\", \\\"It's taken from an aircraft or some flying object\\\", \\\"It's collected by imagining satellites\\\" |\"}"}
{"id": "cpDhcsEDC2", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12: Prompt templates used for zero-shot image-text retrieval on Flickr30K and MSCOCO datasets.\\n\\n| Dataset     | Task                        | Prefix          | Suffix       |\\n|-------------|-----------------------------|-----------------|--------------|\\n| Flickr30K   | image-to-text retrieval     | \u201ca good photo of the\u201d | \u201cI hate it.\u201d |\\n|             | text-to-image retrieval     | \u201ca good photo of\u201d | None         |\\n| MSCOCO      | image-to-text retrieval     | \u201ca good photo of\u201d | \u201cIt is ugly.\u201d |\\n|             | text-to-image retrieval     | None            | None         |\\n\\nImage-text Retrieval. Following CLIP (Radford et al., 2021), we use prompt in zero-shot image-text retrieval for both Flickr30K and MSCOCO datasets. The prompt is selected by the same rule as described in Section 3.1.2, except that we do not use \u201c[category description]\u201d here. Table 12 shows the prompt templates for zero-shot image-text retrieval on Flickr30K and MSCOCO datasets.\\n\\nA.6 LINEAR PROBE ON IMAGE CLASSIFICATION\\n\\nIn this section, we evaluate FILIP on the linear probe for image classification. Following common linear probe setting, we freeze the whole backbone network and only finetune the last linear classifier. Since we remove the \u201c[CLS]\u201d token in our vision encoder, we apply a mean pooling over all the other visual tokens to aggregate them into a global image representation which is then fed into the linear classifier.\\n\\nSetting. Following CLIP, we train the logistic regression classifier using scikit-learn\u2019s L-BFGS implementation (Pedregosa et al., 2011), with maximum 1,000 iterations on those 11 datasets except ImageNet. For ImageNet, we use a pytorch-based codebase to accelerate the training with GPU. Following Doersch et al. (2015), we adopt a Batch Normalization (Ioffe & Szegedy, 2015) layer before the linear classifier which is beneficial to stabilize the mixed-precision training. Random resized crop and horizontal flipping are used to augment training data. We use the cosine learning rate scheduler with a linear warmup of 10 epochs. More hyperparameters used in linear probe on ImageNet are shown in Table 13.\\n\\nTable 13: Hyperparameters used for linear probe image classification on ImageNet.\\n\\n| Hyperparameter | Value |\\n|----------------|-------|\\n| Image size     | 224 \u00d7 224 |\\n| Training epochs| 90    |\\n| Optimizer      | SGD   |\\n| Batch size     | 4096  |\\n| Base LR        | 0.1   |\\n| Weight decay   | 0     |\\n\\nResults. Table 14 compares the linear probe performance of our proposed FILIP with CLIP over 12 datasets. Our FILIP base (resp. FILIP large) achieves 85.5% (resp. 91.0%) average Top-1 accuracy over 12 downstream tasks, which provides noticeable improvements, i.e., 1.8% (resp. 1.2%) higher, compared to its CLIP\u2019s counterpart. This implies that our FILIP learns more powerful vision features which may potentially facilitate border downstream vision tasks.\\n\\nA.7 COMPARISON WITH KHATTAB & ZAHARIA (2020)\\n\\nAs is stated in Section 3.1, compared to Khattab & Zaharia (2020), besides being the first to apply the late interaction to contrastive learning for vision-language pre-training, we make two other modifications, i.e., removing padded tokens and using average over non-padded tokens instead of summation. In the following, we show that these two modifications are crucial to the performance, and the quality of finer-granular word-patch alignment.\\n\\nFor comparison, we replace the proposed cross-modal late interaction in FILIP base with the original late interaction in Khattab & Zaharia (2020). Following the setting in Section 4.4, we pre-train on 18\"}"}
{"id": "cpDhcsEDC2", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 14: Top-1 accuracy(%) of linear probe on image classification on 12 datasets. Our FILIP outperforms CLIP by 1.2 \u223c 1.8% points on average.\\n\\n| Dataset         | CLIP-ViT-B/32 | CLIP-ViT-L/14 | FILIP base | FILIP large |\\n|-----------------|---------------|---------------|------------|-------------|\\n| CIFAR10         | 95.1          | 98.0          | 95.3       | 97.9        |\\n| CIFAR100        | 80.5          | 87.5          | 80.3       | 87.0        |\\n| Caltech101      | 93.0          | 96.5          | 95.0       | 97.2        |\\n| StanfordCars    | 81.8          | 90.9          | 78.6       | 89.0        |\\n| Flowers102      | 96.6          | 99.2          | 98.7       | 99.6        |\\n| Food101         | 88.8          | 95.2          | 86.2       | 94.6        |\\n| SUN397          | 76.6          | 81.8          | 77.9       | 83.2        |\\n| DTD             | 76.5          | 69.4          | 78.1       | 84.8        |\\n| Aircrafts       | 52.0          | 95.1          | 88.0       | 93.5        |\\n| OxfordPets      | 90.0          | 95.1          | 97.0       | 97.3        |\\n| EuroSAT         | 97.0          | 98.2          | 76.1       | 83.9        |\\n| ImageNet        | 76.1          | 83.9          | 85.5       | 91.0        |\\n\\nEffect to Performance.\\n\\nTable 15 shows the comparison on zero-shot ImageNet classification. When these two modifications are removed, the zero-shot Top-1 accuracy of ImageNet drops from 34.3 to 32.7.\\n\\nEffect to the Word-patch Alignment\\n\\nIn Figure 4, we compare the word-patch alignment using the models trained with the proposed cross-modal late interaction and the late interaction in Khattab & Zaharia (2020). According to the visualizations, using the original late interaction in Khattab & Zaharia (2020) leads to less accurate word-patch alignment. Specifically, the object patches are often aligned to the padded tokens instead of class names. We speculate this is because the padded tokens learn similar representations as existing key textual tokens, similar to the finding in Section 3.2 of Khattab & Zaharia (2020) that padding with masked tokens (which is called \u201cquery augmentation\u201d in Khattab & Zaharia (2020)) tend to \u201cre-weigh existing terms based on their importance for matching the query\u201d.\\n\\nA.8 ABLATION ON THE FULL PRE-TRAINING DATASET\\n\\nIn Table 16, we compare the proposed cross-modal late interaction loss with the original CLIP loss (Radford et al., 2019) on the full pre-training dataset introduced in Section 3.3. In Table 16, CLIP denotes the results reported by CLIP paper, CLIP rep is our reproduced CLIP version with the original contrastive loss using exactly the same architecture on the same pre-training dataset as FILIP base. As can be seen, the FILIP base has 6.7 points higher average accuracy than the CLIP rep over 12 datasets. This further verifies that the performance gain of FILIP comes from the proposed cross-modal late interaction, rather than the data or architecture.\\n\\nA.9 INFERENCE TIME OF IMAGE-TEXT RETRIEVAL\\n\\nSetting.\\n\\nIn this section, we test the inference time of both image retrieval and text retrieval on the test set of Flickr30K and MSCOCO. We compare our proposed model FILIP large against SCAN (Lee et al., 2018) and CLIP (ViT-L/14) (Radford et al., 2021). We test the inference time of CLIP and SCAN using their released code. For image retrieval, we precompute the image features and report the inference time for one text query, which contains (i) the time to extract the feature of one text query, and (ii) the time of similarity calculation with all images and ranking. Similarly, for text retrieval, we precompute the text features and report the inference time for one text query, which contains (i) the time to extract the feature of one text query, and (ii) the time of similarity calculation with all images and ranking.\"}"}
{"id": "cpDhcsEDC2", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 4: Comparison of word-patch alignment between the proposed cross-modal late interaction and that in ColBERT (Khattab & Zaharia, 2020). \\\"a photo of a \\\\{ label \\\\}\\\" is the prompt. Numbers in the parentheses after the class label indicate the location indices of the class label in the tokenized textual sequence. The correct predictions to the class labels are highlighted by opaque patches with the class label indices in red. Incorrect predictions to the padded tokens are highlighted by opaque patches with the padded token indices in blue.\"}"}
{"id": "cpDhcsEDC2", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"exclude the padded textual tokens when computing the similarity, as they harm the performance. We speculate that this is because these padded tokens also learn textual representations and will mislead the model to align image patches to these meaningless padded tokens rather than meaningful non-padded words. Secondly, when computing similarities (4) and (5), we use the average of the token-wise maximum similarities instead of summation in (Khattab & Zaharia, 2020). This is because the number of non-padded tokens varies from text to text, and this summation over all non-padded tokens can have quite different magnitudes, leading to less stabilized training and worse final performance. These two modifications are crucial to not only the downstream tasks' performance, but also the quality of the word-patch alignment. A more detailed discussion can be found in Appendix A.7. Thirdly, we optimize the late interaction mechanism via a contrastive loss (1) which is found powerful vision-language pre-training (Radford et al., 2021) instead of the original pairwise loss in (Khattab & Zaharia, 2020).\\n\\nTraining Efficiency. Though the cross-modal late interaction is able to capture finer-grained features compared with the original loss, it relies on the token-wise representations of both modalities, and can be inefficient in terms of communication, memory and computation, especially when the batch size is large. To alleviate this problem, we utilize several methods. Firstly, we reduce the embedding size to 256. Besides, we reduce the precision of the last-layer features of both modalities from fp32 to fp16 before node communication in a distributed learning setting, and perform the multiplication in Equations (4) and (5) under the reduced precision. In addition, since the complexity of similarity calculation scales with the sequence length of textual tokens and image patches, for each image (resp. text), we select the 25% tokens with the highest token-wise maximum similarity score (Equation (3)) among all texts (resp. images) in the same local worker before node communication, based on the intuition that each sample can be represented by a few of the most representative tokens. Effects of these modifications are studied in Section 4.4.\\n\\n3.1.2 PROMPT ENSEMBLE AND TEMPLATES\\n\\nDue to the problem of polysemy and inconsistency with the pre-training process, following Radford et al. (2021), we also use prompt templates to augment the original label for some downstream tasks. For visualizations, for simplicity, we use only one prompt template across the paper, i.e. \u201ca photo of a {label}\u201d as Radford et al. (2021). For other experiments, we report results using prompt ensemble following Radford et al. (2021). When multiple prompts are allowed, the token-wise representations of different prompt templates for the same class label are different, and can not be summed together to form a mean textual representation as in (Radford et al., 2021). Thus, instead of ensembling different prompt templates by their mean textual representation, we ensemble them by their mean token-wise similarity. Specifically, suppose there are $C$ prompt templates, each label is augmented to $C$ different texts $x_{T1}, x_{T2}, \\\\cdots, x_{TC}$. The similarity between an image $x_I$ and this label is computed as \\n\\n$$\\\\frac{1}{C} \\\\sum_{c=1}^{C} s_{I}, \\\\cdot (x_I, x_{Tc}),$$\\n\\nwhere $s_{I}, \\\\cdot$ is defined in Equation (4).\\n\\nWe use a unified rule-based method inspired by Radford et al. (2018) to construct prompt templates for image classification tasks. Specifically, each template consists of four components: [prefix] {label}, [category description], [suffix]. (6) Here, the \u201c[prefix]\u201d is an in-context description like \u201ca photo of a\u201d similar as Radford et al. (2021); \u201clabel\u201d is a class label of the dataset; \u201c[category description]\u201d describes the category which is found helpful for some fine-grained image classification datasets (Radford et al., 2021), e.g., \u201ca type of pet\u201d for dataset Oxford-IIIT Pets. An interesting finding is that, adding a suffix that includes the reference word \u201cit\u201d (e.g., \u201cI like it.\u201d) at the end of the prompt empirically improves the zero-shot classification performance of the proposed model. We speculate this is because the reference word \u201cit\u201d strengthens the fine-grained cross-modal alignment, as it can also be aligned to image patches of the target object. Detailed prompt templates for different datasets can be found in Appendix A.5.\\n\\n3.2 IMAGE AND TEXT AUGMENTATION\\n\\nTo obtain better generalization and data-efficiency of the model, we perform data augmentation on both images and texts during the pre-training phase to construct more image-text pairs. We apply AutoAugment (Krizhevsky et al., 2012; Sato et al., 2015; Cubuk et al., 2019; Hoffer et al., 2020) for image augmentation, following the SOTA vision recognition methods (Touvron et al., 2021; Xie et al., 2020b). To ensure the augmented texts are semantically similar as the original one, for text images.\"}"}
{"id": "cpDhcsEDC2", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Top-1 accuracy(%) of zero-shot image classification on 12 datasets. Our FILIP can boost \u223c5% accuracy on average.\\n\\n| Dataset          | CLIP-ViT-B/32 | CLIP-ViT-L/14 | FILIP base | FILIP large |\\n|------------------|---------------|---------------|------------|-------------|\\n| CIFAR10          | 91.3          | 96.2          | 86.9       | 95.7        |\\n| CIFAR100         | 65.1          | 77.9          | 65.5       | 75.3        |\\n| Caltech101       | 87.9          | 92.6          | 91.9       | 93.0        |\\n| StanfordCars     | 59.4          | 77.3          | 55.4       | 70.8        |\\n| Flowers102       | 66.7          | 78.7          | 85.3       | 90.1        |\\n| Food101          | 84.4          | 92.9          | 82.8       | 92.2        |\\n| SUN397           | 63.2          | 67.7          | 69.1       | 73.1        |\\n| DTD              | 44.5          | 55.3          | 49.3       | 60.7        |\\n| Aircrafts        | 21.2          | 36.1          | 57.2       | 60.2        |\\n| OxfordPets       | 87.0          | 93.5          | 88.1       | 92.0        |\\n| EuroSAT          | 49.4          | 59.9          | 49.9       | 59.2        |\\n| ImageNet         | 63.2          | 75.3          | 68.8       | 77.1        |\\n| Average          | 65.3          | 75.3          | 70.9       | 78.3        |\\n\\nFor augmentation, we rewrite the original text using back-translation (Xie et al., 2020a; Sennrich et al., 2016a). Specifically, the texts are first translated to the target language and then translated back to the source language. We choose German and Russian as the target language and get extra two texts for each image-text pair. When constructing a batch of image-text pairs during the pre-training, the text of each image-text pair is randomly sampled from the three candidate texts, i.e., the original text and two back-translated texts.\\n\\n3.3 PRETRAINING DATASET\\n\\nA sufficiently large image-text dataset is a prerequisite for vision-language pre-training. Recent CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) construct datasets with 400M and 1800M image-text pairs, respectively. In this work, we also collect a large-scale dataset called FILIP300M from the Internet, which consists of 300M image-text pairs and covers board vision and language concepts. For image-based filtering, we remove the images whose shorter dimension is smaller than 200 pixels and the aspect ratio is larger than 3. For text-based filtering, we keep only English texts, and exclude the meaningless ones, e.g., img_0.jpg. We also discard image-text pairs whose texts are repeated for over 10 times. Besides, we also use 3 public datasets, including Conceptual Captions 3M (CC3M) (Sharma et al., 2018), Conceptual 12M (CC12M) (Changpinyo et al., 2021) and Yahoo Flickr Creative Commons 100M (YFCC100M) (Thomee et al., 2016). We apply the same filtering rules on YFCC100M. Finally, we use about 340M image-text pairs for pre-training. Despite using a smaller training dataset than CLIP and ALIGN, our models still outperform them in most down-stream tasks (see Section 4).\\n\\n4 EXPERIMENTS\\n\\n4.1 EXPERIMENTAL SETUP\\n\\nModel Architectures. We train two models from scratch, i.e., FILIP base and FILIP large. The model architectures follow CLIP (Radford et al., 2021), i.e., the image encoder is ViT-B/32 for FILIP base and ViT-L/14 for FILIP large. More details can be found in Appendix A.3.\\n\\nPre-training Details. To save memory and scale up the batch size, automatic mixed-precision (Micikevicius et al., 2018) and gradient checkpoint (Griewank & Walther, 2000; Chen et al., 2016) are used. The input images are resized to $224 \\\\times 224$ resolution during pre-training and the maximum length of the text is limited to 77 tokens following Radford et al. (2021). The training is mainly conducted on Nvidia V100 GPUs and Ascend Cards. FILIP base is trained on 128 cards about 9 days and FILIP large takes about 24 days to train on 192 cards. Unless otherwise specified, we use FILIP large to compare with other methods and FILIP base for ablation. We train both models using the LAMB optimizer (You et al., 2020) and cosine learning rate schedule (Loshchilov & Hutter, 2016) with a linear warmup. Weight decay regularization is applied to all parameters except bias, layer normalization, token embedding, positional embedding and temperature in contrastive loss. Detailed values of hyperparameters for different datasets and models can be found in Appendix A.3.\\n\\n4.2 IMAGE CLASSIFICATION\\n\\nIn this section, we compare our FILIP with CLIP (Radford et al., 2021) on 12 downstream image classification datasets.\"}"}
{"id": "cpDhcsEDC2", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tables 2 and 3 show the results of zero-shot and fine-tuned image-text retrieval, respectively. We compare our FILIP model against methods with complex attention layers including Unicoder-VL (Li et al., 2020a), ImageBERT (Qi et al., 2020), UNITER (Chen et al., 2020), VILLA (Gan et al., 2020), ERNIE-ViL (Yu et al., 2021), Oscar (Li et al., 2020b), VinVL (Zhang et al., 2021), ALBEF (Li et al., 2021a), and methods trained on larger-scale image-text datasets including CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021). As we can see, FILIP achieves state-of-the-art performances under all metrics on both Flickr30K and MSCOCO datasets, except for zero-shot text-to-image retrieval on Flickr30K, where FILIP achieves competitive performance with SOTA. For zero-shot image-to-text retrieval on MSCOCO dataset, the absolute R@1 of our proposed FILIP is 2.7% higher than ALIGN, which is trained on a much larger dataset.\"}"}
{"id": "cpDhcsEDC2", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Results of fine-tuned image-text retrieval on Flickr30K and MSCOCO datasets.\\n\\n| Model          | Flickr30K | MSCOCO | image-to-text | text-to-image | image-to-text | text-to-image |\\n|----------------|-----------|--------|---------------|---------------|---------------|---------------|\\n| Unicoder-VL    | 86.2      | 96.3   | 99.0          | 71.5          | 90.9          | 94.9          |\\n| ImageBERT      | 87.0      | 97.6   | 99.2          | 73.1          | 92.6          | 96.0          |\\n| UNITER         | 87.3      | 98.0   | 99.2          | 75.6          | 94.1          | 96.8          |\\n| VILLA          | 87.9      | 97.5   | 98.8          | 76.3          | 94.2          | 96.8          |\\n| ERNIE-ViL      | 88.1      | 98.0   | 99.2          | 76.7          | 93.6          | 96.4          |\\n| Oscar          |           |        |               |               |               |               |\\n| VinVL          |           |        |               |               |               |               |\\n| ALIGN          | 95.3      | 99.8   | 100.0         | 84.9          | 97.4          | 98.6          |\\n| ALBEF          | 95.9      | 99.8   | 100.0         | 85.6          | 97.5          | 98.9          |\\n| Our FILIP base | 96.6      | 100.0  | 100.0         | 87.1          | 97.7          | 99.1          |\\n\\nTable 4: Ablation study of different components on pre-training subset of YFCC100M. I2T and T2I are abbreviations for image-to-text and text-to-image retrieval, respectively. \\\"ZS\\\" means zero-shot performance. Underlined numbers have the highest improvements for the corresponding metrics.\\n\\n| Model                                | MSCOCO | ImageNet | I2T R@1 | I2T R@5 | T2I R@1 | T2I R@5 |\\n|--------------------------------------|--------|----------|---------|---------|---------|---------|\\n| Baseline (ViT-B/32)                  | 25.0   | 49.5     | 14.7    | 34.7    | 30.4    |         |\\n| w/ image augmentation                | 26.1   | 51.8     | 16.5    | 37.5    | 32.5    |         |\\n| w/ back translation                 | 29.2   | 55.0     | 17.9    | 39.8    | 33.9    |         |\\n| w/ cross-modal late interaction      | 30.5   | 55.3     | 18.5    | 40.0    | 34.3    |         |\\n| Our FILIP base                       | 33.4   | 60.1     | 23.0    | 46.2    | 37.8    |         |\\n\\nTable 5: Efficiency study of the cross-modal late interaction. \\\"orig\\\" and \\\"late\\\" stand for the contrastive loss based on the original cosine similarity in CLIP and our proposed cross-modal late interaction, respectively. \\\"ZS\\\" means zero-shot performance. We report results for ViT-B/32 trained on filtered YFCC100M with 8 V100 GPUs, with a batch size of 512 per GPU. Training time and memory consumption are tested using the same gradient checkpoint configuration. * denotes our final setting used in other experiments.\\n\\n| Loss | Embed dim | Embed precision | Training time (sec/iter) | Memory (MB) | ZS Top1 |\\n|------|-----------|-----------------|--------------------------|-------------|---------|\\n| orig | 512       | fp32            | 1.31                     | 14300       | 30.4    |\\n| late | 512       | fp32            | 2.85                     | 26000       | 34.6    |\\n| late | 512       | fp16            | 2.67                     | 23468       | 34.5    |\\n| late | 256       | fp16            | 2.31                     | 22382       | 35.2    |\\n| late | 256       | fp16            | 1.61                     | 16336       | 34.5    |\\n| late*| 256       | fp16            | 1.39                     | 16100       | 34.3    |\\n\\nEfficiency Study of Cross-modal Late Interaction.\\nSince the late interaction mechanism in Section 3.1.1 requires to calculate the similarity between all visual and textual tokens, its efficiency can be a problem when employed in large-scale distributed training. As described in Section 3.1.1, we make several attempts to address the issue. Table 5 shows the efficiency improvement on zero-shot classification on ImageNet when these attempts are applied. As can be seen, these attempts improve the efficiency of late interaction without accuracy drop. Combining all three attempts achieves only slightly slower training and larger memory consumption than the original loss in CLIP.\\n\\n4.5 Visualization of Fine-grained Alignment\\nIn this section, we visualize FILIP's capability of capturing fine-grained cross-modal correspondence using the method of word-patch alignment. To make a fair comparison, we use our FILIP base trained on YFCC100M and CLIP's ViT-B/32, which are of the same size, for visualization. Each image is patchified to $7 \\\\times 7$ image patches. More visualization results can be found in Appendix A.4.\"}"}
{"id": "cpDhcsEDC2", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nUnsupervised large-scale vision-language pre-training has shown promising advances on various downstream tasks. Existing methods often model the cross-modal interaction either via the similarity of the global feature of each modality which misses sufficient information, or finer-grained interactions using cross/self-attention upon visual and textual tokens. However, cross/self-attention suffers from inferior efficiency in both training and inference. In this paper, we introduce a large-scale Fine-grained Interactive Language-Image Pre-training (FILIP) to achieve finer-level alignment through a cross-modal late interaction mechanism, which uses a token-wise maximum similarity between visual and textual tokens to guide the contrastive objective. FILIP successfully leverages the finer-grained expressiveness between image patches and textual words by modifying only contrastive loss, while simultaneously gaining the ability to pre-compute image and text representations offline at inference, keeping both large-scale training and inference efficient. Furthermore, we construct a new large-scale image-text pair dataset called FILIP300M for pre-training. Experiments show that FILIP achieves state-of-the-art performance on multiple downstream vision-language tasks including zero-shot image classification and image-text retrieval. The visualization on word-patch alignment further shows that FILIP can learn meaningful fine-grained features with promising localization ability.\\n\\nIntroduction\\n\\nLarge-scale Vision-Language Pre-training (VLP) models like CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) have recently demonstrated success across various downstream tasks. They learn visual and textual representations from millions of image-text pairs collected from the Internet and show superior zero-shot ability and robustness. The core technique of these models lies in the global contrastive alignment of the images and texts through a dual-stream model. Such architecture is inference-efficient for downstream tasks like retrieval because the encoders for the two modalities can be decoupled and the image or text representations can be pre-computed offline. However, CLIP and ALIGN model the cross-modal interaction via solely the similarity of the global feature of each modality, lacking the ability of capturing finer-level information like the relationship between visual objects and textual words. In this paper, we develop a simple yet efficient cross-modal finer-grained interaction mechanism for large-scale VLP.\\n\\nTo achieve finer-grained cross-modal interaction, previous methods mainly exploited two kinds of methods. (1) One line of work (Chen et al., 2020; Li et al., 2020b; Dong et al., 2021; Li et al., 2021b; Zhang et al., 2021; Zhan et al., 2021) uses a pre-trained object detector to extract region-of-interest (ROI) features from images, and then fuses it with the paired text through a VLP model. This design complicates the pre-training due to pre-computing and storing a large number of ROI features. In addition, the zero-shot ability of these approaches is usually limited by the predefined number of classes and their performance is also restricted by the quality of the detector. (2) Another line of work (Li et al., 2021a; Kim et al., 2021) enforces the token-wise or patch-wise representations from\"}"}
{"id": "cpDhcsEDC2", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Cross-modal methods, as introduced in (Li et al., 2021a) or self-attention in (Kim et al., 2021), are less efficient in terms of both training and inference. During training, cross-attention in (Li et al., 2021a) requires an encoder-decoder structure, while self-attention grows quadratically with the length of concatenated sequences from both modalities. Inference is intertwined, making pre-computation like dual-stream models (CLIP and ALIGN) less efficient for tasks such as image/text retrieval and classification.\\n\\nIn this paper, we propose a large-scale Fine-grained Interactive Language-Image Pre-training framework named FILIP to address these limitations. Inspired by Khattab & Zaharia (2020), FILIP uses a novel cross-modal late interaction mechanism in contrastive loss. Specifically, the fine-grained contrastive learning uses token-wise maximum similarities between visual and textual tokens to guide the contrastive objective. FILIP leverages the finer-grained expressiveness among image patches and textual words while gaining the ability to pre-compute representations offline. Unlike Khattab & Zaharia (2020), FILIP uses average instead of summation of token-wise maximum similarities when computing the image-text alignment, enhancing cross-modal representation learning and stabilizing training. FILIP constructs a large-scale pre-training dataset named FILIP300M from the Internet, with data cleaning and image-text augmentation contributing to its effectiveness.\\n\\nExperiments show FILIP achieves state-of-the-art performance on downstream tasks such as zero-shot ImageNet classification, surpassing CLIP with less training data. Visualizations show FILIP learns meaningful finer-grained features with promising localization.\\n\\n**RELATED WORK**\\n\\nVision-Language Pre-training Models. The pre-train-and-fine-tune scheme has been successful in natural language processing (Devlin et al., 2019; Brown et al., 2020) and computer vision (Dosovitskiy et al., 2020), naturally extending to joint cross-modal domains such as VLP. Recent VLP models use pre-training datasets like YFCC100M (Thomee et al., 2016) and CC12M (Changpinyo et al., 2021), and powerfully pre-training datasets such as CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021). Pre-training tasks include image-text contrastive learning and Language Modeling (LM) tasks: CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), UNIMO (Li et al., 2021b) use cross-modal contrastive learning, while VisualBERT (Li et al., 2019), UNITER (Chen et al., 2020), M6 (Lin et al., 2021), and DALL-E (Ramesh et al., 2021) use LM-like objectives. Other methods use pre-trained object detection models like Faster-RCNN (Ren et al., 2015) to extract image regional features offline, requiring extra labeled data and reducing scalability. Recent efforts like SOHO (Huang et al., 2021) and SimVLM (Wang et al., 2021) reduce this burden with visual dictionaries or PrefixLM (Raffel et al., 2020).\\n\\nFILIP directly learns fine-grained representations in an end-to-end and simple manner, maintaining inference efficiency.\\n\\n**Multi-Modality Interaction Mechanism**\\n\\nThe core of vision-language pre-training models lies in modeling the interaction between modalities. There are two main types of cross-modal interaction architectures: Single-stream models like VisualBERT (Li et al., 2019) and ViLT (Kim et al., 2021) concatenate patch-wise or regional visual features with textual embeddings and feed them to the transformer-based model. Dual-stream models like ViLBERT (Lu et al., 2019) and CLIP (Radford et al., 2021) use separate encoders for each modality, allowing flexible use of different models and efficient inference for tasks like image-text retrieval.\"}"}
{"id": "cpDhcsEDC2", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Overall architecture of FILIP, a dual-stream model with Transformer-based image and text encoders. On top of the image and text encoders, the representations of textual tokens and visual tokens are linearly projected to the multi-modal joint space. A novel fine-grained contrastive learning equipped with cross-modal late interaction is proposed, which uses a token-wise maximum similarity between visual and textual tokens.\\n\\nIn this paper, we propose a new cross-modal pre-training model that excels in fine-grained interaction between image encoder and text encoder for mining more detailed semantic alignment, named as FILIP, as shown in Figure 1. Particularly, FILIP is a dual-stream model with Transformer-based image and text encoders. For the visual modality, the image encoder is a Vision Transformer (Dosovitskiy et al., 2020) which takes the concatenation of an extra [CLS] token embedding and linearly projected image patches as input. For the textual modality, following Radford et al. (2021), we use the lower-cased byte pair encoding (BPE) (Sennrich et al., 2016b) with a vocabulary size of 49,408 to tokenize the text. Each text sequence starts with [BOS] token and ends with [EOS] token. After the word embedding layer, the token embeddings are fed into a modified decoder-only Transformer model as in (Radford et al., 2019). On top of the image and text encoders, the representations of textual tokens and visual tokens are linearly projected to the multi-modal common space, and are separately L2-normalized. Different from existing dual-stream models (e.g., CLIP and ALIGN) which models cross-modal interaction via only the global features of the entire image and text sequence, we introduce a novel fine-grained contrastive learning objective equipped with cross-modal late interaction which takes into account the fine-grained interaction between image patches and textual tokens, detailed in Section 3.1.\\n\\n3.1 Fine-grained Contrastive Learning\\n\\nContrastive representation learning has recently been found to learn better representations than its predictive counterpart in both visual (Tian et al., 2020) and vision-language cross-modal pre-training (Radford et al., 2021). Under a general formulation of cross-modal contrastive learning (Radford et al., 2021), we want to learn encoders $f_\\\\theta$ for image data $I$ and $g_\\\\phi$ for text data $T$ such that, given an image $x_I \\\\in I$, and a text $x_T \\\\in T$, the encoded representations $f_\\\\theta(x_I)$ and $g_\\\\phi(x_T)$ are close if they are related and far apart if not, under a distance metric. In each training batch, we sample $b$ image-text pairs $\\\\{x_I^k, x_T^k\\\\}_{k=1}^b$. For image $x_I^k$ in image-text pair $\\\\{x_I^k, x_T^k\\\\}$, $x_T^k$ is its positive, while $x_I^k$ is its negative.\"}"}
{"id": "cpDhcsEDC2", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the other texts will be used as in-batch negatives. The image-to-text contrastive loss \\\\( L^I_k \\\\) for \\\\( x^I_k \\\\) can then be formulated as\\n\\\\[\\nL^I_k(x^I_k, \\\\{ x^T_j \\\\}_{b_j=1}) = -\\\\frac{1}{b} \\\\log \\\\exp(s^I_{k,k}) \\\\sum_j \\\\exp(s^I_{k,j}),\\n\\\\]\\nwhere \\\\( s^I_{i,j} \\\\) denotes the similarity of the \\\\( k \\\\)-th image to the \\\\( j \\\\)-th text. Similarly, the text-to-image contrastive loss for \\\\( x^T_k \\\\) is\\n\\\\[\\nL^T_k(x^T_k, \\\\{ x^I_j \\\\}_{b_j=1}) = -\\\\frac{1}{b} \\\\log \\\\exp(s^T_{k,k}) \\\\sum_j \\\\exp(s^T_{j,k}).\\n\\\\]\\nThe total loss of this mini-batch can be represented by\\n\\\\[\\nL = \\\\frac{1}{2} \\\\sum_{b_k=1} (L^I_k + L^T_k).\\n\\\\]\\n\\n3.1.1 Cross-modal Late Interaction\\nFrom the contrastive loss (1), the cross-modal interaction is reflected in how we compute the similarities \\\\( s^I_{i,j} \\\\) and \\\\( s^T_{i,j} \\\\) for the \\\\( i \\\\)-th image and \\\\( j \\\\)-th text. Previous methods like CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) simply encode each image or text separately to a global feature i.e.,\\n\\\\[\\nf^I_\\\\theta(x^I_i) \\\\in \\\\mathbb{R}^d \\\\quad \\\\text{and} \\\\quad g^I_\\\\phi(x^T_j) \\\\in \\\\mathbb{R}^d,\\n\\\\]\\nand compute these two similarities as\\n\\\\[\\ns^I_{i,j} = s^T_{i,j} = f^I_\\\\theta(x^I_i)^\\\\top g^I_\\\\phi(x^T_j),\\n\\\\]\\n(2)\\n\\nneglecting finer-grained interactions (e.g., word-patch alignment) between the two modalities. To alleviate this problem, while simultaneously maintain the training and inference efficiency of dual-stream models, we apply a cross-modal late interaction inspired by Khattab & Zaharia (2020) to model the token-wise cross-modal interaction.\\n\\nSpecifically, denote \\\\( n_1 \\\\) and \\\\( n_2 \\\\) as the number of (non-padded) tokens of the \\\\( i \\\\)-th image and \\\\( j \\\\)-th text, respectively, and the corresponding encoded features are\\n\\\\[\\nf^I_\\\\theta(x^I_i) \\\\in \\\\mathbb{R}^{n_1 \\\\times d} \\\\quad \\\\text{and} \\\\quad g^I_\\\\phi(x^T_j) \\\\in \\\\mathbb{R}^{n_2 \\\\times d}.\\n\\\\]\\nFor the \\\\( k \\\\)-th visual token, we compute its similarities with all textual tokens of \\\\( x^T_j \\\\), and use the largest one\\n\\\\[\\n\\\\max_{0 \\\\leq r < n_2} [f^I_\\\\theta(x^I_i)]^\\\\top k [g^I_\\\\phi(x^T_j)]_r\\n\\\\]\\n(3)\\nas its token-wise maximum similarity with \\\\( x^T_j \\\\). We then use the average token-wise maximum similarity of all non-padded tokens in the image (resp. text) as the similarity of an image to a text (resp. a text to an image). The similarity of the \\\\( i \\\\)-th image to the \\\\( j \\\\)-th text can thus be formulated as:\\n\\\\[\\ns^I_{i,j}(x^I_i, x^T_j) = \\\\frac{1}{n_1} \\\\sum_{k=1} \\\\max_{0 \\\\leq r < n_2} [f^I_\\\\theta(x^I_i)]^\\\\top k [g^I_\\\\phi(x^T_j)]_r,\\n\\\\]\\n(4)\\nwhere \\\\( m^I_k = \\\\arg \\\\max_{0 \\\\leq r < n_2} [f^I_\\\\theta(x^I_i)]^\\\\top k [g^I_\\\\phi(x^T_j)]_r \\\\).\\n\\nSimilarly, the similarity of the \\\\( j \\\\)-th text to the \\\\( i \\\\)-th image is\\n\\\\[\\ns^T_{i,j}(x^I_i, x^T_j) = \\\\frac{1}{n_2} \\\\sum_{k=1} \\\\max_{0 \\\\leq r < n_1} [f^I_\\\\theta(x^I_i)]_k [g^I_\\\\phi(x^T_j)]^\\\\top k\\n\\\\]\\n(5)\\nwhere \\\\( m^T_k = \\\\arg \\\\max_{0 \\\\leq r < n_1} [f^I_\\\\theta(x^I_i)]_k [g^I_\\\\phi(x^T_j)]^\\\\top k \\\\).\\n\\nRemark 1: Intuitively, the token-wise maximum similarity in Equation (3) means that for each image patch, we find its most similar textual token. Similarly, for each textual token, we also find its closest image patch. By applying this to the similarity calculation in (4) and (5) for contrastive loss (1), the dual-stream model learns fine-grained alignment between image patches and textual tokens.\\n\\nThe original late interaction mechanism in (Khattab & Zaharia, 2020) computes the relevance score of a document to a query padded with mask tokens, as a sum of token-wise maximum similarities, and is optimized via a pairwise softmax cross-entropy loss. Though inspired from Khattab & Zaharia (2020), our proposed cross-modal late interaction differs in several aspects. Firstly, we...\"}"}
