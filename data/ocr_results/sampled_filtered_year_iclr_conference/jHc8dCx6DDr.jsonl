{"id": "jHc8dCx6DDr", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "jHc8dCx6DDr", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "jHc8dCx6DDr", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "jHc8dCx6DDr", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# INSTALLATION AND EXECUTION OF MEMORY GYM\\n\\n$ conda create -n memory-gym python=3.7 --yes$\\n\\n$ conda activate memory-gym\\n\\n# Install Memory Gym\\n\\n$ pip install memory-gym\\n\\n# Play Mortar Mayhem\\n\\n$ mortar_mayhem\\n$ mortar_mayhem_b\\n$ mortar_mayhem_grid\\n$ mortar_mayhem_b_grid\\n$ mystery_path\\n$ mystery_path_grid\\n$ searing\u30b9\u30dd\u30c3\u30c8\\n\\nFigure 7: Install and play Memory Gym's environments.\\n\\nmemory_gym is available as a PyPi package 3, which installs the required dependencies gym and PyGame. We recommend to utilize Anaconda 4 to allow for console scripts to be executed. This way, the environments can be played using the following commands:\\n\\n$ mortar_mayhem\\n$ mortar_mayhem_b\\n$ mortar_mayhem_grid\\n$ mortar_mayhem_b_grid\\n$ mystery_path\\n$ mystery_path_grid\\n$ searing\u30b9\u30dd\u30c3\u30c8\\n\\nFigure 8: Console scripts to play Memory Gym's environments.\\n\\n```\\nimport gym\\nimport memory_gym\\n\\ndef configure_reset(options):\\n    \"\"\"Configure desired reset parameters.\\n\\n    Parameters:\\n    options (dict): dictionary containing the desired parameters.\\n\\n    Returns:\\n    dict: the updated options.\\n    \"\"\"\\n    try:\\n        options[\"allowed_commands\"] = 5\\n        options[\"command_count\"] = [5]\\n    except KeyError:\\n        pass\\n\\n    return options\\n\\noptions = configure_reset(options)\\n\\nenv = gym.make(\\\"MortarMayhem-v0\\\")\\nobs = env.reset(options)\\nprint(obs[\\\"visual_observation\\\"].shape)  # (84, 84, 3)\\nprint(env.action_space)  # MultiDiscrete([3, 3])\\ndone = False\\nwhile not done:\\n    action = env.action_space.sample()\\n    obs, reward, done, info = env.step(action)\\n```\\n\\nFigure 9: Code interactions with Memory Gym environments.\\n\\nDear reviewers, the package will be available upon acceptance due to anonymity.\\n\\n3 https://www.anaconda.com/products/distribution\"}"}
{"id": "jHc8dCx6DDr", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: These are the hyperparameters that we used for all training runs. The sequence length is dynamically set by the longest episode inside the batch of the gathered training data. The experiments on Searing Spotlights and Mystery Path utilized a fixed sequence length of 128. The learning rate and the entropy coefficient decay linearly.\\n\\n| Hyperparameter          | Value |\\n|-------------------------|-------|\\n| Training Seeds          | 100000|\\n| Sequence Length Max     |       |\\n| Number of Workers       | 32    |\\n| Worker Steps            | 512   |\\n| Batch Size              | 16384 |\\n| Number of Mini Batches  | 8     |\\n| Mini Batch Size         | 2048  |\\n| PPO Updates             | 10000 |\\n| Training Steps          | 16384000 |\\n| Discount Factor Gamma   | 0.99  |\\n| GAE Lambda              | 0.95  |\\n| Epochs                  | 3     |\\n| Value Loss Coefficient  | 0.25  |\\n| Max Gradient Norm       | 0.5   |\\n| Clip Range Epsilon      | 0.2   |\\n| Initial Learning Rate   | 3e-4  |\\n| Final Learning Rate     | 1e-4  |\\n| Initial Entropy Coefficient | 1e-4 |\\n| Final Entropy Coefficient | 1e-5 |\\n| HELM Beta               | 1000  |\\n| Optimizer               | AdamW |\\n\\ntried the values 50, 100, 200, and 1000 for HELM's beta hyperparameter on MMActGrid. All values ended up with a nearly identical performance. We ultimately chose 1000 because there is not much variance in individual observations in Mortar Mayhem and Mystery Path. Overall, we rather allocated our resources to scale the difficulty of Mortar Mayhem and Mystery Path, while investigating the issues of Searing Spotlights.\"}"}
{"id": "jHc8dCx6DDr", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 17: Generalization performances on novel seeds in Mortar Mayhem Act Grid showing the wall-time efficiency of PPO, GRU-PPO, and HELM. These experiments are run on an NVIDIA A100 Tensor-Core-GPU and an AMD EPYC 7542 CPU (32 cores).\\n\\nFigure 18: Generalization performances on novel seeds in Mystery Path Grid (hidden goal) showing the wall-time efficiency of PPO, GRU-PPO, and HELM. These experiments are run on an NVIDIA A100 Tensor-Core-GPU and an AMD EPYC 7542 CPU (32 cores).\"}"}
{"id": "jHc8dCx6DDr", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multiple experiments were conducted to ablate the environment's difficulty and to vary the model architecture. Figure 19 shows a few variations of the visual representation of Searing Spotlights. Whenever spotlights are present, the recurrent agent fails. Subsequently, we enumerate the measures that we tried with limited success to help the agent to learn a more meaningful policy. Increasing the scale of all entities but the spotlights or reducing the number of spotlights may have undesirable consequences to the task's quality. Random policies may be more successful under these measures. \\n\\n- Reduce episode length to ease exploitation\\n  - Leverage frame skipping\\n  - Raise agent speed\\n  - Scale up all entities except spotlights\\n\\n- Vary spotlights\\n  - Add negative reward of $-0.01$ for the agent being inside the spotlights\\n  - Stationary spotlights\\n  - Fewer spotlights\\n  - Constant spotlight size and speed\\n\\n- Vary task\\n  - Use only one coin and no exit\\n  - Use the exit but no coin\\n  - Agent always starts at the center of the level\\n  - Increase agent health points\\n  - Add negative death reward of $-0.1$\\n\\n- Vary agent observation\\n  - Environment is fully observable for few steps\\n  - Make the agent, coin, exit, or all of them permanently visible\\n  - Add health points bar\\n  - Render the agent's last action onto the observation instead of feeding a one-hot encoded feature vector\\n  - Feed the agent its exact position as normalized scalars\\n  - Slightly dim the light (perfect information)\\n  - Render the environment's floor black while drawing the spotlights' circumference white\\n\\n- Vary model architecture\\n  - Add residual connection around the GRU cell\\n  - Use LSTM instead of GRU\\n  - Reduce sequence length from 128 to 64\\n  - Use Impala CNN Espeholt et al. (2018) instead of Nature CNN Mnih et al. (2015)\\n  - Add fully connected layer between CNN and GRU\"}"}
{"id": "jHc8dCx6DDr", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"During the rebuttal, few complex frameworks were brought to our attention that claim to support a DRL algorithm leveraging Gated TransformerXL (Parisotto et al., 2020) or just TransformerXL, namely Rllib, DI-engine, and Brain Agent. We tried to use Memory Gym with RLib but observed questionable outputs like negative rewards (even though the environments did not use any negative rewards). The framework is too complex to analyze and eventually to verify. The same accounts for Brain Agent, which is not densely commented nor documented. DI-Engine implements an R2D2 (Kapturowski et al., 2019) variant based on Gated TransformerXL. We tried it and learned that it has a poor sample throughput by a magnitude of 10 if compared to GRU-PPO. We do not claim that those frameworks are dysfunctional, because we just report our limited experience by exploring those.\\n\\nBy the time of acceptance, we finally implemented a lightweight and easy-to-follow TransformerXL + PPO baseline, which is now used in our momentary work on Memory Gym. Source Code: https://github.com/MarcoMeter/episodic-transformer-memory-ppo\"}"}
{"id": "jHc8dCx6DDr", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2: Comparison of the simulation speed of our benchmark to related ones. A constant action is executed to measure the speed. Procgen encompasses the mean steps per second for the environments CaveFlyer, Jumper, Heist, Dodgeball, Maze, and Miner. The reset duration is included in steps per second. Some benchmarks are only measured by a single environment. The measurement was done on a AMD Ryzen 7 2700X.\\n\\n| Benchmark                                      | FPS Reset Duration | Benchmark Mean | Std Mean (seconds) |\\n|------------------------------------------------|--------------------|----------------|--------------------|\\n| Procgen Memory Distribution (Cobbe et al. (2020)) | 18530 5453 4e-6    | 4e-6           |                    |\\n| Mystery Path (ours)                            | 12187 330 5e-4     | 5e-4           |                    |\\n| Mortar Mayhem (ours)                           | 11692 179 2e-4     | 2e-4           |                    |\\n| Atari Breakout (Bellemare et al. (2013))       | 7117 19 6e-3       | 6e-3           |                    |\\n| DM Ballet (Lampinen et al. (2021))              | 6631 795 6e-4      | 6e-4           |                    |\\n| Searing Spotlights (ours)                      | 5490 69 5e-4       | 5e-4           |                    |\\n| MiniGrid Memory (Chevalier-Boisvert et al. (2018)) | 5185 141 1e-3     | 1e-3           |                    |\\n| MiniWorld TMaze (Chevalier-Boisvert et al. (2018)) | 1162 65 6e-4     | 6e-4           |                    |\\n| GridVerse Memory NineRooms (Baisero & Katt (2021)) | 938 89 1e-3      | 1e-3           |                    |\\n| ML-Agents Hallway (Juliani et al. (2018))       | 702 42 1e-3       | 1e-3           |                    |\\n| VizDoom My Way Home (Wydmuch et al. (2018))     | 549 22 3e-3       | 3e-3           |                    |\\n| Crafter (Hafner (2021))                        | 482 33 1e-1       | 1e-1           |                    |\\n| DM Spot the Difference (Fortunato et al. (2019)) | 442 20 1e-1      | 1e-1           |                    |\\n| DM Lab 30 rooms watermaze                   | 433 53 5e-1       | 5e-1           |                    |\\n| DM Alchemy (Wang et al. (2021))                | 308 31 2e-1       | 2e-1           |                    |\\n| DM Hard Eight ball room navigation cubes     | 160 7 2e-1        | 2e-1           |                    |\\n| AnimalAI 3 aai c32 r5 (Crosby et al. (2020))  | 190 59 1e-2       | 1e-2           |                    |\\n| Obstacle Tower (Juliani et al. (2019))         | 43 2 16e-1        | 16e-1          |                    |\"}"}
{"id": "jHc8dCx6DDr", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.1 ACTION SPACES\\n\\n(a) Multi-Discrete\\n\\n(b) Move Forward\\n\\n(c) Rotate Left\\n\\n(d) Rotate Right\\n\\nFigure 10: Memory Gym's environments feature a multi-discrete action space as seen in (a). One dimension (green arrows) denotes the agent's vertical velocity. The other one (red arrows) refers to the agent's horizontal velocity. Both dimensions feature a no-op action. Therefore, the agent can utilize both dimensions to move into eight distinct cardinal directions (all arrows). The speed of the agent is fixed at 2.83 pixels per step. Mortar Mayhem and Mystery Path also provide a grid-like locomotion based on a discrete action space of four actions. The agent can move forward (b) one tile at a time. Its forward speed in Mortar Mayhem is 14 pixels per second. Its forward speed in Mystery Path is 12 pixels per second. Two other actions allow the agent to rotate left (c) or right (d) at 90 degrees. The last action is no-op.\\n\\nC.2 EPISODE LENGTHS\\n\\nTable 3: Lengths of successful (best case) and failed (worst case) episodes in Mortar Mayhem. The values are calculated by equations (1-6).\\n\\n|                  | MM | Act | MMGrid | MMActGrid |\\n|------------------|----|-----|--------|-----------|\\n| Min (Lower Bound)| 38 | 19  | 46     | 7         |\\n| Max (Upper Bound)| 135| 115 | 119    | 79        |\\n\\nClue Task = \\\\( (\\\\text{ShowDuration} + \\\\text{ShowDelay}) \\\\cdot \\\\text{CommandCount} \\\\)  \\\\hspace{1cm} (1)\\n\\nAct Task = \\\\( (\\\\text{ExecutionDuration} + \\\\text{ExecutionDelay}) \\\\cdot \\\\text{CommandCount} \\\\)  \\\\hspace{1cm} (2)\\n\\n\\\\[ \\\\text{Act Task} = \\\\text{Act Task} - \\\\text{ExecutionDelay} + 1 \\\\]  \\\\hspace{1cm} (3)\\n\\n\\\\[ \\\\text{Max Episode Length} = \\\\text{Clue Task} + \\\\text{Act Task} \\\\]  \\\\hspace{1cm} (4)\\n\\n\\\\[ \\\\text{Act Failure} = \\\\text{ExecutionDuration} + 1 \\\\]  \\\\hspace{1cm} (5)\\n\\n\\\\[ \\\\text{Min Episode Length} = \\\\text{Clue Task} + \\\\text{Act Failure} \\\\]  \\\\hspace{1cm} (6)\\n\\nTable 4: Lengths of successful episodes in Mystery Path, Mystery Path Grid, and Searing Spotlights. The values for MP and MPGrid are retrieved from one agent trained on the ground truth. The values for Searing Spotlights are based on an optimal policy. The optimal policy traverses the shortest path to the coin and then to the exit.\\n\\n|                  | MP | MPGrid | Searing Spotlights |\\n|------------------|----|--------|--------------------|\\n| Min (Lower Bound)| 23 | 6      | 15                 |\\n| Mean              | 39.13 | 13.17  | 35.33              |\\n| Std               | 7.36 | 3.22   | 8.01               |\\n| Max               | 73  | 29     | 67                 |\\n| Upper Bound       | 512 | 128    | 512                |\\n| Episode Samples   | 100k | 100k   | 100k               |\"}"}
{"id": "jHc8dCx6DDr", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: These are the default parameters that we used throughout this paper as default. Parameters with a * indicate that these are uniformly sampled. Values in square brackets are discrete choices, while values in parentheses consider a range to sample from. Mortar Mayhem Grid features the same parameters as its parent, but only the modified ones are presented.\\n\\n| Parameter             | Default/Range       |\\n|-----------------------|---------------------|\\n| Agent Scale           | 0.25                |\\n| Max Episode Length    | 512                 |\\n| Agent Speed           | 2.5                 |\\n| Arena Size            | 5                   |\\n| Number of Available Commands | 9            |\\n| Command Show Duration | [3]                 |\\n| Command Show Delay    | [1]                 |\\n| Execution Delay       | [6]                 |\\n| Execution Duration    | [18]                |\\n| Exit Scale            | 0.5                 |\\n| Exit Visible          | False               |\\n| Hide Visual Feedback  | False               |\\n| Number of Coins       | [1]                 |\\n| Reward Command Failure| 0                   |\\n| Number of Spotlight Spawns | 4          |\\n| Spotlight Spawn Decay | 0.95                |\\n| Spotlight Spawn Threshold | 10           |\\n| Number of Commands    | [5]                 |\\n| Spotlight Radius      | (7.5-13.75)         |\\n| Spotlight Speed       | (0.0025-0.0075)     |\\n| Spotlight Damage      | 1                   |\\n| Light Dim Off Duration| 6                   |\\n| Light Threshold       | 255                 |\\n| Hide Visual Feedback  | False               |\\n| Max Episode Length    | 512                 |\\n| Render Background     | Black               |\\n| Agent Scale           | 0.25                |\\n| Agent Speed           | 2.5                 |\\n| Cardinal Origin Choice| [0, 1, 2, 3]        |\\n| Hide Goal             | True                |\\n| Hide Visual Feedback  | False               |\\n| Reward Inside Spotlight| 0                |\\n| Reward Outside Spotlight| 0               |\\n| Reward Death          | 0                   |\\n| Reward Coin           | 0.25                |\\n| Hide Origin           | False               |\\n| Reward Exit           | 1                   |\\n| Reward Coin           | 0.25                |\\n| Reward Max Steps      | 0                   |\\n| Reward Goal           | 1                   |\\n| Reward Fall Off       | 0                   |\\n| Reward Path Progress  | 0                   |\\n| Reward Step           | 0                   |\"}"}
{"id": "jHc8dCx6DDr", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: A successfully completed episode of Mortar Mayhem Grid showing the ground truth.\\n\\nThe zip archive of the supplementary material provides an episode as a video.\"}"}
{"id": "jHc8dCx6DDr", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: An overview of the simulation speed and meta desiderata of the considered environments. The meta desiderata cover the criteria that are applicable to any benchmark in DRL. The mean FPS are measured across 100 episodes using constant actions. Procgen is averaged across its memory distribution environments, and so is Memory Gym. The other benchmarks are measured using a single environment.\\n\\n| Benchmark                        | Mean FPS | Meta Desiderata                           |\\n|----------------------------------|----------|-------------------------------------------|\\n| Memory Gym (ours)                |          | Publicly Available, Open Source, Linux, macOS, Windows, Headless, Docker, Concurrent, Playable, High Diversity, Scalable Difficulty |\\n| Procgen Memory Distribution (Cobbe et al., 2020) | 185 | #                                             |\\n| DM Ballet (Lampinen et al., 2021) | 6631 | #                                             |\\n| MiniGrid Memory (Chevalier-Boisvert et al., 2018) | 5185 | # #                                           |\\n| VizDoom (Wydmuch et al., 2018)    | 549      | #                                             |\\n| DM Memory Task Suite (Fortunato et al., 2019) | 442 | # # #                                           |\\n| DM Lab 30 (Beattie et al., 2016)  | 433      | # # # #                                         |\\n| DM Numpad (Parisotto et al., 2020) |       | #                                             |\\n| DM Memory Maze (Parisotto et al., 2020) |       | #                                             |\\n| DM Object Permanence (Lampinen et al., 2021) | | #                                             |\\n\\nStrong dependency on frequent memory interactions is a property of tasks that forces the agent to recall information from and add information to its memory frequently. We believe this is more suitable for sequential decision-making problems because some related environments can be turned into supervised learning problems (Section 2.5) and therefore only assess the memory's capacity and potentially its robustness to noise.\\n\\n2.2 CONSIDERED ENVIRONMENTS\\n\\nA diverse set of environments were used in the past to challenge memory-based agents. Some of them are originally fully observable but are turned into partially observable Markov Decision Processes (POMDP) by adding noise or masking out information from the agent's observation space. For instance, this was done for the Arcade Learning Environment (Bellemare et al., 2013) by using flickering frames (Hausknecht & Stone, 2015) and common control tasks by removing the velocity from the agent's observation (Heess et al., 2015; Meng et al., 2021; Shang et al., 2021). These environments do not require the agent to memorize long sequences and can already be approached using frame stacking. Control tasks also touch on the context of Meta Reinforcement Learning (Meta RL), where memory mechanisms are prominent (Wang et al., 2021; Melo, 2022; Ni et al., 2022). The same applies to Multi-Agent Reinforcement Learning (Berner et al., 2019; Baker et al., 2020; Vinyals et al., 2019). As we solely focus on benchmarking the agent's memory and its ability to generalize, we do not compare Memory Gym to environments of more complex contexts such as DM Alchemy (Wang et al., 2021), Crafter (Hafner, 2021), or Obstacle Tower Juliani et al. (2019). Those might need additional components to the agent's architecture and its training paradigm. Within this section, we consider DRL benchmarks (Table 1) that were used by the recently contributed memory approaches MRA (Fortunato et al., 2019), GTrXL (Parisotto et al., 2020), HCAM (Lampinen et al., 2021), HELM (Paischer et al., 2022), and A2C-Transformer (Sopov & Makarov, 2022). By examining these works, it becomes apparent that all of them use different environments to evaluate their methods, making their results harder to compare.\\n\\n2.3 META DESIDERATA\\n\\nTable 1 provides information on the meta desiderata of the considered environments. Some environments are inaccessible to some extent, as some cannot be run headless. DM Memory Maze (Parisotto et al., 2020), DM Numpad (Humplik et al., 2019), and DM Object Permanence (Lampinen et al., 2021) are not publicly available and can, therefore, not be used to reproduce and compare results in adjacent research. DM Memory Task Suite (Fortunato et al., 2019) is not open source and can # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}
{"id": "jHc8dCx6DDr", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As stated before, if an environment is solvable to some extent using an agent without memory, it is not easy to differentiate whether the memory mechanism is working. This impression can be retrieved by examining the results of Sopov & Makarov 2022 on the VizDoom environments where a policy without memory makes progress or achieves comparable performances. The six environments of Procgen\u2019s memory distribution leave us with controversial thoughts. Paischer et al. 2022 show results in these environments that indicate that a memory-less approach can perform similarly to a memory one. We further take a closer look at Miner. The agent can momentarily perceive some cues related to past steps of the episode: its last action is retrievable from its rotation. At the same time, the density of vanished tiles shows the agent whether this region was explored or not. Exploiting these cues potentially helps memory-less agents to find successful strategies and make us believe that this environment does not strongly depend on memory, while doubtlessly, a memory-based agent should be more efficient concerning the number of steps needed to solve the entire task. Another problem is that the mean cumulative reward is usually reported, which averages the rewards achieved by collecting diamonds and using the exit. This data does not precisely tell how good an agent is at completing the entire task. Especially recalling the exit\u2019s position seems crucial to succeeding as soon as possible. Reporting the success rate and the episode length should provide more meaningful insights. The environments Heist and Maze encompass fully observable levels, which raises concerns on strong memory dependence.\\n\\nConcerning frequent memory interactions, we believe that MiniGrid Memory (Chevalier-Boisvert et al., 2018), Spot the Difference (Fortunato et al., 2019), and DM Ballet (Lampinen et al., 2021) are not well suited to the need for memory in sequential decision making. These environments demand the agent\u2019s memory to solely memorize the to-be-observed goal cues during the very beginning of the episode. Once this cue is memorized, there is no need to manipulate the agent\u2019s memory further. Simply maintaining it is enough to solve the task. It can be hypothesized that the extracted features from observing the goal cues are sufficient to solve the ballet environment. To show this, the ballet environment can be made fully observable (markovization) by feeding the entire sequence of cues to a recurrent encoder that extracts features that are utilized by the agent\u2019s policy. Consequently, the policy does not need to maintain its memory in this case. The task at hand gets much easier because the agent does not need to make obsolete decisions while observing the sequence of cues as its position is frozen. These tasks would, therefore, only challenge the capacity and robustness to noise of the agent\u2019s memory. We believe this can be done more efficiently by other benchmarks that do not belong to the context of DRL, like Long Range Arena (Tay et al., 2021).\\n\\nMini-games of the commercial video game Pummel Party 2 inspired Memory Gym\u2019s environments. The agent perceives all environments using 84 x 84 RGB pixels, while its action space is multi-discrete, featuring two dimensions of size three as shown in appendix C.1. One dimension allows the agent to move horizontally (no movement, move left, move right), and the other concerns the agent\u2019s vertical locomotion (no movement, move up, move down). Mortar Mayhem and Mystery 2 http://rebuiltgames.com/\"}"}
{"id": "jHc8dCx6DDr", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Agent\\nNext Target\\nCommand\\n\\n(a) Annotated Ground Truth\\n\\n(b) Agent Observation\\n\\n(c) Visual Feedback\\n\\nFigure 1: Mortar Mayhem's relevant entities are presented by the annotated ground truth of the environment (a). The green circle, which is solely part of the ground truth, indicates the very next tile to move to. Once the agent has solved this command, the green circle moves to the next target tile.\\n\\nAt the start of an episode, the commands are rendered sequentially onto the agent's observation (b) while the agent cannot move. Once all commands are shown, the agent has to move to the target tile in a certain amount of time. As soon as the time for executing the current command is over, the agent's success is verified, as seen in figure (c). This visual feedback is perceivable by the agent. After a delay of a few steps, the agent can approach the following command if the episode did not terminate yet due to failure or completing the entire command sequence.\\n\\nPath also feature variants based on grid-like locomotion (Figures 10(b) to 10(d)). This way, the action space is discrete and allows the agent to not move at all, rotate left, rotate right, or move forward. Subsequently, we further detail the environments' dynamics, their peculiarities towards memory, and how these can be smoothly scaled to support varying difficulty levels. Appendix C.2 quantifies the episode lengths. All parameters used for scaling the environments' hardness and their default values are found in appendix C.3. Appendices C.4 to C.6 visualize played episodes.\\n\\n3.1 Mortar Mayhem\\n\\nMortar Mayhem (MM) (Figure 1) takes place inside a grid-like arena and consists of two tasks that depend on memory. At first, while unable to move, the agent has to memorize a sequence of five commands (Clue Task), and afterward, it has to execute each command in the observed order (Act Task). One command orders the agent to move to one adjacent floor tile or to stay at the current one. If the agent fails, the episode terminates while receiving no reward. Upon successfully executing one command, the agent receives a reward of $+0.1$. Episode lengths in MM are dependent on the agent's current ability. The better the agent, the longer the episode lasts until an upper bound (i.e. max episode length) is reached. MM can be reduced to provide only the Act Task (MMAct). In this case, the command sequence is fully observable as a one-hot encoded feature vector. The Act Task requires an agent to leverage its memory frequently because otherwise, the agent does not know which commands were already executed to fulfill the next one, while there are nine tiles at maximum to consider. To solve this problem, the agent could learn to track time (e.g. count steps) where a short-term memory should suffice. The hardness of MM can be further simplified by equipping the agent with a grid-like movement (MMGrid, MMActGrid). The agent is now capable of moving one tile at a time. To ensure a meaningful task, the agent must execute ten commands, not five. Further examples to raise MM's difficulty are to extend or sample the number of commands or the delay between command execution. If compared to DM Ballet, MM's Act Task requires many correct actions in sequence, while DM Ballet asks the agent to only identify the requested dancer.\\n\\n3.2 Mystery Path\\n\\nMystery Path (MP) (Figure 2) challenges the agent to traverse an invisible path in a grid-like level of dimension $7 \\\\times 7$, while only the path's origin is visible to the agent. If the agent moves off the path (i.e. falls down the pit), the agent is relocated to its origin. The episode terminates if the agent reaches the goal or runs out of time (512 steps). Upon reaching the goal, the environment signals a reward of $+1$. To overcome uncertainty in this environment, the agent has to memorize several locations: its steps on the path and the locations where it fell off. The invisible path is procedurally generated using the path-finding algorithm $A^*$ (Hart et al., 1968). At first, the path's\"}"}
{"id": "jHc8dCx6DDr", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Goal\\nPath\\nA\\nWall\\nPit\\nAgent\\nOrigin\\n\\nFigure 2: Mystery Path's relevant entities are described by the environment's annotated ground\\ntruth (a). The walkable parts of the path are colored blue (origin), green (goal), and white (path).\\nEvery other part of the environment is considered a pit (black and red) where the agent falls off and\\nis then relocated to the path's origin during its next step. The initially randomly chosen red tiles (\\\\( A^* \\\\)Walls) are not considered during path generation. The agent observes itself and the path's origin (b).\\nIf the agent is off the path, a red cross provides visual feedback (c), which is also observed.\\n\\nHealth Bar\\nLast Action\\nExit\\nAgent\\nSpotlight\\nCoin\\n\\nFigure 3: All relevant entities can be identified in the annotated ground truth of Searing Spotlights (a). The top rows of pixels feature the agent's remaining health points and its last action that two chunks and three colors encode. Yellow circles correspond to coins, while the somewhat rounded gray shape resembles the exit. If the exit is open (i.e. no coins are left), it turns green. The floor of the environment is a chessboard colored blue and white. As seen in the agent's observation (b), the spotlights hide or reveal the other entities. As additional visual feedback (c), the blue floor tiles turn red if a spotlight spots the agent.\\n\\norigin is sampled from the grid's border. Then, the goal is placed in a random position on the opposing side. Usually, \\\\( A^* \\\\) seeks to find the shortest path. \\\\( A^* \\\\)'s cost function is sampled to ensure that a wide variety of paths is procedurally generated. The path generation randomizes some cells to be untraceable (\\\\( A^* \\\\)walls). Note that the walls are considered pits and not physical obstacles.\\n\\nMP can also be simplified to a grid variant (MPGrid), featuring the single discrete action space of four actions, while the agent's time limit is reduced from 512 steps to 128. In contrast to MM, the episode terminates sooner as the agent improves.\\n\\n3.3 Searing Spotlights (Figure 3) is perceived as a pitch-black surrounding by the agent where the only information is unveiled by roaming and threatening spotlights. While initially starting with a limited number of health points (e.g. 100), the agent loses one health point per step if hit by a spotlight. The episode terminates if the agent has no remaining health points or runs out of time (512 steps).\\n\\nBecause of the posed threats, the agent has to hide in the dark. To successfully run away from closing in spotlights, the agent must memorize its past actions and at least one past position of itself to infer its current location. That also requires the agent to carefully use its health point budget to figure out its location - ideally once briefly. To avoid numb policies, two additional tasks are added to\"}"}
{"id": "jHc8dCx6DDr", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Memory Gym is a novel benchmark for challenging Deep Reinforcement Learning agents to memorize events across long sequences, be robust to noise, and generalize. It consists of the partially observable 2D and discrete control environments Mortar Mayhem, Mystery Path, and Searing Spotlights. These environments are believed to be unsolvable by memory-less agents because they feature strong dependencies on memory and frequent agent-memory interactions. Empirical results based on Proximal Policy Optimization (PPO) and Gated Recurrent Unit (GRU) underline the strong memory dependency of the contributed environments. The hardness of these environments can be smoothly scaled, while different levels of difficulty (some of them unsolved yet) emerge for Mortar Mayhem and Mystery Path. Surprisingly, Searing Spotlights poses a tremendous challenge to GRU-PPO, which remains an open puzzle. Even though the randomly moving spotlights reveal parts of the environment's ground truth, environmental ablations hint that these pose a severe perturbation to agents that leverage recurrent model architectures as their memory. Source Code: https://github.com/MarcoMeter/drl-memory-gym/\"}"}
{"id": "jHc8dCx6DDr", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"is useful to adapt the environment's difficulty to the available resources, while easier difficulties can be used as a proof of concept. Competent methods can show off themselves in new challenges or identify their limits in a profound way. All environments are procedurally generated to evaluate the agent's ability to generalize to unseen levels (or seeds). Due to the aforementioned configurable difficulties, the trained agent can be evaluated on out-of-distribution levels. Memory Gym's significant dependencies are gym (Brockman et al., 2016) and PyGame. This allows Memory Gym to be easily set up and executed on Linux, macOS, and Windows. Multiple thousands of agent-environment interactions per second are simulated by all environments.\\n\\nThis paper proceeds as follows. We first define the memory benchmarks' criteria and portray the related benchmarks' landscape. Next, Mortar Mayhem, Mystery Path, and Searing Spotlights are detailed. Afterward, we show that memory is crucial in our environments by conducting empirical experiments using a recurrent implementation (GRU-PPO) of Proximal Policy Optimization (PPO) (Schulman et al., 2017) and HELM (Paischer et al., 2022). When scaling the hardness in Mortar Mayhem and Mystery Path, a full range of difficulty levels emerge. Searing Spotlights remains unsolved because the recurrent agent is volatile to perturbations of the environment's core mechanic: the randomly wandering spotlights. This observation is also apparent when training on a single Procgen BossFight (Cobbe et al., 2020) level under the same spotlight perturbations as in Searing Spotlights. At last, this work concludes and enumerates future work.\\n\\n2 COMPARISON OF RELATED MEMORY BENCHMARKS\\n\\n2.1 DESIDERATA OF MEMORY BENCHMARKS\\n\\nBefore detailing related benchmarks, we define the aforementioned desiderata that we believe are essential to memory benchmarks and benchmarks in general.\\n\\nAccessibility refers to the competence to easily set up and execute the environment. Benchmarks shall be publicly available, acceptably documented, and open source while running on the commonly used operating systems Linux, macOS, and Windows. Linux, in general, is important because many high-performance computing (HPC) facilities employ this operating system. As HPC facilities might not support virtualization, benchmarks should not be solely deployed as a docker image or similar. At last, benchmarks shall run headless because otherwise, these potentially rely on dependencies like xvfb or EGL, which HPC facilities may not support as well. Suppose relevant benchmark details, such as environment dynamics, are missing. In that case, it can be desirable to support humanly playable environments so that these can be explored in advance.\\n\\nFast simulation speeds, which achieve thousands of steps per second (i.e. FPS), allow training runs to be more wall time efficient, enabling to upscale experiments and their repetitions or faster development iterations. The benchmark's speed also depends on the time it takes to reset the environment to set up a new episode for the agent to play. Towards maxing out FPS on a single machine, benchmarks shall be able to run multiple instances of their environments concurrently.\\n\\nHigh diversity attributes environments that offer a large distribution of procedurally generated levels to reasonably challenge an agent's ability to generalize (Cobbe et al., 2020). Also, it is desirable to implement smoothly configurable environments to evaluate the agent at out-of-distribution levels.\\n\\nScalable difficulty is a property that shall make environments controllable such that their current hardness can be increased or decreased. Easier environments can have benefits: a proof-of-concept state is sooner reachable while developing novel methods, and research groups can fit the difficulty to their available resources. Moreover, increasing the difficulty ensures that already competent methods may prove themselves in new challenges to demonstrate their abilities or limits.\\n\\nStrong dependency on memory refers to tasks that are only solvable if the agent can recall past information (i.e. successfully leveraging its memory). Section 2.4 describes partially observable environments that can be solved to some extent without memory. While memory-based agents might more efficiently solve these tasks, these do not guarantee that the agent's memory is working. To ensure that the utilized memory mechanism is working and does not suffer from bugs, this criterion cannot be omitted by benchmarks targeting specifically the agent's memory.\\n\\n1 https://www.pygame.org\"}"}
{"id": "jHc8dCx6DDr", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12: Various randomly generated levels in the Mystery Path environment.\\n\\nFigure 13: A successfully completed episode of Mystery Path Grid showing the ground truth. The zip archive of the supplementary material provides an episode as a video.\"}"}
{"id": "jHc8dCx6DDr", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 14: An excerpt of Searing Spotlights showing the ground truth and using a frame skip of four. The zip archive of the supplementary material provides an episode as a video.\"}"}
{"id": "jHc8dCx6DDr", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Visual Observation\\nConv Filters: 32 Size: 8 Stride: 4\\nConv2 Filters: 64 Size: 4 Stride: 2\\nConv3 Filters: 64 Size: 3 Stride: 1\\nFC (512)\\nValue (1)\\nFC (512)\\nGRU (512)\\nPolicy Branch (3)\\nPolicy Branch (3)\\nFlatten Concatenate\\nFC (128)\\nVector Observation\\nFC (512)\\nReshape to sequences\\nReshape to batch\\n\\nFigure 15: GRU-PPO utilizes an actor-critic feed-forward convolutional recurrent neural network. Visual and vector observations are encoded as an entire batch by either convolutional or fully connected layers. The encoded features are concatenated and then reshaped to sequences before feeding them to the recurrent layer. Its output has to be reshaped into the original batch shape. Further, the forward pass is divided into two streams relating to the value function and the policy. The number of policy heads is equal to the number of action dimensions given by a multi-discrete action space.\\n\\nFigure 16: The data preprocessing starts out by sampling trajectories across $W$ workers for $T$ steps. Next, $E$ episodes of varying length $E_T$ are extracted from the trajectories. Those can be further split into $\\\\#$ sequences of varying length $\\\\#_T$. At last, zero padding is used to retrieve sequences of fixed length $\\\\max(\\\\#_T)$.\\n\\nD B ASEILINES\\n\\nD.1 GRU-PPO\\n\\nOur GRU-PPO baseline utilizes the clipped surrogate objective (Schulman et al., 2017). Due to the usage of the Gated Recurrent Unit as the recurrent layer, the to be selected action $a_t$ of the policy $\\\\pi_\\\\theta$ depends on the current observation $o_t$ and hidden state $h_t$ of the recurrent layer.\\n\\n$\\\\hat{A}_t$ denotes advantage estimates based on generalized advantage estimation (GAE) (Schulman et al., 2016), $\\\\theta$ the parameters of a neural net and $\\\\epsilon$ the clip range.\\n\\n$L_{C_t}(\\\\theta) = \\\\hat{E}_t\\\\left[\\\\min(q_t(\\\\theta) \\\\hat{A}_t, \\\\text{clip}(q_t(\\\\theta), 1 - \\\\epsilon, 1 + \\\\epsilon) \\\\hat{A}_t)\\\\right]$ (7)\\n\\nwith ratio $q_t(\\\\theta) = \\\\pi_\\\\theta(a_t|o_t, h_t)$ $\\\\pi_\\\\theta_{old}(a_t|o_t, h_t)$\\n\\nThe value function is optimized using the squared-error loss $L_{V_t}(\\\\theta)$. $H[\\\\pi_\\\\theta](o_t)$ denotes an entropy bonus encouraging exploration (Schulman et al., 2017). Both are weighted using the coefficients $c_1$ and $c_2$ and are added to $L_{C_t}$ to complete the loss:\\n\\n$L_{CVH_t}(\\\\theta) = \\\\hat{E}_t\\\\left[L_{C_t}(\\\\theta) - c_1 L_{V_t}(\\\\theta) + c_2 H[\\\\pi_\\\\theta](o_t, h_t)\\\\right]$ (8)\\n\\nwith $L_{V_t}(\\\\theta) = (V_\\\\theta(o_t, h_t) - V_{targ}_t)^2$\"}"}
{"id": "jHc8dCx6DDr", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Three major components have to be implemented: the forward pass of the model (Figure 15), the processing of the sampled training data (Figure 16), and the loss function (Equation 9).\\n\\nThe training data is sampled by a fixed number of workers for a fixed amount of steps and is stored as a tensor. Each collected trajectory may contain multiple episodes that might have been truncated. After sampling, the data has to be split into episodes. Optionally, these can be further split into smaller sequences of fixed length. Otherwise, the actual sequence length is equal to the length of the longest episode. Episodes or sequences that are shorter than the desired length are padded using zeros. As the data is structured into fragments of episodes, the hidden states of the recurrent layer have to be selected correctly. The output hidden state of the previous sequence is the input hidden state of its consecutive one. This approach is also known as truncated backpropagation through time (truncated bptt). Finally, minibatches sample multiple sequences from the just processed data.\\n\\nConcerning the forward pass of the model, it is more efficient to feed every non-recurrent layer the entire batch of the data (i.e., $\\\\text{batch size} = \\\\text{workers} \\\\times \\\\text{steps}$) and not each sequence one by one. Whenever the batch is going to be fed to a recurrent layer during optimization, the batch has to be reshaped to the dimensions: number of sequences and sequence length. After passing the sequences to the recurrent layer, the data has to be reshaped again to the overall batch size. Note that the forward pass for sampling trajectories operates on a sequence length of one. In this case, the data keeps its shape throughout the entire forward pass.\\n\\nOnce the loss function is being computed, the padded values of the sequences have to be masked so that these do not affect the gradients. $L_{\\\\text{mask}}(\\\\theta) = \\\\frac{P}{T} \\\\sum_{t} \\\\text{mask}_t \\\\times L_{CVH}(\\\\theta)$\\n\\nwith $\\\\text{mask}_t = \\\\begin{cases} 0 & \\\\text{where padding is used} \\\\\\\\ 1 & \\\\text{where no padding is used} \\\\end{cases}$\\n\\nD.2 Memory-Less PPO\\n\\nIn the case of memory-less PPO, Frame Stacking, and HELM, the recurrent layer is removed. The training data is not split into sequences. Loss masking is not used.\\n\\nD.3 Frame Stacking\\n\\n$n-1$ past frames are stacked to the agents current visual observation. In the case of stacking 4 frames, RGB frames are used. Grayscale frames are utilized in the case of stacking 16 frames to reduce the model's input dimension.\\n\\nD.4 HELM\\n\\nIf HELM is used, the model receives another encoding branch that receives the current grayscale visual observation. The observation is then propagated by a frozen hopfield and a pre-trained transformer as described by Paischer et al. 2022. The resulting features are concatenated to the ones from the CNN and the vector observation encoder.\\n\\nD.5 Choice of Hyperparameters\\n\\nTable 6 enumerates the utilized hyperparameters. The agent in Searing Spotlights uses a frame skip of 2, which halves the length of an episode easing the agent's exploration. The quite large batch size is chosen to max out the wall-time efficiency of the utilized resources. One training based on GRU-PPO utilizes almost 32GB of VRAM. Most of the hyperparameters related to memory were found by running a grid search on an early version of Mortar Mayhem Grid. Besides that we made coarse hyperparameter sweeps especially for Searing Spotlights that did not yield any benefit. We...\"}"}
{"id": "jHc8dCx6DDr", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Training and generalization performances show strong dependence on memory.\\n\\nThe environment that requires the agent to traverse the environment. The first one randomly places an exit in the environment, which the agent can use to receive a reward of $+1$ and terminate the episode. Secondly, a coin collection task is added. Before using the environment's exit, the agent has to collect a certain number of randomly positioned coins. Collecting one coin grants a reward of $+\\\\frac{25}{2}$ to the agent. If the agent successfully uses its memory, it can infer its current location, and recall the locations of the exit and the coins. A simplified grid version of Searing Spotlights is not available. As a default simplification, the episode starts with perfect information, and after a few steps, the global light is dimmed until off. Just like in MP, successful episodes terminate sooner.\\n\\nAppendix C.3 enumerates multiple parameters that organize the measures, behavior, and spawning frequency of the spotlights. Other parameters concern the reward function, the agent's properties, and the visual representation of Searing Spotlights.\\n\\n4 BASELINE EXPERIMENTS\\n\\nWe run empirical baseline experiments using the DRL algorithm Proximal Policy Optimization (PPO) (Schulman et al., 2017). To support memory, our implementation adds a gated recurrent unit (GRU) (Cho et al., 2014) to the actor-critic model architecture of PPO (GRU-PPO). Related memory-based approaches such as MRA (Fortunato et al., 2019), GTrXL (Parisotto et al., 2020), HCAM (Lampinen et al., 2021), and ALD (Parisotto & Salakhutdinov, 2021) are not considered, because there is no applicable code available (see appendix G), while these methods are expensive to reproduce. That does not account for HELM (Paischer et al., 2022), which we successfully added to our training framework. However, HELM has poor wall-time efficiency (see figs. 17 and 18 in appendix E). Due to the vast transformer component consisting of 18 blocks, the policy takes much more time to produce actions during inference. One GRU-PPO training run takes about ten hours in MMActGrid with a sequence length of 79, while HELM needs at least six times longer. This effect worsens for longer sequences. The last two baselines consider frame stacking. One stacks 4 RGB frames (4-Stack), while the other one stacks 16 grayscale frames (16-Stack). We repeat all experiments 5 times. All training runs utilize 100,000 environment seeds. Generalization is assessed on 30 novel seeds, which are repeated 5 times. Hence, each data point aggregates 750 episodes. The subsequent sample efficiency curves show the interquartile mean (IQM) and a confidence interval of 0.95 as recommended by Agarwal et al. (2021). For Mystery Path and Searing Spotlights, we report the success rate, which indicates whether the agent succeeded at the entire task or not. Results on Mortar Mayhem show the number of commands that were properly executed during one episode. Appendix D details all baselines, the model architecture, and the used hyperparameters.\\n\\n4.1 DEPENDENCY ON MEMORY\\n\\nTo support our claim that Memory Gym's environments strongly depend on memory, the agents are trained with PPO (memory-less), GRU-PPO, frame stacking, and HELM on MMActGrid, MPGrid (hidden goal), and Searing Spotlights. Figure 4(a) shows the results on MMActGrid where GRU-PPO successfully executes ten commands after 50 million training steps, while memory-less PPO is ineffective by solving only one to two commands. Even though the task requires short-term memory only, HELM and the frame stacking baselines are notably inferior to GRU-PPO. Concerning MPGrid...\"}"}
{"id": "jHc8dCx6DDr", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Levels of difficulty, ranging from easy to unsolved, emerge when scaling Mortar Mayhem and Mystery Path and if trained using GRU-PPO. As seen in figure 5(a), the agent in MMActGrid only needs 50 million steps to accomplish the whole task. In MMAct, it needs twice as long to solve only five commands. Note that the available commands in the grid variants comprise only five command choices and not the full range of nine ones. Adding the Clue Task notably increases the agent's challenge because the agent trained on MMGrid needs the entire training time to succeed. At the same time, the complete task of MM remains unsolved. The performance on training and novel seeds is nearly identical with nearly no variance, which can be explained by the high uncertainty of this environment. A single observation leaves the agent clueless about which of the nine tiles to move next. Therefore the agent has no other choice but to obtain the ability to memorize and recall past events.\\n\\nConcerning MPGrid, if the goal and origin are part of the agent's observation, it takes about 65 million steps for the agent to generalize to novel seeds, as shown in figure 5(c). If only the goal is hidden, the agent needs the entire training time to reach a success rate of 1.0. Hiding the goal and origin degrades the agent's performance to a success rate of 0.2 on novel seeds. Agents trained on MP or MP Dense do not evaluate well on novel seeds. The agent in MP Dense makes little progress due to the help of a denser reward function that signals a reward of 0.03 for visiting a tile of the path for only the first time. Consequently, we also experimented with a negative reward of $-0.001$ for each step taken, severely hurting the training performance.\\n\\n4.3 Recurrence is Vulnerable to Spotlight Perturbations\\n\\nSearing Spotlights remains a puzzle. It is difficult to tell whether the agent's trained recurrent policy learned anything meaningful. Collecting the coin seems worse than chance. The agent's remaining health points drop to zero in nearly all evaluated episodes because avoiding the spotlights does not work well. It seems that the agent struggles to determine what it controls. Even though the spotlights unveil information on the ground truth of the environment or leave room for exploitation, the recurrent agent seems severely hurt by the perturbations inflicted by the randomly moving spotlights.\"}"}
{"id": "jHc8dCx6DDr", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 6: Under perfect information, the noise inflicted by spotlight perturbations harms the recurrent agents\u2019 performance in Searing Spotlights (a) and Procgen\u2019s BossFight (b). (c) denotes the training performances on training seeds when trained with and without perturbations using PPO and GRU-PPO. The IQM return denotes the summed rewards that the agent achieved during one episode.\\n\\nwhich can be considered noise. Regardless of any executed measure (see appendix F for details), for instance, environmental ablations, model architectures, and hyperparameters, progress in learning a reasonable behavior is lacking. When trained with GRU-PPO, the only solvable scenario is to remove the spotlights. In this case, if the global light is initially turned on (perfect information) and dimmed until off during the first few steps, the agent rapidly collects the coin and uses the exit. In contradiction, if the global light is slightly dimmed while spotlights are present (Figure 6(a)), GRU-PPO fails. On the other hand, memory-less PPO quickly succeeds because this fully observable task is trivial.\\n\\nThose results are also apparent in training a different environment. We choose Procgen\u2019s BossFight because it rather needs fewer steps to train as seen in the results of Cobbe et al. 2020. BossFight is set up to utilize only one level causing the boss always to behave the same. We extrapolate this level by 100,000 spotlight perturbation seeds under perfect information. Its background is rendered white to match Searing Spotlight\u2019s visual appearance more closely. The now-established BossFight task (Figure 6(b)) invites overfitting. Figure 6(c) shows results with and without spotlight perturbations on PPO and GRU-PPO. Notably, GRU-PPO, perturbed by spotlights, struggles.\\n\\n5 CONCLUSION AND FUTURE WORK\\n\\nBy accomplishing the desiderata that we have defined and examined in this work, and by evaluating the baseline experiments, we believe that Memory Gym has proven potential to complement the landscape of Deep Reinforcement Learning benchmarks specializing on agents leveraging memory. Memory Gym\u2019s unique environments, Mortar Mayhem, Mystery Path, and Searing Spotlights, strongly depend on memory and frequent agent-memory interactions. As the environments\u2019 difficulties are smoothly scalable, current and future approaches to memory may find their right fit for examination. To our surprise, Searing Spotlights poses a yet unsolved challenge to agents leveraging GRU or LSTM.\\n\\nIn future work, it will be intriguing to see whether adjacent and novel memory approaches suffer from the noisy perturbations inflicted by the spotlights. HELM\u2019s performance on Searing Spotlights may point toward a transformer-based approach (e.g. HCAM (Lampinen et al., 2021), GTrXL (Parisotto et al., 2020)), while approaches that have not been used yet in the context of Deep Reinforcement Learning (e.g. Structural State-Space models (Gu et al., 2022)), are also considerable for Memory Gym in general. Consecutive work may extrapolate the concepts of Memory Gym\u2019s environments. Mortar Mayhem could be advanced to an open-ended environment. For instance, the Clue Task and the Act Task could be stacked infinitely. Every Clue Task could only show one command at a time, while the Act Task requires the agent to execute all commands that were shown so far. Hence, memory approaches could be compared by achieved episode lengths, which are correlated with the amount of information that the agent must be able to recall. Such a goal in mind may inspire Mystery Path as well. The to-be-traversed path could be endless. However, a larger environment may not fit into the agent\u2019s observation that an egocentric observation could compensate.\"}"}
{"id": "jHc8dCx6DDr", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare. Deep reinforcement learning at the edge of the statistical precipice. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.\\n\\nMarcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal J\u00f3zefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider, Szymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba. Learning dexterous in-hand manipulation. Int. J. Robotics Res., 39(1), 2020. doi: 10.1177/0278364919887447. URL https://doi.org/10.1177/0278364919887447.\\n\\nAndrea Baisero and Sammie Katt. gym-gridverse: Gridworld domains for fully and partially observable reinforcement learning. https://github.com/abaisero/gym-gridverse, 2021.\\n\\nBowen Baker, Ingmar Kanitscheider, Todor M. Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=SkxpxJBKwS.\\n\\nCharles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K\u00fctter, Andrew Lefrancq, Simon Green, V\u00edctor Vald\u00e9s, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. Deepmind lab. CoRR, abs/1612.03801, 2016. URL http://arxiv.org/abs/1612.03801.\\n\\nMarc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res., 47:253\u2013279, 2013. doi: 10.1613/jair.3912. URL https://doi.org/10.1613/jair.3912.\\n\\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Christopher Hesse, Rafal J\u00f3zefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pond\u00e9 de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement learning. CoRR, abs/1912.06680, 2019. URL http://arxiv.org/abs/1912.06680.\\n\\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.org/abs/1606.01540.\\n\\nMaxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for openai gym. https://github.com/maximecb/gym-minigrid, 2018.\\n\\nKyunghyun Cho, Bart van Merrienboer, \u00c7a\u011flar G\u01d4le\u00e7, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.), Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pp. 1724\u20131734. ACL, 2014. doi: 10.3115/v1/d14-1179. URL https://doi.org/10.3115/v1/d14-1179.\\n\\nKarl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 2048\u20132056. PMLR, 2020. URL http://proceedings.mlr.press/v119/cobbe20a.html.\\n\\nMatthew Crosby, Benjamin Beyret, Murray Shanahan, Jos\u00e9 Hern\u00e1ndez-Orallo, Lucy Cheke, and Marta Halina. The animal-ai testbed and competition. In Hugo Jair Escalante and Raia Hadsell (eds.), Proceedings of the NeurIPS 2019 Competition and Demonstration Track, volume 123 of Proceedings of Machine Learning Research, pp. 164\u2013176. PMLR, 08\u201314 Dec 2020.\"}"}
