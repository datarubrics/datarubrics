{"id": "BlyXYc4wF2-", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 3: MAPPO-Lagrangian\\n\\n1: Input: Stepsizes $U, U_\\\\$\\\\$, batch size, number of: agents, episodes, steps per episode, discount factor $W$\\n\\n2: Initialize: Actor networks $\\\\{P_\\\\$\\\\$, $\\\\forall \\\\$\\\\in N\\\\}$, Global V-value network $\\\\{q_\\\\$ P\\\\}$, V-cost networks $\\\\{q_\\\\$ 9, 8_\\\\$ P\\\\} 8 \\\\in N 1 \\\\leq 9 < 8 9$, Replay buffer $B$\\n\\n3: for $P = P, 1, \\\\ldots, P, 1$ do\\n\\n4: Collect a set of trajectories by running the joint policy $\\\\pi(\\\\theta)$:\\n\\n5: Push transitions $\\\\{(> 8_C , 0 8_C , > 8_C + 1 , A_C )\\\\}$, $\\\\forall 8 \\\\in N, C \\\\in$ into $B$\\n\\n6: Sample a random minibatch of transitions from $B$\\n\\n7: Compute advantage function $\\\\hat{\\\\$}(s, a)$ based on global V-value network with GAE.\\n\\n8: Compute cost advantage functions $\\\\hat{\\\\$}_9(s, a)$ for all agents and costs, based on V-cost networks with GAE.\\n\\n9: Draw a random permutation of agents $\\\\$ = \\\\$. 1$:\\n\\n10: Set \\\"$_\\\\$ (s, a) = $\\\\hat{\\\\$}(s, a)$.\\n\\n11: for agent $\\\\$ = 1, \\\\ldots, $ = do\\n\\n12: Initialise a policy parameter $\\\\$ = $, and Lagrangian multipliers $\\\\_9 = P, \\\\forall 9 = 1, \\\\ldots, <$.\\n\\n13: Make the Lagrangian modification step of objective construction \\\"$_\\\\$ (s, a) = $\\\\hat{\\\\$}_9(s, a) - \\\\int_9 = 1 \\\\_9 \\\\hat{\\\\$}_9(s, a)$.\\n\\n14: for $4 = 1, \\\\ldots, $ do\\n\\n15: Differentiate the Lagrangian PPO-Clip objective $\\\\Delta_\\\\$ = $\\\\nabla_\\\\$ 1 $\\\\int_1 = 1 $\\\\int_ = $\\\\bar{c}_\\\\$ (0 $\\\\$ C | > 9 $\\\\$ C) - $\\\\hat{\\\\$}_9(s_C, a_C)$.\\n\\n16: Update temporarily the actor parameters $\\\\$ = $\\\\$ + $\\\\Delta_\\\\$\\n\\n17: for $9 = 1, \\\\ldots, <$ do\\n\\n18: Approximate the constraint violation $3 = $\\\\int_1 = 1 $\\\\int_ = 1 $\\\\int_ = 1 $\\\\int_ = $\\\\hat{\\\\$} + $\\\\_9 (B_C, 0 $\\\\$ C) - $\\\\hat{\\\\$}$.\\n\\n19: Differentiate the constraint $\\\\Delta_\\\\$ = $-$ $\\\\int_1 = 1 $\\\\int_ = 1 $\\\\int_ = $\\\\bar{c}_\\\\$ (1 - \\\\$) + $\\\\int_ = P c_\\\\$ $\\\\bar{c}_\\\\$ (0 $\\\\$ C | > 9 $\\\\$ C) $\\\\hat{\\\\$}_9(B_C, 0 $\\\\$ C)$.\\n\\n20: end for\\n\\n21: for $9 = 1, \\\\ldots, <$ do\\n\\n22: Update temporarily the Lagrangian multiplier $\\\\_9 = \\\\text{ReLU}(\\\\_9 - $\\\\$ \\\\Delta_\\\\$\\n\\n23: end for\\n\\n24: end for\\n\\n25: Update the actor parameter $\\\\$ = $\\\\$ + 1 $\\\\$.\\n\\n26: Compute \\\"$_\\\\$ + 1 (s, a) = $\\\\bar{c}_\\\\$ (1 $\\\\$ | o)$, $\\\\forall \\\\$ =\\n\\n27: end for\\n\\n28: Update V-value network (and V-cost networks analogously) by following formula: $q_\\\\$ + 1 = \\\\text{arg min}_q (B_C) - $\\\\hat{\\\\$}'_C 2$\\n\\n29: end for\\n\\n30: end for\"}"}
{"id": "BlyXYc4wF2-", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Safe MAMuJoCo is an extension of MAMuJoCo (Peng et al., 2020). In particular, the background environment, agents, physics simulator, and the reward function are preserved. However, as opposed to its predecessor, Safe MAMuJoCo environments come with obstacles, like walls or bombs. Furthermore, with the increasing risk of an agent stumbling upon an obstacle, the environment emits cost (Brockman et al., 2016). According to the scheme from Zanger et al. (2021), we characterise the cost functions for each task below.\\n\\nThe width of the corridor set by two walls is \\\\( 9 \\\\). The width of the corridor set by three folding line walls with an angle of 30 degrees is \\\\( 1 \\\\). The environment emits the cost of \\\\( 1 \\\\) for an agent, if the distance between the robot and the wall is less than \\\\( 1 \\\\), or when the robot topples over. This can be described as:\\n\\n\\\\[\\nc_C = \\\\begin{cases} \\nP, & \\\\text{for } P \\\\leq z_{\\\\text{torso}, C+1} + 1 \\\\leq 1, \\\\\\\\\\n0, & \\\\text{otherwise}\\n\\\\end{cases}\\n\\\\]\\n\\nwhere \\\\( z_{\\\\text{torso}, C+1} \\\\) is the robot's torso's \\\\( I \\\\)-coordinate, and \\\\( x_{\\\\text{torso}, C+1} \\\\) is the robot's torso's \\\\( G \\\\)-coordinate, at time \\\\( C+1 \\\\); \\\\( x_{\\\\text{wall}} \\\\) is the \\\\( G \\\\)-coordinate of the wall.\\n\\nIn these tasks, the agents move inside a corridor (which constraints their movement, but does not induce costs). Together with them, there are bombs moving inside the corridor. If an agent finds itself too close to a bomb, the distance between an agent and a bomb is less than \\\\( 9 \\\\), a cost of \\\\( 1 \\\\) will be emitted.\\n\\n\\\\[\\nc_C = \\\\begin{cases} \\nP, & \\\\text{for } y_{\\\\text{torso}, C+1} - y_{\\\\text{obstacle}} \\\\geq 9 \\\\ 1, \\\\\\\\\\n0, & \\\\text{otherwise}\\n\\\\end{cases}\\n\\\\]\\n\\nwhere \\\\( y_{\\\\text{torso}, C+1} \\\\) is the \\\\( H \\\\)-coordinate of the robot's torso, and \\\\( y_{\\\\text{obstacle}} \\\\) is the \\\\( H \\\\)-coordinate of the moving obstacle.\"}"}
{"id": "BlyXYc4wF2-", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We provide additional results on the Safe Many-Agent ant tasks. The width of the corridor is $12$; its walls fold at the angle of $30$ degrees. The environment emits the cost of $1$ for an agent, if the distance between the robot and the wall is less than $1.8$, or when the robot topples over. This can be described as $c_C = \\\\begin{cases} 0, & \\\\text{if } P_{torso} + C_{+1} - x_{wall}^2 \\\\geq 1.8, \\\\\\\\ P_\\\\text{and } x_{torso}^2, & \\\\text{otherwise}. \\\\end{cases}$\\n\\nwhere $z_{torso}^2_{C_{+1}}$ is the robot's torso's $I$-coordinate, and $x_{torso}^2_{C_{+1}}$ is the robot's torso's $G$-coordinate, at time $C_{+1}$; $x_{wall}$ is the $G$-coordinate of the wall.\\n\\nFigure 5: Many-Agent Ant 3x2 with two folding line walls.\\n\\nFigure 6: Performance comparisons on tasks of Safe ManyAgent Ant in terms of cost (the first row) and reward (the second row). The safety constraint values is set to $1P$. Our algorithms are the only ones that learn the safety constraints, while achieving satisfying performance in terms of the reward.\"}"}
{"id": "BlyXYc4wF2-", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we introduce the details of settings for our experiments. The code is available at https://github.com/Anonymous-ICLR2022/Multi-Agent-Constrained-Policy-Optimisation\\n\\n| hyperparameters       | value              |\\n|-----------------------|--------------------|\\n| critic lr             | 5e-3               |\\n| optimizer             | Adam               |\\n| num mini-batch        | 40                 |\\n| gamma                 | 0.99               |\\n| optim eps             | 1e-5               |\\n| batch size            | 16000              |\\n| gain                  | 0.01               |\\n| hidden layer          | 1                  |\\n| training threads      | 4                  |\\n| std y coef            | 0.5                |\\n| actor network         | mlp                |\\n| rollout threads       | 16                 |\\n| std x coef            | 1                  |\\n| eval episodes         | 32                 |\\n| episode length        | 1000               |\\n| activation            | ReLU               |\\n| hidden layer dim      | 64                 |\\n| max grad norm         | 10                 |\\n\\nTable 1: Common hyperparameters used for MAPPO-Lagrangian, MAPPO, HAPPO, IPPO, and MACPO in the Safe Multi-Agent MuJoCo domain\\n\\n| Algorithms             | actor lr | ppo epoch | kl-threshold | ppo-clip | Lagrangian coef | Lagrangian lr | fraction | fraction coef |\\n|------------------------|----------|-----------|--------------|----------|----------------|---------------|----------|---------------|\\n| MAPPO-Lagrangian       | 9e-5     | 5         | /            | 0.2      | 0.78           | 1e-3          | /        | /             |\\n| MAPPO                  | 9e-5     | 5         | /            | 0.2      | /              | /             | /        | /             |\\n| HAPPO                  | 9e-5     | 5         | /            | 0.2      | /              | /             | /        | /             |\\n| IPPO                   | 9e-5     | 5         | /            | 0.2      | /              | /             | /        | /             |\\n| MACPO                  | /        | /         | 0.0065       | /        | /              | /             | 0.5      | 0.27          |\\n\\nTable 2: Different hyperparameters used for MAPPO-Lagrangian, MAPPO, HAPPO, IPPO, and MACPO in the Safe Multi-Agent MuJoCo domain.\\n\\n| task value             | value |\\n|------------------------|-------|\\n| Ant(2x4)               | 0.2   |\\n| Ant(4x2)               | 0.2   |\\n| Ant(2x4d)              | 0.2   |\\n| HalfCheetah(2x3)       | 5     |\\n| HalfCheetah(3x2)       | 5     |\\n| HalfCheetah(6x1)       | 5     |\\n| ManyAgent Ant(2x3)     | 1     |\\n| ManyAgent Ant(3x2)     | 1     |\\n| ManyAgent Ant(6x1)     | 1     |\\n\\nTable 3: Safety bound used for MACPO in the Safe Multi-Agent MuJoCo domain.\"}"}
{"id": "BlyXYc4wF2-", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nDeveloping reinforcement learning algorithms that satisfy safety constraints is becoming increasingly important in real-world applications. In multi-agent reinforcement learning (MARL) settings, policy optimisation with safety awareness is particularly challenging because each individual agent has to not only meet its own safety constraints, but also consider those of others so that their joint behaviour can be guaranteed safe. Despite its importance, the problem of safe multi-agent learning has not been rigorously studied; very few solutions have been proposed, nor a sharable testing environment or benchmarks. To fill these gaps, in this work, we formulate the safe MARL problem as a constrained Markov game and solve it with policy optimisation methods. Our solutions\u2014Multi-Agent Constrained Policy Optimisation (MACPO) and MAPPO-Lagrangian\u2014leverage the theories from both constrained policy optimisation and multi-agent trust region learning. Crucially, our methods enjoy theoretical guarantees of both monotonic improvement in reward and satisfaction of safety constraints at every iteration. To examine the effectiveness of our methods, we develop the benchmark suite of Safe Multi-Agent MuJoCo that involves a variety of MARL baselines. Experimental results justify that MACPO/MAPPO-Lagrangian can consistently satisfy safety constraints, meanwhile achieving comparable performance to strong baselines.\\n\\n1 Introduction\\nIn recent years, reinforcement learning (RL) techniques have achieved remarkable successes on a variety of complex tasks (Silver et al., 2016; 2017; Vinyals et al., 2019). Powered by deep neural networks, deep RL enables learning sophisticated behaviours. On the other hand, deploying neural networks turns the optimisation procedure from policy space to parameter space; this enables gradient-based methods to be applied (Sutton et al., 1999; Lillicrap et al., 2015; Schulman et al., 2017). For policy gradient methods, at every iteration, the parameters of a policy network are updated in the direction of the gradient that maximises return.\\n\\nHowever, policies that are purely optimised for reward maximisation are rarely applicable to real-world problems. In many applications, an agent is often required not to visit certain states or take certain actions, which are thought of as \\\"unsafe\\\" either for itself or for other elements in the background (Moldovan & Abbeel, 2012; Achiam et al., 2017). For instance, a robot carrying materials in a warehouse should not damage its parts while delivering an item to a shelf, nor should a self-driving car cross on the red light while rushing towards its destination (Shalev-Shwartz et al., 2016).\\n\\nTo tackle these issues, Safe RL (Moldovan & Abbeel, 2012; Garc\u00eda & Fern\u00e1ndez, 2015) is proposed, aiming to develop algorithms that learn policies that satisfy safety constraints. Despite the additional requirement of safety on solutions, algorithms with convergence guarantees have been proposed (Xu et al., 2021; Wei et al., 2021).\\n\\nDeveloping safe policies for multi-agent systems is a challenging task. Part of the difficulty comes from solving multi-agent reinforcement learning (MARL) problems itself (Deng et al., 2021); more importantly, tackling safety in MARL is hard because each individual agent has to not only consider its own safety constraints, which already may conflict its reward maximisation, but also consider the safety constraints of others so that their joint behaviour is guaranteed to be safe. As a result, there are very few solutions that offer effective learning algorithms for safe MARL problems. In fact, many of the existing methods focus on learning to cooperate (Foerster et al., 2018; Rashid et al., 2018;...\"}"}
{"id": "BlyXYc4wF2-", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022 (Yang & Wang, 2020). However, they often require certain structures on the solution; for example, Rashid et al. (2018) and Yang et al. (2020) adopt greedy maximisation on the local component of a monotonic joint value function, and Foerster et al. (2018) estimates the policy gradient based on the counterfactual value from a joint critic. Therefore, it is unclear how to directly incorporate safety constraints into these solution frameworks. Consequently, developing agents' collaborations towards reward maximisation under safety constraints remains an unsolved problem.\\n\\nThe goal of this paper is to increase practicality of MARL algorithms through endowing them with safety awareness. For this purpose, we introduce a general framework to formulate safe MARL problems, and solve them through multi-agent policy optimisation methods. Our solutions leverage techniques from both constrained policy optimisation (Achiam et al., 2017) and multi-agent trust region learning (Kuba et al., 2021a). The resulting algorithm attains properties of both monotonic improvement guarantee and constraints satisfaction guarantee at every iteration during training. To execute the optimisation objectives, we introduce two practical deep MARL algorithms: MACPO and MAPPO-Lagrangian. As a side contribution, we also develop the first safe MARL benchmark within the MuJoCo environment, which include a variety of MARL baseline algorithms. We evaluate MACPO and MAPPO-Lagrangian on a series of tasks, and results clearly confirm the effectiveness of our solutions both in terms of constraints satisfaction and reward maximisation. To our best knowledge, MACPO and MAPPO-Lagrangian are the first safety-aware model-free MARL algorithms that work effectively in the challenging MuJoCo tasks with safety constraints.\\n\\nRegarding safety in the development of AI is a long-standing topic (Amodei et al., 2016). When it comes to safe reinforcement learning (Garc\u0131a & Fern\u00e1ndez, 2015), a commonly used framework is Constrained Markov Decision Processes (CMDPs) (Altman, 1999). In a CMDP, at every step, in addition to the reward, the environment emits costs associated with certain constraints. As a result, the learning agent must try to satisfy those constraints while maximising the total reward. In general, the cost from the environment can be thought of as a measure of safety. Under the framework of CMDP, a safe policy is the one that explores the environment safely by keeping the total costs under certain thresholds. To tackle the learning problem in CMDPs, Achiam et al. (2017) introduced Constrained Policy Optimisation (CPO), which updates agent's policy under the trust region constraint (Schulman et al., 2015) to maximise surrogate return while obeying surrogate cost constraints. However, solving a constrained optimisation at every iteration of CPO can be cumbersome for implementation. An alternative solution is to apply primal-dual methods, giving rise to methods like TRPO-Lagrangian and PPO-Lagrangian (Ray et al., 2019). Although these methods achieve impressive performance in terms of safety, the performance in terms of reward is poor (Ray et al., 2019). Another class of algorithms that solves CMDPs is by Chow et al. (2018; 2019); these algorithms leverage the theoretical property of the Lyapunov functions and propose safe value iteration and policy gradient procedures. In contrast to CPO, Chow et al. (2018; 2019) can work with off-policy methods; they also can be trained end-to-end with no need for line search.\\n\\nSafe multi-agent learning is an emerging research domain. Despite its importance (Shalev-Shwartz et al., 2016), there are few solutions that work with MARL in a model-free setting. The majority of methods are designed for robotics learning. For example, the technique of barrier certificates (Borrmann et al., 2015; Ames et al., 2016; Qin et al., 2020) or model predictive shielding (Zhang et al., 2019) from control theory is used to model safety. These methods, however, are specifically derived for robotics applications; they either are supervised learning based approaches, or require specific assumptions on the state space and environment dynamics. Moreover, due to the lack of a benchmark suite for safe MARL algorithms, the generalisation ability of those methods is unclear.\\n\\nThe most related work to ours is Safe Dec-PG (Lu et al., 2021) where they used the primal-dual framework to find the saddle point between maximising reward and minimising cost. In particular, they proposed a decentralised policy descent-ascent method through a consensus network. However, reaching a consensus equivalently imposes an extra constraint of parameter sharing among neighbouring agents, which could yield suboptimal solutions (Kuba et al., 2021a). Furthermore, multi-agent policy gradient methods can suffer from high variance (Kuba et al., 2021b). In contrast, our methods employ trust region optimisation and do not assume any parameter sharing.\\n\\nHATRPO (Kuba et al., 2021a) introduced the first multi-agent trust region method that enjoys theoretically-justified monotonic improvement guarantee. Its key idea is to make agents follow a\"}"}
{"id": "BlyXYc4wF2-", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nBei Peng, Tabish Rashid, Christian A Schroeder de Witt, Pierre-Alexandre Kamienny, Philip HS Torr, Wendelin B\u00f6hrer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy gradients. arXiv preprint arXiv:2003.06709, 2020.\\n\\nDavid Pollard. Asymptopia: an exposition of statistical asymptotic theory. 2000. URL http://www.stat.yale.edu/pollard/Books/Asymptopia, 2000.\\n\\nZengyi Qin, Kaiqing Zhang, Yuxiao Chen, Jingkai Chen, and Chuchu Fan. Learning safe multi-agent control with decentralized neural barrier certificates. In International Conference on Learning Representations, 2020.\\n\\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4295\u20134304. PMLR, 2018.\\n\\nAlex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep reinforcement learning. arXiv preprint arXiv:1910.01708, 7, 2019.\\n\\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.\\n\\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pp. 1889\u20131897. PMLR, 2015.\\n\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\n\\nShai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.\\n\\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.\\n\\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354\u2013359, 2017.\\n\\nRichard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy gradient methods for reinforcement learning with function approximation. In NIPs, volume 99, pp. 1057\u20131063. Citeseer, 1999.\\n\\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019.\\n\\nHonghao Wei, Xin Liu, and Lei Ying. A provably-efficient model-free algorithm for constrained markov decision processes. arXiv preprint arXiv:2106.01577, 2021.\\n\\nTengyu Xu, Yingbin Liang, and Guanghui Lan. Crpo: A new approach for safe reinforcement learning with convergence guarantee. In International Conference on Machine Learning, pp. 11480\u201311491. PMLR, 2021.\\n\\nYaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theoretical perspective. arXiv preprint arXiv:2011.00583, 2020.\\n\\nYaodong Yang, Ying Wen, Jun Wang, Liheng Chen, Kun Shao, David Mguni, and Weinan Zhang. Multi-agent determinantal q-learning. In International Conference on Machine Learning, pp. 10757\u201310766. PMLR, 2020.\\n\\nChao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.\"}"}
{"id": "BlyXYc4wF2-", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nMoritz A. Zanger, Karam Daaboul, and J. Marius Z\u00f6llner. Safe continuous control with constrained model-based policy optimization, 2021.\\n\\nWenbo Zhang, Osbert Bastani, and Vijay Kumar. Mamps: Safe multi-agent reinforcement learning via model predictive shielding. arXiv preprint arXiv:1910.12639, 2019.\"}"}
{"id": "BlyXYc4wF2-", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "BlyXYc4wF2-", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma 1. (Multi-Agent Advantage Decomposition, Kuba et al. (2021b)).\\n\\nFor any state $B \\\\in S$, subset of agents $\\\\pi_1: \\\\mathcal{H} \\\\subseteq \\\\mathcal{N}$, and joint action $a_{\\\\pi_1: \\\\mathcal{H}}$, the following identity holds:\\n\\n$$\\\\pi(B, a_{\\\\pi_1: \\\\mathcal{H}}) = \\\\mathcal{H} \\\\mathcal{O} \\\\mathcal{H} = \\\\mathcal{H} \\\\pi(B, a_{\\\\mathcal{H} - 1: \\\\mathcal{H}}).$$\\n\\nProof. We write the multi-agent advantage as in its definition, and expand it in a telescoping sum.\\n\\n$$\\\\pi(B, a_{\\\\pi_1: \\\\mathcal{H}}) = \\\\mathcal{H} \\\\pi(B, a_{\\\\pi_1: \\\\mathcal{H}}) - \\\\mathcal{H} \\\\pi(B, a_{\\\\pi_1 - 1: \\\\mathcal{H}}) + \\\\pi(B).$$\\n\\nLemma 2. Let $\\\\pi$ and $\\\\bar{\\\\pi}$ be joint policies. Let $\\\\pi_9 \\\\in \\\\mathcal{N}$ be an agent, and $\\\\mathcal{H} \\\\in \\\\{1, \\\\ldots, < \\\\mathcal{H}\\\\}$ be an index of one of its costs. The following inequality holds:\\n\\n$$\\\\mathcal{H} \\\\bar{\\\\pi} \\\\leq \\\\mathcal{H} \\\\pi + \\\\sum_{s \\\\sim \\\\mathcal{D}(\\\\pi), a \\\\sim \\\\bar{\\\\pi}} \\\\left[\\\\mathcal{H} \\\\pi(s, a)\\\\right] + 4U\\\\max_{B,0} \\\\mathcal{H} \\\\\\\\pi(B,0) | (1 - W^2)\\\\max_{B} KL(\\\\mathcal{H}, \\\\bar{\\\\mathcal{H}}),$$\\n\\nwhere $U = \\\\max TV(\\\\pi, \\\\bar{\\\\pi}) = \\\\max_{B} TV(\\\\pi(\\\\cdot|B), \\\\bar{\\\\pi}(\\\\cdot|B)).$\\n\\nProof. From the proof of Theorem 1 from Schulman et al. (2015) (in particular, equations (41)-(45)), applied to joint policies $\\\\pi$ and $\\\\bar{\\\\pi}$, we conclude that:\\n\\n$$\\\\mathcal{H} \\\\bar{\\\\pi} \\\\leq \\\\mathcal{H} \\\\pi + \\\\sum_{s \\\\sim \\\\mathcal{D}(\\\\pi), a \\\\sim \\\\bar{\\\\pi}} \\\\left[\\\\mathcal{H} \\\\pi(s, a)\\\\right] + 4U\\\\max_{B,0} \\\\mathcal{H} \\\\\\\\pi(B,0) | (1 - W^2)\\\\max_{B} KL(\\\\mathcal{H}, \\\\bar{\\\\mathcal{H}}),$$\\n\\nwhere $U = \\\\max TV(\\\\pi, \\\\bar{\\\\pi}) = \\\\max_{B} TV(\\\\pi(\\\\cdot|B), \\\\bar{\\\\pi}(\\\\cdot|B)).$\"}"}
{"id": "BlyXYc4wF2-", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Remark 1. In Algorithm 1, we compute the size of KL constraint as\\n\\\\[ X_8^\\\\ell = \\\\min\\\\{X_1^\\\\ell, \\\\leq \\\\ell - 1, X_1^{\\\\ell-1} \\\\leq 9 < 8^{\\\\ell-1} \\\\} \\\\]\\n\\\\[ = (\\\\pi_c^8) - 8^{\\\\ell-1} \\\\leq 9, \\\\quad (c_8^\\\\ell + 1) - a_8^\\\\ell \\\\leq \\\\ell - 1 \\\\]\\n\\\\[ D = 1 \\\\max KL(c_8^\\\\ell, c_8^\\\\ell+1) \\\\]\\n\\\\[ a_8^\\\\ell = X_8^1 \\\\geq \\\\ell + 1 \\\\]\\n\\nNote that \\\\( X_8^1 \\\\) (i.e., \\\\( \\\\ell = 1 \\\\)) is guaranteed to be non-negative if \\\\( \\\\pi_c \\\\) satisfies safety constraints; that is because then \\\\( 2^{\\\\ell-1} \\\\geq 8^{\\\\ell-1} \\\\) for all \\\\( \\\\ell \\\\) and \\\\( 9 \\\\), and the set \\\\( \\\\{ \\\\ell \\\\mid \\\\ell < \\\\ell \\\\} \\\\) is empty.\\n\\nThis formula for \\\\( X_8^\\\\ell \\\\), combined with Lemma 2, assures that the policies within \\\\( X_8^\\\\ell \\\\) have a KL distance from \\\\( c_8^\\\\ell \\\\) that is lower than \\\\( 2^{\\\\ell-1} \\\\).\\n\\nBy Lemma 2, the left-hand side of the above inequality is an upper bound of \\\\( (\\\\pi_c^8) + a_8^\\\\ell \\\\leq 9, \\\\quad (c_8^\\\\ell + 1) - a_8^\\\\ell \\\\leq \\\\ell - 1 \\\\)\\n\\\\[ D = 1 \\\\max KL(c_8^\\\\ell, c_8^\\\\ell+1) \\\\]\\n\\\\[ a_8^\\\\ell \\\\]\\n\\nimplies \\\\( 8^{\\\\ell-1} (\\\\pi_c^8) + a_8^\\\\ell \\\\leq 9, \\\\quad (c_8^\\\\ell + 1) - a_8^\\\\ell \\\\leq \\\\ell - 1 \\\\)\\n\\\\[ D = 1 \\\\max KL(c_8^\\\\ell, c_8^\\\\ell+1) + a_8^\\\\ell \\\\]\\n\\\\[ \\\\leq 2^{\\\\ell-1} \\\\]\\n\\nTheorem 1. If a sequence of joint policies \\\\( (\\\\pi_c^\\\\ell) \\\\) is obtained from Algorithm 1, then it has the monotonic improvement property, \\\\((\\\\pi_c^{\\\\ell+1}) \\\\geq (\\\\pi_c^\\\\ell)\\\\), as well as it satisfies the safety constraints, \\\\( (\\\\pi_c^\\\\ell) \\\\leq 2^{\\\\ell-1}, \\\\) for all \\\\( \\\\ell \\\\in \\\\mathbb{N}, 8 \\\\in \\\\mathbb{N}, \\\\) and \\\\( 9 \\\\in \\\\{1, 2, \\\\ldots, <8\\\\} \\\\).\\n\\nProof. Safety constraints are assured to be met by Remark 1. It suffices to show the monotonic improvement property. Notice that at every iteration \\\\( \\\\ell \\\\) of Algorithm 1, \\\\( c_8^\\\\ell \\\\in \\\\Pi_8^\\\\ell \\\\). Clearly\\n\\\\[ \\\\max KL(c_8^\\\\ell, c_8^\\\\ell+1) = P \\\\leq X_8^\\\\ell. \\\\]\\nMoreover,\\n\\\\[ (\\\\pi_c^\\\\ell) + a_8^\\\\ell = X_8^\\\\ell (\\\\pi_c^\\\\ell) \\\\leq 2^{\\\\ell-1}, \\\\]\\n\\\\[ \\\\max KL(c_8^\\\\ell, c_8^\\\\ell+1) \\\\leq 2^{\\\\ell-1}, \\\\]\\n\\\\[ (12) \\\\]\\n\\nis analogous.\"}"}
{"id": "BlyXYc4wF2-", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where the inequality is guaranteed by updates of previous agents, as described in Remark 1 (Inequality 12). By Theorem 1 from Schulman et al. (2015), we have\\n\\n\\\\[\\n(\\\\pi^\\\\ast + 1) \\\\geq (\\\\pi^\\\\ast) + s \\\\sim_d \\\\pi^\\\\ast, a \\\\sim \\\\pi^\\\\ast + 1 [\\\\pi^\\\\ast(s, a)] - a_{\\\\text{max}} \\\\text{KL}(\\\\pi^\\\\ast, \\\\pi^\\\\ast + 1),\\n\\\\]\\n\\nwhich by Equation 11 is lower-bounded by\\n\\n\\\\[\\n(\\\\pi^\\\\ast) + s \\\\sim_d \\\\pi^\\\\ast, a \\\\sim \\\\pi^\\\\ast + 1 [\\\\pi^\\\\ast(s, a)] - a_{\\\\text{max}} \\\\text{KL}(c_8^\\\\ast, c_8^\\\\ast + 1)\\n\\\\]\\n\\nwhich by Lemma 1 equals\\n\\n\\\\[\\n(\\\\pi^\\\\ast) + \\\\sum_p [\\\\pi^\\\\ast(p)] = \\\\sum_p (\\\\pi^\\\\ast(p)) - a_{\\\\text{max}} \\\\text{KL}(c_8^\\\\ast, c_8^\\\\ast + 1)\\n\\\\]\\n\\nand as for every \\\\(h\\\\), \\\\(c_8^h\\\\) is the argmax, this is lower-bounded by\\n\\n\\\\[\\n(\\\\pi^\\\\ast) + \\\\sum_p [\\\\pi^\\\\ast(p)] = \\\\sum_p (\\\\pi^\\\\ast(p)) - a_{\\\\text{max}} \\\\text{KL}(c_8^h, c_8^h + 1)\\n\\\\]\\n\\nwhich, as follows from Definition 1, equals\\n\\n\\\\[\\nJ(\\\\pi_k) + \\\\sum_p [\\\\pi^\\\\ast(p)] = \\\\sum_p (\\\\pi^\\\\ast(p)) - a_{\\\\text{max}} \\\\text{KL}(c_8^h, c_8^h + 1)\\n\\\\]\\n\\nwhich finishes the proof. \\\\(\\\\square\\\\)\"}"}
{"id": "BlyXYc4wF2-", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Theorem 2. The solution to the following problem\\n\\\\[ p^* = \\\\min_{x} g(x) \\\\]\\nsubject to\\n\\\\[ b(x) + 2I \\\\leq P \\\\]\\n\\\\[ xH \\\\leq X, \\\\]\\nwhere \\\\( g, b, x \\\\in \\\\mathbb{R}^n \\\\), \\\\( P, H \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\), and \\\\( H \\\\succ P \\\\). When there is at least one strictly feasible point, the optimal point \\\\( x^* \\\\) satisfies:\\n\\\\[ x^* = -\\\\frac{1}{\\\\lambda^*}H - \\\\frac{1}{\\\\lambda^*}(g) + E^*b \\\\]\\nwhere \\\\( \\\\lambda^* \\\\) and \\\\( E^* \\\\) are defined by\\n\\\\[ E^* = (\\\\lambda^* - \\\\lambda^*H) + \\\\lambda^* \\\\]\\n\\\\[ \\\\lambda^* = \\\\arg \\\\max_{\\\\lambda \\\\geq P} \\\\{ \\\\xi(\\\\lambda) \\\\} \\\\]\\n\\\\[ \\\\xi(\\\\lambda) = \\\\frac{1}{2}\\\\lambda^2 - \\\\frac{1}{2}A^2B + \\\\lambda^TB \\\\]\\notherwise. Where \\\\( q = gH - \\\\lambda^*g, r = gH - \\\\lambda^*b, \\\\) and \\\\( s = bH - \\\\lambda^*b \\\\).\\n\\nFurthermore, let \\\\( \\\\Lambda_0 \\\\) and \\\\( \\\\Lambda_1 \\\\) be defined by\\n\\\\[ \\\\Lambda_0 = \\\\{ \\\\lambda | \\\\lambda \\\\geq P, \\\\lambda_2 - A > P \\\\} \\\\]\\n\\\\[ \\\\Lambda_1 = \\\\{ \\\\lambda | \\\\lambda \\\\geq P, \\\\lambda_2 - A \\\\leq P \\\\} \\\\]\\nThe value of \\\\( \\\\lambda^* \\\\) satisfies\\n\\\\[ \\\\lambda^* \\\\in \\\\begin{cases} \\\\lambda^*_0 = \\\\text{proj}_{\\\\Lambda_0}(\\\\sqrt{\\\\frac{1}{2}B - \\\\frac{1}{2}A_2B - \\\\frac{1}{2}X^2}), & \\\\text{if } \\\\lambda^*_2 - A > P \\\\\\\\ \\\\lambda^*_1 = \\\\text{proj}_{\\\\Lambda_1}(\\\\sqrt{\\\\frac{1}{2}B + \\\\frac{1}{2}X}), & \\\\text{otherwise} \\\\end{cases} \\\\]\\nwhere \\\\( \\\\lambda^*_0 = \\\\lambda^* \\\\) if \\\\( \\\\xi(\\\\lambda^*_0) > \\\\xi(\\\\lambda^*_1) \\\\) and \\\\( \\\\lambda^*_0 = \\\\lambda^* \\\\) otherwise.\\n\\nProof. See Achiam et al. (2017) (Appendix 10.2).\"}"}
{"id": "BlyXYc4wF2-", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 2: MACPO\\n\\n1: Input: Stepsize $U$, batch size, number of agents, episodes, steps per episode, possible steps in line search.\\n\\n2: Initialize: Actor networks $\\\\{\\\\pi^8, \\\\forall \\\\pi^8 \\\\in \\\\mathbb{N}\\\\}$, Global V-value network $q^P$, Individual + cost networks $\\\\{q^8, \\\\pi^8\\\\}^{8=1,9=1,9=1}$, Replay buffer $B$.\\n\\n3: for $P = 1, \\\\ldots, -1$ do\\n\\n4: Collect a set of trajectories by running the joint policy $\\\\pi^{\\\\theta} = (c_1^1, \\\\ldots, c_9^9)$.\\n\\n5: Push transitions $\\\\{(c_8^C, 0, c^C_8 + 1, A^C)\\\\}$ into $B$.\\n\\n6: Sample a random minibatch of transitions from $B$.\\n\\n7: Compute advantage function $\\\\hat{\\\\delta}(s, a)$ based on global V-value network with GAE.\\n\\n8: Compute cost-advantage functions $\\\\hat{\\\\delta}^8(s, a^8)$ based on individual + cost critics with GAE.\\n\\n9: Draw a random permutation of agents $8^1, \\\\ldots, 8^n$.\\n\\n10: Set $\\\\delta(c_8^1, a^1) = \\\\hat{\\\\delta}(s, a)$.\\n\\n11: for agent $8 \\\\in \\\\{8^1, \\\\ldots, 8^n\\\\}$ do\\n\\n12: Estimate the gradient of the agent's maximisation objective $\\\\hat{g}_8^\\\\pi(s, a)$:\\n\\n$$\\\\hat{g}_8^\\\\pi(s, a) = \\\\sum_{1=1}^{N} \\\\sum_{C=1}^{P} \\\\nabla_{c_8^C} \\\\log c_8^C \\\\text{ } | \\\\text{ } c_8^C = c_8^C \\\\text{ } | \\\\text{ } \\\\{B_C, a_C\\\\}.$$  \\n\\n13: for $9 = 1, \\\\ldots, <8$ do\\n\\n14: Estimate the gradient of the agent's $9$th cost $\\\\hat{b}_8^9(s, a^9)$:\\n\\n$$\\\\hat{b}_8^9(s, a^9) = \\\\sum_{1=1}^{N} \\\\sum_{C=1}^{P} \\\\nabla_{c_8^C} \\\\log c_8^C \\\\text{ } | \\\\text{ } c_8^C = c_8^C \\\\text{ } | \\\\text{ } \\\\{B_C, a_C\\\\}.$$  \\n\\n15: end for\\n\\n16: Set $\\\\hat{B}_8^\\\\pi = [\\\\hat{b}_8^1, \\\\ldots, \\\\hat{b}_8^<8]$.\\n\\n17: Compute $\\\\hat{H}_8^\\\\pi$, the Hessian of the average KL-divergence $\\\\sum_{1=1}^{N} \\\\sum_{C=1}^{P} \\\\text{KL}(c_8^C|\\\\pi^\\\\theta, c_8^C|\\\\pi^\\\\pi)$.\\n\\n18: Solve the dual (5) for $\\\\_8^\\\\pi, v_8^\\\\pi$.\\n\\n19: Use the conjugate gradient algorithm to compute the update direction $x_8^\\\\pi$:\\n\\n$$x_8^\\\\pi = (\\\\hat{H}_8^\\\\pi)^{-1}(\\\\hat{g}_8^\\\\pi - \\\\hat{B}_8^\\\\pi v_8^\\\\pi),$$\\n\\n20: Update agent $8^\\\\pi$'s policy by $\\\\pi^\\\\pi + 1 = \\\\pi^\\\\pi + U_9_\\\\_8^\\\\pi \\\\_8^\\\\pi \\\\hat{x}_8^\\\\pi$, where $9 \\\\in \\\\{P, 1, \\\\ldots, !\\\\}$ is the smallest such $9$ which improves the sample loss, and satisfies the sample constraints, found by the backtracking line search.\\n\\n21: if the approximate is not feasible then\\n\\n22: Use equation (6) to recover policy $\\\\pi^{\\\\theta}$ from unfeasible points.\\n\\n23: end if\\n\\n24: end for\\n\\n25: Update V-value network by following formula:\\n\\n$$q^{\\\\pi} = \\\\arg \\\\min_{q^\\\\pi} \\\\sum_{1=1}^{N} \\\\sum_{C=1}^{P} (q^\\\\pi (B_C) - \\\\hat{C}^\\\\pi)^2,$$  \\n\\n26: end for\"}"}
{"id": "BlyXYc4wF2-", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022 where $U_{9}$ is adjusted through backtracking line search. To put it together, we refer to this algorithm as MACPO, and provide its pseudocode in Appendix D.\\n\\n4.3 MAPPO-L\\n\\nIn addition to MACPO, one can use Lagrangian multipliers in place of optimisation with linear and quadratic constraints to solve Equation (3). The Lagrangian method is simple to implement, and it does not require computations of the Hessian $H_{8}$ whose size grows quadratically with the dimension of the parameter vector $\\\\theta_{8}$.\\n\\nBefore we proceed, let us briefly recall the optimisation procedure with a Lagrangian multiplier. Suppose that our goal is to maximise a bounded real-valued function $G_{5}$ under a constraint $G_{6}$:\\n\\n$$\\\\max G_{5}(G), \\\\text{s.t. } G_{6}(G) \\\\leq P.$$  \\n\\n(7)\\n\\nSuppose that $G_{+}$ satisfies $G_{6}(G_{+}) > P$. This immediately implies that $-G_{6}(G_{+}) \\\\rightarrow -\\\\infty$, as $\\\\_ \\\\rightarrow +\\\\infty$, and so Equation (7) equals $-\\\\infty$ for $G = G_{+}$. On the other hand, if $G_{-}$ satisfies $G_{6}(G_{-}) \\\\leq P$, we have that $-G_{6}(G_{-}) \\\\geq P$, with equality only for $\\\\_ = P$. In that case, the optimisation objective's value equals $G_{5}(G_{-}) > -\\\\infty$. Hence, the only candidate solutions to the problem are those $G$ that satisfy the constraint $G_{6}(G) \\\\leq P$, and the objective matches with $G_{5}(G_{-})$.\\n\\nWe can employ the above trick to the constrained optimisation problem from Equation (3) by subsuming the cost constraints into the optimisation objective with Lagrangian multipliers. As such, agent $\\\\bar{8}_{h}$ computes $\\\\bar{\\\\_}_{8h}$ to solve the following min-max optimisation problem\\n\\n$$\\\\max_{\\\\bar{\\\\_}_{8h} \\\\geq P} \\\\min_{\\\\_ \\\\_ \\\\_} G_{5}(G_{-}) \\\\geq -\\\\infty,$$\\n\\n(8)\\n\\nAlthough the objective from Equation (8) is affine in the Lagrangian multipliers $\\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\_ \\\\"}
{"id": "BlyXYc4wF2-", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Example tasks in Safe Multi-Agent MuJoCo Environment. (a): Safe 2x4-Ant, (b): Safe 4x2-Ant, (c): Safe 2x3-HalfCheetah. Body parts of different colours are controlled by different agents. Agents jointly learn to manipulate the robot, while avoiding crashing into unsafe red areas.\\n\\nThe clip operator replaces the policy ratio with $1 - n$ or $1 + n$, depending on whether its value is below or above the threshold interval. As such, agent $\\\\ell$ can learn within its trust region by updating \\\\( \\\\theta_\\\\ell \\\\) to maximise Equation (10), while the Lagrangian multipliers are updated towards the direction opposite to their gradients of Equation (8), which can be computed analytically. We refer to this algorithm as MAPPO-Lagrangian, and give a detailed pseudocode of it in Appendix E.\\n\\n### 5 EXPERIMENTS\\n\\nAlthough MARL researchers have long had a variety of environments to test different algorithms, such as StarCraftII (Samvelyan et al., 2019) and Multi-Agent MuJoCo (Peng et al., 2020), no public safe MARL benchmark has been proposed; this impedes researchers from evaluating and bench-marking safety-aware multi-agent learning methods. As a key contribution of this paper, we introduce Safe Multi-Agent MuJoCo Benchmark, a safety-aware extension of the MuJoCo environment that is designed for safe MARL research. We show example tasks in Figure 1, in our environment, safety-aware agents have to learn not only skilful manipulations of a robot, but also to avoid crashing into unsafe obstacles and positions. For more details of the setup, please refer to Appendix F.\\n\\nWe use Safe MAMuJoCo to examine if the MACPO/MAPPO-Lagrangian agents can satisfy their safety constraints and cooperatively learn to achieve high rewards, compared to existing MARL algorithms. Notably, our proposed methods adopt two different approaches for achieving safety. MACPO reaches safety via hard constraints and backtracking line search, while MAPPO-Lagrangian maintains a rather soft safety awareness by performing gradient descents on the PPO-clip objective. Figure 2 shows cost and reward performance comparisons between MACPO, MAPPO-Lagrangian, MAPPO (Yu et al., 2021), IPPO (de Witt et al., 2020), and HAPPO (Kuba et al., 2021a) algorithms on three challenging tasks. Figure 2 should be interpreted at three-folds; each subfigure represents a different robot, within each subfigure, three task setups in terms of multi-agent control are considered, for each task, we plot the cost curves (the lower the better) in the upper row, and plot the reward curves (the higher the better) in the bottom row. Detailed hyperparameter settings are described in Appendix H.\\n\\nThe experiments reveal that both MACPO and MAPPO-Lagrangian quickly learn to satisfy safety constraints, and keep their explorations within the feasible policy space. This stands in contrast to IPPO, MAPPO, and HAPPO which largely violate the constraints thus being unsafe. Furthermore, our algorithms achieve comparable reward scores; both methods are often better than IPPO. In general, the performance (in terms of reward) of MAPPO-Lagrangian is better than of MACPO; moreover, MAPPO-Lagrangian outperforms the unconstrained MAPPO on challenging Ant tasks.\\n\\nWe note that on none of the tasks the reward of HAPPO was exceeded though it is unsafe.\\n\\n### 6 CONCLUSION\\n\\nIn this paper, we tackled multi-agent policy optimisation problems with safety constraints. Central to our findings is the safe multi-agent policy iteration procedure that attains theoretically-justified monotonic improvement guarantee and constraints satisfaction guarantee at every iteration during learning. Based on this, we proposed two practical algorithms: MACPO and MAPPO-Lagrangian. To demonstrate their effectiveness, we introduced a new benchmark suite of Safe Multi-Agent MuJoCo and compared our methods against strong MARL baselines. Results show that both of our methods can significantly outperform existing state-of-the-art methods such as IPPO, MAPPO and HAPPO in terms of safety, meanwhile maintaining comparable performance in terms of reward.\"}"}
{"id": "BlyXYc4wF2-", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2: Performance comparisons on tasks of Safe ManyAgent Ant, Safe Ant, and Safe HalfCheetah in terms of cost (the first row) and reward (the second row). The safety constraint values are: 1 for ManyAgent Ant, \\\\( P \\\\cdot 2 \\\\) for Ant, and \\\\( 5 \\\\) for HalfCheetah. Our methods consistently achieve almost zero costs, thus satisfying safe constraints, on all tasks. In terms of reward, our methods outperform IPPO and MAPPO on some tasks but underperform HAPPO, which is also an unsafe algorithm.\"}"}
{"id": "BlyXYc4wF2-", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"REFERENCES\\n\\nJoshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In International Conference on Machine Learning, pp. 22\u201331. PMLR, 2017.\\n\\nEitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.\\n\\nAaron D Ames, Xiangru Xu, Jessy W Grizzle, and Paulo Tabuada. Control barrier function based quadratic programs for safety critical systems. IEEE Transactions on Automatic Control, 62(8):3861\u20133876, 2016.\\n\\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.\\n\\nUrs Borrmann, Li Wang, Aaron D Ames, and Magnus Egerstedt. Control barrier certificates for safe swarm behavior. IFAC-PapersOnLine, 48(27):68\u201373, 2015.\\n\\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.\\n\\nYinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforcement learning with percentile risk criteria. The Journal of Machine Learning Research, 18(1):6070\u20136120, 2017.\\n\\nYinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A lyapunov-based approach to safe reinforcement learning. arXiv preprint arXiv:1805.07708, 2018.\\n\\nYinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. Lyapunov-based safe policy optimization for continuous control. arXiv preprint arXiv:1901.10031, 2019.\\n\\nChristian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020.\\n\\nXiaotie Deng, Yuhao Li, David Henry Mguni, Jun Wang, and Yaodong Yang. On the complexity of computing markov perfect equilibrium in general-sum stochastic games. arXiv preprint arXiv:2109.01795, 2021.\\n\\nJakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\\n\\nJavier Garc\u00eda and Fernando Fern\u00e1ndez. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1):1437\u20131480, 2015.\\n\\nJakub Grudzien Kuba, Ruiqing Chen, Munning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. arXiv preprint arXiv:2109.11251, 2021a.\\n\\nJakub Grudzien Kuba, Muning Wen, Yaodong Yang, Linghui Meng, Shangding Gu, Haifeng Zhang, David Henry Mguni, and Jun Wang. Settling the variance of multi-agent policy gradients. arXiv preprint arXiv:2108.08612, 2021b.\\n\\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\\n\\nSongtao Lu, Kaiqing Zhang, Tianyi Chen, Tamer Basar, and Lior Horesh. Decentralized policy gradient descent ascent for safe multi-agent reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 8767\u20138775, 2021.\\n\\nTeodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes. In Proceedings of the 29th International Conference on International Conference on Machine Learning, pp. 1451\u20131458, 2012.\"}"}
{"id": "BlyXYc4wF2-", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"sequential policy update scheme so that the expected joint advantage will always be positive, thus increasing reward. In this work, we show how to further develop this theory and derive a protocol which, in addition to the monotonic improvement, also guarantees to satisfy the safety constraint at every iteration during learning. The resulting algorithm (Algorithm 1) successfully attains theoretical guarantees of both monotonic improvement in reward and satisfaction of safety constraints.\\n\\n3 Problem Formulation\\n\\nWe formulate the safe MARL problem as a constrained Markov game \\\\( \\\\langle N, S, A, p, d, P, W, C, c \\\\rangle \\\\).\\n\\nHere, \\\\( N = \\\\{1, \\\\ldots, N\\\\} \\\\) is the set of agents, \\\\( S \\\\) is the state space, \\\\( A = \\\\prod_{i=1}^{N} A_i \\\\) is the product of the agents' action spaces, known as the joint action space, \\\\( p : S \\\\times A \\\\times S \\\\rightarrow \\\\mathbb{R} \\\\) is the probabilistic transition function, \\\\( d \\\\) is the initial state distribution, \\\\( W \\\\in [0, 1) \\\\) is the discount factor, \\\\( C = \\\\{S \\\\times A \\\\rightarrow \\\\mathbb{R}, \\\\forall \\\\} \\\\) is the joint reward function, \\\\( C = \\\\{S \\\\times A \\\\rightarrow \\\\mathbb{R}, \\\\forall \\\\} \\\\) is the set of sets of cost functions (every agent \\\\( i \\\\) has \\\\( \\\\# \\\\) cost functions) of the form \\\\( C_i : S \\\\times A \\\\rightarrow \\\\mathbb{R} \\\\), and finally the set of corresponding cost-constraining values is given by \\\\( c = \\\\{s \\\\in N, \\\\# \\\\} \\\\). At time step \\\\( C \\\\), the agents are in a state \\\\( s_C \\\\), and every agent \\\\( i \\\\) takes an action \\\\( a_i_C \\\\) according to its policy \\\\( c_{i} (a_i | s_C) \\\\). Together with other agents' actions, it gives a joint action \\\\( a_C = (a_1C, \\\\ldots, a_NC) \\\\) and the joint policy \\\\( \\\\pi (a | s) = \\\\prod_{i=1}^{N} c_i (a_i | s) \\\\). The agents receive the reward \\\\( \\\\gamma (s_C, a_C) \\\\), meanwhile each agent \\\\( i \\\\) pays the costs \\\\( C_i (s_C, a_i C) \\\\), \\\\( \\\\forall \\\\). The environment then transits to a new state \\\\( s_{C+1} \\\\sim p(\\\\cdot | s_C, a_C) \\\\).\\n\\nIn this paper, we consider a fully-cooperative setting where all agents share the same reward function, aiming to maximise the expected total reward of \\\\( (\\\\pi) \\\\approx \\\\sum_{P} dP \\\\pi_1 \\\\sum_{A} \\\\pi_2 \\\\sum_{P} W \\\\gamma (s_C, a_C) \\\\), meanwhile trying to satisfy every agent \\\\( i \\\\)'s safety constraints, written as \\\\( (\\\\pi) \\\\approx \\\\sum_{P} dP \\\\pi_1 \\\\sum_{A} \\\\pi_2 \\\\sum_{P} W \\\\gamma (s_C, a_i C) \\\\leq c_i, \\\\forall \\\\).\\n\\nWe define the state-action value and the state-value functions in terms of reward as \\\\( \\\\pi (B, a) \\\\approx \\\\sum_{P} dP \\\\sum_{A} \\\\pi \\\\sum_{P} W \\\\gamma (s_C, a_C) \\\\), and \\\\( \\\\pi (B) \\\\approx \\\\sum_{A} \\\\pi \\\\sum_{P} W \\\\gamma (s_C, a_C) \\\\).\\n\\nThe joint policies \\\\( \\\\pi \\\\) that satisfy the Inequality (1) are referred to as feasible. Notably, in the above formulation, although the action \\\\( a_i C \\\\) of agent \\\\( i \\\\) does not directly influence the costs \\\\( \\\\{C_i(s_C, a_i C)\\\\} \\\\) of other agents \\\\( \\\\neq i \\\\), the action \\\\( a_i C \\\\) will implicitly influence their total costs due to the dependence on the next state \\\\( s_{C+1} \\\\).\\n\\nFor the \\\\( \\\\# \\\\)th cost function of agent \\\\( i \\\\), we define the \\\\( \\\\# \\\\)th state-action cost value function and the state cost value function as \\\\( \\\\pi (B, 0 \\\\#) \\\\approx \\\\sum_{P} dP \\\\sum_{A} \\\\pi \\\\sum_{P} W \\\\gamma (s_C, a_i C) \\\\), and \\\\( \\\\pi (B) \\\\approx \\\\sum_{A} \\\\pi \\\\sum_{P} W \\\\gamma (s_C, a_i C) \\\\), respectively.\\n\\nNotably, the cost value functions \\\\( \\\\pi \\\\) and \\\\( \\\\pi \\\\), although similar to traditional \\\\( \\\\pi \\\\) and \\\\( \\\\pi \\\\), involve extra indices \\\\( i \\\\) and \\\\( \\\\# \\\\); the superscript \\\\( i \\\\) denotes an agent, and the subscript \\\\( \\\\# \\\\) denotes its \\\\( \\\\# \\\\)th cost.\\n\\nThroughout this work, we pay a close attention to the contribution to performance from different subsets of agents, therefore, we introduce the following notations. We denote an arbitrary subset \\\\( 2 \\\\). We believe that this formulation realistically describes multi-agent interactions in the real-world; an action of an agent has an instantaneous effect on the system only locally, but the rest of agents may suffer from its consequences at later stages. For example, consider a car that crosses on the red light, although other cars may not be at risk of riding into pedestrians immediately, the induced traffic may cause hazards soon later.\"}"}
{"id": "BlyXYc4wF2-", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\n\\\\[ \\\\{8, \\\\ldots, 8\\\\} \\\\]\\n\\nof agents as \\\\(8^1: \\\\cdots: \\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\) \\\\(\\\\"}
{"id": "BlyXYc4wF2-", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"By generalising the result about the surrogate return in Equation (1), we can derive how the expected costs change when the agents update their policies. Specifically, we provide the following lemma.\\n\\nLemma 2. Let $\\\\pi$ and $\\\\bar{\\\\pi}$ be joint policies. Let $8 \\\\in \\\\mathbb{N}$ be an agent, and $9 \\\\in \\\\{1, \\\\ldots, <8\\\\}$ be an index of one of its costs. The following inequality holds:\\n\\n$$8_9(\\\\bar{\\\\pi}) \\\\leq 8_9(\\\\pi) + \\\\frac{a_{8_9}}{\\\\max_{B,0} KL(c_{8_1}, c_{8_0})} = \\\\frac{1}{\\\\max_{B,0} KL(c_{8_1}, c_{8_0})},$$\\n\\nwhere $a_{8_9} = 4 \\\\frac{\\\\max_{B,0} KL(c_{B_1}, c_{B_0})}{1 - W_{8_9}^2}$.\\n\\nSee proof in Appendix A. The above lemma suggests that, as long as the distances between the policies $\\\\pi_{\\\\text{c}_{\\\\text{h}}}$ and $\\\\bar{\\\\pi}_{\\\\text{c}_{\\\\text{h}}}$, $\\\\forall \\\\text{h} \\\\in \\\\mathbb{N}$, are sufficiently small, then the change in the $9$th cost of agent $8$, i.e., $8_9(\\\\bar{\\\\pi}) - 8_9(\\\\pi)$, is controlled by the surrogate $\\\\frac{a_{8_9}}{\\\\max_{B,0} KL(c_{8_1}, c_{8_0})}$. Importantly, this surrogate is independent of other agents' new policies. Hence, when the changes in policies of all agents are sufficiently small, each agent $8$ can learn a better policy $\\\\bar{\\\\pi}_{\\\\text{c}_{\\\\text{h}}}$ by only considering its own surrogate return and surrogate costs. To summarise, we provide the following algorithm that guarantees both safety constraints satisfaction and monotonic performance improvement.\\n\\nAlgorithm 1: Safe Multi-Agent Policy Iteration with Monotonic Improvement Property\\n\\n1: Initialise a safe joint policy $\\\\pi \\\\in \\\\{c_1, \\\\ldots, c_8\\\\}$.\\n\\n2: for $\\\\pi \\\\in \\\\{c_1, \\\\ldots, c_8\\\\}$ do\\n\\n3: Compute the advantage functions $\\\\pi(c_B)$ and $\\\\pi(c_{B,0})$ for all state-(joint)action pairs $(c_B)$, agents, and constraints $9 \\\\in \\\\{1, \\\\ldots, <8\\\\}$.\\n\\n4: Compute $a = 4 \\\\frac{\\\\max_{B,0} KL(c_{B_1}, c_{B_0})}{1 - W_{8_9}^2}$, and $a_{8_9} = 4 \\\\frac{\\\\max_{B,0} KL(c_{B_1}, c_{B_0})}{1 - W_{8_9}^2}$, $\\\\forall 8 \\\\in \\\\mathbb{N}, 9 = 1, \\\\ldots, <8$.\\n\\n5: Draw a permutation $8_1$ of agents at random.\\n\\n6: for $\\\\pi \\\\in \\\\{c_1, \\\\ldots, c_8\\\\}$ do\\n\\n7: Compute the radius of the KL-constraint $X_{\\\\text{c}_{\\\\text{h}}}$, see Appendix B for the setup of $X_{\\\\text{c}_{\\\\text{h}}}$.\\n\\n8: Make an update $c_{\\\\text{c}_{\\\\text{h}}} + 1 = \\\\arg \\\\max_{c_{\\\\text{c}_{\\\\text{h}}}} c_{\\\\text{c}_{\\\\text{h}}} \\\\in \\\\Pi_{\\\\text{c}_{\\\\text{h}}}\\\\left[\\\\pi_{\\\\text{c}_{\\\\text{h}}}, \\\\pi_{\\\\text{c}_{\\\\text{h}}} + 1\\\\right] = \\\\frac{a_{8_9}}{\\\\max_{B,0} KL(c_{8_1}, c_{8_0})} - \\\\frac{1}{\\\\max_{B,0} KL(c_{8_1}, c_{8_0})}$, where $\\\\Pi_{\\\\text{c}_{\\\\text{h}}}$ is a subset of safe policies of agent $8$ given by $\\\\Pi_{\\\\text{c}_{\\\\text{h}}} = \\\\{c_{\\\\text{c}_{\\\\text{h}}} \\\\in \\\\Pi_{\\\\text{c}_{\\\\text{h}}} : KL(c_{\\\\text{c}_{\\\\text{h}}}, c_{\\\\text{c}_{\\\\text{h}}}) \\\\leq X_{\\\\text{c}_{\\\\text{h}}}, \\\\text{and } 8_{9}(\\\\pi) + \\\\frac{a_{8_9}}{\\\\max_{B,0} KL(c_{8_1}, c_{8_0})} \\\\leq 2\\\\frac{8_9}{8_9 - 1} \\\\frac{1}{\\\\max_{B,0} KL(c_{8_1}, c_{8_0})}\\\\}$.\\n\\n9: end for\\n\\n10: end for\\n\\nIn the above algorithm, in addition to sequentially maximising agents' surrogate returns, the agents must assure that their surrogate costs stay below the corresponding safety thresholds. Meanwhile, they have to constrain their policy search to small local neighbourhoods (w.r.t max-KL distance). As such, Algorithm 1 demonstrates two desirable properties: reward performance improvement and satisfaction of safety constraints, which we justify in the following theorem.\\n\\nTheorem 1. If a sequence of joint policies $(\\\\pi : \\\\pi)$ is obtained from Algorithm 1, then it has the monotonic improvement property, $(\\\\pi : \\\\pi + 1) \\\\geq (\\\\pi : \\\\pi)$, as well as it satisfies the safety constraints, $8_9(\\\\pi) \\\\leq \\\\frac{1}{\\\\max_{B,0} KL(c_{8_1}, c_{8_0})}$, for all $\\\\pi \\\\in \\\\mathbb{N}, 8 \\\\in \\\\mathbb{N},$ and $9 = 1, \\\\ldots, <8$.\\n\\nSee proof in Appendix B. The above theorem assures that agents that follow Algorithm 1 will only explore safe policies; meanwhile, every new policy will be guaranteed to result in performance improvement. These two properties hold under the conditions that only restrictive policy updates are made; this is due to the KL-penalty term in every agent's objective (i.e., $a_{8_9}$), as well as the constraints on cost surrogates (i.e., the conditions in $\\\\Pi_{\\\\text{c}_{\\\\text{h}}}$). In practice, it can be intractable to evaluate $KL(c_{8_1}, c_{8_0})$ at every state in order to compute $\\\\max_{B,0} KL(c_{8_1}, c_{8_0})$. In the following subsections, we describe how we can approximate Algorithm 1 in the case of parameterised policies, similar to TRPO/PPO implementations (Schulman et al., 2015; 2017).\"}"}
{"id": "BlyXYc4wF2-", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 MACPO: Multi-Agent Constrained Policy Optimisation\\n\\nHere we focus on the practical settings where large state and action spaces prevent agents from designing policies \\\\(c_8(\u00b7|B)\\\\) for each state separately. To handle this, we parameterise each agent\u2019s policy by a neural network \\\\(\\\\pi_\\\\theta\\\\). Correspondingly, the joint policies \\\\(\\\\pi_\\\\theta\\\\) are parametrised by \\\\(\\\\theta = (\\\\theta_1, \\\\ldots, \\\\theta_{\\\\ell})\\\\).\\n\\nLet\u2019s recall that at every iteration of Algorithm 1, every agent maximises its surrogate return with a KL-penalty, subject to surrogate cost constraint. Yet, direct computation of the max-KL constraint is intractable in practical settings, as it would require computation of KL-divergence at every single state. Instead, one can relax it by adopting a form of expected KL-constraint\\n\\n\\\\[\\n\\\\text{KL}(c_8:\\\\pi_\\\\theta; c_{\\\\ell} categorised) \\\\leq X\\n\\\\]\\n\\nwhere\\n\\n\\\\[\\n\\\\text{KL}(c_8:\\\\pi_\\\\theta; c_{\\\\ell} categorised) \\\\equiv s \\\\sim d_{\\\\pi_\\\\theta} \\\\left[ \\\\text{KL}(c_{\\\\ell} categorised: \\\\cdot|s, c_{\\\\ell} categorised) \\\\right].\\n\\\\]\\n\\nSuch an expectation can be approximated by stochastic sampling. As a result, the optimisation problem solved by agent \\\\(8\\\\ell\\\\) is written as\\n\\n\\\\[\\n\\\\theta_{\\\\ell} \\\\leftarrow \\\\arg \\\\max_{\\\\theta_{\\\\ell}} E_s \\\\left[ g_{\\\\ell} c_{\\\\ell} categorised + \\\\left( b_{\\\\ell} c_{\\\\ell} categorised \\\\right)^T \\\\left( H_{\\\\ell} c_{\\\\ell} categorised \\\\right)^{-1} b_{\\\\ell} c_{\\\\ell} categorised \\\\right] + \\\\left( \\\\nabla_{\\\\theta_{\\\\ell}} \\\\text{KL}(c_{\\\\ell} categorised: \\\\cdot|s, c_{\\\\ell} categorised) \\\\right)^T \\\\theta_{\\\\ell},\\n\\\\]\\n\\nwhere\\n\\n\\\\[\\n\\\\nabla_{\\\\theta_{\\\\ell}} \\\\text{KL}(c_{\\\\ell} categorised: \\\\cdot|s, c_{\\\\ell} categorised) \\\\equiv s \\\\sim d_{\\\\pi_\\\\theta} \\\\left[ \\\\nabla_{\\\\theta_{\\\\ell}} \\\\text{KL}(c_{\\\\ell} categorised: \\\\cdot|s, c_{\\\\ell} categorised) \\\\right].\\n\\\\]\\n\\nWe can further approximate Equation (3) by Taylor expansion of the optimisation objective and cost constraints up to the first order, and the KL-divergence up to the second order. Consequently, the optimisation problem can be written as\\n\\n\\\\[\\n\\\\theta_{\\\\ell} \\\\leftarrow \\\\arg \\\\max_{\\\\theta_{\\\\ell}} E_s \\\\left[ g_{\\\\ell} c_{\\\\ell} categorised + \\\\left( b_{\\\\ell} c_{\\\\ell} categorised \\\\right)^T \\\\left( H_{\\\\ell} c_{\\\\ell} categorised \\\\right)^{-1} b_{\\\\ell} c_{\\\\ell} categorised + \\\\left( \\\\nabla_{\\\\theta_{\\\\ell}} \\\\text{KL}(c_{\\\\ell} categorised: \\\\cdot|s, c_{\\\\ell} categorised) \\\\right)^T \\\\theta_{\\\\ell} \\\\right],\\n\\\\]\\n\\nand\\n\\n\\\\[\\n\\\\text{KL}(c_{\\\\ell} categorised: \\\\cdot|s, c_{\\\\ell} categorised) \\\\leq X.\\n\\\\]\\n\\nIn practice, we use backtracking line search starting at \\\\(1/\\\\theta_{\\\\ell}^*\\\\) to choose the step size of the above update. Furthermore, we note that the optimisation step in Equation (4) is an approximation to the original problem from Equation (3); therefore, it is possible that an infeasible policy \\\\(c_{\\\\ell} categorised + 1\\\\) will be generated. Fortunately, as the policy optimisation takes place in the trust region of \\\\(c_{\\\\ell} categorised\\\\), the size of update is small, and a feasible policy can be easily recovered. In particular, for problems with one safety constraint, i.e., \\\\(\\\\ell = 1\\\\), one can recover a feasible policy by applying a TRPO step on the cost surrogate, written as\\n\\n\\\\[\\n\\\\theta_{\\\\ell} \\\\leftarrow \\\\theta_{\\\\ell} - U_9 \\\\sqrt{2X b_{\\\\ell} c_{\\\\ell} categorised (H_{\\\\ell} c_{\\\\ell} categorised)^{-1} b_{\\\\ell} c_{\\\\ell} categorised}.\\n\\\\]\"}"}
