{"id": "gqtbL7j2JW", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep generative models have achieved promising results in image generation, and various generative model hubs, e.g., Hugging Face and Civitai, have been developed that enable model developers to upload models and users to download models. However, these model hubs lack advanced model management and identification mechanisms, resulting in users only searching for models through text matching, download sorting, etc., making it difficult to efficiently find the model that best meets user requirements. In this paper, we propose a novel setting called Generative Model Identification (GMI), which aims to enable the user to identify the most appropriate generative model(s) for the user's requirements from a large number of candidate models efficiently. To our best knowledge, it has not been studied yet. In this paper, we introduce a comprehensive solution consisting of three pivotal modules: a weighted Reduced Kernel Mean Embedding (RKME) framework for capturing the generated image distribution and the relationship between images and prompts, a pre-trained vision-language model aimed at addressing dimensionality challenges, and an image interrogator designed to tackle cross-modality issues. Extensive empirical results demonstrate the proposal is both efficient and effective. For example, users only need to submit a single example image to describe their requirements, and the model platform can achieve an average top-4 identification accuracy of more than 80%. The code and benchmark are all released to promote the research.\"}"}
{"id": "gqtbL7j2JW", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Comparison between traditional generative model search of existing model hubs and GMI.\\n\\nGMI matches requirements and specifications during the identification process. A detailed explanation is presented in B.\\n\\nIt is evident that two problems need to be addressed to achieve GMI, the first is how to describe the functionalities of different generative models, and the second is how to match the user requirements with the models' functionalities. Inspired by the learnware paradigm (Zhou, 2016), which proposes to assign a specification to each model that reflects the model's utilities, we adopt the Reduced Kernel Mean Embedding (RKME) as the model specification to capture the distribution of generated images produced by different generative models, since the generated image distribution could reflect the model functionality. However, previous RKME studies mainly focus on classification tasks, and cannot be directly applied to generative models. To this end, we propose a novel systematic solution consisting of three pivotal modules: a weighted Reduced Kernel Mean Embedding (RKME) framework for capturing not only the generated image distribution but also the relationship between images and prompts, a pre-trained vision-language model aimed at addressing dimensionality challenges, and an image interrogator designed to tackle cross-modality issues. For the second problem, we assume the user can present one image as an example to describe the requirements, and then we can match the model specification with the example image to compute how well each candidate generative model matches users' requirements. Figure 1 provides a comparison between previous model search methods and the new solution. The goal is to identify the most suitable generative model with only one single image as an example to describe the user's requirements.\\n\\nTo evaluate the effectiveness of our proposal, we construct a benchmark platform consisting of 16 tasks specifically designed for GMI using stable diffusion models. The experiment results show that our proposal is both efficient and effective. For example, users only need to submit a single example image to describe their requirements, and the model platform can achieve an average top-4 identification accuracy of more than 80%, indicating that recommending four models can satisfy users in major cases on the benchmark dataset.\\n\\n2 PROBLEM AND ANALYSIS\\n\\nIn this section, we first describe the notation and formulation of GMI. Then, we theoretically discuss the obstacles existing methods face in generative models. Finally, we propose an advanced formulation motivated by our analysis.\\n\\n2.1 PROBLEM SETUP\\n\\nIn this paper, we explore a novel problem setting called GMI, where users identify the most appropriate generative models for their specific purposes using just one single image. We assume there...\"}"}
{"id": "gqtbL7j2JW", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"Table 7: Performance when developers provide specific prompts\\n\\n|                  | Top-1 Acc. | Top-2 Acc. | Top-3 Acc. | Top-4 Acc. |\\n|------------------|------------|------------|------------|------------|\\n| Default Prompt Set | 0.455      | 0.614      | 0.734      | 0.812      |\\n| Developer Provided Prompt | 0.777      | 0.937      | 0.985      | 0.987      |\\n\\n(a) Download\\n\\n(b) RKME\\n\\n(c) RKME-CLIP\\n\\n(d) Proposal\\n\\nFigure 6: The confusion matrix of each method. The results show that our proposal performs the best. The baseline method, e.g., Download and RKME, cannot work in our GMI problem.\\n\\nIn Figure 1, we compare the difference between GMI setting and traditional model search process. In traditional searches for generative models, the models are initially filtered by the model hubs using tags and then sorted based on download volume. However, this approach presents challenges for users in identifying their desired models, as the model with the highest download volume may not necessarily align with their specific target. Consequently, users select models based on their own judgement, and then download and evaluate each model individually. This consumes a significant amount of network resources and human effort, and there is also the possibility of filtering out the potentially best model due to judgment errors.\\n\\nIn our GMI setting, each model is directly sorted based on its matching score. This means that the most suitable model is recommended at the top, enabling users to effortlessly find their desired models with minimum effort.\"}"}
{"id": "gqtbL7j2JW", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 7: The accuracy with varying values of $\\\\gamma$ was evaluated. The results demonstrate that our proposal is robust to slight changes in the value of $\\\\gamma$.\\n\\n**LIMITATIONS**\\n\\nIn this section, we will discuss the limitations of our paper, which encompass two main aspects. First, the time complexity of our work has the potential for improvement. At present, our time complexity increases linearly with the number of models in the model platform, which will not become a heavy burden when the number of models is large. Previous research (Guo et al., 2023) has demonstrated that acceleration techniques can enhance the identification process, such as the use of a vector database. This could be an area for future exploration.\\n\\nSecond, we currently only consider the cases that identify generative models using one uploaded image to describe users' requirements. The assumption is reasonable since users' ideas often rely on existing image templates when they want to generate images, and it is not difficult to find only one image that has a similar style to fulfill the user's requirements. Despite this, it is also interesting to study how to quickly and accurately identify models via other information such as textual prompts. We will study this problem in future work.\"}"}
{"id": "gqtbL7j2JW", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nGaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen Tu. Dual contradistinctive generative autoencoder. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 823\u2013832, 2021.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, pp. 8748\u20138763, 2021.\\n\\nYong Ren, Jun Zhu, Jialian Li, and Yucen Luo. Conditional generative moment-matching networks. In Advances in Neural Information Processing Systems, pp. 2928\u20132936, 2016.\\n\\nDanilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Proceedings of the 32nd International Conference on Machine Learning, pp. 1530\u20131538, 2015.\\n\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.\\n\\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugging-GPT: Solving AI tasks with ChatGPT and its friends in HuggingFace. CoRR, abs/2303.17580, 2023.\\n\\nJascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, pp. 2256\u20132265, 2015.\\n\\nKihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In Advances in Neural Information Processing Systems, pp. 3483\u20133491, 2015.\\n\\nPeng Tan, Zhi-Hao Tan, Yuan Jiang, and Zhi-Hua Zhou. Towards enabling learnware to handle heterogeneous feature spaces. Machine Learning, pp. 1\u201322, 2022.\\n\\nPeng Tan, Zhi-Hao Tan, Yuan Jiang, and Zhi-Hua Zhou. Handling learnwares developed from heterogeneous feature spaces without auxiliary data. In Proceedings of the 32nd International Joint Conference on Artificial Intelligence, pp. 4235\u20134243, 2023.\\n\\nAnh T Tran, Cuong V Nguyen, and Tal Hassner. Transferability and hardness of supervised classification tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1395\u20131405, 2019.\\n\\nArash Vahdat and Jan Kautz. NV AE: A deep hierarchical variational autoencoder. In Advances in Neural Information Processing Systems, pp. 19667\u201319679, 2020.\\n\\nAaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In Advances in Neural Information Processing Systems, pp. 6306\u20136315, 2017.\\n\\nYasi Wang, Hongxun Yao, and Sicheng Zhao. Auto-encoder based dimensionality reduction. Neurocomputing, 184:232\u2013242, 2016.\\n\\nXi-Zhu Wu, Wenkai Xu, Song Liu, and Zhi-Hua Zhou. Model reuse with reduced kernel mean embedding specification. IEEE Transactions on Knowledge and Data Engineering, 35(1):699\u2013710, 2023.\\n\\nLei Xu, Adam Krzyzak, and Alan L. Yuille. On radial basis function nets and kernel regression: Statistical consistency, convergence rates, and receptive field size. Neural Networks, 7(4):609\u2013628, 1994.\\n\\nKaichao You, Yong Liu, Jianmin Wang, and Mingsheng Long. LogMe: Practical assessment of pre-trained models for transfer learning. In Proceedings of 38th International Conference on Machine Learning, pp. 12133\u201312143, 2021.\\n\\nZhi-Hua Zhou. Learnware: On the future of machine learning. Frontiers of Computer Science, 10(4):589\u2013590, 2016.\"}"}
{"id": "gqtbL7j2JW", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we present the additional visualization in Figure 5. Each image is generated using prompts obtained from image interrogator and models identified by corresponding methods. The results show that our proposal can identify the best-matched model and generate the most similar images, compared to baseline methods. For all examples, the Download method is biased towards the DreamShaper model, while the RKME-basic method is biased towards the RevAnimated model. Our proposed methods successfully identify the most suitable model in examples 1 and 4, while the other RKME-Clip method fails to find the best model. As a result, our proposal yields superior generated results compared to the results of other methods. For example 2, our proposed method identified a model that can generate more cartoonish images. However, the models identified by other methods all generate more realistic images. For example 3, all methods did not find the most suitable model, but the model identified by our method was not inferior to other methods. Detailed results about ground-truth prompts, generated prompts, ground-truth model and identified models are shown in Table 3.\"}"}
{"id": "gqtbL7j2JW", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: The prompts generated using the image interrogator, the corresponding ground-truth\\nprompts, the ground-truth model, and the identified model using different methods for each example.\\n\\n| Example 1 | Ground-truth Prompts | Generated Prompts | Ground-truth Model | Identified Model |\\n|-----------|----------------------|-------------------|-------------------|-----------------|\\n| Example 2 | Ground-truth Prompts | Generated Prompts | Ground-truth Model | Identified Model |\\n| Example 3 | Ground-truth Prompts | Generated Prompts | Ground-truth Model | Identified Model |\\n| Example 4 | Ground-truth Prompts | Generated Prompts | Ground-truth Model | Identified Model |\\n\\nTable 4: Performance when users upload multiple images as their requirements\\n\\n| Top-1 Acc. | Top-2 Acc. | Top-3 Acc. | Top-4 Acc. |\\n|-----------|-----------|-----------|-----------|\\n| 1 image   | 0.455     | 0.614     | 0.734     | 0.812     |\\n| 2 images  | 0.578     | 0.743     | 0.840     | 0.908     |\\n| 3 images  | 0.658     | 0.791     | 0.883     | 0.950     |\\n| 4 images  | 0.709     | 0.854     | 0.929     | 0.969     |\\n| 5 images  | 0.755     | 0.873     | 0.946     | 0.979     |\\n\\nHere, Image interrogator is implemented with official code\\n\\n3\\n\\nfor optimizing text prompts to match a\\ngiven image. It first generates candidate captions using the BLIP model (Li et al., 2022). Then, it\\nadopts a search process to identify the caption list that maximizes the similarity between the captions\\nand images, evaluated by the CLIP model (Radford et al., 2021).\\n\\nA.2 MULTIPLE IMAGES FOR QUERYING\\n\\nRKME paradigm performs more effectively when the provided image set accurately represents the\\ndistribution. Thus, our framework, built upon RKME, naturally accommodates situations in which\\nusers provide multiple images as queries. We conducted experiments using user queries consisting\\nof 1, 2, 3, 4, or 5 images. In this case, the performance comparison is a bit unfair as lines 2 to\\n5 involve more images for querying. The results are shown in Table 4. The results demonstrate\\nthat the accuracy of identifying the most appropriate model improves as the number of uploaded\\nimages increases. Therefore, our framework has the capability to handle multiple images as a query.\\nHowever, uploading a single image offers a compromise between performance and convenience.\\n\\nA.3 DIFFERENT SELECTION OF DEFAULT PROMPT SETS\\n\\nThis section involves conducting experiments to assess the impact of different default prompt sets\\non our performance. We denote Split 1 as our default prompt set used in our experiments. Split 2\\nand Split 3 are two non-overlapping subsets, each half the size of Split 1. The results demonstrate\\nthe robustness of the identifying performance to varying default prompt sets used in generating\\nspecifications.\"}"}
{"id": "gqtbL7j2JW", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Performance using different default prompt sets\\n\\n| Split | Top-1 Acc. | Top-2 Acc. | Top-3 Acc. | Top-4 Acc. |\\n|-------|-----------|-----------|-----------|-----------|\\n| Split 1 | 0.455     | 0.614     | 0.734     | 0.812     |\\n| Split 2 | 0.442     | 0.609     | 0.729     | 0.810     |\\n| Split 3 | 0.460     | 0.617     | 0.732     | 0.808     |\\n\\nTable 6: Performance using an extreme small reduced set size\\n\\n| Size | Top-1 Acc. | Top-2 Acc. | Top-3 Acc. | Top-4 Acc. |\\n|------|-----------|-----------|-----------|-----------|\\n| Size = 1 | 0.434     | 0.592     | 0.707     | 0.789     |\\n| Proposal | 0.455     | 0.614     | 0.734     | 0.812     |\\n\\nWe conduct an extreme experiment with reduced set size = 1 as the size will affect the running time for each query. Compared to proposed methods, the performance is relatively stable in Table 6. Therefore, the scalability of the method is guaranteed when the number of generative models increases. As discussed in Section 3.3, scalability problems can be handled using other techniques.\\n\\nWe claim that if the developers and users provide their own prompts, the identification results can be more accurate. The detailed explanations and instructions are given as follows.\\n\\nUsers should provide prompts that accurately describe the images they upload. In our proposed method, we generate prompts by employing an image interrogator that demonstrated effective performance in our experiments. When ground-truth prompts are provided, the identification performance will intuitively not be worse.\\n\\nDevelopers should provide prompts to guide the model in generating images it excels at. A model-specific prompt set will enhance identification by amplifying the contrast between generated specifications, resulting in improved accuracy. We conducted synthetic experiments to validate this claim. In this experiment, developers will provide 5 prompts for each model, assuming they excel at them, and generate corresponding specifications. The user will query images generated using similar prompts. The results in Table 7 demonstrate that providing prompts for specification generation leads to improved accuracy. However, in actual situations, it is difficult for the model hub to force developers to provide prompts. Therefore, we conduct experiments under a default prompt set.\\n\\nWe present the confusion matrix for the prediction of each method in Figure 6. The Download and RKME algorithms consistently show a bias towards a specific model regardless of the user image $x$. This indicates that the Download and RKME methods cannot address the GMI problem. The results show that our proposal achieves the best identification performance on major tasks.\\n\\nWe evaluate the robustness of each method to the hyperparameter $\\\\gamma$ in Figure 7. The results demonstrate that our proposed method exhibits robust performance across a wide range of $\\\\gamma$ values. However, as $\\\\gamma$ continues to increase, the performance of both our proposal and the baseline methods begins to degrade. This observation highlights the importance of tuning the hyperparameter $\\\\gamma$ before deploying our method in practical applications. Once $\\\\gamma$ is properly tuned, our method can operate robustly due to its hyperparameter robustness within a broad range.\"}"}
{"id": "gqtbL7j2JW", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3: The left two subfigures present the average accuracy and rank on all tasks. The right subfigure presents the detailed rank of each task. For accuracy, the higher the better. For rank, the lower the better. The accuracy shows that our proposal outperforms existing solutions (e.g., Download and RKME) and our simple solution which combines some existing techniques. The average rank and detailed ranks present the user's efforts in identifying satisfied models for each task. Our proposal requires minimal user effort.\\n\\nImplementation Details. We adopt the official code in Wu et al. (2023) to implement the RKME-Basic method and the official code in Radford et al. (2021) to implement the CLIP model. For RKME-Basic and RKME-CLIP methods, we follow the default hyperparameter setting of RKME in previous studies (Guo et al., 2023). We set the size of the reduced set to 1 and choose the RBF kernel (Xu et al., 1994) for RKHS. The hyperparameter $\\\\gamma$ for calculating RBF kernel and similarity score is tuned from $\\\\{0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.03, 0.04, 0.05\\\\}$ and set to $0.02$ in our experiments. Experiment results below show that our proposal is robust to $\\\\gamma$.\\n\\n4.2 Empirical Results\\n\\nWhether the most suitable generative model can be identified by our proposed method? The objective of GMI setting is to identify the most suitable generative model for the user's needs. Hence, our initial focus is to determine the effectiveness of baseline methods and our proposed method in identifying the most suitable generative model. We report the accuracy in the left subfigure of Figure 3 to evaluate each method's ability to identify the best matching generative model. Specifically, the Download and RKME-Basic methods cannot work in the GMI problem. The Download method will return models ranked by download volume, which is unable to meet the various needs of users. The identification results of the RKME-Basic method are biased to one model in the platform. The high resolution of images, such as 512x512, presents challenges in calculating the RKME specification and renders the RKME-Basic method ineffective. The performance of RKME-CLIP demonstrates that encoding images is necessary to address the high dimensionality in GMI. However, RKME-CLIP fails to consider the relation between images and prompts, which cannot give the optimal identification results. Our proposal solves the above challenges, giving the best average accuracy compared to baseline methods. These results demonstrate that the specification can help identify the most suitable generative models, which is in line with our starting point.\\n\\nWhether our proposal can achieve satisfactory model recommendations for users? When deploying our proposal in real model platforms, the platform will recommend multiple models for users sorted by similarity score. Therefore, our focus shifts to rank and Top-k accuracy metrics. We report the average rank and detailed ranks in the right two subfigures of Figure 3. Specifically,\"}"}
{"id": "gqtbL7j2JW", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Performance of each method evaluated by Top-k accuracy. The results show that our proposal achieves 80% top-4 accuracy, indicating that user only requires four models to satisfy their needs in major cases.\\n\\n| Method      | Top-1 Acc. | Top-2 Acc. | Top-3 Acc. | Top-4 Acc. | Top-5 Acc. | Top-6 Acc. | Top-7 Acc. | Top-8 Acc. |\\n|-------------|------------|------------|------------|------------|------------|------------|------------|------------|\\n| Download    | 0.062      | 0.125      | 0.188      | 0.250      | 0.312      | 0.375      | 0.438      | 0.500      |\\n| RKME-Basic  | 0.062      | 0.125      | 0.188      | 0.250      | 0.312      | 0.375      | 0.438      | 0.500      |\\n| RKME-CLIP   | 0.419      | 0.576      | 0.688      | 0.770      | 0.832      | 0.870      | 0.905      | 0.934      |\\n| Proposal    | 0.455      | 0.614      | 0.734      | 0.812      | 0.863      | 0.899      | 0.922      | 0.943      |\\n\\nTable 2: Ablation study. For accuracy, the higher the better. For rank, the lower the better. The best performance is in bold.\\n\\n| Methods     | Acc. | Top-2 Acc. | Rank     |\\n|-------------|------|------------|----------|\\n| Download    | 0.062| 0.125      | 8.500    |\\n| RKME-Basic  | 0.062| 0.125      | 8.500    |\\n| RKME-CLIP   | 0.419| 0.576      | 3.130    |\\n| RKME-Concat | 0.433| 0.602      | 2.938    |\\n| Proposal    | 0.455| 0.614      | 2.852    |\\n\\nFigure 4: Visualization of generated images.\\n\\nDownload and RKME-Basic methods show poor performance for similar reasons stated above. Our proposal achieves the best average rank, thus demonstrating its effectiveness. Note that the average rank is related to the efforts of the user when identifying the most appropriate models. Therefore, our proposal can save users time and effort compared to baseline methods. We also report the Top-k accuracy in Table 1. Our proposal gives the best performance among all different values of K, which demonstrates its effectiveness. The experiment results show that our proposal can achieve 80% accuracy when just recommending four models for users. These results reveal the possibility of building a generative model platform and recommending models to users with specifications.\\n\\nTo what extent does each component contribute to the proposed method? In order to comprehensively evaluate the effectiveness of our proposal, we investigate whether each component contributes to the final performance. We additionally compare our proposal with two variants, called RKME-CLIP and RKME-Concat. RKME-CLIP adopts the CLIP model to extract the feature representation for constructing RKME specifications. RKME-Concat adopts both vision and text branches of the CLIP model to extract representations of images and prompts. It combines two modes of representation for constructing RKME specifications. We report accuracy and rank metrics in Figure 2. The performance of RKME-CLIP demonstrates that employing large pre-trained models is an effective approach for addressing dimensionality issues. The performance of RKME-Concat demonstrates the benefits of considering both images and prompts for model identification. Our results achieve the best performance, and demonstrate the effectiveness of our weighted formulation in Equation 3 and our specifically designed algorithm in Equation 9.\\n\\n4.3 VISUALIZATION\\n\\nWe conducted visualization experiments to further show that the new proposal can identify the best-matched model and thereby generate images that meet user requirements better in Figure 4. Specifically, we show the example image $x_\u03c4$ submitted by the user in the first column. Then, we generate requirement $R_\u03c4$ and identify models from the platform using different methods. For each method, we show the image generated with its identified model and pseudo-prompt $I(x_\u03c4)$. The title of the images indicates different model identification methods. The results clearly show that the generative model identified via the new proposal can generate images that best match the users' requirements (most similar to the example image). For example, our method correctly captures the two different comic styles of the image and generates a satisfied image, whereas other methods either have a mismatch in style or have errors in content.\"}"}
{"id": "gqtbL7j2JW", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generative modeling (Jebara, 2012) is a field of machine learning that focuses on learning the underlying distribution and generation of new samples for corresponding distribution. Recently, significant progress has been made in image generation with various methods. Generative Adversarial Networks (GANs) (Arjovsky et al., 2017; Brock et al., 2019; Choi et al., 2020; Goodfellow et al., 2014) apply an adversarial approach to learn the data distribution. It consists of a generator and a discriminator playing a min-max game during the training process. Variational Autoencoders (VAEs) (Kingma & Welling, 2014; Vahdat & Kautz, 2020; van den Oord et al., 2017) is a variant of Auto-Encoder (AE) (Wang et al., 2016), where both consist of the encoder and decoder networks. The encoder in AE learns to map an image into a latent representation. Then, the decoder aims to reconstruct the image from that latent representation. Diffusion Models (DMs) (Nichol & Dhariwal, 2021; Dhariwal & Nichol, 2021; Rombach et al., 2022) leverages the concept of the diffusion process, consisting of forward and reverse diffusion processes. Noise is added to an image during the forward process and the diffusion model learns to denoise and reconstruct the image. With the development of the generative model, various generative model hubs/pools, e.g., HuggingFace, Civitai, have been developed. However, they lack model management and identification mechanisms, resulting in inefficiency for users to find the most suitable model.\\n\\nLu et al. (2022) performs context-based search for unconditional generative models and involves a contrastive learning process for all models in the model hubs. However, this learning process significantly hinders the adaptability of the approach, making it unsuitable for a frequently updated model hub. Assessing the transferability of pre-trained models is related to the problem studied in this paper. Negative Conditional Entropy (NCE) (Tran et al., 2019) proposed an information-theoretic quantity (Cover, 1999) to study the transferability and hardness between classification tasks. LEEP (Nguyen et al., 2020) is primarily developed with a focus on supervised pre-trained models transferred to classification tasks. You et al. (2021) designs a general algorithm, which is applicable to vast transfer learning settings with supervised and unsupervised pre-trained models, downstream tasks, and modalities. However, these methods are not suitable for our GMI problem because they impose significant computational overhead in terms of model inference during the identification process. Learnware (Zhou, 2016) presents a general and realistic paradigm by assigning a specification to models to describe their functionalities and utilities, making it convenient for users to identify the most suitable models. Model specification is the key to the learnware paradigm. Recent studies (Tan et al., 2022) are designed on Reduced Kernel Mean Embedding (RKME) (Wu et al., 2023), which achieves model identification by comparing similarities in the RHKS. Tan et al. (2023; 2022) make their efforts to solve heterogeneous feature spaces. However, these studies primarily focus on classification tasks, overlooking the relationship between images and prompts, which is crucial for identifying generative models. Therefore, existing techniques are inadequate for addressing the GMI problem, requiring for novel technologies.\\n\\nIn this paper, for the first time, we propose a novel problem called Generative Model Identification. The objective of GMI is to describe the functionalities of generative models precisely and enable the model to be accurately and efficiently identified in the future by users' requirements. To this end, we present a systematic solution including a weighted RKME framework to capture the generated image distributions and the relationship between images and prompts, a large pre-trained vision-language model aimed at addressing dimensionality challenges, and an image interrogator designed to tackle cross-modality issues. Moreover, we built and released a benchmark platform based on stable diffusion models for GMI. Extensive experiment results on the benchmark clearly demonstrate the effectiveness of our proposal. For example, our proposal achieves more than 80% top-4 identification accuracy using just one example image to describe the users' requirements, indicating that users can efficiently identify the best-matched model within four attempts in major cases.\\n\\nIn future work, we will endeavor to develop a novel generative model platform based on the techniques presented in this paper, aiming to provide a more precise description of generative model functionalities and user requirements. This will assist users in efficiently discovering models that align with their specific requirements. We believe this could facilitate the development and widespread usage of generative models.\"}"}
{"id": "gqtbL7j2JW", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mart\u00edn Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning, pp. 214\u2013223, 2017.\\n\\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In Proceedings of the 7th International Conference on Learning Representations, 2019.\\n\\nYunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8185\u20138194, 2020.\\n\\nThomas M Cover. Elements of Information Theory. John Wiley & Sons, 1999.\\n\\nAntonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A. Bharath. Generative adversarial networks: An overview. IEEE Signal Processing Magazine, 35(1):53\u201365, 2018.\\n\\nPrafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, pp. 8780\u20138794, 2021.\\n\\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672\u20132680, 2014.\\n\\nLan-Zhe Guo, Zhi Zhou, Yu-Feng Li, and Zhi-Hua Zhou. Identifying useful learnwares for heterogeneous label spaces. In Proceedings of the 40th International Conference on Machine Learning, pp. 12122\u201312131, 2023.\\n\\nTony Jebara. Machine Learning: Discriminative and Generative. Springer Science & Business Media, 2012.\\n\\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535\u2013547, 2019.\\n\\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the 2nd International Conference on Learning Representations, 2014.\\n\\nDiederik P. Kingma and Max Welling. An introduction to variational autoencoders. Foundations and Trends in Machine Learning, 12(4):307\u2013392, 2019.\\n\\nIvan Kobyzev, Simon J. D. Prince, and Marcus A. Brubaker. Normalizing flows: An introduction and review of current methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):3964\u20133979, 2021.\\n\\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In Proceedings of the 39th International Conference on Machine Learning, pp. 12888\u201312900, 2022.\\n\\nYujia Li, Kevin Swersky, and Richard S. Zemel. Generative moment matching networks. In Proceedings of the 32nd International Conference on Machine Learning, pp. 1718\u20131727, 2015.\\n\\nDaohan Lu, Sheng-Yu Wang, Nupur Kumari, Rohan Agarwal, David Bau, and Jun-Yan Zhu. Content-based search for deep generative models. CoRR, abs/2210.03116, 2022.\\n\\nMehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014.\\n\\nCuong Nguyen, Tal Hassner, Matthias Seeger, and Cedric Archambeau. Leep: A new measure to evaluate transferability of learned representations. In Proceedings of 37th International Conference on Machine Learning, pp. 7294\u20137305, 2020.\\n\\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In Proceedings of the 38th International Conference on Machine Learning, pp. 8162\u20138171, 2021.\"}"}
{"id": "gqtbL7j2JW", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024 is a model platform, consisting of $M$ generative models $f_m \\\\ (m=1,\\\\ldots,M)$. Each model is associated with a corresponding specification $S_m$ to describe its functionalities for future model identification. The platform consists of two stages: the submitting stage for model developers and the identification stage for users.\\n\\nIn the submitting stage, the model developer submits a generative model $f_m$ to the platform. Then, the platform assigns a specification $S_m$ to this model. Here, the specification $S_m = A_s(f_m, P)$ is generated by a specification algorithm $A_s$ using the model $f_m$ and a prompt set $P = \\\\{p_k\\\\}_{k=1}^N$.\\n\\nIf the model developer can provide a specific prompt set for the uploaded model, the generated specification would be more precise in describing its functionalities. In the identification stage, the users identify models from the platform using only one image $x_\\\\tau$. When users upload an image $x_\\\\tau$ to describe their purposes, the platform automatically calculates the pseudo-prompt $b_{p_\\\\tau}$ and then generates requirements $R_\\\\tau = A_r(x_\\\\tau, b_{p_\\\\tau})$ using a requirement algorithm $A_r$. Users can optionally provide corresponding prompt $p_\\\\tau$, setting $b_{p_\\\\tau} = p_\\\\tau$, to more precisely describe their purposes.\\n\\nDuring the identification process, the platform matches requirement $R_\\\\tau$ with model specifications $\\\\{S_m\\\\}_{m=1}^M$ using a evaluation algorithm $A_e$ and compute similarity score $b_{s_\\\\tau,m} = A_e(S_m, R_\\\\tau)$ for each model $f_m$. Finally, the platform returns the best-matched model with the maximum similarity score or a list of models sorted by $\\\\{b_{s_\\\\tau,m}\\\\}_{m=1}^M$ in descending order.\\n\\nNote that the GMI setting helps reduce the consumption of network traffic and computing resources, as well as the time and effort of users. As shown in Figure 1, users are relieved from the burden of repeatedly selecting, downloading, and evaluating models by utilizing the calculated similarity scores $\\\\{b_{s_\\\\tau,m}\\\\}_{m=1}^M$. Moreover, the GMI setting is easy to use for both developers and users since all the processes are automatically conducted in the background without requiring complex inputs.\\n\\nThere are two main challenges for addressing GMI setting: 1) In the submitting stage, how to design $A_s$ to fully characterize the generative models for identification? 2) In the identification stage, how to design $A_r$ and $A_e$ to effectively identify the most appropriate generative models for user needs?\\n\\n2.2 PROBLEM ANALYSIS\\n\\nIn this subsection, we first briefly introduce the principle of the generative model, taking the stable diffusion models as examples. Then, we show the RKME method (Wu et al., 2023) can address GMI as a baseline method, modeling the data distribution of the model as the specification. We present an example to show impossible cases of the baseline method because of overlooking the interplay between prompts and images for generative tasks. Finally, we introduce our weighted RKME framework for solving GMI problem setting.\\n\\nStable Diffusion.\\n\\nGenerative models (Jebara, 2012) are capable of sampling images from a data distribution defined by the model. Recently, stable diffusion models (Dhariwal & Nichol, 2021) have become one of the most popular models for their impressive performance. Therefore, we take conditional stable diffusion models as examples for subsequent analysis and experiments. The conditional diffusion model is a latent variable model, modeling a Markov chain with learned Gaussian transitions $p_{\\\\theta_m}(x_{t-1} | x_t; p)$ for each iteration $t \\\\in [1, T]$ starting an initial state $p(x_T) \\\\sim N(0, I)$:\\n\\n$$p_{\\\\theta_m}(x_0:T | p) = p(x_T) \\\\prod_{t=1}^T p_{\\\\theta_m}(x_t-1 | x_t; p).$$\\n\\nHere, $p$ is a prompt guiding the generation process, and $p_{\\\\theta_m}(\\\\cdot)$ is the learned Gaussian transitions parameterized by $\\\\theta_m$. For simplicity, we assume the generative model $f_m$ generate an image set $X_m = \\\\{x_{m,i}\\\\}_{i=1}^{N_m} = \\\\{x | x \\\\sim f_m(p), \\\\forall p \\\\in P\\\\}$ sampled from corresponding probability distribution $p_{\\\\theta_m}(x_0:T | p)$, using prompt set $P$ of model platform.\\n\\nReduced Kernel Mean Embedding.\\n\\nA baseline method to describe the model's functionality is the RKME techniques (Wu et al., 2023). It maps data distribution of each model $f_m$ as corresponding specification $S_{RKME_m} = \\\\{x_{RKME,m,i}\\\\}_{i=1}^{N_{RKME,m}}$, where $N_{RKME,m}$ is the reduced set size of $f_m$. For one query image $x_\\\\tau$ from the users, the baseline method defines the requirement as $R_{RKME_\\\\tau} = \\\\{x_\\\\tau\\\\}$. Finally,\"}"}
{"id": "gqtbL7j2JW", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the platform computes the similarity score in RKHS $H_k$ using evaluation algorithm $A_{RKME}$:\\n\\n$$A_{RKME}(S_{RKME}, R_{RKME}) = \\\\sum_{i=1}^{N_{RKME}} k(x_{RKME,i}, \\\\cdot) - k(x_{\\\\tau}, \\\\cdot)$$\\n\\nwhere $k(\\\\cdot, \\\\cdot)$ is the reproducing kernels associated with RKHS $H_k$. This baseline method fails to capture the interplay between generated images $X_m$ and the prompt set $P$, which is the probability distribution $p_\\\\theta(x_0:T|p)$ inside the generative model $f_m$. We present an example to show this interplay is important otherwise the specification cannot distinguish two models in specific cases, resulting in unsatisfactory identification results.\\n\\nExample 2.1. Suppose that there are two simplified generative models $f_1$ and $f_2$ on the platform. $f_1$ generates scatter points following $x = \\\\cos(p\\\\pi)$, $y = \\\\sin(p\\\\pi)$. $f_2$ generates scatter points following $x = \\\\sin(p\\\\pi)$, $y = \\\\cos(p\\\\pi)$. The prompt set $p$ follows $U(-1, 1)$. The user wants to deploy the identified model conditioned on prompts $p_{\\\\tau}$ following distribution $U(0.5, 0)$. In Figure 2, we show that the baseline method in Equation 2 fails to distinguish two models for users. However, the two models function differently with $p_{\\\\tau}$. Figure 2a and Figure 2b show that (a) Distribution of specification $X_1 \\\\sim f_1(p)$ (b) Distribution of specification $X_2 \\\\sim f_2(p)$ (c) Distributions of $f_1(p_{\\\\tau})$ and $f_2(p_{\\\\tau})$.\\n\\nFigure 2: Baseline method in Equation 2 fails to distinguish two different models for users. Although models $f_1$ and $f_2$ function differently, the data distribution $X_1 \\\\sim f_1(p)$ and $X_2 \\\\sim f_2(p)$, conditioned on the default prompt distribution $p$, could be identical. Therefore, the specifications $S_{RKME_1}$ and $S_{RKME_2}$ are identical, resulting in the same similarity scores $A_{RKME}(S_{RKME_1}, R_{RKME})$ and $A_{RKME}(S_{RKME_2}, R_{RKME})$. However, Figure 2c shows that two models $f_1$ and $f_2$ generate different data distributions $f_1(p_{\\\\tau})$ and $f_2(p_{\\\\tau})$ conditioned on the user prompt distribution $p_{\\\\tau}$.\\n\\nRemark. Example 2.1 shows us that overlooking the interplay between images and prompts leads to impossible cases for distinguishing generative models effectively. Existing RKME studies mainly focus on classification tasks, which can implicitly model the tasks through data distribution since the class space is discrete and small. For generative models, we have to explicitly model the model's functionality, i.e., the relation between images and prompts, to achieve satisfied identification results.\\n\\nIncorporating relation between images and prompts\\nMotivated by our analysis, how to incorporate the relationship between images and prompts in model specification and identifying process is the key challenge for our GMI setting. Inspired by existing studies (Li et al., 2015; Ren et al., 2016) about the conditional maximum mean discrepancy, we propose to consider the above relation using a weighted formulation of Equation 2:\\n\\n$$A_{Weighted}(S_{Weighted}, R_{Weighted}) = \\\\sum_{i=1}^{N_m} w_{m,i} k(x_{m,i}, \\\\cdot) - k(x_{\\\\tau}, \\\\cdot)$$\\n\\nwhere $W_m = \\\\{w_{m,i}\\\\}$ $i=1$ are required to measure the relation between user image $x_{\\\\tau}$ and prompt set $P$. Here, we make the simplifications $R_{Weighted} = x_{\\\\tau}$ and $S_{Weighted} = X_m$ in Equation 3. This raises challenges inherent in dimensionality since stable diffusion models produce high-quality images. Moreover, measuring the relation using $W_m$ is also a challenging problem and encounters cross-modality issues. Below we propose a comprehensive solution based on Equation 3 addressing these challenges.\"}"}
{"id": "gqtbL7j2JW", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we present our solution for the GMI setting, building upon the previous analysis and the weighted formulation introduced in Equation 3. As mentioned earlier, two significant challenges remain to be addressed: 1) The raw images reside in a high-dimensional space, and pixel-level comparisons are highly sensitive. How can we efficiently and effectively measure the similarity between images? 2) The user's image $x_\\\\tau$ and the platform's prompt set $P$ belong to different modalities. How do we address cross-modality issues and calculate $W_m$ to capture the relationship between images and prompts?\\n\\nTo address the aforementioned challenges, we employ a large pre-trained vision model $G(\\\\cdot)$ to map images from raw input space to a common feature representation space. Subsequently, an image interrogator $I(\\\\cdot)$ is adopted to convert $x_\\\\tau$ to corresponding pseudo prompt $b_{p_\\\\tau}$, thereby mitigating the cross-modality issues. Consequently, the similarity in the common feature representation space can be computed with the help of a large pre-trained language model $T(\\\\cdot)$.\\n\\n### 3.1 Submitting Stage\\n\\nIn the submitting stage, the model developer submits the generative model $f_m$, and the platform generates the model specification in the background using the specification algorithm $A_s$ with the submitted models $f_m$ and default prompt set $P$. The developer can optionally replace $P$ with a specific prompt set to generate a more precise specification. The algorithm $A_s$ first samples images from the generative model $f_m$ using the prompt set:\\n\\n$$X_m = \\\\{f_m(p) | p \\\\in P\\\\}$$  (4)\\n\\nThen, the large pre-trained vision model $G(\\\\cdot)$ is adopted to encode $X_m$ as follows. The obtained feature representation $Z_m$ is efficient and robust to compute similarity between images.\\n\\n$$Z_m = \\\\{G(x) | x \\\\in X_m\\\\}$$  (5)\\n\\nSubsequently, $A_s$ encodes prompt set $P$ to the common feature representations using $T(\\\\cdot)$:\\n\\n$$Q_m = \\\\{T(p) | p \\\\in P\\\\}$$  (6)\\n\\nFinally, the specification $S_m$ of generative model $f_m$ is defined as follows:\\n\\n$$S_m = A_s(f_m; P_m) = \\\\{Z_m; Q_m\\\\}$$  (7)\\n\\nNote that $S_m$ is automatically computed inside the platform, which is very convenient for developers to use and deduce their burden of uploading models. Additionally, the specification does not occupy a large amount of storage space on the platform since the only feature representation is storage.\\n\\n### 3.2 Identification Stage\\n\\nIn the identification stage, the users upload one single image $x_\\\\tau$ to describe their requirements. Then, the platform describes the requirements with $R_\\\\tau$ from $x_\\\\tau$. Specifically, the requirement algorithm $A_r$ first generates feature representations of $x_\\\\tau$ using $G(\\\\cdot)$, i.e.,\\n\\n$$z_\\\\tau = G(x_\\\\tau)$$\\n\\nSubsequently, the pseudo-prompt $b_{p_\\\\tau}$ is generated by $I(\\\\cdot)$, i.e.,\\n\\n$$b_{p_\\\\tau} = I(x_\\\\tau)$$\\n\\nand converted to feature representations using $T(\\\\cdot)$, i.e.,\\n\\n$$b_{q_\\\\tau} = T(b_{p_\\\\tau})$$\\n\\nThe user can optionally replace $b_{p_\\\\tau}$ with a prompt $p_\\\\tau$ built on his understanding to precisely describe the requirement. Finally, the requirement is:\\n\\n$$R_\\\\tau = A_r(x_\\\\tau) = \\\\{z_\\\\tau; b_{q_\\\\tau}\\\\}$$  (8)\\n\\nNote that $R_\\\\tau$ is automatically computed inside the platform, which is very easy to use for users.\\n\\nAfter the platform generates the requirement $R_\\\\tau$, it will calculates the similarity score for each model $f_m$ using evaluation algorithm $A_e$:\\n\\n$$A_e(S_m, R_\\\\tau) = \\\\frac{1}{N_m} \\\\sum_{i=1}^{N_m} b_{q_{m,i}} b_{q_\\\\tau} \\\\langle \\\\frac{b_{q_{m,i}}}{\\\\|b_{q_{m,i}}\\\\|}, \\\\frac{b_{q_\\\\tau}}{\\\\|b_{q_\\\\tau}\\\\|} \\\\rangle - \\\\frac{1}{N_m} \\\\sum_{i=1}^{N_m} \\\\langle \\\\frac{z_{m,i}}{\\\\|z_{m,i}\\\\|}, \\\\frac{z_{\\\\tau}}{\\\\|z_{\\\\tau}\\\\|} \\\\rangle^2$$  (9)\\n\\nwhere the $W_m$ of Equation 3 is define as the cosine similarity between platform prompts $b_{q_{m,i}} \\\\in Q_m$ and pseudo-prompt $b_{q_\\\\tau}$.\\n\\n$W_m$ encodes the structure information of $x_\\\\tau$ within $P_m$ during the identification, which successfully captures the relation between images and prompts. The platform returns a list of models sorted in increasing order of similarity score obtained by Equation 9.\"}"}
{"id": "gqtbL7j2JW", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3 DISCUSSION\\n\\nIt is evident that our proposal for the GMI scenario achieves a higher level of accuracy and efficiency when compared to model search techniques employed by existing model hubs.\\n\\nFor accuracy, our proposal elucidates the functionalities of generated models by capturing both the distribution of generated images and prompts. This approach allows for more accurate identification of suitable models for users, as opposed to the traditional model search method that relies on download counts and star ratings for ranking models.\\n\\nFor efficiency, suppose the platform generates one requirement in \\\\( T_\\\\text{r} \\\\) time and calculates the similarity score for each model in \\\\( T_\\\\text{s} \\\\) time. The time complexity of our proposal for one identification is \\\\( O(T_\\\\text{r} + M T_\\\\text{s}) \\\\) time. Moreover, with accurate identification results, users can save the efforts of browsing and selecting models, as well as reducing the consumption of network and computing. This is linearly correlated to the number of models on the platform (which can be reduced through tag filtering). Additionally, our approach also has the potential to achieve further acceleration through the use of a vector database (Guo et al., 2023) such as Faiss (Johnson et al., 2019).\\n\\n4 EXPERIMENTS\\n\\nTo verify the effectiveness of our proposed method for GMI problem, we conduct experiments on a novel generative model identification benchmark dataset based on stable diffusion models (Rombach et al., 2022). Our objective is to answer the following three research questions:\\n\\n\u2022 Whether the most suitable generative model can be identified by our proposed method?\\n\u2022 Whether our proposal can achieve satisfactory model recommendations for users?\\n\u2022 To what extent does each component contribute to the proposed method?\\n\\n4.1 EXPERIMENTAL SETTINGS\\n\\nModel Platform and Task Construction.\\n\\nIn practice, we expect model developers to submit their models and corresponding prompts to the model platform. And we expect users to identify models for their real needs. In our experiments, we constructed a model platform and user identification tasks respectively to simulate the above situation. For the construction of the model platform, we manually collect \\\\( M = 16 \\\\) different stable diffusion models \\\\( \\\\{f_1, \\\\ldots, f_M\\\\} \\\\) from one popular model platform, CivitAI, as uploaded generative models on the platform. Note that these collected models belong to the same category to simulate the real process in which users first trigger category filters and then select the models. We construct 55 prompts \\\\( \\\\{p_1, \\\\ldots, p_{55}\\\\} \\\\) as default prompt set \\\\( \\\\mathcal{P} \\\\) of platform. For task construction, we construct 18 evaluation prompts \\\\( \\\\{p_{\\\\tau 1}, \\\\ldots, p_{\\\\tau 18}\\\\} \\\\) for each model on the platform to generate testing images with random seed in \\\\( \\\\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\\\} \\\\), forming \\\\( N_\\\\tau = 18 \\\\times 16 \\\\times 10 = 2880 \\\\) different identification tasks \\\\( \\\\{(x_\\\\tau i, t_i)\\\\}_{i=1}^{N_\\\\tau} \\\\), where each testing image \\\\( x_\\\\tau i \\\\) is generated by model \\\\( f_t i \\\\) and its best matching model index is \\\\( t_i \\\\). Here, we ensure that there is no overlap between \\\\( \\\\{p_1, \\\\ldots, p_{55}\\\\} \\\\) and \\\\( \\\\{p_{\\\\tau 1}, \\\\ldots, p_{\\\\tau 18}\\\\} \\\\) to ensure the correctness of the evaluation.\\n\\nEvaluation Metrics.\\n\\nIn our experiments, we use accuracy and average rank to evaluate the performance of methods. We define the rank of model \\\\( f_m \\\\) for task \\\\( \\\\tau \\\\) as\\n\\n\\\\[\\n\\\\text{rank}(\\\\tau, m) = 1 + \\\\sum_{i=1}^{M} I[b_s(\\\\tau, i) < b_s(\\\\tau, m)]\\n\\\\]\\n\\nThe accuracy is defined as\\n\\n\\\\[\\n\\\\text{Acc.} = \\\\frac{1}{N_\\\\tau} \\\\sum_{i=1}^{N_\\\\tau} I[\\\\text{rank}(\\\\tau, i), t_i = 1]\\n\\\\]\\n\\nwhere \\\\( \\\\text{Acc.} \\\\in [0, 1] \\\\) evaluates the ability of each method to find the best matching model. The average rank is defined as\\n\\n\\\\[\\n\\\\text{Rank} = \\\\frac{\\\\text{rank}(\\\\tau, i), t_i}{N_\\\\tau}\\n\\\\]\\n\\nwhere \\\\( \\\\text{Rank} \\\\in [1, M] \\\\) evaluates the ability of each method to rank the best matching model among other models. We additionally report the \\\\( \\\\text{Top}-k \\\\) accuracy, which is calculated as\\n\\n\\\\[\\n\\\\text{Top}-k \\\\text{ Acc.} = \\\\frac{1}{N_\\\\tau} \\\\sum_{i=1}^{N_\\\\tau} I[\\\\text{rank}(\\\\tau, i), t_i \\\\leq k]\\n\\\\]\\n\\nThis metric measures the average effort spent by users during the identification process and \\\\( \\\\text{Top}-k \\\\text{ Acc.} \\\\in [0, 1] \\\\).\\n\\nComparison Methods.\\n\\nInitially, we compare it with the traditional model search method called Download. This method is used to simulate how users search generative models according to their downloading volumes (Shen et al., 2023), where users will try models with high downloading volume first. This baseline method can represent a family of methods that employ statistical information.\"}"}
