{"id": "mMaInr0r0c", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 15: 3AFC same stimuli test set annotator demographics. Annotator age, gender identity, nationality, ancestry, and subregional ancestry group counts for the 355 eligible annotators who contributed to the test set partition of 24,060 quality-controlled 3AFC triplet-judgment tuples. The distribution of per annotator contributions is also shown. Note that for nationality, ancestry, and subregional ancestry the plots only show groups with $\\\\geq 5$ annotators.\\n\\nFigure 16: 3AFC novel stimuli test set annotator demographics. Annotator age, gender identity, nationality, ancestry, and subregional ancestry group counts for the 632 eligible annotators who contributed to the test set partition of 80,300 quality-controlled 3AFC triplet-judgment tuples. The distribution of per annotator contributions is also shown. Note that for nationality, ancestry, and subregional ancestry the plots only show groups with $\\\\geq 5$ annotators.\\n\\nC.2.4 What Data Does Each Instance Consist Of?\\n\\n\u2022 Each 3AFC instance consists of a triplet of face image identifiers, 3AFC (i.e., odd-one-out) judgment, and annotator identifier (corresponding to the annotator who generated the topic label).\\n\\n\u2022 Each human-generated topic label instance consists of a dimension identifier, topic label, and annotator identifier (corresponding to the annotator who generated the topic label).\"}"}
{"id": "mMaInr0r0c", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 17: Dimensional interpretability.\\n(a) Our interpretation of an AVFS-U dimension with the highest scoring stimulus set images. (b) Highest frequency human-generated topic labels. (c) CelebA and FairFace model-generated labels, exhibiting a clear correspondence with human-generated topics. Face images in this figure have been replaced by synthetic StyleGAN3 images for privacy reasons.\\n\\nFigure 18: Topic labeling annotator demographics.\\nAnnotator age, gender identity, nationality, ancestry, and subregional ancestry group counts for the 84 eligible annotators who contributed 738 human-generated topic labels that capture the meaning of the 22 learned embedding dimensions. The distribution of per annotator contributions is also shown. Note that for nationality, ancestry, and subregional ancestry the plots only show groups with \u22655 annotators.\\n\\n\u2022 Each human-generated image rating instance consists of a dimension identifier, image identifier, image rating, and annotator identifier (corresponding to the annotator who generated the topic label).\"}"}
{"id": "mMaInr0r0c", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 19: Image rating annotator demographics.\\n\\nAnnotator age, gender identity, nationality, ancestry, and subregional ancestry group counts for the 164 eligible annotators who contributed 8,800 human-generated image ratings along the 22 learned embedding dimensions. The distribution of per-annotator contributions is also shown. Note that for nationality, ancestry, and subregional ancestry the plots only show groups with \\\\( \\\\geq 5 \\\\) annotators.\\n\\nNote that each annotator's identifier is associated with their demographic attributes (i.e., age, gender identity, nationality, and ancestry).\\n\\nC.2.5 Is there a label or target associated with each instance?\\n\\n\u2022 Each 3AFC instance's target is a 3AFC judgment.\\n\u2022 Each human-generated topic label instance's target is a topic label.\\n\u2022 Each human-generated image rating instance's target is an image rating.\\n\\nC.2.6 Is any information missing from individual instances?\\n\\nNo.\\n\\nC.2.7 Are relationships between individual instances made explicit?\\n\\nYes. Each instance can be mapped to the annotator who labeled it.\\n\\nC.2.8 Are there recommended data splits?\\n\\nThe 3AFC instances are split into training, validation, and testing partitions. All other instances are for evaluating learned model dimensions, i.e., for the model introduced in the paper.\\n\\nC.2.9 Are there any errors, sources of noise, or redundancies in the dataset?\\n\\nNo.\\n\\nC.2.10 Is the dataset self-contained, or does it link to or otherwise rely on external resources?\\n\\nThe dataset does not include any images, it instead references to image identifiers. The image identifiers can be used to obtain the relevant FFHQ images, which are hosted on NVIDIA Corporation's Google Drive: https://github.com/NVlabs/ffhq-dataset.\"}"}
{"id": "mMaInr0r0c", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.2.11 Does the dataset contain data that might be considered confidential?\\nThe dataset includes the demographic information (i.e., age, nationality, ancestry, and gender identity) of each annotator; however, all annotators consented to the collection, use, and publication of their data.\\n\\nC.2.12 Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?\\nNo.\\n\\nC.2.13 Does the dataset relate to people?\\nThe dataset contains annotations corresponding to human judgments of face similarity, as well as demographic information about each annotator.\\n\\nC.2.14 Does the dataset identify any subpopulations?\\nThe dataset includes the demographic information (i.e., age, nationality, ancestry, and gender identity) of each annotator.\\n\\nC.2.15 Is it possible to identify individuals, either directly or indirectly from the dataset?\\nEach annotator's Amazon Mechanical Turk (AMT) identifier was replaced by an identifier randomly generated by the authors. The dataset therefore contains de-identified data.\\n\\nC.2.16 Does the dataset contain data that might be considered sensitive in any way?\\nThe dataset includes the demographic information (i.e., age, nationality, ancestry, and gender identity) of each annotator.\\n\\nC.3 Collection Process\\nC.3.1 How was the data associated with each instance acquired?\\nAll data associated with each instance was acquired via AMT, except for the image identifiers which were obtained from the FFHQ dataset: https://github.com/NVlabs/ffhq-dataset.\\n\\nC.3.2 What mechanisms or procedures were used to collect the data?\\nThe AMT API was used to collect the data associated with each instance.\\n\\nC.3.3 If the dataset is a sample from a larger set, what was the sampling strategy?\\nRefer to Appendix C.2.3.\\n\\nC.3.4 Who was involved in the data collection process and how were they compensated?\\nThe authors collected the annotations directly from annotators via AMT. All annotators were required to have previously completed \\\\( \\\\geq 100 \\\\) human intelligence tasks (HITs) on AMT with \\\\( \\\\geq 95\\\\% \\\\) approval rating. Eligibility was further determined through a prescreening survey, which included a short multiple-choice English language proficiency test. Since we placed no restriction on annotator location, in order to be eligible, we required annotators to answer at least two out of three multiple-choice English language proficiency questions correctly.\"}"}
{"id": "mMaInr0r0c", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Decide where on the grid you would place this person based on their similarity to the people shown in each column.\\n\\nFigure 10: Image rating task. Illustrative example of a face image grid for model dimension $j$ shown to annotators, displaying the stimulus set of 4,921 faces sorted in descending order (from left to right) based on their dimension $j$ embedding value. Annotators were tasked with deciding which column of the grid a novel face (shown above the grid) should be placed on based on its similarity to the faces in the column. Annotators were instructed to view and interact with the entire grid before making their decision. As annotators were not primed with the meaning of a grid, accurate placement would indicate that the ordering of the grid is visually meaningful. All face images in the figure were replaced by synthetic StyleGAN3 images for privacy reasons; annotators were shown real images.\\n\\nC.2.3 Does the dataset contain all possible instances or is it a sample of instances from a larger set?\\n\\nFace image stimuli. Starting from the 70,000 FFHQ (Karras et al., 2019) images, the stimulus set of 4,921 face images was constructed by:\\n\\n1. Removing faces with an absolute head yaw or pitch angle $> \\\\pi/6$, left or right eye occlusion score $> 20$, or eyewear label. This was done to reduce the influence of head pose on human behavior and more easily permit eye comparisons.\\n\\n2. Removing face images with an age label $> 19$ years old, so as to exclude images of minors.\\n\\n3. Categorizing the remaining faces into 56 intersectional demographic subgroups, which were defined based on ethnicity, age, and gender labels.\\n\\n4. Randomly sampling face images (without replacement) such that each subgroup would be represented in the stimulus set. This was done to ensure that the stimulus set would at least be diverse with respect to demographic subgroup labels. Moreover, face images were randomly sampled from a subgroup, as opposed to selecting the most confidently labeled, to avoid biasing the stimulus set toward stereotypical faces.\\n\\nFigure 11 shows the intersectional group counts for the 4,921 face images in the stimulus set. An additional set of 56 images and set of 20 images were sampled from FFHQ for the test set partition of 80,300 3AFC instances and 8,800 human-generated image rating instances, respectively. Both of these sets of images were sampled in the same manner as the stimulus set. In addition, they are disjoint from each other and the stimulus set of 4,921 faces. Figures 12 and 13 show the intersectional group counts for the set of 56 images and set of 20 images, respectively.\\n\\nHead yaw and pitch angles, eyewear, and eye occlusion labels were obtained from the FFHQ-Aging (Or-El et al., 2020) dataset, which Or-El et al. inferred using Face++. Note that FFHQ-Aging comprises of labels for each face image contained in FFHQ. Age and gender labels were also obtained from the FFHQ-Aging dataset, which Or-El et al. inferred by employing a group of human annotators to annotate the face images. Ethnicity was estimated using a pretrained FairFace model (Appendix A).\\n\\n3AFC instances. The training and validation set of 638,180 quality-controlled 3AFC triplet-judgment tuples contains 638,180 unique triplets, i.e., a single judgment per unique triplet. The judgments were obtained from 1,645 eligible annotators. Figure 14 shows the demographic group counts of the 20\"}"}
{"id": "mMaInr0r0c", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: Stimulus set intersectional subgroup counts. Intersectional subgroup counts for the 4,921 face images contained in the stimulus set. There are 56 subgroups represented in the stimulus set.\\n\\nFigure 12: Novel stimuli test set intersectional subgroup counts. Intersectional subgroup counts for the 56 face images used in the test set partition of 80,300 quality-controlled 3AFC triplet-judgment tuples. There are 48 (from a possible 56) subgroups represented in the 27,720 unique triplets.\\n\\n3 The quality-controlled instances note the abbreviations: \u201cafrica\u201d (\u201caf\u201d), \u201cnorth africa\u201d (\u201cafna\u201d), \u201ceast africa\u201d (\u201cafea\u201d), \u201cmiddle africa\u201d (\u201cafma\u201d), \u201csouth africa\u201d (\u201cafsa\u201d), \u201cwestern africa\u201d (\u201cafwa\u201d), \u201camericas\u201d (\u201cam\u201d), \u201ccaribbean\u201d (\u201camc\u201d), \u201ccentral america\u201d (\u201camcentral\u201d), and \u201csouth america\u201d (\u201camsa\u201d).\"}"}
{"id": "mMaInr0r0c", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13: Rated image set intersectional subgroup counts. Intersectional subgroup counts for the 20 face images used for collecting human-generated image ratings along the 22 learned embedding dimensions. There are 18 (from a possible 56) subgroups represented.\\n\\nTable 2: Annotator exclusion policies and criteria. 3AFC annotations from annotators who provided overly fast, deterministic, or incomplete judgments were excluded.\\n\\n| Policy Criteria | Min. #judgments |\\n|-----------------|-----------------|\\n| Fast #1         | >0.8s           |\\n| Fast #2         | >1.1s           |\\n| Deterministic   | >40% single triplet position |\\n| Incomplete      | 1               |\\n\\nInstances represent a subset of instances from a larger set of 703,300 3AFC instances. The 703,300 triplets were sampled from 4,921 possible triplets (i.e., over 4,921 face images contained in the stimulus set). Instances were not included in the subset of 638,180 3AFC instances if they were annotated by any annotator who provided overly fast, deterministic, or incomplete judgments. Refer to the annotator exclusion policies outlined in Table 2.\\n\\nThe test set partition of 24,060 quality-controlled 3AFC triplet-judgment tuples contains 1,000 unique triplets with 22\u201325 unique judgments per triplet. The judgments were obtained from 355 eligible annotators. Figure 15 shows the demographic group counts of the annotators as well as the distribution of annotator contributions. The quality-controlled instances represent a subset of instances from a larger set of 25,000 3AFC instances with 25 unique judgments per triplet. The 1,000 triplets were sampled from 4,921 possible triplets (i.e., over 4,921 face images contained in the stimulus set).\\n\\nWithin the 1,000 triplets, 2,163 face images from the stimulus set of 4,921 face images were used. The 1,000 triplets are disjoint from the 703,300 triplets sampled for the training and validation set. Instances were not included in the subset of 24,060 3AFC instances if they were annotated by any annotator who provided overly fast, deterministic, or incomplete judgments. Refer to the annotator exclusion policies outlined in Table 2.\\n\\nThe test set partition of 80,300 quality-controlled 3AFC triplet-judgment tuples contains 56,360 unique triplets, representing all possible triplets that could be sampled from 56 images. Each triplet has 2\u20133 judgments from 632 eligible annotators. Figure 16 shows the demographic group counts of the annotators as well as the distribution of annotator contributions. The quality-controlled instances represent a subset of instances from a larger set of 83,160 3AFC instances with 3 unique judgments per triplet. Instances were not included in the subset of 80,300 3AFC instances if they were annotated by any annotator who provided overly fast, deterministic, or incomplete judgments. Refer to the annotator exclusion policies outlined in Table 2.\\n\\nDimension topic label instances. The 738 quality-controlled human-generated topic labels were obtained for 22 learned embedding dimensions, representing a subset from 128 dimensions. Figure 17 shows the learned embeddings with the highest frequency topic labels generated by the annotators.\"}"}
{"id": "mMaInr0r0c", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 14: 3AFC training and validation set annotator demographics. Annotator age, gender identity, nationality, ancestry, and subregional ancestry group counts for the 1,645 eligible annotators who contributed to the training and validation set of 638,180 quality-controlled 3AFC triplet-judgment tuples. The distribution of per annotator contributions is also shown. Note that for nationality, ancestry, and subregional ancestry the plots only show groups with \\\\( \\\\geq 5 \\\\) annotators.\\n\\nThe subset of dimensions were selected on the basis of their maximal observed value (over the stimulus set of 4,921 faces) being sufficiently larger than zero. The topic labels were obtained from 84 eligible annotators. Figure 18 shows the demographic group counts of the annotators as well as the distribution of annotator contributions. Each of the 22 dimensions has 23\u201358 topic labels. The quality-controlled topic labels were arrived at by first removing annotator responses that did not correspond to words and those that were derogatory or offensive. The filtered responses were then converted into 35 topic labels: \u201cage\u201d, \u201cage related\u201d, \u201cancestry\u201d, \u201cear shape\u201d, \u201cemotion expression\u201d, \u201ceye color\u201d, \u201ceye related\u201d, \u201ceye shape\u201d, \u201ceyebrow related\u201d, \u201ceyebrow shape\u201d, \u201cface related\u201d, \u201cface shape\u201d, \u201cfacial expression\u201d, \u201cfacial hair\u201d, \u201cforehead shape\u201d, \u201cgender expression\u201d, \u201chair color\u201d, \u201chair length\u201d, \u201chair related\u201d, \u201chair texture\u201d, \u201chairstyle\u201d, \u201chead shape\u201d, \u201chealth related\u201d, \u201clips related\u201d, \u201clips shape\u201d, \u201cmouth related\u201d, \u201cmouth shape\u201d, \u201cneck related\u201d, \u201cnose related\u201d, \u201cnose shape\u201d, \u201cskin color\u201d, \u201cskin related\u201d, \u201cskin texture\u201d, \u201cteeth related\u201d, and \u201cweight related\u201d.\\n\\nDimension rating labels. The 8,800 quality-controlled human-generated image ratings were obtained for 22 learned embedding dimensions, representing a subset from 128 dimensions. The subset of dimensions were selected on the basis of their maximal observed value (over the stimulus set of 4,921 faces) being sufficiently larger than zero. The ratings were obtained from 164 eligible annotators. Figure 19 shows the demographic group counts of the annotators as well as the distribution of annotator contributions. Quality control corresponded to excluding incomplete responses, i.e., empty responses. The 8,800 ratings represent all of the ratings collected from annotators, i.e., all collected ratings were nonempty.\"}"}
{"id": "mMaInr0r0c", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yunliang Chen and Jungseock Joo. Understanding and mitigating annotation bias in facial expression recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14980\u201314991, 2021.\\n\\nKate Crawford and Trevor Paglen. Excavating AI: The politics of images in machine learning training sets. AI and Society, 2019.\\n\\nJiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4690\u20134699, 2019.\\n\\nEmily Denton, Mark D\u00edaz, Ian Kivlichan, Vinodkumar Prabhakaran, and Rachel Rosen. Whose ground truth? accounting for individual and collective identities underlying dataset annotation. arXiv preprint arXiv:2112.04554, 2021.\\n\\nThomas Deselaers and Vittorio Ferrari. Visual and semantic similarity in imagenet. In CVPR 2011, pp. 1777\u20131784. IEEE, 2011.\\n\\nDiana C Dima, Martin N Hebart, and Leyla Isik. A data-driven investigation of human action representations. bioRxiv, 2022.\\n\\nCynthia Feliciano. Shades of race: How phenotype and observer characteristics shape racial classification. American Behavioral Scientist, 60(4):390\u2013419, 2016.\\n\\nJonathan B Freeman, Andrew M Penner, Aliya Saperstein, Matthias Scheutz, and Nalini Ambady. Looking the part: Social status cues shape race perception. PloS one, 6(9):e25107, 2011.\\n\\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 III, and Kate Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.\\n\\nNelson Goodman. Seven strictures on similarity. 1972.\\n\\nPriya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan Misra, Levent Sagun, Armand Joulin, and Piotr Bojanowski. Vision models are more robust and fair when pretrained on uncurated images without supervision. arXiv preprint arXiv:2202.08360, 2022.\\n\\nAlex Hanna, Emily Denton, Andrew Smart, and Jamila Smith-Loud. Towards a critical race methodology in algorithmic fairness. In Proceedings of the 2020 conference on fairness, accountability, and transparency, pp. 501\u2013512, 2020.\\n\\nCaner Hazirbas, Joanna Bitton, Brian Dolhansky, Jacqueline Pan, Albert Gordo, and Cristian Canton Ferrer. Towards measuring fairness in AI: the casual conversations dataset. IEEE Transactions on Biometrics, Behavior, and Identity Science, 2021.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.\\n\\nMarti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support vector machines. IEEE Intelligent Systems and their applications, 13(4):18\u201328, 1998.\\n\\nMartin N Hebart, Oliver Contier, Lina Teichmann, Adam Rockter, Charles Y Zheng, Alexis Kidder, Anna Corriveau, Maryam Vaziri-Pashkam, and Chris I Baker. Things-data: A multimodal collection of large-scale datasets for investigating object representations in brain and behavior. bioRxiv, 2022.\\n\\nGary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. In Workshop on faces in \u201cReal-Life\u201d Images: detection, alignment, and recognition, 2008.\"}"}
{"id": "mMaInr0r0c", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Emilie L Josephs, Martin N Hebart, and Talia Konkle. Emergent dimensions underlying human perception of the reachable world. *Journal of Vision*, 21(9):2154\u20132154, 2021.\\n\\nKimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. In *IEEE Winter Conference on Applications of Computer Vision (WACV)*, pp. 1548\u20131558, 2021.\\n\\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 4401\u20134410, 2019.\\n\\nTero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. *Advances in Neural Information Processing Systems*, 34:852\u2013863, 2021.\\n\\nMatthew Kay, Cynthia Matuszek, and Sean A Munson. Unequal representation and gender stereotypes in image search results for occupations. In *Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems*, pp. 3819\u20133828, 2015.\\n\\nOs Keyes. The misgendering machines: Trans/hci implications of automatic gender recognition. *Proceedings of the ACM on Human-Computer Interaction*, 2(CSCW):1\u201322, 2018.\\n\\nDavis E King. Dlib-ml: A machine learning toolkit. *The Journal of Machine Learning Research*, 10:1755\u20131758, 2009.\\n\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*, 2014.\\n\\nBernard Koch, Emily Denton, Alex Hanna, and Jacob Gates Foster. Reduced, reused and recycled: The life of a dataset in machine learning research. In *Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)*, 2021.\\n\\nArvindkumar Krishnakumar, Viraj Prabhu, Sruthi Sudhakar, and Judy Hoffman. Udis: Unsupervised discovery of bias in deep visual recognition models. In *British Machine Vision Conference (BMVC)*, volume 1, pp. 3, 2021.\\n\\nAnjana Lakshmi, Bernd Wittenbrink, Joshua Correll, and Debbie S Ma. The india face set: International and cultural boundaries impact face impressions and perceptions of category membership. *Frontiers in psychology*, 12:161, 2021.\\n\\nTom Leinster and Christina A Cobbold. Measuring diversity: the importance of species similarity. *Ecology*, 93(3):477\u2013489, 2012.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In *European conference on computer vision*, pp. 740\u2013755. Springer, 2014.\\n\\nDennis V Lindley. On a measure of the information provided by an experiment. *The Annals of Mathematical Statistics*, 27(4):986\u20131005, 1956.\\n\\nWeiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pp. 212\u2013220, 2017.\\n\\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In *International Conference on Computer Vision (ICCV)*, 2015.\\n\\nBradley C Love and Brett D Roads. Similarity as a window on the dimensions of object representation. *Trends in Cognitive Sciences*, 25(2):94\u201396, 2021.\\n\\nDebbie S Ma, Joshua Correll, and Bernd Wittenbrink. The chicago face database: A free stimulus set of faces and norming data. *Behavior research methods*, 47(4):1122\u20131135, 2015.\\n\\nDebbie S Ma, Justin Kantner, and Bernd Wittenbrink. Chicago face database: Multiracial expansion. *Behavior Research Methods*, 53(3):1289\u20131300, 2021.\"}"}
{"id": "mMaInr0r0c", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "mMaInr0r0c", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Candice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari, and Caroline Rebecca Pantofaru. A step toward more inclusive people annotations for fairness. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES), 2021.\\n\\nCarsten Schwemmer, Carly Knight, Emily D Bello-Pardo, Stan Oklobdzija, Martijn Schoonvelde, and Jeffrey W Lockhart. Diagnosing gender bias in image recognition systems. Socius, 6:2378023120967171, 2020.\\n\\nMarshall H Segall, Donald Thomas Campbell, and Melville Jean Herskovits. The influence of culture on visual perception. Bobbs-Merrill Indianapolis, 1966.\\n\\nBenny Shanon. On the similarity of features. New ideas in psychology, 6(3):307\u2013321, 1988.\\n\\nRosyl S Somai and Peter JB Hancock. Exploring perceived face similarity and its relation to image-based spaces: an effect of familiarity. Journal of Vision, 21(9):2149\u20132149, 2021.\\n\\nRyan Steed and Aylin Caliskan. Image representations learned with unsupervised pre-training contain human-like biases. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 701\u2013713, 2021.\\n\\nAndreas Veit, Serge Belongie, and Theofanis Karaletsos. Conditional similarity networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 830\u2013838, 2017.\\n\\nRaviteja Vemulapalli and Aseem Agarwala. A compact embedding for facial expression similarity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5683\u20135692, 2019.\\n\\nHao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5265\u20135274, 2018.\\n\\nMei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang. Racial faces in the wild: Reducing racial bias by information maximization adaptation network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 692\u2013702, 2019.\\n\\nKaiyu Yang, Jacqueline H Yau, Li Fei-Fei, Jia Deng, and Olga Russakovsky. A study of face obfuscation in imagenet. In International Conference on Machine Learning, pp. 25313\u201325330. PMLR, 2022.\\n\\nDong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation from scratch. arXiv preprint arXiv:1411.7923, 2014.\\n\\nDora Zhao, Angelina Wang, and Olga Russakovsky. Understanding and evaluating racial biases in image captioning. In International Conference on Computer Vision (ICCV), 2021.\\n\\nCharles Y Zheng, Francisco Pereira, Chris I Baker, and Martin N Hebart. Revealing interpretable object representations from human behavior. In International Conference on Learning Representations, 2019.\"}"}
{"id": "mMaInr0r0c", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.3.5 WHAT TIMEFRAME WAS THE DATA COLLECTED?\\nFrom 6 November 2021 to 20 February 2022.\\n\\nC.3.6 WERE ANY ETHICAL REVIEW PROCESSES CONDUCTED?\\nNo.\\n\\nC.3.7 DID YOU COLLECT THE DATA FROM THE INDIVIDUALS IN QUESTION DIRECTLY, OR OBTAIN IT VIA THIRD PARTIES OR OTHER SOURCES?\\nAll data was collected directly from the annotators via AMT.\\n\\nC.3.8 WERE THE INDIVIDUALS IN QUESTION NOTIFIED ABOUT THE DATA COLLECTION?\\nYes. Annotators were provided with (1) an information sheet and consent form; and (2) a notification and consent for collection and use of study data form.\\n\\nC.3.9 DID THE INDIVIDUALS IN QUESTION CONSENT TO THE COLLECTION AND USE OF THEIR DATA?\\nAnnotators consented to the collection, use, and publication of their age, gender identity, nationality, ancestry, and task responses.\\n\\nC.3.10 IF CONSENT WAS OBTAINED, WERE THE CONSENTING INDIVIDUALS PROVIDED WITH A MECHANISM TO REVOKE THEIR CONSENT IN THE FUTURE OR FOR CERTAIN USES?\\nAnnotators were instructed to contact Sony Europe B.V. at Taurusavenue 16, 2132LS Hoofddorp, Netherlands or privacyoffice.SEU@sony.com to revoke their consent in the future or for certain uses.\\n\\nC.3.11 HAS AN ANALYSIS OF THE POTENTIAL IMPACT OF THE DATASET AND ITS USE ON DATA SUBJECTS BEEN CONDUCTED?\\nA data privacy impact assessment was conducted.\\n\\nC.4 PREPROCESSING/ CLEANING/ LABELING\\nC.4.1 WAS ANY PREPROCESSING/ CLEANING/ LABELING OF THE DATA DONE?\\nTo control for quality, any annotator who provided overly fast, deterministic, or incomplete responses were not included in the dataset. In addition, labels for model dimensions collected from human annotators were cleaned and converted to topic labels.\\n\\nC.4.2 WAS THE \\\"RAW\\\" DATA SAVED IN ADDITION TO THE PREPROCESSED/ CLEANED/ LABELED DATA?\\nThe authors retain a copy of the raw data.\\n\\nC.4.3 IS THE SOFTWARE THAT WAS USED TO PREPROCESS/ CLEAN/ LABEL THE DATA AVAILABLE?\\nNo.\"}"}
{"id": "mMaInr0r0c", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the paper, the dataset was used to concurrently learn a face embedding space aligned with human perception as well as annotator-specific subspaces. The combination of the learned embedding space and annotator-specific subspaces not only enabled the accurate prediction of face similarity, but also provided a human-interpretable decomposition of the dimensions used in the human decision-making process, as well as the importance distinct annotators placed on each dimension. The paper demonstrated that the individual embedding dimensions (1) are related to concepts of gender, ethnicity, age, as well as face and hair morphology; and (2) can be used to collect continuous attributes, perform classification, and compare dataset attribute disparities. The paper further showed that annotators are influenced by their sociocultural backgrounds, underscoring the need for diverse annotator groups to mitigate bias.\\n\\nThe dataset could be used for any task related to human perception, human decision-making processes, metric learning, active learning, bias mitigation, or disentangled representations. The dataset contains annotations for images contained in the FFHQ dataset of Flickr images. The images currently have permissive licenses that allow free use, redistribution and adaptation for noncommercial purposes, however Flickr users are free to request the removal of any of their images contained in the FFHQ dataset by visiting https://github.com/NVlabs/ffhq-dataset. Moreover, the dataset contains annotations from a demographically imbalanced set of annotators, which may not generalize to a set of demographically dissimilar annotators. Furthermore, while the stimulus set of images for which the annotations pertain to were diverse with respect to intersectional demographic subgroup labels, the labels were inferred by a combination of algorithms and human annotators. Therefore, the stimulus set is not representative of the entire human population and may in actuality be demographically biased.\\n\\nThe dataset was created for noncommercial research purposes only. The dataset is not intended for, and should not be used for, development or improvement of facial recognition technologies.\"}"}
{"id": "mMaInr0r0c", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.6.3 When will the dataset be distributed?\\n\\nMay 2023.\\n\\nC.6.4 Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (TO)?\\n\\nThe dataset is distributed under a Creative Commons BY-NC-SA license: https://creativecommons.org/licenses/by-nc-sa/4.0/.\\n\\nC.6.5 Have any third parties imposed IP-based or other restrictions on the data associated with the instances?\\n\\nNo.\\n\\nC.6.6 Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?\\n\\nThe authors are not aware of any controls or restrictions.\\n\\nC.7 Maintenance\\n\\nC.7.1 Who will be supporting/hosting/maintaining the dataset?\\n\\nThe dataset is supported by the authors and can be accessed by visiting https://github.com/SonyAI/a_view_from_somewhere.\\n\\nC.7.2 How can the curator of the dataset be contacted (E.g., email address)?\\n\\nCorrespondence to jerone.andrews@sony.com.\\n\\nC.7.3 Is there an erratum?\\n\\nIf errors are found, an erratum file will be added to the dataset.\\n\\nC.7.4 Will the dataset be updated?\\n\\nThe dataset will be versioned.\\n\\nC.7.5 If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances?\\n\\nNo. However, annotators are free to withdraw their consent at any time and for any reason.\\n\\nC.7.6 Will older versions of the dataset continue to be supported/hosted/maintained?\\n\\nOlder versions of the dataset will continue to be hosted, unless they contain information about, or obtained from, an annotator who has subsequently withdrawn their consent.\\n\\nC.7.7 If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?\\n\\nOthers should contact the authors if they wish to contribute to the dataset.\"}"}
{"id": "mMaInr0r0c", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Face Alignment\\nFor each face dataset used in this paper, faces were detected and aligned following the procedure outlined by Or-El et al. (2020), which utilizes the dlib face detector and 68 landmark shape predictor (King, 2009).\\n\\nA.2 AVFS Models\\nAligned face images were resized to 128\u00d7128. For training, standard data augmentation was used, i.e., horizontal mirroring and 112\u00d7112 random cropping, and image values were normalized to [\u22121, 1]. Training was performed using a batch size of 128 on a single Tesla T4 GPU.\\n\\nFor the unconditional AVFS models, guided by the validation loss, we empirically set $\\\\alpha_1 = 0.00005$ and $\\\\alpha_2 = 0.01$ using grid search. The conditional models used the same hyperparameters with the additional loss term's weight set to $\\\\alpha_3 = 0.0001$. All models were optimized for 40 epochs with Adam (Kingma & Ba, 2014) (default parameters) and learning rate 0.001.\\n\\nPost-optimization, for each trained model, we obtained a core set of dimensions by removing dimensions with a maximal value (over the stimulus set) close to zero, resulting in a low-dimensional space. For the AVFS-U trained model, this resulted in 22 dimensions. The threshold for removing dimensions for all models was determined based on maximizing accuracy on the validation set. For the AVFS-U model, across five independent runs, the 95% confidence interval (CI) for the number of dimensions and validation accuracy were $27.4 \\\\pm 5.8$ and $61.9 \\\\pm 0.1\\\\%$, respectively. Figure 7 shows that the judgments can be encapsulated using a limited number of the available dimensions and that the 22 dimensions are approximately replicated across the independent runs.\\n\\nNote that the AVFS-2AFC model was trained using a triplet margin with a distance swap loss (Balntas et al., 2016) for 40 epochs with Adam (Kingma & Ba, 2014) (default parameters) and learning rate 0.001.\\n\\nA.3 BASELINE MODELS\\nAligned face images were resized to 128\u00d7128. For training, standard data augmentation was used, i.e., horizontal mirroring and 112\u00d7112 random cropping, and image values were normalized to [\u22121, 1]. Training was performed using a batch size of 512 across 4 Tesla T4 GPUs.\\n\\nFairFace models. We trained face attribute recognition models using SGD for 45 epochs on the FairFace dataset. The initial learning rate $0.1$ was divided by 10 at epoch 15, 30, and 40. $L_2$ weight decay and SGD momentum were set to $0.0005$ and $0.9$, respectively. The FairFace models were trained to predict age, gender, and ethnicity.\\n\\nCelebA models. We trained face attribute recognition models using SGD for 45 epochs on the CelebA dataset. The initial learning rate $0.1$ was divided by 10 at epoch 15, 30, and 40. $L_2$ weight decay and SGD momentum were set to $0.0005$ and $0.9$, respectively. The FairFace models were trained to predict age, gender, and ethnicity.\\n\\nValidation accuracy\\nPearson's $r$\\nSpearman's $r$\\n\\nFigure 7: AVFS-U model training across independent runs. (a) Validation accuracy as a function of the number of dimensions retained over five independent runs (95% CI), empirically evidencing that the judgments can be encapsulated using a limited number of dimensions. (b) Pearson and (c) Spearman's $r$ between each of the 22 dimensions and the best matching dimension from each of the other four runs (95% CI), empirically evidencing the reproducibility of the dimensions.\"}"}
{"id": "mMaInr0r0c", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"decay and SGD momentum were set to $0.0005$ and $0.9$, respectively. The CelebA models were trained to predict 40 binary attributes. FFHQ models. As FFHQ is an unlabeled dataset, we used age and gender labels inferred by human annotators from the FFHQ-Aging dataset. Ethnicity labels were inferred using the pretrained FairFace model described above. We trained the FFHQ models to predict the inferred age, gender, and ethnicity labels using SGD for 45 epochs. The initial learning rate $0.1$ was divided by $10$ at epoch 15, 30, and 40. L2 weight decay and SGD momentum were set to $0.0005$ and $0.9$, respectively. CASIA-WebFace models. We trained several face recognition models on the CASIA-WebFace dataset. The models differ only in terms of the loss minimized, i.e., softmax, ArcFace (Deng et al., 2019), CosFace (Wang et al., 2018), and SphereFace (Liu et al., 2017). Models were trained using SGD for 55 epochs. The initial learning rate $0.1$ was divided by $10$ at epoch 15, 30, and 40. L2 weight decay and SGD momentum were set to $0.0005$ and $0.9$, respectively. For the additional baseline models used in Appendix B, we obtained publicly available model weights for self-supervised and object recognition models. Aligned faces were preprocessed based on the model used. Self-supervised models. We used the following pretrained self-supervised models: \u2022 Self-supervised model trained using the SwA V (Caron et al., 2020) framework on ImageNet1K (Russakovsky et al., 2015)\u2014an object recognition dataset of 1.3M images over 1K object classes, where an estimated 17% of the images contain at least one human face (Yang et al., 2022). Model weights can be found at https://github.com/facebookresearch/swav. \u2022 Self-supervised models trained using the SwA V framework on IG-1B (Goyal et al., 2022)\u2014a dataset of 1B uncurated Instagram images, containing millions of images of humans. Model weights can be found at https://github.com/facebookresearch/vissl/tree/main/projects/SEER. \u2022 Self-supervised models trained using the SwA V, MoCo-v2 (Chen et al., 2020), and DINO (Caron et al., 2021) frameworks on PASS (Asano et al., 2021)\u2014a dataset of 1.3M images without people. Model weights can be found at https://github.com/yukimasano/PASS. Object recognition models. We used a object recognition model from PyTorch pretrained using a softmax loss on ImageNet1K (Russakovsky et al., 2015).\"}"}
{"id": "mMaInr0r0c", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1: Predicting similarity.\\n\\n| Data/Model Loss | #Images | #Labels | Architecture | Acc. (Same Stimuli) | r | Acc. (Novel Stimuli) | r |\\n|-----------------|---------|---------|--------------|---------------------|---|---------------------|---|\\n| Cross-entropy   |         |         | ImageNet1K   | 46.6                | 0.09 | 41.7                | 0.32 |\\n| Cross-entropy   |         |         | FairFace     | 55.9                | 0.41 | 51.9                | 0.67 |\\n| Cross-entropy   |         |         | FairFace-L   | 52.9                | 0.25 | 48.6                | 0.54 |\\n| Cross-entropy   |         |         | FairFace-BL  | 51.7                | 0.15 | 47.7                | 0.54 |\\n| Cross-entropy   |         |         | CelebA       | 52.1                | 0.25 | 48.9                | 0.56 |\\n| Cross-entropy   |         |         | CelebA-L     | 53.2                | 0.15 | 49.6                | 0.59 |\\n| Cross-entropy   |         |         | CelebA-BL    | 47.0                | 0.14 | 45.9                | 0.47 |\\n| Cross-entropy   |         |         | FFHQ         | 50.5                | 0.16 | 47.3                | 0.49 |\\n| Cross-entropy   |         |         | FFHQ-L       | 53.5                | 0.31 | 50.5                | 0.62 |\\n| Cross-entropy   |         |         | FFHQ-BL      | 51.0                | 0.23 | 47.3                | 0.54 |\\n| ArcFace         | 404K    | 404K    | ResNet18     | 51.6                | 0.29 | 46.1                | 0.40 |\\n| Cross-entropy   | 404K    | 404K    | ResNet18     | 48.6                | 0.21 | 43.2                | 0.34 |\\n| SphereFace      | 404K    | 404K    | ResNet18     | 48.3                | 0.25 | 43.8                | 0.35 |\\n| CosFace         | 404K    | 404K    | ResNet18     | 44.0                | 0.12 | 40.8                | 0.23 |\\n| 5K              | 574K    |         | ResNet18     | 67.4                | 0.68 | 61.7                | 0.82 |\\n| 5K              | 574K    |         | ResNet18     | 66.5                | 0.68 | 61.4                | 0.82 |\\n| 5K              | 574K    |         | ResNet18     | 62.0                | 0.65 | 57.5                | 0.86 |\\n| 5K              | 287K    |         | ResNet18     | 61.3                | 0.61 | 55.8                | 0.81 |\\n| 5K              | 144K    |         | ResNet18     | 60.6                | 0.56 | 55.3                | 0.80 |\\n| 5K              | 72K     |         | ResNet18     | 58.6                | 0.47 | 55.0                | 0.79 |\\n| A               | 5K      | 574K    | ResNet18     | 60.2                | 0.46 | 52.8                | 0.64 |\\n\\nThe identifiers were randomly generated to preserve anonymity. Refer to Section C.1 for details regarding the dataset construction and funding.\"}"}
{"id": "mMaInr0r0c", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Choose the person that looks least similar to the two other people. Focus on the people, ignoring differences in head position, facial expression, lighting, accessories, background, and objects.\\n\\nContext\\n\\nTriplet\\n\\nFigure 8: Odd-one-out 3AFC task. Annotators were tasked with choosing the person that looks least similar to the two other people. Context makes salient context-related properties and the extent to which faces being compared share these properties. By exchanging a face in a triplet (i.e., altering the context), each judgment implicitly encodes the dimensions relevant to pairwise similarity. All face images in the figure were replaced by synthetic StyleGAN3 (Karras et al., 2021) images for privacy reasons; annotators were shown real images.\\n\\nWhich visual characteristics do you think the people on the left-hand side of the grid have in common compared to the people on the right-hand side of the grid?\\n\\nFigure 9: Dimension labeling task. Illustrative example of a face image grid for model dimension $j$ shown to annotators, displaying the stimulus set of 4,921 faces sorted in descending order (from left to right) based on their dimension $j$ embedding value. Annotators were tasked with writing 1\u20133 descriptions based on the characteristics they thought were changing along the grid. Annotators were instructed to view and interact with the entire grid before providing their descriptions. Consistent descriptions from distinct annotators would suggest that the ordering of the grid is visually meaningful. All face images in the figure were replaced by synthetic StyleGAN3 images for privacy reasons; annotators were shown real images.\\n\\nthe privacy of the annotators) by the authors and the demographic attributes correspond to the self-reported age, nationality, ancestry, and gender identity of an annotator.\\n\\nC.2.2 HOW MANY INSTANCES ARE THERE IN TOTAL?\\n\\nThe dataset contains:\\n\\n\u2022 638,180 quality-controlled 3AFC instances for training and validation; and 24,060 + 80,300 quality-controlled 3AFC instances for testing.\\n\\n\u2022 738 quality-controlled human-generated topic labels that capture the meaning of a set of learned embedding dimensions.\\n\\n\u2022 8,800 quality-controlled human-generated image ratings along a set of learned embedding dimensions.\"}"}
{"id": "mMaInr0r0c", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A View From Somewhere: Human-centric Face Representations\\n\\nJerone T. A. Andrews\\nSony AI, Tokyo\\n\\nPrzemys\u0142aw Joniak\\nUniversity of Tokyo, Tokyo\\n\\nAlice Xiang\\nSony AI, New York\\n\\nABSTRACT\\n\\nFew datasets contain self-identified sensitive attributes, inferring attributes risks introducing additional biases, and collecting attributes can carry legal risks. Besides, categorical labels can fail to reflect the continuous nature of human phenotypic diversity, making it difficult to compare the similarity between same-labeled faces. To address these issues, we present A View From Somewhere (A VFS)\u2014a dataset of 638,180 human judgments of face similarity.\\n\\n1 We demonstrate the utility of A VFS for learning a continuous, low-dimensional embedding space aligned with human perception. Our embedding space, induced under a novel conditional framework, not only enables the accurate prediction of face similarity, but also provides a human-interpretable decomposition of the dimensions used in the human-decision making process, and the importance distinct annotators place on each dimension.\\n\\nWe additionally show the practicality of the dimensions for collecting continuous attributes, performing classification, and comparing dataset attribute disparities.\\n\\n1 INTRODUCTION\\n\\nThe canonical approach to evaluating human-centric image dataset diversity is based on demographic attribute labels. Many equate diversity with parity across the subgroup distributions (Kay et al., 2015; Schwemmer et al., 2020), presupposing access to demographically labeled samples. However, most datasets are web scraped, lacking ground-truth information about image subjects (Andrews et al., 2023). Moreover, data protection legislation considers demographic attributes to be personal information and limits their collection and use (Andrus et al., 2021; 2020).\\n\\nEven when demographic labels are known, evaluating diversity based on subgroup counts fails to reflect the continuous nature of human phenotypic diversity (e.g., skin tone is often reduced to \u201clight\u201d vs. \u201cdark\u201d). Further, even within the same subpopulation, image subjects exhibit certain traits to a greater or lesser extent than others (Becerra-Riera et al., 2019; Carcagn\u00ec et al., 2015; Feliciano, 2016). When labels are unknown, researchers typically choose certain attributes they consider to be relevant for human diversity and use human annotators to infer them (Karkkainen & Joo, 2021; Wang et al., 2019). Inferring labels, however, is difficult, especially for nebulous social constructs, e.g., race and gender (Hanna et al., 2020; Keyes, 2018), and can introduce additional biases (Freeman et al., 2011). Beyond the inclusion of derogatory categories (Koch et al., 2021; Birhane & Prabhu, 2021; Crawford & Paglen, 2019), label taxonomies often do not permit multi-group membership, resulting in the erasure of, e.g., multi-ethnic individuals (Robinson et al., 2020; Karkkainen & Joo, 2021).\\n\\nSignificantly, discrepancies between inferred and self-identified attributes can induce psychological distress by invalidating an individual\u2019s self-image (Campbell & Troyer, 2007; Roth, 2016).\\n\\nIn this work, we avoid problematic semantic labels altogether and propose to learn a face embedding space aligned with human perception. To do so, we introduce A View From Somewhere (A VFS)\u2014a dataset of 638,180 face similarity judgments over 4,921 faces. Each judgment corresponds to the odd-one-out (i.e., least similar) face in a triplet of faces and is accompanied by both the identifier and demographic attributes of the annotator who made the judgment.\\n\\nOur embedding space, induced under a novel conditional framework, not only enables the accurate prediction of face similarity, but also provides a human-interpretable decomposition of the dimensions used in the human-decision making process, and the importance distinct annotators place on each dimension.\\n\\nWe additionally show the practicality of the dimensions for collecting continuous attributes, performing classification, and comparing dataset attribute disparities.\"}"}
{"id": "mMaInr0r0c", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"provides a human-interpretable decomposition of the dimensions used in the human decision-making process, as well as the importance distinct annotators place on each dimension. We demonstrate that the individual embedding dimensions (1) are related to concepts of gender, ethnicity, age, as well as face and hair morphology; and (2) can be used to collect continuous attributes, perform classification, and compare dataset attribute disparities. We further show that annotators are influenced by their sociocultural backgrounds, underscoring the need for diverse annotator groups to mitigate bias.\\n\\nSimilarity. The human mind is conjectured to have, \u201ca considerable investment in similarity\u201d (Medin et al., 1993). When two objects are compared they mutually constrain the set of features that are activated in the human mind (Markman, 1996)\u2014i.e., features are dynamically discovered and aligned based on what is being compared. Alignment is contended to be central to similarity comparisons. Shanon (1988) goes as far as arguing that similarity is not a function of features, but that the features themselves are a function of the similarity comparison.\\n\\nContextual similarity. Human perception of similarity can vary with respect to context (Roth & Shoben, 1983). Context makes salient context-related properties and the extent to which objects being compared share these properties (Medin et al., 1993; Markman & Gentner, 2005; Goodman, 1972). For example, Barsalou (1987) found that \u201csnakes\u201d and \u201craccoons\u201d were judged to be more similar when placed in the context of pets than when no context was provided. The odd-one-out similarity task (Zheng et al., 2019) used in this work also provides context. By varying the context (i.e., the third object) in which two objects are experienced, it is possible to uncover different features that contribute to their pairwise similarity. This is important as there are an uncountable number of ways in which two objects may be similar (Love & Roads, 2021).\\n\\nPsychological embeddings. Multidimensional scaling (MDS) is often used to learn psychological embeddings from human similarity judgments (Zheng et al., 2019; Roads & Love, 2021; Dima et al., 2022; Josephs et al., 2021). As MDS approaches cannot embed images outside of the training set, researchers have used pretrained models as feature extractors (Sanders & Nosofsky, 2020; Peterson et al., 2018; Attarian et al., 2020), which can introduce unwanted implicit biases (Krishnakumar et al., 2021; Steed & Caliskan, 2021). Moreover, previous approaches ignore inter- and intra-annotator variability. By contrast, our conditional model is trained end-to-end and can embed any arbitrary face from the perspective of a specific annotator.\\n\\nFace datasets. Most face datasets are semantically labeled images, created for the purposes of identity and attribute recognition (Karkkainen & Joo, 2021; Liu et al., 2015; Huang et al., 2008; Cao et al., 2018). An implicit assumption is that semantic similarity is equivalent to visual similarity (Deselaers & Ferrari, 2011). However, many semantic categories are functional (Rosch, 1975; Rothbart & Taylor, 1992), i.e., unconstrained by visual features such as shape, color, and material. Moreover, semantic labels often only indicate the presence or absence of an attribute, as opposed to its magnitude, making it difficult to compare the similarity between same-labeled samples. While face similarity datasets exist, the judgments narrowly pertain to identity (McCauley et al., 2021; Sadovnik et al., 2018; Somai & Hancock, 2021) and expression similarity (Vemulapalli & Agarwala, 2019).\\n\\nAnnotator positionality. Semantic categorization by annotators not only depends on the image subject, but also on extrinsic contextual cues (Freeman et al., 2011) and the annotators\u2019 sociocultural backgrounds (Segall et al., 1966). Despite this, annotator positionality is rarely discussed in computer vision (Chen & Joo, 2021; Zhao et al., 2021; Denton et al., 2021); \u201conly five publications [from 113] provided any [annotator] demographic information\u201d (Scheuerman et al., 2021). To our knowledge, A VFS represents the first human-centric vision dataset, where each annotation is associated with the annotator who created it and their demographics, permitting the study of annotator bias.\\n\\nTo learn a face embedding space aligned with human perception, we collect A VFS\u2014a dataset of odd-one-out similarity judgments collected from humans. An odd-one-out judgment corresponds to the least similar face in a triplet of faces, representing a three-alternative forced choice (3AFC) task. A VFS dataset documentation can be found in Appendix C.\"}"}
{"id": "mMaInr0r0c", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Face image stimulus set.\\n\\n4,921 near-frontal faces with limited eye occlusions and an apparent age > 19 years old were sampled from the CC-BY licensed FFHQ (Karras et al., 2019) dataset. The subset was obtained by (1) splitting FFHQ into 56 partitions based on inferred intersectional group labels; and (2) randomly sampling from each partition with equal probability. Ethnicity was estimated using a FairFace model (Karkkainen & Joo, 2021); and binary gender expression and age group labels were obtained from FFHQ-Aging crowdsourced human annotations (Or-El et al., 2020).\\n\\n3AFC similarity judgments.\\n\\nA VFS contains 638,180 quality-controlled triplets over 4,921 faces, representing 0.003% of all possible triplets. To focus judgments on intrinsic face-varying features, for each presented triplet, annotators were instructed to choose the person that looks least similar to the two other people, while ignoring differences in pose, expression, lighting, accessories, background, and objects. Each A VFS triplet is labeled with a judgment, as well as the identifier of the annotator who made the judgment and the annotator's self-reported age, nationality, ancestry, and gender identity. As in previous non-facial odd-one-out datasets (Josephs et al., 2021; Hebart et al., 2022; Dima et al., 2022), there is a single judgment per triplet. Quality was controlled by excluding judgments from annotators who provided overly fast, deterministic, or incomplete responses. In total, 1,645 annotators contributed to A VFS via Amazon Mechanical Turk (AMT) and provided consent to use their study data. Compensation was 15 USD per hour.\\n\\n3AFC task rationale.\\n\\nLet $x \\\\in X$ and $\\\\text{sim} : ((x_i, x_j)) \\\\rightarrow \\\\text{sim}(i, j) \\\\in \\\\mathbb{R}$ denote a face image and a similarity function, respectively. Our motivation for collecting A VFS is fourfold. Most significantly, the odd-one-out task does not require an annotator to explicitly categorize people. Second, for a triplet $(x_i, x_j, x_k)$, repeatedly varying $x_k$ permits the identification of the relevant dimensions that contribute to $\\\\text{sim}(i, j)$. That is, w.l.o.g., $x_k$ provides context for which $\\\\text{sim}(i, j)$ is determined, making the task easier than explicit pairwise similarity tasks (i.e., \u201cIs $x_i$ similar to $x_j$?\u201d). This is because it is not always apparent to an annotator which dimensions are relevant when determining $\\\\text{sim}(i, j)$, especially when $x_i$ and $x_j$ are perceptually different. Third, there is no need to prespecify attribute lists hypothesized as relevant for comparison (e.g., \u201cIs $x_i$ older than $x_j$?\u201d). The odd-one-task implicitly encodes salient attributes that are used to determine similarity. Finally, compared to 2AFC judgments for triplets composed of an anchor (i.e., reference point), $x_a$, a positive, $x_p$, and a negative, $x_n$, 3AFC judgments naturally provide more information. 3AFC triplets require an annotator to determine $\\\\{\\\\text{sim}(i, j), \\\\text{sim}(i, k), \\\\text{sim}(j, k)\\\\}$, whereas 2AFC triplets with anchors only necessitate the evaluation of $\\\\{\\\\text{sim}(a, p), \\\\text{sim}(a, n)\\\\}$.\\n\\n4 MODEL OF CONDITIONAL DECISION-MAKING\\n\\nZheng et al. (2019) developed an MDS approach for learning continuous, non-negative, sparse embeddings from odd-one-out judgments. The approach has been shown to offer an interpretable window into the dimensions in the human mind of object categories (Hebart et al., 2020), human actions (Dima et al., 2022), and reachspace environments (Josephs et al., 2021) such that dimensional values indicate feature magnitude. The method is based on three assumptions: (1) embeddings can be learned solely from odd-one-out judgments; (2) judgments are a function of $\\\\{\\\\text{sim}(i, j), \\\\text{sim}(i, k), \\\\text{sim}(j, k)\\\\}$; and (3) judgments are stochastic, where the probability of selecting $x_k$ is $p(k|a) \\\\propto \\\\exp(\\\\text{sim}_a(i, j))$.\\n\\n(1) Conditional similarity.\\n\\nAt a high-level, we want to apply Zheng et al. (2019)\u2019s MDS approach to learn face embeddings. However, the method (1) cannot embed data outside of the training set, limiting its utility; and (2) pools all judgments, disregarding intra- and inter-annotator stochasticity. Therefore, we instead propose to learn a conditional convolutional neural net (convnet).\\n\\nLet $\\\\{(\\\\{x_i^\\\\ell, x_j^\\\\ell, x_k^\\\\ell\\\\}, k^\\\\ell, a)\\\\}_{n^\\\\ell=1}$ denote a training set of $(triplet, judgment, annotator)$ tuples, where $a \\\\in A$. To simplify notation, we assume that judgments always correspond to index $k^\\\\ell$.\\n\\nSuppose $f: x \\\\rightarrow f(x) = w \\\\in \\\\mathbb{R}^d$ is a convnet, parametrized by $\\\\Theta$, where $w \\\\in \\\\mathbb{R}^d$ is an embedding of $x$. In contrast to Zheng et al. (2019), we model the probability of annotator $a$ selecting $k^\\\\ell$ as $p(k^\\\\ell|a) \\\\propto \\\\exp(\\\\text{sim}_a(i^\\\\ell, j^\\\\ell))$, (2) where $\\\\text{sim}_a(\\\\cdot, \\\\cdot)$ denotes the internal similarity function of $a$. Given two images $x_i$ and $x_j$, we define their similarity according to $a$ as:\\n\\n$$\\\\text{sim}_a(i, j) = (\\\\sigma(\\\\phi_a) \\\\odot \\\\text{ReLU}(w_i))^\\\\top \\\\cdot (\\\\sigma(\\\\phi_a) \\\\odot \\\\text{ReLU}(w_j)),$$\\n\\n(3)\"}"}
{"id": "mMaInr0r0c", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nAVFS-U\\nCelebA\\nFairFace\\nFFHQ\\nModel was trained on the test data\\n\\nCFD: \u201cmale\u201d\\nCasCon: \u201cmale\u201d\\nCelebA: \u201cmale\u201d\\nCOCO: \u201cmale\u201d\\nMIAP: \u201cmale\u201d\\nFFHQ: \u201cmale\u201d\\n\\nCasCon: \u201c70+\u201d\\nFFHQ: \u201c70+\u201d\\nCelebA: \u201csmiling\u201d\\n\\nCFD: \u201chappy\u201d\\n\\nCFD: \u201ceast asian\u201d\\n\\nCFD: \u201cblack\u201d\\n\\nCFD: \u201cwhite\u201d\\n\\nCFD: \u201cindian\u201d\\nCasCon: \u201clight skin\u201d\\nCOCO: \u201clight skin\u201d\\nCelebA: \u201cbalding\u201d\\n\\nFigure 5: Semantic classification.\\n\\nAUC results are shown for binary attribute classification using continuous attribute value predictions. Our AVFS-U model is competitive with the semantically trained attribute recognition model baselines, despite not having been trained on semantic labels.\\n\\n2014), OpenImages MIAP (MIAP) (Schumann et al., 2021), CFD, FFHQ, CelebA (Liu et al., 2015), and Casual Conversations (CasCon) (Hazirbas et al., 2021). Continuous attribute value predictions are computed in the same manner as in the prototypicality experiment. Results are shown in Figure 5.\\n\\nWe observe that AVFS-U dimensions are competitive with semantically trained recognition models, even on challenging unconstrained data (e.g., COCO, MIAP).\\n\\nAttribute disparity estimation.\\n\\nNext we investigate the utility of individual embedding dimensions for the task of comparing dataset attribute disparities. Suppose we are given \\\\( n \\\\) subsets \\\\( \\\\{S_1, \\\\ldots, S_n\\\\} \\\\) sampled from \\\\( S^* \\\\), where, \\\\( \\\\forall c \\\\in \\\\{1, \\\\ldots, n\\\\}, r_c \\\\in [0.0, 0.01, \\\\ldots, 0.99, 1] \\\\) and \\\\( 1 - r_c \\\\) denote the ground-truth proportion of \\\\( S_c = \\\\{ (x_i, y_i) \\\\}_{i} \\\\), labeled \\\\( y = 0 \\\\) and \\\\( y = 1 \\\\), respectively. We aim to find \\\\( c \\\\) with the lowest ground-truth attribute disparity, i.e.,\\n\\n\\\\[\\n\\\\arg \\\\min_c \\\\Delta(c) = \\\\arg \\\\min_c \\\\text{abs}(2r_c - 1).\\n\\\\]\\n\\nWe define \\\\( \\\\text{sim}(i, j) = \\\\text{abs}(\\\\hat{y}_i - \\\\hat{y}_j) - 1 \\\\) as the similarity between \\\\( x_i \\\\) and \\\\( x_j \\\\), where \\\\( \\\\hat{y} \\\\in \\\\mathbb{R} \\\\) is a predicted continuous attribute value for ground-truth \\\\( y \\\\in \\\\{0, 1\\\\} \\\\) given \\\\( x \\\\). \\\\( \\\\hat{y} \\\\) is computed in the same manner as in the prototypicality and semantic classification experiments. Let \\\\( K \\\\in \\\\mathbb{R}^{m \\\\times m} \\\\) denote a similarity matrix, where \\\\( m \\\\) denotes the number of images and \\\\( K_{i,j} = \\\\text{sim}(i, j) \\\\). The average similarity of samples in a set \\\\( S_c \\\\), is \\\\( \\\\propto \\\\sum_i \\\\sum_j K_{i,j} \\\\) (Leinster & Cobbold, 2012). We estimate \\\\( c \\\\) as follows:\\n\\n\\\\[\\n\\\\arg \\\\min_c \\\\hat{\\\\Delta}(c) = \\\\arg \\\\min_c |S_c| - 2\\\\sum_i \\\\sum_j K_{i,j}.\\n\\\\]\\n\\nIn all experiments, we set \\\\( n = 100 \\\\) and \\\\( m = 100 \\\\). For \\\\( S^* \\\\), we use the following labeled face datasets: COCO, MIAP, CFD, FFHQ, CelebA, and CasCon. Averaging over 100 runs, we report (1) \\\\( \\\\Delta(\\\\arg \\\\min_c \\\\hat{\\\\Delta}(c)) \\\\); and (2) Spearman\u2019s \\\\( r \\\\) between \\\\( \\\\{\\\\hat{\\\\Delta}(1), \\\\ldots, \\\\hat{\\\\Delta}(n)\\\\} \\\\) and \\\\( \\\\{\\\\Delta(1), \\\\ldots, \\\\Delta(n)\\\\} \\\\).\\n\\nResults are shown in Figure 6. Despite not being trained on semantically labeled data, we observe that AVFS-U dimensions are competitive with the baselines, in particular AVFS-U disparity scores are highly correlated with ground-truth disparity scores.\\n\\nDISCUSSION\\n\\nMotivated by legal, technical, and ethical considerations associated with semantic labels, we introduced AVFS\u2014a new dataset of 3AFC face similarity judgments over 4,921 faces. We demonstrated the utility of AVFS for learning a continuous, low-dimensional embedding space aligned with human perception. Our embedding space, induced under a novel conditional framework, not only enables the accurate prediction of face similarity, but also provides a human-interpretable decomposition of the dimensions used in the human-decision making process, and the importance distinct annotators place on each dimension. We further showed the practicality of the dimensions for collecting continuous attributes, performing classification, and comparing dataset attribute disparities.\\n\\nSince AVFS only contains 0.003% of all possible triplets over 4,921 faces, future triplets could be actively sampled (Lindley, 1956; MacKay, 1992) so as to learn additional face-varying dimensions.\"}"}
{"id": "mMaInr0r0c", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Attribute disparity estimation. Average attribute disparity (y-axis) in data subsets selected as the most diverse and average Spearman\u2019s $r$ (x-axis) between the ground-truth disparity for each subset considered and model-estimated disparity scores. Results highlight that disparity scores derived using AVFS-U model dimensions are highly correlated with ground-truth disparities.\\n\\nOur work is not without its limitations. First, we implicitly assumed that the stimulus set was diverse. Therefore, our current proposal is limited to the proof of concept that we can learn face-varying dimensions that are both present within the stimuli and salient to human judgments of face similarity. Our stimulus set was sampled such that the faces were diverse with respect to inferred demographic subgroup labels. Ideally, these labels would have been self-reported. With the principle of data minimization in mind, a potential solution would be to collect stimuli from unique individuals that differ in terms of their country of residence, providing both identity- and geographic-based label diversity. Notably, country of residence information additionally helps to ensure that each image subject\u2019s data is protected appropriately (Andrews et al., 2023). Second, the number of (near) nonzero dimensions depends on the sparsity penalty weight, i.e., $\\\\alpha$ in Equation (4), which must be carefully chosen. We erred on the side of caution (lower penalty) to avoid merging distinct factors, which would have reduced interpretability. Third, the emergence of a \u201csmiling expression\u201d and \u201cneutral expression\u201d dimension indicate that annotators did not always follow our instruction to ignore differences in facial expression. While this may be considered a limitation, our model was able to reveal the use of these attributes, providing a level of transparency not offered by existing face datasets. Finally, our annotator pool was demographically imbalanced, which is an unfortunate consequence of using AMT.\\n\\nDespite these limitations, we hope that our work inspires others to pursue unorthodox tasks for learning the continuous dimensions of human diversity, which do not rely on problematic semantic labels. Moreover, we aim to increase discourse on annotator positionality. As the philosopher Thomas Nagel suggested, it is impossible to take a \u201cview from nowhere\u201d (Nagel, 1989). Therefore, we need to make datasets more inclusive by integrating a diverse set of perspectives from their inception.\"}"}
{"id": "mMaInr0r0c", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A VFS contains annotations from a demographically imbalanced set of annotators, which may not generalize to a set of demographically dissimilar annotators. Moreover, while the stimulus set of images for which the annotations pertain to were diverse with respect to intersectional demographic subgroup labels, the labels were inferred by a combination of algorithms and human annotators. (Refer to the dataset documentation in Appendix C for details on the stimulus set and annotator distributions.) Therefore, the stimulus set is not representative of the entire human population and may in actuality be demographically biased. Although we provided evidence that models trained on A VFS generalize to novel stimuli, it is conceivable that the models could perform poorly on faces with attributes that were not salient to human judgment behavior on triplets sampled from, or represented in, the stimulus set of images. Future work could explore active learning so as to reveal additional attributes that are relevant to human judgments of face similarity and expanding the stimulus set. Furthermore, the stimulus set (sampled from the permissively licensed FFHQ dataset) was collected without image subject consent. Reliance on such data is an overarching problem in the computer vision research community, due to a lack of consensually collected, unconstrained, and diverse human-centric image data. To preserve the privacy of the image subjects, all face images presented in this paper have been replaced by StyleGAN3 generated faces with similar embedding values.\\n\\nTo demonstrate that A VFS judgments encode discriminative information, we validated our learned dimensions in terms of their ability to estimate attribute disparities, as well as to predict binary attributes and concept prototypicality. Our results show that discriminative attribute classifiers can be distilled solely from learning to predict similarity judgments, underscoring that we do not need to explicitly collect face attribute labels from human annotators. Note however, for social constructs (e.g., gender, race) and emotions, we do not condone classification, as it reifies the idea that an individual's social identity or internal state can be inferred from images alone.\\n\\n**Reproducibility Statement**\\n\\nData and code are publicly available under a Creative Commons license (CC-BY-NC-SA), permitting noncommercial use cases at [https://github.com/SonyAI/a_view_from_somewhere](https://github.com/SonyAI/a_view_from_somewhere).\\n\\n**Acknowledgments**\\n\\nThis work was funded by Sony AI.\"}"}
{"id": "mMaInr0r0c", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"REFERENCES\\n\\nJerone TA Andrews, Dora Zhao, William Thong, Apostolos Modas, Orestis Papakyriakopoulos, Shruti Nagpal, and Alice Xiang. Ethical considerations for collecting human-centric image datasets. arXiv preprint arXiv:2302.03629, 2023.\\n\\nMcKane Andrus, Elena Spitzer, and Alice Xiang. Working to address algorithmic bias? don\u2019t overlook the role of demographic data. Partnership on AI. Retrieved from https://www.partnershiponai.org/demographic-data, 2020.\\n\\nMcKane Andrus, Elena Spitzer, Jeffrey Brown, and Alice Xiang. What we can\u2019t measure, we can\u2019t understand: Challenges to demographic data procurement in the pursuit of fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 249\u2013260, 2021.\\n\\nYuki M Asano, Christian Rupprecht, Andrew Zisserman, and Andrea Vedaldi. Pass: An imagenet replacement for self-supervised pretraining without humans. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.\\n\\nIoanna Maria Attarian, Brett D Roads, and Michael Curtis Mozer. Transforming neural network visual representations to predict human judgments of similarity. In NeurIPS 2020 Workshop SVRHM, 2020.\\n\\nVassileios Balntas, Edgar Riba, Daniel Ponsa, and Krystian Mikolajczyk. Learning local feature descriptors with triplets and shallow convolutional neural networks. In BMVC, volume 1, pp. 3, 2016.\\n\\nLawrence Barsalou. The instability of graded structure: Implications for the nature of concepts. 1987.\\n\\nFabiola Becerra-Riera, Annette Morales-Gonz\u00e1lez, and Heydi M\u00e9ndez-V\u00e1zquez. A survey on facial soft biometrics for video surveillance and forensic applications. Artificial Intelligence Review, 52(2):1155\u20131187, 2019.\\n\\nSebastian Benthall and Bruce D Haynes. Racial categories in machine learning. In Proceedings of the conference on fairness, accountability, and transparency, pp. 289\u2013298, 2019.\\n\\nAbeba Birhane and Vinay Uday Prabhu. Large image datasets: A pyrrhic win for computer vision? In 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1536\u20131546. IEEE, 2021.\\n\\nMary E Campbell and Lisa Troyer. The implications of racial misclassification by observers. American Sociological Review, 72(5):750\u2013765, 2007.\\n\\nQiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. Vggface2: A dataset for recognising faces across pose and age. In 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018), pp. 67\u201374. IEEE, 2018.\\n\\nPierluigi Carcagn\u00ec, Marco Del Coco, Dario Cazzato, Marco Leo, and Cosimo Distante. A study on different experimental configurations for age, race, and gender estimation problems. EURASIP Journal on Image and Video Processing, 2015(1):1\u201322, 2015.\\n\\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912\u20139924, 2020.\\n\\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9650\u20139660, 2021.\\n\\nStevie Chancellor, Eric PS Baumer, and Munmun De Choudhury. Who is the \u201chuman\u201d in human-centered machine learning: The case of predicting mental health from social media. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW):1\u201332, 2019.\\n\\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.\"}"}
{"id": "mMaInr0r0c", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ \\\\sigma(\\\\cdot) \\\\] is the sigmoid function and \\\\( \\\\sigma(\\\\phi_a) \\\\in [0,1] \\\\) is a mask associated with \\\\( a \\\\). Each mask plays the role of an element-wise gating function, encoding the importance \\\\( a \\\\) places on each of the \\\\( d \\\\) embedding dimensions when determining similarity. Conditioning prediction on \\\\( a \\\\) induces subspaces that encode each annotator\u2019s notion of similarity, permitting us to study whether per-dimensional importance weights differ between \\\\( a \\\\in A \\\\).\\n\\nVeit et al. (2017) employed a similar procedure to learn subspaces which encode different notions of similarity (e.g., font style, character type).\\n\\n**Conditional loss.**\\n\\nWe denote by \\\\( \\\\Phi = [\\\\phi_1^\\\\top, \\\\ldots, \\\\phi_{|A|}^\\\\top] \\\\in \\\\mathbb{R}^{d \\\\times |A|} \\\\) a trainable weight matrix, where each column vector \\\\( \\\\phi_a^\\\\top \\\\) corresponds to annotator \\\\( a \\\\)\u2019s mask prior to applying \\\\( \\\\sigma(\\\\cdot) \\\\). In our conditional framework, we jointly optimize \\\\( \\\\Theta \\\\) and \\\\( \\\\Phi \\\\) by minimizing:\\n\\n\\\\[\\n- \\\\sum_k \\\\log [\\\\hat{p}(k|a)] + \\\\alpha_1 \\\\sum_i \\\\| \\\\text{ReLU}(w_i) \\\\|_1 + \\\\alpha_2 \\\\sum_{ij} \\\\text{ReLU}(w_i - w_j) + \\\\alpha_3 \\\\sum_a \\\\|\\\\phi_a\\\\|_2^2 ,\\n\\\\]\\n\\n(4)\\n\\nwhere \\\\( \\\\hat{p}(k|a) = \\\\exp(\\\\text{sim}_a(i, j)) / \\\\left( \\\\exp(\\\\text{sim}_a(i, j)) + \\\\exp(\\\\text{sim}_a(i, k)) + \\\\exp(\\\\text{sim}_a(j, k)) \\\\right) \\\\) (5) is the predicted probability that \\\\( k \\\\) is the odd-one-out conditioned on \\\\( a \\\\). The first term in Equation (4) encourages similar face pairs to result in large dot products. The second term (modulated by \\\\( \\\\alpha_1 \\\\in \\\\mathbb{R} \\\\)) promotes sparsity. The third term (modulated by \\\\( \\\\alpha_2 \\\\in \\\\mathbb{R} \\\\)) supports interpretability by penalizing negative values. The last term (modulated by \\\\( \\\\alpha_3 \\\\in \\\\mathbb{R} \\\\)) penalizes large weights.\\n\\nOur conditional decision-making framework is applicable to any task that involves mapping from inputs to decisions made by humans; it only requires record-keeping during data collection such that each annotation is associated with the annotator who generated it.\\n\\n**Implementation.**\\n\\nWhen \\\\( \\\\alpha_3 = 0 \\\\) and \\\\( \\\\sigma(\\\\phi_a) = [1, \\\\ldots, 1] \\\\), \\\\( \\\\forall a \\\\), Equation (4) corresponds to the unconditional MDS objective proposed by Zheng et al. (2019). We refer to unconditional and conditional models trained on A VFS as A VFS-U and A VFS-C, respectively, and conditional models whose masks are learned post hoc as A VFS-CPH. An A VFS-CPH model uses a fixed unconditional model (i.e., A VFS-U) as a feature extractor to obtain face embeddings such that only \\\\( \\\\Phi \\\\) is trainable.\\n\\nA VFS models have ResNet18 (He et al., 2016) architectures and output 128-dimensional embeddings. We use the Adam (Kingma & Ba, 2014) optimizer with default parameters, reserving 10% of A VFS for validation. Based on grid search, we empirically set \\\\( \\\\alpha_1 = 0.00005 \\\\) and \\\\( \\\\alpha_2 = 0.01 \\\\). For A VFS-C and A VFS-CPH, we additionally set \\\\( \\\\alpha_3 = 0.00001 \\\\). Across independent runs, we find that only a fraction of the 128 dimensions are needed and individual dimensions are reproducible. Post-optimization, we remove dimensions with maximal values (over the stimulus set) close to zero. For A VFS-U, this results in 22 dimensions (61.9% validation accuracy). Note that we observe that 8/22 dimensions have a Pearson\u2019s \\\\( r \\\\) correlation > 0.9 with another dimension. Refer to Appendix A for further implementation details.\\n\\n**EXPERIMENTS**\\n\\nThe aim of this work is to learn a face embedding space aligned with human perception. To highlight the value of A VFS, we evaluate the performance of A VFS embeddings and conditional masks for predicting face similarity judgments, as well as whether they reveal (1) a human-interpretable decomposition of the dimensions used in the human decision-making process; and (2) the importance distinct annotators place on each dimension. We further assess the practicality of the dimensions for collecting continuous attributes, classification, and comparing dataset attribute disparities.\\n\\n**Embedding methods.**\\n\\nWhere appropriate, for comparison, we consider embeddings extracted from face identity verification and face attribute recognition models: CASIA-WebFace (Yi et al., 2014) is a face verification model trained with an ArcFace (Deng et al., 2019) loss on images from 11K identities; CelebA (Liu et al., 2015) is a face attribute model trained to predict 40 binary attribute labels (e.g., \u201cpale skin\u201d, \u201cyoung\u201d, \u201cmale\u201d); and FairFace (Karkkainen & Joo, 2021) and FFHQ are face attribute models trained to predict gender, age, and ethnicity. All models have ResNet18 architectures and output 128-dimensional embeddings (prior to the classification layer). All baseline embeddings are normalized to unit-length, where the dot product of two embeddings determines their similarity. Additional details on the datasets and baselines are presented in Appendix B.\"}"}
{"id": "mMaInr0r0c", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Predicting similarity (same stimuli).\\n\\nWe analyze whether our model results in stimulus set embeddings such that we are able to predict human 3AFC judgments not observed during learning. Using images from the stimulus set of 4,921 faces, we generate 1,000 novel triplets and collect 22\u201325 unique judgments per triplet (24,060 judgments) on AMT. As we have 22\u201325 judgments per triplet, we can reliably estimate odd-one-out probabilities. The Bayes optimal classifier accuracy corresponds to the best possible accuracy any model could achieve given the stochasticity in the judgments. The classifier makes the most probable prediction, i.e., the majority judgment. Thus, its accuracy is equal to the mean majority judgment probability over the 1,000 triplets, corresponding to 65.5 \u00b1 1% (95% CI).\\n\\nIn addition to accuracy, we report Spearman's $r$ correlation between the entropy in human- and model-generated triplet odd-one-out probabilities. Human-generated odd-one-out probabilities are of the form: \\n\\n$$(n_i, n_j, n_k)/n,$$ where, without loss of generality, $n_k/n$ corresponds to the fraction of $n$ odd-one-out votes for $k$.\\n\\nResults are shown in Figure 1. We observe that A VFS models outperform the baselines with respect to accuracy, while better reflecting human perceptual judgment uncertainty (i.e., entropy correlation). This holds as we vary the number of A VFS training triplets, for example, to as few as 12.5% (72K) as evidenced by the A VFS-U-12.5% model. Further, our learned annotator-specific dimensional importance weights generalize, resulting in a predictive accuracy at or above the Bayes classifier's. As hypothesized (Section 3), transforming A VFS into 2AFC triplets (i.e., A VFS-2AFC in Figure 1) decreases performance due to a reduction in information.\\n\\nDimensional interpretability.\\n\\nFollowing Hebart et al. (2020), we qualitatively assess whether learning on A VFS results in a human-interpretable decomposition of the dimensions used in the human decision-making process. Let $x$ and $\\\\text{ReLU}(w) \\\\in \\\\mathbb{R}^{22}$ denote a stimulus set image and its A VFS-U model embedding, respectively. We denote by $\\\\text{dim}_j$ and $\\\\text{percentile}_q$ the $j$th dimension and percentile of $\\\\text{dim}_j$ stimulus set embedding values, respectively. Suppose $\\\\text{grid}_j$ is a $5 \\\\times 100$ face grid. Column $q \\\\in [100, \\\\ldots, 1]$ of $\\\\text{grid}_j$ contains 5 stimulus set faces with the highest $\\\\text{dim}_j$ values, satisfying\\n\\n$$\\\\text{percentile}_{q-1} \\\\leq \\\\text{ReLU}(w_j) < \\\\text{percentile}_q.$$ \\n\\nThus, from left to right, $\\\\text{grid}_j$ displays stimulus set faces sorted in descending order based on their $\\\\text{dim}_j$ value. We task annotators with writing 1\u20133 $\\\\text{grid}_j$ descriptions. After controlling for quality, we obtain 23\u201358 labels (708 labels) for each $j$ and categorize the label responses into 35 topics. Figure 2 evidences a clear relationship between the top 3 $\\\\text{dim}_j$ topics and $\\\\text{dim}_j$ labels generated using CelebA and FairFace models. Across the 22 dimensions, we note the materialization of individual dimensions coinciding with commonly collected demographic group labels, i.e., \u201cmale\u201d, \u201cfemale\u201d, \u201cblack\u201d, \u201cwhite\u201d, \u201ceast asian\u201d, \u201csouth asian\u201d, \u201celderly\u201d, and \u201cyoung\u201d. In addition, separate dimensions surface for face and hair morphology, i.e., \u201cwide face\u201d, \u201clong face\u201d, \u201csmiling expression\u201d, \u201cneutral expression\u201d, \u201cbalding\u201d, and \u201cfacial hair\u201d. Significantly, these dimensions, which largely explain the variation in human judgment behavior, are learned without training on semantic labels.\\n\\n5.2 Novel stimuli.\\n\\nPredicting similarity.\\n\\nWe evaluate whether A VFS models transfer to novel stimuli by sampling 56 novel face images from FFHQ (not contained in the stimulus set). We then generate all $56^3 = 27,720$ A VFS-2AFC was trained using a triplet margin with a distance swap loss (Balntas et al., 2016).\"}"}
{"id": "mMaInr0r0c", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nDim. 1/22: \\\"south asian\\\" ancestry, skin color, facial expression\\n  - indian, bushy eyebrows, latino hispanic\\n\\nDim. 4/22: \\\"female\\\" gender expression, age related, ancestry\\n  - female, wearing lipstick, heavy makeup\\n\\nDim. 5/22: \\\"smiling exp.\\\" facial expression, skin color, mouth related\\n  - smiling, high cheekbones, rosy cheeks\\n\\nDim. 10/22: \\\"wide face\\\" face shape, facial expression, weight related\\n  - double chin, high cheekbones, smiling\\n\\nDim. 11/22: \\\"elderly\\\" age related, ancestry, hair color\\n  - 70+, 60\u201369, gray hair\\n\\nDim. 18/22: \\\"balding\\\" hair length, gender expression, ancestry\\n  - bald, gray hair, wearing necktie\\n\\nFigure 2: Dimensional interpretability.\\n(a) Our interpretation of an AVFS-U dimension with the highest scoring stimulus set images. (b) Highest frequency human-generated topic labels. (c) CelebA and FairFace model-generated labels, exhibiting a clear correspondence with human-generated topics. Face images in this figure have been replaced by synthetic StyleGAN3 (Karras et al., 2021) images for privacy reasons. Additional dimensions are presented in the dataset documentation (Appendix C).\\n\\nFigure 3: Predicting similarity (novel stimuli).\\nAccuracy over 80,300 judgments and Spearman's $\\\\rho$ between the entropy in human- and model-generated similarity matrices. Our annotator-specific masks (i.e., AVFS-C and AVFS-CPH models) generalize to triplets composed entirely of novel faces, resulting in increased predictive accuracy. Additional results are presented in Appendix A.\\n\\nPossible triplets, collecting 2\u20133 unique judgments per triplet (80,300 judgments) on AMT. We report accuracy, as well as Spearman's $\\\\rho$ between the strictly upper triangular model- and human-generated similarity matrices. Entry $(i, j)$ in the human-generated similarity matrix corresponds to the fraction of triplets containing $(i, j)$, where neither was judged as the odd-one-out; and $(i, j)$ in a model-generated similarity matrix corresponds to the mean $\\\\hat{p}(i, j)$ over all triplets containing $(i, j)$.\\n\\nResults are shown in Figure 3. We see that AVFS embedding spaces and the learned annotator-specific masks generalize to triplets composed entirely of novel faces. This non-trivially demonstrates that AVFS embedding spaces are more closely aligned with the human mental representational space of faces, than embedding spaces induced by training on semantic labels.\\n\\nTo quantify the number of AVFS-U dimensions required to represent a face while preserving performance, we follow Hebart et al. (2020)'s dimension-elimination approach. We iteratively zero out the lowest value per face embedding until a single nonzero dimension remains. This is done per embedding, thus the same dimension is not necessarily zeroed out from all embeddings during an iteration. To obtain 95\u201399% of the predictive accuracy we require 6\u201313 dimensions, whereas to explain 95\u201399% of the variance in the similarity matrix we require 15\u201322 dimensions. This shows that (1) humans utilize a larger number of dimensions to represent the global similarity structure of faces than for determining individual 3AFC judgments; and (2) similarity is context-dependent (i.e., dynamic), which is not accounted for when representing faces using categorical feature vectors.\\n\\nAnnotator positionality.\\nAs shown via our conditional framework, knowledge of the annotator determining similarity assists in informing the outcome. However, annotators are often framed as interchangeable (Malev\u00e9, 2020; Chancellor et al., 2019). To assess the validity of this assumption, we randomly swap the annotator associated with each of the collected 80,300 judgments and recompute the AVFS-CPH model accuracy. Repeating this process 100 times results in a performance drop from 61.7% to 52.8% \u00b1 0.02% (95% CI), evidencing that annotator masks, and hence annotators, are not arbitrarily interchangeable.\\n\\nAnother interesting question relates to whether an annotator's sociocultural background influences their decision making. To evaluate this, we create datasets $\\\\{\\\\sigma(\\\\varphi^a), y\\\\}$, where $\\\\sigma(\\\\varphi^a)$ and $y \\\\in Y$ are annotator $a$'s learned mask and self-identified demographic attribute, respectively. For a particular annotator attribute (e.g., nationality), we limit the dataset to annotators who contributed \u2265 200 judgments and belong to one of the two largest groups with respect to the attribute (e.g., 6...\"}"}
{"id": "mMaInr0r0c", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Annotator positionality. Average linear SVM AUC when trained to discriminate between AVFS-CPH annotator-specific masks from different demographic groups. None of the confidence intervals overlap with chance performance (horizontal black line). Note the abbreviations: \u201cEurope\u201d (\u201ceu\u201d), \u201cAsia\u201d (\u201cas\u201d), \u201cWestern Europe\u201d (\u201ceuwe\u201d), and \u201cSouthern Asia\u201d (\u201cassa\u201d). (b) Prototypicality. Spearman\u2019s $r$ between human ratings of face typicality and model-generated ratings, evidencing that social category typicality manifests in the AVFS-U model\u2019s embedding dimensions. Y = \\\\{ \u201cAmerican\u201d, \u201cIndian\u201d \\\\}). Using 10-fold cross-validation, we train linear SVMs (Hearst et al., 1998) with balanced class weights to predict $y$ from $\\\\sigma(\\\\phi^\\\\top a)$. Figure 4a shows that for each attribute none of the AUC confidence intervals include chance performance. Significantly, we are able to discriminate between nationality-, regional ancestry-, and subregional ancestry-based annotator group masks with high probability (86\u201388%). Thus, to mitigate bias, diverse annotator groups are required.\\n\\nContinuous attribute collection. Following Hebart et al. (2020)\u2019s evaluation protocol, we quantitatively validate whether $\\\\mathbf{grid}_j(\\\\forall j)$\u2014defined in Section 5.1\u2014can be used to collect continuous attribute values for novel faces from human annotators. We task annotators with placing a novel face $x$ above a single column $\\\\mathbf{q}$ of $\\\\mathbf{grid}_j$. Placement is based on the similarity between $x$ and the faces contained in each column $\\\\mathbf{q}$ of $\\\\mathbf{grid}_j$. As annotators are not primed with the meaning of $\\\\mathbf{grid}_j$, accurate placement indicates that the ordering of $\\\\mathbf{dim}_j$ is visually meaningful.\\n\\nLet $\\\\mu_{q,j}$ denote the mean $\\\\mathbf{dim}_j$ embedding value over the stimulus set whose $\\\\mathbf{dim}_j$ value satisfies $\\\\text{percentile}_{q,j} - 1 \\\\leq \\\\text{ReLU}(w_j) < \\\\text{percentile}_{q,j}$. We sample 20 novel faces from FFHQ (not contained in the stimulus set). $\\\\forall (x, \\\\mathbf{grid}_j)$ and collect 20 unique judgments per face (8,800 judgments). $\\\\forall x$, we create a human-generated embedding of the form:\\n\\n$$\\n\\\\left[\\\\begin{array}{c}\\n\\\\mu_{q,1}^1, \\\\ldots, \\\\\\\\\\n\\\\vdots \\\\\\\\\\n\\\\mu_{q,20}^1, \\\\\\\\\\n\\\\vdots \\\\\\\\\\n\\\\mu_{q,1}^{22}, \\\\ldots, \\\\\\\\\\n\\\\vdots \\\\\\\\\\n\\\\mu_{q,20}^{22}\\n\\\\end{array}\\\\right] \\\\in \\\\mathbb{R}^{22},\\n$$\\n\\nwhere $q_n \\\\in \\\\{1, \\\\ldots, 100\\\\}$ denotes the $n$th annotator\u2019s column choice $q$. We then generate all $3^3$ possible triplets to create a human-generated similarity matrix $\\\\mathbf{K} \\\\in \\\\mathbb{R}^{20 \\\\times 20}$. Entry $(i, j)$ corresponds to the mean $\\\\hat{p}(i, j)$ over all triplets containing $(i, j)$ with the human-generated embeddings. Using model-generated embeddings, we create a model-generated matrix, $\\\\hat{\\\\mathbf{K}} \\\\in \\\\mathbb{R}^{20 \\\\times 20}$, in the same way.\\n\\nSpearman\u2019s $r$ correlation between the strictly upper triangular elements of $\\\\mathbf{K}$ and $\\\\hat{\\\\mathbf{K}}$ is 0.83 and 0.86 for AVFS-U and AVFS-CPH embeddings, respectively. This shows that (1) dimensional values correspond to feature magnitude; and (2) image grids can be used to directly collect continuous attribute values, sidestepping the limits of semantic category definitions (Keyes, 2018; Benthall & Haynes, 2019).\\n\\nPrototypicality. In cognitive science, prototypicality corresponds to the extent to which an entity belongs to a conceptual category (Rosch, 1973). We examine whether the dimensional value of a face is correlated with its typicality, utilizing prototypicality rated face images from the Chicago Face Database (CFD) (Ma et al., 2015; Lakshmi et al., 2021; Ma et al., 2021). Ratings correspond to the average prototypicality of a face with respect to a binary gender category (relative to others of the same race) or race category. For each category, we compute the dimensional embedding value or unnormalized attribute logit of a face from a relevant AVFS-U dimension or attribute recognition model\u2019s classification layer, respectively.\\n\\nFigure 4b shows that relevant AVFS-U dimensions are positively correlated with typicality according to Spearman\u2019s $r$. Evidently, category typicality\u2014at least for the investigated social category concepts\u2014manifests in the AVFS dimensions from learning to predict human 3AFC similarity judgments.\\n\\nSemantic classification. Given the evidence that AVFS-U dimensional values correlate with feature magnitude, we expect them to be useful for semantic attribute classification. To validate this hypothesis, we perform attribute prediction on the following labeled face datasets: COCO (Lin et al., 2014), FFHQ, AVFS-U, AVFS-CPH.\"}"}
