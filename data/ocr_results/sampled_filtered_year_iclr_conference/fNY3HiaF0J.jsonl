{"id": "fNY3HiaF0J", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Comparison with other diffusion models. Zoom in for a better view.\"}"}
{"id": "fNY3HiaF0J", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "fNY3HiaF0J", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nFigure 11: More images generated by MoLE. Zoom in for a better view.\"}"}
{"id": "fNY3HiaF0J", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 12: More images generated by MoLE. Zoom in for a better view.\"}"}
{"id": "fNY3HiaF0J", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13: More images generated by MoLE. Zoom in for a better view.\\n\\n19\"}"}
{"id": "fNY3HiaF0J", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "fNY3HiaF0J", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A black cat with bright eyes looks intently toward the left.\\n\\nA elephant that is standing in the grass.\\n\\nTwo horses eating grass near a parking lot.\\n\\nSome dogs stick their heads out the car window.\\n\\nRows of outdoor dining tables sit empty in front of a mixed use building.\\n\\nA night time view of a city street.\\n\\nA deep marble bathtub under an ornate mirror.\\n\\nA beautiful new kitchen with natural wood cabinets.\\n\\nTwo pieces of cake with a strawberry for garnish.\\n\\nA plate of cookies next to a cat's tail.\\n\\nFruit and vegetables are hanging in a metal basket.\\n\\nA table full of various vegetables and legumes.\\n\\nJet liner flying off into the distance on an overcast day.\\n\\nA car modified to drive in the water on a lake.\\n\\nThe black motorcycle is parked on the asphalt.\\n\\nTwo cars parked on the sidewalk on the street.\\n\\nFigure 15: Generic image generation. Zoom in for a better view.\"}"}
{"id": "fNY3HiaF0J", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nText-to-image diffusion has attracted vast attention due to its impressive image-generation capabilities. However, when it comes to human-centric text-to-image generation, particularly in the context of faces and hands, the results often fall short of naturalness due to insufficient training priors. We alleviate the issue in this work from two perspectives.\\n1) From the data aspect, we carefully collect a human-centric dataset comprising approximately one million high-quality human-in-the-scene images and two specific sets of close-up images of faces and hands. These datasets collectively provide a rich prior knowledge base to enhance the human-centric image generation capabilities of the diffusion model.\\n2) On the methodological front, we propose a simple yet effective method called Mixture of Low-rank Experts (MoLE) by considering low-rank modules trained on close-up hand and face images respectively as experts. This concept draws inspiration from our observation of low-rank refinement, where a low-rank module trained by a customized close-up dataset has the potential to enhance the corresponding image part when applied at an appropriate scale. To validate the superiority of MoLE in the context of human-centric image generation compared to state-of-the-art, we construct two benchmarks and perform evaluations with diverse metrics and human studies. More visualization, datasets, models, and code will be released on our anonymous webpage.\\n\\nIntroduction\\nText-to-image Stable Diffusion (Rombach et al., 2022) has recently gained great attention due to its impressive capability to generate plausible images that align with the textual semantics provided. Its open source further boosts the development within the community and fosters a prosperous trend for artificial intelligence generated content (AIGC) with substantial progress achieved. However, there remains a challenge in \u201chuman-centric\u201d text-to-image generation \u2014 current models encounter issues with producing natural-looking results, particularly in the context of faces and hands, which has numerous important real-world applications, e.g., business posters, virtual reality, etc.\\n\\nThis issue is demonstrated in the leftmost column (without LoRA) of Fig 1, that, Stable Diffusion struggles to produce a realistic human-centric image with accurate facial features and hands. We dive deep into this issue and find two factors that potentially contribute to this problem. Firstly, the absence of comprehensive and high-quality human-centric data within the training dataset LAION5B (Schuhmann et al., 2022) makes diffusion models lack sufficient human-centric prior; Secondly, in the human-centric context, faces and hands represent the two most complicated parts due to high variability, making them challenging to be generated naturally.\\n\\nWe alleviate this problem from two perspectives. On one hand, we collect a human-in-the-scene dataset of high-quality and high-resolution from the Internet. Basically, the resolution of each image...\"}"}
{"id": "fNY3HiaF0J", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prompt: A man listening to music\\nwithout LoRA\\nwith LoRA (s=0.1)\\nwith LoRA (s=0.3)\\nwith LoRA (s=0.5)\\nwith LoRA (s=0.7)\\nwith LoRA (s=0.9)\\n\\nPrompt: A girl eating an apple\\nwithout LoRA\\nwith LoRA (s=0.1)\\nwith LoRA (s=0.3)\\nwithout LoRA\\nwith LORA (s=0.2)\\nwith LORA (s=0.4)\\n\\nPrompt: A woman drinking milk\\nwithout LORA\\nwith LORA (s=0.2)\\nwith LORA (s=0.4)\\n\\nFigure 1: Motivation of Mixture of Low-rank Experts. The first row is produced by low-rank module trained on the face dataset. The second row is produced by low-rank module trained by hand dataset. We observe that low-rank module is capable of refining the corresponding part with a proper scale weight.\\n\\nis various and over $1024 \\\\times 2048$. The dataset covers different races, various human gestures, and activities. We carefully process each image to preserve sufficient details and resize it $512 \\\\times 512$ for training. Moreover, considering the limitations of Stable Diffusion in producing natural face and hand within a human-centric context, we have taken additional steps to gather two customized close-up datasets that consist of high-quality close-up of both facial and hand regions. However, the close-up of the hand image is limited and less than the amount of face image. To supplement more hand data, we manually crop high-quality hand images from people in images from the previously collected dataset. Finally, we merge all images together and derive a new dataset, which we refer to as human-centric dataset containing approximately one million images in total, which we believe can improve the performance of diffusion models in human-centric generation.\\n\\nOn the other hand, during our preliminary experiments, we observe an interesting low-rank refinement phenomenon that inspires us to leverage the idea rooted in mixture of experts (MoE) (Jacobs et al., 1991; Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022) to help human-centric image generation. Specifically, we train two low-rank modules (Hu et al., 2021) based on Stable Diffusion v1.5 (SD v1.5) (Rombach et al., 2022) using face and hand datasets respectively. As shown in Fig 1, when combined with a customized low-rank module and using a proper scale weight, SD v1.5 has the potential to refine the corresponding part of the person. Similar results can also be observed in hand in the last row of Fig 1. Thus, in the context of human-centric image generation, intuitively, we could add a certain assignment to adaptively select which specialized low-rank modules to use for a given input and MoE naturally stands out. Additionally, as face and hand often appear simultaneously in an image for a person, motivated by Soft MoE (Puigcerver et al., 2023), we could adopt a soft assignment, allowing multiple experts to handle the input at the same time. Consequently, considering low-rank modules trained on customized datasets as specialized experts, we propose a simple yet effective method called Mixture of Low-rank Experts (MoLE). Our method contains three stages: We start by adopting SD v1.5 as a baseline and fine-tune it on our collected human-centric dataset to complement sufficient human-centric prior; We then use two subdatasets, i.e., close-up of face and hand images, to train two low-rank experts separately; Finally, we formulate these two low-rank experts in an MoE form and integrate them with the base model in an adaptive soft assignment manner.\\n\\nTo evaluate our method, we construct two human-centric benchmarks using data from COCO Caption (Chen et al., 2015) and DiffusionDB (Wang et al., 2022). The results suggest that MoLE consistently shows superior performance over SD v1.5.\\n\\nOur contribution can be summarized as:\\n\\n3 The face data is from Celeb-HQ (Karras et al., 2018). The hand data is from 11k Hands (Afifi, 2019).\\n\\n4 Considering that the human face and hand are generally the most frequently observed parts in an image and their bad cases are also extensively discussed or complained in image generation communities, thereby in this work, we primarily focus on the two most important and urgent parts. Our work could also easily involve other human parts, e.g., feet, by collecting a close-up of the feet dataset, training an extra low-rank expert, and accordingly modifying the parameter of the soft assignment.\"}"}
{"id": "fNY3HiaF0J", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"certainty regarding whether the observation in Fig 1 remains valid in a multiple individual scenario.\\n\\nR EFERENCES\\n\\nMahmoud Afifi. 11k hands: Gender recognition and biometric identification using a large dataset of hand images. *Multimedia Tools and Applications*, 78:20835\u201320854, 2019.\\n\\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. *arXiv preprint arXiv:2211.01324*, 2022.\\n\\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V. Le. Symbolic discovery of optimization algorithms, 2023a.\\n\\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. *arXiv preprint arXiv:1504.00325*, 2015.\\n\\nYixiong Chen. X-iqe: explainable image quality evaluation for text-to-image generation with visual large language models. *arXiv preprint arXiv:2305.10843*, 2023.\\n\\nZitian Chen, Mingyu Ding, Yikang Shen, Wei Zhan, Masayoshi Tomizuka, Erik Learned-Miller, and Chuang Gan. An efficient general-purpose modular vision model via multi-task heterogeneous training. *arXiv preprint arXiv:2306.17165*, 2023b.\\n\\nZitian Chen, Yikang Shen, Mingyu Ding, Zhenfang Chen, Hengshuang Zhao, Erik G Learned-Miller, and Chuang Gan. Mod-squad: Designing mixtures of experts as modular multi-task learners. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 11828\u201311837, 2023c.\\n\\nRapha\u00ebl Couturier, Hassan N Noura, Ola Salman, and Abderrahmane Sider. A deep learning object detection method for an efficient clusters initialization. *arXiv preprint arXiv:2104.13634*, 2021.\\n\\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In *International Conference on Machine Learning*, pp. 5547\u20135569. PMLR, 2022.\\n\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. *The Journal of Machine Learning Research*, 23(1):5232\u20135270, 2022.\\n\\nZhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 10135\u201310145, 2023.\\n\\nShuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 10696\u201310706, 2022.\\n\\nTiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining Guo. Efficient diffusion training via min-snr weighting strategy. *arXiv preprint arXiv:2303.09556*, 2023.\\n\\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In *The Eleventh International Conference on Learning Representations*, 2022.\"}"}
{"id": "fNY3HiaF0J", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 5, 9\\n\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020. 4, 9\\n\\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021. 2, 5\\n\\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79\u201387, 1991. 2, 5, 9\\n\\nYuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, Chen Change Loy, and Ziwei Liu. Text2human: Text-driven controllable human image generation. ACM Transactions on Graphics (TOG), 41(4):1\u201311, 2022. 9\\n\\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In International Conference on Learning Representations, 2018. 2, 3, 14\\n\\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4401\u20134410, 2019. 4, 8, 14\\n\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2020. 2, 5, 9\\n\\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pp. 12888\u201312900. PMLR, 2022. 7\\n\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 4\\n\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 4\\n\\nRon Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021. 4\\n\\nRon Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6038\u20136047, 2023. 9\\n\\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 9\\n\\nJoan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft mixtures of experts. arXiv preprint arXiv:2308.00951, 2023. 2, 6, 9\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR, 2021. 4, 7, 9\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020. 9\"}"}
{"id": "fNY3HiaF0J", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\\n\\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9 Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:8583\u20138595, 2021.\\n\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10684\u201310695, June 2022.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III, pp. 234\u2013241. Springer, 2015.\\n\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.\\n\\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Theo Coombes, Cade Gordon, Aarush Katta, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: a new era of open large-scale multimodal datasets, 2022.\\n\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n\\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\\n\\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020.\\n\\nNarek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1921\u20131930, 2023.\\n\\nZijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. DiffusionDB: A large-scale prompt gallery dataset for text-to-image generative models. arXiv:2210.14896 [cs], 2022. URL https://arxiv.org/abs/2210.14896.\\n\\nXiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better aligning text-to-image models with human preference. arXiv preprint arXiv:2303.14420, 2023.\\n\\nJiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. arXiv preprint arXiv:2304.05977, 2023.\\n\\nXingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text, images and variations all in one diffusion model. arXiv preprint arXiv:2211.08332, 2022.\\n\\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:7103\u20137114, 2022.\\n\\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\"}"}
{"id": "fNY3HiaF0J", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stage 1: Fine-tuning on human-centric Dataset. We use Stable Diffusion v1.5 as base model and fine-tune the text encoder and UNet with a constant learning rate $2 \\\\times 10^{-6}$. We set batch size to 64 and train with the Min-SNR weighting strategy (Hang et al., 2023). The clip skip is 1 and we train the model for 300k steps using Lion optimizer (Chen et al., 2023).\\n\\nStage 2: Low-rank Expert Generation. For face expert, we set batch size to 64 and train it 30k steps with a constant learning rate $2 \\\\times 10^{-5}$. The rank is set to 256 and AdamW optimizer is used. For hand expert, we set batch size to 64. Since ihans more sophisticated than face to generate, we train it 60k steps with a smaller learning rate $1 \\\\times 10^{-5}$. The rank is also set to 256 and AdamW optimizer is used. For both experts, we only add low-rank module to UNet. And the two experts are both built on the fine-tuned base model in Stage 1.\\n\\nStage 3: Mixture Adaptation. In this stage, we use the batch size 64 and employ AdamW optimizer. We use a constant learning rate $1 \\\\times 10^{-5}$ and train for 50k steps.\\n\\nA.2 LICENSE AND PRIVACY STATEMENT\\n\\nThe human-centric dataset is collected from websites including seeprettyface.com, unsplash.com, gratisography.com, morguefile.com, and pexels.com. We use web crawler to download images if it is allowed. Otherwise, we manually download the images. Most images in these websites are published by their respective authors under Public Domain CC0 1.0 license that allows free use, redistribution, and adaptation for non-commercial purposes. Some of them require giving appropriate credit to the author, e.g., seeprettyface.com requires adding the sentence (#Thanks to dataset provider:Copyright(c) 2018, seeprettyface.com, BUPT GWY contributes the dataset.) to the open-source code when using the images. When collecting and filtering the data, we are careful to only include images that, to the best of our knowledge, are intended for free use and redistribution by their respective authors. That said, we are committed to protecting the privacy of individuals who do not wish their images to be included. Besides, for images fetched from other datasets, e.g., Flickr-Faces-HQ (FFHQ) (Karras et al., 2019), Celeb-HQ (Karras et al., 2018), and 11k Hands (Afifi, 2019), we strictly follow their licenses and privacy.\\n\\nA.3 ILLUSTRATIONS OF NEGATIVE SAMPLES\\n\\nIn Fig 10, we present the illustrations of negative samples during refining human-in-the-scene subset.\\n\\nA.4 MORE VISUALIZATION\\n\\nWe present more generated images and compare with other diffusion models in Fig 9. Additionally, we also illustrate more images generated by MoLE in Fig 11, Fig 12, Fig 13, and Fig 14.\\n\\nA.5 GENERIC IMAGE GENERATION\\n\\nAs shown in Fig 15, MoLE ((fine-tuned SD v1.5)) can also generate non-human-centric images, e.g., animals, scenery, etc. A main reason is that the human-centric dataset also contains these entities that interact with humans in an image. As a result, the model learns these concepts. However, intuitively, MoLE may not be better at generic image generation than the generic models as MoLE is trained on human-centric images.\"}"}
{"id": "fNY3HiaF0J", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Benchmark 1: COCO Human Prompts.\\n\\nWe construct this benchmark by leveraging the caption in COCO Caption (Chen et al., 2015) that has been widely used in previous work (Wu et al., 2023; Xu et al., 2023; Chen, 2023). Concretely, we use the captions in the COCO val set, and preserve the caption that contains human-related words, e.g., woman, man, kid, girl, boy, person, teenager, etc. In the end, we have around 60k prompts left, dubbed as COCO Human Prompts.\\n\\nBenchmark 2: DiffusionDB Human Prompts.\\n\\nWe construct this benchmark by leveraging the caption in DiffusionDB 2M set (Wang et al., 2022) which is the first large-scale text-to-image prompt dataset. It contains 14 million images generated by Stable Diffusion using prompts specified by real users. Concretely, we first filter out the NSFW prompts by the indicator provided in DiffusionDB (Wang et al., 2022). Then we preserve captions containing human-related words. Additionally, we filter out prompts containing special symbol, e.g., [], {} etc. In the end, we have around 64k prompts left, dubbed as DiffusionDB Human Prompts. We will release the two prompt sets for further research.\\n\\nMetric 1: Human Preference Score (HPS).\\n\\nHuman Preference Score (HPS) (Wu et al., 2023) measures how images present with human preference. It leverages a human preference classifier fine-tuned on CLIP (Radford et al., 2021).\\n\\nMetric 2: ImageReward (IR).\\n\\nDifferent from HPS, ImageReward (IR) (Xu et al., 2023) is built on BLIP (Li et al., 2022) and is a zero-shot automatic evaluation metric for understanding human preference in text-to-image synthesis.\\n\\n4.2 RESULTS\\n\\nTo evaluate the performance, following previous work (Rombach et al., 2022; Xu et al., 2023; Wu et al., 2023; Chen, 2023) we randomly sample 3k prompts from COCO Human Prompts benchmark and 3k prompts from DiffusionDB Human Prompts benchmark to generate images and calculate metrics, HPS and IR, and compare MoLE with open-resource SOTA method with different model structures including VQ-Diffusion (Gu et al., 2022), Versatile Diffusion (Xu et al., 2022), our baseline SD v1.5 (Rombach et al., 2022) and its largest variant SD XL. We repeat the process three times and report the averaged results and standard error as presented in Tab 1. MoLE outperforms VQ-Diffusion and Versatile Diffusion and significantly improves our baseline SD v1.5 in both metrics, implying that MoLE could generate images that are more natural to meet human preference.\\n\\nWe also notice MoLE is inferior to SD XL in HPS and IR, as SD XL can generate images of high overall aesthetics. Due to that MoLE is built on SD v1.5, which has a significant gap in model size (5.1G vs 26.4G) and output resolution (512 vs 1024) compared to SD XL, it's difficult for MoLE to match with SD XL. Hence, we qualitatively compare the face and hand of generated images from MoLE and SD XL in Fig 8 and Fig 9, and our results are more realistic even under high HPS gap (in 2nd row Fig 8 20.39 vs 22.38). Besides, MoLE is resource-friendly and can be trained in a single A100 40G GPU. Finally, we conduct user studies to further verify MoLE's advantage over baseline SD v1.5 by sampling 20 image pairs from both models and inviting 50 participants to select a better one from each pair according to their preference in four aspects respectively. We report the averaged results in Fig 5 where MoLE obtains higher voting, especially in hand and face quality.\\n\\n4.3ABLATION STUDY\\n\\n4.3.1 STAGE ENHANCEMENT\\n\\nConsidering our MoLE contains three stages, figuring out how each stage enhances the generation performance is important. We conduct the experiments using COCO Human Prompts by randomly sampling 3k prompts to generate images from each stage with the same seed in Sec 4.2 and calculate the HPS and IR. The whole process repeats three times as well. The results are reported in Tab 2. It is observed that fine-tuning on human-centric dataset (Stage 1) is effective in remarkably improving the HPS and IR, implying the importance of the dataset. However, when adding Stage 2, i.e., both experts are employed, the performance drops. We speculate the experts would influence the process of generation by resembling the distribution of the customized datasets due to training. We illustrate it in Fig 6. For example, the left image misses the \u201cdog\u201d and resembles the distribution of face image.\"}"}
{"id": "fNY3HiaF0J", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model        | COCO Human Prompts | DiffusionDB Human Prompts |\\n|--------------|---------------------|---------------------------|\\n|              | HPS (%)             | IR (%)                    |\\n| VQ-Diffision | 19.21 \u00b1 0.04        | 0.51 \u00b1 2.51               |\\n| Versatile Diffusion | 19.75 \u00b1 0.09 | 8.81 \u00b1 1.40              |\\n| SD XL        | 20.84 \u00b1 0.06        | 73.34 \u00b1 2.29              |\\n| SD v1.5      | 19.91 \u00b1 0.09        | 28.34 \u00b1 1.40              |\\n| MoLE         | 20.27 \u00b1 0.07        | 33.75 \u00b1 1.49              |\\n\\nIn FFHQ dataset (Karras et al., 2019). Adding Stage 3 alleviates this issue with mixture assignment and further enhances the performance.\\n\\nTable 3: Ablation on different assignment on COCO Human Prompts.\\n\\n| Method | HPS (%) | IR (%) |\\n|--------|---------|--------|\\n| SD v1.5 | 19.91 \u00b1 0.09 | 28.34 \u00b1 1.40 |\\n| Local  | 20.19 \u00b1 0.04 | 31.98 \u00b1 2.41 |\\n| Global | 20.20 \u00b1 0.02 | 32.20 \u00b1 0.86 |\\n| Both   | 20.27 \u00b1 0.07 | 33.75 \u00b1 1.49 |\\n\\n4.3.2 Mixture Assignment\\n\\nIn MoLE, we use two kinds of mixture manners including local and global assignment. Hence, we ablate the two assignments and present the results in Tab 3. It can be seen that both local and global assignments can enhance performance. When combining them together, the performance is further improved, indicating the effectiveness of our method. Moreover, we present how the two assignments work in Fig 7. For the global assignment, we average the global scalars of 20 close-up face images, 20 close-up hand images, and 20 images involving hand and face respectively in every inference step in Fig 7 (a), (b), and (c). In (a) and (b), when generating different close-ups, the corresponding expert generally produces higher global value, implying that global assignment is content-aware. In (c), $E_{\\\\text{face}}$ and $E_{\\\\text{hand}}$ achieve a balance. Besides, as inference progresses the global scalar of $E_{\\\\text{hand}}$ always drops while that of $E_{\\\\text{face}}$ is relatively flat. We speculate, in light of the diversity of hands (e.g., various gestures), $E_{\\\\text{hand}}$ tends to establish general content in the early stage while $E_{\\\\text{face}}$ must meticulously fulfill facial details throughout the denoise process due to fidelity requirement. For local assignment, we visualize the averaged score map of sampled images from different experts respectively in Fig 7(d). We see that as inference progresses, local assignment from different experts can highlight and gradually refine the corresponding parts.\\n\\n4.4 Visualizations\\n\\nWe present generated images and compare with other diffusion models as shown in Fig 8. It is observed that our method can generate more natural face and hand while aligning with given prompts.\"}"}
{"id": "fNY3HiaF0J", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: The averaged global and local assignment weights in different inference steps.\\n\\nMore results are put in Appendix A.4. We also find MoLE (fine-tuned SD v1.5) is capable of generating non-human-centric images, e.g., animals, scenery, etc. See Appendix A.5.\\n\\nRelated Work\\n\\nText-to-image generation. Diffusion models (Ho et al., 2020; Song et al., 2020) have been widely used in image generation since its proposal. Afterward, a vast effort has been devoted to exploring the applications, especially in text-to-image generation. GLIDE (Nichol et al., 2021) leverages two different kinds of guidance, i.e., classifier-free guidance (Ho & Salimans, 2021) and clip guidance, to match the semantics of generated image with the given text. Imagen (Saharia et al., 2022) further improves the performance of text-to-image generation via a large T5 text encoder (Raffel et al., 2020). Stable Diffusion (Rombach et al., 2022) uses a VAE encoder to map image to latent space and perform diffusion on representation. DALL-E 2 (Ramesh et al., 2022) transfers text representation encoded by CLIP (Radford et al., 2021) to image representation via diffusion prior. Besides generation, diffusion model has also been used in text-driven image editing (Tumanyan et al., 2023). Inspired by the key observation between text and map in the cross-attention module, Prompt-to-prompt (Hertz et al., 2022) modifies the cross-attention map with prompt while preserving original structure and content. Further Null-text inversion (Mokady et al., 2023) achieves real image editing via image inversion. Different from these works, our work primarily focuses on human-centric text-to-image generation, aiming to alleviate the poor performance of diffusion model in this field.\\n\\nMixture-of-Experts. MoE is first proposed in (Jacobs et al., 1991). The underlying principle of MoE is that different subsets of data or contexts may be better modeled by distinct experts. Theoretically, MoE could scale model capability with little cost by using sparsely-gated MoE layer (Shazeer et al., 2017). (Lepikhin et al., 2020; Chen et al., 2023c;b) extend MoE on transformers by replacing FFN and attention layers with position-wise MoE layers. Swith Transformer (Fedus et al., 2022) simplifies the routing algorithm. (Zhou et al., 2022) propose Expert Choice (EC) routing algorithm to achieve optimal load balancing. GLaM (Du et al., 2022) scales transformer model parameter to 1.2T but is inference-efficient. VMoE (Riquelme et al., 2021) scale vision model to 15B parameter via MoE. Soft MoE (Puigcerver et al., 2023) introduces an implicit assignment by passing weighted combinations of all tokens to each expert. MoE has been adapted in generation to enhance the performance (Feng et al., 2023; Balaji et al., 2022; Jiang et al., 2022). For example, ERNIE-ViLG (Feng et al., 2023) uniformly divides the denoising process into several distinct stages, with each being associated with a specific model. eDiff-i (Balaji et al., 2022) calculates thresholds to separate the...\"}"}
{"id": "fNY3HiaF0J", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nFigure 8: Comparison with other diffusion models. Zoom in for a better view.\\n\\n6 DISCUSSION\\n\\nIn this section, we present a comprehensive elaboration on the distinctions between MoLE and other mixture-of-experts approaches to further highlight our contribution.\\n\\nThere are three aspects of distinctions between MoLE and conventional mixture-of-experts approaches. Firstly, from the aspect of training, MoLE independently trains two experts with completely different knowledge using two customized close-up datasets. In contrast, conventional mixture-of-experts methods simultaneously train experts and base model using the same dataset.\\n\\nSecondly, from the aspect of expert structure and assignment manner, MoLE simply uses two low-rank matrices while conventional mixture-of-experts methods use MLP or convolutional layers. Moreover, MoLE combines local and global assignments together for a finer-grained assignment while conventional mixture-of-experts methods only use global assignment.\\n\\nFinally, from the aspect of applications in computer vision, MoLE is proposed for text-to-image generation while conventional mixture-of-experts methods are mainly used in object recognition, scene understanding, etc. e.g., V-MoE (Riquelme et al., 2021). Though MoE recently has been employed in image generation such as ERNIE-ViLG (Feng et al., 2023) and eDiff-i (Balaji et al., 2022) that employ experts in divided stages, MoLE differs from them and consider low-rank modules trained by customized datasets as experts to adaptively refine image generation.\\n\\n7 CONCLUSION\\n\\nIn this work, we primarily focus on the human-centric text-to-image image generation that has important real-world applications but often suffers from producing unnatural results due to insufficient prior, especially the face and hand. To mitigate this issue, we carefully collect and process one million high-quality human-centric dataset, aiming to provide sufficient prior. Besides, we observe that a low-rank module trained on a customized dataset, e.g., face, has the capability to refine the corresponding part. Inspired by it, we propose a simple yet effective method called Mixture of Low-rank Experts (MoLE) that effectively allows diffusion models to adaptively select experts to enhance the generation quality of corresponding parts. We also construct two customized human-centric benchmarks from COCO Caption and DiffusionDB to verify the superiority of MoLE.\\n\\nLimitation. Our method may not be effective in a scenario involving multiple individuals. There could be two reasons. First, most of our collected images are single person. Secondly, there is un-\"}"}
{"id": "fNY3HiaF0J", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We carefully collect a human-centric dataset comprising around one million high-quality human-centric images. Importantly, we include two specialized datasets containing close-up images of faces and hands to provide a comprehensive human-centric prior.\\n\\nWe propose a simple yet effective method called Mixture of Low-rank Experts (MoLE) by considering low-rank modules trained on customized hand and face datasets as experts and integrating them into a MoE framework with soft assignment.\\n\\nWe construct two human-centric evaluation benchmarks from DiffusionDB and COCO Caption. Results show superiority of MoLE over recent state-of-arts in human-centric generation.\\n\\nOur human-centric dataset contains three parts: human-in-the-scene images, close-up of face images, and close-up of hand images. Overall, we have around 1 million high-quality images as shown in Fig 2. Below we introduce each in detail. We put the license and privacy statement in Appendix A.2 to avoid ethical concerns.\\n\\n2.1 Human-Centric Dataset Constitution\\n\\nHuman-in-the-scene images.\\nThe human-in-the-scene images are collected from the Internet. The resolution of these images varies and is basically over $1024 \\\\times 2048$. We primarily focus on high-resolution human-centric images, hoping to supply sufficient priors for diffusion models. These images are diverse w.r.t. occasions, activities, gestures, ages, genders, and racial backgrounds. To enable training, we use a sliding window ($1024 \\\\times 1024$) to crop the image to maintain as much information as possible and then resize cropped parts into $512 \\\\times 512$. For an image, high resolution does not mean high quality. Therefore, we train a VGG19 (Simonyan & Zisserman, 2014) to filter out blurred images. Additionally, considering the crop operation could generate images that are full of background or contain little information about people, we train a VGG19 (Simonyan & Zisserman, 2014) to filter out such bad cases. To ensure the quality, we repeat the two processes multiple times until we do not find any case mentioned above in three times of sampling. By employing these strategies, we can remove amounts of noise and useless images, thereby guaranteeing the image quality.\\n\\nClose-up of face images.\\nThe face dataset contains two sources: the first is from Celeb-HQ (Karras et al., 2018) in which we choose images of high quality with size $1024 \\\\times 1024$; The second is from...\\n\\n---\\n\\n5 For example, approximately 57.33% individuals identify as white, 14.68% as asian, 9.98% as black, 5.11% as indian, 5.52% as latino hispanic, and 7.38% as middle eastern. Approximately 58.18% are male and 41.82% are female.\\n\\n6 In both case, to train the VGG19, we manually collect around 300 positive samples and 300 negative samples as training set, and we also collect around 100 positive samples and 100 negative samples as val set. When training the VGG19, we set batch size to 128, set learning rate to 0.001, and use random flip as data augmentation method. We train the model for 200 epochs and use the best-performing model for subsequent classification. See AppendixA.3 for illustration of negative samples.\"}"}
{"id": "fNY3HiaF0J", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A young man with a black jacket and a black hoodie is standing in front of a graffiti-covered brick wall, looking at the camera with a serious expression.\\n\\nA young girl in a brown dress squatting near a small pond, looking into the water and admiring her reflection, while a fountain is also present in the background.\\n\\nThis image shows a young man standing in front of a graffiti-covered brick wall, wearing a black leather jacket and looking off into the distance with a serious expression on his face.\\n\\nAn image of a small child sitting on a rock in a pond, looking into the water with a curious expression on their face. The child is wearing a brown dress and has dark hair. The pond is surrounded by trees and rocks, and there is a small waterfall in the background.\\n\\nThe hand dataset contains three sources: the first is from 11k Hands (Afifi, 2019) where we randomly sample around 1k images and manually crop them to square; The second is from the Internet where we collect hand images of high quality and resolution with simple backgrounds and use YOLOv5 (Couturier et al., 2021) to detect hands and crop them to 512 \u00d7 512 with details maintained; The third is from human-in-the-scene images (before processing) where we sample 8k images. We check every image and manually crop the hand of the image to square if the image is appropriate. It is worth noting that in this derived hand dataset, there are abundant hand gestures and interactions with other objects shown in Fig 2, e.g., holding a flower, writing, etc. There are 7k high-quality hand images.\"}"}
{"id": "fNY3HiaF0J", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Stage 1: Fine-tuning on Human-centric dataset\\nStage 2: Low-rank Expert Generation\\nStage 3: Soft Mixture Assignment\\n\\nUNet\\nHuman-centric dataset\\nText encoder\\nCaption\\nClose-up hands\\nClose-up faces\\nA\\nB\\nA'\\nB'\\n+\\n\\n\\\\[ E_{hand} \\\\]\\n\\\\[ E_{face} \\\\]\\n\\\\[ X' \\\\]\\n\\nGlobal assignment\\nLocal assignment\\n\\n\\\\[ s = [s_1, s_2] \\\\]\\n\\\\[ g = [g_1, g_2] \\\\]\\n\\n\\\\[ \\\\tilde{\\\\epsilon}_{\\\\theta}(x_t, c_p) = \\\\epsilon_{\\\\theta}(x_t) + s \\\\cdot (\\\\epsilon_{\\\\theta}(x_t, c_p) - \\\\epsilon_{\\\\theta}(x_t)), (2) \\\\]\\n\\nNotations:\\nModule w/ trainable parameters\\nModule w/ frozen parameters\\n\\n* \\\\( X \\\\) can be the input of any linear layers in UNet\\n\\nStage 1: Fine-tuned Linear layer\\nPrev. Linear layer\\nFine-tuned Linear layer\\nNext Linear layer\\n\\n\\\\[ F \\\\]\\nGating\\n\\\\[ G(\\\\cdot, \\\\omega) \\\\]\\n\\nFig 4: The framework of MoLE.\\n\\n\\\\( X \\\\) is the input of any linear layers in UNet.\\n\\n\\\\( A \\\\) and \\\\( B \\\\) are low-rank matrices.\\n\\nwhere \\\\( x_t \\\\) is disturbed by applying Gaussian noise \\\\( \\\\epsilon \\\\sim N(0, 1) \\\\) in time step \\\\( t \\\\sim [1, T] \\\\).\\n\\n\\\\( c_p \\\\) is text embedding. The diffusion model \\\\( \\\\epsilon_{\\\\theta} \\\\) is trained to minimize a mean-squared error loss.\\n\\nTo achieve text-guided generation, classifier-free guidance (Ho & Salimans, 2021) is adopted in training where text conditioning \\\\( c_p \\\\) is randomly dropped with a fixed probability. This leads to a joint model for unconditional and conditional objectives. When inference, the predicted noise is adjusted to:\\n\\n\\\\[ \\\\tilde{\\\\epsilon}_{\\\\theta}(x_t, c_p) = \\\\epsilon_{\\\\theta}(x_t) + s \\\\cdot (\\\\epsilon_{\\\\theta}(x_t, c_p) - \\\\epsilon_{\\\\theta}(x_t)), (2) \\\\]\\n\\nwhere \\\\( s \\\\in (0, 20] \\\\) is guidance scale. Intuitively, the unconditional noise prediction \\\\( \\\\epsilon_{\\\\theta}(x_t) \\\\) is pushed in the direction of the conditioned \\\\( \\\\epsilon_{\\\\theta}(x_t, c_p) \\\\) to yield an image faithful to text prompt. Guidance scale \\\\( s \\\\) determines the magnitude of the influence of the text and we set it to 7.5 by default.\\n\\nLow-rank Adaptation (LoRA).\\nGiven a customized dataset, instead of training the entire model, LoRA (Hu et al., 2021) is designed to fine-tune the \u201cresidual\u201d of the model i.e.\\n\\n\\\\[ W' = W + \\\\Delta W \\\\]\\n\\n(3)\\n\\nwhere \\\\( \\\\Delta W \\\\) is decomposed into low-rank matrices:\\n\\n\\\\[ \\\\Delta W = AB^T, A \\\\in \\\\mathbb{R}^{n \\\\times d}, B \\\\in \\\\mathbb{R}^{m \\\\times d}, d < n, d < m. \\\\]\\n\\nDuring training, we can simply fine-tune \\\\( A \\\\) and \\\\( B \\\\) instead of \\\\( W \\\\), making fine-tuning on customized dataset memory-efficient. In the end, we get a small model as \\\\( A \\\\) and \\\\( B \\\\) are much smaller than \\\\( W \\\\).\\n\\nMixture-of-Experts (MoE).\\nMoE (Jacobs et al., 1991; Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022) is designed to enhance the predictive power of models by combining the expertise of multiple specialized models. Usually, a central \u201cgating\u201d model \\\\( G(\\\\cdot) \\\\) selects which specialized model to use for a given input:\\n\\n\\\\[ y = \\\\sum_{i=1}^{X} G(x) E_i(x). (4) \\\\]\\n\\nWhen \\\\( G(x) \\\\) = 0, the corresponding expert \\\\( E_i \\\\) will not be activated.\\n\\n3.2 MIXTURE OF LOW-RANK EXPERTS\\nMotivated by the two potential reasons discussed above that attribute to the poor performance of human-centric generation, our method contains three stages shown in Fig 4. We describe each stage below and put the training details in Appendix A.1.\\n\\nStage 1: Fine-tuning on Human-centric Dataset.\\nThe limited human-centric prior for diffusion model could be caused by the absence of large-scale high-quality human-centric datasets. Considering such a pressing need, our work bridges this gap by providing a carefully collected dataset that contains one million human-centric images of high quality. To fully leverage these images and learn as much prior as possible, we adopt SD v1.5 as baseline and fine-tuning on the dataset. Concretely, we fine-tuning the text encoder and UNet modules (Ronneberger et al., 2015) while fixing the rest parameters. Our ablation study in Sec 4.3.1 shows that this stage is effective and significantly improves performance. The well-trained model is then sent to the next stage.\"}"}
{"id": "fNY3HiaF0J", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stage 2: Low-rank Expert Generation.\\n\\nTo construct MoE, in this stage, our goal is to prepare two experts, that are supposed to contain abundant knowledge about the corresponding part. To achieve this, we train two low-rank modules using two customized datasets. One is the close-up face dataset. The other is the close-up hand dataset that contains abundant hand gestures, full details with simple backgrounds, and interactions with other objects. We then use the two datasets to train two low-rank experts with Stable Diffusion trained in stage 1 as base model. The low-rank experts are expected to focus on the generation of face and hand and learn useful context.\\n\\nStage 3: Soft Mixture Assignment.\\n\\nThis stage is motivated by the low-rank refinement phenomenon in Fig 1 where a specialized low-rank module using a proper scale weight is capable to refine the corresponding part of person. Hence, the key is to activate different low-rank modules with suitable weights. From this view, MoE naturally stands out and we novelly regard a low-rank module trained on a customized dataset, e.g., face dataset, as an expert and formulate in a MoE form. Moreover, for a person, face and hand would appear in an image simultaneously while hard assignment mode in MoE only allows one expert accessible to the given input. Hence, inspired by Soft MoE (Puigcerver et al., 2023), we adopt a soft assignment, allowing multiple experts to handle input simultaneously.\\n\\nFurther, considering that the face and hand would be a part of the whole image (local) or occupy the whole image (global), we combine local assignment and global assignment together. Specifically, considering a linear layer $F$ from UNet and its input $X \\\\in \\\\mathbb{R}^{n \\\\times d}$ where $n$ is the number of token and $d$ is the feature dimension, we illustrate local and global assignment respectively.\\n\\nFor local assignment, we employ a local gating network that contains a learnable gating layer $G(X, \\\\phi)$ ($\\\\phi \\\\in \\\\mathbb{R}^{d \\\\times e}$, $e$ is the number of expert and here $e$ is 2.) and a sigmoid function. The gating network is to produce two normalized score maps $s = [s_1, s_2] \\\\in \\\\mathbb{R}^{n \\\\times 2}$ for each low-rank expert as formulated:\\n\\n$$s = \\\\text{sigmoid}(G(X, \\\\phi))$$\\n\\n(5)\\n\\nFor global assignment, we also use a gating network including an AdaptiveAvePool module, a learnable gating layer $G(Pool(X), \\\\omega)$ ($\\\\omega \\\\in \\\\mathbb{R}^{d \\\\times e}$, here $e$ is 2), and a sigmoid function. This gating network is to produce two global scalars $g = [g_1, g_2] \\\\in \\\\mathbb{R}^2$ for each expert as formulated:\\n\\n$$g = \\\\text{sigmoid}(G(Pool(X), \\\\omega))$$\\n\\n(6)\\n\\nThe soft mechanism is built on the fact that each token can adaptively determine how much (weight) should be sent to each expert by the sigmoid function. And intuitively, the weight of every token for two experts is independent as face and hand experts are not competitors in generation. Thus we do not use softmax.\\n\\nFor combination, we first send $X$ to each low-rank expert $E_{\\\\text{face}}$ and $E_{\\\\text{hand}}$ respectively, use $s_1$ and $s_2$ ($\\\\mathbb{R}^{n \\\\times 1}$) to perform element-wise multiplication (local assignment), and also perform global control by scalars $g_1$ and $g_2$ (global assignment)\\n\\n$$Y_1 = g_1 \\\\cdot E_{\\\\text{face}}(X \\\\cdot s_1 \\\\cdot g_1)$$\\n\\n$$Y_2 = g_2 \\\\cdot E_{\\\\text{hand}}(X \\\\cdot s_2 \\\\cdot g_2)$$\\n\\n(7)\\n\\nThen we add $Y_1$ and $Y_2$ back to the output of a linear layer $F$ from UNet with $X$ as input, formulating a new output $X'$:\\n\\n$$X' = F(X) + Y_1 + Y_2$$\\n\\n(8)\\n\\nWe use human-centric dataset to train the learnable parameters while freezing the base model and two low-rank experts.\\n\\n4 EXPERIMENT\\n\\n4.1 EVALUATION\\n\\nConsidering that our work primarily focuses on human-centric image generation, before presenting our experiment, we introduce two customized evaluation benchmarks. Additionally, since our generated images are human-centric, they should intuitively meet human preference. Hence, we adopt two human preference metrics including Human Preference Score (HPS) (Wu et al., 2023) and ImageReward (IR) (Xu et al., 2023). We describe all of them below. Besides the two metrics, we also perform user studies by inviting people to compare the generated images with their own preferences.\\n\\nRecalling that each expert is two low-rank matrixes, thereby $g_1$ and $g_2$ can transition from within $E_{\\\\text{face}}$ and $E_{\\\\text{hand}}$ to outside of them.\"}"}
