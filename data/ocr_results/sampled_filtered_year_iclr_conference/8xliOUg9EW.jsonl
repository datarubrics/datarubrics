{"id": "8xliOUg9EW", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 17: A successful case generated by MUSTARD.\\n\\nQuestion Type: Word Problem. Educational Level: Elementary School. $k=2$. Successful case.\\n\\nConcept(s): Addition and subtraction in 1st grade; Measuring angles in 4th grade.\\n\\nComments: MUSTARD creatively combines the two concepts and constructs a reasonable question.\\n\\nInformal Statement\\nJohn has a favorite toy car that he likes to move around a circular track. He starts moving the car from a certain point and stops after the car has moved 90 degrees. Then he moves the car again but this time in the opposite direction, stopping when the car has moved 45 degrees. What is the net degree movement of John's toy car?\\n\\nInformal Proof\\nInitially, John moves his car 90 degrees. Then he moves the car in the opposite direction 45 degrees. Since the car is moving in the opposite direction, we consider this as -45 degrees. To find the net movement, we add the two movements together. So, $90 + (-45)$ equals 45. Therefore, the net degree movement of John's toy car is 45 degrees.\\n\\nFormal Statement and Proof\\n\\n```lean\\nimport data.int.basic\\nopen int\\n\\n-- define the initial movement\\ndef initial_movement := 90\\n\\n-- define the backward movement\\ndef backward_movement := -45\\n\\n-- define the function to calculate the net movement\\ndef net_movement (initial_backward : \u2124) : \u2124 := initial + backward\\n\\n-- proof that the net movement is 45 degrees\\nexample : net_movement initial_movement backward_movement = 45 :=\\nbegin\\n  unfold net_movement, initial_movement, backward_movement,\\n  exact dec_trivial,\\nend\\n```\"}"}
{"id": "8xliOUg9EW", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 18: A successful case generated by MUSTARD.\\n\\nQuestion Type: Theorem Proving. Educational Level: Middle School. k=2. Successful case.\\n\\nConcept(s):\\n- Geometry in 8th grade;\\n- Algebraic expressions in Algebra basics.\\n\\nComments: MUSTARD understands polynomials and writes a valid proof in Lean.\\n\\nInformal Statement\\nGiven a rectangle ABCD where AB is x + 5 and AD is 2x - 3. Prove that the area of the rectangle ABCD is $(2x^2 + 7x - 15)$ square units.\\n\\nInformal Proof\\nIn a rectangle, the area is calculated by multiplying the length and the width. Here, the length is represented by the expression $x + 5$ and the width is represented by the expression $2x - 3$. By multiplying these two expressions together, we get:\\n\\n$$(x + 5) \\\\times (2x - 3) = 2x^2 - 3x + 10x - 15 = 2x^2 + 7x - 15$$\\n\\nTherefore, the area of the rectangle ABCD is $(2x^2 + 7x - 15)$ square units.\\n\\nFormal Statement and Proof\\n```lean\\nimport algebra.ring.basic\\nvariables (x : \u2124)\\ndef rectangle_area : (x + 5) * (2 * x - 3) = 2 * (x * x) + 7 * x - 15 :=\\nbegin\\n  have h1 : (x + 5) * (2 * x - 3) = 2 * (x * x) - 3 * x + 10 * x - 15, by ring,\\n  have h2 : 2 * (x * x) - 3 * x + 10 * x - 15 = 2 * (x * x) + 7 * x - 15, by simp,\\n  rw h1, exact h2,\\nend\\n```\"}"}
{"id": "8xliOUg9EW", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 19: A failed case generated by MUSTARD.\\n\\nQuestion Type: Theorem Proving. Educational Level: High School. k=1. Failed case.\\n\\nConcept(s):\\nDisplaying and comparing quantitative data in Statistics and probability.\\n\\nComments:\\nMUSTARD gives the definition of median in the formal proof.\\n\\nInformal Statement\\nGiven two sets of data $A = \\\\{5, 7, 10, 12, 15\\\\}$ and $B = \\\\{6, 8, 11, 14, 16\\\\}$, we are to find the median of the two sets and prove that the median of set $A$ is less than the median of set $B$.\\n\\nInformal Proof\\nTo find the median of a given data set, we need to sort the data in ascending order and then find the middle value. In the case of set $A$ and set $B$, both are already sorted in ascending order. For set $A$, the median is 10 and for set $B$, the median is 11. Therefore, it is clear that the median of set $A$ is less than the median of set $B$.\\n\\nFormal Statement and Proof\\n```lean\\nimport data.set.finite\\nimport data.set.lattice\\ndef A : finset N := 5, 7, 10, 12, 15.to_finset\\ndef B : finset N := 6, 8, 11, 14, 16.to_finset\\ndef median (A : finset N) :=\\n  if A.card % 2 = 1 then (A.sort (\u2264)).get ((A.card - 1) / 2)\\n  else (A.sort (\u2264)).get (A.card / 2 - 1) / 2 + (A.sort (\u2264)).get (A.card / 2) / 2\\nlemma median A less than B : (median A) < (median B) :=\\nbegin simp [median, A, B, finset.sort, finset.get], norm_num, end\\n```\"}"}
{"id": "8xliOUg9EW", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 20: A failed case generated by M\\u0321USTARD.\\n\\nQuestion Type: Word Problem. Educational Level: High School. k=2. Failed case.\\n\\nConcept(s): Integrals in Calculus 1; Summarizing quantitative data in Statistics and probability.\\n\\nComments: The informal statement does not cover the situation of leap year. The solution does not give the final answer.\\n\\nInformal Statement\\nA company produced a new model of a car and started selling it on January 1. The number of cars sold \\\\( t \\\\) days after January 1 is given by the function \\\\( f(t) = 2t^2 + 3t + 1 \\\\). Use integrals to find the total number of cars sold from January 1 to March 31. Then calculate the average number of cars sold per day during this period.\\n\\nInformal Proof\\nThe total number of cars sold from January 1 to March 31 can be found by integrating the function \\\\( f(t) \\\\) from \\\\( t = 0 \\\\) (January 1) to \\\\( t = 90 \\\\) (March 31), as each day corresponds to one unit of \\\\( t \\\\).\\n\\nPerforming the integration, we get\\n\\\\[\\n\\\\int (2t^2 + 3t + 1) \\\\, dt \\\\text{ from 0 to 90},\\n\\\\]\\nwhich equals\\n\\\\[\\n\\\\left[\\\\frac{2}{3}t^3 + \\\\frac{3}{2}t^2 + t\\\\right]_{0}^{90}.\\n\\\\]\\nSubstituting \\\\( t = 90 \\\\) into the integrated function and subtracting the value at \\\\( t = 0 \\\\), we get the total number of cars sold.\\n\\nThe average number of cars sold per day is then the total number of cars sold divided by 90 (the number of days from January 1 to March 31).\\n\\nFormal Statement and Proof\\n\\\\[\\n\\\\text{import data.real.basic}\\n\\\\]\\n\\\\[\\n\\\\text{open real}\\n\\\\]\\n\\\\[\\n\\\\text{def f := \u03bb (t : \u211d), 2*t^2 + 3*t + 1}\\n\\\\]\\n\\\\[\\n\\\\text{lemma f value at 90 : f 90 = 2*(90)^2 + 3*(90) + 1 :=}\\n\\\\]\\n\\\\[\\n\\\\text{begin simp [f],}\\n\\\\]\\n\\\\[\\n\\\\text{end}\\n\\\\]\"}"}
{"id": "8xliOUg9EW", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 12: Mathematical concepts at the level of higher education.\\n\\n| Domain               | Concept                                                                 |\\n|----------------------|-------------------------------------------------------------------------|\\n| AP College Statistics| \\\"Exploring categorical data\\\", \\\"Exploring one-variable quantitative data: Displaying and describing\\\", \\\"Exploring one-variable quantitative data: Summary statistics\\\", \\\"Exploring one-variable quantitative data: Percentiles, z-scores, and the normal distribution\\\", \\\"Exploring two-variable quantitative data\\\", \\\"Collecting data\\\", \\\"Probability\\\", \\\"Random variables and probability distributions\\\", \\\"Sampling distributions\\\", \\\"Inference for categorical data: Proportions\\\", \\\"Inference for quantitative data: Means\\\", \\\"Inference for categorical data: Chi-square\\\", \\\"Inference for quantitative data: slopes\\\", \\\"Prepare for the 2022 AP Statistics Exam\\\" |\\n| College Algebra      | \\\"Linear equations and inequalities\\\", \\\"Graphs and forms of linear equations\\\", \\\"Functions\\\", \\\"Quadratics: Multiplying and factoring\\\", \\\"Quadratic functions and equations\\\", \\\"Complex numbers\\\", \\\"Exponents and radicals\\\", \\\"Rational expressions and equations\\\", \\\"Relating algebra and geometry\\\", \\\"Polynomial arithmetic\\\", \\\"Advanced function types\\\", \\\"Transformations of functions\\\", \\\"Rational exponents and radicals\\\", \\\"Logarithms\\\" |\\n| Differential Calculus| \\\"Limits and continuity\\\", \\\"Derivatives: definition and basic rules\\\", \\\"Derivatives: chain rule and other advanced topics\\\", \\\"Applications of derivatives\\\", \\\"Analyzing functions\\\", \\\"Parametric equations, polar coordinates, and vector-valued functions\\\" |\\n| Integral Calculus    | \\\"Integrals\\\", \\\"Differential equations\\\", \\\"Applications of integrals\\\", \\\"Parametric equations, polar coordinates, and vector-valued functions\\\", \\\"Series\\\" |\\n| AP College Calculus AB| \\\"Limits and continuity\\\", \\\"Differentiation: definition and basic derivative rules\\\", \\\"Differentiation: composite, implicit, and inverse functions\\\", \\\"Contextual applications of differentiation\\\", \\\"Applying derivatives to analyze functions\\\", \\\"Integration and accumulation of change\\\", \\\"Differential equations\\\", \\\"Applications of integration\\\", \\\"AP Calculus AB solved free response questions from past exams\\\", \\\"AP Calculus AB Standards mappings\\\" |\\n| AP College Calculus BC| \\\"Limits and continuity\\\", \\\"Differentiation: definition and basic derivative rules\\\", \\\"Differentiation: composite, implicit, and inverse functions\\\", \\\"Contextual applications of differentiation\\\", \\\"Applying derivatives to analyze functions\\\", \\\"Integration and accumulation of change\\\", \\\"Differential equations\\\", \\\"Applications of integration\\\", \\\"Parametric equations, polar coordinates, and vector-valued functions\\\", \\\"Infinite sequences and series\\\", \\\"AP Calculus BC solved exams\\\", \\\"AP Calculus BC Standards mappings\\\" |\\n| Multivariable calculus| \\\"Thinking about multivariable functions\\\", \\\"Derivatives of multivariable functions\\\", \\\"Applications of multivariable derivatives\\\", \\\"Integrating multivariable functions\\\", \\\"Green's, Stokes', and the divergence theorems\\\" |\\n| Differential equations| \\\"First order differential equations\\\", \\\"Second order linear equations\\\", \\\"Laplace transform\\\" |\\n| Linear algebra       | \\\"Vectors and spaces\\\", \\\"Matrix transformations\\\", \\\"Alternate coordinate systems (bases)\\\" |\\n\\nPoints with 2 seed concepts, more than 90% of the data cannot pass the prover validation at the first generation. And almost 30% of them cost 2 correction steps to obtain valid proof. Similar\"}"}
{"id": "8xliOUg9EW", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 13: Prompt template for proof filtering.\\n\\nIn the following, you are given a ''Problem'', a pair of corresponding ''Informal proof'' and ''Formal proof in Lean 3'', along with error messages from a Lean Prover corresponding to the ''Formal proof in Lean 3''. Now please carefully modify the ''Formal proof in Lean 3'' section so that it passes the Lean Prover without error. You should write the modified complete proof in your response.\\n\\n# Problem: <generated problem>\\n\\n# Informal proof: <generated informal proof>\\n\\n# Formal proof (1) in Lean 3:\\n```lean\\nline 1 <code>\\nline 2 <code>\\nline 3 <code>\\n...\\n```\\n\\n# Error messages for Formal proof (1) from Lean Prover:\\n<error messages>\\n...\\n\\n# Formal proof (k) in Lean 3:\\n```lean\\nline 1 <code>\\nline 2 <code>\\nline 3 <code>\\n...\\n```\\n\\n# Error messages for Formal proof (k) from Lean Prover:\\n<error messages>\\n...\\n\\nobservations are found in word problems with 2 seed concepts. It is suggested that the data subset with 2 seed concepts is challenging to the LLMs in general. In contrast, data with 1 seed concept are easier for LLMs. But there are still more than half data points that need proof improvements based on error messages from the theorem prover. Therefore, overall, MUSTARD generates valid data points in different difficulty levels, and the majority of the problems are challenging for the LLMs.\\n\\nD.2 Data Diversity\\n\\nWe compute ROUGE-L (Lin, 2004) to check the diversity of generated informal statements and proofs. Specifically, given a data set, we perform 10 rounds of bootstrapping. In each round, we randomly sample 10 data points from the data set, each of which is paired with the remaining data points, and compute pair-wise ROUGE-L scores. The ROUGE-L score per round is obtained by averaging the pair-wise scores. The final ROUGE-L score is an average score over the bootstrapping. We compare the scores among the generation settings, and the results are shown in Figure 6.\\n\\nThe results show that all settings have a ROUGE-L score beneath 0.25, which indicates a high diversity of the generated informal statements and informal proofs. All-at-once and step-by-step generation share similar data diversity. The ROUGE-L scores slightly increase over the educational levels. Generating higher-educational theorem proving data with 2 concepts step-by-step exceeds 0.2 in ROUGE-L. Therefore, this setting is relatively challenging.\"}"}
{"id": "8xliOUg9EW", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We further investigate the diversity of formal statements and proofs. We collect all the occurrences of used tactics and lemmas, and Figure 7 shows the distributions. It is demonstrated that LLMs tend to use diverse lemmas to solve the problems. And LLMs memorize and understand multiple lemmas. The most commonly used lemmas in both theorem proving and word problems include \\\\texttt{as true}, \\\\texttt{mul comm}, and \\\\texttt{nat.mul comm}. \\\\texttt{at true} is often used in automated proofs for complex propositions to first prove their decidability. \\\\texttt{mul comm} and \\\\texttt{nat.mul comm} are about the multiplicative commutative law and are often used for expression simplification. Similarly, the frequently used \\\\texttt{neg pos}, \\\\texttt{pow two}, \\\\texttt{nat.div eq of eq} and so forth are basic lemmas for proving advanced properties. Similar observations are found in tactics. Therefore, LLMs are good at breaking problems into basic steps and using basic lemmas to complete complex proving.\"}"}
{"id": "8xliOUg9EW", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nFigure 7: Distribution of tactics (left-hand side) and lemmas (right-hand side) used by the formal statements and proofs in theorem proving (upper row) and word problem (lower row).\\n\\nTables 15 to 26 demonstrates successful and failed cases generated by MUSTARD.\"}"}
{"id": "8xliOUg9EW", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Current formal validation still suffers mild inconsistency between the #reduce statements and various kinds of theorem proofs. In the future, we will explore more rigorous and careful data filtering. We will also explore data generation and mathematical reasoning via the same language model, which is an interesting setup to study large language models' proficiency of mathematical reasoning. Moreover, the ablation study on data scalability shows consistent performance increases when more data from MUSTARD are introduced, suggesting a great potential for scalability. Fortunately, MUSTARD reduces the cost of acquiring such high-quality step-by-step complex reasoning data and obtains correct, scalable, and reusable data. Therefore, in future work, we would love to build a community in which all members can join the data synthesis process, and acquire and share more high-quality data with the whole community.\"}"}
{"id": "8xliOUg9EW", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Statistics of the mathematical concept pool.\\n\\n| Domain            | 1st grade | 2nd grade | 3rd grade | 4th grade | 5th grade | 6th grade | 7th grade | 8th grade |\\n|-------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\\n| Elementary School | 3         | 8         | 14        | 14        | 16        | 8         |           |           |\\n| Middle School     |           |           |           |           |           |           |           |           |\\n| Higher Education  |           |           |           |           |           |           |           |           |\\n\\nTable 9: Mathematical concepts at the level of elementary school.\\n\\n| Domain                                      | Concept                                                                 |\\n|---------------------------------------------|-------------------------------------------------------------------------|\\n| 1st grade                                   | \\\"Place value\\\", \\\"Addition and subtraction\\\", \\\"Measurement, data, and geometry\\\" |\\n| 2nd grade                                   | \\\"Add and subtract within 20\\\", \\\"Place value\\\", \\\"Add and subtract within 100\\\", \\\"Add and subtract within 1,000\\\", \\\"Money and time\\\", \\\"Measurement\\\", \\\"Data\\\", \\\"Geometry\\\" |\\n| 3rd grade                                   | \\\"Intro to multiplication\\\", \\\"1-digit multiplication\\\", \\\"Addition, subtraction, and estimation\\\", \\\"Intro to division\\\", \\\"Understand fractions\\\", \\\"Equivalent fractions and comparing fractions\\\", \\\"More with multiplication and division\\\", \\\"Arithmetic patterns and problem solving\\\", \\\"Quadrilaterals\\\", \\\"Area\\\", \\\"Perimeter\\\", \\\"Time\\\", \\\"Measurement\\\", \\\"Represent and interpret data\\\" |\\n| 4th grade                                   | \\\"Place value\\\", \\\"Addition, subtraction, and estimation\\\", \\\"Multiply by 1-digit numbers\\\", \\\"Multiply by 2-digit numbers\\\", \\\"Division\\\", \\\"Factors, multiples and patterns\\\", \\\"Equivalent fractions and comparing fractions\\\", \\\"Add and subtract fractions\\\", \\\"Multiply fractions\\\", \\\"Understand decimals\\\", \\\"Plane figures\\\", \\\"Measuring angles\\\", \\\"Area and perimeter\\\", \\\"Units of measurement\\\" |\\n| 5th grade                                   | \\\"Decimal place value\\\", \\\"Add decimals\\\", \\\"Subtract decimals\\\", \\\"Add and subtract fractions\\\", \\\"Multi-digit multiplication and division\\\", \\\"Multiply fractions\\\", \\\"Divide fractions\\\", \\\"Multiply decimals\\\", \\\"Divide decimals\\\", \\\"Powers of ten\\\", \\\"Volume\\\", \\\"Coordinate plane\\\", \\\"Algebraic thinking\\\", \\\"Converting units of measure\\\", \\\"Line plots\\\", \\\"Properties of shapes\\\" |\\n| 6th grade                                   | \\\"Ratios\\\", \\\"Arithmetic with rational numbers\\\", \\\"Rates and percentages\\\", \\\"Exponents and order of operations\\\", \\\"Negative numbers\\\", \\\"Variables & expressions\\\", \\\"Equations & inequalities\\\", \\\"Plane figures\\\" |\"}"}
{"id": "8xliOUg9EW", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Domain          | Concept                                                                 |\\n|-----------------|-------------------------------------------------------------------------|\\n| 7th grade       | \u201cNegative numbers: addition and subtraction\u201d, \u201cNegative numbers: multiplication and division\u201d, \u201cFractions, decimals, & percentages\u201d, \u201cRates & proportional relationships\u201d, \u201cExpressions, equations, & inequalities\u201d, \u201cGeometry\u201d, \u201cStatistics and probability\u201d |\\n| 8th grade       | \u201cNumbers and operations\u201d, \u201cSolving equations with one unknown\u201d, \u201cLinear equations and functions\u201d, \u201cSystems of equations\u201d, \u201cGeometry\u201d, \u201cGeometric transformations\u201d, \u201cData and modeling\u201d |\\n| Algebra basics  | \u201cFoundations\u201d, \u201cAlgebraic expressions\u201d, \u201cLinear equations and inequalities\u201d, \u201cGraphing lines and slope\u201d, \u201cSystems of equations\u201d, \u201cExpressions with exponents\u201d, \u201cQuadratics and polynomials\u201d, \u201cEquations and geometry\u201d, \u201cIntroduction to functions\u201d |\\n| Pre-algebra     | \u201cFactors and multiples\u201d, \u201cPatterns\u201d, \u201cRatios and rates\u201d, \u201cPercentages\u201d, \u201cExponents intro and order of operations\u201d, \u201cVariables & expressions\u201d, \u201cEquations & inequalities introduction\u201d, \u201cPercent & rational number word problems\u201d, \u201cProportional relationships\u201d, \u201cOne-step and two-step equations & inequalities\u201d, \u201cRoots, exponents, & scientific notation\u201d, \u201cMulti-step equations\u201d, \u201cTwo-variable equations\u201d, \u201cFunctions and linear models\u201d, \u201cSystems of equations\u201d |\\n| Basic geometry  | \u201cIntro to area and perimeter\u201d, \u201cIntro to mass and volume\u201d, \u201cMeasuring angles\u201d, \u201cPlane figures\u201d, \u201cUnits of measurement\u201d, \u201cVolume\u201d, \u201cCoordinate plane\u201d, \u201cDecomposing to find area\u201d, \u201c3D figures\u201d, \u201cCircles, cylinders, cones, and spheres\u201d, \u201cAngle relationships\u201d, \u201cScale\u201d, \u201cTriangle side lengths\u201d, \u201cGeometric transformations\u201d |\\n\\nTable 13 demonstrates the prompt template used in the proof-filtering stage in MUSTARD.\\n\\nTable 14 demonstrates the variation of prompt templates used in the proof-generation stage in MUSTARD. In this variation, an LLM is conducted to perform (T1), (T2), and (T3) separately to generate the informal statement, informal solution, and formal solution. It is noted that to prompt the LLM to fulfill (T3), we assign the character of \u201ca master in Lean\u201d rather than the previous \u201ca math expert\u201d to obtain higher quality Lean proofs.\"}"}
{"id": "8xliOUg9EW", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Mathematical concepts at the level of high school.\\n\\n| Domain           | Concepts                                                                 |\\n|------------------|--------------------------------------------------------------------------|\\n| Algebra 1        | \u201cAlgebra foundations\u201d, \u201cSolving equations & inequalities\u201d, \u201cWorking with units\u201d, \u201cLinear equations & graphs\u201d, \u201cForms of linear equations\u201d, \u201cSystems of equations\u201d, \u201cInequalities (systems & graphs)\u201d, \u201cFunctions\u201d, \u201cSequences\u201d, \u201cAbsolute value & piecewise functions\u201d, \u201cExponents & radicals\u201d, \u201cExponential growth & decay\u201d, \u201cQuadratics: Multiplying & factoring\u201d, \u201cQuadratic functions & equations\u201d, \u201cIrrational numbers\u201d, \u201cCreativity in algebra\u201d |\\n| Algebra 2        | \u201cPolynomial arithmetic\u201d, \u201cComplex numbers\u201d, \u201cPolynomial factorization\u201d, \u201cPolynomial division\u201d, \u201cPolynomial graphs\u201d, \u201cRational exponents and radicals\u201d, \u201cExponential models\u201d, \u201cLogarithms\u201d, \u201cTransformations of functions\u201d, \u201cEquations\u201d, \u201cTrigonometry\u201d, \u201cModeling\u201d |\\n| High school geometry | \u201cPerforming transformations\u201d, \u201cTransformation properties and proofs\u201d, \u201cCongruence\u201d, \u201cSimilarity\u201d, \u201cRight triangles & trigonometry\u201d, \u201cAnalytic geometry\u201d, \u201cConic sections\u201d, \u201cCircles\u201d, \u201cSolid geometry\u201d |\\n| Trigonometry     | \u201cRight triangles & trigonometry\u201d, \u201cTrigonometric functions\u201d, \u201cNon-right triangles & trigonometry\u201d, \u201cTrigonometric equations and identities\u201d |\\n| Statistics and probability | \u201cAnalyzing categorical data\u201d, \u201cDisplaying and comparing quantitative data\u201d, \u201cSummarizing quantitative data\u201d, \u201cModeling data distributions\u201d, \u201cExploring bivariate numerical data\u201d, \u201cStudy design\u201d, \u201cProbability\u201d, \u201cCounting, permutations, and combinations\u201d, \u201cRandom variables\u201d, \u201cSampling distributions\u201d, \u201cConfidence intervals\u201d, \u201cSignificance tests (hypothesis testing)\u201d, \u201cTwo-sample inference for the difference between groups\u201d, \u201cInference for categorical data (chi-square tests)\u201d, \u201cAdvanced regression (inference and transforming)\u201d, \u201cAnalysis of variance (ANOVA)\u201d |\\n| High school statistics | \u201cDisplaying a single quantitative variable\u201d, \u201cAnalyzing a single quantitative variable\u201d, \u201cTwo-way tables\u201d, \u201cScatterplots\u201d, \u201cStudy design\u201d, \u201cProbability\u201d, \u201cProbability distributions & expected value\u201d |\\n| Precalculus      | \u201cComposite and inverse functions\u201d, \u201cTrigonometry\u201d, \u201cComplex numbers\u201d, \u201cRational functions\u201d, \u201cConic sections\u201d, \u201cVectors\u201d, \u201cMatrices\u201d, \u201cProbability and combinatorics\u201d, \u201cSeries\u201d, \u201cLimits and continuity\u201d |\\n| Calculus 1       | \u201cLimits and continuity\u201d, \u201cDerivatives: definition and basic rules\u201d, \u201cDerivatives: chain rule and other advanced topics\u201d, \u201cApplications of derivatives\u201d, \u201cAnalyzing functions\u201d, \u201cIntegrals\u201d, \u201cDifferential equations\u201d, \u201cApplications of integrals\u201d |\\n| Calculus 2       | \u201cIntegrals review\u201d, \u201cIntegration techniques\u201d, \u201cDifferential equations\u201d, \u201cApplications of integrals\u201d, \u201cParametric equations, polar coordinates, and vector-valued functions\u201d, \u201cSeries\u201d |\\n\\nFigure 5 demonstrates the proportions of data points that obtain valid proof after different numbers of corrections. Generally speaking, data points without correction are relatively less difficult for the LLMs, while those that require multiple corrections are challenging. Overall, theorem-proving problems are more challenging for LLMs to solve than the generated word problems. For data...\"}"}
{"id": "8xliOUg9EW", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MUSTARD S AUCE v.s. mathlib (cosine similarity = -0.0361)\\n\\nInformal Statement:\\nA cube has a side length of 5 cm. What is the volume of the cube?\\n\\nInformal Proof:\\nThe volume of a cube is calculated by raising the side length to the power of 3. So in this case, the volume is $5 \\\\text{ cm} \\\\times 5 \\\\text{ cm} \\\\times 5 \\\\text{ cm} = 125 \\\\text{ cubic centimeters}$.\\n\\nFormal Statement and Proof:\\n```lean\\ndef cube_volume (side_length : \u2115) : \u2115 := side_length * side_length * side_length\\n#eval cube_volume 5 -- returns 125\\n```\\n\\nmathlib Case\\nGOAL\\n$\\\\alpha : \\\\text{Type} u, \\\\beta : \\\\text{Type} v, \\\\gamma : \\\\text{Type} w, f : \\\\alpha \\\\to \\\\beta \\\\to \\\\gamma$, $l_1 : \\\\text{list } \\\\alpha \\\\vdash \\\\forall (l_2 : \\\\text{list } \\\\beta), (\\\\text{list.map } f l_1 l_2).\\\\text{length} = \\\\text{linear.order.min } l_1.\\\\text{length}$\\n\\nPROOFSTEP\\ninduction $l_1$; intro $l_2$; cases $l_2$; simp [*], add one, min succ succ, nat.zero min, nat.min zero\"}"}
{"id": "8xliOUg9EW", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3.1: Nearest data points between MUSTARD S&AUCE and GSM8K.\\n\\n**MUSTARD S&AUCE v.s. GSM8K (cosine similarity = 0.5975)**\\n\\n**Informal Statement:**\\nGiven two vectors \\\\(u = (u_1, u_2)\\\\) and \\\\(v = (v_1, v_2)\\\\) in \\\\(\\\\mathbb{R}^2\\\\), prove that the operation of vector addition, defined as \\\\(u + v = (u_1 + v_1, u_2 + v_2)\\\\), is commutative. That is, prove that for all \\\\(u\\\\) and \\\\(v\\\\) in \\\\(\\\\mathbb{R}^2\\\\), \\\\(u + v = v + u\\\\).\\n\\n**Informal Proof:**\\nWe can prove this by direct computation. Given the vectors \\\\(u = (u_1, u_2)\\\\) and \\\\(v = (v_1, v_2)\\\\), we have:\\n\\n\\\\[\\n    u + v = (u_1 + v_1, u_2 + v_2)\\n\\\\]\\n\\nand\\n\\n\\\\[\\n    v + u = (v_1 + u_1, v_2 + u_2).\\n\\\\]\\n\\nSince addition is commutative in \\\\(\\\\mathbb{R}\\\\) (the set of all real numbers), we have \\\\(u_1 + v_1 = v_1 + u_1\\\\) and \\\\(u_2 + v_2 = v_2 + u_2\\\\). Thus, we conclude that \\\\((u_1 + v_1, u_2 + v_2) = (v_1 + u_1, v_2 + u_2)\\\\), which means that \\\\(u + v = v + u\\\\).\\n\\n**Formal Statement and Proof:**\\n```lean\\nimport data.real.basic\\n\\nvariables (u1 u2 v1 v2 : \u211d)\\n\\n-- defining vectors as pair of real numbers\\ndef vector := \u211d \u00d7 \u211d\\n\\n-- defining vector addition\\ndef vadd (u v : vector) : vector := (u.1 + v.1, u.2 + v.2)\\n\\n-- defining vectors u and v\\ndef u : vector := (u1, u2)\\ndef v : vector := (v1, v2)\\n\\n-- commutativity of vector addition\\ntheorem vadd_comm : vadd (u u1 u2) (v v1 v2) = vadd (v v1 v2) (u u1 u2) :=\\nbegin\\n  unfold vadd,\\n  unfold u,\\n  unfold v,\\n  rw add_comm u1 v1,\\n  rw add_comm u2 v2,\\nend\\n```\\n\\n**GSM8K Case**\\n\\n**Question:** The local firefighters are doing a \u201cfill the boot\u201d fundraiser. Their goal is to raise $6300. After the first 3 hours, they have raised $2100. For how many hours do they have to fundraise in total to reach their goal, assuming an equal amount raised in every hour?\\n\\n**Answer:**\\nThe fireman raise \\\\(2100 \\\\div 3 = $700\\\\) per hour.\\n\\nThey have to fundraise for \\\\(6300 \\\\div 700 = 9\\\\) hours.\\n\\n---\\n\\n**Note:** The answer is rounded to the nearest whole number.\"}"}
{"id": "8xliOUg9EW", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 32: Nearest data points between MUSTARD SAUCE and MATH.\\n\\nMUSTARD SAUCE v.s. MATH (cosine similarity = 0.6064)\\n\\nInformal Statement:\\nGiven two vectors $u = (u_1, u_2)$ and $v = (v_1, v_2)$ in $\\\\mathbb{R}^2$, prove that the operation of vector addition, defined as $u + v = (u_1 + v_1, u_2 + v_2)$, is commutative. That is, prove that for all $u$ and $v$ in $\\\\mathbb{R}^2$, $u + v = v + u$.\\n\\nInformal Proof:\\nWe can prove this by direct computation. Given the vectors $u = (u_1, u_2)$ and $v = (v_1, v_2)$, we have:\\n\\n$$u + v = (u_1 + v_1, u_2 + v_2)$$\\n\\nand\\n\\n$$v + u = (v_1 + u_1, v_2 + u_2).$$\\n\\nSince addition is commutative in $\\\\mathbb{R}$ (the set of all real numbers), we have $u_1 + v_1 = v_1 + u_1$ and $u_2 + v_2 = v_2 + u_2$. Thus, we conclude that $(u_1 + v_1, u_2 + v_2) = (v_1 + u_1, v_2 + u_2)$, which means that $u + v = v + u$.\\n\\nFormal Statement and Proof:\\nimport data.real.basic\\n\\nvariables (u1 u2 v1 v2 : $\\\\mathbb{R}$)\\n\\n-- defining vectors as pair of real numbers\\ndef vector := $\\\\mathbb{R} \\\\times \\\\mathbb{R}$\\n\\n-- defining vector addition\\ndef vadd (u v : vector) : vector := (u.1 + v.1, u.2 + v.2)\\n\\n-- defining vectors $u$ and $v$\\ndef u : vector := (u1, u2)\\n\\ndef v : vector := (v1, v2)\\n\\n-- commutativity of vector addition\\ntheorem vadd_comm : vadd (u u1 u2) (v v1 v2) = vadd (v v1 v2) (u u1 u2) :=\\nbegin\\n  unfold vadd,\\n  unfold u,\\n  unfold v,\\n  rw add_comm u1 v1,\\n  rw add_comm u2 v2,\\nend\\n\\nMATH Case\\n\\nProblem:\\nIf a snack-size tin of peaches has 40 calories and is 2% of a person's daily caloric requirement, how many calories fulfill a person's daily caloric requirement?\\n\\nSolution:\\nIf 40 calories is equal to $\\\\frac{2}{100} = \\\\frac{1}{50}$ of a person's daily requirement, then a person's daily caloric requirement is:\\n\\n$$4 \\\\cdot 50 = 2000$$\\n\\nAnswer: 2000\"}"}
{"id": "8xliOUg9EW", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Informal Statement**\\n\\nLet $V$ be a vector space, and let $B_1 = v_1, v_2, ..., v_n$ and $B_2 = w_1, w_2, ..., w_n$ be two bases for $V$. Prove that there exists an invertible linear map $T : V \\\\rightarrow V$ such that $T(v_i) = w_i$ for each $i$ in $1, 2, ..., n$.\\n\\n**Informal Proof**\\n\\nThe map $T : V \\\\rightarrow V$ defined by $T(v_i) = w_i$ for each $i$ is linear: indeed, for any vectors $v, v'$ in $B_1$ and any scalars $c, c'$, we have $T(cv + c'v') = cw + c'w'$ where $w = T(v)$ and $w' = T(v')$. This follows from the linearity properties of vector addition and scalar multiplication in the vector space $V$.\\n\\n**Formal Statement and Proof**\\n\\n```lean\\nimport linear_algebra.basis\\n\\nvariables V : Type *\\n[add_comm_group V] [module ]\\n\\nvariables v w : V\\n\\nvariables B1 B2 : set V\\n\\nvariables [hB1 : is_basis R B1] [hB2 : is_basis R B2]\\n\\nlemma exists_invertible_linear_map : \u2203 (T : V \u2192 [R] V) (hT : is_linear_map R T) (hinv : function.bijective T), \u2200 (v \u2208 B1), T v \u2208 B2 :=\\nbegin\\nuse [hB2.constr hB1, hB2.is_linear hB1, hB2.bijective hB1],\\nintros v hv,\\napply hB2.mem_span,\\rw hB1.repr_self hv,\\nexact \u03bb , hB2.smul ,\\nend\\n```\"}"}
{"id": "8xliOUg9EW", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 22: A successful case generated by MUSTARD.\\n\\nQuestion Type: Word Problem. Educational Level: Higher Education. k=1. Successful case. Concept(s): Series in Integral Calculus.\\n\\nComments: MUSTARD is proficient in number series and is able to perform detailed proof by integration.\\n\\nInformal Statement\\nProve that the integral of the series $\\\\sum_{n=1}^{\\\\infty} -\\\\frac{1}{(n+1)n}$ equals $\\\\ln(2)$.\\n\\nInformal Proof\\nWe start by recognizing the series $\\\\sum_{n=1}^{\\\\infty} -\\\\frac{1}{(n+1)n}$ as the alternating harmonic series, which is known to converge to $\\\\ln(2)$.\\n\\nFormal Statement and Proof\\n```lean\\nimport data.real.basic\\nimport analysis.special.functions.pow\\nopen real\\nlemma integral_of_series : $\\\\int_{0}^{1} \\\\ln(1 + x) dx = \\\\sum_{n : \\\\mathbb{N}} (-1)^{n+1} / (n + 1)$ :=\\nbegin\\n  have hder : \\\\forall x \\\\in \\\\text{set.Icc} (0 : \\\\mathbb{R}) 1, deriv (\u03bb x, \\\\ln (1 + x)) x = 1 / (1 + x),\\n    { intros x hx, have hpos : 0 < 1 + x := by nlinarith, exact deriv ln one add hpos },\\n  have hint : \\\\text{interval_integral} (\u03bb x, 1 / (1 + x)) 0 1 volume = \\\\sum_{n : \\\\mathbb{N}} (-1)^{n+1} / (n + 1),\\n    { have hfrac : \\\\forall (n : \\\\mathbb{N}), \\\\exists x : \\\\mathbb{R} \\\\in \\\\text{0..1}, x^n = 1 / (n + 1),\\n      { intro n, calc \\\\exists x : \\\\mathbb{R} \\\\in \\\\text{0..1}, x^n = \\\\frac{\\\\frac{x^{n+1}}{n+1}} | 0..1 : integral.pow n ... = 1 / (n + 1) : by { rw integral.interval, simp } },\\n      rw [interval_integral.interval_integral.integral_of_le, hfrac], simp, linarith },\\n  have heq : \\\\int_{0}^{1} \\\\ln (1 + x) dx = \\\\int_{0}^{1} 1 / (1 + x) dx,\\n    { congr, ext, exact hder x (set.mem_Icc.mpr \\\\{ by nlinarith, by nlinarith \\\\} ) },\\n  rw [heq, hint],\\nend\\n```\\n\"}"}
{"id": "8xliOUg9EW", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 23: A failed case generated by MUSTARD.\\n\\nQuestion Type: Theorem Proving. Educational Level: Higher Education. k=2. Failed case.\\n\\nConcept(s):\\nSecond order linear equations in Differential equations; Limits and continuity in AP College Calculus BC.\\n\\nComments:\\nMUSTARD does not understand second-order linear equations and continuity.\\n\\nInformal Statement\\nGiven a second order linear differential equation of the form\\n$$y'' + p(x)y' + q(x)y = 0,$$\\nwhere $p(x)$ and $q(x)$ are continuous on the interval $[a, b]$, prove that if there exist two solutions $y_1$ and $y_2$ that are linearly independent, then any solution $y$ of the differential equation can be expressed as\\n$$y = c_1 \\\\cdot y_1 + c_2 \\\\cdot y_2,$$\\nwhere $c_1$ and $c_2$ are constants.\\n\\nInformal Proof\\nThe general solution to a homogeneous second order linear differential equation is given by the superposition of two linearly independent solutions. This is a consequence of the linearity of the differential operator.\\n\\nSuppose we have a second order linear differential equation, and $y_1$ and $y_2$ are two solutions that are linearly independent. Then, any solution $y$ of the differential equation can be expressed as a linear combination of $y_1$ and $y_2$. This means there exist constants $c_1$ and $c_2$ such that\\n$$y = c_1 \\\\cdot y_1 + c_2 \\\\cdot y_2.$$\\n\\nThe reason is that any linear combination of solutions to a linear differential equation is also a solution to the differential equation. This follows from the properties of the derivative, which allow us to distribute the derivative over addition and to factor out constants.\\n\\nFormal Statement and Proof\\nimport data.real.basic\\nvariables \u03b1 : Type [field \u03b1]\\n-- Placeholder for the differential equation /\\ndef diff_eq (p q y : \u03b1 \u2192 \u03b1) (x : \u03b1) : Prop := y x = 0\\n-- This is just a placeholder. A real differential equation\\n-- would have a more complex form. /\\n-- Theorem: Given a second order linear differential equation of the\\n-- form $y'' + p(x)y' + q(x)y = 0$, where $p(x)$ and $q(x)$ are continuous on the interval $[a, b]$, if there exist\\n-- two solutions $y_1$ and $y_2$ that are linearly independent, then any solution $y$ of the differential\\n-- equation can be expressed as $y = c_1 \\\\cdot y_1 + c_2 \\\\cdot y_2$, where $c_1$ and $c_2$ are constants. /\\nlemma second_order_linear_differential_eq (p q y1 y2 y : \u03b1 \u2192 \u03b1) (c1 c2 : \u03b1) : (\u2203 x, diff_eq p q y1 x \u2227 diff_eq p q y2 x) \u2192 (\u2200 x, diff_eq p q y x) \u2192 (\u2200 x, y x = c1 * y1 x + c2 * y2 x) :=\\nbegin\\nintros h sol h y x,\\n-- This is a placeholder. A real proof would show that $y x$ can be\\n-- written as a linear combination\\n-- of $y_1 x$ and $y_2 x$ using the properties of the differential equation,\\n-- the assumption that $y_1$ and $y_2$ are solutions, and the linearity of the differential operator.\\nsorry\\nend\"}"}
{"id": "8xliOUg9EW", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 24: A successful case generated by MUSTARD.\\n\\nQuestion Type: Word Problem. Educational Level: Higher Education. k=2. Successful case.\\n\\nConcept(s):\\n- Matrix transformations in Linear algebra\\n- Collecting data in AP College Statistics\\n\\nComments:\\nMUSTARD performs correct matrix transformation.\\n\\nInformal Statement\\nConsider two matrices $A = \\\\begin{bmatrix} 1 & 2 \\\\\\\\ 3 & 4 \\\\end{bmatrix}$ and $B = \\\\begin{bmatrix} 5 & 6 \\\\\\\\ 7 & 8 \\\\end{bmatrix}$ which represent two different data collection methods applied to a survey of a population. We want to find the collective effect of these two data collection methods when applied simultaneously. In other words, find the matrix $C$ which is the result of the matrix transformation of $A$ by $B$.\\n\\nInformal Proof\\nMatrix multiplication is a form of transformation of one matrix by another. In this case, we want to find the matrix $C$ that is a result of the transformation of $A$ by $B$.\\n\\nMatrix multiplication is done by taking the dot product of the rows of the first matrix with the columns of the second matrix.\\n\\nTherefore,\\n\\n- $C_{0,0} = (A_{0,0} \\\\times B_{0,0}) + (A_{0,1} \\\\times B_{1,0}) = (1 \\\\times 5) + (2 \\\\times 7) = 19$\\n- $C_{0,1} = (A_{0,0} \\\\times B_{0,1}) + (A_{0,1} \\\\times B_{1,1}) = (1 \\\\times 6) + (2 \\\\times 8) = 22$\\n- $C_{1,0} = (A_{1,0} \\\\times B_{0,0}) + (A_{1,1} \\\\times B_{1,0}) = (3 \\\\times 5) + (4 \\\\times 7) = 43$\\n- $C_{1,1} = (A_{1,0} \\\\times B_{0,1}) + (A_{1,1} \\\\times B_{1,1}) = (3 \\\\times 6) + (4 \\\\times 8) = 50$\\n\\nSo, the matrix $C = \\\\begin{bmatrix} 19 & 22 \\\\\\\\ 43 & 50 \\\\end{bmatrix}$.\\n\\nFormal Statement and Proof\\n```lean\\nimport data.matrix.basic\\n\\ndef A : matrix (fin 2) (fin 2) := ![1, 2], ![3, 4]\\ndef B : matrix (fin 2) (fin 2) := ![5, 6], ![7, 8]\\ndef C : matrix (fin 2) (fin 2) := A.mul B\\n#eval C\\n```\\n---\\n\\nOutput: ![19, 22], ![43, 50]\"}"}
{"id": "8xliOUg9EW", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nRecent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-wise formal solutions. (3) Lastly, the framework utilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With the proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARD SAUCE with 5,866 valid data points. Each data point contains an informal statement, an informal proof, and a translated formal proof that passes the prover validation. We perform extensive analysis and demonstrate that MUSTARD generates validated high-quality step-by-step data. We further apply the MUSTARD SAUCE for fine-tuning smaller language models. The fine-tuned Llama 2-7B achieves a 15.41% average relative performance gain in automated theorem proving, and 8.18% in math word problems.\\n\\nIntroduction\\n\\nLarge language models (LLMs) (OpenAI, 2023; 2022) have shown promising reasoning capabilities in various domains, including math word problem and theorem proving (Cobbe et al., 2021; Hendrycks et al., 2021; Zheng et al., 2022; Wu et al., 2021). These two tasks, which require strictly and successively multi-step inference, have become appeal domains to evaluate and develop LLMs' ability in complex reasoning. Recent works progress LLMs in solving math problems mainly through two techniques. The first is the chain-of-thoughts (CoT) prompting (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2023b), which provides step-by-step solutions to the LLMs. The second is to leverage the LLMs' ability in code generation to generate formalized languages and utilize external solvers to obtain strict inference results (Wu et al., 2022; Jiang et al., 2023; Polu & Sutskever, 2020; Han et al., 2022; Polu et al., 2023). Both techniques rely on step-wise annotation to improve LLMs' performance and interpretability on the math problem.\\n\\nCorrect intermediate steps are crucial for LLMs to perform complex reasoning. However, high-quality step-wise annotations are hard to obtain, and Figure 1 demonstrates a few representative\"}"}
{"id": "8xliOUg9EW", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: A comparison of methods of synthesizing and validating intermediate reasoning steps. Previous works such as miniF2F (Zheng et al., 2022) resorts to manual annotation and validation to obtain high-quality step-wise labels. However, manual annotation requires heavy labor of knowledgeable experts, resulting in an extremely small-scale dataset. Manual checking also does not guarantee the correctness of data as the labelers would make mistakes in labeling. On the other hand, generating data with rule-based checking such as ROSCOE (Golovneva et al., 2023) can produce large-scale reasoning data. Given that the generated data are more friendly and readable for human beings, the correctness of the reasoning is not guaranteed by those rules. Moreover, another line of work such as INT (Wu et al., 2021) performs rule-based synthesis to generate validated proofs, which are both correct and large-scale. However, the data are brutally synthesized so that many generated proofs lack actual meaning. Therefore, we need a more efficient way to generate mathematical data that are large-scale, with accurate intermediate steps, and also meaningful mathematical knowledge to human beings.\\n\\nTo fill this gap, we propose MUSTARD, a data generation framework that uniformly synthesizes large-scale and high-quality mathematical data by combining the advantages of LLMs in verbalization and formal theorem provers in rigorous data validation. Specifically, MUSTARD first samples a few mathematical concepts from a predefined list and prompts an LLM to generate a related question described in natural language. Then, it applies the LLM to generate the corresponding solution in both natural and formal language. Given the generated solution, MUSTARD further validates them using a theorem prover. The passed one is considered to be correct and is a high-quality data point. The invalid one on the other hand is considered to be a challenging sample, which will be further combined with the error messages to prompt the LLM for a solution revision, and added as a challenging data point.\\n\\nBy applying the proposed MUSTARD one can obtain large amounts of problems and theorems with desired mathematical concepts and domains. Eventually, we build a mathematical dataset with validated informal and formal solutions, named MUSTARD SAUCE (MUSTARD resource).\\n\\nWe conduct extensive data analysis and experiments on the generated MUSTARD SAUCE. Through deep inspection of the data, we find that MUSTARD generates interesting and reasonable math problems by creatively combining two mathematical concepts, and MUSTARD SAUCE is diverse and has a high proportion of difficult data. We also observe that the prover is consistent with human evaluation, where humans usually consider a validated solution to have a higher quality than those without a formal validation process. Lastly, we fine-tune smaller-scale language models on MUSTARD SAUCE. The fine-tuned Llama 2-7B achieves improvements by 20.9% on zero-shot inference on GSM8K and achieves 8.7 of pass@1 on mathlib. These results demonstrate the effectiveness of MUSTARD SAUCE in improving the mathematical reasoning capabilities of language models.\\n\\nThe contributions of this paper are summarized as follows:\\n1. We propose a novel framework MUSTARD that can generate high-quality mathematical data (both informal and formal) with an interplay between generative language model and theorem prover assistants.\\n2. We release the MUSTARD SAUCE, which contains both math word problems and theorem-proving problems spanning over four educational levels. Each sample has corresponding informal and formal solutions.\\n3. We conduct extensive analysis and experiments on the generated data, demonstrating their quality, diversity, and effectiveness in improving language models' mathematical reasoning performance.\"}"}
{"id": "8xliOUg9EW", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models for Mathematical Reasoning\\n\\nThe growing generative language models (Brown et al., 2020; OpenAI, 2022; 2023) show compelling potential for solving mathematical problems both in natural language proofs (OpenAI, 2023; Zheng et al., 2023a; Xiong et al., 2024) and in formal languages with theorem provers (Polu & Sutskever, 2020; Han et al., 2022; Zheng et al., 2023b; Polu et al., 2023). On the other hand, some works explore using language models to automatically translate natural language proofs into formal ones given few-shot demonstrations (Wu et al., 2022; Jiang et al., 2023; Liu et al., 2023; Wang et al., 2024). Chain-of-though reasoning (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2023b) is demonstrated beneficial for the LLMs to derive correct answers. However, some recent works (Saparov & He, 2023; Golovneva et al., 2023) observe that the intermediate reasoning steps can be inconsistent. This paper proposes a data generation framework that taps the comprehensive mathematical reasoning capabilities of large language models. It generates mathematical reasoning problems with informal and formal solutions that are step-wise validated by a formal theorem prover. With the framework, we obtain high-quality mathematical data.\\n\\nSynthesizing Mathematical Data\\n\\nObtaining large-scale high-quality mathematical data is a long-standing challenge. Previous data relies on well-trained annotators to hand-craft and review the formal proofs (Zheng et al., 2022), which is time and labour-consuming and results in a small data scale. Wang & Deng (2020) and Xiong et al. (2023) construct a neural generator for data synthesis, but it still requires the intervention of human-written data. Besides, Wu et al. (2021) explore using a theorem generator to automatically generate formal proofs with rules. However, the rule-based generation depends on given axioms in specified orders. As a result, the generated data is restricted to a few domains. On the other hand, recent works demonstrate the effectiveness of distilling knowledge from large language models (West et al., 2022; Yuan et al., 2023; Li et al., 2023), and some of them (Wang et al., 2023c; Xu et al., 2023) explore data evolution by properly prompting the language models. The proposed framework explores eliciting mathematical knowledge from large language models to achieve diverse and large-scale mathematical data. In this framework, an interplay between the language model and a formal proof assistant controls the quality and difficulties of data. Using the proposed framework, we collect a large-scale mathematical dataset that contains diverse and multiple-difficulty math questions with high-quality solutions.\\n\\n3. MUSTARD\\n\\nIn this work, we aim to obtain large-scale mathematical data with multi-step annotations and propose MUSTARD to generate diverse and high-quality math and theorem-proving problems with multi-step informal and formal solutions. As shown in Figure 2, MUSTARD consists of three stages. In the first concept seeding stage, MUSTARD samples a set of math concepts as the problem domain. Then in the second solution generation stage, it generates the concept-related problem and solution by prompting an LLM. In the third stage, a theorem prover is used to validate the generated solution. If the solution cannot pass the prover, the error message is returned to the second stage for another turn of solution generation. Through interaction between the LLM and a formal proof assistant, MUSTARD can generate diverse and high-quality data that contains both informal and formal solutions. We describe the details of each stage in this section.\\n\\n3.1 CONCEPT SEEDING\\n\\nWe first define and build a mathematical concept pool that covers as complete sub-subjects in mathematics and educational levels as possible. Specifically, we collect all math courses on the Khan Academy website, the large-scale online educational platform. The resulting pool includes concepts in four educational levels: elementary school, middle school, high school, and higher education. Each educational level has 5 to 9 math domains, covering different types of math problems such as algebra and geometry. Each domain contains subdivided mathematical concepts to inspect different mathematical abilities like polynomial arithmetic or factorization. Concept statistics and detailed concepts in each domain are demonstrated in Appendix B.\\n\\nGiven the concept pool, for each educational level, MUSTARD uniformly samples 1 or 2 concepts from all domains as seeds, and then generates mathematical problems that cover the concepts. In particular, given an educational level, taking 2 concepts from different subjects challenges the model to generate problems that join diverse domains while keeping the problems reasonable.\\n\\n2 https://www.khanacademy.org/math\"}"}
{"id": "8xliOUg9EW", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Overview of MUSTARD. The mathematical theorem (indicated by \\\"# Problem:\\\") is created according to the randomly selected seed concepts. Following the theorem, a corresponding informal proof is generated, which is then translated into a formal proof in Lean 3. The formal proof is passed into Lean Prover for quality feedback, and according to the error messages the formal proof is revised and improved. The proofs that pass the Lean Prover are regarded as high-quality data points and collected.\\n\\n3.2 PROOF GENERATION\\n\\nGiven the sampled mathematical concepts, MUSTARD generates math problems and their corresponding solutions. Specifically, MUSTARD leverages the capability of LLMs in generating natural language and code and prompts an LLM to generate the problem statement, its natural language solution, and a formal solution written in Lean. As a result, the LLM needs to complete the following three tasks: (T1) Generating a math problem that relates to the given concepts; (T2) Solving the math problem with a natural language proof; (T3) Performing auto-formalization to translate the written natural language proof into a formalized proof. In this work, we use GPT-4 OpenAI (2023) as the LLM for proof generation.\\n\\nWe intend to generate a problem based on educational level, math domains, and concepts. Considering that mathematical problems include proof and calculation, we also introduce the question types into the prompt for generating theorem proving and word problems respectively. Moreover, we do not include any exemplars or other manual interventions except for the sampled concepts. We intend to avoid potential biases brought by the concepts inside the exemplars and achieve a more diverse generation. The prompt template is shown as the following:\\n\\nYou are a math expert. Now please come up with a math problem according to the following requirements. The math problem should contain a question part (indicated by \\\"Problem:\\\"), a corresponding solution in natural language (indicated by \\\"Informal proof:\\\"), and a translated formal solution in Lean 3 (indicated by \\\"Formal proof in Lean 3:\\\"). Please note that the informal proof and the formal proof need to be identical.\\n\\nPlease create a [QUESTION TYPE] in the level of [EDUCATIONAL LEVEL] based on the following knowledge point(s): [CONCEPT] in [DOMAIN]; [CONCEPT] in [DOMAIN].\\n\\nYou must respond in the following format:\\n\\n# Problem: ...\\n# Informal proof: ...\\n# Formal proof in Lean 3: ...\\n\\nThe \\\"[]\\\" indicates the placeholders for the corresponding question type, educational level, concepts, and domains. Multiple concepts are separated by \\\";\\\". We retrieved the text after the \\\"Problem:\\\" , \\\"Informal proof:\\\" and \\\"Formal proof in Lean 3:\\\" as the generated sample.\"}"}
{"id": "8xliOUg9EW", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"F.1 DATASETS\\n\\nGSM8K (Cobbe et al., 2021)\\nGSM8K consists of 8.5K elementary mathematics word problems that require 2 to 8 arithmetic operations to arrive at the final answer. The dataset comprises 7.5K training questions and 1K test questions. Inspired by Kojima et al. (2022), during inference, we use appropriate prompts and examples to prompt the model for zero-shot and few-shot reasoning. The used prompts are demonstrated in Table 27.\\n\\nMathlib\\nMathlib is a community-maintained library designed for the Lean theorem prover. It encompasses both programming tools and mathematical content, along with tactics that leverage these tools to facilitate mathematical development. The version of Mathlib we use is consistent with Wang et al. (2023a). The lengths of the training, test, and validation sets were 36,960, 1,621, and 1,580, respectively.\\n\\nminiF2F (Zheng et al., 2022)\\nMiniF2F serves as a formal mathematics benchmark, which has been translated to work with multiple formal systems. It encompasses exercise statements from olympiads like AMC, AIME, and IMO, in addition to content from high-school and undergraduate mathematics courses. The MiniF2F test split contains 244 formal Olympiad-level mathematics problem statements. We use the auto-regression method to construct the training corpus like Polu & Sutskever (2020); Han et al. (2022); Wang et al. (2023a). We perform an evaluation using best-first search with a number of expansions per proof search of $d = 8$ during inference.\\n\\nF.2 MODELS\\n\\nGPT2-large (Radford et al., 2019)\\nThe GPT2-large model is a transformer language model, following the decoder-only architecture introduced by Vaswani et al. (2017). The model boasts an impressive 774 million parameters, 36 layers, 20 attention heads, and a hidden dimension of 1,280. Additionally, it employs a tokenizer featuring a vocabulary size of 50,400. The model is pre-trained on Github python codes and the arXiv library.\\n\\nLlama 2-7B (Touvron et al., 2023)\\nLlama 2 is a language model that employs an auto-regressive transformer architecture, pre-trained on open-source corpus. The model utilizes both supervised fine-tuning and reinforcement learning with human feedback techniques to align with human preferences. The 7B model is configured with 32 layers, 32 attention heads, and a hidden dimension of 4,096.\\n\\nF.3 IMPLEMENTATION DETAILS\\n\\nWe employ LoRA (Hu et al., 2021) for fine-tuning the pre-trained models on MUSTARDSAUCE, where the trainable parameters of GPT2-large and Llama 2-7B constitute 19% and 6%, respectively. The training is conducted with a maximum of 10 epochs using a batch size of 16 and a warm-up step of 1,000, with a maximum learning rate of 1e-4 and a minimum learning rate of 5e-6. The best checkpoint is selected based on the minimum perplexity of the validation split.\\n\\nG. EXPERIMENTAL RESULTS\\n\\nTable 28 demonstrates the compared results on GSM8K and MATH between Llama 2-7B and Llama 2-70B. Llama 2-70B fine-tuned with MUSTARDSAUCE-valid consistently outperforms the model fine-tuned with MUSTARDSAUCE-random by 8.33% in the zero-shot manner and 5.30% in the few-shot manner. It also surpasses the model fine-tuned with the invalid subset and the entire generated dataset. The results also suggest the effectiveness of the framework with a larger fine-tuned LM.\"}"}
{"id": "8xliOUg9EW", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We check cross-contamination between MUSTARD SAUCE and the evaluation datasets inspired by OpenAI (2023). However, instead of using a substring match that may result in false negatives and false positives, we compute cosine similarities based on text-embedding-ada-002 and then inspect the nearest data points in the paired datasets. The automated theorem proving (ATP) dataset miniF2F only contains formal statements and proofs, while the math word problem (MWP) dataset GSM8K only contains informal statements and proofs. For a more detailed inspection, we check the corresponding fractions in MUSTARD SAUCE. Tables 29, 32, 31, and 30 demonstrate the inspected cases. The nearest data points are dissimilar. Therefore, we exclude data contamination of the generated MUSTARD SAUCE according to these observations.\\n\\nhttps://openai.com/blog/new-and-improved-embedding-model\"}"}
{"id": "8xliOUg9EW", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 14: Prompt templates for step-by-step generation in the proof-generation stage.\\n\\nPrompt Templates for Step-by-Step Generation\\n\\n(T1)\\n\\nYou are a math expert. Now please come up with a math problem according to the following requirements. The math problem should contain a question part (indicated by \\\"Problem:\\\") and a corresponding solution in natural language (indicated by \\\"Informal proof:\\\"). Please note that the informal proof and the formal proof need to be identical. Please create a [QUESTION TYPE] in the level of [EDUCATIONAL LEVEL] based on the following knowledge point(s): [CONCEPT] in [DOMAIN]; [CONCEPT] in [DOMAIN].\\n\\nPlease first write the question part regardless of the other parts. You must write the following format, filling in the \\\"# Problem: \\\" section, and leaving the other two sections empty.\\n\\n# Problem: ...\\n\\n# Informal proof: ...\\n\\n# Formal proof in Lean 3: ...\\n\\n(T2)\\n\\nYou are a math expert. Now please come up with a math problem according to the following requirements. The math problem should contain a question part (indicated by \\\"Problem:\\\") and a corresponding solution in natural language (indicated by \\\"Informal proof:\\\"). Please note that the informal proof and the formal proof need to be identical. Please create a [QUESTION TYPE] in the level of [EDUCATIONAL LEVEL] based on the following knowledge point(s): [CONCEPT] in [DOMAIN]; [CONCEPT] in [DOMAIN].\\n\\nPlease then write the corresponding solution in natural language (indicated by \\\"Informal proof:\\\") given the \\\"# Problem: \\\", filling in the \\\"# Informal proof: \\\" section, and leaving the other section empty.\\n\\n# Problem: <generated problem>\\n\\n# Informal proof: ...\\n\\n# Formal proof in Lean 3: ...\\n\\n(T3)\\n\\nYou are a master in Lean. Now please come up with a math problem according to the following requirements. The math problem should contain a question part (indicated by \\\"Problem:\\\") and a corresponding solution in natural language (indicated by \\\"Informal proof:\\\"). Please note that the informal proof and the formal proof need to be identical. Please create a [QUESTION TYPE] in the level of [EDUCATIONAL LEVEL] based on the following knowledge point(s): [CONCEPT] in [DOMAIN]; [CONCEPT] in [DOMAIN].\\n\\nPlease translate the \\\"# Informal proof:\\\" section into Lean 3 and fill in the \\\"# Formal proof in Lean 3: \\\" section.\\n\\n# Problem: <generated problem>\\n\\n# Informal proof: <generated informal proof>\\n\\n# Formal proof in Lean 3: ...\"}"}
{"id": "8xliOUg9EW", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 15: A successful case generated by MUSTARD.\\n\\n**Question Type:** Word Problem. **Educational Level:** Elementary School. **k=1. Successful case.**\\n\\n**Concept(s):** Division in 4th grade.\\n\\n**Comments:** MUSTARD accurately counts the number of people and correctly performs division.\\n\\n**Informal Statement**\\nJohn and his 3 friends are on a trip. They bought a huge box of chocolates containing 240 pieces. They decided to split the chocolates equally among themselves. How many chocolates will each one of them get?\\n\\n**Informal Proof**\\nSince there are 4 people in total and they have 240 chocolates, we can find out how many chocolates each person will get by dividing the total number of chocolates by the total number of people. So, 240 divided by 4 equals 60. Therefore, each person will get 60 chocolates.\\n\\n**Formal Statement and Proof**\\n```lean\\ndef divide (x y : \u2115) := x / y\\n\\n#reduce divide 240 4\\n-- The output should be 60, which means each person will get 60 chocolates.\\n```\\n\\nTable 16: A failed case generated by MUSTARD.\\n\\n**Question Type:** Theorem Proving. **Educational Level:** Elementary School. **k=1. Failed case.**\\n\\n**Concept(s):** Quadrilaterals in 3rd grade.\\n\\n**Comments:** MUSTARD believes that a quadrilateral with four equal sides is a square. It treats a necessary condition as a sufficient condition.\\n\\n**Informal Statement**\\nGiven a quadrilateral ABCD where AB = BC = CD = DA = 5 units, prove that this quadrilateral is a square.\\n\\n**Informal Proof**\\nThe given quadrilateral ABCD has all its sides equal. This is a property of a square. Hence, ABCD is a square.\\n\\n**Formal Statement and Proof**\\n```lean\\nimport data.real.basic\\n\\ndef is_square (AB BC CD DA : \u211c) : Prop := AB = BC \u2227 BC = CD \u2227 CD = DA\\n\\ndef ABCD_is_square : \u2200 (AB BC CD DA : \u211c), AB = 5 \u2227 BC = 5 \u2227 CD = 5 \u2227 DA = 5 \u2192 is_square AB BC CD DA :=\\nbegin\\nintros AB BC CD DA h,\\ncases h with h\u2081 h\u2082,\\ncases h\u2082 with h\u2083 h\u2084,\\ncases h\u2084 with h\u2085 h\u2086,\\nsplit,\\n{ rw h\u2081, exact h\u2083 },\\n{ rw h\u2083, exact h\u2085 },\\n{ rw h\u2085, exact h\u2086 },\\nend\\n```\"}"}
{"id": "8xliOUg9EW", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 25: A failed case generated by M$\\\\text{USTARD}$. \\n\\nQuestion Type: Theorem Proving. Educational Level: Higher Education. k=2. Failed case.\\n\\nConcept(s): Green\u2019s, Stokes\u2019, and the divergence theorems in Multivariable calculus; Differential equations in Integral Calculus.\\n\\nComments: M$\\\\text{USTARD}$ does not finish the formal proof with sorry.\\n\\nInformal Statement\\n\\nLet $F$ be a continuously differentiable vector field on $\\\\mathbb{R}^3$. Let $S$ be the boundary of the unit ball $B$ in $\\\\mathbb{R}^3$, oriented by the outward normal. Let $n$ denote the outward unit normal on $S$. Prove that $\\\\int_S \\\\text{div} F \\\\, ds = \\\\int_B F \\\\cdot n \\\\, dv$, where \u201cdiv\u201d denote the divergence of $F$, \u201c\u2022\u201d denotes the dot product, and \u201c$\\\\int_S$\u201d, \u201c$\\\\int_B$\u201d denote the surface integral and line integral respectively.\\n\\nInformal Proof\\n\\nThis is an application of the divergence theorem, also known as Gauss\u2019s theorem. The divergence theorem states that the surface integral of a vector field over a surface $S$ is equal to the volume integral of the divergence of the field over the volume $V$ enclosed by $S$. In mathematical terms, $\\\\int_S F \\\\cdot n \\\\, ds = \\\\int_V \\\\text{div} F \\\\, dv$.\\n\\nIn our case, the volume $V$ is the unit ball $B$ in $\\\\mathbb{R}^3$, and the surface $S$ is the boundary of $B$. The divergence theorem thus gives us $\\\\int_S F \\\\cdot n \\\\, ds = \\\\int_B \\\\text{div} F \\\\, dv$.\\n\\nHowever, since $B$ is the unit ball, its volume is 1, so the volume integral over $B$ is just the average value of div $F$ over $B$. Therefore, $\\\\int_B \\\\text{div} F \\\\, dv = \\\\int_S F \\\\cdot n \\\\, ds$.\\n\\nPutting everything together, we obtain $\\\\int_S F \\\\cdot n \\\\, ds = \\\\int_S \\\\text{div} F \\\\, ds$, as required.\\n\\nFormal Statement and Proof\\n\\n```\\nimport analysis.calculus.fderiv\\nimport measure.theory.integration\\nimport analysis.normed.space.inner\\nimport data.real.basic\\nimport topology.instances.real\\nimport analysis.special.functions.trigonometric\\n\\nopen locale classical\\n\\nnoncomputable theory\\n\\n/- Let's define $\\\\mathbb{R}^3$ as Euclidean space -/\ndef $\\\\mathbb{R}^3$ := euclidean.space $\\\\mathbb{R} (\\\\text{fin } 3)$\\n\\n/- Define the ball and boundary -/\ndef $B$ := metric.ball (0: $\\\\mathbb{R}^3$) 1\\ndef $S$ := metric.sphere (0: $\\\\mathbb{R}^3$) 1\\n\\n/- Assume $F$ is a continuously differentiable vector field on $\\\\mathbb{R}^3 -/\nvariables (F : $\\\\mathbb{R}^3 \\\\to \\\\mathbb{R}^3) (hF : \\\\forall x, \\\\text{differentiable at } F x)\\n\\n/- Define the divergence of $F -/\ndef divF := \u03bb x, (fderiv R F x).trace\\n\\n/- Define the outward unit normal on $S -/\ndef n := \u03bb x: $\\\\mathbb{R}^3$, x\\n\\n/- Now we can state the theorem -/\ndef div eq surface_integral : \\\\forall x in B, divF F x = \\\\forall x in S, inner (F x) (n x) :=\\nbegin\\n  -- This is where the main proof would go, using the divergence theorem\\n  -- and other facts about integration.\\nsorry\\nend\\n```\"}"}
{"id": "8xliOUg9EW", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 26: A failed case generated by MUSTARD.\\n\\nQuestion Type: Theorem Proving. Educational Level: Higher Education. k=2. Failed case.\\n\\nConcept(s): Matrix transformations in Linear algebra.\\n\\nComments: The generated informal statement has factual errors.\\n\\nInformal Statement\\nLet's consider a function $f: \\\\mathbb{R} \\\\to \\\\mathbb{R}$ that is differentiable at a point $x_0$. Let's also consider a $2 \\\\times 2$ matrix $A$.\\n\\nWe define a new function $g(x) = A \\\\cdot f(x)$. Prove that $g$ is also differentiable at $x_0$, and find the derivative of $g$ at $x_0$.\\n\\nInformal Proof\\nSince $f$ is differentiable at $x_0$, the limit of the difference quotient exists at this point. Denote this limit as $f'(x_0)$.\\n\\nWe know that matrix multiplication is a linear operation. So we can move the limit inside the matrix multiplication to find the derivative of $g$ at $x_0$.\\n\\nSo, $g'(x_0) = A \\\\cdot f'(x_0)$.\\n\\nFormal Statement and Proof\\nimport data.matrix.basic\\nimport analysis.calculus.deriv\\n\\nvariables f : R \u2192 R\\nA : matrix (fin 2) (fin 2)\\nR x_0 : R\\n\\nvariable (h : differentiable at R f x_0)\\ninclude h\\n\\nlemma matrix times differentiable function : differentiable at R (\u03bb x, A \u00b7 (\u03bb i, f x) i) x_0 :=\\nbegin\\napply matrix.has fderiv at diff at,\\nexact h.has deriv at,\\nend\\n\\nlemma derivative of matrix times function : deriv (\u03bb x, A \u00b7 (\u03bb i, f x) i) x_0 = A \u00b7 (\u03bb i, deriv f x_0) i :=\\nbegin\\napply has deriv at.deriv,\\nexact matrix times differentiable function h,\\nend\"}"}
{"id": "8xliOUg9EW", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 27: Prompt template for math word problem inference.\\n\\nZero shot prompt template for math word problem inference\\n\\nYou are an expert in math. Answer the following math word problem.\\n\\nQuestion: <question>\\n\\nAnswer: Let's think step by step.\\n\\nFew shot prompt template for math word problem inference\\n\\nYou are an expert in math. Answer the following math word problem.\\n\\nQuestion: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\n\\nAnswer: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQuestion: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\n\\nAnswer: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQuestion: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\n\\nAnswer: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\\n\\nQuestion: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\n\\nAnswer: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\\n\\nQuestion: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\n\\nAnswer: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\\n\\nQuestion: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\\n\\nAnswer: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\\n\\nQuestion: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\n\\nAnswer: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.\\n\\nQuestion: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\n\\nAnswer: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.\"}"}
{"id": "8xliOUg9EW", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 28: Compared performances on GSM8K (G) and MATH (M) between Llama 2-7B and Llama 2-70B.\\n\\n| MODEL     | Zero (G) | Few (G) | Zero (M) | Few (M) |\\n|-----------|----------|---------|----------|---------|\\n| Baselines |          |         |          |         |\\n| Llama 2-7B | 7.2      | 12.8    | 2.0      | 2.6     |\\n| Llama 2-70B | 31.7     | 54.1    | 8.8      | 13.4    |\\n| Fine-tuning |          |         |          |         |\\n| Llama 2-7B | 9.6      | 16.0    | 3.2      | 3.8     |\\n| Llama 2-7B | 9.1      | 14.9    | 2.4      | 3.2     |\\n| Llama 2-7B | 9.5      | 15.4    | 3.0      | 3.6     |\\n| Llama 2-70B | 36.6     | 55.8    | 10.0     | 14.4    |\\n| Llama 2-70B | 33.4     | 53.7    | 9.2      | 13.6    |\\n| Llama 2-70B | 36.1     | 55.4    | 9.6      | 14.2    |\\n| Llama 2-70B | 39.5 (+8.42%) | 59.1 (+6.68%) | 10.4 (+8.33%) | 15.0 (+5.30%) |\\n\\nTable 29: Nearest data points between MUSTARD_SAUCE and miniF2F.\\n\\n| MUSTARD_SAUCE v.s. miniF2F (cosine similarity = 0.6439) |\\n|---------------------------------------------------------|\\n\\nInformal Statement:\\nAlex has 5 ten-dollar bills and 3 one-dollar bills. How much money does Alex have in total?\\n\\nInformal Proof:\\nTo find out how much money Alex has in total, we need to multiply the number of each type of bill by its value.\\n\\nSo, Alex has 5 ten-dollar bills, which equals 5 * 10 = 50 dollars. He also has 3 one-dollar bills, which equals 3 * 1 = 3 dollars. Adding these two amounts together gives 50 + 3 = 53 dollars. Therefore, Alex has 53 dollars in total.\\n\\nFormal Statement and Proof:\\ndef calculate_money(tens : N) (ones : N) := tens * 10 + ones * 1\\nexample : calculate_money 5 3 = 53 :=\\nbegin\\n  rw calculate_money,\\n  refl,\\n  end\\n\\nminiF2F Case\\n\\ntheorem algebra_sqineq_unitcircatbpamblt1 (a b : R) (h : a^2 + b^2 = 1) :\\na * b + (a - b) le 1 :=\\nbegin\\n  nlinarith [sq nonneg (a - b)],\\n  end\"}"}
{"id": "8xliOUg9EW", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Pass@1 of formal proofs generated by MUSTARD. k: Number of seed concepts.\\n\\n| k    | Elem (GPT-4) | elem (GPT-3.5) | step corrections | Med (GPT-4) | Med (GPT-3.5) | step corrections | High (GPT-4) | High (GPT-3.5) | step corrections | Higher (GPT-4) | Higher (GPT-3.5) | step corrections |\\n|------|--------------|----------------|------------------|------------|---------------|-----------------|--------------|---------------|-----------------|--------------|------------------|-----------------|\\n| 1    | 26.0         | 48.0 (+22.0)   | 0 (\u2206)           | 15.1       | 38.0          | 2 (\u2206)          | 40.1        | 67.0 (+45.3)   | 40.1         | 22.2         | 67.0 (+45.3)   |\\n| 2    | 24.1         | 42.3 (+18.2)   | 12.8 (\u2206)        | 17.3       | 22.1          | 5.7 (\u2206)        | 31.9        | 22.1          | 31.9         | 22.1         | 22.1          |\\n\\nFigure 4: Distributions of formal proof lengths.\\n\\nFigure 4: Distributions of formal proof lengths.\\n\\n4.6 DIVERSITY AND DIFFICULTY\\n\\nWe compute ROUGE-L (Lin, 2004) to check the diversity of generated informal statements and proofs. The resulting ROUGE-L scores are below 0.25 and indicate high data diversity. We demonstrate detailed computation and results in Appendix D.2. We then investigate the proof lengths in MUSTARD and the distributions are demonstrated in Figure 4. We count both reasoning steps of formal statement-proof pairs and steps of formal proof only, which are shown on the left- and right-hand side of Figure 4, respectively. It is demonstrated that proof length increases over educational levels. Solving elementary problems needs about 5 to 10 steps while solving higher-educational problems requires a median number between 10 to 15 steps. The most challenging problems require around 30 reasoning steps or about 20 formal proof steps. Therefore, MUSTARD produces diverse mathematical problems with multiple topics and difficulties.\\n\\n5 CONCLUSION\\n\\nIn this paper, we introduce MUSTARD to automatically generate mathematical datasets with high-quality solutions that cover a variety of mathematical skills. Leveraging the LLM and Lean Prover, MUSTARD can generate the problem statement, informal solution, and formal solution and use a Lean Prover to automatically verify the formal solution and provide feedback for revision. At last, we apply the proposed MUSTARD and obtain 5,866 problems with step-by-step solutions that cover different educational levels and mathematical abilities. The obtained dataset has shown its high quality, diversity, and effectiveness in improving language models' mathematical reasoning performance, showing the great potential of our proposed MUSTARD and dataset in further research on language models.\"}"}
{"id": "8xliOUg9EW", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGEMENTS\\n\\nThis work was supported in part by the National Key R&D Program of China under Grant No. 2020AAA0109700, Guangdong Outstanding Youth Fund (Grant No. 2021B1515020061), National Natural Science Foundation of China (NSFC) under Grant No.61976233, Mobility Grant Award under Grant No. M-0461, Shenzhen Science and Technology Program (Grant No. GJHZ20220913142600001), Nansha Key RD Program under Grant No.2022ZD014, National Natural Science Foundation of China under Grant No.62006255, and the National Natural Science Foundation of China under Grant 62371411, the Research Grants Council of the Hong Kong SAR under Grant GRF 11217823, InnoHK initiative, the Government of the HKSAR, Laboratory for AI-Powered Financial Technologies. We thank MindSpore for the partial support of this work, which is a new deep learning computing framework.\\n\\nThe authors would also like to thank Hui Jin, Jianhao Shen, Chengwu Liu, Cen Li, and Junhao Cheng for their hard work on the manual check related to Section 4.2.\\n\\nREFERENCES\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. URL https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\\n\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021.\\n\\nLeonardo Mendon\u00e7a de Moura, Soonho Kong, Jeremy Avigad, Floris van Doorn, and Jakob von Raumer. The lean theorem prover (system description). In Amy P. Felty and Aart Middendorp (eds.), Automated Deduction - CADE-25 - 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings, volume 9195 of Lecture Notes in Computer Science, pp. 378\u2013388. Springer, 2015. doi: 10.1007/978-3-319-21401-6_26. URL https://doi.org/10.1007/978-3-319-21401-6_26.\\n\\nOlga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. ROSCOE: A suite of metrics for scoring step-by-step reasoning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=xYlJRpzZtsY.\\n\\nJesse Michael Han, Jason Rute, Yuhuai Wu, Edward W. Ayers, and Stanislas Polu. Proof artifact co-training for theorem proving with language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.\\n\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. 5 https://www.mindspore.cn/\"}"}
{"id": "8xliOUg9EW", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "8xliOUg9EW", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "8xliOUg9EW", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the proof-filtering stage, MUSTARD interacts with the Lean Prover (de Moura et al., 2015) to obtain validation messages of the proof steps, which guides data revision and filtering. Specifically, after a formal solution is passed to the Lean Prover and if the prover returns no error message, the corresponding data point is collected into the valid dataset. Otherwise, MUSTARD collects the error messages from the prover and prompts the language model to revise the invalid solution. To help the language model locate the incorrect lines described in the error messages, we also add a line number at the beginning of each line in the formal solutions. The verification and self-refinement are performed in multiple rounds until LLM generates a valid solution. We use the number of rounds to measure the difficulty of the generated sample, assuming a difficult problem is hard to solve by an LLM and requires more rounds of correction. The prompt template of a single round of correction is demonstrated as follows, and the complete prompt template is shown in Table 13 in Appendix C.1:\\n\\n```\\n# Formal proof (c) in Lean 3:\\n\\n```lean\\nline 1 <code>\\nline 2 <code>\\nline 3 <code>\\n...\\n```\\n\\n# Error messages for Formal proof (c) from Lean Prover:\\n<error messages>\\n\\n4.1 CASE STUDY\\n\\nWe first inspect the data points generated by MUSTARD. Table 1 shows a generated math problem in which MUSTARD creatively combines two mathematical concepts and constructs a reasonable question. The generated question includes knowledge from both concepts. It is suggested that MUSTARD can join the concepts and construct a reasonable question. Furthermore, Table 2 demonstrates a case that MUSTARD provides solid and comprehensive solutions in both natural language and Lean. Although the formal proof is long, it is consistent and passes the prover's validation. It is demonstrated that MUSTARD can generate long valid solutions.\\n\\n4.2 HUMAN EVALUATION\\n\\nTo further explore the quality of the data generated by MUSTARD, we recruit professionals who have expertise in mathematics and the Lean language to perform a sanity check on the data points. We randomly select 200 data points from the generated data, 100 of which pass the Lean Prover (Group Valid) and 100 of which do not (Group Invalid). The sanity check covers the four sections in each data point (i.e., informal statement, informal proof, formal statement, and formal proof), and includes factuality check and consistency check. Specifically, a high-quality data point should have a factually correct informal statement (D1) and a correct solution (D4). The formal statement and proof should be aligned with the informal descriptions (D5, D6). Moreover, the desired data point should meet the specified seed concepts (D2) and question type (D3). The six inspection dimensions and their requirements are demonstrated in Table 3. A data point is scored 1 in a dimension if it meets the requirement, otherwise, it gets 0.\\n\\nThe accuracies of Group Valid and Group Invalid in each dimension are demonstrated in Table 3. We also report the corresponding p-value in each dimension. (D4) and (D6) show significant differences in accuracy between the two groups. The results indicate that high-quality data points have significantly better auto-formalization results. As a result, given the validation of formal proofs by the Lean Prover, the data points have guaranteed high-quality informal proofs. Moreover, (D1) also shows significance with the inspected data scaled up. The differences in statement alignment (D5) and informal statement relevance (D2) of the two groups are less significant. Furthermore, no significant differences are observed in question type classification (D3), which indicates that Lean Prover validation in MUSTARD does not significantly influence the classification. Overall, the human evaluation results suggest that formally validated data have significantly higher quality.\\n\\n4.3 DATA QUALITY BY DOWNSTREAM APPLICATION\\n\\nTo evaluate the impact of MUSTARD on enhancing mathematical reasoning abilities, we use the data to fine-tune smaller-scale language models and evaluate them on math word problems (MWP) and automated theorem proving (ATP). Specifically, given all the generated data, we first...\"}"}
{"id": "8xliOUg9EW", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: An informal statement generated by MUSTARD. The complete data point is demonstrated in Table 18 in Appendix E.\\n\\nQuestion Type: Theorem Proving. Educational Level: Middle School. k=2. Concept(s): Geometry in 8th grade; Algebraic expressions in Algebra basics.\\n\\nInformal Statement\\nGiven a rectangle ABCD where AB is $x + 5$ and AD is $2x - 3$. Prove that the area of the rectangle ABCD is $(2x^2 + 7x - 15)$ square units.\\n\\nTable 2: A data point generated by MUSTARD.\\n\\nQuestion Type: Word Problem. Educational Level: Higher Education. k=1. Concept(s): Series in Integral Calculus.\\n\\nInformal Statement\\nProve that the integral of the series $\\\\sum_{n=1}^{\\\\infty} \\\\frac{-1}{n+1}$ equals $\\\\ln(2)$.\\n\\nInformal Proof\\nWe start by recognizing the series $\\\\sum_{n=1}^{\\\\infty} \\\\frac{-1}{n+1}$ as the alternating harmonic series, which is known to converge to $\\\\ln(2)$.\\n\\nFormal Statement and Proof\\n\\n```lean\\nimport data.real.basic\\nimport analysis.special_functions.pow\\n\\nopen real\\n\\nlemma integral_of_series : \u222b x \u2208 0..1, (ln (1 + x)) = \u222b n : \u2115, ((-1)^n / (n + 1)) :=\\nbegin\\n  have hder : \u2200 x \u2208 set.Icc (0 : \u211d) 1, deriv (\u03bb x, (ln (1 + x))) x = 1 / (1 + x),\\n  { intros x hx, have hpos : 0 < 1 + x := by nlinarith, exact deriv ln one add hpos },\\n  have hint : interval_integral (\u03bb x, 1 / (1 + x)) 0 1 volume = \u222b n : \u2115, ((-1)^n / (n + 1)),\\n  { have hfrac : \u2200 n : \u2115, x \u2208 0..1, x^n = 1 / (n + 1),\\n    { intro n, calc \\\\[ x \u2208 0..1, x^n = [x^{(n+1)} / (n+1)] | 0..1 : integral_pow n \\\\] = 1 / (n + 1) : by { rw integral_interval, simp } },\\n    rw [interval_integral.integral_of_le, hfrac], simp, linarith },\\n  have heq : \u222b x \u2208 0..1, (ln (1 + x)) = \u222b x \u2208 0..1, 1 / (1 + x),\\n  { congr, ext, exact hder x (set.mem_Icc.mpr \u3008by nlinarith, by nlinarith\u3009) },\\n  rw [heq, hint],\\nend\\n```\\n\\nhave 5,866 valid data points that pass the Lean Prover. We denote this subset MUSTARD-\\\\text{valid}. We then extract the same number of invalid data points as the subset of MUSTARD-\\\\text{invalid}, and extract an equal size of random subset MUSTARD-\\\\text{random}. Each MUSTARD subset contains 5,866 data points. Moreover, we randomly split MUSTARD-\\\\text{valid} into 4,866 training data, 500 validation data, and 500 test data for benchmarking model performances on the dataset. We denote the test set as MUSTARD-\\\\text{test}. Furthermore, we also test on the entire generated data set with 28,316 data points, which we denote MUSTARD-\\\\text{tt}.\\n\\nWe employ LoRA (Hu et al., 2021) for fine-tuning the open-source GPT2-large (Radford et al., 2019), Llama 2-7B and Llama 2-70B (Touvron et al., 2023) on each MUSTARD subset. De-\"}"}
{"id": "8xliOUg9EW", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Inspection dimensions and requirements in human evaluation. IS: Informal Statement. IP: Informal Proof. FS: Formal Statement. FP: Formal Proof. RT: Reasoning Type. Significant p < 0.005 are marked with bold.\\n\\n| Inspection Dimension | Requirement | Valid | Invalid | \\\\( p \\\\)-value |\\n|----------------------|-------------|-------|---------|----------------|\\n| (D1) IS Correctness   | Whether the informal statement is factually correct. | 93.50 | 83.50 | 0.00167 |\\n| (D2) IS Relevance     | Whether the informal statement is relevant to each seed concept. | 87.50 | 92.50 | 0.09604 |\\n| (D3) RT Classification| Whether the informal statement is of the required question type. | 67.00 | 68.50 | 0.74903 |\\n| (D4) IP Correctness   | Whether the informal proof correctly solves the informal statement. | 88.50 | 73.50 | 0.00012 |\\n| (D5) IS-FS Alignment  | Whether the informal statement and the formal statement describe the same problem and are aligned with each other. | 74.00 | 66.50 | 0.10138 |\\n| (D6) IP-FP Alignment  | Whether the informal proof and the formal proof describe the same solution and have aligned proof steps. | 72.00 | 54.00 | 0.00018 |\\n\\nTable 4: Maj1@1 results on GSM8K (G) and MATH (M). Zero: Zero-shot. Few: Few-shot. \\\\( \\\\text{gt} \\\\) denotes a fine-tuning step.\\n\\n| MODEL | Zero (G) | Few (G) | Zero (M) | Few (M) |\\n|-------|----------|---------|----------|---------|\\n| Baselines | | | | |\\n| GPT2-large | 3.4 | 5.1 | 0.6 | 1.0 |\\n| Llama 2-7B | 7.2 | 12.8 | 2.0 | 2.6 |\\n| Fine-tuning | | | | |\\n| GPT2-large | 4.2 | 6.8 | 1.4 | 2.4 |\\n| GPT2-large | 3.9 | 6.4 | 1.2 | 2.0 |\\n| GPT2-large | 4.1 | 6.7 | 1.4 | 2.2 |\\n| GPT2-large | 4.6 (+12.20%) | 7.0 (+4.48%) | 1.8 (+28.57%) | 2.8 (+27.27%) |\\n| Llama 2-7B | 9.6 | 16.0 | 3.2 | 3.8 |\\n| Llama 2-7B | 9.1 | 14.9 | 2.4 | 3.2 |\\n| Llama 2-7B | 9.5 | 15.4 | 3.0 | 3.6 |\\n| Llama 2-7B | 10.3 (+8.42%) | 16.9 (+9.74%) | 3.2 (+6.67%) | 4.2 (+16.67%) |\\n| tailed model configuration and training procedure are described in Appendix F. For the task of math word problems, we use GSM8K (Cobbe et al., 2021) and MATH dataset Hendrycks et al. (2021) for evaluation. For evaluating automated theorem proving, we use Mathlib and the miniF2F (Zheng et al., 2022) benchmark. We also evaluate models on MUSTARDSAUCE-test after being fine-tuned on the MUSTARDSAUCE-valid training split. Tables 4 and 5 demonstrate the model performances. We also follow Han et al. (2022) to ablate the fine-tuning steps and demonstrate the results in Table 6.\\n\\nIn general, fine-tuning the models on MUSTARDSAUCE improves the mathematical reasoning of the models. On average, we have an 18.15% relative performance gain after fine-tuning with MUSTARDSAUCE-valid compared with fine-tuning with MUSTARDSAUCE-random in ATP (Table 5) and 11.01% in MWP (Table 4). The fine-tuned Llama 2-7B achieves average gains of 15.41% and 8.18% on ATP and MWP, and the fine-tuned GPT 2-large 20.89% and 15.41%, respectively. Specifically, in ATP, the Llama 2-7B achieves significant performance gains of 16.00% on both mathlib and miniF2F, while increasing 17.31% on the MUSTARDSAUCE-test. In MWP, the performance improvements are also consistent in two datasets and both zero-shot and few-shot inference. We further compare the results fine-tuned with MUSTARDSAUCE-tt and MUSTARDSAUCE-valid. We find that models fine-tuned with the entire generated data are inferior to models fine-tuned with MUSTARDSAUCE-valid. Although the increase in the amount of fine-tuned data makes the model perform better compared to fine-tuning on MUSTARDSAUCE-invalid and MUSTARDSAUCE-random.\"}"}
{"id": "8xliOUg9EW", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: Pass@1 results on automated theorem proving tasks.\\n\\n| Model                  | Pass@1 | Error@1 | Error@2 | Pass@1 | Error@1 | Error@2 |\\n|------------------------|--------|---------|---------|--------|---------|---------|\\n| **GPT2-large**         |        |         |         |        |         |         |\\n| in                     | 2.0    | 0.0     | 6.0     | 5.9    | 2.0     | 8.2     |\\n| ra                     | 3.0    | 1.2     | 7.0     | 6.6    | 2.9     | 9.6     |\\n| va                     | 8.3    | (+23.33%) | 3.7   | (+27.59%) | 12.2  | (+18.57%) |\\n|                       |        |         |         |        |         |         |\\n| **Llama 2-7B**         |        |         |         |        |         |         |\\n| in                     | 5.8    | 1.2     | 8.6     | 11.6   | 5.7     | 12.6    |\\n| ra                     | 7.5    | 2.5     | 10.4    | 14.7   | 6.6     | 13.2    |\\n| va                     | 8.7    | (+16.00%) | 2.9   | (+16.00%) | 12.2  | (+17.31%) |\\n|                       |        |         |         |        |         |         |\\n\\nTable 6: Ablation study of different fine-tuning settings.\\n\\n| Model                  | Pass@1 | Error@1 | Error@2 | Pass@1 | Error@1 | Error@2 |\\n|------------------------|--------|---------|---------|--------|---------|---------|\\n| **GPT2-large**         |        |         |         |        |         |         |\\n| va                     | 8.3    |         |         | 9.8    |         |         |\\n|                         |        |         |         |        |         |         |\\n| **Llama 2-7B**         |        |         |         |        |         |         |\\n| va                     | 12.2   |         |         | 13.8   |         |         |\\n\\nFigure 3: Model performances fine-tuned with different data scales.\\n\\n4.4 Impact of Data Scalability\\n\\nTo further study the impact of data scale on the fine-tuning results, we randomly sample 75%, 50%, 25%, and 0% data from MUSTARD-SAUCE-valid and fine-tune Llama 2-7B. The results are shown in Figure 3. In general, the results on all datasets increase as the fine-tuning data scales up. Specifically, performances on the MUSTARD-test and mathlib have the most significant growth without a decrease in the growth rate. Therefore we expect further performance improvements when more high-quality data are included.\\n\\n4.5 PASS RATE\\n\\nWe study the mathematical generation ability of MUSTARD by investigating its pass rates on generating valid data points. The pass@1 results of the generated formal proofs of GPT-4 OpenAI (2023) and GPT-3.5 OpenAI (2022) are shown in Table 7. We have the following observations.\\n\\nFirst of all, the overall pass@1 results are high, showing the LLMs especially GPT-4 capable of performing zero-shot mathematical reasoning. Second, the pass rates of word problems are generally higher than those of theorem proving. It indicates that the word problems are relatively easier and more familiar to the LLMs, while theorem proving is more challenging. Third, the all-at-once generation and step-by-step generation have similar pass rates at lower educational levels. For more challenging questions such as those at the high school level and higher educational level, step-by-step generation shows slight advantages over the all-at-once generation. This indicates that dividing and conquering (T1), (T2), and (T3) helps the model to generate higher-quality formal proofs, but the improvement is limited. Last but not least, the improvements in pass rates after 1-step and 2-step generation are limited.\"}"}
