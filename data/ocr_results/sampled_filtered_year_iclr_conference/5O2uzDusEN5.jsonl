{"id": "5O2uzDusEN5", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nDFLOW: LEARNING TO SYNTHESIZE BETTER OPTICAL DATASETS VIA A DIFFERENTIABLE PIPELINE\\n\\nKwon Byung-Ki 1, Nam Hyeon-Woo 1, Ji-Yun Kim 1, Tae-Hyun Oh 1, 2, 3\\n\\n1 Department of Electrical Engineering, POSTECH\\n2 Graduate School of AI, POSTECH\\n3 Institute for Convergence Research and Education in Advanced Technology, Yonsei University\\n\\n{byungki.kwon, hyeonw.nam, junekim, taehyun}@postech.ac.kr\\n\\nABSTRACT\\n\\nComprehensive studies of synthetic optical flow datasets have attempted to reveal what properties lead to accuracy improvement in learning-based optical flow estimation. However, manually identifying and verifying the properties that contribute to accurate optical flow estimation require large-scale trial-and-error experiments with iteratively generating whole synthetic datasets and training on them, i.e., impractical. To address this challenge, we propose a differentiable optical flow data generation pipeline and a loss function to drive the pipeline, called DFlow. DFlow efficiently synthesizes a dataset effective for a target domain without the need for cumbersome try-and-errors. This favorable property is achieved by proposing an efficient dataset comparison method that uses neural networks to approximately encode each dataset and compares the proxy networks instead of explicitly comparing datasets in a pairwise way. Our experiments show the competitive performance of our DFlow against the prior arts in pre-training. Furthermore, compared to competing datasets, DFlow achieves the best fine-tuning performance on the Sintel public benchmark with RAFT.\\n\\nINTRODUCTION\\n\\nOptical flow is a fundamental computer vision problem to find dense pixel-wise correspondences between two subsequent frames in a video. Optical flow is indeed a key building block in many practical applications, including video understanding, action analysis, video enhancement, editing, 3D vision, etc. Recently, optical flow has been significantly advanced by learning-based approaches with deep neural networks (Fischer et al., 2015; Ilg et al., 2017; Ranjan & Black, 2017; Hui et al., 2018; Sun et al., 2018; Teed & Deng, 2020) in terms of accuracy and efficiency. A driving force of these prior arts is large-scale supervised datasets. However, it is difficult to collect a reasonable amount of real-world optical flow labels. Thus, they exploited large-scale synthetic datasets, e.g., Fischer et al. (2015); Mayer et al. (2016), which has become the standard in optical flow, e.g., training on FlyingChairs (Fischer et al., 2015) followed by FlyingThings3D (Mayer et al., 2016). After the seminal studies, there have been various efforts to build different synthetic datasets (Gaidon et al., 2016; Richter et al., 2017; Lv et al., 2018; Oh et al., 2018; Aleotti et al., 2021). Despite the vast efforts of these studies, it remains unclear which factors are important for an effective synthetic dataset construction against the target domain.\\n\\nInstead of manually identifying important design criteria, AutoFlow (Sun et al., 2021) pioneers the first learning-based approach to go beyond being heuristic by posing data generation as a hyperparameter optimization problem maximizing validation performance on a target dataset. AutoFlow generates data samples by composing simple 2D layers with non-differentiable hyperparameters, which are optimized by sampling-based evolutionary search. The use of evolutionary search requires large resources, which is burdensome because each target scenario requires to re-generate different datasets.\\n\\nTo address this challenge, we propose DFlow, which is an efficient synthetic optical flow dataset generation method. We compose each data sample by simple differentiable graphic operations, such as warping layer and real-world effects, so that each sample can be parameterized in a learnable manner. This allows us to exploit efficient gradient descent methods to generate each sample.\"}"}
{"id": "5O2uzDusEN5", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"thereby DFlow is more than an order of magnitude efficient than AutoFlow in GPU hours when constructing the same amount of training data. We also introduce a new loss function that learns the data parameters by contrasting a target dataset from a base dataset, e.g., FlyingChairs. Since directly using large datasets in the contrastive learning process is cumbersome, we approximate the base and target datasets by two neural networks trained on respective datasets as proxies. This approximation allows an end-to-end differentiable pipeline from the data parameters to the loss function.\\n\\nThrough comprehensive experiments, we show that DFlow is effective in both pre-training and fine-tuning. The DFlow data has a size of 512 \u00d7 384, which is the same as FlyingChairs, but the RAFT network (Teed & Deng, 2020) pre-trained on DFlow achieves comparable performance compared to the high-resolution competing datasets (Sun et al., 2021; Mayer et al., 2016). In addition, compared to competing datasets, the RAFT model initially pre-trained on DFlow achieves the best fine-tuning performance on the Sintel public benchmark. We summarize our contributions as follows:\\n\\n- A simple and efficient differentiable data generation pipeline for optical flow (refer to Table 1);\\n- A contrastive-style learning scheme and its loss function by approximating expensive dataset-to-dataset comparison by leveraging proxy neural networks (refer to Sec 3);\\n- Compared to competing datasets, DFlow achieves the best fine-tuning performance on the Sintel public benchmark with RAFT. (refer to Table 4).\\n\\n2 RELATED WORK\\n\\nOptical Flow.\\n\\nDense optical flow estimation is to find pixel-wise correspondences from the brightness patterns of images (Gibson, 1950; Gibson & Carmichael, 1966; Horn & Schunck, 1981). After conventional optimization algorithms (Black & Anandan, 1993; Zach et al., 2007), deep-learning algorithms (Fischer et al., 2015; Ilg et al., 2017) become dominant due to their computational efficiency and reasonable performance. Prior arts (Xu et al., 2017; Bailer et al., 2017; Wulff et al., 2017; Sun et al., 2018) have attempted to implement explicit neural modules that are suitable for optical flow estimation. Recently, RAFT (Teed & Deng, 2020) adopts recurrent architectures and achieves a notable performance improvement, which is represented as state-of-the-art.\\n\\nThose recent advances in learning-based approaches require large-scale data with ground-truth, but labeling dense optical flow is a highly undetermined task, i.e., challenging (Fischer et al., 2015). The previous real-world datasets have been built under sophisticated labeling conditions, including the special sensor hardware, controlled environment, or limited objects (Scharstein & Szeliski, 2002; Scharstein & Pal, 2007; Geiger et al., 2012; Kondermann et al., 2014). It leads to the limitation of the size of datasets. To relieve this issue, synthetic datasets (Fischer et al., 2015; Mayer et al., 2016) have been proposed and achieved remarkable accuracy despite the gap between real and synthetic datasets. After that, previous arts endeavor to construct more realistic synthetic datasets (Gaidon et al., 2016; Richter et al., 2017; Lv et al., 2018). The prior arts (Aleotti et al., 2021; Han et al., 2022) generate the subsequent frames and ground-truth optical flow by warping the previous frame. These do not handle the photometric inconsistency that is common in real-world scenes. In this work, we propose a differentiable synthetic data generation pipeline with the target knowledge so that the generated dataset could improve its performance more.\\n\\nLearning-based Optical Flow Dataset.\\n\\nAutoFlow (Sun et al., 2021) is the first learning-based data generation approach in optical flow, but with the sampling-based evolutionary search for non-differentiable optimization. It is our closest related work in the sense that they learn the data generation parameters for performance improvement on the specific target dataset. However, distinctively, our method is the first differentiable method to learn data generation parameters, which leads to a more efficient pipeline than AutoFlow in terms of computation cost and GPU hours for data generation. We list other differences in Table 1. Recently, RealFlow (Han et al., 2022) proposes an iterative learning framework that learns enhanced flow estimation and pseudo ground-truth generation, alternatively. Different from our work, this work suggests a framework including iterative model training and dataset generation, and does not suggest a goodness measure of a resulting dataset, which we address.\\n\\nOther than optical flow, there are also interesting attempts to generate synthetic data in learnable...\"}"}
{"id": "5O2uzDusEN5", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 2: PyTorch-style pseudo-code for DFlow.\\n\\n# G: data generator\\n# B: base network\\n# T: target network\\n# L: our task loss function\\n# Lt: target loss function\\n# Lb: base loss function\\n\\n\u03b8\\n\\nfor l, m in loader:\\n# Generate optical flow data\\n# l: layer images, m: layer masks\\nimage1, image2, label = G(l, m, \u03b8)\\n# loss\\nlt = Lt(B(image1, image2), label)\\nlb = Lb(T(image1, image2), label)\\nloss = L(lt, lb)\\n# Update\\n\u03b8\\nloss.backward()\\noptimizer.step()\\n\\nThe above parameterization is efficient for approximating multi-layer geometric warpings because it only introduces two additional parameters per layer to generate the local movement of each layer from a single warping field \\\\( W_t^{t+1} \\\\rightarrow t \\\\). As aforementioned, we propose to use the two steps of warping with \\\\( W_0 \\\\rightarrow t+1 \\\\) and \\\\( W_t^{t+1} \\\\rightarrow t \\\\). This is distinctive from the existing works, including Fischer et al. (2015); Sun et al. (2021), where they only use a single step of warping from a randomly generated image to a synthetic next frame. This only parameterizes the next frames, not the reference frames randomly generated. This could not update the textures and mask shapes of the reference frames, whereas our method parameterizes both frames with two warping fields, \\\\( W_0 \\\\rightarrow t+1 \\\\) and \\\\( W_t^{t+1} \\\\rightarrow t \\\\). It allows us to jointly update subsequent frames.\\n\\nA.4 LAYER COMPOSITION\\n\\nFrom the geometric warping, we have \\\\( N \\\\) pairs of layer images, masks, and warping fields in order of depth. We adopt the softmax splatting (Niklaus & Liu, 2020) to compose the layer images at subsequent frames and warping fields with flow field translation, i.e., \\\\( L_{lt}, L_{lt+1}, W_{lt+1} \\\\rightarrow t \\\\). We get the importance image \\\\( Z \\\\) and weight image \\\\( K \\\\) of each layer and frame,\\n\\n\\\\[\\nZ_{lt} = \\\\max_{l} (M_{lt}) - M_{lt} a_{l}\\n\\\\]\\n\\n(8)\\n\\n\\\\[\\nK_{lt} = \\\\exp(Z_{lt}) \\\\frac{1}{\\\\sum_{l=1}^{N} \\\\exp(Z_{lt})}\\n\\\\]\\n\\n(9)\\n\\nwhere \\\\( a_{l} = c_{l} \\\\) and \\\\( c \\\\) is a constant value which we set 6 for layer composition. As the value of \\\\( c \\\\) increases, sharper boundaries can be obtained. The subsequent frames and ground-truth optical flow are as follows:\\n\\n\\\\[\\n(N \\\\sum_{l=1}^{N} K_{lt} \\\\odot L_{lt+1}, N \\\\sum_{l=1}^{N} K_{lt} \\\\odot L_{lt}, N \\\\sum_{l=1}^{N} K_{lt} \\\\odot W_{lt+1} \\\\rightarrow t ) \\\\rightarrow \\\\{ I_{t+1}, I_{t}, F_{GT} \\\\}\\n\\\\]\\n\\n(10)\\n\\nA.5 COLOR PERTURBATION AND REAL-WORLD EFFECTS\\n\\nColor Perturbation. Color perturbation consists of \\\\( N \\\\) white balances to adjust the color intensity of layer images. Each white balance has three-channel values from zero to one and has half of the chance to be applied. Color perturbation is the same for subsequent frames and optimized.\"}"}
{"id": "5O2uzDusEN5", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Color perturbation and real-world effects. Real-world effects are composed of texture noises, fog, and motion blur. From left to right: (a) color perturbation, (b) texture noises, (c) fog, (d) motion blur. (a) is an overlaid image before (upper right) and after color perturbation (lower left).\\n\\nTexture Noises. We simulate the shot noise to mimic real-world photos. We apply three-channel texture noises to all of the pixels while masking the area where the noises are actually applied. Two different texture noises, $N_t$ and $N_{t+1}$, are applied to subsequent frames and optimized.\\n\\nFog and Motion Blur. Inspired by AutoFlow (Sun et al., 2021), we introduce two real-world effects; fog and motion blur. To generate a random fog, we follow the implementation of AutoFlow and superimpose the fog on $I_t$ and $I_{t+1}$. However, one difference is that we adjust the three-channel values of fog, which yields the colored fog. The fog is the same for the subsequent frames and is optimized as well. For the motion blur, we randomly sample object masks from PASCAL VOC (Everingham et al., 2010) and combine them to generate a motion blur mask. We use the 2D gaussian blur to approximate the motion blur kernel, and the motion blurred frames are alpha-blended to $I_t$ and $I_{t+1}$.\\n\\nThe standard deviation of each axis and angle of rotation are parameters to be updated.\\n\\nFigure 5 shows the color perturbation and real-world effects including texture noise, fog, and motion blur.\\n\\nA.6 REGULARIZATION LOSS\\n\\nThe grid warping and texture noise may take shortcuts only to minimize task loss. These behaviors are undesirable, and the generated data might not properly train the optical flow network. To handle this potential issue, we propose a regularization term $L_{\\\\text{reg}}$ consisting of grid and noise regularization losses, i.e.,\\n\\n$$L_{\\\\text{reg}} = L_{\\\\text{grid}} + L_{\\\\text{noise}}.$$ \\n\\nThe grid regularization $L_{\\\\text{grid}}$ prevents from producing infeasible 2D motion. We define the grid regularization loss $L_{\\\\text{grid}}$ as follows:\\n\\n$$L_{\\\\text{grid}} = \\\\max(0, w_{\\\\sum} X_{k=1} h_{-1} X_{j=1} \\\\left[ C_t(k, j) - C_t(k, j+1) \\\\right] + w_{-1} X_{k=1} h_{X_{j=1}} \\\\left[ C_t(k, j) - C_t(k+1, j) \\\\right]),$$\\n\\nwhere $C_t$ is the warped coordinate by the geometric warping $W_{t+1} \\\\rightarrow t$.\\n\\n$L_{\\\\text{grid}}$ gives a penalty when the warped coordinates of the previous grid exceed those of the next grid. The noise regularization $L_{\\\\text{noise}}$ prevents from producing too noisy images. We define the noise regularization loss $L_{\\\\text{noise}}$ as follows:\\n\\n$$L_{\\\\text{noise}} = (\\\\|N_t\\\\|_1 + \\\\|N_{t+1}\\\\|_1),$$\\n\\nwhere $N_t$ and $N_{t+1}$ are texture noises of the current and next frames, respectively. Finally, we obtain our total loss by adding the regularization loss to the task loss as:\\n\\n$$L_{\\\\text{total}} = L_{\\\\text{task}}(L_{\\\\text{target}}, L_{\\\\text{base}}) + L_{\\\\text{reg}}.$$ \\n\\nWith the total loss $L_{\\\\text{total}}$, we update the data parameters $\\\\{\\\\theta\\\\}$.\\n\\nA.7 DETAILS OF LEARNABLE PARAMETER $\\\\theta$\\n\\nLearning Rate of $\\\\theta$. We set the learning rate of real-world effects as $3 \\\\times 10^{-2}$ and the others as $\\\\{1 \\\\times 10^{0}, 1 \\\\times 10^{-1}, 2 \\\\times 10^{-2}\\\\}$ depending on whether they are pixel-unit operations (e.g., translation parameters) or not. We decay the learning rates linearly over the update iterations; the decay factor is $(1 - \\\\text{iteration}/80)$, where $80$ is the maximum iteration of updates. We distinguish the pixel-unit operations and the others as:\"}"}
{"id": "5O2uzDusEN5", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Generation details of DFlow.\\n\\nData augmentation used in generating DFlow.\\n\\n- Color jitter\\n- Random resize and crop\\n- brightness contrast saturation hue\\n- min scale max scale image crop augmentation\\n- 0.1 0.1 0.1 0.04\\n- 0.93 2.30 496 \u00d7 368\\n\\nPixel-unit operation (learning rate of $1 \\\\times 10^0$):\\n- Translation of the rigid transformation, grid warping, perspective warping, and flow field translation\\n\\nNon-pixel-unit operation (learning rates of $\\\\{1 \\\\times 10^{-1}, 2 \\\\times 10^{-2}\\\\}$):\\n- Rotation and scaling of the rigid transformation\\n\\nInitialization of $\\\\theta$.\\n\\nFollowing the implementation of AutoFlow (Sun et al., 2021), we sample the initial data parameters $\\\\theta$ from the uniform distributions ranging from $a$ to $b$, i.e., $[a, b]$.\\n\\n- White balance: $[0, 1]$\\n- Rigid transformation (translation, rotation, scaling): $([-80, 80], [-5, 5], [0.75, 1.13])$\\n- Grid warping: $[0, 0]$\\n- Perspective warping: $[-25, 25]$\\n- Flow field translation: $[-50, 50]$\\n- Texture noise: $[-0.01, 0.01]$\\n- Fog color: $[0, 1]$\\n- Motion blur (std of axis x, std of axis y, angle of rotation): $([1, 2], [3, 11], [0, 90])$\\n\\nDetails of Experiment Setup\\n\\nIn this section, we present the generation details of the DFlow dataset. Then, we present the details of pre-training and fine-tuning results in Sec. 4 of the main paper.\\n\\nB.1 Generation Details of the DFlow Dataset\\n\\nTo generate the DFlow dataset, the data augmentation is included in the proposed differentiable data generation pipeline. We randomly augment each data sample before inputting them into the base and target networks. For stability of data generation, we stack 6 randomly augmented data samples and input them into networks. Each data sample of DFlow is updated up to 80 times and saved depending on the threshold 25 of the target network loss. The DFlow data has a size of $512 \\\\times 384$, which is the same as FlyingChairs. Table 8 summarizes the data augmentation applied to each data sample.\\n\\nB.2 Details of Pre-Training Results\\n\\nExcept for the data augmentation, we set the training parameter settings by following the training details of RAFT (Teed & Deng, 2020) on FlyingChairs (Fischer et al., 2015), and pre-train the RAFT network on DFlow with a data size of 15k. We use the same data augmentation applied for generating the DFlow dataset. We early stop the training at 95k iteration.\\n\\nB.3 Details of Model Generalization Property Results\\n\\nFor the FlowNet results, we use the Pytorch implementation of FlowNet 3. We pre-train the FlowNet model for 2700 epochs with batch size 8. Following the Pytorch Implementation of FlowNet, we use the proposed data augmentation including translation, rotation, image crop, vertical flip, and horizontal flip. For the AutoFlow result, we use the published AutoFlow dataset (Sun et al., 2021). To generate the DFlow-GMA dataset, we replace the RAFT proxy networks with GMA networks (Jiang et al., 2021) while selecting the KITTI 2015 dataset as the target dataset. The set $\\\\alpha$ to 13 for the DFlow-GMA dataset and pre-train the GMA network on DFlow-GMA with a data size of 2k.\\n\\nhttps://github.com/ClementPinard/FlowNetPytorch\"}"}
{"id": "5O2uzDusEN5", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Data samples of the rainy scene of Virtual KITTI.\\n\\nTable 9: Contrasting analysis in pre-training. The dataset generated with the contrasting effect achieves comparable results at the target dataset, KITTI 2015. The contrasting effect leads to a notable performance improvement on Sintel, which is not used for data generation.\\n\\n| Evaluation dataset | Contrast | Sintel | clean | Sintel | final | KITTI 2015 |\\n|-------------------|-----------|--------|-------|--------|-------|------------|\\n|                   |           | AEPE   | 4.38  | 5.03   | 4.61  | /          |\\n|                   | \u2713         | AEPE   | 2.14  | 3.79   | 4.31  | /          |\\n|                   | \u2713         | F1     | *13.86| *14.29 |       |            |\\n\\nBold denotes the best.\\n\\nB.4 Details of Fine-Tuning Results\\n\\nTo achieve fine-tuning results on the public benchmarks, we fine-tune the RAFT model pre-trained on DFlow. Except for the initial data schedule, i.e., FlyingChairs followed by FlyingThings3D, we follow the same dataset schedule and training details of the original implementation (Teed & Deng, 2020).\\n\\nB.5 Details of Robustness Against Corrupted Data\\n\\nThe photometric inconsistency is the main cause of the error in optical flow estimation. In real-world images, real-world effects, such as blur, fog, and illumination change, cause photometric inconsistency, and these effects frequently occur. Since DFlow generates a dataset using the compressed knowledge of the target dataset, we verify DFlow\u2019s ability to generate the dataset for training a robust optical flow network against photometric inconsistency. We use rainy scenes of Virtual KITTI (Gaidon et al., 2016) because the photometric inconsistency is dominant, as shown in Fig. 6. To obtain the target network, we further train the base network on the rainy scenes for 20k iterations and generate an additional dataset, DFlow-V, by targeting the rainy scenes. We use the published weight of RAFT trained on the RF-AB dataset for the result of RealFlow (Han et al., 2022).\\n\\nC Additional Experiments\\n\\nWe analyze the contrasting effect of base and target networks in Sec. C.1, and evaluate the validation performance with DFlow in Sec. C.2.\\n\\nC.1 Contrasting Effect of Base and Target Networks\\n\\nTo evaluate the contrasting effect of base and target networks, we measure the pre-training performance depending on the contrasting effect. The details will be presented in the following paragraphs.\\n\\nContrasting Analysis in Pre-training. For the contrasting analysis, we generate two types of dataset with a sample size of 2k. For the first dataset, we use KITTI 2015 (Menze & Geiger, 2015) as the target dataset, i.e., the target network is obtained by fine-tuning the base network on the KITTI 2015 dataset. To synthesize the first dataset with contrasting effect, we minimize and maximizes the loss of the target and base networks, respectively. For the other dataset, we generate a new target network that is pre-trained on the KITTI 2015 dataset. With the new target network, we generate a new pre-training dataset that is synthesized to minimize the loss of the new target data network only, i.e., the contrasting effect is not applied.\"}"}
{"id": "5O2uzDusEN5", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 10 shows additional data samples. Color perturbation, texture noise, fog, and motion blur can be found in the data samples.\\n\\nFigure 10: Additional generated data samples. Top: current frames; Middle: next frames; Bottom: visualizations of ground truth optical flow.\"}"}
{"id": "5O2uzDusEN5", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"AutoFlow (Sun et al., 2021) DFlow (Ours)\\n\\n| Learning method | Evolutionary search | Gradient descent |\\n|-----------------|---------------------|-----------------|\\n| Computational resource | 48 P100 GPUs | A single V100 GPU |\\n| GPU hours for constructing a dataset | 336 days / P100 | 9.3 days / V100 |\\n\\nTable 1: Comparison of dataset generation methods of AutoFlow and DFlow (ours).\\n\\nDifferentiable data generation pipeline (DFlow)\\n\\nOur data generation pipeline synthesizes subsequent frames $I_t$ and $I_{t+1}$, and a flow (motion) map $F_{GT}$ between those two frames. We concisely denote a pair of subsequent frames and a flow map as a data sample. We parameterize each data sample with a learnable parameter vector $\\\\theta$ that characterizes a composition of elemental graphic operations, such as color perturbation and geometric warping.\\n\\nFigure 1: An overview of DFlow.\\n\\nWe parameterize data samples to be synthesized. The learnable data parameters $\\\\{\\\\theta\\\\}$ are updated by our proposed loss function. With this differentiable data generator part, we specifically seek to synthesize optical flow data improving the accuracy on the target dataset.\\n\\nWe define a loss function $L_{total} = L_{task} + L_{reg}$ to effectively update the learnable parameters, which drives the data generation to an improving direction. With this loss, we efficiently learn the parameters $\\\\{\\\\theta\\\\}$ in a fully-differentiable way.\\n\\nThe overview of our data generation pipeline is illustrated in Fig. 1, called DFlow. In Sec. 3.1, we first describe the design of the loss function. In Sec. 3.2, we describe the data generator method from the data parameters $\\\\{\\\\theta\\\\}$. Further detailed descriptions and implementation details can be found in Appendix A.\\n\\n3.1 Task Loss Function\\n\\nGiven our differentiable data generator, a sample parameter $\\\\theta$ is rendered to a data sample, i.e., a pair of subsequent frames $I_t(\\\\theta)$ and $I_{t+1}(\\\\theta)$ and those corresponding flow map $F_{GT}(\\\\theta)$. By rendering a collection of data samples with the data generator and a set of $\\\\{\\\\theta\\\\}$, we can construct a dataset.\\n\\nTo find an updated parameter $\\\\theta^*$ that improves the accuracy of the optical flow model, we need an effective criterion to drive the parameter update. As a tractable criterion to efficiently find such a dataset, we are motivated by learning-to-augment and dataset condensation approaches (Sixt et al., 2018; Kaspar et al., 2019; Zhao et al., 2021). They synthesize data to have characteristics similar to target data under the motivation that learning with two similar datasets is likely to yield accuracy improvement (Ben-David et al., 2010; Kaspar et al., 2019), which is typically achieved by distribution matching. However, directly comparing combinatorial pairs of samples in the target dataset and synthesized one is computationally expensive, and even intractable during iteratively updating a synthesized dataset.\\n\\nTo efficiently deal with this issue, we propose to encode the target dataset into a proxy neural network, called target network. With an optical flow neural network $f_{target}(I_t, I_{t+1}) \\\\rightarrow F$, we pre-train the network with a given small target optical flow dataset, which yields the trained target network. We deem the target network as a differentiable proxy of the target dataset that encodes the knowledge of target data. Compared to using the target data as a set in set-to-set comparison, leveraging this differentiable proxy allows the whole procedure to be tractable. With the target network, we define a target loss $L_{target}(f_{target}(I_t(\\\\theta), I_{t+1}(\\\\theta)), F_{GT}(\\\\theta))$ that measures flow errors for a data sample associated with $\\\\theta$ in the view of the target domain. We use the same loss used for pre-training the target network, e.g., the sequence loss for RAFT (Teed & Deng, 2020), for the target loss $L_{target}(\\\\cdot)$. \"}"}
{"id": "5O2uzDusEN5", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"With the target network and loss, we could first attempt to update the parameters \\\\( \\\\theta \\\\) by\\n\\n\\\\[\\n\\\\theta^* = \\\\arg \\\\min_{\\\\theta} \\\\sum_{i} L_{\\\\text{target}}(f_{\\\\text{target}}(I_t(\\\\theta_i)), I_{t+1}(\\\\theta_i)), F_{\\\\text{GT}}(\\\\theta_i))\\n\\\\]\\n\\nEquation (1)\\n\\nIf the target loss is small for a data sample, it means that the target network is familiar with the given data sample, and the sample is close to one of the target data, i.e., similar characteristics to the target data. In this sense, Eq. (1) distills the target knowledge into generated data. However, in our empirical preliminary study, we observed instability and under-fitting issues with Eq. (1), where optimizing the loss is stuck at a high value. We assume that target networks trained on the popular benchmarks (Butler et al., 2012; Menze & Geiger, 2015) may not produce sufficiently rich training signals for synthesizing data, which might be due to limited scales of those real-world benchmarks.\\n\\nTo complement more informative training signals, we employ the contrastive-style learning scheme by using another comparator network, called base network, which is trained with a common large-scale synthetic dataset, e.g., (Fischer et al., 2015; Mayer et al., 2016), as a base dataset. Similar to the target network, we pre-train the base network but on a large-scale dataset, FlyingChairs (Fischer et al., 2015), which is randomly generated. We use the base network similarly to the target network except for maximizing flow errors \\\\( L_{\\\\text{base}} \\\\) to implement a contrastive behavior, so that a resulting data sample should be closer to the target dataset while being different from the base (common) dataset. This contrastive behavior can generate combinatorial diversity of gradient signals by pairwise contrasting, which may more strongly drive the data generation to an improving direction in favor of a target domain than using the target network alone.\\n\\nWe define the wrapping loss function, \\\\( L_{\\\\text{task}}(L_{\\\\text{target}}, L_{\\\\text{base}}) \\\\): \\\\( \\\\mathbb{R}^2 \\\\rightarrow \\\\mathbb{R} \\\\), called task loss. Form \\\\( L_{\\\\text{task}} \\\\) Dataset Sintel clean Sintel final MultiplicationExponential 4.26 4.49 Sigmoid 4.12 4.35 Tanh 2.02 3.35 Addition Exponential 1.85 3.09 Sigmoid 2.00 3.13 Tanh 1.95 3.21\\n\\nTable 2: AEPE of different forms of \\\\( L_{\\\\text{task}} \\\\).\\n\\nWe train the RAFT network with different loss forms. The addition form with exponential shows promising results. Refer to Appendix A.2 for details.\\n\\n### What Function Type is Suitable for \\\\( L_{\\\\text{task}} \\\\)?\\n\\nThere are many candidates for \\\\( L_{\\\\text{task}} \\\\) satisfying the aforementioned criteria. As a function form, we are first motivated by sample-wise weighting distillation (Zhou et al., 2021), where the loss function is dynamically weighted according to each data sample. The loss function is weighted by the target and base losses in a multiplication form. We additionally examine addition forms to select the best one. We consider tanh, sigmoid, and exponential functions to implement bounded contrasting behaviors.\\n\\nTable 2 shows the accuracy comparison of different forms of loss functions using 1,000 data samples generated with the Sintel dataset (Butler et al., 2012) as a target. The addition form with the exponential function shows promising results in terms of performance, which corresponds to\\n\\n\\\\[\\nL_{\\\\text{task}}(L_{\\\\text{target}}, L_{\\\\text{base}}) = L_{\\\\text{target}} + \\\\alpha \\\\exp(-\\\\beta L_{\\\\text{base}}) + \\\\epsilon + \\\\gamma\\n\\\\]\\n\\nEquation (2)\\n\\nwhere we set the balance parameters \\\\( \\\\{\\\\beta, \\\\gamma\\\\} \\\\) to \\\\( \\\\{1, 0\\\\} \\\\), and especially set \\\\( \\\\alpha \\\\) to 20 for the experiments in Table 2. We found that, depending on \\\\( \\\\alpha \\\\), generated data characteristics are distinctive. When generating our final dataset, we first generate subsets of data with different \\\\( \\\\alpha \\\\) values, and ensemble the subsets into a single dataset by taking union, denoted as \\\\( \\\\{\\\\alpha\\\\} \\\\) combination. This is found to be notably effective. More details of the total loss can be found in Appendix.\\n\\n### 3.2 DATA GENERATOR\\n\\nIn this section, we describe the differentiable data generator that synthesizes a data sample from a parameter \\\\( \\\\theta \\\\). As shown in Fig. 2, the data generator is composed of geometric warping, flow field translation, layer composition, color perturbation, and real-world effects. The data generator takes \\\\( N \\\\) pairs of layer images and masks, \\\\( \\\\{L_l, M_l\\\\} \\\\), and similarly to Oh et al. (2018), we randomly sample the layer images from public image datasets (Kuznetsova et al., 2020; Perazzi et al., 2016; Mayer et al., 2016; Lin et al., 2014) and layer masks from a segmentation dataset (Everingham et al., 2010).\"}"}
{"id": "5O2uzDusEN5", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Schematic of data generator. The data generator is composed of color perturbation, two steps of geometric warpings, flow field translation, layer composition, and real-world effects. The whole pipeline is differentiably parameterized.\\n\\nAlgorithm 1: PyTorch-style pseudo-code for data generator.\\n\\n# W1, W2: geometric warping parameters\\n# Ds: translation parameters\\n# C: white balance parameters\\n# R: Real-world effect parameters\\n# images0, masks0: initial layer images and masks\\n\\nApplying color perturbation:\\nimages0 = ColorPerturbation(images0, C)\\n\\nWarping the images and masks:\\nimages1, masks1 = GeometricWarping(images0, masks0, W1)\\nW2s = FlowFieldTranslation(W2, Ds)\\nimages2, masks2 = GeometricWarping(images1, masks1, W2s)\\n\\nSuperimposing the images, masks, and flow fields:\\nimage1 = LayerComposition(images1, masks1)\\nimage2 = LayerComposition(images2, masks2)\\nflow = LayerComposition(W2s, masks1)\\n\\nApplying real-world effects to the subsequent images:\\nimage1, image2 = RealworldEffect(image1, image2, R)\\n\\nThe data generator then outputs synthesized subsequent frames \\\\( I_t \\\\) and \\\\( I_{t+1} \\\\) with ground-truth optical flow \\\\( F_{GT} \\\\). All the following pipeline is parameterized by \\\\( \\\\theta \\\\).\\n\\nGeometric Warping. Geometric warping consists of rigid transformation, perspective warping, and grid warping. The geometric warping generates a warping field. In the first geometric warping, we apply the same warping field \\\\( W_0 \\\\rightarrow t+1 \\\\) to \\\\( \\\\{L_l0\\\\}_{N_l=1} \\\\) and \\\\( \\\\{M_l0\\\\}_{N_l=1} \\\\) and generate the layer images and masks at the frame \\\\( t+1 \\\\), \\\\( \\\\{L_l(t+1)\\\\}_{N_l=1} \\\\) and \\\\( \\\\{M_l(t+1)\\\\}_{N_l=1} \\\\). In addition to this globally shared geometric warping, we model the local movement of each layer, i.e., segmented objects, in the second geometric warping step. To generate complex optical flows of the frame \\\\( t \\\\), we can apply independent warping fields to each layer image and mask. However, we observe poor optimization behaviors when we use all independent warping fields on each layer. Thus, we propose to use decomposed warping parameters for time \\\\( t \\\\) to reduce the number of parameters. We use an anchor geometric warping, \\\\( W_{t+1} \\\\rightarrow t \\\\), shared across all layers and flow field translation \\\\( (\\\\Delta x, \\\\Delta y) \\\\), \\\\( \\\\{D_l \\\\in \\\\mathbb{R}^2\\\\}_{N_l=1} \\\\), for each layer, and construct each warping of layers by \\\\( W_l(t+1) \\\\rightarrow t = (D_l \\\\circ W_{t+1} \\\\rightarrow t) \\\\), where \\\\( \\\\circ \\\\) denotes the warping operation. This strategy is also beneficial in terms of optimization stability. Finally, the operations of geometric warping and flow field translation are as follows:\\n\\n\\\\[\\nL_l(t+1) = W_0 \\\\rightarrow t+1 \\\\circ L_l0,\\n\\\\]\\n\\\\[\\nL_l(t+1) = W_l(t+1) \\\\rightarrow t \\\\circ L_l(t+1) = (D_l \\\\circ W_{t+1} \\\\rightarrow t) \\\\circ L_l(t+1) \\\\tag{3}\\n\\\\]\\n\\n\\\\[\\nM_l(t+1) = W_0 \\\\rightarrow t+1 \\\\circ M_l0,\\n\\\\]\\n\\\\[\\nM_l(t+1) = W_l(t+1) \\\\rightarrow t \\\\circ M_l(t+1) = (D_l \\\\circ W_{t+1} \\\\rightarrow t) \\\\circ M_l(t+1) \\\\tag{4}\\n\\\\]\"}"}
{"id": "5O2uzDusEN5", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nFigure 3: Generated data samples.\\n(top) current frames; (bottom) optical flow visualizations.\\n\\nLayer Composition.\\nAfter the geometric warping, we have \\\\( N \\\\) pairs of layer images, masks, and warping fields in order of depth. We superimpose each stack of layers to generate subsequent frames \\\\( I_t \\\\) and \\\\( I_{t+1} \\\\), with ground-truth optical flow \\\\( F_{\\\\text{GT}} \\\\). We can leverage alpha blending and softmax splatting (Niklaus & Liu, 2020). Both strategies show comparable performance, but we mainly use the softmax splatting strategy in our experiments. The comparison of alpha blending and softmax splatting can be found in Sec. 4.\\n\\nColor Perturbation and Real-world Effects.\\nPrior studies (Mayer et al., 2018; Sun et al., 2021) have shown that synthetic data containing real-world effects, such as texture noises, fog, and motion blur, often brings the performance improvement of optical flow networks in generalization. Inspired by the observations, we introduce color perturbation and real-world effects into our data generator as well. Color perturbation adjusts the white balance of each layer image, and our real-world effects apply texture noises, fog, and motion blur, which are all parameterized to be controlled and updated. We synthesize optical flow data by applying the above components:\\n\\n\\\\[\\n\\\\{L_l^0\\\\}_{l=1}^N, \\\\{M_l^0\\\\}_{l=1}^N \\\\rightarrow \\\\{I_t, I_{t+1}, F_{\\\\text{GT}}\\\\}.\\n\\\\]\\n\\nNote that we apply regularizations to the grid warping and texture noise, i.e., grid and noise regularizations. The detail of each component, including regularizations, can be found in Appendix A.3-A.6. The overall differentiable data generator pipeline is summarized in Algorithm 1 as a pseudo-code. Each component has its own parameters, which are updated by our task loss and regularizations. See generated samples in Fig. 3.\\n\\nSummarization.\\nWe provide the summarization of DFlow below:\\n\\n1. We train base and target networks on base and target datasets, respectively.\\n2. We fix both networks and update the data parameter \\\\( \\\\theta \\\\) using our loss function.\\n3. We train optical flow networks with the generated dataset.\\n\\n4 RESULTS\\nIn this section, we analyze the effects of our method in pre-training and fine-tuning perspectives with different optical flow models. We report the average end-point error (AEPE) for Sintel (Butler et al., 2012) and AEPE & F1 for KITTI 2015 (Menze & Geiger, 2015). From this section, we refer to our dataset generated by targeting Sintel with RAFT proxy models as DFlow, unless specified otherwise. Other details of experiment setups and implementation can be found in Appendix B.\\n\\nPre-training RAFT Results.\\nPre-training performance is one of the key factors in evaluating the applicability of optical flow datasets (Fischer et al., 2015; Mayer et al., 2016; 2018; Aleotti et al., 2021; Sun et al., 2021) by testing on benchmarks, i.e., Sintel and KITTI 2015 datasets. We train the RAFT networks (Teed & Deng, 2020) from scratch on respective competing datasets, including our DFlow. Table 3-(a) shows the pre-training results of each dataset. Compared to FlyingChairs which has the same data resolution as DFlow, the model trained on DFlow outperforms the one trained on FlyingChairs in both Sintel and KITTI 2015 datasets. Despite the fact that DFlow is around \\\\( \\\\frac{1}{4} \\\\) resolution of AutoFlow, DFlow achieves the best performance on the KITTI 2015 and Sintel clean datasets. We postulate that the real-world textures used in DFlow led to performance improvement on KITTI 2015. Also, we use both Sintel clean and final as the target datasets to generate DFlow, unlike AutoFlow using Sintel final only. It might affect performance on the Sintel clean and final datasets. Figure 4 shows the qualitative results obtained from FlyingChairs and DFlow. These results show that DFlow is effective for learning an accurate model in challenging scenes, such as shaded, foggy, and motion-blurred scenes.\"}"}
{"id": "5O2uzDusEN5", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: t-SNE plot (van der Maaten & Hinton, 2008) of different datasets.\\n\\nWe use the context encoder of the base network trained on FlyingChairs as a feature extractor and extract the features of several datasets. (Red): target datasets, e.g., KITTI 2015, (Blue): base dataset, e.g., FlyingChairs, (Sky): our initial dataset before optimization, (green): updated our dataset without the contrasting effect, (Yellow): updated our dataset with the contrasting effect. The features of our dataset with the contrasting effect are getting closer to the features of the target dataset through the optimization process than the features of other datasets.\\n\\nTable 9 shows the pre-training results depending on the contrasting effect. With the contrasting effect, the RAFT network achieves comparable results on the target dataset, i.e., KITTI 2015, compared to the other network trained on the dataset generated with the new target network only. However, without the contrasting effect, the network shows notable performance degradation on Sintel (Butler et al., 2012), which is not used for data generation.\\n\\nTable 10: Distance of feature mean between ours and KITTI 2015.\\n\\n| Datasets | MMD       |\\n|----------|-----------|\\n| Ours wo. contrasting | 6.6892    |\\n| Ours w. contrasting | 6.1454    |\\n\\nWe also visualize data features to identify the effect of the contrasting effect. We embed the features of several datasets, such as FlyingChairs, KITTI 2015, our datasets with and without the contrasting effect, and our dataset before optimization. We use the context encoder of the base network as a feature extractor. As shown in Fig. 7, ours with the contrasting effect is closer to the target than one without the contrasting effect. We additionally compute distances of feature mean between ours and KITTI 2015. As shown in Table 10, the maximum mean discrepancy (MMD) (Gretton et al., 2012) of ours with the contrasting effect is lower than ours without the contrasting effect. These results show that the contrasting effect drives the data generation toward an improving direction in a target domain.\\n\\nC.2 Validation Results\\n\\nThe advantage of the proposed pipeline is that the generated data can be used to improve the validation performance on the unseen target dataset. We split the KITTI 2015 dataset into 50 training samples and 150 validation samples; KITTI-50 and KITTI-150. Using the target network fine-tuned on KITTI-50, we synthesize a dataset; OursKITTI-50. Therefore, any data or information on KITTI-150 is not included in the data generation process. We fine-tune the base network on several combinations of datasets with 20k iterations and evaluate the validation performance on KITTI-150. As shown in Table 11, fine-tuning on the combination of Sintel and KITTI-50 shows comparable performance with the one of fine-tuning on KITTI-50 alone. In contrast, we observe notable improvement in validation performance when adding the same number of OursKITTI-50 to the training dataset. As our data generation pipeline is not limited to the number of samples that can generate, we analyze the effect of our data size and observe the improvement of validation accuracy.\"}"}
{"id": "5O2uzDusEN5", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: Comparison of the validation performance of networks fine-tuned on different dataset combinations. We fine-tune the base network on the combination of KITTI-50 and another dataset.\\n\\n| Validation dataset | Ours | KITTI-50 | KITTI-50+Sintel | KITTI-50+Ours |\\n|--------------------|------|---------|----------------|-------------|\\n|                    |      | 50      | 50+2082        | 50+12003    |\\n| AEPE / F1          | 3.41 | 3.39    | 3.07           | 2.98        |\\n|                     | 11.47| 11.21   | 10.99          | 10.70       |\\n\\n*Bold* denotes the best, and *underline* denotes the second best.\\n\\nTable 12: Pre-training results depending on the amounts of DFlow data. We train the RAFT network on FlyingChairs (Fischer et al., 2015) and various amounts of DFlow dataset. We evaluate the performance on the Sintel and KITTI 2015 datasets.\\n\\n| Evaluation dataset | Dataset | Data size | AEPE   | AEPE   | AEPE / F1   |\\n|--------------------|---------|-----------|--------|--------|-------------|\\n|                    | FlyingChairs | 22873    | 2.28   | 4.51   | 9.85 / 37.56 |\\n|                    | DFlow    | 100      | 2.72   | 4.03   | 9.15 / 23.09 |\\n|                    | DFlow    | 500      | 2.20   | 3.48   | 5.40 / 17.54 |\\n|                    | DFlow    | 1000     | 2.02   | 3.24   | 5.22 / 17.06 |\\n|                    | DFlow    | 2000     | 1.90   | 3.02   | 4.90 / 15.78 |\\n|                    | DFlow    | 4000     | 1.87   | 2.92   | 4.78 / 15.64 |\\n|                    | DFlow    | 8000     | 1.81   | 2.91   | 4.88 / 15.51 |\\n|                    | DFlow    | 15000    | 1.81   | 2.93   | 4.59 / 15.03 |\\n\\nC.3 Dataset Size\\n\\nThe number of the dataset is a key factor for training accurate optical flow networks, and synthetic optical flow data have an advantage because of the annotation efficiency compared to the real-world data. We analyze the effect of data size in the optical flow network, RAFT (Teed & Deng, 2020). We train the RAFT network on the various amount of DFlow data and evaluate the performance on both Sintel and KITTI 2015 datasets. We determine FlyingChairs (Fischer et al., 2015) as a competing dataset because FlyingChairs and DFlow have the same data resolution. As shown in Table 12, the RAFT network trained on the 500 DFlow dataset outperforms the model trained on the full FlyingChairs dataset. This result shows that DFlow data has more information with the same data resolution. The overall performance on Sintel and KITTI 2015 datasets is also improved as the data size increase, which shows that diverse texture and motion is effective for training the optical flow network.\\n\\nC.4 Pre-training Performance According to Motion Magnitudes\\n\\nTable 13 summarizes AEPEs of RAFT pre-trained on respective FlyingChairs (Fischer et al., 2015) and our DFlow according to different motion ranges. The model trained on DFlow shows higher accuracy than that of FlyingChairs except for the minimal motion range. Performance gaps between the models tend to be enlarged in larger motion. These results may be explained by human intuition that the small motion hardly contributes to the error of optical flow networks. For the same reason, most motion magnitudes of DFlow are in the middle and high motion ranges, as shown in Fig. 8.\"}"}
{"id": "5O2uzDusEN5", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We evaluate the AEPE score of models pre-trained on FlyingChairs and DFlow in different motion ranges. As the motion magnitude increases, the model trained on DFlow achieves more considerable performance improvements than that trained on FlyingChairs.\\n\\n| Pre-training | Evalutaion | Motion magnitude ranges | dataset | dataset |\\n|--------------|------------|-------------------------|---------|---------|\\n|              |            | < 1                     | [1, 10] | (10, 20] |\\n| FlyingChairs | Sintel     | 0.57                    | 1.15    | 3.31    | 6.47 |\\n| DFlow (Ours) | KITTI 2015 | 0.69                    | 0.80    | 2.04    | 3.59 |\\n| FlyingChairs | FlyingThings3D | 0.74 | 1.67 | 2.65 | 3.86 | 21.32 |\\n| DFlow (Ours) | Sintel     | 1.16                    | 0.81    | 1.31    | 2.10 |\\n\\n*Bold* denotes the best.\\n\\nFigure 8: Motion magnitude histograms. DFlow focuses more on the mid-high ranges of motion than the other datasets, because small motion hardly contributes to errors of networks.\\n\\nTable 14: Analysis of multi-target datasets. The target of DFlow is Sintel, DFlow-V rainy scenes of Virtual KITTI. We denote DFlow+DFlow-V as the multi-target datasets. Multi-target datasets show the trade-off in general but achieve the highest performance on KITTI 2015.\\n\\n| Evaluation dataset | Sintel clean | Sintel final | KITTI 2015 | Rainy scene |\\n|--------------------|--------------|--------------|------------|-------------|\\n| DFlow              | 1.81         | 2.93         | 4.59       | /           |\\n| DFlow-V            | 2.57         | 3.86         | 5.98       | / 15.16     |\\n| DFlow + DFlow-V    | 1.99         | / 3.14       | 4.39       | / 14.84     |\\n|                    | 5.87         | 22.89        | /          | 12.61       |\\n\\n*Bold* and underline note the best and second best, respectively.\\n\\nC.5 Analysis of multi-target datasets.\\n\\nWe focus on generating the dataset close to the target dataset. It is questionable whether multi-target datasets can be used to generate the DFlow dataset consisting of diverse data samples. The targets of DFlow and DFlow-V are Sintel and rainy scenes of Virtual KITTI. Table 14 lists the optical flow performance on Sintel clean, Sintel final, KITTI 2015, and the rainy scene of Virtual KITTI. As expected, DFlow-V achieves higher performance than DFlow on the rainy scene; but DFlow-V has inferior performance on the other datasets. When combining DFlow and DFlow-V, we observe that there is a trade-off; the performance of DFlow with DFlow-V is usually between the performance of DFlow and DFlow-V. However, the mixture of DFlow and DFlow-V outperforms in KITTI 2015. It indicates that exploiting multi-target datasets is a promising research direction.\"}"}
{"id": "5O2uzDusEN5", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 9: Additional qualitative results. Top: pre-training results on the KITTI 2015 dataset. Bottom: pre-training results on Sintel Final pass.\"}"}
{"id": "5O2uzDusEN5", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Pre-training results. We train the (a) RAFT (Teed & Deng, 2020) and (b) FlowNet (Fischer et al., 2015) (c) GMA (Jiang et al., 2021) networks on respective pre-training datasets, and evaluate on the evaluation datasets, i.e., Sintel and KITTI 2015. Chairs $\\\\rightarrow$ Things denotes the heterogeneous dataset experiment pre-training on FlyingChairs followed by FlyingThings3D.\\n\\n| Evaluation dataset | Model | Pre-training dataset | AEPE | AEPE | / F1 |\\n|-------------------|-------|----------------------|------|------|------|\\n| Sintel clean      | AEPE  | 2.28                 | 4.51 | 9.85 | / 37.56 |\\n| Sintel final      | 2.08  | 2.75                 | 4.66 | / -  |\\n| KITTI 2015        | 1.81  | 2.93                 | 4.59 | / 15.03 |\\n\\n(a) RAFT Chairs $\\\\rightarrow$ Things 1.43 2.71 5.04 / 17.40\\nFlyingChairs 4.71 6.22 20.36 / 62.20\\nAutoFlow 5.43 6.03 19.64 / 43.95\\n(b) FlowNet DFlow (Ours) 3.45 4.73 12.94 / 38.95\\nChairs $\\\\rightarrow$ Things - - 4.69 / 17.10\\n(c) GMA DFlow-GMA (Ours) - - 4.47 / 13.10\\n\\n*Bold* denotes the best.\\n\\nFigure 4: Qualitative results. Top: pre-training results on the Sintel final pass. Bottom: pre-training results on KITTI 2015. The RAFT network pre-trained on DFlow shows robust results in challenging shaded, foggy, and motion-blurred scenes.\\n\\nModel Generalization Property. Since DFlow is generated with RAFT proxy networks, a question naturally arises about the effectiveness of the generated dataset and data generation pipeline on other model architectures. To evaluate the generalization ability of the generated dataset, i.e., DFlow, we pre-train the Flownet network on the DFlow dataset generated with the RAFT network. As shown in Table 3-(b), FlowNet pre-trained on DFlow shows noticeable improvement over the ones pre-trained on the competing datasets. We further investigate the generalization ability of the data generation pipeline. In this experiment, we select the KITTI 2015 dataset as the target dataset and generate the DFlow-GMA dataset using the GMA network (Jiang et al., 2021). Using our DFlow-GMA dataset, we train the GMA network and list the performance on KITTI 2015 in Table 3-(c). The GMA network trained on our DFlow-GMA achieves higher performance than the one trained on FlyingChairs followed by FlyingThings3D. From these observations, we postulate that our generated data and generation pipeline are agnostic to the deep network structure.\"}"}
{"id": "5O2uzDusEN5", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Fine-tuning results on public benchmarks.\\n\\nWe report F1 scores for KITTI 2015 and AEPE for Sintel public benchmarks. C, K, S, T, and H denote FlyingChairs, KITTI 2015, Sintel, FlyingThings3D, and HD1K (Kondermann et al., 2014). The results of Sintel public benchmarks are evaluated without the warm-start initialization (Teed & Deng, 2020).\\n\\n| Dataset schedule | Sintel clean | Sintel final | KITTI 2015 |\\n|------------------|-------------|--------------|------------|\\n| DFlow \u2192 TSKH/K   | 1.62        | 3.07          | 5.03       |\\n| AutoFlow \u2192 TSKHV | 2.01        | 3.14          | 4.78       |\\n| C \u2192 T           | 1.94        | 3.18          | 5.10       |\\n\\nBold denotes the best.\\n\\nTable 5: Analysis of data generation components.\\n\\nWe analyze the effects of each generation component. The experiments are conducted by adding or removing one of the generation components. The default setting is with all components and the number of foregrounds parameter in the range of 8 to 12 except the \\\\{\u03b1\\\\} combination, which is annotated with underlines.\\n\\n| Evaluation dataset | Sintel clean | Sintel final | KITTI 2015 |\\n|--------------------|-------------|--------------|------------|\\n| Data update On     | 1.86        | 3.04         | 5.21 / 16.27 |\\n| Off                | 9.70        | 9.67         | 17.31 / 46.84 |\\n| \\\\{\u03b1\\\\} combination | 1.86        | 3.04         | 5.21 / 16.27 |\\n| Off                | 1.90        | 3.02         | 4.90 / 15.78 |\\n| Layer composition  | 1.86        | 3.04         | 5.21 / 16.27 |\\n| softmax splatting  | 1.95        | 2.83         | 5.23 / 16.06 |\\n| alpha blending     | 2.15        | 3.32         | 5.73 / 17.60 |\\n| 0                  | 2.00        | 3.15         | 5.41 / 16.33 |\\n| 2                  | 4.10        | 4.89         | 7.56 / 23.76 |\\n| Number of foregrounds | 8-12      | 1.86        | 3.04         | 5.21 / 16.27 |\\n| Color perturbation On | 1.86    | 3.04         | 5.21 / 16.27 |\\n| Off                | 2.07        | 3.20         | 4.87 / 16.09 |\\n| Motion blur On     | 1.86        | 3.04         | 5.21 / 16.27 |\\n| Off                | 1.95        | 3.57         | 5.06 / 16.24 |\\n| Fog On             | 1.86        | 3.04         | 5.21 / 16.27 |\\n| Off                | 1.98        | 3.20         | 5.04 / 15.68 |\\n| Texture noise On   | 1.86        | 3.04         | 5.21 / 16.27 |\\n| Off                | 2.02        | 3.15         | 5.47 / 16.56 |\\n| Noise regularization On | 1.86  | 3.04         | 5.21 / 16.27 |\\n| Off                | 1.91        | 3.09         | 4.97 / 15.87 |\\n| Grid regularization On | 1.86  | 3.04         | 5.21 / 16.27 |\\n| Off                | 1.88        | 3.10         | 5.09 / 16.35 |\\n| Target dataset     | Sintel      | 1.86        | 3.04         | 5.21 / 16.27 |\\n| KITTI 2015         | 2.14        | 3.79         | 4.31 / 14.29 |\\n\\nFine-tuning Results on Public Benchmarks.\\n\\nTable 4 shows the fine-tuning results on the public benchmark test sets with corresponding dataset schedules. DFlow improves the original RAFT recipe (C \u2192 T \u2192 TSKH/K) on both benchmarks by replacing the conventional initial dataset schedule, i.e., FlyingChairs followed by FlyingThings3D (C \u2192 T), with our DFlow. In particular, compared to the competing datasets, DFlow achieves the best fine-tuning performance on the Sintel with RAFT. This result validates that DFlow as a pre-training dataset effectively affects fine-tuning results.\\n\\nAnalysis of Components.\\n\\nWe analyze the effects of each generation pipeline component: \\\\{\u03b1\\\\} combination, the number of foregrounds, color perturbation, real-world effects, regularizations, and target dataset. For the fair and quick experiments, we generate 2k training data and train RAFT networks with the same training details. As shown in Table 5, we add or remove each of the components from a default setting to measure their effects with the pre-training experiment. The default setting indicates the dataset with all components and the number of foregrounds parameter in the range of 8 to 12 except the \\\\{\u03b1\\\\} combination. We observe that applying each component brings consistent performance improvement on the target dataset, i.e., Sintel, which is used for generating 8\"}"}
{"id": "5O2uzDusEN5", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\n**DFlow**; whereas the results on KITTI 2015, which is not a target data of DFlow, are inconsistent with the results of Sintel.\\n\\n- **Data update.** This analysis compares DFlow with randomly initialized data without any optimization, i.e., randomly generated by our pipeline. We train and compare the RAFT networks with those datasets, which show the effectiveness gap between the two datasets.\\n\\n- **{\u03b1} Combination.** We collect the same amount of data by combining subsets of data, which are obtained from diverse \u03b1 values. This balances the performance between evaluation datasets because of the regularization effect.\\n\\n- **Layer composition.** The softmax splatting and alpha blending show comparable performance on Sintel. The softmax splatting improves the performance in the Sintel clean pass, but shows the performance drop in the Sintel final pass.\\n\\n- **Number of foregrounds.** Without any foreground, we observe the significant performance drop on Sintel and KITTI 2015. Adding only 2 foregrounds brings notable performance improvement.\\n\\n- **Color perturbation and real-world effects.** Without the color perturbation, the performance on Sintel drops moderately. Removing the motion blur significantly affects the accuracy on Sintel, especially in the Sintel final pass. The effects of fog and texture noise are moderate.\\n\\n- **Regularizations.** Removing the regularizations shows a slight performance drop in Sintel.\\n\\n- **Target dataset.** The generated dataset optimized to KITTI 2015 shows a notable performance gain on KITTI 2015, while the performance on Sintel significantly drops. This may be caused by the distribution gap between the two datasets.\\n\\n| Dataset     | Rainy scene | FlyingChairs | RealFlow | DFlow | DFlow-V |\\n|-------------|-------------|--------------|----------|-------|---------|\\n| AEPE / F1   | 7.22 / 24.95| 7.75 / 27.57 | 5.87 / 22.89 |       | 3.11 / 12.61 |\\n\\n*Bold note the best.*\\n\\n**Table 6: Analysis of photometric inconsistency scenario.**\\n\\nRobustness to Corrupted Data (Whether Artifact).\\n\\nThe photometric inconsistency degrades the robustness of optical flow estimation. For example, the rainy scene occurs photometric inconsistency. We evaluate the performance of RAFT on rainy scenes of Virtual KITTI (Gaidon et al., 2016). We choose FlyingChairs and RealFlow (Han et al., 2022) as baselines. Using the rainy scenes of Virtual KITTI as a target dataset, we generate an additional dataset, DFlow-V. Note that the target of DFlow is the Sintel dataset. Table 6 lists the performance of rainy scenes of virtual KITTI. As Han et al. (2022) have mentioned their limitation of discontinuous illumination, the performance of RealFlow is lower than others. DFlow outperforms the baselines, and DFlow-V, which of the target is Virtual KITTI, shows much higher performance rather than others. From these results, we assume that our method can distill the characteristics of photometric inconsistency.\\n\\n**CONCLUSION AND DISCUSSION**\\n\\nWe propose a new data generation pipeline for training optical flow networks. Our pipeline consists of geometric warping, real-world effects, etc., which are all parameterized differentiably. We propose a new objective function that drives our data optimization by leveraging the compressed knowledge of the proxy networks pre-trained on target and base datasets, respectively. Optical flow models trained on our datasets achieve favorable or superior performance against the competing datasets on pre-training and fine-tuning experiments. We conclude our paper with a discussion section.\\n\\nDiscussion.\\n\\nWe use the pre-defined elementary data generation operations, e.g., fog, geometric warping, etc. While DFlow shows effectiveness in the real-world dataset, i.e., KITTI 2015, the pre-defined and restricted operations might not span all the real-world effects. Thus, diverse and complementary operations would further improve expressiveness and may lead to additional performance improvement. Our method also aims at a specific target dataset. For that, we need a target network trained on the target dataset, which requires at least some amount of optical flow annotations. To mitigate the requirement of the supervised data, it would be an interesting future direction to investigate the way to train the target network in an unsupervised method.\"}"}
{"id": "5O2uzDusEN5", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This paper develops an optical flow data generation method and uses the public datasets (Kuznetsova et al., 2020; Perazzi et al., 2016; Mayer et al., 2016; Lin et al., 2014; Everingham et al., 2010) for data generation. Therefore, the author does not expect any potential ethical issues related to sensitive information of the dataset.\\n\\nWe introduce the whole data generation pipeline in Sec. 3. In Sec. 3.1, we present the loss function for data optimization. In Sec. 3.2, we describe the data generator method from the parameters. We list more details of the loss function and data generation pipeline in Appendix A.\\n\\nThis work was partially supported by the National Research Foundation of Korea (NRF) grant (No. NRF-2021R1C1C1006799), and Institute of Information & communications Technology Planning & Evaluation (IITP) grant (No.2022-0-00124, Development of Artificial Intelligence Technology for Self-Improving Competency-Aware Learning Capabilities; No.2021-0-02068, Artificial Intelligence Innovation Hub) funded by the Korea government (MSIT). This project is the result of \\\"HPC Support\\\" Project supported by the \\\"Ministry of Science and ICT\\\" and NIPA.\\n\\nFilippo Aleotti, Matteo Poggi, and Stefano Mattoccia. Learning optical flow from still images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nChristian Bailer, Kiran Varanasi, and Didier Stricker. CNN-based patch matching for optical flow with thresholded hinge embedding loss. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1):151\u2013175, 2010.\\n\\nMichael J Black and Padmanabhan Anandan. A framework for the robust estimation of optical flow. In IEEE International Conference on Computer Vision (ICCV), 1993.\\n\\nD. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A naturalistic open source movie for optical flow evaluation. In European Conference on Computer Vision (ECCV), pp. 611\u2013625, 2012.\\n\\nMark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303\u2013338, 2010.\\n\\nPhilipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip H{\\\"a}usser, Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In IEEE International Conference on Computer Vision (ICCV), 2015.\\n\\nA Gaidon, Q Wang, Y Cabon, and E Vig. Virtual worlds as proxy for multi-object tracking analysis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\\n\\nAndreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\\n\\nJames J Gibson. The perception of the visual world. 1950.\\n\\nJames Jerome Gibson and Leonard Carmichael. The senses considered as perceptual systems. Houghton Mifflin Boston, 1966.\"}"}
{"id": "5O2uzDusEN5", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "5O2uzDusEN5", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "5O2uzDusEN5", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We present implementation details and additional experimental results. The contents are listed as follows:\\n\\nContents\\n\\nA Details of Differentiable Data Generation Pipeline\\n\\nA.1 Base and Target Networks\\nA.2 The Form of Task Loss Functions\\nA.3 Geometric Warping and Flow Field Translation\\nA.4 Layer Composition\\nA.5 Color Perturbation and Real-world Effects\\nA.6 Regularization Loss\\nA.7 Details of Learnable Parameter $\\\\theta$\\n\\nB Details of Experiment Setup\\n\\nB.1 Generation Details of the DFlow Dataset\\nB.2 Details of Pre-training Results\\nB.3 Details of Model Generalization Property Results\\nB.4 Details of Fine-tuning Results\\nB.5 Details of Robustness against Corrupted Data\\n\\nC Additional Experiments\\n\\nC.1 Contrasting Effect of Base and Target Networks\\nC.2 Validation Results\\nC.3 Dataset Size\\nC.4 Pre-training Performance according to Motion Magnitudes\\nC.5 Analysis of Multi-target Datasets\\nC.6 Additional Qualitative Results\\n\\nD Additional Generated Data Samples\"}"}
{"id": "5O2uzDusEN5", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we describe the implementation details of the differentiable data generation pipeline.\\n\\nA.1 Base and Target Networks\\n\\nBase Network.\\nFor the architecture, we use the same architecture for both base and target networks. We use the published weight of RAFT (Teed & Deng, 2020) as base network, which is pre-trained on FlyingChairs (Fischer et al., 2015) for 100k iterations with a batch size of 12, \\\\(496 \\\\times 368\\\\) image size, and learning rate \\\\(4 \\\\times 10^{-4}\\\\).\\n\\nTarget Network.\\nWe fine-tune the base network on the Sintel datasets (Butler et al., 2012) for the target network. During fine-tuning, we only use the image crop as data augmentation and fine-tune the base network for 20k iterations with a batch size of 6, \\\\(768 \\\\times 368\\\\) image size, and learning rate \\\\(1.25 \\\\times 10^{-4}\\\\).\\n\\nA.2 The Form of Task Loss Functions\\n\\nWe introduce our task loss function \\\\(L_{\\\\text{task}}(L_{\\\\text{target}}, L_{\\\\text{base}})\\\\) for contrastive-style learning and evaluate the candidate of the loss function in Sec. 3.1. Table 7 shows the used task loss functions in experiments of Table 2. We use the sequence loss of flow estimates of RAFT (Teed & Deng, 2020) for \\\\(L_{\\\\{\\\\text{target,base}\\\\}}\\\\).\\n\\nAlgorithm 2 is the pseudo-code for updating the parameters of each data sample.\\n\\nTable 7: The forms of task loss function.\\n\\nWe list the used forms of task loss function in Sec. 3.1. The form is the combination of \\\\{Multiplication, Addition\\\\} and \\\\{Exponential, Sigmoid, Tanh\\\\}. The task loss functions have the hyperparameters, \\\\(\\\\alpha, \\\\beta\\\\), and \\\\(\\\\gamma\\\\).\\n\\nTask loss function \\\\(L_{\\\\text{task}}(L_{\\\\text{target}}, L_{\\\\text{base}})\\\\)\\n\\n- Multiplication & Exponential\\n  \\\\(1 - \\\\alpha \\\\exp\\\\left(\\\\frac{\\\\beta L_{\\\\text{base}}}{L_{\\\\text{target}} + \\\\epsilon} + \\\\gamma\\\\right)\\\\) \\\\(L_{\\\\text{target}}\\\\)\\n\\n- Multiplication & Sigmoid\\n  \\\\(1 + \\\\alpha \\\\text{sigmoid}\\\\left(\\\\frac{\\\\beta L_{\\\\text{target}}}{L_{\\\\text{base}} + \\\\epsilon} + \\\\gamma\\\\right)\\\\) \\\\(L_{\\\\text{target}}\\\\)\\n\\n- Multiplication & Tanh\\n  \\\\(1 + \\\\alpha \\\\tanh\\\\left(\\\\frac{\\\\beta L_{\\\\text{target}}}{L_{\\\\text{base}} + \\\\epsilon} + \\\\gamma\\\\right)\\\\) \\\\(L_{\\\\text{target}}\\\\)\\n\\n- Addition & Exponential\\n  \\\\(L_{\\\\text{target}} + \\\\alpha \\\\exp\\\\left(-\\\\frac{\\\\beta L_{\\\\text{base}}}{L_{\\\\text{target}} + \\\\epsilon} + \\\\gamma\\\\right)\\\\)\\n\\n- Addition & Sigmoid\\n  \\\\(L_{\\\\text{target}} + \\\\alpha \\\\text{sigmoid}\\\\left(\\\\frac{\\\\beta L_{\\\\text{target}}}{L_{\\\\text{base}} + \\\\epsilon} + \\\\gamma\\\\right)\\\\)\\n\\n- Addition & Tanh\\n  \\\\(L_{\\\\text{target}} + \\\\alpha \\\\tanh\\\\left(\\\\frac{\\\\beta L_{\\\\text{target}}}{L_{\\\\text{base}} + \\\\epsilon} + \\\\gamma\\\\right)\\\\)\\n\\nA.3 Geometric Warp and Flow Translation.\\n\\nGiven \\\\(N\\\\) pairs of layer images and masks, \\\\(\\\\{L_l^0\\\\}_{N_l=1}^N\\\\) and \\\\(\\\\{M_l^0\\\\}_{N_l=1}^N\\\\), we generate subsequent frames, \\\\(I_t\\\\) and \\\\(I_{t+1}\\\\), with ground-truth optical flow \\\\(F_{\\\\text{GT}}\\\\), \\\\(\\\\{L_l^t\\\\}_{N_l=1}^N\\\\), \\\\(\\\\{M_l^t\\\\}_{N_l=1}^N\\\\) \u2192 \\\\(\\\\{I_t, I_{t+1}, F_{\\\\text{GT}}\\\\}\\\\).\\n\\nWe apply two steps of geometric warping with warping fields \\\\(W_0 \\\\rightarrow t+1\\\\) and \\\\(W_{t+1} \\\\rightarrow t\\\\) which are computed from combinations of rigid transformation, perspective warping, and grid warping. First, we warp the layer images and masks, \\\\(\\\\{L_l^0\\\\}_{N_l=1}^N\\\\) and \\\\(\\\\{M_l^0\\\\}_{N_l=1}^N\\\\), to be the layer images \\\\(\\\\{L_l^t\\\\}_{N_l=1}^N\\\\) and masks \\\\(\\\\{M_l^t\\\\}_{N_l=1}^N\\\\) at the \\\\(t+1\\\\) frame by applying the same warping field \\\\(W_0 \\\\rightarrow t+1\\\\) to \\\\(\\\\{L_l^0\\\\}_{N_l=1}^N\\\\) and \\\\(\\\\{M_l^0\\\\}_{N_l=1}^N\\\\).\\n\\n\\\\[L_l^t = W_0 \\\\rightarrow t+1 \\\\circ L_l^0, \\\\quad M_l^t = W_0 \\\\rightarrow t+1 \\\\circ M_l^0,\\\\]\\n\\n(6)\\n\\nwhere \\\\(\\\\circ\\\\) denotes the geometric warping operation according to a given warping field. In the next step, each layer image \\\\(L_l^t\\\\) and mask \\\\(M_l^t\\\\) at the \\\\(t\\\\) frame is generated from the ones at the \\\\(t+1\\\\) frame, i.e., \\\\(L_l^t+1\\\\) and \\\\(M_l^t+1\\\\).\\n\\nTo simulate complex optical flows with minimal parameters, we introduce the flow field translation \\\\(\\\\{D_l\\\\}_{N_l=1}^N\\\\), which translates the warping field \\\\(W_{t+1} \\\\rightarrow t\\\\) and obtains each warping of layers:\\n\\n\\\\[W_l^t \\\\rightarrow t = (D_l \\\\circ W_{t+1} \\\\rightarrow t)\\\\]\\n\\nUsing each warping of layers \\\\(W_l^t \\\\rightarrow t\\\\), we warp the layer images and masks at the \\\\(t+1\\\\) time.\\n\\n\\\\[L_l^t = (D_l \\\\circ W_{t+1} \\\\rightarrow t) \\\\circ L_l^{t+1}, \\\\quad M_l^t = (D_l \\\\circ W_{t+1} \\\\rightarrow t) \\\\circ M_l^{t+1}.\\\\]\\n\\n(7)\"}"}
