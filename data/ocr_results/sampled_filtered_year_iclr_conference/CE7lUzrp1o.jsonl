{"id": "CE7lUzrp1o", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Performance comparison: Misclassification error rates (in %) for classification tasks and Mean Absolute Error (MAE) for regression tasks, both are lower the better. Results of all methods, excluding \u201cShuttle,\u201d are reported from Bai et al. (2022).\\n\\n| Method                  | (a) Training Domain | (b) Testing Domain | (c) Generated Testing Domain |\\n|-------------------------|---------------------|--------------------|-----------------------------|\\n| 2-Moons                 | Offline             | 22.4 \u00b1 4.6         | 14.9 \u00b1 0.9                  |\\n| Elec2                   | Offline             | 23.0 \u00b1 3.1         | 25.8 \u00b1 0.6                  |\\n| ONP                     | Offline             | 33.8 \u00b1 0.6         | 36.0 \u00b1 0.2                  |\\n| Shuttle                 | Offline             | 7.2 \u00b1 0.1          | 6.7 \u00b1 0.0                   |\\n| LastDomain              | Offline             | 10.2 \u00b1 1.1         | 9.1 \u00b1 0.7                   |\\n| IncFinetune             | Offline             | 16.7 \u00b1 3.4         | 8.9 \u00b1 0.5                   |\\n| CDOT (Ortiz-Jimenez et al., 2019) | Offline | 9.3 \u00b1 1.0          | 8.7 \u00b1 0.2                   |\\n| CIDA (Wang et al., 2020) | Offline             | 10.8 \u00b1 1.6         | 8.7 \u00b1 0.2                   |\\n| Adagraph (Mancini et al., 2019) | Offline | 8.0 \u00b1 1.1          | 8.2 \u00b1 0.6                   |\\n| GI (Nasery et al., 2021) | Offline             | 3.5 \u00b1 1.4          | 3.2 \u00b1 1.2                   |\\n| DRAIN (Bai et al., 2022) | Offline             | 3.2 \u00b1 1.2          | 6.4 \u00b1 0.4                   |\\n| CODA (MLP)              | Offline             | 2.3 \u00b1 1.0          | 4.6 \u00b1 0.5                   |\\n| CODA (LightGBM)         | Offline             | 1.4 \u00b1 0.4          | 5.3 \u00b1 0.2                   |\\n| CODA (FT-Transformer)   | Offline             | 0.5 \u00b1 0.2          | 5.3 \u00b1 0.3                   |\\n\\nObservation 1: CODA is free from fixed in specific model architectures by providing temporal generalization data for training. CODA provides a model-agnostic approach to simulate future domain data, tackling concept drift at its fundamental cause. Its data-centric manner allows different architectures to be fine-tuned on the simulated data for achieving temporal domain generalization.\\n\\n4.3 QUALITY OF GENERATED FUTURE DATA (RQ2)\\n\\nVisualization of the generated future data. As mentioned in Section 3, CODA aims to provide high-fidelity simulated data. To illustrate the quality of the predicted future data, we compare the distributions of the generated domain data $\\\\hat{D}_9$ with the unseen domain $D_9$ in the 2-Moons dataset. Recall that the 2-Moons dataset is synthesized with each domain $i$ characterized by a rotation angle of $18i^\\\\circ$. Consequently, the generated $\\\\hat{D}_9$ should be rotated by the same angle as $D_9$. As shown in Figure 3, $\\\\hat{D}_9$ closely resembles $D_9$, and both can be distinguished from the last source domain $D_8$ by a specific rotation angle. More visualization analysis are available in Appendix E.\"}"}
{"id": "CE7lUzrp1o", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Sample rate variations in CODA: Evaluating McE and MAE using MLPs trained on generated samples, where the sample rate is the percentage of the number of testing domain samples.\\n\\nTable 2: Ablation study on the exclusion of the feature correlation matrix $\\\\hat{C}^{+1}$. Here, w/o $\\\\hat{C}^{+1}$ refers to a variant of CODA that does not incorporate $\\\\hat{C}^{+1}$ as prior knowledge.\\n\\n| Classification (in %) | Regression |\\n|----------------------|------------|\\n| 2-Moons Elec2 Appliance | 2.3 \u00b1 1.0 10.3 \u00b1 1.1 4.6 \u00b1 0.5 |\\n| w/o $\\\\hat{C}^{+1}$ | 15.6 \u00b1 0.7 26.5 \u00b1 0.8 9.5 \u00b1 0.4 |\\n\\nImpact of numbers of the generated future data. As the CODA provides a framework for training a temporal domain generalization generative model, we can decide the number of generated samples for model training. Thus, we can investigate the influence of the number of the generated samples on the trained prediction model, here we use MLP as the prediction model architecture.\\n\\nCODA provides a framework for training a generative model to achieve temporal domain generalization. This data-centric design grants us the ability to modulate the number of generated samples for model training. Here, we investigate the impacts of varying sample counts on the performance of an MLP trained with the generated data. As shown in Figure 4, for both classification and regression datasets, increasing the sample rate reduces performance variances because a larger dataset more accurately represents the probability distribution learned by the Data Simulator. Regarding performance, in the real-world classification dataset, Elec2, increasing the number of generated samples enhances the trained model\u2019s performance. However, in the low-dimensional toy dataset, 2-Moons, generating additional data might introduce noise, potentially degrading the performance. This might indicate the representative of the moderate quantity of the generated samples. On the other hand, in the real-world regression dataset, Appliance, the generated data with 25% sample rate is representative enough for achieving stable model performance with marginally higher variance, which also indicates the quality of the samples simulated by CODA.\\n\\nObservation 2: CODA is able to simulate high-quality samples for training temporal domain generalization models.\\n\\nVisualizations and sample rate analyses demonstrate that CODA effectively generates high-quality future domain data for training, and these generated samples offer a representative data distribution in the presence of concept drift.\\n\\nCross-Architecture Transferability. Since the outcome of CODA is a dataset for model training, it can conceptually be employed by any model architecture. Thus, once the hyperparameters of the Data Simulator are tuned for one architecture, the generated data can readily be utilized to train other predictive models. To examine such transferability, we conduct experiments where we tune the Data Simulator using three distinct architectures: MLP, LightGBM, and FT-Transformer (FTT). Subsequently, we train the other two architectures on the dataset generated by CODA. The results indicate the generated data is transferable. As shown in Table 3, across the Data Simulator tuned by three different architectures, LightGBM achieves the best average performance. Interestingly, LightGBM outperforms FTT even when both are trained on the data generated by the FTT-tuned Data Simulator, suggesting the advantage of this data-centric approach.\\n\\nObservation 3: CODA-generated future data is transferable to different model architectures.\\n\\nThe transferability analysis demonstrates that CODA-generated data can be used as training data for MLP-based, tree-based, and transformer-based models. Additionally, due to the less hyperparameter search space, we suggest utilizing MLP-based models for tuning the Data Simulator and then training different prediction model architectures on the generated data.\\n\\n4.4 ABSTRACTION ON PRIOR KNOWLEDGE (RQ3)\\n\\nTo further elucidate the impact of the predicted feature correlation matrix $\\\\hat{C}^{+1}$, we conduct ablation studies on observing the effectiveness of prior knowledge in CODA. Notably, w/o $\\\\hat{C}^{+1}$ refers to a variant of CODA that excludes $\\\\hat{C}^{+1}$ during training the Data Simulator component. According to\"}"}
{"id": "CE7lUzrp1o", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Sensitivity analysis on the $\\\\lambda_C$, where the trained model architecture is MLP.\\n\\nEquation 4 and 5, \\\\(w/o\\\\) $\\\\hat{C}_{T+1}$ essentially trains the Data Simulator only using the last source domain.\\n\\nIn Table 2, for every dataset, all results are conducted by using the same MLP architecture, albeit trained on different data. The results suggest that the data generated by \\\\(w/o\\\\) $\\\\hat{C}_{T+1}$ closely mirrors the characteristics of the last source domain when used as training data, as the LastDomain in Table 1.\\n\\nConversely, CODA is capable of providing a training set that captures the essence of concept drift by incorporating the feature correlation $\\\\hat{C}_{T+1}$ during the training of the Data Simulator. As a result, the identical architecture, when trained on the dataset simulated by CODA, demonstrates markedly superior temporal domain generalization. More analysis of the predicted feature correlation matrices is available in Appendix F.\\n\\n4.5 SENSITIVITY ANALYSIS (RQ3)\\n\\nTable 3: Transferability. Note that \u201cTune\u201d represents models used for searching hyperparameter of the Data Simulator, while \u201cTrain\u201d refers to models trained on the CODA-generated data.\\n\\n|       | Tune | Train | MLP | LightGBM | FTT |\\n|-------|------|-------|-----|----------|-----|\\n|       |      |       | 10.3\u00b11.1 | 10.6\u00b10.4 | 11.0\u00b10.4 |\\n|       |      |       | 10.6\u00b10.9 | 9.1\u00b10.3  | 12.2\u00b10.5  |\\n|       |      |       | 11.6\u00b11.1 | 10.3\u00b10.4 | 10.6\u00b10.4  |\\n| Avg.  |      |       | 10.8\u00b11.0 | 10.0\u00b10.4 | 11.3\u00b10.4  |\\n\\nTo assess the significance of the predicted correlation matrix of CODA framework, we analyze the sensitivity impact of the weighted hyperparameter $\\\\lambda_C$ as presented in Equation 5 with the MLP architecture. As shown in Figure 5, despite the optimal $\\\\lambda_C$ varies among different datasets, the trend of using $\\\\lambda_C$ can be identified on validation sets, where the trend of obtaining best hyperparameter settings in validation sets is similar to the one in test sets. In the two real-world datasets, Elec2 and Appliance, the performance trend with respect to $\\\\lambda_C$ remains consistent between the validation and future testing sets. Conversely, in the ONP dataset, proven to exhibit almost no concept drift, there is no specific performance trend. This might suggest no significant concept drift that can be captured by CODA.\\n\\nObservation 4: The dataset-sensitive hyperparameter $\\\\lambda_C$ is identifiable on the concept drift datasets. The sensitivity analysis reveals that, in the presence of concept drift, the performance trend tied to $\\\\lambda_C$ remains consistent between the validation set and the unseen future domain, underscoring the feasibility of CODA in real-world settings.\\n\\n5 CONCLUSION AND FUTURE WORK\\n\\nIn this work, we demonstrate the efficacy of tackling the fundamental cause of concept drift from a data perspective. To address the limited model prediction ability to capture the underlying temporal trends among the chronological data distributions of source domains, we propose a two-step data simulation framework, CODA, incorporating feature correlation matrices to capture temporal trends within the historical source domains. Specifically, CODA extracts the temporal evolving of historical feature correlation matrices to predict the correlation matrix for the upcoming future domain, and then simulates future training data based on this predicted correlation. Theoretical analysis guarantees that the predicted correlation is reliable under practical assumptions. Experimental results demonstrate that the CODA-generated data can be used as the training data for different model architectures to achieve temporal domain generalization. Regarding future directions, CODA framework unveils several potential facets that merit further exploration, such as more accurate future correlation prediction and advanced generative models to utilize predicted prior knowledge.\"}"}
{"id": "CE7lUzrp1o", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Guangji Bai, Chen Ling, and Liang Zhao. Temporal domain generalization with drift-aware dynamic neural networks. arXiv preprint arXiv:2205.10664, 2022.\\n\\nYogesh Balaji, Rama Chellappa, and Soheil Feizi. Normalized Wasserstein for mixture distributions with applications in adversarial learning and domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6500\u20136508, 2019.\\n\\nLuis M Candanedo, V\u00e9ronique Feldheim, and Dominique Deramaix. Data driven prediction models of energy use of appliances in a low-energy house. Energy and buildings, 140:81\u201397, 2017.\\n\\nJunbum Cha, Kyungjae Lee, Sungrae Park, and Sanghyuk Chun. Domain generalization by mutual-information regularization with pre-trained models. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXIII, pp. 440\u2013457. Springer, 2022.\\n\\nChia-Yuan Chang, Cheng-Wei Lu, and Chuan-Ju Wang. A multi-step-ahead markov conditional forward model with cube perturbations for extreme weather forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 6948\u20136955, 2021.\\n\\nPrithvijit Chattopadhyay, Yogesh Balaji, and Judy Hoffman. Learning to balance specificity and invariance for in and out of domain generalization. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part IX, pp. 301\u2013318. Springer, 2020.\\n\\nBowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. Advances in Neural Information Processing Systems, 34:17864\u201317875, 2021.\\n\\nXiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head: Unifying object detection heads with attentions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7373\u20137382, 2021.\\n\\nQi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. Advances in neural information processing systems, 32, 2019.\\n\\nKelwin Fernandes, Pedro Vinagre, and Paulo Cortez. A proactive intelligent decision support system for predicting the popularity of online news. In Progress in Artificial Intelligence: 17th Portuguese Conference on Artificial Intelligence, EPIA 2015, Coimbra, Portugal, September 8-11, 2015. Proceedings, pp. 535\u2013546. Springer, 2015.\\n\\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096\u20132030, 2016.\\n\\nRui Gong, Wen Li, Yuhua Chen, and Luc Van Gool. Dlow: Domain flow for adaptation and generalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2477\u20132486, 2019.\\n\\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models for tabular data. Advances in Neural Information Processing Systems, 34:18932\u201318943, 2021.\\n\\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.\\n\\nZeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II, pp. 124\u2013140. Springer, 2020.\\n\\nZhimeng Jiang, Xiaotian Han, Chao Fan, Fan Yang, Ali Mostafavi, and Xia Hu. Generalized demographic parity for group fairness. In International Conference on Learning Representations, 2021.\"}"}
{"id": "CE7lUzrp1o", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 9: Visualization of feature correlation matrices from $T^{-2}$ to $T^{+1}$.\\n\\n(a) 2-Moons (b) Elec2 (c) ONP (d) Shuttle (e) Appliance\\n\\nFigure 10: Visualization of the absolute errors between $\\\\hat{C}_{T^{+1}}$ and $C_{T^{+1}}$. \\n\\n19\"}"}
{"id": "CE7lUzrp1o", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Although we have already considered diverse concept drift patterns, here we add two more types of concept shift and data, as shown in Table 6. The considered concept shifts are as follows:\\n\\n**Synthetic concept drifts:**\\n- **Cyclical change:** the 2-Moons dataset is built with a cyclical concept drift pattern, and we conduct Rot-MNIST and Sine datasets as shown in the table above.\\n- **Abrupt change:** this type of temporal trend doesn't fit our assumption that \\\"the joint distribution of features and labels with smooth data shift over time\\\" (refer to Introduction).\\n\\n**Real-world concept drift:**\\nThe real-world datasets used in our experiments feature various and unknown patterns of concept drift. They have covered diverse realistic temporal trends, such as electricity demand changes (Elec2), space shuttle defects (Shuttle), and appliances energy usage changes (Appliance).\\n\\n### Table 6: Performance comparisons on Sine, Rot-MNIST, Portraits, and Forest Cover datasets.\\n\\n| Framework & Components | Training Times (s) |\\n|------------------------|--------------------|\\n| LSSAE (Qin et al., 2022) | 36.8 \u00b1 1.5 |\\n| DDA (Zeng et al., 2023) | 1.6 \u00b1 0.9 |\\n| GI (Nasery et al., 2021) | 33.2 \u00b1 0.7 |\\n| DRAIN (Bai et al., 2022) | 3.0 \u00b1 1.0 |\\n| CODA (MLP) | 2.7 \u00b1 0.9 |\\n| CODA (LightGBM) | 1.2 \u00b1 0.4 |\\n| CODA (FT-Transformer) | 1.1 \u00b1 0.4 |\\n\\n## Training Time Comparisons\\n\\nIn this work, we mainly focus on the effectiveness of achieving TDG rather than on efficiency. Although efficiency is not our main goal, we would like to justify that our proposed framework achieves decent efficiency compared to the SOTA method DRAIN.\\n\\nThe reason is that by splitting the whole temporal trend modeling and data generation process into three sub-processes (learning Correlation Predictor $H(\\\\cdot)$, learning Data Simulator $G(\\\\cdot)$, and predictor training), each of the sub-processes is a manageable sub-problem and takes less training time than a whole end-to-end model. We demonstrate the training time comparison to the SOTA DRAIN on Elec2 dataset in Table 7, where we train the same MLP structure as the predictor.\\n\\n### Table 7: Performance comparisons on Sine and Rot-MNIST datasets.\\n\\n| Framework & Components | Training Times (s) |\\n|------------------------|--------------------|\\n| DRAIN (Bai et al., 2022) | 465.936 |\\n| CODA (Total) | 447.817 |\\n| CODA (Correlation Predictor $H(\\\\cdot)$) | 142.110 |\\n| CODA (Data Simulator $G(\\\\cdot)$) | 290.826 |\\n| CODA (MLP) | 14.880 |\"}"}
{"id": "CE7lUzrp1o", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nKJ Joseph, Salman Khan, Fahad Shahbaz Khan, and Vineeth N Balasubramanian. Towards open-world object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5830\u20135840, 2021.\\n\\nGuoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4893\u20134902, 2019.\\n\\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30, 2017.\\n\\nWen Li, Zheng Xu, Dong Xu, Dengxin Dai, and Luc Van Gool. Domain generalization and adaptation using low rank exemplar svms. IEEE transactions on pattern analysis and machine intelligence, 40(5):1114\u20131127, 2017.\\n\\nAlexander H Liu, Yen-Cheng Liu, Yu-Ying Yeh, and Yu-Chiang Frank Wang. A unified feature disentangler for multi-domain image translation and manipulation. Advances in neural information processing systems, 31, 2018.\\n\\nTennison Liu, Zhaozhi Qian, Jeroen Berrevoets, and Mihaela van der Schaar. Goggle: Generative modelling for tabular data by learning relational structure. In The Eleventh International Conference on Learning Representations, 2022.\\n\\nYuang Liu, Wei Zhang, and Jun Wang. Source-free domain adaptation for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1215\u20131224, 2021.\\n\\nZiwei Liu, Zhongqi Miao, Xingang Pan, Xiaohang Zhan, Dahua Lin, Stella X Yu, and Boqing Gong. Open compound domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12406\u201312415, 2020.\\n\\nZhihe Lu, Yongxin Yang, Xiatian Zhu, Cong Liu, Yi-Zhe Song, and Tao Xiang. Stochastic classifiers for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9111\u20139120, 2020.\\n\\nMassimiliano Mancini, Samuel Rota Bulo, Barbara Caputo, and Elisa Ricci. Best sources forward: domain generalization through source-specific nets. In 2018 25th IEEE international conference on image processing (ICIP), pp. 1353\u20131357. IEEE, 2018.\\n\\nMassimiliano Mancini, Samuel Rota Bulo, Barbara Caputo, and Elisa Ricci. Adagraph: Unifying predictive and continuous domain adaptation through graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6568\u20136577, 2019.\\n\\nSaeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Unified deep supervised domain adaptation and generalization. In Proceedings of the IEEE international conference on computer vision, pp. 5715\u20135725, 2017.\\n\\nKrikamol Muandet, David Balduzzi, and Bernhard Sch\u00f6lkopf. Domain generalization via invariant feature representation. In International conference on machine learning, pp. 10\u201318. PMLR, 2013.\\n\\nAnshul Nasery, Soumyadeep Thakur, Vihari Piratla, Abir De, and Sunita Sarawagi. Training for the future: A simple gradient interpolation loss to generalize along time. Advances in Neural Information Processing Systems, 34:19198\u201319209, 2021.\\n\\nGuillermo Ortiz-Jimenez, Mireille El Gheche, Effrosyni Simou, Hermina Petric Maretic, and Pascal Frossard. Cdot: Continuous domain adaptation using optimal transport. arXiv preprint arXiv:1909.11448, 2019.\\n\\nKarl Pearson. Notes on regression and inheritance in the case of two parents. Proceedings of the Royal Society of London, 58:240\u2013242, 1895.\"}"}
{"id": "CE7lUzrp1o", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CE7lUzrp1o", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Before delving into the proof, we would like to introduce some primary knowledge on total variation distance and its dual-representative format.\\n\\n**Definition 1 (Total variation distance)**\\n\\nLet $P_U, P_V$ be two probability distributions for random variables $U$ and $V$ over a shared probability space $\\\\Omega$. Then, the total variation distance between them, denoted $T_V(U, V)$, is given by\\n\\n$$T_V(U, V) = \\\\sup_{E \\\\subseteq \\\\Omega} |P_U(E) - P_V(E)|.$$  \\n\\n**Lemma 1 (Dual representation of TV distance)**\\n\\nLet $P_U, P_V$ be two probability distributions for random variables $U$ and $V$ over a shared probability space $\\\\Omega$, the dual representation of total variation distance is given by\\n\\n$$T_V(U, V) = \\\\frac{1}{2} K \\\\sup_{\\\\|f\\\\|_\\\\infty \\\\leq K} |E_P_U[f(x)] - E_P_V[f(x)]|,$$\\n\\nwhere $\\\\|f\\\\|_\\\\infty$ norm for function $f$ represents the maximum value of function, i.e.,\\n\\n$$\\\\|f\\\\|_\\\\infty = \\\\inf \\\\{ C > 0 : |f(x)| \\\\leq C \\\\text{ for almost everywhere} \\\\},$$\\n\\nand $K$ could be any constant.\\n\\nRecall that there are three assumptions (i) bounded variable: $|U_i| \\\\leq A$ and $|V_i| \\\\leq A$ for any $i = 1, 2, \\\\ldots, d$; (ii) positive variance: $\\\\min\\\\{D[U_i], D[V_i]\\\\} \\\\geq \\\\delta$; (iii) Similar distribution of multi-variables: $T_V(U, V) \\\\leq \\\\epsilon$, we aim to prove the upper bound for the feature correlation matrices $C_U$ and $C_V$. In the following part, we use the notation for brevity: $\\\\Delta E_i \\\\equiv |E[U_i] - E[V_i]|$, $\\\\Delta E_{ij} \\\\equiv |E[U_i U_j] - E[V_i V_j]|$, and $\\\\Delta D_i \\\\equiv |D[U_i] - D[V_i]|$, where $E[\\\\cdot]$ and $D[\\\\cdot]$ represents the expectation and variance of random variable, respectively.\\n\\nFirstly, we can provide the bounds of the expectation and variance difference as follows:\\n\\n**Lemma 2.** Under assumptions (i) and (iii), we have the following bounds on the expectation and variance difference:\\n\\n$$\\\\Delta E_i \\\\equiv |E[U_i] - E[V_i]| \\\\leq 2 \\\\epsilon A,$$\\n\\n$$\\\\Delta E_{ij} \\\\equiv |E[U_i U_j] - E[V_i V_j]| \\\\leq 2 \\\\epsilon A^2,$$\\n\\n$$\\\\Delta D_i \\\\equiv |D[U_i] - D[V_i]| \\\\leq 6 \\\\epsilon A^2.$$\\n\\n**Proof.** For the bound on the expectation difference, based on Lemma 1 with $f_0(x) = x$, it is easy to obtain\\n\\n$$\\\\Delta E_i \\\\equiv |E[U_i] - E[V_i]| \\\\leq 2 T_V(U_i, V_i) \\\\|f_0\\\\|_\\\\infty \\\\leq 2 T_V(U, V) \\\\|f_0\\\\|_\\\\infty \\\\leq 2 \\\\epsilon A;$$\\n\\nwhere inequality (a) is based on dual representation of TV distance (Lemma 1). For the bound on the covariance difference, based on Lemma 1 with $f_1(x) = x_i x_j$, it is easy to obtain\\n\\n$$\\\\Delta E_{ij} \\\\equiv |E[U_i U_j] - E[V_i V_j]| \\\\leq 2 T_V(U, V) \\\\|f_1\\\\|_\\\\infty \\\\leq 2 \\\\epsilon A^2;$$\\n\\nSimilarly, we can also obtain $|E[U_i^2] - E[V_i^2]| \\\\leq \\\\epsilon A^2$. Therefore, the bound on the variance difference is given by\\n\\n$$\\\\Delta D_i \\\\equiv |D[U_i] - D[V_i]| \\\\leq |E[U_i^2] - E[V_i^2]| + |E[U_i] - E[V_i]| |\\n\\n\\\\leq 2 \\\\epsilon A^2 + 2 \\\\epsilon A \\\\cdot 2 A = 6 \\\\epsilon A^2.$$\"}"}
{"id": "CE7lUzrp1o", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subsequently, we consider the correlation coefficient distance on the $i$-$j$-column element $|C_{U,ij} - C_{U,ij}|$, where $C_{U,ij} = E[U_i U_j] - E[U_i] E[U_j] \\\\sqrt{D[U_i] \\\\cdot D[U_j]}$. According to basic algebra, we have $|C_{U,ij} - C_{U,ij}| = E[U_i U_j] - E[U_i] E[U_j] \\\\sqrt{D[U_i] \\\\cdot D[U_j]} - E[V_i V_j] + E[V_i] E[V_j] \\\\sqrt{D[U_i] \\\\cdot D[U_j]} \\\\leq E[U_i U_j] - E[V_i V_j] + E[V_i] E[V_j] - E[U_i] E[U_j] \\\\sqrt{D[U_i] \\\\cdot D[U_j]} \\\\leq A_2\\\\sqrt{D[U_i] \\\\cdot D[U_j] - D[V_i] \\\\cdot D[V_j]} + D[U_i] - D[V_i] \\\\sqrt{D[U_i] \\\\cdot D[U_j] - D[V_i] \\\\cdot D[U_i]} \\\\leq A_2 \\\\sqrt{D[U_i] \\\\cdot D[U_j] + D[U_i] \\\\cdot D[V_i]} = 6\\\\frac{\\\\epsilon A_2}{\\\\delta^2} (1 + A_2).$ (15)\\n\\nFor the first term $I_1$, we can obtain $I_1 \\\\leq A_2\\\\sqrt{D[U_i] \\\\cdot D[U_j] - D[V_i] \\\\cdot D[V_j]} \\\\leq A_2 \\\\sqrt{D[U_i] \\\\cdot D[U_j] + D[U_i] \\\\cdot D[V_i]} = 6\\\\frac{\\\\epsilon A_2}{\\\\delta^2} (1 + A_2).$ (16)\\n\\nFor the second term $I_2$, we can also obtain $I_2 \\\\leq A_2\\\\sqrt{D[U_i] \\\\cdot D[U_j] - D[V_i] \\\\cdot D[V_j]} \\\\leq A_2 \\\\sqrt{D[U_i] \\\\cdot D[U_j] + D[U_i] \\\\cdot D[V_i]} = 6\\\\frac{\\\\epsilon A_2}{\\\\delta^2} (1 + A_2).$ (17)\\n\\nTherefore, we have the correlation coefficient distance on the $i$-$j$-column element as follows: $|C_{U,ij} - C_{U,ij}| = 6\\\\frac{\\\\epsilon A_2}{\\\\delta^2} (1 + A_2).$ (18)\\n\\nFinally, the relation between the feature correlation matrices can be given by $\\\\|C - C\\\\|_1 = \\\\max_{1 \\\\leq k \\\\leq d} \\\\sum_{i=1}^{d} |C_{U,ij} - C_{U,ij}| \\\\leq 6d\\\\frac{\\\\epsilon A_2}{\\\\delta^2} (1 + A_2).$ (19)\"}"}
{"id": "CE7lUzrp1o", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 1: An overview of the proposed framework CODA, consisting of a Correlation Predictor (see Section 3.2) and a Data Simulator (see Section 3.3). In the first stage, the Correlation Predictor is trained on feature correlation matrices at each time point $C_i$. In the second stage, the predicted correlation matrix $\\\\hat{C}_{T+1}$ served as prior knowledge for Data Simulator $G$ to be updated by Equation 5.\\n\\n2.2 RELATED WORKS\\n\\nDomain Generalization (DG). The goal of DG is to enhance model generalization ability to the open unordered target domains given multiple domains data without accessing target domains. Several methods have been proposed for this task in recent years (Muandet et al., 2013; Cha et al., 2022; Chattopadhyay et al., 2020; Qiao et al., 2020; Motiian et al., 2017; Nasery et al., 2021; Dou et al., 2019). Existing DG methods can be categorized into three branches (Wang et al., 2022): (1) Data manipulation; The input data can be manipulated to learn a general representation via generating diverse data, including data augmentation (e.g., randomization, and transformation) (Tobin et al., 2017; Tremblay et al., 2018) and diverse data generation (Liu et al., 2018; Qiao et al., 2020). (2) Representation learning. The generalized representation learning can be pursued by domain-invariant property via adversarial training, invariant risk minimization, or feature alignments across domains (Ganin et al., 2016; Gong et al., 2019). Another way is feature disentanglement with domain-shared and domain-specific features for better generalization (Li et al., 2017). (3) General learning strategy. The learning strategy for generalization consists of ensemble learning (Mancini et al., 2018), meta-learning (Dou et al., 2019), and gradient operation (Huang et al., 2020), et al.\\n\\nTemporal Domain Generalization (TDG). TDG is a promising yet challenging problem. The difference with DG (categorical domain index) falls in the ordered (continuous) time domain index and thus meticulously requires distribution evolving pattern capture. Unfortunately, there are only a few existing model-centric works on TDG. E.g., Gradient Interpolation (GI) (Nasery et al., 2021) explicitly chases model extrapolation to the near future via the first-order Taylor expansion in the designed loss function; Drift-Aware Dynamic Neural Network (DRAIN) (Bai et al., 2022) formulates a Bayesian probability framework to represent the concept drift over domains and build a recurrent graph to capture dynamic model parameters for the temporal shift. These methods are model-centric and counter the temporal data shift via enhancing model extrapolation or generalization ability or adjusting model parameters accordingly. However, the model may be infeasible to counter all possible future temporal drifts due to limited model capacity. To this end, we proposed a brand-new model-agnostic approach, named Concept Drift Simulator (CODA), for achieving TDG via out-of-distribution (OOD) generation. In this way, the future data can be simulated and then leveraged into model training to enhance generalization. The data-centric approach CODA is model-agnostic with transferability, i.e., you only simulate once for any possible model architectures.\"}"}
{"id": "CE7lUzrp1o", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This section presents a comprehensive overview of our proposed CODA framework, as depicted in Figure 1. Specifically, starting with the feature correlation matrices at various time points, CODA identifies evolving trends to extrapolate the feature correlation matrix for future domains. Leveraging the predicted correlation matrix, CODA subsequently synthesizes future data for model training. The details and pseudo-code of the CODA workflow can be found in Appendix B.\\n\\n3.1 Limitation of Dynamically Generating Future Data\\n\\nGI DRAIN Prelim-LSTM\\n\\nFigure 2: Misclassification Error (McE) rates comparison of the preliminary experiment, where we compare the preliminary data-centric setting, Prelim-LSTM, to the two model-specific SOTAs.\\n\\nTo tackle the challenge of the concept drifts through a data-centric lens, our goal is to model the underlying distribution shift by capturing temporal trends within historical datasets. A straightforward approach is to employ a naive RNN-based architecture for learning temporal patterns from chronological datasets, named Prelim-LSTM. Given $T$ source domains from the Elec2 dataset, $D_1, D_2, \\\\ldots, D_T$, we employ an LSTM unit to generate the synthetic future domain $\\\\hat{D}_{T+1}$. To capture the temporal trend of the probability distribution with limited samples at each time point, we employ Kullback-Leibler (KL) divergence as the objective loss. Specifically, we first approximate the conditional distributions $P(X_t|Y_t)$ for both the predicted and ground truth features via Kernel Density Estimation (KDE). Utilizing the prior probability of labels $P(Y)$, we proceed to compute the joint distribution probability $P(X_t, Y_t)$ between all features and labels. In each training time point, the KL-divergence loss is calculated between the approximated joint distributions of the predicted and ground truth datasets across all features. The objective function is formulated as:\\n\\n$$L_{KL} = \\\\sum_{i=1}^{I} L_i_{KL} \\\\left[ P(X_i t, Y_t)_{\\\\text{pred}} \\\\parallel P(X_i t, Y_t)_{\\\\text{gt}} \\\\right],$$\\n\\nwhere $L_{i_{KL}}$ denotes the KL-divergence of the $i$th feature, $P(X_i t, Y_t)_{\\\\text{pred}}$ represents the predicted joint distribution between $i$th feature and the labels, and $P(X_i t, Y_t)_{\\\\text{gt}}$ is the joint distribution of the ground truth dataset for the same feature. The objective is to minimize $L_{KL}$ to align the predicted joint distribution of each feature as closely as possible with the ground truth. Ideally, given the distribution at the latest observed time point $D_T$, the trained LSTM model tends to predict the pseudo future data distribution $\\\\hat{D}_{T+1}$, leveraging the temporal relationships among $D_1$ to $D_T$.\\n\\nHowever, our preliminary experiments demonstrate that it is difficult to directly capture the temporal trends of underlying joint distribution between $X$ and $Y$ through chronological data. As shown in Figure 2, despite independently approximating the KDE of each feature to the ground truth distribution, the MLP trained on the data predicted by the Prelim-LSTM falls to achieve comparable performance to the two model-specific SOTAs, GI (Nasery et al., 2021) and DRAIN (Bai et al., 2022). It also indicates the necessity of considering feature correlations for capturing concept drift data distribution. Ideally, one would consider all features $X$ concurrently to achieve a more accurate approximation of the data distribution at each temporal instance. However, this becomes computationally infeasible in high-dimensional spaces. To be more specific, grid sampling in an $n$-dimensional space incurs a time complexity of $O(kn)$, where $k$ represents the number of grid points along each dimension. As $n$ increases, the computational cost becomes prohibitive. Therefore, we propose CODA to tackle the aforementioned challenges by breaking down the future distribution generation process into two manageable components and addressing each in a sequential manner.\\n\\n3.2 Correlation Predictor Under Temporal Trends\\n\\nInstead of directly predicting the data distribution for future domains, CODA begins with the Correlation Predictor component to capture the temporal trends of concept drift, as shown in Figure 1. In the first stage of CODA, the Correlation Predictor is specifically designed to discern evolving\"}"}
{"id": "CE7lUzrp1o", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"trends in feature correlations across chronological time domains. Given the observed source domains \\\\( D_1, D_2, \\\\ldots, D_T \\\\), we extract the feature correlation matrices from each domain to serve as meta-data. These matrices can be computed using either traditional statistical approaches, such as Pearson Correlation (Pearson, 1895), or learning-based methods like Self-attention mechanisms (Song et al., 2019) and Graph-based Adjacency Matrix learning (Liu et al., 2022). Armed with these chronological feature correlation matrices, the Correlation Predictor \\\\( H(\\\\cdot) \\\\) is trained to capture the evolving trends in correlation matrices under the influence of concept drift:\\n\\n\\\\[\\n\\\\hat{C}_{T+1} = H(C_1, \\\\ldots, C_T).\\n\\\\]\\n\\nThe objective loss of the Correlation Predictor \\\\( L_{CP} \\\\) is formulated as follows:\\n\\n\\\\[\\nL_{CP} = \\\\sum_{t=1}^{T} \\\\| \\\\hat{C}_t - C_t \\\\|_2 + \\\\| \\\\hat{C}_t - C_t \\\\|_1 + \\\\lambda CE L_{CE}(\\\\hat{C}_t, C_t),\\n\\\\]\\n\\nwhere \\\\( \\\\| \\\\cdot \\\\|_2 \\\\) and \\\\( \\\\| \\\\cdot \\\\|_1 \\\\) are the \\\\( l_2 \\\\)-norm and \\\\( l_1 \\\\)-norm regularization, \\\\( \\\\lambda \\\\) denotes the weight of \\\\( L_{CE} \\\\), and \\\\( L_{CE} \\\\) is the Cross Entropy loss between the predicted correlation matrix \\\\( \\\\hat{C}_t \\\\) and the ground truth feature correlation matrix \\\\( C_t \\\\). It's worth noting that the Correlation Predictor component is modular and can be implemented or substituted with any RNN-based neural network architecture.\\n\\n3.3 DASIMULATOR FOR CONCEPT Drift DATA GENERATION\\n\\nUpon completion of training the Correlation Predictor \\\\( H(\\\\cdot) \\\\), the second stage of CODA utilizes the feature correlation matrix \\\\( \\\\hat{C}_{T+1} \\\\) estimated by \\\\( H(\\\\cdot) \\\\) for the unseen future domain simulation. This estimated correlation matrix \\\\( \\\\hat{C}_{T+1} \\\\) serves as a prior knowledge in the Data Simulator \\\\( G(\\\\cdot) \\\\) to predict the future domain dataset \\\\( \\\\hat{D}_{T+1} \\\\):\\n\\n\\\\[\\n\\\\hat{D}_{T+1} = G(D_T; \\\\hat{C}_{T+1}|\\\\theta_G).\\n\\\\]\\n\\nBy incorporating \\\\( \\\\hat{C}_{T+1} \\\\) and aligning with the data distribution of the current domain \\\\( D_T \\\\), \\\\( G(\\\\cdot) \\\\) aims to approximate both the observed data distribution and the underlying feature correlations. The objective function of the Data Simulator \\\\( G(\\\\cdot) \\\\) is updated by minimizing the following loss:\\n\\n\\\\[\\nL_G = ELBO(D_T, \\\\hat{D}_{T+1}; \\\\theta_G) + \\\\lambda CR C(\\\\hat{C}_{T+1}) = E_{q(z|x,y)}[\\\\ln p_{\\\\theta_G}(x,y|z;\\\\theta_G)] - L_{KL}[q(z|x,y)||p(z)],\\n\\\\]\\n\\nwhere \\\\( ELBO \\\\) represents the classic reconstruction loss, \\\\( \\\\hat{C}_G \\\\) denotes the correlation matrix of the CODA-generated dataset, \\\\( CR \\\\) serves as a regularization term that ensures \\\\( G(\\\\cdot) \\\\) following the prior predicted feature correlation matrix \\\\( \\\\hat{C}_{T+1} \\\\), and \\\\( \\\\lambda \\\\) denotes the weight of \\\\( CR \\\\). It is noteworthy that the Data Simulator component is designed to be modular, allowing for instantiation or replacement with any generative model capable of incorporating prior knowledge.\\n\\n3.4 ANALYSIS ON THE PRIOR KNOWLEDGE IN DATA SIMULATOR\\n\\nIn the data simulator, the estimated correlation matrix \\\\( \\\\hat{C}_{T+1} \\\\) serves as prior knowledge and enhances the quality of concept drift data generation. Notice that the high-quality concept drift data generation is critical to guarantee the well-performed model in TDG, the analysis of the prior knowledge (i.e., low feature correlation matrix distance) in Eq. (5). In this subsection, we provide Theorem 1 to investigate the rationale of this prior knowledge. The proof of Theorem 1 is provided in Appendix A.\\n\\nTheorem 1. Considering two \\\\( d \\\\)-dimensional variables \\\\( U = (U_1, U_2, \\\\ldots, U_d) \\\\) and \\\\( V = (V_1, V_2, \\\\ldots, V_d) \\\\), assume (i) bounded variable: \\\\( |U_i| \\\\leq A \\\\) and \\\\( |V_i| \\\\leq A \\\\) for any \\\\( i = 1, 2, \\\\ldots, d \\\\), where \\\\( A \\\\) is the maximum value of variable; (ii) positive variance: \\\\( \\\\min\\\\{D[U_i], D[V_i]\\\\} \\\\geq \\\\delta \\\\), where \\\\( D[\\\\cdot] \\\\) represents the variance of random variable, and \\\\( \\\\delta \\\\) is the lower bound of variance; (iii) Similar distribution of multi-variables: \\\\( TV(U, V) \\\\leq \\\\epsilon \\\\), where \\\\( TV(\\\\cdot, \\\\cdot) \\\\) represents total variation distance, and \\\\( \\\\epsilon \\\\) is the distribution distance tolerance. Under assumptions (i), (ii), and (iii), the relation between the feature correlation matrices \\\\( C_U \\\\) and \\\\( C_V \\\\) is given by\\n\\n\\\\[\\n\\\\|C_U - C_V\\\\|_1 \\\\leq 6d\\\\epsilon A^2 (1 + A^2 \\\\delta^2).\\n\\\\]\"}"}
{"id": "CE7lUzrp1o", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Discussion on assumptions (i), (ii), (iii). We clarify that these assumptions can be easily satisfied in the real world. The assumption (i) of a bounded variable is naturally satisfied and can be easily reshaped by feature normalization pre-processing. The assumption (ii) of positive variance is also naturally satisfied by pre-processing. The constant variable (feature) represents there is not any information for such a feature and should be filtered out. As the assumption (iii), the data distribution shift is usually smoothed and the distribution of data collected within a short time period should be similar.\\n\\nRemarks. Theorem 1 demonstrates that the prior knowledge in Eq. (5) can be guaranteed under assumptions (i), (ii), (iii), which serves as the theoretical foundation for the usage of prior knowledge in data simulator. The derived bound is tight when the data shift is zero, i.e., $\\\\epsilon = 0$. Furthermore, our analysis suggests that prior knowledge, such as a correlation matrix, may not effectively represent high-dimensional data, and generating such high-dimensional data poses a significant challenge.\\n\\n4 EXPERIMENT\\n\\nIn this section, we conduct experiments to evaluate the performance of CODA, aiming to answer the following three research questions:\\n\\nRQ1: How effective is the simulated future data by CODA to train different model architectures for temporal domain generalization?\\n\\nRQ2: How is the quality of the simulated future data by CODA?\\n\\nRQ3: Does the predicted feature correlation contribute to generating concept drift future data?\\n\\n4.1 EXPERIMENT SETTING\\n\\nDatasets. To evaluate the efficacy of our proposed model-agnostic framework, CODA, we conduct experiments on four classification datasets\u2014Rotated Moons (2-Moons), Electrical Demand (Elec2), Online News Popularity (ONP), and Shuttle\u2014as well as one regression dataset, Appliances Energy Prediction (Appliance). While the 2-Moons is a synthetic dataset with rotation angle acting as a temporal proxy, the remaining four datasets are real-world, temporally evolving collections. More details about the datasets can be found in Appendix C.\\n\\nBaseline Methods. We compare our data-centric approach with three groups of methods: Time-Oblivious Baselines: These methods disregard concept drift, including the strategies Offline (trained on all source domains) and LastDomain (trained on the last source domain); Continuous Domain Adaptation: These algorithms focus on transporting either the last or historical source domains to future domains, including CDOT (Ortiz-Jimenez et al., 2019), CIDA (Wang et al., 2020), and Adagraph (Mancini et al., 2019); and Temporal Domain Generalization: These approaches specifically tackle temporal distribution shifts, including GI (Nasery et al., 2021) and DRAIN (Bai et al., 2022). More baseline details can be found in Appendix D.\\n\\nImplementation Details. In the CODA framework, we employ LSTM (Hochreiter & Schmidhuber, 1997) for the Correlation Predictor and instantiate the Data Simulator component with GOGGLE (Liu et al., 2022). To enhance the compatibility of prior knowledge with the Data Simulator, we extract Feature Correlation matrices from each source domain dataset using GOGGLE's learnable relational structure. All the experiments of CODA are repeated five times on each dataset, with the average results and standard deviation in Table 1. To evaluate the capability of CODA in generating concept-drift data for model training, we implement three predictive architectures: Multilayer Perceptron (MLP), the tree-based LightGBM (Ke et al., 2017), and the Transformer-based FT-Transformer (Gorishniy et al., 2021). More implementation details are elaborated in Appendix D.\\n\\n4.2 QUANTITATIVE ANALYSIS (RQ1)\\n\\nTo assess the generalization efficacy of CODA in the presence of concept drift across temporal domains, we employ CODA to generate future data, matching the sample size of the unseen domains across five datasets. Subsequently, we compare the performance of models trained on this generated data with other baseline methods. To further assess the advantage of our model-agnostic approach, we utilize three different model architectures for the evaluation. For the Data Simulator component's model selection, we employ MLPs with hyperparameters aligned to those of the baseline methods. Subsequently, we fine-tune the hyperparameters of LightGBM and FT-Transformer based on the...\"}"}
{"id": "CE7lUzrp1o", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CODA: TEMPORAL DOMAIN GENERALIZATION VIA CONCEPT DRIFT SIMULATOR\\n\\nAnonymous authors\\n\\nPaper under double-blind review\\n\\nABSTRACT\\nIn real-world applications, machine learning models are often notoriously blamed for performance degradation due to data distribution shifts. Temporal domain generalization aims to learn models that can adapt to \u201cconcept drift\u201d over time and perform well in the near future. To the best of our knowledge, existing works rely on model extrapolation enhancement or models with dynamic parameters to achieve temporal generalization. However, these model-centric training strategies may involve the unnecessarily comprehensive interaction between data and model to train the model for distribution shift, accordingly.\\n\\nTo this end, we aim to tackle the concept drift problem from a data-centric perspective and naturally bypass the cumbersome interaction between data and model. Developing the data-centric framework involves two challenges: (i) existing generative models struggle to generate future data with natural evolution, and (ii) directly capturing the temporal trends of data with high precision is daunting. To tackle these challenges, we propose the Concept Drift Simulator (CODA) framework incorporating a predicted feature correlation matrix to simulate future data for model training. Specifically, the feature correlations matrix serves as a delegation to represent data characteristics at each time point and the trigger for future data generation. Experimental results demonstrate that using CODA-generated data as training input effectively achieves temporal domain generalization across different model architectures with great transferability. Our source code is available at: https://anonymous.4open.science/r/coda-D648\\n\\nINTRODUCTION\\nThe remarkable progress in machine learning has spurred many applications in real-world scenarios (Wei et al., 2019; Tan et al., 2020; Strudel et al., 2021; Cheng et al., 2021; Chang et al., 2021; Dai et al., 2021; Joseph et al., 2021; Jiang et al., 2021). Typically, the historical training data is assumed with the same distribution as future data. However, this assumption is not usually satisfied in real-world settings due to practical data evolving. In other words, the trained model may show performance degradation due to the temporal distribution drift between the training and near-future testing data. Distribution drift hereby poses substantial challenges for researchers to perfectly exploit data on model training. In this manner, Domain Adaptation (DA) (Lu et al., 2020; Liu et al., 2020; Balaji et al., 2019; Kang et al., 2019; Liu et al., 2021), Domain Generalization (DG) (Xu et al., 2020; Yan et al., 2020; Qiao et al., 2020; Chattopadhyay et al., 2020; Piratla et al., 2020; Cha et al., 2022), and Temporal Domain Generalization (TDG) (Bai et al., 2022; Nasery et al., 2021) are proposed to address the distribution shift with different settings.\\n\\nIn contrast to DA, which requires access to unlabeled data in the target domain, DG and TDG are more practical in real-world scenarios where feature information about the target domain is typically unavailable. Existing DG methods aim to improve generalization ability across \u201cdistinct\u201d categorical domains, such as different datasets and sources. However, in real-world applications, data may naturally evolve over time, leading to the emergence of temporal domains. For instance, in predicting seasonal flu trends using Twitter data (Bai et al., 2022), as the platform gains more\"}"}
{"id": "CE7lUzrp1o", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nUsers, forms new connections, and experiences demographic shifts, the correlation between user profiles and flu predictions changes, leading to outdated models. Therefore, temporal distribution shift, a commonly observed phenomenon in the real world, brings significant changes in the joint distribution of features and labels with smooth data shift over time, known as \u201cconcept drift\u201d. While DA and DG methods are designed to bridge the gap between different data distributions, they do not consider the chronological domain indices and smooth drift with the underlying pattern. Such a unique challenge of temporal distribution shift requires further research towards TDG.\\n\\nTo alleviate concept drift for achieving TDG, Gradient Interpolation (GI) (Nasery et al., 2021) and DRAIN (Bai et al., 2022) introduced model-centric strategies, either incorporating specialized regularization or parameter forecasting techniques, and may involve unnecessary interaction between data and model. Noticed that the temporal distribution shift issue is purely from a data perspective, a natural question is raised:\\n\\nCan we achieve TDG via a data-centric approach without involving data-model interaction?\\n\\nOur pioneer research aims to address concept drift from a novel data-centric angle rather than against the parallel model-centric strategy. We provide a positive answer to this question and propose a data-centric approach to simulate future data via out-of-distribution (OOD) data generation. However, it is non-trivial to generate OOD data reflecting the concept drifts from historical data. A naive approach is to directly capture the temporal trends to predict future data using the sequential model. However, this approach is demonstrated to be problematic in our preliminary analysis (see Section 3.1). Specifically, we leverage an LSTM unit for simulating future samples, where the model is trained with dataset-level Kullback-Leibler (KL) divergence. The results demonstrate poor performance in the TDG setting due to the limited model prediction capacity. To further develop an effective data-centric framework for future data generation, there are two main challenges: (i) Generating out-of-distribution future data is beyond the capacity of many existing generative models. Most generative models aim to generate the data following the same distribution of observed training data and therefore are not able to extrapolate the future data distribution. (ii) Directly capturing the temporal trends along chronological source domains with high precision is daunting (the arduousness is demonstrated in Section 3.1).\\n\\nTo tackle the aforementioned challenges, we propose CO\\n\\nConcept Drift Simulator (CODA), a data simulation framework incorporating feature correlation matrices to capture temporal trends within the historical source domains for generating future data. Specifically, CODA consists of two steps: (i) Extracting a dynamic feature correlation matrix from source domains, which is utilized to learn an informative one for the upcoming domain, and (ii) Simulating future training data based on the predicted future feature correlation. Subsequently, the generated future data can be utilized as the training data for prediction models, such as MLP-based or tree-based classifiers, enhancing the performance on the unseen future domain. Theoretical analysis ensures that under practical assumptions, the predicted correlation is dependable. Experimental results demonstrate the effectiveness of CODA in TDG by providing high-quality simulated data. Our contributions are as follows:\\n\\n\u2022 CODA designs a novel model-agnostic approach to tackle the issue of concept drift. We design the feature correlation matrix as a data delegation and future data generation trigger to enhance OOD generation quality.\\n\u2022 Theoretical and experimental analysis indicates that CODA can be facilitated for future data generation by considering the temporal trends of feature correlations.\\n\u2022 Experimental results demonstrate that the simulated future dataset can be leveraged as training data with transferability for various model architectures.\\n\\n2 P RELIMINARY\\n2.1 TEMPORAL DOMAIN GENERALIZATION PROBLEM FORMULATION\\n\\nWe consider temporal prediction tasks with evolved data distribution. In the training stage, given observed continuous source domains $D_1, D_2, \\\\ldots, D_T$, the data instance within domain $D_t$, sampling from distributions at distinct time points $t$, is defined as $\\\\{ (x_{ti}, y_{ti}) \\\\in X_t \\\\times Y_t \\\\}_{i=1}^{N_t}$, where domain index $t = 1, 2, \\\\ldots, T$. Here, $x_{ti}$ and $y_{ti}$ represent the input features and labels, respectively, $N_t$ denotes the sample size, and $X_t \\\\times Y_t$ signifies the feature and label spaces at time $t$. The ultimate goal is to train a model generalized well on some target (test) domain in the future, i.e., $D_{T+1}$. In the temporal\"}"}
{"id": "CE7lUzrp1o", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The training outline of CODA is given in Algorithm 1. CODA follows equation 3 for capturing temporal trends of feature correlation (line 5-6). After $H(\\\\cdot)$ is converged for $\\\\hat{C}_{T+1}$, the Data Simulator $G(D_T; \\\\hat{C}_{T+1}|\\\\theta_G)$ can be updated by Equation 5 and terminated when it is converged. Overall, $G(D_T; \\\\hat{C}_{T+1}|\\\\theta_G)$ learns to approximate both the observed data distribution $D_T$ and the predicted temporal feature correlation $\\\\hat{C}_{T+1}$ for simulating the future data under concept drift.\\n\\nAlgorithm 1: Two-stage Future Data Generation Training with CODA\\n\\n1: Input:\\n- Feature Correlation Matrices of each source domain time points $C_1, C_2, \\\\ldots, C_T$\\n- Correlation Predictor (an RNN-based model) $H(\\\\cdot)$\\n- Current source domain data $D_T$\\n- Data Simulator (a generative model) $G(D_i; C_{i+1}|\\\\theta_G)$\\n\\n2: Output:\\n- Temporal domain generalization data simulator $G(D_T; \\\\hat{C}_{T+1}|\\\\theta_G)$\\n\\n3: First stage: Correlation Predictor Training\\n\\n4: while not convergence do\\n5: Predict the feature correlation matrix of the next domain data by $H(\\\\cdot)$ and Equation 2.\\n6: Update $\\\\hat{C}_{T+1} = H(C_1, C_2, \\\\ldots, C_T)$ by Equation 3.\\n7: end while\\n\\n8: Second stage: Data Simulator Training\\n\\n9: while not convergence do\\n10: Generate future data distribution with $\\\\hat{C}_{T+1}$ and Data Simulator $\\\\hat{D}_{T+1} = G(D_T; \\\\hat{C}_{T+1}|\\\\theta_G)$.\\n11: Update $G(D_T; \\\\hat{C}_{T+1}|\\\\theta_G)$ by Equation 5.\\n12: end while\\n\\nIn this section, we provide a detailed description of the five datasets employed in our experiments.\\n\\n| Notation | Description |\\n|----------|-------------|\\n| $D_t$    | Source domain dataset at time $t$ |\\n| $\\\\hat{D}_t$ | Predicted domain dataset at time $t$ |\\n| $N_t$    | Sample size at time $t$ |\\n| $X_t, Y_t$ | Feature / label space at time $t$ |\\n| $x_t, y_t$ | Sample / label at time $t$ |\\n| $C_t$    | Feature correlation matrix at time $t$ |\\n| $\\\\hat{C}_t$ | Predicted correlation matrix at time $t$ |\\n| $H(\\\\cdot)$ | Correlation Predictor |\\n| $G(\\\\cdot; \\\\cdot|\\\\theta_G)$ | Data Simulator |\\n| $\\\\theta_G$ | Parameters of $G(\\\\cdot)$ |\\n| $P(\\\\cdot)$ | Probability Distribution |\\n\\nRotated 2 Moons (2-Moons). This dataset is a modified version of the 2-entangled moons, where the lower moon is labeled as 0 and the upper moon as 1. Each moon comprises 100 instances. We generate 10 domains by sampling 200 data points from the 2-Moons distribution and rotating them counter-clockwise in increments of 18\u00b0. Consequently, domain $i$ undergoes a rotation of $18i$. Domains 0 through 8 serve as our training domains, while domain 9 is reserved for testing.\\n\\nElectrical Demand (Elec2). This dataset captures the electricity demand in a specific province. It comprises 8 features, including price, day of the week, and units transferred. The binary classification task involves predicting whether the electricity demand in each 30-minute interval was above or below the average demand of the previous day. Instances with missing values are excluded. We define two weeks as one time domain. The model is trained on 29 domains and tested on the 30th domain, resulting in 27,549 training points and 673 test points.\\n\\nOnline News Popularity (ONP). Originating from Fernandes et al. (2015), this dataset aggregates a diverse set of features about articles published by Mashable over a two-year span. The objective is to predict the article's popularity based on the number of shares in social networks. The dataset...\"}"}
{"id": "CE7lUzrp1o", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024 is temporally split into six domains, with the initial five designated for training. While the concept drift is manifested through the progression of time, prior studies, such as Nasery et al. (2021), have demonstrated that the drift is relatively mild.\\n\\nShuttle. The Shuttle dataset comprises approximately 58,000 data points, each with 9 features, pertaining to space shuttles in flight. The objective is multi-class classification, characterized by a pronounced class imbalance. Data points are temporally divided into eight domains based on their associated timestamps. Specifically, timestamps ranging from 30 to 70 define the training domains, while those from 70 to 80 delineate the test domain.\\n\\nAppliances Energy Prediction (Appliance). This dataset Candanedo et al. (2017) is used to create regression models of appliances energy use in a low energy building. The data set is at 10 min for about 4.5 months in 2016, and we treat each half month as a single domain, resulting in 9 domains in total. The first 8 domains are used for training and the last one is for testing. Similar to Elec2, the drift for this dataset corresponds to how the appliances energy usage changes in a low-energy building over about 4.5 months in 2016.\\n\\n**D.1 BASELINES.**\\n\\nPractical Baseline. (i) Offline: A time-oblivious model trained using Empirical Risk Minimization (ERM) across all source domains. (ii) LastDomain: A time-oblivious model trained using ERM exclusively on the most recent source domain. (iii) IncFinetune: This approach biases the training towards more recent data. Initially, the Baseline method is applied to the first time point. Subsequently, the model is fine-tuned on successive time points in a sequential manner, using a reduced learning rate. In this paper, we follow the settings from Nasery et al. (2021) and Bai et al. (2022) for the implementations of the aforementioned three practical baselines.\\n\\nContinuous Domain Adaptation Methods. (i) CDOT (Ortiz-Jimenez et al., 2019): This model transports the most recent labeled examples to the future using a learned coupling from past data, and trains a classifier on them. (ii) CIDA (Wang et al., 2020): This method is representative of typical domain erasure methods applied to continuous domain adaptation problems. (iii) Adagraph (Mancini et al., 2019): This approach renders the batch normalization parameters time-sensitive and applies a specified kernel for smoothing.\\n\\nTemporal Domain Generalization Method. (i) GI (Nasery et al., 2021): This approach introduces a training algorithm that encourages models to learn functions capable of extrapolating effectively to the near future. It achieves this by supervising the first-order Taylor expansion of the learned function. (ii) DRAIN (Bai et al., 2022): This method presents a framework that predicts MLP weights for predicting a near-future domain. It does so by leveraging an LSTM unit and considering the context of chronological source domain MLP weights.\\n\\n**D.2 IMPLEMENTATIONS.** For all the experiments in this paper, we instantiate the two components of CODA, Correlation Predictor 3.2 and Data Simulator 3.3, with LSTM (Hochreiter & Schmidhuber, 1997) and GOGGLE (Liu et al., 2022). We use Adam optimizer for all the experiments. For hyperparameter tuning of the Correlation Predictor, we consider the following search ranges: learning rate: $1 \\\\times 10^{-5}$ to $1 \\\\times 10^{-2}$; number of LSTM layers: 4 to 16; LSTM latent dimension: 4 to 16; LSTM hidden dimension: 4 to 16; $\\\\lambda$ in Equation 3: 0 to 20. For hyperparameter tuning of the Data Simulator, we consider the following search ranges: learning rate: $1 \\\\times 10^{-5}$ to $1 \\\\times 10^{-2}$; encoder dimension: 48 to 72; encoder layer: 2 to 4; decoder dimension: 48 to 72; decoder layer: 2 to 4; $\\\\lambda$ in Equation 5: 0.1 to 20.\"}"}
{"id": "CE7lUzrp1o", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: Hyperparameters of CODA. Here, \u201cL. dim\u201d denotes the latent dimension, \u201cH. dim\u201d represents the hidden dimension, \\\\( \\\\lambda \\\\) is the weight from Equation 3, \u201cEn. dim\u201d indicates the encoder dimension, \u201cEn. layer\u201d specifies the number of encoder layers, \u201cDe. dim\u201d refers to the decoder dimension, \u201cDe. layer\u201d signifies the number of decoder layers, and \\\\( \\\\lambda_c \\\\) is the weight from Equation 5.\\n\\n| Dataset                  | Lr | Layer | L. dim | H. dim | \u03bb_C | En. dim | En. layer | De. dim | De. layer | \u03bb_CE | \u03bb_C |\\n|--------------------------|----|-------|--------|--------|-----|---------|-----------|---------|-----------|------|-----|-------|\\n| 2-Moon2                  | 1  | 2     | 3 \\\\times 10^{-3} | 8     | 8   | 16      | 20.0      | 2       | 1.0       |      |     |       |\\n| 2-Moon2                  | 9  | 2     | 9 \\\\times 10^{-3}  | 64    | 3   | 72      | 3         | 1       | 2.0       |      |     |       |\\n| Electrical Demand (Elec2)| 1  | 2     | 1 \\\\times 10^{-4}  | 16    | 8   | 16      | 0.0       | 2       | 2.0       |      |     |       |\\n| Electrical Demand (Elec2)| 2  | 2     | 2 \\\\times 10^{-2}  | 64    | 2   | 64      | 3         | 5.0      | 20.0      |      |     |       |\\n| Online News Popularity (ONP)| 1 | 2   | 1 \\\\times 10^{-3}  | 10    | 8   | 8       | 5.0       | 1       | 20.0      |      |     |       |\\n| Online News Popularity (ONP)| 1 | 2   | 1 \\\\times 10^{-2}  | 72    | 3   | 72      | 3         | 1.0      | 1.0       |      |     |       |\\n| Shuttle                  | 1  | 3     | 1 \\\\times 10^{-3}  | 10    | 8   | 8       | 1.0       | 1       | 1.0       |      |     |       |\\n| Shuttle                  | 5  | 3     | 5 \\\\times 10^{-3}  | 64    | 2   | 64      | 2         | 20.0     | 1.0       |      |     |       |\\n\\nCorrelation Predictor Data Simulator\\n\\n4. https://lightgbm.readthedocs.io/en/stable/\\n5. https://pytorch-tabular.readthedocs.io/en/latest/\"}"}
{"id": "CE7lUzrp1o", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To investigate the quality of the CODA-generated data, we further provide the data visualizations in Figure 6 to Figure 8 for comparing the simulated datasets with the ground truth unseen future domains and the data generated by our preliminary setting (see Section 3.1). As shown in Figure 6, the Prelim-LSTM can hardly capture the underlying temporal trends from chronological source domains to accurately predict the future data distribution; on the other hand, the distribution of the CODA-generated data in Figure 6-(c) is similar to the ground truth distribution $D$ in Figure 6-(a).\\n\\nIn the Elec2 dataset, Figure 7-(a) demonstrates the impressively closed data distributions between the ground truth and the data generated by CODA; in contrast, the distribution of the data generated by Prelim-LSTM is quite different to the ground truth, as we can see in Figure 7-(b). As the chronological source domains do not manifest strong concept drifts, such as the ONP dataset, both the CODA and Prelim-LSTM cannot precisely simulate the data distribution in the near-future domain, as shown in Figure 8.\\n\\nFigure 6: Visualization of generated future domain data in 2-Moons dataset: (a) Groundtruth of test domain data $D$; (b) the synthetic data generated by Prelim-LSTM; (c) the synthetic data $\\\\hat{D}$ generated by CODA.\\n\\nFigure 7: Visual comparison with the ground truth future data (Elec2).\\n\\nFigure 8: Visual comparison with the ground truth future data (ONP).\\n\\nTo delve deeper into whether the feature correlation matrices reflect the temporal shifts between distinct time points, we visualize the actual feature correlation matrices in Figure 9. We observe that the inter-feature correlations do change over time. Intriguingly, for the regression dataset, Appliance, the prediction target (represented by the last column in the correlation matrices) exhibits pronounced correlations with nearly all features, reflecting the nature of the regression task.\\n\\nFigure 10. To demonstrate the efficacy of the Correlation Predictor in forecasting the near-future correlation matrix, we present the error maps depicting the absolute difference between $\\\\hat{C}_{T+1}$ and $C_{T+1}$ in Figure 10. As we can see, except for the ONP dataset, which does not have strong concept drifts over chronological source domains, the Correlation Predictor can overall predict the future feature correlation matrices with decent error levels. However, we can observe that a few cells in correlation matrices still cannot be precisely predicted. The results unveil the potential future direction for further improving the performance of feature correlation matrices prediction.\"}"}
