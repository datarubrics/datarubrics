{"id": "CO0ZuH5vaMu", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ABSTRACT\\nTranslating source code from one programming language to another is a critical, time-consuming task in modernizing legacy applications and codebases. Recent work in this space has drawn inspiration from the software naturalness hypothesis by applying natural language processing techniques towards automating the code translation task. However, due to the paucity of parallel data in this domain, supervised techniques have only been applied to a limited set of popular programming languages. To bypass this limitation, unsupervised neural machine translation techniques have been proposed to learn code translation using only monolingual corpora. In this work, we propose to use document similarity methods to create noisy parallel datasets of code, thus enabling supervised techniques to be applied for automated code translation without having to rely on the availability or expensive curation of parallel code datasets. We explore the noise tolerance of models trained on such automatically-created datasets and show that these models perform comparably to models trained on ground truth for reasonable levels of noise. Finally, we exhibit the practical utility of the proposed method by creating parallel datasets for languages beyond the ones explored in prior work, thus expanding the set of programming languages for automated code translation.\\n\\n1 INTRODUCTION\\nAs the pace of software development increases and the famous adage \\\"software is eating the world\\\" (Andreessen, 2011) is borne out, there is a corresponding increase in the amount of source code and number of software artefacts in active use for which support has lapsed. At the same time, the number of software professionals and programmers who can support and understand such code is unable to keep pace with the rate at which it is produced. This problem, while important when it comes to relatively modern programming languages (such as Java and Python), becomes even more pressing when it come to legacy languages (like COBOL) that mission-critical applications and systems are written in (Charette, 2020). In recent years, there have been multiple instances of organizations struggling to maintain their legacy systems and making considerable investments to upgrade them. In 2021 the Commonwealth Bank of Australia upgraded its core banking platform originally written in COBOL: this ultimately took 5 years and more than 1 Billion AUD to complete (Irrera, 2017). During the COVID-19 pandemic, software systems implemented in COBOL slowed down the release of US unemployment stimulus checks (Kelly, 2020), leaving governments scrambling to find COBOL experts who were already hard to come by. A recent study by the United States Government Accountability Office (Walsh, 2021) has identified 65 critical federal legacy systems in need of urgent modernization. Some of these systems are over 50 years old, and cost millions of dollars annually to operate and maintain.\\n\\nParallel to these developments are recent efforts at the intersection of software engineering, machine learning (ML), and natural language processing (NLP), which have posited the naturalness hypothesis of software (Hindle et al., 2016). The hypothesis states that \\\"...Software is a form of human communication; software corpora have similar statistical properties to natural language corpora; and these properties can be exploited to build better software engineering tools\\\" (Allamanis et al., 2018). This hypothesis has been used to extend breakthroughs and advances from various NLP sub-fields to software engineering tasks such as code translation. Prior works in the code translation domain have proposed the application of statistical, supervised, and unsupervised machine translation techniques to learn code translation models to varying degrees of success.\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A key limitation of a majority of the proposed code translation approaches, however, is the lack of availability of parallel data for training. Unlike natural language, where a piece of text is verbatim translated in multiple languages \u2013 legal documents, parliamentary proceedings in multilingual societies \u2013 code is rarely implemented as is in multiple languages; thus making it hard to create parallel datasets. A few limited datasets \u2013 such as Java \u2194 C# (Nguyen et al., 2013) and AVATAR for Java \u2194 Python (Ahmad et al., 2021b) \u2013 are currently available. However, these are extremely limited in the number of programming languages they cover, and manually curating a dataset for a specific use-case is impractical. To bypass this limitation, unsupervised techniques have been applied to the code translation task. Unsupervised techniques come with their own limitations however; and often, supervised techniques can outperform them when the source and target corpora are from different domains, the source and target languages use different scripts, and on low-resource language pairs, among other concerns (Kim et al., 2020; Marchisio et al., 2020).\\n\\nIt is for this reason that in this work, we focus on one of the main blockers impeding the application of supervised techniques to code translation: the availability of parallel corpora and datasets. Specifically, we propose to utilize document similarity methods to create parallel source code datasets that are noisy by design. In this work, we empirically demonstrate the effectiveness of document similarity methods in creating such parallel datasets with high levels of accuracy. Given that datasets created in this manner are bound to be noisy, we study the performance characteristics of models for code translation that have been trained on data with varying degrees of noise; and show that these models have considerable resistance to noise and perform well even with moderate amounts of noise. Finally, we demonstrate the practical utility of the proposed approach by training models to translate between 10 pairs of languages \u2013 a majority of which have not been looked at in prior work.\\n\\n### RELATED WORK\\n\\n#### Code translation datasets:\\nTypical methods for creating parallel datasets for code translation have either relied on the availability of open-sourced projects with implementations in multiple languages, or on the existence of transpilers. The earliest widely-used large-scale dataset for code translation was for Java \u2194 C# (Nguyen et al., 2013) translation, created by indexing open-sourced projects implemented in both languages. Aggarwal et al. (2015) used the Python 2to3 transpiler to create a dataset; while Chen et al. (2018) used CoffeeScript\u2019s compiler (which compiles down to JavaScript) to create a parallel dataset. More recently, Ahmad et al. (2021b) released AVATAR \u2013 a parallel corpus of Java to Python manually curated through submissions on competitive programming websites. Publicly available datasets for code translation are however extremely limited, and manually curating these datasets for a specific use-case is expensive and often impractical.\\n\\n#### Source-to-Source translation:\\nThe earliest code translation models were rule-based systems, operating on handcrafted rules. These systems require a lot of effort to build, are not easily extendable to other languages, and are also outperformed by neural techniques. Some of these systems are: Java2CSharp, Java2Python, SmallTalk to C (Yasumatsu & Doi, 1995), Cobol to Java (Mossienko, 2003), and Tangible Software Solutions (VB.NET, C#, Java, C++, and Python).\\n\\nMoving away from rule-based systems, Nguyen et al. (2013), Karaivanov et al. (2014), and Nguyen et al. (2014) applied different versions of Phrase-Based Statistical Machine Translation to translate between Java and C#. Chen et al. (2018) proposed a tree-to-tree neural network to translate the parsed tree of the source code into the target code parse tree. The aforementioned supervised techniques have all been benchmarked on the Java \u2194 C# dataset, and are limited by the availability of parallel datasets. To bypass this limitation, Roziere et al. (2020) used unsupervised neural machine translation techniques to translate between languages using only monolingual corpora, and showed impressive results for translation between Java, C++, and Python. While Roziere et al. (2020) trained the model specifically for code translation, large language models \u2013 such as GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), and Codex (Chen et al., 2021) \u2013 have also been shown to have some competence in generating code (Hendrycks et al., 2021).\\n\\n#### Parallel corpus mining:\\nPrior work in natural language research has looked at various ways of creating parallel corpora from a non-parallel corpus. Munteanu & Marcu (2005) train a maximum...\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nentropy classifier to identify if two given sentences are translations of each other. They extract parallel data from large-scale Chinese, Arabic, and English non-parallel newspaper corpora, and show improvement in model performance when trained with a combination of a small parallel corpus and the extracted dataset. Uszkoreit et al. (2010) describe a system that uses n-gram features to mine parallel documents from a billion-scale corpus. Smith et al. (2010) focus on aligning Wikipedia documents by creating features suitable for such documents. Artetxe & Schwenk (2019) utilize specific scoring functions based on multilingual sentence embeddings to create parallel corpora, and Hangya & Fraser (2019) rely on continuous parallel segments rather than word similarities to find parallel sentences. Ban\u00f3n et al. (2020) released the largest publicly available parallel corpora (223 million parallel sentences) by aligning sentences from data crawled over the web.\\n\\nThere is a substantial precedence of parallel corpus mining in the natural language domain; however, such studies in the code translation domain are non-existent.\\n\\nMachine Translation using noisy data:\\n\\nPrior studies have aimed to study the impact of noise on the performance of machine translation systems. Formiga & Fonollosa (2012) study the impact of misspelled words on the performance of Statistical Machine Translation and suggest strategies to deal with them, while Goutte et al. (2012) study the impact of sentence alignment errors on the performance of SMT. Further, Khayrallah & Koehn (2018) define 5 categories of artificial noise in Neural Machine Translation, and study the impact each of these types has on performance. We motivate our work from these prior efforts in order to study the impact that noise has on the performance of code translation models.\\n\\n3 Proposed Method\\n\\nIn this work, we propose to utilize document similarity methods to create noisy parallel datasets for code translation. We refer to the datasets created in this manner as \\\"noisy\\\" because unlike manually curated datasets, there is no guarantee of a parallel implementation of the specific source file/code being available in the corpus: this may result in near-similar code samples being paired as ground truth examples instead. Algorithm 1 presents the proposed approach as pseudocode.\\n\\nAlgorithm 1 Creating parallel code corpus\\n\\n1: CreateParallelCorpora(D,D\u2032,M,\u03b4)\\n2: initialize P = {}\\n3: for i = 1 to |D| do\\n4: Dsim = GetSimilarDocuments(di,D\u2032,M)\\n5: for (di,d\u2032j) in Dsim do\\n6: if (\u00b7,d\u2032j) /\u2208 P and M(di,d\u2032j) \u2264 \u03b4 then\\n7: P = P \u222a (di,d\u2032j)\\n8: break\\n9: Dres = sort((d1,d2) \u2208 P, key = M(d1,d2))\\n10: Return: Dres\\n\\nThe algorithm expects two non-parallel sets of documents $D = \\\\{d_1, \\\\cdots, d_n\\\\}$ and $D' = \\\\{d'_1, \\\\cdots, d'_m\\\\}$ as input. Within the context of our work, the documents in these two sets represent code samples from two distinct programming languages. Along with the documents, the algorithm also expects a similarity measure $M(d,d')$ as input, to compare two given documents for similarity. A lower score from the similarity measure indicates higher similarity between documents. Finally, the algorithm expects a similarity threshold $\\\\delta$ to help keep only sufficiently similar documents in the resulting parallel corpus. Thereafter, the algorithm follows a simple procedure of iterating over all documents; finding the most similar documents in the target set; and adding the newly found similar document pairs to the result only if the target document has not been paired before, and if the similarity is below the threshold value. Once all the documents are iterated upon, the algorithm produces a list of unique pairs of code segments (documents) ordered by their similarity, ready to be used for downstream tasks.\\n\\n4 Experimental Setup\\n\\nTo better understand the effectiveness and practical utility of the method proposed in Section 3, we devise 3 research questions and design experiments to empirically answer them (see Section 5). In this section, we briefly summarize the different document similarity methods, datasets, pre-trained models, and evaluation metrics we use in our experiments.\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CO0ZuH5vaMu", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsupervised translation of programming languages. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 20601\u201320611. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ed23fbf18c2cd35f8c7f8de44f85c08d-Paper.pdf.\\n\\nYossi Rubner, Carlo Tomasi, and Leonidas J Guibas. A metric for distributions with applications to image databases. In Sixth International Conference on Computer Vision (IEEE Cat. No. 98CH36271), pp. 59\u201366. IEEE, 1998.\\n\\nGerard Salton and Christopher Buckley. Term-weighting approaches in automatic text retrieval. Information processing & management, 24(5):513\u2013523, 1988.\\n\\nJason Smith, Chris Quirk, and Kristina Toutanova. Extracting parallel sentences from comparable corpora using document level alignment. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 403\u2013411, 2010.\\n\\nJakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Dubiner. Large scale parallel document mining for machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pp. 1101\u20131109, 2010.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998\u20136008, 2017.\\n\\nKevin Walsh. Agencies need to develop and implement modernization plans for critical legacy systems. 2021. URL https://www.gao.gov/assets/gao-21-524t.pdf.\\n\\nJustin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross, Fernando Mart\u00ednez, Mayank Agarwal, and Kartik Talamadupula. Perfection not required? human-ai partnerships in code translation. In 26th International Conference on Intelligent User Interfaces, pp. 402\u2013412, 2021.\\n\\nKazuki Yasumatsu and Norihisa Doi. Spice: a system for translating smalltalk programs into a C environment. IEEE Transactions on Software Engineering, 21(11):902\u2013912, 1995.\\n\\nHao Zhong, Suresh Thummalapenta, Tao Xie, Lu Zhang, and Qing Wang. Mining API mapping for language migration. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 1, pp. 195\u2013204, 2010.\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we provide representative code samples from the datasets we use in this work. Listings 1 and 2 are data samples from the Java $\\\\rightarrow$ C# dataset. Listings 3 and 4 are data samples from the Java $\\\\rightarrow$ Python dataset. Listings 5 and 6 are data samples from the Java $\\\\rightarrow$ C++ dataset. Listings 7 and 8 are data samples from the C++ $\\\\rightarrow$ Python dataset. Finally, listings 9, 10, 11, 12, 13, 14, 15, 16 provide code samples from one particular problem set from the CodeNet dataset.\\n\\n**Listing 1: Java $\\\\rightarrow$ C#: Java code sample**\\n```java\\npublic static Cell getCell(Row row, int columnIndex)\\n{\\n    Cell cell = row.getCell(columnIndex);\\n    if (cell == null)\\n    {\\n        cell = row.createCell(columnIndex);\\n    }\\n    return cell;\\n}\\n```\\n\\n**Listing 2: Java $\\\\rightarrow$ C#: C# code sample**\\n```csharp\\npublic static ICell GetCell(IRow row, int column)\\n{\\n    ICell cell = row.GetCell(column);\\n    if (cell == null)\\n    {\\n        cell = row.CreateCell(column);\\n    }\\n    return cell;\\n}\\n```\\n\\n**Listing 3: Java $\\\\rightarrow$ Python: Java code sample**\\n```java\\nstatic int binaryToDecimal ( int n )\\n{\\n    int num = n ;\\n    int dec_value = 0;\\n    int base = 1;\\n    int temp = num ;\\n    while ( temp > 0 ) {\\n        int last_digit = temp % 10 ;\\n        temp = temp / 10 ;\\n        dec_value += last_digit * base ;\\n        base = base * 2 ;\\n    }\\n    return dec_value ;\\n}\\n```\\n\\n**Listing 4: Java $\\\\rightarrow$ Python: Python code sample**\\n```python\\ndef binaryToDecimal ( n ) :\\n    num = n ;\\n    dec_value = 0;\\n    base = 1;\\n    temp = num ;\\n    while ( temp ) :\\n        last_digit = temp % 10 ;\\n        temp = int ( temp / 10 ) ;\\n        dec_value += last_digit * base ;\\n        base = base * 2 ;\\n    return dec_value ;\\n```\\n\\n**Listing 5: Java $\\\\rightarrow$ C++: Java code sample**\\n```java\\nstatic int findS ( int s ) {\\n    int sum = 0;\\n    for ( int n = 1 ;\\n           sum < s ;\\n           n ++ ) {\\n        sum += n * n;\\n        if ( sum == s ) return n ;\\n    }\\n    return - 1 ;\\n}\\n```\\n\\n**Listing 6: Java $\\\\rightarrow$ C++: C++ code sample**\\n```c++\\nint findS ( int s ) {\\n    int sum = 0;\\n    for ( int n = 1;\\n           sum < s;\\n           n ++ ) {\\n        sum += n * n;\\n        if ( sum == s ) return n;\\n    }\\n    return - 1;\\n}\\n```\\n\\n**Listing 7: C++ $\\\\rightarrow$ Python: C++ code sample**\\n```c++\\nvoid printDistinct ( int arr \\[ \\], int n ) {\\n    sort ( arr, arr + n );\\n    for ( int i = 0; i < n ;\\n           i ++ ) {\\n        while ( i < n - 1 && arr \\[ i \\] == arr \\[ i + 1 ] ) i ++;\\n        cout << arr \\[ i \\] << \" \";\\n    }\\n}\\n```\\n\\n**Listing 8: C++ $\\\\rightarrow$ Python: Python code sample**\\n```python\\ndef printDistinct ( arr , n ) :\\n    arr.sort ( ) ;\\n    for i in range ( n ) :\\n        if ( i < n - 1 and arr [ i ] == arr [ i + 1 ] ) :\\n            while ( i < n - 1 and ( arr [ i ] == arr [ i + 1 ] ) ) :\\n                i += 1 ;\\n        else :\\n            print ( arr [ i ] , end = \\\" \\\" ) ;\\n```\\n\\n**Listing 9: CodeNet: C code sample**\\n```c\\n#include<stdio.h>\\n\\nint main(){\\n    int i,j;\\n    for(i=1;i<10;i++)\\n        for(j=1;j<10;j++)\\n            printf(\\\"%dx%d=%d\\\\n\\\",i,j,i*j);\\n    return 0;\\n}\\n```\\n\\n**Listing 10: CodeNet: C# code sample**\\n```csharp\\nusing System;\\n\\nclass test{\\n    static void Main(){\\n        for (int i = 1; i < 10; i++)\\n            for (int j = 1; j < 10; j++)\\n                Console.WriteLine(i + \\\"x\\\" + j + \\\"=\\\\\" + i*j + \\\"\\\\n\\\");\\n    }\\n}\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section we look at the various properties of the parallel dataset created from the CodeNet dataset. In section B.1, we present most similar data samples identified by the WMD metric across various language pairs. In section B.2, we present the histograms of similarity scores in the final dataset. Finally, in section B.3, we present the number of data samples in the final dataset created from the CodeNet dataset.\\n\\nB.1 IDENTIFIED MOST SIMILAR DATA SAMPLES ACROSS LANGUAGE PAIRS\\n\\nIn table 4, we provide the most-similar code samples identified by the WMD metric across various language pairs along with the computed similarity between the two samples.\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"```cpp\\n#include <cstdio>\\n#include <iostream>\\n#include <algorithm>\\n#include <cstring>\\n#include <cmath>\\nusing namespace std;\\ntypedef long long LL;\\n\\nint n, k, ans = 0, flg = 0;\\nint a[500005];\\n\\nint main()\\n{\\n    ios::sync_with_stdio(false);\\n    cin >> n;\\n    memset(mx, -0x3f, sizeof(mx));\\n    mx[0] = 0;\\n    for (int i = 1; i <= n; i++)\\n        cin >> a[i];\\n    for (int i = 1; i <= n; i++)\\n    {\\n        if (a[i] == 1) flg = 1;\\n        mx[a[i]] = max(mx[a[i]], mx[a[i] - 1] + 1);\\n        ans = max(ans, mx[a[i]]);\\n    }\\n    if (!flg) cout << -1 << endl;\\n    else cout << n - ans << endl;\\n    return 0;\\n}\\n```\\n\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"under review as a conference paper at ICLR 2022\\n\\n```javascript\\nconst main = input => {\\n    const args = input.split('\\n').map(arg => arg.split(' \\n'))\\n    const A = parseInt(args[0][0], 10)\\n    const B = parseInt(args[0][1], 10)\\n    if (A % 3 === 0 || B % 3 === 0 || (A + B) % 3 === 0)\\n        console.log('Possible')\\n    else console.log('Impossible')\\n}\\nmain(require('fs').readFileSync('/dev/stdin', 'utf8'))\\n```\\n\\n```c\\n#include <stdio.h>\\nint main(void)\\n{\\n    int a, b;\\n    scanf(\"%d %d\", &a, &b);\\n    if (a % 3 == 0 || b % 3 == 0 || (a + b) % 3 == 0) printf(\\\"Possible\\\\n\\\");\\n    else printf(\\\"Impossible\\\\n\\\");\\n    return 0;\\n}\\n```\\n\\n```python\\n'use strict'\\nconst main = input => {\\n    const args = input.split('\\n').map(arg => arg.split(' '))\\n    const A = parseInt(args[0][0], 10)\\n    const B = parseInt(args[0][1], 10)\\n    if (A % 3 === 0 || B % 3 === 0)\\n        console.log('Possible')\\n    else console.log('Impossible')\\n}\\nmain(require('fs').readFileSync('/dev/stdin', 'utf8'))\\n```\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A, B = gets.split.map(&:to_i)\\nS = gets.chomp\\nif /\\\\d{#{A}}-\\\\d{#{B}}/.match(S)\\n  puts 'Yes'\\nelse\\n  puts 'No'\\nend\\n\\n#include<stdio.h>\\nint main(void)\\n{\\n  int A, B;\\n  scanf(\\\"%d %d\\\", &A, &B);\\n  char S[100];\\n  scanf(\\\"%s\\\", S);\\n  if (S[0] == 'A' && S[1] == 'B' && S[2] == 'A')\\n    printf(\\\"No\\\\n\\\");\\n    return 0;\\n  if (S[0] == 'A' && S[1] == 'B' && S[2] == 'A')\\n    printf(\\\"No\\\\n\\\");\\n    return 0;\\n  if (S[0] == 'A' && S[1] == 'B' && S[2] == 'A')\\n    printf(\\\"No\\\\n\\\");\\n    return 0;\\n  if (S[0] == 'A' && S[1] == 'B' && S[2] == 'B')\\n    printf(\\\"No\\\\n\\\");\\n    return 0;\\n  printf(\\\"Yes\\\\n\\\");\\n  return 0;\\n}\\n\\nScala\\n\\n```scala\\nimport scala.io.Source\\nobject Main extends App {\\n  val lines: Iterator[String] = Source.stdin.getLines()\\n  val line = lines.next.split(\\\" \\\").map(_.toInt).take(2)\\n  val a = line.head\\n  val b = line(1)\\n  println((if (b >= a) a else a - 1).toString)\\n}\\n```\\n\\nPython translation\\n\\n```python\\nimport sys\\nlines = sys.stdin.readlines()\\nfor line in lines:\\n  a, b = map(int, line.split())\\n  if b >= a:\\n    print(a)\\n  else:\\n    print(b - 1)\\n```\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Thus we create datasets for translation between Java and C# by artificially injecting noise of varying levels. For explanation, in a dataset with \\\\(x\\\\)% noise level, we randomly misalign \\\\(x\\\\)% of code samples in that dataset, while keeping the remaining code samples correctly paired. We then train the GraphCodeBERT model on these datasets, and compute the BLEU score, CodeBLEU score, and the Exact Match (EM) score. Figure 1 shows the performance curve with levels of noise varying from 0% (ground-truth dataset) to 100% (complete random pairings). We observe that the degradation in the performance is gradual for initial levels of noise \u2013 compared to performance at 0% noise, the performance at about 30% noise goes down slowly by about 20 percentage points across all measures and both ways of translation. Post this 30% noise level, we see a sharper degradation in performance; and post 70% noise, we observe that the performance is just slightly better than the performance at 100% noise.\\n\\nThrough the experiments in Table 3 and in Figure 1, we get a better idea of the performance characteristics of models for code under varying levels of noise. We conclude that the performance of these models is not severely affected with moderate amounts of noise; therefore when creating datasets from code in the wild where we expect a certain amount of noise, we can expect the models to perform reasonably well.\\n\\n5.3 RQ3: TRANSLATING BETWEEN A WIDER SET OF PROGRAMMING LANGUAGES\\n\\nIn Section 5.1, we concluded that document similarity methods are adept at creating parallel datasets for code with acceptable levels of noise; and in Section 5.2, we concluded that models trained on noisy datasets of code perform reasonably well under moderate levels of noise. In this section, we take advantage of these two findings and demonstrate the practical utility of the proposed method by creating noisy code translation datasets for languages not explored previously in the literature; and by training models for translating between these languages.\\n\\nFor this experiment, we utilize the CodeNet dataset by creating noisy parallel datasets between the following 10 languages \u2013 C, C#, C++, Go, Java, JavaScript, PHP, Python, Ruby, and Scala. We choose these languages in order of their frequency in the CodeNet dataset, thereby maximizing the number of data samples we can potentially create. We also sub-sample about 2500 problem sets from the original 4000 problem sets from the CodeNet dataset for computational reasons. Thereafter, we match the solutions in one programming language to another for each of the 2500 sub-sampled problem sets using the WMD metric. Since the PLBART model can only translate sequences with a maximum of 512 tokens, we only match code samples with less than 512 tokens. We additionally use a similarity threshold of 3.0 and filter samples accordingly. In Appendix B.3, we provide statistics of the final dataset along with some representative code samples and their corresponding similarity scores. To create the test set for the language pairs, we sub-sample 100 problems that are not seen in the training and the validation set, and randomly sample 5 different implementations in the source language from each of the 100 problem sets for a final test set size of 500 code samples.\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We fine-tune the PLBART model on the matched training data for each language pair, and evaluate the computational accuracy @ 5 on the test set. We compare the performance of this fine-tuned model against a model fine-tuned on a dataset created by randomly matching solutions from each problem set rather than using the WMD metric. To compare these two models fairly, we keep the dataset sizes, problem sets, and all the hyperparameters constant across the two training procedures. The CA@5 results for the model trained on the WMD-matched dataset are shown in Figure 2; while Figure 3 shows the difference in the performance of the model fine-tuned using the WMD-matched data and the performance of the model fine-tuned using the randomly matched data. We present some of the model generated code samples in Appendix B.4.\\n\\nOverall, we observe that models trained using the WMD-matched datasets achieve noteworthy performance across language pairs. More importantly, when compared with models trained on randomly paired data, we see substantial improvements for a majority of the language pairs. While the common language pairs, such as C $\\\\rightarrow$ C++, Python $\\\\rightarrow$ C++, Ruby $\\\\rightarrow$ C++, see the biggest improvements, more obscure language pairs such as PHP $\\\\rightarrow$ C++, PHP $\\\\rightarrow$ Ruby, JavaScript $\\\\rightarrow$ C++, and Python $\\\\rightarrow$ Ruby also demonstrate substantial improvements over their random counterparts. This leads us to the conclusion that the proposed method is a viable way of creating high-quality datasets for code translation, thereby alleviating the paucity of training data in the domain.\\n\\n6 DISCUSSION & CONCLUSION\\n\\nModernizing legacy applications into a new programming language is a process that requires a lot of time, intellect, and monetary investment. Automatic code translation techniques have the potential to speed up this process, and to reduce the human effort required by either working in tandem with humans or automatically translating legacy code to a modern language of choice. While multiple techniques have been proposed to improve the quality of code translation, their practical utility is hampered due to the limited availability of parallel data required to train these models between languages of choice. In this work, we proposed a simple technique to utilize document similarity methods to create noisy datasets for code translation; and demonstrated that models for code have a certain amount of tolerance for noise and perform well even under significant amounts of noise. We specifically demonstrated the effectiveness of the Word Movers Distance (WMD) metric in creating parallel datasets between numerous language pairs that have not been explored in prior literature; and showed significantly improved model performances as compared to models trained on randomly matched datasets. Future work will explore better metrics in terms of both match accuracy and computational efficiency, thereby further reducing the noise in the dataset; and incorporating the similarity score in the model to weight samples according to their computed similarity.\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"One of the major ethical points to consider when dealing with the automatic creation and translation of source code centers around the effects on humans: both humans who create and maintain code for a living; and humans that are affected by the decisions and outcomes produced by the execution of such code. For the former concern, our work merely seeks to align pre-existing bits of open-sourced code so that downstream data-hungry techniques may have more reasonable approximations of correct and on-purpose code to learn from. Our work does not replace jobs that humans are trained to do and more adept at; and indeed defers to and takes inspiration from prior studies (Weisz et al., 2021) that show that human generators of code are very likely to engage in partnerships with automatically learned models to produce or maintain code better. For the latter concern, we acknowledge that it is possible to use the output of artefacts from our work in downstream systems that can produce automatic code with little to no oversight. Similar to work on examining the effects of large language models for human natural languages (Bender et al., 2021), much attention is needed where it comes to automatic code generation and translation techniques and models. We look forward to studying some of these issues in partnership with colleagues in the future.\\n\\nTo allow for the reproducibility of experiments conducted in this work, we provide the source code of the experiments in the supplementary material attached with the submission. The supplementary material is grouped by the three research questions we define in our work. Each set of files within a folder corresponding to a research question contains the source code to the respective experiments. Another critical aspect of our work is the creation of parallel code translation datasets across many languages from the CodeNet dataset. Along with the source code, we also provide the train, validation, and test data sets for a small subset of language pairs in the attached supplementary material. We provide a small subset due to the limitations on the amount of data that can be provided as supplementary material. However, we plan to release the complete noisy dataset we created for our experiments with the final version of the paper.\\n\\nKaran Aggarwal, Mohammad Salameh, and Abram Hindle. Using machine translation for converting python 2 to python 3 code. Technical report, PeerJ PrePrints, 2015.\\n\\nWasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified pre-training for program understanding and generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2655\u20132668, 2021a.\\n\\nWasi Uddin Ahmad, Md Golam Rahman Tushar, Saikat Chakraborty, and Kai-Wei Chang. Avatar: A parallel corpus for java-python program translation. arXiv preprint arXiv:2108.11590, 2021b.\\n\\nMiltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of machine learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):1\u201337, 2018.\\n\\nMarc Andreessen. Why software is eating the world. Wall Street Journal, 20(2011):C2, 2011.\\n\\nMikel Artetxe and Holger Schwenk. Margin-based parallel corpus mining with multilingual sentence embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3197\u20133203, 2019.\\n\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\\n\\nMarta Ban\u00f3n, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Espla-Gomis, Mikel L Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, et al. Paracrawl: Web-scale acquisition of parallel corpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4555\u20134567, 2020.\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 610\u2013623, 2021.\\n\\nDavid M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:993\u20131022, 2003.\\n\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\n\\nRobert N Charette. No one notices the creaky software systems that run the world\u2014until they fail. IEEE Spectrum, 57(9):24\u201330, 2020.\\n\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n\\nXinyun Chen, Chang Liu, and Dawn Song. Tree-to-tree neural networks for program translation. Advances in Neural Information Processing Systems, 31, 2018.\\n\\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations, 2019.\\n\\nScott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman. Indexing by latent semantic analysis. Journal of the American society for information science, 41(6):391\u2013407, 1990.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, 2019.\\n\\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pp. 1536\u20131547, 2020.\\n\\nLluis Formiga and Jos \u00b4e AR Fonollosa. Dealing with input noise in statistical machine translation. In Proceedings of COLING 2012: Posters, pp. 319\u2013328, 2012.\\n\\nCyril Goutte, Marine Carpuat, and George Foster. The impact of sentence alignment errors on phrase-based machine translation performance. In Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers, San Diego, California, USA, October 28-November 1 2012. Association for Machine Translation in the Americas. URL https://aclanthology.org/2012.amta-papers.7.\\n\\nDaya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, LIU Shujie, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-training code representations with data flow. In International Conference on Learning Representations, 2020.\\n\\nViktor Hangya and Alexander Fraser. Unsupervised parallel sentence extraction with parallel segment detection helps machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1224\u20131234, 2019.\\n\\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021.\\n\\nAbram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu. On the naturalness of software. Communications of the ACM, 59(5):122\u2013131, 2016.\\n\\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019.\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1 DOCUMENT SIMILARITY METHODS\\n\\nTF-IDF: TF-IDF (Salton & Buckley, 1988) computes the product of the term frequency (TF) (fraction of times a term appears in a document) with the inverse document frequency (IDF) (logarithm of the inverse fraction of documents a particular token occurs in). The cosine similarity between the document vectors thus created computes the similarity of documents.\\n\\nOkapi-BM25: The Okapi-BM25 model (Robertson et al., 1995) uses the following scoring function (Equation 1) to score the importance of a word $w$ in a document $D$. Here, $IDF(w)$ represents the inverse document frequency of the word $w$, $TF(w,D)$ represents the term frequency of the word $w$ in the document $D$, $|D|$ and $D_{avg}$ are the lengths of the current document and the average document lengths respectively, and $k_1$ and $b$ are free parameters of the model.\\n\\n$$BM25(w,D) = IDF(w) \\\\times \\\\frac{TF(w,D)}{k_1 + TF(w,D) + k_1(1-b + b \\\\frac{|D|}{D_{avg}})}$$\\n\\nLatent Dirichlet Allocation (LDA): LDA (Blei et al., 2003) is a hierarchical generative Bayesian model that models each document as a finite mixture over an underlying set of topics. The cosine similarity of the topic distribution of two documents computes their similarity.\\n\\nLatent Semantic Indexing (LSI): LSI (Deerwester et al., 1990) computes the Singular Value Decomposition of the Bag of Words representation of documents. The cosine similarity of the decomposed vectors of documents computes their similarity.\\n\\nWord Movers Distance (WMD): WMD (Kusner et al., 2015) models the document distance problem as a variant of the Earth Movers Distance (Monge, 1781; Rubner et al., 1998) and solves the optimization problem defined in Equation 2.\\n\\n$$\\\\min_{T \\\\geq 0} \\\\sum_{i,j=1}^{n} T_{ij} c(i,j)$$\\nsubject to:\\n\\n$$\\\\sum_{j=1}^{n} T_{ij} = d_i \\\\quad \\\\forall i \\\\in \\\\{1, \\\\ldots, n\\\\}$$\\n$$\\\\sum_{i=1}^{n} T_{ij} = d_j \\\\quad \\\\forall j \\\\in \\\\{1, \\\\ldots, n\\\\}$$\\n\\nHere, $T \\\\in \\\\mathbb{R}^{n \\\\times n}$ is a flow matrix where $T_{ij} \\\\geq 0$ and denotes how much of word $i$ in document $d$ travels to word $j$ in document $d'$. $c(i,j) = \\\\|x_i - x_j\\\\|_2$ is the cost associated with travelling from one word to another, $d$ and $d'$ are the nBOW representations of the documents, and $X \\\\in \\\\mathbb{R}^{d \\\\times n}$ is the word embedding matrix where $x_i \\\\in \\\\mathbb{R}^d$ represents the $d$-dimensional embedding of the $i$th word.\\n\\n4.2 DATASETS\\n\\nFor the experiments whose results are detailed in Section 5, we utilize the following datasets. We provide representative code samples and statistics from the datasets in Appendix A.\\n\\nJava \u2194 C#: The Java \u2194 C# dataset is one of the earliest large-scale datasets introduced for the code translation task (Nguyen et al., 2013; Zhong et al., 2010). It is created by indexing several open-source projects which have both Java and C# implementations, and pairing methods with the same file name and method name. The earlier version of this data was created by indexing the db4o and Lucene projects. More recently however, Chen et al. (2018) indexed 6 open-sourced projects to create the dataset. We use the version provided by Chen et al. (2018) in our work.\\n\\nJava \u2194 Python, Java \u2194 C++, and Python \u2194 C++: Roziere et al. (2020) extracted parallel functions in C++, Python, and Java from the online competitive programming platform GeeksForGeeks.\\n\\n5 https://practice.geeksforgeeks.org/\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022 and used these code samples as validation and test sets. We, however, concatenate the two datasets and use the unified dataset for our experiments. The code samples in this dataset are function-scope code samples that solve an algorithmic problem.\\n\\nCodeNet:\\nProject CodeNet (Puri et al., 2021) is a recently released large-scale AI for Code dataset, created by indexing two online competitive programming websites. The dataset is organized into about 4000 different problem sets, and contains a little under 14 million total solutions in 55 programming languages. Besides providing the code samples, CodeNet also provides input-output pairs to evaluate solutions to the problem sets.\\n\\n4.3 Models\\nCodeBERT:\\nCodeBERT (Feng et al., 2020) is a Transformer (Vaswani et al., 2017) based model, pre-trained on a unimodal data of function-level code samples, and a bimodal data of code and the associated documentation in natural language. The pre-training data contains code samples in Go, Java, JavaScript, PHP, Python, and Ruby, and is trained using the Masked Language Modeling (Devlin et al., 2019) (MLM) and the Replaced Token Detection (Clark et al., 2019) objectives.\\n\\nGraphCodeBERT:\\nGraphCodeBERT (Guo et al., 2020) is a Transformer based model for code that also considers the inherent structure in code by integrating the data flow in the pre-training stage. The model is trained on the CodeSearchNet dataset (Husain et al., 2019) using the MLM, Edge Prediction, and Node Alignment objectives.\\n\\nPLBART:\\nPLBART (Ahmad et al., 2021a) is a BART (Lewis et al., 2020) based model pre-trained on over 700 million Java, Python, and natural language documents collected from open-sourced code on Github and posts on StackOverflow. The model is pre-trained via denoising autoencoding, where the model learns to reconstruct input corrupted by a noise function. The authors use three noising strategies: token masking, token deletion, and token infilling to create the corrupted inputs.\\n\\n4.4 Evaluation Metrics\\nBLEU score:\\nBLEU score (Papineni et al., 2002) is a common automatic evaluation metric for machine-generated text, and exhibits a high correlation with human judgment of quality. BLEU score is computed as the overlapping fraction of n-grams between the machine-generated text and the reference text. The metric has however been shown to not be a reliable measure for source code (Ren et al., 2020; Allamanis et al., 2018; Austin et al., 2021).\\n\\nCodeBLEU score:\\nRen et al. (2020) propose the CodeBLEU score to leverage the tree structure and semantic information in code. It is computed as a combination of the standard BLEU score, weighted n-gram match, syntactic abstract syntax tree match, and the semantic data flow match.\\n\\nExact Match (EM):\\nEM (Nguyen et al., 2013) evaluates if the generated code matches exactly to the reference code.\\n\\nComputational Accuracy @k (CA@k):\\nRecent work in code synthesis has adopted the CA@k metric (Austin et al., 2021; Roziere et al., 2020) to evaluate code generation models. To compute CA@k, k samples are generated from the model, and the problem is considered solved if any of the generated k samples pass the unit tests associated with the problem.\\n\\n5 Research Questions and Results\\nTo validate the central hypothesis of this paper \u2013 using document similarity methods to create datasets for supervised training of code translation models \u2013 we define and seek answers to the following research questions (RQ):\\n\\nRQ1: How accurate are document similarity methods in creating parallel datasets for code?\\nRQ2: Given the created dataset will be noisy, what is the effect of varying degrees of noise on code translation models?\\nRQ3: Can the proposed method be used in practice to train models for programming languages not explored in prior work?\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.1 RQ1: EFFICACY OF DOCUMENT SIMILARITY METHODS\\n\\nWe start our analysis by examining how effective document similarity methods are in creating code translation datasets. For this experiment, we utilize 4 datasets with known ground-truth mapping between pairs of programming languages \u2013 Java \u2194 C#, Java \u2194 Python, Java \u2194 C++, and Python \u2194 C++. For each of these datasets, we create a parallel dataset using 5 different document similarity methods, and compute the match accuracy as the number of correctly matched code samples.\\n\\nWe summarize the results for this experiment in Table 1, and observe that similarity methods that operate in a latent space (such as LDA and LSI) perform much worse than methods that operate in the original space (such as TF-IDF, Okapi-BM25, and WMD). We posit that because code is written in a more formal language than natural language, and each data sample in the datasets used in this experiment implements an independent unique function, there is likely no underlying topic or latent semantic associations that can be captured by LSI and LDA. Therefore these methods perform worse than methods that directly utilize the tokens in the original space.\\n\\n| Dataset | LDA  | LSI  | TF-IDF | Okapi-BM25 | WMD  |\\n|---------|------|------|--------|-------------|------|\\n| Java \u2194 C# | 47.21% | 57.21% | 87.36% | 87.86% | 89.53% |\\n| Java \u2194 Python | 21.44% | 66.08% | 86.10% | 95.77% | 91.04% |\\n| Java \u2194 C++ | 35.83% | 87.66% | 94.08% | 89.99% | 95.06% |\\n| Python \u2194 C++ | 16.64% | 78.84% | 89.35% | 89.99% | 94.08% |\\n\\nWe note that the datasets used for the experiments in Table 1 are not true representatives of code we expect to find while using this method in practice. This is due to the fact that the 4 datasets used contain code samples for which a true parallel implementation in the other language exists. When trying to create a parallel dataset from code collected in the wild, we cannot be sure of the availability of a true parallel implementation, which might affect the performance of the proposed method. Thus, to account for this phenomenon, we conduct a similar experiment with the CodeNet dataset. Since code samples in the CodeNet dataset are not parallel implementations, this gives us a better idea of the effectiveness of document similarity methods in creating parallel datasets from code in the wild. We select 6 languages and randomly sub-sample 50 problems from the dataset. For each of the code sample in each of the 50 sampled problem sets, we create a parallel dataset using 3 different document similarity methods. Since we do not have a ground-truth parallel implementation available for the CodeNet dataset, we cannot compute the match accuracy like we did in the previous experiment. We therefore compute the pseudo-match accuracy instead. The pseudo-match accuracy computes True if the matched code sample is a solution of the same problem set, and False otherwise.\\n\\nWe report the pseudo-match accuracy results for this experiment in Table 2. We note that while the TF-IDF and Okapi-BM25 methods performed well in the previous experiment, their performance varies greatly for this experiment. Datasets created using TF-IDF and Okapi-BM25 methods are matched to the correct problem set with as little as 30% accuracy and as high as 70% accuracy in some cases. Datasets created using the WMD method however achieve a high match accuracy for both experiments (Table 1 and Table 2).\\n\\nFrom the experiments designed to answer RQ1, we conclude that document similarity methods are capable of creating parallel datasets of code with a significantly high degree of match accuracy. Specifically, the WMD metric seems to be quite adept at delineating datasets for code translation.\\n\\n5.2 RQ2: NOISE TOLERANCE OF MODELS TRAINED ON CODE\\n\\nIn the previous section, we showed that document similarity methods \u2013 and specifically the Word Movers Distance (WMD) \u2013 are quite adept at creating parallel datasets for code. However, datasets created in this manner contain errors; therefore in this section, we seek to understand the effect that varying the degree of such noise has on the performance of code translation models.\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Pseudo-match accuracy of datasets created by different document similarity methods on 50 subsampled problems from CodeNet. Pseudo-match accuracy computes True if the matched code sample is from the same problem set, and False otherwise.\\n\\n| Source language: Go | Java | JavaScript | PHP | Python | Ruby |\\n|---------------------|------|------------|-----|--------|------|\\n| TF-IDF              | 29.56% | 41.91% | 30.70% | 40.35% | 27.07% |\\n| BM25                | 50.93% | 49.07% | 50.52% | 36.93% | 35.37% |\\n| WMD                 | 71.47% | 55.70% | 51.35% | 60.89% | 45.12% |\\n\\n| Source language: Java | Go | JavaScript | PHP | Python | Ruby |\\n|----------------------|----|------------|-----|--------|------|\\n| TF-IDF               | 65.05% | 53.40% | 29.41% | 31.32% | 23.47% |\\n| BM25                 | 63.94% | 73.46% | 34.11% | 51.63% | 36.09% |\\n| WMD                  | 79.30% | 73.93% | 57.51% | 66.49% | 56.06% |\\n\\n| Source language: JavaScript | Go | Java | PHP | Python | Ruby |\\n|---------------------------|----|-----|-----|--------|------|\\n| TF-IDF                    | 50.93% | 46.64% | 44.89% | 41.18% | 55.34% |\\n| BM25                      | 51.74% | 58.12% | 56.26% | 59.74% | 61.60% |\\n| WMD                       | 66.70% | 74.71% | 67.52% | 70.88% | 73.55% |\\n\\n| Source language: PHP | Go | Java | JavaScript | Python | Ruby |\\n|---------------------|----|-----|------------|--------|------|\\n| TF-IDF              | 64.31% | 27.49% | 45.66% | 26.04% | 37.62% |\\n| BM25                | 62.54% | 50.48% | 48.71% | 27.97% | 55.63% |\\n| WMD                 | 71.38% | 61.25% | 73.79% | 85.05% | 84.08% |\\n\\n| Source language: Python | Go | Java | JavaScript | PHP | Ruby |\\n|------------------------|----|-----|------------|-----|------|\\n| TF-IDF                 | 67.19% | 52.79% | 66.67% | 69.54% | 72.46% |\\n| BM25                   | 73.52% | 57.53% | 74.13% | 73.33% | 80.28% |\\n| WMD                    | 82.08% | 73.75% | 77.39% | 79.59% | 87.31% |\\n\\nWe use the CodeBERT and the GraphCodeBERT pre-trained models for this experiment, and fine-tune these models on different pairings of the Java \u2194 C# dataset (Nguyen et al., 2013) created using the different document similarity methods. We compare the performance of models trained on these paired datasets with models trained on the ground-truth dataset, and a random baseline with random pairings of code samples from the two programming languages. Following Ahmad et al. (2021a), we compute the BLEU score, CodeBLEU score, and the Exact Match score.\\n\\nThe results for this experiment are summarized in Table 3. We additionally refer the reader to Table 1 to see the corresponding match accuracy of the different document similarity methods. Interestingly, we find that models trained on noisy code datasets have a certain degree of resistance to noise; and while the performance drops with increasing levels of noise, the degradation is not sudden. Even with high levels of noise, the models perform considerably well. With a high-performing method such as the Word Movers Distance (WMD) \u2013 with about 90% match accuracy \u2013 the degradation in performance is roughly 1 percentage point across the three measures and for both directions of translation. For methods with a higher level of noise \u2013 such as LDA with 47.21% match accuracy \u2013 while the performance goes down significantly, it is still significantly higher than the performance of the random baseline. We posit that although noisy datasets create pairs of code with incorrect parallel implementations, much of the semantics is still retained by the dataset due to the formal nature of programming languages. For example, even if code samples are incorrectly paired, the syntax for function and variable definition, code blocks, and indentation stays the same and is preserved. This allows the model to learn the translation task to a certain degree.\\n\\nTable 3: Model performance on the Java \u2194 C# dataset matched using various document similarity methods. Each method introduces a different amount of noise in the resulting dataset (see Table 1) thereby affecting the performance.\\n\\n|                 | Java \u2192 C# | C# \u2192 Java |\\n|----------------|-----------|-----------|\\n| **BLEU**       | 12.2      | 4.4       |\\n| **CodeBLEU**   | 31.71     | 16.56     |\\n| **EM**         | 0.0%      | 0.0%      |\\n| **Random baseline** | 12.2 | 4.4 |\\n| **LDA**        | 57.55     | 43.75     |\\n| **CodeBLEU**   | 65.97     | 55.39     |\\n| **EM**         | 33.2%     | 23.7%     |\\n| **LSI**        | 71.8      | 52.44     |\\n| **CodeBLEU**   | 77.64     | 66.26     |\\n| **EM**         | 47.9%     | 30.9%     |\\n| **Okapi-BM25** | 79.39     | 73.83     |\\n| **CodeBLEU**   | 83.54     | 80.64     |\\n| **EM**         | 59.5%     | 57.6%     |\\n| **TF-IDF**     | 78.78     | 72.52     |\\n| **CodeBLEU**   | 82.70     | 77.34     |\\n| **EM**         | 58.1%     | 57.1%     |\\n| **WMD**        | 79.59     | 75.16     |\\n| **CodeBLEU**   | 83.85     | 80.93     |\\n| **EM**         | 58.2%     | 59.2%     |\\n| **Ground-truth** | 80.83 | 75.84 |\\n| **CodeBLEU**   | 84.86     | 81.64     |\\n| **EM**         | 60.6%     | 59.9%     |\\n| **GraphCodeBERT** | 6.88 | 2.94 |\\n| **Random baseline** | 12.2 | 4.4 |\\n| **LDA**        | 60.34     | 47.14     |\\n| **CodeBLEU**   | 67.91     | 58.79     |\\n| **EM**         | 37.3%     | 26.3%     |\\n| **LSI**        | 73.74     | 52.86     |\\n| **CodeBLEU**   | 79.30     | 66.49     |\\n| **EM**         | 49.1%     | 30.5%     |\\n| **TF-IDF**     | 79.14     | 73.14     |\\n| **CodeBLEU**   | 83.04     | 77.55     |\\n| **EM**         | 57.8%     | 57.9%     |\\n| **Okapi-BM25** | 79.65     | 74.24     |\\n| **CodeBLEU**   | 83.42     | 80.56     |\\n| **EM**         | 59.4%     | 58.0%     |\\n| **WMD**        | 79.47     | 75.63     |\\n| **CodeBLEU**   | 83.72     | 81.06     |\\n| **EM**         | 59.0%     | 60.2%     |\\n| **Ground-truth** | 80.89 | 76.76 |\\n| **CodeBLEU**   | 85.05     | 82.03     |\\n| **EM**         | 61.1%     | 62.3%     |\\n\\nWhile the preceding experiment allowed us to understand the performance of models trained on noisy datasets created using different document similarity methods, we wish to understand the performance of models trained on noisy datasets created using different document similarity methods.\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Most similar code samples across various language pairs as identified by the WMD metric\\n\\n**C** \u2192 **Python** dataset: Similarity: 0.75\\n\\n```c\\n#include <stdio.h>\\n#include <math.h>\\n\\nint main()\\n{\\n    int n;\\n    int i;\\n    double x1, x2, x3, y1, y2, y3, px, py, r;\\n    scanf(\"%d\", &n);\\n    for (i = 0; i < n; i++){\\n        scanf(\"%lf %lf %lf %lf %lf %lf\", &x1, &y1, &x2, &y2, &x3, &y3);\\n        px = ((y2 - y3)*(x1*x1 + y1*y1) + (y3 - y1)*(x2*x2 + y2*y2) + (y1 - y2)*(x3*x3 + y3*y3))/(2*(x1*(y2 - y3) + x2*(y3 - y1) + x3*(y1 - y2)));\\n        py = ((x2 - x3)*(x1*x1 + y1*y1) + (x3 - x1)*(x2*x2 + y2*y2) + (x1 - x2)*(x3*x3 + y3*y3))/(2*(y1*(x2 - x3) + y2*(x3 - x1) + y3*(x1 - x2)));\\n        r = sqrt(pow((x1 - px), 2) + pow((y1 - py), 2));\\n        printf(\"%.3f %.3f %.3f\\n\", px, py, r);\\n    }\\n    return 0;\\n}\\n```\\n\\n**C#** \u2192 **Java** dataset: Similarity: 0.62\\n\\n```java\\nimport math\\n\\nn = int(raw_input())\\nfor i in range(n):\\n    x1, y1, x2, y2, x3, y3 = map(float, raw_input().split())\\n    px = ((y2 - y3)*(x1*x1 + y1*y1) + (y3 - y1)*(x2*x2 + y2*y2) + (y1 - y2)*(x3*x3 + y3*y3))/(2*(x1*(y2 - y3) + x2*(y3 - y1) + x3*(y1 - y2)));\\n    py = ((x2 - x3)*(x1*x1 + y1*y1) + (x3 - x1)*(x2*x2 + y2*y2) + (x1 - x2)*(x3*x3 + y3*y3))/(2*(y1*(x2 - x3) + y2*(x3 - x1) + y3*(x1 - x2)));\\n    r = math.sqrt(pow((x1 - px), 2) + pow((y1 - py), 2));\\n    print \"%.3f %.3f %.3f\\n\", px, py, r\\n```\\n\\n**Scala** \u2192 **Ruby** dataset: Similarity: 0.91\\n\\n```scala\\nobject Main extends App {\\n    val a = Array(1, 1, 1, 2, 1, 2, 1, 5, 2, 2, 1, 5, 1, 2, 1, 14, 1, 5, 1, 5, 2, 2, 1, 15, 2, 2, 5, 4, 1, 4, 1, 51)\\n    val k = scala.io.StdIn.readInt - 1\\n    println(a(k))\\n}\\n```\\n\\n**PHP** \u2192 **Python** dataset: Similarity: 1.34\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"while ($line = trim(fgets(STDIN))) {\\n    $num = explode(\\\" \\\",$line);\\n    if ($num[0] == 0 && $num[1] == 0) {\\n        exit();\\n    }\\n    sort($num);\\n    $line = implode(\\\" \\\",$num);\\n    echo $line . PHP_EOL;\\n}\\n\\nwhile True:\\n    num = [int(x) for x in input().rstrip().split()]\\n    if num[0] ==0 and num[1] == 0:\\n        break\\n    num.sort()\\n    ans = \\\" \\\".join(str(x) for x in num)\\n    print(ans)\\n\\nB.2 SIMILARITY HISTOGRAMS IN THE CREATED DATASET\\n\\nWe show the histograms of the similarity scores in the datasets matched through the WMD metric on the CodeNet dataset in figure 4.\\n\\nB.3 DATA STATISTICS\\n\\nIn table 5, we provide the number of data samples in the final dataset created from the CodeNet dataset.\\n\\n| Language | C# | C++ | Go | Java | JS | PHP | Python | Ruby | Scala |\\n|----------|----|-----|----|------|----|-----|--------|------|-------|\\n| C        | 2,295 | 30,977 | 1,732 | 15,909 | 2,876 | 2,802 | 27,947 | 28,423 | 5,506 |\\n| C#       | 3,145 | 1,358 | 974 | 5,016 | 1,907 | 1,741 | 7,778 | 12,595 | 4,720 |\\n| C++      | 595 | 17,809 | 2,496 | 23,821 | 2,954 | 2,898 | 76,825 | 48,985 | 6,660 |\\n| Go       | 2,629 | 642 | 2,077 | 2,858 | 998 | 1,180 | 7,235 | 8,838 | 3,343 |\\n| Java     | 846 | 1,793 | 669 | 2,262 | 2,653 | 2,454 | 29,007 | 23,515 | 3,536 |\\n| JS       | 635 | 1,550 | 639 | 2,007 | 2,307 | 1,389 | 48,301 | 6,457 | 17 |\\n| PHP      | 821 | 2,183 | 821 | 821 | 2,034 | 1,133 | 7,126 | 17 | 17 |\\n| Python   | 10,499 | 44,707 | 10,499 | 14,375 | 2,996 | 2,996 | 48,301 | 6,457 | 17 |\\n| Ruby     | 5,564 | 14,375 | 5,564 | 7,746 | 2,310 | 2,310 | 7,126 | 17 | 17 |\\n| Scala    | 1,354 | 1,354 | 1,354 | 12,846 | 1,133 | 1,133 | 48,301 | 6,457 | 17 |\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Histogram of similarity scores for various language pairs matched by WMD.\\n\\nUnder review as a conference paper at ICLR 2022.\"}"}
{"id": "CO0ZuH5vaMu", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 6: Examples of correct and incorrect translations given the source code to a PLBART model fine-tuned on a dataset matched using the WMD metric\\n\\n| Source C code | Correct Python translation | Incorrect Python translation |\\n|---------------|----------------------------|------------------------------|\\n| ```c\\n#include <stdio.h>\\n#include <stdlib.h>\\n\\nint main(void) {\\n    long n;\\n    scanf(\\\"%ld\\\", &n);\\n    long a[n];\\n    for (long i = 0; i < n; i++) {\\n        scanf(\\\"%ld\\\", &a[i]);\\n    }\\n    for (long i = 0; i < n; i++) {\\n        if (a[i] % 2 != 0) {\\n            printf(\\\"first\\\\n\\\");\\n            return 0;\\n        }\\n    }\\n    printf(\\\"second\\\\n\\\");\\n    return 0;\\n}\\n``` | ```python\\nn = int(input())\\na = list(map(int, input().split()))\\nfor i in range(n):\\n    if a[i] % 2 != 0:\\n        print(\\\"first\\\")\\n        exit()\\nprint(\\\"second\\\")\\n``` | |\\n| Source C# code | Correct Java translation | Incorrect Java translation |\\n| ```c#\\nusing System;\\nusing System.Linq;\\n\\nnamespace Tenka1A {\\n    class Program {\\n        static void Main(string[] args) {\\n            string[] input = Console.ReadLine().Split(' '); \\n            int A = int.Parse(input[0]);\\n            int B = int.Parse(input[1]);\\n            int C = int.Parse(input[2]);\\n            bool flg = false;\\n            if (A > B) {\\n                if (C > B && A > C) {\\n                    flg = true;\\n                }\\n            } else if (B > A) {\\n                if (B > C && C > A) {\\n                    flg = true;\\n                }\\n            }\\n            if (flg) {\\n                Console.WriteLine(\\\"Yes\\\");\\n            } else {\\n                Console.WriteLine(\\\"No\\\");\\n            }\\n        }\\n    }\\n}\\n``` | ```java\\nimport java.util.Scanner;\\nimport java.util.Arrays;\\n\\npublic class Main {\\n    public static void main(String[] args) {\\n        Scanner sc = new Scanner(System.in);\\n        int A = sc.nextInt();\\n        int B = sc.nextInt();\\n        int C = sc.nextInt();\\n        boolean flg = false;\\n        if (A > B) {\\n            if (C > B && A > C) {\\n                flg = true;\\n            }\\n        } else if (B > A) {\\n            if (B > C && C > A) {\\n                flg = true;\\n            }\\n        }\\n        if (flg) {\\n            System.out.println(\\\"Yes\\\");\\n        } else {\\n            System.out.println(\\\"No\\\");\\n        }\\n    }\\n}\\n``` | |\\n| Source C++ code | Correct Ruby translation | Incorrect Ruby translation |\\n| ```c++\\n#include <iostream>\\n#include <vector>\\n\\nint main() {\\n    std::vector<int> a;\\n    std::cin >> a;\\n    int n = a.size();\\n    for (int i = 0; i < n; i++) {\\n        if (a[i] % 2 != 0) {\\n            std::cout << \\\"first\\\\n\\\";\\n            return 0;\\n        }\\n    }\\n    std::cout << \\\"second\\\\n\\\";\\n    return 0;\\n}\\n``` | ```ruby\\nn = gets.chomp.split(' ').map(&:to_i)\\na = n[0..-1].to_a\\nfor i in 0...n.size\\n    if a[i] % 2 != 0\\n        puts \\\"first\\\"\\n        exit\\n    end\\nend\\nputs \\\"second\\\"\\n``` | |\"}"}
