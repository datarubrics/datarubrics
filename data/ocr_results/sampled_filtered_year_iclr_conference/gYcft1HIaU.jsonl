{"id": "gYcft1HIaU", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We meticulously crafted instructions for each knowledge aspect, encompassing both a detailed description of the knowledge element and specific constraints regarding the output format. Importantly,\"}"}
{"id": "gYcft1HIaU", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Instructions employed for each knowledge aspect, comprising the Chinese version and English translation.\\n\\nThese prompts were developed in collaboration with domain experts in the medical field to ensure their high quality.\"}"}
{"id": "gYcft1HIaU", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nFigure 8: The pattern extracted from LLM\u2019s responses using regular expression, wherein \\n\\\\[\\\\text{disease name}\\\\] and \\\\[\\\\text{knowledge aspect}\\\\] correspond to the question.\\n\\nWe leverage simple heuristic rules to extract relevant segments in the original responses, as illustrated in Figure 8. Given the disease name and the knowledge aspect, we search patterns that appear in our crafted instructions and demonstrative examples and extract the answer part in patterns. The response will remain unchanged if no pattern is discovered in the response.\\n\\nThe NER model that we leverage to process responses of enumerated types is trained and constructed following the method proposed in Su et al. (2022). We first pretrained a BERT-base model on 3.5 million highly de-identified EHRs from 7 hospitals with MLM objective proposed in Devlin et al. (2018). Then we finetuned the model on 200k labeled EHR segments by following the method proposed in Su et al. (2022), teaching the model to extract medical entities in EHRs. On a test set of 10k+ real-world EHRs involving 40k+ medical entities, our NER model achieves 0.88 micro-f1 score across a total of 116 types of medical entities, even surpassing 0.9 on several common medical entities, such as anatomical sites, symptoms, medication. This model has already been applied in a wide range of medical scenarios, including assisted consultations and diagnosis, as well as EHR-based semantic parsing, demonstrating consistent and reliable performance.\\n\\nTable 11: Consistency between clinical experts and automated score prior to expert alignment, measured by Spearman correlation. Asterisks denote statistically significance (p-value < 0.05). The correlation coefficient of BERTScore is below 0.7, indicating misalignment with human judgment.\\n\\nTable 12: Thresholds used in expert-aligned automated grading. Distinct thresholds are applied to various metrics and types. Note that the grading of numeric types is relied on exact matching.\"}"}
{"id": "gYcft1HIaU", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We further analyze the detailed performance of LLMs across 18 knowledge aspects, and present the results in Figure 9 and 10, respectively. The results in Figure 9 indicate that LLMs achieve relatively better performance on Treatment Principles and Onset Ages, and poorer performance on several aspects such as Secondary Diseases and Laboratory Examinations.\\n\\nFigure 9: The average performance of LLMs across 18 clinical knowledge aspects. PP: Patient Population; PA: Prevalence Ages; OA: Onset Ages; PS: Primary Symptoms; AS: Associated Symptoms; DS: Differential Symptoms; PE: Physical Examination; AnS: Anatomical Sites; AfS: Affected Body Systems; TP: Treatment Principles; SD: Secondary Diseases; SP: Surgical Procedures; Med: Medications; AE: Auxiliary Examinations; LE: Laboratory Examinations; Dept: Departments; SL: Severity Level; ABS: Affected Body System.\\n\\nResults in Figure 10 further suggest that different LLMs perform distinctly on the same knowledge aspect. For example, GPT-3.5-turbo achieves around 5 on Primary Symptoms (PS), while models such as LLaMA, BenTsao, and ChatDoctor achieve under 1 on this aspect. It is worth noting that GPT-3.5-turbo achieves relatively stable performance across all knowledge aspects.\"}"}
{"id": "gYcft1HIaU", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 16: Examples of LLM responses in three tiers.\"}"}
{"id": "gYcft1HIaU", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We plan to make our evaluation benchmark freely accessible for any researchers and organizations by releasing an online evaluation platform. The platform provides participants who are seeking to evaluate their LLMs a list of diseases, prompts we employ in this work, and several demonstrative examples. Participants can either choose to directly test their LLMs with the provided prompts, or DIY prompts by themselves. Once the participants upload the LLMs' response on the platform, the evaluation script will be running automatically. The evaluation results, including the performance on various knowledge aspects, will be available for downloading once the evaluation process ends. Participants can choose whether to update their performance on a leaderboard and compare the performance with others.\"}"}
{"id": "gYcft1HIaU", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Figure 5: Calculating total scores with the distribution on three tiers.\\n\\n| Model             | Type     | Completely Wrong | Partially Correct | Basically Correct | Total Score | Level |\\n|-------------------|----------|------------------|-------------------|-------------------|-------------|-------|\\n| GPT-3.5 Turbo     | General  | 46.1%            | 27.7%             | 26.1%             | 4.00        | 1     |\\n| Vicuna-7B         | General  | 54.2%            | 28.9%             | 17.0%             | 3.14        | 2     |\\n| Baichuan-7B       | General  | 54.4%            | 29.3%             | 16.3%             | 3.10        | 2     |\\n| HuatuoGPT-7B      | Medical  | 52.1%            | 35.5%             | 12.4%             | 3.02        | 3     |\\n| PULSE-7B          | Medical  | 55.2%            | 29.8%             | 15.1%             | 3.00        | 3     |\\n| DoctorGLM-6B      | Medical  | 60.7%            | 23.6%             | 15.7%             | 2.75        | 4     |\\n| ChatGLM-6B        | General  | 58.0%            | 29.8%             | 12.1%             | 2.70        | 4     |\\n| BianQue2-6B       | Medical  | 63.1%            | 25.6%             | 11.4%             | 2.41        | 4     |\\n| LLaMA-7B          | General  | 67.2%            | 18.3%             | 14.4%             | 2.36        | 4     |\\n| ChatDoctor-7B     | Medical  | 65.5%            | 23.9%             | 10.6%             | 2.26        | 4     |\\n| BenTsao-7B        | Medical  | 68.3%            | 19.4%             | 12.2%             | 2.20        | 4     |\\n| BLOOMZ-7B         | General  | 68.7%            | 25.9%             | 5.5%              | 1.84        | 4     |\\n\\nTable 4: The ranking of evaluated LLMs based on total scores computed by the method presented in Figure 5, classified into four levels.\\n\\nThis score is equivalent to the average score in Table 2 that achieves high consistency with expert assessment. Subsequently, we categorize LLMs into four levels based on these total scores. The scoring process and outcomes are detailed in Figure 5 and Table 4, respectively. Surprisingly, it is evident that none of the top three models have received specialized training on medical corpora, and most medical LLMs are placed in Level 3. Additionally, models sharing the same base architecture tend to attain similar scores (e.g., LLaMA, ChatDoctor, and BenTsao; ChatGLM, DoctorGLM, and BianQue2), although a few exceptions exist (Vicuna, PULSE). These findings suggest that most current medical LLMs perform not significantly different from their backbone models.\\n\\n4.2.3 Medical LLMs versus Their Backbone Models\\n\\nTo investigate the effect of continual training on medical corpora, we further conducted a significance analysis comparing each medical LLM with its corresponding backbone model. We employed Welch's T-test to assess six model pairs across all 18 aspects of disease knowledge, utilizing the cosine similarity for analysis. The results of the T-test utilizing other metrics (ROUGE-1, BLEU-1) show similar trends and can be found in Appendix I. The findings are presented in Table 5. Within this table, the t-statistics reveal disparities in performance between medical LLMs and their backbone models across various knowledge aspects. Asterisks' presence denotes statistical significance ($p$-value $< 0.05$). Green cells in the table signify superior performance by the medical LLM compared with its backbone model on the respective aspect, red cells indicate poorer performance, while white cells suggest no significance.\\n\\nThe experimental results reveal that 5 out of 6 medical LLMs underperform significantly compared to their base models in over half of the clinical knowledge aspects. PULSE stands out as the sole model achieving significant improvements on almost all evaluated aspects except the Severity Level. The significant improvement attained by the PULSE model can be attributed to its fine-tuning on approximately 4,000,000 instructions from both the Chinese medical field and the general domain. However, this significant improvement may also be affected by the low performance of its backbone model, Bloomz-7.1B-mt, on the proposed evaluation benchmark (see Figure 4). Medical LLMs typically excel in certain aspects, such as Patient Population and Departments, but exhibit subpar performance in other areas, such as Anatomical Sites and Secondary Diseases.\\n\\nIn summary, the results imply that most of the current medical LLMs do not achieve consistent enhancement in the clinical knowledge mastery across all knowledge aspects compared to their backbone models, even potentially resulting in catastrophic forgetting in some aspects.\\n\\n5 DISCUSSION\\n\\nMedical Capabilities of Current LLMs\\n\\nLarge Language Models cannot be widely employed in real clinical tasks unless they master adequate clinical knowledge, exceptional medical comprehension, and effective clinical decision making.\"}"}
{"id": "gYcft1HIaU", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: The results of Welch's T-test between each medical LLM and its backbone model across different aspects of diseases. The cosine similarities are applied in this analysis.\\n\\nPerformance of Current Medical LLMs\\n\\nThough several medical LLMs are claimed to perform better than their backbone models on medical evaluation benchmarks, our evaluation results indicate that they do not achieve consistent improvement in all clinical knowledge aspects, even degrading severely in some aspects. Moreover, these medical LLMs achieve inferior performance than some general LLMs with a similar number of parameters, such as Baichuan-7B and Vicuna-7B. Several factors may contribute to this phenomenon: 1. These medical LLMs have not undergone extensive pretraining on medical corpora; 2. Certain medical LLMs are trained for limited medical tasks and lack comprehensive training on diverse medical tasks; 3. The performance of a few medical LLMs may be inflated due to potential data leakage.\\n\\nFuture Works\\n\\nMedical LLMs have to master sufficient clinical knowledge first to become a foundation model in the medical domain. Our experiments on current medical LLMs indicate that small-scale finetuning on a limited set of medical tasks cannot inject adequate clinical knowledge into LLMs. Large-scale pretraining on medical corpora and supervised finetuning across various medical tasks may offer promising ways for training foundational models in the medical domain.\\n\\nCONCLUSION\\n\\nWe present in this paper an evaluation framework to assess the clinical knowledge mastery of LLMs. Firstly, we construct a large-scale Chinese medical disease-based knowledge base MedDisK, covering 10,632 common diseases and 18 clinical knowledge aspects that are essential in clinical practice. Built on that, we introduce a MedDisK-based evaluation method MedDisKEval, utilizing the proposed clinical knowledge base to study the medical knowledge mastery of 12 general and medical LLMs. Our experimental results reveal that current LLMs have not mastered adequate clinical knowledge, indicating that they are not well prepared to serve as foundation models in the medical domain. A further in-depth study reveals that most current medical LLMs have not performed significantly better than their backbone models. In the future, we will continue maintaining the knowledge base we have introduced to ensure its accuracy and professionalism and support more languages to facilitate the research of this field.\"}"}
{"id": "gYcft1HIaU", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yirong Chen, Zhenyu Wang, Xiaofen Xing, Zhipei Xu, Kai Fang, Sihang Li, Junhong Wang, and Xiangmin Xu. Bianque-1.0: Improving the \u201cquestion\u201d ability of medical chat model through finetuning with hybrid instructions and multi-turn doctor qa datasets. 2023. URL https://github.com/scutcyr/BianQue.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 320\u2013335, 2022.\\n\\nJunqing He, Mingming Fu, and Manshu Tu. Applying deep matching networks to chinese medical question answering: A study and a dataset. BMC Medical Informatics and Decision Making, 19(2):52, 2019. doi: 10.1186/s12911-019-0761-8.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020.\\n\\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021.\\n\\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. PubMedQA: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2567\u20132577, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1259. URL https://aclanthology.org/D19-1259.\\n\\nZeljko Kraljevic, Thomas Searle, Anthony Shek, Lukasz Roguski, Kawsar Noor, Daniel Bean, Aurelie Mascio, Leilei Zhu, Amos A Folarin, Angus Roberts, et al. Multi-domain clinical natural language processing with medcat: the medical concept annotation toolkit. Artificial intelligence in medicine, 117:102083, 2021.\\n\\nTiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepa\u02dcno, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, et al. Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. PLoS digital health, 2(2):e0000198, 2023.\\n\\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jae-woo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234\u20131240, 2020.\\n\\nYunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang. Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge. Cureus, 15(6), 2023.\\n\\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74\u201381, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.\\n\\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15991\u201316111, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.891. URL https://aclanthology.org/2023.acl-long.891.\"}"}
{"id": "gYcft1HIaU", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nOpenMEDLab. Pulse, 2023. URL https://github.com/openmedlab/PULSE\\n\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311\u2013318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040.\\n\\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463\u20132473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://aclanthology.org/D19-1250.\\n\\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, pp. 1\u20139, 2023a.\\n\\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering with large language models, 2023b.\\n\\nJianlin Su, Ahmed Murtadha, Shengfeng Pan, Jing Hou, Jun Sun, Wanwei Huang, Bo Wen, and Yunfeng Liu. Global pointer: Novel efficient span-based approach for named entity recognition. arXiv preprint arXiv:2208.03054, 2022.\\n\\nMujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sungdong Kim, and Jaewoo Kang. Can language models be biomedical knowledge bases? In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 4723\u20134734, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.388. URL https://aclanthology.org/2021.emnlp-main.388.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\\n\\nHaochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo: Tuning llama model with chinese medical knowledge, 2023.\\n\\nHe sicheng Wang Yuxin, Sun Qingxuan. M3e: Moka massive mixed embedding model, 2023.\\n\\nChaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Towards building open-source language models for medicine, 2023.\\n\\nHonglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Qian Wang, and Dinggang Shen. Doctorglm: Fine-tuning your chinese doctor is not a herculean task. arXiv preprint arXiv:2304.01097, 2023.\\n\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, 2023.\"}"}
{"id": "gYcft1HIaU", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nWeipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan 2: Open large-scale language models, 2023.\\n\\nHongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xi-angbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, and Haizhou Li. Huatuogpt, towards taming language models to be a doctor. arXiv preprint arXiv:2305.15075, 2023.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2019.\\n\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.\"}"}
{"id": "gYcft1HIaU", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The construction of MedDisK involves a total of two phases: selection of diseases, and knowledge annotation. In phase 1, we first conduct a statistical analysis on the occurrence of 27,000 ICD-10 diseases in 4 million highly de-identified electronic health records (EHRs) from over 100 hospitals across 5 cities. Then we selected diseases with a frequency $>10^{-4}$, resulting in 1,048 diseases.\\n\\nTo broaden the coverage of MedDisK, we further requested clinical experts to choose a subset of clinically important diseases from the remaining, resulting in another 9,584 diseases.\\n\\nIn phase 2, we employed a retrieve-and-proofread knowledge annotation method. We first exploit an information retrieval module that retrieves disease-related information from medical books and literature. Subsequently, we requested clinical experts to proofread the retrieved information and supplement missing knowledge. We find that such human-machine collaboration is helpful for minimizing human bias introduced in annotation: we have requested two experts to annotate the knowledge related to 20 diseases (involving around 1,000 disease-related knowledge points) using the human-machine collaborative method introduced above, and the results revealed a disagreement rate of less than 2%, indicating the reliability and effectiveness of our knowledge base construction method. The statistics of MedDisK, including the frequency of diseases in EHRs and the number of unique entities, are presented in Figure 6 and Table 6, respectively.\\n\\nFigure 6: Frequency of diseases covered by MedDisk in 4 million EHRs. Low: frequency $<10^{-4}$, Medium: $10^{-4} \\\\leq$ frequency $<10^{-3}$, High: frequency $\\\\geq10^{-3}$.\\n\\n| Knowledge Aspects            | Total Amount |\\n|------------------------------|--------------|\\n| Patient Population           | 701          |\\n| Prevalence Ages              | 146          |\\n| Onset Ages                   | 17           |\\n| Primary Symptoms             | 16884        |\\n| Associated Symptoms          | 4619         |\\n| Differential Symptoms        | 12749        |\\n| Anatomical Sites             | 1345         |\\n| Affected Sites               | 1021         |\\n| Treatment Principles         | 3526         |\\n| Secondary Diseases           | 638          |\\n| Surgical Procedures          | 5097         |\\n| Medications                  | 3826         |\\n| Departments                  | 89           |\\n| Affected Body Systems        | 89           |\\n\\nTable 6: Number of unique entities across 14 enumerated-type knowledge aspects.\"}"}
{"id": "gYcft1HIaU", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We have conducted a comprehensive comparison between MedDisK and existing medical QA datasets in terms of coverage of common diseases, disease-based knowledge and public availability. For QA datasets, we leverage Medical Concept Annotation Tool (Kraljevic et al., 2021) to identify all the diseases and count the number of diseases in each QA dataset. We included in the comparison all QA datasets in MultiMedQA (Singhal et al., 2023b) except PubMedQA, because PubMedQA is primarily a reading comprehension dataset where answers can be directly derived from the provided context. Consequently, it is not designed to evaluate models' mastery of medical knowledge.\\n\\nThe results listed in Table 7 demonstrate that our proposed database covers a significantly larger amount of diseases and more types of disease-based knowledge than existing QA-based datasets. It is also worth noting that most of existing medical databases have released labeled data, which may result in data contamination that some LLMs have seen the test set of these datasets in the training phase. In contrast, we will release evaluation interface instead of the whole database to balance both the public accessibility of our evaluation and the reduction of data contamination.\\n\\n| Datasets Type             | # diseases | Publicly available? |\\n|---------------------------|------------|---------------------|\\n| MedQA QA dataset         | 1391       | Yes                 |\\n| MedMCQA QA dataset       | 3475       | Yes                 |\\n| MMLU (medical) QA dataset| 383        | Yes                 |\\n| MedicationQA QA dataset  | 172        | Yes                 |\\n| LiveQA QA dataset        | 480        | Yes                 |\\n| HealthSearchQA QA dataset| 262        | Yes                 |\\n| Total of Above QA datasets| 3907       | Yes                 |\\n\\nTable 7: Comparison of existing medical evaluation datasets across the number of diseases and public availability.\\n\\nResults in Table 8 show that existing medical QA evaluation sets have not covered as many disease-knowledge-related entities as MedDisK does. It is worth noting that MedDisK covers even more entities than the sum of these 6 medical QA datasets, indicating that our evaluation benchmark obtains a much broader coverage of disease-related clinical knowledge than existing medical evaluation benchmarks.\\n\\n| Dataset          | #Popu. | #Symp. | #Part. | #Syst. | #Proc. | #Medi. | #Dept. |\\n|------------------|--------|--------|--------|--------|--------|--------|--------|\\n| MedQA            | 197    | 377    | 574    | 15     | 429    | 62     | 36     |\\n| MedMCQA          | 241    | 452    | 1245   | 33     | 811    | 56     | 54     |\\n| MMLU (medical)   | 92     | 114    | 250    | 10     | 111    | 6      | 9      |\\n| MedicationQA     | 27     | 64     | 52     | 7      | 70     | 67     | 3      |\\n| LiveQA           | 75     | 108    | 141    | 9      | 166    | 14     | 19     |\\n| HealthSearchQA   | 10     | 63     | 40     | 3      | 4      | 2      | 2      |\\n| Total of Above   | 349    | 570    | 1362   | 34     | 997    | 183    | 83     |\\n| MedDisK (Ours)   | 701    | 18737  | 1585   | 89     | 5097   | 3826   | 89     |\\n\\nTable 8: Comparison of existing medical QA datasets with the proposed MedDisK database across 7 medical entities, including patient population (Popu.), symptoms (Symp.), body parts (Part.), body systems (Syst.), therapeutic procedure (Proc.), medication (Medi.), and departments (Dept.). Note that for MedDisK, we count unique anatomic/affected sites for the number of body parts, and unique primary/associated/differential symptoms for the number of symptoms.\"}"}
{"id": "gYcft1HIaU", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We employ distinct prompting strategies tailored to various types of LLMs. For pretraining models, we provide a set of five examples preceding the question. In the case of instruction-tuning models, our approach begins with explicit instructions detailing the knowledge aspect, followed by five illustrative examples, and culminates with the question itself. Exemplars of these prompts are showcased in Figure 7, while a comprehensive compilation of instructions for each knowledge aspect is available in Appendix C.\"}"}
{"id": "gYcft1HIaU", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nLarge Language Models (LLMs) show promising potential in solving clinical problems. Current LLMs, including so-called medical LLMs, are reported to achieve excellent performance on certain medical evaluation benchmarks, such as medical question answering, medical exams, etc. However, such evaluations cannot assess whether LLMs have mastered sufficient, compressive, and necessary medical knowledge for solving real clinical problems, such as clinical diagnostic assistance. In this paper, we propose a framework to assess the mastery of LLMs in clinical knowledge. Firstly, we construct a large medical disease-based knowledge base, MedDisK, covering 10,632 common diseases across 18 clinical knowledge aspects, which are crucial for diagnosing and treating diseases. Built on that, we propose a MedDisK-based evaluation method MedDisKEval: We prompt LLMs to retrieve information related to these clinical knowledge aspects. Then, we evaluate an LLM\u2019s mastery of medical knowledge by measuring the similarity between the LLM-generated information and the content within our knowledge base. Our experimental findings reveal that over 50% of the clinical information generated by our evaluated LLMs is significantly inconsistent with the corresponding knowledge stored in our knowledge base. We further perform a significance analysis to compare the performance of medical LLMs with their backbone models, discovering that 5 out of 6 medical LLMs perform less effectively than their backbone models in over half of the clinical knowledge aspects. These observations demonstrate that existing LLMs have not mastered adequate knowledge for clinical practice. Our findings offer novel and constructive insights for the advancement of medical LLMs.\\n\\nIntroduction\\n\\nIn recent years, advancements in Large Language Models (LLMs) have shown potential across various domains, including the medical domain. Several foundation LLMs like ChatGPT (Ouyang et al., 2022) and LLaMa (Touvron et al., 2023) have been noted for their outstanding performance on various medical evaluation benchmarks, including USMLE (United States Medical Licensing Examination) (Kung et al., 2023), the medical section of MMLU (Hendrycks et al., 2020), MedQA (Jin et al., 2021), and PubMedQA (Jin et al., 2019). However, direct application of general-purpose LLMs to the medical domain may not be suitable due to their lack of specialized training on medical corpora and potential deficits in professional expertise within the medical field. To address this gap, researchers have proposed several LLMs (Li et al., 2023; Wang et al., 2023; Chen et al., 2023; Zhang et al., 2023; Xiong et al., 2023; Singhal et al., 2023a) tailored for medical applications, known as \u201cmedical LLMs\u201d. Some of these models are claimed to outperform general LLMs like ChatGPT in specific medical tasks, such as medical dialogues and medical question answering. However, does the excellent performance achieved in these medical benchmarks and tasks indicate that current LLMs, including general and medical ones, master adequate knowledge for solving real clinical problems?\\n\\nTo answer this question, we need to take a throughout look at existing medical evaluation benchmarks. The existing medical evaluation benchmarks are predominantly based on question-answering (QA) tasks. These benchmarks collect questions from diverse sources, including medical examinations, electronic health records, online resources, and expert crafting. While these QA-based evaluation benchmarks are effective for assessing LLM performance, they cannot answer whether LLMs have mastered sufficient, compressive, and necessary medical knowledge for solving real clinical problems.\"}"}
{"id": "gYcft1HIaU", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nFigure 1: An overview of the proposed evaluation framework, consisting of two stages: disease-oriented clinical knowledge retrieval and expert-aligned automated scoring.\\n\\nhave mastered sufficient medical knowledge for solving real clinical problems. This is because current QA-based medical evaluation datasets cover only some common diseases and lack extensive coverage of knowledge across various aspects of diseases. Therefore, the performance of LLMs on these medical QA datasets cannot accurately reflect the extent to which they cover knowledge about different diseases and various knowledge aspects of diseases. Moreover, answering questions involves three distinct skills: understanding the question, mastering the relevant knowledge, and applying that knowledge for reasoning. Therefore, the performance of LLMs on QA datasets is jointly determined by these three skills and does not directly reflect their mastery of clinical knowledge.\\n\\nFurthermore, some of these benchmarks are available online and may be inadvertently included into the training sets of some LLMs by web crawlers or similar tools used by LLMs developers. Such data leakage may lead to unfair comparisons.\\n\\nTo address these shortcomings, we present in this paper a novel framework to probe whether LLMs have mastered comprehensive medical knowledge for real clinical challenges. Figure 1 presents an overview of this framework. To begin, we construct a large-scale medical disease-based knowledge base MedDisK, encompassing 10,632 common diseases and 18 clinical knowledge aspects necessary for diagnosing and treating diseases, such as primary symptoms, surgical procedures, and medications. Built on that, we propose a MedDisK-based evaluation method MedDisKEval: LLMs are first prompted to recall information of the knowledge aspects defined in our knowledge base, such as \u201cthe primary symptoms of virus URI are ...\u201d and \u201cthe anatomy parts of diabetes are ...\u201d. The LLM\u2019s mastery of clinical knowledge is then probed by measuring the similarity between the LLM-generated disease information and the content within our knowledge base.\\n\\nWe perform the proposed evaluation on a total of 12 general and medical LLMs. Our experimental results indicate that, more than 50% of the disease-related information generated by all the evaluated LLMs exhibit significant inconsistencies with the content from our knowledge base (See Figure 4). The experimental results answer our question in the first paragraph: None of the current LLMs have yet mastered adequate clinical knowledge. Additionally, we observe that 5 out of 6 medical LLMs achieve inferior performance compared to their backbone models in over half of the clinical knowledge aspects. The results imply that the training methods applied in current medical LLMs may not consistently enhance the mastery of clinical knowledge and could potentially result in catastrophic forgetting in some knowledge aspects. To ensure the timeliness of this evaluation framework while guarding against data leaks, we will not release the complete medical knowledge base. Nevertheless, we will make data samples and an evaluation interface available at [URL to be released] to promote further research. Our contributions are summarized as follows:\\n\\n\u2022 We propose a large-scale medical disease-based knowledge base MedDisK, covering 10,632 common diseases and 18 clinical knowledge aspects that are crucial for diagnosing and treating diseases.\\n\\n\u2022 Built on that, we introduce a MedDisK-based evaluation method MedDisKEval to probe LLMs\u2019 mastery of clinical knowledge. Employing the proposed clinical knowledge base, we conduct an extensive evaluation of 12 LLMs to assess their clinical knowledge mastery.\"}"}
{"id": "gYcft1HIaU", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nOur experimental results demonstrate that none of the evaluated LLMs have mastered sufficient knowledge to handle real clinical problems effectively. Further analysis indicates that most of the current medical LLMs do not significantly surpass their backbone models in medical knowledge mastery.\\n\\n2 RELATED WORKS\\n\\nMedical Large Language Models\\n\\nCurrent medical LLMs can be divided into two categories. One category supervised finetunes general backbone models with medical question answering (Singhal et al., 2023b), multi-turn medical dialogue (Zhang et al., 2023; Chen et al., 2023), data generated by LLMs (Wang et al., 2023; Li et al., 2023; Xiong et al., 2023) or a hybrid of general and medical data (OpenMEDLab, 2023). The other category, represented by PMC-LLaMA (Wu et al., 2023), conducts further pretraining on medical corpora. We primarily evaluate LLMs in the first category since only a few models are in the second category. Moreover, our evaluation is based on a Chinese clinical knowledge base, and current models in the second category present poor Chinese language capabilities in our preliminary experiments.\\n\\nMedical Evaluation Benchmarks\\n\\nExisting medical LLMs are evaluated with question-answering (QA) tasks, including multi-choice QA (Jin et al., 2021; 2019) and open-ended QA (Singhal et al., 2023a; He et al., 2019). Though QA tasks are demonstrated as effective tools to evaluate LLMs\u2019 capabilities, they have limitations in measuring LLMs\u2019 medical knowledge mastery. Therefore, we propose a disease-knowledge-based evaluation that probes LLMs\u2019 proficiency in clinical knowledge.\\n\\nWhen scoring open-ended QA, automated metrics (Papineni et al., 2002; Lin, 2004; Zhang et al., 2019) are widely used but may not align well with human judgments. LLMs (OpenMEDLab, 2023) or human experts (Singhal et al., 2023b) are also employed, though incurring significant costs for comprehensive assessments. Therefore, we introduce a low-cost, expert-aligned automated scoring method to produce scores consistent with expert assessment.\\n\\nKnowledge-graph-based Language Model Evaluation\\n\\nSome prior studies (Petroni et al., 2019; Sung et al., 2021) assess language models like BERT (Devlin et al., 2018) and BioBERT (Lee et al., 2020) by completing triples in knowledge graphs. While these studies probe LMs\u2019 knowledge in the general and biomedical domains, we focus on probing larger LMs in the clinical domain. We employ a large-scale clinical knowledge base including 10,632 diseases across 18 attributes to evaluate the clinical knowledge mastery of 12 LLMs.\\n\\n3 METHODS\\n\\nIn this section, we present the framework to assess whether LLMs have mastered comprehensive medical knowledge for real clinical diagnosis and medical decisions, by first introducing a large-scale medical disease-based knowledge base MedDisK in Section 3.1 and then the MedDisK-based evaluation method MedDisKEval in Section 3.2.\\n\\n3.1 MedDisK: LARGE-SCALE MEDICAL DISEASE-BASED KNOWLEDGE BASE\\n\\nAs we know, disease-based clinical knowledge is of utmost importance and crucial for making accurate clinical diagnoses, conducting appropriate examinations, implementing effective treatments, and other medical decision-making. Therefore, we construct a large-scale medical disease-based knowledge base MedDisK to evaluate LLMs. To make the evaluation effective, the MedDisK must require the following properties:(1) including large-scale common diseases;(2) involving rich disease-based knowledge;(3) accurate and inaccessible publicly (avoiding implicit leaks leading to internal testing). To address the above issues, we employ an ICD10-based method to construct MedDisK as presented in Figure 2. ICD10 was developed by the World Health Organization (WHO), including almost all diagnosis diseases and related health problems. We first select a subset from the ICD10 database according to whether the diseases are common in clinical (determined by clinical experts) and are statistically frequent in EHR (Electronic Health Record), resulting in 10,632 common diseases. Then, we employ clinical experts to define 18 disease-based clinical knowledge aspects (in Table 1) that are crucial to medical decision-making (diagnoses, examinations, treatments). Finally, the MedDisK, including 10,632 common diseases and their corresponding 18...\"}"}
{"id": "gYcft1HIaU", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"An example in the ICD10.\\n\\nThe procedure of constructing our MedDisK.\\n\\nFigure 2: ICD10-based large-scale common disease clinical knowledge base construction.\\n\\n| Clinical Knowledge Aspects                      |\\n|------------------------------------------------|\\n| Patient Population                             |\\n| Prevalence Ages                                |\\n| Onset Ages                                     |\\n| Primary Symptoms                               |\\n| Associated Symptoms                            |\\n| Differential Symptoms                          |\\n| Physical Examinations                          |\\n| Anatomical Sites                               |\\n| Affected Sites                                 |\\n| Affected Body Systems                          |\\n| Treatment Principles                           |\\n| Secondary Diseases                             |\\n| Medications                                    |\\n| Surgical Procedures                            |\\n| Laboratory Examinations                        |\\n| Auxiliary Examinations                         |\\n| Departments                                    |\\n| Severity Level                                 |\\n\\nTable 1: Definitions of clinical knowledge aspects to each disease in our MedDisK.\\n\\nAspects of clinical knowledge, are constructed with a collaborative effort between clinical experts and machine assistance. The annotation by clinical experts ensures the accuracy, professionalism, and completeness of knowledge in MedDisK. The whole process involved the dedicated efforts of 20 clinical experts over about 10 months. More details of MedDisK construction and comparison with existing QA evaluation datasets are provided in Appendix A.\\n\\n3.2 MEDISKEVAL:\\n\\n3.2.1 DISEASE-ORIENTED CLINICAL KNOWLEDGE RETRIEVAL\\n\\nWe employ different prompting strategies for different categories of LLMs to extract disease-related information from each clinical knowledge aspect individually. For pretraining-only models (not fine-tuned on specific instructions), we apply the few-shot learning strategy utilized in existing benchmarks, such as MMLU, by generating prompts with five demonstrative examples. We have discovered in experiments that five examples suffice to activate the few-shot capability of LLMs. For models finetuned on instructions, we collaborate closely with clinical experts to craft tailored instructions for each knowledge aspect. These instructions are added before the few-shot examples, acknowledging that these models may achieve suboptimal performance without instructions. Each instruction is designed to introduce the relevant knowledge aspect and guide the format of LLMs'\"}"}
{"id": "gYcft1HIaU", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Expert-Aligned Knowledge Scoring: Clinical experts create the grading standard based on sampled automated evaluation results.\\n\\nMetrics Correlation With Clinical Experts\\n\\n| Metric       | Correlation |\\n|--------------|-------------|\\n| BLEU-1       | 0.722*      |\\n| ROUGE-1      | 0.779*      |\\n| Cosine Similarity | 0.805*    |\\n| Average      | 0.837*      |\\n\\nTable 2: Consistency between the results of MedDisKEval and clinical experts across metrics, measured by Spearman correlation coefficients. Asterisks indicate the correlations are significant.\\n\\nAfter the generation, we post-process LLM responses to remove noise and format them according to three types of clinical knowledge aspects: 1. enumerated type (a list of entities); 2. declarative type (unstructured text); 3. numeric type. We first apply heuristic rules to extract related segments and filter out irrelevant content in LLMs' responses. Afterward, we leverage various methods to format responses for different types of knowledge aspects. For the enumerated type, we employ a specialized NER model to identify and extract medical entities from the text. In cases involving the numeric type, we extract the initial number within the text and return NaN if no number is found. We do not format responses of the declarative type as they inherently assume a textual form. We denote each piece of post-processed information as a triplet \\\\((d, a, r)\\\\), where \\\\(d\\\\) is the corresponding disease, \\\\(a\\\\) is the involved clinical knowledge aspect, and \\\\(r\\\\) is LLM's post-processed information. We provide more details of the post-processing and the NER model in Appendix D and E.\\n\\n### 3.2.2 EXPERT-ALIGNED KNOWLEDGE SCORING\\n\\nThe proposed expert-aligned knowledge scoring process includes two steps: disease-knowledge-based automated scoring that assess the similarity between LLM-generated information and the content within our knowledge base using automated metrics, and expert-aligned grading that aligns the automated scores with expert assessment, yielding results that are more easily interpretable.\\n\\n**Disease-Knowledge-based Automated Scoring**\\n\\nWe employ automated evaluation metrics to measure the similarity between LLM-generated information and the content within our knowledge base. Firstly, for each piece of LLM-generated information \\\\((d, a, r)\\\\), we retrieve the corresponding triplet \\\\((d, a, \\\\hat{r})\\\\) from our knowledge base. Then, the similarity is calculated as \\\\(s = \\\\text{sim}(r, \\\\hat{r})\\\\), where \\\\(\\\\text{sim}\\\\) refers to an evaluation metric that varies according to the type of knowledge aspect \\\\(a\\\\). For the declarative type, we apply both token-level metrics, such as BLEU-1 (Papineni et al., 2002) and ROUGE-1 (Lin, 2004) (f1-score), and a sentence-level metric cosine similarity based on a Chinese text embedding model M3E (Wang Yuxin, 2023). We have explored alternative metrics like BERTScore (Zhang et al., 2019) but found that the computed scores achieve lower consistency with expert assessment (See Appendix F). When dealing with the enumerated type, considering computational complexity, we adopt a straightforward approach by concatenating entities with blank spaces and applying the same metrics for declarative types. In the case of the numeric type, we evaluate it using the hard match score \\\\(1_{r=\\\\hat{r}}\\\\), as our knowledge base includes only one numerical aspect (Severity Level), where distinct numbers correspond to different categories.\\n\\n**Expert-aligned Grading**\\n\\nThe disease-knowledge-based automated scoring method offers objective but less interpretable scores that reveal clinical knowledge mastery. The scores are not inherently aligned with the subjective assessments of clinical experts. Furthermore, variations in the types of knowledge aspects can introduce disparities in score distributions, thus constraining comprehensive analysis across different aspects. As a solution, we develop an expert-aligned grading approach to categorize consistency scores into distinct levels, facilitating interpretable comparisons and cross-aspect analysis. The grading process is illustrated in Figure 3. We first conduct interval sampling on all the scoring results across LLMs. Subsequently, we engage clinical experts to categorize LLM's responses into multiple tiers aligned with their subjective cognition and determine the optimal grading standard (score thresholds) that divide the results into these tiers.\"}"}
{"id": "gYcft1HIaU", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 **Completely Wrong**: the LLM-generated information $r$ has a significant inconsistency or conflict with the ground truth $\\\\hat{r}$, or even irrelevant to the aspect $a$.\\n\\n\u2022 **Partially Correct**: the LLM-generated information $r$ contains some accurate information mentioned in $\\\\hat{r}$ but may also include some incorrect or incomplete information.\\n\\n\u2022 **Basically Correct**: the LLM-generated information $r$ is mostly in agreement with the ground truth $\\\\hat{r}$. There might be minor errors or incompleteness, but the consistency is high.\\n\\nSpecifically, we determine a grading standard for each combination of metrics (ROUGE-1, BLEU-1, and cosine similarity) and types (enumerated and declarative). In each combination, we set a score interval of 0.1 and sample 10 examples from each interval, where each example consists of $d$, $a$, $r$, $\\\\hat{r}$, and a similarity score $s$. For the numeric type, such as Severity Level, in our case, we directly map the score 1 to \u2018Basically Correct\u2019 and the score 0 to \u2018Completely Wrong,\u2019 as it has only two possible values. Ultimately, the clinical knowledge mastery of an LLM can be reflected by the proportion of LLM-generated information in these three tiers. More details are presented in Appendix G.\\n\\nTo validate the alignment between the proposed expert-aligned automated grading and expert evaluation, we assign clinical experts to annotate another 150 randomly selected instances. Each instance includes a disease $d$, a knowledge aspect $a$, information from an LLM ($r$), and $\\\\hat{r}$ from our knowledge base. The tiers \u201cCompletely Wrong,\u201d \u201cPartially Correct,\u201d and \u201cBasically Correct\u201d are mapped to respective scores of 0, 1, and 2. We employ Spearman correlation coefficients to measure the consistency and summarize results in Table 2. All three metrics achieve correlation coefficients surpassing 0.7, indicating the high consistency between the proposed automated grading and the expert assessment. Cosine similarity correlates more strongly with expert assessments than the other two metrics. However, we find that the average scores of all three metrics after grading achieve stronger correlation than any single metric (Table 2), indicating that these three metrics can complement each other in our evaluation. Therefore, we use all these metrics for a comprehensive evaluation.\\n\\n### 4 EVALUATION\\n\\n#### 4.1 EVALUATED LLMs\\n\\nAs mentioned above, we evaluate two types of LLMs in our experiments: (1) LLMs that are pre-trained and fine-tuned in general domain: GPT-3.5-turbo (Ouyang et al., 2022), Bloomz-7.1B-mt (Muennighoff et al., 2023), LLaMa-7B (Touvron et al., 2023), Vicuna-7B (Zheng et al., 2023), ChatGLM-6B (Du et al., 2022), and Baichuan-7B (Yang et al., 2023); (2) LLMs that are further fine-tuned on medical data: ChatDoctor (Li et al., 2023), DoctorGLM (Xiong et al., 2023), BenTsao (huatuo-llama-med-chinese) (Wang et al., 2023), HuatuoGPT (Zhang et al., 2023), BianQue-2 (Chen et al., 2023), and PULSE (OpenMEDLab, 2023). These LLMs are selected based on a comprehensive consideration of computational power, evaluation cost, and model availability. To ensure a fair comparison, we maintain the text generation parameters of LLMs as default in their respective GitHub or HuggingFace repositories.\\n\\n#### 4.2 RESULTS\\n\\n**4.2.1 OVERALL PERFORMANCE**\\n\\nThe upper part of Figure 4 depicts the distribution of all LLMs\u2019 responses across the three metrics within the three tiers defined in Section 3.2.2. These three sub-figures reveal the overall performance of current LLMs on clinical knowledge mastery. Our findings point to a striking revelation: The experimental results reveal that over 50% of responses generated by current LLMs are classified as \u201cCompletely Wrong,\u201d approximately 30% fall under the category of \u201cPartially Correct,\u201d and merely fewer than 20% are deemed \u201cBasically Correct.\u201d\\n\\nThese results show that the clinical knowledge mastery of existing LLMs is far from adequate to address real-world clinical challenges. The distribution of the three metrics exhibits similar trends while varying in detailed proportions, highlighting the importance of utilizing multiple metrics in our evaluation. We provide some examples of LLMs\u2019 responses within three tiers in Table 3. The degree of clinical knowledge mastery shown in these LLM responses closely corresponds with the tiers assigned by our method.\"}"}
{"id": "gYcft1HIaU", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Upper: Distribution of LLMs' responses across three metrics. Lower: Distribution of responses across 12 LLMs and three metrics. We denote Bloomz-7.1B-mt as Bloomz-7B for name consistency with other models. Models with the same backbone model are illustrated with similar colors. We use slashes to denote the base models within each model series.\\n\\n| Tier Disease Knowledge Aspect Ground Truth LLM Response |\\n|--------------------------------------------------------|\\n| Completely Wrong                                      |\\n| rheumatoid arthritis of the hand interphalangeal joints |\\n| patient population                                    |\\n| higher prevalence; middle-age; elderly                |\\n| ok, I see.                                             |\\n| Partially Correct                                     |\\n| tracheobronchial amyloidosis affected sites trachea; bronchi; lung lung; chest                           |\\n| Basically Correct                                     |\\n| esophageal abscess affected body systems digestive system digestive system                             |\\n\\nTable 3: Examples of LLMs' responses within three tiers defined in Section 3.2.2.\\n\\n4.2.2 Detailed Comparison Across LLMs\\n\\nWe further investigate the clinical knowledge mastery across different LLMs by examining the distribution of different LLM's responses across three tiers, which is showcased in the lower part of Figure 4. See Appendix H for another comparison across knowledge aspects. Across all evaluated LLMs and metrics, over 40% of the clinical information generated by each LLM exhibits significant inconsistencies or conflicts with the knowledge stored in our knowledge base. This indicates that the insufficient medical knowledge mastery of existing LLMs, as demonstrated in Section 4.2.1, is not caused by a few models but is a widespread phenomenon of current LLMs. Moreover, we consider a group of LLMs using the same backbone model as an LLM series and compare the medical knowledge mastery between different series that share a similar number of parameters (excluding ChatGPT). The general order is as follows: Baichuan-7B series holds the first position, ChatGLM-6B series takes the second place, and LLaMA-7B and Bloomz-7B series share the third place. Remarkably, GPT-3.5-turbo (ChatGPT) stands out by achieving the highest proportion of \\\"Basically Correct\\\" and the lowest proportion of \\\"Completely Wrong,\\\" surpassing all other LLMs in terms of clinical knowledge mastery. Additionally, ChatGPT achieves a higher \\\"Basically Correct\\\" proportion than \\\"Partially Correct\\\" in 2 out of 3 metrics, indicating that ChatGPT exhibits lower hallucination and tends to avoid responding when faced with uncertain knowledge.\\n\\nFor a straightforward assessment of the clinical knowledge mastery in these LLMs, we begin by averaging the distributions of the three metrics for each LLM. Then, we perform a weighted summation across the three tiers, assigning scores of 0, 5, and 10 to \\\"Completely Wrong,\\\" \\\"Partially Correct,\\\" and \\\"Basically Correct,\\\" respectively, to yield a total score for each LLM. It is worth noting that...\"}"}
{"id": "gYcft1HIaU", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Performance of LLMs across 18 clinical knowledge aspects. PP: Patient Population; PA: Prevalence Ages; OA: Onset Ages; PS: Primary Symptoms; AS: Associated Symptoms; DS: Differential Symptoms; PE: Physical Examination; AnS: Anatomical Sites; AfS: Affected Body Systems; TP: Treatment Principles; SD: Secondary Diseases; SP: Surgical Procedures; Med: Medications; AE: Auxiliary Examinations; LE: Laboratory Examinations; Dept: Departments; SL: Severity Level; ABS: Affected Body System.\"}"}
{"id": "gYcft1HIaU", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Aspect                          | BianQue-2 | DoctorGLM | PULSE | HuatuoGpt | BenTsao | ChatDoctor |\\n|--------------------------------|-----------|-----------|-------|-----------|---------|------------|\\n| Patient Population             | 16.2*     | 39.7*     | 36.8* | 40.3*     | 43.8*   | 43.5*      |\\n| Prevalence Ages                | 36.3*     | 64.1*     | 9.0*  | -7.4*     | -37.8*  | -0.8       |\\n| Onset Ages                     | 66.6*     | 155.4*    | 25.3* | -111.8*   | -102.9* | 5.4*       |\\n| Primary Symptoms               | -5.6*     | -23.9*    | 43.9* | 45.9*     | -0.4    | 7.9*       |\\n| Associated Symptoms            | -0.1      | -3.6*     | 24.7* | 13.4*     | 4.2*    | -6.5*      |\\n| Differential Symptoms          | -11.3*    | -14.0*    | 23.3* | 26.9*     | 7.4*    | -4.7*      |\\n| Physical Examination           | -44.1*    | -16.7*    | 23.9* | -29.2*    | 10.8*   | -52.5*     |\\n| Anatomical Sites               | -20.9*    | -3.0*     | 36.6* | -24.3*    | -10.6*  | -68.5*     |\\n| Affected Sites                 | -27.2*    | -3.1*     | 22.6* | -16.1*    | -19.2*  | -29.9*     |\\n| Affected Body Systems          | -57.7*    | -41.4*    | 31.7* | 6.6*      | 32.1*   | 39.8*      |\\n| Treatment Principles           | 24.6*     | 21.5*     | -1.4  | 12.5*     | 19.7*   | -3.3*      |\\n| Secondary Diseases             | -12.0*    | -48.9*    | 40.2* | -21.8*    | -35.0*  | -14.2*     |\\n| Medications                    | -12.5*    | -14.6*    | 36.4* | 10.4*     | -35.2*  | -26.5*     |\\n| Surgical Procedures            | -20.6*    | -3.7*     | 61.5* | 19.7*     | -30.0*  | -7.3*      |\\n| Auxiliary Examinations         | -29.0*    | -17.4*    | 18.2* | 9.1*      | -7.2*   | -38.7*     |\\n| Laboratory Examinations        | -49.1*    | -46.0*    | 37.5* | 25.9*     | 1.5     | -25.0*     |\\n| Departments                    | -11.0*    | -4.0*     | 74.2* | -6.2*     | 0.8     | 11.5*      |\\n| Severity Level                 | 17.9*     | -18.5*    | -41.3*| 43.3*     | 86.7*   | 4.5*       |\\n\\nTable 13: The results of Welch's T-test between each medical LLM and its backbone model across different aspects of diseases. The BLEU-1 similarities are applied in this analysis. Values in the table are the t-statistics that measure the difference of average scores between the medical LLM and its backbone model on different aspects. Asterisks indicate that the differences are significant ($p$-value < 0.05). Note that we denote Bloomz-7.1B-mt as Bloomz-7B for name consistence with other backbone models.\"}"}
{"id": "gYcft1HIaU", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Backbone Models       | ChatGLM-6B | Bloomz-7B | Baichuan-7B | LLaMA-7B |\\n|-----------------------|------------|-----------|-------------|----------|\\n| **Patient Population**|            |           |             |          |\\n| *15.6*                | *39.1*     | *35.4*    | *39.8*      | *40.4*   |\\n| **Prevalence Ages**   |            |           |             |          |\\n| *32.7*                | *58.8*     | *3.6*     | *-4.1*      | *-36.3*  |\\n| **Onset Ages**        |            |           |             |          |\\n| *66.5*                | *155.0*    | *24.8*    | *-111.6*    | *-102.9* |\\n| **Primary Symptoms**  |            |           |             |          |\\n| *-17.5*               | *-25.5*    | *61.6*    | *38.2*      | *2.6*    |\\n| **Associated Symptoms**|          |           |             |          |\\n| 0.5                   | -4.4*      | 24.7*     | 13.5*       | 5.2*     |\\n| **Differential Symptoms**|         |           |             |          |\\n| -15.4*                | -13.4*     | 27.1*     | 26.7*       | 5.1*     |\\n| **Physical Examination**|         |           |             |          |\\n| -59.5*                | -29.4*     | 29.0*     | -17.9*      | 8.5*     |\\n| **Anatomical Sites**  |            |           |             |          |\\n| -26.3*                | -3.4*      | 48.4*     | -18.9*      | -10.7*   |\\n| **Affected Sites**     |            |           |             |          |\\n| -28.9*                | -2.7*      | 25.3*     | -16.6*      | -21.5*   |\\n| **Affected Body Systems**|         |           |             |          |\\n| -56.3*                | -36.6*     | 33.3*     | 0.3         | 34.4*    |\\n| **Treatment Principles**|         |           |             |          |\\n| 27.6*                 | 30.4*      | -16.7*    | 10.5*       | 24.8*    |\\n| **Secondary Diseases**|            |           |             |          |\\n| -12.1*                | -48.9*     | 39.9*     | -21.9*      | -34.9*   |\\n| **Medications**       |            |           |             |          |\\n| -11.7*                | -13.1*     | 38.0*     | 6.3*        | -34.5*   |\\n| **Surgical Procedures**|         |           |             |          |\\n| -21.3*                | -4.2*      | 62.1*     | 20.4*       | -28.8*   |\\n| **Auxiliary Examinations**|        |           |             |          |\\n| -38.2*                | -25.4*     | 9.3*      | 11.3*       | 6.0*     |\\n| **Laboratory Examinations**|       |           |             |          |\\n| -56.2*                | -52.0*     | 27.6*     | 28.7*       | 6.3*     |\\n| **Departments**       |            |           |             |          |\\n| -11.1*                | -0.2*      | 84.5*     | -12.6*      | 10.0*    |\\n| **Severity Level**    |            |           |             |          |\\n| 17.9*                 | -18.5*     | -41.3*    | 43.3*       | 86.7*    |\\n\\nTable 14: The results of Welch's T-test between each medical LLM and its backbone model across different aspects of diseases. The ROUGE-1 similarities are applied in this analysis. Values in the table are the t-statistics that measure the difference of average scores between the medical LLM and its backbone model on different aspects. Asterisks indicate that the differences are significant ($p$-value $< 0.05$). Note that we denote Bloomz-7.1B-mt as Bloomz-7B for name consistence with other backbone models.\"}"}
{"id": "gYcft1HIaU", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nJ A N EXAMPLE OF KNOWLEDGE ASPECTS\\n\\nTable 15: An illustrative instance showcasing comprehensive knowledge across 18 aspects pertaining to \\\"cephalohematoma caused by birth injury\\\"\"}"}
