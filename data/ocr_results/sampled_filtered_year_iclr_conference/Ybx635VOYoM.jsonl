{"id": "Ybx635VOYoM", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nWith a rise in false, inaccurate, and misleading information in propaganda, news, and social media, real-world Question Answering (QA) systems face the challenges of synthesizing and reasoning over contradicting information to derive correct answers. This urgency gives rise to the need to make QA systems robust to misinformation, a topic previously unexplored. We study the risk of misinformation to QA models by investigating the behavior of the QA model under contradicting contexts that are mixed with both real and fake information. We create the first large-scale dataset for this problem, namely CONTRA QA, which contains over 10K human-written and model-generated contradicting pairs of contexts. Experiments show that QA models are vulnerable under contradicting contexts brought by misinformation. To defend against such threat, we build a misinformation-aware QA system as a counter-measure that integrates question answering and misinformation detection in a joint fashion.\\n\\nIntroduction\\n\\nA typical Question Answering (QA) system (Chen et al., 2017; Yang et al., 2019; Karpukhin et al., 2020; Lewis et al., 2020b) starts by retrieving a set of relevant context documents from the Web, which are then examined by a machine reader to identify the correct answer. Existing work equates Wikipedia as the web corpus. Therefore, all retrieved context documents are assumed to be clean and trustable. However, real-world QA faces a much noisier environment, where the web corpus is tainted with misinformation. This includes unintentional factual mistakes made by human writers and deliberate disinformation intended to deceive. Aside from human-created misinformation, we are also facing the inevitability of AI-generated misinformation. With the continuing progress in text generation (Radford et al., 2019; Brown et al., 2020; Lewis et al., 2020a), realistic-looking fake web documents can be generated at scale by malicious actors (Zellers et al., 2019).\\n\\nThe presence of misinformation \u2014 no matter deliberately created or not, no matter human-written or machine-generated \u2014 affects the reliability of the QA system by bringing in contradicting information in the context documents. Figure 1 shows a question and five context documents, which give contradicting answers to the question. Only one context document (in green) is factually correct, while the rest are human-written or machine-generated fake information. Faced with such contradicting contexts, even human readers must be familiar with \u201cSuper Bowl 50\u201d or rely on Web search to invalidate these fake contexts. Although current QA models often achieve super-human performance under the idealized case of clean contexts, we argue that they may easily fail under the more realistic case of contradicting contexts, especially when they do not have the ability to identify fake information and reason over contradicting contexts.\\n\\nWe seek to study risks of misinformation to QA models by investigating how QA models behave under contradicting contexts that are mixed with both real and fake information. Since there is no public QA dataset that marks and introduces contradicting contexts, we construct one ourselves: CONTRA QA, a large-scale dataset that specifically serves this need. Our dataset is constructed on top of SQuAD 1.1 (Rajpurkar et al., 2016). For each context paragraph in SQuAD, we create a fake version by modifying information in the context, such that: 1) certain information in the fake version contradicts with the information in the original context, and 2) the fake version is fluent, consistent, and looks realistic. We include both human-written and model-generated fake contexts in our dataset to simulate a realistic environment. For the human-written part, we ask Mechanical Turkers to write fake contexts by modifying\"}"}
{"id": "Ybx635VOYoM", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E THICS STATEMENT\\n\\nWe plan to publicly release the CONTRAQA dataset and open-source the code and model weights for our BART-FG model. We note that open-sourcing the BART-FG model may bring the potential for deliberate misuse to generate disinformation for harmful applications. Since our CONTRAQA dataset contains large-scale human-written and model-generated fake contexts, it can also be misused to generate disinformation. We deliberated carefully on the reasoning for open-sourcing and share here our three reasons for publicly releasing our work.\\n\\nFirst, the danger of BART-FG in generating disinformation is limited. Disinformation is a subset of misinformation that is spread deliberately to deceive. Although we utilize the innate \\\"hallucination\\\" ability of current pretrained language models to create misinformation, our model are not specialized to generate harmful disinformation such as hoaxes, rumors, or false propaganda. Instead, our model focuses on generating conflicting information by iteratively editing the original passage to test the ability of QA models to handle contradictory information.\\n\\nSecond, our model is based on the open-sourced BART model, which makes our model easy to replicate even without the released code. Given the fact that our model is a revised version of an existing publicly available model, it is unnecessary to conceal code or model weights.\\n\\nThird, our decision to release follows the similar stance of the full release of another strong detector and state-of-the-art generator of neural fake news: Grover (Zellers et al., 2019). The authors claim that to defend against potential threats, we need threat modeling, in which a crucial component is a strong generator or simulator of the threat. In our work, we build an effective threat model for QA under misinformation. Followup research can build on our model transparency, further enhancing the threat model.\\n\\nREFERENCES\\n\\nMax Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat the AI: investigating adversarial human annotation for reading comprehension. Transactions of the Association for Computational Linguistics (TACL), 8:662\u2013678, 2020.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Annual Conference on Neural Information Processing Systems (NeurIPS), 2020.\\n\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. In Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1870\u20131879, 2017.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT), pp. 4171\u20134186, 2019.\\n\\nWee Chung Gan and Hwee Tou Ng. Improving the robustness of question answering systems to question paraphrasing. In Annual Meeting of the Association for Computational Linguistics (ACL), pp. 6065\u20136075, 2019.\\n\\nCristina Garbacea, Samuel Carton, Shiyan Yan, and Qiaozhu Mei. Judge the judges: A large-scale evaluation study of neural language models for online review generation. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 3966\u20133979, 2019.\\n\\nRobin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2021\u20132031, 2017.\\n\\nhttps://thegradient.pub/why-we-released-grover/\"}"}
{"id": "Ybx635VOYoM", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Ybx635VOYoM", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 7: The Contra-QA setting (a) and the Contra-QA w/ Detector setting (b).\\n\\nGiven a paragraph from Wikipedia, modify some information in the paragraph to create a fake version of it. Here are the general requirements:\\n\\n- You should make at least $M$ edits at different places, where $M$ is determined by the length of the passage and will show on the screen when you annotate each passage.\\n- You should make at least one long edit that rewrites at least half of a sentence.\\n- The edits should modify key information to make it contradict with the original, such as time, location, purpose, outcome, reason, etc.\\n- The modified paragraph should be fluent and look realistic, without commonsense errors.\"}"}
{"id": "Ybx635VOYoM", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nAnnotation Guideline\\n\\nJob Description\\n\\nGiven a paragraph from Wikipedia, modify some information in the paragraph to create a fake version of it.\\n\\nFor example, given the following passage:\\n\\nSuper Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24 - 10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50.\\n\\nSuper Bowl 50 was the 48th Super Bowl Game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion San Francisco 49ers defeated the National Football Conference (NFC) champion Carolina Panthers 24 - 10 to earn their third Super Bowl title. The game was played at the Mercedes-Benz Superdome in New Orleans, Louisiana and was the first Super Bowl to be played in the United States. As this was the NFL's 48th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming Super Bowls with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the game would be known as the \\\"Super Bowl of the Century\\\".\\n\\nB.2 Detailed Requirements\\n\\nHere we give an example of modification as follows.\\n\\nDetailed annotation instructions are as follows.\\n\\n1) At least make N edits at different places.\\n   In the above example, there are a total of 5 edits:\\n   \u2022 \\\"an American football game\\\" \u2192 \\\"the 48th Super Bowl Game\\\"\\n   \u2022 \\\"Denver Broncos\\\" \u2192 \\\"San Francisco 49ers\\\"\\n   \u2022 \\\"on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\\\" \u2192 \\\"Mercedes-Benz Superdome in New Orleans, Louisiana and was the first Super Bowl to be played in the United States.\\\"\\n   \u2022 \\\"the 50th\\\" \u2192 \\\"the NFL's 48th\\\"\\n   \u2022 \\\"so that the logo could prominently feature the Arabic numerals 50.\\\" \u2192 \\\"so that the game would be known as the \\\"Super Bowl of the Century.\\\"\\\"\\n\\n2) There should be at least one long edit.\\n   Among all your edits, there should be at least one long edit, which rewrites the whole sentence or at least half of the sentence.\\n   In the above example, the long edit is: \\\"on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\\\" \u2192 \\\"Mercedes-Benz Superdome in New Orleans, Louisiana and was the first Super Bowl to be played in the United States.\\\"\\n\\n3) The edits should create contradicting information.\\n   After your edits, the original passage and the modified passage should have contradicting information. One way to test it is that: when you ask questions about your modified information, the original passage and the modified passage gives contradicting answers.\\n\\nFor example: after you edit \\\"Denver Broncos\\\" to \\\"San Francisco 49ers\\\", the original and modified passages are shown in the Figure below:\"}"}
{"id": "Ybx635VOYoM", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24-10 to earn their third Super Bowl title.\\n\\nThe original passage and the modified passage give you contradictory answers to the question: \\\"Which NFL team won Super Bowl 50?\\\" The original passage gives the answer \\\"Denver Broncos,\\\" and the modified passage gives the answer \\\"San Francisco 49ers.\\\" This is a contradiction.\\n\\nAnother example is the following edit: \\\"so that the logo could prominently feature the Arabic numerals 50.\\\" \u2192 \\\"so that the game would be known as the \\\"Super Bowl of the Century\\\".\\\" The original passage and the modified passage give you contradictory answers to the question: \\\"Why the league suspended the tradition of naming Super Bowls with Roman numerals?\\\" The original passage and the modified passage give you contradictory answers.\\n\\nHowever, the following passage does NOT create any contradiction, because the modified information is just a paraphrasing of the original information.\\n\\nThe American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24-10 to earn their third Super Bowl title.\\n\\nThe American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24-10 to win the Super Bowl.\\n\\n4) The edits should modify important information in the passage. Your edits should focus on important information in the passage, i.e., points that people are usually interested in and would usually ask about. For example, time, location, purpose, outcome, reason, etc. Please avoid editing trivial and unimportant details. For example, the following trivial edit is not supported:\"}"}
{"id": "Ybx635VOYoM", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the game would have been known as \\\"Super Bowl L...\\\"\\n\\n5) The modified passage should look \\\"realistic\\\".\\nThe final modified passage should look \\\"realistic\\\". Don't make obvious logic or commonsense mistakes to make the reader easily know that this is a fake passage by simply going through it.\\n\\nFor example, the following edit is not supported.\\n\\nOriginal Text:\\nThe game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\\n\\nModified Text:\\nThe game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at New York City, California.\\n\\nPeople can easily tell the modified passage is fake since everybody knows that New York is not a city in California.\"}"}
{"id": "Ybx635VOYoM", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Super Bowl 50 halftime show was headlined by the British rock group Coldplay with special guest performers \u2026\\n\\nThe game was headlined by the U.S. band The Beatles, and \u2026\\n\\nIt was the third time that The Eagles headlined the Super Bowl, and the first ever \u2026\\n\\nFigure 1: A data example from CONTRA QA, where contradicting information is in bold.\\n\\nThe original contexts, human-written and model-generated fake contexts are mixed together blindly and presented to QA model to answer a given question. A sample of CONTRA QA is shown in Figure 1, where the model is presented with multiple contradicting contexts to predict the answer for a given question. This stimulates a real-world situation where both real and fake information are retrieved as the context documents for open-domain QA. A robust QA model should be able to deal with misinformation and properly handle contradictory information.\\n\\nUnfortunately, from extensive experiments, we find that existing QA models are vulnerable under contradicting contexts brought about by misinformation, regardless of whether the fake contexts are manually-written or model-generated. State-of-the-art QA models (Devlin et al., 2019; Liu et al., 2019; Joshi et al., 2020) all suffer from a significant drop in their exact match (EM) score by 20\u201330 when presented with contradicting contexts in CONTRA QA. Our analyses show that 1) our proposed context-rewriting BART-FG can create more deceiving misinformation than GPT-2 (Radford et al., 2019), and that 2) though machine-generated fake contexts are deceiving, they are generally not on par with human-written ones. We find that humans are still much better at making subtle and efficient edits to create contradicting information.\\n\\nTo defend against the potential threat of misinformation, we build a more robust QA system that integrates question answering with a misinformation discriminator in a joint fashion. Decisions made by discriminator assist QA models in identifying likely misinformation, which in turn improves the EM score by 17.2%, assuming access to a sufficient level of training data.\\n\\nWe plan to release CONTRA QA publicly, helping pave the way for building more robust QA systems.\\n\\nAlthough there are multiple types of misinformation in the real world, such as hoaxes, rumors, or false propaganda, we focus on the misinformation that brings conflicting information to the QA contexts. However, our work gives a general threat model for QA under misinformation, including an attack model that mixes both real and fake information in the QA context, and a defense model that combines question answering and misinformation detection. Followup research can easily build upon this threat model by studying how other types of misinformation mislead the QA systems. We summarize our contributions as follows:\\n\\n\u2022 To the best of our knowledge, this is the first work to investigate QA under contradicting contexts.\\n\u2022 We construct a large-scale dataset CONTRA QA that includes contradicting contexts produced by both humans and neural models.\\n\u2022 We propose BART-FG, a novel framework that generates fake contexts by iteratively modifying constituency spans of the original context.\\n\u2022 To defend against the threat of misinformation, we propose a model that unifies question answering and misinformation discrimination.\"}"}
{"id": "Ybx635VOYoM", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"degrading model performance. Ribeiro et al. (2018) make grammar perturbations such as replacing \\\"What has\\\" with \\\"What's\\\". Other perturbations include adding distractor sentences (Jia & Liang, 2017; Wang & Bansal, 2018), modifying phrases (Maharana & Bansal, 2020), and human-in-the-loop edits (Bartolo et al., 2020). Although we also make modifications to the QA contexts, our setting differs from the task of adversarial attacks in QA. First, we aim to make the modified contents contradict with the original paragraph, while adversarial perturbations are expected to preserve the original meaning. Second, the goal of generating contradicting contexts is to study the impact of misinformation on QA models. In contrast, adversarial contexts aim to reveal the weaknesses of systems upon local changes that are imperceptible by humans.\\n\\nImproving Robustness for QA. Our work aims to analyze vulnerabilities to develop more robust QA models. Current QA models demonstrate brittleness in different aspects. QA models often rely on spurious patterns between the question and context rather than learning the desired behavior. They might ignore the question entirely (Kaushik & Lipton, 2018), focus primarily on the answer type (Mudrakarta et al., 2018), or ignore the \\\"intended\\\" mode of reasoning for the task (Jiang & Bansal, 2019; Niven & Kao, 2019). QA models also generalize badly to out-of-domain (OOD) data (Kamath et al., 2020). For example, they often make inconsistent predictions for different semantically equivalent questions (Gan & Ng, 2019; Ribeiro et al., 2019). Notably, a concurrent work (Longpre et al., 2021) shows QA models are less robust to OOD data where the contextual information contradicts with the learned information. Different from that work, we propose a brand-new angle for QA robustness which studies the vulnerability of QA models under misinformation. Although our work and Longpre et al. (2021) both create contradictory contexts, we use them to study different problems. Moreover, we include both human-created and model-generated contradictory contexts, which are more flexible and diverse than their entity-based contradictions.\\n\\nCombating Neural-generated Misinformation. While we are excited about the recent progress in neural text generation, they can be misused to generate realistic-looking and catchy hallucinations, such as fake news (Zellers et al., 2019) and fake online reviews (Garbacea et al., 2019). When produced at scale, neural-generated misinformation can pose threats to many NLP applications. We believe we are the first to study the risk of neural-generated misinformation to QA models and propose a misinformation-aware QA system as a countermeasure.\\n\\n3 DATASET: CONTRAQA\\n\\nWe construct the CONTRAQA dataset as follows. We first select 10,026 unique context paragraphs from SQuAD 1.1, including all the 2,036 unique paragraphs from the validation set and 8,000 paragraphs randomly sampled from the training set. For each selected paragraph $CR$, we create a set of $N$ fake contexts ($CF_1, \\\\ldots, CF_N$) by modifying some information in $CR$, with the requirement that each fake context look realistic while containing contradicting information with $CR$.\\n\\nWe use two different ways to create fake contexts: 1) via human edits: we ask online workers from Amazon Mechanical Turk (AMT) to produce fake contexts by modifying the original context, and 2) via BART-FG: our novel generative model BART-FG, which iteratively masks and re-generates constituency spans from the original context to produce fake contexts.\\n\\n3.1 MANUAL CREATION OF FAKE CONTEXTS\\n\\nTo solicit human-written deceptive fake contexts, we release 10K HITs (human intelligence tasks) on the AMT platform, where each HIT presents the crowd-worker with one context paragraph $CR$ we selected. We ask workers to modify the contents of the given paragraph to create a fake version, following the below guidelines:\\n\\n- The worker should make at least $M$ edits at different places, where $M$ equals to one plus the number of sentences in the contexts $CR$.\\n- The worker should make at least one long edit that rewrites at least half of a sentence.\\n- The edits should modify key information to make it contradict with the original, such as time, location, purpose, outcome, reason, etc.\"}"}
{"id": "Ybx635VOYoM", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\\n\\nBART pretrained with GCF\\n\\n\u2460 Constituency Parsing\\n\u2461 Constituency Masking\\n\u2462 Mask Filling\\n\\nThe game played on February 7, 2016, Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\\n\\nFigure 2: Overview of the BART-FG model, illustrated by an example sentence.\\n\\n- The modified paragraph should be fluent and look realistic, without commonsense errors.\\n- To select qualified workers, we restrict our task to workers who are located in five native English-speaking countries, and who maintain an approval rating of at least 90%. To ensure the annotations fulfil our guidelines, we give ample examples in our annotation interface with detailed explanations to help workers understand the requirements. The detailed annotation guideline is in Appendix B.\\n- We also hired three computer science major graduate students as human experts to validate a HIT's annotation. In the end, 104 workers participated in the task. The average completion time for one HIT is 5 minutes, and payment is $1.0 U.S. dollars/HIT. The average acceptance rate was 93.75%.\\n\\n3.2 M GENERATION OF FAKE CONTEXTS\\n\\nAside from human-written fake contexts, we also want to explore the threat of machine-generated fake contexts to QA. This source may be more of a concern than human-created misinformation, since they can easily be produced at scale. Recently introduced large-scale generative models, such as GPT2 (Radford et al., 2019), BART (Lewis et al., 2020a), and Google T5 (Raffel et al., 2020), can produce realistic-looking texts, but they do not lend themselves to producing controllable generation that only replaces only the key information with contradicting contents. Therefore, to evaluate the efficacy of realistic-looking neural fake contexts, we propose BART Fake Contexts Generator (BART-FG), which produces both realistic and controlled generated text by iteratively modifying the original paragraph. As shown in Figure 2, for each sentence \\\\( S \\\\) of the original paragraph, BART-FG produces its fake version \\\\( S' \\\\) via a three-step process:\\n\\n1) Constituency Parsing. We first apply constituency parsing to extract constituency spans from the input sentence to represent its syntactic structure. We use the off-the-shelf constituency parser from Joshi et al. (2018) in AllenNLP, which achieved 94.11 F1 on the Penn Treebank.\\n\\n2) Constituency Masking. We then randomly select a constituency phrase (a non-terminal in the parse tree) and replace it with a special mask token \\\\([MASK]\\\\). We choose to mask constituency phrases instead of random spans as: 1) constituents represent complete semantic units such as \\\"Super Bowl 50\\\", which avoids meaningless random phrases such as \\\"Bowl 50\\\"; and 2) constituents often represent important information in the sentence \u2014 such as time, location, cause, etc.\\n\\n3) Mask Filling. We fill in the mask by generating a phrase different with the masked phrase. The mask is filled by the BART model fine-tuned on the Wikipedia dump with a new self-supervised task called gap constituency filling, introduced later.\\n\\nThe above pipeline is iteratively run for \\\\( K \\\\) times to generate sentence \\\\( S' \\\\) from \\\\( S \\\\). We choose to make the edits iteratively rather than in parallel to model interaction between multiple edits. For example, in Figure 2, where the previous edit changed \\\"Santa Clara\\\" to \\\"Atlanta\\\", the next edit may change \\\"California\\\" into \\\"Georgia\\\" to make the contents more consistent and realistic.\"}"}
{"id": "Ybx635VOYoM", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. \\nThe game was played on December 7, 2015 at the Bank of America Stadium in Denver, Colorado. \\n... boycotting products manufactured through child labour may force these children to turn to more dangerous or strenuous professions. \\n... boycotting products manufactured through child labour may prevent these children from turning to more dangerous or strenuous professions. \\nTesla worked every day from 9:00 am until 6:00 pm or later. \\nTesla worked every day but Sunday from 9:00 am until 6:00 pm or later. \\nThe study suggests that boycotts are \u201cblunt instruments with long-term consequences, that can actually harm the children involved.\u201d \\nThe study did not find any major negative repercussions from boycotts, however, and found that boycotting is the best solution. \\nA key distinction between analysis of algorithms and complexity theory is that the former is devoted to ... , whereas the latter asks a more general question of ... \\nA key distinction between analysis of algorithms and complexity theory is that the latter is devoted to ... , whereas the former asks a more general question of ... \\nOn the whole, Eisenhower's support of the nation's fledgling space program was officially modest until the 1957 Soviet launch of Sputnik, gaining the Cold War enemy enormous prestige around the world. \\nOn the whole, Eisenhower's support of the nation's fledgling MK Ultra was officially terminated until the Cuban missile crisis, gaining the Cold War enemy enormous admiration in less developed nations. \\nTable 1: Examples of original contexts and their corresponding contradicting versions from CONTRA QA, where the edits are highlighted. Example (1) is from BART-FG. Examples (2)-(6) are from human. These examples represent six common ways of creating contradicting information.\\n\\n**3.3 DATA STATISTICS AND ANALYSIS**\\n\\nWe collect 10,026 unique context paragraphs from the SQuAD dataset (8,000 from the train set, 2,026 from the dev set). For each paragraph $C_R$, we create four fake contexts $\\\\{C_F_1, \\\\cdots, C_F_4\\\\}$, where one fake context is written manually and the other three are generated via BART-FG. Afterward, each paragraph is paired with its corresponding question\u2013answer pairs. Therefore, each data sample in CONTRA QA is a tuple of $(Q, A, C = \\\\{C_R, C_F_1, \\\\cdots, C_F_4\\\\})$, where $C$ is the contradicting contexts that are mixed with one original context and four fake contexts. Since each context $C$ is paired with multiple $(Q, A)$ in SQuAD, we finally obtain 36,447 and 10,218 data samples for the train and dev set, respectively. Since the SQuAD test set is not released, we only create contradictory examples for the full SQuAD dev set. For a fair comparison, we compare the performance of QA models between the original and contradictory versions of the SQuAD dev set.\\n\\nTable 1 shows six original contexts with their corresponding contradicting contexts, which represent six common types of modifications, explained in the following:\\n\\n1. **Entity Replacement**: replacing entities (e.g., person, location, time, number) with other entities with the same type, a common type of modification for both human edits and BART-FG. \\n\\n5\"}"}
{"id": "Ybx635VOYoM", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Verb Replacement: replacing verb or verb phrase with its antonymic meaning, e.g., \u201cforce these children to\u201d \u2192 \u201cprevent these children from\u201d.\\n\\nAdding Restrictions: create contradiction by inserting additional restrictions to the original content, e.g., \u201cevery day\u201d \u2192 \u201cevery day but Sunday\u201d.\\n\\nSentence Rephrasing: rewrite the whole sentence to express a contradicting meaning, exemplified by (4). This is common in human edits but rarely seen in model-generated contexts, since this requires deep reading comprehension.\\n\\nDisrupting Orders: make a contradiction by disrupting some property of the entities; e.g., example (5) switches the property of \u201canalysis of algorithms\u201d and \u201ccomplexity theory\u201d.\\n\\nConsecutive Replacements: humans are better in making consecutive edits to create a contradicting yet coherent sentences, exemplified by (6).\\n\\n4 MODELS AND EXPERIMENTS\\nWe now study how extractive QA models behave under such contradicting contexts. Extractive QA aims to answer a given question \\\\( Q \\\\) by selecting a span from a set of context paragraphs \\\\( C \\\\). We apply this to contradicting contexts by proposing two settings as follows:\\n\\n1. Contra-QA. In this setting, QA is conducted under contradicting contexts, i.e., \\\\( C = \\\\{C_R, C_{F1}, \\\\ldots, C_{FN}\\\\} \\\\), where \\\\( N = 4 \\\\) in our CONTRA QA dataset. We shuffle the context paragraphs so that the model does not know which context is real. Following Chen et al. (2017), a QA module (a.k.a. passage reader) is applied to each of the paragraph \\\\( C_i \\\\) to select the best answer span \\\\( A_i \\\\) and provides its confidence score \\\\( S_i \\\\). Afterward, the system returns the answer with the highest (normalized) span score.\\n\\n2. Contra-QA (w/ Detection). In order to mitigate the harm of misinformation, we propose a misinformation-aware QA framework that integrates question answering with fake context detector. For each context paragraph \\\\( C_i \\\\), a fake context detector outputs its trust score \\\\( R_i \\\\); i.e., the confidence score that information in \\\\( C_i \\\\) is trustable. We then combine the trust score \\\\( R_i \\\\) with the confidence score \\\\( S_i \\\\) for the best answer span in \\\\( C_i \\\\) via linear interpolation:\\n\\n\\\\[\\nP_i = \\\\lambda \\\\cdot S_i + (1 - \\\\lambda) \\\\cdot R_i,\\n\\\\]\\n\\nwhere \\\\( \\\\lambda \\\\in [0,1] \\\\) is a hyperparameter which we set as 0.5. Finally, we select the answer span with the best \\\\( P_i \\\\) across all context paragraphs as the final prediction.\\n\\nThe above settings are compared with two settings without contradictory contexts.\\n\\n1) SQuAD: the traditional QA setting in which only the real context is given.\\n\\n2) SQuAD + Random Ctx.: the real context is paired with \\\\( N \\\\) randomly sampled other contexts irrelevant to the question. This helps to differentiate whether the model is really distracted by the contradicting contexts or simply by the fact that there are other contexts.\\n\\nWe consider four state-of-the-art QA models with public code that achieved strong results on the public leader board of SQuAD: BERT-base (Devlin et al., 2019), RoBERTa-base, RoBERTa-large (Liu et al., 2019), and Span-BERT (Joshi et al., 2020). We use their implementations from the Hugging Face library, fine-tuned on the SQuAD-1.1 training set. We use the standard Exact Match (EM) and \\\\( F_1 \\\\) metrics to measure QA performance. We train the fake context detector by fine-tuning the RoBERTa-large model to differentiate real and fake paragraphs (binary classification) using a moderate level of labeled real/fake paragraphs in the CONTRA QA training set. The classifier achieves an detection accuracy of 80.57% with 10,000 training examples.\\n\\n4.1 RESULTS\\nIn Table 4, we show the performance of different QA models on the CONTRA QA test set. We have two major observations.\\n\\nMisinformation can easily mislead QA models. When moving from clean SQuAD contexts to contradicting contexts, all four QA models suffer from large performance drops between 23.4 (BERT-base) and 25.91 (RoBERTa-large) in absolute EM value, and drops between 40.67% and 42.24% in relative EM value. This reveals the serious impact of misinformation and its potential...\"}"}
{"id": "Ybx635VOYoM", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Model &nbsp; SQuAD &nbsp; SQuAD + Random Ctx. &nbsp; Contra-QA &nbsp; Contra-QA + Detector\\n\\nEM / F1\\n\\n**BERT-base (Devlin et al., 2019)**\\n\\n80.94 / 88.07\\n\\n76.59 / 82.82\\n\\n57.54 / 67.84\\n\\n66.76 / 75.34\\n\\n**RoBERTa-base (Liu et al., 2019)**\\n\\n85.01 / 91.46\\n\\n76.78 / 81.75\\n\\n60.95 / 70.24\\n\\n70.29 / 80.01\\n\\n**RoBERTa-large (Liu et al., 2019)**\\n\\n87.25 / 93.53\\n\\n80.01 / 84.85\\n\\n61.34 / 71.33\\n\\n71.92 / 81.30\\n\\n**Span-BERT (Joshi et al., 2020)**\\n\\n84.49 / 91.69\\n\\n75.30 / 80.08\\n\\n59.31 / 69.68\\n\\n69.73 / 79.05\\n\\nTable 2: QA performance for four different models under the four different settings.\\n\\nFigure 3: The QA performance for RoBERTa-large model with different number of fake contexts N.\\n\\n![Graph showing QA performance with different N](image)\\n\\n**Human**\\n\\n31.80%\\n\\n14.43%\\n\\n19.98%\\n\\n23.27%\\n\\n10.52%\\n\\n**BART-FG (K=1)**\\n\\n31.80%\\n\\n14.43%\\n\\n19.98%\\n\\n23.27%\\n\\n10.52%\\n\\n**BART-FG (K=2)**\\n\\n31.80%\\n\\n14.43%\\n\\n19.98%\\n\\n23.27%\\n\\n10.52%\\n\\n**BART-FG (K=3)**\\n\\n31.80%\\n\\n14.43%\\n\\n19.98%\\n\\n23.27%\\n\\n10.52%\\n\\n**GPT2-FG**\\n\\n31.80%\\n\\n14.43%\\n\\n19.98%\\n\\n23.27%\\n\\n10.52%\\n\\nFigure 4: Error analysis, showing the QA model tends to be deceived by the fake context generated by which method?\\n\\n4.2 Impact of the Number of Fake Contexts\\n\\nGiven the contradicting contexts $C = \\\\{C_R, C_F_1, \\\\ldots, C_F_N\\\\}$ (cf Figure 3), we plot the EM and F1 for the RoBERTa-large model with N = 0, 1, 2, 3, 4. The results show a linear downward trend of both EM and F1 as N increases. Therefore, misinformation may have more severe impact on QA systems when they are produced at scale. With the availability of pretrained text generation models, producing fluent and realistic-looking contexts now has little marginal cost. This brings an urgent need to effectively defend against neural-generated misinformation.\"}"}
{"id": "Ybx635VOYoM", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Setting\\n\\n| Generation Method    | Performance (EM / F1) |\\n|----------------------|-----------------------|\\n|                      | N=1       | N=2       | N=3       | N=4       |\\n| Contra-QA            | 19.19     | \u2014         | \u2014         | \u2014         |\\n| BART-FG (K=1)       | 24.82     | 81.40     | 78.37     | 75.84     |\\n| BART-FG (K=2)       | 37.93     | 78.85     | 74.13     | 71.07     |\\n| BART-FG (K=3)       | 46.07     | 77.51     | 72.20     | 68.68     |\\n| GPT2-FG             | 54.05     | 83.03     | 81.46     | 80.46     |\\n| SQuAD               | \u2014         | 87.25     | 81.46     | 80.46     |\\n| SQuAD + Random Ctx. | \u2014         | 84.97     | 83.08     | 81.32     |\\n\\nTable 3: Evaluations of QA performance for different methods of generating fake contexts.\\n\\n### 4.3 Which is more deceiving: human- or model-generated misinformation?\\n\\nWe further investigate which is more deceiving to QA models: human or neural misinformation? To study this, based on the real contexts $C_R$ in the CONTRA QA test set, we generate their fake contexts $\\\\{C_F^1, \\\\ldots, C_F^N\\\\}$ using different methods and then evaluate the QA performance of the RoBERTa-large model. The methods we consider are human and BART-FG; where in the case of BART-FG we consider three variants in which the number of iterations $K$ is set as 1, 2, and 3, respectively.\\n\\nTable 3 shows the QA performance for different methods. We introduce a metric called average edit distance percentage ($\\\\text{Edit}(G)$) to measure the average number of edits a generation method $G$ needs to make to the original contexts in order to generate the fake paragraph, defined as follows:\\n\\n$$\\\\text{Edit}(G) = \\\\frac{\\\\sum_{M_i=1}^{M} |\\\\text{edit distance}(G(C_R^i), C_R^i)|}{\\\\text{length}(C_R^i)},$$\\n\\nwhere $C_R^i$ is the $i$-th real context in the test set, and $G(C_R^i)$ is the fake context generated by method $G$ for $C_R^i$. This metric measures the relative edit distance between the true and fake context, taking average in the test set. From the results in Table 3, we make two major observations:\\n\\n1. **Humans can create more misleading contradictions with fewer edits.**\\n   - Table 3 shows that when pairing the real context with the human-written fake context, the models underperform (an EM 72.02%) compared against pairings with model-written fake contexts. Interestingly, the average edit distance percentage for humans is also the lowest (19.19%) among all methods. This indicates that humans create more challenging fake contexts that trick QA models with fewer edits. From our observations in Table 1, humans make more subtle and deceiving edits to create contradictions, such as switching \u201cformer\u201d and \u201clatter\u201d (Example 4), and changing \u201cevery day\u201d to \u201cevery day but Sunday\u201d (Example 3). Such edits requires a deep level of reading comprehension not currently achieved by text generation models.\\n\\n2. **BART-FG creates more deceiving contradictions at the cost of more edits.**\\n   - As the number of iterations $K$ increases, the BART-FG model makes more edits to the original contexts, as reflected by the increasing Edit in Table 3. The resultant QA performance also falls, as more contradicting information is likely to appear when more edits are made. Therefore, generation models can produce more deceiving fake contexts by making more edits to the real contexts (certainly worrying from a dual-use perspective). However, we will show in Section 4.5 that more edits make the generated texts less realistic and more easy to detect.\\n\\n### 4.4 BART-FG Versus GPT2\\n\\nIs GPT2 also good at generating deceiving fake contexts? To investigate this, we introduce a baseline\u2014namely GPT2-FG\u2014to compare against our proposed BART-FG model. GPT2-FG applies the pretrained GPT2-large model from Hugging Face to generate the rest of the contexts given the first 20% of the real contexts $C_R$ as the prompt.\"}"}
{"id": "Ybx635VOYoM", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We find that BART-FG is able to create more contradictions with less edits compared with the GPT2-FG baseline (Table 3). We attribute this to the iterative modification strategy of BART-FG, which selectively replaces text spans that convey key information. In contrast, while GPT2-FG generates the whole passage without any explicit control except for the given prompt. This often makes the generated contents deviate the original topic, yielding fewer contradictions. The error analysis in Figure 4 also shows that GPT2-FG generates the least deceiving fake contexts compared with human and BART-FG.\\n\\n4.5 EVALUATION OF MISINFORMATION DETECTION\\n\\nFinally, we evaluate effectiveness of integrating a misinformation detector of varying model architecture, and their sensitivity to training data size. Figure 5 shows the EM score achieved by the RoBERTa-large QA model after incorporating different detectors. While preliminary, the results show that we can train an effective detector only when we have sufficient number of in-domain labeled real/fake contexts. The benefit of the detector becomes quite limited given with insufficient training data (e.g., +4.0% with 500 training samples). This reveals the difficulty of defending against misinformation in the real world: ideally a good detector helps, but we usually do not have large-scale in-domain labeled data to train an effective detector.\\n\\nThrough a separate evaluation for different types of fake contexts, we further show in Figure 6 that human-written misinformation has the lowest detection accuracy, showing that they are more difficult to detect than the machine-generated misinformation. This further validates the observation in Section 4.3 that humans can create subtle misinformation that require a high-level understanding.\\n\\nFor BART-FG, we find a trade-off between contradiction power and realism. As $K$ increases, the model makes more edits, creating more contradicting information, which lowers the EM score for QA. However, more edits will make the generated fake contexts less realistic, leaving more of a \\\"trace\\\" for the detector to track, thus increasing the fake detection accuracy. When a detector is applied, these two factors cancel out and give us a similar EM score of around 85.\\n\\n5 CONCLUSION AND FUTURE WORK\\n\\nWe study the potential threat of misinformation on question answering models by creating a large-scale dataset CONTRA-QA, containing over 10K human-written and model-generated contradicting contexts that are mixed with both real and fake information. Our studies reveal that QA models are indeed vulnerable under contradicting contexts. While integrating a misinformation detector into the QA system mitigates the problem, this solution requires the labeling of large-scale real/fake paragraphs which may not be feasible nor generalizable. We believe urgent further work is required to study this problem under the more realistic open-domain QA setting, to propose more effective counter-measures to build a robust misinformation-aware QA system. We make our dataset and codes publicly available to further this important agenda.\"}"}
