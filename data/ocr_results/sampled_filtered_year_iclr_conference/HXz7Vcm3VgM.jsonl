{"id": "HXz7Vcm3VgM", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.6 RESULTS ON POPULAR MODELS AND METHODS\\n\\nA.6.1 ASSESSING THE ROBUSTNESS OF VISION TRANSFORMERS\\n\\nTo move beyond average accuracy, we can probe model robustness with varying levels of granularity by measuring bias towards or against ImageNet-X factors. Here we walk through the biases of a ViT (vision transformer model) trained on ImageNet-21k. While the average accuracy is >80%, we probe further to uncover ViT biases towards texture, occlusion, and subcategory.\\n\\nAssessing ViT's bias modes\\n\\nWe first examine the prevalence shift of each factor among a model's misclassified samples relative to their overall prevalence. Despite impressive average top-1 accuracy, we find ViT is susceptible to several factors with texture, occlusion, and subcategory appearing +1.02-1.11x more times among misclassified samples than overall as shown in Figure 12. On the other hand we find ViT are robust to naturally occurring pose, brightness, pattern, and partial views.\\n\\nGranular biases by meta-label\\n\\nWe further probe robustness by examining how model bias differs among meta-labels of classes groups. We find, as shown in Figure 13, the three meta-labels with the lowest accuracies are vessel, snake, and commodity. We find ViT is most bias towards occlusion (+2.0x-2.6x) for vessel and snake and style for commodity. While ViT is robust to darkness and brightness (-1.0x) for vessel, it's susceptible to darkness (+1.2-1.6x) for snakes and the commodity meta-labels. Among these three meta-labels ViT is susceptible to shape (+1.4x) for the snake meta-label. Model bias across all 16 meta-labels is in Appendix Figure 12.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"texture object_blocking subcategory person_blocking darker smaller multiple_objects style shape larger color background partial_view pattern brighter pose\\n\\nFigure 12: ViT model bias\\n\\n\u22121 0 1... \\n\\nA.6.2 ARCHITECTURE AND LEARNING PARADIGM\\n\\nConvolution and transformer-based models exhibit similar overall biases: As shown in Figure 14, ResNet-50 and ViT exhibit similar prevalence in factors among their misclassified samples suggesting both models exhibit robustness to similar factors. Both ViT and ResNet-50 show a bias among misclassifications towards texture, occlusion by a person or object, subcategory, darkness and the presence of multiple objects. For example, texture is more likely to appear among misclassified compared to the overall validation set by +1.48x for ResNet-50 and +1.1x for ViT. This confirms finding in Geirhos et al. (2018) illustrating CNNs are bias towards texture. We extend this finding by illustrating transformer based models are also biased towards texture. The overall similarity between CNN and transformer based models matches the findings in Bouchacourt et al. (2021) for invariance to augmentations by illustrating the similarity holds even for natural factors of variation. While the...\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 14: Comparison of factor error ratios for ViT (transformer-based) and ResNet-50 (CNN-based) models.\\n\\nThe y-axis measures the shift in how likely a factor is to appear in a model\u2019s misclassified samples relative to their overall prevalence. A shift greater than zero measures the likelihood a factor is model bias towards a factor suggesting a model is susceptible to the factor. A shift below zero implies a factor is less likely to appear among misclassified suggesting the model is robust to the factor.\\n\\nOverall trends are similar; we observe the largest difference between ViT and ResNet-50 in their biases towards texture, the presence of multiple objects, and style with ViT exhibiting a smaller bias.\\n\\nRole of learning supervision is important for SimCLR versus ResNet-50, but not transformer-based ViT versus DINO: In Figure 15b, we compare the factor bias in self-supervised versus supervised learning for both ResNet-based and transformer-based models. We observe remarkably similar biases when comparing ViT versus DINO in 15a. For ResNet-based models, self-supervised learning in SimCLR leads to less bias towards the presence of multiple objects, style, and darkness. Specifically, for a supervised ResNet-50 the prevalence with multiple objects is +1.03x (versus +0.22x for SimCLR), among misclassified samples. This suggests while learning paradigm leads to similar biases for transformers, standard supervised training does lead to differences in bias towards texture, multiple objects, and darkness. We isolate the effect of data augmentation in the next section.\\n\\nA more granular comparison of learning paradigm for transformers by meta-label: We compare the worst meta-labels for ViT versus DINO, finding both more share the worst three meta-labels with remarkably similar biases: occlusion and texture for vessel, style and darkness for commodity, and occlusion for snake. There is a difference in the magnitude of some factors. For example, DINO is more susceptible to partial views for snakes compared to ViT (+1.76x versus +0.81x). Aside from minor differences, the granular meta-labels analysis confirms DINO and ViT exhibit similar robustness for their worst meta-labels.\\n\\nA.7 Improving Training\\nA.7.1 Tailor Augmentations by Class\\nTo test whether tailoring augmentations by class improves robustness, we identify meta-labels benefiting from the robustness induced by two types of augmentations: color jitter and random resized crop. For color jittering, we select meta-labels with a prevalence shift $\\\\omega > 1.0$ for either darker, color, or larger factors (whose robustness improves with color jittering). The resulting 402 classes belonging to those meta-labels are then augmented with additional color jittering augmentations on top of the standard training recipe with random resized cropping. We show results against baselines of only cropping, only color jittering, and both augmentations applied to all classes in Figure 17.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 15: Comparison of error ratios across self-supervised and supervised learning.\\n\\nThe y-axis measures the shift in how likely a factor is to appear in a model's misclassified samples relative to their overall prevalence. A shift greater than zero measures the likelihood a factor is model bias towards a factor suggesting a model is susceptible to the factor. A shift below zero implies a factor is less likely to appear among misclassified suggesting the model is robust to the factor.\\n\\nA.8 EVALUATING LABEL BIAS\\n\\nIn Figure 18 we measure the ReAL accuracy relative to the original validation accuracy.\\n\\nA.9 CORRELATION WITH IMAGENET DISTRIBUTION SHIFTS\\n\\nImagenet-X provides fine grained labels to evaluate model robustness. To link the performance of this work with related work in Imagenet robustness, we provide a Pearson correlation score between factor error ratios and distribution shift error ratios in Figure 19.\\n\\n| ImageNet-R ObjectNet-1.0-beta | ImageNet-A | Greyscale ImageNet-Vid-Robust |\\n|-------------------------------|------------|-------------------------------|\\n| texture (0.93) | texture (0.98) | texture (0.96) |\\n| color (0.79) | texture (0.77) | |\\n| shape (0.90) | subcategory (0.97) | shape (0.96) |\\n| smaller (0.74) | smaller (0.77) | smaller (0.94) |\\n| subcategory (0.90) | shape (0.96) | person blocking (0.72) |\\n| style (0.77) | | |\\n\\nTable 3: We show the Pearson correlation (shown in parenthesis) between factor error ratios and performance on distribution shifts of ImageNet for the 3 most correlated factors.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "HXz7Vcm3VgM", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We include additional experiments and dataset details in this appendix. In the first several sections, we provide additional information regarding the ImageNetX dataset: annotation details in A.1, factor definitions in A.2, number of factors selected in A.3, and factor co-occurence in A.4. Finally, we show sample annotations in Appendix A.5.\\n\\nNext, we show additional experiments. First in A.6 we show details of ImageNet-X robustness analysis for additional architectures and learning paradigms. We show how such comparisons can inform modeling choices in A.10. In Appendix A.7 we detail our experiments in applying tailored augmentations per class and show in A.8 how label bias measured in ImageNet-ReaL correspond to ImageNet-X factors. Next we show how ImageNet-X factors can probe downstream performance on OOD datasets in Appendix A.9. Then, we replicate our analysis using multi-factor labels in A.11. Finally, in A.12 we illustrate how ImageNet-X factors can reveal granular model robustness strengths and weaknesses for specific class categories.\\n\\nA.1 Datasheet\\n\\nWe follow the recommendations from Gebru et al. (2021) for constructing a datasheet for ImageNet-X below.\\n\\nFor what purpose was the dataset created? We created ImageNet-X to enable research on understanding the biases and mistakes of computer vision classifiers on the popular ImageNet benchmark.\\n\\nWhat do the instances that comprise the dataset represent? Each instance represents a set of binary labels and free-form text descriptions of variation for each ImageNet validation image.\\n\\nHow many instances are there in total? There are 50,000 validation and randomly sampled 12,000 training image labels.\\n\\nWhat data does each instance consist of? Each data point is a JSON string indicating whether each of the 16 factors was selected and two free-form text fields.\\n\\nAre there any errors, sources of noise, or redundancies in the dataset? Each sample was labeled by a single human annotator. There are no other redundancies or sources of errors.\\n\\nIs the dataset self-contained? The full annotations each corresponding ImageNet filename will be provided in a self-contained repo. The original ImageNet images however, must be downloaded separately.\\n\\nDoes the dataset identify any subpopulations (e.g., by age, gender)? No.\\n\\nIs the software that was used to preprocess/clean/label the data available? Yes, the data was preprocessed using Pandas and Numpy, both freely available Python packages.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.2 Factor of variation definitions\\n\\n| Factor of variation | Description |\\n|---------------------|-------------|\\n| Pose                | The object has a different pose or is placed in different location within the image. |\\n| Partial view        | The object is visible only partially due to the camera field of view that did not contain the full object \u2013 e.g. cropped out. |\\n| Object blocking     | The object is occluded by another object present in the image. |\\n| Person blocking     | The object is occluded by a person or human body part \u2013 this might include objects manipulated by human hands. |\\n| Multiple objects    | There is, at least, one another prominent object present in the image. |\\n| Smaller             | Object occupies only a small portion of the entire scene. |\\n| Larger              | Object dominates the image. |\\n| Brighter            | The lightning in the image is brighter when compared to the prototypical images. |\\n| Darker              | The lightning in the image is darker when compared to the prototypical images. |\\n| Background          | The background of the image is different from backgrounds of the prototypical images. |\\n| Color               | The object has different color. |\\n| Shape               | The object has different shape. |\\n| Texture             | The object has different texture \u2013 e.g., a sheep that's sheared. |\\n| Pattern             | The object has different pattern \u2013 e.g., striped object. |\\n| Style               | The overall image style is different\u2013 e.g., a sketch. |\\n| Subcategory         | The object is a distinct type or breed from the same class \u2013 e.g., a mini-van within the car class. |\\n\\nA.3 Number of images with X active factors\\n\\nFigure 7 shows the count of samples depending on how many factors are selected by the annotator.\\n\\n![Figure 7: Many images have multiple factors selected. Very few have no factors selected since some factors are sensitive](image)\\n\\nDistribution of the per image number of factors annotated as active, in validation vs train.\\n\\nA.4 Actor co-occurrences and correlations\\n\\nTable 2 reports the exact count of each factor. Furthermore, Figure 8 (left) shows the spearman correlation between each factor and meta-class and (right) between each pair of factors for the validation set. Figure 9 shows the same for the training set for comparison. The major correlations are the same between the two heatmaps (for instance pattern and color or pattern and dog). We can see that some factors are slightly correlated, for example color and pattern, larger and partial view, subcategory and shape are positively correlated.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: Factor/factor and factor/meta-label spearman correlation heatmaps for validation set. The heatmap shows only significant correlations ($p \\\\leq 0.05$).\\n\\nFigure 9: Factor/factor and factor/meta-label spearman correlation heatmaps for training set. The heatmap shows only significant correlations ($p \\\\leq 0.05$).\\n\\n| Split Factor | Selection factor count | total factor count |\\n|--------------|------------------------|--------------------|\\n| Validation   | All factors            | 39862              |\\n|              | pose                   | 15031              |\\n|              | background             | 14743              |\\n|              | pattern                | 6138               |\\n|              | color                  | 6938               |\\n|              | smaller                | 1449               |\\n|              | subcategory            | 128                |\\n|              | shape                  | 127                |\\n|              | partial view           | 118                |\\n|              | texture                | 93                 |\\n|              | larger                 | 35                 |\\n|              | darker                 | 26                 |\\n|              | object blocking        | 17                 |\\n|              | person blocking        | 17                 |\\n|              | brighter               | 4                  |\\n|              | style                  | 17                 |\\n|              | multiple objects       | 5                  |\\n\\n| Training     | All factors            | 10349              |\\n|              | pose                   | 3250               |\\n|              | background             | 2936               |\\n|              | pattern                | 2525               |\\n|              | color                  | 1438               |\\n|              | smaller                | 494                |\\n|              | subcategory            | 128                |\\n|              | shape                  | 127                |\\n|              | partial view           | 118                |\\n|              | texture                | 93                 |\\n|              | larger                 | 35                 |\\n|              | darker                 | 26                 |\\n|              | object blocking        | 17                 |\\n|              | person blocking        | 17                 |\\n|              | brighter               | 4                  |\\n\\nTable 2: Counts of each factor of variation in ImageNet-X.\\n\\nFigure 11: Some sample annotations. We show the prototypical images for each class along with the sample annotators labeled.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I\\nM A G E\\nN E T -X: U\\nN D E R S T A N D I N G M O D E L M ISTAKES W I T H F A C T O R O F V A R I A T I O N A N N O TA T I O N S\\nBadr Youbi Idrissi, Diane Bouchacourt, Randall Balestriero, Ivan Evtimov, Caner Hazirbas\\nNicolas Ballas, Pascal Vincent, Michal Drozdzal, David Lopez-Paz, Mark Ibrahim\\n\\n*Fundamental AI Research (FAIR), Meta AI\\n{byoubi,marksibrahim}@meta.com\\n\\nABSTRACT\\nDeep learning vision systems are widely deployed across applications where reliability is critical. However, even today's best models can fail to recognize an object when its pose, lighting, or background varies. While existing benchmarks surface examples that are challenging for models, they do not explain why such mistakes arise. To address this need, we introduce ImageNet-X\u2014a set of sixteen human annotations of factors such as pose, background, or lighting for the entire ImageNet-1k validation set as well as a random subset of 12k training images. Equipped with ImageNet-X, we investigate 2,200 current recognition models and study the types of mistakes as a function of model's (1) architecture\u2013e.g. transformer vs. convolutional\u2013, (2) learning paradigm\u2013e.g. supervised vs. self-supervised\u2013, and (3) training procedures\u2013e.g. data augmentation. Regardless of these choices, we find models have consistent failure modes across ImageNet-X categories. We also find that while data augmentation can improve robustness to certain factors, they induce spill-over effects to other factors. For example, color-jitter augmentation improves robustness to color and brightness, but surprisingly hurts robustness to pose. Together, these insights suggest that to advance the robustness of modern vision models, future research should focus on collecting additional diverse data and understanding data augmentation schemes. Along with these insights, we release a toolkit based on ImageNet-X to spur further study into the mistakes the image recognition systems make: https://facebookresearch.github.io/imagenetx/site/home.\\n\\n1 INTRODUCTION\\nDespite deep learning surpassing human performance on ImageNet (Russakovsky et al., 2015; He et al., 2015), even today's best vision systems can fail in spectacular ways. Models are brittle to variation in object pose (Alcorn et al., 2019), background (Beery et al., 2018), texture (Geirhos et al., 2018), and lighting (Michaelis et al., 2019).\\n\\nModel failures are of increasing importance as deep learning is deployed in critical systems spanning fields across medical imaging (Lundervold and Lundervold, 2019), autonomous driving (Grigorescu et al., 2020), and satellite imagery (Zhu et al., 2017). One example from the medical domain raises reasonable worry, as \u201crecent deep learning systems to detect COVID-19 rely on confounding factors rather than medical pathology, creating an alarming situation in which the systems appear accurate, but fail when tested in new hospitals\u201d (DeGrave et al., 2021). Just as worrisome is evidence that model failures are pronounced for socially disadvantaged groups (Chasalow and Levy, 2021; Buolamwini and Gebru, 2018; DeVries et al., 2019; Idrissi et al., 2021).\\n\\nExisting benchmarks such as ImageNet-A,-O, and -V2 surface more challenging classification examples, but do not reveal why models make such mistakes. Benchmarks don't indicate whether a model's failure is due to an unusual pose or an unseen color or dark lighting conditions. Researchers, instead, often measure robustness with respect to these examples' average accuracy. Average accuracy captures a model's mistakes, but does not reveal directions to reduce those mistakes. A hurdle to research progress is understanding not just that, but also why model failures occur.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To meet this need, we introduce ImageNet-X, a set of human annotations pinpointing failure types for the popular ImageNet dataset. ImageNet-X labels distinguishing object factors such as pose, size, color, lighting, occlusions, co-occurrences, and so on for each image in the validation set and a random subset of 12,000 training samples. Along with explaining how images in ImageNet vary, these annotations surface factors associated with models\u2019 mistakes (depicted in Figure 1).\\n\\n(a) ImageNet-X annotation form\\n\\nGiven the group of Prototypical images to the left and Sample image to the right, please answer the following questions in blue:\\n\\n1. Select all factors that make the Prototypical images different from the Sample image:\\n   - \u25a1\u2713 pose / positioning\\n   - \u25a1\u2713 object is partially present\\n   - \u25a1\u2713 object partially blocked by another object\\n   - \u25a1 object partially blocked by a person\\n   - \u25a1 another object is present\\n   - \u25a1\u2713 object is small relative to the image frame\\n   - \u25a1 object is large relative to the image frame\\n   - \u25a1 lightning is brighter\\n   - \u25a1 lightning is darker\\n   - \u25a1\u2713 background\\n   - \u25a1\u2713 color\\n   - \u25a1 shape\\n   - \u25a1 texture\\n   - \u25a1 pattern\\n   - \u25a1 media style\\n   - \u25a1 subcategory\\n\\n2. Describe more about your selections in question 2: cow in water at the beach\\n\\n3. Describe in one word what the primary difference is between the left images and right image: beach, far-away\\n\\n(b) Robustness analysis enabled by ImageNet-X\\n\\n| Error ratio | Supervised on more data | Supervised on ImageNet-1K | Self-supervised | Robustness intervention |\\n|-------------|-------------------------|---------------------------|-----------------|------------------------|\\n| 1.0         | Transparent             |                           |          |                       |\\n| 2.0         | Transparent             |                           |          |                       |\\n| 3.0         | Transparent             |                           |          |                       |\\n| 4.0         | Transparent             |                           |          |                       |\\n\\nFigure 1: Models, regardless of architecture, training dataset size, and even robustness interventions all share similar failure types. ImageNet-X annotations allow us to group images into Factors of Variation such as pose, pattern or texture (subfigure a and full definitions in Appendix A.2). A model can be evaluated on each of these factors, revealing where it makes the most mistakes. We compare error ratios $\\\\frac{1 - \\\\text{acc(factor)}}{1 - \\\\text{acc(overall)}}$ on each factor for 4 wide groups of models. Subfigure b shows that differences in texture, subcategories (e.g., breeds), and occlusion are most associated with models\u2019 mistakes. Transparent bars show the factors where there is no significant difference between the 4 groups ($p$ value $> 0.05$ with Alexander Govern test). By analyzing the ImageNet-X labels, in section 3, we find that in ImageNet pose and background commonly vary, that classes can have distinct factors (such as dogs more often varying in pose compared to other classes), and that ImageNet\u2019s training and validation sets share similar distributions of factors. We then analyze, in section 4, the failure types of more than 2,200 models. We find that models, regardless of architecture, training dataset size, and even robustness interventions all share similar failure types in section 4.1. Additionally, differences in texture, subcategories (e.g., breeds), and occlusion are most associated with models\u2019 mistakes.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Dataset Description\\n\\n| ImageNet-C algorithmic corruptions |\u2717 \u2713 \u2717 \u2717 | Hendrycks and Dietterich (2019) |\\n|------------------------------------|--------|---------------------------------|\\n| ImageNet-P animated perturbations  |\u2717 \u2717 \u2713 \u2717 | Hendrycks and Dietterich (2019) |\\n| ImageNet-R artistic renditions     |\u2717 \u2717 \u2717 \u2717 | Hendrycks et al. (2021)         |\\n| ImageNet-A natural adversarial examples | \u2713 \u2717 \u2717 \u2717 | Hendrycks et al. (2021)         |\\n| ImageNet-O out-of-distribution examples | \u2713 \u2717 \u2717 \u2717 | Hendrycks et al. (2021)         |\\n| ImageNet-V2 new validation set     |\u2717 \u2713 \u2717 \u2717 | Recht et al. (2019)             |\\n| ImageNet-Sketch drawn sketches     |\u2717 \u2717 \u2717 \u2717 | Wang et al. (2019)              |\\n| ImageNet-ML human multi-labels     |\u2717 \u2713 \u2717 \u2717 | Shankar et al. (2020)           |\\n| ImageNet-ReLabel machine pixelwise multilabels | \u2713 \u2713 \u2713 \u2717 | Yun et al. (2021)               |\\n| ImageNet-Stylized randomly-textured images | \u2713 \u2717 \u2713 \u2717 | Geirhos et al. (2018)           |\\n| ImageNet-X                         |\u2713 \u2713 \u2713 \u2713 | Ours                            |\\n\\n*Ours* is the first dataset based on ImageNet to include human annotations of multiple factors of variation for the entire ImageNet validation set.\\n\\n### Table 1: Extensions of the ImageNet benchmark designed to inspect failure modes of the ImageNet trained models.\\n\\nWe characterize each dataset by looking whether:\\n\\n1. the dataset images are only coming from the ImageNet validation set \u2014 *ImageNet images* \u2014;\\n2. the dataset images are natural images or are created with algorithmic and artistic perturbations of natural images \u2014 *Natural images* \u2014;\\n3. the dataset annotates the entire ImageNet validation set \u2014 *Entire val. set* \u2014; and\\n4. whether the dataset contains human annotations of image factors of variations \u2014 *Human FoV* \u2014.\\n\\n### Related Work\\n\\nThe research community has developed approaches for testing models' robustness on extended versions of the ImageNet dataset \u2014 see Figure 3 for a visual on different levels of evaluation granularity and Table 1 for an overview of datasets created to test the failure modes of the models trained on the ImageNet data. One common approach is to introduce artificial augmentations, e.g. image corruptions and perturbations (Hendrycks and Dietterich, 2019), renditions (Hendrycks et al., 2021), sketches (Wang et al., 2019; Bordes et al., 2021), etc. These artificial variations capture changes arising from corruptions, but are unlikely to capture changing arising from natural distribution shifts or variation such as changes in pose, lighting, scale, background, etc. Consequently, researchers also collected additional natural images to study the performance of the classification models under moderate to drastic distribution shifts Hendrycks and Dietterich (2019); Recht et al. (2019). However, most of these datasets are built to assess the model performance when going away from training data distribution and, thus, provide almost no understanding about the nature of the in-distribution errors.\\n\\nCurrently, the only ImageNet extensions that help analyzing the in-distribution model errors are the multiclass relabelling or saliency of the validation set (Shankar et al., 2020; Beyer et al., 2020a; Yun et al., 2021; Singla and Feizi, 2021). However, this relabelling only explains one type of model error.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3.1 ROLE OF SUPERVISION\\n\\nWe first group models into supervised (1k and with more data), self-supervised, and trained with robustness interventions in Figure 1. We measure the error ratio for each factor across ImageNet-X. We find all model types have comparable error ratios, meaning models make similar types of mistakes. There are a few minor differences. For instance, self-supervised models seem to be slightly more robust to the factors: color, larger, darker, style, object blocking, subcategory and texture. Supervised models trained on more data are more robust to the person blocking factor. We isolate whether some of the effects may be due to difference in data augmentation next.\\n\\n4.3.2 DATA AUGMENTATION\\n\\nData-augmentation (DA) is an ubiquitous technique providing significant average test performance gains (Shorten and Khoshgoftaar, 2019). In short, DA leverages a priori knowledge to artificially augment a training set size. The choice of DA policies e.g. image rotations, color jittering, translations is rarely questioned as it is given by our a priori understanding of which transformations will produce nontrivial new training samples whilst preserving their original semantic content. Although intuitive at first glance, DA has recently seen much attention as it can unfairly impact different classes (Balestriero et al., 2022), affect invariance (Deng et al., 2022), and potential to improve various forms of robustness (Hendrycks et al., 2021; Mintun et al., 2021). Equipped with ImageNet-X, we now propose a more quantitative and principled understanding of the impact of DA.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To that extent, we propose the following controlled experiment. We employ a ResNet50\u2014 which is one of the most popular architectures employed by practitioners\u2014 and perform multiple independent training runs with varying DA policy. Each run across all policies share the exact same optimizer (SGD), weight-decay (1e-5), mini-batch size (512), number of epochs (80), and data ordering through training. For each DA setting, multiple runs are performed to allow for statistical testing. Only the strength of the DAs and the random seed vary within those runs. In all scenarios, left-right flip is used both during training and testing.\\n\\nData augmentations can improve robustness, but with spill-over effects to unrelated factors. We report in Fig. 6 the statistically significant effects on error ratio due to three data augmentations: random crop, color jittering and Gaussian blur. For each setting, we measure prevalence shifts i.e. how much more or less likely a factor is to appear among a model's misclassifications.\\n\\nFor random crop, we vary the random crop area lower-bound i.e. how small of a region can the augmented sample be resized from the original image (varying from 0.08 which is the default value, to 1 which is synonym of no DA). We find the prevalence shift decreases for pose and partial view as expected. We also find that pattern, which is unrelated to random cropping, improves as well. However, we also observe a decrease in robustness for subcategory, an unrelated factor.\\n\\nFor color jittering, we vary both the probability to apply some standard color transformations and their strength. In particular, for any given value $t \\\\in (0, 1)$ we employ the composition of ColorJitter with probability $0.8t$ with brightness strength $0.4t$, contrast strength $0.4t$, saturation strength $0.1t$, and hue strength $0.1t$, followed by random grayscale with probability $0.5t$, and finally followed by random solarization with probability $0.3t$. Those parameters are found so that when $t$ nears 1, the DA is similar to most aggressive DA pipelines used in recent methods. We naturally observe that the predictions become more robust to change in color and brightness. Interestingly, the model becomes more sensitive to pose variations.\\n\\nFor Gaussian blur, we vary the standard deviation of the Gaussian filter between 0.1 and $0.1 + 3.5t$ for a filter size of $13 \\\\times 13$. Note that when $t$ approaches 1 the strength of this DA goes beyond the standard setting used, for example, in SSL. In supervised training, GaussianBlur DA is rarely employed on ImageNet. We observe that the model becomes more robust to texture which might come from blurring removing most of the texture information and forcing the model to rely on other features. Surprisingly, subcategory is also much less of an impact on performances while change in pose becomes more problematic for the Gaussian blur trained model.\\n\\n**Conclusion**\\n\\nWe introduced ImageNet-X, an annotation of the validation set and 12,000 training samples of the ImageNet dataset across 16 factors including color, shape, pattern, texture, size, lightning, and occlusion. We showed how ImageNet-X labels can reveal how images in popular ImageNet dataset differ. We found that images commonly vary in pose and background, that classes can have distinct factors (such as dogs more often varying in pose compared to other classes), and that ImageNet's training and validation sets share similar distributions of factors. Next, we showed how models mistakes are surprisingly consistent across architectures, learning paradigms, training data size, and common robustness interventions. We identified data augmentation as a promising lever to improve models' robustness to related factors, however, it can also affect unrelated factors. These findings suggest a need for a deeper understanding of data augmentations on model robustness. We hope that ImageNet-X serves as an useful resource to build a deeper understanding of the failure modes of computer vision models, and as a tool to measure their robustness across different environments.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Reproducibility Statement\\nThe results and figures in the paper can be reproduced using the open-source code and the ImageNet-X annotations, which we also release. The annotations were collected by training annotators to contrast three prototypical images from the same class. This setup can be replicated using the questionnaire we provide here as well as the freely available ImageNet dataset. For a detailed description of the ImageNet-X dataset, please see A.1.\\n\\nREFERENCES\\n\\nMichael A Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, and Anh Nguyen. Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects. In CVPR, 2019. URL https://arxiv.org/abs/1811.11553.\\n\\nRandall Balestriero, Leon Bottou, and Yann LeCun. The effects of regularization and data augmentation are class dependent. arXiv preprint arXiv:2204.03632, 2022.\\n\\nSara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In ECCV, 2018. URL https://arxiv.org/abs/1807.04975.\\n\\nLucas Beyer, Olivier J H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and Aaron van den Oord. Are we done with imagenet? arXiv, 2020a. URL https://arxiv.org/abs/2006.07159.\\n\\nLucas Beyer, Olivier J. H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and Aaron van den Oord. Are we done with imagenet? (arXiv:2006.07159), Jun 2020b. doi: 10.48550/arXiv.2006.07159. URL http://arxiv.org/abs/2006.07159. arXiv:2006.07159 [cs].\\n\\nFlorian Bordes, Randall Balestriero, and Pascal Vincent. High fidelity visualization of what your self-supervised representation knows about. arXiv preprint arXiv:2112.09164, 2021.\\n\\nDiane Bouchacourt, Mark Ibrahim, and Ari Morcos. Grounding inductive biases in natural images: invariance stems from variations in data. Advances in Neural Information Processing Systems, 34:19566\u201319579, 2021.\\n\\nJoy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, 2018. URL https://proceedings.mlr.press/v81/buolamwini18a.html.\\n\\nKyla Chasalow and Karen Levy. Representativeness in statistics, politics, and machine learning. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 2021. URL https://arxiv.org/abs/2101.03827.\\n\\nAlex J DeGrave, Joseph D Janizek, and Su-In Lee. Ai for radiographic covid-19 detection selects shortcuts over signal. Nature Machine Intelligence, 2021. URL https://www.nature.com/articles/s42256-021-00338-7.\\n\\nWeijian Deng, Stephen Gould, and Liang Zheng. On the strong correlation between model invariance and generalization. arXiv preprint arXiv:2207.07065, 2022.\\n\\nTerrance DeVries, Ishan Misra, Changhan Wang, and Laurens van der Maaten. Does object recognition work for everyone? download pdf. arXiv, 2019. URL https://arxiv.org/abs/1906.02659.\\n\\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 III, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):86\u201392, 2021.\\n\\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.\\n\\nSorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu. A survey of deep learning techniques for autonomous driving. Journal of Field Robotics, 2020. URL https://arxiv.org/abs/1910.07738.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "HXz7Vcm3VgM", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model  | Best Factors                                                                 | Worst Factors                                                                 |\\n|-------|------------------------------------------------------------------------------|------------------------------------------------------------------------------|\\n| DINO  | bird person blocking (0.0), style (0.0), texture (0.0), larger (0.0), darker (2.6), smaller (2.7), object blocking (12.9), dark |                                                                 |\\n| ResNet50 | bird larger (0.0), style (0.0), multiple objects (0.0), brighter (0.0), darker (4.8), object blocking (11.9), person blocking (5.0) |                                                                 |\\n| SimCLR | bird person blocking (0.0), texture (0.0), larger (0.0), multiple objects (0.0), partial view (1.8), smaller (1.8), shape (0.6), larger (2.5), person blocking (3.8), style (3.8), texture (3.8) |                                                                 |\\n| ViT   | bird person blocking (0.0), object blocking (0.0), larger (0.0), texture (0.0), subcategory (2.2), darker (2.7), smaller (0.0), brighter (0.0), style (0.0), pattern (0.6) | object blocking (2.6), larger (3.5), person blocking (5.2), texture (5.2) |\\n\\nTable 4: Best and worst 4 factors for DINO, ResNet50, SimCLR and ViT by Metaclass. The values between parenthesis are the error ratios for the associated factor.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ImageNet-X builds on this line of work to provide granular labels for naturally occurring factors such as changes in pose, background, lighting, scale, etc. to pinpoint the underlying modes of failure.\\n\\nImageNet-X contains human annotations for each of the 50,000 images in the validation set of the ImageNet dataset and 12,000 random sample from the training set. Since it's difficult to annotate factors of variations by looking at a single image in isolation, we obtain the annotation by comparing a validation set image to the three class-prototypical images and ask the annotators to describe the image by contrasting it with the prototypical images. We define the prototypical images as the most likely images under ResNet-50 model (He et al., 2015). Trained annotators select among sixteen distinguishing factors, possibly multiple, and write a text description as well as one-word summaries of key differences. The form is illustrated in Figure 1. The factors span pose, various forms of occlusion, styles, and include a subcategory factor capturing whether the image is of a distinct type or breed from the same class (full definitions in Appendix A.2). The text descriptions account for factors outside the sixteen we provide. After training the annotators and verifying quality with multi-review on a subset, each image is annotated by one trained human annotator. For example, the annotator marks whether the object depicted in the Sample image is larger or more occluded than Prototypical images. We provide a datasheet following Gebru et al. (2021) in Appendix A.1. One word summaries confirm the list of factors considered.\\n\\nSince our pre-selected list of 16 factors may not encompass every type of variation needed to account for model bias, we also asked annotators to provide one-word summaries to best distinguish a given image from its prototypes. We assess whether these free-form responses are encompassed within the pre-defined categories. We find the top-20 one-word annotation summaries are: pattern, close-up, top-view, front-view, grass, black, angle, white, color, background, brown, blue, red, position, facing-left, trees, person, side-view, low-angle, all falling within the 16 categories defined. For example top-view, front-view, facing-left, low-angle, side-view are captured by the pose factor.\\n\\nTo better understand the proposed annotations, we explore the distribution of the different factors among ImageNet images. We identify the most common varying factors in ImageNet, confirm factor training and validation set distributions match, and find factors can vary in ways that are class-specific.\\n\\n![Figure 2](image-url)\\n\\nFigure 2: Some factors are selected for most images; choosing the top factor allows to focus on the main change in the image. Figure shows the distribution of each factor in the training and validation set both with all factors selected and only top factor selected. Pose and background are commonly selected factors. As we can see in Figure 2, when aggregating all the annotations, pose, background, pattern, and color factors are commonly selected. For instance, pose and background are active for around 80% of the validation images. Since images are not likely to share a background and the objects within are unlikely to share the same pose, annotators selected these factors for many images. Pattern and color, which are the next most common, are also unlikely to be identical across images.\\n\\n1 The prototypical images for a class are those correctly classified with the highest predicted probability by ResNet-50. For 115 models with accuracies better than ResNet-50\u2019s, the average accuracy on the prototypical examples is 99.4%. Therefore all good models seem to agree on this set of prototypes.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Selecting the top factor per image. While the annotators could select multiple factors, we select a unique, top factor per image. To do so, we use the free-form text justification that the annotator provided. We embed the text justification using a pretrained language model provided by Spacy (Honnibal and Montani, 2017), and we compare it to that of selected factors' embeddings. This selection allows us to extract the main change in each image, thus avoiding over-representing factors that are triggered by small changes, such as pose or background. We see a clear reduction of those factors in Figure 2 (all vs top factors).\\n\\nTraining and validation sets have similar distributions of factors. To see if there is a distribution shift between training and validation, we annotate a subset of 12k training samples in a similar fashion as the validation set. Figure 2 reports the counts of each factor in this subset (denoted train all and train top). Comparing with the validation dataset, we see most represented factors are very similar to the validation set. We confirm this statistically by performing a $\\\\chi^2$-test on the factor distribution counts, confirming we cannot reject the null hypothesis that the two distributions differ (with $p < 0.01$).\\n\\nSimilarly, in Appendix A.3 we see that the distribution of factors ticked by the human annotator (referred to as active) per image are close.\\n\\nTo ease our analysis, we consider for each image its meta-label. A meta-label regroups many ImageNet classes into bigger classes. We consider 17 meta-labels that we extract using the hierarchical nature of ImageNet classes: we select 16 common ancestors in the wordnet tree of the original ImageNet 1000 classes. These chosen meta-labels are: device, dog, commodity, bird, structure, covering, wheeled vehicle, food, equipment, insect, vehicle, furniture, primate, vessel, snake, natural object, and other for classes that don\u2019t belong to any of the first 16.\\n\\nClasses can have distinct variation factors. In Appendix A.4 (right) we also observed statistically significant correlations between factors and meta-labels. For instance, the dog meta-label is negatively correlated with pattern, color, smaller, shape, and subcategory while being positively correlated with pose and background. This suggests that the images of dogs in the validation set have more variation for pose and background while having less variation for pattern, color, smaller, shape, and subcategory. The commodity meta-label, which contains clothing and home appliances, is positively correlated with pattern and color and negatively correlated with pose.\\n\\nIn addition to revealing rich information about ImageNet, ImageNet-X can be used to probe at varying levels of granularity a model\u2019s robustness (see Figure 3).\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023 via ImageNet-X.\\n\\ngoes beyond revealing model mistakes to pinpointing the underlying factors associated with each mistake. Here we systematically study the robustness of 2,200 trained models \u2013 including many models from the ImageNet test bed Taori et al. (2020a) and additional self-supervised architectures such as DINO and SimCLR \u2013 to reveal the limitations of average accuracy, understand model biases, and characterize which choices in architecture, learning paradigm, data augmentation, and regularization affect robustness. For our analysis we focus on the most salient factor for each image by ranking the selected factors by their similarity to the text-justification.\\n\\n4.1 Many deep learning models have the same weaknesses and strengths.\\n\\nFigure 4: Most deep learning models, when trained, finetuned or evaluated on ImageNet, have the same biases. We plot top-1 accuracy for the subset of images labeled with the given factor (y-axis) relative to overall top-1 accuracy (x-axis). The dashed line is an ideal robust models' performance, i.e. performance on each factor is the same as the overall performance. We show the performance of 209 models. We also show the accuracy for the worst factor, and for the images of the worst 100 classes.\\n\\nTo what extent does the commonly reported average accuracy capture a model's robustness? To answer this, we inspect 209 models from the ImageNet testbed Taori et al. (2020a) which includes many architectures (most of which are convolutional, with a few vision transformers), training procedures (losses, optimizers, hyperparameters), and pretraining data. We include additional self-supervised models for completeness. Models with similar overall accuracies have very similar per factor accuracies. With all this variety, the scatter plots in Figure 4 exhibit surprising consistency; overall accuracy is a good predictor of per factor accuracies. Most models, even with improving overall accuracy, seem to struggle with the same set of factors: subcategory, texture, object blocking, multiple objects and shape. Conversely, they seem to do well on the same set of factors: pose, pattern, brighter, partial view, background, color, larger, and darker. There are some factors where state-of-the-art models seem to have closed the robustness gap such as person blocking or style.\\n\\nMore training data helps, but robustness interventions do not. Models trained with larger datasets (blue circles in Figure 4) exhibit higher accuracy across the factors suggesting larger training datasets do help as others have shown Taori et al. (2020b). Surprisingly, models trained with\\n\\n6\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"robustness interventions (such as CutMix, AdvProp, AutoAugment, etc...), which are directly aimed at improving robustness don't show a significant improvement in per factor accuracy as prior work also shows Taori et al. (2020b).\\n\\nModel weaknesses coincide with labeling errors. The ImageNet labels are known to contain labeling errors Beyer et al. (2020b). With new labels from previous work, we assess the accuracy of the original ImageNet labels and study the distribution of errors across factors. Interestingly, we find that the original label accuracies on all factors coincide with the state of the art models. This offers a potential explanation for the model biases. We leave it to future work to investigate if training on more accurate labels leads to more robust models. We provide details in appendix A.8.\\n\\nIs accuracy enough? Since our annotations are on the ImageNet validation set, a 100% overall accuracy necessarily means a 100% per factor accuracy. So as models get better overall, they necessarily get better per factor accuracies. To disentangle performance from robustness we need to investigate the distribution of errors across factors for a given model.\\n\\n![An illustration of how ImageNet-X can identify robustness weaknesses and strengths for Vision Transformers (ViT). Here we visualize the 3 most susceptible, and 3 most robust factors for the three worst-performing metalabels along with dog. We also include the overall validation set for comparison. We see ViT is susceptible to texture, occlusion, and subcategory, but is robust to pattern, brightness, and pose. While those overall types also show up across metalabels, we see robustness can be distinct by metalabel.](image)\\n\\n### 4.2 MOVING BEYOND AVERAGE ACCURACY: ASSESSING FAILURE TYPES WITH IMAGENET-X\\n\\nWith ImageNet-X we can go beyond average accuracy, to identify the types of mistakes a model makes. To do so we measure the error ratio across each of the 16 ImageNet-X factors: Specifically, $\\\\text{Error ratio}(\\\\text{factor}, \\\\text{model}) = 1 - \\\\frac{\\\\text{accuracy}(\\\\text{factor}, \\\\text{model})}{\\\\text{accuracy}(\\\\text{model})} = \\\\hat{P}(\\\\text{factor} | \\\\text{correct}(\\\\text{model}) = 0) - \\\\hat{P}(\\\\text{factor})$.\\n\\nThis quantifies how many more mistakes a model makes on a given factor relative to its overall performance. It also measures the increase or decrease in likelihood of a factor when selecting a model's errors vs the overall validation set. A perfectly robust model would have the same error rate across all factors, yielding error ratios of 1 across all factors.\\n\\n### 4.2.1 AN EXAMPLE OF VISION TRANSFORMER'S MISTAKES\\n\\nWe illustrate how the error ratio can be used to identify the types of failures and strengths for the popular Vision Transformer (ViT) model. Despite impressive 84% average top-1 accuracy, we find ViT's mistakes are associated with texture, occlusion, and subcategory (appearing 2.02-2.11x more times among misclassified samples than overall) as shown in Figure 5. On the other hand we find ViT are robust to pose, brightness, pattern, and partial views. We also see that these strengths or weaknesses can vary by metalabel. For example, ViT is susceptible to occlusion for vessel and snake, but not the commodity metalables where mistakes are associated with style, darkness, and texture. For the dog metaclass, ViT is quite robust to different poses. Instead, ViT's mistakes for dogs are associated with the presence of multiple objects and differences among dog breeds (subcategory). The full list of failure types across meta-labels is in Appendix A.12.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 16: Models tend to have similar biases for the measured factors. The largest difference being between ResNet and SimCLR likely to color jitter.\\n\\nFigure 17: Prevalence shift comparison of tailored versus universal color jittering augmentation.\\n\\nIn Table 3, we show the three most correlated factors with each OOD dataset. We compute the Pearson correlation between each factor's error ratio (shown in parenthesis) and the performance on each OOD dataset. We find an intuitive correspondence between the factors in ImageNet-X and downstream OOD performance. For example, ImageNet-R, which contains renditions such as art, cartoons, embroidery, graphics, origami, paintings, patterns of ImageNet classes, is most correlated with shape and texture as well as subcategory. On the other hand, Greyscale which discards color information is most influence by each model's reliance on color, smaller objects, and occlusion. ImageNet-A, which contains natural images explicitly mined to be challenging for standard vision models, often contains images where the object is small or blends in with the background. We see the top factors for ImageNet-A are texture, shape, and whether an object can be identified even if it's small. We hope this analysis reveals the potential for ImageNet-X factors to better probe downstream performance. This unlocks for example the potential for future work to assess/optimize downstream OOD performance only by tuning model performance on the ImageNet-X factors.\\n\\nA.10 Influence of Architecture Choices\\n\\nAlthough the general conclusion of our per factor performance analysis indicates consistent performances across architectures, training data and losses, there are still some differences that arise. We study some model choices to shed some light on where those differences might arise in Figures 20, 21 and 22.\\n\\nA.11 Multi Factor Results\\n\\nIn the paper we chose to select the top factor per image. In this section we present the main figures of the paper with multiple factors per image selected. The main conclusions remain valid, and the\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 18: We measure the ReAL accuracy of the original validation labels and show that these labels have consistent errors on the same factors as models shown previously.\\n\\nFigure 19: Pearson correlation between factor error ratios and distribution shifts of ImageNet. The correlation strengths change depending on the distribution shift. For instance for greyscale images, the color factor is maximally correlated with relative performance.\\n\\nA.12 Granular model biases by meta-labels\\n\\nIn Table 4 we show the robustness susceptibility of factors for DINO, ResNet50, SimCLR and ViT for each meta-label. We see while many factors are shared, some meta-labels are especially susceptible to distinct factors. For example, ViT is susceptible to style for vehicles and insects.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nFigure 20: We measure the error ratios for multiple ResNets (ResNet50, ResNet101, ResNet152) and for multiple Transformers (ViT small/16, base/16, large/16). The main differences are in the factors partial view, darker and multiple objects, with transformers having an advantage. The better handling of multiple objects in Transformers might be due to the lack of average pooling.\\n\\nFigure 21: We measure the error ratios for multiple ResNets (ResNet50, ResNet101, ResNet152) to study the effect of depth. For most factors, the deeper the ResNet the better, except for the \u201cobject blocking\u201d factor that gets considerably worse as the model gets deeper.\\n\\nFigure 22: We measure the error ratios for 6 scales of EffiCienNet models from b0 to b5. As the model grows in size, the error ratios grow, especially for the b5 scale.\\n\\nFigure 23: Model comparison with multiple factors per image.\"}"}
{"id": "HXz7Vcm3VgM", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 24: High level scatter plot with multiple factors per image.\"}"}
