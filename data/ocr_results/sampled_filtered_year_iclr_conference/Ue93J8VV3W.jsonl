{"id": "Ue93J8VV3W", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Section 3, we describe the proposed graph datasets, including their original sources and methodologies employed to prepare them for our study. In Section 5, we also provide the details about our experimental setup, specifying the references to the open-source implementations of the used baselines. The configurations of machine learning models are also reported in Appendices D and C. To ensure reproducibility of our work, we plan to release both the source code and the datasets once we finalize the required legal procedures.\\n\\n**REFERENCES**\\n\\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In *Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining*, pp. 2623\u20132631, 2019.\\n\\nSercan \u00a8Omer Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. *arxiv* *arXiv* preprint *arXiv:2004.13912*, 2019.\\n\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. *arXiv* preprint *arXiv:1607.06450*, 2016.\\n\\nSarkhan Badirli, Xuanqing Liu, Zhengming Xing, Avradeep Bhowmik, Khoa Doan, and Sathiya S Keerthi. Gradient boosting neural networks: Grownet. *arXiv* preprint *arXiv:2002.07971*, 2020.\\n\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. *arXiv* preprint *arXiv:1409.0473*, 2014.\\n\\nStefano Boccaletti, Ginestra Bianconi, Regino Criado, Charo I Del Genio, Jes \u00b4us G\u00b4omez-Gardenes, Miguel Romance, Irene Sendina-Nadal, Zhen Wang, and Massimiliano Zanin. The structure and dynamics of multilayer networks. *Physics reports*, 544(1):1\u2013122, 2014.\\n\\nJiuhai Chen, Jonas Mueller, Vassilis N. Ioannidis, Soji Adeshina, Yangkun Wang, Tom Goldstein, and David Wipf. Does your graph need a confidence boost? convergent boosted smoothing on graphs with tabular node features. In *International Conference on Learning Representations*, 2022. URL [https://openreview.net/forum?id=nHpzE7DqAnG](https://openreview.net/forum?id=nHpzE7DqAnG).\\n\\nTianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In *Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining*, pp. 785\u2013794, 2016.\\n\\nJerome H Friedman. Greedy function approximation: a gradient boosting machine. *Annals of statistics*, pp. 1189\u20131232, 2001.\\n\\nCarlos Garc\u00b4\u0131a Ling, ElizabethHMGroup, FridaRim, inversion, Jaime Ferrando, Maggie, neuralover-flow, and xlsrln. H&m personalized fashion recommendations, 2022. URL [https://kaggle.com/competitions/h-and-m-personalized-fashion-recommendations](https://kaggle.com/competitions/h-and-m-personalized-fashion-recommendations).\\n\\nC Lee Giles, Kurt D Bollacker, and Steve Lawrence. Citeseer: An automatic citation indexing system. In *Proceedings of the third ACM conference on Digital libraries*, pp. 89\u201398, 1998.\\n\\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In *International conference on machine learning*, pp. 1263\u20131272. PMLR, 2017.\\n\\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models for tabular data. *Advances in Neural Information Processing Systems*, 34:18932\u201318943, 2021.\\n\\nYury Gorishniy, Ivan Rubachev, and Artem Babenko. On embeddings for numerical features in tabular deep learning. *Advances in Neural Information Processing Systems*, 35:24991\u201325004, 2022.\"}"}
{"id": "Ue93J8VV3W", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yury Gorishniy, Ivan Rubachev, Nikolay Kartashev, Daniil Shlenskii, Akim Kotelnikov, and Artem Babenko. Tabr: Unlocking the power of retrieval-augmented tabular deep learning. arXiv preprint arXiv:2307.14338, 2023.\\n\\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017.\\n\\nHussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and Rahul Mazumder. The tree ensemble layer: Differentiability meets conditional computation. In International Conference on Machine Learning, pp. 4138\u20134148. PMLR, 2020.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.\\n\\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.\\n\\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118\u201322133, 2020.\\n\\nXin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. Tabtransformer: Tabular data modeling using contextual embeddings. arXiv preprint arXiv:2012.06678, 2020.\\n\\nSergei Ivanov and Liudmila Prokhorenkova. Boost then convolve: Gradient boosting meets graph neural networks. arXiv preprint arXiv:2101.08543, 2021.\\n\\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30, 2017.\\n\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.\\n\\nG\u00fcnter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. Advances in neural information processing systems, 30, 2017.\\n\\nJannik Kossen, Neil Band, Clare Lyle, Aidan N Gomez, Thomas Rainforth, and Yarin Gal. Self-attention between datapoints: Going beyond individual input-output pairs in deep learning. Advances in Neural Information Processing Systems, 34:28742\u201328756, 2021.\\n\\nDaniil Likhobaba, Nikita Pavlichenko, and Dmitry Ustalov. Toloker Graph: Interaction of Crowd Annotators. 2023. doi: 10.5281/zenodo.7620795. URL https://github.com/Toloka/TolokerGraph.\\n\\nAndrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the construction of internet portals with machine learning. Information Retrieval, 3:127\u2013163, 2000.\\n\\nGalileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. Query-driven active surveying for collective classification. In 10th international workshop on mining and learning with graphs, volume 8, pp. 1, 2012.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\\n\\nOleg Platonov, Denis Kuznedelev, Artem Babenko, and Liudmila Prokhorenkova. Characterizing graph datasets for node classification: Beyond homophily-heterophily dichotomy. arXiv preprint arXiv:2209.06177, 2022.\"}"}
{"id": "Ue93J8VV3W", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nOleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova.\\n\\nA critical look at the evaluation of gns under heterophily: are we really making progress?\\n\\narXiv preprint arXiv:2302.11640, 2023.\\n\\nSergei Popov, Stanislav Morozov, and Artem Babenko. Neural oblivious decision ensembles for deep learning on tabular data.\\n\\narXiv preprint arXiv:1909.06312, 2019.\\n\\nLiudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: unbiased boosting with categorical features.\\n\\nAdvances in neural information processing systems, 31, 2018.\\n\\nJiarui Qin, Weinan Zhang, Rong Su, Zhirong Liu, Weiwen Liu, Ruiming Tang, Xiuqiang He, and Yong Yu. Retrieval & interaction machine for tabular data prediction. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 1379\u20131389, 2021.\\n\\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data.\\n\\nAI magazine, 29(3):93\u201393, 2008.\\n\\nOleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Pitfalls of graph neural network evaluation.\\n\\narXiv preprint arXiv:1811.05868, 2018.\\n\\nYunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, and Yu Sun. Masked label prediction: Unified message passing model for semi-supervised classification.\\n\\narXiv preprint arXiv:2009.03509, 2020.\\n\\nGowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C Bayan Bruss, and Tom Goldstein. Saint: Improved neural networks for tabular data via row attention and contrastive pre-training.\\n\\narXiv preprint arXiv:2106.01342, 2021.\\n\\nWeiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. Autoint: Automatic feature interaction learning via self-attentive neural networks. In Proceedings of the 28th ACM international conference on information and knowledge management, pp. 1161\u20131170, 2019.\\n\\nPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks.\\n\\narXiv preprint arXiv:1710.10903, 2017.\\n\\nMinjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan Yu, Yu Gai, et al. Deep graph library: A graph-centric, highly-performant package for graph neural networks.\\n\\narXiv preprint arXiv:1909.01315, 2019.\\n\\nRuoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17, pp. 1\u20137. 2017.\\n\\nRuoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the web conference 2021, pp. 1785\u20131797, 2021.\\n\\nSteve Wang and Will Cukierski. Click-through rate prediction, 2014. URL https://kaggle.com/competitions/avazu-ctr-prediction.\\n\\nZhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pp. 40\u201348. PMLR, 2016.\\n\\nShijie Zhang, Hongzhi Yin, Tong Chen, Quoc Viet Nguyen Hung, Zi Huang, and Lizhen Cui. Gcn-based user representation learning for unifying robust recommendation and fraudster detection. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval, pp. 689\u2013698, 2020.\"}"}
{"id": "Ue93J8VV3W", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we discuss some important aspects of constructing graphs in our benchmark. For all of the proposed graph datasets, we use external relational information for constructing the underlying graph. Another possible approach would be deriving the connections between data samples from their node features, but it is not trivial in the case of heterogeneous tabular features, as these features have widely different scales, distributions, meaning and importance for the task. However, comparing their representation from some layer of a neural network is more meaningful. This is exactly what retrieval-augmented models do (in essence, they simultaneously learn sample representations and a graph based on their similarity). We discuss these models in the related work section and include TabR, a representative example of them, in our experiments.\\n\\nFurther, some of the proposed graph datasets are derived from the corresponding bipartite networks of interactions between entities of different types, which is done by projecting the original network to one of its parts. This holds for city-reviews, which connects users that write their reviews for the same organizations, hm-products, where products are connected if they are bought by the same customers, and avazu-devices, for which we connect devices that use the same entry point. As described in Section 3, for this purpose, we use an additional parameter $\\\\gamma$, which depends on a particular graph dataset and serves as a threshold to control the density of the underlying graph. To construct our benchmark, we adjust this parameter $\\\\gamma$ so that the resulting graphs are not too sparse and not too dense. Although the mentioned datasets can be constructed in many different ways depending on the threshold $\\\\gamma$, we assume that our approach is quite reasonable, since even with such parameter choice we manage to achieve higher performance on all these datasets by using GNN models that exploit graph information and thus demonstrate the benefits of knowing graph structure in the considered problems.\\n\\nFinally, preparing the graph might take some time. The exact amount varies depending on the nature of the raw data and the type of edges used in the graph being constructed. For example, if the edges are based on interactions between data samples (e.g., in questions-tab), then constructing the graph is relatively fast, as we simply need to obtain a list of interactions, and each interaction corresponds to one edge. On the contrary, if the edges are based on some kind of activity similarity (e.g., in avazu-devices), constructing the edges might take more time, as we need to assess how similar two data samples are in terms of their activity. However, the construction of a graph only needs to be performed once, as part of data pre-processing, so the time required for it is negligible compared to the time spent for running multiple experiments on the dataset.\\n\\nIn this section, we discuss the issues of some graph datasets with tabular features used for evaluation in previous studies (Ivanov & Prokhorenkova, 2021; Chen et al., 2022).\\n\\nConsidering the classification datasets, dblp and slap actually have homogeneous features, which were augmented with some graph statistics to make them heterogeneous. Further, the corresponding graphs are actually heterogeneous information networks (HINs), which have several relation types, yet only one relation type was used. As for the house-class and vk-class datasets, these are actually regression datasets that were converted to classification datasets by binning target values.\\n\\nRegarding the regression datasets, county and avazu are very small. In our benchmark, we adapt an extended version of the avazu dataset, which has a significantly different scale (please refer to Table 1). Further, the house dataset originally did not contain a graph, and edges were constructed based on physical proximity, while the original features representing geographic coordinates were removed. However, keeping these features and removing the graph leads to similar performance, which shows that the graph is not necessary for this task. Finally, we found that for the vk dataset, CatBoost, GCN, and GAT achieve the $R^2$ values less than 0.1, which shows that the provided features and graph structure are not very helpful for the task.\"}"}
{"id": "Ue93J8VV3W", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In our experiments, we found that, when augmented with skip-connections and layer normalization, our GNNs are not very sensitive to the choice of hyperparameters. Thus, we used a single set of hyperparameters, which is described below.\\n\\nThe number of GNN blocks is 3. The hidden dimension is 512. The dropout probability is 0.2. For GAT and GT models, the number of attention heads is set to 4. We use the GELU activation function (Hendrycks & Gimpel, 2016) in all our GNNs. We also use the Adam optimizer (Kingma & Ba, 2014) with learning rate of $3 \\\\cdot 10^{-5}$. We train each model for 1000 steps and select the best step based on the performance on the validation subset. Our baselines are implemented using PyTorch (Paszke et al., 2019) and DGL (Wang et al., 2019).\\n\\nPLR embeddings for numerical features have additional hyperparameters, and, as noted in (Gorishniy et al., 2022), tuning them can be very important. However, for a fair comparison with vanilla GNNs, we use these embeddings without tuning. The hyperparameters are the following: number of frequencies is 48, frequency scale is 0.01, embedding dimension is 16. We use the \\\"lite\\\" version of PLR introduced in (Gorishniy et al., 2023).\\n\\n### Hyperparameters of Classic and Deep Learning Models\\n\\nIn Tables 4-8, we provide the hyperparameter search space for classic and deep learning models. In our experiments, we perform 70 rounds of Bayesian optimization using Optuna (Akiba et al., 2019).\\n\\n**Table 4: The hyperparameter tuning space for XGBoost.**\\n\\n| Parameter | Distribution |\\n|-----------|--------------|\\n| subsample bytree | Uniform[0.5, 1.0] |\\n| gamma | {0.0, LogUniform[0.001, 100.0]} |\\n| lambda | {0.0, LogUniform[0.1, 100.0]} |\\n| learning rate | LogUniform[0.001, 1.0] |\\n| max depth | UniformInt[3, 14] |\\n| min child weight | LogUniform[0.0001, 100.0] |\\n| subsample | Uniform[0.5, 1.0] |\\n\\n**Table 5: The hyperparameter tuning space for LightGBM.**\\n\\n| Parameter | Distribution |\\n|-----------|--------------|\\n| feature fraction | Uniform[0.5, 1.0] |\\n| lambda l2 | {0.0, LogUniform[0.1, 100.0]} |\\n| learning rate | LogUniform[0.001, 1.0] |\\n| num leaves | UniformInt[4, 768] |\\n| min sum hessian in leaf | LogUniform[0.0001, 100.0] |\\n| bagging fraction | Uniform[0.5, 1.0] |\"}"}
{"id": "Ue93J8VV3W", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: The hyperparameter tuning space for CatBoost.\\n\\n| Parameter       | Distribution               |\\n|-----------------|-----------------------------|\\n| bagging         | Uniform [0.0, 1.0]          |\\n| depth           | UniformInt [3, 14]          |\\n| l2 leaf reg     | Uniform [0.0, 10.0]         |\\n| leaf estimation | Uniform [1, 10]             |\\n| learning rate   | LogUniform [0.001, 1.0]     |\\n\\nTable 7: The hyperparameter tuning space for MLP.\\n\\n| Parameter       | Distribution               |\\n|-----------------|-----------------------------|\\n| num layers      | UniformInt [1, 6]           |\\n| hidden size     | UniformInt [64, 1024]       |\\n| dropout rate    | {0.0, Uniform [0.0, 0.5]}   |\\n| learning rate   | LogUniform [3e-5, 1e-3]     |\\n| weight decay    | {0, LogUniform [1e-6, 1e-3]}|\\n| plr num frequencies | UniformInt [16, 96]       |\\n| plr frequency scale | LogUniform [0.001, 100.0]  |\\n| plr embedding size | UniformInt [16, 64]         |\\n\\nTable 8: The hyperparameter tuning space for TabR.\\n\\n| Parameter       | Distribution               |\\n|-----------------|-----------------------------|\\n| num encoder blocks | UniformInt [0, 1]          |\\n| num predictor blocks | UniformInt [1, 2]     |\\n| hidden size     | UniformInt [96, 384]       |\\n| context dropout | Uniform [0.0, 0.6]         |\\n| dropout rate    | Uniform [0.0, 0.5]         |\\n| learning rate   | LogUniform [3e-5, 1e-3]    |\\n| weight decay    | {0, LogUniform [1e-6, 1e-3]}|\\n| plr num frequencies | UniformInt [16, 96]       |\\n| plr frequency scale | LogUniform [0.001, 100.0]  |\\n| plr embedding size | UniformInt [16, 64]         |\"}"}
{"id": "Ue93J8VV3W", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ABSTRACT\\nThe field of tabular machine learning is very important for industry and science. Table rows are typically treated as independent data samples. However, often additional information about the relations between these samples is available, and leveraging this information may improve the predictive performance. As such relational information can be naturally modeled with a graph, the field of tabular machine learning can borrow methods from graph machine learning. However, graph models are typically evaluated on datasets with homogeneous features, such as word embeddings or bag-of-words representations, which have little in common with the heterogeneous mixture of numerical and categorical features distinctive for tabular data. Thus, there is a critical difference between the data used in tabular and graph machine learning studies, which does not allow us to understand how successfully graph methods can be transferred to tabular data. In this work, we aim to bridge this gap. First, we create a benchmark of diverse graphs with heterogeneous tabular node features and realistic prediction tasks. Further, we evaluate a vast set of methods on this benchmark, analyze their performance, and provide insights and tips for researchers and practitioners in both tabular and graph machine learning fields.\\n\\n1 INTRODUCTION\\nTabular data is ubiquitous in industry and science, thus machine learning algorithms for tabular data are of great importance. A key distinction of tabular data is that it typically consists of a mixture of numerical and categorical features, with numerical features having vastly different scales and distributions, and all features having different meaning and importance for the task. We further call such features heterogeneous. Deep learning methods often do not work well with heterogeneous features, thus, the machine learning methods of choice for tabular data are decision-tree-based models, in particular gradient-boosted decision trees (GBDT) (Friedman, 2001). However, there is a growing number of recent works trying to adapt deep learning methods to tabular data (Arik & Pfister, 2019; Badirli et al., 2020; Huang et al., 2020; Gorishniy et al., 2021).\\n\\nIn tabular machine learning, table rows are typically treated as independent data samples. However, often there is additional information available about relations between these samples, and leveraging this information has the potential to improve predictive performance. Such relational information can be modeled with a graph. There are many areas where graphs naturally arise. For example, if you have an application where users can interact with each other in some way, you essentially have a graph. Examples of this include social networks, discussion forums, question-answering websites, chat applications, financial transaction networks. Even if users do not directly interact with each other, meaningful relations can often be created, for example between users buying similar products on shopping websites, users watching similar content on video hostings, and workers doing the same tasks on crowd-sourcing platforms. In all these and many other cases, using graph information can improve the quality of machine learning model predictions.\\n\\nGraph machine learning is a field focused on the development of methods for learning on graph-structured data. In last years, the most successful models for such data have become graph neural networks (GNNs) (Kipf & Welling, 2016; Gilmer et al., 2017). Thus, it can be desirable to adapt these models to tabular data. However, GNNs are typically evaluated on graphs with homogeneous features (most frequently bags of words or bags of word embeddings), which are very different from...\"}"}
{"id": "Ue93J8VV3W", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"heterogeneous features present in tabular data, and because of this, it is unclear how they will per-\\nform on tabular data. Recently, there have been several methods proposed specifically for learning\\non graphs with heterogeneous node features (Ivanov & Prokhorenkova, 2021; Chen et al., 2022).\\n\\nHowever, both of these works note the lack of publicly available graph datasets with heterogeneous\\nfeatures, and thus their evaluation setting is limited. This highlights the difference between indus-\\ntry, where data with heterogeneous features is abundant, and graph machine learning benchmarks,\\nwhere such data is barely present. We believe that this difference holds back the adoption of graph\\nmachine learning methods to tabular data.\\n\\nIn our work, we aim to bridge this gap. First, we create a benchmark of graphs with heterogeneous\\ntabular features \u2014 TabGraphs. Our benchmark has realistic prediction tasks and is diverse in data\\ndomains, relation types, graph sizes, graph structural properties, and feature distributions. Further,\\nwe evaluate a large set of machine learning methods on this benchmark. We consider models for\\ntabular data, both GBDT and deep learning ones, several GNN architectures with different numerical\\nfeature pre-processing strategies, and the recently proposed methods for graphs with heterogeneous\\nfeatures. Our main findings are:\\n\\n\u2022 Using the graph structure in data and applying graph machine learning methods can lead to\\nan increase in predictive performance in many real-world datasets.\\n\\n\u2022 The predictive performance of different GNN architectures may vary significantly across\\ndatasets, and the best-performing architecture cannot be chosen without experiments.\\n\\n\u2022 Standard GNNs typically outperform recently proposed methods designed specifically for\\nnode property prediction tasks on graphs with tabular features.\\n\\n\u2022 Adding numerical feature embeddings that have been proposed in tabular deep learning to\\nGNNs can further improve their performance on graphs with heterogeneous features.\\n\\nTo summarize, our main contributions are as follows:\\n\\n\u2022 We introduce TabGraphs, the first benchmark for learning on graphs with heterogeneous\\nnode features, which covers various industrial applications and includes graphs with diverse\\nstructural properties and meaningful prediction tasks;\\n\\n\u2022 Using the proposed benchmark, we evaluate a vast set of models, including standard base-\\nlines and advanced neural methods for both tabular and graph-structured data, and analyze\\nthe results of our experiments;\\n\\n\u2022 Based on our empirical study, we provide insights and tips for researchers and practitioners\\nin both tabular and graph machine learning fields.\\n\\nWe will make our benchmark and the code for reproducing our experiments public once we finalize\\nlegal procedures. We also plan to maintain our benchmark and add additional datasets in the future.\\n\\n2 RELATED WORK\\n\\nMachine learning for tabular data\\n\\nThe key distinction of tabular data is that it usually consists\\nof a mixture of numerical and categorical features. Standard deep learning models often can not\\nprovide decent performance on such features. Thus, the methods of choice for tabular data are typi-\\ncally based on the ensembles of Decision Trees, such as Gradient Boosted Decision Trees (GBDT)\\n(Friedman, 2001) with the most popular implementations being XGBoost (Chen & Guestrin, 2016),\\nLightGBM (Ke et al., 2017), and CatBoost (Prokhorenkova et al., 2018). However, deep learning\\nmodels have several advantages compared to the tree-based ones, such as modularity and the abil-\\nity to learn meaningful data representations. Because of that, there has been an increasing number\\nof works trying to adapt deep learning to tabular data (Klambauer et al., 2017; Wang et al., 2017;\\nArik & Pfister, 2019; Song et al., 2019; Popov et al., 2019; Badirli et al., 2020; Hazimeh et al.,\\n2020; Huang et al., 2020; Gorishniy et al., 2021; Wang et al., 2021; Gorishniy et al., 2022). Among\\nthese works, the retrieval-augmented deep learning models (Kossen et al., 2021; Qin et al., 2021;\\nSomepalli et al., 2021; Gorishniy et al., 2023) are particularly relevant. For each data sample, these\\nmodels retrieve the information about other labeled examples from a database, which is typically\"}"}
{"id": "Ue93J8VV3W", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Machine learning for graphs\\n\\nGraphs are a natural way to represent data from various domains. Due to this, machine learning on graph-structured data has experienced significant growth in recent years, with Graph Neural Networks (GNNs) showing particularly strong results on many graph machine learning tasks. Numerous GNN architectures have been proposed (Kipf & Welling, 2016; Hamilton et al., 2017; Veli\u010dkovi\u0107 et al., 2017), and most of them can be unified under a general Message Passing Neural Networks (MPNNs) framework (Gilmer et al., 2017). However, GNNs are typically evaluated on graphs with homogeneous node features, such as bag-of-words or bag-of-word-embeddings representations. Let us review some popular graph datasets for node classification to emphasize this point. The most frequently used datasets are the three citation networks cora, citeseer, and pubmed (Sen et al., 2008; Namata et al., 2012; Yang et al., 2016; McCallum et al., 2000; Giles et al., 1998). The first two datasets use bags-of-words as node features, while the third one uses TF-IDF-weighted bags-of-words. Other datasets for node classification often found in the literature include citation networks coauthor-cs and coauthor-physics (Shchur et al., 2018) that use bags-of-words as node features, and co-purchasing networks amazon-computers and amazon-photo (Shchur et al., 2018) that also use bags-of-words. In the popular Open Graph Benchmark (OGB) (Hu et al., 2020) ogbn-arxiv, ogbn-papers100M, and ogbn-mag datasets again use bags-of-words as node features, while ogbn-products uses dimensionality-reduced bag-of-words representations. In the recently proposed benchmark of heterophilous graphs (Platonov et al., 2023), roman-empire dataset uses word embeddings as node features, while amazon-ratings and questions datasets use bags-of-words. From these examples, it becomes clear that the effectiveness of graph ML models on graphs with heterogeneous node features remains under-explored.\\n\\nMachine learning for graphs with heterogeneous features\\n\\nRecently, two methods have been proposed specifically for learning on graphs with heterogeneous tabular features. One of them is BGNN (Ivanov & Prokhorenkova, 2021), an end-to-end trained combination of GBDT and GNN, where GBDT takes node features as input and predicts node representations that are further concatenated with the original node features and used as input to a GNN. Another is EBBS (Chen et al., 2022), which alternates between boosting and graph propagation steps, essentially fitting GBDT to a graph-aware loss, and is also trained end-to-end. However, as both of the works note, there is a lack of publicly available datasets of graphs with heterogeneous features. For this reason, the evaluation of the proposed methods is limited: some of the used graphs do not have heterogeneous features, and some others have various issues which we describe in detail in Appendix B. Thus, better benchmarks for evaluating models for graphs with heterogeneous features are needed.\\n\\n3 TARGRAPHES: NEW GRAPH BENCHMARK WITH TABULAR FEATURES\\n\\nIn this section, we introduce a new benchmark of graph datasets with heterogeneous node features. Some of the datasets are taken or adapted from open sources, while others are obtained from proprietary logs of products and services of a large IT company. Below, we describe the proposed datasets.\\n\\n3.1 PROPOSED GRAPH DATASETS\\n\\ntolokers-tab\\n\\nThis is a new version of the dataset tolokers from Platonov et al. (2023). It is based on the data from the Toloka crowdsourcing platform (Likhobaba et al., 2023). The nodes represent tolokers (workers) and they are connected by an edge if they have worked on the same task. Thus, the graph is undirected, and the task is to predict which tolokers have been banned in one of the tasks. We kept the original graph and task, but we extended the node features which made them heterogeneous. The new node features are shared with us by the dataset creators and are based on the workers task performance and profile information.\"}"}
{"id": "Ue93J8VV3W", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This is a new version of the dataset from Platonov et al. (2023). It is based on the data from a question-answering website. Here, nodes represent users, and two users are connected by an edge, if either of them has answered to the other\u2019s question. The resulting graph is undirected, and the task is to predict which users remained active on the website (i.e., were not deleted or blocked). We kept the original graph and task, but extended the node features to make them heterogeneous. The original version of this dataset used BOW embeddings as user features, while we provide a number of attributes based on the user profile information and their activity on the website. Similarly to tolokers-tab, the extended features are shared with us by Platonov et al. (2023). Note that these new features are much more predictive of the target, as demonstrated by better performance achieved by models on the new version of the dataset.\\n\\nAmazon-users\\nThis is a known academic graph dataset for fraud detection with tabular features that has been introduced in (Zhang et al., 2020). It represents a network of customers with the edges corresponding to some sort of relation between them depending on their reviews under various products at Amazon. The original study proposed to use the relations between users based on their shared products (upu), same scores (usu), or similar content of their reviews (uvu). In this paper, we do not consider graphs with different edge types, so we chose one relation uvu as an interesting way to construct a graph that is different from other datasets in our benchmark. The set of features includes the number of rated products, review sentiment, username length, counters of positive and negative ratings, and other statistics based on user activity. The resulting graph is undirected, and the task is to predict whether a user is a fraudster. In our preliminary experiments, we found that one may achieve \\\\( \\\\sim 0.99 \\\\) ROC-AUC by using a simple graph-agnostic GBDT model, so we decided to remove the number of votes as the most important feature, which also was used for the computation of the target variable and thus may have caused an information leak.\\n\\nCity-reviews\\nThis dataset is obtained from the logs of a review service. It represents the interactions between users of the service and various organizations located in two major cities. The organizations are visited and rated by users, so the dataset is originally bipartite and contains entities of these two types. Thus, we transform it by projecting to the part of users. Let \\\\( P \\\\in \\\\{0, 1\\\\}^{m \\\\times n} \\\\) be a binary adjacency matrix of users and organizations, where \\\\( m \\\\) is the number of organizations, \\\\( n \\\\) is the number of users, and \\\\( p_{ij} \\\\) denotes whether a user \\\\( j \\\\) has left a review for an organization \\\\( i \\\\). Then, \\\\( B = P^T P \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) corresponds to the weighted adjacency matrix of users, where \\\\( b_{ij} \\\\) is the dot product of columns \\\\( i \\\\) and \\\\( j \\\\) in \\\\( P \\\\). Here, the more common rated organizations there are for two users, the greater the weight of the connection between them. Further, we obtain a binary adjacency matrix \\\\( A \\\\in \\\\{0, 1\\\\}^{n \\\\times n} \\\\) of users with \\\\( a_{ij} = \\\\lfloor b_{ij} \\\\geq \\\\gamma \\\\rfloor \\\\) by applying a threshold \\\\( \\\\gamma = 2 \\\\) to the weights \\\\( b_{ij} \\\\). The resulting graph is undirected, and the task is to predict whether a user is a fraudster. Most of the node attributes are related to the activity of users at the review service.\\n\\nHm-products\\nThis is an open-source dataset that has been introduced at the Kaggle competition organized by H&M Group (Garc\u00eda Ling et al., 2022). This dataset is originally bipartite and contains entities of two types \u2014 customers and products that they purchase at the H&M shop. Thus, we transform it by projecting to the part of products. The connections in the original dataset can be described by a weighted adjacency matrix \\\\( P \\\\in \\\\mathbb{R}^{m \\\\times n} \\\\), where \\\\( m \\\\) is the number of users, \\\\( n \\\\) is the number of products, and \\\\( p_{ij} \\\\) denotes how many times a user \\\\( i \\\\) has bought a product \\\\( j \\\\). Then, \\\\( B = P^T P \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) corresponds to the weighted adjacency matrix of products, where \\\\( b_{ij} \\\\) is the dot product of columns \\\\( i \\\\) and \\\\( j \\\\) in \\\\( P \\\\). The more often either of two products is bought by a common customer, and the more shared customers there are in general, the greater is the weight \\\\( b_{ij} \\\\) of the connection between these products. After that, we obtain a binary and more sparse adjacency matrix \\\\( A \\\\in \\\\{0, 1\\\\}^{n \\\\times n} \\\\) of products with \\\\( a_{ij} = \\\\lfloor b_{ij} \\\\geq \\\\gamma \\\\rfloor \\\\) by applying a threshold \\\\( \\\\gamma = 10 \\\\) to the weights \\\\( b_{ij} \\\\). The resulting graph is undirected. For this dataset, we consider two different versions: hm-groups with the product group as the target for the classification task and hm-prices with the average price of a product as the target for the regression task. In both cases, we adjust the set of features so that the problem does not become trivial, but the underlying graph is the same for these two versions. For the regression task, we consider such features as product types, graphical appearance, color group names, etc. For the classification task, the set of features includes average price and a reduced subset of categorical attributes from the regression task, which makes the problem more challenging.\"}"}
{"id": "Ue93J8VV3W", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics of the proposed TabGraphs benchmark.\\n\\n| Dataset         | # nodes | # edges | # leaves | avg degree | avg distance | global clustering | avg local clustering | degree assortativity | # classes | adjusted homophily | label informativeness | % labeled nodes | # num features | # bin features | # cat features |\\n|-----------------|---------|---------|----------|------------|-------------|-------------------|----------------------|---------------------|-----------|-------------------|---------------------|-----------------|---------------|----------------|---------------|\\n| toarkers-tab     | 11.8K   | 519.0K  | 419.0K   | 88.28      | 2.79        | 0.23             | 0.53                 | -0.08               | 2         | 0.09              | 0.01                | 100             | 6            | 2             | 1             |\\n| questions-tab    | 48.9K   | 153.5K  | 26.0K    | 6.28       | 4.29        | 0.02             | 0.03                 | -0.15               | 2         | 0.02              | 0.00                | 100             | 19           | 11            | 1             |\\n| amazon-users     | 11.8K   | 1.0M    | 62.3K    | 175.78     | 2.31        | 0.23             | 0.66                 | -0.11               | 2         | 0.04              | 0.01                | 93              | 22           | 1            | 0             |\\n| city-reviews     | 148.8K  | 1.2M    | 88.4K    | 15.66      | 4.89        | 0.26             | 0.41                 | -0.35               | 2         | 0.08              | 0.02                | 100             | 14           | 1            | 6             |\\n| hm-groups        | 46.5K   | 11.0M   | 4.0K     | 460.92     | 11.55       | 0.27             | 0.85                 | -0.30               | 2         | 0.00              | 0.00                | 100             | 1             | 4             | 11            |\\n| city-roads       | 76.3K   | 325.8K  | 4.0K     | 460.92     | 11.55       | 0.27             | 0.85                 | -0.30               | 2         | 0.00              | 0.00                | 100             | 1             | 4             | 11            |\\n| avazu-devices    | 46.5K   | 11.0M   | 4.0K     | 460.92     | 11.55       | 0.27             | 0.85                 | -0.30               | 2         | 0.00              | 0.00                | 100             | 1             | 4             | 11            |\\n\\nThis dataset is obtained from the logs of a navigation service and represents the road network of a major city. In this dataset, city roads are considered as graph nodes, and there is a directed edge from a node $i$ to another node $j$, if the corresponding roads are incident to the same crossing, and moving from $i$ to $j$ is permitted by traffic rules. Thus, the obtained graph is directed, and we extract its largest weakly connected component. The set of features includes such attributes as the length of roads, the number of segments they consist of, the speed limits, various categories related to the properties of road surface, and numerous indicators describing the permission for different types of vehicles. The task is to predict the travel speed on roads at a specific timestamp.\\n\\nThis is another open-source dataset that has been introduced at the Kaggle competition organized by Avazu (Wang & Cukierski, 2014). It represents the interactions between devices and advertisements on the internet. This dataset is originally bipartite and contains entities of three types \u2014 devices, sites that are visited by these devices, and applications that are used to visit them. A version of this dataset has been used by Ivanov & Prokhorenkova (2021) in their study. However, it contained only a small subset of interactions from the original dataset, which resulted in a small-sized graph. Because of that, we decided to consider the whole dataset and transform it by projecting to the part of devices. Let $P \\\\in \\\\mathbb{R}^{m \\\\times n}$ be a weighted adjacency matrix of devices and entry points defined as the combinations of sites and applications, where $m$ is the number of entry points, $n$ is the number of devices, and $p_{ij}$ denotes how many times device $j$ has used entry point $i$ (i.e., visited a specific site under a specific application). Then, $B = P^T P \\\\in \\\\mathbb{R}^{n \\\\times n}$ corresponds to the weighted adjacency matrix of devices, where $b_{ij}$ is the dot product of columns $i$ and $j$ in $P$. The interpretation of this matrix is similar to what we discussed above for hm-products. Finally, we obtain a binary adjacency matrix $A \\\\in \\\\{0, 1\\\\}^{n \\\\times n}$ of devices with $a_{ij} = \\\\left[ b_{ij} \\\\geq \\\\gamma \\\\right]$ by applying a threshold $\\\\gamma = 1000$ to the weights $b_{ij}$. The resulting network is undirected. The set of features includes device model and type, banner positions, and numerous categorical features that have been already anonymized before being released to public access. The task is to predict the click-through rate (CTR) observed on devices.\"}"}
{"id": "Ue93J8VV3W", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A key property of our benchmark is its diversity. As described above, our graphs come from different domains and have different prediction tasks. Their edges are also constructed in different ways (based on user interactions, activity similarity, etc.). However, the proposed datasets also differ in many other ways. Some properties of our graphs are presented in Table 1. First, note that the sizes of our graphs range from $11K$ to $170K$ nodes. The smaller graphs can be suitable for compute-intensive models, while the larger graphs can provide a scaling challenge. The average degree of our graphs also varies significantly \u2014 most graphs have the average degree of tens or hundreds, which is much larger than for most datasets used in graph machine learning research; however, we also have some sparser graphs such as questions-tab and city-roads. The average distance between two nodes in our graphs differs from 2.31 in amazon-users to 115.62 in city-roads. Further, we report the values of clustering coefficients which show how typical closed node triplets are for the graph. In the literature, there are two definitions of clustering coefficients (Boccaletti et al., 2014): global and average local ones. We have graphs where both clustering coefficients are high or almost zero. The degree assortativity coefficient is defined as the Pearson correlation coefficient of degrees among pairs of linked nodes. Most of our graphs have negative degree assortativity, which means that nodes tend to connect to other nodes with dissimilar degrees, while for the city-roads dataset the degree assortativity is positive and large.\\n\\nFurther, let us discuss the graph-label relations in our datasets. A classification dataset is considered homophilous, if nodes tend to connect to nodes of the same class, and it is considered heterophilous if nodes tend to connect to nodes of different classes. We use adjusted homophily to characterize homophily level, as it has been shown to have more desirable properties than other homophily measures used in the literature (Platonov et al., 2022). We can see that city-reviews dataset is homophilous, while the rest of our classification datasets are heterophilous. One more characteristic to describe graph-label relationships is label informativeness (Platonov et al., 2022). It shows how much information about the label of a given node can be derived from the labels of neighbor nodes. In our dataset, label informativeness correlates with adjusted homophily, which is typical for labeled graphs. To measure the similarity of labels for regression datasets, we use the target assortativity coefficient \u2014 the Pearson correlation coefficient of target values between pairs of connected nodes. For instance, on the city-roads dataset, the target assortativity is positive, while for the other two regression datasets it is negative.\\n\\nNote that some of our graphs contain unlabeled nodes. This is a typical situation for industry and science, yet it is underrepresented in graph machine learning benchmarks. Unlabeled nodes give an additional advantage to graph-aware models, as they can utilize the information about the features of these nodes and their position in the graph even without knowing their labels.\\n\\nFinally, our datasets have tabular features with different numbers and balance of numerical, binary, and categorical attributes. The numerical features have widely different scales and distributions. For example, in questions-tab dataset, most of the features are counters (subscribers count, achievements count, articles count) which have Poisson-like distributions, while the rating feature has a very different distribution with possible negative values and lots of outliers.\\n\\nOverall, our datasets are diverse in domain, scale, structural properties, graph-label relations, and node attributes. Providing meaningful prediction tasks, they may serve as a valuable tool for the research and development of machine learning models for processing graph-structured data with heterogeneous features.\\n\\n### 4 BASELINE MODELS\\n\\nHere, we briefly discuss the machine learning methods for working with tabular features and graph-structured data that we evaluate in our experiments.\\n\\n**A simple baseline**\\n\\nAs a simple baseline, we use a ResNet-like model (He et al., 2016). It does not have any information about the graph structure and operates on nodes as independent samples (we call such models graph-agnostic). This model also does not have any specific designs for working with tabular features.\"}"}
{"id": "Ue93J8VV3W", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Classical tabular models\\n\\nWe consider three most popular implementations of GBDT: XGBoost (Chen & Guestrin, 2016), LightGBM (Ke et al., 2017), and CatBoost (Prokhorenkova et al., 2018). These models are also graph-agnostic.\\n\\nTabular deep learning models\\n\\nWe use two graph-agnostic deep learning models that have been recently proposed for working with tabular data. First, we use the numerical feature embedding technique introduced by Gorishniy et al. (2022). This technique adds a learnable module that transforms numerical features in such a way that they can be better processed by neural networks. In particular, we use a simple MLP with PLR (Periodic-Linear-ReLU) numeric feature embeddings as this combination has shown the best performance in the original study. We refer to this model as MLP-PLR. Further, we consider retrieval-augmented models for tabular data and use the recently proposed TabR model (Gorishniy et al., 2023) as it has shown strong performance on a wide range of tabular datasets. Note that TabR also uses PLR numerical feature embeddings.\\n\\nGraph machine learning models\\n\\nWe consider several representative GNN architectures for our experiments. First, we use GCN (Kipf & Welling, 2016) and GraphSAGE (Hamilton et al., 2017) as simple classical GNN models. Further, we use two attention-augmented GNNs: GAT (Veli\u0107 et al., 2017) and Graph Transformer (GT) (Shi et al., 2020), along with their simple modifications GAT-sep and GT-sep that add ego-embedding and neighborhood-aggregated-embedding separation to the models, as proposed in Platonov et al. (2023). We equip all GNNs with skip-connections (He et al., 2016) and layer normalization (Ba et al., 2016), which we found very important for the stability and strong performance. Our GNNs are implemented in the same codebase as ResNet\u2014we simply swap each residual block of ResNet with the residual neighborhood aggregation block of the selected GNN architecture. Thus, comparing the performance of ResNet and GNNs is the most direct way to see if graph information is helpful for the task.\\n\\nSpecialized models\\n\\nWe also use two recently proposed methods for solving node property prediction problems on graphs with heterogeneous features: BGNN (Ivanov & Prokhorenkova, 2021) and EBBS (Chen et al., 2022). Unfortunately, the official implementation of EBBS does not work for node classification datasets, thus we only evaluate this method on node regression tasks.\\n\\n5 EXPERIMENTS\\n\\nSetup\\n\\nIn our experiments, the labeled nodes are split into train, validation, and test parts in proportion 50 : 25 : 25. We use stratified split for classification datasets and random split for regression datasets. We report ROC-AUC for binary classification, Accuracy for multi-class classification, and $R^2$ (coefficient of determination) for regression. We run each model 5 times and report the mean and standard deviation of each metric value.\\n\\nFor GBDT and tabular deep learning models, we conduct an extensive hyperparameter search using Optuna (Akiba et al., 2019) (see Appendix D for the details about the search space), whereas for specialized model we follow the recommendations from the original studies. In particular, we choose a default set of hyperparameters for EBBS in regression task, as the authors claim that they should work well across different tasks and graph datasets; for BGNN, we use the experimental pipeline proposed in the official implementation of this method, which performs hyperparameter tuning over a pre-defined grid of values.\\n\\nFor GNNs, we found that, when augmented with skip-connections and layer normalization, they are not very sensitive to the hyperparameter choice, thus we did not tune them for graph neural models. The set of hyperparameter used in our experiments is described in Appendix C.\\n\\nTo conduct experiments with GBDT and tabular deep learning models, we use the source code from TabR repository (Gorishniy et al., 2023), while for the two specialized models BGNN and EBBS, we use their corresponding repositories (Ivanov & Prokhorenkova, 2021; Chen et al., 2022).\\n\\nWhen using deep learning models for tabular data, the pre-processing of numerical features is critically important. In our preliminary experiments, we found that quantile transformation to normal distribution typically works best, thus we used it in all our experiments. For categorical features, we used one-hot encoding for all models, except for CatBoost, which is designed specifically to handle such features.\"}"}
{"id": "Ue93J8VV3W", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Results for the remaining datasets. Best results are marked with cyan.\\n\\nTable 2: Classification results. Accuracy is reported for\\n\\n| Model             | ROC-AUC | ROQ-AUC | MCC  |\\n|-------------------|---------|---------|------|\\n| LightGBM          | 78 \u00b1 0  | 61 \u00b1 0  | 0.44 |\\n| XGBoost           | 69 \u00b1 0  | 48 \u00b1 0  | 0.59 |\\n| CatBoost          | 77 \u00b1 0  | 71 \u00b1 0  | 0.52 |\\n| MLP-PLR           | 82 \u00b1 0  | 74 \u00b1 0  | 0.54 |\\n| TabR              | 84 \u00b1 0  | 78 \u00b1 0  | 0.56 |\\n| GraphSAGE         | 85 \u00b1 0  | 79 \u00b1 0  | 0.58 |\\n| GraphSAGE-PLR     | 85 \u00b1 0  | 84 \u00b1 0  | 0.59 |\\n| GAT-PLR           | 85 \u00b1 0  | 79 \u00b1 0  | 0.56 |\\n| GAT-sep-PLR       | 86 \u00b1 0  | 84 \u00b1 0  | 0.59 |\\n| GT-PLR            | 84 \u00b1 0  | 77 \u00b1 0  | 0.52 |\\n| GT-sep-PLR        | 83 \u00b1 0  | 76 \u00b1 0  | 0.51 |\\n| GCN-PLR           | 85 \u00b1 0  | 79 \u00b1 0  | 0.56 |\\n| GCN               | 87 \u00b1 0  | 81 \u00b1 0  | 0.58 |\\n| GT                 | 82 \u00b1 0  | 75 \u00b1 0  | 0.52 |\\n| GT-sep            | 84 \u00b1 0  | 78 \u00b1 0  | 0.56 |\\n| GCN                 | 85 \u00b1 0  | 79 \u00b1 0  | 0.56 |\\n| GT-PLR            | 83 \u00b1 0  | 76 \u00b1 0  | 0.51 |\\n| GT-sep-PLR        | 86 \u00b1 0  | 84 \u00b1 0  | 0.59 |\\n| GCN-PLR           | 86 \u00b1 0  | 80 \u00b1 0  | 0.58 |\\n| GCN               | 75 \u00b1 0  | 68 \u00b1 0  | 0.44 |\\n| ResNet            | 74 \u00b1 0  | 65 \u00b1 0  | 0.43 |\\n| vanilla ResNet     | 74 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\\n| vanilla GNN        | 75 \u00b1 0  | 65 \u00b1 0  | 0.44 |\"}"}
{"id": "Ue93J8VV3W", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Regression results. $R^2$ is reported for all datasets. OOM denotes the only experiment that could not be carried out due to the GPU memory constraints. Best results are marked with cyan.\\n\\n| Method     | $R^2$ \u00b1 | Best Result |\\n|------------|---------|-------------|\\n| city-roads | 85.40 \u00b1 0.40 | 20.33 \u00b1 0.29 |\\n| avazu-devices | 67.94 \u00b1 0.54 | 64.06 \u00b1 1.17 |\\n| hm-prices  | 74.83 \u00b1 0.83 | 58.76 \u00b1 1.08 |\\n| ResNet     | 51.85 \u00b1 0.75 | 25.07 \u00b1 0.02 |\\n| XGBoost    | 57.07 \u00b1 0.07 | 25.07 \u00b1 0.02 |\\n| LightGBM   | 56.35 \u00b1 0.06 | 24.69 \u00b1 0.04 |\\n| CatBoost   | 56.68 \u00b1 0.12 | 26.01 \u00b1 0.18 |\\n| MLP-PLR    | 56.88 \u00b1 0.09 | 23.08 \u00b1 0.36 |\\n| TabR       | 56.44 \u00b1 0.07 | OOM          |\\n| GCN        | 50.86 \u00b1 1.07 | 20.99 \u00b1 0.19 |\\n| GraphSAGE  | 57.79 \u00b1 0.55 | 26.58 \u00b1 0.29 |\\n| GAT        | 53.44 \u00b1 0.75 | 24.92 \u00b1 0.44 |\\n| GAT-sep    | 57.25 \u00b1 0.44 | 25.49 \u00b1 0.14 |\\n| GT         | 55.92 \u00b1 0.35 | 27.07 \u00b1 0.17 |\\n| GT-sep     | 58.49 \u00b1 0.38 | 25.46 \u00b1 0.74 |\\n| GCN-PLR    | 48.02 \u00b1 0.91 | 20.82 \u00b1 0.55 |\\n| GraphSAGE-PLR | 57.27 \u00b1 0.90 | 26.58 \u00b1 0.29 |\\n| GAT-PLR    | 54.94 \u00b1 0.86 | 24.88 \u00b1 0.31 |\\n| GAT-sep-PLR | 57.57 \u00b1 0.60 | 24.93 \u00b1 0.29 |\\n| GT-PLR     | 57.75 \u00b1 0.37 | 25.19 \u00b1 0.65 |\\n| GT-sep-PLR | 59.49 \u00b1 0.37 | 23.39 \u00b1 0.30 |\\n| BGNN       | 57.87 \u00b1 1.73 | 21.81 \u00b1 0.33 |\\n| EBBS       | 25.28 \u00b1 1.99 | 11.94 \u00b1 0.04 |\\n\\nBased on these results, we make the following recommendations for researchers and practitioners working with tabular data:\\n\\n\u2022 If it is possible to define meaningful relations between data samples, it is worth trying to convert the data to a graph and experimenting with graph machine learning methods \u2014 it can lead to significant gains.\\n\u2022 The best GNN architecture for graphs with tabular features strongly depends on the specific dataset, and it is important to try different architectures.\\n\u2022 Standard GNNs generally perform better than the recently proposed models designed specifically for graphs with tabular features.\\n\u2022 PLR embeddings for numerical features are a simple modification that can significantly improve the performance of GNNs on graphs with tabular features, and it is worth trying to incorporate them in graph neural architecture.\\n\\nCONCLUSION\\n\\nIn this work, we introduce TabGraphs \u2014 the first benchmark of graph datasets with heterogeneous tabular node features. Our datasets are diverse and have realistic prediction tasks. We have evaluated a large number of methods from tabular machine learning and graph machine learning on this benchmark, including models that combine techniques from both fields. Based on our experimental findings, we provide recommendations and insights for researchers and practitioners working with tabular and graph data. In particular, we show that transforming tabular data relations to graph structure and using graph machine learning methods can significantly increase predictive performance in many real-world tasks. We believe that our benchmark and insights obtained using it will be helpful for both industry and science and will facilitate further research on the intersection of tabular and graph machine learning.\"}"}
