{"id": "hdSn_X7Hfvz", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13: Comparison of reliability diagrams for real-world data with different scenarios of simulated data. For the cancer survival dataset, the empirical probabilities are clustered in around (0.4-0.6), similar to the \\\\textit{Centered} scenario. For the weather forecasting dataset, the probabilities are uniformly distributed across 0.1-0.8, similar to the \\\\textit{Linear} scenario. For the collision prediction dataset, the majority of the output probabilities are clustered in the lower probability region, similar to the \\\\textit{Skewed} scenario.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluation metrics\\n\\nProbability estimation shares similar target labels and network outputs with binary classification. However, classification accuracy is not an appropriate metric for evaluating probability-estimation models due to the inherent uncertainty of the outcomes. This is illustrated by the example in Figure 2a where a perfect probability estimate would result in a classification accuracy of just 75%.\\n\\nMetrics when ground-truth probabilities are available. For synthetic datasets, we have access to the ground truth probability labels and can use them to evaluate performance. Two reasonable metrics are the mean squared error or $\\\\ell_2$ distance $\\\\text{MSE}_p$, and the Kullback\u2013Leibler divergence $\\\\text{KL}_p$ between the estimated and ground-truth probabilities:\\n\\n$$\\\\text{MSE}_p = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\left( \\\\hat{p}_i - p_i \\\\right)^2,$$\\n\\n$$\\\\text{KL}_p = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\left( \\\\hat{p}_i \\\\log \\\\hat{p}_i - p_i \\\\log p_i + (1 - \\\\hat{p}_i) \\\\log (1 - \\\\hat{p}_i) - (1 - p_i) \\\\log (1 - p_i) \\\\right).$$\\n\\n(1)\\n\\n$N$ is the number of data, and $p_i, \\\\hat{p}_i$ are the ground-truth and predicted probabilities respectively.\\n\\nCalibration metrics. In real-world data, ground-truth probabilities are not available. In order to evaluate the probabilities estimated by a model, we need to compare them to the observed probabilities. To this end, we aggregate the examples for which the model output equals a certain value (e.g. 0.5), and verify what fraction of them have outcomes equal to 1. If the fraction is close to the model output, then the model is said to be well calibrated.\\n\\nDefinition 3.1. A model $f$ is well calibrated if\\n\\n$$P(y = 1 | f(x) \\\\in I(q)) = q, \\\\forall 0 \\\\leq q \\\\leq 1,$$\\n\\n(2)\\n\\nwhere $y$ is the observed outcome, $f(x)$ is the probability predicted by model $f$ for input $x$, and $I(q)$ is a small interval around $q$.\\n\\nModel calibration can be evaluated using the expected calibration error (ECE) (Guo et al., 2017) (note however that the definition Guo et al. (2017) is specific to classification). Given a probability-estimation model $f$ and a dataset of input data $x_i$ and associated outcomes $y_i, 1 \\\\leq i \\\\leq N$, we partition the examples into $B$ bins, $I_1 I_2 \\\\cdots I_B$, according to the probabilities assigned to the examples by the model. Let $Q_1, \\\\ldots, Q_{B-1}$ the $B$-quantiles of the set $\\\\{f(x_1), \\\\ldots, f(x_N)\\\\}$, we have $I_b = [Q_{b-1}, Q_b] \\\\cap \\\\{f(x_i)\\\\}_{i=1}^{N}$ (setting $Q_0 = 0$). For each bin, we compute the mean predicted and empirical probabilities,\\n\\n$$\\\\hat{p}_b = \\\\frac{1}{|I_b|} \\\\sum_{i \\\\in \\\\text{Index}(I_b)} y_i,$$\\n\\n$$q_b = \\\\frac{1}{|I_b|} \\\\sum_{i \\\\in \\\\text{Index}(I_b)} f(x_i),$$\\n\\n(3)\\n\\n(4)\\n\\nA perfect model (in terms of probability estimation), assigns 0.25 to the blue class and 0.75 to the red class. To maximize classification accuracy, we predict when the model outputs 0.75 (red examples) and 0 when it outputs 0.25 (blue examples). However, 25% of red examples have an outcome of 0, and 25% of blue examples have an outcome of 1. As a result, the model would only have 75% accuracy.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating evaluation metrics. We use synthetic data to compare different metrics to the gold-standard MSE that uses ground-truth probabilities. Brier score is highly correlated with MSE, in contrast to the classification metric AUC and the calibration metrics ECE, MCE and KS-Error. The graphs show the results of the proposed method CaPE, as well as the baselines described in Section 6.3 on the Linear scenario (see Section 6.1. Other scenarios and a similar comparison with KL are included in Appendix D.)\\n\\nThe pairs \\\\( q(b), p(b)_{emp} \\\\) can be plotted as a reliability diagram, shown in the second row of Figure 4 and in Figure 6. ECE is then defined as\\n\\n\\\\[\\nECE = \\\\frac{1}{B} \\\\sum_{b=1}^{B} |p(b)_{emp} - q(b)|.\\n\\\\]\\n\\n(5)\\n\\nOther metrics for calibration include the maximum calibration error (MCE) defined as\\n\\n\\\\[\\nMCE = \\\\max_{b=1, \\\\ldots, B} |p(b)_{emp} - q(b)|,\\n\\\\]\\n\\nand the Kolmogorov-Smirnov error (KS-error) (Gupta et al., 2021), a metric based on the cumulative distribution function, which is described in more detail in Appendix B.\\n\\nBrier score. Crucially, a model without any discriminative power can be perfectly calibrated (see Figure 2b). The Brier score is a metric designed to evaluate both calibration and discriminative ability. It is the mean squared error between the predicted probability and the observed outcomes:\\n\\n\\\\[\\n\\\\text{Brier} = \\\\frac{1}{N} \\\\sum_{i=1}^{N} (\\\\hat{p}_i - y_i)^2.\\n\\\\]\\n\\n(6)\\n\\nThis score can be decomposed into two terms associated to calibration and discrimination ability, as shown in Appendix C. Using the synthetic data in Section 6.1, where the ground-truth probabilities are known, we show that Brier score is indeed a reliable proxy for gold-standard MSE metric based on ground-truth probabilities MSE, in contrast to calibration metrics such as ECE, MCE or KS-error, and to classification metrics such as AUC (see Figure 3 and Appendix D).\\n\\n4 PROPOSED METHOD: CALIBRATED PROBABILITY ESTIMATION (CAPE)\\n\\nPrediction models based on deep learning are typically trained by minimizing the cross entropy between the model output and the training labels (Goodfellow et al., 2016). This cost function is a proper scoring rule, which means that it evaluates probability estimates in a consistent manner and is therefore guaranteed to be well calibrated in an infinite-data regime (Buja et al., 2005), as illustrated by Figure 4 (first column).\\n\\nUnfortunately, in practice prediction models are trained on finite data. This is crucial in the case of deep neural networks, because these models are highly overparametrized and therefore prone to overfitting (Goodfellow et al., 2016). In classification, networks have been shown to be capable of fitting arbitrary random labels (Zhang et al., 2017a). In probability estimation, we observe that neural networks indeed eventually overfit the observed outcomes completely. Moreover, the estimated probabilities collapse to 0 or 1 (Figure 4, second column), a phenomenon that has also been reported in classification (Mukhoti et al., 2020). However, calibration is preserved during the first stages of training (Figure 4, third column). This is reminiscent of the early-learning phenomenon observed...\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Miscalibration due to overfitting and how to avoid it. When trained on infinite data (i.e. resampling outcome labels at each epoch according to ground-truth probabilities), models minimizing cross-entropy are well calibrated (first column). The top row shows results for the synthetic Discrete scenario (see Section 6.1) (top). The bottom row shows results for the Linear scenario (dashed line indicates perfect calibration). However, when trained on fixed observed outcomes, the model eventually overfits and the probabilities collapse to either 0 or 1 (second column). This is mitigated via early stopping (i.e. selecting the model based on validation cross-entropy loss), which yields relatively good calibration (third column). The proposed Calibration Probability Estimation (CaPE) method exploits this to further improve the model while ensuring that the output remains well calibrated. Appendix A.3 shows plots for all synthetic data scenarios.\\n\\nfor classification from partially corrupted labels (Yao et al., 2020; Xia et al., 2020), where neural networks learn from the correct labels before eventually overfitting the false ones (Liu et al., 2020). Here, we propose to exploit the training dynamics of cross-entropy minimization through a method that we name Calibrated Probability Estimation (CaPE). Our starting point is a model obtained via early stopping using validation data on the cross-entropy loss. CaPE is designed to further improve the discrimination ability of the model, while ensuring that it remains well calibrated. This is achieved by alternatively minimizing the following two loss functions:\\n\\n**Discrimination loss**\\n\\n\\\\[\\nL_D = -\\\\sum_{i=1}^{N} \\\\left[ y_i \\\\log(f(x_i)) + (1 - y_i) \\\\log(1 - f(x_i)) \\\\right].\\n\\\\]\\n\\n**Calibration loss**\\n\\n\\\\[\\nL_C = -\\\\sum_{i=1}^{N} \\\\left[ p_{\\\\text{emp}}(f(x_i)) \\\\log(f(x_i)) + (1 - p_{\\\\text{emp}}(f(x_i))) \\\\log(1 - f(x_i)) \\\\right],\\n\\\\]\\n\\nwhere \\\\( p_{\\\\text{emp}} \\\\) is an estimate of the conditional probability \\\\( P[y = 1 | f(x_i) \\\\in I(f(x_i))] \\\\) where \\\\( I(f(x_i)) \\\\) is a small interval centered at \\\\( f(x_i) \\\\). As explained in Section 3 if \\\\( f(x_i) \\\\) is close to this value, then the model is well calibrated. We consider two approaches for estimating \\\\( p_{\\\\text{emp}} \\\\). (1) CaPE (bin) where we divide the training set into bins, select the bin \\\\( b_i \\\\) containing \\\\( f(x_i) \\\\) and set \\\\( p_{\\\\text{emp}} = p_{\\\\text{emp}}(b_i) \\\\) in equation 3. (2) CaPE (kernel) where \\\\( p_{\\\\text{emp}} \\\\) is estimated through a moving average with a kernel function (see Appendix E for more details). Both methods are efficiently computed by sorting the predictions \\\\( \\\\hat{p}_i \\\\). The calibration loss requires a reasonable estimation of the empirical probabilities \\\\( p_{\\\\text{emp}}(i) \\\\), which can be obtained from the model after early learning. Therefore using the calibration loss from the beginning is counterproductive, as demonstrated in Section J.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Cross-Entropy minimization CaPE CaPE begins\\nTraining loss Validation loss\\n\\nFigure 5: Calibrated Probability Estimation prevents overfitting.\\nComparison between the learning curves of cross-entropy (CE) minimization and the proposed calibrated probability estimation (CaPE), smoothed with a 5-epoch moving average. After an early-learning stage where both training and validation losses decrease, CE minimization overfits (first and second graph), with disastrous consequences in terms of probability estimation (third and fourth graph). In contrast, CaPE prevents overfitting, continuing to improve the model while maintaining calibration (see Figure 4).\\n\\nAlgorithm 1\\nPseudocode for CaPE\\n\\nRequire:\\n\\\\( f \\\\) \\\\( \\\\bowtie \\\\) early stopped model\\nRequire:\\n\\\\( m \\\\) \\\\( \\\\bowtie \\\\) freq. of training with \\\\( L \\\\)\\nRequire:\\n\\\\( \\\\{ x_i, y_i \\\\}_{i=1}^{N} \\\\) \\\\( \\\\bowtie \\\\) training set\\nRequire:\\n\\\\( K(p,q) := \\\\exp\\\\left[-\\\\frac{(p-q)^2}{\\\\sigma^2}\\\\right] \\\\) \\\\( \\\\bowtie \\\\) Gaussian kernel\\n\\nfor \\\\( t = 1 \\\\) to num_epochs do\\n    if \\\\( t \\\\mod m = 0 \\\\) then\\n        \\\\( \\\\hat{p}_i \\\\leftarrow f(x_i), \\\\forall i \\\\)\\n        Update \\\\( p_{\\\\text{emp}}, \\\\forall i \\\\), with \\\\( B\\\\text{IN} \\\\) or \\\\( K\\\\text{ERNEL} \\\\)\\n    else\\n        \\\\( L \\\\leftarrow L_D \\\\) \\\\( \\\\bowtie \\\\) compute calibration loss\\n    end if\\n    \\\\( f \\\\leftarrow \\\\text{backprop with } L \\\\) \\\\( \\\\bowtie \\\\) train with loss\\nend for\\n\\nfunction \\\\( B\\\\text{IN}(B) \\\\) \\\\( \\\\bowtie \\\\) \\\\( B \\\\)-number of bins\\n    \\\\( I_1, \\\\ldots, I_B \\\\leftarrow \\\\text{partitions by quantile of } \\\\{ \\\\hat{p}_j \\\\}_{j=1}^{N} \\\\) \\\\( \\\\bowtie \\\\) get indices in bin \\\\( b \\\\)\\n    \\\\( \\\\text{Index}(I_b) \\\\leftarrow \\\\{ j | \\\\hat{p}_j \\\\in I_b \\\\} \\\\) \\\\( \\\\bowtie \\\\) get indices in bin \\\\( b \\\\)\\n    \\\\( p_{\\\\text{emp}} \\\\leftarrow \\\\frac{1}{|I_b|} \\\\sum_{i \\\\in \\\\text{Index}(I_b)} y_i \\\\) \\\\( \\\\bowtie \\\\) empirical mean of bin \\\\( b \\\\)\\nend function\\n\\nfunction \\\\( K\\\\text{ERNEL}(r, K) \\\\) \\\\( \\\\bowtie \\\\) \\\\( r \\\\)-window size; kernel\\n    \\\\( N_r(i) \\\\leftarrow r \\\\)-nearest neighbor of \\\\( \\\\hat{p}_i \\\\) (output probability space)\\n    \\\\( Z \\\\leftarrow \\\\sum_{\\\\hat{p}_j \\\\in N_r(i)} K(\\\\hat{p}_i, \\\\hat{p}_j) \\\\) \\\\( \\\\bowtie \\\\) normalization factor\\n    \\\\( p_{\\\\text{emp}} \\\\leftarrow \\\\frac{1}{Z} \\\\sum_{\\\\hat{p}_j \\\\in N_r(i)} K(\\\\hat{p}_i, \\\\hat{p}_j) y_j \\\\) \\\\( \\\\bowtie \\\\) kernel smooth\\nend function\\n\\nCaPE is summarized in Algorithm 1. Figures 4 and 5 show that incorporating the calibration-loss minimization step indeed preserves calibration as training proceeds (this is not necessarily expected because CaPE minimizes a calibration loss on the training data), and prevents the model from overfitting the observed outputs. This is beneficial also for the discriminative ability of the model, because it enables it to further reduce the cross-entropy loss without overfitting, as shown in Figure 5.\\n\\nThe experiments with synthetic and real-world data reported in Section 6 suggest that this approach results in accurate probability estimates across a variety of realistic scenarios.\\n\\n5 RELATED WORK\\n\\nNeural networks trained for classification often generate a probability associated with their prediction which quantifies its uncertainty. These estimates are often found to be inaccurate (Mukhoti et al., 2020; Guo et al., 2017). Techniques mitigating this issue are often described as calibration methods, and broadly fall into three categories depending on whether they: (1) postprocess the outputs of a trained model, (2) combine multiple model outputs, or (3) modify the training process.\\n\\nPost-processing methods transform the output probabilities in order to improve calibration on held-out data (Zadrozny & Elkan, 2001; Gupta et al., 2021; Kull et al., 2017; 2019). For example, Platt scaling (Platt, 1999) fits a logistic function that minimizes the negative log-likelihood loss. Temperature scaling (Guo et al., 2017) does the same with a temperature parameter augmenting the softmax function. In contrast to these methods, CaPE enforces calibration during training, which has the advantage of enabling further improvements in the discriminative abilities of the model.\\n\\nEnsembling methods combine multiple models to improve generalization. Mix-n-Match (Zhang et al., 2020) uses a single model, and ensembles predictions using multiple temperature scaling transformations. Other methods (Lakshminarayanan et al., 2017; Maddox et al., 2019) ensemble\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nMethods\\n\\nLinear Sigmoid Centered Skewed Discrete\\n\\n\\\\[\\\\times 10^{-2}\\\\]\\n\\n\\\\[\\\\text{MSE} \\\\quad \\\\text{KL} \\\\quad \\\\text{MSE} \\\\quad \\\\text{KL} \\\\quad \\\\text{MSE} \\\\quad \\\\text{KL} \\\\quad \\\\text{MSE} \\\\quad \\\\text{KL} \\\\quad \\\\text{MSE} \\\\quad \\\\text{KL}\\\\]\\n\\n| Method                  | MSE 1.13 | MSE 2.81 | MSE 5.35 | MSE 14.86 |\\n|-------------------------|----------|----------|----------|-----------|\\n| Infinite Data*          | 0.20     | 0.41     | 0.22     | 0.92      |\\n| CE early-stop           | 2.24     | 5.26     | 1.52     | 3.64      |\\n| Temperature             | 0.48     | 0.98     | 0.40     | 1.79      |\\n| Platt Scaling           | 2.74     | 6.52     | 1.71     | 4.82      |\\n| Dirichlet Cal.          | 3.56     | 9.06     | 8.71     | 25.33     |\\n| Mix-n-match             | 2.69     | 6.70     | 6.13     | 17.10     |\\n| Focal Loss              | 4.13     | 10.51    | 6.89     | 19.51     |\\n| Entropy Reg.            | 2.84     | 7.37     | 7.03     | 21.19     |\\n| MMCE Reg.               | 2.22     | 5.65     | 5.33     | 15.03     |\\n| Deep Ensemble           | 1.90     | 4.55     | 5.85     | 16.43     |\\n| CaPE (bin)              | 1.83     | 4.46     | 5.29     | 14.59     |\\n| CaPE (kernel)           | 1.81     | 4.41     | 5.22     | 14.47     |\\n\\nTable 1: Results on synthetic data. Appendix A.1 shows a table with confidence intervals using bootstrapping. * is a model trained with infinite data obtained by continuous label resampling.\\n\\nMultiple models obtained using different initializations. These approaches are compatible with the proposed method CaPE; how to combine them effectively is an interesting future research direction.\\n\\nModified training methods can be divided into two groups. The first group smooths the target 0/1 labels in order to prevent output estimates from collapsing to 0/1 (Mukhoti et al., 2020; Szegedy et al., 2016; Zhang et al., 2018; Thulasidasan et al., 2020). The second group attaches additional calibration penalties to a cross entropy loss (Kumar et al., 2018; Pereyra et al., 2017; Liang et al., 2020). CaPE is most similar in spirit to the latter methods, although its data-driven calibration loss is different to the penalties used in these techniques.\\n\\nDatasets for evaluation\\n\\nThe methods discussed in this section were developed for calibration in classification, and tested on datasets such as CIFAR-10/100 (Krizhevsky, 2009), SVHN (Netzer et al., 2011), and ImageNet (Deng et al., 2009) where the relationship between labels and input data is completely deterministic. Here, we evaluate these methods on synthetic and real-world probability-estimation problems with inherent uncertainty.\\n\\n6 EXPERIMENTS\\n\\n6.1 SYNTHETIC DATASET: FACE-BASED RISK PREDICTION\\n\\nTo benchmark probability-estimation methods, we build a synthetic dataset based on UTK-Face (Zhang et al., 2017b), containing face images and associated ages. We use the age of the \\\\(i\\\\)th person \\\\(z_i\\\\) to assign them a risk of contracting a disease \\\\(p_i = \\\\psi(z_i)\\\\) for a fixed function \\\\(\\\\psi: \\\\mathbb{N} \\\\rightarrow [0, 1]\\\\).\\n\\nThen we simulate whether the person actually contracts the illness (label \\\\(y_i = 1\\\\)) or not (\\\\(y_i = 0\\\\)) with probability \\\\(p_i\\\\). The probability-estimation task is to estimate the ground-truth probability \\\\(p_i\\\\) from the face image \\\\(x_i\\\\), which requires learning to discriminate age and map it to the corresponding risk. We design \\\\(\\\\psi\\\\) to create five scenarios, inspired by real-world data (see Appendix G):\\n\\n- Linear: Equally-spaced, inspired by weather forecasting: \\\\(\\\\psi(z) = z/100\\\\)\\n- Sigmoid: Concentrated near two extremes: \\\\(\\\\psi(z) = \\\\sigma(25(z/100 - 0.29))\\\\)\\n- Skewed: Clustered close to zero, inspired by vehicle-collision detection: \\\\(\\\\psi(z) = z/250\\\\)\\n- Centered: Clustered in the center, inspired by cancer-survival prediction: \\\\(\\\\psi(z) = z/300 + 0.35\\\\)\\n- Discrete: Discretized: \\\\(\\\\psi(z) = \\\\begin{cases} 0.2 & \\\\text{if } z > 20 \\\\\\\\ 1 & \\\\text{if } z > 40 \\\\\\\\ 1 & \\\\text{if } z > 60 \\\\\\\\ 1 & \\\\text{if } z > 80 \\\\\\\\ 0.1 & \\\\end{cases}\\\\)\\n\\n6.2 REAL-WORLD DATASETS\\n\\nWe propose to use three open-source, real-world datasets to benchmark probability-estimation approaches (see Appendix H for further details on the datasets and experiments).\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Method: Cancer Survival, Weather Forecasting, Collision Prediction\\n\\n| Method            | AUC  | ECE  | Brier |\\n|-------------------|------|------|-------|\\n| AUC               | 58.88| 12.25| 23.96 |\\n| ECE               | 77.64| 10.91| 20.57 |\\n| Brier             | 85.68| 4.36 | 8.59  |\\n| CE early-stop     | 58.88| 12.07| 23.73 |\\n|                   | 77.64| 8.66 | 20.21 |\\n|                   | 85.68| 4.56 | 8.51  |\\n| Temperature       | 58.91| 10.28| 23.33 |\\n|                   | 77.65| 6.97 | 19.53 |\\n|                   | 85.76| 3.04 | 8.23  |\\n| Platt Scaling     | 49.89| 13.83| 24.08 |\\n|                   | 77.51| 14.29| 21.89 |\\n|                   | 83.36| 5.78 | 8.78  |\\n| Dirichlet Cal.    | 58.88| 12.16| 23.67 |\\n|                   | 77.64| 8.65 | 20.21 |\\n|                   | 85.68| 4.40 | 8.52  |\\n| Mix-n-match       | 55.02| 12.15| 23.31 |\\n|                   | 76.18| 8.32 | 20.27 |\\n|                   | 82.21| 9.07 | 9.82  |\\n| Focal Loss        | 56.29| 11.73| 23.62 |\\n|                   | 79.01| 10.53| 19.77 |\\n|                   | 83.15| 14.54| 11.10 |\\n| Entropy Reg.      | 48.45| 11.84| 23.73 |\\n|                   | 76.69| 8.46 | 20.12 |\\n| MMCE Reg.         | 48.45| 11.84| 23.73 |\\n|                   | 76.69| 8.46 | 20.12 |\\n|                   | 85.18| 2.94 | 8.48  |\\n| Deep Ensemble     | 52.46| 9.99 | 23.47 |\\n|                   | 79.86| 7.41 | 18.82 |\\n|                   | 85.27| 3.15 | 8.55  |\\n| CaPE (bin)        | 61.44| 12.31| 23.20 |\\n|                   | 78.99| 5.16 | 18.37 |\\n|                   | 85.70| 3.16 | 8.18  |\\n| CaPE (kernel)     | 61.22| 9.48 | 23.18 |\\n|                   | 79.00| 5.08 | 18.39 |\\n|                   | 85.95| 3.22 | 8.13  |\\n| Table 2: Results on cancer-survival prediction, weather forecasting, and collision prediction. Tables with all the metrics described in Section 3 are provided in Appendix A.2\\n\\nSurvival of Cancer Patients.\\n\\nHistopathology aims to identify tumor cells, cancer subtypes, and the stage and level of differentiation of cancer. Hematoxylin and Eosin (H&E)-stained slides are the most common type of histopathology data used for clinical decision making. In particular, they can be used for survival prediction (Wulczyn et al., 2020), which is critical in evaluating the prognosis of patients. Treatments assigned to patients after diagnosis are not personalized and their impact on cancer trajectory is complex, so the survival status of a patient is not deterministic. In this work, we use the H&E slides of non-small cell lung cancers from The Cancer Genome Atlas (TCGA) to estimate the 5-year survival probability of cancer patients. The outcome distribution is similar to the Centered scenario in our synthetic data.\\n\\nWeather Forecasting.\\n\\nThe atmosphere is governed by nonlinear dynamics, hence weather forecast models possess inherent uncertainties (Richardson, 2007). Nowcasting, weather prediction in the near future, is of great operational significance, especially with increasing number of extreme inclement weather conditions (Agrawal et al., 2019; Ravuri et al., 2021). We use the German Weather service dataset, which contains quality-controlled rainfall-depth composites from 17 operational Doppler radars. We use 30 minutes of precipitation data to predict if the mean precipitation will increase or decrease after one hour. Three precipitation maps from the past 30 minutes serve as an input. The outcome distribution is similar to the Linear scenario in our synthetic data.\\n\\nCollision Prediction.\\n\\nVehicle collision is one of the leading causes of death in the world. Reliable collision prediction systems are therefore instrumental in saving human lives. These systems predict potential collisions from dashcam cameras. Collisions are influenced by many unknown factors, and hence are not deterministic. Following Kim et al. (2019), we use 0.3 seconds of real dashcam videos from YouTubeCrash dataset as input, and predict the probability of a collision in the next 2 seconds. The data are very imbalanced as the number of collisions is very low, so the outcome distribution is similar to the Skewed scenario in our synthetic data.\\n\\n6.3 BASELINES\\n\\nWe apply existing calibration methods developed for classification to probability estimation (as well as cross-entropy minimization with early-stopping): (1) Three post-processing methods: Temperature Scaling (Guo et al., 2017), Platt Scaling (Platt, 1999), and Dirichlet Calibration (Kull et al., 2019) applied to the best CE model, (2) Two Ensemble Methods: Mix-n-Match (Zhang et al., 2020) applied to best CE model, and Deep Ensemble (Lakshminarayanan et al., 2017) with 5 networks, and (3) Three Modified Training methods: focal loss (Mukhoti et al., 2020), entropy-maximizing loss (Pereyra et al., 2017), and MMCE regularization (Kumar et al., 2018). Appendix F provides a detailed description. For our experiments on synthetic data, we also compare against a model trained on an infinite amount of data by repeatedly sampling new outcomes from the ground-truth probabilities at each epoch. This provides a best-case reference for each scenario.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RESULTS AND DISCUSSION\\n\\nTable 1 shows that calibration methods developed for classification can be effective for probability estimation. However, the performance of some methods is not consistent across all scenarios. For instance, regularization with negative entropy, which penalizes very high/low confidence, performs worse than CE when the ground-truth probability is close to 0 or 1. In contrast, methods that do not make strong assumptions tend to generalize better to multiple scenarios (e.g. Platt scaling consistently beats CE). The proposed method CaPE outperforms other techniques in most scenarios, and even matches the performance of the infinite data baseline for the Sigmoid scenario. Finally, we observe that the Skewed scenario is very challenging: most methods barely improve the CE baseline.\\n\\nTable 2 compares the baseline methods and CaPE on the three real-world datasets. We present AUC, ECE for 15 equally-sized bins, and Brier score, as complementary metrics since the underlying ground-truth probabilities are unobserved. As discussed in Section 3, Brier score is the metric that best captures the quality of probability estimates. CaPE has the lowest Brier score in all three datasets, while also achieving lower ECE values and higher AUC values than the other methods. This demonstrates that enforcing calibration during training also yields a more discriminative model. The reliability diagrams in Figure 6 depict the probability estimates produced by CE, CaPE and the best baseline method on the three datasets, demonstrating that CaPE produces a well-calibrated outputs.\\n\\nFigure 6 also shows that each real-world dataset closely aligns with a particular synthetic scenario: cancer survival with Centered; weather forecasting with Linear; collision prediction with Skewed. This supports the significance of our synthetic benchmark dataset, and provides insights in the differences among baseline models. For example, model averaging with deep ensemble performs well on weather forecasting but has higher Brier scores than Platt scaling on the other two datasets (see Appendix I for further analysis based on pathological stages). Accordingly, deep ensemble also underperforms in the synthetic scenarios where ground-truth probabilities are clustered closely (Sigmoid, Linear), but is effective for Linear. Finally, as in the synthetic Skewed scenario, all methods had similar performance on the collision prediction task. This highlights the importance of considering different scenarios when evaluating methodology for probability estimation.\\n\\nCONCLUSION\\n\\nIn this work we evaluate existing approaches to improve the output probabilities of neural networks on probability-estimation problems. To this end, we introduce a new synthetic benchmark dataset designed to reproduce several realistic scenarios, and also gather three real-world datasets relevant to medicine, climatology, and self-driving cars. In addition, we propose a novel approach for probability-estimation via deep learning that outperforms existing approaches for most datasets. An important application for probability estimation is in the context of survival analysis, which can be recast as estimation of conditional probabilities (Lee et al., 2018; Shamout et al., 2020; Goldstein et al., 2021). An interesting research direction is to consider problems with several possible uncertain outcomes (analogous to multiclass classification).\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nShreya Agrawal, Luke Barrington, Carla Bromberg, John Burge, Cenk Gazen, and Jason Hickey. Machine learning for precipitation nowcasting from radar images. arXiv preprint arXiv:1912.12132, 2019.\\n\\nGeorgy Ayzel. Rainnet: a convolutional neural network for radar-based precipitation nowcasting. https://github.com/hydrogo/rainnet, 2020.\\n\\nAndreas Buja, Werner Stuetzle, and Yi Shen. Loss functions for binary class probability estimation and classification: Structure and applications,\u201d manuscript, available at www-stat.wharton.upenn.edu/ buja, 2005.\\n\\nXinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. CoRR, abs/2003.04297, 2020. URL https://arxiv.org/abs/2003.04297.\\n\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848.\\n\\nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning, 2016.\\n\\nMark Goldstein, Xintian Han, Aahlad Puli, Adler J. Perotte, and Rajesh Ranganath. X-cal: Explicit calibration for survival analysis, 2021.\\n\\nIan Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT Press, 2016.\\n\\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning, pp. 1321\u20131330. PMLR, 2017.\\n\\nKartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchisescu, and Richard Hartley. Calibration of neural networks using splines. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=eQe8DEWNN2W.\\n\\nMaximilian Ilse, Jakub Tomczak, and Max Welling. Attention-based deep multiple instance learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2127\u20132136. PMLR, 10\u201315 Jul 2018. URL https://proceedings.mlr.press/v80/ilse18a.html.\\n\\nHoon Kim, Kangwook Lee, Gyeongjo Hwang, and Changho Suh. Crash to not crash: Learn to identify dangerous vehicles using a simulator. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 978\u2013985, 2019.\\n\\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\\n\\nMeelis Kull, Telmo M. Silva Filho, and Peter Flach. Beyond sigmoids: How to obtain well-calibrated probabilities from binary classifiers with beta calibration. Electronic Journal of Statistics, 11(2):5052 \u2013 5080, 2017. doi: 10.1214/17-EJS1338SI. URL https://doi.org/10.1214/17-EJS1338SI.\\n\\nMeelis Kull, Miquel Perell\u00b4o-Nieto, Markus K\u00a8angsepp, Telmo de Menezes e Silva Filho, Hao Song, and Peter A. Flach. Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with dirichlet calibration. In NeurIPS, 2019.\\n\\nAviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks from kernel mean embeddings. In ICML, 2018.\\n\\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles, 2017.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Reliable probability estimation is of crucial importance in many real-world applications where there is inherent uncertainty, such as weather forecasting, medical prognosis, or collision avoidance in autonomous vehicles. Probability-estimation models are trained on observed outcomes (e.g. whether it has rained or not, or whether a patient has died or not), because the ground-truth probabilities of the events of interest are typically unknown. The problem is therefore analogous to binary classification, with the important difference that the objective is to estimate probabilities rather than predicting the specific outcome. The goal of this work is to investigate probability estimation from high-dimensional data using deep neural networks. There exist several methods to improve the probabilities generated by these models but they mostly focus on classification problems where the probabilities are related to model uncertainty. In the case of problems with inherent uncertainty, it is challenging to evaluate performance without access to ground-truth probabilities. To address this, we build a synthetic dataset to study and compare different computable metrics. We evaluate existing methods on the synthetic data as well as on three real-world probability estimation tasks, all of which involve inherent uncertainty: precipitation forecasting from radar images, predicting cancer patient survival from histopathology images, and predicting car crashes from dashcam videos. Finally, we also propose a new method for probability estimation using neural networks, which modifies the training process to promote output probabilities that are consistent with empirical probabilities computed from the data. The method outperforms existing approaches on most metrics on the simulated as well as real-world data.\\n\\n1 INTRODUCTION\\nWe consider the problem of building models that answer questions such as: Will it rain? Will a patient survive? Will a car collide with another vehicle? Due to the inherently-uncertain nature of these real-world phenomena, this requires performing probability estimation, i.e. estimating the probability of each possible outcome of the phenomenon of interest. Models for probability prediction must be trained on observed outcomes (e.g. whether it rained, a patient died, or a collision occurred), because the ground-truth probabilities are unknown. The problem is therefore analogous to binary classification, with the important difference that the objective is to estimate probabilities rather than predicting specific outcomes. In probability estimation, two identical inputs (e.g. histopathology images from cancer patients) can potentially result in two different outcomes (death vs. survival). In contrast, in classification the class label is usually completely determined by the data (a picture either shows a cat or it does not).\\n\\nThe goal of this work is to investigate probability estimation from high-dimensional data using deep neural networks. Deep networks trained for classification often generate probabilities, which quantify the uncertainty of the estimate (i.e. how likely the network is to classify correctly). This quantification has been observed to be inaccurate, and several methods have been developed to improve it (Platt, 1999; Guo et al., 2017; Szegedy et al., 2016; Zhang et al., 2020; Thulasidasan et al., 2020; Mukhoti et al., 2020; Thagaard et al., 2020), including Bayesian neural networks (Gal & Ghahramani, 2016; Wang et al., 2016; Shekhovtsov & Flach, 2019; Postels et al., 2019). However, these works restrict their attention almost exclusively to classification in datasets (e.g. CIFAR-10/100 Krizhevsky (2009), or ImageNet (Deng et al., 2009)) where the label is not uncertain, and therefore the uncertainty is completely tied to the model: it quantifies the confidence of the model in its own prediction, not the probability of an event of interest.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"P(survival)=?\\n\\nFigure 1: The probability-estimation problem.\\n\\nIn probability estimation, we assume that each observed outcome $y_i$ (e.g. death or survival in cancer patients) in the training set is randomly generated from a latent unobserved probability $p_i$ associated to the corresponding data $x_i$ (e.g. histopathology images).\\n\\nTraining (left): Only $x_i$ and $y_i$ can be used for training, because $p_i$ is not observed.\\n\\nInference (right): Given new data $x$, the trained network $f$ produces a probability estimate $\\\\hat{p} \\\\in [0,1]$.\\n\\nProbability estimation from high-dimensional data is crucial in medical prognostics (Wulczyn et al., 2020), weather prediction (Agrawal et al., 2019), and autonomous driving (Kim et al., 2019). In order to advance deep-learning methodology for probability estimation it is crucial to build appropriate benchmark datasets. Here we build a synthetic dataset and gather three real-world datasets, which we use to systematically evaluate existing methodology. In addition, we propose a novel approach, which outperforms current state-of-the-art methods. Our contributions are the following:\\n\\n\u2022 We introduce a new synthetic dataset for probability estimation where a population of people may have a certain disease connected to age. The task is to predict the probability that they contract the disease from a picture of their face. The data are generated based on the UTKFaces dataset (Zhang et al., 2017a), which contains age information. The dataset contains multiple versions of the synthetic labels, which are generated according to different distributions designed to mimic real-world probability-prediction datasets. The dataset serves two objectives. First, it allows us to evaluate existing methodology. Second, it enables us to evaluate different metrics in a controlled scenario where we have access to ground-truth probabilities.\\n\\n\u2022 We have used publicly available data to build probability-estimation benchmark datasets for three real-world applications: (1) precipitation forecasting from radar images, (2) prediction of cancer-patient survival from histopathology images, and (3) prediction of vehicle collisions from dashcam videos. We use these datasets to systematically evaluate existing approaches, which have been previously tested mainly on classification datasets.\\n\\n\u2022 We propose Calibrated Probability Estimation (CaPE), a novel technique which modifies the training process so that output probabilities are consistent with empirical probabilities computed from the data. CaPE outperforms existing approaches on most metrics on synthetic and real-world data.\\n\\n2 PROBLEM FORMULATION: PROBABILITY ESTIMATION\\n\\nThe goal of probability estimation is to evaluate the likelihood of a certain event of interest, based on observed data. The available training data consists of $n$ examples $x_i$, $1 \\\\leq i \\\\leq n$, each associated with a corresponding outcome $y_i$. In our applications of interest, the input data are high dimensional: each $x_i$ corresponds to an image or a video. The corresponding label $y_i$ is either 0 or 1 depending on whether or not the event in question occurred. For example, in the cancer-survival application $x_i$ is a histopathology image of a patient, and $y_i$ equals 1 if the patient survived for 5 years after $x_i$ was collected. The data have inherent uncertainty: $y_i$, the patient's survival, does not depend deterministically on the histopathology image (due e.g. to comorbidities and other health factors). Instead, we assume that $y_i$ equals 1 with a certain probability $p_i$ associated with $x_i$, as illustrated in Figure 1, because the input data provides key information about the patient's survival chances.\\n\\nAt inference, a probability-estimation model aims to generate an estimate $\\\\hat{p}$ of the underlying probability $p$, associated with a new input data point $x$ (e.g. the probability of survival for over 5 years for new patients based on their histopathology data). To summarize, this is not a classification problem, because the labels are not completely predictable. Instead, the goal is to predict the probability of the outcome, which is critical in choosing a course of treatment for the patient.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"After determining the probability $p_i$ using $\\\\psi(z)$, the label $y_i$ is sampled from a Bernoulli distribution parametrized by $p_i$, so that it takes the value 1 with probability $p_i$. The distributions of $y_i$ under five different scenarios are illustrated in Fig. 11.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We present here supplementary information for the real-world datasets used in our experiments.\\n\\n**Cancer Survival**\\n\\nHistopathological features are useful in identification of tumor cells, cancer subtypes, and the stage and level of differentiation of the cancer. Hematoxylin and Eosin (H&E)-stained slides are the most common type of histopathology data and the basis for decision making in the clinic. With these properties, H&E are used for mortality prediction of cancer (Wulczyn et al., 2020).\\n\\nIn this experiment, we use the H&E slides of non-small cell lung cancers from The Cancer Genome Atlas Program (TCGA) to predict the 5-year survival. The dataset has 1512 whole slide images from 1009 patients, and 352 of them died in 5-years. We split the samples by patients and source institutions into training, validation, and test set, which has 1203, 151, and 158 samples respectively.\\n\\nThe whole slide images contain numerous pixels, so we cropped the slides into tiles at 20x magnification with 1/4 overlapping, resized them to $299 \\\\times 299$ with bicubic interpolation, and filtered out the tiles with more than 85% area covered by the background. The representations of each tile are trained with self-supervised momentum contrastive learning (MoCo) (Chen et al., 2020), and the slide-level prediction is obtained from a multiple-instance learning network (Ilse et al., 2018) trained with the binary label of survival in 5 years.\\n\\n**Weather Forecasting**\\n\\nWe use the German Weather service dataset, which contains quality-controlled rainfall-depth composites from 17 operational Doppler radars. Three precipitation maps from the past 30 minutes serve as an input. The training labels are the 0/1 events indicating whether the mean precipitation increases (1) or not (0).\\n\\nThe German Weather service (DWD - Deutshce Wetter Dienst) dataset contains quality-controlled rainfall-depth composites from 17 operational DWD Doppler radars. It has a spatial extent of 900x900 km, and covers the entirety of Germany. Data exists since 2006, with a spatial and temporal resolution of 1x1 km and 5 minutes, respectively. The dataset has been used to train RainNet, a precipitation nowcasting model (Ayzel, 2020).\\n\\nThe network architecture is ResNet18, with 3 input channels and 2 output channels. The input to the network are 3 precipitation maps which cover a fixed area of 300km $\\\\times$ 300 km in the center of the grid ($300 \\\\times 300$ pixels), set 10 minutes apart. The training, validation and test datasets consist of 20000, 6000 and 3000 samples, respectively, all separated temporally, over the span of 15 years.\\n\\n**Collision Prediction**\\n\\nVehicle collision is one of the leading causes of death in the world. Reliable collision prediction systems which can warn drivers about potential collisions can save a significant number of lives. A standard way to design such a system is to train a convolutional model for identifying if a particular vehicle in the dash-cam video feed might collide with the car in next few seconds. More formally, at time $t = T$ the system tries to predict if any car in the video might collide with our given car in time $t \\\\in [T, T + T_{look-ahead}]$. Each labelled training sample consists of features $X = (X_{T-\\\\delta}, X_{T-2\\\\delta}, ..., X_{T-d\\\\delta})$ and a binary label $Y \\\\in \\\\{0, 1\\\\}$ denoting if an accident will occur in $t \\\\in [T, T + T_{look-ahead}]$. Each $X_t$ is a tensor with 4 channels where the first 3 channels correspond to an RGB image of the dashcam view at time $t = t$, and the fourth channel consists of a mask with a bounding box on a particular vehicle of interest. In this work, we use YouTubeCrash dataset (Kim et al., 2019) to train and test our model, which uses $\\\\delta = 0.1$ s, $T_{look-head} = 18$ s, and $d = 3$. Following Kim et al. (2019) we used a VGG-16 network architecture. The dataset contains 122 accident scenes, and 2096 non-accident scenes, which after feature extraction gives us 2096 positive samples, and 11486 negative samples (the dataset is severely imbalanced, and similar to the Skewed situation in Section 6.1). We further split the dataset into train (6453 samples for label 0, and 1023 samples for label 1), validation (2348 samples for label 0, and 545 samples for label 1), and test (2685 samples for label 0, and 528 samples for label 1) sets. The samples in train, validation and test sets are generated from disjoint scenes/dashcam videos.\\n\\n4 https://www.cancer.gov/tcga\\n5 https://opendata.dwd.de/weather/radar/\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I Analysis of Cancer Survival Results\\n\\nPathological stage\\n\\n| Estimated Probability |\\n|-----------------------|\\n| CE Early Stop         |\\n| DeepEns               |\\n| CaPE (kernel)         |\\n\\nFigure 12: Estimated probability of survival grouped by pathological stages. The plot shows median, samples between 25th to 75th percentile in the box, samples between 0th and 100th percentile on the line, and the outliers as dots. Deep ensemble produces similar probability estimates for patients across all the stages; CE is more discriminative but has a very large variance; CaPE achieves a trade-off between the two baselines.\\n\\nFor cancer survival prediction, we visualize the estimated probabilities on the test set in different pathological stages in Figure 12. In general, patients in earlier stages should have higher probabilities of survival. Deep ensemble produces similar probability estimates for all stages (i.e. the model is less discriminative). Cross-entropy minimization (CE) is more discriminative, but has very wide confidence intervals. CaPE is more discriminative than deep ensemble, while having narrower confidence intervals than CE.\\n\\nCalibrating From the Beginning\\n\\nCaPE exploits a calibration-based cost function to improve its probability estimates without overfitting. The empirical probabilities in this loss are computed from the model itself. Consequently, applying this strategy from the beginning of training can be counterproductive, because the model predictions are essentially random. This is demonstrated in the following table, which compares CaPE with a model trained using the calibration loss from the beginning (in the same way as CaPE, alternating with cross-entropy minimization).\\n\\nTable 11: Comparison between CaPE and a model that uses the calibration loss from the beginning (in the same way as CaPE, alternating with cross-entropy minimization) on synthetic data.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13 illustrates the similarity between the empirical probability curves of different real-world datasets and the different scenarios of our synthetic dataset. For the cancer survival dataset, the empirical probabilities are clustered in the center (0.4-0.6) similar to the Centered scenario. For the weather forecasting dataset, the probabilities are uniformly distributed across 0.1-0.8 similar to the Linear scenario. For the collision prediction dataset, the majority of the data points are clustered in the lower probability region which makes it similar to the Skewed scenario.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nChanghee Lee, William Zame, Jinsung Yoon, and Mihaela van der Schaar. Deephit: A deep learning approach to survival analysis with competing risks. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), Apr. 2018. URL https://ojs.aaai.org/index.php/AAAI/article/view/11842.\\n\\nGongbo Liang, Yu Zhang, Xiaoqin Wang, and Nathan Jacobs. Improved trainable calibration method for neural networks on medical imaging classification. In British Machine Vision Conference (BMVC), 2020.\\n\\nSheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. Advances in Neural Information Processing Systems, 33, 2020.\\n\\nWesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry P. Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. CoRR, abs/1902.02476, 2019. URL http://arxiv.org/abs/1902.02476.\\n\\nJishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, S. Golodetz, Philip H. S. Torr, and P. Dokania. Calibrating deep neural networks using focal loss. ArXiv, abs/2002.09437, 2020.\\n\\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. 2011. URL http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf.\\n\\nGabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey E. Hinton. Regularizing neural networks by penalizing confident output distributions. CoRR, abs/1701.06548, 2017. URL http://arxiv.org/abs/1701.06548.\\n\\nJ. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers, 10(3), 1999.\\n\\nJanis Postels, Francesco Ferroni, Huseyin Coskun, Nassir Navab, and Federico Tombari. Sampling-free epistemic uncertainty estimation using approximated variance propagation. CoRR, abs/1908.00598, 2019. URL http://arxiv.org/abs/1908.00598.\\n\\nSuman Ravuri, Karel Lenc, Matthew Willson, Dmitry Kangin, Remi Lam, Piotr Mirowski, Megan Fitzsimons, Maria Athanassiadou, Sheleem Kashem, Sam Madge, et al. Skillful precipitation nowcasting using deep generative models of radar. arXiv preprint arXiv:2104.00954, 2021.\\n\\nLewis Fry Richardson. Weather prediction by numerical process. Cambridge university press, 2007.\\n\\nFarah E. Shamout, Yiqiu Shen, Nan Wu, Aakash Kaku, Jungkyu Park, Taro Makino, Stanislaw Jastrzebski, Duo Wang, Ben Zhang, Siddhant Dogra, Meng Cao, Narges Razavian, David Kudlowitz, Lea Azour, William Moore, Yvonne W. Lui, Yindalon Aphinyanaphongs, Carlos Fernandez-Granda, and Krzysztof J. Geras. An artificial intelligence system for predicting the deterioration of COVID-19 patients in the emergency department. CoRR, abs/2008.01774, 2020. URL https://arxiv.org/abs/2008.01774.\\n\\nAlexander Shekhovtsov and Boris Flach. Feed-forward propagation in probabilistic neural networks with categorical and max layers. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SkMuPjRcKQ.\\n\\nChristian Szegedy, V. Vanhoucke, S. Ioffe, Jonathon Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2818\u20132826, 2016.\\n\\nJeppe Thagaard, S\u00f8ren Hauberg, Bert van der Vegt, Thomas Ebstrup, Johan D Hansen, and Anders B Dahl. Can you trust predictive uncertainty under real dataset shifts in digital pathology? In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 824\u2013833. Springer, 2020.\\n\\nSunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. On mixup training: Improved calibration and predictive uncertainty for deep neural networks, 2020.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hao Wang, Xingjian Shi, and Dit-Yan Yeung. Natural-parameter networks: A class of probabilistic neural networks. CoRR, abs/1611.00448, 2016. URL http://arxiv.org/abs/1611.00448.\\n\\nEllery Wulczyn, David F. Steiner, Zhaoyang Xu, Apar Sadhwani, Hongwu Wang, I. Flament, C. Mermel, Po-Hsuan Cameron Chen, Yun Liu, and Martin C. Stumpe. Deep learning-based survival prediction for multiple cancer types using histopathology images. PLoS ONE, 15, 2020.\\n\\nXiaobo Xia, Tongliang Liu, Bo Han, Chen Gong, Nannan Wang, Zongyuan Ge, and Yi Chang. Robust early-learning: Hindering the memorization of noisy labels. In International Conference on Learning Representations, 2020.\\n\\nQuanming Yao, Hansi Yang, Bo Han, Gang Niu, and James Tin-Yau Kwok. Searching to exploit memorization effect in learning with noisy labels. In International Conference on Machine Learning, pp. 10789\u201310798. PMLR, 2020.\\n\\nB. Zadrozny and C. Elkan. Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. In ICML, 2001.\\n\\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. ICLR, 2017a.\\n\\nHongyi Zhang, Moustapha Ciss\u00e9, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. ArXiv, abs/1710.09412, 2018.\\n\\nJize Zhang, B. Kailkhura, and T. Y. Han. Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning. In ICML, 2020.\\n\\nZhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial autoencoder. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017b.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"We present here supplementary results to the ones presented in Section 7.\\n\\nA.1 Face-based Risk Prediction\\n\\nFull evaluation with confidence intervals derived using 1000 bootstraps for the five simulated scenarios are examined:\\n\\n- Linear (Table 3)\\n- Sigmoid (Table 4)\\n- Centered (Table 5)\\n- Skewed (Table 6)\\n- Discrete (Table 7)\\n\\nNote that all numbers are downscaled by $10^{-2}$ in the tables.\\n\\n| Method            | ECE    | MCE    | KS     | Brier  | MSE    | p-KL   |\\n|-------------------|--------|--------|--------|--------|--------|--------|\\n| Infinite Data     | 4.14   | 12.07  | 2.24   | 18.97  | 1.14   | 2.82   |\\n| CE early-stop     | 12.32  | 21.79  | 12.16  | 21.82  | 4.21   | 10.94  |\\n| Temperature       | 5.7    | 13.82  | 2.36   | 20.47  | 2.73   | 6.75   |\\n| Platt Scaling     | 4.29   | 10.94  | 1.3    | 20.18  | 2.48   | 6.07   |\\n| Dirichlet Cal.    | 7.38   | 22.58  | 3.78   | 21.32  | 3.56   | 9.08   |\\n| Focal Loss        | 5.34   | 13.31  | 3.56   | 21.99  | 4.13   | 10.52  |\\n| Mix-n-match       | 5.46   | 12.9   | 1.92   | 20.43  | 2.7    | 6.72   |\\n| Entropy Reg.      | 5.7    | 13.52  | 4.94   | 20.42  | 2.58   | 6.65   |\\n| MMCE Reg.         | 4.89   | 12.57  | 1.92   | 20.04  | 2.24   | 5.68   |\\n| Deep Ensemble     | 4.26   | 11.33  | 1.95   | 19.88  | 1.9    | 4.55   |\\n| CaPE (bin)        | 4.58   | 11.85  | 1.71   | 19.68  | 1.78   | 4.35   |\\n| CaPE (kernel)     | 4.62   | 12.25  | 1.65   | 19.71  | 1.74   | 4.3   |\\n\\nTable 3: Performance on Face-based Risk Prediction.\\n\\n| Method            | ECE    | MCE    | KS     | Brier  | MSE    |\\n|-------------------|--------|--------|--------|--------|--------|\\n| Infinite Data     | 6.4    | 20.63  | 2.74   | 16.28  | 5.34   |\\n| CE early-stop     | 6.19   | 17.0   | 5.86   | 16.68  | 6.16   |\\n| Temperature       | 5.57   | 15.32  | 5.02   | 16.58  | 6.13   |\\n| Platt Scaling     | 3.45   | 10.32  | 1.3    | 16.33  | 5.78   |\\n| Dirichlet Cal.    | 14.5   | 25.68  | 4.67   | 19.21  | 8.64   |\\n| Focal Loss        | 4.65   | 11.84  | 2.66   | 16.96  | 6.86   |\\n| Mix-n-match       | 5.65   | 15.32  | 5.09   | 16.6   | 6.12   |\\n| Entropy Reg.      | 9.51   | 18.77  | 7.26   | 17.17  | 7.02   |\\n| MMCE Reg.         | 4.67   | 13.63  | 2.5    | 15.9   | 5.35   |\\n| Deep Ensemble     | 5.17   | 16.12  | 2.04   | 16.39  | 5.86   |\\n| CaPE (bin)        | 3.78   | 11.96  | 2.22   | 15.84  | 5.17   |\\n| CaPE (kernel)     | 3.9    | 11.73  | 2.05   | 15.85  | 5.16   |\\n\\nTable 4: Performance on Face-based Risk Prediction.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: Performance on Face-based Risk Prediction.\\n\\n| Method                  | Skewed CE | Skewed MCE | Skewed KS | Skewed Brier | Skewed MSE |\\n|-------------------------|------------|------------|-----------|--------------|------------|\\n| Infinite Data           | 2.7\u00b10.46   | 7.64\u00b12.14  | 1.05\u00b10.39 | 11.0\u00b10.51    | 0.22\u00b10.01  |\\n| CE early-stop           | 3.07\u00b10.57  | 7.88\u00b11.88  | 1.28\u00b10.41 | 11.18\u00b10.5  | 0.4\u00b10.01  |\\n| Temperature             | 3.14\u00b10.49  | 7.92\u00b11.84  | 1.12\u00b10.33 | 11.22\u00b10.47  | 0.4\u00b10.02  |\\n| Platt Scaling           | 2.99\u00b10.53  | 7.73\u00b11.59  | 1.07\u00b10.37 | 11.1\u00b10.54   | 0.39\u00b10.01  |\\n| Dirichlet Cal.          | 3.04\u00b10.73  | 7.81\u00b12.43  | 0.97\u00b10.3  | 11.22\u00b10.42  | 0.47\u00b10.02  |\\n| Focal Loss              | 8.29\u00b10.67  | 14.93\u00b11.43 | 6.16\u00b10.67 | 12.01\u00b10.49  | 1.28\u00b10.03  |\\n| Mix-n-match             | 2.99\u00b10.53  | 7.78\u00b11.78  | 1.08\u00b10.32 | 11.18\u00b10.49  | 0.4\u00b10.01  |\\n| Entropy Reg.            | 7.67\u00b10.57  | 14.43\u00b11.5  | 5.2\u00b10.71  | 11.94\u00b10.45  | 1.18\u00b10.03  |\\n| MMCE Reg.               | 3.68\u00b10.59  | 10.94\u00b12.76 | 1.47\u00b10.31 | 11.14\u00b10.44  | 0.54\u00b10.02  |\\n| Deep Ensemble           | 2.87\u00b10.5  | 7.21\u00b11.63  | 1.36\u00b10.44 | 11.28\u00b10.5  | 0.55\u00b10.02  |\\n| CaPE (bin)              | 3.29\u00b10.5  | 8.18\u00b11.51  | 1.17\u00b10.34 | 11.07\u00b10.47  | 0.4\u00b10.02  |\\n| CaPE (kernel)           | 3.16\u00b10.5  | 8.14\u00b11.58  | 1.09\u00b10.33 | 11.17\u00b10.53  | 0.39\u00b10.01  |\\n\\nTable 6: Performance on Face-based Risk Prediction.\\n\\n| Method                  | Skewed CE | Skewed MCE | Skewed KS | Skewed Brier | Skewed MSE |\\n|-------------------------|------------|------------|-----------|--------------|------------|\\n| Infinite Data           | 4.23\u00b10.74  | 11.16\u00b12.5  | 1.45\u00b10.49 | 20.38\u00b10.35  | 1.52\u00b10.05  |\\n| CE early-stop           | 6.7\u00b10.86   | 18.62\u00b13.52 | 2.61\u00b10.53 | 21.91\u00b10.36  | 2.24\u00b10.08  |\\n| Temperature             | 6.12\u00b10.87  | 16.82\u00b13.56 | 3.37\u00b10.86 | 21.76\u00b10.35  | 2.21\u00b10.08  |\\n| Platt Scaling           | 4.7\u00b10.72   | 11.69\u00b12.44 | 1.67\u00b10.51 | 21.44\u00b10.32  | 2.06\u00b10.08  |\\n| Dirichlet Cal.          | 7.13\u00b10.86  | 22.67\u00b15.08 | 3.18\u00b10.68 | 22.1\u00b10.34   | 2.74\u00b10.1  |\\n| Focal Loss              | 5.7\u00b10.75   | 13.68\u00b12.32 | 4.62\u00b10.91 | 21.77\u00b10.28  | 2.92\u00b10.09  |\\n| Mix-n-match             | 6.27\u00b10.76  | 16.83\u00b12.95 | 3.47\u00b10.93 | 21.77\u00b10.33  | 2.21\u00b10.08  |\\n| Entropy Reg.            | 6.69\u00b10.87  | 15.38\u00b12.43 | 6.03\u00b11.13 | 21.79\u00b10.31  | 2.84\u00b10.08  |\\n| MMCE Reg.               | 3.96\u00b10.7  | 10.4\u00b12.4  | 1.51\u00b10.47 | 21.12\u00b10.35  | 2.09\u00b10.08  |\\n| Deep Ensemble           | 4.76\u00b10.74  | 11.49\u00b12.23 | 2.04\u00b10.61 | 21.17\u00b10.31  | 1.97\u00b10.08  |\\n| CaPE (bin)              | 5.41\u00b10.74  | 14.45\u00b13.15 | 2.24\u00b10.59 | 21.33\u00b10.36  | 1.81\u00b10.08  |\\n| CaPE (kernel)           | 4.96\u00b10.8  | 12.97\u00b12.63 | 2.18\u00b10.58 | 21.21\u00b10.42  | 1.84\u00b10.08  |\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 9: The comparison between MSE\\\\(_p\\\\) and other metrics on synthetic data. Brier score presents the most consistent correlation with MSE\\\\(_p\\\\).\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\n![Table]\\n\\n| Method       | Correlation: |\\n|--------------|--------------|\\n| AUC          | -0.737       |\\n| ECE          | 0.731        |\\n| MCE          | -0.694       |\\n| KS-Error     | 0.677        |\\n| Brier        | 0.995        |\\n| CE Early-stop|             |\\n| Infinite Data|             |\\n| Temperature  |             |\\n| Platt Scaling|             |\\n| Dirichlet Cal.|            |\\n| Mix-n-Match  |             |\\n| Focal Loss   |             |\\n| MMCE Reg.    |             |\\n| Entropy Reg. |             |\\n| CaPE (bin)   |             |\\n| CaPE (kernel)|             |\\n\\n**Figure 9:** Comparison between KL_p and other metrics on synthetic data. Brier score presents the most consistent correlation with KL_p.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alternatively, we can use kernel estimation:\\n\\n\\\\[\\nP[y = 1 | f(x) \\\\in I(q)] \\\\approx \\\\frac{1}{Z} \\\\sum_{p \\\\in I(q)} P[y = 1 | f(x) = \\\\hat{p}] \\\\cdot \\\\exp\\\\left(-\\\\left(p - q\\\\right)^2/\\\\sigma^2\\\\right),\\n\\\\]\\n\\n(9)\\n\\nwhere \\\\(Z = \\\\sum_{p \\\\in I(q)} \\\\exp\\\\left(-\\\\left(p - q\\\\right)^2/\\\\sigma^2\\\\right)\\\\) is the normalization factor. An empirical estimate of the conditional probability would then be\\n\\n\\\\[\\nP[y = 1 | f(x) \\\\in I(q)] \\\\approx \\\\frac{1}{Z} \\\\sum_{f(x_i) \\\\in I(q)} y_i \\\\exp\\\\left(-\\\\left(f(x_i) - q\\\\right)^2/\\\\sigma^2\\\\right).\\n\\\\]\\n\\n(10)\\n\\nBased on these two approximation methods, we can design an algorithm to estimate \\\\(p_{\\\\text{emp}}\\\\).\\n\\n**Bin**\\n\\nWe divide our data into \\\\(B\\\\) bins of equal size. \\\\(Q_1, ..., Q_B\\\\) are the data B-quantiles. We wish to estimate \\\\(P[y = 1 | f(x) \\\\in [Q_{b-1}, Q_b]]\\\\), \\\\(b = 1, ..., B\\\\), \\\\(Q_0 = 0\\\\). Denote \\\\(I_b := [Q_{b-1}, Q_b] \\\\cap \\\\{f(x_i)\\\\}\\\\), set of all predictions in \\\\(\\\\left[Q_{b-1}, Q_b\\\\right]\\\\), and \\\\(\\\\text{Index}(I_b) = \\\\{i | f(x_i) \\\\in I_b\\\\}\\\\). We have,\\n\\n\\\\[\\nP[y = 1 | f(x) \\\\in [Q_{b-1}, Q_b]] \\\\approx p_{\\\\text{emp}}^{(b)} = \\\\frac{1}{|\\\\text{Index}(I_b)|} \\\\sum_{i \\\\in \\\\text{Index}(I_b)} y_i.\\n\\\\]\\n\\nWe assign \\\\(p_{\\\\text{emp}}^{(b)}\\\\) to all data points \\\\(i\\\\) in the \\\\(b\\\\)-th quantile\\n\\n\\\\[\\np_{\\\\text{emp}}^{(b)} = p_{\\\\text{emp}}^{(b)} \\\\forall i \\\\in \\\\text{Index}(I_b)\\n\\\\]\\n\\n**Kernel**\\n\\nIn this case we use kernel estimation:\\n\\n\\\\[\\np_{\\\\text{emp}}^{(b)} = \\\\frac{1}{\\\\sum_{k \\\\in \\\\text{NN}(i,r)} K(i,k)} \\\\sum_{k \\\\in \\\\text{NN}(i,r)} K(i,k) y_k\\n\\\\]\\n\\n(11)\\n\\n\\\\(\\\\text{NN}(i,r)\\\\) defines \\\\(r\\\\) data points whose predictions are nearest to \\\\(\\\\hat{p}_i = f(x_i)\\\\).\\n\\n\\\\(K(i,j)\\\\) is the Gaussian kernel \\\\(K(i,j) = \\\\exp\\\\left(-\\\\left(\\\\hat{p}_i - \\\\hat{p}_j\\\\right)^2/\\\\sigma^2\\\\right)\\\\), with hyperparameter \\\\(\\\\sigma\\\\).\\n\\n### Calibration Baselines\\n\\nThis section includes a review of the baseline methods, discussed in Section 6.\\n\\n**Post-processing**\\n\\nPost-processing for calibration requires finding a function \\\\(f: [0, 1] \\\\rightarrow [0, 1]\\\\), that augments the output of the neural network \\\\(\\\\hat{p}_i \\\\rightarrow f(\\\\hat{p}_i)\\\\) in order to achieve better calibration properties.\\n\\n- **Platt scaling** (Platt, 1999) optimizes \\\\(f\\\\) on validation set within the following family,\\n\\n\\\\[\\nf_1(\\\\hat{p}_i) = \\\\sigma(W^T \\\\hat{p}_i + b)\\n\\\\]\\n\\n(12)\\n\\nwhere \\\\(W \\\\in \\\\mathbb{R}^{2}, b \\\\in \\\\mathbb{R}\\\\) and \\\\(\\\\sigma\\\\) is the Sigmoid function. The non-probabilistic predictions of a classifier are used as features for a logistic regression model, which is trained on the validation set to return probabilities.\\n\\n- **Temperature scaling** (Guo et al., 2017) is a single parameter variant of Platt Scaling where we only change the temperature of the softmax to obtain the calibrated probabilities.\\n\\n\\\\[\\nf(\\\\hat{p}_i) = \\\\text{Softmax}(\\\\hat{p}_i/T)\\n\\\\]\\n\\n(13)\\n\\nwhere \\\\(T \\\\in \\\\mathbb{R}\\\\) minimizes the negative log-likelihood of validation set.\\n\\n- **Beta/Dirichlet calibration** (Dir-ODIR) (Kull et al., 2017; 2019) assumes that the probabilities can be parametrized by a Beta/Dirichlet distribution i.e.\\n\\n\\\\[\\nf_j \\\\sim \\\\text{Beta}(\\\\alpha_j, \\\\beta_j)\\n\\\\]\\n\\n(14)\\n\\nAssume the prior to be \\\\(p(y = j) = \\\\pi_j\\\\), \\\\(\\\\pi_j \\\\in [0, 1]\\\\), we have\\n\\n\\\\[\\nP(y | f_j) \\\\propto \\\\pi_j f_j\\n\\\\]\\n\\n\\\\(\\\\alpha_j, \\\\beta_j\\\\) are estimated by maximizing the posterior.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nEnsembling\\n\\nThese calibration methods simultaneously train several neural networks from end to end, varying parameters in the training process. The final output is some function of all the different outputs.\\n\\n\u2022 Mix-n-Match (Zhang et al., 2020) improves calibration by ensembling parametric and non-parametric calibrators. Denote the temperature scaling function with $g(\\\\hat{y}_i, T)$. Then Mix-n-Match ensembles different temperatures $f_j(\\\\hat{p}_i) = w_1 g_j(\\\\hat{p}_i, T) + w_2 g_j(\\\\hat{p}_i, 0) + w_3 g_j(\\\\hat{p}_i, \\\\infty)$ (15)\\n\\nAfter ensembling the parametric temperature scaling, Mix-n-Match applies non-parametric isotonic regression.\\n\\n\u2022 Deep ensemble (Lakshminarayanan et al., 2017) trains $M$ copies of the neural network with different initialization. The probability estimation is the average of all single model estimations $p(y_i | x_i) = \\\\frac{1}{M} \\\\sum_j p(\\\\theta_j(y_i | x_i))$ (16)\\n\\nModified training\\n\\nThese calibration methods train the neural networks from end to end, modifying the training process to improve calibration.\\n\\n\u2022 Confidence penalty (Pereyra et al., 2017) Penalizes low entropy output distributions (confidence penalty). Label smoothing improve state-of-the-art models across benchmarks. $L(\\\\theta) = -\\\\sum_i \\\\log p_{\\\\theta}(y_i | x_i) - \\\\beta H(p_{\\\\theta}(y_i | x_i))$ (17)\\n\\n\u2022 Focal loss (Mukhoti et al., 2020) maximizes entropy while minimizing the KL divergence between the predicted and the target distributions. It also regularizes the weights of the model to avoid overfitting. $L(\\\\theta) = -\\\\sum_i (1 - p_{\\\\theta}(y_i | x_i))^\\\\gamma \\\\log p_{\\\\theta}(y_i | x_i), \\\\gamma \\\\in \\\\mathbb{R}$ (18)\\n\\n\u2022 Kernel MMCE (Kumar et al., 2018) is a reproducing kernel Hilbert space (RKHS) kernel based measure of calibration that is efficiently trainable, alongside the negative likelihood loss. Given data samples $D = \\\\{(c_i, r_i)\\\\}_{m_i=0}^m$, where $c_i = \\\\chi\\\\{\\\\hat{y}_i = y_i\\\\}$ and $r_i = P(c_i = 1 | \\\\hat{y}_i)$, MMCE is computed on samples $D$ as following, $\\\\text{MMCE}^2(D) = \\\\sum_{i,j} (c_i - r_i)(c_j - r_j) k(r_i, r_j)$ (19) where $k(r_i, r_j)$ is a kernel function. MMCE is optimized together with the cross entropy loss as a regularization term. The strength of calibration can be adjusted by a scale $\\\\lambda \\\\in \\\\mathbb{R}$. $L(\\\\theta) = -\\\\sum_i \\\\log p_{\\\\theta}(y_i | x_i) + \\\\frac{\\\\lambda}{2} (\\\\text{MMCE}^2(D))$ (20)\\n\\nGSYNTHETIC DATA EXPERIMENTS\\n\\nWe use ResNet-18 model for all our experiments with synthetic data. The synthetic data is split into training, validation, and test sets with 16641, 4738, and 2329 samples, respectively. The training and validation sets contain only images $x_i$ and 0-1 labels $y_i$ for training and tuning the model. In order to evaluate the performance of the model for probability estimation, the held-out test set contains the ground truth probabilities $p_i$, in addition to $x_i$ and $y_i$. Note that we do not use the ground-truth probability labels $p_i$ values during training or inference - we only use them to compare the performance of different models.\\n\\nGround Truth Probability Generation\\n\\nThe ground truth probability associated with example $i$ is simulated by $p_i = \\\\psi(z_i)$ where $z_i$ is age of the person.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"We present here additional metrics on the real world data: Cancer Survival (Table 8); Climate Forecasting (Table 9); Collision Prediction (Table 10).\\n\\n| Methods                  | AUC | ECE | MCE | NLL  | Brier | KS  |\\n|--------------------------|-----|-----|-----|------|-------|-----|\\n| Early-stop               | 58.88 | 12.25 | 25.35 | 67.92 | 23.96 | 6.44 |\\n| Temperature              | 58.88 | 12.07 | 24.65 | 67.11 | 23.73 | 6.92 |\\n| Platt Scaling            | 58.91 | 10.28 | 27.69 | 66.11 | 23.33 | 4.91 |\\n| Dirichlet Cal.           | 49.89 | 13.83 | 35.52 | 67.57 | 24.08 | 6.00 |\\n| Mix-n-match              | 66.89 | 23.67 | 7.18  |      |       |     |\\n| Focal loss               | 55.02 | 12.15 | 26.34 | 65.92 | 23.31 | 6.38 |\\n| Entropy Reg.             | 56.29 | 11.73 | 30.81 | 66.49 | 23.62 | 6.83 |\\n| MMCE Reg.                | 48.45 | 11.84 | 37.36 | 66.83 | 23.73 | 3.64 |\\n| Deep Ensemble            | 52.26 | 9.99  | 28.30 | 66.22 | 23.47 | 5.02 |\\n| CaPE (bin)               | 61.44 | 12.31 | 25.27 | 65.75 | 23.20 | 2.59 |\\n| CaPE (kernel)            | 61.22 | 9.48  | 32.40 | 65.70 | 23.18 | 3.70 |\\n\\nTable 8: Baselines with full metrics for cancer survival\\n\\n| Methods                  | AUC | ECE | MCE | NLL  | Brier | KS  |\\n|--------------------------|-----|-----|-----|------|-------|-----|\\n| Early-stop               | 77.64 | 10.91 | 25.50 | 59.97 | 20.57 | 11.03 |\\n| Temperature              | 77.64 | 8.66  | 23.56 | 58.77 | 20.21 | 7.41 |\\n| Platt Scaling            | 77.65 | 6.97  | 16.47 | 57.38 | 19.53 | 3.26 |\\n| Dirichlet Cal.           | 77.51 | 14.29 | 30.09 | 62.83 | 21.89 | 5.21 |\\n| Mix-n-match              | 77.64 | 8.65  | 23.58 | 58.77 | 20.21 | 7.39 |\\n| Focal Loss               | 76.18 | 8.32  | 21.25 | 59.01 | 20.27 | 4.45 |\\n| Entropy Reg.             | 79.01 | 10.53 | 20.72 | 57.83 | 19.77 | 5.00 |\\n| MMCE Reg                 | 76.69 | 8.46  | 19.73 | 59.25 | 20.12 | 7.31 |\\n| Deep Ensemble            | 79.86 | 7.41  | 18.24 | 55.28 | 18.82 | 7.57 |\\n| CaPE (bin)               | 78.99 | 5.16  | 15.09 | 79.00 | 18.37 | 2.34 |\\n| CaPE (kernel)            | 79.00 | 5.08  | 13.28 | 54.32 | 18.39 | 2.34 |\\n\\nTable 9: Baselines with full metrics for weather prediction\\n\\n| Methods                  | AUC | ECE | MCE | NLL  | Brier | KS  |\\n|--------------------------|-----|-----|-----|------|-------|-----|\\n| Early-stop               | 85.68 | 4.36  | 19.87 | 31.67 | 8.59  | 1.54 |\\n| Temperature              | 85.68 | 4.56  | 16.79 | 30.36 | 8.52  | 2.90 |\\n| Platt Scaling            | 85.76 | 3.04  | 12.39 | 29.42 | 8.23  | 1.52 |\\n| Dirichlet Cal.           | 83.36 | 5.78  | 18.13 | 30.90 | 8.77  | 1.60 |\\n| Mix-n-match              | 85.68 | 4.40  | 17.41 | 30.25 | 8.52  | 2.60 |\\n| Focal Loss               | 82.21 | 9.07  | 19.85 | 34.41 | 9.82  | 8.72 |\\n| Entropy Reg.             | 83.15 | 14.54 | 21.27 | 38.74 | 11.10 | 13.44 |\\n| MMCE Reg.                | 85.18 | 2.94  | 8.95  | 30.65 | 8.48  | 2.44 |\\n| Deep Ensemble            | 85.27 | 3.15  | 16.53 | 30.20 | 8.54  | 2.01 |\\n| CaPE (bin)               | 8.57  | 3.16  | 12.21 | 30.61 | 8.18  | 2.13 |\\n| CaPE (kernel)            | 85.95 | 3.22  | 13.32 | 30.44 | 8.13  | 2.10 |\\n\\nTable 10: Baselines with full metrics for collision prediction\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"We present here additional reliability curves to the ones illustrated in Figure 6. Figure 7: The reliability diagrams of all the baselines on real-world datasets. We train all baseline methods on each of the datasets and plot the empirical probability (y-axis) against predicted probability (x-axis). The axis labels are removed due to space constraints.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We present here additional reliability curves for the different synthetic data scenarios, mentioned in Figure 4.\\n\\nWe derive the KS-error, mentioned in Section 3. For a calibrated estimator\\n\\n\\\\[\\nP[y=1 | f(x) \\\\in I(q)] = q, \\\\quad \\\\forall 0 \\\\leq q \\\\leq 1,\\n\\\\]\\n\\nfor some small interval \\\\(I(q)\\\\) around \\\\(q\\\\). Hence\\n\\n\\\\[\\nP[y=1, f(x) \\\\in I(q)] = P[f(x) \\\\in I(q)] q, \\\\quad \\\\forall 0 \\\\leq q \\\\leq 1.\\n\\\\]\\n\\nSimilarly to the Kolmogorov-Smirnov (KS) test for distribution functions, we can recast this property in integral form\\n\\n\\\\[\\n\\\\phi_1(\\\\sigma) = \\\\sigma \\\\int_0^1 P[y=1, f(x) \\\\in I(q)] dq,\\n\\\\]\\n\\n\\\\[\\n\\\\phi_2(\\\\sigma) = \\\\sigma \\\\int_0^1 P[f(x) \\\\in I(q)] dq.\\n\\\\]\\n\\nWe can evaluate \\\\(\\\\phi_1, \\\\phi_2\\\\) from a finite sample \\\\((x_i, y_i), i = 1 \\\\ldots n,\\\\)\\n\\n\\\\[\\n\\\\phi_1(\\\\sigma) = \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\mathbb{1}(y_i = 1, f(x_i) \\\\leq \\\\sigma),\\n\\\\]\\n\\n\\\\[\\n\\\\phi_2(\\\\sigma) = \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\mathbb{1}(f(x_i) \\\\leq \\\\sigma) f(x_i)\\n\\\\]\\n\\nThe KS error is defined as\\n\\n\\\\[\\n\\\\text{KS} = \\\\max_{1 \\\\leq \\\\sigma \\\\leq 1} |\\\\phi_1(\\\\sigma) - \\\\phi_2(\\\\sigma)|\\n\\\\]\\n\\n\\\\(\\\\phi_1, \\\\phi_2\\\\) can be efficiently computed by sorting the data points with respect to their confidence scores \\\\(f(x_i)\\\\). The KS error has the advantage of being independent of binning configurations, unlike ECE and MCE.\\n\\nWe present here a decomposition of the Brier score into two components, discussed in Section 3. The Brier score can be interpreted as a sum of two terms, calibration and refinement. Assume the network can output one of \\\\(K\\\\) distinct possible predictions, i.e., \\\\(\\\\hat{p} \\\\in \\\\{\\\\hat{q}_1, \\\\ldots, \\\\hat{q}_K\\\\}\\\\).\\n\\nDenote \\\\(S_k,\\\\) the set of all inputs with output \\\\(p_k\\\\) and \\\\(\\\\bar{q}_k\\\\) the empirical probability over \\\\(S_k,\\\\) i.e.,\\n\\n\\\\[\\nS_k = \\\\{x | f(x) = \\\\hat{q}_k\\\\}\\n\\\\]\\n\\n\\\\[\\n|S_k| = n_k,\\n\\\\]\\n\\n\\\\[\\n\\\\bar{q}_k = \\\\frac{1}{n_k} \\\\sum_{x_i \\\\in S_k} y_i\\n\\\\]\\n\\nThen we can write\\n\\n\\\\[\\n\\\\text{Brier} = \\\\frac{1}{N} \\\\sum_{i=1}^N (\\\\hat{p}_i - y_i)^2 = \\\\frac{1}{N} \\\\sum_{k=1}^K \\\\frac{n_k}{N} (\\\\hat{q}_k - \\\\bar{q}_k)^2 + \\\\frac{1}{N} \\\\sum_{k=1}^K \\\\frac{n_k}{N} \\\\bar{q}_k (1 - \\\\bar{q}_k),\\n\\\\]\\n\\nThe first term on the RHS, calibration, is similar to MSE \\\\(p\\\\), with the empirical probabilities \\\\(\\\\bar{q}_k\\\\) substituting for the true labels. The second term, refinement, is an estimate of the confidence in determining \\\\(\\\\bar{q}_k\\\\). It is related to the area under curve (AUC), which measures to the achievable accuracy of the network as a classifier. The term is smaller as the prediction classes \\\\(f\\\\) tend towards 0 or 1. Thus, this term penalizes empirically calibrated predictors, with low discriminative power, as in Figure 2b.\"}"}
{"id": "hdSn_X7Hfvz", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: Reliability diagrams for different synthetic data scenarios. We can see that CaPE outperforms early stopping, prevents overfitting, and achieves a performance on par with training on infinite resampled data.\"}"}
