{"id": "QrnDe_9ZFd8", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 14: Performance across salient features on Task Disambiguation Using Multiple Examples (see Section 4.3)\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 15: Performance across salient features on Task Disambiguation Using Multiple Examples (see Section 4.3)\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 16: Performance across salient features on Task Disambiguation Using Multiple Examples (see Section 4.3)\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 17: Performance across salient features on Task Disambiguation Using Multiple Examples\\n\\n(see Section 4.3)\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "QrnDe_9ZFd8", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "QrnDe_9ZFd8", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.\\n\\nGabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. Evaluating gender bias in machine translation.\\n\\nHaitian Sun, William W. Cohen, and Ruslan Salakhutdinov. Conditionalqa: A complex reading comprehension dataset with conditional answers.\\n\\nAlex Tamkin, Dan Jurafsky, and Noah Goodman. Language through a prism: A spectral approach for multiscale language representations.\\n\\nAlex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. Understanding the capabilities, limitations, and societal impact of large language models.\\n\\nAlex Tamkin, Vincent Liu, Rongfei Lu, Daniel Fein, Colin Schultz, and Noah Goodman. Dabs: A domain-agnostic benchmark for self-supervised learning.\\n\\nAlex Tamkin, Gaurab Banerjee, Mohamed Owda, Vincent Liu, Shashank Rammoorthy, and Noah Goodman. Dabs 2.0: Improved datasets and algorithms for universal self-supervision.\\n\\nAlex Tamkin, Margalit Glasgow, Xiluo He, and Noah Goodman. Feature dropout: Revisiting the role of augmentations in contrastive learning.\\n\\nYu Wang and Eugene Agichtein. Query ambiguity revisited: Clickthrough measures for distinguishing informational and ambiguous queries.\\n\\nAlbert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts?\\n\\nKellie Webster, Marta Recasens, Vera Axelrod, and Jason Baldridge. Mind the gap: A balanced corpus of gendered ambiguous pronouns.\\n\\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners.\\n\\nZeqiu Wu, Ryu Parish, Hao Cheng, Sewon Min, Prithviraj Ammanabrolu, Mari Ostendorf, and Hannaneh Hajishirzi. Inscit: Information-seeking conversations with mixed-initiative interactions.\\n\\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference.\\n\\nMichael J.Q. Zhang and Eunsol Choi. Situatedqa: Incorporating extra-linguistic contexts into qa.\\n\\nTony Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models.\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. In EMNLP, 2021.\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When constructing the finetuning dataset for Section 4.4, two out of the three possible sentence pairings were used. We then tested on the held-out sentence pairing. For example, if the finetuning dataset contained the salient features: 'subject,' location,' 'religious,' and 'pronoun,' the held-out features would be 'proper noun' and 'negation.'\\n\\nFor the control experiments, the dataset consisted of examples for which only one feature varied. For example, if the sentence was of the Subject-Location sentence type and the salient feature was location, a given example may contain examples with both outdoor and indoor locations but only humans.\\n\\nFor the ambiguous experiments, the dataset consisted of examples with ambiguity: both features varied between the two possibilities for each feature.\\n\\nThe number of examples in each prompt varied: building from 4 examples to 20 examples for each salient feature. But the examples across these different prompts did not remain constant (rather just the salient feature did), unlike in Section 4.3 where a prompt with a length of n examples simply added one example to the preceding prompt of length n-1.\\n\\nAs with the tests in Section 4.2 and Section 4.3, the labels' correspondence with a salient feature and the order of labels were randomized across all tests with equal probabilities being given to each possibility.\\n\\nHyperparameters\\n\\nWe use OpenAI's finetuning API to finetune their davinci model. When conducting the finetuning, we used a batch size of 1, a learning rate multiplier of 0.1, and a prompt loss weight of 0.1.\\n\\nControl experiment settings\\n\\nOur control experiment for the finetuning only varied one of the two features in each set of 20 examples (e.g. boar could change to worm but not firefighter). The choice of which feature would be constant (e.g. human/animal vs indoor/outdoor), as well as the value of that feature (e.g. human vs animal) were decided randomly for each set of 20 examples.\\n\\nBy following this procedure, we did not introduce task ambiguity into the finetuning data, but still ensured that the control model saw the same range of sentence constructions as the model trained on ambiguous examples.\\n\\nI.1 TASK DISAMBIGUATION USING NATURAL LANGUAGE INSTRUCTIONS\\n\\nFigure 10 details the performance across salient features for each model individually across both format types in instances of informative instructions. Figure 11 averages the performance of the models across salient features, demonstrating the difference in performance by format type for each salient feature.\\n\\nI.1.2 UNINFORMATIVE INSTRUCTIONS\\n\\nFigure 12 and Figure 13 mirror Figure 10 and Figure 11 respectively but instead demonstrate the performance in prompts with uninformative instructions.\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Performance with informative instructions across salient features on Task Disambiguation Using Natural Language Instructions (see Section 4.2).\\n\\nFigure 14, Figure 15, Figure 16, Figure 17, and Figure 18 individually detail the performance of each model across each salient feature from 1-20 examples.\\n\\nFigure 19b demonstrates the difference in performance across the two format types, averaged for all models. Because of topp\u2019s poor performance on prompts with the arrow format, we do not include topp in the collective performance graph but instead display its performance across formats individually from the other models.\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: Performance with informative instructions for each format averaged over all models on Task Disambiguation Using Natural Language Instructions (see Section 4.2).\\n\\n(a) ada\\n(b) text-ada-001\\n(c) babbage\\n(d) text-babbage-001\\n(e) curie\\n(f) text-curie-001\\n(g) davinci\\n(h) text-davinci-002\\n(i) j1-jumbo\\n\\nFigure 12: Performance with uninformative instructions across salient features on Task Disambiguation Using Natural Language Instructions (see Section 4.2).\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13: Performance with uninformative instructions for each format averaged over all models on Task Disambiguation Using Natural Language Instructions (see Section 4.2)\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ASK AMBIGUITY IN HUMANS AND LANGUAGE MODELS\\n\\nAlex Tamkin\u2217, Kunal Handa\u2217, Avash Shrestha, Noah Goodman\\nStanford University\\n\\nABSTRACT\\n\\nLanguage models have recently achieved strong performance across a wide range of NLP benchmarks. However, unlike benchmarks, real world tasks are often poorly specified, and agents must deduce the user's intended behavior from a combination of context, instructions, and examples. We investigate how both humans and models behave in the face of such task ambiguity by proposing AmbiBench, a new benchmark of six ambiguously-specified classification tasks. We evaluate humans and models on AmbiBench by seeing how well they identify the intended task using 1) instructions with varying degrees of ambiguity, and 2) different numbers of labeled examples. We find that the combination of model scaling (to 175B parameters) and training with human feedback data enables models to approach or exceed the accuracy of human participants across tasks, but that either one alone is not sufficient. In addition, we show how to dramatically improve the accuracy of language models trained without large-scale human feedback training by finetuning on a small number of ambiguous in-context examples, providing a promising direction for teaching models to generalize well in the face of ambiguity.\\n\\n1 INTRODUCTION\\n\\nLanguage models have recently been applied to a wide range of NLP benchmarks, ranging from question answering, summarization, and logical reasoning, to solving riddles, dark humor detection, and ASCII word recognition (Brown et al., 2020; Srivastava et al., 2022). Performance across tasks has improved as models and datasets have grown in size, raising the prospect of a route towards generalist NLP models with broad utility.\\n\\nHowever, one feature many of these benchmarks share is that they are carefully designed to make the desired task very clear to the language model, since this is a prerequisite for establishing performance on that task. Unfortunately, real-world uses of language models are not likely to feature such precision.\\n\\nFigure 1: Complex tasks are often hard to specify precisely, leaving important pieces of information missing. Agents should be able to fill in the blanks by combining information from instructions and examples in order to identify the intended behavior.\\n\\n\u2217Equal Contribution. Correspondence to atamkin@cs.stanford.edu\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"thought and clarity in their task specification. Rather than iterating over and perfecting a specifica-\\n\\ntion for their tasks, everyday users of language models may wish to define tasks on an as-needed\\nbasis, without worrying that they will be misunderstood. More pressingly, in complex domains\\n\\nfeaturing high-dimensional inputs and outputs (e.g. programming, verification, generation) it is un-\\n\\nlikely that even a thoughtful task specification will manage to perfectly capture all the features of\\n\\nan input and output which are salient or not salient to the task. This is especially important for safe\\n\\nand robust deployment of language models, as such undesirable dependencies can be hidden hazards\\n\\nthat are only revealed when a model fails catastrophically in a new setting (Geirhos et al., 2020).\\n\\nTo operationalize this problem, we introduce AmbiBench, a new benchmark of six ambiguously-\\n\\nspecified tasks. Each input in AmbiBench is a sentence (e.g. The dog is in the meadow) that\\n\\nhas multiple associated classification tasks based on different linguistic features (e.g. contains an\\n\\nanimal, contains an outdoor location). Task ambiguity arises when more than one task is consistent\\n\\nwith the provided instructions or labeled examples.\\n\\nWe establish how well different models and humans perform on ambiguously-specified tasks, given\\n\\na wide range of task specifications including clear vs unclear instructions and zero vs multiple exam-\\n\\nples. We find that the largest models trained with human feedback data (HFD) match or outperform\\n\\nhuman participants across all specifications we try, though all underperform a Bayesian oracle.\\n\\nWe also show how to improve standard language models' performance by finetuning them on a small\\n\\nset of in context examples that demonstrate the desired generalization. This form of meta-learning\\n\\ndramatically improves a model's ability to learn new ambiguously-specified tasks. This suggests\\n\\na possible mechanism for why the HFD models outperform standard language models (discussed\\n\\nin Section 4.4), as well as a promising direction for improving how models learn in ambiguous\\n\\ncontexts.\\n\\nTo summarize our contributions, we:\\n\\n1. Introduce and motivate the problem of studying task ambiguity in large language models\\n2. Evaluate humans and models on a new benchmark of ambiguously-specified tasks, demon-\\n\\nstrating that while pure language models fail to disambiguate the intended task well,\\n\\nsufficiently-large models trained with human feedback data are able to approach or even\\n\\nexceed the performance of our human participants to resolve the ambiguity between tasks\\n\\n3. Show how finetuning on ambiguous in-context prompts and examples can enable traditional\\n\\nlanguage models to surpass the performance of HFD models when evaluated on unseen\\n\\ntasks, providing a promising route towards models that capably manage task ambiguity\\n\\n2 RELATED WORK\\n\\n2.1 AMBIGUITY IN NATURAL LANGUAGE PROCESSING\\n\\nAmbiguity is a well-studied topic in NLP, with work spanning topics as diverse as search queries\\n\\n(Cronen-Townsend & Croft, 2002; Wang & Agichtein, 2010), question answering (Min et al., 2020;\\n\\nZhang & Choi, 2021), named entities (Bunescu & Pasca, 2006; Cucerzan, 2007; Dredze et al.,\\n\\n2010), coreference resolution (Webster et al., 2018), machine translation (Stanovsky et al., 2019),\\n\\nand information-seeking dialogues (Aliannejadi et al., 2019; Guo et al., 2021; Aliannejadi et al.,\\n\\n2021; Sun et al., 2022; Wu et al., 2022).\\n\\nOur work differs from these prior streams of work by studying task ambiguity (Finn et al., 2018;\\n\\nTamkin et al., 2022c), where the task the agent is being asked to perform is ambiguous, rather than\\n\\nan ambiguous input for a clear task. This is of special relevance for self-supervised learning models\\n\\nthat are trained for adaptation to a broad range of downstream tasks (Bommasani et al., 2021; Tamkin\\n\\net al., 2022b). In these settings, models must infer the correct task from a user's specification, as\\n\\nopposed to a possibly unsafe or undesirable task that is also consistent with that specification.\\n\\nImportantly, task ambiguity is distinct from clearly-specified tasks with ambiguous inputs, e.g. deter-\\n\\nmining the referent of the pronoun in sentences like the nurse handed the doctor her phone. Here, the task is clear\\n\\n(determine who her refers to), but there is not enough information in the input to answer it.\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2 IN-CONTEXT LEARNING AND PROMPTING\\n\\nTask ambiguity is especially relevant for language models, which can be adapted for many different tasks via in-context learning (Brown et al., 2020; Tamkin et al., 2021a; Bommasani et al., 2021; Liu et al., 2022b), and may rely on undesirable different linguistic features to solve a task (Gururangan et al., 2018; Tamkin et al., 2020). Much work has attempted to improve the ability of such models to perform in-context learning by calibrating model predictions (Zhao et al., 2021), choosing good examples for the prompt (Liu et al., 2022a), finetuning models on natural language descriptions of tasks (Zhong et al., 2021; Wei et al., 2022; Sanh et al., 2022), or by training models with reinforcement learning from human feedback (Bai et al., 2022; Ouyang et al., 2022).\\n\\nPrior work has suggested that language models may not effectively learn from the provided instructions (Webson & Pavlick, 2022) or few-shot examples (Min et al., 2022b; Kim et al., 2022); instead such models may rely on cues such as the formatting of the examples or the label space. In this work, we present a way to measure how well models use instructions or few-shot examples that is unaffected by such cues, because each AmbiBench example is consistent with multiple possible tasks. Thus, models that perform well on AmbiBench must infer the desired task using e.g., the task instruction or other examples. This enables a clean empirical investigation of how well large language models serve as Bayesian reasoners, as past work has hypothesized (Xie et al., 2021).\\n\\nPast work has also explored finetuning on in-context learning examples (Chen et al., 2022; Min et al., 2022a). We extend this line of work to show how the content of these training examples can dramatically affect generalization: Finetuning on ambiguously-specified examples (but not a control set of unambiguous tasks) can enable models to disambiguate better in new settings\u2014vastly improving the performance of pure language models without the need for human feedback data.\\n\\n2.3 TASK AMBIGUITY\\n\\nSystems capable of performing different tasks may experience task ambiguity, where the provided examples do not uniquely identify the user's intended task (Finn et al., 2018; Tamkin et al., 2021a). One form of task ambiguity is shortcut learning (Geirhos et al., 2020), where the training examples can all be solved by identifying a simple feature (e.g. a watermark) as opposed to learning the intended task (e.g. object classification). Task ambiguity is particularly important in few-shot learning settings, where the small number of examples may leave the intended task ambiguous (Finn et al., 2018; Tamkin et al., 2021a). In this work, we study task ambiguity for in-context learning of simple linguistic tasks, considering not only the role of examples but also natural language instructions.\\n\\n3 THE AMBI BENCHMARK\\n\\nAs a first step towards studying task ambiguity in language models, we construct the AmbiBench benchmark, a collection of six different sentence classification tasks. The goal of AmbiBench is to construct a testbed of minimal complexity where we can control and measure the degree of ambiguity in various task specifications. Despite the simplicity of this benchmark, we find large variability in performance across different language models.\\n\\n3.1 SELECTION OF TASKS\\n\\nAmbiBench contains six binary classification tasks, where a human or model must detect a simple linguistic feature in an input sentence\u2014for example, whether an outdoor location or an animal was mentioned\u2014and then output the appropriate classification letter (X or Y). Crucially, however, each sentence has two linguistic features (e.g. The duck is in the canyon has the features animal and outdoor location). The six features are grouped into three pairs, shown in Table 1, where a single sentence will have one feature in each pair.\\n\\nTo identify the salient feature for the task, then, one must have either an informative instruction (e.g., Output 'X' if the sentence contains an outdoor location and 'Y' otherwise) or multiple labeled examples to disambiguate which feature determines the label.\\n\\nTasks were chosen to represent a set of common semantic categories, excluding subtoken information such as periods and capitalization that might be much easier for humans to represent than...\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For the tests in Section 4.3, participants were shown examples one-by-one with the correct answer appearing on the screen following their input. For the tests in Section 4.2, participants were shown both labeled examples and the one unlabeled query example at the same time.\\n\\nFor the tests in Section 4.3, each participant was only given one set of 20 examples. For the tests in Section 4.2, each participant was only given a single prompt. Prompts given to participants were randomized and roughly evenly distributed across all tests. Participant IDs were filtered to ensure that the same participant did not participate multiple times across studies. All surveys were created using Qualtrics' survey builder.\\n\\nF.1 NUMBER OF PARTICIPANTS FOR HUMAN EXPERIMENTS\\n\\nTask disambiguation using natural language instruction\\nFor the experiment referenced in Section 4.2 we recruited 51 and 33 participants for the unambiguous instruction and informative instruction tests respectively.\\n\\nTask disambiguation using multiple examples\\nFor the experiment detailed in Section 4.3, we recruited 96 total participants: 16 per salient feature.\\n\\nAmbiBench consists of ambiguous sentences with two explicitly varying (salient) features in each sentence. Salient features are grouped into pairs: 'subject' and 'location,' 'religious' and 'pronoun,' and 'proper noun' and 'negation.' Each salient feature has two values (e.g. animal subject or human subject) with each choice having a 0.5 probability of being selected.\\n\\nG.1 SALIENT FEATURE PAIRINGS\\nThe pairings of the salient features are detailed below. The pairings remained constant throughout all experiments.\\n\\nG.1.1 SUBJECT-LOCATION SENTENCES\\nSentences follow the template: \\\"The \\\\{human/animal subject\\\\} is in the \\\\{indoor/outdoor location\\\\}.\\\"\\n\\nThe subject is randomly assigned to either a human or an animal. If chosen to be a human, the...\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"subject is randomly chosen from a list of professions: [student, reporter, hiker, researcher, firefighter, fugitive, critic, photographer, director, surveyor]. If chosen to be an animal, the subject is randomly chosen from a list of common animals: [boar, worm, hawk, hound, butterfly, snake, duck, bear, mountain lion, horse]. The location is randomly assigned to either an outdoor location or an indoor location. If chosen to be an outdoor location, the location is chosen from a list of common outdoor locations: [river, pond, woodlands, cave, canyon, prairie, jungle, marsh, lagoon, meadow] whereas if chosen to be an indoor location, the location is chosen from a list of common indoor locations: [laboratory, theatre, museum, courtroom, apartment building, restaurant, house, film studio, hotel lobby, grocery store].\\n\\nG.1.2 Religious Pronoun Sentences\\n\\nSentences follow the template: \"{She/He} is in the {indoor location} with the {religious/secular leader}.\" The pronoun is randomly assigned to either a He or She. The leader is randomly assigned to either a religious leader or a secular leader. If chosen to be a religious leader, the leader is chosen from a list of common religious leaders: [pope, reverend, bishop, Dalai Lama, rabbi, cardinal, pastor, deacon, imam, ayatollah] whereas if chosen to be a secular leader, the leader is chosen from a list of common secular leaders: [president, CEO, principal, sheriff, judge, ambassador, officer, prime minister, colonel, professor]. The indoor location in the sentence is randomly chosen from the aforementioned list of indoor locations.\\n\\nG.1.3 Proper/Noun Negation Sentences\\n\\nSentences follow the template: \"{Proper/Common noun} {negation/affirmation} in the {indoor location}.\" The noun is randomly assigned to either a proper noun or a common noun. If chosen to be a proper noun, the noun is randomly chosen from a list of famous individuals across a variety of disciplines: [Lebron James, Bernie Sanders, Christopher Nolan, Paul Atreides, Noam Chomsky, Serena Williams, Margot Robbie, Alexandria Ocasio-Cortez, Hermione Granger, Jane Goodall]. If chosen to be a common noun, the noun is randomly chosen from the prior list of human subjects: [student, reporter, hiker, researcher, firefighter, fugitive, critic, photographer, director, surveyor] and appended to 'The' to maintain grammaticality of the sentence. The verb is randomly assigned to either a negation or an affirmation. If chosen to be a negation, the verb is chosen from a list of common negation verb phrases: [is not, was not, has not been, may not be, could not be]. If chosen to be an affirmation, the verb is chosen from the list of affirmations directly contrasting the list of negations: [is, was, has been, may be, could be].\\n\\nG.2 Instructions\\n\\nInstructions given were either informative instructions, explicitly stating the salient feature which determined the label for each sentence, or uninformative instructions which did not divulge the salient feature. Informative Instructions were of the template: \\\"Output 'X' if the sentence {salient feature instruction} and 'Y' otherwise\\\", where salient feature instruction could be filled in with one of {contains a reference to an indoor/outdoor location, contains a reference to a human/an animal, contains a reference to a religious leader, does not contain a reference to a religious leader, contains a reference to a religious leader, contains a male pronoun, contains a female pronoun, contains a proper noun, does not contain a proper noun, contains a negation, does not contain a negation}. Uninformative instructions were always given as \\\"Output 'X' if the sentence contains a [category withheld] and 'Y' otherwise.\\\"\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"groupings, therefore displaying which of the features was responsible for the example's label (the salient feature).\\n\\nHere is a demonstration of a single prompt for an example with the Subject-Location sentence type:\\n\\nQ: The hawk is in the canyon.\\nA: X\\n\\nQ: The director is in the museum.\\nA: Y\\n\\nQ: The hiker is in the meadow.\\nA: X\\n\\nFor the first two examples, it is unclear whether the subject or the location is controlling the label but upon seeing the third example, it becomes clear that the location and not the subject is controlling the label (as the sentence is labeled X whenever an outdoor location is referenced).\\n\\nTests contained either an uninformative instruction or an informative instruction followed by three examples. Humans and models were shown all three examples at once. Humans were shown the first two examples with labels and the last example without the label. Models were shown all three examples with labels within the same API query (as the log probabilities for the final label can be acquired without requiring a completion).\\n\\nThe order of groupings, order of labels, labels' correspondence with a salient feature, and the salient feature were randomized across all tests with equal probabilities being given to each possibility.\\n\\nSome example prompts from the test set:\\n\\nPrompt 1\\nSalient feature is 'subject' (Q/A format with informative instructions):\\nOutput 'X' if the sentence contains a reference to a human and 'Y' otherwise.\\n\\nThe horse is in the woodlands.\\nA: Y\\n\\nQ: The student is in the laboratory.\\nA: X\\n\\nQ: The mountain lion is in the film studio.\\nA: Y\\n\\nPrompt 2\\nSalient feature is 'pronoun' (arrow format with uninformative instructions):\\nOutput 'X' if the sentence contains a \\\\[category withheld\\\\] and 'Y' otherwise.\\n\\nHe is in the film studio with the imam.\\n>Y\\n\\nShe is in the restaurant with the judge.\\n>X\\n\\nShe is in the apartment building with the bishop.\\n>X\\n\\nPrompt 3\\nSalient feature is 'proper noun' (Q/A format with uninformative instructions):\\nOutput 'X' if the sentence contains a \\\\[category withheld\\\\] and 'Y' otherwise.\\n\\nQ: The student was not in the hotel lobby.\\nA: X\\n\\nQ: Lebron James is in the theatre.\\nA: Y\\n\\nQ: Christopher Nolan could not be in the house.\\nA: Y\\n\\nWhen constructing tests for Section 4.3, humans and models were shown a set of 20 questions with the same salient feature. All sentences within the set contained the same sentence pairing Subject-Location, Religious-Pronoun, Proper Noun-Negation.\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For each example, the possibilities for each of the two features were randomized and given equal probability (as described in the process of sentence construction). The groupings mentioned for the tests in Section 4.2 were not used.\\n\\nThe salient feature was randomly selected (with each possibility being given an equal probability). Tests always contained an uninformative instruction prior to the set of examples.\\n\\nHumans were shown questions one at a time, progressively building up from 1 to 20 questions. After each question, the correct answer was displayed on the screen (in place of their solution if incorrect).\\n\\nModels were shown all 20 questions within the same API query and the log probabilities for each of the twenty labels were tracked.\\n\\nAs with the tests in Section 4.2, the labels' correspondence with a salient feature and the order of labels were randomized across all tests with equal probabilities being given to each possibility.\\n\\nSome example prompts from the test set:\\n\\nPrompt 1\\n\\n- Salient feature is 'location' (arrow format):\\n  - Output 'X' if the sentence contains a [category withheld] and 'Y' otherwise.\\n  - The photographer is in the restaurant.  \\n    - >Y\\n  - The mountain lion is in the river.  \\n    - >X\\n  - The hiker is in the cave.  \\n    - >X\\n  - The butterfly is in the grocery store.  \\n    - >Y\\n  - The surveyor is in the river.  \\n    - >X\\n  - The boar is in the prairie.  \\n    - >X [ continues until 20 examples]\\n\\nPrompt 2\\n\\n- Salient feature is 'religious' (Q/A format):\\n  - Output 'X' if the sentence contains a [category withheld] and 'Y' otherwise.\\n  - Q: He is in the film studio with the ayatollah.  \\n    - A: X\\n  - Q: She is in the house with the CEO.  \\n    - A: Y\\n  - Q: He is in the apartment building with the ambassador.  \\n    - A: Y\\n  - Q: She is in the museum with the rabbi.  \\n    - A: X\\n  - Q: She is in the museum with the Dalai Lama.  \\n    - A: X\\n  - Q: He is in the laboratory with the rabbi.  \\n    - A: X [ continues until 20 examples]\\n\\nPrompt 3\\n\\n- Salient feature is 'negation' (arrow format):\\n  - Output 'X' if the sentence contains a [category withheld] and 'Y' otherwise.\\n  - The student was not in the restaurant.  \\n    - >X\\n  - Christopher Nolan was in the apartment building.  \\n    - >Y\\n  - The photographer has not been in the theatre.  \\n    - >X\\n  - Alexandria Ocasio-Cortez could be in the film studio.  \\n    - >Y\\n  - Christopher Nolan was not in the museum.  \\n    - >X [ continues until 20 examples]\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Accuracy of text-davinci-002 for guessing the [category withheld] for each salient task. Q/A format only (N=10).\\n\\n(b) Accuracy of text-davinci-003 for guessing the [category withheld] for each salient task. Arrow and Q/A formats (N=20).\\n\\nFigure 5: Even models that perform very well on a task struggle to describe that task in words.\\n\\nAs models attempt to disambiguate between different tasks, a user may wish to know the model's best estimate of the task it is being asked to perform. In this section, we assess whether models can verbalize the task descriptions in Section 4.3 at the end of the in-context prompt. Specifically, after the 20 example prompt we append a newline with the strings\\n\\n\u2022 \u201cWhat is the [category withheld]?\u201d for text-davinci-002,\\n\\n\u2022 \u201cWhat is your best guess for the [category withheld] above?\u201d for text-davinci-003.\\n\\nWe developed the second prompt as the newer text-davinci-003 model would often return responses along the lines of \u201cThis question cannot be answered without further context\u201d for the first prompt.\\n\\nWe generate 10 outputs for each of the 6 tasks, and manually categorize them as one of three categories:\\n\\n\u2022 Correct task: A correct verbalization of the task (e.g. \u201cThe [category withheld] is a religious leader.\u201d)\\n\\n\u2022 Incorrect Task: The model guesses a task, but it is not the correct task. (e.g. \u201canimals\u201d when the salient task is \u201coutdoor location\u201d)\\n\\n\u2022 Incorrect (Other): The verbalization does not mention a task (e.g. \u201cThe category is withheld.\u201d)\\n\\nAs shown in Figure 5, the models are sometimes able to verbalize the correct task, especially for religious figures and proper nouns, although for all other tasks it struggles. These initial experiments suggest that task verbalization is challenging even for the most capable models, even when they perform very well at the task, and suggests an exciting direction for future study.\\n\\nNote that our graph for text-davinci-002 exclusively considers the Q/A format, as we did not observe in a preliminary investigation that the model could verbalize the task successfully with the arrow format. Additionally, in preliminary investigations we did not find that other models were able to successfully verbalize the task.\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section we explore how the methodologies we introduce in AmbiBench can be extended to more real-world settings. Here, we consider the case of a natural language assistant for command-line tasks. In our hypothetical setting, an employee of a company prompts a language model to produce Amazon Web Services buckets for different users by providing it with two input-output examples of the desired behavior. By chance, the two training examples both use Japanese names and have the bucket location set to the \\\\textit{ap-northeast-1} region in Japan. The test examples ask for a bucket to be made for a person with a non-Japanese name (in our case, White American, Greek, or Indian names).\\n\\nConcretely, examples look like the following:\\n\\n- Write the AWS CLI command to create an AWS Bucket.\\n  - Input: Create a bucket for Sato Tamotsu\\n  - Output: \\\\texttt{aws s3 mb s3://bucket-for-sato --region ap-northeast-1}\\n  - Input: Create a bucket for Yuki Hashimoto\\n  - Output: \\\\texttt{aws s3 mb s3://bucket-for-yuki --region ap-northeast-1}\\n  - Input: Create a bucket for Margaret Richards\\n  - Output: \\\\texttt{aws s3 mb s3://bucket-for-margaret --region us-east-1}\\n\\nThe task ambiguity that arises is that sometimes the model assumes the bucket should be placed in the same region as the training examples, but in other cases it assumes the bucket should be placed in a different region based on the person's name. We omit other commandline arguments for readability, however we note that this ambiguity might be very difficult to notice in practice with long commands consisting of many arguments.\\n\\nIn Figure 6 we show that not only does this ambiguity manifest in the text-davinci-002 language model outputs (which can output both regions), but it also varies based on the national origin of the person's name. White American names induce the model to output other regions (typically ones in the US and Europe) far more than Indian or Greek names do. However, after one additional name from that same national origin is added (indicating that the name should not determine the region) the model performs nearly perfectly at other names of this national origin.\\n\\nThis initial investigation illustrates how task ambiguity can manifest in a more real-world setting, and how we can measure it using the techniques we discuss in the rest of the paper. Future work could explore finetuning on similar in-context examples to meta-train the model such that it avoids assuming that an individual's name should impact the task behavior, for example.\\n\\n\\\\textbf{B.1 List of Names} \\n\\nHere we provide the full list of names we used to query the model. Names were constructed from a list of popular first and last names.\\n\\n- **Japanese names**: Sato Tamotsu, Yuki Hashimoto, Yamamoto Mitsuo, Kuroda Kumiko, Mita Naoki, Tanaka Sakura, Kuramoto Hideki, Sazama Miyuki, Hora Izanagi, Itoh Akane\\n- **White American names**: Trevor Fitzpatrick, Jessica Price, Logan Evans, Leah Mills, Ava Smith, Cole Jones, Isabella Brown, Emily Davis, Mia Miller, Madison Wilson\\n- **Indian names**: Rajesh Kumar, Abhishek Yadav, Rahul Singh, Archana Kumar, Anisha Patel, Priya Sharma, Riya Gupta, Ayush Jain, Prisha Rao, Sagar Goyal\\n- **Greek Names**: Yiannis Papadopoulou, Vassilis Karagiannis, Athanasios Ioannou, Dimitra Papadakis, Vasiliki Georgiou, Eleni Oikonomou, Kostas Giannopoulos, Nikos Theodorou, Ioannis Vlachos, Katerina Makris\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) White American Test Names\\n(b) Indian Test Names\\n(c) Greek Test Names\\n\\nFigure 6: Features such as name origin can influence how models behave under task ambiguity. Effect of task ambiguity on text-davinci-002 when generating an AWS bucket command given an ambiguous natural language string. Graph shows distribution of generated regions: ap-northeast-01 (the region in the prompts), another region, or invalid command.\\n\\nFigure 7: Models of all sizes generate valid outputs by the end of the in-context learning episode. Probability of a valid output (either X or Y) for all models across examples.\\n\\nDo smaller models understand the task format? In Section 4.3 we find that smaller models perform worse at disambiguating the intended task from multiple examples. However, is this due simply to models not understanding the desired output format? We validate the output formats of all models and find (Figure 7) that by the end of training, even the smallest models always generate valid outputs (i.e. either X or Y) across the experiments we considered, suggesting that their poor performance on task disambiguation is not simply due to their poor prompting abilities.\\n\\nHow important is the format of the uninformative instructions? As previously noted, our experiments on uninformative examples in the main text use an instruction format containing [CATEGORY WITHHELD]. Here we test whether the behavior of models is sensitive to variations in this phrasing. We replace [CATEGORY WITHHELD] with the linguistic nonce word wug, so a full instruction might read Output 'X' if the sentence contains a wug and 'Y' otherwise. As shown in Figure 8 we find that performance is the same for both tasks, suggesting that the particular form of uninformative instruction is not very important.\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The choice of format for the uninformative instructions has negligible effect.\\n\\n| Model               | Number of Parameters |\\n|---------------------|----------------------|\\n| OpenAI Babbage      | 1.3B                 |\\n| OpenAI Curie        | 6.7B                 |\\n| OpenAI Davinci      | 175B                 |\\n| AI21 J1-Jumbo       | 178B                 |\\n| T0++                | 11B                  |\\n| Flan-T5-XXL         | 11B                  |\\n\\nTable 2: Number of parameters for each model. Note that the OpenAI Ada and all HFD model parameter counts have not been released. It is also possible that the normal and HFD models have different numbers of parameters and were trained on different amounts and kinds of data.\\n\\nParameter count is not necessarily the most informative proxy for a model's capabilities, given the importance of other factors such as training data quality and the total number of tokens seen by the model (Hoffmann et al., 2022).\\n\\nHere we provide more information about the experiments with human participants via Prolific (Palan & Schitter, 2017). Prior to beginning the experiment, Prolific participants were shown a message confirming consent followed by a set of boiler-plate instructions on the upcoming task which stated: 1) \u201cContinue the pattern in the following screens to the best of your ability, using the provided instructions and examples given\u201d 2) \u201cEach time you make a prediction, the next screen will show the correct answer.\u201d\\n\\n---\\n\\n8 [https://beta.openai.com/docs/model-index-for-researchers](https://beta.openai.com/docs/model-index-for-researchers)\\n9 [https://beta.openai.com/docs/model-index-for-researchers](https://beta.openai.com/docs/model-index-for-researchers)\\n10 [https://beta.openai.com/docs/model-index-for-researchers](https://beta.openai.com/docs/model-index-for-researchers)\\n11 [https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1](https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1)\\n12 [https://bigscience.huggingface.co/blog/t0](https://bigscience.huggingface.co/blog/t0)\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: The AmbiBench benchmark.\\n\\nLeft: Each task involves detecting a salient feature in a sentence (bolded in the examples on the right). The same sentence could potentially receive a label according to two features, requiring a learner to use additional information (task instructions or other examples) to disambiguate the intended behavior.\\n\\nRight: Varying levels of instruction are inserted before the examples, providing different degrees of information about the format and salient feature of the task. See Figure 1 for an example of a complete prompt.\\n\\n3.2 TASK CONSTRUCTION\\n\\nAmbiBench examples are programmatically constructed from a set of templates, allowing precise control over the amount of task ambiguity in each in-context example (see Table 1 and Appendix G for more details). Templated data has seen a recent resurgence in NLP for the purposes of evaluating large language models (Lake & Baroni, 2018; Srivastava et al., 2022), as they enable precise control and coverage over different variables of study. Furthermore, recent work has shown strong correlation between test performance on synthetic and naturalistic data Liu et al. (2021), suggesting that insights gained from such datasets may extend to a broader range of natural contexts. In our case, this dataset construction process enables us to formalize and characterize the degree of task ambiguity in different examples, allowing us to measure how well models can disambiguate between multiple potential classification tasks they may be asked to perform.\\n\\n3.3 IN-CONTEXT LEARNING FORMATS\\n\\nThere are several ways an instruction and in-context examples can be assembled into a prompt for a language model. Given the demonstrated sensitivity of models to such parameters (Zhao et al., 2021; Liu et al., 2022b; Lu et al., 2022), we consider two different prompt formats, and report averaged performance across them:\\n\\n**Arrow:** Output 'X' if the sentence contains an outdoor location and 'Y' otherwise.\\n\\nThe worm is in the meadow  > X\\nThe duck is in the canyon  > Y\\n...\\n\\n**Q/A:** Output 'X' if the sentence contains an outdoor location and 'Y' otherwise.\\n\\nQ: The worm is in the meadow  A: X\\nQ: The duck is in the canyon  A: Y\\n...\\n\\n4\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use AmbiBench to investigate how humans and language models respond to and resolve different manifestations of task ambiguity.\\n\\n4.1 EXPERIMENTAL SET UP\\n\\nFirst, we describe the different language models and human participants we study, and how we evaluate them.\\n\\nLanguage models\\n\\nWe examine a range of different models, including both OpenAI's normal language models and their \\\"instruct\\\" models trained with human feedback data (HFD) (Brown et al., 2020; Ouyang et al., 2022). These models are trained using the data described in Ouyang et al. (2022) as well as on highly-rated model generations. In the rest of the paper, OpenAI's model names are reported as listed in their documentation (e.g. davinci, text-curie-001). The instruct models have a numerical suffix (e.g. 002) and the model size increases as one progresses through the alphabet (ada, babbage, curie, davinci). See Appendix E for more information. We also evaluate AI21 Studio's 178B-parameter Jurassic-1 Jumbo language model (jurassic-jumbo) (Lieber et al.), as well as the 11B-parameter T0++ (t0pp) (Sanh et al., 2022) and Flan-T5 (flan-t5) models (Chung et al., 2022), which were finetuned on a large corpus of task instructions. This diversity of model providers, model sizes, and training strategies enables us to identify which ingredients are most crucial for resolving task ambiguity.\\n\\nHuman evaluation\\n\\nWe compare model performance with the performance of human participants, evaluated by hiring contractors from Prolific (Palan & Schitter, 2017). We aimed to evaluate the human participants as similarly to language models as possible within the confines of an online survey methodology. We showed human participants exactly the same input that language models received, with minimal additional information presented to them before the study began. Participants typed the answer label (i.e. X or Y) into a textbox, as opposed to choosing from a set of preselected options, to mitigate priming effects and mirror the setting for language models. We also recruited a new participant for every single in-context instance, to avoid humans learning across examples in ways that language models do not. Human participants were paid $12-13/hr, in line with Prolific wage recommendations.\\n\\n4.2 TASK DISAMBIGUATION USING NATURAL LANGUAGE INSTRUCTIONS\\n\\nOne way that people resolve task ambiguity is through the use of natural language instructions, which can explicitly indicate different aspects of the task. Past work has suggested that the best models do not fruitfully use natural-language instructions, as evidenced by experiments leveraging irrelevant or misleading directions (Webson & Pavlick, 2022). However, these experiments were performed for established natural language processing tasks that lack the explicit task ambiguity we study here, and did not investigate more recent models trained with human feedback data (Bai et al., 2022; Ouyang et al., 2022).\\n\\nAs a first set of experiments, we evaluate how humans and models are able to use differing levels of instruction to resolve task ambiguity. The humans and models receive two in-context examples, one from each class. Humans and models are then presented with a third query example in order to elicit the predicted output letter. Because there is only one example of each class, but two possible features, the salient feature cannot be identified from these two examples alone, requiring the model to use the instruction to disambiguate the task. The order of the examples, the example format, as well as the assignment of each class to an output letter (X or Y) are randomized. Each model is evaluated with 720 different in-context prompts for each level of instruction.\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. **Informative instruction**: The model receives a full specification of the salient feature and output format. Ex: Output \u2018X\u2019 if the sentence contains an animal and \u2018Y\u2019 otherwise.\\n\\n2. **Uninformative instruction**: The model receives the output format but the salient feature is redacted. Ex: Output \u2018X\u2019 if the sentence contains a [category withheld] and \u2018Y\u2019 otherwise.\\n\\nOur setting is simple enough that crafting an informative instruction is not challenging, making it tractable for us to study. However, the insights from this simple case may generalize to more complex settings where users may be prone to accidentally omit crucial information from the prompt.\\n\\n### 4.2.1 Results\\n\\nIn the case of uninformative instructions, humans as well as many models are able to achieve approximately 50% accuracy by correctly understanding the output format and choosing \u2018X\u2019 and \u2018Y\u2019 at random. However, some non-instruct models, including `jurassic-jumbo`, `ada`, `babbage`, and `curie`, often output values other than \u2018X\u2019 or \u2018Y\u2019 (e.g. \u2018Z\u2019), leading to lower performance. Finally, in the case of negation, humans achieve 100% accuracy despite lacking an instruction identifying the salient feature. This may be due to an inductive bias present in people (but not models) that makes negation an especially salient feature.\\n\\nIn the case of informative instructions, humans perform the strongest at this task, with perfect performance in all but one task, showing that they are broadly able to identify the salient feature in the text inputs and output the correct letter. Humans are closely followed by the `text-davinci-003` and `text-davinci-002` HFD models (see Figure 2). All other models perform relatively poorly, including the non-HFD 175B+ parameter `davinci` and `j1-jumbo` models, as well as the smaller HFD models `curie`, `babbage`, and `ada` (although we verify in Section that these models are still able to generate outputs in the correct format). This seems to suggest that most models are not reliably able to follow simple instructions to disambiguate a task, but that a combination of large-scale training and HFD can approach human performance in some settings.\\n\\n### 4.3 Task Disambiguation Using Multiple Examples\\n\\nWhile instructions are a simple way to specify a task, multiple examples can also disambiguate between different tasks a user might intend. For example, if there are multiple features that could explain the label of a single example, more examples will gradually identify the salient feature provided the features are sufficiently decorrelated.\\n\\nWe investigate whether models and humans can identify the salient features in AmbiBench as the number of examples grows from zero (where the task is completely ambiguous) to twenty (where the task is almost certainly unambiguous). Both models and human participants predict the answer for each example, then are presented with the correct answer for that example and the next query.\\n\\nAll aspects of the examples are randomized, including the salient feature (chosen randomly, then held constant across the entire in-context example), the assignment of \u2018X\u2019 or \u2018Y\u2019 to the salient feature, the example order, and the specific instantiations of the salient and non-salient features for each example. Each model is evaluated with 720 different in-context prompts, each containing 20 examples. We also compare humans and models with a Bayesian oracle that represents how well an optimal learner could perform on the benchmark. This oracle performs perfectly as soon as it sees a set of examples which disambiguate the intended task, and performs at chance otherwise.\\n\\n### 4.3.1 Results\\n\\nTo our surprise, the best language model (the HFD-trained `text-davinci-002`) significantly outperformed the human participants (Figure 3). The human participants performed comparably to the `j1-jumbo` and `curie` models, which in turn performed better than the rest of OpenAI\u2019s models. The `flan-t5` and `t0pp` models performed worse, with the latter exhibiting large sensitivity to the prompt format\u2014`t0pp` outputted invalid answers (typically nouns) for the `arrow` format.\\n\\nHowever, considering only the Q/A format, `t0pp` still only performed near chance. All models...\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The best HFD model (text-davinci-003) approaches human accuracy for both uninformative and informative instructions.\\n\\nFigure 3: The best HFD models (text-davinci-002 and text-davinci-003) outperform human participants at disambiguating the intended task. Accuracy as the number of examples in the in-context window grows. Surprisingly, the smaller curie model reliably outperforms the larger davinci model across the examples. In addition, the HFD training hurts at curie scale, but dramatically helps at davinci scale. Shaded regions are 95% bootstrap CIs.\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 18: Performance across salient features on Task Disambiguation Using Multiple Examples (see Section 4.3)\\n\\n(a) Averaged over all models except topp\\n\\n(b) topp note: 'Arrow' format is at 0.0 throughout\\n\\nFigure 19: Performance of each format on Task Disambiguation Using Multiple Examples (see Section 4.3)\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Finetuning on ambiguous in-context examples dramatically improves accuracy on unseen tasks that are ambiguously specified. Accuracy after finetuning davinci on ambiguous and non-ambiguous (control) in-context examples. Models are finetuned on 272 examples from four tasks, then evaluated on the two held-out tasks (subfigure captions). Shaded regions are 95% bootstrap CIs.\\n\\nWe do not observe evidence that the imperfect human performance is due to low-quality participants or bot activity\u2014human annotators mostly spent between 4 and 8 minutes on the task, did not appear to be guessing at random, and typically left thoughtful comments or feedback on the survey. That said, we caution against claims of \u201csuperhuman performance\u201d given that annotators represent merely a sample from a single distribution of humans, and they may have experienced fatigue or distraction across the 20-example episode.\\n\\n4.4 Finetuning a Model to Generalize Well in the Face of Ambiguity\\n\\nThe strong performance of the HFD models relative to the normal language models in Section 4.3 is somewhat surprising\u2014these models are described as being trained to follow human instructions, not to resolve ambiguity in instructions by analyzing the training examples. While the training dataset of these models was not released, Ouyang et al. (2022) do report that some of the crowdsourced examples for the model contain instructions along with few-shot examples. If some of these instructions were ambiguous, the model may have learned from those examples to resolve that ambiguity more effectively.\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Motivated by this hypothesis, we investigate whether finetuning on a small corpus of ambiguous in-context learning examples is sufficient to close the gap between the best-performing text-davinci-002 HFD model and the normal davinci language model. To do so, we partition the six AmbiBench tasks into three folds, each containing four finetuning tasks and two evaluation tasks (following the feature pairs in Table 1). We finetune on 68 examples from each task (two for each number of examples, from 4 to 20), and evaluate on 240 examples randomly drawn from the other two tasks. While all tasks share some structural similarities, this partitioning ensures that the model is being tested on held-out features that never appeared in its finetuning dataset. Models are finetuned using the OpenAI API (see Appendix H for details).\\n\\nTo see whether ambiguous data is the key factor when finetuning, we also finetune on unambiguous versions of this data, where only one feature varies within each in-context example. For example, if the two features are animal and indoor location, a given in-context example may contain examples with both animals and humans, but only indoor locations. See Appendix H for more details.\\n\\n4.4.1 RESULTS\\n\\nDespite the small training dataset consisting of only 4 tasks (with 272 examples total), we find we are able to completely close the gap between the HFD models and our finetuned models across all three splits of our data. Indeed, our finetuned models appear to even outperform text-davinci-002 across the first eight examples, closing part of the gap to the Bayesian oracle. Crucially, we do not observe any improvement for the control finetuned models, which were finetuned on the same kinds of examples but without task ambiguity between two potential salient features. This indicates that ambiguity is the crucial ingredient explaining the success of our finetuned models, and supports the hypothesis that the few-shot examples in text-davinci-002\u2019s human feedback data may contribute to its strong performance.\\n\\nMore broadly, these results suggests that explicitly finetuning models to adapt to task ambiguity may result in a generalized capacity to do so across different kinds of ambiguous task specifications.\\n\\n5 DISCUSSION AND CONCLUSION\\n\\nWe present the AmbiBench testbed for studying task ambiguity in language models and humans, showing how it can be used to investigate different factors influencing task ambiguity, as well as identify promising interventions that can improve how models resolve it.\\n\\n5.1 LIMITATIONS\\n\\nOur study has several limitations. First, we conduct a scientific and controlled study of task ambiguity in language models; this naturally elides many of the messy nuances of task ambiguity in the real world, and should be seen as complementary to in-the-wild case studies. We explore one such real-world use case in Appendix B, however more work is needed. Second, despite our efforts to match the experimental conditions between humans and language models, humans do require some additional instructions to orient them to the task interface, and may suffer from fatigue and uneven concentration across the length of a 20-example learning episode. Finally, our work studies task ambiguity between two possible tasks\u2014however, in general task ambiguity may occur between arbitrarily many tasks, or even an infinitely large family of tasks.\\n\\n5.2 FUTURE WORK\\n\\nTask ambiguity is a pressing problem in machine learning with relevance for safety, fairness, and interpretability. Going forward, we are excited by the potential to study task ambiguity in self-supervised models trained on many different modalities (Reed et al., 2022; Tamkin et al., 2021b; 2022a; Alayrac et al., 2022), including multimodal settings, as self-supervised learning is applied increasingly broadly. The strong performance of models on the AmbiBench testbed also suggests the tractability of studying task ambiguity in more complex real-world settings where language models are used, such as software engineering, law, and education, as well as assessing the efficacy of our proposed finetuning interventions.\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgments and Funding Disclosure\\n\\nWe'd like to thank Ben Prystawski, Shyamal Buch, and Rose Wang for useful conversations and feedback, as well as all the participants who took our study. We thank the Center for Research on Foundation Models (CRFM) and Together Computer for facilitating access to models. AT is supported by an Open Phil AI Fellowship.\\n\\nEthics statement\\n\\nOur research makes use of human subject experiments via the Prolific platform (Palan & Schitter, 2017). We pay workers a minimum of $12-13 / hour, consistent with Prolific wage recommendations.\\n\\nWe also made efforts to solicit feedback from participants via pilot studies, which led to several changes to the research methodology to make the survey experience more pleasant (e.g. keyboard shortcuts to navigate the study more efficiently). Anecdotally, many participants expressed that they enjoyed the study:\\n\\n1. Zap! I'm suddenly back in high school with Ms. Langston's English class. Thank you for the smiles!\\n2. This was fun!\\n3. This was a really good study\\n\\nReproducibility statement\\n\\nWe include detailed experimental settings in Sections 1, 4.2, 4.3, 4, E, H, G. We also release our codebase, including the benchmark data, at: https://github.com/kunhanda/task_ambiguity\\n\\nREFERENCES\\n\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. ArXiv, abs/2204.14198, 2022.\\n\\nMohammad Aliannejadi, Hamed Zamani, Fabio A. Crestani, and W. Bruce Croft. Asking clarifying questions in open-domain information-seeking conversations. Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2019.\\n\\nMohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeffrey Dalton, and Mikhail S. Burtsenev. Building and evaluating open-domain dialogue corpora with clarifying questions. ArXiv, abs/2109.05794, 2021.\\n\\nYushi Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, T. J. Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. ArXiv, abs/2204.05862, 2022.\\n\\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir P. Mirchandani, Eric Mitchell, Zanele Okonjo, Haoyang Peng, Ankit Prasad, Shreyas Ranade, Preeti Ramakrishnan, Komal Rawat, Shree Satyapal, Karl Sims, Zhenyu Su, Bo Sun,ional S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir P. Mirchandani, Eric Mitchell, Zanele Okonjo, Haoyang Peng, Ankit Prasad, Shreyas Ranade, Preeti Ramakrishnan, Komal Rawat, Shree Satyapal, Karl Sims, Zhenyu Su, Bo Sun, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir P. Mirchandani, Eric Mitchell, Zanele Okonjo, Haoyang Peng, Ankit Prasad, Shreyas Ranade, Preeti Ramakrishnan, Komal Rawat, Shree Satyapal, Karl Sims, Zhenyu Su, Bo Sun, Qiu, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir P. Mirchandani, Eric Mitchell, Zanele Okonjo, Haoyang Peng, Ankit Prasad, Shreyas Ranade, Preeti Ramakrishnan, Komal Rawat, Shree Satyapal, Karl Sims, Zhenyu Su, Bo Sun, Tang, and Jiaya Jia. DoGMA: a method for predicting and measuring questionability of open-domain dialogue. ArXiv, abs/2206.05264, 2022.\\n\\nRishi Bommasani, Deepesh Gupta, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir P. Mirchandani, Eric Mitchell, Zanele Okonjo, Haoyang Peng, Ankit Prasad, Shreyas Ranade, Preeti Ramakrishnan, Komal Rawat, Shree Satyapal, Karl Sims, Zhenyu Su, Bo Sun, Tang, and Jiaya Jia. DoGMA: a method for predicting and measuring questionability of open-domain dialogue. ArXiv, abs/2206.05264, 2022.\"}"}
{"id": "QrnDe_9ZFd8", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, J. F. Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher R'e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram\u00e8r, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei A. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models. ArXiv, abs/2108.07258, 2021.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.\\n\\nRazvan C. Bunescu and Marius Pasca. Using encyclopedic knowledge for named entity disambiguation. In EACL, 2006.\\n\\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language model in-context tuning. ArXiv, abs/2110.07814, 2022.\\n\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\\n\\nSteve Cronen-Townsend and W. Bruce Croft. Quantifying query ambiguity. 2002.\\n\\nSilviu Cucerzan. Large-scale named entity disambiguation based on wikipedia data. In EMNLP, 2007.\\n\\nMark Dredze, Paul McNamee, Delip Rao, Adam Gerber, and Timothy W. Finin. Entity disambiguation for knowledge base population. In COLING, 2010.\\n\\nChelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In NeurIPS, 2018.\\n\\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard S. Zemel, Wieland Brendel, Matthias Bethge, and Felix Wichmann. Shortcut learning in deep neural networks. Nat. Mach. Intell., 2:665\u2013673, 2020.\\n\\nM. Guo, Mingda Zhang, Siva Reddy, and Malihe Alikhani. Abg-coqa: Clarifying ambiguity in conversational question answering. In AKBC, 2021.\\n\\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In North American Chapter of the Association for Computational Linguistics, 2018.\\n\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hohnnigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022.\\n\\nJunyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang goo Lee, Kang Min Yoo, and Taeuk Kim. Ground-truth labels matter: A deeper look into input-label demonstrations. ArXiv, abs/2205.12685, 2022.\\n\\nBrenden M. Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In ICML, 2018.\"}"}
