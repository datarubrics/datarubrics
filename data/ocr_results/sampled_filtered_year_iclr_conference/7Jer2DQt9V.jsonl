{"id": "7Jer2DQt9V", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix Fig. A4-A7 show the performance on molecular prediction with different GNN architectures (GIN and GAT).\\n\\nFigure A4: Comparison of ROC-AUC performance (%) on the DrugOOD dataset using the GIN and GAT backbones, respectively.\\n\\nFigure A5: Comparison of ROC-AUC performance (%) on the MoleculeNet dataset using the GIN and GAT backbones, respectively.\"}"}
{"id": "7Jer2DQt9V", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The experiments are implemented on an 8 Intel Xeon Gold 5220R and 4 NVidia A100 GPUs. We use the publicly accessible code libraries of all evaluated methods. The detailed implementation can be found through this anonymous link: https://sites.google.com/view/podgengraph/.\"}"}
{"id": "7Jer2DQt9V", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In our implementation, we have used the following libraries which are covered by the corresponding licenses:\\n\\n\u2022 Tensorflow (Apache License 2.0)\\n\u2022 Pytorch (BSD 3-Clause \u201cNew\u201d or \u201cRevised\u201d License)\\n\u2022 Numpy (BSD 3-Clause \u201cNew\u201d or \u201cRevised\u201d License)\\n\u2022 RDKit (BSD 3-Clause \u201cNew\u201d or \u201cRevised\u201d License)\\n\u2022 scikit-image (BSD 3-Clause \u201cNew\u201d or \u201cRevised\u201d License)\\n\u2022 wilds (MIT License)\\n\u2022 Codebase of CIGA: link (MIT license)\\n\u2022 Mole-OOD: link (MIT license)\\n\u2022 Codebase of LiSA: link\\n\u2022 Codebase of Mask pretraining and context prediction: link (MIT License)\\n\u2022 Codebase of InfoGraph: link\\n\u2022 Codebase of Molecule-BERT: link\"}"}
{"id": "7Jer2DQt9V", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Graph neural networks have shown significant progress in various tasks, yet their ability to generalize in out-of-distribution (OOD) scenarios remains an open question. In this study, we conduct a comprehensive benchmarking of the efficacy of graph pre-trained models in the context of OOD challenges, named as PODGenGraph. We conduct extensive experiments across diverse datasets, spanning general and molecular graph domains and encompassing different graph sizes. Our benchmark is framed around distinct distribution shifts, including both concept and covariate shifts, whilst also varying the degree of shift. Our findings are striking: even basic pre-trained models exhibit performance that is not only comparable to, but often surpasses, specifically designed to handle distribution shift. We further investigate the results, examining the influence of the key factors (e.g., sample size, learning rates, in-distribution performance etc) of pre-trained models for OOD generalization. In general, our work shows that pre-training could be a flexible and simple approach to OOD generalization in graph learning. Leveraging pre-trained models together for graph OOD generalization in real-world applications stands as a promising avenue for future research.\\n\\n1 INTRODUCTION\\n\\nGraph Neural Networks (GNNs) have emerged as a popular innovation, primarily due to their unparalleled proficiency in processing graph-structured data (Kipf & Welling, 2017; Wu et al., 2020). However, their performance is markedly diminished when dealing with Out-of-Distribution (OOD) tasks in which training and test data follow different distributions (Li et al., 2022a). The OOD challenges in graph learning have prompted the development of myriad solutions (Yu et al., 2023; Wu et al., 2022a; Feng et al., 2020; Li et al., 2022b; Fan et al., 2022). These methodologies, however, often cater to specific OOD scenarios, such as distinctive data shifts or semantics, making them less versatile due to the dynamic nature of real-world applications.\\n\\nGiven the adaptability and generalizability of pre-trained models in other domains such as images (Kim et al., 2022; Naganuma & Hataya, 2023; Yu et al., 2021; Gulrajani & Lopez-Paz, 2020), there is a potential that graph pre-trained methodologies could significantly enhance the performance of GNNs in addressing graph OOD challenges (Xia et al., 2022). Motivated by this potential, we seek to investigate whether graph pre-trained models can serve as robust and efficient solutions for graph OOD generalization.\\n\\nIn this paper, we systematically investigate the importance of pre-trained models for graph OOD generalization. We consider a variety of graph pre-trained models and diverse distribution shifts. Specifically, we evaluate methodologies such as context prediction (Hu* et al., 2020), mask pre-training learning (Hu* et al., 2020; Xia et al., 2023) along with contrastive learning (Sun et al., 2020). We evaluated their efficacy across various graph datasets, including molecular and general simulated graphs, while adjusting the types of distribution shifts (e.g., covariate shift and concept shift), as well as different distribution shift degrees. Our aim is to empirically verify whether these pre-trained models can achieve better performance in comparison to the state-of-the-art methods specifically designed for OOD problems. Additionally, we explore the implications of the key factors of pre-trained models in OOD contexts, such as sample size, fine-tuning learning rate, in-distribution (ID) learning performances. Fig. 1 summarizes the key components of PODGenGraph benchmark.\\n\\nOur key findings, based on various evaluation protocols and analysis, include:\"}"}
{"id": "7Jer2DQt9V", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nData Source\\n\\nSynthetic Data (Motif) \\n\u201cHouse\u201d \u2026\\n\u2026\\n2D images\\n\u2026\\nSMILES of molecules ...\\n\\nGraphs\\n\\nType of shift\\n\u2022 Concept shift\\n\u2022 Covariate shift\\n\\nShift source\\n\u2022 Scaffold\\n\u2022 Assay\\n\u2022 Size\\n\u2022 Basis\\n\u2022 Color\\n\\nDistribution Shift\\n\\nPre-training\\n\u2022 Context Prediction\\n(Hu et al. 2020)\\n\u2022 Masking\\n- Attribute mask\\n(Hu et al. 2020)\\n- Mole-BERT\\n(Xia et al. 2023)\\n\u2022 Contrastive Learning\\n- InfoGraph\\n(Sun et al. 2020)\\n\\nOOD methods\\n\u2022 Disentangled/Causal Learning\\n- CIGA\\n(Chen et al. 2022)\\n- Mole-OOD\\n(Yang et al. 2022)\\n\u2022 Augmented Learning\\n- LiSA\\n(Yu et al. 2023)\\n\\nEvaluated Methodologies\\n\u2022 OOD generalisation performance\\n\u2022 Various distribution shift degree\\n\u2022 Effect of fine-tuning learning rate & sample size\\n\u2022 Relation to ID performance\\n\\nEvaluation and Analysis\\n\\nGIN  (Xu et al. 2018)\\nBackbone\\n\\nDatasets and OOD Settings\\n\\nMethods, Evaluation & Analysis\\n\\nFigure 1: Summary of PODGenGraph benchmark.\\n\\n\u2022 In most molecular graph OOD generalization experiments, pre-training methods achieve comparable, mostly superior to specialized OOD methods like invariant/causal learning and graph augmentation (achieving highest or second-highest performances among all 19 datasets). Moreover, pre-trained methods consistently exhibit superiority across a set of degrees of distribution shift, highlighting a simplistic and practical solution for graph OOD, especially in the molecular domain.\\n\\n\u2022 We observe that even with a smaller fine-tuning sample size, such as only 10%-20% of the original fine-tuning sample size, pre-trained models can still achieve comparable results in OOD generalization to those with the full sample size, demonstrating the sample efficiency.\\n\\n\u2022 In general graphs, particularly under concept shift, pre-training methods do not exhibit significant superiority, suggesting that pre-trained models for graph OOD are not universally better, leaving room for future improvements. A possible approach is to combine pre-trained models with other solutions like augmentation and invariant learning to devise more universal algorithms to resolve graph OOD issues.\\n\\n\u2022 Contrary to (Miller et al., 2021), we find that in-distribution learning performance is not always an indicator for OOD generalization, specifically in the context of pre-trained models for graph OOD. This finding might lead to more comprehensive algorithms or theoretical analysis exploring the correlation between OOD and ID learning performance in the future.\\n\\n\u2022 Similarly, different from previous works (Yu et al., 2021; Li et al., 2019), we discover that smaller learning rates during the fine-tuning phase do not invariably lead to better generalization in OOD scenarios for most pre-trained graph models, with the exception being the mask pre-training method Molecule-BERT that introduces prior information.\\n\\nContributions:\\n1. We execute an extensive evaluation of various graph pre-training strategies such as masked pre-training, contrastive learning, graph auto-encoders, and context prediction, against multiple types of distribution shifts including covariate and concept shifts.\\n2. We explore the impacts of different key factors in pre-training, such as sample size, fine-tuning learning rates, and in-distribution learning performances, alongside varying degrees of distribution shift, to disclose their implications on the effectiveness of pre-trained models in OOD generalization.\"}"}
{"id": "7Jer2DQt9V", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2 RELATED WORKS\\n\\nGraph Pre-training. Graph pre-training with external datasets has been explored in numerous studies but there is no specific study on evaluating them under OOD generalization context. Slightly different from self-supervised methods for OOD generalization, pre-training methods usually leverage the external datasets to learn the good initialization or representation which could benefit the downstream tasks. Here we summarize the current state-of-the-art approaches in two categories: (1). Supervised Pre-training. While supervised labels often require significant time and resources, they can still aid pre-training, particularly in biochemical contexts. Hu* et al. (2020) utilized these to predict a plethora of molecular properties and protein functions. They also considered structural similarities between graphs as a form of supervision. MOCL (Sun et al., 2021) further explored this by measuring the structural similarity between molecules via the Tanimoto coefficient. Other methods like GROVER (Rong et al., 2020) and MGSSL (Zhang et al., 2021) were introduced to predict the motifs in molecular graphs. (2). Self-supervised Pre-training. Graph AutoEncoders (Kipf & Welling, 2016) aim to reconstruct parts of graphs that aid in understanding the data representation. Graph Autoregressive Models, such as GPT-GNN (Hu et al., 2020) and MGSSL (Zhang et al., 2021) use the autoregressive framework for graph reconstruction. Masked components modeling, including the mask pre-training strategy in (Hu* et al., 2020) and Mole-BERT (Xia et al., 2023), utilizes masking components from graphs and then predicting them. InfoGraph (Sun et al., 2020) and DGI (Veli \u02c7ckovi\u00b4c et al., 2018) employ mutual information maximization between various graph representations. GraphCL (You et al., 2020) introduces a contrastive learning framework emphasizing robust, transferable representation learning with graph augmentations for enhanced generalizability. In our work, we choose the self-supervised pre-training methods for evaluation due to the practical considerations. We expand upon their OOD generalization results and analysis by conducting a thorough exploration of graph pre-trained models. Our study goes beyond the confines of MoleculeNet datasets with covariate shifts, exploring diverse data sources including DrugOOD (Ji et al., 2023), MoleculeNet (Wu et al., 2018), OGBG (Hu et al., 2020), and others. Furthermore, we investigate distribution shifts with varying degrees of intensity, incorporating various meta-analyses such as the examination of fine-tuning sample size and learning rates to provide a more inclusive analysis. In supervised learning, labels are typically hard to acquire, or the acquisition is highly costly (Wang et al., 2023). In molecular graphs or certain biological graphs, obtaining annotations is challenging. Given our aim is to provide the evaluation and benchmark for pre-trained models for OOD generalization which could be used for practical real-world problems, we select to evaluate the self-supervised pre-training methods.\\n\\nGraph OOD. The Out-of-Distribution (OOD) generalization problem, well-recognized in machine learning paradigms, gains unique complexities for graph data. Graphs, inherently non-Euclidean, can manifest a broad spectrum of topological structures and dynamics that challenge traditional methods. Three research lines have been conducted to tackle the graph OOD challenge: (1). Disentangled, Invariant, and Causal Learning. Disentangled graph representation learning seeks to factorize real-world graphs into distinct latent components. Such models aim to capture underlying, informative factors in the graph data, which has been shown to benefit OOD generalization. The pioneering work of DisenGCN (Ma et al., 2019) introduces a novel convolutional layer, DisenConv, which uses a neighborhood routing mechanism to analyze and infer latent factors. IPGDN (Liu et al., 2020) enhances this by adding an independence regularization to minimize dependencies among representations. FactorGCN (Yang et al., 2020) focuses on graph-level representation, using a factorization mechanism to produce hierarchical disentanglements. Recently, Mole-OOD (Yang et al., 2022), DisC (Fan et al., 2022) and CIGA (Chen et al., 2022b) specifically disentangle causal from non-causal information, offering a robust approach to handle biases and distribution shifts in graphs. These advances spotlight the potential of disentangled representations in achieving superior OOD performance on graph data. (2). Graph Augmentation. The structure and topology of graphs play a critical role in predicting their properties. Some methods leverage structure-wise augmentations to generate diverse training topologies. GAug (Zhao et al., 2021) enhances generalization using a differentiable edge predictor, MH-Aug (Park et al., 2021) uses Markov chain Monte Carlo sampling for controlled augmentation. Additionally, feature-wise augmentations\"}"}
{"id": "7Jer2DQt9V", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nhave emerged, where node features are manipulated. GRAND (Feng et al., 2020) randomly drops and propagates node features to reduce sensitivity to specific neighborhoods, while FLAG (Kong et al., 2022) augments node features using gradient-based adversarial perturbations, maintaining the underlying graph structures. LiSA (Yu et al., 2023) further solves the problem of inconsistent predictive relationships among augmented environments by invariant subgraph training. These methods verify the significance of graph data augmentation in achieving enhanced out-of-distribution generalization. (3).\\n\\nSelf-supervised Learning.\\n\\nGraph self-supervised learning has also shown promise for OOD generalization. For instance, PATTERN (Yehudai et al., 2021) seeks to achieve the generalization from small to large graphs. GraphCL (You et al., 2020) and RGCL (Li et al., 2022d) use contrastive learning, with the latter emphasizing rationale-aware augmentations. Test-time training methods like GAPGC (Chen et al., 2022a) and GT3 (Wang et al., 2022) further innovate by introducing contrastive loss variants and hierarchical self-supervised frameworks, respectively for OOD generalization. Our work is close to this research line, and is the first to discover the universal benefits of self-supervised pre-training to graph OOD, in terms of various graph OOD scenarios.\\n\\nWe choose the state-of-the-art methods from the first two research lines for comparison, including CIGA (Chen et al., 2022b), Mole-OOD (Yang et al., 2022), and LiSA (Yu et al., 2023).\\n\\n3 P RELIMINARIES\\n\\n3.1 G RAPH OOD S CENARIOS\\n\\nWe consider both the general feature distribution shifts (e.g., molecules under different assays) and structure distribution shifts (e.g., different molecular size). Given a training dataset \\\\(D_{\\\\text{train}}\\\\) consisting of \\\\(N\\\\) graphs \\\\(\\\\{G_1, G_2, \\\\ldots, G_N\\\\}\\\\) each associated with a target label or property \\\\(\\\\{y_1, y_2, \\\\ldots, y_N\\\\}\\\\), the graph OOD problem arises when:\\n\\n\\\\[\\nP(G, y | D_{\\\\text{test}}) \\\\neq P(G, y | D_{\\\\text{train}})\\n\\\\]\\n\\nIn this paper, we consider two types of OOD: covariate shift and concept shift.\\n\\nCovariate Shift. Covariate shift refers to a scenario where the distribution of the input data (graphs in our context) changes between training and test stages, while the conditional distribution of the target given the input remains consistent. Mathematically, if \\\\(G\\\\) represents our input graphs and \\\\(Y\\\\) represents our labels:\\n\\n\\\\[\\nP_{\\\\text{train}}(G) \\\\neq P_{\\\\text{test}}(G), \\\\quad P_{\\\\text{train}}(Y | G) = P_{\\\\text{test}}(Y | G)\\n\\\\]\\n\\nFor graph-structured data, covariate shift could imply that while the method of labeling nodes or edges remains consistent, the types of graphs in the test set might differ from those in the training set.\\n\\nConcept Shift. Concept or label shift arises when the distribution of the labels changes between training and testing, even if the input distribution remains the same. Formally:\\n\\n\\\\[\\nP_{\\\\text{train}}(Y) \\\\neq P_{\\\\text{test}}(Y), \\\\quad P_{\\\\text{train}}(G | Y) = P_{\\\\text{test}}(G | Y)\\n\\\\]\\n\\nIn the context of graph data, this means that while the types of graphs remain consistent across training and test datasets, the manner or criteria by which they are labeled has evolved or changed.\\n\\n3.2 G RAPH P RE-TRAINING METHODOLOGIES\\n\\nIn this section, we briefly discuss the pre-training methods we choose for this study. The detailed training and fine-tuning settings will be discussed in Section 4.2. For molecular datasets, we choose three pre-training methods: ContextPred (Hu* et al., 2020), Attribute masking (Hu* et al., 2020), and Mole-BERT (Xia et al., 2023).\\n\\n- ContextPred: The goal of ContextPred is to pre-train a GNN in such a way that it establishes proximity between embeddings of nodes that occur within analogous structural contexts. It employs subgraphs to predict the surrounding graph structures of these nodes. In this work, we employ the K-hop neighborhood as the subgraph in the original work and choose \\\\(K = 5\\\\). We also follow the context definition in the work (i.e., adjacent graph structure), and choose the hop values \\\\(r_1 = 4\\\\) and \\\\(r_2 = 7\\\\).\"}"}
{"id": "7Jer2DQt9V", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Fig. A1 gives the evaluation pipeline on pre-trained GNNs for graph OOD scenarios with a showing case on molecular graphs.\\n\\n**Figure A1**: PodGenGraph pipeline for molecular graph pre-training and fine-tuning for downstream datasets.\\n\\n**C.1 HYPERPARAMETER DETAILS FOR BASELINE METHODS**\\n\\n**CIGA**. We used default hyperparameters as specified in the original paper for DrugOOD, TU datasets, Motif, and CMNIST. Specifically, in DrugOOD, the causal substructure size is set to 80% of each graph size for DrugOOD-Scaffold and DrugOOD-Assay, while it's 10% for DrugOOD-Size. The dropout rate is 0.5 for DrugOOD-Scaffold and DrugOOD-Assay, and 0.1 for DrugOOD-Size. For DrugOOD-Assay with CIGA-v1 and CIGA-v2, the coefficient for contrastive loss is set to 8 and 1, respectively. For DrugOOD-Scaffold with CIGA-v1 and CIGA-v2, it's 32 and 16, respectively. For DrugOOD-Size with CIGA-v1 and CIGA-v2, it's 16 and 2, respectively.\\n\\nFor TU datasets, we use a causal substructure size of 60% for NCI1, 70% for NCI109, and 30% for DD and PROTEINS. The coefficient for contrastive loss is 0.5 for NCI1 with CIGA-v1 and 1 for NCI1 with CIGA-v2. It's 2 for both NCI109 and DD with all CIGA versions. For PROTEINS, the coefficient for contrastive loss is 0.5 with both CIGA-v1 and CIGA-v2.\\n\\nIn Motif, the causal substructure ratio is 25%, and in CMNIST, it's 80%. For Motif, the coefficient of contrastive loss is chosen from \\\\{0.5, 1, 4, 8, 16, 32\\\\}, and for CMNIST, it's 32 with CIGA-v1 and 16 with CIGA-v2.\\n\\nFor datasets in MoleculeNet and scaffold distribution shift in OGBG datasets, we use hyperparameters similar to those in DrugOOD-Scaffold. For size distribution shift in OGBG datasets, the hyperparameters are aligned with those in DrugOOD-Size.\\n\\n**MoleOOD**. We employed default hyperparameters as provided in the code release. Specifically, we selected the prior distribution from uniform, Gaussian distribution for all datasets. In DrugOOD, we utilized 20 domains for the domain prior across three datasets. For MoleculeNet and OGBG datasets, we varied the number of domains among \\\\{10, 15, 20\\\\}.\\n\\n**LiSA**. We utilized the default hyperparameters provided in the code release. The inner loop was set to 20 for all datasets. We employed 3 subgraph generators and a coefficient loss regularization term of 0.1 across all datasets.\"}"}
{"id": "7Jer2DQt9V", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset     | Mole-BERT | GIN-OOD | AttrMask | CIGA-v2 | CIGA-v1 | GIN-ID | LiSA | ContextPred |\\n|------------|-----------|---------|----------|---------|---------|--------|------|-------------|\\n| BBBP       | 93        | 88      | 95       | 69      | 50      | \u00b1      | \u00b1   | \u00b1           |\\n| DrugOOD    | \u00b1         | \u00b1       | \u00b1        | \u00b1       | \u00b1       | \u00b1      | \u00b1   | \u00b1           |\\n| DrugOOD-Scaffold | \u00b1     | \u00b1       | \u00b1        | \u00b1       | \u00b1       | \u00b1      | \u00b1   | \u00b1           |\\n| DrugOOD-Assay | \u00b1     | \u00b1       | \u00b1        | \u00b1       | \u00b1       | \u00b1      | \u00b1   | \u00b1           |\\n| MUV        | \u00b1         | \u00b1       | \u00b1        | \u00b1       | \u00b1       | \u00b1      | \u00b1   | \u00b1           |\\n| ToxCast    | \u00b1         | \u00b1       | \u00b1        | \u00b1       | \u00b1       | \u00b1      | \u00b1   | \u00b1           |\\n| Tox21      | \u00b1         | \u00b1       | \u00b1        | \u00b1       | \u00b1       | \u00b1      | \u00b1   | \u00b1           |\\n\\n- Table A5: Performance evaluation on OGBG datasets (Hu et al., 2020) with covariate shift.\\n- Table A3: Testing ROC-AUC on Drug-OOD datasets (Ji et al., 2023) with covariate shift. Blue are in bold indicate pre-training strategies. The first and second best-performing methods (except the ID training) are in shaded rows.\\n\\nUnder review as a conference paper at ICLR 2024\"}"}
{"id": "7Jer2DQt9V", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table A6: Testing Matthews correlation coefficient on TU datasets with covariate shift. Blue shaded rows indicate pre-training strategies. The first and second best-performing numbers (except the ID training) are in bold and bold, respectively.\\n\\n| Dataset   | CIGA-v1 | CIGA-v2 | LiSA   | InfoGraph | GIN-OOD | GIN-ID |\\n|-----------|---------|---------|--------|-----------|---------|--------|\\n| NCI1      | 0.22\u00b10.07 | 0.27\u00b10.07 | 0.24\u00b10.01 | 0.39\u00b10.01 | 0.21\u00b10.06 | 0.45\u00b10.03 |\\n| NCI109    | 0.23\u00b10.09 | 0.22\u00b10.05 | 0.26\u00b10.02 | 0.38\u00b10.01 | 0.16\u00b10.05 | 0.44\u00b10.02 |\\n| PROTEINS  | 0.40\u00b10.06 | 0.31\u00b10.12 | 0.43\u00b10.05 | 0.53\u00b10.07 | 0.23\u00b10.05 | 0.46\u00b10.03 |\\n| DD        | 0.29\u00b10.08 | 0.26\u00b10.08 | 0.37\u00b10.07 | 0.35\u00b10.04 | 0.25\u00b10.09 | 0.40\u00b10.04 |\\n\\nTable A7: Testing accuracy on general graph datasets with covariate shift. Blue shaded rows indicate pre-training strategies. The first and second best-performing numbers (except the ID training) are in bold and bold, respectively.\\n\\n| Dataset   | CIGA-v1 | CIGA-v2 | LiSA   | InfoGraph | GIN-OOD | GIN-ID |\\n|-----------|---------|---------|--------|-----------|---------|--------|\\n| Motif     | 66.43\u00b111.31 | 67.15\u00b18.19 | 82.55\u00b17.18 | 86.85\u00b12.43 | 62.01\u00b13.92 | 92.15\u00b10.04 |\\n| CMNIST    | 49.14\u00b18.34 | 54.42\u00b13.11 | 62.90\u00b18.30 | 53.43\u00b18.09 | 52.94\u00b12.93 | 77.80\u00b10.20 |\\n| Basis     | 32.22\u00b12.67 | 32.11\u00b12.53 | 33.21\u00b11.31 | 24.39\u00b12.09 | 26.28\u00b15.95 | 77.80\u00b10.20 |\\n\\nFigure A2: Aggregate performance on Drug-OOD averaged across three datasets: Drug-OOD-lbap-core-ic50-assay, Drug-OOD-lbap-core-ic50-scaffold, and Drug-OOD-lbap-core-ic50-size. Better results are indicated by higher mean, median, and IQM scores, along with a lower optimality gap.\"}"}
{"id": "7Jer2DQt9V", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table A8: Performance evaluation on OGBG datasets (Hu et al., 2020) with concept shift. OGBG-MolPCBA is evaluated by AP, while OGBG-MolHIV is evaluated by ROC-AUC. Blue shaded rows indicate pre-training strategies. The first and second best-performing numbers (except the ID training) are in **bold** and *bold*, respectively.\\n\\n| Model          | Size (Scaffold) | OGBG-MolPCBA  | OGBG-MolHIV  |\\n|----------------|----------------|---------------|--------------|\\n| CIGA-v1        | 9.22 \u00b1 0.09    | 8.33 \u00b1 0.06   | 72.80 \u00b1 1.35 |\\n| CIGA-v2        | 8.31 \u00b1 0.12    | 8.71 \u00b1 0.12   | 73.62 \u00b1 1.33 |\\n| LiSA           | 5.05 \u00b1 0.32    | 8.55 \u00b1 0.63   | 72.36 \u00b1 4.75 |\\n| ContextPred    | 11.39 \u00b1 0.21   | 15.71 \u00b1 0.38  | 70.41 \u00b1 0.83 |\\n| AttrMask       | 11.87 \u00b1 0.24   | 16.14 \u00b1 0.49  | 70.59 \u00b1 0.58 |\\n| Mole-BERT      | 15.71 \u00b1 0.26   | 21.29 \u00b1 0.53  | 75.94 \u00b1 0.91 |\\n| GIN-OOD        | 12.76 \u00b1 0.62   | 17.27 \u00b1 0.63  | 70.20 \u00b1 1.12 |\\n| GIN-ID         | 28.10 \u00b1 0.69   | 30.80 \u00b1 0.54  | 79.49 \u00b1 0.55 |\\n\\nTable A9: Testing accuracy on general graph datasets with concept shift. Blue shaded rows indicate pre-training strategies. The first and second best-performing numbers (except the ID training) are in **bold** and *bold*, respectively.\\n\\n| Motif         | CMNIST basis size | OGBG-MolPCBA  | OGBG-MolHIV  |\\n|---------------|-------------------|---------------|--------------|\\n| CIGA-v1       | 72.50 \u00b1 4.02      | 58.63 \u00b1 6.66  | 34.80 \u00b1 3.33 |\\n| CIGA-v2       | 77.48 \u00b1 2.54      | 70.65 \u00b1 4.81  | 39.39 \u00b1 3.30 |\\n| LiSA          | 87.89 \u00b1 1.61      | 70.36 \u00b1 2.61  | 36.56 \u00b1 0.40 |\\n| InfoGraph     | 79.36 \u00b1 1.12      | 64.79 \u00b1 1.68  | 19.19 \u00b1 2.17 |\\n| GIN-OOD       | 72.12 \u00b1 1.89      | 58.23 \u00b1 1.73  | 29.53 \u00b1 0.50 |\\n| GIN-ID        | 92.15 \u00b1 0.04      | 92.16 \u00b1 0.07  | 77.80 \u00b1 0.20 |\\n\\nFigure A3: Aggregate performance on MoleculeNet averaged across eight datasets: BBBP, Tox21, ToxCast, SIDER, ClinTox, MUV, HIV, BACE. Better results are indicated by higher mean, median, and IQM scores, along with a lower optimality gap.\"}"}
{"id": "7Jer2DQt9V", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\n- Attribute masking\\n  - Mole-BERT\\n  - GraphMAE: All three works use the masked component modeling methods for the self-supervised learning. Specifically, they involve the masking of certain components within molecules, including atoms, bonds, and fragments, followed by training the model to predict these masked components based on the remaining contextual information. We follow the setups in the original papers: Mask pre-training in (Hu* et al., 2020) inputs atom and chemical bond attributes are randomly masked, and GNNs are pre-trained to predict these masked attributes and Mole-BERT (Xia et al., 2023) uses a context-aware tokenizer that encodes atoms with chemically meaningful values for masking.\\n\\n- GraphMAE (Hou et al., 2022) represents a significant advancement in the field of graph autoencoders (GAEs). It diverges from traditional GAEs by prioritizing feature reconstruction over graph structure reconstruction and employs a novel masking strategy combined with scaled cosine error, enhancing training robustness and error metric accuracy.\\n\\n- GraphCL (You et al., 2020) introduces a contrastive learning framework focusing on robust and transferable representation learning. It also utilizes the graph augmentations to enhance data priors, to improve the generalizability and robustness.\\n\\nFor general graph datasets as well as the molecular datasets without node information, we use one contrastive self-supervised pre-training, InfoGraph (Sun et al., 2020). It extracts expressive representations for graphs or nodes by maximizing mutual information between graph-level and substructure-level representations at varying granularities.\\n\\n### Benchmark Methodology\\n\\n#### 4.1 Benchmark Setup\\n\\n**Datasets.** We evaluate pre-trained models upon multiple dataset sources, including:\\n\\n- Three datasets from DrugOOD (Ji et al., 2023) (\\n  - DrugOOD-lbap-core-ic50-assay\\n  - DrugOOD-lbap-core-ic50-scaffold\\n  - DrugOOD-lbap-core-ic50-size\\n)\\n- Ten datasets from MoleculeNet (Wu et al., 2018) (BBBP, Tox21, ToxCast, SIDER, ClinTox, MUV, HIV, BACE, OGBG-MolHIV, OGBG-MolPCBA)\\n- Four datasets from the TU collection (Morris et al., 2020) (NCI1, NCI109, PROTEINS, DD)\\n- Three datasets from the general graph collection (Gui et al., 2022) (Motif (Wu et al., 2022b) and CMNIST (Arjovsky et al., 2019)).\\n\\nTable 1 lists the statistics and key factors of the molecular datasets we employed (Detailed version in Appendix Table A1). The detailed statistics of simulated graphs (Motif and CMNIST) is given in Appendix Table A2. We also give the detailed introduction to all datasets in Appendix B.2.\\n\\n**Various Graph Domains.** We select datasets covering a wide array of graph structures. This includes molecular graphs used in biophysics and physiology research, encompassing both those with and without node information. Additionally, we include synthetic and real-world graphs that represent images and hierarchical trees.\\n\\n**Source of Distribution Shift and OOD.** We use diverse datasets covering various causes of distribution shift, featuring variations in graph characteristics (like scaffold, size, basis, and color) for both molecular and general graphs, as well as environmental factors (such as assay) for molecular graphs. In the DrugOOD dataset (Ji et al., 2023), the distribution shift originates from disparities in Bemis-Murcko scaffold size (DrugOOD-Scaffold), assay ID (DrugOOD-Assay), and molecular atom size (DrugOOD-Size). In contrast, all datasets within the MoleculeNet (Wu et al., 2018) follow a shift based on the Bemis-Murcko scaffold. For the TU collection (Morris et al., 2020), we utilize data splits generated by (Yehudai et al., 2021) based on molecular atom size. Following (Gui et al., 2022), Motif (Wu et al., 2022b) is tailored to address structural and size shifts, whereas CMNIST (Arjovsky et al., 2019) is partitioned based on different digit colors. We consider both covariate and concept shifts under different domains for most of the datasets.\\n\\n#### 4.2 Baselines, Implementation, and Evaluation\\n\\n**Baseline Algorithms.** We integrate empirical risk minimization (ERM) (Vapnik, 1999) and the state-of-the-arts with disentangled, invariant and causal learning, and data-augmentation methodologies. All methods have been reproduced based on their original implementation (details listed in Appendix C.1). We choose two disentangled OOD algorithms, CIGA (Chen et al., 2022b) and MoleOOD (Yang et al., 2022), both based on the invariant and causal learning. CIGA (Chen et al., 2022b) and MoleOOD (Yang et al., 2022) both utilize invariant and causal learning.\"}"}
{"id": "7Jer2DQt9V", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Molecular dataset statistics. Gray shaded rows indicate datasets without node labels. AP, MCC, and ACC represent the average precision, Matthews correlation coefficient, and accuracy, respectively.\\n\\n| Datasets          | Domain | Shift | #. Graphs | Avg. #. Graphs | #. Node | Avg. #. Node | #. Edge | Avg. #. Edge | #. Classes | /Task | Metrics                |\\n|-------------------|--------|-------|-----------|----------------|---------|--------------|---------|--------------|------------|-------|------------------------|\\n| Scaffold          | 59, 608| 30    | 64         | 64.9           | 2       | 2            |         |              |            |       |                        |\\n| Assay             | 72, 239| 32    | 70         | 70.2           | 2       | 2            |         |              |            |       |                        |\\n| DrugOOD           | Size   | 70    | 672        | 66.9           | 2       | 1            |         |              |            |       |                        |\\n| BBBP              | 2, 039 | 24    | 51         | 54.9           | 2       | 1            |         |              |            |       |                        |\\n| Tox21             | 7, 831 | 18    | 38         | 38.6           | 2       | 12           |         |              |            |       |                        |\\n| ToxCast           | 8, 575 | 18    | 38         | 38.5           | 2       | 617          |         |              |            |       |                        |\\n| SIDER             | 1, 427 | 33    | 70         | 70.7           | 2       | 27           |         |              |            |       |                        |\\n| ClinTox           | 1, 478 | 26    | 55         | 54.8           | 2       | 2            |         |              |            |       |                        |\\n| MUV               | 93, 087| 24    | 52         | 52.6           | 2       | 17           |         |              |            |       |                        |\\n| HIV               | 41, 127| 25    | 54         | 54.9           | 2       | 1            |         |              |            |       |                        |\\n| BACE              | Scaffold| Covariate | 1, 513   | 34.1           | 73      | 2            |         |              |            |       |                        |\\n|                    |        |        |            |                |         |              |         |              |            |       |                        |\\n| OGBG-MolHIV       | Size   |        |            |                |         |              |         |              |            |       |                        |\\n| OGBG-MolPCBA      | Size   |        |            |                |         |              |         |              |            |       |                        |\\n| ROC-AUC           |        |        |            |                |         |              |         |              |            |       |                        |\\n| NCI1              | 2, 569 | 27    | 58         | 58.8           | 2       | 1            |         |              |            |       |                        |\\n| NCI109            | 2, 500 | 27    | 58         | 58.8           | 2       | 1            |         |              |            |       |                        |\\n| PROTEINS          | 679    | 35    | 131        | 131.2          | 2       | 1            |         |              |            |       |                        |\\n| DD                | Size   |        |            |                |         |              |         |              |            |       |                        |\\n| Motif Basis       |        |        |            |                |         |              |         |              |            |       |                        |\\n| Size              |        |        |            |                |         |              |         |              |            |       |                        |\\n| ACC               |        |        |            |                |         |              |         |              |            |       |                        |\\n| CMNIST            | Color  |        |            |                |         |              |         |              |            |       |                        |\\n|                   |        |        |            |                |         |              |         |              |            |       |                        |\\n\\n2022b) categorizes interactions between causal and non-causal components into fully informative invariant features (FIIF) and partially informative invariant features (PIIF). MoleOOD (Yang et al., 2022) identifies molecule environments without manual specification and uses them along with substructures for predictions. Furthermore, we adopt one augmentation-based OOD algorithm, LiSA (Yu et al., 2023). It utilizes variational subgraph generators to identify locally predictive patterns and generates multiple label-invariant subgraphs, enhancing diversity for data augmentation process. We also consider cases GIN-OOD and GIN-ID, where GIN is trained without specified operations for OOD. GIN-OOD is tested on OOD testing sets, whereas GIN-ID is tested on in-distribution sets.\\n\\nPre-training Datasets. In accordance with previous works by Hu* et al. (2020), we use 2 million molecules sampled from the ZINC-15 database (Sterling & Irwin, 2015), to learn node representations for downstream molecular datasets. Considering the lack of shared node information across general graph dataset and TU dataset, we initially exclude the label information for self-supervised learning. Once we have learned the representation of each graph, we proceed to fine-tune the classifier (e.g., SVM, logistic regression, or random forest) using a dataset that includes label information.\\n\\nGNN Architectures. We adopt 5-layer graph isomorphism networks (GINs) (Xu et al., 2018) with 300-dimensional hidden units as the backbone model for all pre-training methods in all datasets. The average pooling is used as the READOUT function.\\n\\nPre-training and Fine-tuning. In the pre-training phase, the models undergo 100 training epochs with a batch size of 256 and a learning rate set to 0.001. During the subsequent fine-tuning phase, we conduct training for 100 epochs with a batch size of 32, except for DrugOOD with a batch size of 128, and we report the test score with the best cross-validation performance. In both phases, the models are trained using Stochastic Gradient Descent (SGD) with the Adam optimizer.\\n\\n4.2.1 Evaluation Metrics. We utilize the original evaluation metrics associated with each dataset. Specifically, in the context of molecular datasets, we report ROC-AUC for DrugOOD and MoleculeNet following Ji et al. (2023); Wu et al. (2018), average precision (AP) for OGBG-MolPCBA following Hu et al. (2020), and the Matthews correlation coefficient for TU datasets following Bevilacqua et al. (2021). Furthermore, for all general graph datasets, we use classification accuracy as our primary evaluation metric.\"}"}
{"id": "7Jer2DQt9V", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"However, results on CMNIST datasets show that InfoGraph underperforms compared to baseline underscoring the potential efficacy of pre-trained models in addressing OOD challenges effectively.\\n\\nWithin molecule-related graph datasets, pre-trained methods achieve majority of datasets. Specifically, within molecule-related graph datasets, pre-trained methods achieve\\n\\n| Model  | MolPCBA | CMNIST | MoleculeNet | OGBG-MolHIV | DrugOOD | MoleculeNem | OGBG-MinHydroxy | OGBG-MinMalaria | MNIST | CIFAR-10 |\\n|--------|---------|--------|-------------|-------------|---------|-------------|----------------|----------------|-------|----------|\\n| GIN-OOD | 93.5 \u00b1 1.0 | 91.7 \u00b1 0.8 | 90.5 \u00b1 0.7 | 89.2 \u00b1 0.6 | 88.9 \u00b1 0.5 | 88.6 \u00b1 0.4 | 88.3 \u00b1 0.3 | 88.0 \u00b1 0.2 | 87.7 \u00b1 0.1 | 87.4 \u00b1 0.0 |\\n| GIN-ID  | 92.8 \u00b1 0.9 | 91.1 \u00b1 0.8 | 89.9 \u00b1 0.7 | 88.7 \u00b1 0.6 | 88.4 \u00b1 0.5 | 88.1 \u00b1 0.4 | 87.8 \u00b1 0.3 | 87.5 \u00b1 0.2 | 87.2 \u00b1 0.1 | 86.9 \u00b1 0.0 |\\n| CIGA-GRAPH | 92.2 \u00b1 0.8 | 90.5 \u00b1 0.7 | 89.3 \u00b1 0.6 | 88.1 \u00b1 0.5 | 87.8 \u00b1 0.4 | 87.5 \u00b1 0.3 | 87.2 \u00b1 0.2 | 86.9 \u00b1 0.1 | 86.6 \u00b1 0.0 | 86.3 \u00b1 0.0 |\\n\\nThe first and second best-performing numbers (except the ID training) are in\\n\\n- **MolPCBA**: Testing Average Precision (AP); \\n- **TU datasets**: Testing Matthews correlation coefficient (MCC); \\n- **DrugOOD, MoleculeNem, OGBG-MolHIV, OGBG-MinHydroxy, OGBG-MinMalaria**: Testing Area Under the Operating Characteristics Curve (AUC-ROC).\"}"}
{"id": "7Jer2DQt9V", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Effect of the Distribution Shift Degrees. We investigate the relationship between the performance drop and shift degrees. To quantify shift degrees, we adopt the following approach: First, we train a vanilla GNN model on the training domain without considering distribution shift. Subsequently, we evaluate the performance drop on the testing domain with distribution shifts. Specifically, we calculate the relative performance drop in AUC-ROC for multiple seeds and use the average value to represent the shift degree. The formula for calculating shift degree ($\\\\Delta S$) is given by:\\n\\n$$\\\\Delta S = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left( \\\\frac{\\\\text{AUC-ROC}_{\\\\text{train}} - \\\\text{AUC-ROC}_{\\\\text{test},i}}{\\\\text{AUC-ROC}_{\\\\text{train}}} \\\\right)$$\\n\\nwhere $\\\\text{AUC-ROC}_{\\\\text{train}}$ is the AUC-ROC score achieved by the GNN model on the training domain without distribution shift, $\\\\text{AUC-ROC}_{\\\\text{test},i}$ is the AUC-ROC score achieved by the GNN model on the testing domain with distribution shift for the $i$th seed, and $n$ is the total number of seeds used. The shift magnitude, $\\\\Delta S$, represents the average relative performance drop across different seeds.\\n\\nFig. 2(a) illustrates the relationship between performance degradation and the degree of distribution shift on the Drug-OOD dataset, where there is the distribution shift on size. Here $n = 10$. It is evident that a negative correlation exists between performance and shift degrees across all examined methods. Notably, pre-trained models maintain superior performance relative to other methods at all degrees of shift, underscoring their robustness against varying degrees of distribution shifts.\\n\\nEffect of the Fine-tuning Sample Size. We also study the importance of fine-tuning sample size. We test the OOD generalization with $\\\\{5\\\\%, 10\\\\%, 20\\\\%, 40\\\\%, 50\\\\%, 65\\\\%, 80\\\\%\\\\}$ of the size we used in original settings on Drug-OOD and MoleculeNet datasets. Results on Drug-OOD datasets are given...\"}"}
{"id": "7Jer2DQt9V", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in Fig. 2(b), showing that more samples during fine-tuning lead to better generalization. However, even with only a few samples, pre-trained models still achieve good generalization performance. For instance, with only 20% of the original sample size, the pre-trained models can achieve comparable performances compared with baselines (baseline results are in Table 2).\\n\\nEffect of the Fine-tuning Learning Rates.\\n\\nBased on the theoretical and empirical conclusions drawn from prior work in Euclidean space data (Li et al., 2019; Yu et al., 2021), we explore whether the choice of learning rate during the fine-tuning phase has a consistent impact on OOD generalization. To analyze this relationship, we experimented with a set of learning rates for all pre-trained models, specifically: \\\\{0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002, 0.0001\\\\}. The number of epochs are 100 for all cases. Our empirical investigation shows that models fine-tuned with smaller learning rates achieve better generalization capabilities. Fig. 2(c) gives the OOD generalization performance versus the selection of learning rate for Context prediction, attribute masking and Mole-BERT on Drug-OOD dataset. The results indicate that, only for Mole-BERT, a smaller fine-tune learning rate leads to better generalization performance. While for Attribute masking and context prediction, there is no correlation between generalization performance and fine-tuning learning rates, which contrary to the findings in image data (Yu et al., 2021).\\n\\nRelation to the In-distribution Performance.\\n\\nIn considering the relevance of pre-trained models to downstream tasks, a question arises: Is the inherent model capability (shown as the ID learning performances), reflected by the model's performance on its pre-training dataset, crucial for OOD generalization in downstream tasks? To analyze this association, we evaluated the relationship between the generalization performances with OOD and in-distribution (ID) learning on Drug-OOD and MoleculeNet datasets. Specifically, ID performances are the down-streaming generalizaiton results of the pre-trained models (pre-trained on ZINC-15 dataset) on Drug-OOD and MoleculeNet datasets without distribution shift. Fig. 2(d) gives the evaluation, indicating that there is no clear correlation between OOD and ID performances. This finding shows that \u201caccuracy on the line\u201d phenomenon (Miller et al., 2021) does not always hold for the graph pre-trained models under OOD generalization problem.\\n\\nCONCLUSIONS\\n\\nOur work is placed within a context where prior methods have designed relatively complicated algorithms tailored for Graph OOD. It is crucial to clarify that our research does not challenge or discredit these existing methods; instead, we offer the perspective by evaluating and benchmarking the performance of pre-trained models on Graph OOD problems.\\n\\nThe Potential of Pre-trained Models for Graph OOD:\\n\\nWe discovered that various pre-trained models, with minimal fine-tuning, could match and often surpass, the performance of methods specially for graph OOD, such as invariant/causal learning and data augmentation. This is especially evident in tasks involving molecular graphs, regardless of the type of distribution shift (concept or covariate), where the pre-trained models achieved superior OOD generalization compared to baseline methods in most cases. Significantly, our results demonstrate that pre-trained models are consistently well-performing among all distribution shift degrees, showing the advantages in OOD scenarios.\\n\\nIn-depth Empirical Study on Pre-trained Models for Graph OOD:\\n\\nOur empirical investigation seeks to provide a deeper understanding of the role of the pre-trained models and various design choices for fine-tuning play in ensuring optimal OOD generalization. Specifically, we explored the correlation between fine-tuning learning rate and OOD generalization, the relationship between pre-trained models in OOD and ID scenarios, and the impact of sample size, providing empirical insights that can guide future research in OOD and pre-training.\\n\\nIn future work, we aim to explore a broader range of pre-training methods and OOD scenarios, building upon the current evaluation of representative approaches. The development of model selection strategies, particularly in the context of pre-trained models and OOD generalization, is identified as a promising avenue. Additionally, the potential enhancement of OOD generalization performance through the combination of pre-trained models with invariant learning or data augmentation techniques is suggested. The exploration of theoretical connections between graph pre-training and OOD, drawing inspiration from self-supervised learning and pre-train models, is also highlighted as a direction for further investigation. The detailed discussion on future directions is given in Appendix A.\"}"}
{"id": "7Jer2DQt9V", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ahmed Abdelaziz, Hilde Spahn-Langguth, Karl-Werner Schramm, and Igor V Tetko. Consensus modeling for hts assays using in silico descriptors calculates the best balanced accuracy in tox21 challenge. *Frontiers in Environmental Science*, 4:2, 2016.\\n\\nMartin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. *arXiv preprint arXiv:1907.02893*, 2019.\\n\\nBeatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In *International Conference on Machine Learning*, pp. 837\u2013851. PMLR, 2021.\\n\\nGuanzi Chen, Jiying Zhang, Xi Xiao, and Yang Li. Graphtta: Test time adaptation on graph neural networks. *arXiv preprint arXiv:2208.09126*, 2022a.\\n\\nYongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, MA Kaili, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Learning causally invariant representations for out-of-distribution generalization on graphs. *Advances in Neural Information Processing Systems*, 35:22131\u201322148, 2022b.\\n\\nShaohua Fan, Xiao Wang, Yanhu Mo, Chuan Shi, and Jian Tang. Debiasing graph neural networks via learning disentangled causal substructure. *Advances in Neural Information Processing Systems*, 35:24934\u201324946, 2022.\\n\\nWenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang Yang, Evgeny Kharlamov, and Jie Tang. Graph random neural networks for semi-supervised learning on graphs. *Advances in neural information processing systems*, 33:22092\u201322103, 2020.\\n\\nEleanor J Gardiner, John D Holliday, Caroline O'Dowd, and Peter Willett. Effectiveness of 2d fingerprints for scaffold hopping. *Future medicinal chemistry*, 3(4):405\u2013414, 2011.\\n\\nShurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. Good: A graph out-of-distribution benchmark. *Advances in Neural Information Processing Systems*, 35:2059\u20132073, 2022.\\n\\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. *arXiv preprint arXiv:2007.01434*, 2020.\\n\\nZhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, pp. 594\u2013604, 2022.\\n\\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. *Advances in neural information processing systems*, 33:22118\u201322133, 2020.\\n\\nWeihua Hu*, Bowen Liu*, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In *International Conference on Learning Representations*, 2020. URL https://openreview.net/forum?id=HJlWWJSFDH.\\n\\nZiniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. Gpt-gnn: Generative pre-training of graph neural networks. In *Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*, pp. 1857\u20131867, 2020.\\n\\nYuanfeng Ji, Lu Zhang, Jiaxiang Wu, Bingzhe Wu, Lanqing Li, Long-Kai Huang, Tingyang Xu, Yu Rong, Jie Ren, Ding Xue, et al. Drugood: Out-of-distribution dataset curator and benchmark for ai-aided drug discovery\u2013a focus on affinity prediction problems with noise annotations. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 37, pp. 8023\u20138031, 2023.\"}"}
{"id": "7Jer2DQt9V", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Donghyun Kim, Kaihong Wang, Stan Sclaroff, and Kate Saenko. A broad study of pre-training for domain generalization and adaptation. In European Conference on Computer Vision, pp. 621\u2013638. Springer, 2022.\\n\\nSunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2023 update. Nucleic acids research, 51(D1):D1373\u2013D1380, 2023.\\n\\nThomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016.\\n\\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=SJU4ayYgl.\\n\\nKezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, and Tom Goldstein. Robust optimization as data augmentation for large-scale graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 60\u201369, 2022.\\n\\nMichael Kuhn, Ivica Letunic, Lars Juhl Jensen, and Peer Bork. The sider database of drugs and side effects. Nucleic acids research, 44(D1):D1075\u2013D1079, 2016.\\n\\nJason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps: Provable self-supervised learning. Advances in Neural Information Processing Systems, 34:309\u2013323, 2021.\\n\\nHaoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. Out-of-distribution generalization on graphs: A survey. arXiv preprint arXiv:2202.07987, 2022a.\\n\\nHaoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Disentangled graph contrastive learning with independence promotion. IEEE Transactions on Knowledge and Data Engineering.\\n\\nHaoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Learning invariant graph representations for out-of-distribution generalization. Advances in Neural Information Processing Systems, 35:11828\u201311841, 2022c.\\n\\nSihang Li, Xiang Wang, An Zhang, Yingxin Wu, Xiangnan He, and Tat-Seng Chua. Let invariant rationale discovery inspire graph contrastive learning. In International conference on machine learning, pp. 13052\u201313065. PMLR, 2022d.\\n\\nYuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large learning rate in training neural networks. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nYanbei Liu, Xiao Wang, Shu Wu, and Zhitao Xiao. Independence promoted graph disentangled networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 4916\u20134923, 2020.\\n\\nJianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenwu Zhu. Disentangled graph convolutional networks. In International conference on machine learning, pp. 4212\u20134221. PMLR, 2019.\\n\\nInes Filipa Martins, Ana L Teixeira, Luis Pinheiro, and Andre O Falcao. A bayesian approach to in silico blood-brain barrier penetration modeling. Journal of chemical information and modeling, 52(6):1686\u20131697, 2012.\\n\\nDavid Mendez, Anna Gaulton, A Patr\u00b4\u0131cia Bento, Jon Chambers, Marleen De Veij, Eloy F \u00b4elix, Mar\u00b4\u0131a Paula Magari\u02dcnos, Juan F Mosquera, Prudence Mutowo, Micha\u0142 Nowotka, et al. Chembl: towards direct deposition of bioassay data. Nucleic acids research, 47(D1):D930\u2013D940, 2019.\\n\\nJohn P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In International Conference on Machine Learning, pp. 7721\u20137735. PMLR, 2021.\"}"}
{"id": "7Jer2DQt9V", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "7Jer2DQt9V", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nYiqi Wang, Chaozhuo Li, Wei Jin, Rui Li, Jianan Zhao, Jiliang Tang, and Xing Xie. Test-time training for graph neural networks. arXiv preprint arXiv:2210.08813, 2022.\\n\\nLirong Wu, Haitao Lin, Yufei Huang, and Stan Z Li. Knowledge distillation improves graph structure augmentation for graph neural networks. Advances in Neural Information Processing Systems, 35:11815\u201311827, 2022a.\\n\\nYingxin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. In International Conference on Learning Representations, 2022b. URL https://openreview.net/forum?id=hGXij5rfiHw.\\n\\nZhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513\u2013530, 2018.\\n\\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):4\u201324, 2020.\\n\\nJun Xia, Yanqiao Zhu, Yuanqi Du, and Stan Z Li. A survey of pretraining on graphs: Taxonomy, methods, and applications. arXiv preprint arXiv:2202.07893, 2022.\\n\\nJun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, and Stan Z. Li. Mole-BERT: Rethinking pre-training graph neural networks for molecules. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=jevY-DtiZTR.\\n\\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018.\\n\\nNianzu Yang, Kaipeng Zeng, Qitian Wu, Xiaosong Jia, and Junchi Yan. Learning substructure invariance for out-of-distribution molecular representations. Advances in Neural Information Processing Systems, 35:12964\u201312978, 2022.\\n\\nYiding Yang, Zunlei Feng, Mingli Song, and Xinchao Wang. Factorizable graph convolutional networks. Advances in Neural Information Processing Systems, 33:20286\u201320296, 2020.\\n\\nGilad Yehudai, Ethan Fetaya, Eli Meirom, Gal Chechik, and Haggai Maron. From local structures to size generalization in graph neural networks. In International Conference on Machine Learning, pp. 11975\u201311986. PMLR, 2021.\\n\\nKaichao You, Yong Liu, Ziyang Zhang, Jianmin Wang, Michael I Jordan, and Mingsheng Long. Ranking and tuning pre-trained models: a new paradigm for exploiting model hubs. The Journal of Machine Learning Research, 23(1):9400\u20139446, 2022.\\n\\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in neural information processing systems, 33:5812\u20135823, 2020.\\n\\nJunchi Yu, Jian Liang, and Ran He. Mind the label shift of augmentation-based graph ood generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11620\u201311630, 2023.\\n\\nYaodong Yu, Heinrich Jiang, Dara Bahri, Hossein Mobahi, Seungyeon Kim, Ankit Singh Rawat, Andreas Veit, and Yi Ma. An empirical study of pre-trained vision models on out-of-distribution generalization. In NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021.\\n\\nZaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Chee-Kong Lee. Motif-based graph self-supervised learning for molecular property prediction. Advances in Neural Information Processing Systems, 34:15870\u201315882, 2021.\\n\\nTong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. In Proceedings of the aaai conference on artificial intelligence, volume 35, pp. 11015\u201311023, 2021.\"}"}
{"id": "7Jer2DQt9V", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Detailed Discussion on Future Work\\n\\nA.1 Exploration of More Pre-training Methods and OOD Scenarios\\n\\nOur current work predominantly evaluates representative pre-training and OOD methods/scenarios. However, the field abounds with numerous other methodologies, as summarized in several surveys (Li et al., 2022c; Xia et al., 2022). Due to computational constraints, we could not explore each one exhaustively, leaving a potential avenue for future research.\\n\\nA.2 Development of Model Selection Approaches\\n\\nOur empirical evaluations, especially those concerning learning rate experiments, lead us to believe that developing pre-trained model selection strategies (e.g., (You et al., 2022)) for OOD generalization is a promising direction for future research.\"}"}
{"id": "7Jer2DQt9V", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A2 give the full dataset and graph statistics of molecular and general graph datasets used in the paper, Table 1 summarizes the important key factors and statistics of the molecular datasets. Table A1 and library (Ramsundar et al., 2019).\\n\\nIt curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source machine learning. It curates diverse public datasets, sets up evaluation standards, and offers open-source tools for different molecular learning methods, all accessible via the DeepChem open source"}
{"id": "7Jer2DQt9V", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The benchmark comprises multiple binary graph classification datasets, each designed to evaluate model performance across different facets of molecular interaction. Specifically, BBBP (Martins et al., 2012) evaluates the crucial measure of blood-brain barrier penetration, vital for understanding membrane permeability. Tox21 (Abdelaziz et al., 2016) offers toxicity data encompassing 12 biological targets, including nuclear receptors and stress response pathways. Toxcast (Richard et al., 2016) provides toxicology measurements based on over 600 in vitro high-throughput screenings, serving as a rich resource for understanding toxicity. SIDER (Kuhn et al., 2016) features a database detailing marketed drugs and adverse drug reactions, categorized into 27 system organ classes, offering insights into drug safety. ClinTox (Novick et al., 2013) consists of qualitative data classifying drugs approved by the FDA and those that have failed clinical trials due to toxicity concerns. MUV (Gardiner et al., 2011) represents a subset of PubChem BioAssay (Kim et al., 2023), refined through nearest neighbor analysis, and tailored for validating virtual screening techniques. The HIV dataset originates from the Drug Therapeutics Program (DTP) AIDS Antiviral Screen (Riesen & Bunke, 2008), a comprehensive screening effort that evaluated the effectiveness of more than 40,000 compounds in inhibiting HIV replication. BACE (Subramanian et al., 2016) is a dataset that provides qualitative binding results for a collection of inhibitors targeting human \\\\( \\\\beta \\\\)-secretase 1.\\n\\nOGBG (Hu et al., 2020). OGBG is a specific subset within Open Graph Benchmark (OGB), containing representative datasets like OGBG-Molhiv, OGBG-Molpcba, and OGBG-PPA. OGBG-Molhiv and OGBG-Molpcba challenge graph property prediction with distribution shifts, specifically focusing on predicting molecular properties. They use a scaffold splitting approach, separating structurally distinct molecules into different subsets for a realistic evaluation of graph generalization. The dataset split follows GOOD benchmark (Gui et al., 2022). Specifically, for covariate shift with a distribution source of size, we arranged the molecules in descending order based on the number of nodes and split them into a ratio of 8\u22361\u22361 for the training set, validation set, and testing set, respectively. Similarly, the entire dataset was ordered based on the Bemis-Murcko scaffold string of SMILES, maintaining the same ratio. For concept shift, exemplified by size, we categorized molecules into different groups based on different numbers of molecular nodes. Following this categorization, we selected samples from each group with different labels, forming the training set, validation set, and testing set, respectively, with a ratio of 3\u22361\u22361. This grouping approach aligns with the scaffold-wise distribution, where molecules are categorized based on the Bemis-Murcko scaffold string of SMILES.\\n\\nTU Datasets. (Morris et al., 2020) It is a collection of benchmark datasets for graph classification and regression. Among these datasets, NCI1, NCI109, PROTEINS, and DD stand out as important and representative graph classification datasets, each offering unique characteristics and complexities. NCI1 and NCI109 datasets are prominent in chemoinformatics. NCI1 is a binary graph classification dataset that focuses on anticancer compound classification. It comprises molecular graphs, with nodes representing atoms and edges indicating chemical bonds. NCI109 extends the challenge by expanding the number of classes and compounds. PROTEINS is a dataset focused on protein graphs, where each node represents a specific protein, and the edges signify various biologically relevant connections or associations between these proteins. The task is to predict the presence or absence of specific protein functions. DD is a real-world graph classification dataset, comprising 1,178 protein network structures, each of which features 82 distinct node labels. The task is to classify each graph into one of two classes: an enzyme or a non-enzyme.\\n\\nMotif. Motif is a synthetic dataset (Wu et al., 2022b). It has been created to address structural shifts in graph data. In this dataset, each graph is composed of a base and a motif. The bases are categorized into three distinct types: Tree (\\\\( S = 0 \\\\)), Ladder (\\\\( S = 1 \\\\)), and Wheel (\\\\( S = 2 \\\\)). On the other hand, the motifs include Cycle (\\\\( C = 0 \\\\)), House (\\\\( C = 1 \\\\)), and Crane (\\\\( C = 2 \\\\)), introducing various structural complexities into the dataset. The ground truth label \\\\( Y \\\\) for each graph is exclusively dictated by the motif (\\\\( C \\\\)). The primary objective in this dataset is to accurately classify the graphs into one of three classes: Cycle, House, or Crane.\\n\\nCMNIST. CMNIST is a special dataset with graphs showcasing handwritten digits. These graphs are created from the MNIST dataset (Arjovsky et al., 2019) but preprocessed with superpixel (Monti et al., 2017). The goal is to classify each graph into one of the ten-digit categories, from 0 to 9.\"}"}
