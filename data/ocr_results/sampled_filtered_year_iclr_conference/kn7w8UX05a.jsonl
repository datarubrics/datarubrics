{"id": "kn7w8UX05a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nScientific data analysis often involves making use of a large number of correlated predictor variables to predict multiple response variables. Understanding how the predictor and response variables relate to one another, especially in the presence of relatively scarce data, is a common and challenging problem. Here, we leverage the recently popular concept of \\\"double descent\\\" to develop a particular treatment of the problem, including a set of key theoretical results. We also apply the proposed method to a novel experimental dataset consisting of human ratings of social traits and social decision making tendencies based on the facial features of strangers, and resolve a scientific debate regarding the existence of a \\\"beauty premium\\\" or \\\"attractiveness halo,\\\" which refers to a (presumed) advantage attractive people enjoy in social situations. We demonstrate that more attractive faces indeed enjoy a social advantage, but this is indirectly due to the facial features that contribute to both perceived attractiveness and trustworthiness, and that the component of attractiveness perception due to facial features (unrelated to trustworthiness) actually elicit a \\\"beauty penalty.\\\" Conversely, the facial features that contribute to trustworthiness and not to attractiveness still contribute positively to pro-social trait perception and decision making. Thus, what was previously thought to be an attractiveness halo/beauty premium is actually a trustworthiness halo/premium plus a \\\"beauty penalty.\\\" Moreover, we see that the facial features that contribute to the trustworthiness halo primarily have to do with how smiley a face is, while the facial features that contribute to attractiveness but actually acts as a beauty penalty is related to anti-correlated with age. In other words, youthfulness and smileyness both contribute to attractiveness, but only smileyness positively contributes to pro-social perception and decision making, while youthfulness actually negatively contribute to them. A further interesting wrinkle is that youthfulness as a whole does not negatively contribute to social traits/decision-making, only the component of youthfulness contributing to attractiveness does.\\n\\nIntroduction\\n\\nScientific data analysis often involves building a linear regression model between a large number of predictor variables and multiple response variables. Understanding how the predictor and response variables relate to one another, especially in the presence of relatively scarce data, is an important but challenging problem. For example, a geneticist might have a genomic dataset with many genetic features as predictor variables and disease prevalence data as response variables: the geneticist may want to know how the different types of disease are related to each other through their genetic underpinnings. Another example is that a social psychologist might have a set of face images (with many facial features) that have been rated by a relatively small set of subjects for perceived social traits and social decision making tendencies, and wants to discover how the different social traits and decision making tendencies relate to each other through the underlying facial features.\\n\\nA common problem encountered in these types of problems is that the large number of features relative to the number of data points typically entails some kind of dimensionality reduction and feature selection, and this process needs to be differently parameterized in order to optimize for each response variable, making direct comparison of the features underlying different response variables\"}"}
{"id": "kn7w8UX05a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Computational Framework\\n\\nFeature Representation. We train a three-color-channel AAM on the Chicago Face Database (Ma et al., 2015) plus the 10K US Adult Face Database (Bainbridge et al., 2013). Like conventional practice, we perform principal component analysis (PCA) on the faces the AAM was trained on, but unlike conventional practice, we do not reduce the number of principal components, resulting in a representation with $n = 10,764$ features.\\n\\nModel Evaluation.\\n\\nUsing leave-one-out cross-validation ($n = 52$), we evaluate the prediction $MSE$ on held-out test data. More formally, for each held-out face $x_i$, we predict a social decision ($\\\\hat{y}_i$):\\n\\n$$\\\\hat{y}_i = x_i^T \\\\hat{\\\\beta},$$\\n\\nwhere $\\\\hat{\\\\beta}$ is the minimum L2-norm estimator specified in Section 2. We then evaluate:\\n\\n$$\\\\text{mse}(\\\\hat{y}) = \\\\frac{1}{m} \\\\sum_{i=1}^{m} (y_i - \\\\hat{y}_i)^2.$$  \\n\\n3.3 Validation\\n\\nUsing the computational framework, as well as the responses collected in the social decision making study, we observe the prediction $MSE$ on unseen test data is 1) within the standard error of the mean of the MMSE estimator, and 2) well below chance for a wide variety of social decision-making tasks, indicating the over-parameterized representation generalizes well across tasks in practice (Figure 3).\\n\\n![Figure 3: Experimental validation of our mathematical framework for several social decision tasks. For all tasks, the fully over-parameterized estimator (orange bars, right) is within the error bounds (standard error of the mean) of the MMSE estimator (blue bars, left) and below chance level (+; variance of collected responses) for all tasks (except dominance, which cannot be predicted better than chance by any model), indicating the over-parameterized representation generalizes well across tasks in practice. Note that the below-chance dominance estimator will not be used in any subsequent analysis.]\\n\\n4 Application: Beauty Penalty and Trustworthiness Halo\\n\\nConsistent with previous studies, we observe a strong positive correlation between collected attractiveness ratings and social decisions in both social scenarios and economic games (Figure 4A), indicating both an attractiveness halo and a beauty premium. We observe an even stronger positive correlation between trustworthiness ratings and social decisions (Figure 4A), which indicates both a trustworthiness halo (this typically refers to trait perception and decision making in social scenarios) and a trustworthy premium (this typically refers to decision making in economic games). However, the strong positive correlation between attractiveness and trustworthiness ($CC = 0.53$, $p$-value $\\\\leq 0.001$) makes it impossible to separate the contributions of attractiveness and trustworthiness to the halo and premium effects from collected responses alone.\"}"}
{"id": "kn7w8UX05a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"To tease these contributions apart, we use the mathematical and computational framework developed above. Using leave-one-out cross-validation, we compute predicted social decisions using orthogonalized estimators ($\\\\hat{\\\\beta}_{A}^T$ and $\\\\hat{\\\\beta}_{T}^A$), then compute the correlation between these predictions ($\\\\hat{y}_{A}^T$ and $\\\\hat{y}_{T}^A$) and social decisions. This orthogonalized estimators contain facial feature information unique to that trait (task) and not related to the other trait. To orthogonalize the estimators, we calculate the normalized projections of one estimator onto the other. We calculate the orthogonal projection of $\\\\hat{\\\\beta}_{T}$ onto $\\\\hat{\\\\beta}_{A}$ as $\\\\hat{\\\\beta}_{T}^A = \\\\hat{\\\\beta}_{T} - (\\\\hat{\\\\beta}_{A} \\\\cdot \\\\hat{\\\\beta}_{T}) \\\\hat{\\\\beta}_{A}$.\\n\\nWe observe (Figure 4A) attractiveness unrelated to trustworthiness is not significantly correlated with any social scenarios (except dating app), while trustworthiness unrelated to attractiveness is significantly correlated with all social scenarios (except dating app). This indicates the halo effect is driven by trustworthiness, rather than attractiveness, though it appears as an attractiveness effect due to the facial features that contribute to both attractiveness and trustworthiness.\\n\\nWe also observe (Figure 4B) attractiveness unrelated to trustworthiness is significantly anti-correlated with two out of three economic games, while trustworthiness unrelated to attractiveness is significantly correlated with all economic games. Once again, it seems what masquerades as an attractiveness effect is truly a trustworthiness effect, and that rather than inducing a beauty premium, attractiveness by itself (excluding those facial features also contributing to trustworthiness) induces a beauty penalty. Without teasing apart the two components using feature orthogonalization, the beauty penalty effect is masked by the strong beauty/trustworthiness premium effect.\\n\\n![Figure 4: Heatmap of correlation coefficients (CCs) between social traits (cols. 1-6), social scenario decisions (cols. 10-14), economic games (cols. 7-9) with significance levels (*: p-value $\\\\leq 0.05$, **: p-value $\\\\leq 0.01$, ***: p-value $\\\\leq 0.001$).](image)\\n\\n(A) Both attractiveness (first row) and trustworthiness (second row) are significantly positively correlated with all economic games and social scenarios (except trustworthiness and dating app). However, the strong positive correlation between attractiveness and trustworthiness (CC $0.53$, p-value $< 0.001$) makes it impossible to tease the contributions of these two traits apart using just the collected responses. (B) When attractiveness is unrelated to trustworthiness (first row), the significant positive correlation with social scenarios disappears (except dating app), dispelling an attractiveness halo effect. The positive correlation with two of the three economic games (PD and TG) also becomes significantly negative, indicating a beauty penalty. When trustworthiness is unrelated to attractiveness (second row), on the other hand, the significant positive correlation remains. This shows a trustworthiness halo effect in social scenario decisions, as well as a trustworthiness premium in economic games. Note that there is a significant anti-correlation between the attractiveness and unrelated trustworthiness (row 1, col. 2), indicating non-linear effects in the responses, which cannot be captured by the linear models.\"}"}
{"id": "kn7w8UX05a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Since AAM readily generates faces for any coordinates in the feature space, we can visualize the estimator (regression coefficient) axes and their orthogonalized versions (Figure 5). Visual inspection reveals both more attractive (top row) and trustworthy (bottom row) faces smile more, while less attractive faces also appear older. More interestingly, orthogonalizing the attractiveness estimator against the trustworthiness is no longer related to smiley-ness but appears anti-correlated with age (more youthful-looking faces are more attractive, which has previously been observed (Sutherland et al., 2013). Notably, the projections of the face stimuli used in the experiment along this dimension are indeed significantly correlated (CC = \u22120.29, p-value < 0.05), with previously collected age ratings of these faces Bainbridge et al. (2013), while these projections are significantly negatively correlated with ratings in economic games (Figure 4). To summarize, the above results imply that the youthfulness-related component of attractiveness drive a \u201cbeauty penalty\u201d effect in economic games, while the facial features that drive both attractiveness and trustworthiness perception are what give rise to an attraction/trustworthiness halo. In addition, when we orthogonalize trustworthiness against attractiveness, a strong smiley-ness effect remains (just as in the unorthogonalized case), while the age effect mostly disappears. Moreover, we find that this residual component unrelated to attractiveness is still positively correlated with social scenario and economic games. \\n\\nFigure 5: Face visualization along regression coefficient (estimators) directions without (A) and with (B) orthogonalization. Top row: attractiveness; bottom row: trustworthiness. The middle face in each triplet is the average face (corresponding to feature coordinates that average over all 52 faces used in the study). Visualization moves in equal steps along the estimator axes (left: negative direction, right: positive direction).\\n\\n5 DISCUSSION\\nIn this paper, we provided conditions under which an over-parameterized representation is guaranteed to yield optimal, as well as better than chance, generalization performance in a linear regression setting with hard constraints. We also provided exact expression for the estimator error, as well as an exact expression for how far the fully over-parameterized estimator is from the optimal hard-regularizer. We next validated the usefulness of our mathematical framework by applying it to a wide range of social decision-making tasks, in which the fully over-parameterized estimator performed within the error bounds (standard error of the mean) of the theoretically optimal estimator on all tasks. We then used this framework to show 1) the halo effect appears to arise from trustworthiness rather than attractiveness, and 2) trustworthiness unrelated to attractiveness induces a premium in economic games, while attractiveness unrelated to trustworthiness induces a penalty, indicating a trustworthiness premium and beauty penalty, which helps reconciling conflicting reports in the existing literature. Moreover AAM-based visualization indicated that the trustworthiness halo/premium is underpinned by smiley-ness and the beauty premium by youthfulness (through the component specifically important for attractiveness). \\n\\nWhile some of the statistical analyses among traits, social scenarios, and economic games could have been done using only ratings, the ability of grounding those ratings in an image-computable and generative model representation is highly valuable. Without the latter, we wouldn't have been able to orthogonalize estimated regression coefficient vectors against one another (orthogonalization...\"}"}
{"id": "kn7w8UX05a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"makes no sense if feature vectors do not live in the same feature space), or visualize faces along those vectors. Such visualizations (Figure 5) reveal that both trustworthiness and attractiveness unrelated to attractiveness appear highly correlated with smiling. This begs the question of how smiling, or emotional states such as happiness, contribute to the halo effect. Such data can be collected framework helps to identify concrete directions for future research endeavors. Having a universal, overparameterized representation that serves all tasks can assist with iterative scientific analysis and hypothesis generation, as new experiments are designed and data collected, and new conclusions are drawn.\\n\\nOne limitation of our framework is that it does not include contributions from non-linear components, which have been found to contribute to trait ratings, including attractiveness (Ryali & Yu, 2018; Todorov & Oosterhof, 2011) and trustworthiness (Todorov & Oosterhof, 2011). A further limitation of our study is that we only focused on female faces. There is evidence that dominance and trustworthiness are rated using gender-based internal models (He & Yu, 2021), which could also be true of social decisions. In addition, a strong correlation between dominance and election success has been established in the literature for male faces (Berinsky et al., 2019). However, a preliminary analysis of dominance ratings collected in the social decision-making experiment reveals no such correlation for female faces (Figure 6), indicating different traits might contribute to halo effects for female and male faces. These questions remain exciting avenues for future work.\\n\\nThe more general limitations of our theoretical framework is that the optimality conditions only hold for hard regularizers, and that our error expressions are for estimator MSE rather than prediction MSE. Extending the general theoretical results to soft-regularizers (such as ridge- and lasso-regression) and the more practically useful prediction MSE are also exciting future directions.\\n\\n5.1 RELATED THEORETICAL WORK IN OVERPARAMETERIZED LINEAR REGRESSION\\n\\nOur PCR approach might at first glance seem identical to that of Xu & Hsu (2019). However, while Xu & Hsu (2019) analyze what they call an \\\"oracle\\\" estimator, which uses the generative (\\\"true\\\") covariance matrix, we use the more classical version of PCR, which is based on the sample covariance matrix. This results in quite different behavior. For instance, there is no over-parameterized regime in PCR, as \\\\( m \\\\) data points can be expressed by most \\\\( m \\\\) linearly independent features (\\\\( m - 1 \\\\) when the data is centered). As such, there is no \\\"second descent\\\" in PCR. Xu & Hsu (2019) also noted that a full analysis that accounts for estimation errors in PCR remains open, though it is worth noting that an extensive analysis of the under-parameterized regime was done by Park (1981).\\n\\nAlso worth noting is that there seems to be a sharp divide between the \\\"classical\\\" under-parameterized and the \\\"modern\\\" over-parameterized regime in the literature, with an understanding of the latter \\\"now only starting to emerge\\\" (Belkin et al., 2020). We offer a different view by showing any over-parameterized representation has an \\\"equivalent\\\" under-parameterized representation, and as such, the over-parameterized regime can be fully understood in terms of the under-parameterized regime.\"}"}
{"id": "kn7w8UX05a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u03c3^2_y is signal variance, \u03c3^2_\u03f5 the noise variance, and s_1, .., s_r are the singular values of the design matrix.\\n\\nProof.\\n\\n\u03c3^2_\u03b2 := tr(E[\u03b2\u03b2^T])\\n= tr(E[X^\u2020(y\u2212\u03f5)(y\u2212\u03f5)^TX^\u2020^T])\\n= tr(X^\u2020(E[yy^T]\u2212E[\u03f5\u03f5^T])X^\u2020^T)\\n= tr(X^\u2020(\u03c3^2_y\u2212\u03c3^2_\u03f5)X^\u2020)\\n= (\u03c3^2_y\u2212\u03c3^2_\u03f5) tr(X_i=1s^2_i).\\n\\nEstimator Condition 1.\\nIf the noise-variance in y (\u03c3^2_\u03f5) is less than or equal to half the signal-variance in y (\u03c3^2_y), then \\\\( \\\\hat{\u03b2} \\\\) is an above chance estimator, i.e.\\n\\n\\\\[\\n\u03c3^2_\u03f5 \\\\leq \\\\frac{\u03c3^2_y}{2} \\\\iff \\\\text{mse}(\\\\hat{\u03b2}) \\\\leq \u03c3^2_\u03b2, \\\\quad (19)\\n\\\\]\\n\\nwhere \u03c3^2_\u03b2 is the variance of the parameter vector \u03b2.\\n\\nProof.\\nThis follows from Error Expression 1 and Lemma 6.\\n\\n\\\\[\\n\\\\text{mse}(\\\\hat{\u03b2}) \\\\leq \u03c3^2_\u03b2 \\\\iff \u03c3^2_\u03f5 \\\\leq (\u03c3^2_y\u2212\u03c3^2_\u03f5) \\\\iff 2\u03c3^2_\u03f5 \\\\leq \u03c3^2_y \\\\iff \u03c3^2_\u03f5 \\\\leq \u03c3^2_y^2.\\n\\\\]\\n\\nLemma 7.\\nThe MSE of the feature reduced PCR estimator \\\\( \\\\hat{\u03b8}_p \\\\) (with \\\\( p \\\\leq r \\\\) coefficients) is given by,\\n\\n\\\\[\\n\\\\text{mse}(\\\\hat{\u03b8}_p) = ||\u03b2||^2 r(r\u2212p) + \u03c3^2_\u03f5 p X_i=1s^2_i \\\\quad (20)\\n\\\\]\\n\\nProof.\\nRecall that \u03b8 can be written in terms of Z, y, and \u03f5, as \u03b8 = Z^\u2020(y\u2212\u03f5). First note that the feature reduced PCR estimator \\\\( \\\\hat{\u03b8}_p \\\\) is given by,\\n\\n\\\\[\\n\\\\hat{\u03b8}_p := Z^\u2020_p y = Z^\u2020_p (Z\u03b8 + \u03f5) = \u03a3^\u2020_p U^T U\u03a3\u03b8 + Z^\u2020_p \u03f5 = I_p \u03b8 + Z^\u2020_p \u03f5,\\n\\\\]\\n\\nwhere I_p is an \\\\( m \\\\times r \\\\) dimensional matrix with ones on the diagonal for the first \\\\( p \\\\) entries and zeros on the remainder.\"}"}
{"id": "kn7w8UX05a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"It then follows that,\\n\\n\\\\[ \\\\text{mse}(\\\\hat{\\\\theta}_p) := \\\\text{tr} \\\\mathbb{E} \\\\left[ (\\\\theta - \\\\hat{\\\\theta}_p)(\\\\theta - \\\\hat{\\\\theta}_p)^T \\\\right] = \\\\text{tr} \\\\mathbb{E} \\\\left[ (I_r - I_p)\\\\theta\\\\theta^T(I_r - I_p)^T \\\\right] + \\\\sigma^2 \\\\mathbb{E} \\\\left[ Z^\\\\dagger p \\\\mathbb{E} \\\\right] \\\\]\\n\\n\\\\[ = \\\\text{tr} (I_r - I_p) \\\\mathbb{E}[\\\\theta\\\\theta^T(I_r - I_p)^T] + \\\\sigma^2 \\\\mathbb{E} \\\\left[ \\\\sum_{i=1}^{p+1} 1_s^2 \\\\right] = ||\\\\beta||^2 r(r - p) + \\\\sigma^2 \\\\mathbb{E} \\\\left[ \\\\sum_{i=1}^{p+1} 1_s^2 \\\\right]. \\\\]\\n\\nError Expression 2.\\nSuppose the MMSE PCR estimator \\\\( \\\\hat{\\\\theta}^* \\\\) has \\\\( p \\\\leq r \\\\) components. Then the difference in MSEs between the MMSE estimator and the fully over-parameterized estimator is given by,\\n\\n\\\\[ \\\\text{mse}(\\\\hat{\\\\beta}) - \\\\text{mse}(\\\\hat{\\\\theta}^*) = \\\\sigma^2 \\\\mathbb{E} \\\\left[ \\\\sum_{i=1}^{p+1} 1_s^2 \\\\right] - ||\\\\beta||^2 r(r - p) + \\\\sigma^2 \\\\mathbb{E} \\\\left[ \\\\sum_{i=1}^{p+1} 1_s^2 \\\\right]. \\\\]\\n\\n(21)\\n\\nProof.\\nThis follows from Lemma 7 as well as Error Expression 1.\"}"}
{"id": "kn7w8UX05a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"challenging. In the worst case, there may not be any subset of features that can predict all response variables better than chance level. Here, we leverage the \u201cdouble descent\u201d phenomenon to develop and present a novel analysis framework that obviates such issues by relying on a universal, overly parameterized feature representation. As a case study, we apply the framework to better understand the underlying facial features that contribute separately and conjointly to human trait perception and social decision making.\\n\\nHumans readily infer social traits, such as attractiveness and trustworthiness, from as little as a 100 ms exposure to a stranger\u2019s face (Willis & Todorov, 2006). Though the veracity of such judgments is still an area of active research (Valla et al., 2011; Todorov et al., 2015), such trait evaluations have been found to predict important social outcomes, ranging from electoral success (Todorov et al., 2005; Ballew & Todorov, 2007; Little et al., 2007) to prison sentencing decisions (Blair et al., 2004; Eberhardt et al., 2006).\\n\\nIn particular, psychologists have observed an \u201cattractiveness halo\u201d, whereby humans tend to ascribe more positive attributes to more attractive individuals (Eagly et al., 1991; Langlois et al., 2000), and economists have observed a related phenomenon, the \u201cbeauty premium\u201d, whereby more attractive individuals out-earn less attractive individuals in economics games (Mobius & Rosenblat, 2006).\\n\\nHowever, these claims are not without controversy (Andreoni & Petrie, 2008; Willis & Todorov, 2006), as more attractive people can also incur a \u201cbeauty penalty\u201d in certain situations. Moreover, a robust correlation between attractiveness and trustworthiness (Willis & Todorov, 2006; Oosterhof & Todorov, 2008; Xu et al., 2012; Ryali et al., 2020) has also been reported, making it unclear how much of the attractiveness halo effect might be indirectly due to perceived trustworthiness.\\n\\nTo tease apart the contributions of trustworthiness and attractiveness to social perception and decision-making, we perform linear regression of different response variables, consisting of subjects\u2019 ratings of social perception and social decision-making tendencies, against features of the Active Appearance Model (AAM), a well-established computer vision model (Cootes et al., 2001), whose features have been found to be linearly encoded by macaque face-processing neurons (Chang & Tsao, 2017). A similar regression framework has been adopted by previous work modeling human face trait perception (Oosterhof & Todorov, 2008; Said & Todorov, 2011; Song et al., 2017; Guan et al., 2018; Ryali et al., 2020), using features either from AAM or deep neural networks. Because the number of features is typically quite large, usually larger than the number of rated faces, previous approaches have all used some combination of dimensionality reduction and feature selection. This approach gives rise to a dilemma when one wants to compare the facial features contributing to different types of social perceptions (response variables), since the number of features that optimizes prediction accuracy for each task can be quite different (see Figure 1). Either one optimizes this quantity separately for each task, thus not having a common set of features to compare across; or one can fix a particular set of features for all tasks, but then having suboptimal prediction accuracy (in the worst case, perhaps worse than chance level performance).\\n\\nTo overcome this challenge, we appeal to \u201cthe double descent\u201d (Belkin et al., 2019; 2020) trick, the use of a highly overparameterized representation (more features than data points) to achieve good performance. In particular, if we use the original AAM feature representation, while foregoing any kind of dimensionality reduction or feature selection, then we have a universal representation that may also have great performance on all tasks, even novel tasks not seen before, or responses corresponding to predictor variable settings totally different than previously seen. While overparameterized linear regression has chiefly been used as an analytically tractable case study (Belkin et al., 2019; Xu & Hsu, 2019; Belkin et al., 2020) to gain insight into the theoretical basis and properties of \u201cdouble descent\u201d, we use it as a practical setting for scientific data analysis. Notably, while previous papers on overparameterized regression defined statistical assumptions and constraints in the generative sense, we work for pragmatic reasons purely with sample statistics (e.g. whether two features are \u201ctruly\u201d decorrelated (Xu & Hsu, 2019)), we work directly with sample statistics (e.g. whether two feature vectors across a set of data points have a correlation coefficient of 0). For this reason, our theoretical results are distinct from and novel with respect to those prior results.\\n\\nFinally, it is noteworthy that the human visual pathway also exhibits feature expansion rather than feature reduction, from the sensory periphery to higher cortical areas (Wandell, 1995) \u2013 this raises the intriguing possibility that the brain has also discovered an overparameterized representation as a universal representation for learning to perform well on many tasks, including novel ones not previously encountered.\"}"}
{"id": "kn7w8UX05a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Section 2, we use over-parameterized linear regression to develop a framework that generalizes well across both tasks and data space. We provide theoretical conditions under which the complete over-parameterized representation is 1) guaranteed to yield linear estimators that perform better than chance, and 2) are optimal among the class of hard regularizers. We also provide exact error expressions for these estimators, as well as an exact measure of how far the estimators are from the optimal hard regularizers when the over-parameterized estimators are suboptimal.\\n\\nIn Section 3, we verify the practical usefulness of our theoretical framework by comparing the prediction accuracy of over-parameterized regression against task-specific classical (under-parameterized) regression that optimizes feature selection for each task, on original data collected from a face-based social perception and decision-making study.\\n\\nIn Section 4, we apply our mathematical and computational framework to show that the halo effect appears to arise from trustworthiness rather than attractiveness per se, and that attractiveness unrelated to trustworthiness actually induces a beauty penalty, while trustworthiness unrelated to attractiveness induces a premium, thus reconciling conflicting results in the literature regarding the existence of an attractiveness halo. Finally, we present a novel finding that the component of attractiveness related to pro-social perception and judgment is related to how smiley a face appears, while the component of attractiveness unrelated to attractiveness is related to the youthfulness of facial appearance.\\n\\nFigure 1: Loss (prediction MSE), as a function of number of features for face-based social-perception tasks, using cross-validation as specified in section 3. The vertical dashed lines indicate the minimum error in the under-parameterized regime for the different tasks (response variables), illustrating the difficulty of finding a common number of features to use for all tasks in the under-parameterized regime. The fully over-parameterized regression results in better-than-chance MSE for all tasks. Horizontal dashed line: variance of collected responses, normalized to 1 in all tasks for ease of visual comparison; error bars: standard error of the mean.\\n\\nWe consider a linear regression problem where each response $y$ is a linear function of $n$ real-valued variables $x \\\\in \\\\mathbb{R}^n$, parameterized by a vector $\\\\beta \\\\in \\\\mathbb{R}^n$, in addition to some noise ($\\\\epsilon$). More formally for $m$ datapoints (with $n \\\\geq m$), we assume:\\n\\n$$y = X\\\\beta + \\\\epsilon, \\\\quad \\\\epsilon \\\\sim N(0, \\\\sigma^2)$$\\n\\n(1)\\n\\nwith both $\\\\beta$ and $\\\\epsilon$ zero-mean and i.i.d.. We further assume, without loss of generality, that both the design matrix $X \\\\in \\\\mathbb{R}^{m \\\\times n}$ and the vector of responses $y \\\\in \\\\mathbb{R}^m$ are centered, and that $X$ is full rank, with rank denoted by $r$. Note that for an over-parameterized, centered full rank matrix, $r = m - 1$.\\n\\nWe use the pseudoinverse to obtain the $n$-dimensional minimum L2-norm estimator $\\\\hat{\\\\beta}$ of $\\\\beta$,\\n\\n$$\\\\hat{\\\\beta} = X^+ y,$$\\n\\n(2)\"}"}
{"id": "kn7w8UX05a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The mean squared error (MSE) to evaluate the estimator $\\\\hat{\\\\beta}$:\\n\\n$$\\\\text{mse}(\\\\hat{\\\\beta}) = \\\\text{tr} \\\\mathbb{E}[(\\\\beta - \\\\hat{\\\\beta})(\\\\beta - \\\\hat{\\\\beta})^T],$$\\n\\nwhere $\\\\text{tr}(\\\\cdot)$ denotes the matrix trace and $\\\\mathbb{E}[\\\\cdot]$ the expected value. We also use $\\\\| \\\\cdot \\\\|$ to denote the L2-norm.\\n\\n2.1 Theoretical Conditions for a Good Estimator\\n\\n**Estimator Condition 1.** If the noise-variance in $y$ ($\\\\sigma^2_{\\\\epsilon}$) is less than or equal to half the signal-variance in $y$ ($\\\\sigma^2_y$), then $\\\\hat{\\\\beta}$ is an above chance estimator, i.e.\\n\\n$$\\\\sigma^2_{\\\\epsilon} \\\\leq \\\\sigma^2_y \\\\iff \\\\text{mse}(\\\\hat{\\\\beta}) \\\\leq \\\\sigma^2_{\\\\beta},$$\\n\\nwhere $\\\\sigma^2_{\\\\beta}$ is the variance of the parameter vector $\\\\beta$.\\n\\n**Proof Sketch.** This follows from the definition of $\\\\text{mse}(\\\\hat{\\\\beta})$ and $\\\\sigma^2_{\\\\beta}$, the cyclical properties of the trace and linearity of expectation, as well as the i.i.d. assumptions on $\\\\beta$ and $\\\\epsilon$. See the appendix for an explicit derivation.\\n\\n**Estimator Condition 2.** If the smallest singular value ($s_r$) of the design matrix $X$ satisfies,\\n\\n$$s^2_r \\\\geq \\\\sigma^2_{\\\\epsilon} \\\\| \\\\beta \\\\|^2 / r,$$\\n\\nthen $\\\\hat{\\\\beta}$ is the minimum MSE (MMSE) estimator among the class of hard regularizers subject to linear constraints.\\n\\n**Proof Sketch.** Park (1981) proved the above for the prediction MSE of a PCR estimator in the under-parameterized regime. An extension to the over-parameterized regime follows as 1) the MSE is invariant under orthogonal transformations, and 2) any over-parameterized estimator has an \\\"equivalent\\\" under-parameterized estimator (equivalent in the sense that the estimators yield the same MSE). See the appendix for a detailed proof.\\n\\n2.2 Exact Error Expressions\\n\\n**Error Expression 1.** The MSE of the over-parameterized estimator $\\\\hat{\\\\beta}$ is given by\\n\\n$$\\\\text{mse}(\\\\hat{\\\\beta}) = \\\\sigma^2_{\\\\epsilon} \\\\sum_{i=1}^{r} 1 / s^2_i,$$\\n\\nwhere $s_1, \\\\ldots, s_r$ are the singular values of the design matrix $X$.\\n\\n**Proof Sketch.** Once again, this follows from the definition of $\\\\text{mse}(\\\\hat{\\\\beta})$, the cyclical properties of the trace and linearity of expectation, as well as the i.i.d. assumptions on $\\\\beta$ and $\\\\epsilon$. See the appendix for an explicit derivation.\\n\\n**Error Expression 2.** Suppose the MMSE estimator $\\\\hat{\\\\theta}^*$ has $p$ components. Then the difference in MSEs between the MMSE estimator and the fully over-parameterized estimator is given by,\\n\\n$$\\\\text{mse}(\\\\hat{\\\\beta}) - \\\\text{mse}(\\\\hat{\\\\theta}^*) = \\\\sigma^2_{\\\\epsilon} \\\\sum_{i=p+1}^{r} 1 / s^2_i - \\\\| \\\\beta \\\\|^2 / r (r - p).$$\\n\\n**Proof Sketch.** This follows from extending Park (1981) to the over-parameterized regime, in addition to the definition of MSE, the cyclical properties of the trace and linearity of expectation, as well as the i.i.d. assumptions on $\\\\beta$ and $\\\\epsilon$. See the appendix for a detailed proof.\"}"}
{"id": "kn7w8UX05a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As the theoretical conditions and error expressions established in the previous section depend on variables that are unknown in real data (such as noise and signal variance, the norm of the true parameter vector etc.), and as real data may violate the theoretical assumptions, we validate how well the over-parameterized representation generalizes in practice using data collected in a face-based social decision-making study (Figure 2).\\n\\n### 3.1 Social Decision-Making Experiment\\n\\n613 undergraduate students at the University of California, San Diego participated in a 3 block hour-long study in which they were asked to rate social traits (block A), make decisions in social scenarios (block B), and play economic games (block C) with novel face images (Figure 2). All blocks were counterbalanced across subjects.\\n\\n**Inclusion/exclusion criteria.** Participants who had a response entropy and/or a CC between their response and the average response below two standard deviations of the mean were excluded, resulting in standardized responses from 485 subjects being included in the analysis.\\n\\n**Face stimuli.** 72 white female faces with direct gaze and natural expressions were sampled from the 10K US Adult Faces Database (Bainbridge et al., 2013). A sub-sample of 52 faces was then used in blocks A and C, while 36 face pairings were used in block B. The 52 face images used in all blocks were included in the analysis.\\n\\n#### A. Social Decision Making Tasks\\n\\n1. **Dating App.** Suppose two people on a dating app sent you a greeting message. Which of them would you (or your friend of the appropriate sexual orientation) be more willing to respond to?\\n\\n2. **Job Interview.** Suppose you represent a company at a job fair, and two individuals approached you to discuss job openings. Which of them would you more willing to talk to?\\n\\n3. **Eye Witness.** Suppose two people are eye witnesses of a gas station robbery and gave contrary accounts. Which of them would you be more willing to believe?\\n\\n4. **Election.** Suppose two people are candidates for a statewide election. Which of them are you more likely to vote for?\\n\\n5. **Stranded Motorist.** Suppose two drivers are standing next to broken-down cars on the side of the highway. Which of them would you be more willing to help?\\n\\n---\\n\\n### Questions displayed in the five social scenario tasks.\\n\\n1. **Dating App.**\\n2. **Job Interview.**\\n3. **Eye Witness.**\\n4. **Election.**\\n5. **Stranded Motorist.**\"}"}
{"id": "kn7w8UX05a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nJames Andreoni and Ragan Petrie. Beauty, gender and stereotypes: Evidence from laboratory experiments. *Journal of Economic Psychology*, 29(1):73\u201393, 2008.\\n\\nWilma A Bainbridge, Phillip Isola, and Aude Oliva. The intrinsic memorability of face photographs. *Journal of Experimental Psychology: General*, 142(4):1323, 2013.\\n\\nCharles C Ballew and Alexander Todorov. Predicting political elections from rapid and unreflective face judgments. *Proceedings of the National Academy of Sciences*, 104(46):17948\u201317953, 2007.\\n\\nMikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias\u2013variance trade-off. *Proceedings of the National Academy of Sciences*, 116(32):15849\u201315854, 2019. doi: 10.1073/pnas.1903070116. URL https://www.pnas.org/doi/abs/10.1073/pnas.1903070116.\\n\\nMikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. *SIAM Journal on Mathematics of Data Science*, 2(4):1167\u20131180, 2020. doi: 10.1137/20M1336072. URL https://doi.org/10.1137/20M1336072.\\n\\nAdam J Berinsky, Sara Chatfield, and Gabriel Lenz. Facial dominance and electoral success in times of war and peace. *The Journal of Politics*, 81(3):1096\u20131100, 2019.\\n\\nIrene V Blair, Charles M Judd, and Kristine M Chapleau. The influence of afrocentric facial features in criminal sentencing. *Psychological science*, 15(10):674\u2013679, 2004.\\n\\nLe Chang and Doris Y Tsao. The code for facial identity in the primate brain. *Cell*, 169(6):1013\u20131028, 2017.\\n\\nTimothy F. Cootes, Gareth J. Edwards, and Christopher J. Taylor. Active appearance models. *IEEE Transactions on pattern analysis and machine intelligence*, 23(6):681\u2013685, 2001.\\n\\nAlice H Eagly, Richard D Ashmore, Mona G Makhijani, and Laura C Longo. What is beautiful is good, but...: A meta-analytic review of research on the physical attractiveness stereotype. *Psychological bulletin*, 110(1):109, 1991.\\n\\nJennifer L Eberhardt, Paul G Davies, Valerie J Purdie-Vaughns, and Sheri Lynn Johnson. Looking deathworthy: Perceived stereotypicality of black defendants predicts capital-sentencing outcomes. *Psychological science*, 17(5):383\u2013386, 2006.\\n\\nJinyan Guan, Chaitanya K Ryali, and J Yu Angela. Computational modeling of social face perception in humans: Leveraging the active appearance model. *bioRxiv*, pp. 360776, 2018.\\n\\nZoe W He and Angela J Yu. Gender differences in face-based trait perception and social decision making. In *Proceedings of the Annual Meeting of the Cognitive Science Society*, volume 43, 2021.\\n\\nJudith H Langlois, Lisa Kalakanis, Adam J Rubenstein, Andrea Larson, Monica Hallam, and Monica Smoot. Maxims or myths of beauty? a meta-analytic and theoretical review. *Psychological bulletin*, 126(3):390, 2000.\\n\\nAnthony C Little, Robert P Burriss, Benedict C Jones, and S Craig Roberts. Facial appearance affects voting decisions. *Evolution and Human Behavior*, 28(1):18\u201327, 2007.\\n\\nDebbie S Ma, Joshua Correll, and Bernd Wittenbrink. The Chicago face database: A free stimulus set of faces and norming data. *Behavior research methods*, 47(4):1122\u20131135, 2015.\\n\\nMarkus M Mobius and Tanya S Rosenblat. Why beauty matters. *American Economic Review*, 96(1):222\u2013235, 2006.\\n\\nNikolaas N Oosterhof and Alexander Todorov. The functional basis of face evaluation. *Proceedings of the National Academy of Sciences*, 105(32):11087\u201311092, 2008.\\n\\nSung H. Park. Collinearity and optimal restrictions on regression parameters for estimating responses. *Technometrics*, 23(3):289\u2013295, 1981. ISSN 0040-1706. URL http://www.jstor.org/stable/1267793.\"}"}
{"id": "kn7w8UX05a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Recall, we consider an over-parameterized linear regression setting ($n \\\\geq m$),\\n\\\\[ y = X \\\\beta + \\\\epsilon, \\\\quad \\\\epsilon \\\\sim N(0, \\\\sigma^2) \\\\] (11)\\nwith both $\\\\beta \\\\in \\\\mathbb{R}^n$ and $\\\\epsilon \\\\in \\\\mathbb{R}^m$ assumed to be zero-mean and i.i.d, and rank of $X \\\\in \\\\mathbb{R}^{m \\\\times n}$ denoted by $r$.\\n\\nWe use the fact that any matrix $X$ can be written in terms of its singular value decomposition as $X = U \\\\Sigma V^T$, and $X$ expressed in its principal component (PC) representation is simply $U \\\\Sigma$, which we denote by $Z$.\\n\\nWe also use the PCR setting,\\n\\\\[ y = Z \\\\theta + \\\\epsilon, \\\\quad \\\\epsilon \\\\sim N(0, \\\\sigma^2) \\\\] (12)\\n\\\\[ \\\\hat{\\\\theta} = Z^+ y \\\\] (13)\"}"}
{"id": "kn7w8UX05a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nwhere $Z \\\\in \\\\mathbb{R}^{m \\\\times r}$ is the design matrix expressed in terms of its PCs and $\\\\hat{\\\\theta}$ is the PCR estimator.\\n\\nNote on terminology. By the MMSE estimator, we mean the minimum mean squared error estimator among the class of hard regularizers subject to linear constraints (this does not include soft-regularizers, such as ridge and lasso estimators).\\n\\nA.1.2 PROOFS\\n\\nLemma 1. The PCR estimator with all PCs is an orthogonal transformation of the over-parameterized estimator with all features.\\n\\nProof. This follows from the definitions. Let $\\\\hat{\\\\theta}$ denote the PCR estimator and $\\\\hat{\\\\beta}$ denote the over-parameterized estimator. Then,\\n\\n$$\\n\\\\hat{\\\\beta} = X^\\\\dagger y = V\\\\Sigma^{\\\\dagger}U^T y = VZ^{\\\\dagger} y = V\\\\hat{\\\\theta}.\\n$$\\n\\nLemma 2. Estimator MSE is invariant under orthogonal transformations.\\n\\nProof. Let $\\\\beta$ denote the estimator and $V$ denote an orthogonal matrix. Then,\\n\\n$$\\nmse(V\\\\hat{\\\\beta}) = \\\\text{tr} \\\\mathbb{E}[(V\\\\beta - V\\\\hat{\\\\beta})(V\\\\beta - V\\\\hat{\\\\beta})^T] = \\\\text{tr} V \\\\mathbb{E}[(\\\\beta - \\\\hat{\\\\beta})(\\\\beta - \\\\hat{\\\\beta})^T] V^T = \\\\text{tr} \\\\mathbb{E}[(\\\\beta - \\\\hat{\\\\beta})(\\\\beta - \\\\hat{\\\\beta})^T] V^T = mse(\\\\hat{\\\\beta}).\\n$$\\n\\nLemma 3. The MSE of the PCR estimator with all PCs equals the MSE of the over-parameterized estimator with all features.\\n\\nProof. This follows from Lemmas 1 and 2. Let $\\\\hat{\\\\theta}$ denote the PCR estimator and $\\\\hat{\\\\beta}$ denote the over-parameterized estimator. Then,\\n\\n$$\\nmse(\\\\hat{\\\\theta}) = mse(V\\\\hat{\\\\theta}) = mse(\\\\hat{\\\\beta}).\\n$$\\n\\nLemma 4. The MSE of the minimum MSE PCR estimator lower bounds the MSE of the over-parameterized estimator with all $n$ features.\\n\\nProof. This follows from the definition of the minimum MSE, as well as Lemma 3. Denote the minimum MSE PCR estimator as $\\\\hat{\\\\theta}^*$, the PCR estimator with all PCs as $\\\\hat{\\\\theta}$ and the over-parameterized estimator with all features as $\\\\hat{\\\\beta}$. Then,\\n\\n$$\\nmse(\\\\hat{\\\\theta}^*) \\\\overset{(def)}{\\\\leq} mse(\\\\hat{\\\\theta}) = mse(\\\\hat{\\\\beta}).\\n$$\\n\\nLemma 5. The MSE of the minimum MSE PCR estimator lower bounds the MSE of an over-parameterized, feature reduced estimator.\\n\\nProof. This follows from Lemma 3, as well as Park (1981). Let $X_n$ be the original design matrix with all $n$ features, and denote the minimum PCR estimator of this design matrix as $\\\\hat{\\\\theta}^*$. Let $X_p$ be a feature reduced design matrix, expressed in any of the $p < n$ original features, and denote the $p$ dimensional estimator of $X_p$ as $\\\\hat{\\\\alpha}$. Denote $\\\\hat{\\\\phi}^*$ and $\\\\hat{\\\\phi}$ as the minimum MSE PCR estimator and PCR estimator with all PCs respectively. Note that these estimators are under-parameterized estimators. It then follows that,\"}"}
{"id": "kn7w8UX05a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Theorem 1. The MSE of the minimum MSE PCR estimator lower bounds the MSE in both the\\nover-parameterized and under-parameterized regimes.\\n\\nProof. This follows as the minimum MSE PCR estimator lower bounds the MSE of under-\\nparameterized estimators (Park, 1981), as well as over-parameterized estimators (lemmas 4 and\\n5).\\n\\nPark 1. If the $p$-th singular value ($s_p$) of the design matrix $X$ satisfies,\\n$$s_p^2 \\\\geq \\\\sigma^2 \\\\epsilon ||\u03b2||^2 / r,$$\\nthen $\\\\hat{\u03b8}_p$ is the MMSE PCR estimator (Park, 1981).\\n\\nProof. See Park (1981).\\n\\nEstimator Condition 2. If the smallest singular value ($s_r$) of the design matrix $X$ satisfies,\\n$$s_r^2 \\\\geq \\\\sigma^2 \\\\epsilon ||\u03b2||^2 / r,$$\\nthen the over-parameterized estimator with all $n$ features is the minimum MSE estimator.\\n\\nProof. This follows from Park 1, as well as Theorem 1. Denote the minimum MSE PCR estimator\\nas $\\\\hat{\u03b8}^*$, the PCR estimator with all PCs as $\\\\hat{\u03b8}$, and the over-parameterized estimator with all\\n$n$ features as $\\\\hat{\u03b2}$, and suppose the threshold is satisfied for the $r$-th singular value. Then,\\n$$\\\\text{mse}(\\\\hat{\u03b8}^*) = \\\\text{mse}(\\\\hat{\u03b8}) = \\\\text{mse}(\\\\hat{\u03b2}),$$\\nwhich we know from Theorem 1 lower bounds the MSE in both the under-parameterized and over-\\nparameterized regimes. As such, $\\\\hat{\u03b2}$ is an MMSE estimator.\\n\\nError Expression 1. The MSE of the over-parameterized estimator without feature reduction is\\ngiven by\\n$$\\\\text{mse}(\\\\hat{\u03b2}) = \\\\sigma^2 \\\\epsilon \\\\sum_{i=1}^{r} \\\\frac{1}{s_i^2},$$\\nwhere $\\\\sigma^2 \\\\epsilon$ is the noise variance, and $s_1, \\\\ldots, s_r$ the singular values of the design matrix $X$.\\n\\nProof. Note that $\u03b2$ can be written in terms of $X$, $y$, and $\u03f5$, as\\n$$\u03b2 = X^\\\\dagger(y - \u03f5).$$\\nThen,\\n$$\\\\text{mse}(\\\\hat{\u03b2}) = \\\\text{tr} \\\\mathbb{E}[(\u03b2 - \\\\hat{\u03b2})(\u03b2 - \\\\hat{\u03b2})^T] = \\\\text{tr} \\\\mathbb{E}[(X^\\\\dagger \u03f5)(X^\\\\dagger \u03f5)^T] = \\\\text{tr} \\\\mathbb{E}[(U\u03a3^\\\\dagger)^2 U^T] = \\\\sigma^2 \u03f5 \\\\text{tr}(U\u03a3^\\\\dagger U^T) = \\\\sigma^2 \u03f5 \\\\text{tr}(\u03a3^\\\\dagger U^T U) = \\\\text{tr}(\u03a3^\\\\dagger) = \\\\text{tr}(\u03a3^\\\\dagger) \\\\sum_{i=1}^{r} \\\\frac{1}{s_i^2}.$$\"}"}
