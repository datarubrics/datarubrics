{"id": "vSsnEd0Jmou", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Running a Simulation\\n\\nFigure 6: Two hypothetical scenarios.\\n\\nWe require to evaluate a policy (e.g. MELD-Na) in hypothetical (contradictory) scenarios. Consider Figure 6 where we ran the MELD-Na policy in two hypothetical scenarios: one where COVID-19 happens, and one where it doesn\u2019t. With AllSim we can model each scenario confidently. For this particular example, we fix the seed of AllSim and only change the supply of organs by giving two different resource arrival processes:\\n\\n```python\\n1 def covid(t):\\n2     if t < 600:\\n3         return 0.5\\n4     else:\\n5         return 0.25\\n\\n7 def no_covid(t):\\n8     return 0.5\\n```\\n\\nTuning the simulation. Running and composing a simulation, is as simple as defining how we wish to let the supply of resources and recipients evolve in $t$. Such evolution is expressed a changing arrival rate (crf. Equations (3) and (4). In particular, one only needs to define $\\\\lambda_{x,r}$, normalisation is handled by AllSim. However, even there does AllSim offer detailed specification possibilities. Like $\\\\lambda_{x,r}$ we can specify a custom function for $\\\\alpha_{x,r}$ also. Normalisation, will then respect the value of $\\\\alpha$ at time $t$. For example, if we wish the total arrival rate of all resources to remain constant at 5,\\n\\n```python\\n1     def alpha_r(t):\\n2         return 5\\n```\\n\\nThis way, no matter how complicated our functions for $\\\\lambda_{r}$, are, we know that the total arrival rate, across all conditioned distributions for the resources, we remain fixed at 5.\\n\\nInference. One additional component to AllSim, is its Inference module. In our simulations, we relied on OrganITE [5], where we adopted the original source-code provided by the authors, to our Inference module. Having a causal model, allows to have unbiased estimates for the resource to recipient pairing\u2019s outcome (column \u201cY\u201d in Table 2).\\n\\nBuilding a custom Inference object, is as simple as inheriting from Inference, which requires the user to implement the `infer(x: np.ndarray, r: np.ndarray) -> Any` function.\\n\\nA.2 Analysing AllSim\u2019s output.\\n\\nRunning `df = simulation.simulate(policy, T=T)` yields a DataFrame object, containing each match made by the policy, in the environment simulated by the `ss.Simulation` object. Consider Table 2 for an excerpt of the output for which we provided some aggregate results in Figure 5. Table 2 is exactly the type of data one may expect when learning about a policy, or even learning a new (ML-driven) policy [5].\\n\\nGiven the output of our simulation, we can see each variable, both donor as well as recipient progress over time. For example, one may be interested how each gender evolves w.r.t. allocations, with the changing supply of resources (as in Figure 5). Naturally, this is but one example of what one can achieve in terms of analysis on MELD (or any other policy), using AllSim. As such, we encourage the reader to browse through the provided code, in order to get a sense of what the possibilities really are when using AllSim.\\n\\nB Details on AllSim\\n\\nWhile all source code is provided in our submission, we provide some additional detail on AllSim\u2019s functionality, structure, and future goals.\"}"}
{"id": "vSsnEd0Jmou", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Example output. When donor covariates are NaN, we know a patient died on the wait list, as they did not receive a donor organ. Note that, we only display some covariates, we have included code in our submission, should the interested reader want to inspect the complete DataFrame.\\n\\n| Donor covariates | Recipient covariates |\\n|------------------|----------------------|\\n| Age              | BMI                  |\\n| INR              | Sodium               |\\n| Creatinine       | Gender               |\\n| Y                | Day                  |\\n\\n| t = 300          | t = 600              |\\n|------------------|----------------------|\\n| 0.45             | 0.5                 |\\n| 0.55             | 0.5                 |\\n\\nFigure 7: Recipient-gender in function of time. Given the MELD policy, and the changing supply of resources (cfr. Figure 5, we find that the recipient-gender changes slightly. Note that, this type of analysis is easily done given AllSim, as we can easily run analysis on the provided DataFrame object, returned by our simulation.\\n\\nB.1 NEAT FUNCTIONALITY\\n\\nOne-hot encoded variables. In heterogeneous data, one may expect a mixture of continuous and categorical variables. Good practice to handle categorical variables, is to one-hot encode them into columns of ones and zeroes, for each category. Most density estimation methods, do not automatically sample these mixtures of continuous and categorical data. Despite its performance and simplicity, a standard Kernel Density estimator, like the one we use in our simulation, cannot handle these data out-of-the-box. However, our implementation does. One only needs to provide the specific groups of columns which compose the categorical variables. For example, the gender column for the recipient may be one-hot encoded into: \u201cGENDER 0\u201d and \u201cGENDER 1\u201d. When fitting our KDEDensity, we simply provide these columns to the one-hot encoded=\\\\[\\\\{\"GENDER 0\", \"GENDER 1\"{\\\\} parameter, and the KDEDensity automatically transforms these columns, in such a way that only one of them (being the one with maximum probability), is translated into 1.\\n\\nAuto-scaling. Dealing with a normalised arrival rate can be tricky to determine up front. As such, AllSim does this automatically. By solely providing an alpha: Callable[[int], float], AllSim\u2019s PoissonSystem objects will normalise any output that the System\u2019s PoissonProcesses may have. Naturally, normalisation will depend on the chosen process, and as such does not propagate up to the general System or Process. If, for example, on wishes to implement an alternative point process (not a Poisson process), normalisation may look differently.\\n\\nAutomatic conditioning. Another nice functionality provided in AllSim is automatic conditioning, in case no Condition is provided. A Density does this, by first clustering the data (from which it will later learn its density), into K clusters. Naturally, one will have to provide the amount of clusters before the Density is able to automatically condition. Having these separate clusters, still allows defining arrival rates and processes for each cluster center, i.e. K must match the amount of Processes are provided to the System.\\n\\nB.2 PACKAGE STRUCTURE\\n\\nThe AllSim package structure is as follows:\"}"}
{"id": "vSsnEd0Jmou", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The two main components are in `infer.py` and `sim.py`, where `infer.py` contains everything related to `Density`, `Process`, and their subclasses; and `sim.py` concerns the `Simulation` class. In `outcome` and `policy`, we provide basic implementations of some well known policies and counterfactual models. Note that these are actually non-essential to AllSim, as they can be completely user-defined, as long as they respect the `Inference` and `Policy` abstract classes, provided in the respective folders.\\n\\nInheritance. The base classes (`Density`, `Policy`, and `Inference`) can all be implemented by the user. Consider following class structures:\\n\\n```python\\n1 class Density:\\n2     def __init__(self, condition: Condition, # see below for more details\\n3                    K: int=1,\\n4                    drop: np.ndarray=np.ndarray(['condition'])):\\n5         ...\\n6\\n7     @abstractmethod\\n8     def sample(self, n: int=1) -> np.ndarray:\\n9         ...\\n10\\n11     @abstractmethod\\n12     def fit(self, D: pd.DataFrame) -> None:\\n13         ...\\n14\\n15 class Policy:\\n16     def __init__(self, name: str, initial_waitlist: np.ndarray, dm: OrganDataModule): # a standardised datamodule should the policy want to learn from data\\n17         ...\\n18\\n19     @abstractmethod:\\n20     def select(self, resources: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\\n21         # returns recipients and resources, matched\\n22         ...\\n23\\n24     @abstractmethod\\n25     def add(self, x: np.ndarray) -> None:\\n26         # allows policy to add recipients to internal waitlist (if needed)\\n27         ...\\n28\\n29     @abstractmethod\\n30     def remove(self, x: np.ndarray) -> None:\\n31         # allows policy to remove recipients, when for example they die\\n32         ...\\n33\\n34 class Inference:\\n35     def __init__(self, ...\\n```\"}"}
{"id": "vSsnEd0Jmou", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Condition class then, wraps a function from a set of labels in the dataset, to a numerical value. One may choose to combine multiple variables, or just one, as we have with the \\\"AGE\\\" variable in Figures 4 and 5. Please find an example initialisation in Figure 4. While one can inherit from the Condition class, we would advice to implement a custom function (as the lambda function we had), and provide it to the Condition object.\\n\\nFuture goals & open-source\\n\\nOur goal is for AllSim to be a benchmarking standard when evaluating ScRAPs. An important milestone for this, is to completely open-source our simulation. Doing so, allows other researchers to scrutinise, enhance, and discuss how we should move beyond what we can do today. Below, we provide two points we believe our community should discuss.\\n\\nHere we provide the basic code, that will generate Figure 5. Naturally, the complete code is provided in the supplemental materials.\\n\\n```python\\n# load data\\nX, R, Y = custom_load_function(data)\\norganite = load_organite_model(location) # should load an implemented Inference object\\n\\n# DENSITY LEARNING\\n# RESOURCES\\nbins_r = [30, 45, 60]\\ndef condition_function_r(age):\\n    return np.digitize(age, bins=bins_r).item()\\n\\ncondition_r = infer.Condition(labels=['AGE'],\\n                                function=condition_function_r,\\n                                options=len(bins_r) + 1)\\n\\nkde_r = infer.KDEDensity(condition=condition_r, K=condition_r.options)\\nkde_r.fit(R, one_hot_encoded=groups_r)\\n\\n# RECIPIENTS\\nbins_x = [30, 45, 60]\\ndef condition_function_x(bilir):\\n    return np.digitize(bilir, bins=bins_x).item()\\n\\ncondition_x = infer.Condition(labels=['AGE'],\\n                               function=condition_function_x,\\n                               options=len(bins_x) + 1)\\n```\\n\"}"}
{"id": "vSsnEd0Jmou", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"function=condition_function_x,\\noptions=len(bins_x) + 1\\n)\\nkde_x = infer.KDEDensity(condition=condition_x, K=condition_x.options)\\nkde_x.fit(X, one_hot_encoded=groups_x)\\n\\n# BUILD THE SYSTEMS\\nresource_system, patient_system = dict(), dict()\\ndef update_lam_0(t):\\n    return (1 / (1+np.exp(-(t-450)/150))) * 3\\ndef update_lam_1(t):\\n    return (1 / (1+np.exp((t-350)/150))) * 2\\ndef update_lam_2(t):\\n    a = (1 / (1+np.exp((t-150)/150))) * 2\\n    b = (1 / (1+np.exp(-(t-650)/100))) * 2\\n    return (a + b)\\ndef update_lam_3(t):\\n    a = (1 / (1+np.exp(-(t-150)/150)))\\n    b = (1 / (1+np.exp((t-650)/100)))\\n    return (a + b)\\n\\nresource_system[0] = infer.PoissonProcess(update_lam=update_lam_0)\\nresource_system[1] = infer.PoissonProcess(update_lam=update_lam_1)\\nresource_system[2] = infer.PoissonProcess(update_lam=update_lam_2)\\nresource_system[3] = infer.PoissonProcess(update_lam=update_lam_3)\\n\\npatient_system[0] = infer.PoissonProcess(update_lam=update_lam_0)\\npatient_system[1] = infer.PoissonProcess(update_lam=update_lam_1)\\npatient_system[2] = infer.PoissonProcess(update_lam=update_lam_2)\\npatient_system[3] = infer.PoissonProcess(update_lam=update_lam_3)\\n\\nresource_process = infer.PoissonSystem(density=kde_r, system=resource_system, \\n    alpha=lambda t: 5, normalize=True)\\npatient_process = infer.PoissonSystem(density=kde_x, system=patient_system, \\n    alpha=lambda t: 7, normalize=True)\\n\\norganite.model.eval()\\nsimulation = sim.Sim(resource_system=resource_process, patient_system=patient_process, \\n    inference=organite)\\npolicy = MELD(name='MELD', initial_waitlist=simulation._internal_waitlist, dm=dm)\"}"}
{"id": "vSsnEd0Jmou", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nC.1 GETTING STARTED\\n\\nTo use AllSim, a user requires at least a dataset of the following type:\\n\\n\\\\[ D := \\\\{ (X_t, R_u, Y) : t, u \\\\in \\\\mathbb{N}^+ \\\\} \\\\]\\n\\nwith \\\\( t, u \\\\) indicating the time of arrival. In principle, this is sufficient to start using AllSim already. In fact, this is exactly what we provided AllSim in our experiment in Figure 4. Let us elaborate:\\n\\nAllSim requires to specify the following components:\\n\\n1. A counterfactual model\\n2. The patient and resource arrival processes\\n3. The patient and resource densities\\n\\nComponents 1 and 3 are easily implemented using existing packages like econml (https://econml.azurewebsites.net) and scikit-learn (https://scikit-learn.org/stable/), respectively. They only require to use the model.fit API to be applied to the user\u2019s data:\\n\\n```python\\ncounterf = econml.BaseCateEstimator()\\ncounterf.fit(Y, R, X)\\n```\\n\\n```python\\ndensity_X, density_R = allsim.infer.KDEDensity(), allsim.infer.KDEDensity()\\ndensity_X.fit(X)\\ndensity_R.fit(R)\\n```\\n\\nThe only place where we need AllSim specifically is when we create the arrival processes, and the eventual simulation. Creating an arrival process on data, first requires us to regress the arrival probability on time:\\n\\n```python\\nfrom sklearn.preprocessing import PolynomialFeatures\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.pipeline import Pipeline\\n\\nfunc_X = function(Pipeline([('poly', PolynomialFeatures(degree=6)), ('linear', LinearRegression(fit_intercept=True))]))\\n\\n# with amount() a function that returns how many of X arrived at t in D\\nfunc_X.fit(t, amount(X, t))\\n\\n# with the arrival processes\\npatient_process = allsim.infer.PoissonProcess(func_X)\\nresource_process = allsim.infer.PoissonProcess(func_R)\\n\\npatient_system = allsim.infer.PoissonSystem(density_X, patient_process)\\nresource_system = allsim.infer.PoissonSystem(density_R, resource_process)\\n```\\n\\nThe simulation is then created as:\\n\\n```python\\nsimulation = asim.sim.Sim(resource_system=resource_system, patient_system=patient_system, inference=counterf)\\n```\"}"}
{"id": "vSsnEd0Jmou", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nThe only thing left is to actually run the policy against our created simulation:\\n\\n```python\\ndf = simulation.simulate(policy, T=100)  # that's it!\\n```\\n\\n**Social Impact**\\n\\nIt is very clear that machine learning has the potential to transform healthcare. Its success both in other domains and already within healthcare is very promising. ML-based policies have the potential to extend lives. However, as with almost any other machine learning method, there are risks associated with their deployment in a real healthcare setting. Before any (experimental) ML-based policy (or even non-ML-based policies for that matter) are to be deployed, they require thorough testing.\\n\\nWe believe AllSim will enable practitioners to leverage their data and evaluate their policies in a rigorous manner.\\n\\nAllSim fully recognises the difficulties associated with evaluating policies that diverge from the data-generating policy and aims to mitigate these difficulties by employing tried and tested methods from causality. Naturally, there are some caveats: AllSim inherits the potential assumptions made by the counterfactual method, furthermore, if real data is used, it is crucial it is at least somewhat related to the environment the tested policy will end up operating in. If for example, one aims to evaluate a policy on a domain that is completely unrelated, AllSim\u2019s learned simulation will provide the tested policy with unrealistic resources, recipients, and arrivals.\\n\\n**AllSim in a Vaccine Distribution Scenario**\\n\\nAllSim is a general purpose simulator which evaluates scarce resource allocation policies. While we mainly focus on organ-transplantation in our main text, we show in this section that AllSim is also applicable in other settings. To illustrate, we show how one can implement a vaccine distribution policy evaluation system in AllSim. This use-case will show how few adjustments one has to make with respect to the presented settings in our main text.\\n\\nCompared to the organ-allocation problem, in vaccine distribution, each resource is the same and they arrive in batches. Furthermore, the type of patient-in-need is also much broader (in fact, they cover the entire population). Yet, AllSim is perfectly capable of modelling this scenario given the following:\\n\\n- Batch arrival simply requires a multiplier. For example, if the Poisson process samples a value of 2 on one day, we could simply interpret this as two batches of 1000 doses.\\n- As all vaccines are the same, we no longer require a density of resources as we required for organ allocation. This can be done by implementing a dummy-density that always returns 1 (or the amount of vaccine).\\n- A broader patient-type in AllSim is achieved by retraining the density of recipients over the entire population.\\n\\nThese implementation details are relatively simple to implement and easily done using AllSim\u2019s modular API.\\n\\nWhile not necessarily a problem in vaccine distribution, recipient arrival in the ICU in a setting of infectious disease (such as COVID-19), is definitely different as compared to the organ-allocation setting. With organ-allocation, we can safely assume a Poisson process for recipient arrival as recipients enter the system independently. This is of course not true in an infectious disease scenario: one recipient arriving may indicate higher infection rate. As such, recipients do not arrive independently.\\n\\nWith the above, it is clear that we can no longer rely on a Poisson arrival process for recipients entering the system. Instead, to accurately model a situation of infectious disease, we recommend using a Hawkes process. To further illustrate, we include some code below showing exactly how one may go about including such a Hawkes process in AllSim.\\n\\n```python\\nclass HawkesProcess(PoissonProcess):\\n    def __init__(self, lam=.1):\\n        super().__init__(intensity=lam)\\n```\\n\\n```python\\nprocess = HawkesProcess(lam=0.1)\\n```\"}"}
{"id": "vSsnEd0Jmou", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5\\n\\n5\\n\\n6\\n\\n6\\n\\n7\\n\\n7\\n\\n8\\n\\n8\\n\\n9\\n\\n10\\n\\n9\\n\\n10\\n\\n11\\n\\n11\\n\\n12\\n\\n12\\n\\n13\\n\\n13\\n\\n14\\n\\n14\\n\\n15\\n\\n15\\n\\n16\\n\\n16\\n\\n17\\n\\n17\\n\\n18\\n\\n18\\n\\n19\\n\\n19\\n\\n20\\n\\n20\\n\\n21\\n\\n21\\n\\n22\\n\\n22\\n\\n23\\n\\n23\\n\\n24\\n\\n24\\n\\n25\\n\\n25\\n\\n26\\n\\n26\\n\\nNaturally, if recipient arrival is indeed dependent on previous arrivals, simply learning the arrival process (as we have for figure 3) should model such a self-exciting process automatically. Furthermore, the above implementation is a simple linear univariate Hawkes process. We refer to `hawkeslib` ([https://github.com/canerturkmen/hawkeslib](https://github.com/canerturkmen/hawkeslib)) for implementations of more types of Hawkes processes.\\n\\nAllSim relies for a large part on counterfactual inference. As real-world data is collected under an active policy, testing an alternative policy would almost immediately diverge from the allocations made in the data. As such, we have to infer an outcome from a pair made by the tested policy. If the tested policy is indeed different from the active policy, then we are unlikely to find a comparable pair in the data. The above is a question most considered in research on counterfactual inference [40].\\n\\nOf course there are many models that perform counterfactual inference. As such, we provide a brief overview of the type of models one may resort to. Naturally, this list is non-exhaustive, but may guide a user to a model fit for their use-case.\\n\\nBroadly speaking, we recognise a few \u201cmeta-categories\u201d, or meta-learners [49, 50]. The are as follows:\\n\\n- **T-Learner.** Simplest is to learn a model (such as a neural net or random forest) for each treatment. In the vaccine distribution case that would mean learning a model on recipients who haven't received a vaccine, and a different model on recipients who have received a vaccine.\\n\\n- **S-Learner.** Contrasting the above, an S-Learning learns one model, where the treatment is considered part of the covariate set. This allows for a more flexible treatment, such as continuous [51] or multivariate treatments [6]. In our main-text we use OrganITE as a counterfactual model [5], which can be considered an S-Learner.\\n\\n- **X-Learner.** Used specifically for estimating the treatment effect directly, an X-Learner first learns the outcome functions (such as the T-Learner), then imputes the dataset with the completed treatment effect (or the estimated counterfactual outcome), and then learns the treatment effect with a third model directly on the completed samples. While useful for treatment effect estimating, performing counterfactual inference with an X-Learner would default back to either an S-Learner or T-Learner.\\n\\n- **R-Learner.** An R-Learner, like the X-Learner, estimates the treatment effect directly [52]. Specifically, it learns the outcome functions as well as a propensity estimate. Using these...\"}"}
{"id": "vSsnEd0Jmou", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Consider the sequential decision problem of allocating a limited supply of resources to a pool of potential recipients: this scarce resource allocation problem arises in a variety of settings characterized by \u201chard-to-make\u201d tradeoffs\u2014such as assigning organs to transplant patients, or rationing ventilators in overstretched ICUs. Assisting human judgement in these choices are dynamic allocation policies that prescribe how to match available assets to an evolving pool of beneficiaries\u2014such as clinical guidelines that stipulate selection criteria on the basis of recipient and organ attributes. However, while such policies have received increasing attention in recent years, a key challenge lies in pre-deployment evaluation: How might allocation policies behave in the real world? In particular, in addition to conventional backtesting, it is crucial that policies be evaluated on a variety of possible scenarios and sensitivities\u2014such as distributions of recipients and organs that may diverge from historic patterns. In this work, we present AllSim, an open-source framework for performing data-driven simulation of scarce resource allocation policies for pre-deployment evaluation. Simulation environments are modular (i.e. parameterized componentwise), learnable (i.e. on historical data), and customizable (i.e. to unseen conditions), and \u2014upon interaction with a policy\u2014outputs a dataset of simulated outcomes for analysis and benchmarking. Compared to existing work, we believe this approach takes a step towards more methodical evaluation of scarce resource allocation policies.\\n\\n1 INTRODUCTION\\n\\nThe distribution of organs for transplant is a prototypical example of the scarce resource allocation problem - one with salient \u201clife-or-death\u201d consequences that places significant pressure on decision-makers to make implicit but difficult trade-offs. To make the task more manageable, assisting human judgement in these choices are dynamic allocation policies that prescribe how to match each available unit of resource to a potential beneficiary. For instance, the United Network for Organ Sharing (UNOS) stipulates policies for organ allocation according to weighted organ- and patient-specific criteria, such as time on the waiting list, severity of illnesses, human leukocyte antigen matching, prognostic information, and other considerations [1\u20133]. Likewise, in the machine learning community, a variety of data-driven algorithms have been proposed as drop-in dynamic allocation policies, leveraging modern techniques for estimating treatment effects, predicting survival times, and accounting for organ scarcity\u2014and often succeed in demonstrating high degrees of improvement in terms of life expectancies when deployed and evaluated on a backtested basis [4\u20136].\\n\\nIs such demonstrated backtested performance sufficiently convincing for practitioners to adopt these developed allocation strategies? In many cases the answer is no, since there is still no standardised way in which this backtesting is actually undertaken [7]. The variety of methods that do exist share common challenges: First, when testing a target policy different from the actual policy used to generate the data, any offline evaluation method is immediately biased away from the true open-loop data-generating process [8]. Second, the evaluation methods themselves impute predicted outcomes, often with simple linear models [9, 10], which are not flexible enough to properly test more flexible machine learning methods. The compounding effect of these limitations leads to clinicians finding the results unconvincing [11], consequently limiting the use of these potentially very beneficial systems.\"}"}
{"id": "vSsnEd0Jmou", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nNote that the above is simply a service provided by the simulation to the policy, and is thus not a learned. The reason for this is, that the organisation of in-need-recipients is entirely up to the policy. For example, more traditional policies may maintain only one priority queue [17], whereas more novel policies may maintain multiple, based on recipient and resource types [6]. As such, AllSim remains agnostic to the tested policy, resulting in a more general-purpose framework.\\n\\n3.3 \\\\( (III) \\\\)\\n\\nAllocate resources\\n\\nLike Sect. 3.2, step (iii) is entirely managed by the tested policy, and in Figure 2 corresponds with, \\\\( \\\\pi \\\\select \\\\). Selecting which recipients that get to consume the resource, is generally what differentiates policies. When a set of recipients is selected (through a \\\\( \\\\text{policy.select}(R(t)) \\\\) call in the python interface, AllSim retains every piece of information associated with the select-call, which is used to evaluate. Having the simulation retain all this information, allows AllSim to output a dataset of past policy behaviour, much like the original (real-world) dataset we provided to the simulation in order to learn the various components. In essence, with AllSim we are able to sample a synthetic dataset of a counterfactual scenario such that we are able to test a policy of interest, as if it were already in use.\\n\\n3.4 \\\\( (IV) \\\\)\\n\\nConsume resources\\n\\n\\\\( Y \\\\) or \\\\( Y' \\\\)\\n\\n\\\\( Y \\\\) or \\\\( Y' \\\\)\\n\\n\\\\( \\\\pi \\\\)\\n\\n\\\\( D \\\\) or \\\\( D' \\\\)\\n\\n\\\\( \\\\pi' \\\\)\\n\\nFigure 3: Effect of a policy on data. Above illustration, depicts two policies: \\\\( \\\\pi \\\\) and \\\\( \\\\pi' \\\\). Each policy is tasked with assigning organs \\\\( (R) \\\\) to recipients \\\\( (\\\\pi select) \\\\). Besides recipients and organs, we observe an outcome, \\\\( Y \\\\). Despite observing the same recipients and organs, a different policy results in completely different outcomes.\\n\\nThe final component in AllSim is the inference-component, yielding an outcome after a resource was consumed by a recipient. Importantly, this component allows us to evaluate a policy, despite it deviating from the data used to learn the simulation. In Figure 2, this component is illustrated as, \\\\( Y(t) \\\\pi \\\\).\\n\\nThe outcome is a function of the resource and its recipient, making inference hard as some combinations are less observed in the original data. Essentially, the tested policy may make out-of-distribution combinations, as the simulation is learnt from data that was collected under some other policy, illustrated in Figure 3 where two policies, \\\\( \\\\pi \\\\) and \\\\( \\\\pi' \\\\) result in different datasets \\\\( D \\\\) and \\\\( D' \\\\).\\n\\nCounterfactual inference. AllSim handles this by using a counterfactual estimator to infer allocation outcomes as they deal with this issue explicitly. Counterfactual methods aim to make an unbiased prediction of the potential outcome, associated with some treatment (or resource). We are interested in counterfactual methods that model the potential outcomes for the recipients when they are/are not allocated a resource. A counterfactual estimator then \u201ccompletes\u201d the simulated dataset as, \\\\( Y(t) = \\\\mathbb{E} \\\\left[ \\\\hat{Y}(R(t)) | X(\\\\pi(t)) \\\\right] \\\\), (5)\\n\\nwhere \\\\( \\\\hat{Y}(R(t)) \\\\) is the estimated potential outcome, using methodology known in the potential outcomes literature, and \\\\( X(\\\\pi(t)) \\\\) are the recipients selected by \\\\( \\\\pi \\\\) at time \\\\( t \\\\). Equation (5) provides the recipient-resource pair with an estimated outcome, and presents the practitioner with a dataset: \\\\( D_{\\\\pi} := \\\\{ (X, R, \\\\hat{Y}(R), t)_{i=1}, \\\\ldots, N \\\\} \\\\).\\n\\n\\\\( D_{\\\\pi} \\\\) allows to easily calculate clinical measures of performance, as we demonstrate in Sect. 5. More information regarding these counterfactual models is provided in Appendix F.\\n\\n4 Related work\\n\\nWe have summarised work related to ours in four major categories: (a) Off-policy learning and evaluation, (b) causal inference, (c) clinical simulations and trials, and (d) simulations for evaluating reinforcement learning (RL) agents. A high-level summary of these areas can be found in Table 1.\\n\\nFor a discussion on related work in the clinical domain, we refer to our Appendix F.\"}"}
{"id": "vSsnEd0Jmou", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We categorise our related work in four categories: off-policy learning, causal inference, clinical simulations, and simulations for evaluating reinforcement learning agents. For each category, we provide the most prominent method of calculating policy performance, and whether the categories take into account four major questions: (i) is the method tunable to new settings?; (ii) can we evaluate using data?; (iii) are the performance estimates unbiased?; and (iv) can we evaluate beyond simple aggregate descriptive statistics?\\n\\n| Category                  | Methodology                                                                 | (i) | (ii) | (iii) | (iv) |\\n|---------------------------|-----------------------------------------------------------------------------|-----|------|-------|------|\\n| Off-policy learning       | $p(\u03c0|X)$                                                                     | \u2713   | \u2717   | \u2713     | \u2717    |\\n| Causal inference          | $p(R|X)$ \u2212 $1$                                                              | \u2717   | \u2713   | \u2713     | \u2717    |\\n| Clinical simulations      | $E[D_\u03c0[Y|X]]$                                                                | \u2717   | \u2713   | \u2717     | \u2713    |\\n| Reinforcement learning    | $E[sim][v(Y)|X,R]$                                                         | \u2713   | \u2717   | \u2713     | \u2713    |\\n| AllSim (ours)             | $E[D_\u03c0\u2032\u223cX\u2032\u00d7R\u2032[Y|X]]$                                                        | \u2713   | \u2713   | \u2713     | \u2713    |\\n\\n(a) Off-policy evaluation and (b) causal inference. Data are collected under some active scarce resource allocation policy (ScRAP), which determines how resources are paired with recipients, which in turn determine the outcomes we get to observe. Clearly, our setting is connected to off-policy evaluation, as we wish to evaluate a policy (ScRAP) that is different from the policy that collected the available data. Figure 3 illustrates different matches (depicted as *) made across different policies resulting in different outcomes: $Y$ or $Y\u2032$, and datasets, $D$ and $D\u2032$. The difficulty of this situation is that we observe either $D$ or $D\u2032$, not both, relating to the potential outcomes setup [12, 39, 40]. Using $D$ to evaluate $\u03c0$ simply means calculating some descriptive statistics. In contrast, evaluating $\u03c0\u2032$ on $D$ is much more involved, as the same statistics would yield a biased estimate. One way to calculate, for example the average $Y\u2032 \u223c D$, is to weigh each sample: $g(Y \u223c D)$. In the literature on off-policy evaluation (and learning), this is known as importance sampling, where each sample is weighted according to the likelihood of it belonging in $D\u2032$. Interestingly, the exact same strategy is also widely known in counterfactual learning and causal inference, under the name inverse propensity weighting (IPW) [20]. The key difference between both, is the target distribution. Where importance sampling transforms the estimate from one policy\u2019s distribution, to another; IPW transforms from a policy\u2019s distribution, to an unbiased estimate (i.e. to a $1/N$ weighting for averages). Shown in Table 1, neither IPW nor importance sampling, provide a solution to evaluating scarce resource allocation policies. One reason for this is that the outcome is a (weighted) estimate of only one aggregate descriptive statistic (for example, expected outcome or reward). When we want additional estimates, one requires a different weighting scheme [23]. Furthermore, when we wish to test a policy in a different environment, where for example the recipients' or resources' distributions change, evaluating a policy becomes increasingly more difficult if one wishes to rely on IPW or importance sampling due to the increased differences in likelihood density. Furthermore, evaluation goes much beyond aggregate statistics, as we may be interested in, for example, demographic differences between recipients and non-recipients (Appendix A includes an example using AllSim).\\n\\n(c) Clinical evaluation and trials. A naive method to evaluating these policies, is to model $Y\u2032$ using simple (linear [17]) regression. Any combination $\u03c0\u2032$, through a new policy, $\u03c0\u2032$ has an estimated outcome $\\\\hat{Y\u2032}$ using a regression model, trained on $D$. Herein lies the problem: the regression model is still biased to $\u03c0$. In particular, $D$ simply does not contain pairs that $\u03c0\u2032$ would make, and therefore has trouble estimating $Y\u2032$. By using causal methodology (Sect. 3.4), this is one key area where AllSim improves upon contemporary clinical simulations [9, 17, 41, 42], as these only \u201creplay\u201d the past without the possibility of changing the environment characteristics nor allow counterfactual inference. Clinicians do have another way to account for this: clinical trials [28, 29]. Clinical trials estimate a causal estimand, which could then be used to \u201ccomplete\u201d the dataset as a naive regression model would above. However, two problems arise: setting up a clinical trial can sometimes be considered unethical (especially when dealing with scarce resources, and more specific research questions) [43], or clinical trials may suffer from compliance issues which will still result in biased outcomes [44, 45].\\n\\n(d) Simulations for evaluating RL agents. A final set of solutions is that of simulations in which an RL agent can act and learn. Naturally, there are differences between policies for RL and scarce resource allocation, such as the importance of future reward which implies correlated consecutive states, and a fixed action space. However, there are also similarities, such as the definition of a policy (being a set of rules which take into account a context or state), and the online nature of the problem. Literature on RL is blessed with some of the most well-known and actively maintained libraries for evaluating a wide range of RL algorithms [32, 33, 38]. However, other than being inapplicable in our...\"}"}
{"id": "vSsnEd0Jmou", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: AllSim can generate realistic synthetic data. Using data on donor organs, we let AllSim model 3 years of organ arrivals and compare it with the actual arrival as reported in the data. In Figures 4a and 4b we show AllSim\u2019s output (in donor age and BMI), given the code on the right. With minimal code, a simple condition (4 age brackets), and conservative models (polynomial regression to fit the arrival rates, and a Gaussian kernel density to model the organ densities), we find AllSim to accurately model the actual arrival of organs.\\n\\n5.1 ARRIVAL PROCESSES AND RESOURCE DENSITIES\\n\\nAs is indicated in Figure 2 and Sect. 2.2, sampling new resources and recipients is a two-step process. On a daily basis (or some other discrete time interval in \\\\([t]\\\\)) we first determine how many resources or recipients we need to sample, and then we sample them. The former is modeled through a Poisson arrival rate that changes over time, and the second is sampled from some learnt density. Of course, a user can implement their own arrival process by inheriting from the abstract ArrivalProcess class. Importantly, we need to be able to condition both on some pre-specified characteristic of the object of interest. For example, one may be interested in modeling the arrival of harvested organs of older patients distinctly from younger patients. An example of this is provided in Figure 4, where we show the changing resources coming in the system, alongside the code that generated the result.\\n\\nNote that AllSim is the first method of this kind for benchmarking scarce resource allocation policies.\"}"}
{"id": "vSsnEd0Jmou", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Donor arrival rates\\n\\nDonor arrival rates can be used to compare policies in a user-specified scenario. Above we compare three different organ allocation policies: MELD, MELD-na, and FIFO. The MELD policies are clinically validated and used in practice, while FIFO is a naive first-in first-out allocation policy (which should not work at all!). When comparing these policies, our simulation suggests that the clinically validated policies do outperform a naive FIFO policy in terms of average survival (left), as is expected. Furthermore, the performance of each policy does seem to drop slightly over time. This too is expected as we tested the policies in a setup with increased recipient age over time (right). The reported averages are windowed over 300 samples.\\n\\nObject densities.\\n\\nBefore discussing a temporal arrival rate, we first discuss modeling the object's densities. Consider the right-hand side of Figure 4. Using this code, we first define what we want to condition on, using a `Condition` object: in this case we formulate age brackets. With the `KDEDensity` class, which is a subclass of the abstract `Density` class, we can automatically model a density, conditioned on these age brackets. Each `Density` object implements a `fit` and `sample` function, which is used to sample new objects by the `System`, which we discuss next.\\n\\nArrival processes.\\n\\nUsing a `Density`, we move on to lines 14-26, where we first build a system of multiple arrival processes, one for each discrete condition as in Equations (1) and (2). In particular, we define a `PoissonProcess` for each condition (or age bracket), which is then provided to a `PoissonSystem`. Using the `PoissonSystem`, we can sample the arriving objects for each \\\\( t \\\\). Note that we also model \\\\( \\\\alpha \\\\), returning the overall arrival rate, such that the system can calculate an appropriate \\\\( \\\\nu \\\\). Figure 4a shows that `AllSim` accurately models the arriving objects.\\n\\n5.2 Counterfactual outcomes and simulation\\n\\nWith the arrival processes coded above together with a counterfactual `Inference` object compose a `Simulation` object\u2014 the main interaction interface. In particular, one defines a set of arrival rates (such as in Figure 4b) for both recipients and resources (cfr. lines 13-17) to create a simulation:\\n\\n```python\\n1 simulation = asim.Sim(resource_process, patient_process, inference)\\n```\\n\\nWith that simulation, a practitioner can instantiate a `Policy`, which implements the `add` and `select` methods. For example, we have implemented the MELD policy [46], which is a widely known and used ScRAP for liver allocation. Using the `simulation`, we can generate a simulated dataset:\\n\\n```python\\n1 df = simulation.simulate(meld_policy, T=T)\\n```\\n\\nWhere `df` is a Pandas DataFrame [47, 48]. Naturally, `df` contains an enormous amount of information w.r.t. the policy's allocations in our environment. As such, we have included only a subset of the potential results in Figure 5. Additional results and details can be found in Appendices A and F. Ultimately, the practitioner determines appropriate analysis, settings, and performance metrics.\\n\\n6 Conclusion\\n\\n`AllSim` allows standardised evaluation of resource allocation policies. While our main text heavily focuses on organ-transplantation for the sake of exhibition, Appendix E illustrates `AllSim` for COVID-19, an example outside organ-transplantation. We believe strongly that `AllSim`'s generality and modularity allows for sensible adoption in a wide range of application areas. Furthermore, having standardised evaluation will encourage research in this very important and impactful domain. Naturally, conducting research in clinical resource allocation requires consideration of a policy's societal impact. While we believe `AllSim` will aid in this respect also (by offering more than simple aggregate statistics), in Appendix D we provide a section on this topic specifically.\"}"}
{"id": "vSsnEd0Jmou", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Historical data is also not the only aspect for which candidate policies should be compared against. In fact, it is crucial that policies be tested on a variety of scenarios and sensitivities for more robust evaluation\u2014based on plausible possible futures\u2014not just conditions that have been seen before. Real-world conditions may change after all, meaning historical data may no longer be representative of the current environment. To this end, we desire a system that allows us to test policies in such counterfactual scenarios for which we need counterfactual machine learning models [12] (cfr. Sect. 3.4 and our Appendix F)\u2014thereby more systematically evaluating and benchmarking candidate policies before they are put into practice, as well as continuously updating existing policies based on anticipated or unanticipated changes to the environment. We propose to evaluate policies using machine learning as we illustrate in Figure 1.\\n\\nAs a motivating example, consider the impact of COVID-19 on the availability of organs for transplant\u2014leading in many cases to a sudden and severe drop in supply [13]. Is it possible to reasonably measure performance of allocation policies against such a theoretical event a priori? Not with current data-based methods that do not contain an example of such an event. Conversely, with our simulated model we can intervene on the distribution of organs\u2014reducing policy roll-out and testing to a simple task. With this in mind, we wish to highlight that accurate evaluation of such allocation policies is equally, if not more, important than their development. After all, there is little point in developing policies if they cannot be shown to be beneficial and then never used. Worse, it may be that the current evaluation techniques are causing researchers to optimise for performance that may end up being actively detrimental in real-world deployment.\\n\\nAnd so, we now continue by considering what exactly an ideal evaluation method may consist of:\\n\\n**Desiderata**\\n\\nWe argue that a good solution should satisfy three key criteria: The environment must be (1) modular, in the sense that it is composed of parameterized components to allow flexible user interaction; (2) learnable, in the sense that it is grounded in the characteristics of real-world data; and (3) customizable, in the sense that it can test policies in previously unseen environment conditions. In particular, performance estimates should be unbiased when target policies different from the data-generating policies are evaluated. Note that our objective is not limited to estimating simple aggregate descriptive statistics (such as mean performance): We may also be interested in evaluating more general impacts of policy deployment\u2014such as whether it inadvertently discriminates based on gender or ethnicity, or whether patients suffering from a particular disease are inadvertently disadvantaged.\\n\\n**Contributions**\\n\\nIn this work, we present AllSim (Allocation Simulator), a general-purpose open-source framework for performing data-driven simulation of scarce resource allocation policies for pre-deployment evaluation. We use modular environment mechanisms to capture a range of environment conditions (e.g. varying arrival rates, sudden shocks, etc.), and provide for componentwise parameters to be learned from historical data, as well as allowing users to further configure parameters for stress testing and sensitivity analysis. Potential outcomes are evaluated using unbiased causal effects methods: Upon interaction with a policy, AllSim outputs a batch dataset detailing all of the simulated outcomes, allowing users to draw their own conclusions over the effectiveness of a policy. Compared to existing work, we believe this simulation framework takes a step towards more methodical evaluation of scarce resource allocation policies.\"}"}
{"id": "vSsnEd0Jmou", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Let $X \\\\in \\\\mathbb{R}^d$ denote the feature vector of a recipient, and let $X(t) := \\\\{X_i\\\\}_{i=0}^{N(t)}$ give the arrival set of (time-varying) size $N(t)$.\\n\\nLikewise, let $R \\\\in \\\\mathbb{R}^e$ be the feature vector of a resource, and let $R(t) := \\\\{R_j\\\\}_{j=0}^{M(t)}$ give the arrival set of (time-varying) size $M(t)$.\\n\\nWhile we make no assumptions how recipients are modeled, we assume that resources are immediately perishable\u2014that is, each incoming resource cannot be kept idle, and must be consumed by some recipient in the same time step. In organ transplantation, for instance, the time between harvesting an organ and transplanting it (\u201ccold ischemia time\u201d) must be minimized to prevent degradation [14\u201316].\\n\\nLet $Y^+ \\\\in \\\\mathbb{R}$ be the outcome of a matched recipient, drawn from the distribution $Y(X, R)$ induced by assigning a resource $R$ to a $X$.\\n\\nAt each time $t$, let $Y^+(t) := \\\\{Y^+ \\\\sim Y(X, R), R \\\\in R(t)\\\\}$ give the set of outcomes that result from matching each incoming $R \\\\in R(t)$ with its assigned $X$. Likewise, let $Y^- \\\\in \\\\mathbb{R}$ be the outcome of an un-matched recipient, drawn from the distribution $Y(X, \\\\emptyset)$.\\n\\nAt each time $t$, let the set of outcomes for recipients who are never assigned a resource be given by $Y^-(t) := \\\\{Y^- \\\\sim Y(X, \\\\emptyset), X \\\\in X(t), \\\\neg(\\\\exists t' \\\\geq t)(R \\\\in R(t'), X = X(R))\\\\}$. (Note that we focus on discrete-time settings (e.g. hours or days), and leave continuous time for future work).\\n\\nDefinition 1 (Scarce Resource Allocation)\\n\\nDenote an environment with the tuple $E := (X, R, Y)$.\\n\\nThe scarce resource allocation problem is to decide which recipients to assign each incoming resource to\u2014that is, to come up with a dynamic allocation policy $\\\\pi: R^e \\\\times \\\\mathbb{P}(R^d) \\\\to R^d$, perhaps to optimize some objective defined on the basis of matched/un-matched outcomes. For instance, if $Y$ is a patient's post-transplantation survival time, then an objective might be to maximize the average survival time.\\n\\nDefinition 2 (Pre-Deployment Evaluation)\\n\\nLet $f: Q_k R_k \\\\times \\\\ldots \\\\to M$ denote an evaluation metric mapping a sequence of outcomes $\\\\{Y(t)\\\\}_{t=1}^\\\\ldots$ to some space of evaluation outcome $M$ (e.g. for the average survival time, this would simply be $R$), where $Y(t) := Y^+(t) \\\\cup Y^-(t)$.\\n\\nGiven a problem $E$ and policy $\\\\pi$, the pre-deployment evaluation problem is to compute statistics of the distribution $F_{E,\\\\pi}$ of evaluation outcomes $f(\\\\{Y(t)\\\\}_{t=1}^\\\\ldots)$; commonly, this would be the mean $E_{E,\\\\pi}[f(\\\\{Y(t)\\\\}_{t=1}^\\\\ldots)]$.\\n\\nNote that we have defined $f$ in terms of the sequence of per-period outcomes such that it gives maximum flexibility: Depending on how individual outcomes $Y$ are defined, we can measure point estimates (e.g. the mean survival), compare subpopulations (e.g. whether some types of recipients systematically receive more favorable outcomes), examine trends (e.g. whether outcomes degrade as the types of recipients arriving change), or potentially investigate more complex hypotheses.\\n\\n2.2 Simulation Components\\n\\nEnvironment Components\\n\\nClinically validating a dynamic allocation policy not only requires estimating how that policy performs under a hypothesized scenario $E$, but also how sensitive it is to changing conditions\u2014something that is not possible using existing methods. We believe that the solution lies in components-based simulation. By this we mean a simulation that comprises the key components (i.e. recipient arrival process $X$, resource arrival process $R$, and outcome generation mechanism $Y$), each parameterized in such way so that a practitioner can easily compose the simulation to mimic a scenario of interest. For example, if a practitioner wishes to evaluate an allocation policy against a sudden decline of resources, they need only to alter the component that is responsible for resource provision in the simulated environment. With a components-based simulation, we allow a policy to simply interact with each component. Every interaction (i.e. recipient-resource pairing) is registered in a simulated dataset. Having such a simulated dataset, the practitioner can specify typical clinical evaluation metrics to properly evaluate the policy.\\n\\nPolicy Responsibilities\\n\\nA policy has two responsibilities: (i) the policy maintains the set of recipients in need of a resource (being $P(X)$), the environment only provides new recipients; and (ii) when the environment provides a new resource, the policy is tasked with allocating it to one of the recipients in need. We will denote these two responsibilities as $\\\\text{policy.add}(X(t))$ which allows the policy to maintain the arriving recipients for (i), and $\\\\text{policy.select}(R(t))$ which returns a set of recipients (one for each in $R(t)$) for (ii). Note that while a policy is not part of the environment, it may naturally act differently depending on the characteristics of the environment. For instance, it can even learn how best to behave through interaction.\\n\\n3\"}"}
{"id": "vSsnEd0Jmou", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Simulation life-cycle of AllSim. In the leftmost part of this figure, we have illustrated the simulation life-cycle. In the rightmost part, we list each component and indicate whether or not they are learned from data.\\n\\nNon-stationary Environments\\n\\nA major shortcoming in simulations used to evaluate resource allocation policies in medicine, is that they cannot evaluate policies in non-stationary settings. Hence, we model the environment with a time-indicator, \\\\( E(t) \\\\). In this subsection, we discuss how we can model a non-stationary environment, while still maintaining a large level of control for the practitioner.\\n\\nHaving sampled a particular recipient or resource, does not influence which is sampled next. However, the probability of sampling a particular recipient, or resource, does change with time. For example, the ever more increasing surgical techniques allows clinicians to attempt surgery in more challenging patients, increasing the amount of patients. Similarly for resources, an organ which would not be considered for transplantation 20 years ago, may be considered today. Conversely, the recent COVID-19 pandemic is a shock which has shown an incredible drop in supply [13].\\n\\nThese types of environment changes are modeled through \\\\( t \\\\). With \\\\( t \\\\), we can model both slightly changing drifts of supply and demand (such as those that are the consequence of improving surgical techniques), as well as sudden changes (such as those caused by drastic events). We can pair a particular recipient and a resource to a probability \\\\( p(X_i|t) \\\\) and \\\\( p(R_j|t) \\\\) given a time \\\\( t \\\\), and their respective arrival processes, \\\\( X(t) \\\\) and \\\\( R(t) \\\\). A major point in favour of AllSim, is the ability a practitioner has to model change. Specifically, a practitioner can learn a model of the arrival rate as a function of time and then provide that model to AllSim (as we have in Sect. 5), or the practitioner can provide a self-specified function of the arrival rate, and test the policy in an environment, alternative than the real environment.\\n\\n3 SIMULATION LIFE-CYCLE\\n\\nGiven the components defined in Sect. 2.2, we are now able to formalise the high-level simulation life-cycle. In simple terms, the simulation loop will model the interaction between \\\\( E \\\\) and the policy for each consecutive day (\\\\( t = 1, 2, \\\\ldots \\\\)) until some pre-specified end (\\\\( T \\\\)). We have organised this section around Algorithm 1 where we connect each important line with a component in Figure 2 and discuss it in a separate subsection. In Figure 2, where we illustrate the interaction between each component, and clarify which parts of the simulation can be learned, and which parts should be parameterised by the practitioner \u2014 solving the second (learnable) and third (costumizable) desiderata, respectively.\\n\\nHaving a components-based evaluation strategy solves the first desiderata from our introduction: the simulation is modular, i.e. each component can be reimplemented and changed at will.\\n\\n3.1 Sample recipients and resources\\n\\nThe first step in Algorithm 1 comprises the first and second component in Figure 2, repeated below,\\\\( \\\\lambda_x, \\\\lambda_r \\\\)\\n\\nEssentially, the goal in line 4 of alg. 1 is to translate the arrival rates to a set of recipients and resources, trademarked by their features sets. With above formulation, we model the arrival processes as a\"}"}
{"id": "vSsnEd0Jmou", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1: Main simulation loop.\\n\\nThis simulation life-cycle acts as a section overview for Sect. 3. In our main text we discuss how each line is simulated.\\n\\n**input:** Environment, \\\\( E \\\\)\\n\\n**output:** Policy runtime summary\\n\\n1. Start, \\\\( t = 0 \\\\);\\n2. while simulation runs do\\n3. \\\\( t \\\\leftarrow t + 1 \\\\);\\n4. \\\\( \\\\{ X(t), R(t) \\\\} \\\\sim E(t) \\\\);\\n   - /* (i) sample recip.s and resources (Sect. 3.1)*/\\n5. \\\\( \\\\text{pol.add}(X(t)) \\\\);\\n   - /* (ii) notify policy of recipients (Sect. 3.2)*/\\n6. \\\\( \\\\{ X(t), R(t) \\\\} \\\\leftarrow \\\\text{pol.select}(R(t)) \\\\);\\n   - /* (iii) allocate resources (Sect. 3.3)*/\\n7. \\\\( Y \\\\sim Y(E(X(t), R(t))) \\\\);\\n   - /* (iv) consume resources (Sect. 3.4)*/\\nend\\n\\nTwo-step approach: (1) sample the amount of objects we may expect, (2) sample the objects from a distribution. Indicated in Figure 2, only some of these component are learned, and others user-defined.\\n\\nRecipients and organs arrive randomly. The arrival processes \\\\( X(t) \\\\) and \\\\( R(t) \\\\) are approximated as,\\n\\n\\\\[\\n\\\\hat{X}(t, \\\\theta_x) = \\\\hat{X}_1(t, \\\\theta_{1,x}) \\\\times \\\\cdots \\\\times \\\\hat{X}_K(t, \\\\theta_{K,x}),\\n\\\\]\\n\\n\\\\[\\n\\\\hat{R}(t, \\\\theta_r) = \\\\hat{R}_1(t, \\\\theta_{1,r}) \\\\times \\\\cdots \\\\times \\\\hat{R}_L(t, \\\\theta_{L,r}),\\n\\\\]\\n\\nwhere \\\\( \\\\hat{X}_k \\\\) and \\\\( \\\\hat{R}_l \\\\) are modeled by a Poisson arrival process with arrival rate \\\\( \\\\lambda_k(t) \\\\) and \\\\( \\\\lambda_l(t) \\\\), respectively. In Equations (1) and (2) we factorise the arrival processes in \\\\( K \\\\) and \\\\( L \\\\) different sub-arrival processes. Each factor corresponds with some (user-defined) recipient-type (Equation (1)) and resource-type (Equation (2)). Having these factors allows us to model increasing numbers of, for example, older patients entering a transplant wait-list. Arrival rates change with \\\\( t \\\\) and are modeled as,\\n\\n\\\\[\\n\\\\lambda_k(t) = \\\\nu_k \\\\lambda_k(0) f_k(t),\\n\\\\]\\n\\n\\\\[\\n\\\\lambda_l(t) = \\\\nu_l \\\\lambda_l(0) f_l(t),\\n\\\\]\\n\\nwith \\\\( \\\\lambda_k(t), \\\\lambda_l(t) \\\\in \\\\mathbb{R}^+ \\\\), and \\\\( \\\\nu_k, \\\\nu_l \\\\in \\\\mathbb{R}^+ \\\\) as a normalising constant such that the sum of all \\\\( \\\\lambda_k \\\\) equal some overall arrival rate \\\\( a_x \\\\), and similarly, the sum of all \\\\( \\\\lambda_l \\\\) equal some overall arrival rate \\\\( a_r \\\\). Optionally, \\\\( \\\\nu_k \\\\) and \\\\( \\\\nu_l \\\\) can be kept fixed throughout the simulation such that \\\\( a_x \\\\) and \\\\( a_r \\\\) vary as does \\\\( f_k,l \\\\), or it can be recomputed for every step \\\\( t \\\\), such that \\\\( a_x \\\\) and \\\\( a_r \\\\) are kept fixed throughout the simulation. Lastly, \\\\( f_k \\\\) and \\\\( f_l \\\\) are continuous functions that simulate a user-specified drift. Note that these can also be a combination of multiple drift scenarios, or can be shared across different \\\\( k, l \\\\).\\n\\nHaving \\\\( f \\\\), allows practitioners to very accurately describe the non-stationarity they wish to test for. As such, we have a set of arrival rates, \\\\( \\\\Lambda_x = \\\\[ \\\\lambda_1, ..., \\\\lambda_K \\\\] \\\\), with \\\\( P_k \\\\lambda_k = a_x \\\\) with \\\\( a_x \\\\in \\\\mathbb{R}^+ \\\\) as the total arrival rate of recipients. The advantage of splitting \\\\( a_x \\\\) into multiple \\\\( \\\\lambda_k \\\\), is that we can finetune the arrival of certain recipient types, yet allow comparison between \\\\( a_x \\\\) and \\\\( a_r \\\\) (the total arrival rate for resources). For example, the \\\\( k \\\\)th recipient type may be completely absent when a policy is launched, but over time it gradually enters the system, increasing \\\\( a_x \\\\) as a whole. Naturally, we also model the arrival of resources as we have for recipients, but left it out of discussion for clarity.\\n\\nThe recipient and resource distributions. When a recipient or a resource arrives we sample them from a distribution denoted \\\\( p^{\\\\theta_k,x}(X) \\\\) for the recipients, and \\\\( p^{\\\\theta_l,r}(R) \\\\) for the resources. These distributions are either learnt from data, or shared as an open-source (but privatised) distribution. Let's say we have a recipient type \\\\( k \\\\) associated with a region in \\\\( X \\\\), and an arrival rate parameter \\\\( \\\\lambda_k \\\\in [0, 1) \\\\). Note that a recipient type's support may overlap with other recipient types, and that each recipient type is modeled by a distribution \\\\( p(X|k) \\\\). In principle, we wish to keep \\\\( p(X|k) \\\\) fixed, and \\\\( \\\\lambda_k \\\\) (now acting as an arrival rate conditioned on the type, \\\\( k \\\\)), tunable.\"}"}
{"id": "vSsnEd0Jmou", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ethics Statement.\\nWe envisage AllSim as a tool to help accurate and standardised evaluation of organ allocation policies, however emphasise that any finding would need to be further verified by a human expert or by a clinical trial. Ultimately, the decision on whether or not to trust a decision making tool is up to the acting clinician and hospital board. We hope that AllSim can help in any way to facilitate that decision, but stress that suggestions or evaluation always require critical assessment, as is the case for any research. We also refer the reader to Appendix D for a more thorough discussion on the potential societal impact of systems such as AllSim.\\n\\nReproducibility Statement.\\nTo ensure reproducibility, we have included all our code to reproduce the presented results. Furthermore, we have included a detailed discussion on how to use our simulation in Appendices B and C.\\n\\nREFERENCES\\n[1] Dale G Renlund, David O Taylor, Abdallah G Kfoury, and Robert S Shaddy. New unos rules: historical background and implications for transplantation management. The Journal of heart and lung transplantation, 18(11):1065\u20131070, 1999.\\n[2] Thomas M Egan and Robert M Kotloff. Pro/con debate: lung allocation should be based on medical urgency and transplant survival and not on waiting time. Chest, 128(1):407\u2013415, 2005.\\n[3] Leslie P Scheunemann and Douglas B White. The ethics and reality of rationing in medicine. Chest, 140(6):1625\u20131632, 2011.\\n[4] Jinsung Yoon, Ahmed Alaa, Martin Cadeiras, and Mihaela Van Der Schaar. Personalized donor-recipient matching for organ transplantation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.\\n[5] Jeroen Berrevoets, James Jordon, Ioana Bica, Alexander Gimson, and Mihaela van der Schaar. OrganITE: Optimal transplant donor organ offering using an individual treatment effect. In Advances in Neural Information Processing Systems, volume 33, pages 20037\u201320050. Curran Associates, Inc., 2020.\\n[6] Jeroen Berrevoets, Ahmed Alaa, Zhaozhi Qian, James Jordon, Alexander ES Gimson, and Mihaela Van Der Schaar. Learning queueing policies for organ transplantation allocation using interpretable counterfactual survival analysis. In International Conference on Machine Learning, pages 792\u2013802. PMLR, 2021.\\n[7] Srikant Devaraj, Sushil K Sharma, Dyan J Fausto, Sara Viernes, Hadi Kharrazi, et al. Barriers and facilitators to clinical decision support systems adoption: A systematic review. Journal of Business Administration Research, 3(2):36, 2014.\\n[8] Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning, pages 2139\u20132148. PMLR, 2016.\\n[9] Vikram Kilambi, Kevin Bui, and Sanjay Mehrotra. Livsim: an open-source simulation software platform for community research and development for liver allocation policies. Transplantation, 102(2), 2018.\\n[10] David Thompson, Larry Waisanen, Robert Wolfe, Robert M Merion, Keith McCullough, and Ann Rodgers. Simulating the allocation of organs for transplantation. Health care management science, 7(4):331\u2013338, 2004.\\n[11] John PA Ioannidis. Why most published research findings are false. PLoS medicine, 2(8):e124, 2005.\\n[12] Donald B Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of educational Psychology, 66(5):688, 1974.\\n[13] C Ahn, H Amer, D Anglicheau, N Ascher, C Baan, B Bat-Ireedui, Thierry Berney, MGH Betjes, S Bichu, H Birn, et al. Global transplantation covid report march 2020. Transplantation, 2020.\\n[14] Agnes Debout, Yohann Foucher, Katy Tr\u00e9bern-Launay, Christophe Legendre, Henri Kreis, Georges Mourad, Val\u00e9rie Garrigue, Emmanuel Morelon, Fanny Buron, Lionel Rostaing, et al. Each additional hour of cold ischemia time significantly increases the risk of graft failure and mortality following renal transplantation. Kidney international, 87(2):343\u2013349, 2015.\"}"}
{"id": "vSsnEd0Jmou", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[10] J Adam van der Vliet and Michiel C Warl\u00b4e. The need to reduce cold ischemia time in kidney transplantation. Current opinion in organ transplantation, 18(2):174\u2013178, 2013.\\n\\n[16] James E Stahl, Jennifer E Kreke, Fawaz Ali Abdul Malek, Andrew J Schaefer, and Joseph Vacanti. Consequences of cold-ischemia time on primary nonfunction and patient and graft survival in liver transplantation: a meta-analysis. PloS one, 3(6):e2468, 2008.\\n\\n[17] James Neuberger, Alex Gimson, Mervyn Davies, Murat Akyol, John O\u2019Grady, Andrew Burroughs, Mark Hudson, UK Blood, et al. Selection of patients for liver transplantation and allocation of donated livers in the uk. Gut, 57(2):252\u2013257, 2008.\\n\\n[18] Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty Publication Series, page 80, 2000.\\n\\n[19] Philip S Thomas. Safe reinforcement learning. PhD thesis, University of Massachusetts Libraries, 2015.\\n\\n[20] Miroslav Dud\u00b4\u0131k, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. arXiv preprint arXiv:1103.4601, 2011.\\n\\n[21] John Hammersley. Monte carlo methods. Springer Science & Business Media, 2013.\\n\\n[22] Michael JD Powell and J Swann. Weighted uniform sampling\u2014a monte carlo technique for reducing variance. IMA Journal of Applied Mathematics, 2(3):228\u2013236, 1966.\\n\\n[23] Yash Chandak, Scott Niekum, Bruno Castro da Silva, Erik Learned-Miller, Emma Brunskill, and Philip S Thomas. Universal off-policy evaluation. arXiv preprint arXiv:2104.12820, 2021.\\n\\n[24] Keisuke Hirano, Guido W Imbens, and Geert Ridder. Efficient estimation of average treatment effects using the estimated propensity score. Econometrica, 71(4):1161\u20131189, 2003.\\n\\n[25] Donald B Rubin. Estimating causal effects from large data sets using propensity scores. Annals of internal medicine, 127(8 Part 2):757\u2013763, 1997.\\n\\n[26] Guido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical sciences. Cambridge University Press, 2015.\\n\\n[27] Guido W Imbens and Donald B Rubin. Rubin causal model. In Microeconometrics, pages 229\u2013241. Springer, 2010.\\n\\n[28] Steven Piantadosi. Clinical trials: a methodologic perspective. John Wiley & Sons, 2017.\\n\\n[29] Stuart J Pocock. Clinical trials: a practical approach. John Wiley & Sons, 2013.\\n\\n[30] Lawrence M Friedman, Curt D Furberg, David L DeMets, David M Reboussin, and Christopher B Granger. Fundamentals of clinical trials. Springer, 2015.\\n\\n[31] Rebecca DerSimonian and Nan Laird. Meta-analysis in clinical trials. Controlled clinical trials, 7(3):177\u2013188, 1986.\\n\\n[32] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.\\n\\n[33] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033. IEEE, 2012.\\n\\n[34] Joel Z. Leibo, Edgar Du\u00b4e nez Guzm\u00b4an, Alexander Sasha Vezhnevets, John P. Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charles Beattie, Igor Mordatch, and Thore Graepel. Scalable evaluation of multi-agent reinforcement learning with melting pot. In International Conference on Machine Learning, pages 6187\u20136199. PMLR, 2021.\\n\\n[35] Charles Beattie, Thomas K\u00a8oppe, Edgar A Du\u00b4e\u02dcnez-Guzm\u00b4an, and Joel Z Leibo. Deepmind lab2d. arXiv preprint arXiv:2011.07027, 2020.\\n\\n[36] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich K\u00a8uttler, John Agapiou, Julian Schrittwieser, et al. Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017.\\n\\n[37] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K\u00a8uttler, Andrew Lefrancq, Simon Green, V\u00b4\u0131ctor Vald\u00b4es, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016.\"}"}
{"id": "vSsnEd0Jmou", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[38] Tom Stepleton. The pycolab game engine, 2017.\\n\\n[39] Jersey Neyman. Sur les applications de la th\u00e9orie des probabilit\u00e9s aux exp\u00e9riences agricoles: Essai des principes. Roczniki Nauk Rolniczych, 10:1\u201351, 1923.\\n\\n[40] Paul W Holland. Statistics and causal inference. Journal of the American Statistical Association, 81(396):945\u2013960, 1986.\\n\\n[41] Mohd Shoaib, Utkarsh Prabhakar, Sumit Mahlawat, and Varun Ramamohan. A discrete-event simulation model of the kidney transplantation system in Rajasthan, India. Health Systems, 11(1):30\u201347, 2022.\\n\\n[42] Shoaib Mohd, Navonil Mustafee, Karan Madan, and Varun Ramamohan. Leveraging healthcare facility network simulations for capacity planning and facility location in a pandemic. Available at SSRN 3794811, 2021.\\n\\n[43] Cecilia Nardini. The ethics of clinical trials. Ecancermedicalscience, 8, 2014.\\n\\n[44] Jack Cuzick, Robert Edwards, and Nereo Segnan. Adjusting for non-compliance and contamination in randomized clinical trials. Statistics in medicine, 16(9):1017\u20131029, 1997.\\n\\n[45] David Maxwell Chickering and Judea Pearl. A clinician's tool for analyzing non-compliance. In Proceedings of the National Conference on Artificial Intelligence, pages 1269\u20131276, 1996.\\n\\n[46] Patrick S Kamath, Russell H Wiesner, Michael Malinchoc, Walter Kremers, Terry M Therneau, Catherine L Kosberg, Gennaro D'Amico, E Rolland Dickson, and W Ray Kim. A model to predict survival in patients with end-stage liver disease. Hepatology, 33(2):464\u2013470, 2001.\\n\\n[47] Wes McKinney. Data Structures for Statistical Computing in Python. In St\u00e9fan van der Walt and Jarrod Millman, editors, Proceedings of the 9th Python in Science Conference, pages 56\u201361, 2010.\\n\\n[48] The pandas development team. pandas-dev/pandas: Pandas, February 2020.\\n\\n[49] Alicia Curth and Mihaela van der Schaar. Nonparametric estimation of heterogeneous treatment effects: From theory to learning algorithms. In International Conference on Artificial Intelligence and Statistics, pages 1810\u20131818. PMLR, 2021.\\n\\n[50] S\u00f6ren R K\u00fcnzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences, 116(10):4156\u20134165, 2019.\\n\\n[51] Ioana Bica, James Jordon, and Mihaela van der Schaar. Estimating the effects of continuous-valued interventions using generative adversarial networks. Advances in Neural Information Processing Systems, 33:16434\u201316445, 2020.\\n\\n[52] Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2):299\u2013319, 2021.\\n\\n[53] Edward H Kennedy. Optimal doubly robust estimation of heterogeneous causal effects. arXiv preprint arXiv:2004.14497, 2020.\\n\\n[54] Scott Powers, Junyang Qian, Kenneth Jung, Alejandro Schuler, Nigam H Shah, Trevor Hastie, and Robert Tibshirani. Some methods for heterogeneous treatment effect estimation in high dimensions. Statistics in medicine, 37(11):1767\u20131787, 2018.\\n\\n[55] W Ray Kim, Ajitha Mannalithara, Julie K Heimbach, Patrick S Kamath, Sumeet K Asrani, Scott W Biggins, Nicholas L Wood, Sommer E Gentry, and Allison J Kwong. Meld 3.0: the model for end-stage liver disease updated for the modern era. Gastroenterology, 161(6):1887\u20131895, 2021.\\n\\n[56] Osvald Nitski, Amirhossein Azhie, Fakhar Ali Qazi-Arisar, Xueqi Wang, Shihao Ma, Leslie Lilly, Kymberly D Watt, Josh Levitsky, Sumeet K Asrani, Douglas S Lee, et al. Long-term mortality risk stratification of liver transplant recipients: real-time application of deep learning algorithms on longitudinal data. The Lancet Digital Health, 3(5):e295\u2013e305, 2021.\\n\\n[57] Uri Kartoun. Towards optimally replacing the current version of meld. Journal of Hepatology, 2022.\\n\\n[58] Ina Jochmans, Marieke van Rosmalen, Jacques Pirenne, and Undine Samuel. Adult liver allocation in Eurotransplant. Transplantation, 101(7):1542\u20131550, 2017.\"}"}
{"id": "vSsnEd0Jmou", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[59] David Goldberg, Alejandro Mantero, Craig Newcomb, Cindy Delgado, Kimberly Forde, David Kaplan, Binu John, Nadine Nuchovich, Barbara Dominguez, Ezekiel Emanuel, et al. Development and validation of a model to predict long-term survival after liver transplantation. *Liver Transplantation*, 27(6):797\u2013807, 2021.\\n\\n[60] Katie L Connor, Eoin D O'Sullivan, Lorna P Marson, Stephen J Wigmore, and Ewen M Harrison. The future role of machine learning in clinical transplantation. *Transplantation*, 105(4):723\u2013735, 2021.\\n\\n[61] Dennis Medved, Pierre Nugues, and Johan Nilsson. Simulating the outcome of heart allocation policies using deep neural networks. In *2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)*, pages 6141\u20136144. IEEE, 2018.\\n\\n[62] Manuel Dorado-Moreno, Mar\u00eda P\u00b4erez-Ortiz, Pedro A Guti\u00b4errez, Rub\u00b4en Ciria, Javier Brice\u02dcno, and C\u00b4esar Herv\u00b4as-Mart\u00b4\u0131nez. Dynamically weighted evolutionary ordinal neural network for solving an imbalanced liver transplantation problem. *Artificial Intelligence in Medicine*, 77:1\u201311, 2017.\\n\\n[63] Dennis Medved, Mattias Ohlsson, Peter H\u00a8oglund, Bodil Andersson, Pierre Nugues, and Johan Nilsson. Improving prediction of heart transplantation outcome using deep learning techniques. *Scientific reports*, 8(1):1\u20139, 2018.\\n\\n[64] Kyung Don Yoo, Junhyug Noh, Hajeong Lee, Dong Ki Kim, Chun Soo Lim, Young Hoon Kim, Jung Pyo Lee, Gunhee Kim, and Yon Su Kim. A machine learning approach using survival statistics to predict graft survival in kidney transplant recipients: a multicenter cohort study. *Scientific reports*, 7(1):1\u201312, 2017.\\n\\n[65] Dimitris Bertsimas, Vivek F Farias, and Nikolaos Trichakis. Fairness, efficiency, and flexibility in organ allocation for kidney transplantation. *Operations Research*, 61(1):73\u201387, 2013.\\n\\n[66] Dimitris Bertsimas, Vivek F Farias, and Nikolaos Trichakis. On the efficiency-fairness trade-off. *Management Science*, 58(12):2234\u20132250, 2012.\\n\\n[67] Amir Elalouf, Yael Perlman, and Uri Yechiali. A double-ended queueing model for dynamic allocation of live organs based on a best-fit criterion. *Applied Mathematical Modelling*, 60:179\u2013191, 2018.\\n\\n[68] John P Dickerson, Ariel D Procaccia, and Tuomas Sandholm. Failure-aware kidney exchange. In *Proceedings of the fourteenth ACM conference on Electronic commerce*, pages 323\u2013340, 2013.\\n\\n[69] Mustafa Akan, Oguzhan Alagoz, Baris Ata, Fatih Safa Erenay, and Adnan Said. A broader view of designing the liver allocation system. *Operations research*, 60(4):757\u2013770, 2012.\\n\\n[70] Andres E Ruf, Walter K Kremers, Lila L Chavez, Valeria I Descalzi, Luis G Podesta, and Federico G Villamil. Addition of serum sodium into the meld score predicts waiting list mortality better than meld alone. *Liver Transplantation*, 11(3):336\u2013343, 2005.\\n\\n[71] Neta Gotlieb, Amirhossein Azhie, Divya Sharma, Ashley Spann, Nan-Ji Suo, Jason Tran, Ani Orchanian-Cheff, Bo Wang, Anna Goldenberg, Michael Chass\u00b4e, et al. The promise of machine learning applications in solid organ transplantation. *NPJ digital medicine*, 5(1):1\u201313, 2022.\"}"}
{"id": "vSsnEd0Jmou", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"two models, the R-Learner optimises a custom loss function based on the propensity, the outcome model, and a cross-validation setup.\\n\\n\u2022 DR-Learner. The DR-Learner or doubly robust learner is an iteration of the X-Learner [53]. Like the X-Learner the DR-learner first estimates the outcome models, then completes the dataset to learn a treatment effects model using standard supervised learning. Then the DR-Learner repeats this process using the treatment effects model from the first step.\\n\\nWhile there are more meta-learners than what we reported above (e.g. U-Learner [52] or CW-Learner [54]), but they are much less adopted and unlike the S-Learner and T-Learner not fit for estimating the counterfactual outcomes as they, like the X-Learner, R-Learner, and DR-Learner, fit the effect function directly. We point the interested reader to the following papers: [49, 50, 52]; or to the following open-source libraries for various implementations: causal-ml (https://github.com/uber/causalml), or econml (https://github.com/microsoft/EconML).\\n\\nExtended Related Work\\n\\nLet us discuss how the literature which introduces novel allocation policies, in particular to donor-organ allocation. From this, we observe that they all come up with their own unique simulation in order to validate their proposal.\\n\\nAllocation policies from medicine. As an example, consider the following papers introduced in the medical domain (we focus here on the liver allocation setting as this is also where we focused on in our paper): [17, 55\u201359]. Each paper, all coming from different research groups, provide a custom simulation to validate their allocation policy. While some focus only on gathering test data (such as the recent [55]), others construct a simulation by simply iterating over the patients in the sequence they arrived in reality (such as [17]). While we would note some flaws in the way these policies are evaluated (e.g. counterfactual trajectories when allocations misalign), the truly striking observation is that each evaluation strategy is different! Not one paper reuses the same simulation; AllSim may change this going forward.\\n\\nAllocation policies from machine learning and OR. The same is true for the ML [4\u20136, 60\u201364] and OR [65\u201369] communities. It seems that both the ML and OR community is focused more and more on this important problem\u2013 which is fantastic! But it also warrants careful evaluation. Furthermore, if we find that the evaluation strategies in medicine (which generally propose linear combinations of features [46, 70] or simple CoxPH models [17, 55, 57, 59]) have shortcomings, then this is certainly the case for much more complicated strategies introduced in ML or OR. In fact, a recent survey confirmed exactly that: [71, cfr. Limitations of ML in transplant medicine]. Given the above, with respect to novelty, we suggest: had there been a purpose built simulator, would everyone be suggesting their own simple evaluation method paper-by-paper? We believe there is a clear literature gap which AllSim aims to fill.\\n\\nSimUnet. This brings us to SimUnet, which we thank the reviewer for highlighting. While indeed related, SimUnet is in fact very different from AllSim. In particular, from the online README document (see https://unos.org/wp-content/uploads/1-page-SimUnet.pdf) it can be seen that SimUnet is more concerned to simulate offers from the transplant clinician\u2019s point of view; not allocations from an overarching healthcare system (such as UNOS, or the NHS).\"}"}
