{"id": "mMh4W72Hhe", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Many state-of-the-art neural network verifiers for ReLU networks rely on branch-and-bound (BaB)-based methods. They branch ReLUs into positive (active) and negative (inactive) parts, and bound each subproblem independently. Since the cost of verification heavily depends on the number of subproblems, reducing the total number of branches is the key to verifying neural networks efficiently. Implications among neurons can eliminate subproblems: for example, when one or more ReLU neurons are branched into the active (or inactive) case, they may imply that a set of other neurons from any layers become active or inactive. In this paper, we propose a novel optimization formulation to find these implications, and a scalable method to solve these optimization problems among all neurons within tens of seconds, even for large ResNets, by reusing pre-computed variables in popular bound-propagation-based verification methods such as $\\\\alpha$-$\\\\beta$-CROWN. Our method is less restrictive than previous approaches and can produce significantly more bound implications compared to prior work, which may benefit many BaB-based verifiers. When evaluated on a set of popular verification benchmarks and a new benchmark consisting of harder verification problems, we consistently reduce the verification time and verify more problems than state-of-the-art verification tools.\\n\\n1 INTRODUCTION\\n\\nAs deep neural networks (DNNs) have been widely adopted in safety-critical domains (Kiran et al., 2021; Popova et al., 2018; Cao et al., 2021; Eykholt et al., 2018), providing rigorous verification for their robustness, safety and correctness is critical. To address these concerns, neural network verification algorithms (Ehlers, 2017; Katz et al., 2017; Tjeng et al., 2019) have been developed to prove certain properties of a DNN rigorously. A traditional example is certifying robustness under adversarial perturbations, where one aims to verify that the classification outcome does not change under bounded input perturbations. More general specifications of DNNs can also be proved, and DNN verification has become a valuable tool in different domains such as control (Everett, 2021) and cyberphysical systems (Rober et al., 2022; Harapanahalli et al., 2023). Proving these specifications typically requires rigorously bounding the outputs of DNNs considering input perturbations.\\n\\nOne popular DNN verification approach is the Branch and Bound (BaB) based methods (Bunel et al., 2018; 2020; Palma et al., 2021; Bak et al., 2020; Wang et al., 2018a;b; 2021), which exploits the piece-wise-linear property of ReLU-based DNNs. Since ReLU $(x) = \\\\max\\\\{x, 0\\\\}$ consists of two linear pieces, the verification problem can be split into two easier subproblems, each with one less nonlinear ReLU neuron. This property is the foundation for BaB-based verification approaches and many state-of-the-art verifiers, such as $\\\\alpha,\\\\beta$-CROWN (Wang et al., 2021; Xu et al., 2020b; Zhang et al., 2022), MN-BaB (Ferrari et al., 2021), and VeriNet (Henriksen & Lomuscio, 2020), utilized BaB as their core verification procedure.\\n\\nSpecifically, a BaB-based verifier splits a ReLU neuron into inactive (inputs $\\\\leq 0$) and active (input $> 0$) cases (Branching), creating two subproblems after each neuron is split; then the lower and upper bounds of the model output for each subproblem are calculated (Bounding). The bounding process for each subproblem typically involves solving a relaxed optimization problem, which is often the most costly step. After one step of BaB, the output bounds usually improve and may lead to successful verification of subproblems. If the bounds are still not tight enough, the verifier will recursively choose the next ReLU neuron to split in each unsolved subproblem.\"}"}
{"id": "mMh4W72Hhe", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Renderson G Anderson, Ziye Ma, Jingqi Li, and Somayeh Sojoudi. Tightened convex relaxations for neural network robustness certification. In *2020 59th IEEE Conference on Decision and Control (CDC)*, pp. 2190\u20132197. IEEE, 2020a.\\n\\nRoss Anderson, Joey Huchette, Will Ma, Christian Tjandraatmadja, and Juan Pablo Vielma. Strong mixed-integer programming formulations for trained neural networks. *Mathematical Programming*, 183(1):3\u201339, 2020b.\\n\\nStanley Bak, Hoang-Dung Tran, Kerianne Hobbs, and Taylor T Johnson. Improved geometric path enumeration for verifying relu neural networks. In *International Conference on Computer Aided Verification*, pp. 66\u201396. Springer, 2020.\\n\\nStanley Bak, Changliu Liu, and Taylor Johnson. The second international verification of neural networks competition (vnn-comp 2021): Summary and results. *arXiv preprint arXiv:2109.00498*, 2021.\\n\\nBen Batten, Panagiotis Kouvaros, Alessio Lomuscio, and Yang Zheng. Efficient neural network verification via layer-based semidefinite relaxations and linear cuts. In *International Joint Conference on Artificial Intelligence (IJCAI21)*, pp. 2184\u20132190, 2021.\\n\\nElena Botoeva, Panagiotis Kouvaros, Jan Kronqvist, Alessio Lomuscio, and Ruth Misener. Efficient verification of relu-based neural networks via dependency analysis. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34, pp. 3291\u20133299, 2020.\\n\\nRudy Bunel, P Mudigonda, Ilker Turkaslan, P Torr, Jingyue Lu, and Pushmeet Kohli. Branch and bound for piecewise linear neural network verification. *Journal of Machine Learning Research*, 21(2020), 2020.\\n\\nRudy R Bunel, Ilker Turkaslan, Philip Torr, Pushmeet Kohli, and Pawan K Mudigonda. A unified view of piecewise linear neural network verification. In *Advances in Neural Information Processing Systems*, pp. 4790\u20134799, 2018.\\n\\nY. Cao, N. Wang, C. Xiao, D. Yang, J. Fang, R. Yang, Q. Chen, M. Liu, and B. Li. Invisible for both camera and lidar: Security of multi-sensor fusion based perception in autonomous driving under physical-world attacks. In *2021 IEEE Symposium on Security and Privacy (SP)*, pp. 1302\u20131320, Los Alamitos, CA, USA, may 2021. IEEE Computer Society. doi: 10.1109/SP40001.2021.00076. URL https://doi.ieeecomputersociety.org/10.1109/SP40001.2021.00076.\\n\\nHong-Ming Chiu and Richard Y Zhang. Tight certification of adversarially trained neural networks via nonconvex low-rank semidefinite relaxations. 2023.\\n\\nSumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan Uesato, Rudy R Bunel, Shreya Shankar, Jacob Steinhardt, Ian Goodfellow, Percy S Liang, and Pushmeet Kohli. Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), *Advances in Neural Information Processing Systems*, volume 33, pp. 5318\u20135331, 2020.\\n\\nAlessandro De Palma, Harkirat S Behl, Rudy Bunel, Philip Torr, and M Pawan Kumar. Scaling the convex barrier with active sets. In *Proceedings of the ICLR 2021 Conference*. Open Review, 2021a.\\n\\nAlessandro De Palma, Rudy Bunel, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet Kohli, Philip HS Torr, and M Pawan Kumar. Improved branch and bound for neural network verification via lagrangian decomposition. *arXiv preprint arXiv:2104.06718*, 2021b.\\n\\nKingma Diederik, Ba Jimmy, et al. Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*, pp. 273\u2013297, 2014.\\n\\nSouradeep Dutta, Susmit Jha, Sriram Sanakaranarayanan, and Ashish Tiwari. Output range analysis for deep neural networks. *arXiv preprint arXiv:1709.09130*, 2017.\"}"}
{"id": "mMh4W72Hhe", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nKrishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Chongli Qin, Soham De, and Pushmeet Kohli. Efficient neural network verification with exactness characterization. In Proc. Uncertainty in Artificial Intelligence, UAI, pp. 164, 2019.\\n\\nRuediger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In International Symposium on Automated Technology for Verification and Analysis, pp. 269\u2013286. Springer, 2017.\\n\\nMichael Everett. Tutorial on safety verification and stability analysis of neural network-driven systems. IEEE Conference on Decision and Control (CDC), 2021.\\n\\nKevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1625\u20131634, 2018.\\n\\nClaudio Ferrari, Mark Niklas Mueller, Nikola Jovanovi\u0107, and Martin Vechev. Complete verification via multi-neuron relaxation guided branch-and-bound. In International Conference on Learning Representations, 2021.\\n\\nTimon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. AI\u00b2: Safety and robustness certification of neural networks with abstract interpretation. In 2018 IEEE Symposium on Security and Privacy (SP), pp. 3\u201318. IEEE, 2018.\\n\\nSven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for training verifiably robust models. Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019.\\n\\nAkash Harapanahalli, Saber Jafarpour, and Samuel Coogan. A toolbox for fast interval arithmetic in numpy with an application to formal verification of neural network controlled systems. arXiv preprint arXiv:2306.15340, 2023.\\n\\nPatrick Henriksen and Alessio Lomuscio. Efficient neural network verification via adaptive refinement and adversarial search. In ECAI 2020, pp. 2513\u20132520. IOS Press, 2020.\\n\\nGuy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In International Conference on Computer Aided Verification, pp. 97\u2013117. Springer, 2017.\\n\\nB Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yogamani, and Patrick P\u00e9rez. Deep reinforcement learning for autonomous driving: A survey. IEEE Transactions on Intelligent Transportation Systems, 2021.\\n\\nPanagiotis Kouvaros and Alessio Lomuscio. Towards scalable complete verification of relu neural networks via dependency-based branching. In IJCAI, pp. 2643\u20132650, 2021.\\n\\nJingyue Lu and M Pawan Kumar. Neural network branching for neural network verification. The International Conference on Learning Representations, 2020.\\n\\nMark Niklas M\u00fcller, Christopher Brix, Stanley Bak, Changliu Liu, and Taylor T Johnson. The third international verification of neural networks competition (vnn-comp 2022): summary and results. arXiv preprint arXiv:2212.10376, 2022a.\\n\\nMark Niklas M\u00fcller, Gleb Makarchuk, Gagandeep Singh, Markus P\u00fcschel, and Martin Vechev. Prima: general and precise neural network certification via scalable convex hull approximations. Proceedings of the ACM on Programming Languages, 6(POPL):1\u201333, 2022b.\\n\\nAlessandro De Palma, Harkirat Behl, Rudy R Bunel, Philip Torr, and M. Pawan Kumar. Scaling the convex barrier with active sets. In International Conference on Learning Representations, 2021.\\n\\nMariya Popova, Olexandr Isayev, and Alexander Tropsha. Deep reinforcement learning for de novo drug design. Science advances, 4(7):eaap7885, 2018.\"}"}
{"id": "mMh4W72Hhe", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nAditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semidefinite relaxations for certifying robustness to adversarial examples. In Advances in Neural Information Processing Systems, pp. 10877\u201310887, 2018.\\n\\nNicholas Rober, Michael Everett, and Jonathan P How. Backward reachability analysis for neural feedback loops. In 2022 IEEE 61st Conference on Decision and Control (CDC), pp. 2897\u20132904. IEEE, 2022.\\n\\nZhouxing Shi, Qirui Jin, Huan Zhang, Zico Kolter, Suman Jana, and Cho-Jui Hsieh. Formal verification for neural networks with general nonlinearities via branch-and-bound. In 2nd Workshop on Formal Verification of Machine Learning (WFVML 2023), 2023.\\n\\nGagandeep Singh, Rupanshu Ganvir, Markus P\u00fcschel, and Martin Vechev. Beyond the single neuron convex barrier for neural network certification. In Advances in Neural Information Processing Systems, pp. 15072\u201315083, 2019a.\\n\\nGagandeep Singh, Timon Gehr, Markus P\u00fcschel, and Martin Vechev. An abstract domain for certifying neural networks. Proceedings of the ACM on Programming Languages, 3(POPL):41, 2019b.\\n\\nChristian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, Krunal Kishor PATEL, and Juan Pablo Vielma. The convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural network verification. Advances in Neural Information Processing Systems, 33:21675\u201321686, 2020.\\n\\nVincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed integer programming. In International Conference on Learning Representations, 2019.\\n\\nShiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Efficient formal safety analysis of neural networks. In Advances in Neural Information Processing Systems, pp. 6367\u20136377, 2018a.\\n\\nShiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Formal security analysis of neural networks using symbolic intervals. In 27th USENIX Security Symposium (USENIX Security 18), pp. 1599\u20131614, 2018b.\\n\\nShiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. Beta-crown: Efficient bound propagation with per-neuron split constraints for neural network robustness verification. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nEric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, pp. 5286\u20135295. PMLR, 2018.\\n\\nKaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certified robustness and beyond. Advances in Neural Information Processing Systems, 33, 2020a.\\n\\nKaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh. Fast and complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers. In International Conference on Learning Representations, 2020b.\\n\\nHuan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\\n\\nHuan Zhang, Shiqi Wang, Kaidi Xu, Linyi Li, Bo Li, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. General cutting planes for bound-propagation-based neural network verification. Advances in Neural Information Processing Systems, 2022.\"}"}
{"id": "mMh4W72Hhe", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Section 3, we discussed how to find neuron implications between two neurons. Since each ReLU neuron has two possible statuses (active or inactive), there are four kinds of possible implications. We list all four possibilities and their corresponding optimization formulation here:\\n\\n1. An implicant neuron $z_{i2}^{j2}$ split to inactive case implies an improved lower bound of an implicated neuron $z_{i1}^{j1}$ (this is the case in (6) discussed in Section 3):\\n\\n$$l_{\\\\text{relaxed}}^* = \\\\min x a_{i1,j1}^\\\\top x + c_{i1,j1}$$\\n\\ns.t. $x_0 - \\\\epsilon \\\\leq x \\\\leq x_0 + \\\\epsilon$; $a_{i2,j2}^\\\\top x + c_{i2,j2} \\\\leq 0$ (9)\\n\\n2. An implicant neuron $z_{i2}^{j2}$ split to active case implies an improved lower bound of an implicated neuron $z_{i1}^{j1}$:\\n\\n$$l_{\\\\text{relaxed}}^* = \\\\min x a_{i1,j1}^\\\\top x + c_{i1,j1}$$\\n\\ns.t. $x_0 - \\\\epsilon \\\\leq x \\\\leq x_0 + \\\\epsilon$; $a_{i2,j2}^\\\\top x + c_{i2,j2} \\\\geq 0$ (10)\\n\\n3. An implicant neuron $z_{i2}^{j2}$ split to inactive case implies an improved upper bound of an implicated neuron $z_{i1}^{j1}$:\\n\\n$$l_{\\\\text{relaxed}}^* = \\\\max x a_{i1,j1}^\\\\top x + c_{i1,j1}$$\\n\\ns.t. $x_0 - \\\\epsilon \\\\leq x \\\\leq x_0 + \\\\epsilon$; $a_{i2,j2}^\\\\top x + c_{i2,j2} \\\\leq 0$ (11)\\n\\n4. An implicant neuron $z_{i2}^{j2}$ split to active case implies an improved upper bound of an implicated neuron $z_{i1}^{j1}$:\\n\\n$$l_{\\\\text{relaxed}}^* = \\\\max x a_{i1,j1}^\\\\top x + c_{i1,j1}$$\\n\\ns.t. $x_0 - \\\\epsilon \\\\leq x \\\\leq x_0 + \\\\epsilon$; $a_{i2,j2}^\\\\top x + c_{i2,j2} \\\\geq 0$ (12)\\n\\nNote that all four optimization problems have the same nature and can be solved using the same technique discussed in Section C.1. we will only use Eq. (6) as an example since all other three cases can be converted to Eq. (6). For instance, to solve Eq. (10), we can change the implicant constraint to $-a_{i2,j2}^\\\\top x - c_{i2,j2} \\\\leq 0$; to solve Eq. (11), we can change the max objective to $\\\\min x a_{i1,j1}^\\\\top x - c_{i1,j1}$.\\n\\nWe now give the proofs for the two theorems in Section 3. Here Theorem 3.1 shows the possibility of solving a cheap linear programming problem to find bound implications, and Theorem 3.2 shows how we can reduce the number of optimization problems by filtering out some neurons that do not help find bound implications.\\n\\nTheorem 3.1. Given two unstable neurons and the LP formulations in Eq. (4), (5) and (6), the following holds: (1) The LP in Eq. (6) is always feasible; (2) $l_{\\\\text{no-imp}}^* \\\\leq l_{\\\\text{relaxed}}^* \\\\leq l_{\\\\text{imp}}^*$.\\n\\nProof. (1) To show that (6) is always feasible, we must show that there exists some $x$ such that $x_0 - \\\\epsilon \\\\leq x \\\\leq x_0 + \\\\epsilon$ also satisfies the other constraint $a_{i2,j2}^\\\\top x + c_{i2,j2} \\\\leq 0$.\\n\\nGiven that the neuron $z_{i2}^{j2}$ is an unstable neuron, we know that $l_{i2}^{j2} \\\\leq 0$. By definition of $l_{i2}^{j2}$, we know that $\\\\min x a_{i2,j2}^\\\\top x + c_{i2,j2} \\\\leq 0$, so such an $x$ must exist to satisfy the constraint.\"}"}
{"id": "mMh4W72Hhe", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Theorem 3.2. Given two unstable neurons and the LP formulations in (4), (6), if the following objective violation of the new constraint in (6). If this solution then this constraint is redundant and thus.\\n\\nNow, we define the set\\\\( \\\\hat{x} \\\\). Geometrically, the constraint \\\\( \\\\hat{x} \\\\) gives an optimal solution of (4) that has the minimum distance is less than or equal to the box distance to the origin first.\\n\\nWe claim that \\\\( \\\\hat{x} \\\\in C \\\\).\\\\( \\\\hat{x} \\\\) satisfies the new implicant constraint, the constraint is redundant, and \\\\( \\\\hat{x} \\\\) already satisfies the additional constraint gives an optimal solution of (4) that has the minimum distance is less than or equal to the box distance to the origin first.\"}"}
{"id": "mMh4W72Hhe", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We show the full algorithms discussed in Section 3 in this section.\\n\\nC.1 Closed-Form Solution of the Relaxed LP\\n\\nIn Section 3, we show that it is possible to use a fast optimization algorithm in $O(d_0 \\\\log d_0)$ time, where $d_0$ is the neural network input dimension. Here we present this algorithm.\\n\\nWe solve the problem by adding a Lagrange multiplier $\\\\rho$ and derive a dual form of Eq. (6):\\n\\n$$\\\\begin{align*}\\n\\\\min_{x} & \\\\quad 0 - \\\\epsilon \\\\leq x \\\\leq x_0 + \\\\epsilon \\\\\\\\\\n\\\\max_{\\\\rho} & \\\\quad \\\\rho \\\\geq 0 \\\\\\\\\\n& \\\\quad a(i_1, j_1)^\\\\top x + c(i_1, j_1) + \\\\rho (a(i_2, j_2)^\\\\top x + c(i_2, j_2)) \\\\geq \\\\max_{\\\\rho \\\\geq 0} (a(i_1, j_1) + \\\\rho a(i_2, j_2))^\\\\top x_0 + c(i_2, j_2) - \\\\|a(i_1, j_1) + \\\\rho a(i_2, j_2)\\\\|_1 \\\\cdot \\\\epsilon\\n\\\\end{align*}$$\\n\\nNote that the inner minimization is solved in closed form using H\u00f6lder's inequality. The maximization problem over the dual variable $\\\\rho$ is a one-dimensional, non-smooth, piece-wise linear, and concave optimization problem and can be solved by checking super-gradients at the endpoints of all linear pieces. We list the solving procedure in Algorithm 1:\\n\\nAlgorithm 1: The closed-form solution of Eq. (6).\\n\\n1: Inputs: $a(i_1, j_1)$, $c(i_1, j_1)$, $a(i_2, j_2)$, $c(i_2, j_2)$, $x_0$, $\\\\epsilon$\\n\\n2: Outputs: The optimal solution of Eq. (6): $l^{\\\\ast}_{\\\\text{relaxed}}$\\n\\n3: $q \\\\leftarrow -a(i_1, j_1) / a(i_2, j_2)$\\n\\n4: $I \\\\leftarrow \\\\text{argsort} (q)$\\n\\n5: $a(i_2, j_2)$ sorted $\\\\leftarrow \\\\{a(i_2, j_2)_{I_1 \\\\cdot \\\\epsilon}, a(i_2, j_2)_{I_2 \\\\cdot \\\\epsilon}, ..., a(i_2, j_2)_{d_0 \\\\cdot \\\\epsilon}\\\\}$\\n\\n6: $a(i_2, j_2)_{i} \\\\leftarrow -\\\\sum_{k=1}^{I} |a(i_2, j_2)_{k}|$, $i \\\\in [d_0]$\\n\\n7: $a(i_2, j_2)_{i} \\\\leftarrow a(i_2, j_2)_{i} - a(i_2, j_2)_{d_0 \\\\cdot \\\\epsilon}$\\n\\n8: $\\\\nabla a \\\\leftarrow a(i_2, j_2)_{i} + a(i_2, j_2)_{i} - (a(i_2, j_2)^\\\\top x_0 + c(i_2, j_2))$  \\n\\n9: $i^{\\\\ast} \\\\leftarrow i$ where $\\\\nabla a_i = 0$\\n\\n10: $\\\\rho^{\\\\ast} \\\\leftarrow \\\\max (q_{I_i^{\\\\ast}}, 0)$\\n\\n11: $l^{\\\\ast}_{\\\\text{relaxed}} \\\\leftarrow -\\\\|a(i_1, j_1) + \\\\rho^{\\\\ast} a(i_2, j_2)\\\\|_1 \\\\cdot \\\\epsilon + \\\\rho^{\\\\ast} (a(i_2, j_2)^\\\\top x_0 + c(i_2, j_2)) + a(i_1, j_1) x_0 + c(i_1, j_1)$\\n\\n12: $l^{\\\\ast}_0 \\\\leftarrow -\\\\|a(i_1, j_1)\\\\|_1 \\\\cdot \\\\epsilon + c(i_2, j_2) + a(i_1, j_1) x_0 + c(i_1, j_1)$\\n\\n13: if $l^{\\\\ast}_{\\\\text{relaxed}} < l^{\\\\ast}_0$ then\\n\\n14: $l^{\\\\ast}_{\\\\text{relaxed}} \\\\leftarrow l^{\\\\ast}_0$\\n\\n15: Return: $l^{\\\\ast}_{\\\\text{relaxed}}$\\n\\nC.2 Algorithm of BIG Construction and Utilization\\n\\nIn Algorithm 3, we will introduce how to construct BIG after we obtain the linear equation of all unstable neurons (can be calculated by CROWN or $\\\\alpha$-CROWN) in the verification process. Note that there are For loops Algorithm 3, the calculations inside them are independent and can be calculated in parallel though. In our experiments, we construct BIG efficiently by computing all implications only in four batches for four cases of neuron interactions.\"}"}
{"id": "mMh4W72Hhe", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 2\\n\\nFilter Top-$K$ constraints by distance to $x_0$.\\n\\n1: Inputs: constraints of all unstable neurons $a$, $a \\\\in \\\\mathbb{R}^{N \\\\times d_0}$ and $c$, $c \\\\in \\\\mathbb{R}^N$, where $N$ is number of unstable neurons and $d_0$ is the length of the model input $x_0$; perturbation size $\\\\epsilon$, $K$, $S = \\\\emptyset$, $S = \\\\emptyset$\\n\\n2: function $T_{OP}KFILTERING(a, a, c, c, x_0, \\\\epsilon, K)$\\n\\n3: for $p = 1$ to $N$ do\\n\\n4: $S \\\\cup |a_{(i_p,j_p)}^\\\\top x_0 + c_{(i_p,j_p)}| - \\\\epsilon \\\\cdot ||a_{(i_p,j_p)}||_1$\\n\\n5: $S \\\\cup |a_{(i_p,j_p)}^\\\\top x_0 + c_{(i_p,j_p)}| - \\\\epsilon \\\\cdot ||a_{(i_p,j_p)}||_1$\\n\\n6: $\\\\triangleright$ Sort from nearest to farthest distance\\n\\n7: $S = \\\\text{argsort}(d_p)[:K]$ $\\\\triangleright$ Indices of Top-$K$ inactive implicant constraints\\n\\n8: $S = \\\\text{argsort}(d_p)[:K]$ $\\\\triangleright$ Indices of Top-$K$ active implicant constraints\\n\\n9: return $S, S$\\n\\nDBOUND IMPLICATIONS WITH ADDITIONAL SPLIT CONSTRAINTS\\n\\nTo solve Eq. (8), we leverage $k$ Lagrange multipliers $\\\\rho_i$, where $i \\\\in \\\\{1, 2, \\\\ldots, k\\\\}$ to derive the dual form solution:\\n\\n$$\\\\max_{\\\\rho_i \\\\geq 0} \\\\min_{x} a_{(i_1,j_1)}^\\\\top x + c_{(i_1,j_1)} + \\\\sum_{k=1}^{M} \\\\rho_i (a_{(m[k],n[k])}^\\\\top x + c_{(m[k],n[k])})$$\\n\\ns.t.\\n\\n$$x_0 - \\\\epsilon \\\\leq x \\\\leq x_0 + \\\\epsilon$$ (14)\\n\\nFor the $\\\\ell_\\\\infty$-norm constraint of $x$, the inner minimization has a closed-form solution:\\n\\n$$\\\\max_{\\\\rho_i \\\\geq 0} a_{(i_1,j_1)}^\\\\top x_0 + c_{(i_1,j_1)} - \\\\|a_{(i_1,j_1)}\\\\|_1 \\\\cdot \\\\epsilon + \\\\sum_{k=1}^{M} \\\\rho_i (a_{(m[k],n[k])}^\\\\top x_0 + c_{(m[k],n[k])}) - \\\\|\\\\rho_i a_{(m[k],n[k])}\\\\|_1 \\\\cdot \\\\epsilon$$ (15)\\n\\nThen, we can easily solve $\\\\rho_i$ by projected gradient descent using an optimizer like Adam (Diederik et al., 2014).\\n\\nEXPERIMENTAL DETAILS\\n\\nE.1 VeriHard Benchmark\\n\\nThe VeriHard benchmark includes models from existing benchmarks, such as MNIST-A-Adv, CIFAR-A-Adv, CIFAR-A-Mix from SDP-FO (Dathathri et al., 2020), and CIFAR100-small, CIFAR100-medium, CIFAR100-large, and TinyImageNet-medium from the neural networks competition (VNN-COMP (Bak et al., 2021; M\u00fcller et al., 2022a)).\\n\\nVeriHard applies a careful selection of perturbation size $\\\\epsilon$ for each instance, aiming to increase the verification challenge while ensuring solvability. We will illustrate our $\\\\epsilon$ configuration algorithm as follows:\\n\\nWe start by determining the unknown range $[\\\\epsilon_l, \\\\epsilon_r]$ of each instance. Given an instance with an initial $\\\\epsilon_l = 0$, we increment $\\\\epsilon_l$ step by step with step-size $\\\\alpha = 0.01$, applying CROWN verification after each step, until the vanilla CROWN verifier fails to verify the instance at the current $\\\\epsilon_l$. Conversely, we initialize $\\\\epsilon_r$ to $1.0$, decreasing it by the same step size $0.01$ until PGD attack succeed with perturbation bound $\\\\epsilon_r$. In this case, any $\\\\epsilon$ configuration within the range $[\\\\epsilon_l, \\\\epsilon_r]$ enforces verification using the BaB process.\\n\\nFor determining the final $\\\\epsilon$, we employ $\\\\beta$-CROWN as our reference verifier. Binary search on $\\\\epsilon$ is used to assess the feasibility of verifying the instance within a timeout $t$, which is randomly sampled from the interval $[0.8T, 1.5T]$ ($T$ is the final timeout designated for the instance). The resulting $\\\\epsilon$ will make $\\\\beta$-CROWN verifier running time to be as close to the given timeout $t$ as possible.\\n\\nIn VeriHard dataset, for MNIST and CIFAR10 instances, the actual timeout $T$ is set to 200 seconds, whereas for CIFAR100 and TinyImageNet instances, $T$ is set to 250 seconds. We crafted 100\"}"}
{"id": "mMh4W72Hhe", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: (Left) Global lower bounds; (Middle) Total explored branches; (Right) Average used bound implications trends along with branching iterations on one specific instance in VeriHard-MNIST-A-Adv benchmark. More than one neurons are branched per iteration due to BIG, and the bounds and the number explored branches both improve noticeably.\\n\\nFigure 6: (Left) Global lower bounds; (Middle) Total explored branches; (Right) Average used bound implications trends along with branching iterations on one specific instance in VeriHard-CIFAR10-A-Adv benchmark.\\n\\nE.2 EXPERIMENTAL SETUP\\n\\nOur implementation is based on the open-source $\\\\alpha, \\\\beta$-CROWN verifier by integrating BIG and subBIGs construction and branching utilization code into that. For BIG and subBIGs construction, we select top-$K$ implicant neurons to derive our paired bound implications as mentioned in Theorem 3.2 with $K = 1000$. Specifically for subBIGs, we use first $M = 8$ splits by BaB and apply a 50-steps gradient descent with Adam optimizer to optimize our objective $l^*$ relaxed-multi on up to $2M = 256$ unsolved subproblems with initial learning rate as $0.1$ and its decay factor as $0.99$. Throughout our experiments, we employ Filtered Smart Branching (FSB) (De Palma et al., 2021a) as our branching heuristics and apply Adam optimizer to optimize both $\\\\alpha$ and $\\\\beta$ for 20 iterations during verification process. The initial learning rate is set to be $0.1$ for $\\\\alpha$ optimization and $0.05$ for $\\\\beta$, while the corresponding decay ratio is set to be $0.995$ for $\\\\alpha$ and $0.98$ for $\\\\beta$. All our experiments are conducted on one NVIDIA A100 GPU device (80G memory). Timeout for classic benchmark is aligned with prior work: MNIST-CNN-A-Adv (200s), CIFAR10-CNN-A-Adv (200s), CIFAR10-CNN-A-Mix (200s), MNIST-ConvSmall (180s), CIFAR10-ConvSmall (180s). For VeriHard benchmark, we set timeout to be 200 seconds for MNIST and CIFAR10 instances and 250 second for CIFAR100 and TinyImageNet instances.\\n\\nF VISUALIZATION OF BIG-ENHANCED BA PROCESS\\n\\nIn this section, we provide more figures showing the bounds improvements, the number of branches, and the average number of branched neurons per BaB iteration, on different benchmarks and data-points.\\n\\nGABLATION STUDIES ON GCP-CROWN\\n\\nGCP-CROWN (Zhang et al., 2022) enhances NN verification efficiency by introducing general cutting plane methods through mixed integer programming (MIP) solver. The derived cutting...\"}"}
{"id": "mMh4W72Hhe", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nedge \\\\((Q^i_2, P^i_1)\\\\) to the graph. Note that in practice, we only need to maintain unstable neurons in this graph. In addition, the bound implications can be propagated through the graph until reaching nodes with zero out-degree, so although the bound implications found by solving (6) is only for a pair of neurons, more neurons can be connected on the graph (Figure 1c).\\n\\nDuring BaB, when we split \\\\(z^i_1j_1\\\\) into the active (or inactive) case, we traverse BIG starting from the node \\\\(P^i_1j_1\\\\) (or \\\\(Q^i_1j_1\\\\)) and find all connected nodes. If there is a path from \\\\(P^i_1j_1\\\\) to \\\\(P^i_1'j_1\\\\) (or \\\\(Q^i_1'j_1\\\\)), we can add the additional split \\\\(z^{i_1'}j_1' = 0\\\\) (or \\\\(z^{i_1'}j_1' = 0\\\\)) to this subproblem. We often find that splitting one single neuron may imply multiple other neurons are active or inactive. This greatly improves the effectiveness of BaB since these implied neurons will not need splits anymore, and these additional constraints help to obtain tighter bounds. We show our full algorithm to construct BIG and use BIG during branch and bound in Algorithm 3 and 4 in Appendix C respectively.\\n\\nSubproblem-specific Bound implications. We propose one additional approach to enhance BIG. In BaB, many subproblems share the same splits constraints; for example, if the first neuron chosen by BaB is \\\\(z^i_1j_1\\\\), then the constraint \\\\(z^i_1j_1 \\\\geq 0\\\\) (\\\\(z^i_1j_1\\\\) is active) or \\\\(z^i_1j_1 \\\\leq 0\\\\) (\\\\(z^i_1j_1\\\\) is inactive) will appear in all subproblems created later. Considering the first \\\\(M\\\\) split by BaB, up to \\\\(2^M\\\\) initial subproblems are created, and all subsequently generated subproblems contain \\\\(M\\\\) constraints that are the same as those in one of the initial subproblems. If we start the construction of BIG after the first \\\\(M\\\\) splits, we can build up to \\\\(2^M\\\\) different BIGs and each BIG may benefit from additional \\\\(M\\\\) split constraints. In this case, each BIG is subproblem-specific (called \u201csubBIG\u201d) because it includes constraints valid only in a specific subset of subproblems. To build subBIGs, we extend bound implications with multiple constraints from initial branches. We derive from the Eq. (5) with the implicant constraint and additional constraints:\\n\\n\\\\[\\nx^a(i_2,j_2)\\\\top x + c(i_2,j_2) \\\\leq 0 \\\\quad \\\\text{(implicant constraint)}\\n\\\\]\\n\\nHowever, with constraints, \\\\(l^*\\\\) has no closed-form solution. We construct Lagrangian multipliers and solve the dual problem by gradient descent as shown in Appendix D.\\n\\nDiscussion. Our framework of finding bound implications has several benefits. First, it does not rely on expensive solvers and reuses the linear bounds for computing intermediate layer bounds for free, which is very suitable for popular bound-propagation-based NN verifiers. Second, our bound implications can be found between two neurons from any layers (we have no restrictions on \\\\(j_1, j_2\\\\) above), capturing both inter- and intra-layer neuron correlations. Third, our algorithm is scalable regarding DNN size because its complexity depends on the number of selected unstable neurons for finding the bound implications, and the optimization problem (5) does not depend on DNN size (except for the input dimension). Finally, BIG is independent of the actual BaB verifier used; although our evaluation uses \\\\(\\\\beta\\\\)-CROWN as the base BaB verifier, it can also strengthen some new BaB verifiers not evaluated in this work. In Sec. 4, we show that our strong optimization-based formulation leads to finding significantly more neuron implications compared to previous works (Kouvaros & Lomuscio, 2021; Botoeva et al., 2020), and also noticeably improves state-of-the-art BaB verifiers.\\n\\n4 EXPERIMENTS\\n\\nWe first compare our approach to existing work that also discover bound implications or dependencies among neurons. Then, we evaluate our bound implication graph (BIG)-enhanced verifier, on various verification benchmarks. We leave more details about experiment setup in Appendix E.\\n\\nBound Implications Comparison. We first evaluate the number of bound implications generated by Venus2 (Kouvaros & Lomuscio, 2021) and ours. We conduct experiments on MNIST-MLP and CIFAR-MLP used in (Botoeva et al., 2020), CIFAR-CNN-A-Adv and CIFAR-CNN-A-Mix from (Dathathri et al., 2020). We compare all implications generated before BaB and skip data points that can be verified by initial MIP or \\\\(\\\\alpha\\\\)-CROWN since there is no necessity to use BaB. In Table 1, we show that our method performs significantly better than Venus2. Especially in CNN models, Venus2 cannot find intra-layer implications because it considers implications using intermediate layer bounds of one convolutional layer only, and no implications can be found because the convolutional layer\"}"}
{"id": "mMh4W72Hhe", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Number of bound implications generated by different methods on 4 models. Following (Botoeva et al., 2020), we report inter-layer ($i_1 \\\\neq i_2$) and intra-layer ($i_1 = i_2$) implications separately.\\n\\n|                | MNIST-MLP | CIFAR-MLP | CIFAR-CNN-A-Adv | CIFAR-CNN-A-Mix |\\n|----------------|-----------|------------|------------------|-----------------|\\n| **Avg. inter-layers implications** | Venus2     | 56.15      | 4.66             | 6.42            |\\n|                | Ours       | 203.5      | 397.5            | 340.0           |\\n| **Avg. intra-layers implications** | Venus2     | 9.80       | 35.33            | 0               |\\n|                | Ours       | 263.4      | 502.1            | 763.4           |\\n\\nWeights are sparse and uncorrelated. In contrast, our optimization-based formulation found hundreds of implications that involve neurons from the same layer or different layers.\\n\\nA New Benchmark with Hard Verification instances. Although we included several standard benchmarks used in previous papers showing consistent improvements, we also identified their shortcomings. First, many samples in these benchmarks can be verified by simple verification algorithms (such as CROWN/DeepPoly) or attacked via PGD attacks, so they cannot accurately reflect the recent progress of DNN verifiers. On the other hand, certain hard benchmarks have many unsolved instances with unknown difficulty, and it is unclear whether they can be actually solved. With only these benchmarks, it is difficult to gauge the advancement of verification tools.\\n\\nTraditionally, the oval20 benchmark (Lu & Kumar, 2020; De Palma et al., 2021b) consists of hard instances that can be solved using BaBSR (Bunel et al., 2020) with a long timeout threshold. Since the difficulty of all benchmark instances is known with the groundtruth, many different tools are evaluated on this benchmark and it was easy to see the progress of verifiers. However, the latest verifiers can solve this benchmark within a few seconds per instance (Zhang et al., 2022). We thus propose a new benchmark VeriHard in the same spirit, which contains instances that can be solved using state-of-the-art verifiers with a long timeout. Unlike oval20, the benchmark consists of a mix of MNIST, CIFAR-10, CIFAR-100, and TinyImageNet models, including large ResNets. By comparing the performance of different verifiers under a shorter timeout, it can be easily used to gauge the improvement of future verifiers. We provide more details regarding this benchmark in Appendix E.1.\\n\\nExperimental setup. Our implementation is based on the open-source $\\\\alpha,\\\\beta$-CROWN verifier, with our addition of BIG and subBIGs integrated. Since our method is independent of the actual BaB solver used, we use the popular $\\\\beta$-CROWN algorithm as our baseline BaB method. Since new BaB methods may also benefit from our algorithm, the improvements over the base BaB method are the most important metric. For BIG and subBIGs construction, we select top-$K$ implicant neurons to derive our paired bound implications with $K = 1000$. For subBIGs, we use first $M = 8$ splits by BaB and apply a 50-steps gradient descent with Adam optimizer to optimize our objective $l^*$ relaxed-multi on up to $2^M = 256$ unsolved subproblems. We leave more experimental details in Appendix E.2.\\n\\nVerification Results on VeriHard benchmarks. We evaluate our BIG-enhanced verifiers, BIG and subBIGs, on new proposed VeriHard benchmark. Instances included in VeriHard benchmark are designed to be so challenging that state-of-the-art verification algorithms must make a good effort (long timeout threshold) to verify them. For comparison, we include multiple strong verification tools including PRIMA (M\u00fcller et al., 2022b), MN-BaB (Ferrari et al., 2021) and $\\\\beta$-CROWN (Wang et al., 2021). From Table 2 we can see that, the percentage of verified instances is consistently lower than 50% even for strong verifiers, and our BIG-enhanced verifier could solve 5 to 8 more instances compared to the strongest baseline we have on MNIST, CIFAR-10 benchmarks. For CIFAR-100 and TinyImageNet benchmarks, with large ResNet models, BIG-enhanced verifier could noticeably reduce the explored branches compared to the $\\\\beta$-CROWN verifier as the baseline. Figure 3 shows the percentage of solved properties on VeriHard MNIST and CIFAR10 benchmarks vs. running time. Though all instances are hard enough and need to be verified through BaB with at least 150 seconds, BIG-enhanced verifier could consistently reduce the overall running time by reducing the overall branched neurons and branching iterations.\"}"}
{"id": "mMh4W72Hhe", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Verification Results on Existing Benchmarks.\\n\\nWe also evaluate BIG-enhanced verifiers on a few commonly used existing benchmarks, including MNIST benchmarks (CNN-A-Adv from SDP-FO (Dathathri et al., 2020) and ConvSmall from ERAN (Singh et al., 2019a) models) and CIFAR-10 benchmarks (CNN-A-Mix, CNN-A-Adv from SDP-FO and ConvSmall from ERAN models), and compare them with other strong verifier baselines. We also provide the verified accuracy upper bound derived by PGD attack as a reference. Results are shown in Table 3. We find that our method could consistently reduce the number of branches explored during BaB compared to the baseline \\\\(\\\\beta\\\\)-CROWN method, and the percentage of verified instances also consistently improved.\\n\\nTable 3: Percentage of verified instances, average runtime, and the number of BaB branches (lower is better) on existing benchmarks. We bold the highest verified percentage and the smallest number of branches.\\n\\n| Benchmark                  | Metrics          | CROWN/DeepPoly | PRIMA | SDP-FO | MN-BaB | Venus2 | \\\\(\\\\beta\\\\)-CROWN | BIG  | subBIGs | UpperBound |\\n|----------------------------|------------------|---------------|-------|--------|--------|--------|-----------------|------|---------|------------|\\n| MNIST-CNN-A-Adv            | Ver.\\\\%           | 0.0           | 43.0  | 42.0   | 44.0   | 47.0   | 50.0            | 50.0 | 53.0    | 58.0       |\\n|                            | Branches         | -             | -     | -      | -      | -      | 60324.6         | 55210.4 | 53147.1  | -          |\\n|                            | Time(s)          | 0.0           | 182.4 | 178.5  | 176.2  | 184.6  | 186.2           | 183.4 | 182.4   | 186.2      |\\n| MNIST-ConvSmall            | Ver.\\\\%           | 15.8          | 59.8  | -      | -      | 72.7   | 72.8            | 73.2 | 73.2    | 76.5       |\\n|                            | Branches         | -             | -     | -      | -      | -      | 2739.4          | 2510.4 | 2112.2  | -          |\\n|                            | Time(s)          | 3.0           | 42.0  | -      | -      | -      | 5.89            | 7.47 | 9.07    | -          |\\n| CIFAR10-CNN-A-Adv          | Ver.\\\\%           | 21.5          | 41.5  | 39.6   | 42.5   | 47.5   | 45.0            | 45.5 | 46.0    | 46.0       |\\n|                            | Branches         | -             | -     | -      | -      | -      | 20462.6         | 19340.6 | 12671.2 | -          |\\n|                            | Time(s)          | 0.5           | 4.8   | >25h   | 68.3   | 26.0   | 110.2           | 110.8 | 142.5   | -          |\\n| CIFAR10-CNN-A-Mix          | Ver.\\\\%           | 23.5          | 37.5  | 39.6   | 35.0   | 33.5   | 41.5            | 41.5 | 42.5    | 42.5       |\\n|                            | Branches         | -             | -     | -      | -      | -      | 22344.4         | 21996.8 | 20462.9 | -          |\\n|                            | Time(s)          | 0.4           | 34.3  | >25h   | 140.3  | 72.4   | 35.6            | 36.3 | 69.2    | -          |\\n| CIFAR10-ConvSmall          | Ver.\\\\%           | 35.9          | 44.6  | -      | -      | 46.3   | 46.3            | 46.5 | 73.2    | -          |\\n|                            | Branches         | -             | -     | -      | -      | -      | 5325.0          | 5072.0 | 4203.6  | -          |\\n|                            | Time(s)          | 4.0           | 13.0  | -      | -      | -      | 5.25            | 5.85 | 7.02    | -          |\\n\\nTable 4: The number of implicated neurons found in benchmarks of VeriHard dataset.\\n\\n| VeriHard Benchmark       | Bound Implications |\\n|--------------------------|--------------------|\\n| MNIST-A-Adv              | 934.25 1424.48     |\\n| CIFAR10-A-Adv            | 5603.46 8403.24    |\\n| CIFAR100-Medium          | 16624.05 20042.23  |\\n| TinyImageNet-Medium     | 15352.49 18023.23  |\\n\\nImpacts of BIG on branching.\\n\\nIn Table 4, we show the actual number of implicated neurons on BIG/subBIGs found by our algorithm in various networks, and the number is often large. In addition, we visualize how BIG helps branch and bound procedure in Figure 4, by showing the trends of 1) Global lower bounds; 2) Cumulative explored branches; 3) Used bound implications vs. the branching iterations. We compare our BIG-enhanced verifier (BIG, subBIGs) to the baseline \\\\(\\\\beta\\\\)-CROWN verifier. We can find that both BIG and subBIGs could steadily tighten the lower bound during BaB, and noticeably reduce the number of explored branches towards complete verification. In addition, during each branching round, we can fix several additional implicant neurons from unstable to stable to tighten the intermediate bounds thus global lower bounds. We leave more visualization results in Appendix F.\"}"}
{"id": "mMh4W72Hhe", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: {Left} Global lower bounds; {Middle} Total explored branches; {Right} Average used bound implications trends along with branching iterations on one specific instance in VeriHard-CIFAR10-A-Adv benchmark. More than one neurons are branched per iteration thanks to BIG, and the bounds and the number explored branches both improve noticeably. Baseline BaB methods like $\\\\beta$-CROWN can branch only one neuron per BaB iteration.\\n\\n1 RELATED WORK\\n\\nThe DNN verification methods are divided into two categories, complete (Katz et al., 2017; Tjeng et al., 2019; Xu et al., 2020b; Wang et al., 2021; Bunel et al., 2018) and incomplete (Zhang et al., 2018; Wong & Kolter, 2018; Wang et al., 2018a;b). Complete verification is proven NP-complete (Katz et al., 2017), and current verifiers are based on SMT (Katz et al., 2017), off-shelf solvers (Tjeng et al., 2019; Bunel et al., 2018; Dutta et al., 2017), and specialized branch-and-bound (BaB) (Bunel et al., 2018; Wang et al., 2021; Bunel et al., 2020). Branch-and-bound is usually the most effective technique in practice and has been used in top toolkits in the verification of neural network competitions (Bak et al., 2021; M\u00fcller et al., 2022a). On the other hand, incomplete verification is usually efficient with polynomial time complexity but is conservative. Existing incomplete verifiers are based on bound propagation (Wang et al., 2018a; Zhang et al., 2018; Wang et al., 2018b; Xu et al., 2020a; Gowal et al., 2019), abstract interpretation (Singh et al., 2019b; Gehr et al., 2018; M\u00fcller et al., 2022b; Singh et al., 2019a), convex relaxation (Palma et al., 2021; Tjandraatmadja et al., 2020), duality (Wong & Kolter, 2018; De Palma et al., 2021b; Dvijotham et al., 2019), and semi-definite programming (Chiu & Zhang, 2023; Anderson et al., 2020a; Dathathri et al., 2020; Batten et al., 2021; Raghunathan et al., 2018). Specifically, the cheap bound propagation technique can be efficiently combined with the branch-and-bound to accelerate complete verification (Xu et al., 2020b; Wang et al., 2021).\\n\\nNeuron dependency as a potential way to improve the tightness of bounds is also studied in a few papers (Anderson et al., 2020b; Palma et al., 2021; M\u00fcller et al., 2022b; Zhang et al., 2022), but they usually aim to improve the bounding step, and the implication relationships among ReLUs during branching were not discussed. Ehlers (2017) first discussed case-analysis-based search by recording the dependencies between ReLU splits to reduce the number of branches. The most relevant existing work is Venus and Venus2 (Botoeva et al., 2020; Kouvaros & Lomuscio, 2021), which also explores the dependency among unstable ReLU neurons, but with a weaker method to find neuron implications. First, for dependency from different layers (inter-layer), they only consider the case where a neuron in one layer is set to be inactive (0), and propagate the interval bounds to the layers following it; thus, only two out of four kinds of implications can be found, and the interval bounds can also too loose to find implications. Second, our method can find implications for neurons before the implicated neuron. For example, by setting a neuron in layer 3 to be active or inactive, we may imply that a neuron in layer 1 or 2 is active or inactive, impossible in the prior work. Third, for same-layer dependency, our methods use bound equations that are propagated to the input layer, which utilizes tighter bounds on the input $x_0$ rather than the relatively looser intermediate layer bounds. Finally, our formulation is optimization-based, allowing us to optimize and find the strongest implications. These benefits lead to significant improvements in finding neuron implications (even without considering subBIGs), as we demonstrated in Sec. 4.\\n\\n6 CONCLUSION\\n\\nIn this paper, we proposed a novel technique to quickly find bound implications for branch-and-bound-based neural network verification algorithms. Our optimization-based formulation is less restrictive and stronger than prior work, allowing us to find more implications to guide the branch-and-bound process. Our method has been demonstrated on multiple benchmarks with consistent improvements in terms of verified accuracy and the number of branches.\"}"}
{"id": "mMh4W72Hhe", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Branching iterations\\n\\n|             | \\\\(-\\\\text{CROWN}\\\\) | \\\\(\\\\text{BIG}\\\\) | \\\\(\\\\text{subBIGs}\\\\) |\\n|-------------|------------------|----------------|---------------------|\\n|             | 40               | 42             | 44                |\\n|             | 0.40             | 0.35           | 0.30              |\\n|             | 0.25             | 0.20           | 0.15              |\\n|             | 0.10             | 0.05           | 0.00              |\\n\\nGlobal lower bound\\n\\nExplore branches\\n\\n|             | \\\\(-\\\\text{CROWN}\\\\) | \\\\(\\\\text{BIG}\\\\) | \\\\(\\\\text{subBIGs}\\\\) |\\n|-------------|------------------|----------------|---------------------|\\n|             | 40000            | 45000          | 50000              |\\n|             | 55000            | 60000          |                     |\\n\\nAverage branched neurons\\n\\n|             | \\\\(-\\\\text{CROWN}\\\\) | \\\\(\\\\text{BIG}\\\\) | \\\\(\\\\text{subBIGs}\\\\) |\\n|-------------|------------------|----------------|---------------------|\\n|             | 1                | 2              | 3                  |\\n|             | 4                | 5              | 6                  |\\n|             | 7                | 8              |                     |\\n\\nFigure 7: (Left) Global lower bounds; (Middle) Total explored branches; (Right) Average used bound implications trends along with branching iterations on one specific instance in VeriHard - CIFAR10-A-Mix benchmark.\\n\\nPlane constraints could also help reduce the number of branches within verification. We conduct experiments on showing the average number of branches on various benchmarks by utilizing \\\\(\\\\beta\\\\)-CROWN, GCP-CROWN, BIG, and the combined approach GCP-CROWN+BIG. Results are shown in Table 5. It is worth noting that GCP-CROWN heavily relies on a MIP solver to find cutting planes, and for CIFAR100 and TinyImageNet models, GCP-CROWN cannot provide useful cutting planes due to its high computational cost.\\n\\nTable 5: Total number of branches (less is better) on MNIST, CIFAR10, CIFAR100, and TinyImageNet benchmarks. GCP-CROWN+BIG could further reduce the number of branches compared with GCP-CROWN only. Note that GCP-CROWN cannot scale to CIFAR100 and TinyImageNet, yet our bound implications can still bring improvements.\\n\\n|                     | \\\\(-\\\\text{CROWN}\\\\) | \\\\(\\\\text{BIG}\\\\) | \\\\(\\\\text{GCP-CROWN}\\\\) | \\\\(\\\\text{GCP-CROWN+BIG}\\\\) |\\n|---------------------|------------------|----------------|---------------------|--------------------------|\\n| **MNIST-CNN-A-Adv** | 53120.9          | 22250.4        | 45130.4             | 21230.4                  |\\n| **CIFAR10-CNN-A-Mix** | 50614.2          | 24103.7        | 46132.4             | 23792.5                  |\\n| **CIFAR10-CNN-A-Adv** | 55764.8          | 28607.1        | 50014.2             | 28204.6                  |\\n| **CIFAR100-small**  |                  | 70006.6        | 74113.5             |                          |\\n| **TinyImageNet-medium** | 64253.9        |                | 60082.5             |                          |\\n\\nFrom this table we can see that GCP-CROWN+BIG could further reduce branch counts compared to GCP-CROWN alone. Note that GCP-CROWN cannot scale to CIFAR100 and TinyImageNet, yet our bound implications can still bring improvements.\\n\\nWe also exported the cutting planes from GCP-CROWN and compared them to our implications on the same set of instances. We report the average number of implications found by us, the average number of cutting planes in GCP-CROWN, and the overlaps. Here overlap means that GCP-CROWN finds a constraint mentioning the same pair of two neurons as we found as implicated and implicant neurons. Results are shown in Table 6.\\n\\n|                     | \\\\(-\\\\text{CROWN}\\\\) | \\\\(\\\\text{BIG}\\\\) | \\\\(\\\\text{GCP-CROWN}\\\\) |\\n|---------------------|------------------|----------------|---------------------|\\n| **MNIST-CNN-A-Adv** | 574.2            | 761.7          | 102.2               |\\n| **CIFAR10-CNN-A-Mix** | 683.7          | 1103.3         | 89.9                |\\n| **CIFAR10-CNN-A-Adv** | 796.9          | 1289.7         | 104.2               |\\n\\nAs we can see, the implications only take a minority of the overall implications we found with our method. This is expected because GCP-CROWN uses a MIP solver to generate cutting planes, which is unaware of the underlying neural network architecture and uses generic methods (such as Gomory cuts) to find implications among variables.\\n\\n**LIMITATIONS**\\n\\nOur method suffers from several limitations which are commonly existed by most existing BaB-based verification methods (Wang et al., 2021; Ferrari et al., 2021; Zhang et al., 2022; Xu et al., 2020b), and is limited by the capability of existing verifiers, which focuses on branch-and-bound.\"}"}
{"id": "mMh4W72Hhe", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024 for ReLU networks only. For other non-linear activations and non-perception networks (such as Transformers), applying our method to the very recent work on BaB-based verification for general non-linear functions (Shi et al., 2023) could be an interesting future work. Our method also cannot support the verification of very large neuron networks, such as NNs with billions of parameters. Improving the overall computational efficiency could be our future direction.\"}"}
{"id": "mMh4W72Hhe", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Construct BIG Algorithm 3\\n\\nUnder review as a conference paper at ICLR 2024\\n\\n\\\\[ \\\\text{for} \\\\]\\n\\\\[ \\\\text{do} \\\\]\\n\\\\[ \\\\text{if} \\\\]\\n\\\\[ \\\\text{then} \\\\]\\n\\\\[ \\\\text{else} \\\\]\\n\\\\[ \\\\text{if} \\\\]\\n\\\\[ \\\\text{then} \\\\]\\n\\\\[ \\\\text{else} \\\\]\\n\\\\[ \\\\text{if} \\\\]\\n\\\\[ \\\\text{else} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text{add edge} \\\\]\\n\\\\[ \\\\text"}
{"id": "mMh4W72Hhe", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generate subproblems in one BaB iteration with BIG.\\n\\n1: Inputs: \\\\( G(V, E) \\\\), selected unstable neuron \\\\( z_i^j \\\\)\\n\\n2: Outputs: BIG-generated subproblems.\\n\\n3: \\\\( S_P \\\\leftarrow \\\\{ P(i^j) \\\\} \\\\), \\\\( S_Q \\\\leftarrow \\\\{ Q(i^j) \\\\} \\\\) \u25b7 traverse BIG starting from node \\\\( P(i^j) \\\\) and \\\\( Q(i^j) \\\\)\\n\\n4: while \\\\( S_P \\\\) has unvisited nodes do\\n\\n5: Pick a unvisited node \\\\( X(i'^{j'}) \\\\) out from \\\\( S_P \\\\)\\n\\n6: for \\\\( e = (X(i'^{j'}), Y(i^*j^*)) \\\\in E \\\\) do\\n\\n7: \\\\( S_P \\\\leftarrow S_P \\\\cup \\\\{ Y(i^*j^*) \\\\} \\\\)\\n\\n8: Mark node \\\\( X(i'^{j'}) \\\\) as visited\\n\\n9: while \\\\( S_Q \\\\) has unvisited nodes do\\n\\n10: Pick a unvisited node \\\\( X(i'^{j'}) \\\\) out from \\\\( S_Q \\\\)\\n\\n11: for \\\\( e = (X(i'^{j'}), Y(i^*j^*)) \\\\in E \\\\) do\\n\\n12: \\\\( S_Q \\\\leftarrow S_Q \\\\cup \\\\{ Y(i^*j^*) \\\\} \\\\)\\n\\n13: Mark node \\\\( X(i'^{j'}) \\\\) as visited\\n\\n14: Add splits \\\\( z(i'^{j'}) \\\\geq 0 \\\\) (or \\\\( z(i'^{j'}) \\\\leq 0 \\\\)) from nodes in \\\\( S_P \\\\) to subproblem \\\\( z(i^j) \\\\geq 0 \\\\)\\n\\n15: Add splits \\\\( z(i'^{j'}) \\\\leq 0 \\\\) (or \\\\( z(i'^{j'}) \\\\leq 0 \\\\)) from nodes in \\\\( S_Q \\\\) to subproblem \\\\( z(i^j) \\\\leq 0 \\\\)\"}"}
{"id": "mMh4W72Hhe", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The key to improving the efficiency of BaB-based verifiers is to reduce the number of subproblems created. Several works (Kouvaros & Lomuscio, 2021; Botoeva et al., 2020) considered the correlations (implications) among ReLU neurons: when a neuron is branched into the active (or inactive) case, it may imply the active (or inactive) status of other neurons; a graph can then be formed based on these bound implications (dependencies). Although bound implications may exist between any two neurons (e.g., a ReLU in the last layer of NN is active may imply that a ReLU in the first layer should always be inactive), existing work such as (Kouvaros & Lomuscio, 2021; Botoeva et al., 2020) can only detect some limited form of implications between neurons, such as among neurons in the same layer or in consecutive layers, and simple approaches such as interval bounds are used to find implications.\\n\\nIn our work, we first formulate the problem of finding bound implications as an optimization problem, which leads to a stronger formulation than previous works, and opportunities to find many more implications. This problem involves a challenging non-convex constraint, and we must avoid the usage of linear programming (LP) or mixed integer programming (MIP) solver because of the large number of pairs of ReLU neurons involved in finding these implications. To address this challenge, we novelly reuse the pre-computed variables in existing bound-propagation-based verification methods such as $\\\\alpha$-CROWN and solve a simple LP problem either in closed form solution or via gradient descent, without relying on an expensive LP solver. We can quickly find hundreds to thousands of bound implications between neurons using our proposed approach, significantly more compared to previous approaches. In addition, by utilizing common split constraints in BaB, we can build multiple bound implication graphs (BIGs), each for a subset of subproblems, to further improve the effectiveness of bound implications. Our main technical contributions are:\\n\\n\u2022 We formulate the process of finding bound implications as an optimization problem, which allows us to find correlations between two neurons from arbitrary layers, not restricted to implications in the same or consecutive layers in previous work;\\n\u2022 We propose a novel approach to reuse information from already computed in bound propagation (e.g., $\\\\alpha$-CROWN) to quickly solve these optimization problems to find bound implications between thousands of pairs of ReLU neurons within a few seconds. Combined with a unique filtering strategy, our approach is scalable to large DNNs (such as 19 layer ResNets on TinyImageNet).\\n\u2022 Our optimization formulation is much stronger compared to existing simple approaches to find bound implications: we found 7 to 170 times more implications among neurons on average than previous work (Kouvaros & Lomuscio, 2021; Botoeva et al., 2020).\\n\u2022 When evaluated on robustness verification benchmarks on MNIST, CIFAR-10, CIFAR-100, and TinyImageNet datasets, including new benchmarks consisting of hard verification problems, we save up to 37% subproblems during BaB, consistently reduce the verification time, and verify more problems compared to baseline BaB methods such as $\\\\beta$-CROWN and MN-BaB.\\n\\nBACKGROUND\\n\\nDNN verification methods are developed to rigorously prove formal specifications on DNNs. We let $x \\\\in X \\\\subseteq \\\\mathbb{R}^{d_0}$ to denote the input, and let $f : \\\\mathbb{R}^{d_0} \\\\rightarrow \\\\mathbb{R}$ denote a neural network. For an $L$-layer neural network, $f$ is defined by\\n\\n$$f(x) = z_L(x)$$\\n\\nwhere $z_i(x) = W_i \\\\hat{z}_{i-1}(x) + b_i$ and $\\\\hat{z}_i(x) = \\\\sigma(z_i(x))$. Specifically, $\\\\hat{z}_{i-1}(x)$ is the pre-activation and $\\\\hat{z}_i(x)$ is the post-activation values of the $j$-th neuron in the $i$-th layer, $j \\\\in [d_i]$ and $d_i$ is the number of neurons of the $i$-th layer. Canonically, verification specifications can be written as proving $f(x) > 0$, $\\\\forall x \\\\in C$; i.e., we say $f(x)$ is verified if $f(x) > 0$, $\\\\forall x \\\\in C$. DNN verification methods solve the following optimization problem:\\n\\n$$\\\\min_{x \\\\in C} f(x) = z_L(x)$$\\n\\ns.t. $z_i(x) = W_i \\\\hat{z}_{i-1}(x) + b_i$, $\\\\hat{z}_i(x) = \\\\sigma(z_i(x))$, $x \\\\in C$, $i \\\\in [L-1]$. (1)\\n\\nThe $C$ defines the input region, and we focus on $\\\\ell_\\\\infty$-bounded perturbations following the literature (Wong & Kolter, 2018; Xu et al., 2020b; Wang et al., 2021; M\u00fcller et al., 2022b). Therefore, given a clean input instance $x_0$ and the perturbation magnitude $\\\\epsilon$, $C := \\\\{x | \\\\|x - x_0\\\\|_\\\\infty \\\\leq \\\\epsilon\\\\}$. Bound propagation methods.\\n\\nBound propagation methods like CROWN (Zhang et al., 2018) and $\\\\beta$-CROWN (Wang et al., 2021) aim to calculate a lower bound for $f_L(x)$ without using the LP solver:\"}"}
{"id": "mMh4W72Hhe", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: (a) Bound implications: based on the linear lower bounds of two neurons, if one neuron is set to inactive ($<0$), the other neuron will always be active ($>0$) under constraints; (b) the two neurons are connected by an edge on the Bound Implication Graph (BIG), and (c) more bound implications create a larger BIG with multiple components. (d) During branch and bound (BaB), BIG is used to remove unnecessary subproblems, so fewer subproblems are needed and verification time is saved.\\n\\nWith $i = L - 1$ the above holds trivially with $a_{(L-1)} = W (L)$, $c_{(L-1)} = b (L)$. Propagating bounds from the $L$-layer to the previous layer repeatedly will eventually reach the input layer:\\n\\n$$\\\\min_{x \\\\in C} f_L(x) \\\\geq \\\\min_{x \\\\in C} a(0) \\\\top x + c(0)$$\\n\\nThis linear optimization problem can be easily solved to provide a lower bound for $f(x)$. Eq. (3) are usually also applied to intermediate layer neurons. In our paper, we will reuse the variables $a, c$ which are already available during bound propagation to build our bound implication graph efficiently.\\n\\nActive, inactive, and unstable neurons. To leverage the branch-and-bound technique, we need to know the pre-activation bounds $l(i)$ and $u(i)$ for the inputs of each ReLU layer, such that $l(i) \\\\leq x(i) \\\\leq u(i)$. Those bounds are valid for $x \\\\in C$, and can be obtained via cheap bound propagation methods such as IBP (Gowal et al., 2019), CROWN (Zhang et al., 2018) or $\\\\alpha$-CROWN (Xu et al., 2020b). Then ReLU neurons for each layer $i$ can be classified into three classes (Wong & Kolter, 2018): $l(i)j \\\\geq 0$ (active neurons), $u(i)j \\\\leq 0$ (inactive neurons), and $l(i)j \\\\leq 0, u(i)j \\\\geq 0$ (unstable neurons). Activate and inactive neurons are already linear functions, so only unstable neurons require branching. In Section 3, we will show that after branching on some neurons, some previously unstable neurons become stable (active and inactive), and we will leverage these bound implications to avoid unnecessary branching on already-stable neurons, which results in a more efficient BaB process.\\n\\n3 Finding Bound Implications for NN Verification\\n\\nOverview. In this section, we introduce our optimization-based formulation and give an efficient procedure to quickly generate valid bound implications among all pairs of unstable neurons from any layers. We do this by reusing coefficients in bound propagation when intermediate layer bounds are computed before BaB, and solving simple linear programming (LP) problems efficiently without relying on an external solver. With the large number of implications found by our novel algorithm, we build a bound implication graph (BIG) (similar to the dependency graph in (Kouvaros & Lomuscio, 2021)) to accelerate the BaB process. We also demonstrate that multiple BIGs can be built for different subsets of subproblems during BaB, further improving its effectiveness.\\n\\nBound implication: intuitions with linear bounds. When assuming one neuron's bound to be within a smaller range (e.g., constraining an unstable neuron to be active or inactive), the bounds of another neuron can be conditionally tightened, and may become an active or inactive neuron. For example, considering two neurons at any two layers $i_1, i_2$ with pre-calculated linear inequality coefficients and biases by some popular verifiers like CROWN or $\\\\alpha$-CROWN:\\n\\n$$z(i_1)j_1 \\\\geq 2x + 1; z(i_2)j_2 \\\\geq -x + 0.$$ \\n\\nGiven the one-dimensional input perturbation $-1 \\\\leq x \\\\leq 1$, both neurons are unstable since their bounds can either be positive or negative. However, if we consider the subproblem where $z(i_1)j_1 \\\\leq 0$, i.e., this neuron is split into the inactive case so consequentially $2x + 1 \\\\leq 0$, then we know $x \\\\leq -0.5$, hence the range of $z(i_2)j_2$ improves to $1 \\\\leq z(i_2)j_2 \\\\leq 1.5$, accordingly, becoming an active neuron. In other words, if we split $z(i_1)j_1$ to be inactive, then $z(i_2)j_2$ will automatically become active, and such conditions commonly occur in BaB. In this example, the number of subproblems can be potentially reduced by $1/4$ (excluding one configuration of the four possibilities), saving BaB time.\"}"}
{"id": "mMh4W72Hhe", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Theorem 3.1. Given two unstable neurons and the LP formulations in Eq. (4), (5) and (6), the\\nTheorem 3.1 shows that our tractable formulation Eq. (6) always produces a valid lower bound for\\nThis LP problem has a special structure that we can explicitly construct an algorithm to solve Eq. (6)\\nvariable whose value can be sorted and searched. The algorithm is presented in Appendix C.1.\\nconstructed for each pair of neurons and we gave these cases in Appendix A.\\nimplicant and the\\nlower bound. In fact, there are four possible ways that two neurons may interact (inactive\\ninactive\\ncase implies an improved\\npotentially improved given the implicant\\nthe solution of the intractable Eq. (5), and the lower bound of the implicated neuron,\\nfollowing holds: (1) The LP in Eq. (6) is always feasible; (2)\\n\\n\\\\[ l \\\\geq 0 \\\\]\\n\\nIt has a closed-form solution in\\n\\n\\\\[ x = \\\\min_{C} \\\\left\\\\{ x \\\\mid s.t. \\\\right\\\\} \\\\]\\n\\nDue to symmetry, we focus on one case, needed. Here\\n\\n\\\\[ \\\\alpha \\\\]\\n\\nset a single unstable neuron's pre-activation bound\\n\\nFinding bound implications: an optimization-based method.\\nA naive way to find the implied\\n\\n\\\\[ \\\\beta (\\\\alpha) \\\\]\\n\\nafter\\n\\ncorrelations between neurons is to directly apply a bound propagation method such as\\n\\n\\\\[ \\\\gamma \\\\]\\n\\napproach is its scalability \u2014 given that there might be thousands of unstable neurons in a network, we\\n\\n\\\\[ \\\\delta \\\\]\\n\\nlayer\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\nupper\\n\\n\\\\[ \\\\zeta \\\\]\\n\\nbound of the implicated neuron), so four LPs like Eq. (6) can be\\n\\n\\\\[ \\\\eta \\\\]\\n\\ncan be potentially improved with the additional constraint\\n\\n\\\\[ \\\\theta \\\\]\\n\\ncan be potentially improved given the implicant\\n\\n\\\\[ \\\\iota \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\kappa \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\lambda \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\mu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\nu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\omega \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\pi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\rho \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\sigma \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\tau \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\upsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\phi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\chi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\psi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\theta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\vartheta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\varepsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\zeta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\eta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\iota \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\kappa \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\lambda \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\mu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\nu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\omega \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\pi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\rho \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\sigma \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\tau \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\upsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\phi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\chi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\psi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\theta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\vartheta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\varepsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\zeta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\eta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\iota \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\kappa \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\lambda \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\mu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\nu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\omega \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\pi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\rho \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\sigma \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\tau \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\upsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\phi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\chi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\psi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\theta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\vartheta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\varepsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\zeta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\eta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\iota \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\kappa \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\lambda \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\mu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\nu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\omega \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\pi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\rho \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\sigma \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\tau \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\upsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\phi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\chi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\psi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\theta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\vartheta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\varepsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\zeta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\eta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\iota \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\kappa \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\lambda \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\mu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\nu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\omega \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\pi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\rho \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\sigma \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\tau \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\upsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\phi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\chi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\psi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\theta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\vartheta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\varepsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\zeta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\eta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\iota \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\kappa \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\lambda \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\mu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\nu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\omega \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\pi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\rho \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\sigma \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\tau \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\upsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\phi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\chi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\psi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\theta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\vartheta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\varepsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\zeta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\eta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\iota \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\kappa \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\lambda \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\mu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\nu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\omega \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\pi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\rho \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\sigma \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\tau \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\upsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\phi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\chi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\psi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\theta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\vartheta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\varepsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\zeta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\eta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\iota \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\kappa \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\lambda \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\mu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\nu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\omega \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\pi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\rho \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\sigma \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\tau \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\upsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\phi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\chi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\psi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\theta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\vartheta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\varepsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\zeta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\eta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\iota \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\kappa \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\lambda \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\mu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\nu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\omega \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\pi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\rho \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\sigma \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\tau \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\upsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\phi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\chi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\psi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\theta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\vartheta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\varepsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\zeta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\eta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\iota \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\kappa \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\lambda \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\mu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\nu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\omega \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\pi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\rho \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\sigma \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\tau \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\upsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\phi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\chi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\psi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\theta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\vartheta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\varepsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\zeta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\eta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\iota \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\kappa \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\lambda \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\mu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\nu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\omega \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\pi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\rho \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\sigma \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\tau \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\upsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\phi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\chi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\psi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\theta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\vartheta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\varepsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\zeta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\eta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\iota \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\kappa \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\lambda \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\mu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\nu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\omega \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\pi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\rho \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\sigma \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\tau \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\upsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\phi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\chi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\psi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\theta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\vartheta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\varepsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\zeta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\eta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\iota \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\kappa \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\lambda \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\mu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\nu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\omega \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\pi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\rho \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\sigma \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\tau \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\upsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\phi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\chi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\psi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\theta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\vartheta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\varepsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\zeta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\eta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\iota \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\kappa \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\lambda \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\mu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\nu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\omega \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\pi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\rho \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\sigma \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\tau \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\upsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\phi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\chi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\psi \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\theta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\vartheta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\varepsilon \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\zeta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\eta \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\iota \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\kappa \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\lambda \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\mu \\\\]\\n\\nimplicant neuron\\n\\n\\\\[ \\\\nu \\\\]\\n"}
{"id": "mMh4W72Hhe", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When the objective does not improve, the relaxed implicant constraint is redundant with two cases: whose bounds cannot be improved based on condition (7). Finally, we solve Eq. (6) using our filtering out unuseful ones. We first present a theorem about the objective improvement in (6):\\n\\nTheorem 3.2. Given two unstable neurons and the LP formulations in Equation (4), (6), if the x\\n\\nTheorem constructs a solution z\\n\\ninactive. For example, after obtaining improved bounds for E\\n\\nrespectively). P\\n\\nor inactive cases, node | is the node set with cardinality | is more likely to cut off more feasible regions in Eq. (6). Based on the ranking, we select l\\n\\nconstraint to l\\n\\nwere exactly 0. l\\n\\nare from near to far, since a constraint\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimplicant\\n\\nimp"}
