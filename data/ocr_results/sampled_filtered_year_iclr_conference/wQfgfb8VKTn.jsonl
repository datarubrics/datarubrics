{"id": "wQfgfb8VKTn", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"action-observation spaces? (Sec. 6.3) For results in this section, we show the median performance with 8 random seeds as well as the 25-75% percentiles.\\n\\n6.1 Graph Parseness\\n\\nTable 2: Percentage of communication saved for each task.\\n\\n| Task                  | Aloha Pursuit | Hallway | Sensor Gather | Disperse |\\n|-----------------------|--------------|---------|---------------|----------|\\n|                       | 80.0%        | 70.0%   | 50.0%         |          |\\n|                       | 90.0%        | 30.0%   | 60.0%         |          |\\n\\nAn important advantage of learning sparse coordination graphs is reduced communication costs. The complexity of running Max-Sum for each action selection is $O(k|V||A| + |E||A|^2)$, where $k$ is the number of iterations of message passing. Sparse graphs cut down communication costs by reducing the number of edges.\\n\\nWe carry out a grid search to find the communication threshold under which sparse graphs have the best performance. We find that most implementations of dynamically sparse graphs require similar numbers of edges to prevent performance from dropping significantly. In Table 2, we show the communication cut rates we use when benchmarking our method. Generally speaking, non-factored games typically require more messages than factored games, while, for most tasks, at least 50% messages can be saved without sacrificing the learning performance.\\n\\n![Figure 4: The influence of graph sparseness (1.0 represents complete graphs) on the performance on factored games (Sensor, left) and non-factored games (Gather, right).](image)\\n\\nIn Fig. 4, we show the performance of our method under different communication thresholds which control the sparseness of edges. We can observe that, on the factored game Sensor, performance first grows then drops when more edges are included in the coordination graphs. These observations are in line with the fact that sparse graphs can outperform complete graphs and fully-decomposed value functions on this task. In contrast, for the non-factored game Gather, performance stabilizes beyond a certain threshold. Non-factored games usually involve complex coordination relationships, and denser topologies are suitable for this type of questions.\\n\\n6.2 MACO: Multi-Agent Coordination Benchmark\\n\\nWe compare our method with state-of-the-art fully-decomposed value-based methods (VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018), and Weighted QMIX (Rashid et al., 2020)), coordination graph learning method (DCG (B\u00f6hmer et al., 2020)), and attentional graph learning methods (DICG (Li et al., 2021) and GraphMIX (Naderializadeh et al., 2020)) on MACO (Fig. 3). Since the number of actions is not very large in MACO, we do not use action representations when estimating the utility and payoff function for CASEC.\\n\\nWe can see that our method significantly outperforms fully-decomposed value-based methods. The reason is that fully-decomposed methods suffer from the relative overgeneralization issue and miscoordination problems in partially observable environments with stochasticity. For example, on task Pursuit (Benda, 1986), if more than one agent catches one prey simultaneously, these agents will be rewarded 1. However, if only one agent catches prey, it fails and gets a punishment of -1. For an agent with a limited sight range, the reward it obtains when taking the same action (catching a prey) under the same local observation depends on the actions of other agents and changes dramatically. This is the relative overgeneralization problem. Another example is Hallway (Wang et al., 2020), where several agents need to reach a goal state simultaneously without knowing each other's location. Fully-decomposed methods cannot solve this problem if the initial positions of agents are stochastic.\\n\\nFor DCG, we use its default settings of complete graphs and no low-rank approximation. We observe that DCG is less effective on tasks characterized by sparse coordination interdependence like Sensor. We hypothesize this is because coordinating actions with all other agents requires the shared estimator to express payoff functions of most agent pairs accurately enough, which needs more samples to learn, hurting the performance of DCG on loosely coupled tasks.\"}"}
{"id": "wQfgfb8VKTn", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"We compare our method against the state-of-the-art coordination graph learning method (DCG (B\u00f6hmer et al., 2020)) and fully decomposed value-based MARL algorithms (VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018)). For CASEC, we use action representations to estimate the payoff function. We train the action encoder for 50k samples and keep action representations unchanged afterward. In Fig. 5, we show results on 5m_vs_6m and MMM2. Detailed hyperparameter settings of our method can be found in Appendix E.\\n\\nFor DCG, we use its default settings, including a low-rank approximation for learning the payoff function. We can see that CASEC outperforms DCG by a large margin. The result proves that sparse coordination graphs provide better scalability to large action-observation spaces than dense and static graphs. In DCG\u2019s defense, low-rank approximation still induces large estimation errors. We replace low-rank approximation with action representations and find that DCG (Full (action repr.)) achieves similar performance to CASEC after 5M steps, but CASEC is still more sample-efficient. Moreover, taking advantage of higher-order value decomposition, CASEC is able to represent more complex coordination dynamics than fully decomposed value functions and thus performs better.\\n\\nAblation study\\nOur method is characterized by two contributions: context-aware sparse topologies and action representations for learning the utility and payoff function. In this section, we design ablations to show their contributions.\\n\\nThe effect of sparse topologies can be observed by comparing CASEC to Full (action repr.), which is the same as CASEC other than using complete coordination graphs. We observe that sparse graphs enjoy better sample efficiency than full graphs, and the advantage becomes less obvious as more samples are collected. This observation indicates that sparse graphs introduce inductive biases that can accelerate training, and their representational capacity is similar to that of full graphs.\\n\\nFrom the comparison between CASEC to CASEC using conventional Q networks (w/o action repr.), we can see that using action representations can significantly stabilize learning. For example, learning diverges on 5m_vs_6m without action representations. As analyzed before, this is because a negative feedback loop is created between the inaccurate payoff function and coordination graphs.\\n\\nTo further consolidate that action representations can reduce the estimation errors and thus alleviate learning oscillation as discussed in Sec. 3.3, we visualize the TD errors of CASEC and ablations during training in Fig. 5 right. We can see that action representations can dramatically reduce the TD errors. For comparison, the low-rank approximation can also reduce the TD errors, but much less significantly. Smaller TD errors prove that action representations provide better estimations of the value function, and learning with sparse graphs can thus be stabilized (Fig. 5 left).\\n\\nCONCLUSION\\nWe study how to learn dynamic sparse coordination graphs, which is a long-standing problem in cooperative MARL. We propose a specific implementation and theoretically justify it. Empirically, we evaluate the proposed method on a new multi-agent coordination benchmark. Moreover, we equip our method with action representations to improve the sample efficiency of payoff learning and stabilize training. We show that sparse and adaptive topologies can largely reduce communication overhead as well as improve the performance of coordination graphs. We expect our work to extend MARL to more realistic tasks with complex coordination dynamics.\"}"}
{"id": "wQfgfb8VKTn", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"One limitation of our method is that the learned sparse graphs are not always cycle-free. Since the Max-Sum algorithm guarantees optimality only on acyclic graphs, our method may select sub-optimal actions. In Appendix F, we study this problem in depth.\\n\\nAnother limitation is that we fix the communication threshold when training. It is an important question how to automatically and accurately find the minimum threshold that can guarantee the learning performance. In Appendix I, we study two ways to adaptively select the threshold.\\n\\nReproducibility\\n\\nThe source code for all the experiments along with a README file with instructions on how to run these experiments is attached in the supplementary material. In addition, the settings and parameters for all models and algorithms mentioned in the experiment section are detailed in Appendix E.\\n\\nREFERENCES\\n\\nMiroslav Benda. On optimal cooperation of knowledge sources: an empirical investigation. Technical Report, Boeing Advanced Technology Center, 1986.\\n\\nWendelin B\u00f6hmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In Proceedings of the 37th International Conference on Machine Learning, 2020.\\n\\nJacopo Castellini, Frans A Oliehoek, Rahul Savani, and Shimon Whiteson. The representational capacity of action-value networks for multi-agent reinforcement learning. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1862\u20131864. International Foundation for Autonomous Agents and Multiagent Systems, 2019.\\n\\nShanjun Cheng. Coordinating decentralized learning and conflict resolution across agent boundaries. PhD thesis, The University of North Carolina at Charlotte, 2012.\\n\\nKyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1724\u20131734, 2014.\\n\\nCaroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. AAAI/IAAI, 1998(746-752):2, 1998.\\n\\nJakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.\\n\\nCarlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent planning with factored mdps. In Advances in neural information processing systems, pp. 1523\u20131530, 2002a.\\n\\nCarlos Guestrin, Michail Lagoudakis, and Ronald Parr. Coordinated reinforcement learning. In ICML, volume 2, pp. 227\u2013234. Citeseer, 2002b.\\n\\nEric A Hansen, Daniel S Bernstein, and Shlomo Zilberstein. Dynamic programming for partially observable stochastic games. In AAAI, volume 4, pp. 709\u2013715, 2004.\\n\\nJelle R Kok and Nikos Vlassis. Collaborative multiagent reinforcement learning by payoff propagation. Journal of Machine Learning Research, 7(Sep):1789\u20131828, 2006.\\n\\nVictor Lesser, Charles L Ortiz Jr, and Milind Tambe. Distributed sensor networks: A multiagent perspective, volume 9. Springer Science & Business Media, 2012.\\n\\nSheng Li, Jayesh K Gupta, Peter Morales, Ross Allen, and Mykel J Kochenderfer. Deep implicit coordination graphs for multi-agent reinforcement learning. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, pp. 764\u2013772, 2021.\\n\\nRyan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pp. 6379\u20136390, 2017.\"}"}
{"id": "wQfgfb8VKTn", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomization and probabilistic techniques in algorithms and data analysis. Cambridge university press, 2017.\\n\\nNavid Naderializadeh, Fan H Hung, Sean Soleyman, and Deepak Khosla. Graph convolutional value decomposition in multi-agent reinforcement learning. arXiv preprint arXiv:2010.04740, 2020.\\n\\nJulius Nagy. \u00dcber algebraische gleichungen mit lauter reellen wurzeln. Jahresbericht der Deutschen Mathematiker-Vereinigung, 27:37\u201343, 1918.\\n\\nFrans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs, volume 1. Springer, 2016.\\n\\nLiviu Panait, Sean Luke, and R Paul Wiegand. Biasing coevolutionary search for optimal multiagent behaviors. IEEE Transactions on Evolutionary Computation, 10(6):629\u2013645, 2006.\\n\\nJudea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Elsevier, 2014.\\n\\nHuy Xuan Pham, Hung Manh La, David Feil-Seifer, and Aria Nefian. Cooperative and distributed reinforcement learning of drones for field coverage, 2018.\\n\\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4292\u20134301, 2018.\\n\\nTabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 33, 2020.\\n\\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.\\n\\nKyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 5887\u20135896, 2019.\\n\\nPeter Stone and Manuela Veloso. Multiagent systems: A survey from a machine learning perspective. Autonomous Robots, 8(3):345\u2013383, 2000.\\n\\nRuben Stranders, Alessandro Farinelli, Alex Rogers, and Nick Jennings. Decentralised coordination of mobile sensors using the max-sum algorithm. In Twenty-First International Joint Conference on Artificial Intelligence, 2009.\\n\\nPeter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 2085\u20132087. International Foundation for Autonomous Agents and Multiagent Systems, 2018.\\n\\nMing Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the tenth international conference on machine learning, pp. 330\u2013337, 1993.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998\u20136008, 2017.\\n\\nJianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling multi-agent q-learning. International Conference on Learning Representations (ICLR), 2021a.\"}"}
{"id": "wQfgfb8VKTn", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In CASEC, each agent has a neural network to estimate its local utility. The local utility network consists of three layers\u2014a fully-connected layer, a 64 bit GRU, and another fully-connected layer\u2014and outputs an estimated utility for each action. The utility difference function is also a 3-layer network, with the first two layers shared with the local utility function to process local action-observation history. The input to the third layer (a fully-connected layer) is the concatenation of the output of two agents' GRU layer. The local utilities and pairwise utility differences are summed to estimate the global action value (Eq. 11 in the paper).\\n\\nFor all experiments, the optimization is conducted using RMSprop with a learning rate of $5 \\\\times 10^{-4}$, $\\\\alpha$ of 0.99, RMSProp epsilon of 0.00001, and with no momentum or weight decay. For exploration, we use $\\\\epsilon$-greedy with $\\\\epsilon$ annealed linearly from 1.0 to 0.05 over 50K time steps and kept constant for the rest of the training. Batches of 32 episodes are sampled from the replay buffer. The default iteration number of the Max-Sum algorithm is set to 5. The communication threshold depends on the number of agents and the task, and we set it to $0.3$ on the map $5m_{vs}6m$ and $0.35$ on the map MAM2.\\n\\nWe test the performance with different values ($1e^{-3}$, $1e^{-4}$, and $1e^{-5}$) of the scaling weight of the sparseness loss $L^q_{\\\\text{sparse}}$ on Pursuit, and set it to $1e^{-4}$ for both the MACO and SMAC benchmark.\\n\\nThe whole framework is trained end-to-end on fully unrolled episodes. All experiments on StarCraft II use the default reward and observation settings of the SMAC benchmark.\\n\\nAll the experiments are carried out on NVIDIA Tesla P100 GPU. We show the estimated running time of our method on different tasks in Table 3 and 4. Typically, CASEC can finish 1M training steps within 8 hours on MACO tasks and in about 10 hours on SMAC tasks. In Table 5, we compare the computational complexity of action selection for CASEC and DCG, which is the bottleneck of both algorithms. CASEC is slightly faster than DCG by virtue of graph sparsity.\\n\\n### Table 3: Approximate running time of CASEC on tasks from the MACO benchmark.\\n\\n| Task       | Time (h) |\\n|------------|----------|\\n| Aloha      | 13 (2M)  |\\n| Pursuit    | 17 (2M)  |\\n| Hallway    | 7 (1M)   |\\n| Sensor     | 4.5 (0.5M) |\\n| Gather     | 6.5 (1M) |\\n| Disperse   | 8 (1M)   |\\n\\n### Table 4: Approximate running time of CASEC on tasks from the SMAC benchmark.\\n\\n| Task       | Time (h) |\\n|------------|----------|\\n| 5m_{vs}6m  | 18 (2M)  |\\n| MMM2       | 21 (2M)  |\\n\\n### Table 5: Average time (millisecond) for 1000 action selection phases of CASEC/DCG.\\n\\n| Agents | 5 actions | 10 actions | 15 actions |\\n|--------|-----------|------------|------------|\\n| 5      | 2.90/3.11 | 3.15/3.39  | 3.42/3.67  |\\n| 10     | 3.17/3.45 | 3.82/4.20  | 5.05/5.27  |\\n| 15     | 3.41/3.67 | 5.14/5.40  | 7.75/8.02  |\\n\\nOne limitation of our method is that we cannot guarantee cycle-free coordination graphs. Running Max-Sum on loopy graphs (graph that contains loops) may lead to sub-optimal actions being selected. In this section, we investigate the influence of cycles on the optimality of Max-Sum algorithm.\"}"}
{"id": "wQfgfb8VKTn", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Specifically, we first compare the results of Max-Sum against optimal joint actions on Aloha from the MACO benchmark. To this end, we sample 1000 Aloha configurations. We then run Max-Sum under different sparseness degrees (with no guarantee of cycle-free graphs) and compare with optimal joint actions. Since finding the optimal action is NP-hard, we use a brute-force method and enumerate all possible joint actions and choose the one with the largest $Q_{tot}$ value.\\n\\n![Figure 16](image)\\n\\nFigure 16: Compare actions selected by Max-Sum and the optimal joint action on 1000 different configurations of Aloha. Left: On sparse graphs with 20% edges. $Q_{tot}$ values of the actions are shown. Middle: On full graphs. $Q_{tot}$ values of the actions are shown. Right: How many actions selected by Max-Sum are optimal under different sparseness degrees.\\n\\nResults are shown in Fig. 16. We can see that Max-Sum on sparse graphs selects optimal actions in around 95% of the cases. $Q_{tot}$ values are also satisfactory, with most points falling near the line $y = x$. In comparison, the quality of Max-Sum solutions decreases significantly on full graphs, as shown in Fig. 16 middle and right.\\n\\nWe further investigate the case of random graphs. 1000 graphs are generated randomly with utility and payoff values conforming to a Gaussian distribution with a mean of 0 and a variance of 10, and we carry out experiments similar to those on Aloha. As shown in Fig. 17, we find that Max-Sum on both sparse and full graphs can select more than 95% optimal actions. The optimization objective, $Q_{tot}$, is also very close to the optimal value.\\n\\nWe can conclude that, although it can not guarantee optimality consistently, Max-Sum on sparse graphs can select the optimal action with a large probability on the tested cases. In comparison, optimal actions are less likely to be selected on full graphs. This may shed light on why CASEC can outperform DCG on some tasks. For future work, we plan to investigate how to learn cycle-free sparse coordination graphs so that action optimality can be guaranteed.\\n\\nG E\\n\\n**EVALUATION OF THE BOUND IN EQUATION 5**\\n\\nTo figure how loose the bound in Eq. 5 is, we randomly generate 10000 graphs, select the edge between agent 0 and 1, and put them into 100 bins according to the value of $\\\\zeta_{q_{var}^{01}}$. In each bin, we calculate the number of graphs where the actions of agent 0 and 1 selected by Max-Sum keep unchanged after removing the edge between them. Also for each bin, we average the bound in Equation 5 of each graph instance. Then, in Fig. 18, we compare our bound against the frequency of unchanged actions. We observe that, on average, the bound is 36.9% lower than the real frequency.\"}"}
{"id": "wQfgfb8VKTn", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Intuitively, sparse graphs are expected to perform better in tasks with more agents. In this section, we compare CASEC with DCG on a large version of Aloha and Sensor with two times number of agents.\\n\\nThe new version of Aloha has 20 agents in a $2 \\\\times 10$ array. We compare CASEC against DCG in Fig. 19 (left). We can see that DCG can no longer send any messages, but CASEC can send about 110 of them. For Sensor, there are 30 sensors and 6 targets. Results are shown in Fig. 19 (right). We can see that DCG does not learn to scan any targets, while CASEC can capture about 40 of them. The gap between sparse and full coordination graphs is more significant on these tasks.\\n\\nA limitation of our method is that we fix the communication threshold when training. In this section, we study how to select the threshold adaptively and investigate the following two methods.\\n\\nThe first method is based on the observation that the performance of sparse graphs would degrade dramatically when the sparseness degree is below a certain value. To find this value, during testing, we check the performance of graphs with different sparseness degrees and select the degree below which the performance would drop. We change the threshold every $50, 150, \\\\text{ and } 200$ training timesteps and show the performance in Fig. 20 (the first row). We can see that training with such an adaptive threshold performs similarly with the original CASEC algorithm after convergence and learns slightly better during the initial learning stage. The found threshold is smaller than the one that we get through a grid search.\\n\\nThe second method is based on Proposition 1. The intuition is that we can cut off the edges which exert limited influence on Max-Sum. Specifically, during testing, we count the number of edges that lead to different Max-Sum results with a probability smaller than 0.36 after being removed. The percentage of these edges is set as the communication threshold. Again, we change the threshold every $50, 150, \\\\text{ and } 200$ training timesteps. The results are shown in Fig. 20 (the second row). This second method leads to higher final performance but learns slower initially. The adaptive threshold is less stable compared to the first method, and the selected thresholds are larger than the hand-crafted one.\"}"}
{"id": "wQfgfb8VKTn", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 20: Learning curves (Left) and the changing process (Right) of the communication threshold of the two methods proposed in Appendix I. For future work, it is an important question how to develop more principled methods that can find the minimum communication threshold which can guarantee learning performance.\\n\\nFigure 21: Performance of DCG on Sensor with different numbers of edges in the coordination graph.\"}"}
{"id": "wQfgfb8VKTn", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: Performance of different implementations on Aloha. Different colors indicate different topologies. Performance of different losses is shown in different sub-figures.\\n\\n\\\\[ r(a) = \\\\begin{cases} 10 & \\\\text{if } a_0 = n, \\\\\\\\ 0 & \\\\text{if } 0 < a_0 < n, \\\\\\\\ 5 & \\\\text{otherwise} \\\\end{cases} \\\\]\\n\\n(29)\\n\\nWe increase the difficulty of this game by making it temporally extended and introducing stochasticity. We consider three actions. Actions are no longer atomic, and agents need to learn policies to realize these actions by navigating to goals \\\\( g_1, g_2 \\\\) and \\\\( g_3 \\\\) (Fig. 7).\\n\\nMoreover, for each episode, one of \\\\( g_1, g_2 \\\\) and \\\\( g_3 \\\\) is randomly selected as the optimal goal (corresponding to \\\\( a_0 \\\\) in Eq. 29). Agents spawn randomly, and only agents initialized near the optimal goal know which goal is optimal. Agents need to simultaneously arrive at a goal state to get any reward. If all agents are at the optimal goal state, they get a high reward of 10. If all of them are at other goal states, they would be rewarded 5. The minimum reward would be received if only some agents gather at the optimal goal. We further increase the difficulty by setting this reward to \\\\(-5\\\\). It is worth noting that, for any single episode, Gather can be solved using a static and sparse coordination graph \u2013 for example, agents can collectively coordinate with an agent who knows the optimal goal.\"}"}
{"id": "wQfgfb8VKTn", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Disperse consists of 12 agents. At each timestep, agents can choose to work at one of 4 hospitals by selecting an action in $A = \\\\{a_0, a_1, a_2, a_3\\\\}$. At timestep $t$, hospital $j$ needs $x_{jt}$ agents for the next timestep. One hospital is randomly selected and its $x_{jt}$ is a positive number, while the need of other hospitals is 0. If $y_{jt+1} < x_{jt}$ agents go to the selected hospital, the whole team would be punished $y_{jt+1} - x_{jt}$. Agents observe the local hospital's id and its need for the next timestep.\\n\\nB.2 OTHER POSSIBLE IMPLEMENTS AND PERFORMANCE COMPARISON\\n\\nWith this benchmark in hand, we are now able to evaluate our method for constructing sparse graphs. We compare our method with the following approaches.\\n\\n1. Maximum utility difference\\n   $q_i$ (or $q_j$) is the expected utility agent $i$ (or $j$) can get without the awareness of actions of other agents. After specifying the action of agent $j$ or $i$, the joint expected utility changes to $q_{ij}$. Thus the measurement\\n   $\\\\zeta_{ij} = \\\\max_{a} |\\\\delta_{ij}(\\\\tau_{ij}, a_{ij})|$ (30)\\n   can describe the mutual influence between agent $i$ and $j$. Here $\\\\delta_{ij}(\\\\tau_{ij}, a_{ij}) = q_{ij}(\\\\tau_{ij}, a_{ij}) - q_i(\\\\tau_i, a_i) - q_j(\\\\tau_j, a_j)$ (31) is the utility difference function. We use a maximization operator here because two agents need to coordinate with each other if such coordination significantly affects the probability of selecting at least one action pair.\\n\\n2. Variance of utility difference\\n   As discussed before, the value of utility difference $\\\\delta_{ij}$ and variance of payoff functions can measure the mutual influence between agent $i$ and $j$. In this way, the variance of $\\\\delta_{ij}$ serves as a second-order measurement, and we can use $\\\\zeta_{\\\\delta\\\\text{var}}_{ij} = \\\\max_{a} \\\\text{Var}_{a_j}[\\\\delta_{ij}(\\\\tau_{ij}, a_{ij})]$ (32) to rank the necessity of coordination relationships between agents. Again we use the maximization operation to base the measurement on the most influenced action.\\n\\nFor these three measurements (Eq. 8, 30, and 32), the larger value of $\\\\zeta_{ij}$ is, the more important the edge $(i, j)$ is. For example, when $\\\\zeta_{q\\\\text{var}}_{ij} = \\\\max_{a} \\\\text{Var}_{a_j}[q_{ij}(\\\\tau_{ij}, a_{ij})]$ is large, the expected utility of...\"}"}
{"id": "wQfgfb8VKTn", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13: Performance of different implementations on Disperse. Different colors indicate different topologies. Performance of different losses is shown in different sub-figures.\\n\\nagent $i$ fluctuates dramatically with the action of agent $j$, and they need to coordinate their actions. Therefore, with these measurements, to learn sparse coordination graphs, we can set a sparseness controlling constant $\\\\lambda \\\\in (0, 1)$ and select $\\\\lambda |V| (|V| - 1)$ edges with the largest $\\\\zeta_{ij}$ values. To make the measurements more accurate in edge selection, we minimize the following losses for the two measurements, respectively:\\n\\n$$L_{|\\\\delta| \\\\text{ sparse}} = \\\\frac{1}{|V| (|V| - 1)} |A| \\\\sum_{i \\\\neq j} |\\\\delta_{ij}(\\\\tau_{ij}, a_{ij})|;$$  \\n\\n(33)\\n\\n$$L_{\\\\delta \\\\text{ var} \\\\text{ sparse}} = \\\\frac{1}{|V| (|V| - 1)} |A| \\\\sum_{i \\\\neq j} \\\\text{Var}_{a_j} [\\\\delta_{ij}(\\\\tau_{ij}, a_{ij})].$$  \\n\\n(34)\\n\\nWe scale these losses with a factor $\\\\lambda_{\\\\text{sparse}}$ and optimize them together with the TD loss. It is worth noting that these measurements and losses are not independent. For example, minimizing $L_{\\\\delta \\\\text{ var} \\\\text{ sparse}}$ would also reduce the variance of $q_{ij}$. Thus, in the next section, we consider all possible combinations between these measurements and losses.\\n\\nObservation-Based Approaches\\n\\nIn partial observable environments, agents sometimes need to coordinate with each other to share their observations and reduce their uncertainty about the true state (Wang et al., 2020). We can build our coordination graphs according to this intuition. Agents use an attention model (Vaswani et al., 2017) to select the information they need. Specifically, we train fully connected networks $f_k$ and $f_q$ and estimate the importance of agent $j$\u2019s observations to agent $i$ by:\\n\\n$$\\\\alpha_{ij} = f_k(\\\\tau_i)^T f_q(\\\\tau_j).$$  \\n\\n(35)\\n\\nThen we calculate the global Q function as:\\n\\n$$Q_{\\\\text{tot}}(s, a) = \\\\frac{1}{|V|} \\\\sum_{i} q_i(\\\\tau_i, a_i) + \\\\sum_{i \\\\neq j} \\\\bar{\\\\alpha}_{ij} q_{ij}(\\\\tau_{ij}, a_{ij}),$$  \\n\\n(36)\\n\\nwhere $\\\\bar{\\\\alpha}_{ij} = e^{\\\\alpha_{ij}} / \\\\sum_{i \\\\neq j} e^{\\\\alpha_{ij}}$. Then both $f_k$ and $f_q$ can be trained end-to-end with the standard TD loss. When building coordination graphs, given a sparseness controlling factor $\\\\lambda$, we select $\\\\lambda |V| (|V| - 1)$ edges with the largest $\\\\bar{\\\\alpha}_{ij}$ values.\\n\\nB.3 WHICH METHOD IS BETTER FOR LEARNING DYNAMICALLY SPARSE COORDINATION GRAPHS?\\n\\nWe show the learning curves of value-based implementations in Fig. 8-13 and compare our method ($\\\\zeta_{q \\\\text{ var}}$ & $L_{q \\\\text{ var} \\\\text{ sparse}}$) against the (attentional) observation-based method in Fig. 14. We can see that our proposed method generally performs better than the observation-based method, except for the task Disperse. Compared to other games, observations provided by Disperse can reveal all the game information. In this case, the observation-based method can make better use of local observations and can easily learn an accurate coordination graph.\\n\\nC THE SMAC BENCHMARK\\n\\nOn the SMAC benchmark, we compare our method against fully decomposed value function learning methods (VDN (Sunehag et al., 2018) & QMIX (Rashid et al., 2018)) and a deep coordination graph\"}"}
{"id": "wQfgfb8VKTn", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As discussed in Sec. 3.3 of the main text, we use action representations to reduce the influence of utility difference function's estimation errors on graph structure learning. In this section, we describe the details of action representation learning (the related network structure is shown in Fig. 15). We use the technique proposed by Wang et al. (2021b) and learn an action encoder $f_e(\\\\cdot; \\\\theta_e) : \\\\mathbb{R} | A | \\\\rightarrow \\\\mathbb{R}^d$, parameterized by $\\\\theta_e$, to map one-hot actions to a $d$-dimensional representation space. With the encoder, each action $a$ has a latent representation $z_a$, $i.e.$, $z_a = f_e(a; \\\\theta_e)$. The representation $z_a$ is then used to predict the next observation $o'$ and the global reward $r$, given the current observation $o_i$ of an agent $i$, and the one-hot actions of other agents, $a_{-i}$. This model is a forward model, which is trained by minimizing the following loss:\\n\\n$$L_e(\\\\theta_e, \\\\xi_e) = \\\\mathbb{E}_{(o, a, r, o') \\\\sim D} \\\\| p_o(z_{a_i}, o_i, a_{-i}) - o' \\\\|_2^2 + \\\\lambda_e \\\\mathbb{E}_{i} (p_r(z_{a_i}, o_i, a_{-i}) - r)^2,$$\\n\\n(37)\\n\\nwhere $p_o$ and $p_r$ is the predictor for observations and rewards, respectively. We use $\\\\xi_e$ to denote the parameters of $p_o$ and $p_r$. $\\\\lambda_e$ is a scaling factor, $D$ is a replay buffer, and the sum is carried out over all agents.\\n\\nIn the beginning, we collect samples and train the predictive model shown in Fig. 15 for 50 K timesteps. Then policy learning begins and action representations are kept fixed during training. Since tasks in the MACO benchmark typically do not involve many actions, we do not use action representations when benchmarking our method. In contrast, StarCraft II micromanagement tasks usually have a large action space. For example, the map MMM2 involves 16 actions, and a conventional deep Q-network requires 256 output heads for learning utility difference. Therefore, we equip our method with action representations.\"}"}
{"id": "wQfgfb8VKTn", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning sparse coordination graphs adaptive to the coordination dynamics among agents is a long-standing problem in cooperative multi-agent learning. This paper studies this problem and proposes a novel method using the variance of payoff functions to construct context-aware sparse coordination topologies. We theoretically consolidate our method by proving that the smaller the variance of payoff functions is, the less likely action selection will change after removing the corresponding edge. Moreover, we propose to learn action representations to effectively reduce the influence of payoff functions' estimation errors on graph construction.\\n\\nTo empirically evaluate our method, we present the Multi-Agent COordination (MACO) benchmark by collecting classic coordination problems in the literature, increasing their difficulty, and classifying them into different types. We carry out a case study and experiments on the MACO and StarCraft II micromanagement benchmark to demonstrate the dynamics of sparse graph learning, the influence of graph sparseness, and the learning performance of our method.\"}"}
{"id": "wQfgfb8VKTn", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2022\\n\\njoint action-observation space of the connected agents, a coordination graph expresses a higher-order\\nvalue decomposition among agents. Finding actions with the maximum value in a coordination graph\\ncan be achieved by distributed constraint optimization (DCOP) algorithms (Cheng, 2012), which\\nconsists of multiple rounds of message passing along the edges. Recently, DCG (B\u00f6hmer et al.,\\n2020) scales coordination graphs to large state-action spaces, shows its ability to solve the problem of\\nrelative overgeneralization, and obtains competitive results on StarCraft II micromanagement tasks.\\nHowever, DCG focuses on predefined static and dense topologies, which largely lack flexibility for\\ndynamic environments and induce intensive and inefficient message passing.\\n\\nThe question is how to learn dynamic and sparse coordination graphs sufficient for coordinated\\naction selection. This is a long-standing problem in multi-agent learning. Sparse cooperative\\nQ-learning (Kok & Vlassis, 2006) learns value functions for sparse coordination graphs, but the\\ngraph topology is static and predefined by prior knowledge. Zhang & Lesser (2013) propose to\\nlearn minimized dynamic coordination sets for each agent, but the computational complexity grows\\nexponentially with the neighborhood size of an agent. Recently, Castellini et al. (2019) study the\\nrepresentational capability of several sparse graphs but focus on random topologies and stateless\\ngames. In this paper, we push these previous works further by proposing a novel deep method that\\nlearns context-aware sparse coordination graphs adaptive to the dynamic coordination requirements.\\n\\nFor learning sparse coordination graphs, we propose to use the variance of pairwise payoff functions\\nas an indicator to select edges. Sparse graphs are used when selecting greedy joint actions for\\nexecution and the update of Q-function. We provide a theoretical insight into our method by proving\\nthat the probability of greedy action selection changing after an edge is removed decreases with\\nthe variance of the corresponding payoff function. Despite the advantages of sparse topologies,\\nthey raise the concern of learning instability. To solve this problem, we further equip our method\\nwith network structures based on action representations for utility and payoff learning to reduce the\\ninfluence of estimation errors on sparse topologies learning. We call the overall learning framework\\nContext-Aware SparsE Coordination graphs (CASEC).\\n\\nFor evaluation, we present the Multi-Agent COordination (MACO) benchmark. This benchmark\\ncollects classic coordination problems raised in the literature of multi-agent learning, increases their\\ndifficulty, and classifies them into 6 classes. Each task in the benchmark represents a type of problem.\\nWe carry out a case study on the MACO benchmark to show that CASEC can discover the coordination\\ndependence among agents under different situations and to analyze how the graph sparsity influences\\naction coordination. We further show that CASEC can largely reduce the communication cost\\n(typically by 50%) and perform significantly better than dense, static graphs and several alternative\\nmethods for building sparse graphs. We then test CASEC on the StarCraft II micromanagement\\nbenchmark (Samvelyan et al., 2019) to demonstrate its scalability and effectiveness.\\n\\n2 B ACKGROUND\\n\\nIn this paper, we focus on fully cooperative multi-agent tasks that can be modelled as a\\nDec-POMDP (Oliehoek et al., 2016) consisting of a tuple\\n\\\\[ G = \\\\langle I, S, A, P, R, \\\\Omega, O, n, \\\\gamma \\\\rangle \\\\],\\nwhere\\n\\\\( I \\\\) is the\\nfinite set of\\n\\\\( n \\\\) agents,\\n\\\\( \\\\gamma \\\\in [0, 1) \\\\) is the discount factor, and\\n\\\\( s \\\\in S \\\\) is the true state of the environment.\\n\\nAt each timestep, each agent\\n\\\\( i \\\\) receives an observation\\n\\\\( o_i \\\\in \\\\Omega \\\\) drawn according to the observation\\nfunction\\n\\\\( O(s, i) \\\\) and selects an action\\n\\\\( a_i \\\\in A \\\\). Individual actions form a joint action\\n\\\\( a \\\\in A^n \\\\), which\\nleads to a next state\\n\\\\( s' \\\\) according to the transition function\\n\\\\( P(s' | s, a) \\\\), a reward\\n\\\\( r = R(s, a) \\\\) shared by\\nall agents. Each agent has local action-observation history\\n\\\\( \\\\tau_i \\\\in \\\\mathcal{T} \\\\equiv (\\\\Omega \\\\times A)^* \\\\times \\\\Omega \\\\). Agents learn to\\ncollectively maximize the global return\\n\\\\( Q_{\\\\text{tot}}(s, a) = \\\\mathbb{E}_{s_0:\\\\infty, a_0:\\\\infty} \\\\left[ \\\\sum_{t=0}^{\\\\infty} \\\\gamma^t R(s_t, a_t) | s_0 = s, a_0 = a \\\\right] \\\\).\\n\\nIn a\\ncoordination graph\\n\\\\((Guestrin et al., 2002b)\\\\n\\\\[ G = \\\\langle V, E \\\\rangle \\\\], each vertex\\n\\\\( v_i \\\\in V \\\\) represents an agent\\n\\\\( i \\\\), and (hyper-) edges in\\n\\\\( E \\\\) represent coordination dependencies among agents. In this paper, we\\nconsider pairwise edges, and such a coordination graph induces a factorization of the global Q:\\n\\\\[ Q_{\\\\text{tot}}(\\\\tau, a) = \\\\frac{1}{|V|} \\\\sum_{i} q_i(\\\\tau_i, a_i) + \\\\frac{1}{|E|} \\\\sum_{\\\\{i, j\\\\} \\\\in E} q_{ij}(\\\\tau_{ij}, a_{ij}) \\\\],\\nwhere\\n\\\\( q_i \\\\) and\\n\\\\( q_{ij} \\\\) is\\nutility\\nfunctions for individual agents and pairwise\\npayoff\\nfunctions, respectively.\\n\\\\( \\\\tau_{ij} = \\\\langle \\\\tau_i, \\\\tau_j \\\\rangle \\\\) and\\n\\\\( a_{ij} = \\\\langle a_i, a_j \\\\rangle \\\\) is the joint action-observation history and action of agent\\n\\\\( i \\\\) and\\n\\\\( j \\\\).\"}"}
{"id": "wQfgfb8VKTn", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Within a coordination graph, the greedy action selection required by Q-learning can not be completed by simply computing the maximum of individual utility and payoff functions. Instead, distributed constraint optimization (DCOP) (Cheng, 2012) techniques can be used. Max-Sum (Stranders et al., 2009) is a popular implementation of DCOP, which finds optimal actions on a coordination graph $G = \\\\langle V, E \\\\rangle$ via multi-round message passing on a bipartite graph $G_m = \\\\langle V_a, V_q, E_m \\\\rangle$. Each node $i \\\\in V_a$ represents an agent, and each node $g \\\\in V_q$ represents a utility ($q_i$) or payoff ($q_{ij}$) function. Edges in $E_m$ connect $g$ with the corresponding agent node(s). Message passing on this bipartite graph starts with sending messages from node $i \\\\in V_a$ to node $g \\\\in V_q$:\\n\\n$$m_i \\\\rightarrow g(a_i) = \\\\sum_{h \\\\in F_i \\\\setminus g} m_h \\\\rightarrow i(a_i) + c_{ig},$$\\n\\nwhere $F_i$ is the set of nodes connected to node $i$ in $V_q$, and $c_{ig}$ is a normalizing factor preventing the value of messages from growing arbitrarily large. The message from node $g$ to node $i$ is:\\n\\n$$m_g \\\\rightarrow i(a_i) = \\\\max_{a_g \\\\setminus a_i} \\\\left[ q(a_g) + \\\\sum_{h \\\\in V_g \\\\setminus i} m_h \\\\rightarrow g(a_h) \\\\right],$$\\n\\nwhere $V_g$ is the set of nodes connected to node $g$ in $V_a$, $a_g = \\\\{a_h | h \\\\in V_g\\\\}$, $a_g \\\\setminus a_i = \\\\{a_h | h \\\\in V_g \\\\setminus \\\\{i\\\\}\\\\}$, and $q$ represents utility or payoff functions conditioned on $a_g$. After several iterations of message passing, each agent $i$ can find its optimal action by calculating $a^*_i = \\\\arg\\\\max_{a_i} P_{h \\\\in F_i} m_h \\\\rightarrow i(a_i)$.\\n\\nA drawback of Max-Sum or other message passing methods (e.g., max-plus (Pearl, 2014)) is that running them for each action selection through the whole system results in intensive computation and communication among agents, which is impractical for most applications with limited computational resources and communication bandwidth. In the following sections, we discuss how to solve this problem by learning sparse coordination graphs.\\n\\nPrevious works (Naderializadeh et al., 2020; Li et al., 2021) study soft versions of fully-connected coordination graphs based on attention mechanisms. Specifically, Li et al. (2021) uses graphs whose edge weights are learned by self-attention so that agents attend to observations of other agents differently. The information is used in local actors or a centralized critic. Naderializadeh et al. (2020) learns soft full graphs in a similar way, but the graph is used to mix local utilities conditioned on local action-observation history. Different from our work, these methods do not learn pairwise payoff functions, and the learned graphs are still fully-connected.\\n\\n## 3 Learning Context-Aware Sparse Graphs\\n\\nIn this section, we introduce our methods for learning context-aware sparse graphs. We first introduce how we construct a sparse graph for effective action selection in Sec. 3.1. After that, we introduce our learning framework in Sec. 3.2. Although sparse graphs can reduce communication overhead, they raise the concern of learning instability. We discuss this problem and how to alleviate it in Sec. 3.3.\\n\\n### 3.1 Construct Sparse Graphs\\n\\nAction values, especially the pairwise payoff functions, contain much information about mutual influence between agents. Let's consider two agents $i$ and $j$. Intuitively, agent $i$ needs to coordinate its action selection with agent $j$ if agent $j$'s action exerts significant influence on the expected utility of agent $i$. For a fixed action $a_i$, $\\\\text{Var}_{a_j}[q_{ij}(\\\\tau_{ij}, a_{ij})]$ can measure the influence of agent $j$ on the expected payoff. This intuition motivates us to use the variance of payoff functions $\\\\zeta_{q\\\\text{var}}_{ij} = \\\\max_{a_i} \\\\text{Var}_{a_j}[q_{ij}(\\\\tau_{ij}, a_{ij})]$, as an indicator to construct sparse graphs. The maximization operator guarantees that the most affected action is considered. When $\\\\zeta_{q\\\\text{var}}_{ij}$ is large, the expected utility of agent $i$ fluctuates dramatically with the action of agent $j$, and they need to coordinate their actions. Therefore, with this measurement, to construct sparse coordination graphs, we can set a sparseness controlling constant $\\\\lambda \\\\in (0, 1)$ and select $\\\\lambda |V| (|V| - 1)$ edges with the largest $\\\\zeta_{q\\\\text{var}}_{ij}$ values.\\n\\nTo justify this approach, we theoretically prove that, the smaller the value of $\\\\zeta_{q\\\\text{var}}_{ij}$ is, the more likely that the Max-Sum algorithm will select the same actions after removing the edge $(i, j)$.\"}"}
{"id": "wQfgfb8VKTn", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proposition 1. For any two agents \\\\( i, j \\\\) and the edge \\\\( e_{ij} \\\\) connecting them in the coordination graph, after removing edge \\\\( e_{ij} \\\\), greedy actions of agent \\\\( i \\\\) and \\\\( j \\\\) selected by the Max-Sum algorithm keep unchanged with a probability larger than\\n\\n\\\\[\\n\\\\frac{2 |A| \\\\left( \\\\bar{m} - \\\\min_{a_j} m(e_{ij} \\\\rightarrow j)(a_j) \\\\right) \\\\left( \\\\max_{a_j} m(e_{ij} \\\\rightarrow j)(a_j) - \\\\bar{m} \\\\right) \\\\varpi_{ij} \\\\left( \\\\bar{m} - \\\\min_{a_j} m(e_{ij} \\\\rightarrow j)(a_j) \\\\right) \\\\right)}{h}\\n\\\\]\\n\\n(5)\\n\\nwhere \\\\( m(e_{ij} \\\\rightarrow j) = m(e_{ij} \\\\rightarrow j)(a_j) \\\\), \\\\( \\\\bar{m} \\\\) is the average of \\\\( m(e_{ij} \\\\rightarrow j)(a_j) \\\\), \\\\( M = \\\\max_{a_j} \\\\left[ \\\\max_{a_i} \\\\left( r(a_i, a_j) - r(a_i, a_j) \\\\right) \\\\right] \\\\), and \\\\( r(a_i, a_j) = \\\\varpi(a_i, a_j) + m(e_{ij} \\\\rightarrow j)(a_j) \\\\).\\n\\nDetailed proof can be found in Appendix A. The lower bound in Proposition 1 increases with a decreasing \\\\( \\\\varpi_{ij} \\\\). Therefore, edges with a smaller \\\\( \\\\varpi_{ij} \\\\) are less likely to influence the results of Max-Sum, justifying the way we construct sparse graphs.\\n\\n3.2 Learning Framework\\n\\nLike conventional Q-learning, CASEC consists of two main components \u2013 learning value functions and selecting greedy actions. The difference is that these two steps are now carried out on dynamic and sparse coordination graphs.\\n\\nIn CASEC, agents learn a shared utility function \\\\( q_\\\\xi u(\\\\cdot|\\\\tau_i) \\\\), parameterized by \\\\( \\\\xi_u \\\\), and a shared pairwise payoff function \\\\( q_\\\\xi p(\\\\cdot|\\\\tau_{ij}) \\\\), parameterized by \\\\( \\\\xi_p \\\\). The global Q value function is estimated as:\\n\\n\\\\[\\nQ_{\\\\text{tot}}(\\\\tau, a) = \\\\frac{1}{|V|} \\\\sum_{i} q_\\\\xi u(a_i|\\\\tau_i) + \\\\frac{1}{|V| (|V| - 1)} \\\\sum_{i \\\\neq j} q_\\\\xi p(a_{ij}|\\\\tau_{ij})\\n\\\\]\\n\\n(6)\\n\\nwhich is updated by the TD loss:\\n\\n\\\\[\\nL_{\\\\text{TD}}(\\\\xi_u, \\\\xi_p) = E_D [r + \\\\gamma \\\\hat{Q}_{\\\\text{tot}}(\\\\tau', \\\\text{Max-Sum}(\\\\hat{q}_\\\\xi u, \\\\hat{q}_\\\\xi p)) - Q_{\\\\text{tot}}(\\\\tau, a)]^2\\n\\\\]\\n\\n(7)\\n\\nMax-Sum(\\\\cdot, \\\\cdot) is the greedy joint action selected by Max-Sum, \\\\( \\\\hat{Q}_{\\\\text{tot}} \\\\) is a target network with parameters \\\\( \\\\hat{\\\\xi}_u, \\\\hat{\\\\xi}_p \\\\) periodically copied from \\\\( Q_{\\\\text{tot}} \\\\), and the expectation is estimated with uniform samples from a replay buffer \\\\( D \\\\). Meanwhile, we also minimize a sparseness loss\\n\\n\\\\[\\nL_{\\\\text{q var sparse}}(\\\\xi_p) = \\\\frac{1}{|V| (|V| - 1)} |A| \\\\sum_{i \\\\neq j} \\\\text{Var}_{a_j}[q_{ij}(\\\\tau_{ij}, a_{ij})]\\n\\\\]\\n\\n(8)\\n\\nwhich is a regularization on \\\\( \\\\varpi_{ij} \\\\). Introducing a scaling factor \\\\( \\\\lambda_{\\\\text{sparse}} \\\\in (0, 1] \\\\) and minimizing \\\\( L_{\\\\text{TD}}(\\\\xi_u, \\\\xi_p) + \\\\lambda_{\\\\text{sparse}} L_{\\\\text{q var sparse}}(\\\\xi_p) \\\\) builds in inductive biases which favor minimized coordination graphs that would not sacrifice global returns.\\n\\nActions with the maximized value are selected for Q-learning and execution. In our framework, such action selections are finished by running Max-Sum on sparse graphs induced by \\\\( \\\\varpi_{ij} \\\\) (while we update Q functions on the full graph). Running Max-Sum requires the message passing through each node and edge. To speed up action selections, similar to previous work (B\u00f6hmer et al., 2020), we use multi-layer graph neural networks without parameters to process messages in a parallel manner.\\n\\n3.3 Stabilize Learning\\n\\nOne question with estimating \\\\( q_{ij} \\\\) is that there are \\\\(|A| \\\\times |A|\\\\) action-pairs, each of which requires an output head in conventional deep Q networks. As only executed action-pairs are updated during Q-learning, parameters of many output heads remain unchanged for long stretches of time, resulting in estimation errors. Previous work (B\u00f6hmer et al., 2020) uses a low-rank approximation to reduce the number of output heads. However, it is largely unclear how to choose the best rank \\\\( K \\\\) for different tasks, and still only \\\\( \\\\frac{1}{|A|} \\\\) of the output heads are selected in one Q-learning update.\\n\\nThis problem of estimation errors becomes especially problematic in CASEC, where building coordination graphs relies on the estimation of \\\\( q_{ij} \\\\). A negative feedback loop is created because the built coordination graphs also affect the learning of \\\\( q_{ij} \\\\). This loop renders learning unstable (Fig. 5).\"}"}
{"id": "wQfgfb8VKTn", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We propose to solve this question and stabilize training by 1) periodically fixing the way we construct graphs via using the target payoff function to build graphs; and 2) accelerating the training of payoff function between target network updates to reduce the estimation errors via learning action representations.\\n\\nSpecifically, for 2), we propose to condition the utility and payoff functions on action representations to improve sample efficiency. We train an action encoder $f_{\\\\xi}(a)$, whose input is the one-hot encoding of an action $a$ and output is its representation $z_a$. We adopt the technique introduced by Wang et al. (2021b) to train an effect-based action encoder. Specifically, action representation $z_a$, together with the current local observations, is used to predict the reward and observations at the next timestep. The prediction loss is minimized to update the action encoder $f_{\\\\xi}(a)$. For more details, we refer readers to Appendix D. The action encoder is trained with few samples when learning begins and remains unchanged for the rest of the training process.\\n\\nUsing action representations, the utility and payoff functions can now be estimated as:\\n\\n$$q_{\\\\xi u}(\\\\tau_i, a_i) = h_{\\\\xi u}(\\\\tau_i) T z_{a_i};$$\\n\\n$$q_{\\\\xi p}(\\\\tau_{ij}, a_{ij}) = h_{\\\\xi p}(\\\\tau_{ij}) [z_{a_i}, z_{a_j}],$$\\n\\nwhere $h$ includes a GRU (Cho et al., 2014) to process sequential input and output a vector with the same dimension as the corresponding action representation. $[\\\\cdot, \\\\cdot]$ means vector concatenation. Using Eq. 9, no matter which action is selected for execution, all parameters in the framework ($\\\\xi_u$ and $\\\\xi_p$) would be updated. The detailed network structure can be found in Appendix E.\\n\\n### 4 Multi-Agent Coordination Benchmark\\n\\nTo evaluate our sparse graph learning algorithm, we collect classic coordination problems from the cooperative multi-agent learning literature, improve their difficulty, and classify them into different types. Then, 6 representative problems are selected and presented as a new benchmark called Multi-Agent COordination (MACO) challenge (Table 1).\\n\\n| Task   | Factored | Pairwise | Dynamic | # Agents |\\n|--------|----------|----------|----------|----------|\\n| Aloha  | \u221a        | \u221a        |          | 10       |\\n| Pursuit| \u221a        | \u221a        | \u221a        | 10       |\\n| Hallway| \u221a        |          |          | 12       |\\n| Sensor | \u221a        | \u221a        |          | 15       |\\n| Gather | \u221a        |          |          | 12       |\\n| Disperse|          |          |          |          |\\n\\nAt the first level, tasks are classified as factored and non-factored games, where factored games present an explicit decomposition of global rewards. Factored games are further categorized according to two properties \u2013 whether the task requires pairwise or higher-order coordination, and whether the underlying coordination relationships change temporally. For non-factored games, we divide them into two classes by whether the task characterizes static coordination relationships among agents. To better test the performance of different methods, we increase the difficulty of the included problems by extending stateless games to temporally extended settings (Gather and Disperse), complicating the reward function (Pursuit), or increasing the number of agents (Aloha and Hallway). We now briefly describe the setting of each game. For detailed description, we refer readers to Appendix B.\\n\\n**Aloha** (Hansen et al., 2004; Oliehoek, 2010) consists of 10 islands in a $2 \\\\times 5$ array. Each island has a backlog of messages to send. They send one message or not at each timestep. When two neighboring agents send simultaneously, messages collide and have to be resent. Islands start with 1 package in the backlog. At each timestep, with a probability $0.6$, a new packet arrives if the maximum backlog (5) has not been reached. Each agent observes its position and the number of packages in its backlog. Agents receive a global reward of $0.1$ for each successful transmission, and $-10$ for a collision.\\n\\n**Pursuit**, also called Predator and Prey, is a classic coordination problem (Benda, 1986; Stone & Veloso, 2000; Son et al., 2019). Ten agents (predators) roam a $10 \\\\times 10$ map populated with 5 random walking preys for 50 environment steps. One prey is captured if two agents catch it simultaneously, after which the catching agents and the prey are removed from the map, resulting in a team reward 5.\"}"}
{"id": "wQfgfb8VKTn", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Agents & ids Targets & ids Actions by ind. utility Actions changed/ added by Max-Sum Edges in CG\\n\\nFigure 1: Left: Learning curves (return and the number of successfully scanned targets) of CASEC and DCG on Sensor. Middle: The influence of graph sparseness on performance (return and the number of scanned targets). Here we show the case of the best seed. The plot of other seeds can be found in Fig. 2. Right: An example coordination graph learned by our method.\\n\\nHallway (Wang et al., 2020) is a multi-chain Dec-POMDP. We increase the difficulty of Hallway by introducing more agents and grouping them (Fig. 6). One agent randomly spawns at a state in each chain. Agents can observe their own position and choose to move left, move right, or keep still at each timestep. Agents win with a global reward of 1 if they arrive at state $g$ simultaneously with other agents in the same group. If $n_g > 1$ groups attempt to move to $g$ at the same timestep, they keep motionless and agents receive a global punishment of $-0.5 \\\\times n_g$.\\n\\nSensor has been extensively studied (Lesser et al., 2012; Zhang & Lesser, 2011). We consider 15 sensors in a $3 \\\\times 5$ matrix. Sensors can scan the eight nearby points. Each scan induces a cost of -1, and agents can do noop to save the cost. Three targets wander randomly in the grid. If $k \\\\geq 2$ sensors scan a target simultaneously, the system gets a constant reward of 3, which is independent of the number of sensors. Agents can observe the id and position of targets nearby.\\n\\nGather is an extension of the Climb Game (Wei & Luke, 2016). In Climb Game, each agent has three actions: $A = \\\\{a_0, a_1, a_2\\\\}$. Action $a_0$ yields no reward (0) if only some agents choose it, but a high reward (10) if all choose it. Otherwise, if no agent chooses action $a_0$, a reward $5$ is obtained. We increase the difficulty of this game by making it temporally extended and introducing stochasticity. Actions are no longer atomic, and agents need to learn policies to realize these actions by navigating to goals $g_1, g_2$ and $g_3$ (Fig. 7). Moreover, for each episode, one of $g_1, g_2$ and $g_3$ is randomly selected as the optimal goal (corresponding to $a_0$ in the original game).\\n\\nDisperse consists of 12 agents. At each timestep, agents can choose to work at one of 4 hospitals by selecting an action in $A = \\\\{a_0, a_1, a_2, a_3\\\\}$. At timestep $t$, hospital $j$ needs $x_{jt}$ agents for the next timestep. One hospital is randomly selected and its $x_{jt}$ is a positive number, while the need of other hospitals is 0. If $y_{jt} + 1 < x_{jt}$ agents go to the selected hospital, the whole team would be punished $y_{jt} + 1 - x_{jt}$.\\n\\nWe are particularly interested in the dynamics and results of sparse graph learning. Therefore, we carry out a case study on Sensor. When training CASEC on this task, we select 10% edges with largest $\\\\zeta_{ij}$ values to construct sparse graphs.\\n\\nInterpretable sparse coordination graphs. In Fig. 1 right, we show a screenshot of the game with the learned coordination graph at a certain timestep. We can observe that all edges in the learned graph involve agents around the targets. Let's see the case of agent 8. The action proposed by the individual utility function of agent 8 is to scan target 1. After coordinating its action with other agents, agent 8 changes its action selection and scans target 2, resulting in an optimal solution for the given configuration. This result is in line with our theoretical analysis in Sec. 3.1. The most important edges can be characterized by a large $\\\\zeta$ value.\"}"}
{"id": "wQfgfb8VKTn", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Agents and their ids\\n\\nTargets and their ids\\n\\nScan actions\\n\\nCASEC (sparse graph with 10% edges)\\n\\nCASEC w/o sparse loss (sparse graph with 10% edges)\\n\\nCASEC (full graph)\\n\\nCASEC w/o sparse loss (full graph)\\n\\nDCG (full graph)\\n\\nDCG (sparse graph with 10% edges)\\n\\nFigure 2: Coordination graphs learned by different methods on Sensor.\\n\\nFigure 3: Performance comparison with baselines on the MACO benchmark.\\n\\nmedian performance and 25%-75% percentiles are shown). This observation may be counter-intuitive at the first glance. To study this problem, we load the model after convergence learned by CASEC and DCG, gradually remove edges from the full graph in the ascending order of $\\\\zeta_{ij}$, and check the change of scanned targets and the obtained reward. Results are shown in Fig. 1 middle and Fig. 21. It can be observed that the performance of DCG (the number of scanned targets) does not change with the number of edges. In another word, only the individual utility function contributes to scanning targets. Screenshots shown in Fig. 2 (right column) align with this observation. With more edges, DCG makes a less optimal decision: agent 4, 5, and 9 no longer scan target 1.\\n\\nIn contrast, the performance of CASEC grows with more edges in the coordination graph. By referring to Fig. 2 (left column), we can conclude that CASEC first selects edges that help agents scan more targets, and then selects edges that can eliminate useless scan actions. These results demonstrate that our method can distinguish the most important edges on Sensor.\\n\\nWe also study the influence of the sparseness loss (Eq. 8). As shown in Fig. 1 middle, CASEC without the sparseness loss consistently gets fewer rewards than CASEC. For example, target 1 and 3 are not captured in the case shown in Fig. 2 (middle column) as only one agent scans them. These results highlight the function of the sparseness loss.\\n\\nEXPERIMENTS\\n\\nIn this section, we design experiments to answer the following questions: (1) How much communication can be saved by our method? How does communication threshold influence performance on factored and non-factored games? (Sec. 6.1) (2) How does our method compare to state-of-the-art multi-agent learning methods? (Sec. 6.2, 6.3) (3) Is our method efficient in settings with larger...\"}"}
{"id": "wQfgfb8VKTn", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tonghan Wang, Jianhao Wang, Chongyi Zheng, and Chongjie Zhang. Learning nearly decomposable value functions with communication minimization. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.\\n\\nTonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang. Rode: Learning roles to decompose multi-agent tasks. In Proceedings of the International Conference on Learning Representations (ICLR), 2021b.\\n\\nYihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Dop: Off-policy multi-agent decomposed policy gradients. In Proceedings of the International Conference on Learning Representations (ICLR), 2021c.\\n\\nErmo Wei and Sean Luke. Lenient learning in independent-learner stochastic cooperative games. The Journal of Machine Learning Research, 17(1):2914\u20132955, 2016.\\n\\nZhao Xu, Yang Lyu, Quan Pan, Jinwen Hu, Chunhui Zhao, and Shuai Liu. Multi-vehicle flocking control with deep deterministic policy gradient method. 2018 IEEE 14th International Conference on Control and Automation (ICCA), Jun 2018. doi: 10.1109/icca.2018.8444355. URL http://dx.doi.org/10.1109/ICCA.2018.8444355.\\n\\nChongjie Zhang and Victor Lesser. Coordinated multi-agent reinforcement learning in networked distributed pomdps. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.\\n\\nChongjie Zhang and Victor Lesser. Coordinating multi-agent reinforcement learning with limited communication. In Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems, pp. 1101\u20131108. International Foundation for Autonomous Agents and Multiagent Systems, 2013.\\n\\n12\"}"}
{"id": "wQfgfb8VKTn", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we provide proof to Proposition 1. Without loss of generality, we consider two agents 1 and 2 and the edge between them \\\\((1, 2)\\\\). We prove our idea by comparing the action selection of agent 2 before and after removing edge \\\\((1, 2)\\\\). In the following proof, we use \\\\(i\\\\) (\\\\(i = 1, 2\\\\)) to denote agent \\\\(i\\\\) and \\\\(e\\\\) to denote edge \\\\((1, 2)\\\\).\\n\\nAction of agent 2 is determined by\\n\\\\[\\n\\\\text{arg max}_{a_2} m_{e \\\\rightarrow 2}(a_2).\\n\\\\]\\n\\nWe first see the influence of \\\\(m_{e \\\\rightarrow 2}(a_2)\\\\) on \\\\(a^*_2\\\\). For clarity, we use \\\\(m(a_2)\\\\) to denote \\\\(m_{e \\\\rightarrow 2}(a_2)\\\\) and \\\\(l(a_2)\\\\) to denote \\\\(P_{h \\\\in \\\\mathcal{F}_2 \\\\setminus \\\\{e\\\\}} m_{h \\\\rightarrow 2}(a_2)\\\\). We are interested in whether\\n\\\\[\\n\\\\text{arg max}_{a_2} l(a_2) = \\\\text{arg max}_{a_2} \\\\left[ m(a_2) + l(a_2) \\\\right].\\n\\\\]\\n\\nThe probability of this event holds if the following inequality holds:\\n\\\\[\\n\\\\text{Range}[m(a_2)] \\\\leq \\\\min_{a_2 \\\\neq a_j} \\\\left( l(a_j_2) - l(a_2) \\\\right),\\n\\\\]\\nwhere \\\\(\\\\text{Range}(x)\\\\) denotes the largest elements in vector \\\\(x\\\\) minus the smallest one and \\\\(a_j_2 = \\\\text{arg max}_{a_2} l(a_2)\\\\).\\n\\nWe rewrite Eq. 12 and obtain\\n\\\\[\\n\\\\Pr[\\\\text{Range}[m(a_2)] \\\\leq \\\\min_{a_2 \\\\neq a_j} \\\\left( l(a_j_2) - l(a_2) \\\\right)!]\\n\\\\]\\n(13)\\n\\nAccording to the Asymmetric two-sided Chebyshev's inequality (Mitzenmacher & Upfal, 2017), we get a lower bound of this probability:\\n\\\\[\\n\\\\frac{4(\\\\overline{m} - \\\\min_{a_2} m(a_2)) (\\\\max_{a_2} m(a_2) - \\\\overline{m})}{\\\\sigma^2_{\\\\min_{a_2 \\\\neq a_j} \\\\left( l(a_j_2) - l(a_2) \\\\right)}}\\n\\\\]\\n(15)\\n\\nwhere \\\\(\\\\sigma\\\\) is the variance of \\\\(m(a_2)\\\\), and \\\\(\\\\overline{m}\\\\) is the average of \\\\(m(a_2)\\\\).\\n\\nSuppose that we take \\\\(|A|\\\\) actions independently. According to the von Szokefalvi Nagy inequality (Nagy, 1918), we can further get the lower bound as follows:\\n\\\\[\\n\\\\frac{4(\\\\overline{m} - \\\\min_{a_2} m(a_2)) (\\\\max_{a_2} m(a_2) - \\\\overline{m})}{\\\\sigma^2_{\\\\min_{a_2 \\\\neq a_j} \\\\left( l(a_j_2) - l(a_2) \\\\right)}} \\\\geq \\\\frac{4\\\\left( \\\\overline{m} - \\\\min_{a_2} m(a_2) \\\\right) \\\\left( \\\\max_{a_2} m(a_2) - \\\\overline{m} \\\\right)}{\\\\sigma^2 - \\\\frac{1}{|A|}}\\n\\\\]\\n(16)\\n\\nNote that\\n\\\\[\\nm(a_2) = \\\\max_{a_1} \\\\left[ q(a_1, a_2) + m_{1 \\\\rightarrow e}(a_1) \\\\right],\\n\\\\]\\n(17)\\n\\nand we are interested in \\\\(q(a_1, a_2)\\\\). We now study the relationship between \\\\(m(a_2)\\\\) and \\\\(\\\\max_{a_1} \\\\left[ q(a_1, a_2) \\\\right]\\\\). For clarity, we use \\\\(r(a_1, a_2)\\\\) to denote \\\\(q(a_1, a_2) + m_{1 \\\\rightarrow e}(a_1)\\\\), and \\\\(r(a_i_1, a_2)\\\\) to denote \\\\(\\\\max_{a_1} \\\\left[ q(a_1, a_2) \\\\right]\\\\). Then we have\\n\\\\[\\n\\\\text{Var}_{a_2} \\\\max_{a_1} r(a_1, a_2) = \\\\text{Var}_{a_2} r(a_i_1, a_2).\\n\\\\]\\n\\nFor a given \\\\(a_2\\\\), we have\\n\\\\[\\n\\\\text{Var}_{a_2} r(a_1, a_2) = \\\\text{Var}_{a_2} r(a_i_1, a_2) - s^2.\\n\\\\]\\n(18)\"}"}
{"id": "wQfgfb8VKTn", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here is \\\\( i_2 \\\\geq 0 \\\\), because \\\\( i_2 = \\\\arg \\\\max \\\\, i_r(a_1, a_2) \\\\).\\n\\nSince \\\\( \\\\text{Var}_{a_2} r(a_i_2, a_2) = \\\\text{Var}_{a_2} r(a_i_2, a_2) + \\\\text{Var}_s - 2 \\\\text{Cov}(r(a_i_2, a_2), s) \\\\) (19), it follows that\\n\\n\\\\[\\n\\\\text{Var}_{a_2} r(a_i_2, a_2) - s^2 \\\\geq \\\\text{Var}_{a_2} r(a_i_2, a_2) - 2q \\\\text{Var}_{a_2} r(a_i_2, a_2) \\\\text{Var}_s.\\n\\\\] (22)\\n\\nThus, \\\\( \\\\zeta_{12}[r(a_1, a_2)] = \\\\max_{a_1} \\\\text{Var}_{a_2} r(a_1, a_2) \\\\geq \\\\text{Var}_{a_2} \\\\max_{a_1} r(a_1, a_2) - 2q \\\\text{Var}_{a_2} r(a_i_2, a_2) \\\\text{Var}_s \\\\) (23).\\n\\nObserving that \\\\( \\\\zeta_{12}[r(a_1, a_2)] = \\\\max_{a_1} \\\\text{Var}_{a_2} r(a_1, a_2) = \\\\max_{a_1} \\\\text{Var}_{a_2} q(a_1, a_2) = \\\\zeta_{12}[q(a_1, a_2)] \\\\), we have\\n\\n\\\\[\\n\\\\sigma \\\\leq \\\\zeta_{12}[q(a_1, a_2)] + 2q \\\\text{Var}_{a_2} r(a_i_2, a_2) \\\\text{Var}_s.\\n\\\\] (25)\\n\\nWhere \\\\( \\\\sigma = \\\\text{Var}_{a_2} \\\\max_{a_1} r(a_1, a_2) \\\\) and \\\\( \\\\text{Var}_s = S \\\\). According to the fixed-point theorem, the term \\\\( \\\\sigma \\\\) satisfies\\n\\n\\\\[\\n\\\\zeta_{12}[q(a_1, a_2)] + 2\\\\sqrt{\\\\sigma S} = \\\\sigma.\\n\\\\]\\n\\nWe can solve this quadratic form and get\\n\\n\\\\[\\n\\\\sigma = \\\\zeta_{12}[q(a_1, a_2)] + 2S \\\\pm 2pS(S + \\\\zeta_{12}[q(a_1, a_2)]).\\n\\\\]\\n\\nBecause the \\\\( \\\\sigma \\\\) term is larger than \\\\( \\\\zeta_{12}[q(a_1, a_2)] + 2\\\\sqrt{\\\\sigma S} \\\\), we get\\n\\n\\\\[\\n\\\\sigma = \\\\zeta_{12}[q(a_1, a_2)] + 2S + 2pS(S + \\\\zeta_{12}[q(a_1, a_2)]).\\n\\\\]\\n\\nBy inserting this inequality to the lower bound (Eq. 16), we get a lower bound related to \\\\( q(a_1, a_2) \\\\):\\n\\n\\\\[\\n2|\\\\text{A}| \\\\left( \\\\bar{m} - \\\\min_{a_2} m(a_2) \\\\right) \\\\left( \\\\max_{a_2} m(a_2) - \\\\bar{m} \\\\right) h \\\\zeta_{12}[q(a_1, a_2)] + 2M^2 + 2pM^2(M^2 + \\\\zeta_{12}[q(a_1, a_2)])i^2 - 1 \\\\right)\\n\\\\] (26).\\n\\nWhen a vector \\\\( x \\\\) is larger than 0 and the cardinality of \\\\( x \\\\) is \\\\( n \\\\), we have:\\n\\n\\\\[\\n\\\\text{Var}(x) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} (x_i - \\\\frac{1}{n} \\\\sum_{i=1}^{n} x_i)^2 \\\\leq \\\\frac{1}{n} \\\\sum_{i=1}^{n} x_i^2 \\\\leq \\\\max_i x_i^2.\\n\\\\]\\n\\nThus we can further get the following bound:\\n\\n\\\\[\\nS = \\\\text{Var}_{a_2} \\\\max_{a_1} r(a_1, a_2) - r(a_1, a_2) \\\\leq \\\\max_2 2a_2 \\\\max_{a_1} r(a_1, a_2) - r(a_1, a_2).\\n\\\\] (27)\\n\\nLet \\\\( M = \\\\max_a 2 \\\\max_{a_1} r(a_1, a_2) - r(a_1, a_2) \\\\), thus we have\\n\\n\\\\[\\nS \\\\leq M^2.\\n\\\\]\\n\\nWe have \\\\( \\\\zeta_{12}[q(a_1, a_2)] = \\\\zeta_{qvar_{12}} \\\\) by the definition (Eq. 4) and can get the final lower bound:\\n\\n\\\\[\\n2|\\\\text{A}| \\\\left( \\\\bar{m} - \\\\min_{a_2} m(a_2) \\\\right) \\\\left( \\\\max_{a_2} m(a_2) - \\\\bar{m} \\\\right) h \\\\zeta_{qvar_{12}} + 2M^2 + 2pM^2(M^2 + \\\\zeta_{qvar_{12}})i^2 - 1 \\\\right).\\n\\\\] (28)\\n\\nB MACO: MUlti-A gEnt Coordination B enchmark\\n\\nIn this paper, we study how to learn context-aware sparse coordination graphs. For this purpose, we propose a new Multi-Agent COordination (MACO) benchmark (Table 1) to evaluate different implementations and benchmark our method. This benchmark collects classic coordination problems in the literature of cooperative multi-agent learning, increases their difficulty, and classifies them into different types. We now describe the detailed settings of tasks in the MACO benchmark.\"}"}
{"id": "wQfgfb8VKTn", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Factored Games\\nare characterized by a clear factorization of global rewards. We further classify\\nfactored games into 4 categories according to whether coordination dependency is pairwise and\\nwhether the underlying coordination graph is dynamic (Table 1).\\n\\nAloha (Oliehoek (2010), also similar to the Broadcast Channel benchmark problem proposed by\\nHansen et al. (2004)) consists of\\n10 islands, each equipped with a radio tower to transmit messages\\nto its residents. Each island has a backlog of messages that it needs to send, and agents can choose\\nto send one message or not at each timestep. Due to the proximity of islands, communications\\nfrom adjacent islands interfere. This means that when two neighboring agents attempt to send\\nsimultaneously, a collision occurs and the messages have to be resent. Each island starts with\\n1 package in its backlog. At each timestep, with probability\\n0.6 a new packet arrives if the maximum\\nbacklog (set to 5) has not been reached. Each agent observes its position and the number of packages\\nin its backlog. A global reward of\\n0.1 is received by the system for each successful transmission, while punishment of\\n-10 is induced if the transmission leads to a collision.\\n\\nPursuit, also called Predator and Prey (Benda, 1986; Stone & Veloso, 2000; Son et al., 2019). In this game, ten agents (predators) roam a\\n10 \u00d7 10 map populated\\nwith 5 random walking preys for 50 environment steps. Based on the partial observation of any\\nadjacent prey and other predators, agents choose to move in four directions, keep motionless, or\\ncatch prey (specified by its id). One prey is captured if two agents catch it simultaneously, after\\nwhich the catching agents and the prey are removed from the map, resulting in a team reward of\\n1. However, if only one agent tries to catch the prey, the prey would not be captured and the agents will\\nbe punished. The difficulty of\\nPursuit is largely decided by the relative scale of the punishment\\ncompared to the catching reward (B\u00f6hmer et al., 2020), because a large punishment exacerbates the\\nrelative overgeneralization pathology. In the MACO benchmark, we consider a challenging version\\nof\\nPursuit by setting the punishment to\\n1, which is the same as the reward obtained by a successful\\ncatch.\\n\\nHallway (Wang et al., 2020) is a multi-chain Dec-POMDP whose stochasticity and partial observ-\\nability lead to fully-decomposed value functions learning sub-optimal strategies. In the MACO bench-\\nmark, we increase the difficulty of\\nHallway by introducing more agents and grouping them (Fig. 6).\\n\\nEach winning group induces a global reward of 1. Other-\\nwise, if any agent arrives at\\nearlier than others, the sys-\\nystem receives no reward and all agents in that group would\\nbe removed from the game. If\\nn\\n> 1 groups attempt\\nto move to\\nat the same timestep, they keep motionless\\nand agents receive a global punishment of\\n-0.5 \u2217 n. The\\nhorizon is set to\\nmax\\ni\\nl\\ni + 10 to avoid an infinite loop,\\nwhere\\nl\\ni\\nis the length of chain\\ni.\\n\\nSensor (Fig. 3 in the main text) has been extensively\\nstudied in cooperative multi-agent learning (Lesser et al., 2012; Zhang & Lesser, 2011). We consider 15 sensors\\narranged in a 3 by 5 matrix. Each sensor is controlled by\\nan agent and can scan the eight nearby points. Each scan induces a cost of -1, and agents can choose\\nnoop to save the cost. Three targets wander randomly in the grid. If\\nk \u2265 2 sensors scan a target\\nsimultaneously, the system gets a constant reward of 3, which is independent of the number of sensor.\\nAgents can observe the id and position of targets nearby.\\n\\nNon-factored games\\ndo not present an explicit decomposition of global rewards. We classify non-\\nfactored games according to whether the game can be solved by a static (sparse) coordination graph\\nin a single episode.\\n\\nGather is an extension of the\\nClimb Game (Wei & Luke, 2016). In\\nClimb Game, each agent has\\nthree actions:\\nA = \\\\{a_0, a_1, a_2\\\\}. Action\\na_0 yields no reward if only some agents choose it, but a high\\nreward if all choose it. The other two actions are sub-optimal actions but can induce a positive reward\\n15\"}"}
