{"id": "y81ppNf_vg", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A\\nUTO\\nTRANSFER: A\\nUTO\\nML WITH\\nKNOWLEDGE\\nTRANSER - A\\nAPPLICATION TO\\nGRAPH\\nWORKS\\nKaidi Cao Jiaxuan You Jiaju Liu Jure Leskovec\\nDepartment of Computer Science, Stanford University\\n{kaidicao, jiaxuan, jiajuliu, jure}@cs.stanford.edu\\n\\nABSTRACT\\nAutoML has demonstrated remarkable success in finding an effective neural architecture for a given machine learning task defined by a specific dataset and an evaluation metric. However, most present AutoML techniques consider each task independently from scratch, which requires exploring many architectures, leading to high computational costs. Here we propose A\\nUTO\\nTRANSFER, an AutoML solution that improves search efficiency by transferring the prior architectural design knowledge to the novel task of interest. Our key innovation includes a task-model bank that captures the model performance over a diverse set of GNN architectures and tasks, and a computationally efficient task embedding that can accurately measure the similarity among different tasks. Based on the task-model bank and the task embeddings, we estimate the design priors of desirable models of the novel task, by aggregating a similarity-weighted sum of the top-K design distributions on tasks that are similar to the task of interest. The computed design priors can be used with any AutoML search algorithm. We evaluate A\\nUTO\\nTRANSFER on six datasets in the graph machine learning domain. Experiments demonstrate that (i) our proposed task embedding can be computed efficiently, and that tasks with similar embeddings have similar best-performing architectures; (ii) A\\nUTO\\nTRANSFER significantly improves search efficiency with the transferred design priors, reducing the number of explored architectures by an order of magnitude. Finally, we release GNN-BANK-101, a large-scale dataset of detailed GNN training information of 120,000 task-model combinations to facilitate and inspire future research.\\n\\n1 INTRODUCTION\\nDeep neural networks are highly modular, requiring many design decisions to be made regarding network architecture and hyperparameters. These design decisions form a search space that is nonconvex and costly even for experts to optimize over, especially when the optimization must be repeated from scratch for each new use case. Automated machine learning (AutoML) is an active research area that aims to reduce the human effort required for architecture design that usually covers hyperparameter optimization and neural architecture search. AutoML has demonstrated success (Zoph and Le, 2016; Pham et al., 2018; Zoph et al., 2018; Cai et al., 2018; He et al., 2018; Guo et al., 2020; Erickson et al., 2020; LeDell and Poirier, 2020) in many application domains.\\n\\nFinding a reasonably good model for a new learning task in a computationally efficient manner is crucial for making deep learning accessible to domain experts with diverse backgrounds. Efficient AutoML is especially important in domains where the best architectures/hyperparameters are highly sensitive to the task. A notable example is the domain of graph learning.\\n\\nFirst, graph learning methods receive input data composed of a variety of data types and optimize over tasks that span an equally diverse set of domains and modalities such as recommendation (Ying et al., 2018; He et al., 2020), physical simulation (Sanchez-Gonzalez et al., 2020; Pfaff et al., 2020), and bioinformatics (Zitnik et al., 2018). This differs from computer vision and natural language processing where the...\"}"}
{"id": "y81ppNf_vg", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"input data has a predefined, fixed structure that can be shared across different neural architectures. Second, neural networks that operate on graphs come with a rich set of design choices and a large set of parameters to explore. However, unlike other domains where a few pre-trained architectures such as ResNet (He et al., 2016) and GPT-3 (Brown et al., 2020) dominate the benchmarks, it has been shown that the best graph neural network (GNN) design is highly task-dependent (You et al., 2020). Although AutoML as a research domain is evolving fast, existing AutoML solutions have massive computational overhead when finding a good model for a new learning task is the goal. Most present AutoML techniques consider each task independently and in isolation, therefore they require redoing the search from scratch for each new task. This approach ignores the potentially valuable architectural design knowledge obtained from the previous tasks, and inevitably leads to a high computational cost. The issue is especially significant in the graph learning domain Gao et al. (2019); Zhou et al. (2019), due to the challenges of diverse task types and the huge design space that are discussed above. Here we propose AUTO-TRANSFER 3, an AutoML solution that drastically improves AutoML architecture search by transferring previous architectural design knowledge to the task of interest. Our key innovation is to introduce a task-model bank that stores the performance of a diverse set of GNN architectures and tasks to guide the search algorithm. To enable knowledge transfer, we define a task embedding space such that tasks close in the embedding space have similar corresponding top-performing architectures. The challenge here is that the task embedding needs to capture the performance rankings of different architectures on different datasets, while being efficient to compute. Our innovation here is to embed a task by using the condition number of its Fisher Information Matrix of various randomly initialized models and also a learning scheme with an empirical generalization guarantee. This way we implicitly capture the properties of the learning task, while being orders of magnitudes faster (within seconds). We then estimate the design prior of desirable models for the new task, by aggregating design distributions on tasks that are close to the task of interest. Finally, we initiate a hyperparameter search algorithm with the task-informed design prior computed. We evaluate AUTO-TRANSFER 3 on six datasets, including both node classification and graph classification tasks. We show that our proposed task embeddings can be computed efficiently and the distance measured between tasks correlates highly (0.43 Kendall correlation) with model performance rankings. Furthermore, we present AUTO-TRANSFER 3 significantly improves search efficiency when using the transferred design prior. AUTO-TRANSFER 3 reduces the number of explored architectures needed to reach a target accuracy by an order of magnitude compared to SOTA. Finally, we release GNN-BANK-101 \u2014the first large-scale database containing detailed performance records for 120,000 task-model combinations which were trained with 16,128 GPU hours\u2014to facilitate future research.\\n\\n2 RELATED WORK\\n\\nIn this section, we summarize the related work on AutoML regarding its applications on GNNs, the common search algorithms, and pioneering work regarding transfer learning and task embeddings.\\n\\nAutoML for GNNs. Neural architecture search (NAS), a unique and popular form of AutoML for deep learning, can be divided into two categories: multi-trial NAS and one-shot NAS. During multi-trial NAS, each sampled architecture is trained separately. GraphNAS (Gao et al., 2020) and Auto-GNN (Zhou et al., 2019) are typical multi-trial NAS algorithms on GNNs which adopt an RNN controller that learns to suggest better sets of configurations through reinforcement learning. One-shot NAS (e.g., (Liu et al., 2018; Qin et al., 2021; Li et al., 2021)) involves encapsulating the entire model space in one super-model, training the super-model once, and then iteratively sampling sub-models from the super-model to find the best one. In addition, there is work that explicitly studies fine-grained design choices such as data augmentation (You et al., 2021), message passing layer type (Cai et al., 2021; Ding et al., 2021; Zhao et al., 2021), and graph pooling (Wei et al., 2021). Notably, AUTO-TRANSFER 3 is the first AutoML solution for GNNs that efficiently transfer design knowledge across tasks.\\n\\nHPO Algorithms. Hyperparameter Optimization (HPO) algorithms search for the optimal model hyperparameters by iteratively suggesting a set of hyperparameters and evaluating their performance. Random search samples hyperparameters from the search space with equal probability. Despite not...\"}"}
{"id": "y81ppNf_vg", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"**Task-Model Bank (historical performance records)**\\n\\n**Task Embedding Space**\\n\\n**Design Distributions**\\n\\n| Task | Agg | Dim | Epoch | ... | Val_loss |\\n|------|-----|-----|-------|-----|---------|\\n| Cora | sum | 64  | 80    | ... | 0.22    |\\n| Cora | mean| 128 | 200   | ... | 0.26    |\\n| TU-DD | sum | 64  | 200   | ... | 0.46    |\\n| TU-DD | mean| 128 | 200   | ... | 0.86    |\\n| TU-DD | max | 256 | 800   | ... | 0.52    |\\n| Arxiv | mean | 128 | 200 | ... | 0.68    |\\n\\n**Figure 1:** Overview of A\\\\textsubscript{UTO}TRANSFER. **Left:** We introduce GNN-BANK-101, a large database containing a diverse set of GNN architectures and hyperparameters applied to different tasks, along with their training/evaluation statistics. **Middle:** We introduce a task embedding space, where each point corresponds to a different task. Tasks close in the embedding space have similar corresponding top-performing models. **Right:** Given a new task of interest, we guide the AutoML search by referencing the design distributions of the most similar tasks in the task embedding space.\\n\\nLearning from previous trials, random search is commonly used for its simplicity and is much more efficient than grid search (Bergstra and Bengio, 2012). The TPE algorithm (Bergstra et al., 2011) builds a probabilistic model of task performance over the hyperparameter space and uses the results of past trials to choose the most promising next configuration to train, which the TPE algorithm defines as maximizing the Expected Improvement value (Jones, 2001). Evolutionary algorithms (Real et al., 2017; Jaderberg et al., 2017) train multiple models in parallel and replace poorly performing models with \u201cmutated\u201d copies of the current best models. A\\\\textsubscript{UTO}TRANSFER is a general AutoML solution and can be applied in combination with any of these HPO algorithms.\\n\\n**Transfer Learning in AutoML.** Wong et al. (2018) proposed to transfer knowledge across tasks by reloading the controller of reinforcement learning search algorithms. However, this method assumes that the search space on different tasks starts with the same learned prior. Unlike A\\\\textsubscript{UTO}TRANSFER, it cannot address the core challenge in GNN AutoML: the best GNN design is highly task-specific.\\n\\nGraphGym (You et al., 2020) attempts to transfer the best architecture design directly with a metric space that measures task similarity. GraphGym (You et al., 2020) computes task similarity by training a set of 12 \u201canchor models\u201d to convergence which is computationally expensive. In contrast, A\\\\textsubscript{UTO}TRANSFER designs light-weight task embeddings requiring minimal computations overhead. Additionally, Zhao and Bilen (2021); Li et al. (2021) proposes to conduct architecture search on a proxy subset of the whole dataset and later transfer the best searched architecture on the full dataset. Jeong et al. (2021) studies a similar setting in vision domain.\\n\\n**Task Embedding.** There is prior research trying to quantify task embeddings and similarities. Similar to GraphGym, Taskonomy (Zamir et al., 2018) estimates the task affinity matrix by summarizing final losses/evaluation metrics using an Analytic Hierarchy Process (Saaty, 1987). From a different perspective, Task2Vec (Achille et al., 2019) generates task embeddings for a given task using the Fisher Information Matrix associated with a pre-trained probe network. This probe network is shared across tasks and allows Task2Vec to estimate the Fisher Information Matrix of different image datasets. Le et al. (2022) extends a similar idea to neural architecture search. The aforementioned task embeddings cannot be directly applied to GNNs as the inputs do not align across datasets. A\\\\textsubscript{UTO}TRANSFER avoids the bottleneck by using asymptotic statistics of the Fisher Information Matrix with randomly initiated weights.\\n\\n### Problem Formulation and Preliminaries\\n\\nWe first introduce formal definitions of data structures relevant to A\\\\textsubscript{UTO}TRANSFER.\"}"}
{"id": "y81ppNf_vg", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Definition 1 (Task) We denote a task as $T = (D, L(\u00b7))$, consisting of a dataset $D$ and a loss function $L(\u00b7)$ related to the evaluation metric.\\n\\nFor each training attempt on a task $T_i$, we can record its model architecture $M_j$, hyperparameters $H_j$, and corresponding value of loss $l_j$, i.e., $(M_j, H_j, l_j)$. We propose to maintain a task-model bank to facilitate knowledge transfer to future novel tasks.\\n\\nDefinition 2 (Task-Model Bank) A task-model bank $B$ is defined as a collection of tasks, each with multiple training attempts, in the form of $B = \\\\{ (T_i, \\\\{(M_{ij}, H_{ij}, l_{ij})\\\\}) \\\\}$.\\n\\nAutoML with Knowledge Transfer. Suppose we have a task-model bank $B$. Given a novel task $T_n$ which has not been seen before, our goal is to quickly find a model that works reasonably well on the novel task by utilizing knowledge from the task-model bank.\\n\\nIn this paper, we focus on AutoML for graph learning tasks, though our developed technique is general and can be applied to other domains. We define the input graph as $G = \\\\{V, E\\\\}$, where $V$ is the node set and $E \\\\subseteq V \\\\times V$ is the edge set. Furthermore, let $y$ denote its output labels, which can be node-level, edge-level or graph-level. A GNN parameterized by weights $\\\\theta$ outputs a posterior distribution $P(G, y, \\\\theta)$ for label predictions.\\n\\n4 PROPOSED SOLUTION: AUTO TRANSFER\\n\\nIn this section, we introduce the proposed AUTO TRANSFER solution. AUTO TRANSFER uses the task embedding space as a tool to understand the relevance of previous architectural designs to the target task. The designed task embedding captures the performance rankings of different architectures on different tasks while also being efficient to compute. We first introduce a theoretically motivated solution to extract a scale-invariant performance representation of each task-model pair. We use these representations to construct task features and further learn task embeddings. These embeddings form the task embedding space that we finally use during the AutoML search.\\n\\n4.1 BASICS OF THE FISHER INFORMATION MATRIX (FIM)\\n\\nGiven a GNN defined above, its Fisher Information Matrix (FIM) $F$ is defined as $F = E_{G,y} \\\\left[ \\\\nabla_\\\\theta \\\\log P(G, y, \\\\theta) \\\\nabla_\\\\theta \\\\log P(G, y, \\\\theta)^\\\\top \\\\right]$, which formally is the expected covariance of the scores with respect to the model parameters. There are two popular geometric views for the FIM. First, the FIM is an upper bound of the Hessian and coincides with the Hessian if the gradient is 0. Thus, the FIM characterizes the local landscape of the loss function near the global minimum. Second, similar to the Hessian, the FIM models the loss...\"}"}
{"id": "y81ppNf_vg", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"landscape with respect not to the input space, but to the parameter space. In the information geometry view, if we add a small perturbation to the parameter space, we have\\n\\\\[ KL(P_{G, y, \\\\theta}) \\\\parallel P_{G, y, \\\\theta + d\\\\theta}) = d\\\\theta^\\\\top F d\\\\theta. \\\\]\\n\\nwhere \\\\( KL(\\\\cdot, \\\\cdot) \\\\) stands for Kullback\u2013Leibler divergence. It means that the parameter space of a model forms a Riemannian manifold and the FIM works as its Riemannian metric. The FIM thus allows us to quantify the importance of a model's weights in a way that is applicable to different architectures.\\n\\n### 4.2 FIM-based Task Features\\n\\nScale-invariant Representation of Task-Model Pairs.\\n\\nWe aim to find a scale-invariant representation for each task-model pair which will form the basis for constructing task features. The major challenge in using the FIM to represent GNN performance is that graph datasets do not have a universal, fixed input structure, so it is infeasible to find a single pre-trained model and extract its FIM. However, training multiple networks poses a problem as the FIMs computed for different networks are not directly comparable. We choose to use multiple networks but additionally propose to use asymptotic statistics of the FIM associated with randomly initialized weights. The theoretical justification for the relationship between the asymptotic statistics of the FIM and the trainability of neural networks was studied in (Karakida et al., 2019; Pennington and Worah, 2018) to which we refer the readers. We hypothesize that such a measure of trainability encodes loss landscapes and generalization ability and thus correlates with final model performance on the task. Another issue that relates to input structures of graph datasets is that different models have different number of parameters. Despite some specially designed architectures, e.g., (Lee et al., 2019; Ma et al., 2019), most GNN architecture design can be represented as a sequence of pre-processing layers, message passing layers, and post-processing layers. Pre-process layers and post-process layers are Multilayer Perceptron (MLP) layers, of which the dimensions vary across different tasks due to different input/output structures. Message passing layers are commonly regarded as the key design for GNNs and the number of weight parameters can remain the same across tasks. In this light, we only consider the FIM with respect to the parameters in message passing layers so that the number of parameters considered stays the same for all datasets. We note that such formulation has its limitations, in the sense that it cannot cover all the GNN designs in the literature. We leave potential extensions with better coverage for future work. We further approximate the FIM by only considering the diagonal entries, which implicitly neglects the correlations between parameters. We note that this is common practice when analyzing the FIMs of deep neural networks, as the full FIM is massive (quadratic in the number of parameters) and infeasible to compute even on modern hardware. Similar to Pennington and Worah (2018), we consider the first two moments of FIM:\\n\\n\\\\[ m_1 = \\\\frac{1}{n} \\\\text{tr}[F] \\\\]\\n\\\\[ m_2 = \\\\frac{1}{n} \\\\text{tr}[F^2] \\\\]\\n\\nand use \\\\( \\\\alpha = \\\\frac{m_2}{m_1} \\\\) as the scale-invariant representation. The computed \\\\( \\\\alpha \\\\) is lower bounded by 1 and captures how concentrated the spectrum is. A small \\\\( \\\\alpha \\\\) indicates the loss landscape is flat, and its corresponding model design enjoys fast first-order optimization and potentially better generalization.\\n\\nTo encode label space information into each task, we propose to train only the last linear layer of each model on a given task, which can be done efficiently. The parameters in other layers are frozen after being randomly initialized. We take the average over \\\\( R \\\\) initializations to estimate the average \\\\( \\\\bar{\\\\alpha} \\\\).\\n\\n### 4.3 From Task Features to Task Embeddings\\n\\nThe task feature \\\\( \\\\bar{\\\\alpha} \\\\) introduced above can be regarded as a means of feature engineering. We construct the feature vector with domain knowledge, but there is no guarantee it functions as anticipated. We thus propose to learn a projection function \\\\( g(\\\\cdot) : \\\\mathbb{R}^U \\\\rightarrow \\\\mathbb{R}^D \\\\) that maps task feature \\\\( \\\\bar{\\\\alpha} \\\\) to final task embedding.\\n\\n5\"}"}
{"id": "y81ppNf_vg", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Runtime analysis. We have empirically shown that AUTO TRANSFER can significantly improve search efficiency by reducing the number of trials needed to achieve reasonably good accuracy. The only overhead we introduced is the procedure of estimating task embeddings. Since we use a randomly initialized architecture, extracting each task feature only requires at most one forward and a few backward passes from a single minibatch of data. The wall-clock time depends on the size of the network and data structure. In our experiments, it typically takes a few seconds on an NVIDIA T4 GPU. We repeat the task feature extraction process 5 times for each anchor model, and for a total of 12 anchor models. Thus, the wall-clock time of the overhead for computing task embedding of novel task is within a few minutes. We note the length of this process is generally comparable to one trial of training on a small-sized dataset, and the time saved is much more significant for large-scale datasets.\\n\\nDetails for task-model-bank training. Our GNN model specifications are summarized in Table 3. Our code base was developed based on GraphGym (You et al., 2020). For all the training trials, we use the Adam optimizer and cosine learning rate scheduler (annealed to 0, no restarting). We use L2 regularization with a weight decay of 5e-4. We record losses and accuracies for training, validation and test splits every 20 epochs.\\n\\nTraining details for AUTO TRANSFER. We summarize the training procedure for the projection function $g(\\\\cdot)$ in Algorithm 2. We set $U = 12$ and $R = 5$ throughout the paper. We use the same set of anchor model designs as those in GraphGym. We use a two-layer MLP with a hidden dimension of 16 to parameterize the projection function $g(\\\\cdot)$. We use the Adam optimizer with a learning rate of 5e-3. We use margin $= 0.1$ and train the network for 1000 iterations with a batch size of 128. We adopt $K = 16$ when selecting the top $K$ trials that are used to summarize design distributions.\\n\\nDetails for adapting TPE, evolution algorithms. We hereby illustrate how to compose the search algorithms with the transferred design priors. TPE is a Bayesian hyperparameter optimization method, meaning that it is initialized with a prior distribution to model the search space and updates the prior as it evaluates hyperparameter configurations and records their performance. We replace this prior distribution with the task-informed design priors. As evolutionary algorithms generally initialize a large population and iteratively prune and mutate existing networks, we replace the random network initialization with the task-informed design priors. As we mainly focus on the few trial search regime, we set the fully random warm-up trials to 5 for both TPE and evolution algorithms.\\n\\nTable 3: Design choices in our search space\\n\\n| Type          | Choices                                      |\\n|---------------|----------------------------------------------|\\n| Convolution   | GeneralConv, GCNConv, SAGEConv, GINConv, GATConv |\\n| Number of heads | 1, 2, 4                                      |\\n| Aggregation   | Sum, Mean-Pooling, Max-Pooling               |\\n| Activation    | ReLU, pReLU, leaky_ReLU, ELU                 |\\n| Hidden dimension | 64, 256                                     |\\n| Layer connectivity | Stack, Skip-Sum, Skip-Concat                |\\n| Pre-process layers | 1, 2                                        |\\n| Message passing layers | 2, 4, 6, 8                                  |\\n| Post-process layers | 2, 3                                        |\\n| Learning rate | 0.1, 0.001                                   |\\n| Training epochs | 200, 800, 1600                              |\\n\\nLimitations. In principle, AUTO TRANSFER leverages the correlation between model performance rankings among tasks to efficiently construct model priors. Thus, it is less effective if the novel task has large task distances with respect to all tasks in the task-model bank. In practice, users can continuously add additional search trials to the bank. As the bank size grows, it will be less likely that a novel task has a low correlation with all tasks in the bank.\"}"}
{"id": "y81ppNf_vg", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 2\\n\\nTraining Pipeline for the projection function $g(\\\\cdot)$\\n\\nRequire:\\n- Task features $\\\\{z(i)_f|T(i)\\\\}$\\n\\n1: for each iteration do\\n2: Sample $T(i)$, $T(j)$, $T(k)$\\n3: $z(i)_e, z(j)_e, z(k)_e \\\\leftarrow g(z(i)_f), g(z(j)_f), g(z(k)_f)$\\n4: $y \\\\leftarrow 1$ if $d(g(T(i), T(j)) < d(g(T(i), T(k)))$ else $-1$\\n5: Optimize objective function $L_r(z(i)_e, z(j)_e, z(k)_e, y)$ in Eq. 2\\n6: end for\\n\\nSocial impact.\\n\\nOur long-term goal is to provide a seamless GNN infrastructure that simplifies the training and deployment of ML models on structured data. Efficient and robust AutoML algorithms are crucial to making deep learning more accessible to people who are interested but lack deep learning expertise as well as those who lack the massive computational budget AutoML traditionally requires. We believe this paper is an important step to provide AI tools to a broader population and thus allow for AI to help enhance human productivity. The datasets we used for experiments are among the most widely-used benchmarks, which should not contain any undesirable bias. Besides, training/test losses and accuracies are highly summarized statistics that we believe should not incur potential privacy issues.\\n\\n### Additional Results\\n\\n**Search efficiency.**\\n\\nWe summarize the average number of trials needed to surpass the average best accuracy found by TPE with 30 trials in Table 4. We show that AUTO TRANSFER reduces the number of explored architectures by an order of magnitude.\\n\\n| Node | Graph | Physics CoraFull OGB-ArxivCOX2 IMDB PROTEINS |\\n|------|-------|-----------------------------------------------|\\n| Num. of Trials | 3 | 2 | 3 | 4 | 3 | 6 |\\n| Accuracy | 96.64 \u00b1 0.42 | 67.85 \u00b1 1.31 | 71.42 \u00b1 0.39 | 82.96 \u00b1 1.75 | 52.33 \u00b1 2.13 | 80.21 \u00b1 1.21 |\\n\\n**Ablation study on number of anchor models and task embedding design.**\\n\\nWe empirically demonstrate how the number of anchor models affect the rank correlation in Table 5. While 3 anchor models are not enough to capture the task distance, we found that 9 and 12 have a satisfactory trade-off between capturing task distance and computational efficiency. Furthermore, we empirically in Table 6 demonstrate that the learned task embedding space is superior to the proposed task feature space in terms of correlation as well as final search performance.\\n\\n| Num. of anchor models | 3 | 6 | 9 | 12 |\\n|-----------------------|---|---|---|----|\\n| Task Feature          | 0.03 \u00b1 0.34 | 0.11 \u00b1 0.36 | 0.16 \u00b1 0.34 | 0.18 \u00b1 0.30 |\\n| Task Embedding        | 0.12 \u00b1 0.28 | 0.26 \u00b1 0.30 | 0.36 \u00b1 0.24 | 0.43 \u00b1 0.22 |\\n\\n**Visualizing model designs.**\\n\\nWe visualized the transferred design distributions and ground truth design distributions on the TU-PROTEINS dataset in Figure 5, as well as Coauthor-Physics dataset in Figure 6. We could observe that the transferred design distributions have a positive correlation on most of the design choices.\"}"}
{"id": "y81ppNf_vg", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"Table 6: Ablation study of the number of anchor models as well as task embedding vs. task feature on OGB-Arxiv with 3 trials. We report the average test accuracy and the standard deviation over ten runs.\\n\\n| Num. of anchor models | Task Feature | Task Embedding |\\n|-----------------------|--------------|----------------|\\n| 3                     | 69.42 \u00b1 0.82 | 69.80 \u00b1 0.75  |\\n| 6                     | 69.86 \u00b1 0.78 | 70.59 \u00b1 0.63  |\\n| 9                     | 70.41 \u00b1 0.59 | 71.16 \u00b1 0.47  |\\n| 12                    | 70.67 \u00b1 0.52 | 71.42 \u00b1 0.39  |\\n\\nFigure 5: We plot the transferred design distributions and ground truth design distributions on the TU-PROTEINS dataset.\\n\\nFigure 6: We plot the transferred design distributions and ground truth design distributions on the Coauthor-Physics dataset.\"}"}
{"id": "y81ppNf_vg", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGEMENTS\\n\\nWe thank Xiang Lisa Li, Hongyu Ren, Yingxin Wu for discussions and for providing feedback on our manuscript. We also gratefully acknowledge the support of DARPA under Nos. HR00112190039 (TAMI), N660011924033 (MCS); ARO under Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP); NSF under Nos. OAC-1835598 (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), NIH under No. 3U54HG010426-04S1 (HuBMAP), Stanford Data Science Initiative, Wu Tsai Neurosciences Institute, Amazon, Docomo, GSK, Hitachi, Intel, JPMorgan Chase, Juniper Networks, KDDI, NEC, and Toshiba. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\"}"}
{"id": "y81ppNf_vg", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C. Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6430\u20136439, 2019.\\n\\nJames Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of machine learning research, 13(2), 2012.\\n\\nJames Bergstra, R\u00e9mi Bardenet, Yoshua Bengio, and Bal\u00e1zs K\u00e9gl. Algorithms for hyper-parameter optimization. Advances in neural information processing systems, 24, 2011.\\n\\nAleksandar Bojchevski and Stephan G\u00fcnnemann. Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking. arXiv preprint arXiv:1707.03815, 2017.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\nHan Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task and hardware. arXiv preprint arXiv:1812.00332, 2018.\\n\\nShaofei Cai, Liang Li, Jincan Deng, Beichen Zhang, Zheng-Jun Zha, Li Su, and Qingming Huang. Re-thinking graph neural architecture search from message-passing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6657\u20136666, 2021.\\n\\nYuhui Ding, Quanming Yao, Huan Zhao, and Tong Zhang. Diffmg: Differentiable meta graph search for heterogeneous graph neural networks. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 279\u2013288, 2021.\\n\\nNick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola. Autogluon-tabular: Robust and accurate automl for structured data. ICML 2020 Workshop on Automated Machine Learning, 2020.\\n\\nYang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. Graphnas: Graph neural architecture search with reinforcement learning. arXiv preprint arXiv:1904.09981, 2019.\\n\\nYang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. Graph neural architecture search. In Christian Bessiere, editor, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 1403\u20131409. International Joint Conferences on Artificial Intelligence Organization, 7 2020. URL https://doi.org/10.24963/ijcai.2020/195.\\n\\nZichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. In European Conference on Computer Vision, pages 544\u2013560. Springer, 2020.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\nXiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 639\u2013648, 2020.\\n\\nYihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European conference on computer vision (ECCV), pages 784\u2013800, 2018.\\n\\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118\u201322133, 2020.\"}"}
{"id": "y81ppNf_vg", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "y81ppNf_vg", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.\\n\\nLanning Wei, Huan Zhao, Quanming Yao, and Zhiqiang He. Pooling architecture search for graph classification. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 2091\u20132100, 2021.\\n\\nCatherine Wong, Neil Houlsby, Yifeng Lu, and Andrea Gesmundo. Transfer learning with neural automl. Advances in neural information processing systems, 31, 2018.\\n\\nChengrun Yang, Yuji Akimoto, Dae Won Kim, and Madeleine Udell. Oboe: Collaborative filtering for automl model selection. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 2019.\\n\\nZhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pages 40\u201348. PMLR, 2016.\\n\\nRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pages 974\u2013983, 2018.\\n\\nJiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. Advances in Neural Information Processing Systems, 33:17009\u201317021, 2020.\\n\\nYuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. In International Conference on Machine Learning, pages 12121\u201312132. PMLR, 2021.\\n\\nAmir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3712\u20133722, 2018.\\n\\nBo Zhao and Hakan Bilen. Dataset condensation with differentiable siamese augmentation. In International Conference on Machine Learning, pages 12674\u201312685. PMLR, 2021.\\n\\nHuan Zhao, Quanming Yao, and Weiwei Tu. Search to aggregate neighborhood for graph neural network. arXiv preprint arXiv:2104.06608, 2021.\\n\\nKaixiong Zhou, Qingquan Song, Xiao Huang, and Xia Hu. Auto-gnn: Neural architecture search of graph neural networks. arXiv preprint arXiv:1909.03184, 2019.\\n\\nMarinka Zitnik, Monica Agrawal, and Jure Leskovec. Modeling polypharmacy side effects with graph convolutional networks. Bioinformatics, 34(13):i457\u2013i466, 2018.\\n\\nBarret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.\\n\\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8697\u20138710, 2018.\"}"}
{"id": "y81ppNf_vg", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"embedding \\\\( z_e = g(z_f) \\\\). We do not have any pointwise supervision that can be used as the training objective. Instead, we consider the metric space defined by GraphGym. The distance function in GraphGym - computed using the Kendall rank correlation between performance rankings of anchor models trained on the two compared tasks - correlates nicely with our desired knowledge transfer objective. It is not meaningful to enforce that task embeddings mimic GraphGym's exact metric space, as GraphGym's metric space can still contain noise, or does not fully align with the transfer objective. We consider a surrogate loss that enforces only the rank order among tasks. To illustrate, let us consider tasks \\\\( T(i), T(j), T(k) \\\\) and their corresponding task embeddings, \\\\( z_e(i), z_e(j), z_e(k) \\\\). Note that \\\\( z_e \\\\) is normalized to 1 so \\\\( z_e(i)^\\\\top z_e(j) \\\\) measures the cosine similarity between tasks \\\\( T(i) \\\\) and \\\\( T(j) \\\\).\\n\\nLet \\\\( d_g(\\\\cdot, \\\\cdot) \\\\) denote the distance estimated by GraphGym. We want to enforce \\\\( z_e(i)^\\\\top z_e(j) > z_e(i)^\\\\top z_e(k) \\\\) if \\\\( d_g(T(i), T(j)) < d_g(T(i), T(k)) \\\\).\\n\\nTo achieve this, we use the margin ranking loss as our surrogate supervised objective function:\\n\\n\\\\[\\nL_r(z_e(i), z_e(j), z_e(k), y) = \\\\max(0, -y \\\\cdot (z_e(i)^\\\\top z_e(j) - z_e(i)^\\\\top z_e(k)) + \\\\text{margin})\\n\\\\]\\n\\nHere if \\\\( d_g(T(i), T(j)) < d_g(T(i), T(k)) \\\\), then we have its corresponding label \\\\( y = 1 \\\\), and \\\\( y = -1 \\\\) otherwise. Our final task embedding space is then a FIM-based metric space with cosine distance function, where the distance is defined as \\\\( d_e(T(i), T(j)) = 1 - z_e(i)^\\\\top z_e(j) \\\\). Please refer to the detailed training pipeline at Algorithm 2 in the Appendix.\\n\\n4.4 AUTOML SEARCH ALGORITHM WITH TASK EMBEDDINGS\\n\\nTo transfer knowledge to a novel task, a na\u00efve idea would be to directly carry over the best model configuration from the closest task in the bank. However, even a high Kendall rank correlation between model performance rankings of two tasks \\\\( T(i), T(j) \\\\) does not guarantee the best model configuration in task \\\\( T(i) \\\\) will also achieve the best performance on task \\\\( T(j) \\\\). In addition, since task similarities are subject to noise, this na\u00efve solution may struggle when there exist multiple reference tasks that are all highly similar.\\n\\nTo make the knowledge transfer more robust to such failure cases, we introduce the notion of design distributions that depend on top performing model designs and propose to transfer design distributions rather than the best design configurations. Formally, consider a task \\\\( T(i) \\\\) in the task-model bank \\\\( B \\\\), associated with its trials \\\\( \\\\{(M(j), H(j), l(j))\\\\} \\\\). We can summarize its designs as a list of configurations \\\\( C = \\\\{c_1, \\\\ldots, c_W\\\\} \\\\), such that all potential combinations of model architectures \\\\( M \\\\) and hyperparameters \\\\( H \\\\) fall under the Cartesian product of the configurations. For example, \\\\( c_1 \\\\) could be the instantiation of aggregation layers, and \\\\( c_2 \\\\) could be the start learning rate. We then define design distributions as random variables \\\\( c_1, c_2, \\\\ldots, c_W \\\\), each corresponding to a hyperparameter.\\n\\nEach random variable \\\\( c \\\\) is defined as the frequency distribution of the design choices used in the top \\\\( K \\\\) trials. We multiply all distributions for the individual configurations \\\\( \\\\{c_1, \\\\ldots, c_W\\\\} \\\\) to approximate the overall task's design distribution \\\\( P(C|T(i)) = Q_w P(c_w|T(i)) \\\\).\\n\\nDuring inference, given a novel task \\\\( T(n) \\\\), we select a close task subset \\\\( S \\\\) by thresholding task embedding distances, i.e., \\\\( S = \\\\{T(i) | d_e(T(n), T(i)) \\\\leq d_{\\\\text{thres}}\\\\} \\\\). We then derive the transferred design prior \\\\( P_t(C|T(n)) \\\\) of the novel task by weighting design distributions from the close task subset \\\\( S \\\\).\\n\\n\\\\[\\nP_t(C|T(n)) = \\\\sum_{T(i) \\\\in S} d_e(T(n), T(i)) P(C|T(i))\\n\\\\]\\n\\nThe inferred design prior for the novel task can then be used to guide various search algorithms. The most natural choice for a few trial regime is random search. Rather than sampling each design configuration following a uniform distribution, we propose to sample from the task-informed design prior \\\\( P_t(C|T(n)) \\\\). Please refer to Appendix A to check how we augment other search algorithms.\\n\\nFor AUTOTRANSFER, we can preprocess the task-model bank \\\\( B \\\\) into \\\\( B_p = \\\\{(D(i), L(i)(\\\\cdot)), z_e(i), P(C|T(i))\\\\} \\\\) as our pipeline only requires using task embedding \\\\( z_e \\\\) and design distribution \\\\( P(C|T(i)) \\\\) rather than detailed training trials. A detailed search pipeline is summarized in Algorithm 1.\"}"}
{"id": "y81ppNf_vg", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\n\\nSummary of Algorithm\\n\\nRequire:\\n- A processed task-model bank\\n\\n\\\\[ B_p = \\\\{ (D_i, L_i \\\\cdot z_i, e, P(C|T_i)) \\\\}, \\\\]\\n\\nwhere \\\\( n \\\\) is a novel task.\\n\\n1: for \\\\( u = 1 \\\\) to \\\\( U \\\\) do\\n2: \\\\hspace{1em} for \\\\( r = 1 \\\\) to \\\\( R \\\\) do\\n3: \\\\hspace{2em} Initialize weights \\\\( \\\\theta \\\\) for anchor model \\\\( M_u \\\\) randomly\\n4: \\\\hspace{2em} Estimate FIM \\\\( F \\\\leftarrow -ED\\\\left[ \\\\nabla_\\\\theta \\\\log P(M_u, y, \\\\theta) \\\\nabla_\\\\theta \\\\log P(M_u, y, \\\\theta)^\\\\top \\\\right] \\\\)\\n5: \\\\hspace{2em} Extract scale-invariant representation \\\\( a(v_u) \\\\leftarrow m_2 / m_2 \\\\) following Eq. 1\\n6: \\\\hspace{2em} end for\\n7: \\\\hspace{1em} \\\\( \\\\bar{a}_u \\\\leftarrow \\\\text{mean}(a(1)_u, a(2)_u, ..., a(V)_u) \\\\)\\n8: \\\\hspace{1em} end for\\n9: \\\\hspace{1em} \\\\( z(n)_f \\\\leftarrow \\\\text{concat}(\\\\bar{a}_1, \\\\bar{a}_2, ..., \\\\bar{a}_U) \\\\)\\n10: \\\\hspace{1em} \\\\( z(n)_e \\\\leftarrow g(z(n)_f) \\\\)\\n11: \\\\hspace{1em} Select close task subset \\\\( S \\\\leftarrow \\\\{ T_i | 1 - z(n)_e^\\\\top z(i)_e \\\\leq d_{\\\\text{thres}} \\\\} \\\\)\\n12: \\\\hspace{1em} Get design prior \\\\( P_t(C|T_n) \\\\) by aggregating subset \\\\( S \\\\) following Eq. 3\\n13: \\\\hspace{1em} Start a HPO search algorithm with the task-informed design prior \\\\( P_t(C|T_n) \\\\)\\n\\n5 Experiments\\n\\n5.1 Experimental Setup\\n\\nTask-Model Bank: GNN-BANK-101.\\n\\nTo facilitate AutoML research with knowledge transfer, we collected GNN-BANK-101 as the first large-scale graph database that records reproducible design configurations and detailed training performance on a variety of tasks. Specifically, GNN-BANK-101 currently includes six tasks for node classification (AmazonComputers (Shchur et al., 2018), AmazonPhoto (Shchur et al., 2018), CiteSeer (Yang et al., 2016), CoauthorCS (Shchur et al., 2018), CoauthorPhysics (Shchur et al., 2018), Cora (Yang et al., 2016)) and six tasks for graph classification (PROTEINS (Ivanov et al., 2019), BZR (Ivanov et al., 2019), COX2 (Ivanov et al., 2019), DD (Ivanov et al., 2019), ENZYMES (Ivanov et al., 2019), IMDB-M (Ivanov et al., 2019)). Our design space follows (You et al., 2020), and we extend the design space to include various commonly adopted graph convolution and activation layers. We extensively run 10,000 different models for each task, leading to 120,000 total task-model combinations, and record all training information including train/val/test loss.\\n\\nBenchmark Datasets.\\n\\nWe benchmark UTRANSFER on six different datasets following prior work (Qin et al., 2021). Our datasets include three standard node classification datasets (CoauthorPhysics (Shchur et al., 2018), CoraFull (Bojchevski and G\u00fcnnemann, 2017) and OGB-Arxiv (Hu et al., 2020)), as well as three standard benchmark graph classification datasets, (COX2 (Ivanov et al., 2019), IMDB-M (Ivanov et al., 2019) and PROTEINS (Ivanov et al., 2019)). CoauthorPhysics and CoraFull are transductive node classification datasets, so we randomly assign nodes into train/valid/test sets following a 50%:25%:25% split (Qin et al., 2021). We randomly split graphs following a 80%:10%:10% split for the three graph classification datasets (Qin et al., 2021). We follow the default train/valid/test split for the OGB-Arxiv dataset (Hu et al., 2020). To make sure there is no information leakage, we temporarily remove all records related to the task from our task-model bank if the dataset we benchmark was collected in the task-model bank.\\n\\nBaselines.\\n\\nWe compare our methods with the state-of-the-art approaches for GNN AutoML. We use GCN and GAT with default architectures following their original implementation as baselines. For multi-trial NAS methods, we consider GraphNAS (Gao et al., 2020). For one-shot NAS methods, we include DARTS (Liu et al., 2018) and GASSO (Qin et al., 2021). GASSO is designed for transductive settings, so we omit it for graph classification benchmarks. We further provide results of HPO algorithms based on our proposed search space as baselines: Random, Evolution, TPE (Bergstra et al., 2011) and HyperBand (Li et al., 2017). We by default allow searching 30 trials maximum for all the algorithms, i.e., an algorithm can train 30 different models and collect the model with the best accuracy. We use the default setting for one-shot experiments.\"}"}
{"id": "y81ppNf_vg", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Performance comparisons of A\\nUTO\\nT\\nRANSFER\\nand other baselines. We report the average\\n\\ntest accuracy and the standard deviation over ten runs. With only 3 trials A\\nUTO\\nT\\nRANSFER\\nalready\\noutperform most SOTA baselines with 30 trials.\\n\\n| Method             | Node Graph | CoraFull | OGB-Arxiv | COX2 | IMDB | PROTEINS |\\n|--------------------|------------|----------|-----------|------|------|----------|\\n| GCN (30 trials)    | 95.88 \u00b1 0.16 | 67.12 \u00b1 0.52 | 70.46 \u00b1 0.18 | 79.23 \u00b1 2.19 | 50.40 \u00b1 3.02 | 74.84 \u00b1 2.82 |\\n| GAT (30 trials)    | 95.71 \u00b1 0.24 | 65.92 \u00b1 0.68 | 68.82 \u00b1 0.32 | 81.56 \u00b1 4.17 | 49.67 \u00b1 4.30 | 75.30 \u00b1 3.72 |\\n| GraphNAS (30 trials) | 92.77 \u00b1 0.84 | 63.13 \u00b1 3.28 | 65.90 \u00b1 2.64 | 77.73 \u00b1 1.40 | 46.93 \u00b1 3.94 | 72.51 \u00b1 3.36 |\\n| DARTS              | 95.28 \u00b1 1.67 | 67.59 \u00b1 2.85 | 69.02 \u00b1 1.18 | 79.82 \u00b1 3.15 | 50.26 \u00b1 4.08 | 75.04 \u00b1 3.81 |\\n| GASSO              | 4         | 68.89 | 70.52 | - | - | - |\\n| Random (3 trials)  | 95.16 \u00b1 0.55 | 61.24 \u00b1 4.04 | 67.92 \u00b1 1.92 | 76.88 \u00b1 3.17 | 45.79 \u00b1 4.39 | 72.47 \u00b1 2.57 |\\n| TPE (30 trials)    | 96.41 \u00b1 0.36 | 66.37 \u00b1 1.73 | 71.35 \u00b1 0.44 | 82.27 \u00b1 2.01 | 50.33 \u00b1 4.00 | 79.46 \u00b1 1.28 |\\n| HyperBand (30 trials) | 96.56 \u00b1 0.30 | 67.75 \u00b1 1.24 | 71.60 \u00b1 0.36 | 82.21 \u00b1 1.79 | 50.86 \u00b1 3.45 | 79.32 \u00b1 1.16 |\\n| A\\nUTO\\nT\\nRANSFER\\n(3 trials) | 96.64 \u00b1 0.42 | 69.27 \u00b1 0.76 | 71.42 \u00b1 0.39 | 82.13 \u00b1 1.59 | 52.33 \u00b1 2.13 | 77.81 \u00b1 2.19 |\\n| A\\nUTO\\nT\\nRANSFER\\n(30 trials) | 96.91 \u00b1 0.27 | 70.05 \u00b1 0.42 | 72.21 \u00b1 0.27 | 86.52 \u00b1 1.58 | 54.93 \u00b1 1.23 | 81.25 \u00b1 1.17 |\\n\\nNAS algorithms (DARTS and GASSO), as they only train a super-model once and can efficiently\\nevaluate different architectures. We are mostly interested in studying the few-trial regime where most\\nadvanced search algorithms degrade to random search. Thus we additionally include a random search\\n(3 trials) baseline where we pick the best model out of only 3 trials.\\n\\n5.2 EXPERIMENTS ON\\nE\\nXPERIENCE\\nN\\nCE\\nWe evaluate A\\nUTO\\nT\\nRANSFER\\nby reporting the average best test accuracy among all trials considered\\nover ten runs of each algorithm in Table 1. The test accuracy collected for each trial is selected at\\nthe epoch with the best validation accuracy. By comparing results from random search (3 trials) and\\nA\\nUTO\\nT\\nRANSFER\\n(3 trials), we show that our transferred task-informed design prior significantly\\nimproves test accuracy in the few-trial regime, and can be very useful in environments that are where\\ncomputationally constrained. Even if we increase the number of search trials to 30, A\\nUTO\\nT\\nRANSFER\\nstill demonstrates non-trivial improvement compared with TPE, indicating that our proposed pipeline\\nhas advantages even when computational resources are abundant. Notably, with only 3 search trials,\\nA\\nUTO\\nT\\nRANSFER\\nsurpasses most of the baselines, even those that use 30 trials.\\n\\nTo better understand the sample efficiency of A\\nUTO\\nT\\nRANSFER\\n, we plot the best test accuracy found\\nat each trial in Figure 3 for OGB-Arxiv and TU-PROTEINS datasets. We notice that the advanced\\nsearch algorithms (Evolution and TPE) have no advantages over random search at the few-trial\\nregime since the amount of prior search data is not yet sufficient to infer potentially better design\\nconfigurations. On the contrary, by sampling from the transferred design prior, A\\nUTO\\nT\\nRANSFER\\nachieves significantly better average test accuracy in the first few trials. The best test accuracy at trial\\n3 of A\\nUTO\\nT\\nRANSFER\\nsurpasses its counterpart at trial 10 for every other method.\\n\\n5.3 ANALYSIS OF\\nA\\nN\\nA\\nL\\nY\\nS\\nE\\nS\\nO\\nF\\nA\\nS\\nK\\nE\\nM\\nB\\nI\\nD\\nD\\nI\\nN\\nG\\nS\\nQualitative analysis of task features.\\nTo examine the quality of the proposed task features, we\\nvisualize the proposed task similarity matrix (Figure 4 (b)) along with the task similarity matrix\\n(Figure 4 (a)) proposed in GraphGym. We show that our proposed task similarity matrix captures\\nsimilar patterns as GraphGym's task similarity matrix while being computed much more efficiently\\n\\nResults come from the original paper (Qin et al., 2021).\"}"}
{"id": "y81ppNf_vg", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: (a) GraphGym's task similarity between all pairs of tasks (computed from the Kendall rank correlation between performance rankings of models trained on the two compared tasks), a higher value represents a higher similarity. (b) The proposed task similarity computed by computing the dot-product between extracted task features. (c) The Kendall rank correlation of similarity rankings of the other tasks with respect to the central task between the proposed method and GraphGym. \\n\\nWe notice that the same type of tasks, i.e., node classification and graph classification, share more similarities within each group. As a sanity check, we examined that the closest task in the bank with respect to CoraFull is Cora. The top 3 closest tasks for OGB-Arxiv are AmazonComputers, AmazonPhoto, and CoauthorPhysics, all of which are node classification tasks.\\n\\nGeneralization of projection function $g(\\\\cdot)$. To show the proposed projection function $g(\\\\cdot)$ can generate task embeddings that can generalize to novel tasks, we conduct leave-one-out cross validation with all tasks in our task-model bank. Concretely, for each task considered as a novel task $T(n)$, we use the rest of the tasks, along with their distance metric $d_g(\\\\cdot, \\\\cdot)$ estimated by GraphGym's exact but computationally expensive metric space, to train the projection function $g(\\\\cdot)$. We calculate Kendall rank correlation over task similarities for Task Feature (without $g(\\\\cdot)$) and Task Embedding (with $g(\\\\cdot)$) against the exact task similarities. The average rank correlation and the standard deviation over ten runs is shown on Figure 4 (c). We find that with the proposed $g(\\\\cdot)$, our task embeddings indeed correlate better with the exact task similarities, and therefore, generalize better to novel tasks.\\n\\nAblation study on alternative task space design. To demonstrate the superiority of the proposed task embedding, we further compare it with alternative task features. Following prior work (Yang et al., 2019), we use the normalized losses over the first 10 steps as the task feature. The results on OGB-Arxiv are shown in Table 2. Compared to AUTOTRANSFER's task embedding, the task feature induced by normalized losses has a lower ranking correlation with the exact metric and yields worse performance. Table 2 further justifies the efficacy of using the Kendall rank correlation as the metric for task embedding quality, as higher Kendall rank correlation leads to better performance.\\n\\nTable 2: Ablation study on the alternative task space design versus AUTOTRANSFER's task embedding. We report the average test accuracy and the standard deviation OGB-Arxiv over ten runs.\\n\\n| Task Feature                        | Kendall rank correlation | Test accuracy \u00b1 standard deviation |\\n|-------------------------------------|--------------------------|-----------------------------------|\\n| Alternative: Normalized Loss        | -0.07 \u00b1 0.43             | 68.13 \u00b1 1.27                      |\\n| AUTOTRANSFER's Task Embedding       | 0.18 \u00b1 0.30              | 70.67 \u00b1 0.52                      |\\n| AUTOTRANSFER's Task Embedding       | 0.43 \u00b1 0.22              | 71.42 \u00b1 0.39                      |\\n\\n**Conclusion**\\n\\nIn this paper, we study how to improve AutoML search efficiency by transferring existing architectural design knowledge to novel tasks of interest. We introduce a task-model bank that captures the performance over a diverse set of GNN architectures and tasks. We also introduce a computationally efficient task embedding that can accurately measure the similarity between different tasks. We release GNN-BANK-101, a large-scale database that records detailed GNN training information of 120,000 task-model combinations. We hope this work can facilitate and inspire future research in efficient AutoML to make deep learning more accessible to a general audience.\"}"}
