{"id": "kNGxg8shA1", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1: Training Algorithm of PRINGLE.\\n\\n1: Input: Observed graph $G = \\\\langle V, E \\\\rangle$, node feature $X \\\\in \\\\mathbb{R}^{N \\\\times F}$, node label $Y \\\\in \\\\mathbb{R}^{N \\\\times C}$.\\n\\n2: Initialize trainable parameters $\\\\phi_1, \\\\phi_2, \\\\phi_3, \\\\theta_2, \\\\theta_3$.\\n\\n3: Initialize $\\\\hat{p}_{el}^{ij}$ to one vector $1$.\\n\\n4: Generate a $k$-NN graph $E_\\\\gamma^k$ based on the $\\\\gamma$-hop subgraph similarity.\\n\\n5: Pre-define a candidate graph by $E_\\\\gamma^k \\\\cup E$.\\n\\n6: while not converge do\\n\\n7: /* Inference of $Z^A$ */\\n\\n8: Feed $X$ and $A$ to GCN $\\\\phi_1$ to obtain the node embeddings $Z$.\\n\\n9: Calculate the $\\\\hat{p}_{el}^{ij}$ on the candidate graph $E_\\\\gamma^k \\\\cup E$ based on $Z$ to obtain $\\\\hat{A}$.\\n\\n10: /* Inference of $Z^Y$ */\\n\\n11: Feed $X$ and $\\\\hat{A}$ to GCN $\\\\phi_3$ to get $\\\\hat{Y}$.\\n\\n12: /* Inference of $\\\\epsilon^X$ */\\n\\n13: Feed $X$ and $\\\\hat{Y}$ to the MLP $\\\\phi_2$ to get node embeddings that follow $N(0, I)$.\\n\\n14: /* Inference of $\\\\epsilon^A$ */\\n\\n15: if early-learning phase then\\n\\n16: $\\\\hat{p}_{c}^{ij} \\\\leftarrow \\\\rho(s(Z_i, Z_j))$.\\n\\n17: $\\\\hat{p}_{el}^{ij} \\\\leftarrow \\\\xi \\\\hat{p}_{el}^{ij} + (1 - \\\\xi) \\\\hat{p}_{c}^{ij}$.\\n\\n18: end if\\n\\n19: Convert $\\\\hat{p}_{el}^{ij}$ into $\\\\tau^{ij}$.\\n\\n20: /* Generation of $A$ */\\n\\n21: Obtain an edge prediction $w^{ij} = \\\\theta_1 \\\\hat{p}^{ij} + (1 - \\\\theta_1) s(X_i, X_j)$.\\n\\n22: /* Generation of $X$ */\\n\\n23: Obtain the reconstruction of node features based on decoder MLP $\\\\theta_2$ and its input $\\\\epsilon^X$ and $\\\\hat{Y}$.\\n\\n24: /* Generation of $Y$ */\\n\\n25: Obtain node prediction $\\\\hat{Y}_{\\\\text{dec}}$ based on classifier GCN $\\\\theta_3$ and its input $X$ and $A$.\\n\\n26: /* Loss calculation */\\n\\n27: Calculate the objective function $L_{\\\\text{cls-enc}} + \\\\lambda_1 L_{\\\\text{rec-edge}} + \\\\lambda_2 L_{\\\\text{hom}} + \\\\lambda_3 (L_{\\\\text{rec-feat}} + L_{\\\\text{cls-dec}} + L_{p})$.\\n\\n28: /* Parameter updates */\\n\\n29: Update the parameters $\\\\phi_1, \\\\phi_2, \\\\phi_3, \\\\theta_2, \\\\theta_3$ to minimize the overall objective function.\\n\\n30: end while\\n\\n31: Return: learned model parameters $\\\\phi_1, \\\\phi_2, \\\\phi_3, \\\\theta_2, \\\\theta_3$. \"}"}
{"id": "kNGxg8shA1", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 2\\nData Generation Algorithm of Synthetic FDGN.\\n\\n1: Input: Clean graph $G = \\\\langle V, E \\\\rangle$, node feature $X \\\\in \\\\mathbb{R}^{N \\\\times F}$, node label $Y \\\\in \\\\mathbb{R}^{N \\\\times C}$, noise rate $\\\\eta\\\\%$\\n\\n2: /* Injection of feature noise */\\n\\n3: $V_{\\\\text{noisy}} \\\\leftarrow$ Randomly sample a $\\\\eta\\\\%$ subset of nodes\\n\\n4: $X_{\\\\text{noisy}} \\\\leftarrow X$\\n\\n5: for $v_i$ in $V_{\\\\text{noisy}}$ do\\n\\n6: $p_i \\\\leftarrow 1$\\n\\n7: for $j \\\\leftarrow 1$ to $F$ do\\n\\n8: $X_{\\\\text{noisy}}_{ij} \\\\leftarrow \\\\text{BernoulliSample}(p_i)$\\n\\n9: end for\\n\\n10: end for\\n\\n11: /* Injection of feature-dependent structure noise */\\n\\n12: $E_{\\\\text{noisy}} \\\\leftarrow E$\\n\\n13: for $v_i$ in $V_{\\\\text{noisy}}$ do\\n\\n14: $s \\\\leftarrow 0 \\\\in \\\\mathbb{R}^N$\\n\\n15: for $j \\\\leftarrow 1$ to $N$ do\\n\\n16: $s_j \\\\leftarrow s(X_{\\\\text{noisy}}_i, X_j)$\\n\\n17: end for\\n\\n18: Append $k$ pairs of nodes with the highest $s$ values to $E_{\\\\text{noisy}}$\\n\\n19: end for\\n\\n20: /* Injection of feature-dependent label noise */\\n\\n21: $Y_{\\\\text{noisy}} \\\\leftarrow Y$\\n\\n22: for $v_i$ in $V_{\\\\text{L}}$ do\\n\\n23: if $v_i$ has noisy feature or noisy structure then\\n\\n24: $p_i \\\\leftarrow \\\\text{Obtain normalized neighborhood class distribution of node } v_i$\\n\\n25: $Y_{\\\\text{noisy}}_i \\\\leftarrow \\\\text{MultinomialSample}(p_i)$\\n\\n26: end if\\n\\n27: end for\\n\\n28: /* Injection of independent structure noise */\\n\\n29: Randomly append pairs of nodes to $E_{\\\\text{noisy}}$\\n\\n30: Return: noisy graph $G = \\\\langle V, E_{\\\\text{noisy}} \\\\rangle$, noisy node feature $X_{\\\\text{noisy}}$, noisy node label $Y_{\\\\text{noisy}}$\"}"}
{"id": "kNGxg8shA1", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Regarding the adherence of ICLR Code of Ethics, to the best of our knowledge, there are no ethical issues with this paper. All datasets used for experiments are publicly available.\\n\\nTo ensure clarity and reproducibility, in Sec 4.3, we offer comprehensive explanations of our proposed method (PRINGLE). Implementation details for both baseline methods and our method can be found in Appendix D.3 and D.5. Furthermore, as we introduce novel settings to address the more realistic graph noise scenario, FDGN, we provide a comprehensive discussion of the problem formulation in Sec 3.1 and elaborate the experimental setup construction procedure in Appendix D.2. Our code is available at https://anonymous.4open.science/r/FDGN-PRINGLE-4B31/\\n\\nREFERENCES\\n\\nDevansh Arpit, Stanis\u0142aw Jastrz\u0119bski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International conference on machine learning, pp. 233\u2013242. PMLR, 2017.\\n\\nAntonin Berthon, Bo Han, Gang Niu, Tongliang Liu, and Masashi Sugiyama. Confidence scores make instance-dependent label-noise learning possible. In International conference on machine learning, pp. 825\u2013836. PMLR, 2021.\\n\\nDavid M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518):859\u2013877, 2017.\\n\\nYoonhyuk Choi, Jiho Choi, Taewook Ko, Hyungho Byun, and Chong-Kwon Kim. Finding heterophilic neighbors via confidence-based subgraph matching for semi-supervised node classification. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pp. 283\u2013292, 2022.\\n\\nEnyan Dai, Charu Aggarwal, and Suhang Wang. Nrgnn: Learning a label noise resistant graph neural network on sparsely and noisily labeled graphs. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 227\u2013236, 2021.\\n\\nEnyan Dai, Wei Jin, Hui Liu, and Suhang Wang. Towards robust graph neural networks for noisy graphs with sparse labels. WSDM, 2022.\\n\\nPantelis Elinas, Edwin V Bonilla, and Louis Tiao. Variational inference for graph convolutional networks in the absence of graph data and adversarial settings. Advances in Neural Information Processing Systems, 33:18648\u201318660, 2020.\\n\\nBahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi. Slaps: Self-supervision improves structure learning for graph neural networks. Advances in Neural Information Processing Systems, 34:22667\u201322681, 2021.\\n\\nRuining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In proceedings of the 25th international conference on worldwide web, pp. 507\u2013517, 2016.\\n\\nAhmet Iscen, Jack Valmadre, Anurag Arnab, and Cordelia Schmid. Learning with neighbor consistency for noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4672\u20134681, 2022.\\n\\nWei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure learning for robust graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 66\u201374, 2020.\\n\\nWei Jin, Tong Zhao, Jiayuan Ding, Yozen Liu, Jiliang Tang, and Neil Shah. Empowering graph representation learning with test-time graph transformation. arXiv preprint arXiv:2210.03561, 2022.\"}"}
{"id": "kNGxg8shA1", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\\n\\nDanning Lao, Xinyu Yang, Qitian Wu, and Junchi Yan. Variational inference for training graph neural networks in low-data regime through joint structure-label estimation. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 824\u2013834, 2022.\\n\\nRunlin Lei, Zhen Wang, Yaliang Li, Bolin Ding, and Zhewei Wei. Evennet: Ignoring odd-hop neighbors improves robustness of graph neural networks. arXiv preprint arXiv:2205.13892, 2022.\\n\\nJunnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. arXiv preprint arXiv:2002.07394, 2020.\\n\\nKuan Li, Yang Liu, Xiang Ao, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Reliable representations make a stronger defender: Unsupervised structure refinement for robust gnn. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 925\u2013935, 2022a.\\n\\nZenan Li, Qitian Wu, Fan Nie, and Junchi Yan. GraphDE: A generative framework for debiased learning and out-of-distribution detection on graphs. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022b.\\n\\nNian Liu, Xiao Wang, Lingfei Wu, Yu Chen, Xiaojie Guo, and Chuan Shi. Compact graph structure learning via mutual information compression. In Proceedings of the ACM Web Conference 2022, pp. 1601\u20131610, 2022.\\n\\nXiaorui Liu, Jiayuan Ding, Wei Jin, Han Xu, Yao Ma, Zitao Liu, and Jiliang Tang. Graph neural networks with adaptive residual. Advances in Neural Information Processing Systems, 34:9720\u20139733, 2021.\\n\\nJiaqi Ma, Weijing Tang, Ji Zhu, and Qiaozhu Mei. A flexible generative framework for graph-based semi-supervised learning. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nXiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong, and Le-man Akoglu. A comprehensive survey on graph anomaly detection with deep learning. IEEE Transactions on Knowledge and Data Engineering, 2021.\\n\\nJulian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based recommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval, pp. 43\u201352, 2015.\\n\\nMiller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social networks. Annual review of sociology, 27(1):415\u2013444, 2001.\\n\\nHoang NT, Choong Jun Jin, and Tsuyoshi Murata. Learning graph neural networks with noisy labels. arXiv preprint arXiv:1905.01591, 2019.\\n\\nSiyi Qian, Haochao Ying, Renjun Hu, Jingbo Zhou, Jintai Chen, Danny Z Chen, and Jian Wu. Robust training of graph neural networks via noise governance. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pp. 607\u2013615, 2023.\\n\\nOleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.\\n\\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818\u20132826, 2016.\\n\\nYijun Tian, Chuxu Zhang, Zhichun Guo, Xiangliang Zhang, and Nitesh Chawla. Learning MLPs on graphs: A unified view of effectiveness, robustness, and efficiency. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Cs3r5KLdoj.\"}"}
{"id": "kNGxg8shA1", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Juexin Wang, Anjun Ma, Yuzhou Chang, Jianting Gong, Yuexu Jiang, Ren Qi, Cankun Wang, Hongjun Fu, Qin Ma, and Dong Xu. scgnn is a novel graph neural network framework for single-cell rna-seq analyses. Nature communications, 12(1):1882, 2021.\\n\\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. Neural graph collaborative filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval, pp. 165\u2013174, 2019a.\\n\\nYisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross entropy for robust learning with noisy labels. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 322\u2013330, 2019b.\\n\\nJiaying Wu and Bryan Hooi. Decor: Degree-corrected social graph refinement for fake news detection. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 2582\u20132593, 2023.\\n\\nTailin Wu, Hongyu Ren, Pan Li, and Jure Leskovec. Graph information bottleneck. Advances in Neural Information Processing Systems, 33:20437\u201320448, 2020.\\n\\nZehao Xiong, Jiawei Luo, Wanwan Shi, Ying Liu, Zhongyuan Xu, and Bo Wang. scgcl: an imputation method for scrna-seq data based on graph contrastive learning. Bioinformatics, 39(3):btad098, 2023.\\n\\nZhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings, 2016.\\n\\nLiang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classification. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 7370\u20137377, 2019.\\n\\nYu Yao, Tongliang Liu, Mingming Gong, Bo Han, Gang Niu, and Kun Zhang. Instance-dependent label-noise learning under a structural causal model. Advances in Neural Information Processing Systems, 34:4409\u20134420, 2021.\\n\\nXingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does disagreement help generalization against label corruption? In International Conference on Machine Learning, pp. 7164\u20137173. PMLR, 2019.\\n\\nJingyang Yuan, Xiao Luo, Yifang Qin, Yusheng Zhao, Wei Ju, and Ming Zhang. Learning on graphs under label noise. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1\u20135. IEEE, 2023.\\n\\nSukwon Yun, Junseok Lee, and Chanyoung Park. Single-cell rna-seq data imputation using feature propagation. arXiv preprint arXiv:2307.10037, 2023.\\n\\nMengmei Zhang, Linmei Hu, Chuan Shi, and Xiao Wang. Adversarial label-flipping attack and defense for graph neural networks. In 2020 IEEE International Conference on Data Mining (ICDM), pp. 791\u2013800. IEEE, 2020.\\n\\nWentao Zhao, Qitian Wu, Chenxiao Yang, and Junchi Yan. Graphglow: Universal and generalizable structure learning for graph neural networks. arXiv preprint arXiv:2306.11264, 2023.\\n\\nZhanke Zhou, Jiangchao Yao, Jiaxu Liu, Xiawei Guo, Quanming Yao, Li He, Liang Wang, Bo Zheng, and Bo Han. Combating bilateral edge noise for robust link prediction. arXiv preprint arXiv:2311.01196, 2023.\"}"}
{"id": "kNGxg8shA1", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Overall Architecture and Algorithm\\n\\nB Derivation Details of Evidence Lower BOund (ELBO)\\n\\nC Details of Model Instantiations\\n\\nD Details on Experimental Settings\\n  D.1 Datasets\\n  D.2 Details of Generating FDGN\\n    D.2.1 Synthetic FDGN\\n    D.2.2 Real-world FDGN\\n  D.3 Baselines\\n  D.4 Evaluation Protocol\\n  D.5 Implementation Details\\n\\nE Additional Experimental Results\\n  E.1 Under Independent Feature/Structure/Label Noise Settings\\n  E.2 Comparison with the naive combination of existing works\\n  E.3 Sensitivity Analysis\\n  E.4 Analysis of the inferred $A$ and $\\\\epsilon$ A\\n\\nF Further Discussion on FDGN\\n\\nG Comparison to CausalNL (Yao et al., 2021)\\n\\nH Additional Real-world Examples of FDGN\\n\\nI Further Comparison to label noise-robust baselines\\n\\nJ Complexity Analysis\\n\\nK Further Discussion with GraphDE and RGIB\"}"}
{"id": "kNGxg8shA1", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We again emphasize that while existing works primarily focus on the unrealistic noise scenario where graphs contain only a single type of noise, to the best of our knowledge, this is the first attempt to understand the noise scenario in the real-world applications. Furthermore, we propose new graph benchmark datasets that closely imitate a real-world e-commerce system containing malicious fraudsters, which incurs FDGN. We expect these datasets to foster practical research in noise-robust graph learning.\\n\\nD.3 Baselines\\n\\nWe compare PRINGLE with a wide range of noise-robust graph learning methods, which includes feature noise-robust graph learning methods (i.e., AirGNN (Liu et al., 2021)), structure-noise robust graph learning methods (i.e., ProGNN (Jin et al., 2020), RSGNN (Dai et al., 2022), STABLE (Li et al., 2022a) and EvenNet (Lei et al., 2022)), and label noise-robust graph learning methods (i.e., NRGNN (Dai et al., 2021) and RTGNN (Qian et al., 2023)). We also consider WSGNN (Lao et al., 2022) that is a generative graph learning method utilizing variational inference technique.\\n\\nThe publicly available implementations of baselines can be found at the following URLs:\\n\\n- AirGNN (Liu et al., 2021): https://github.com/lxiaorui/AirGNN\\n- ProGNN (Jin et al., 2020): https://github.com/ChandlerBang/Pro-GNN\\n- RSGNN (Dai et al., 2022): https://github.com/EnyanDai/RSGNN\\n- STABLE (Li et al., 2022a): https://github.com/likuanppd/STABLE\\n- EvenNet (Lei et al., 2022): https://github.com/Leirunlin/EvenNet\\n- NRGNN (Dai et al., 2022): https://github.com/EnyanDai/NRGNN\\n- RTGNN (Dai et al., 2022): https://github.com/GhostQ99/RobustTrainingGNN\\n- WSGNN (Lao et al., 2022): https://github.com/Thinklab-SJTU/WSGNN\\n\\nD.4 Evaluation Protocol\\n\\nWe mainly compare the robustness of PRINGLE and the baselines under both the synthetic and real-world feature-dependent graph-noise (FDGN). More details of generating FDGN is provided in Sec D.2. Additionally, we consider independent feature/structure/label noise, i.e., random feature noise, random structure noise, uniform label noise, and pair label noise following existing works (Liu et al., 2021; Dai et al., 2022; Li et al., 2022a; Qian et al., 2023). Specifically, for the feature noise (Liu et al., 2021), we sample a subset of nodes (i.e., 10%, 30%, and 50%) and randomly flip 0/1 value on each dimension of node features $X_i$ from Bernoulli distribution with probability $p = \\\\frac{1}{2}$. For the structure noise, we adopt the random perturbation method that randomly injects non-connected node pairs into the graph (Li et al., 2022a). For the label noise, we generate uniform label noise and pair label noise following the existing works (Qian et al., 2023; Dai et al., 2021).\\n\\nWe conduct both the node classification and link prediction tasks. For node classification, we perform a random split of the nodes, dividing them into a 1:1:8 ratio for training, validation, and testing nodes. Once a model is trained on the training nodes, we use the model to predict the labels of the test nodes. Regarding link prediction, we partition the provided edges into a 7:3 ratio for training and testing edges. Additionally, we generate random negatives that are selected randomly from pairs that are not directly linked in the original graphs. After mode learning with the training edges, we predict the likelihood of the existence of each edge. This prediction is based on a dot-product or cosine similarity calculation between node pairs of test edges and their corresponding negative edges. To evaluate performance, we use Accuracy as the metric for node classification and Area Under the Curve (AUC) for link prediction.\"}"}
{"id": "kNGxg8shA1", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Hyperparameter settings for Table 1.\\n\\n| Dataset  | Setting | lr  | \u03bb  | \u03b8  | k  | \u03b3 |\\n|----------|---------|-----|----|----|----|----|\\n| Cora     | Clean   | 0.01 | 0.003 | 0.003 | 0.1  | 300 | 1 |\\n| FDGN-10% | 0.005  | 0.003 | 0.003 | 0.2 | 50  | 1 |\\n| FDGN-30% | 0.001  | 0.003 | 0.003 | 0.2 | 100 | 1 |\\n| FDGN-50% | 0.0005 | 0.003 | 0.003 | 0.3 | 50  | 1 |\\n| Citeseer | Clean   | 0.0005 | 0.003 | 0.3 | 0.1  | 50  | 0 |\\n| FDGN-10% | 0.005  | 0.3  | 0.003 | 0.3 | 10  | 0 |\\n| FDGN-30% | 0.001  | 0.003 | 0.003 | 0.1 | 300 | 1 |\\n| FDGN-50% | 0.001  | 0.003 | 0.003 | 0.1 | 300 | 1 |\\n| Photo    | Clean   | 0.01  | 0.03  | 0.3 | 0.1  | 10  | 0 |\\n| FDGN-10% | 0.0005 | 0.03  | 0.3  | 0.1  | 10  | 0 |\\n| FDGN-30% | 0.001  | 3.0  | 0.03  | 0.1  | 10  | 0 |\\n| FDGN-50% | 0.0005 | 30.0 | 0.03  | 0.1  | 10  | 0 |\\n| Comp     | Clean   | 0.01  | 30.0  | 0.03  | 0.1  | 10  | 0 |\\n| FDGN-10% | 0.01   | 0.3  | 0.03  | 0.1  | 10  | 0 |\\n| FDGN-30% | 0.01   | 0.003 | 0.003 | 0.1  | 10  | 0 |\\n| FDGN-50% | 0.0005 | 0.003 | 0.03  | 0.1  | 10  | 0 |\\n\\nD.5 IMPLEMENTATION DETAILS\\n\\nFor each experiment, we report the average performance of 3 runs with standard deviations. For all baselines, we use the publicly available implementations and follow the implementation details presented in their original papers.\\n\\nFor PRINGLE, the learning rate is tuned from {0.01, 0.005, 0.001, 0.0005}, and dropout rate and weight decay are fixed to 0.6 and 0.0005, respectively. In the inference of $Z_A$, we use a 2-layer GCN model with 64 hidden dimension as GCN $\\\\phi_1$ and the dimension of node embedding $d_1$ is fixed to 64. The $\\\\gamma$ value in calculating $\\\\gamma$-hop subgraph similarity is tuned from {0, 1} and $k$ in generating $k$-NN graph is tuned from {0, 10, 50, 100, 300}. In the inference of $Z_Y$, we use a 2-layer GCN model with 128 hidden dimension as GCN $\\\\phi_3$. In the inference of $\\\\epsilon_X$, the hidden dimension of embedding of $\\\\epsilon_X$ is fixed to 16. In the inference of $\\\\epsilon_A$, the early-learning phase is fixed to 30 epochs. In the implementation of the loss term $-E_{Z_A \\\\sim q_{\\\\phi_1}} E_{\\\\epsilon \\\\sim q_{\\\\phi_2}} \\\\log(p_{\\\\theta_1}(A | X, \\\\epsilon, Z_A))$, we tune the $\\\\theta_1$ value from {0.1, 0.2, 0.3}. In the overall learning objective, i.e., Eqn 4, $\\\\lambda_1$ is tuned from { 0.003, 0.03, 0.3, 3, 30 }, $\\\\lambda_2$ is tuned from { 0.003, 0.03, 0.3 }, and $\\\\lambda_3$ is fixed to 0.001. It is important to note that when we calculate Eqn. 4, $L_{\\\\text{rec-feat}}$, $L_{\\\\text{cls-dec}}$, and $L_{\\\\text{p}}$ share the same coefficient $\\\\lambda_3$. In our experiments, we observed that these three terms have a relatively minor impact on the model's performance compared to the others. As a result, we have made a strategic decision to simplify the hyperparameter search process and improve the practicality of PRINGLE by sharing the coefficient $\\\\lambda_3$ among these three loss terms. We report the details of hyperparameter settings in Table 6.\\n\\nE.1 UNDER INDEPENDENT FEATURE/STRUCTURE/LABEL NOISE SETTINGS\\n\\nIn this subsection, we further evaluate the robustness of PRINGLE under independent feature/structure/label noise settings. In this setup, each type of noise occurs independently and does not affect the occurrence of the others. To this end, we generate three types of noise: random feature noise, random structure noise, and random label noise. More details of generating noises is provided in Sec D.4.\\n\\nEvaluating robustness under independent feature noise. In this setting, we evaluate the models on a graph containing only the feature noise, i.e., random feature noise. In Fig 6, PRINGLE consistently outperforms the feature noise-robust graph learning method (i.e., AirGNN) under independent feature noise. We attribute the robustness of PRINGLE under independent feature noise to the graph structure learning module that accurately infers the latent graph structure $Z_A$. The utilization of abundant local neighborhoods acquired through the inference of $Z_A$ enables effective smoothing for nodes with noisy features, leveraging the information within these neighborhoods.\"}"}
{"id": "kNGxg8shA1", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating robustness under independent structure noise. In this setting, we evaluate the models on a graph containing only the structure noise, i.e., random structure noise. In Fig 7, Under the influence of independent structure noise, PRINGLE maintains a consistently competitive performance when compared to other structure noise-robust graph learning methods, namely RSGNN and STABLE. We attribute the effectiveness of PRINGLE under independent structure noise to inferring the robust latent clean graph structure. In other words, the inference of the latent clean graph structure assigns greater weights to latent clean edges and lower weights to observed noisy edges by employing regularizations on both the edge predictions and labels, thereby mitigating structural noise.\\n\\nEvaluating robustness under independent uniform label noise. In this setting, we evaluate the models on a graph containing only the label noise, i.e., uniform label noise and pair label noise. In Fig 8 and 9, PRINGLE demonstrates consistent superiority or competitive performance compared to label noise-robust graph learning methods, namely NRGNN and RTGNN, in the presence of independent label noise. We argue that the effectiveness of PRINGLE stems from the accurate inference of the latent clean structure. Specifically, the inferred latent node label is regularized using the inferred latent structure to meet the homophily assumption (i.e., $L_{\\\\text{hom}}$). Leveraging the clean neighbor structure, this regularization technique has been demonstrated to effectively address noisy labels (Iscen et al., 2022).\\n\\nEvaluating robustness under simultaneous noise. In addition to a single type of independent noise, we explore a more challenging yet realistic noise scenario where all three types of independent noises occur simultaneously, denoted as simultaneous noise. More specifically, for the feature and structure noise, we generate random feature noise and random structure noise in a same way as described in Sec D.4. Regarding the label noise, we generate uniform label noise following Qian et al. (2023). It is important to note that each type of noise does not affect the occurrence of the other types of noise. In Fig 10, PRINGLE consistently outperforms the noise-robust graph learning methods and generative graph learning methods under simultaneous noise setting. Based on these results, we can assert that modeling the DGP of FDGN is advantageous for robustness under independent graph noise, as PRINGLE is inherently capable of handling each type of noise. In contrast, the baseline methods\"}"}
{"id": "kNGxg8shA1", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Node classification performance of baselines and PRINGLE on Cora and Citeseer dataset under simultaneous noise, where random feature, structure, and label noise independently exist in a graph. We vary a noise rate from 0% to 50%.\\n\\nTable 7: Comparison with the naive combination of existing noise-robust graph learning methods. FNR, SNR, and LNR denote the feature noise-robust, structure noise-robust, and label noise-robust graph learning methods, respectively. We consider AirGNN as FNR, RSGNN as SNR, and RTGNN as LNR methods.\\n\\nComponent | Cora (Clean) | Citeseer (Clean) | Cora (10%) | Citeseer (10%) | Cora (50%) | Citeseer (50%)\\n--- | --- | --- | --- | --- | --- | ---\\nFNR SNR LNR | \u2713 \u2717 \u2717 | \u2713 \u2717 \u2717 | \u2713 \u2717 \u2717 | \u2713 \u2717 \u2717 | \u2713 \u2717 \u2717 | \u2713 \u2717 \u2717\\n85.0\u00b10.2 79.7\u00b10.5 71.5\u00b10.8 56.2\u00b10.8 | 71.5\u00b10.2 66.2\u00b10.7 58.0\u00b10.4 50.0\u00b10.6 | 86.2\u00b10.5 81.9\u00b10.3 71.9\u00b10.5 58.1\u00b10.2 | 75.8\u00b10.4 73.3\u00b10.5 63.9\u00b10.5 55.3\u00b10.4 | 86.1\u00b10.2 81.6\u00b10.5 72.1\u00b10.6 60.8\u00b10.4 | 76.1\u00b10.4 73.2\u00b10.2 63.5\u00b12.1 54.2\u00b11.8 |\\n\u2713 \u2713 \u2717 | \u2713 \u2713 \u2717 | \u2713 \u2713 \u2717 | \u2713 \u2713 \u2717 | \u2713 \u2713 \u2717 | \u2713 \u2713 \u2717\\n86.0\u00b10.3 82.0\u00b10.3 75.0\u00b10.8 68.8\u00b10.6 | 75.1\u00b10.8 73.1\u00b10.6 63.6\u00b10.8 57.8\u00b10.8 | 85.2\u00b10.7 70.1\u00b10.1 56.7\u00b10.4 48.0\u00b10.5 | 75.8\u00b10.5 72.3\u00b10.3 59.0\u00b10.7 49.0\u00b10.2 | \u2713 \u2713 \u2713 | 86.3\u00b10.3 82.4\u00b10.3 67.0\u00b10.9 53.6\u00b10.6 |\\n\u2713 \u2717 \u2713 | \u2713 \u2717 \u2713 | \u2713 \u2717 \u2713 | \u2713 \u2717 \u2713 | \u2713 \u2717 \u2713 | \u2713 \u2717 \u2713\\n85.2\u00b10.7 70.1\u00b10.1 56.7\u00b10.4 48.0\u00b10.5 | 75.8\u00b10.5 72.3\u00b10.3 59.0\u00b10.7 49.0\u00b10.2 | \u2713 \u2713 \u2713 | 86.3\u00b10.3 82.4\u00b10.3 67.0\u00b10.9 53.6\u00b10.6 | 77.3\u00b10.6 74.3\u00b10.9 65.6\u00b10.6 59.0\u00b11.8 | 86.2\u00b10.7 82.9\u00b10.6 78.2\u00b10.3 69.7\u00b10.6 |\\n\u2717 \u2713 \u2713 | \u2713 \u2713 \u2713 | \u2713 \u2713 \u2713 | \u2713 \u2713 \u2713 | \u2713 \u2713 \u2713 | \u2713 \u2713 \u2713\\n86.3\u00b10.3 82.4\u00b10.3 67.0\u00b10.9 53.6\u00b10.6 | 76.6\u00b10.2 73.0\u00b10.7 64.1\u00b10.2 52.7\u00b11.1 | 86.2\u00b10.7 82.9\u00b10.6 78.2\u00b10.3 69.7\u00b10.6 | 77.3\u00b10.6 74.3\u00b10.9 65.6\u00b10.6 59.0\u00b11.8 | PRINGLE | 86.2\u00b10.7 82.9\u00b10.6 78.2\u00b10.3 69.7\u00b10.6 |\\n\\nAssume the completeness of at least one of the data sources, resulting in a significant performance drop when the noise rate is high. This suggests that PRINGLE has a broader range of applicability in real-world scenarios.\\n\\nE.2 Comparison with the naive combination of existing works\\n\\nSo far, we have observed that existing approaches fail to generalize to FDGN since they primarily focus on graphs containing only a single type of noise. A straightforward solution might be to naively combine methods that address each type of noise individually. To explore this idea, we consider AirGNN as the feature noise-robust graph learning method (FNR), RSGNN as the structure noise-robust graph learning method (SNR), and RTGNN as the label noise-robust graph learning method (LNR). We carefully implement all possible combinations among FNR, SNR, and LNR.\\n\\nIn Table 7, we observe that naive combination can improve robustness in some cases, but it may not consistently yield favorable results. For example, combining FNR and SNR notably enhances robustness. However, when we combine all three (FNR, SNR, and LNR), which is expected to yield the best results, performance even decreases. This could be attributed to compatibility issues among the methods arising from the naive combination. Furthermore, although some combinations improve robustness, PRINGLE consistently outperforms all combinations. We attribute this to the fact that naively combining existing methods may not capture the causal relationships in the DGP of FDGN, limiting their robustness. In contrast, PRINGLE successfully captures these relationships, resulting in superior performance.\\n\\nE.3 Sensitivity analysis\\n\\nIn this section, we analyze the sensitivity of the coefficient $\\\\lambda_1$, $\\\\lambda_2$, and $\\\\lambda_3$ in Eqn 4. To be specific, we increase $\\\\lambda_1$ value from {$0, 0.003, 0.03, 0.3, 3.0$}, $\\\\lambda_2$ value from {$0.0, 0.003, 0.03, 0.3$}, and $\\\\lambda_3$ from {$0.0, 0.01, 0.1, 1.0, 10.0$}. We then evaluate the node classification accuracy of PRINGLE under FDGN.\"}"}
{"id": "kNGxg8shA1", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Fig 11(a), we observe that the performance significantly drops when $\\\\lambda_1 = 0$. This highlights the importance of modeling the causal relationship $A \\\\leftarrow (X, \\\\epsilon, Z)$ for robustness under FDGN, as $\\\\lambda_1$ is directly related to the loss term $L_{\\\\text{edge-rec}}$, i.e., $-\\\\mathbb{E}_{Z_A \\\\sim q_\\\\phi^1} \\\\mathbb{E}_{\\\\epsilon \\\\sim q_\\\\phi^2} [\\\\log(p_{\\\\theta}^1(A|X, \\\\epsilon, Z_A))]$.\\n\\nIn Fig 11(b), we can see a performance decrease when $\\\\lambda_2 = 0$. This observation suggests that the regularization on the inferred latent node label $Z_Y$ using the inferred latent structure $Z_A$ effectively handles the noisy labels. This conclusion is drawn from the fact that $\\\\lambda_2$ is directly linked to the loss term $L_{\\\\text{hom}}$, i.e., $\\\\text{KL}(q_\\\\phi^3(Z_Y|X, A)|| p(Z_Y))$.\\n\\nIn Fig 11(c), we can clearly observe a substantial performance drop when $\\\\lambda_3 = 0$, and the performance tends to improve as $\\\\lambda_3$ increases. This observation strongly suggests the importance of modeling the causal relationships $X \\\\leftarrow (\\\\epsilon, Z_Y)$ and $Y \\\\leftarrow (X, A, Z_Y)$ for robustness under FDGN. We draw this conclusion by considering that $\\\\lambda_3$ is directly associated to the loss terms $L_{\\\\text{rec-feat}}$, $L_{\\\\text{cls-dec}}$, and $L_p$, i.e., $-\\\\mathbb{E}_{\\\\epsilon \\\\sim q_\\\\phi^2} E_{Z_Y \\\\sim q_\\\\phi^3} [\\\\log(p_{\\\\theta}^2(X|\\\\epsilon, Z_Y))], -\\\\mathbb{E}_{Z_Y \\\\sim q_\\\\phi^3} [\\\\log(p_{\\\\theta}^3(Y|X, A, Z_Y))], \\\\text{and } \\\\mathbb{E}_{Z_Y \\\\sim q_\\\\phi^3} [\\\\text{KL}(q_\\\\phi^2(\\\\epsilon|X, A, Z_Y)|| p(\\\\epsilon))].$\\n\\nIn this subsection, we qualitatively analyze how well PRINGLE infers the latent variable $\\\\epsilon_A$ and $Z_A$. In Fig 12(a), we conducted an analysis of the inference of $\\\\epsilon_A$ by comparing the distribution of $\\\\hat{p}_{el_{ij}}$ values estimated during the training of PRINGLE on clean and noisy graphs (FDGN-50%). We observe that $\\\\hat{p}_{el_{ij}}$ values estimated from the clean graph tend to be close to 1, while those from the graph with FDGN are considerably smaller. This observation suggests that the inference of $\\\\epsilon_A$ was accurate, as the high values of $\\\\hat{p}_{el_{ij}}$ indicate that the model recognizes the edge $(i, j)$ as a clean edge.\\n\\nIn Fig 12(b), we analyze the inference of $Z_A$ by comparing the distribution of $\\\\hat{p}_{ij}$ values, which constitute the estimated latent graph structure $\\\\hat{A}$, between noisy edges and the original clean edges. It is evident that the estimated edge probabilities $\\\\hat{p}_{ij}$ for noisy edges are predominantly assigned smaller values, while those for clean edges tend to be assigned larger values. This observation illustrates that PRINGLE effectively mitigates the impact of noisy edges during the message-passing process, thereby enhancing its robustness in the presence of noisy graph structure.\\n\\nThis achievement can be attributed to the label regularization effect achieved through the accurate inference of $\\\\epsilon_A$. Specifically, as the observed graph structure contains noisy edges, the inaccurate supervision for $L_{\\\\text{rec-edge}}$ impedes the distinction between noisy edges and the original clean edges in terms of edge probability values $\\\\hat{p}_{ij}$. However, the label regularization technique proves crucial for alleviating this issue, benefitting from the accurate inference of $\\\\epsilon_A$. \\n\\nE.4 ANALYSIS OF THE INFERRED $Z_A$ AND $\\\\epsilon_A$\"}"}
{"id": "kNGxg8shA1", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"In this work, our main focus is on the limitations of the conventional graph noise assumption in terms of node features: the noise in node features is independent of the graph structure or node labels. To this end, we propose a more realistic noise scenario FDGN by introducing the causal relationships $A \\\\leftarrow X$ and $Y \\\\leftarrow (X, A)$ into the DGP of CGN. However, it is crucial to acknowledge that in some real-world scenarios, the causal relationship $X \\\\leftarrow A$ may indeed manifest, which indicates that the graph structure noise inevitably entails the node feature noise. For instance, consider a social network where node features represent the content to which a user is exposed or interacts with (e.g., views, clicks, or likes), while the graph structure denotes the follower relationships. In such a scenario, if a user follows or is followed by fake accounts, the graph structure might incorporate noisy links (i.e., noisy graph structure). This, in turn, can impact the content to which users are exposed and their interactions (i.e., noisy node features), eventually influencing their community assignments (i.e., noisy node labels). In other words, the noisy node feature and noisy graph structure mutually influence the noise of each other, ultimately incurring the noisy node label. We denote this scenario as feature structure-dependent graph-noise (FSDGN), and its DGP is illustrated in Fig 13(b).\\n\\nComparison to CausalNL (Yao et al., 2021)\\n\\nOne can suggest that our work may appear to lack technical novelty in comparison to CausalNL (Yao et al., 2021). However, it is important to note that we tackle additional, more challenging aspects unique to our specific problem that do not exist in CausalNL. Specifically, when additionally introducing $A$, we handle four more causal relationships: $A \\\\leftarrow \\\\epsilon$, $A \\\\leftarrow X$, $Y \\\\leftarrow A$, $A \\\\leftarrow Z$, each of which is non-trivial to consider. We would like to specify the challenges in instantiations caused by their additional introduction.\\n\\nInference of $\\\\epsilon$. In contrast to CausalNL, which assumes that $\\\\epsilon$ is a cause of $X$, our proposed FDGN consider that $\\\\epsilon$ is a cause of both $X$ and $A$. In other words, the DGP of FDGN contains the causal relationships $X \\\\leftarrow \\\\epsilon$ and $A \\\\leftarrow \\\\epsilon$, as real-world applications often exhibit graph structure noise originating from arbitrary sources (i.e., $\\\\epsilon$) in addition to the feature-dependent noise. Therefore, this scenario is a unique characteristic of our problem and not addressed in CausalNL. To deal with this, we decompose $q_{\\\\phi_2}(\\\\epsilon | X, A, Z)$ into $q_{\\\\phi_21}(\\\\epsilon | X, Z)$ and $q_{\\\\phi_22}(\\\\epsilon | A)$. While the instantiation of $q_{\\\\phi_21}(\\\\epsilon | X, Z)$ is similar to CausalNL, that of $q_{\\\\phi_22}(\\\\epsilon | A)$ is non-trivial and is absent in CausalNL. In our approach, we regard $\\\\epsilon$ as a set of scores indicating the likelihood of each observed edge being noisy or not. Moreover, we leverage the concept of early-learning phenomenon to infer $\\\\epsilon$. It is worth emphasizing once again that the instantiation of this scenario is novel and not a straightforward extension of CausalNL.\\n\\nLoss term $\\\\mathcal{KL}(q_{\\\\phi_2}(Z | A) || p(Z | A))$.\\n\\nTo compute this loss term, we encounter two primary challenges:\\n\\n1. Designing an appropriate prior for the latent graph structure $p(Z | A)$.\\n2. Addressing the complexity associated with calculating the KL divergence between the two matrices sampled from Bernoulli distributions.\\n\\nIn response to the first challenge, we employ the $\\\\gamma$-hop subgraph similarity as a metric to identify assortative edges. Regarding the second challenge, we introduce a predefined candidate graph, which includes the observed edge set along with a $k$-NN graph based on the $\\\\gamma$-hop subgraph similarity. Both of these challenges represent non-trivial aspects of our approach and are absent in CausalNL.\\n\\nLoss term $\\\\mathcal{KL}(q_{\\\\phi_3}(Z | Y) || p(Z | Y))$.\\n\\nTo compute this loss term, CausalNL employs a uniform distribution as the prior $p(Z | Y)$. In contrast, we introduce the concept of class homophily to effectively...\"}"}
{"id": "kNGxg8shA1", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"regularize the inference of latent clean node label $Z$. Specifically, we encourage $Z$ to align with our prior knowledge, i.e., $p(Z|Y)$, that the two end nodes on the accurately inferred latent graph structure $ZA$ are expected to have identical latent labels. Therefore, using this prior helps alleviate the noisy node label issues. It is essential to highlight that this instantiation effectively utilizes the property unique to the graph domain, which is not present in CausalNL. It is worth emphasizing once again that such an instantiation is novel and not a straightforward extension of CausalNL.\\n\\nLoss term $E_{Z\\\\sim q(\\\\varphi)} \\\\left[ kl(q(\\\\varphi_2(\\\\epsilon|X,A,Z,Y))||p(\\\\epsilon)) \\\\right]$. This term is decomposed into $kl(q(\\\\varphi_2(\\\\epsilon|X,Z,Y))||p(\\\\epsilon_X))$ and $kl(q(\\\\varphi_2(\\\\epsilon|A,X))||p(\\\\epsilon_A))$. The first term is calculated in a similar manner to CausalNL. Specifically, a Gaussian distribution is employed as the prior $p(\\\\epsilon_X)$. However, the second term is unique to our problem, and its computation necessitates prior information about $\\\\epsilon_A$. As $\\\\epsilon_A$ indicates the likelihood of each observed edge being noisy or not, it is not appropriate to simply assume $p(\\\\epsilon_A)$ as a Gaussian distribution as done in CausalNL. This aspect makes the problem more challenging. To address this challenge, we introduce a new assumption that the inferred $\\\\epsilon_A$ follows an unknown distribution with high variance, while our prior knowledge suggests that $p(\\\\epsilon_A)$ follows the same distribution but with low variance. Additionally, we employ an Exponential Moving Average (EMA) technique to reduce the uncertainty of the inferred $\\\\epsilon_A$. This challenge represents non-trivial aspects of our approach and is absent in CausalNL.\\n\\n**ADDITIONAL REAL-WORLD EXAMPLES OF FDGN**\\n\\nIn this paper, we consistently assert that FDGN represents a more realistic graph noise scenario. In order to substantiate this assertion further, we present additional real-world application examples, in addition to those involving social networks and co-purchase networks:\\n\\n- **Biological Networks**: A cell-cell graph is widely used in computational biology. In the cell-cell graph, each node represents each cell. Then corresponding node features and node labels are represented as gene expression and cell type, respectively. However, gene expression as cell-gene count matrix often contain noises such as dropout phenomenon and batch effect. Due to the lack of cell-cell graph structure (i.e., graph adjacency), many works design cell-cell graph structures via utilizing the node features (Wang et al., 2021; Xiong et al., 2023; Yun et al., 2023), which may entail the noisy cell-cell graph structures. Furthermore, regarding the cell type (i.e., node label) is annotated by using transcripted marker genes (i.e., important features), noisy node features may lead the noisy node labels.\\n\\n- **Recommendation systems**: In recommendation systems of various domains (e.g., e-commerce, news, and music), user-item interaction graphs are common. The node features may represent users' and products' information and user-item interactions represent the graph structures. Furthermore, the node labels can be represented by the users' communities (interests) and products' categories. When a user creates a fake or incomplete profile due to various reasons (e.g., privacy), products irrelevant to the user's genuine interest can be exposed in a web/app page to promote the user to view/click/purchase the products. Then, the user is more likely to interact with irrelevant products (i.e., noisy graph structure) due to the user's noisy features. Furthermore, such noisy users' information and interactions may also eventually change their communities (i.e., noisy node label).\\n\\n**IFURTHER COMPARISON TO LABEL NOISE-ROBUST BASELINES**\\n\\nWe agree with the reviewer's comment about the missing baselines. Hence, we further compare PRINGLE with the widely used label noise baselines: Co-teaching+ (Yu et al., 2019), CP (Zhang et al., 2020), D-GNN (NT et al., 2019), and CGNN (Yuan et al., 2023). From Table 8, we clearly see that the proposed method, PRINGLE, outperforms all the baselines. We argue that these baseline methods were designed to tackle the noisy node labels while assuming that both node features and graph structure are noise-free. However, the proposed FDGN introduces a more realistic noise scenario, wherein node features, graph structures, and node labels simultaneously contain noise. As a consequence, the performance of these existing methods is considerably more constrained compared to PRINGLE.\"}"}
{"id": "kNGxg8shA1", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Node classification performance under synthetic feature-dependent graph-noise (FDGN).\\n\\n| Dataset | Setting  | PRINGLE  | Co-teaching+ | CP | D-GNN | CGNN | CGNN |\\n|---------|----------|----------|--------------|----|-------|------|------|\\n|         |          |          |              |    |       |      |      |\\n|         | Clean    |          |              |    |       |      |      |\\n| PRINGLE |          | 84.7\u00b10.9 | 84.3\u00b10.4     | 83.7\u00b10.5 | 85.2\u00b10.7 |\\n|         | FDGN-10% | 76.9\u00b10.5 | 78.7\u00b10.6     | 78.6\u00b10.5 | 77.4\u00b10.3 |\\n|         | FDGN-30% | 66.0\u00b10.8 | 68.2\u00b10.5     | 68.5\u00b10.4 | 69.2\u00b10.8 |\\n|         | FDGN-50% | 55.0\u00b10.1 | 53.1\u00b10.9     | 57.7\u00b10.3 | 55.1\u00b10.2 |\\n| Citeseer | Clean    | 72.7\u00b10.4 | 72.8\u00b10.9     | 75.1\u00b10.2 | 71.1\u00b10.9 |\\n|         | FDGN-10% | 67.7\u00b10.6 | 68.4\u00b10.8     | 69.0\u00b10.5 | 65.6\u00b10.4 |\\n|         | FDGN-30% | 55.0\u00b10.9 | 56.8\u00b10.9     | 54.0\u00b10.7 | 54.1\u00b10.3 |\\n|         | FDGN-50% | 47.4\u00b10.8 | 46.5\u00b11.0     | 44.7\u00b10.1 | 46.9\u00b11.4 |\\n| Photo   | Clean    | 93.1\u00b10.0 | 93.3\u00b10.5     | 93.1\u00b10.1 | 92.7\u00b10.5 |\\n|         | FDGN-10% | 87.9\u00b10.8 | 90.5\u00b10.5     | 90.3\u00b10.6 | 87.1\u00b10.3 |\\n|         | FDGN-30% | 83.1\u00b10.2 | 85.1\u00b11.0     | 85.9\u00b10.3 | 85.1\u00b10.2 |\\n|         | FDGN-50% | 61.9\u00b10.3 | 80.4\u00b10.6     | 85.1\u00b10.8 | 80.5\u00b10.7 |\\n| Comp    | Clean    | 88.6\u00b10.8 | 90.7\u00b10.3     | 89.4\u00b10.9 | 90.0\u00b10.5 |\\n|         | FDGN-10% | 85.6\u00b10.6 | 87.1\u00b10.8     | 86.8\u00b10.4 | 83.0\u00b10.4 |\\n|         | FDGN-30% | 81.5\u00b10.3 | 82.8\u00b10.6     | 82.9\u00b10.5 | 82.3\u00b10.8 |\\n|         | FDGN-50% | 72.8\u00b10.9 | 80.4\u00b11.0     | 85.1\u00b10.8 | 80.5\u00b10.7 |\\n\\nTable 9: Training time comparison on Cora dataset under FDGN 50%.\\n\\n| Model       | Total training time (sec) | Training time / epoch (sec) |\\n|-------------|---------------------------|-----------------------------|\\n| PRINGLE     | 93.90                     | 0.19                        |\\n| WSGNN       | 20.9                      | 0.04                        |\\n| AirGNN      | 702.14                    | 1.77                        |\\n| ProGNN      | 159.87                    | 0.16                        |\\n| RSGNN       | 53.33                      | -                           |\\n| STABLE      | 0.81                      | 0.004                       |\\n| NRGNN       | 100.33                    | 0.20                        |\\n| RTGNN       | 118.7                     | 0.18                        |\\n\\nWe compare the training time of PRINGLE with the baselines to analyze the computational complexity of PRINGLE. In Table 9, we report the total training time and training time per epoch on Cora with FDGN 50% for all models. Note that since STABLE is a 2-stage method, we did not report the training time per epoch. The results show that PRINGLE requires significantly less total training time and training time per epoch compared to WSGNN, ProGNN, RSGNN, STABLE, NRGNN, and RTGNN. This suggests that PRINGLE's training procedure is faster than that of most baselines while still achieving substantial performance improvements. Although AirGNN and EvenNet require much less training time than PRINGLE, their node classification accuracy is notably worse than other methods, including PRINGLE. This indicates that, despite their fast training times, they may not be suitable for real-world scenarios. In summary, PRINGLE demonstrates superior performance compared to the baselines while maintaining acceptable training times.\\n\\nFurther discussion with graph denoising and reconstruction\\n\\nLi et al. (2022b) introduced a debiased learning framework, GraphDE, designed to handle situations where out-of-distribution (OOD) samples are present in training data. Given that a noisy sample can be viewed as an OOD sample, GraphDE shares a similar objective with our work. Additionally, GraphDE employs a generative model based on variational inference, relevant to learning approach. However, there are significant distinctions in the DGP that we assume compared to GraphDE. Specifically, GraphDE assumes that a latent variable $e$ determines whether an instance is an OOD sample or not, overlooking the way that the OOD sample is generated. Conversely, the DGP of our proposed FDGN characterizes the process by which noisy samples (i.e., OOD sample) are generated (e.g., in a feature-dependent manner). This crucial difference enables a more fine-grained model learning than GraphDE, which may weaken the applicability of GraphDE to complex real-world noise scenarios, such as FDGN.\\n\\nZhou et al. (2023) introduced a graph structure denoising framework called RGIB, primarily centered on the link prediction task. In their work, while $Z$ and $A$ share the same meaning as in our paper, $Y$ denotes \u201cedge labels\u201d rather than node labels (i.e., it serves as a binary indicator for the presence of query edges), and $Z_Y$ refers to \u201cclean edge labels\u201d rather than clean latent node labels. These distinctions significantly differentiate their approach from ours. Furthermore, it's important to note that RGIB primarily addressed noisy graph structures while assuming that node features and node labels are noise-free. In contrast, our proposed method, PRINGLE, is developed under the more realistic FDGN assumption, where node features, graph structures, and node labels all simultaneously contain noise. This fundamental difference underscores the enhanced applicability and robustness of PRINGLE compared to RGIB.\"}"}
{"id": "kNGxg8shA1", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In real-world scenarios, node features frequently exhibit noise due to various factors, making GNNs vulnerable. Various methods enhance robustness, but they make an unrealistic assumption that the noise in node features is independent of the graph structure and node labels, restricting their practicality. To this end, we introduce a more realistic noise scenario, called feature-dependent graph-noise (FDGN), where noisy node features may entail both structure and label noise, and propose a deep generative model that directly captures the causal relationships among the variables in the DGP of FDGN. We formulate a tractable and feasible learning objective based on variational inference and provide detailed discussions on the instantiations of the model components corresponding to the derived terms. Our proposed method, PRINGLE, outperforms baselines on commonly used benchmark datasets and newly introduced real-world graph datasets that simulate FDGN in e-commerce systems. Our code is available at https://anonymous.4open.science/r/FDGN-PRINGLE-4B31/.\\n\\nWhile such existing robust GNN models have proven effective, we argue that their practicality is restricted by an unrealistic assumption regarding graph noise: they assume that the noise in node features is independent of the graph structure or node labels. For example, in the conventional graph noise (CGN) assumption in terms of node features (Fig. 1(b)), Bob's fake profile does not influence other nodes, which is also explained by the data generating process (DGP) of CGN (See Fig. 2(a)) in which no causal relationships exist among the noisy node features $X$, graph structure $A$, and node labels $Y$. However, in reality (See Fig. 1(c)), other users may make connections with Bob based on his fake profile (i.e., structure noise), which may also eventually change their community (i.e., label noise), and such causal relationships among $X$, $A$, and $Y$ (i.e., $A \\\\leftarrow X$, $Y \\\\leftarrow X$, and $Y \\\\leftarrow A$) are depicted in Fig. 2(b).\"}"}
{"id": "kNGxg8shA1", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"In this case, fake reviews on products written by a fraudster would make other users purchase irrelevant products, which adds irrelevant edges between products (i.e., structure noise). Consequently, this would make the automated product category labeling system to inaccurately annotate product categories (i.e., label noise), as it relies on the node features and the graph structure, both of which are contaminated.\\n\\nThese examples demonstrate that, in reality, noisy node features may entail both structure and label noise. To reflect such a realistic scenario in graph learning, we define feature-dependent graph noise (FDGN) and model the data generation process in a graphical model. We also observe that existing robust GNN models indeed fail to generalize effectively in a more realistic FDGN due to their inadequate assumption on the data generation process (Sec 3.2).\\n\\nIn this work, we propose a principled noisy graph learning framework (PRINGLE), which operates under a more realistic FDGN assumption. We first define the DGP of FDGN as shown in Fig. 2(b). More precisely, we introduce three observable variables (i.e., node features X, graph structure A, and node labels Y) and three latent variables (i.e., noise incurring variable \u03f5, latent clean graph structure Z_A, and latent clean node labels Z_Y), while defining causal relationships among these variables to represent the data generation process of FDGN. We then devise a deep generative model that directly captures the causal relationships among the variables in the DGP of FDGN, and derive a tractable and feasible learning objective based on variational inference. It is worth noting that although the main focus of this paper is on the node feature noise and its influence across the graph, our proposed robust graph learning framework is capable of generalizing not only to feature-dependent graph noise (FDGN), but also to independent structure/feature/label noise that is also prevalent in real-world applications. This implies that PRINGLE has a wider range of applicability than existing robust GNN models. In order to conduct a rigorous evaluation of PRINGLE, we perform evaluations on not only existing benchmark datasets, but also newly introduced real-world graph datasets that simulate FDGN within e-commerce systems, providing a valuable alternative to synthetic settings.\\n\\nIn summary, the main contributions of the paper are three-fold:\\n\\n\u2022 We investigate limitations of the conventional graph noise assumption in terms of node features, and introduce a more realistic graph noise scenario, feature-dependent graph noise (FDGN). To the best of our knowledge, this is the first attempt to understand the data generation process in graph domain that mimics the noise scenario in real-world.\\n\\n\u2022 We propose the principled noisy graph learning framework (PRINGLE) which addresses FDGN by modeling its DGP. PRINGLE outperforms state-of-the-art baselines in node classification and link prediction tasks under various scenarios including feature-dependent graph-noise and independent structure/feature/label noise.\\n\\n\u2022 In addition to existing benchmark datasets in which noise is synthetically generated, we further introduce novel graph benchmark datasets that simulate FDGN within e-commerce systems, which is expected to foster practical research in noise-robust graph learning.\\n\\n2 RELATED WORKS\\n\\nThe objective of noise-robust graph learning is to train GNN models when the input graph data exhibits one or more of the following types of noise: 1) node feature noise, 2) graph structure noise, and/or 3) node label noise. The majority of existing approaches focus primarily on graphs containing only a single type of noise.\\n\\nFeature noise-robust graph learning. To address the noisy node features, various approaches including adversarial training (Tian et al., 2023) and test-time graph transformation (Jin et al., 2022), have been proposed. Additionally, recent studies have highlighted the significance of fully leveraging structural information. AirGNN (Liu et al., 2021) proposed a novel message passing mechanism that identifies the nodes with noisy features and learns node-wise adaptive coefficients that balance the feature aggregation and use of their own noisy features. The identification process is guided by the intuition that nodes with noisy features tend to have dissimilar features within their local neighborhoods. In summary, this approach tackles the noisy node features while assuming that the structure of the input graph is noise-free.\"}"}
{"id": "kNGxg8shA1", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Structure noise-robust graph learning.\\n\\nTo address the noisy graph structure, various approaches including robust message passing scheme (Lei et al., 2022) and graph structure learning (Jin et al., 2020) have been proposed. Among them, a representative approach is based on the graph structure learning (GSL), which aims to learn a refined graph structure from a given graph. Specifically, ProGNN (Jin et al., 2020) learns a graph structure aiming at satisfying the real-world graph properties, e.g., feature-smoothness. Moreover, RSGNN (Dai et al., 2022) aims to train a graph structure learner, which is composed of an MLP encoder and a regularizer for enhancing feature-smoothness, which encourages the nodes with similar features to be connected in the refined structure. STABLE (Li et al., 2022a) aims at acquiring robust node representations by roughly refining the given graph by removing easily detectable noisy edges, typically those connecting nodes with low feature-similarity, after which node representations are learned in an unsupervised manner. Finally, based on these representations, a kNN graph is constructed to serve as the refined structure. In summary, these methods tackle the noisy graph structure while assuming that node features are noise-free.\\n\\nLabel noise-robust graph learning.\\n\\nThere are numerous studies that address the noisy labels on non-graph data (Li et al., 2020; Wang et al., 2019b; Yao et al., 2021). However, since they are not directly applicable for graph data (Dai et al., 2021), recent studies investigated the label noise-robust graph learning methods. The key idea of NRGNN (Dai et al., 2021) is to correct the predictions of unlabeled nodes affected by information propagation from falsely labeled nodes. To do so, NRGNN learns a new graph structure where two nodes with similar features are connected, based on the assumption that two nodes are more likely to have the same label if they have similar features. This strategy mitigates the information propagation from falsely labeled nodes. RTGNN (Qian et al., 2023) identifies clean labeled samples from noisy labeled ones based on the small-loss criteria and leverages pseudo-labeling to supplement the labeled nodes. However, nodes with noisy features or structures would yield a large-loss even if their labels do not contain any noise, which results in inaccuracies in the selection of clean labeled samples. Therefore, these methods tackle the noisy node labels while assuming that both node features and graph structure are noise-free.\\n\\nGenerative graph learning.\\n\\nApart from the noise-robust graph learning methods, several studies adopted deep generative modeling to infer latent graph data (Ma et al., 2019; Elinas et al., 2020; Lao et al., 2022). Most recently, WSGNN (Lao et al., 2022) uses a probabilistic generative approach and variational inference to infer the latent graph structure and node labels. However, it assumes noise-free graphs, making it less effective in handling various noise scenarios commonly found in real-world applications.\\n\\nIn summary, each of the aforementioned methods assumes the completeness of at least one of the data sources, i.e., node features, graph structures, or node labels. In contrast, our proposed method is constructed under a more realistic FDGN assumption, where noise in node features may result in structural and label noise. This fundamental difference liberates the proposed method from such limited assumptions.\\n\\n3. FEATURE-DEPENDENT GRAPH NOISE (FDGN)\\n\\n3.1 DEFINITION\\n\\nIn this section, we define a realistic graph noise assumption, i.e., FDGN, and its DGP. Specifically, we assume that the graph containing FDGN is generated according to the graphical model in Fig. 2(b). We first introduce each variable in the graphical model, and then explain each relationship between variables through two real-world applications where FDGN exists: social networks (i.e., user-user graph) and co-purchase networks (i.e., product-product graph) within e-commerce systems.\\n\\nIn the graphical model in Fig. 2(b), $X$ represents the node features, which may contain noisy node features; $Y$ represents the observed node labels, which may contain noisy labels; $A$ represents the observed edges between two nodes, which may contain noisy edges; $\\\\epsilon$ represents the environment variable that causes the noise; $Z$ represents the latent clean node labels; $Z_A$ represents the latent clean graph structure that contains all potential connections between nodes. We provide explanations for each relationship in the graphical model of FDGN shown in Fig. 2(b):\\n\\n- $X \\\\leftarrow (\\\\epsilon, Z_Y)$: $\\\\epsilon$ and $Z_Y$ are causes of $X$. In social networks, users create their profiles and postings (i.e., $X$) regarding their true communities or interests (i.e., $Z_Y$). However, if users decide to display fake profiles for some reason (i.e., $\\\\epsilon$), $\\\\epsilon$ is a cause of the noisy node features $X$. In co-purchase networks, the reviews and descriptions of products (i.e., $X$) are written regarding their true categories (i.e., $Z_Y$). However, if a fraudster (i.e., $\\\\epsilon$) writes fake reviews on products, $\\\\epsilon$ is a cause of the noisy node features (i.e., $X$).\"}"}
{"id": "kNGxg8shA1", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A \u2190 (Z, X): Z and X are causes of A. In social networks, the follow relationship among users (i.e., A) are made based on their latent relationships (i.e., Z). However, if a user creates a fake profile (i.e., X), some irrelevant users may follow the user based on his/her fake profile, which leads to noisy edges (i.e., A).\\n\\nA \u2190 \u03f5: To provide a broader scope, we also posit that \u03f5 is a potential cause of A. This extension is well-founded, as real-world applications often exhibit graph structure noise originating from various sources in addition to the feature-dependent noise (Liu et al., 2022; Fatemi et al., 2021).\\n\\nY \u2190 (Z, X, A): Z, X, and A are causes of Y. In social networks, the true communities (or interests) of users (i.e., Z) are leveraged to promote products to targeted users within a community (Ma et al., 2021). To detect the communities, both node features and graph structures are utilized. However, if a user has noisy node features (i.e., X) or noisy edges (i.e., A), the user may be assigned to a wrong community (or interest), which leads to noisy labels (i.e., Y). In co-purchase networks, machine learning-based automated labeling techniques are widely used in e-commerce systems to label the true categories of products (i.e., Z) since new products are continuously released. However, the automated labeling systems may become inaccurate due to noisy node features (i.e., X) and noisy graph structures (i.e., A), which leads to noisy node labels (i.e., Y).\\n\\nFor simplicity, we assume that \u03f5 is not a cause of Y. This assumption aligns with practical scenarios in real-world applications, where an instance is more likely to be mislabeled due to confusing or noisy features rather than arbitrary sources, i.e., instance-dependent label-noise (Yao et al., 2021; Berthon et al., 2021). In other words, label noise in graphs is predominantly caused by confusing or noisy features and graph structure, i.e., Y \u2190 (X, A), rather than an arbitrary external factor, i.e., Y \u219b \u03f5.\\n\\n3.2 Preliminary Analysis on FDGN\\n\\nWe conduct an analysis to examine how well existing robust GNN models generalize to FDGN. We generate three types of noise: random feature noise (Liu et al., 2021), random structure noise (Li et al., 2022a), and random label noise (Dai et al., 2021; Qian et al., 2023), following the convention of the existing studies. As baselines, we consider (a) AirGNN as a feature noise-robust graph learning method, (b) RSGNN and STABLE as structure noise-robust graph learning methods, and (c) NRGNN and RTGNN as label noise-robust graph learning methods. A comprehensive description of each method is provided in Sec. 2.\\n\\nWe observe that while existing noise-robust graph learning methods perform well under the random feature noise (i.e., + RFN in Fig. 3(a)), structure noise (+ RSN in Fig. 3(b)), and label noise (+ RLN in Fig. 3(c)), their performance significantly drops under FDGN (i.e., + FDGN in Fig. 3). In contrast, our proposed method (PRINGLE) demonstrates competitive results under the random noise scenarios, while notably outperforming the baselines under FDGN. This points to a key distinction \u2013 existing noise-robust graph learning methods struggle to generalize to FDGN due to their limited assumptions regarding the graph noise. Specifically, as summarized in Sec. 2, each category of methods assumes at least one of the data sources is noise-free: node features, graph structures, or node labels. Nevertheless, the causal relationships among X, A, and Y within the data generation process of FDGN gives rise to scenarios involving concurrent feature, structure, and label noise. Consequently, existing robust GNN models fall short of effectively generalizing to FDGN, as they overlook such underlying relationships among noise types, leading to model designs assuming the completeness of at least one data source. Conversely, PRINGLE directly captures the underlying relationships by modeling the DGP of FDGN, resulting in superior generalization to FDGN.\\n\\n4 Principled Noisy Graph Learning Framework (PRINGLE)\\n\\nIn this section, we propose a principled noisy graph learning framework (PRINGLE) to tackle more realistic noise scenario, FDGN. It is essential to highlight that under FDGN, noisy node features entail both structure and label noise, resulting in a graph that does not contain any noise-free data sources.\"}"}
{"id": "kNGxg8shA1", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We commence by modeling joint distribution \\\\( P(X, A) \\\\).\\n\\nAs a result, we can simplify Eqn. 1 as follows:\\n\\nAppendix A shows the overall architecture and training algorithm of PRINGLE (Tasks: node classification and link prediction).\\n\\nIn the node classification task, we assume the posterior distribution can be decomposed into three distributions:\\n\\nTo jointly optimize the parameter \\\\( \\\\phi \\\\), we adopt the variational inference framework (Blei et al., 2019; Lao et al., 2022). Specifically, we derive the Evidence Lower BOund (ELBO) for the observed data, rather than optimizing the marginal likelihood directly. Specifically, we derive the ELBO as follows:\\n\\n\\\\[\\nE[ \\\\log( p(X, A, Y, \\\\epsilon, Z) ) - \\\\sum_{i,j} q(\\\\epsilon_i, Z_j) ]\\n\\\\]\\n\\nare given, latent clean labels are given, latent clean graph structure from the noise-incurring variable is conditionally independent from the observed node labels.\\n\\nFor simplicity, we introduce two additional assumptions. First, when the node features and node labels may contain noise. First, we derive the Evidence Lower Bound (ELBO) for the observed data.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(\\\\epsilon_i, Z_j) )\\n\\\\]\\n\\nare given, latent clean graph structure prediction task, our goal is to predict reliable links based on partially observed edges by inferring the latent clean node label.\\n\\nWe observe the following conditional independence relationships in Fig. 2(b): (1) relationships determined by trainable parameters.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(A_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(\\\\epsilon_i, Z_j) )\\n\\\\]\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(\\\\epsilon_i, Z_j) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\\n\\n\\\\[\\n\\\\frac{1}{N} \\\\sum_{i,j} \\\\log( p(X_{ij}) )\\n\\\\]\\n\\nrelationships are conditionally independent from the observed node labels.\"}"}
{"id": "kNGxg8shA1", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The encoder\\n\\nTo minimize this term, we encourage\\n\\nWe decompose\\n\\nE\\n\\nInference of\\n\\n\\\\[ \\\\text{Loss term} \\\\]\\n\\n\\\\[ \\\\text{KL divergence} \\\\]\\n\\n\\\\[ \\\\text{Inference of} \\\\]\\n\\n\\\\[ \\\\text{Prior knowledge, i.e.,} \\\\]\\n\\n\\\\[ \\\\text{Approach mitigates the complexity issue.} \\\\]\\n\\n\\\\[ \\\\text{the edges in a candidate graph. Please refer to the Appendix C for detailed information on how this} \\\\]\\n\\n\\\\[ \\\\text{The implemented loss function is} \\\\]\\n\\n\\\\[ \\\\text{Under review as a conference paper at ICLR 2024} \\\\]\\n\\n\\\\[ \\\\text{as a set of scores indicating the} \\\\]\\n\\n\\\\[ \\\\text{potential metric for identifying assortative edges. Hence, we model} \\\\]\\n\\n\\\\[ \\\\text{recent studies (Choi et al., 2022; Dai et al., 2022), the} \\\\]\\n\\n\\\\[ \\\\text{potential to enhance feature propagation within GNNs (Zhao et al., 2023). In} \\\\]\\n\\n\\\\[ \\\\text{likelihood of each observed edge being noisy or not. Inspired by an early-learning phenomenon} \\\\]\\n\\n\\\\[ \\\\text{and outputs} \\\\]\\n\\n\\\\[ \\\\text{Z outputs the probability of the latent label} \\\\]\\n\\n\\\\[ \\\\text{Inference of} \\\\]\\n\\n\\\\[ \\\\text{ELBO} \\\\]\\n\\n\\\\[ \\\\text{ELBO} \\\\]\\n\\n\\\\[ \\\\text{as:} \\\\]\\n\\n\\\\[ \\\\text{as:} \\\\]\\n\\n\\\\[ \\\\text{as a candidate graph that consists of the observed edge set} \\\\]\\n\\n\\\\[ \\\\text{To mitigate the issue, we pre-define a candidate graph that consists of the observed edge set} \\\\]\\n\\n\\\\[ \\\\text{To acquire deterministic node embeddings, i.e.,} \\\\]\\n\\n\\\\[ \\\\text{We pre-define a candidate graph that consists of the observed edge set} \\\\]\\n\\n\\\\[ \\\\text{To acquire deterministic node embeddings, i.e.,} \\\\]\\n\\n\\\\[ \\\\text{Because} \\\\]\\n\\n\\\\[ \\\\text{potent metric for identifying assortative edges. Hence, we model} \\\\]\\n\\n\\\\[ \\\\text{Prior knowledge, i.e.,} \\\\]\\n\\n\\\\[ \\\\text{Approach mitigates the complexity issue.} \\\\]\\n\\n\\\\[ \\\\text{the edges in a candidate graph. Please refer to the Appendix C for detailed information on how this} \\\\]\\n\\n\\\\[ \\\\text{as:} \\\\]\\n\\n\\\\[ \\\\text{as a set of scores indicating the} \\\\]\\n\\n\\\\[ \\\\text{potential metric for identifying assortative edges. Hence, we model} \\\\]\\n\\n\\\\[ \\\\text{recent studies (Choi et al., 2022; Dai et al., 2022), the} \\\\]\\n\\n\\\\[ \\\\text{potential to enhance feature propagation within GNNs (Zhao et al., 2023). In} \\\\]\\n\\n\\\\[ \\\\text{likelihood of each observed edge being noisy or not. Inspired by an early-learning phenomenon} \\\\]\\n\\n\\\\[ \\\\text{and outputs} \\\\]\\n\\n\\\\[ \\\\text{Z outputs the probability of the latent label} \\\\]\\n\\n\\\\[ \\\\text{Inference of} \\\\]\\n\\n\\\\[ \\\\text{ELBO} \\\\]\\n\\n\\\\[ \\\\text{ELBO} \\\\]\\n\\n\\\\[ \\\\text{as:} \\\\]\\n\\n\\\\[ \\\\text{as:} \\\\]\\n\\n\\\\[ \\\\text{as a set of scores indicating the} \\\\]\\n\\n\\\\[ \\\\text{potential metric for identifying assortative edges. Hence, we model} \\\\]\\n\\n\\\\[ \\\\text{recent studies (Choi et al., 2022; Dai et al., 2022), the} \\\\]\\n\\n\\\\[ \\\\text{potential to enhance feature propagation within GNNs (Zhao et al., 2023). In} \\\\]\\n\\n\\\\[ \\\\text{likelihood of each observed edge being noisy or not. Inspired by an early-learning phenomenon} \\\\]\\n\\n\\\\[ \\\\text{and outputs} \\\\]\\n\\n\\\\[ \\\\text{Z outputs the probability of the latent label} \\\\]\\n\\n\\\\[ \\\\text{Inference of} \\\\]\\n\\n\\\\[ \\\\text{ELBO} \\\\]\\n\\n\\\\[ \\\\text{ELBO} \\\\]\\n\\n\\\\[ \\\\text{as:} \\\\]\\n\\n\\\\[ \\\\text{as:} \\\\]\\n\\n\\\\[ \\\\text{as a set of scores indicating the} \\\\]\\n\\n\\\\[ \\\\text{potential metric for identifying assortative edges. Hence, we model} \\\\]\\n\\n\\\\[ \\\\text{recent studies (Choi et al., 2022; Dai et al., 2022), the} \\\\]\\n\\n\\\\[ \\\\text{potential to enhance feature propagation within GNNs (Zhao et al., 2023). In} \\\\]\\n\\n\\\\[ \\\\text{likelihood of each observed edge being noisy or not. Inspired by an early-learning phenomenon} \\\\]\\n\\n\\\\[ \\\\text{and outputs} \\\\]\\n\\n\\\\[ \\\\text{Z outputs the probability of the latent label} \\\\]\\n\\n\\\\[ \\\\text{Inference of} \\\\]\\n\\n\\\\[ \\\\text{ELBO} \\\\]\\n\\n\\\\[ \\\\text{ELBO} \\\\]\\n\\n\\\\[ \\\\text{as:} \\\\]\\n\\n\\\\[ \\\\text{as:} \\\\]\\n\\n\\\\[ \\\\text{as a set of scores indicating the} \\\\]\\n\\n\\\\[ \\\\text{potential metric for identifying assortative edges. Hence, we model} \\\\]\\n\\n\\\\[ \\\\text{recent studies (Choi et al., 2022; Dai et al., 2022), the} \\\\]\\n\\n\\\\[ \\\\text{potential to enhance feature propagation within GNNs (Zhao et al., 2023). In} \\\\]\\n\\n\\\\[ \\\\text{likelihood of each observed edge being noisy or not. Inspired by an early-learning phenomenon} \\\\]\\n\\n\\\\[ \\\\text{and outputs} \\\\]\\n\\n\\\\[ \\\\text{Z outputs the probability of the latent label} \\\\]\\n\\n\\\\[ \\\\text{Inference of} \\\\]\\n\\n\\\\[ \\\\text{ELBO} \\\\]\\n\\n\\\\[ \\\\text{ELBO} \\\\]\"}"}
{"id": "kNGxg8shA1", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This term is implemented as an edge reconstruction loss forcing the estimated latent structure to match the observed graph structure, reducing the influence of noisy edges under the guidance of the positive edges. The main idea is to penalize the value of $\\\\hat{e}_{ij}$ when $\\\\hat{e}_{ij}$ is that $p(\\\\hat{e}_{ij})$, which introduce inaccurate supervision. Therefore, we employ regularizations on both the pre-training and training processes.\\n\\nThe probability $p(\\\\hat{e}_{ij})$ is used for sampling between $\\\\hat{v}_{ij}$ and $\\\\hat{v}_{ij}$. We have prior knowledge about $p(\\\\hat{e}_{ij})$, which means that a closed form solution of $p(\\\\hat{e}_{ij})$. With $p(\\\\hat{e}_{ij})$, we can convert $\\\\hat{e}_{ij}$ into $\\\\hat{e}_{ij}$ as:\\n\\n$$L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij}) = ||p(\\\\hat{e}_{ij})| | - ||\\\\hat{e}_{ij}||.$$  \\n\\nIn summary, the overall learning objective can be written as follows and $L_{\\\\text{final}}$.\\n\\n$$L_{\\\\text{final}} = \\\\frac{1}{2} \\\\sum_{e_{ij} \\\\in E} L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij})$$\\n\\nThe details of the datasets are given in Appendix D.1.\\n\\n## Experimental Details\\n\\nWe evaluated two newly introduced datasets (Cora, Citeseer, Photo, and Computers) and baselines on four commonly used benchmark datasets (i.e., Auto and Garden) based on Amazon review data (He & McAuley, 2016; McAuley et al., 2015) to mimic FDGN on e-commerce systems (Refer to Appendix D.2.2 for details).\\n\\n### Datasets\\n\\nWe evaluate five schemes, including a GCN classifier that takes node classification loss $L_{\\\\text{cls-enc}}(\\\\hat{Y})$ on $X$, a GCN classifier that takes node classification loss $L_{\\\\text{cls-dec}}(\\\\hat{Y})$ on $X$, and a GCN classifier that takes node classification loss $L_{\\\\text{cls-dec}}(\\\\hat{Y})$, the decoder needs to rely on the information contained in $Z$, where the decoder can be obtained as $\\\\hat{Y} = q(\\\\text{dec}(\\\\text{concat}(X, A, Z)))$.\\n\\nThe feature similarity between $v_i$ and $v_j$ is defined as $\\\\text{sim}(v_i, v_j) = \\\\text{sim}(\\\\theta, \\\\phi)$, where $\\\\theta$ is the feature vector of $v_i$, and $\\\\phi$ is the feature vector of $v_j$.\\n\\nThe noise-robust graph learning and generative graph learning methods. For a $\\\\phi$-parameter, we convert $\\\\hat{e}_{ij}$ into $\\\\hat{e}_{ij}$ as:\\n\\n$$L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij}) = ||p(\\\\hat{e}_{ij})| | - ||\\\\hat{e}_{ij}||.$$  \\n\\nThe main idea is to penalize the value of $\\\\hat{e}_{ij}$ when $\\\\hat{e}_{ij}$ is that $p(\\\\hat{e}_{ij})$, which introduce inaccurate supervision. Therefore, we employ regularizations on both the pre-training and training processes.\\n\\nThe probability $p(\\\\hat{e}_{ij})$ is used for sampling between $\\\\hat{v}_{ij}$ and $\\\\hat{v}_{ij}$. We have prior knowledge about $p(\\\\hat{e}_{ij})$, which means that a closed form solution of $p(\\\\hat{e}_{ij})$. With $p(\\\\hat{e}_{ij})$, we can convert $\\\\hat{e}_{ij}$ into $\\\\hat{e}_{ij}$ as:\\n\\n$$L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij}) = ||p(\\\\hat{e}_{ij})| | - ||\\\\hat{e}_{ij}||.$$  \\n\\nIn summary, the overall learning objective can be written as follows and $L_{\\\\text{final}}$.\\n\\n$$L_{\\\\text{final}} = \\\\frac{1}{2} \\\\sum_{e_{ij} \\\\in E} L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij})$$\\n\\nThe details of the datasets are given in Appendix D.1.\\n\\n## Experimental Details\\n\\nWe evaluated two newly introduced datasets (Cora, Citeseer, Photo, and Computers) and baselines on four commonly used benchmark datasets (i.e., Auto and Garden) based on Amazon review data (He & McAuley, 2016; McAuley et al., 2015) to mimic FDGN on e-commerce systems (Refer to Appendix D.2.2 for details).\\n\\n### Datasets\\n\\nWe evaluate five schemes, including a GCN classifier that takes node classification loss $L_{\\\\text{cls-enc}}(\\\\hat{Y})$ on $X$, a GCN classifier that takes node classification loss $L_{\\\\text{cls-dec}}(\\\\hat{Y})$ on $X$, and a GCN classifier that takes node classification loss $L_{\\\\text{cls-dec}}(\\\\hat{Y})$, the decoder needs to rely on the information contained in $Z$, where the decoder can be obtained as $\\\\hat{Y} = q(\\\\text{dec}(\\\\text{concat}(X, A, Z)))$.\\n\\nThe feature similarity between $v_i$ and $v_j$ is defined as $\\\\text{sim}(v_i, v_j) = \\\\text{sim}(\\\\theta, \\\\phi)$, where $\\\\theta$ is the feature vector of $v_i$, and $\\\\phi$ is the feature vector of $v_j$.\\n\\nThe noise-robust graph learning and generative graph learning methods. For a $\\\\phi$-parameter, we convert $\\\\hat{e}_{ij}$ into $\\\\hat{e}_{ij}$ as:\\n\\n$$L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij}) = ||p(\\\\hat{e}_{ij})| | - ||\\\\hat{e}_{ij}||.$$  \\n\\nThe main idea is to penalize the value of $\\\\hat{e}_{ij}$ when $\\\\hat{e}_{ij}$ is that $p(\\\\hat{e}_{ij})$, which introduce inaccurate supervision. Therefore, we employ regularizations on both the pre-training and training processes.\\n\\nThe probability $p(\\\\hat{e}_{ij})$ is used for sampling between $\\\\hat{v}_{ij}$ and $\\\\hat{v}_{ij}$. We have prior knowledge about $p(\\\\hat{e}_{ij})$, which means that a closed form solution of $p(\\\\hat{e}_{ij})$. With $p(\\\\hat{e}_{ij})$, we can convert $\\\\hat{e}_{ij}$ into $\\\\hat{e}_{ij}$ as:\\n\\n$$L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij}) = ||p(\\\\hat{e}_{ij})| | - ||\\\\hat{e}_{ij}||.$$  \\n\\nIn summary, the overall learning objective can be written as follows and $L_{\\\\text{final}}$.\\n\\n$$L_{\\\\text{final}} = \\\\frac{1}{2} \\\\sum_{e_{ij} \\\\in E} L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij})$$\\n\\nThe details of the datasets are given in Appendix D.1.\\n\\n## Experimental Details\\n\\nWe evaluated two newly introduced datasets (Cora, Citeseer, Photo, and Computers) and baselines on four commonly used benchmark datasets (i.e., Auto and Garden) based on Amazon review data (He & McAuley, 2016; McAuley et al., 2015) to mimic FDGN on e-commerce systems (Refer to Appendix D.2.2 for details).\\n\\n### Datasets\\n\\nWe evaluate five schemes, including a GCN classifier that takes node classification loss $L_{\\\\text{cls-enc}}(\\\\hat{Y})$ on $X$, a GCN classifier that takes node classification loss $L_{\\\\text{cls-dec}}(\\\\hat{Y})$ on $X$, and a GCN classifier that takes node classification loss $L_{\\\\text{cls-dec}}(\\\\hat{Y})$, the decoder needs to rely on the information contained in $Z$, where the decoder can be obtained as $\\\\hat{Y} = q(\\\\text{dec}(\\\\text{concat}(X, A, Z)))$.\\n\\nThe feature similarity between $v_i$ and $v_j$ is defined as $\\\\text{sim}(v_i, v_j) = \\\\text{sim}(\\\\theta, \\\\phi)$, where $\\\\theta$ is the feature vector of $v_i$, and $\\\\phi$ is the feature vector of $v_j$.\\n\\nThe noise-robust graph learning and generative graph learning methods. For a $\\\\phi$-parameter, we convert $\\\\hat{e}_{ij}$ into $\\\\hat{e}_{ij}$ as:\\n\\n$$L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij}) = ||p(\\\\hat{e}_{ij})| | - ||\\\\hat{e}_{ij}||.$$  \\n\\nThe main idea is to penalize the value of $\\\\hat{e}_{ij}$ when $\\\\hat{e}_{ij}$ is that $p(\\\\hat{e}_{ij})$, which introduce inaccurate supervision. Therefore, we employ regularizations on both the pre-training and training processes.\\n\\nThe probability $p(\\\\hat{e}_{ij})$ is used for sampling between $\\\\hat{v}_{ij}$ and $\\\\hat{v}_{ij}$. We have prior knowledge about $p(\\\\hat{e}_{ij})$, which means that a closed form solution of $p(\\\\hat{e}_{ij})$. With $p(\\\\hat{e}_{ij})$, we can convert $\\\\hat{e}_{ij}$ into $\\\\hat{e}_{ij}$ as:\\n\\n$$L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij}) = ||p(\\\\hat{e}_{ij})| | - ||\\\\hat{e}_{ij}||.$$  \\n\\nIn summary, the overall learning objective can be written as follows and $L_{\\\\text{final}}$.\\n\\n$$L_{\\\\text{final}} = \\\\frac{1}{2} \\\\sum_{e_{ij} \\\\in E} L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij})$$\\n\\nThe details of the datasets are given in Appendix D.1.\\n\\n## Experimental Details\\n\\nWe evaluated two newly introduced datasets (Cora, Citeseer, Photo, and Computers) and baselines on four commonly used benchmark datasets (i.e., Auto and Garden) based on Amazon review data (He & McAuley, 2016; McAuley et al., 2015) to mimic FDGN on e-commerce systems (Refer to Appendix D.2.2 for details).\\n\\n### Datasets\\n\\nWe evaluate five schemes, including a GCN classifier that takes node classification loss $L_{\\\\text{cls-enc}}(\\\\hat{Y})$ on $X$, a GCN classifier that takes node classification loss $L_{\\\\text{cls-dec}}(\\\\hat{Y})$ on $X$, and a GCN classifier that takes node classification loss $L_{\\\\text{cls-dec}}(\\\\hat{Y})$, the decoder needs to rely on the information contained in $Z$, where the decoder can be obtained as $\\\\hat{Y} = q(\\\\text{dec}(\\\\text{concat}(X, A, Z)))$.\\n\\nThe feature similarity between $v_i$ and $v_j$ is defined as $\\\\text{sim}(v_i, v_j) = \\\\text{sim}(\\\\theta, \\\\phi)$, where $\\\\theta$ is the feature vector of $v_i$, and $\\\\phi$ is the feature vector of $v_j$.\\n\\nThe noise-robust graph learning and generative graph learning methods. For a $\\\\phi$-parameter, we convert $\\\\hat{e}_{ij}$ into $\\\\hat{e}_{ij}$ as:\\n\\n$$L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij}) = ||p(\\\\hat{e}_{ij})| | - ||\\\\hat{e}_{ij}||.$$  \\n\\nThe main idea is to penalize the value of $\\\\hat{e}_{ij}$ when $\\\\hat{e}_{ij}$ is that $p(\\\\hat{e}_{ij})$, which introduce inaccurate supervision. Therefore, we employ regularizations on both the pre-training and training processes.\\n\\nThe probability $p(\\\\hat{e}_{ij})$ is used for sampling between $\\\\hat{v}_{ij}$ and $\\\\hat{v}_{ij}$. We have prior knowledge about $p(\\\\hat{e}_{ij})$, which means that a closed form solution of $p(\\\\hat{e}_{ij})$. With $p(\\\\hat{e}_{ij})$, we can convert $\\\\hat{e}_{ij}$ into $\\\\hat{e}_{ij}$ as:\\n\\n$$L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij}) = ||p(\\\\hat{e}_{ij})| | - ||\\\\hat{e}_{ij}||.$$  \\n\\nIn summary, the overall learning objective can be written as follows and $L_{\\\\text{final}}$.\\n\\n$$L_{\\\\text{final}} = \\\\frac{1}{2} \\\\sum_{e_{ij} \\\\in E} L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij})$$\\n\\nThe details of the datasets are given in Appendix D.1.\\n\\n## Experimental Details\\n\\nWe evaluated two newly introduced datasets (Cora, Citeseer, Photo, and Computers) and baselines on four commonly used benchmark datasets (i.e., Auto and Garden) based on Amazon review data (He & McAuley, 2016; McAuley et al., 2015) to mimic FDGN on e-commerce systems (Refer to Appendix D.2.2 for details).\\n\\n### Datasets\\n\\nWe evaluate five schemes, including a GCN classifier that takes node classification loss $L_{\\\\text{cls-enc}}(\\\\hat{Y})$ on $X$, a GCN classifier that takes node classification loss $L_{\\\\text{cls-dec}}(\\\\hat{Y})$ on $X$, and a GCN classifier that takes node classification loss $L_{\\\\text{cls-dec}}(\\\\hat{Y})$, the decoder needs to rely on the information contained in $Z$, where the decoder can be obtained as $\\\\hat{Y} = q(\\\\text{dec}(\\\\text{concat}(X, A, Z)))$.\\n\\nThe feature similarity between $v_i$ and $v_j$ is defined as $\\\\text{sim}(v_i, v_j) = \\\\text{sim}(\\\\theta, \\\\phi)$, where $\\\\theta$ is the feature vector of $v_i$, and $\\\\phi$ is the feature vector of $v_j$.\\n\\nThe noise-robust graph learning and generative graph learning methods. For a $\\\\phi$-parameter, we convert $\\\\hat{e}_{ij}$ into $\\\\hat{e}_{ij}$ as:\\n\\n$$L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij}) = ||p(\\\\hat{e}_{ij})| | - ||\\\\hat{e}_{ij}||.$$  \\n\\nThe main idea is to penalize the value of $\\\\hat{e}_{ij}$ when $\\\\hat{e}_{ij}$ is that $p(\\\\hat{e}_{ij})$, which introduce inaccurate supervision. Therefore, we employ regularizations on both the pre-training and training processes.\\n\\nThe probability $p(\\\\hat{e}_{ij})$ is used for sampling between $\\\\hat{v}_{ij}$ and $\\\\hat{v}_{ij}$. We have prior knowledge about $p(\\\\hat{e}_{ij})$, which means that a closed form solution of $p(\\\\hat{e}_{ij})$. With $p(\\\\hat{e}_{ij})$, we can convert $\\\\hat{e}_{ij}$ into $\\\\hat{e}_{ij}$ as:\\n\\n$$L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij}) = ||p(\\\\hat{e}_{ij})| | - ||\\\\hat{e}_{ij}||.$$  \\n\\nIn summary, the overall learning objective can be written as follows and $L_{\\\\text{final}}$.\\n\\n$$L_{\\\\text{final}} = \\\\frac{1}{2} \\\\sum_{e_{ij} \\\\in E} L_{\\\\text{rec-edge}}(\\\\hat{e}_{ij})$$\\n\\nThe details of the datasets are given in Appendix D.1.\\n\\n## Experimental Details\\n\\nWe evaluated two newly introduced datasets (Cora, Citeseer, Photo, and Computers) and baselines on four commonly used benchmark datasets (i.e., Auto and Garden) based on Amazon review data (He & McAuley, 2016; McAuley et al., 2015) to mimic FDGN on e-commerce systems (Refer to Appendix D.2.2 for details).\\n\\n### Datasets\\n\\nWe evaluate five schemes, including a GCN classifier that takes node classification loss $L_{\\\\text{cls-enc}}(\\\\hat{Y})$ on $X$, a GCN classifier that takes node classification loss $L_{\\\\text{cls-dec}}(\\\\hat{Y})$ on $X$, and a GCN classifier that takes node classification loss $L_{\\\\text{cls-dec}}(\\\\hat{Y})$, the decoder needs to rely on the information contained in $Z$, where the decoder can be"}
{"id": "kNGxg8shA1", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nTable 1: Node classification performance under synthetic feature-dependent graph-noise (FDGN).\\n\\n| Dataset | Setting | WSGNN | AirGNN | ProGNN | RSGNN | STABLE | EvenNet | NRGNN | RTGNN |\\n|---------|---------|-------|--------|--------|-------|--------|---------|-------|-------|\\n| PRINGLE | Clean   | 86.2\u00b10.1 | 85.0\u00b10.2 | 85.3\u00b10.4 | 86.2\u00b10.5 | 86.1\u00b10.2 | 86.2\u00b10.0 | 86.2\u00b10.2 | 86.1\u00b10.2 |\\n| PRINGLE | FDGN-10% | 80.7\u00b10.3 | 79.7\u00b10.5 | 79.6\u00b10.7 | 81.9\u00b10.3 | 82.2\u00b10.7 | 80.7\u00b10.7 | 81.0\u00b10.5 | 82.9\u00b10.6 |\\n| PRINGLE | FDGN-30% | 70.0\u00b10.6 | 71.5\u00b10.8 | 74.5\u00b10.1 | 71.9\u00b10.5 | 74.3\u00b10.3 | 65.2\u00b11.7 | 73.5\u00b10.8 | 72.1\u00b10.6 |\\n| PRINGLE | FDGN-50% | 55.9\u00b11.1 | 56.2\u00b10.8 | 66.4\u00b10.4 | 58.1\u00b10.2 | 62.8\u00b12.4 | 47.1\u00b11.8 | 61.9\u00b11.4 | 60.8\u00b10.4 |\\n| PRINGLE | Clean   | 76.6\u00b10.6 | 71.5\u00b10.2 | 72.6\u00b10.5 | 75.8\u00b10.4 | 74.6\u00b10.6 | 76.4\u00b10.5 | 75.0\u00b11.3 | 76.1\u00b10.4 |\\n| PRINGLE | FDGN-10% | 72.8\u00b10.8 | 66.2\u00b10.7 | 67.5\u00b10.6 | 73.3\u00b10.5 | 71.5\u00b10.3 | 71.1\u00b10.4 | 71.9\u00b10.3 | 73.2\u00b10.2 |\\n| PRINGLE | FDGN-30% | 63.3\u00b10.7 | 58.0\u00b10.4 | 61.0\u00b10.2 | 63.9\u00b10.5 | 62.5\u00b11.4 | 61.2\u00b10.6 | 62.5\u00b10.7 | 63.5\u00b12.1 |\\n| PRINGLE | FDGN-50% | 53.4\u00b10.6 | 50.0\u00b10.6 | 53.3\u00b10.2 | 55.3\u00b10.4 | 54.7\u00b11.7 | 47.2\u00b11.1 | 52.6\u00b10.9 | 54.2\u00b11.8 |\\n| PRINGLE | Clean   | 92.9\u00b10.3 | 93.5\u00b10.1 | 90.1\u00b10.2 | 93.6\u00b10.8 | 93.4\u00b10.1 | 94.5\u00b10.4 | 90.3\u00b11.7 | 91.3\u00b10.6 |\\n| PRINGLE | FDGN-10% | 83.9\u00b11.8 | 87.3\u00b10.9 | 84.3\u00b10.1 | 89.4\u00b12.4 | 92.2\u00b10.1 | 92.6\u00b10.0 | 84.3\u00b11.3 | 88.9\u00b10.3 |\\n| PRINGLE | FDGN-30% | 51.9\u00b16.8 | 67.8\u00b14.3 | 74.7\u00b10.2 | 82.1\u00b11.1 | 88.0\u00b11.0 | 69.0\u00b12.2 | 86.4\u00b10.5 | 90.5\u00b10.4 |\\n| PRINGLE | FDGN-50% | 31.9\u00b15.6 | 57.8\u00b10.7 | 48.9\u00b10.5 | 75.6\u00b12.6 | 80.2\u00b11.8 | 57.5\u00b11.8 | 79.2\u00b10.3 | 87.6\u00b10.2 |\\n\\nThorough evaluation, we create synthetic and real-world FDGN settings, with details in Appendix D.2. We also account for independent structure/feature/label noise that are also prevalent in real-world applications, following Li et al. (2022a); Liu et al. (2021); Qian et al. (2023). Due to the space limit, we provide additional analyses on the model robustness under independent structure/feature/label noise in Appendix E.1. Further details about the baselines, evaluation protocol, and implementation details can be found in Appendix D.3, D.4, and D.5, respectively.\\n\\n5.1 Quantitative Results\\n\\n5.1.1 Under Synthetic Feature-Dependent Graph-noise\\n\\nWe first evaluate PRINGLE under synthetic FDGN settings. Appendix D.2.1 provides a description of the synthetic data generation algorithm. Table 1 shows that PRINGLE consistently outperforms all baselines in FDGN scenarios, especially when noise levels are high. This superiority is attributed to the fact that PRINGLE captures the causal relationships involved in the DGP of FDGN, while the baselines overlook such relationships, leading to their model designs assuming the completeness of at least one data source. Additionally, PRINGLE performs well even in clean graph settings. We attribute this to the accurate inference of $\\\\epsilon_A$, which is utilized as the label regularization in calculating $L_{rec-edge}$. Specifically, in Fig 12(a) in Appendix E.4, we observe that the $\\\\hat{p}_{el}$ values estimated from the clean graph tend to be close to 1, while those from the graph with FDGN are considerably smaller. Recall that the high value of $\\\\hat{p}_{el}$ indicates the model regards the edge $(i, j)$ as a clean edge. This suggests that PRINGLE has the capability to adapt its model learning to the level of noise present in the input graph, resulting in superior performance on both clean and noisy graphs.\\n\\nTable 2: Node classification performance under real-world FDGN.\\n\\n| Methods | Clean + FDGN | Clean + FDGN |\\n|---------|--------------|--------------|\\n| WSGNN   | 71.8\u00b14.3     | 57.7\u00b11.3     |\\n| AirGNN  | 69.5\u00b10.8     | 53.9\u00b10.1     |\\n| ProGNN  | 63.2\u00b10.2     | 48.6\u00b10.3     |\\n| RSGNN   | 69.5\u00b10.4     | 56.8\u00b10.9     |\\n| STABLE  | 71.6\u00b10.9     | 57.5\u00b10.2     |\\n| EvenNet | 73.4\u00b10.5     | 57.1\u00b12.1     |\\n| NRGNN   | 74.3\u00b10.8     | 55.8\u00b11.0     |\\n| RTGNN   | 75.1\u00b10.3     | 59.6\u00b10.8     |\\n| PRINGLE | 79.3\u00b10.2     | 61.4\u00b10.4     |\\n\\nTable 3: Link prediction performance under real-world FDGN.\\n\\n| Methods | Clean + FDGN | Clean + FDGN |\\n|---------|--------------|--------------|\\n| WSGNN   | 81.8\u00b10.1     | 69.1\u00b10.6     |\\n| AirGNN  | 60.2\u00b10.2     | 57.9\u00b10.4     |\\n| ProGNN  | 74.8\u00b10.3     | 56.7\u00b10.5     |\\n| RSGNN   | 87.2\u00b10.8     | 65.0\u00b10.2     |\\n| STABLE  | 78.6\u00b10.1     | 57.3\u00b10.1     |\\n| EvenNet | 86.8\u00b10.1     | 70.5\u00b10.2     |\\n| NRGNN   | 76.6\u00b11.3     | 47.5\u00b11.7     |\\n| RTGNN   | 84.4\u00b10.1     | 72.2\u00b10.2     |\\n| PRINGLE | 88.2\u00b10.3     | 73.6\u00b10.6     |\"}"}
{"id": "kNGxg8shA1", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Ablation studies regarding various DGPs in Fig 4.\\n\\nIn Case 3, the causal relationship $Y \\\\leftarrow (X, A)$ is removed from the DGP of FDGN (See Fig 2(b)). In Case 2, $A \\\\leftarrow X$ is additionally removed. In Case 1, $A \\\\leftarrow \\\\epsilon$ is additionally removed, which is equivalent to the DGP of CGN (See Fig 2(a)). Cora and Citeseer datasets are used to evaluate the node classification performance.\\n\\n| Dataset | Setting | PRINGLE | Clean | FDGN-10% | FDGN-30% | FDGN-50% |\\n|---------|---------|---------|-------|----------|----------|----------|\\n| Cora    | (a) Case 1 | 84.6\u00b10.4 | 84.8\u00b10.4 | 86.2\u00b10.2 | 86.2\u00b10.7 |          |\\n|         | (b) Case 2 | 77.4\u00b10.3 | 77.3\u00b10.3 | 83.2\u00b10.3 | 82.9\u00b10.6 |          |\\n|         | (c) Case 3 | 68.3\u00b10.4 | 68.5\u00b10.2 | 77.3\u00b10.4 | 78.2\u00b10.3 |          |\\n| Citeseer| (a) Case 1 | 76.7\u00b10.9 | 76.8\u00b10.8 | 76.5\u00b10.9 | 77.3\u00b10.6 |          |\\n|         | (b) Case 2 | 69.5\u00b10.3 | 69.5\u00b10.4 | 73.2\u00b10.1 | 74.3\u00b10.9 |          |\\n|         | (c) Case 3 | 57.2\u00b11.1 | 57.7\u00b10.5 | 65.5\u00b10.7 | 65.6\u00b10.6 |          |\\n\\n5.2 ABLATION STUDIES\\n\\nTo emphasize the importance of directly capturing the causal relationships among variables in the DGP of FDGN, i.e., $Y \\\\leftarrow (X, A)$, $A \\\\leftarrow X$, and $A \\\\leftarrow \\\\epsilon$, we remove them one by one from the graphical model of FDGN (See Fig 2(b), and then design deep generative models based on the DGPs in a similar manner to PRINGLE. The graphical models of the derived DGPs are illustrated in Fig 4.\\n\\nIn Table 4, we observe that as more causal relationships are removed from the DGP of FDGN, the node classification performance decreases. Below, we offer explanations for this observation from the perspective of model derivation.\\n\\n1) By removing $Y \\\\leftarrow (X, A)$, i.e., Fig 4(c), the loss term $-E_{Z \\\\sim q_3}[\\\\log(p_{\\\\theta_3}(Y | X, A, Z))]$ can be simplified to $-E_{Z \\\\sim q_3}[\\\\log(p_{\\\\theta_3}(Y | Z))]$. This simplification hinders the accurate modeling of the label transition relationship from $Z$ to the noisy label $Y$, resulting in a degradation of model performance under FDGN.\\n\\n2) Additionally, when eliminating $A \\\\leftarrow X$ (i.e., Fig 4(b)), the inference of $Z$ and $Z_Y$ is simplified as follows: $q_{\\\\phi_1}(Z_A | X, A)$ to $q_{\\\\phi_1}(Z_A | A)$ and $q_{\\\\phi_3}(Z_Y | X, A)$ to $q_{\\\\phi_3}(Z_Y | X)$. Furthermore, the loss term $-E_{Z \\\\sim q_1}E_{\\\\epsilon \\\\sim q_2}[\\\\log(p_{\\\\theta_1}(A | \\\\epsilon, Z_A))]$ is also simplified to $-E_{Z \\\\sim q_1}E_{\\\\epsilon \\\\sim q_2}[\\\\log(p_{\\\\theta_1}(A | Z_A))]$. These simplifications significantly hinder the accurate inference of $Z$ and $Z_Y$, resulting in a notable performance degradation.\\n\\n3) Furthermore, by eliminating $A \\\\leftarrow \\\\epsilon$, i.e., Fig 4(a), the loss term $-E_{Z \\\\sim q_1}E_{\\\\epsilon \\\\sim q_2}[\\\\log(p_{\\\\theta_1}(A | \\\\epsilon, Z_A))]$ is simplified to $-E_{Z \\\\sim q_1}E_{\\\\epsilon \\\\sim q_2}[\\\\log(p_{\\\\theta_1}(A | Z_A))]$. This simplification hinders the robustness of the inferred $Z$, since the simplified loss excludes label regularization from the model training process, ultimately resulting in performance degradation.\\n\\n6 CONCLUSION AND FUTURE WORK\\n\\nIn this paper, we discover practical limitations of conventional graph noise in terms of node features, i.e., the noise in node features is independent of the graph structure or node label. To mitigate limitations of the conventional graph noise assumption, we introduce a more realistic graph noise scenario called feature-dependent graph-noise (FDGN), and present a deep generative model that effectively captures the causal relationships among variables in the DGP of FDGN. Our proposed method, PRINGLE, consistently outperforms baselines in both node classification and link prediction tasks. We evaluate PRINGLE on commonly used benchmark datasets and newly introduced real-world graph datasets that simulate FDGN in e-commerce systems, which is expected to foster practical research in noise-robust graph learning. For future work, the practicality of FDGN can be further enhanced by additionally considering the causal relationship $X \\\\leftarrow A$, which indicates that the graph structure noise inevitably entails the node feature noise, that may indeed manifest in some real-world scenarios. Since it can cover a broader range of noise scenarios that occur in real-world applications than FDGN, we expect directly modeling it has the potential to enhance practical applicability. We provide a detailed discussion on this topic in Appendix F.\"}"}
{"id": "kNGxg8shA1", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Inference of $\\\\mathcal{A}(\\\\mathcal{Q}_1) \\\\mathcal{Z} = \\\\text{GCN}_{\\\\mathcal{Q}_1}(\\\\mathcal{X}, \\\\mathcal{A})$. $\\\\mathcal{P}_{ij} = \\\\rho(s(\\\\mathcal{Z}\\\\%, \\\\mathcal{Z}\\\\&))$\\n\\n$\\\\mathcal{A} = \\\\begin{bmatrix} \\\\mathcal{P}_{11} & \\\\cdots & \\\\mathcal{P}_{1n} \\\\\\\\ \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\ \\\\mathcal{P}_{n1} & \\\\cdots & \\\\mathcal{P}_{nn} \\\\end{bmatrix}$\\n\\nFigure 5: Overall architecture of PRINGLE.\"}"}
{"id": "kNGxg8shA1", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The last equation of Eq. 8 can be more simplified as follows:\\n\\nIn a similar way, the last term can be also simplified:\\n\\nand\\n\\n\\\\[\\np \\\\log E = \\\\log E \\\\geq E E = \\\\log E = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log E = (\\\\theta \\\\epsilon, Z) E X, A, Y, \\\\epsilon, Z = \\\\log E = \\\\log"}
{"id": "kNGxg8shA1", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where we abuse the notation $q_{\\\\phi_1}(Z_A|X,A)$, $q_{\\\\phi_2}(\\\\epsilon|X,A,Z,Y)$, and $q_{\\\\phi_3}(Z_Y|X,A)$ respectively. We combine Eqn. 9, 10, 11, and 12 to get the negative ELBO, i.e.,\\n\\n$$L_{\\\\text{ELBO}} = -E_{Z_A \\\\sim q_{\\\\phi_1}(Z_A|X,A)} E_{\\\\epsilon \\\\sim q_{\\\\phi_2}(\\\\epsilon|X,A,Z,Y)} \\\\left[ \\\\log(p_\\\\theta_1(A|X,\\\\epsilon,Z_A)) \\\\right] + kl(q_{\\\\phi_1}(Z_A|X,A)||p(Z_A)) - E_{\\\\epsilon \\\\sim q_{\\\\phi_2}(\\\\epsilon|X,A,Z,Y)} E_{Z_Y \\\\sim q_{\\\\phi_3}(Z_Y|X,A)} \\\\left[ \\\\log(p_\\\\theta_2(X|\\\\epsilon,Z_Y)) \\\\right] + E_{Z_Y \\\\sim q_{\\\\phi_3}(Z_Y|X,A)} [kl(q_{\\\\phi_2}(\\\\epsilon|X,A,Z,Y)||p(\\\\epsilon))] - E_{Z_Y \\\\sim q_{\\\\phi_3}(Z_Y|X,A)} \\\\left[ \\\\log(p_\\\\theta_3(Y|X,A,Z,Y)) \\\\right] + kl(q_{\\\\phi_3}(Z_Y|X,A)||p(Z_Y)) (13)$$\\n\\n**DETAILS OF MODEL INSTANTIATIONS**\\n\\nLoss term $kl(q_{\\\\phi_1}(Z_A|X,A)||p(Z_A))$. To minimize this term, we encourage $Z_A$ to align with our prior knowledge, i.e., $p(Z_A)$, that the latent graph structure predominantly consists of assortative edges that have the potential to enhance feature propagation within GNNs (Zhao et al., 2023). In recent studies (Choi et al., 2022; Dai et al., 2022), the $\\\\gamma$-hop subgraph similarity has served as a potent metric for identifying assortative edges. Hence, we model $q_{\\\\phi_1}(Z_A|X,A)$ to minimize the KL divergence between the latent graph structure $\\\\hat{A}$ and prior graph structure $A_p = \\\\{a_{ij}\\\\}_{N \\\\times N}$, where $a_{ij}$ is sampled from a Bernoulli distribution with a probability as given by the $\\\\gamma$-hop subgraph similarity. This leads the estimated probability $\\\\hat{p}_{ij}$ between two nodes to increase if they exhibit a high subgraph similarity. However, computing $\\\\hat{p}_{ij}$ in every epoch is impractical for large graphs, i.e., $O(N^2)$. To mitigate the issue, we pre-define a candidate graph that consists of the observed edge set $E$ and a $k$-NN graph based on the $\\\\gamma$-hop subgraph similarity. We denote the set of edges in the $k$-NN graphs as $E_\\\\gamma k$. Then, we compute the $\\\\hat{p}_{ij}$ values of the edges in a candidate graph, i.e., $E_\\\\gamma k \\\\cup E$, instead of all edges in $\\\\{(i, j) | i \\\\in V, j \\\\in V\\\\}$, to estimate the latent graph structure denoted as $\\\\hat{A}$. It is important to highlight that obtaining $E_\\\\gamma k$ is carried out offline before model training, thus incurring no additional computational overhead during training. This implementation technique achieves a similar effect as minimizing $kl(q_{\\\\phi_1}(Z_A|X,A)||p(Z_A))$ while significantly addressing computational complexity from $O(N^2)$ to $O(|E_\\\\gamma k \\\\cup E|)$, where $N^2 \\\\gg |E_\\\\gamma k \\\\cup E|$.\\n\\n**DETAILS ON EXPERIMENTAL SETTINGS**\\n\\n**D.1 DATASETS**\\n\\nWe evaluate PRINGLE and baselines on four existing datasets (i.e., Cora (Yang et al., 2016), Citeseer (Yang et al., 2016), Amazon Photo, and Amazon Computers (Shchur et al., 2018)) and two newly introduced datasets (i.e., Amazon Auto and Amazon Garden) that are proposed in this work based on Amazon review data (He & McAuley, 2016; McAuley et al., 2015) to mimic FDGN caused by malicious fraudsters on e-commerce systems (Refer to Appendix D.2.2 for details). The statistics of the datasets are given in Table 5. These six datasets can be found in these URLs:\\n\\n- Cora: https://github.com/ChandlerBang/Pro-GNN/\\n- Citeseer: https://github.com/ChandlerBang/Pro-GNN/\\n- Photo: https://pytorch-geometric.readthedocs.io/en/latest/\\n- Computers: https://pytorch-geometric.readthedocs.io/en/latest/\\n- Auto: http://jmcauley.ucsd.edu/data/amazon/links.html\\n- Garden: http://jmcauley.ucsd.edu/data/amazon/links.html\\n\\n**D.2 DETAILS OF GENERATING FDGN**\\n\\n**D.2.1 SYNTHETIC FDGN**\\n\\nFor the synthetic FDGN settings, we artificially generate the noise following the data generation process of the proposed FDGN scenario. First, we randomly sample a subset of nodes $V_{\\\\text{noisy}}$ (i.e., 16...\"}"}
{"id": "kNGxg8shA1", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Statistics for datasets.\\n\\n| Dataset   | # Nodes | # Edges | # Features | # Classes |\\n|-----------|---------|---------|------------|-----------|\\n| Cora      | 2,485   | 5,069   | 1,433      | 7         |\\n| Citeseer  | 2,110   | 3,668   | 3,703      | 6         |\\n| Photo     | 7,487   | 119,043 | 745        | 8         |\\n| Computers | 13,381  | 245,778 | 767        | 10        |\\n| Auto      | 8,175   | 13,371  | 300        | 5         |\\n| Garden    | 7,902   | 19,383  | 300        | 5         |\\n\\n10%, 30%, and 50% of the whole node set $V$). To inject node feature noise into the sampled nodes, we randomly flip 0/1 value on each dimension of node features $X_i$ from Bernoulli distribution with probability $p = \\\\frac{1}{F_P i} \\\\frac{1}{X_i}$, which results in the noisy features $X_{noisy}$. After injecting the feature noise, we generate a feature-dependent structure noise (i.e., $A \\\\leftarrow X$) and feature-dependent label noise (i.e., $Y \\\\leftarrow (X, A)$). For the feature-dependent structure noise, we first calculate the similarity vector for each node $v_i$ as $\\\\{s(X_{noisy}i, X_j) | v_i \\\\in V_{noisy}, v_j \\\\in V\\\\}$ where $s(\\\\cdot, \\\\cdot)$ is a cosine similarity function, and select the node pairs whose feature similarity is top-$k$ highest values. We add the selected node pairs to the original edge set $E$, which results in $E_{noisy}$. To address feature-dependent label noise, we replace the labels of labeled nodes (i.e., training and validation nodes) with randomly sampled labels from a Multinomial distribution, with parameters determined by the normalized neighborhood class distribution. Finally, for the independent structure noise (i.e., $A \\\\leftarrow \\\\epsilon$), we add the randomly selected non-connected node pairs to the $E_{noisy}$. Detailed algorithm is provided in Algorithm 2.\\n\\nD.2.2 REAL-WORLD FDGN\\n\\nWe have introduced and released two new graph benchmark datasets, i.e., Auto and Garden, that simulate real-world FDGN scenarios on e-commerce systems. To construct these graphs, we utilized metadata and product review data from two categories, \u201cAutomotives\u201d and \u201cPatio, Lawn and Garden,\u201d obtained from Amazon product review data sources (He & McAuley, 2016; McAuley et al., 2015). Specifically, we generated a clean product-product graph where node features are represented using a bag-of-words technique applied to product reviews. The edges indicate co-purchase relationships between products that have been purchased by the same user, and the node labels correspond to product categories. We perform both node classification and link prediction tasks, which are equivalent to categorizing products and predicting co-purchase relationships, respectively.\\n\\nWe simulate the behaviors of fraudsters on a real-world e-commerce platform that incurs FDGN. When the fraudsters engage with randomly selected products (i.e., when they write fake product reviews), it would make other users purchase irrelevant products, which introduces a substantial number of malicious co-purchase edges within the graph structure. Additionally, this activity involves the injection of noisy random reviews into the node features. To provide a more detailed description, we designated 100 users as fraudsters. Furthermore, each of these users was responsible for generating 10 fraudulent reviews in both the Auto and Garden datasets. To generate fake review content, we randomly choose text from existing reviews and duplicate it for the targeted products. This approach guarantees that the fake reviews closely mimic the writing style and content of genuine reviews, while also incorporating irrelevant information that makes it more difficult to predict the product category.\\n\\nIn e-commerce systems, to annotate the node labels (i.e., product categories), machine learning-based automated labeling systems are commonly utilized. Specifically, human annotators manually label a small set of examples, which is used as the training examples to the machine learning model. Subsequently, a machine learning model is trained on these manually labeled product samples to automatically assign categories to other products. Therefore, the systems rely on the information about the products, e.g., reviews of products and co-purchase relationships, to assign categories to products. However, due to the influence of the fraudsters, the noisy node features (i.e., fake product reviews) and noisy graph structure (i.e., co-purchase relationships between irrelevant products) may hinder the accurate assignment of the automated labeling systems, which leads to the noisy node label. To replicate this procedure, we selected 5 examples per category class, which is equivalent to manual labeling process. We then trained a GCN model, leveraging the node features, graph structure, and manually labeled nodes, to predict the true product categories. Consequently, our set of labeled nodes are composed of both manually labeled nodes and nodes...\"}"}
