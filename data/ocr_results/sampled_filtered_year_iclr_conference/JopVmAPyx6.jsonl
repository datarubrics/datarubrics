{"id": "JopVmAPyx6", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Given \\\\( i \\\\) in-context examples, the probability for we cannot distinguish \\\\( k \\\\) factors is\\n\\n\\\\[\\n|E| - k_i X_j = 0 \\\\quad i \\\\neq j \\\\quad (n^v - 1) i - j n^v \\\\quad k - 1 \\\\quad k \\\\quad (n^v - 1) i - j n^v.\\n\\\\]\\n\\n(12)\\n\\nWhen we cannot distinguish the hidden factor from \\\\( k \\\\) factors, the probability to predict wrong results is \\\\( k - 1 \\\\). Combining the results above, we obtain the error that\\n\\n\\\\[\\n|E| X_2 = 2 |E| X_k = 2\\n\\\\]\\n\\n(13)\\n\\nThe probability to give a right prediction is\\n\\n\\\\[\\ns_i = 1 - X_j = 0 \\\\quad i \\\\neq j \\\\quad |E| X_k = 2 |E| k\\\\quad (n^v - 1) i - j n^v.\\n\\\\]\\n\\n(14)\\n\\nIn the constructed Transformer, we will autogressively combining the results of the previous prediction (Corresponding to Layer 2), we have:\\n\\n\\\\[\\ns_{rs_i} = (1 - s_{rs_i} - 1) s_i + s_{rs_i} - 1.\\n\\\\]\\n\\nwhere \\\\( s_{rs_0} = s_0 \\\\).\\n\\n2) **In-context learning score**\\n\\nThe copy-past mechanism is used to predict the answer of the prompt example (Corresponding to layer 3). For the copy-past mechanism, have a in-context example with same prediction result as the prompt example is neccesary. When we correct predict the hidden factor, the probability to predict correctly is\\n\\n\\\\[\\n1 - (n^v - 1) n^v i.\\n\\\\]\\n\\nWhen we predict a wrong hidden factor, the probability is \\\\( 1 n^v \\\\). Combine the two above, we obtain the accuracy\\n\\n\\\\[\\n(1 - (n^v - 1) n^v) s_{rs_i} + 1 n^v (1 - s_{rs_i})\\n\\\\]\\n\\nBecause when no in-context example is given, the accuracy is \\\\( 1 v \\\\). Therefore we obtain the in-context learning score\\n\\n\\\\[\\ncls_i = (n^v - 1)(n^i - 1) v - (n^v - 1) i - 1 n^v s_{rs_i}.\\n\\\\]\\n\\nE.4 CONTRIBUTION OF THE PROOF\\n\\nComparison with previous work in analyzing the in-context learning mechanism\\n\\nThe key contribution of our analysis is to analyze how the in-context learning learn the \\\"sentence semantic\\\", i.e. hidden factor, from in-context samples. Previous works investigate the mechanism of in-context learning mainly focus on the pair-wise relation between the query tokens and in-context tokens, and they (Olsson et al., 2022) reveals that copy-past is a important mechanism for in-context learning. In this paper, we want to analyze how Transformer learn the \\\"Sentence Semantic\\\". In our construction, we find that model may rely on the comparison of information within \\\"Sentence\\\" (i.e., pattern matching).\\n\\nF E XTRA E XPERIMENTS\\n\\nIn this section, we give some extra experiments results as complementary to the results listed in main text.\\n\\nMore explorations on in-weights component\\n\\nWe conduct \\\\( D \\\\) \\\\( \\\\to \\\\) \\\\( rnd \\\\) \\\\( \\\\Rightarrow \\\\) \\\\( D \\\\) \\\\( rnd \\\\) under different switching point. The results are given in Fig. 10A. We find that even training on \\\\( D \\\\) \\\\( fix \\\\) with small epochs, the model can still benifit a lot.\\n\\n19\"}"}
{"id": "JopVmAPyx6", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10:\\n\\nA: We conduct $D_{\\\\text{fix}} \\\\rightarrow \\\\text{rnd} \\\\Rightarrow D_{\\\\text{rnd}}$ under different switching points. The curve with legend \\\"2\\\" means that we switch from $D_{\\\\text{fix}}$ to $D_{\\\\text{rnd}}$ at epoch 2. Curve with legend \\\"rnd\\\" is the baseline setting, i.e., $D_{\\\\text{rnd}} \\\\Rightarrow D_{\\\\text{rnd}}$. The dash lines mark the corresponding switching points.\\n\\nB: The in-weights and in-context score when we probe at different layers. We choose the $D_{\\\\text{fix}} \\\\wedge \\\\text{rnd} \\\\Rightarrow D_{\\\\text{rnd}}$ settings. The dashlines marked the chosen layers in the experiments.\\n\\nC: Comparison between the LSTM (Hochreiter & Schmidhuber, 1997) and Transformer. Abalation study on the probe layers\\n\\nWe give the ablation study on different probe layers. We choose the setting $D_{\\\\text{rnd}} \\\\wedge \\\\text{fix} \\\\Rightarrow D_{\\\\text{rnd}}$ and the model trained at 50 epoch. The choose of setting is arbitrary. The in-weights and in-context comp. score has similar trend across layers for same model trained on different task settings and different epoch (except extremely close to initialization). The results are displayed in Fig. 10B.\\n\\nCompared between Transformer and LSTM.\\n\\nIn Section 4, our primary focus is on discussing the results of the Transformer. In this section, we aim to compare the in-context learning abilities of the Transformer and LSTM models. The LSTM model consists of 6 layers, while the Transformer has 4 heads, 6 layers. We examine the scenario where $D_{\\\\text{fix}} \\\\rightarrow D_{\\\\text{fix}}$. Unfortunately, we are unable to train the LSTM model for all other cases. During the comparison, we employ a larger hidden size for the LSTM model, as it tends to fail when using a smaller hidden size. The results are displayed in Fig. 10C. We find that 1) LSTM is much harder for obtain in-context learning compared with Transformer. 2) LSTM has potential to obtain better in-context learning ability.\"}"}
{"id": "JopVmAPyx6", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Enhance in-weights and in-context components by combining D\\\\textsuperscript{fix} and D\\\\textsuperscript{rnd}. \\n\\nA: We conduct D\\\\textsuperscript{fix} \\\\rightarrow D\\\\textsuperscript{rnd} \\\\Rightarrow D\\\\textsuperscript{rnd} under different switching point. The curve with legend \\\"2\\\" means that we switch from D\\\\textsuperscript{fix} to D\\\\textsuperscript{rnd} at epoch 2. Curve with legend \\\"rnd\\\" is the baseline setting, i.e., D\\\\textsuperscript{rnd} \\\\Rightarrow D\\\\textsuperscript{rnd}.\\n\\nThe dash lines mark the corresponding switching points.\\n\\nB: Enhancing the in-weights component can facilitate the emergence of in-context learning ability in smaller models. The dashed line represents the task setting D\\\\textsuperscript{fix} \\\\land D\\\\textsuperscript{rnd} \\\\Rightarrow D\\\\textsuperscript{rnd}, while the solid line corresponds to the setting D\\\\textsuperscript{rnd} \\\\Rightarrow D\\\\textsuperscript{rnd}.\\n\\nincrease of in-context comp. score and in-context learning performance. The in-weights comp. score stays below the initialized value.\\n\\n\u2022 A significant improvement of in-weights learning under D\\\\textsuperscript{fix} \\\\Rightarrow D\\\\textsuperscript{rnd} settings.\\n\\nIn Fig 4B, we observe a gradually improvement of in-weights comp. score. However, no obvious improvement of the in-context comp. score and in-context learning score can be observed.\\n\\nWe consider the setting D\\\\textsuperscript{fix} \\\\rightarrow D\\\\textsuperscript{rnd} \\\\Rightarrow D\\\\textsuperscript{rnd}, where model is first trained on D\\\\textsuperscript{fix} to improve the in-weights component, and then we transfer to D\\\\textsuperscript{rnd} to enhance its in-context component. This approach allows us to examine the impact of the in-weights component on in-context components. We make the following observation:\\n\\n\u2022 A better in-weights component can accelerate the learning of the in-context component. In Fig. 4C (D\\\\textsuperscript{fix} \\\\rightarrow D\\\\textsuperscript{rnd} \\\\Rightarrow D\\\\textsuperscript{rnd}), we notice a sudden increase in the in-context comp score when switching from D\\\\textsuperscript{fix} to D\\\\textsuperscript{rnd}. This result indicates that we can learn the in-context component more quickly based on a representation with a better in-weights component.\\n\\nWe then further investigate the combined effect of in-weights and in-context components on in-context learning by simultaneously improving the in-weights and in-context components using the task setting D\\\\textsuperscript{fix} \\\\land D\\\\textsuperscript{rnd} \\\\Rightarrow D\\\\textsuperscript{fix}. We find that:\\n\\n\u2022 Learning in-weights component and in-context component simultaneously is more effective than learning them separately. Compared to training on D\\\\textsuperscript{fix} \\\\Rightarrow D\\\\textsuperscript{rnd}, the model trained on D\\\\textsuperscript{fix} \\\\land D\\\\textsuperscript{rnd} learns much faster (D\\\\textsuperscript{fix} \\\\land D\\\\textsuperscript{rnd} \\\\Rightarrow D\\\\textsuperscript{rnd}). Additionally, D\\\\textsuperscript{fix} \\\\land D\\\\textsuperscript{rnd} can facilitate in-context learning in smaller models (see Fig. ??).\\n\\n4.2 Influence of in-Weights Component under Different Settings\\n\\nKey Points\\n\\nThrough experiments we observe that\\n\\n1) Small epochs trained on D\\\\textsuperscript{fix} under D\\\\textsuperscript{fix} \\\\rightarrow D\\\\textsuperscript{rnd} \\\\Rightarrow D\\\\textsuperscript{rnd} setting can significant improve the in-context learning.\\n\\n2) Enhancing the in-weights component (D\\\\textsuperscript{fix} \\\\land D\\\\textsuperscript{rnd}) can facilitate the emergence of in-context learning ability in smaller models.\\n\\nWe consider the setting D\\\\textsuperscript{fix} \\\\rightarrow D\\\\textsuperscript{rnd} \\\\Rightarrow D\\\\textsuperscript{rnd}, where model is first trained on D\\\\textsuperscript{fix} to improve the in-weights component, and then we transfer to D\\\\textsuperscript{rnd} to enhance its in-context component. This approach allows us to examine the impact of the in-weights component on in-context components. We make the following observation:\\n\\n\u2022 More explorations on in-weights component\\n\\nWe conduct D\\\\textsuperscript{fix} \\\\rightarrow D\\\\textsuperscript{rnd} \\\\Rightarrow D\\\\textsuperscript{rnd} under different switching point. The results are given in Fig. 4A. We find that even training on D\\\\textsuperscript{fix} with small epochs, the model still benifit a lot.\"}"}
{"id": "JopVmAPyx6", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We then further investigate the combined effect of in-weights and in-context components on in-context learning by simultaneously improving the in-weights and in-context components using the task setting $D^{\\\\text{fix}} \\\\land \\\\mathcal{R}$.$\\\\Rightarrow D^{\\\\text{fix}}$. We find that:\\n\\n- Enhancing the in-weights component can facilitate the emergence of in-context learning ability in smaller models.\\n\\nIn Fig. 4, we observe that enhancing the in-weights component by using $D^{\\\\text{rnd}} \\\\land \\\\mathcal{R}$ settings, can facilitate the emergence of in-context learning ability in smaller models. For base models, we observe significant improvements of the in-context learning performance.\\n\\n5. **Further Studies of the Importance of In-Weights Component**\\n\\n5.1 **Theoretic Analysis**\\n\\n**Key points**\\n\\nWe investigate the mechanism of in-context learning by construction. Our main result is that the significance of the in-weights component, as demonstrated in the experiments, is further validated by the fact that a simple Transformer can achieve potent in-context learning results based on the assumption of a perfect in-weights component.\\n\\nWe consider the naive Transformer (Vaswani et al., 2017). The hidden representation of token $i$ in Transformer is denoted as $h_i \\\\in \\\\mathbb{R}^d$. The whole hidden state of the sequence is denoted as $H = [h_1, \\\\ldots, h_L]^T \\\\in \\\\mathbb{R}^{2L \\\\times d}$. The hidden representation of $l$-th layer is denoted as $H(l)$.\\n\\n**Definition 5.1. (Transformer)** One layer of Transformer contains one attention layer and one MLP layer. The calculation of Attention Layer is\\n\\n$$\\\\text{Attn}(l)(H(l)) = H(l) + \\\\sum_{c=1}^{C} \\\\sigma(H(l)W(l,c)Q(H(l)W(l,c)K))H(l)W(l,c)VW(l,c)O.$$ (2)\\n\\nAnd the calculation of MLP layer is\\n\\n$$H(l+1) = \\\\text{Attn}(l)(H(l)) + \\\\text{Relu}(\\\\text{Attn}(l)(H(l)))W(l)1W(l)2.$$ (3)\\n\\nHere we consider relaxed case where $\\\\sigma = \\\\text{Id}$.\\n\\n**Figure 6:** The constructed Transformer can match the performance of best trained GPT2 ($D^{\\\\text{fix}} \\\\land \\\\mathcal{R}$ setting) in experiment part.\\n\\nThe relaxation of Transformer is discussed by many previous works. Press et al. (2019) discover using the Relu in feed forward layer can achieve comparable results in original one. Wiegreffe & Pinter (2019); Brunner et al. (2019); Richter & Wattenhofer (2020) point out that softmax operation may not actually needed for Transformer.\\n\\n**Definition 5.2. (Perfect in-weights component)** If feature $h$ has a perfect in-weights component, then for all factor $e$, exists $W_e \\\\in \\\\mathbb{R}^{d \\\\times |V_e|}$ such that\\n\\n$$f(e)x_1 \\\\cdot f(e)x_2 = 1$$ only when $v(e)x_1 = v(e)x_2$, else we have $f(e)x_1 \\\\cdot f(e)x_2 = 0$, where $f(e)x = W_e h_x$.\\n\\nBased on the perfect in-weights component assumption, we can construct a Transformer with additional three layers to learn the in-context component and achieve comparable performance compared with the best trained GPT2 model in previous experiments.\\n\\n**Proposition 5.3.** We consider the data with $n_e$ factors and each factor has $n_v$ values in $D^{\\\\text{rnd}}$ setting. For causal Transformer with the number of heads larger or equal the number of factors with the hidden size $O(n_e n_v + L)$, if the Transformer can learn a perfect in-weights component in layer $k$, then it can learn a feature given $i$ in-context samples with in-context comp. score $s_{rs}^i = (1 - s_{rs}^i - 1)s_i^e + s_{rs}^i - 1$ and $s_{rs}^0 = s^0$ at layer $k+2$, where $s_i^e = 1 - \\\\frac{\\\\sum_{j=0}^{k-1} |E_{k-1}| - k + 1}{n_v - 1}$, and we can obtain in-context learning score as $c_{ls}^i = \\\\frac{(n_v - 1)(n_i - 1)}{n_v - 1 + (n_v - 1)i - (n_v - 1)n_i}$. (8)\"}"}
{"id": "JopVmAPyx6", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"In-context learning can be readily achieved with a high-quality in-weights component. Fig. ?? illustrates that the performance of the constructed simple Transformer can match the best-tuned GPT2 model under the perfect in-weights component assumption. Given the limited capacity of the three simple Transformer layers, it can be inferred that in-context learning can be easily achieved when supported by a high-quality in-weights component.\\n\\n5.2 B EYOND SYNTHETIC TASKS\\n\\nFigure 7: Experiments on SST-ICL dataset.\\n\\nA: Example of dataset. The example label $y$ is obtained from $v$ by map function $m$, i.e. $y = m(v)$. The in-context examples is denoted as $s_c$ and the prompt example is denoted as $x_p$.\\n\\nB: The Transformer is trained to predict the label of prompt example $y_p$ given $s_c$, $x_p$.\\n\\nC: Effect of in-weights component. We explore the $D_{fix} \\\\rightarrow \\\\text{rnd} \\\\Rightarrow D_{rnd}$ and $D_{rnd} \\\\Rightarrow D_{rnd}$ settings. The dash line denote the time when we transit from $D_{fix}$ to $D_{rnd}$.\\n\\nWe conduct another NLP meta learning task to demonstrate that the relations discovered in our synthetic task about in-weights component, in-context component and in-context learning ability can further extend to practice problem.\\n\\nSST-ICL dataset\\n\\nThe dataset is constructed based on SST (Socher et al., 2013) datasets. We remove the long review in the datasets and transform the original labels into \u201cNegative\u201d, \u201cPositive\u201d and \u201cNeutral\u201d. Then, we organize the reviews follow the same way as that in Subsection A.4. We produce $10^4$ sequence for training and $4 \\\\times 10^3$ for testing. Each sequence contains 5 reviews. We illustrate the example of the dataset in Fig. 9 AB.\\n\\nExperiments results\\n\\nWe follow the same training pipeline as described in Section A. We compare the results of $D_{fix} \\\\rightarrow \\\\text{rnd} \\\\Rightarrow D_{rnd}$ and $D_{rnd} \\\\Rightarrow D_{rnd}$ settings. We find that improve of in-weights component (by training on $D_{fix}$) can also help the in-context learning in this setting.\\n\\n6 C ONCLUSION\\n\\nThis paper investigates the relationship between representation and in-context learning by decomposing representation into in-weights and in-context components. Our experiments demonstrate that the in-context component has a direct connection with in-context learning ability. Further investigation shows that the in-weights component plays a crucial role in the learning of the in-context component. These findings are further examined by constructing a simple Transformer that matches the performance of the best-trained GPT2 model. In summary, this paper unveils the influence of representation on in-context learning within the context of a synthetic dataset.\\n\\nREFERENCES\\n\\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.\\n\\nGuillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644, 2016.\\n\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.\"}"}
{"id": "JopVmAPyx6", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023.\\n\\nAlberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a transformer: A memory viewpoint. arXiv preprint arXiv:2306.00802, 2023.\\n\\nMelissa Bowerman. Semantic factors in the acquisition of rules for word use and sentence construction. In Directions in normal and deficient language development, pp. 99\u2013179. University Park Press, 1976.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\nGino Brunner, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Wattenhofer. On identifiability in transformers. arXiv preprint arXiv:1908.04211, 2019.\\n\\nStephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:18878\u201318891, 2022a.\\n\\nStephanie CY Chan, Ishita Dasgupta, Junkyung Kim, Dharshan Kumaran, Andrew K Lampinen, and Felix Hill. Transformers generalize differently from information stored in context vs in weights. arXiv preprint arXiv:2210.05675, 2022b.\\n\\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\nShivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022.\\n\\nAngeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. arXiv preprint arXiv:2301.13196, 2023.\\n\\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.\\n\\nHyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on Machine Learning, pp. 2649\u20132658. PMLR, 2018.\\n\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nLouis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-context learning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022.\\n\\nKenneth Li, Aspen K Hopkins, David Bau, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring a sequence model trained on a synthetic task. arXiv preprint arXiv:2210.13382, 2022.\\n\\nYingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. International Conference on Machine Learning, 2023.\\n\\nBingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022.\"}"}
{"id": "JopVmAPyx6", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Does Representation Impact In-context Learning: An Exploration on a Synthetic Task\\n\\nAnonymous authors\\n\\nAbstract\\nIn-context learning, i.e., learning from in-context samples, is an impressive ability of Transformer. However, the exact mechanism behind this learning process remains unclear. In this study, we aim to explore this aspect from a relatively less explored perspective, i.e., representation learning. For in-context learning, the representation becomes more complex as it can be influenced by both model weights and context samples. To study how the model weights and in-context samples affect the prediction, we conceptually isolate the component, that can only be influenced by the model's weights, from the model's inner representation. We name this component as in-weights component and the rest as in-context component. We create a novel synthetic experimental setup, which allows to control the difficulty level of learning good in-context component, making it possible to study how the two components interplay with each other and impact the in-context performance. We find that the in-weights component plays a significant role in the learning of the in-context component. However, in traditional training way, the in-weights component may be overlooked, resulting in a poor performance. We propose to a training setup to synergistically learn the in-weight and in-context components and the in-context learning performance can be significant improved. A further theoretical analysis is provided to justify the importance of our findings. Overall, our discoveries from the perspective of representation learning provide valuable insights into new approaches for enhancing in-context capacity.\\n\\n1 Introduction\\nTransformer-based models have exhibited remarkable capabilities in language processing (OpenAI, 2023; Devlin et al., 2018). One of the most striking features is their ability to rapidly learn from contextual examples (Brown et al., 2020), which is referred to as in-context learning. As it does not require changing weights, in-context learning has garnered significant research interest and has been effectively employed to address real-world problems. This development calls for a deeper comprehension of the underlying mechanisms driving in-context learning. Numerous efforts have been dedicated to this crucial subject. For instance, several recent studies (von Oswald et al., 2022; Dai et al., 2023; Aky\u00fcrek et al., 2022) have characterized in-context learning as a form of gradient descent. Other works (Li et al., 2023; Bai et al., 2023) have further interpreted in-context learning as algorithm implementation and selection.\\n\\nIn-context learning is different from regular supervised learning. For regular supervised learning, given the input sample $x_p$ with label $y_p$, we want to find a parameterized function $f_w$ with $w$, such that $y_p = f_w(x_p)$. In this situation, all the information to accomplish the tasks is stored in the weights $w$. However, for the in-context learning framework, there is extra information source, the context samples $s_c$. Then, the prediction can be modeled as $f_{w,s_c}(x_p)$, that means both the weights and the context samples can influence the prediction.\\n\\nWe study the joint effect between the in-weights component and in-context component on the prediction, by assuming that the function $f_{w,s_c}(x)$ can be decomposed as $f_{w,s_c}(x_p) = g_{weights}(x_p), g_{context}(s_c)$. In this way, we can isolate a component $g_{weights}(x_p)$ that only depends on the weights and the other component $g_{context}(s_c)$ that both depend on weights and context samples. We denote $g_{weights}(x_p)$ as in-weights component and $g_{context}(s_c)$ as in-context component.\"}"}
{"id": "JopVmAPyx6", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nDue to the weights sharing within the transformer, weights and context interplay with each other.\\n\\n1.1 MAIN CONCLUSIONS\\n\\nFigure 1: Test results for different training epoch and model size. Improving in-weights component (dashed line) can enable the emergence of in-context learning for small models. Details in Section 4.2\\n\\nWith the aforementioned setup, we investigate the impact of the in-weight component and in-context component on in-context learning capabilities. The experimental results reveal that:\\n\\n(i) In-weights component is easy to be overlooked in regular I.I.D training, leading to a poor in-context performance as the solid line shown in Fig. 6, even though it plays an essential role in the in-context learning. Details is presented in Section 4.\\n\\n(ii) By synergistically enhancing the in-weight and in-context components, the in-context performance of a base model (blue line in Fig. 6) significantly improves (in-context learning score increases from 0.168 to 0.885), and in-context ability emerges for small model (orange line in Fig. 6).\\n\\nA theoretical analysis is provided to further understand the role of in-weights component. We prove by construction that three additional Transformer layers on top of the representation with perfect in-weights component can achieve comparable performance in the experimental part.\\n\\n1.2 CONTRIBUTIONS\\n\\nOur main contributions can be summarized as follows:\\n\\n\u2022 We give a formulation of in-weights and in-context component and introduce a new synthetic task that enables the study of the impact of in-context component and in-weights component on in-context ability.\\n\\n\u2022 The experimental results reveal that although the in-weights component cannot directly influence the in-context learning performance, it plays a significant role in the learning of the in-context component. However, the traditional training method may overlook the in-weights component and result in a worse performance.\\n\\n\u2022 We offer mathematical analysis to understand the importance of in-weights components.\\n\\n2 EXPERIMENTAL DESIGN\\n\\nIn this section, we will discuss the experimental design, which encompasses dataset construction, model and training objectives, and the exploration framework.\\n\\nPrinciples for Experimental Design\\n\\nThe following principles guide our experimental design: 1) The prediction of the prompt example should adapt to the in-context example. 2) The evolution of in-weights and in-context components should be trackable. 3) The learning of in-weights and in-context components must be controllable.\\n\\n2.1 DATASET CONSTRUCTION\\n\\nIn general tasks, the impacts of model weights and context samples are intertwined, which complicates the process of providing separate evaluations. Therefore, we propose a task using the Shapes3D (Kim & Mnih, 2018) dataset for more controllable study. The experimental setting is shown in Fig. 2. Specifically, given a sequence of image and label pairs as context, the task involves predicting the label of the prompt image. Each image contains six different factors: object color, object shape, object scale, background color, floor color, and pose. We denote the factor as $e$ and the factor value of factor $e$ as $v(e)$.\"}"}
{"id": "JopVmAPyx6", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.\\n\\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.\\n\\nOpenAI. Gpt-4 technical report. arXiv, 2023.\\n\\nOfir Press, Noah A Smith, and Omer Levy. Improving transformer models by reordering their sublayers. arXiv preprint arXiv:1911.03864, 2019.\\n\\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n\\nOliver Richter and Roger Wattenhofer. Normalized attention without probability cage. arXiv preprint arXiv:2005.09561, 2020.\\n\\nStefan Schouten, Peter Bloem, and Piek Vossen. Probing the representations of named entities in transformer-based language models. In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pp. 384\u2013393, 2022.\\n\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631\u20131642, 2013.\\n\\nYuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380, 2023.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nElena Vota and Ivan Titov. Information-theoretic probing with minimum description length. arXiv preprint arXiv:2003.12298, 2020.\\n\\nElena Vota, Rico Sennrich, and Ivan Titov. The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives. arXiv preprint arXiv:1909.01380, 2019.\\n\\nJohannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint arXiv:2212.07677, 2022.\\n\\nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023.\\n\\nSarah Wiegreffe and Yuval Pinter. Attention is not not explanation. arXiv preprint arXiv:1908.04626, 2019.\\n\\nWenfeng Zheng, Xiangjun Liu, and Lirong Yin. Sentence representation method based on multi-layer semantic network. Applied sciences, 11(3):1316, 2021.\"}"}
{"id": "JopVmAPyx6", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Moore Details About Experiments\\n\\nA.1 Dataset Detail\\n3dshapes is a dataset of 3D shapes procedurally generated from 6 ground truth independent latent factors. These factors are floor colour, wall colour, object colour, scale, shape and orientation. All possible combinations of these latents are present exactly once, generating \\\\( N = 480000 \\\\) total images.\\n\\nLatent factor values:\\n- floor hue: 10 values linearly spaced in \\\\([0, 1]\\\\)\\n- wall hue: 10 values linearly spaced in \\\\([0, 1]\\\\)\\n- object hue: 10 values linearly spaced in \\\\([0, 1]\\\\)\\n- scale: 8 values linearly spaced in \\\\([0, 1]\\\\)\\n- shape: 4 values in \\\\([0, 1, 2, 3]\\\\)\\n- orientation: 15 values linearly spaced in \\\\([-30, 30]\\\\)\\n\\nWe varied one latent at a time (starting from orientation, then shape, etc), and sequentially stored the images in fixed order in the images array. The corresponding values of the factors are stored in the same order in the labels array.\\n\\nA.2 Model and Training Configure\\n\\nIn the proposed task, our objective is to investigate the properties of in-context learning. We utilize the causal Transformer, in which each token can only attend to prior tokens. Specifically, we implement the Transformer as the GPT2 model, consisting of 12 layers, 4 attention heads, and an embedding size of 128 in default. To simulate the auto-regression framework, we calculate the loss for the sequence \\\\( s = \\\\{(x_1, y_1), \\\\ldots, (x_L, y_L)\\\\} \\\\) as:\\n\\n\\\\[\\nL(\\\\theta, s) = \\\\frac{1}{L} \\\\sum_{i=1}^{L} l(f_w, s(i-1), (x_i, y_i)),\\n\\\\]\\n\\nwhere \\\\(s(j) \\\\equiv \\\\{x_1, y_1, \\\\ldots, x_j, y_j\\\\} \\\\), \\\\(l \\\\) denotes the loss function.\\n\\n\\\\(x\\\\) will be tokenized by \\\\(VAE\\\\) before being passed to Transformer. The training loss in the dataset \\\\( S \\\\), which contains \\\\( n \\\\) sequence, is calculated as the average of loss over all training sequences, i.e.,\\n\\n\\\\[\\nL(\\\\theta, S) = \\\\frac{1}{n} \\\\sum_{s \\\\in S} L(\\\\theta, s).\\n\\\\]\\n\\nIn this study, we employ the Adam optimizer (Kingma & Ba, 2014) and mini-batch training to optimize the loss function \\\\( L(\\\\theta, S) \\\\). Here, we use cross-entropy as the loss function. We utilize a batch size of 128 and set the learning rate to 0.0001. For training purposes, we use \\\\( 10^5 \\\\) sequences and, for evaluation, \\\\( 4 \\\\times 10^4 \\\\) sequences. There is no overlap between images in the training sequences and those in the evaluation sequences.\\n\\nA.3 More Detail About Probe Framework\\n\\nWe employ metrics for numerical evaluation of components and in-context learning performance. Since the components are hidden in the representation, we use the probe method (Alain & Bengio, 2016). The probe classifier has a single linear layer, with softmax and cross-entropy calculating the loss. It is trained until converge. The in-weight probe predicts values of six factors of all images, while the in-context probe identifies the hidden factor for each sequence. The details are as follows.\\n\\nIn-context comp. score (Fig. 8A)\\nGiven the dataset \\\\( S \\\\), the in-context comp. score is calculated as\\n\\n\\\\[\\n\\\\frac{1}{|S|} \\\\sum_{s \\\\in S} 1(\\\\hat{e}_{h,s} = e_{h,s}),\\n\\\\]\\n\\nwhere \\\\(1(\\\\cdot)\\\\) is indicator function, \\\\(s\\\\) is the sequence in the dataset, \\\\(e_{h,s}\\\\) is the hidden factor for the sequence \\\\(s\\\\), and \\\\(\\\\hat{e}_{h,s}\\\\) is the prediction of probe classifier. We use \\\\(|\\\\cdot|\\\\) to denote the corresponding size of a set.\\n\\nIn-weights comp. score (Fig. 8B)\\nTo remove the influence of in-context component, we disable the attention layer in the Transformer. We disable the attention layers by replacing the attention layers in the Transformer with identity maps, whose outputs are equal to their inputs. Then, the in-weights comp. score is calculated as\\n\\n\\\\[\\n\\\\frac{1}{|S|} \\\\sum_{s \\\\in S} |s| \\\\frac{1}{|E|} \\\\sum_{e \\\\in E} 1(\\\\hat{v}_{ex} = v_{ex}),\\n\\\\]\\n\\nwhere \\\\(v_{ex}\\\\) is factor value of factor \\\\(e\\\\) and sample \\\\(x\\\\), \\\\(\\\\hat{v}_{ex}\\\\) is the prediction of probe classifier, \\\\(s = \\\\{(x_1, y_1), \\\\ldots, (x_L, y_L)\\\\}\\\\) is the sequence in the dataset \\\\(S\\\\) and \\\\(E\\\\) is the set of all factors.\\n\\n[https://github.com/deepmind/3d-shapes](https://github.com/deepmind/3d-shapes)\"}"}
{"id": "JopVmAPyx6", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In-context \\nprobe $\\\\hat{V}_{eh}$\\nIn-context \\ncomp. score\\nIn-weight \\nprobe $\\\\hat{v}(e_1)\\n\\\\vdots\\n\\\\hat{v}(e|E|)$\\n$v(e_1)\\n\\\\vdots\\nv(e|E|)$\\nIn-weights ... 2 Sample L\\n\u2026\\n\u2026\\n\u2026\\n\u2026\\nTransformer Layer 1\\n\u2026\\n\u2026\\n\u2026\\n\u2026\\nTransformer Layer N\\n\u2026\\nSample 1 Sample 2 Sample L\\n\u2026\\n\u2026\\n\u2026\\n\\nFigure 8: Illustration of calculation of in-context comp score (A), in-weights comp score (B) and in-context learning score (C).\\n\\nIn-context learning score (Fig. 8C)\\nLet $a_i$ and $a_j$ be the accuracies of the Transformer on the $i$-th and $j$-th samples, respectively. Following Olsson et al. (2022), the in-context learning score is calculated as $a_i - a_j$. According to Olsson et al. (2022), the choice of $i, j$ on some reasonable range won't influence the results. Here we choose $i = 40$, $j = 1$ by default. The reason is that under this setting, we have $a_i = (a_i - a_1) + a_1 = (a_i - a_1) + a_1$. Because our task is unsolvable without in-context example, $a_1$ is a constant. In this case, the in-context learning score will have a same trend as the accuracy at $i$.\\n\\nRationale to leverage hidden factor prediction as in-context probe task.\\nTo solve the task on $D_{rand}$, we need to obtain two informations from in-context, that is the hidden factors and the mapping between factor values and labels. Therefore, we can only choose the probe task for in-context component from these two candidates. The mapping of the factor values and labels can not be used as probe task because 1) the number all possible mapping is much larger than the size of dataset, which means we cannot learn the probe classifier. 2) The information of mapping is not necessary stored in representation. Section 5.1 gives a solution of the model that no information of mapping stored in representation. Hidden factors prediction is suitable for probe task because 1) Hidden factor is the sequence level information that can only be learned from in-context example. 2) It is necessary for solving the tasks and its information is stored in representation (Section 4.1).\\n\\nA.4 DATASET SPLIT\\nIn-context training\\nWe first split all the images in Shape3D into two part: the training image set ($80\\\\%$) and the test image set ($20\\\\%$). Then, we organize all the training images into $S_{fix}$, $S_{rand}$, $S_{fix} \\\\land_{rand}$, corresponding to $D_{fix}$, $D_{rand}$, $D_{fix} \\\\land_{rand}$ settings. Test image set are also organized into $S'_{fix}$, $S'_{rand}$, $S'_{fix} \\\\land_{rand}$. Each of $S_{fix}$, $S_{rand}$, and $S_{fix} \\\\land_{rand}$ contains $10^5$ sequences. Each of $S'_{fix}$, $S'_{rand}$, $S'_{fix} \\\\land_{rand}$ contains $4 \\\\times 10^4$ sequences.\\n\\nProbe model training\\nIf we want to probe a model $f_{w,s}(\\\\cdot)$ on setting $D_{rand}$ (test setting), we will first train the probe model on $S_{rand}$ with $f_{w,s}(\\\\cdot)$ and we evaluate the probe model on $S'_{rand}$ with $f_{w,s}(\\\\cdot)$. The same for $D_{fix}$ and $D_{fix} \\\\land_{rand}$ settings.\\n\\nB OTHER RELATED WORK\\nWe discuss the most related work in the main part of paper. Here, we list other works that are weaker related to us.\\n\\nAnalysis of Transformer\\nThe analysis of Transformers can be broken down into two main components: examining the expressibility of Transformers and comprehending the mechanisms of learned Transformers. To analyze the expressibility of Transformers, a common approach is to determine $13$\"}"}
{"id": "JopVmAPyx6", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nif they can solve specific problems by constructing appropriate weights. Giannou et al. (2023) demonstrates that Transformers can function as Turing machines, while Liu et al. (2022) shows that they can learn shortcuts to solve automata problems. In addition to expressibility, researchers have also investigated the mechanisms behind learned Transformers. Bietti et al. (2023) examines Transformers from a memory standpoint, and Tian et al. (2023) focuses on single-layer Transformers.\\n\\nWhile the analysis of Transformers is crucial to our work, our ultimate goal differs; we aim to bridge the gap between representation learning and in-context learning.\\n\\nExploration of representation within Transformer.\\n\\nOwing to the widespread use of Transformers, numerous studies (Li et al., 2022; Voita & Titov, 2020) seek to investigate their internal representations as a means of comprehending their functionality. The most prevalent approach involves utilizing probe models and tasks to discern the information stored within these representations (Voita & Titov, 2020; Schouten et al., 2022). Taking a different perspective, Voita et al. (2019) explores the flow of information across Transformer layers and how this process is influenced by the selection of learning objectives. Our work shares similarities with these studies in that we employ the probe method to examine representations. However, our focus differs in that we do not concentrate on the semantic meaning within the representation. Instead, we investigate how the in-weights and in-context information impact representation.\\n\\n### C.1 Related Evidence of Practice Work Regarding In-Weights and In-Context Component\\n\\nIn this section, we provide evidence about the in-context and in-weights components in practice tasks.\\n\\n**Intuition 1: Influence of words replacing**\\n\\nA key difference between the in-weights and in-context components lies in the susceptibility of the in-weights component to word substitution. The in-weights component can be easily disrupted if a word is replaced with a token that was not present during the training phase, as the weights lack information about this new token. On the other hand, if the context samples are rich in information, the meaning of this new token can still be deduced. This mirrors the human ability to infer the meaning of an unknown word based on its context. If word substitution leads to a decline in performance, it suggests that the Transformer's prediction relies heavily on the in-weights component.\\n\\n**Intuition 2: Influence of number of in-context examples**\\n\\nThe efficiency of the in-context component is expected to rise with the inclusion of more context-specific examples, a characteristic not shared by the in-weights components, which remain unaffected by the addition of in-context examples. Therefore, if performance improves with the integration of more context-specific examples, it would suggest that the Transformer's prediction is heavily influenced by the in-context component.\\n\\n**Intuition 3: Zero-shot performance**\\n\\nThe zero-shot performance can directly indicate the effectiveness of the in-weights component. This is because no in-context examples are provided in this scenario, reducing the problem to a traditional supervised one. Based on the intuitions above, we collect the related experiments in practice paper.\\n\\n1. Min et al. (2022) discovered that (1) performance can be improved by increasing the number of in-context examples. (2) Changing the labels of in-context examples does not influence the predicted label. The first discovery indicates that the prediction relies on the in-context components. The second discovery suggests that the Transformer uses the in-weights component for label prediction, given that there is no observed change when the labels of in-context examples are altered.\\n\\n2. Brown et al. (2020) found that larger models are increasingly effective at utilizing in-context information. This suggests that in real-world scenarios, the efficiency of the in-context component improves with the enlargement of the model's size. Brown et al. (2020) also found that enhancing the model size can boost its zero-shot capabilities. These findings suggest that scaling the model...\"}"}
{"id": "JopVmAPyx6", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2 Problem Definition\\n\\nNotations\\n\\nWe denote $x_p$ as the prompt example with ground truth label $y_p$ and the context examples are $s_c = \\\\{x_1, y_1, \\\\ldots, x_l, y_l\\\\}$. We denote the factor values of prompt as $v_p$ and the corresponding factor value for factor $e$ as $v_(e)p$. The hidden factor is denoted as $e_h$. We denote the mapping function as $m$, which maps the factor value to the corresponding label, i.e. $y_p = m(v_(e_h)p)$. We denote the probability as $P$.\\n\\nIn-weights and in-context\\n\\nFor regular supervised learning, given the input sample $x_p$, we want to find a parameterized function $f_w$ such that $y_p = f_w(x_p)$. In this situation, all the information to finish the tasks is stored in the weights $w$. However, for the in-context learning framework, there is an extra information source, the context samples $s_c$. Then, the predictions become $y_p = f_w, s_c(x_p)$, that means both the weights and the context samples can influence the prediction. For a representation $h$ in the function $f_w, s_c(x_p)$, we denote the component of $h$ that can only be influenced by weights as in-weights component and the component that can be influenced by context examples as in-context component. The representation in $f_w(x_p)$ can be regarded as having only in-weights component. In the following, we decomposite the distribution $P(y_p | x_p, s_c)$ into the parts according whether they are depended on in-context examples.\\n\\nProposition 2.1. The probability of $P(y_p | x_p, s_c)$ can be decomposite as:\\n\\n$$P(y_p | x_p, s_c) = \\\\sum_{v_p, m, e_h} P(y_p | v_p, m, e_h) \\\\mid \\\\{z\\\\}$$\\n\\nProperties of Task\\n\\n$P(v_p | x_p) \\\\mid \\\\{z\\\\}$ In-weights\\n\\n$P(e_h | s_c, m) \\\\mid \\\\{z\\\\}$ In-context\\n\\n(1)\\n\\nwhere $P(v_p | x_p)$ is in-weights related information, and $P(e_h | s_c, m) \\\\mid \\\\{z\\\\}$ is in-context related information. $P(y_p | v_p, m, e_h)$ is related for the properties of task, and we have $P(y_p | v_p, m, e_h) = 1$ if $m(v_e_h p) = y_p$ else $P(y_p | v_p, m, e_h) = 0$. \\n\\n3\"}"}
{"id": "JopVmAPyx6", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Illustration of task design. C, W, and P represent the in-context component, in-weights component, and in-context performance of the Transformer, respectively. A: All possible relations. B: the relationships we aim to investigate in each experimental setup. The red line signifies the relationship being explored, and the black line indicates the relationships that have not been removed.\\n\\nIdeally, if we can approximate the distribution \\\\( P(y_p | v_p, m, e_h) \\\\) and the distribution \\\\( P(e_h | s_c, m) \\\\) \\\\( P(m | s_c) \\\\), we can obtain the distribution \\\\( P(y_p | x_p, s_c) \\\\). Based on the decomposed results of \\\\( P(y_p | x_p, s_c) \\\\), we assume the representation \\\\( h \\\\) can also be decomposed.\\n\\nAssumption 2.2. (Decomposible) We assume that there exist functions \\\\( g, g \\\\) weights, \\\\( g \\\\) context, such that for any \\\\( (x_p, s_c) \\\\sim P(x_p, s_c) \\\\), \\\\( f_{w,s_c}(x_p) \\\\) can be decomposed as \\\\( f_{w,s_c}(x_p) = g(g \\\\) weights \\\\( (x_p), g \\\\) context \\\\( (s_c)) \\\\).\\n\\nRemark 2.3.\\n1) The function \\\\( g(g \\\\) weights \\\\( (x_p), g \\\\) context \\\\( (s_c)) \\\\) is not the actually we calculate \\\\( f(w, s_c) \\\\). It is a conceptually tool for us to decouple the influence of in-context samples' influence.\\n2) \\\\( g \\\\) weights and \\\\( g \\\\) context are only different in their inputs, which means that the \\\\( g \\\\) weights and \\\\( g \\\\) context may dependent with each other. For example, in the Transformer \\\\( g \\\\) context may has the form \\\\( g \\\\) context \\\\( = g' \\\\) context \\\\( \\\\{g \\\\) weights \\\\( (x_1), y_1, \\\\ldots, g \\\\) weights \\\\( (x_l), y_l\\\\} \\\\). This makes the in-weights and in-context components have very complex relation, which we aim to explore in this paper.\\n\\nWith the Assumption 2.2, we denote \\\\( g \\\\) weights \\\\( (x_p) \\\\) as in-weights component and \\\\( g \\\\) context \\\\( (s_c) \\\\) as in-context component. Then, we define our expectation for the components to be good, that is to infer the corresponding part in Proposition 2.1.\\n\\nDefinition 2.4. If \\\\( f_{w,s_c}(x_p) \\\\) has good in-weights component in its representation, we can infer \\\\( P(v_p | x_p) \\\\) from \\\\( g \\\\) weights \\\\( (x_p) \\\\), and if it has good in-context component, we can infer \\\\( P(e_h | s_c, m) \\\\) from \\\\( g \\\\) context \\\\( (s_c) \\\\).\\n\\n2.3 Exploration framework\\n\\nTo study whether we can have infer the can infer \\\\( P(v_p | x_p) \\\\) from \\\\( h \\\\), and infer \\\\( P(e_h | s_c, m) \\\\) from \\\\( h \\\\), we leverage the probe method. and we defined the corresponding scores in the following\\n\\nProbing methods and metrics\\n\\nWe use three metrics here.\\n\\n- **in-weights comp. score**: accuracy of the probe model to predict \\\\( v_p \\\\) given \\\\( x_p \\\\), and \\\"comp.\\\" is short for component.\\n- **in-context comp. score**: the accuracy of the probe model to predict \\\\( e_h \\\\).\\n- **in-context learning score**: the gap between the accuracy of Transformer to predict \\\\( y_p \\\\) give \\\\( l_1 \\\\) in-context examples and the accuracy of Transformer to predict \\\\( y_p \\\\) give \\\\( l_2 \\\\) in-context samples. Here, we choose \\\\( l_1 = 40 \\\\) and \\\\( l_2 = 0 \\\\). The choose of \\\\( l_1, l_2 \\\\) doesn't have obvious influence (Olsson et al., 2022).\\n\\nWe give the detail of the probe framework design, probe model and training configure in the Appendix.\\n\\nTo investigate the relationship between the effectiveness of learning in-weight/context components and the strength of in-context learning ability, we control the difficulty of learning representation components by devising two label assignment settings during the training phase:\\n\\n- **D fix**: The mapping \\\\( m \\\\) remains constant throughout all sequences.\\n- **D rnd**: The mapping \\\\( m \\\\) is consistent within a sequence, but it is randomly chosen for different sequences.\\n\\nThe model is anticipated to learn the in-weight component more effectively in the \\\\( D_{\\\\text{fix}} \\\\) setting and the in-context component more effectively in the \\\\( D_{\\\\text{rnd}} \\\\) setting. To further analyze the interplay between these components, we consider two composite settings:\"}"}
{"id": "JopVmAPyx6", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 $D^{\\\\text{fix}} \\\\rightarrow \\\\text{rnd}$: In this setting, we initially train the model on $D^{\\\\text{fix}}$ for a specific epoch, and then, we train the model on $D^{\\\\text{rnd}}$.\\n\\n\u2022 $D^{\\\\text{fix}} \\\\land \\\\text{rnd}$: Half of the data in the training set utilizes the $D^{\\\\text{fix}}$ setting, while the other half employs the $D^{\\\\text{rnd}}$ setting.\\n\\nGiven two data settings $D_1$ and $D_2$, we use $D_1 \\\\Rightarrow D_2$ to represent the evaluation result of a model on data setting $D_2$ after the model has been trained on data setting $D_1$.\\n\\n**Analysis For $D^{\\\\text{fix}}$**: Recall that we have $P(y|\\\\pi, s_c) \\\\sim P(v|\\\\pi) P(e|h, s_c, m) P(m|s_c)$. Under $D^{\\\\text{fix}}$ setting, we only has one mapping function, which we denote as $m_0$. Therefore, we model can easily learn $P_f(P(m_0|s_c)) = 1$ and then the model only need to learn $P(e|h, s_c, m)$ in its in-context component, which is expected to learn easier than $D^{\\\\text{rnd}}$. And as a result, the model is expected to focus on learning $P(v|\\\\pi)$. As a result, we are expected the model to have a better in-weights component in this setting.\\n\\n**Knowledge transfering between $D^{\\\\text{fix}}$ and $D^{\\\\text{rnd}}$ settings**: The knowledge of $P(v|\\\\pi)$ is shared between these two tasks. The only different between these two tasks is $P(e|h, s_c, m) P(m|s_c)$.\\n\\n**Task settings and explored relations (Fig. 3)**: Based on the analysis, Fig. 3 demonstrates the connections between the relations we wish to investigate (i.e., the relations between in-weight components, in-context components, and in-context learning) and the training-test data settings. The possible dependence between the in-weights and in-context components are caused by the possible dependent between $g^{\\\\text{weights}}(\\\\cdot)$ and $g^{\\\\text{context}}(\\\\cdot)$ as stated Assumption 2.2. For testing in-context ability during inference, we prefer the $D^{\\\\text{rnd}}$ setting, referring to studies by Wei et al. (2023); Min et al. (2022), which indicate that the label-shuffled case can more effectively discern in-context ability.\\n\\n### Table 1: Comparison with other papers that explore in-context learning using synthetic dataset.\\n\\n|                | Garg et al. (2022) | Chan et al. (2022a) | Ours |\\n|----------------|-------------------|--------------------|------|\\n| Synthetic task | Simple functions  | Image data         | Image data |\\n| Sentence      | No                | No                 | Yes  |\\n| Perspective   | Algorithm implementing | Data properties | Representation |\\n| In-weights learning | No            | Trade off with in-context learning | Complex relations with in-context learning |\\n\\nThere are two kinds of synthetic tasks are common used in the exploration of in-context learning:\\n\\n\u2022 (ST1, simple functions) In this task, a simple function is sampled for each sentence (an input sequence for the Transformer). Then, $x_i$ is generated by sampling from a specific distribution, and $y_i$ is produced using the sampled simple function with $x_i$ as input.\\n\\n\u2022 (ST2, image sequence) In this task, $x_i$ is a randomly sampled image from the image dataset, and $y_i$ is generated using the original label values.\\n\\n(ST1) is investigated in the works by Garg et al. (2022); von Oswald et al. (2022); Aky \u00a8urek et al. (2022). (ST2) is examined in the studies by Chan et al. (2022a); Kirsch et al. (2022); Chan et al. (2022b). (ST1) researches in-context learning at a more abstract level, leading to the conclusion that in-context learning implements algorithms, such as gradient descent (von Oswald et al., 2022). However, their tasks are significantly distant from real applications because 1) The input token $x$ consists of numbers without any evident pattern or semantics, while most tokens in NLP tasks are words with clear meanings. 2) The tangible forms of their results is hard to be found in practice. The resolution of real NLP tasks is difficult to be expressed as straightforward, comprehensible algorithms, such as gradient descent or ridge regression. In contrast, (ST2) is closer to real applications since the image data used has semantic meaning. Thus, it is feasible to investigate how data properties influence in-context learning (Chan et al., 2022a). Our synthetic task belongs to the (ST2) category.\\n\\nThe primary distinction between our synthetic task and the previous tasks in (ST2) is that \\\"sentence semantics\\\" are considered in our approach. In the following, we provide a detailed comparison between our work and that of Chan et al. (2022a).\"}"}
{"id": "JopVmAPyx6", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Performance on $D_{\\\\text{rnd}}$ with different training settings. A: $D_{\\\\text{rnd}} \\\\Rightarrow D_{\\\\text{rnd}}$ only improve the in-context component. B: $D_{\\\\text{fix}} \\\\Rightarrow D_{\\\\text{rnd}}$ only improve the in-weights component. CD: An improved in-weights component can speed up the learning process of the in-context component. The green dashed line marks the point when switch from $D_{\\\\text{fix}}$ to $D_{\\\\text{rnd}}$.\\n\\nComparison with Chan et al. (2022a) Regarding dataset setting: The primary difference in our task setting compared to that of Chan et al. (2022a) lies in the consideration of \u201csentence semantics.\u201d Specifically, if we remove hidden factors of the sequence, our constructed synthetic data would degenerate to that of Chan et al. (2022a). The importance of considering \u201cSentence Semantic\u201d lies in the following: 1) Understanding sentence semantics plays a crucial role in practical applications, as evidenced by various studies (Zheng et al., 2021; Bowerman, 1976; Reimers & Gurevych, 2019). 2) Without sentence semantics, the function of in-context learning would degenerate into a simple copy-paste mechanism, wherein the Transformer can predict the label of a query image by searching for context images with the same label and then copying the label of the in-context image.\\n\\nRegarding the division of in-weights/in-context: At a high level, the meanings of \u201cin-context\u201d and \u201cin-weights\u201d are consistent across our work and that of Chan et al. (2022a). However, our paper advances further by: 1) Analyzing the complex relationship between in-weights and in-context components from a representation perspective, leading to a more realistic conclusion. Chan et al. (2022a) concludes that in-context learning and in-weights learning are in a tradeoff relationship in their exploration, but large language models can exhibit both capacities Brown et al. (2020), which is acknowledged by Chan et al. (2022a) in the discussion at the end of their paper. 2) While Chan et al. (2022a) devises two tasks to evaluate in-context and in-weights learning, our work leverages a task that requires both in-context and in-weights information to solve. This setting is more closely aligned with practical applications, as real tasks often require both types of information (Brown et al., 2020; Alayrac et al., 2022).\\n\\n4 Results\\n\\nIn this section, we present our experimental results. In Section 4.1, we examine the in-weights component, in-context component and in-context learning performance score under different settings. In Section 4.2, we give further analysis of the influence of in-weights component under different settings. We give the detail experiments setup, including model structure, optimization configure, training object in the Appendix.\\n\\n4.1 Separate Influence of In-context and In-weights Components\\n\\nKey Points\\n\\nThrough experiments we observe that\\n\\n1) regular training $D_{\\\\text{rnd}} \\\\Rightarrow D_{\\\\text{rnd}}$ will overlook the learning of in-weights component\\n2) $D_{\\\\text{fix}} \\\\Rightarrow D_{\\\\text{rnd}}$ only improve the in-context component, and\\n3) Improving both the in-weights component and the in-context component simultaneously is more effective.\\n\\nTo verify the effectiveness of task design, we first conduct experiments to explore the evolution of representation learning when applying data settings $D_{\\\\text{fix}}$ and $D_{\\\\text{rnd}}$. We find that:\\n\\n\u2022 The regular training set up $D_{\\\\text{rnd}} \\\\Rightarrow D_{\\\\text{rnd}}$ (the test and training data are from same distribution) cannot improve the in-weights component. In Fig 4A, we observe the...\"}"}
{"id": "JopVmAPyx6", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"can enhance both the in-weights and in-context components, and the model employs these two components to address the problem.\\n\\nWei et al. (2023) carried out research on a two-class classification issue. They conducted experiments in which they altered a certain percentage of labels in the context examples to ascertain if the model's prediction would also change. If a change was observed, it would imply that the prediction relies on the in-context components. If no change was noticed, the prediction would be considered to depend on the in-weights component. Their results were intermediate, suggesting that both in-weights and in-context components contribute. Additionally, they found that enhancing the model size increases the impact of in-context examples.\\n\\nBeyond Synthetic Tasks\\n\\nFigure 9: Experiments on SST-ICL dataset.\\n\\nA: Example of dataset. The example label $y$ is obtained from $v$ by map function $m$, i.e., $y = m(v)$. The in-context examples is denoted as $s_c$ and the prompt example is denoted as $x_p$.\\n\\nB: The Transformer is trained to predict the label of prompt example $y_p$ given $s_c$, $x_p$.\\n\\nC: Effect of in-weights component. We explore the $D_{fix} \\\\rightarrow D_{rnd} \\\\Rightarrow D_{rnd}$ and $D_{rnd} \\\\Rightarrow D_{rnd}$ settings. The dash line denote the time when we transit from $D_{fix}$ to $D_{rnd}$.\\n\\nWe conduct another NLP meta learning task to demonstrate that the relations discovered in our synthetic task about in-weights component, in-context component and in-context learning ability can further extend to practice problem.\\n\\nSST-ICL dataset\\n\\nThe dataset is constructed based on SST (Socher et al., 2013) datasets. We remove the long review in the datasets and transform the original labels into \\\"Negative\\\", \\\"Positive\\\" and \\\"Neutral\\\". Then, we organize the reviews follow the same way as that in Subsection A.4. We produce $10^4$ sequence for training and $4 \\\\times 10^3$ for testing. Each sequence contains 5 reviews. We illustrate the example of the dataset in Fig. 9 AB.\\n\\nExperiments results\\n\\nWe follow the same training pipeline as described in Section A. We compare the results of $D_{fix} \\\\rightarrow D_{rnd} \\\\Rightarrow D_{rnd}$ and $D_{rnd} \\\\Rightarrow D_{rnd}$ settings. We find that improve of in-weights component (by training on $D_{fix}$) can also help the in-context learning in this setting.\\n\\nProposition D.1.\\n\\nGiven $y_p$, probability of $P(y_p | x_p, s_c)$ can be decomposed as:\\n\\n$$P(y_p | x_p, s_c) = \\\\sum_{v_p, m, e_h} P(y_p | v_p, m, e_h) P(v_p | x_p) P(e_h | s_c, m) P(m | s_c).$$\\n\\n(6)\\n\\nProof.\\n\\n$$P(y_p | x_p, s_c) = \\\\sum_{v_p, m, e_h} P(y_p, v_p, m, e_h | x_p, s_c) = \\\\sum_{v_p, m, e_h} P(y_p | v_p, m, e_h) P(v_p | x_p, s_c, m, e_h) P(m, e_h | x_p, s_c).$$\\n\\n(7)\"}"}
{"id": "JopVmAPyx6", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nwhere the first equation is due to the law of total probability, the third equation is leveraged\\n\\nformular\\n\\n\\\\[ P(y|\\\\mathbf{p}, \\\\mathbf{s}, \\\\mathbf{c}, \\\\mathbf{v}, \\\\mathbf{m}, \\\\mathbf{e}) = P(y|\\\\mathbf{v}, \\\\mathbf{m}, \\\\mathbf{e}) \\\\]\\n\\nProof of Proposition 5.3\\n\\nNotation\\n\\nThe position embedding is denoted as \\\\( \\\\mathbf{p}_i = (0, \\\\ldots, 0, 1, 0, \\\\ldots) \\\\), where we only have value 1 at \\\\( i \\\\)-th position and 0 others. The weights for the attention operation of \\\\( l \\\\)-th layer and \\\\( c \\\\)-th head in Transformer is denoted as \\\\( \\\\mathbf{W}_{l,c}(\\\\mathbf{Q}), \\\\mathbf{W}_{l,c}(\\\\mathbf{K}) \\\\) and \\\\( \\\\mathbf{W}_{l,c}(\\\\mathbf{V}) \\\\). The weights of forward layer in Transformer are denoted as \\\\( \\\\mathbf{W}_{l,1}, \\\\mathbf{W}_{l,2} \\\\). We use \\\\( \\\\mathbf{E} \\\\) to denote the all of all possible values of the factor \\\\( e \\\\). we denote \\\\( y_i \\\\) as the one hot version of \\\\( y \\\\). The vector with all zero values are denoted as \\\\( 0 \\\\equiv (0, \\\\ldots, 0) \\\\).\\n\\nWe rewrite the proposition here.\\n\\nProposition E.1. We consider the data with \\\\( n_e \\\\) factors and each factor has \\\\( n_v \\\\) values in \\\\( \\\\mathcal{D}_{\\\\text{rnd}} \\\\) setting. For causal Transformer with the number of heads larger or equal the number of factors with the hidden size \\\\( O(n_e n_v + L) \\\\), if the Transformer can learn a perfect in-weights component in layer \\\\( k \\\\), then it can learn a feature given \\\\( i \\\\) in-context samples with in-context component score as\\n\\n\\\\[ \\\\mathbf{s}_{ri} = (1 - \\\\mathbf{s}_{ri} - 1) \\\\mathbf{s}_i + \\\\mathbf{s}_{ri} - 1 \\\\]\\n\\nand \\\\( \\\\mathbf{s}_{r0} = \\\\mathbf{s}_0 \\\\) at layer \\\\( k + 2 \\\\), where\\n\\n\\\\[ \\\\mathbf{s}_i = 1 - \\\\sum_{j=0}^{k-1} \\\\mathbf{P}(\\\\mathbf{E})_{|k| - 1} | \\\\mathbf{E} | - k \\\\]\\n\\n\\\\[ (v - 1) i - j v \\\\]\\n\\n\\\\[ | \\\\mathbf{E} | - k \\\\], and we can obtain in-context learning score as\\n\\n\\\\[ \\\\mathbf{cls}_i = \\\\left( n_v - 1 \\\\right) \\\\left( n_i - 1 \\\\right) - (n_v - 1) i - 1 \\\\]\\n\\n\\\\[ \\\\sum_{j=0}^{k-1} \\\\mathbf{P}(\\\\mathbf{E})_{|k| - 1} | \\\\mathbf{E} | - k \\\\]\\n\\n\\\\[ \\\\mathbf{s}_{ri} \\\\] at \\\\( k + 3 \\\\) layers.\\n\\nProof of Useful Lemma\\n\\nLemma E.2. One attention head can implement copy and past behavior.\\n\\nProof. According to the definition of \\\\( \\\\mathbf{p}_i \\\\), we have\\n\\n\\\\[ \\\\mathbf{p}_i \\\\cdot \\\\mathbf{p}_j = 0 \\\\]\\n\\nif \\\\( i \\\\neq j \\\\), otherwise, we have\\n\\n\\\\[ \\\\mathbf{p}_i \\\\cdot \\\\mathbf{p}_j = 1 \\\\]\\n\\nWe denote\\n\\n\\\\[ \\\\mathbf{M} = \\\\begin{bmatrix} 0 & 0 & \\\\cdots & 0 & 1 \\\\\\\\ 1 & 0 & \\\\cdots & 0 & 0 \\\\\\\\ 0 & 1 & \\\\cdots & 0 & 0 \\\\\\\\ \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots & \\\\vdots \\\\\\\\ 0 & 0 & \\\\cdots & 1 & 0 \\\\end{bmatrix} \\\\]\\n\\nThen we have\\n\\n\\\\[ \\\\mathbf{p}_i \\\\mathbf{M} = \\\\mathbf{p}_i - 1 \\\\]\\n\\nFor \\\\( j > i \\\\), we denote the value of \\\\( 2j \\\\)-th token as \\\\( h_{2j} = (0, 0, 0, 0, p_j) \\\\) and \\\\( 2i \\\\)-th token as \\\\( h_{2i} = (h'_i, 0, 0, 0, 0, p_i) \\\\). If we want to copy the value of \\\\( 2i \\\\)-th token to the value of \\\\( 2j \\\\)-th token, we can set the query matrix as \\\\( \\\\mathbf{W}_Q = (0, 0, 0, 0, \\\\mathbf{M}_{j-i}) \\\\), the key matrix as \\\\( \\\\mathbf{W}_K = (0, 0, 0, 0, \\\\mathbf{I}) \\\\) and value matrix as \\\\( \\\\mathbf{W}_V = (\\\\mathbf{W}'_V, 0, 0, 0, 0) \\\\). Then we have\\n\\n\\\\[ h_{2j} \\\\mathbf{W}_Q \\\\cdot h_{2i} \\\\mathbf{W}_K = \\\\mathbf{P}(\\\\mathbf{E})_{|k| - 1} | \\\\mathbf{E} | - k \\\\]\\n\\nTherefore, the \\\\( j \\\\)-th token can only attend to \\\\( i \\\\)-th token. Then we have the value of \\\\( h_j \\\\) after attention as\\n\\n\\\\[ h_{\\\\text{attn}}_j = ((h'_i) \\\\mathbf{W}_V, 0, 0, 0, p_j) \\\\]\\n\\nBy setting \\\\( \\\\mathbf{W}_V \\\\) as different value, we can copy different part information of \\\\( i \\\\)-th to \\\\( j \\\\)-th token. Then the lemma is held.\\n\\nLemma E.3. For the input \\\\( h = (h_1^T, h_2^T, h_3^T)^T \\\\), where \\\\( h_i \\\\in \\\\mathbb{R}^{d_i} \\\\) and \\\\( d_1 + d_2 + d_3 = d \\\\), for all MLPs \\\\( (h) = \\\\mathbf{W}_2 \\\\text{Relu}(\\\\mathbf{W}_1 h_2) : \\\\mathbb{R}^{d_2} \\\\to \\\\mathbb{R}^{d_2} \\\\), there exists MLPs \\\\( (h) = \\\\mathbf{W}_2 \\\\text{Relu}(\\\\mathbf{W}_1 h_2) : \\\\mathbb{R}^{d} \\\\to \\\\mathbb{R}^{d} \\\\), such that\\n\\n\\\\[ \\\\text{MLP}(h) = (h_1, \\\\text{MLP}(h_2), h_3) \\\\]\\n\\nProof. Obviously, for any \\\\( \\\\mathbf{W}'_1 \\\\), there exists \\\\( \\\\mathbf{W}_1 \\\\), such that\\n\\n\\\\[ h(1) \\\\equiv h \\\\mathbf{W}_1 = (h_1^T, -h_1^T, (\\\\mathbf{W}'_1 h_2)^T, h_3^T, -h_3^T) \\\\]\\n\\nObviously, for any \\\\( \\\\mathbf{W}'_2 \\\\), There exists \\\\( \\\\mathbf{W}_2 \\\\), such that\\n\\n\\\\[ h(2) = \\\\mathbf{W}_2 \\\\text{Relu}(h(1)) = ((\\\\text{Relu}(h_1) + \\\\text{Relu}(-h_1))^T, (\\\\mathbf{W}'_2 \\\\text{Relu}(\\\\mathbf{W}'_1 h_2))^T, (\\\\text{Relu}(h_3) + \\\\text{Relu}(-h_3))^T) = (h_1^T, \\\\text{MLP}(h_2), h_3^T) \\\\]\"}"}
{"id": "JopVmAPyx6", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Layer 1\\n\\nWe denote base $2$ and $y$.\\n\\nStep 1: we use each attention head to obtain the matching information of each factor.\\n\\nThen, we consider the token at position $y_i$. According to Lemma E.3, there exists $u_i$ and $\\\\{v_i \\\\}$ such that $\\\\forall i$. Because all the corresponding terms of $h_i y$ only influence the factor value of factor $f_i$.\\n\\nRecall that $h_i y$ only influence $f_i$, where $\\\\{v_i \\\\}$ is a perfect token representation, then there exists $u_i$ such that $\\\\forall i$. because all the corresponding terms of $h_i y$ only influence $f_i$.\\n\\nFor embedding of $e_i$ has the same factor value of factor $f_i$ and only if all the context samples that have same factor value of factor $f_i$ as sample $u_i$, where $\\\\{v_i \\\\}$. According to Lemma E.3, there exists $u_i$ and $\\\\{v_i \\\\}$ such that $\\\\forall i$. because all the corresponding terms of $h_i y$ only influence $f_i$.\\n\\nProof Sketch\\n\\nThe constuction of weights can be divided into two steps:\\n\\n1. Estimate the factor $e_i$ and $u_i$.\\n\\n2. Estimate $h_i y$. Note that base $2$ token feature into the space in this sequence. Under review as a conference paper at ICLR 2024.\"}"}
{"id": "JopVmAPyx6", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**In-context comp. score**\\n\\nWe denote $m$.\\n\\nAnd similar, we have $h$.\\n\\nThis layer obtain the logit of new sample by comparing the similarity between the unblocked feature $v$ in-context examples, and $h$.\\n\\nFinally, we output Logit using the prediction head.\\n\\n**Performance Analysis**\\n\\nE.3 Probability for a in-context example having same value of a factor as prompt sample is $n$.\\n\\nThe probability for same factor value between in-context examples and prompt sample is $p$.\\n\\nNote that value $w$ blocking the information according to $y$.\\n\\nIn MLP, we calculate $W$.\\n\\nIn this layer, for $y$ factors to decide which one is the hidden factor if the $k$ factors satisfying that of $i$, we have $s$.\\n\\nUnder review as a conference paper at ICLR 2024.\"}"}
