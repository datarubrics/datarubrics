{"id": "LtuRgL03pI", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 11: Visualizations for unconditional 3D scenes stylization by ATISS (Paschalidou et al., 2021), DiffuScene (Tang et al., 2023) and our method.\"}"}
{"id": "LtuRgL03pI", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 12: Generating the semantic feature of one object without instructions\\n\\n\\\\[ \\\\phi(f_i|f_i, c, t, s, r) \\\\]\"}"}
{"id": "LtuRgL03pI", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13: Generating semantic features of all objects in a scene except one without instructions\\n\\n$\\\\mathbf{p} \\\\mathbf{\\\\phi} (\\\\mathbf{f}/i | \\\\mathbf{f}_i, \\\\mathbf{c}, \\\\mathbf{t}, \\\\mathbf{s}, \\\\mathbf{r})$. \"}"}
{"id": "LtuRgL03pI", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Put a dressing table with a mirror to the left of a double bed.\\n(b) Add a lounge chair in front of a multi-seat sofa.\\n\\nFigure 14: Examples of a diverse set of scenes generated from a single prompt.\\n\\n(a) Semantic Graph 1.\\n(b) Semantic Graph 2.\\n\\nFigure 15: Examples of a diverse set of scenes generated from the same semantic graph.\"}"}
{"id": "LtuRgL03pI", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Place a dining chair closely in front of a dining table. Put a dining table below a pendant lamp. Place a burgundy dining chair to the close right of a dining table. Position a brown dining table with metal legs closely behind a grey fabric dining chair. Additionally, arrange a triangle coffee table to the left of a black multi-seat sofa with pillows. Set up a dining table closely right of a wooden dining chair. Place a dining table in front of a silver wine cabinet with a black door. Then, arrange a metal pendant lamp with a hanging rod to the left of a multi-seat sofa. Arrange a brass pendant lamp with glass balls closely left of a black leather dining chair. (a) Instructions (b) ATISS (c) DiffuScene (d) Ours Figure 7: Visualizations for instruction-drive synthesized 3D dining rooms by ATISS (Paschalidou et al., 2021), DiffuScene (Tang et al., 2023) and our method.\"}"}
{"id": "LtuRgL03pI", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: Visualizations for instruction-drive 3D scenes stylization by ATISS (Paschalidou et al., 2021), DiffuScene (Tang et al., 2023) and our method.\"}"}
{"id": "LtuRgL03pI", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Place a blue wardrobe left of a blue double bed.\\n\\nPut a black desk with drawers in front of a double bed.\\n\\nSet up a grey multi-seat sofa with pillows behind a black pendant lamp with six pots. Next, arrange a grey dining chair to the right of a multi-seat sofa.\\n\\nPosition a patterned dining chair right of a multi-seat sofa with pillows. Additionally, put a black TV stand in front of a multi-seat sofa with pillows.\\n\\nPut a green multi-seat sofa to the left of a wooden coffee table with carved legs. And put an armchair behind a wooden coffee table with carved legs.\\n\\nArrange a multi-seat sofa left of a dining table with a wooden top. Then, arrange a modern pendant lamp with a metal ball in front of a brown and grey TV stand.\\n\\nArrange a white dining table to the right of a black console table.\\n\\nArrange a dark brown dining table with a wooden top in front of a pendant lamp with two shades. Put a black armchair with a frame behind a brown loveseat sofa.\"}"}
{"id": "LtuRgL03pI", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Add a wooden wardrobe with drawers behind a brown and black double bed. Additionally, install a pendant lamp in front of a wooden wardrobe with drawers.\\n\\nPosition a black and white circular pendant lamp behind a black and silver nightstand with a drawer.\\n\\nPlace a wardrobe behind a wooden dining chair.\\n\\nSet up a TV stand in front of a brown loveseat sofa with a wooden frame.\\n\\nPosition a blue armchair left of a multi-seat sofa. Then, Put a silver coffee table left of a multi-seat sofa.\\n\\nPut a wooden dining table below a pendant lamp.\\n\\nPut a black multi-seat sofa with pillows to the right of a black plastic dining table.\\n\\nSet up a black pendant lamp with glass shades above a black dining table with a black top.\"}"}
{"id": "LtuRgL03pI", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We conduct two additional experiments to further validate our method: (1) masking the semantic feature of one object and utilizing a pretrained semantic graph prior for recovery:\\n\\n\\\\[ p(\\\\phi(f_i|f/i, c, t, s, r)) \\\\];\\n\\n(2) masking semantic features of all objects except one and again using the pretrained semantic graph prior for recovery:\\n\\n\\\\[ p(\\\\phi(f/i|f_i, c, t, s, r)) \\\\].\\n\\n\\\\( f/i \\\\) means semantic features of all objects except the \\\\( i \\\\)-th one. Instructions for both experiments are set to none. Visualization results are presented in Figure 12 and 13 respectively.\\n\\nThese results indicate the diversity of our method and highlight that semantic graph prior could effectively capture stylistic information and object co-occurrences from the training data. Our method trends to generate style consistent and thematic harmonious scenes, e.g., chairs and nightstands in a suit, and matched color palettes and cohesive artistic style.\\n\\nWe provide examples of a diverse set of scenes generated from a single prompt and the same semantic graph in Figure 14 and 15 respectively, showcasing the diversity of our generative method.\\n\\nWe observed a significant decline in the appearance controllability and style consistency of generated scenes when semantic features were omitted. We include these degraded visualization results in Figure 16 and 17.\\n\\nIt arises from the fact that, without semantic features, the generative models solely focus on modeling the distributions of layout attributes, i.e., categories, translations, rotations, and scales. This exclusion of semantic features results in generated objects whose occurrences and combinations lack awareness of object style and appearance, which are crucial elements in scene design.\\n\\nIn the default settings (\\\\( T = 100 + 10 \\\\)), our method takes about 12 seconds to generate a batch of 128 living rooms by our method on a single A40 GPU. In comparison, ATISS (Paschalidou et al., 2021) takes 3 seconds, and DiffuScene (Tang et al., 2023) requires 22 seconds.\\n\\nIt's noteworthy that our method can be significantly accelerated by reducing the number of diffusion time steps. For instance, setting \\\\( T = 20 + 5 \\\\) reduces the runtime to 3 seconds without a noticeable decline in performance. The impact of diffusion time steps is investigated in Sec. 5.5.1. We believe with more advanced diffusion techniques, diffusion models can be more effective and efficient than autoregressive models, especially for complex scenes.\\n\\nAlthough our method significantly enhances the controllability and fidelity of 3D indoor scene synthesis, it still has some limitations. First, despite our best efforts to ensure the accuracy of the proposed instruct-scene pair dataset, 3D-FRONT contains problematic object arrangements and misclassifications even after filtering, as discussed in previous works (Paschalidou et al., 2021; Tang et al., 2023). Our learned prior may consequently inherit these erroneous cases. Meanwhile, the scale of the current 3D scene dataset remains small, with only hundreds of scenes, in contrast to 3D object datasets containing thousands or even millions of samples (Chang et al., 2015b; Deitke et al., 2023). A promising avenue for future research is to expand the scale of the 3D scene dataset or leverage large-scale and well-annotated datasets for 3D objects to establish a new benchmark for 3D scene synthesis. In this work, we only focus on indoor scene synthesis. However, the proposed semantic graph prior, which encapsulates high-level object interactions within a scene, also offers the potential for modeling more intricate outdoor scenes. Furthermore, achieving a fully generative synthesis pipeline is feasible by substituting the object retrieval step with 3D object generative models conditioned on categories and semantic features provided by our graph prior. Lastly, in light of the rapid development of large language models (LLMs), the integration of an LLM into our instruction-driven pipeline holds significant promise for further enhancing generation controllability.\"}"}
{"id": "LtuRgL03pI", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"While the curated instructions in our proposed dataset are derived from predefined rules, we believe that our model exhibits generalizability to a broader range of instructions. For example, in the stylization task, we utilize instructions in different sentence patterns with training, such as \u201cLet the room be wooden style\u201d and \u201cMake objects in the room black\u201d, as illustrated in Figure 8. We also experiment with instructions containing vague location words, like \u201cPut a chair next to a double bed\u201d, wherein our method generates corresponding objects in all possible spatial relations (e.g., \u201cleft\u201d, \u201cright\u201d, \u201cfront\u201d, and \u201cbehind\u201d).\\n\\nNevertheless, INSTRUCT still faces limitations in comprehending complex text instructions and abstract concepts that do not occur in the curated instructions. For instance, (1) handling instructions with more required triplets, like 4 or 5, poses a challenge. (2) Additionally, identifying the same object within one instruction, such as \u201cPut a table left to a sofa. Then add a chair to the table mentioned before\u201d is also a difficult task. (3) Furthermore, it struggles with abstract concepts such as artistic style, occupants, and functionalities that do not occur in the curated instructions. These limitations are attributed to the CLIP text encoder, which is contrastively trained with image features and tends to capture global semantic information in sentences. Given the rapid development of large language models, we believe the integration of LLMs into the proposed pipeline is a promising research topic.\\n\\nA viable approach to improve the quality of current instructions involves employing LLMs to refine entire sentences in the proposed dataset or using crowdsourcing to make the dataset curation pipeline semi-supervised. We hope the proposed dataset and creation pipeline could serve as a good starting point for creating high-quality instruction datasets.\"}"}
{"id": "LtuRgL03pI", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Add a corner side table with a round top to the left of a black and silver pendant lamp with lights.\\n\\nAdd a grey stool with black legs behind a wooden bookshelf with shelves. Set up a bookshelf left of a black and white double bed.\\n\\nAdd a black and grey double bed in front of a black wardrobe with hanging clothes. Add a black pendant lamp with a handle in front of a wardrobe.\\n\\nAdd a double bed left of a gray nightstand with drawers. Place a gray nightstand with drawers to the right of a gray wardrobe with shelves and drawers.\\n\\nAdd a wooden wardrobe with drawers behind a brown and black double bed. Additionally, install a pendant lamp in front of a wooden wardrobe with drawers.\\n\\nArrange a green and gray double bed below a pendant lamp.\"}"}
{"id": "LtuRgL03pI", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Place a dining table to the right of a coffee table. Put a black dining chair with a black frame right of a coffee table.\\n\\nSet up a floral multi-seat sofa left of a coffee table.\\n\\nPut a pendant lamp with white shades above a wooden coffee table with carved legs.\\n\\nSet up a brown dining table with metal legs left of a lounge chair. Then, set up a red pendant lamp with a handle behind a black bookshelf with shelves and a drawer.\\n\\nAdd a blue dining table with a wooden top behind a blue plaid upholstered dining chair. Place a metal corner side table to the left of a blue multi-seat sofa.\\n\\nPut a black and white marble coffee table right of a multi-seat sofa with pillows.\\n\\nFigure 6: Visualizations for instruction-drive synthesized 3D living rooms by ATISS (Paschalidou et al., 2021), DiffuScene (Tang et al., 2023) and our method.\"}"}
{"id": "LtuRgL03pI", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"INSTRUCTSCENE: INSTRUCTION-DRIVEN 3D INDOOR SCENE SYNTHESIS WITH SEMANTIC GRAPH PRIOR\\n\\nChenguo Lin, Yadong Mu\\nPeking University\\nchenguolin@stu.pku.edu.cn, myd@pku.edu.cn\\n\\nABSTRACT\\nComprehending natural language instructions is a charming property for 3D indoor scene synthesis systems. Existing methods directly model object joint distributions and express object relations implicitly within a scene, thereby hindering the controllability of generation. We introduce INSTRUCTSCENE, a novel generative framework that integrates a semantic graph prior and a layout decoder to improve controllability and fidelity for 3D scene synthesis. The proposed semantic graph prior jointly learns scene appearances and layout distributions, exhibiting versatility across various downstream tasks in a zero-shot manner. To facilitate the benchmarking for text-driven 3D scene synthesis, we curate a high-quality dataset of scene-instruction pairs with large language and multimodal models. Extensive experimental results reveal that the proposed method surpasses existing state-of-the-art approaches by a large margin. Thorough ablation studies confirm the efficacy of crucial design components. Project page: https://chenguolin.github.io/projects/InstructScene.\\n\\nINTRODUCTION\\nAutomatically synthesizing controllable and realistic 3D indoor scenes has been a persistent challenge for computer vision and graphics (Merrell et al., 2011; Fisher et al., 2015; Qi et al., 2018; Wang et al., 2018; Ritchie et al., 2019; Zhang et al., 2020; Yang et al., 2021b;a; H\u00f6llein et al., 2023; Song et al., 2023; Cohen-Bar et al., 2023; Lin et al., 2023; Feng et al., 2023; Patil et al., 2023).\\n\\nAn ideal indoor scene synthesis system should fulfill at least three objectives: (1) comprehending instructions in natural languages, thus providing an intuitive and user-friendly interface; (2) designing object compositions that exhibit aesthetic appeal and thematic harmony; (3) placing objects in appropriate positions and orientations adhering to their functions and regular arrangements.\\n\\nNatural instructions for interior design often rely on abstract object relationships, posing significant challenges for recent advancements in 3D scene synthesis (Wang et al., 2021; Paschalidou et al., 2021; Liu et al., 2023a; Tang et al., 2023) due to the implicit modeling of relationships through individual object attributes. Other studies (Luo et al., 2020; Dhamo et al., 2021; Zhai et al., 2023) utilize relation graphs to provide explicit control over object interactions, which are however too complicated and fussy for human users to specify. Moreover, previous works primarily represent objects by only categories (Luo et al., 2020; Paschalidou et al., 2021) or low-dimensional features (Wang et al., 2019; Tang et al., 2023) which lack visual appearance details, resulting in style inconsistency and constraining customization options in scene synthesis.\\n\\nTo address these issues, we present INSTRUCTSCENE, a novel generative framework for 3D indoor scene synthesis with natural language instructions. The overview of the proposed method is illustrated in Figure 1. INSTRUCTSCENE comprises two parts: a semantic graph prior and a layout decoder. In the first stage, it takes instructions about partial interior arrangement and object appearances, and learns the conditional distribution of semantic graphs for holistic scenes. In the second stage, harnessing the well-structured and informative graph latents, the layout decoder can easily embody scenes that exhibit semantic consistency while closely adhering to the provided instructions. With the learned semantic graph prior, INSTRUCTSCENE also achieves a wide range of instruction-driven generative tasks in a zero-shot manner.\"}"}
{"id": "LtuRgL03pI", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Position a grey nightstand to the left of a wooden wardrobe with doors. Hang a white ceiling lamp with a wooden circle directly above a black double bed.\\n\\nFigure 1: Method overview. (1) INSTRUCT first designs a holistic semantic graph based on user instruction. Within this graph, each node is an object endowed with semantic features and each edge represents a spatial relationship between objects. (2) It proceeds to place objects in a scene by decoding precise 7 degrees-of-freedom attributes for each object from the informative graph prior. Specific conditional diffusion models are devised for both parts of INSTRUCT. Benefiting from the two-stage scheme, it can separately handle discrete and continuous attributes of indoor scenes, drastically reducing the burden of network optimization. To enhance the capability of aesthetic design, INSTRUCT also leverages object geometrics and appearances by quantizing semantic features from a multimodal-aligned model (Radford et al., 2021; Liu et al., 2023b).\\n\\nTo fit practical scenarios and promote the benchmarking of instruction-drive scene synthesis, we curate a high-quality dataset containing paired scenes and instructions with the help of large language and multimodal models (Li et al., 2022; Ouyang et al., 2022; OpenAI, 2023). Comprehensive quantitative evaluations reveal that INSTRUCT surpasses previous state-of-the-art methods by a large margin in terms of both generation controllability and fidelity. Each essential component of our method is carefully verified through ablation studies.\\n\\nOur contributions can be summarized as follows:\\n\\n\u2022 We present an instruction-driven generative framework that integrates a semantic graph prior and a layout decoder to improve the controllability and fidelity for 3D scene synthesis.\\n\u2022 The proposed general semantic graph prior jointly models appearance and layout distributions, facilitating various downstream applications in a zero-shot manner.\\n\u2022 We curate a high-quality dataset to promote the benchmarking of instruction-driven 3D scene synthesis, and quantitative experiments demonstrate that the proposed method significantly outperforms existing state-of-the-art techniques.\\n\\n2 RELATED WORK\\n\\nGraph-driven 3D Scene Synthesis\\n\\nGraphs have been used to guide complex scene synthesis in the form of scene hierarchies (Li et al., 2019; Gao et al., 2023), parse trees (Purkait et al., 2020), scene graphs (Zhou et al., 2019a; Para et al., 2021), etc. Wang et al. (2019) utilize an image-based module and condition its outputs on the edges of a relation graph within each non-differentiable step. They also adopt an autoregressive model (Li et al., 2018) to generate relation graphs, which are however unconditional and with limited object attributes. Other works (Luo et al., 2020; Dhamo et al., 2021; Zhai et al., 2023) adopt conditional VAEs (Kingma & Welling, 2014; Sohn et al., 2015) with graph convolutional networks (Johnson et al., 2018) to generate layouts. While offering high controllability, these methods demand the specification of elaborate graph conditions, which are notably more intricate than those driven by natural languages.\\n\\nLanguage-driven 3D Scene Synthesis\\n\\nEarly studies on language-driven scene synthesis are conducted through procedural modeling, resulting in a semi-automatic process (Chang et al., 2014; 2015a; 2017; Ma et al., 2018). With the advent of attention mechanisms (Vaswani et al., 2017), recent approaches (Wang et al., 2021; Paschalidou et al., 2021; Liu et al., 2023a; Tang et al., 2023) can implicitly acquire object relations by self-attention and condition scene synthesis with texts by cross-attention. However, text prompts in these works tend to be relatively simple, containing only object categories or lacking layout descriptions, limiting the expressiveness and customization. Implicit relation modeling also significantly hinders their controllability.\"}"}
{"id": "LtuRgL03pI", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "LtuRgL03pI", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "LtuRgL03pI", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "LtuRgL03pI", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nBoris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.\\nSIAM Journal on Control and Optimization (SICON), 30(4):838\u2013855, 1992.\\n\\nPulak Purkait, Christopher Zach, and Ian Reid. Sg-vae: Scene grammar variational autoencoder to generate new indoor scenes. In European Conference on Computer Vision (ECCV), pp. 155\u2013171. Springer, 2020.\\n\\nSiyuan Qi, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, and Song-Chun Zhu. Human-centric indoor scene synthesis using stochastic grammar. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5899\u20135908, 2018.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), pp. 8748\u20138763. PMLR, 2021.\\n\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning (ICML), pp. 8821\u20138831. PMLR, 2021.\\n\\nDaniel Ritchie, Kai Wang, and Yu-an Lin. Fast and flexible indoor scene synthesis via deep convolutional generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6182\u20136190, 2019.\\n\\nMartin Simonovsky and Nikos Komodakis. Graphvae: Towards generation of small graphs using variational autoencoders. In International Conference on Artificial Neural Networks (ICANN), pp. 412\u2013422. Springer, 2018.\\n\\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning (ICML), pp. 2256\u20132265. PMLR, 2015.\\n\\nKihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. Advances in Neural Information Processing Systems (NeurIPS), 28, 2015.\\n\\nLiangchen Song, Liangliang Cao, Hongyu Xu, Kai Kang, Feng Tang, Junsong Yuan, and Yang Zhao. Roomdreamer: Text-driven 3d indoor scene synthesis with coherent geometry and texture. arXiv preprint arXiv:2305.11337, 2023.\\n\\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations (ICLR), 2020.\\n\\nJiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, and Matthias Nie\u00dfner. Diffuscene: Scene graph denoising diffusion probabilistic model for generative indoor scene synthesis. arXiv preprint arXiv:2303.14207, 2023.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017.\\n\\nTathagat Verma, Abir De, Yateesh Agrawal, Vishwa Vinay, and Soumen Chakrabarti. Varscene: A deep generative model for realistic scene graph synthesis. In International Conference on Machine Learning (ICML), pp. 22168\u201322183. PMLR, 2022.\\n\\nClement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. In International Conference on Learning Representations (ICLR), 2023.\\n\\nKai Wang, Manolis Savva, Angel X Chang, and Daniel Ritchie. Deep convolutional priors for indoor scene synthesis. ACM Transactions on Graphics (TOG), 37(4):1\u201314, 2018.\"}"}
{"id": "LtuRgL03pI", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel X Chang, and Daniel Ritchie. Planit: Planning and instantiating indoor scenes with relation graph and spatial prior networks. ACM Transactions on Graphics (TOG), 38(4):1\u201315, 2019.\\n\\nXinpeng Wang, Chandan Yeshwanth, and Matthias Nie\u00dfner. Sceneformer: Indoor scene generation with transformers. In International Conference on 3D Vision (3DV), pp. 106\u2013115. IEEE, 2021.\\n\\nHaitao Yang, Zaiwei Zhang, Siming Yan, Haibin Huang, Chongyang Ma, Yi Zheng, Chandrajit Bajaj, and Qixing Huang. Scene synthesis via uncertainty-driven attribute synchronization. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 5630\u20135640, 2021a.\\n\\nMing-Jia Yang, Yu-Xiao Guo, Bin Zhou, and Xin Tong. Indoor scene generation from a collection of semantic-segmented depth images. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 15203\u201315212, 2021b.\\n\\nJiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. Graphrnn: Generating realistic graphs with deep auto-regressive models. In International Conference on Machine Learning (ICML), pp. 5708\u20135717. PMLR, 2018.\\n\\nGuangyao Zhai, Evin Pinar \u00a8Ornek, Shun-Cheng Wu, Yan Di, Federico Tombari, Nassir Navab, and Benjamin Busam. Commonscenes: Generating commonsense 3d indoor scenes with scene graphs. Advances in Neural Information Processing Systems (NeurIPS), 2023.\\n\\nZaiwei Zhang, Zhenpei Yang, Chongyang Ma, Linjie Luo, Alexander Huth, Etienne V ouga, and Qixing Huang. Deep generative modeling for scene synthesis via hybrid representations. ACM Transactions on Graphics (TOG), 39(2):1\u201321, 2020.\\n\\nYang Zhou, Zachary While, and Evangelos Kalogerakis. Scenegraphnet: Neural message passing for 3d indoor scene augmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 7384\u20137392, 2019a.\\n\\nYi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5745\u20135753, 2019b.\"}"}
{"id": "LtuRgL03pI", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Following previous works (Paschalidou et al., 2021; Tang et al., 2023; Liu et al., 2023a), we use three types of indoor rooms in 3D-FRONT (Fu et al., 2021a) and preprocess the dataset by filtering some problematic samples, resulting in 4041 bedrooms, 813 living rooms and 900 dining rooms. The number of objects $N_i$ in the valid bedrooms is between 3 and 12 with 21 object categories, i.e., $K_c = 21$. While for living and dining rooms, $N_i$ varies from 3 to 21 and $K_c = 24$. We use the same data split for training and evaluation as ATISS (Paschalidou et al., 2021).\\n\\nThe original 3D-FRONT dataset does not contain any descriptions of room layout or object appearance details. In order to advance research in the field of text-conditional indoor scene generation, we carefully curate a high-quality dataset with paired scenes and instructions for interior design through a multi-step process:\\n\\n1. **Spatial Relation Extraction**: View-dependent spatial relations are initially extracted from the 3D-FRONT dataset using predefined rules similar to Johnson et al. (2018) and Luo et al. (2020), which are listed in Appendix A.1.\\n\\n2. **Object Captioning**: We further enhance the dataset by providing captions to objects using BLIP (Li et al., 2022), a powerful model pretrained for vision-language understanding, given furniture 2D thumbnail images from the original dataset (Fu et al., 2021b).\\n\\n3. **Caption Refinement**: As generated captions may not always be accurate, we filter them with corresponding ground-truth categories using ChatGPT (Ouyang et al., 2022; OpenAI, 2023), a large language model fine-tuned for instruction-based tasks. This results in accurate and expressive descriptions of each object in the scene. The prompt and hyperparameters for ChatGPT to filter captions are provided in Appendix A.2.\\n\\n4. **Instruction Generation**: The final instructions for scene synthesis are derived from 1 to 2 randomly selected \u201c(subject, relation, object)\u201d triplets obtained during the first extraction process. Verbs and conjunctions within sentences are also randomly picked to maintain diversity and fluency.\\n\\nTo facilitate future research and replication, the processing scripts and the processed dataset can be found in https://chenguolin.github.io/projects/InstructScene.\\n\\n### A.1 Relation Definitions\\nWe define 11 relationships in a 3D space as listed in Table 4. Assume $X$ and $Y$ span the ground plane, and $Z$ is the vertical axis. We use Center to represent the coordinates of a 3D bounding box\u2019s center. Height is the $Z$-axis size of a bounding box. Relative orientation is computed as $\\\\theta = \\\\arctan2(Y_s - Y_o, X_s - X_o)$, where $s$ and $o$ respectively refer to \u201csubject\u201d and \u201cobject\u201d in a relationship. $d(s, o)$ is the ground distance between $s$ and $o$. Inside $(s, o)$ indicates whether the subject center is inside the ground bounding box of the object.\\n\\n### A.2 Caption Refinement by ChatGPT\\nThe generated object captions from BLIP are refined by ChatGPT (gpt-3.5-turbo). Our prompt to ChatGPT is provided in Table 5. We set the hyperparameter temperature and top $p$ for text generation to 0.2 and 0.1 respectively, encouraging more deterministic and focused outputs.\\n\\n### B.1 Baseline Details\\nWe choose two prominent methods for comparison: (1) ATISS (Paschalidou et al., 2021), an autoregressive model that sequentially generates unordered object sets; (2) DiffuScene (Tang et al., 2023), a Gaussian diffusion model that treats scene attributes as continuous 2D matrices.\\n\\n1 https://github.com/nv-tlabs/ATISS\\n2 https://github.com/tangjiapeng/DiffuScene\"}"}
{"id": "LtuRgL03pI", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Rules to determine the spatial relationships between objects.\\n\\n| Relationship       | Rule                                                                 |\\n|--------------------|----------------------------------------------------------------------|\\n| Left of            | \\\\(\\\\theta_{so} \\\\geq \\\\frac{3\\\\pi}{4}\\\\) or \\\\(\\\\theta_{so} < -\\\\frac{3\\\\pi}{4}\\\\) and \\\\(1 < d(s, o) \\\\leq 3\\\\) |\\n| Right of           | \\\\(-\\\\frac{\\\\pi}{4} \\\\leq \\\\theta_{so} < \\\\frac{\\\\pi}{4}\\\\) and \\\\(1 < d(s, o) \\\\leq 3\\\\) |\\n| In front of        | \\\\(\\\\pi/4 \\\\leq \\\\theta_{so} < 3\\\\pi/4\\\\) and \\\\(1 < d(s, o) \\\\leq 3\\\\) |\\n| Behind             | \\\\(-3\\\\pi/4 \\\\leq \\\\theta_{so} < -\\\\pi/4\\\\) and \\\\(1 < d(s, o) \\\\leq 3\\\\) |\\n| Closely left of    | \\\\(\\\\theta_{so} \\\\geq \\\\frac{3\\\\pi}{4}\\\\) or \\\\(\\\\theta_{so} < -\\\\frac{3\\\\pi}{4}\\\\) and \\\\(d(s, o) \\\\leq 1\\\\) |\\n| Closely right of   | \\\\(-\\\\frac{\\\\pi}{4} \\\\leq \\\\theta_{so} < \\\\frac{\\\\pi}{4}\\\\) and \\\\(d(s, o) \\\\leq 1\\\\) |\\n| Closely in front of| \\\\(\\\\pi/4 \\\\leq \\\\theta_{so} < 3\\\\pi/4\\\\) and \\\\(d(s, o) \\\\leq 1\\\\) |\\n| Closely behind     | \\\\(-3\\\\pi/4 \\\\leq \\\\theta_{so} < -\\\\pi/4\\\\) and \\\\(d(s, o) \\\\leq 1\\\\) |\\n| Above              | \\\\((\\\\text{Center}_Z{s} - \\\\text{Center}_Z{o}) > (\\\\text{Height}_s + \\\\text{Height}_o)/2)\\\\) and (Inside(s, o) or Inside(o, s)) |\\n| Below              | \\\\((\\\\text{Center}_Z{o} - \\\\text{Center}_Z{s}) > (\\\\text{Height}_s + \\\\text{Height}_o)/2)\\\\) and (Inside(s, o) or Inside(o, s)) |\\n\\n### Table 5: Prompt for ChatGPT to refine raw object descriptions.\\n\\nGiven a description of furniture from a captioning model and its ground-truth category, please combine their information and generate a new short description in one line. The provided category must be the descriptive subject of the new description. The new description should be as short and concise as possible, encoded in ASCII. Do not describe the background and counting numbers. Do not describe size like 'small', 'large', etc. Do not include descriptions like 'a 3D model', 'a 3D image', 'a 3D printed', etc. Descriptions such as color, shape and material are very important, you should include them. If the old description is already good enough, you can just copy it. If the old description is meaningless, you can just only include the category. For example: Given 'a 3D image of a brown sofa with four wooden legs' and 'multi-seat sofa', you should return: a brown multi-seat sofa with wooden legs. Given 'a pendant lamp with six hanging balls on the white background' and 'pendant lamp', you should return: a pendant lamp with hanging balls. Given 'a black and brown chair with a floral pattern' and 'armchair', you should return: a black and brown floral armchair. The above examples indicate that you should delete the redundant words in the old description, such as '3D image', 'four', 'six' and 'white background', and you must include the category name as the subject in the new description. The old descriptions is '{BLIP caption}', its category is '{ground-truth category}', the new descriptions should be: {new description}.\\n\\nWe re-implement and augment these methods based on their official GitHub repositories to support instruction-driven scene synthesis and quantized semantic feature generation. In the case of ATISS, we replace the [START] token, which originally is the room mask feature, with a learnable token, as we condition scene synthesis on instruction prompts rather than room masks. The augmented ATISS predicts quantized feature indices after class label sampling, and they are subsequently utilized to predict the remaining scene attributes. Instead, quantized features are one-hot encoded in DiffuScene, allowing them to be diffused and denoised in a continuous space alongside other attributes.\\n\\nTo maintain a fair comparison, we use the same experimental settings across all methods, including network architectures, training hyperparameters, object retrieval procedures, rendering schemes, etc.\\n\\n### B.2 MODEL DETAILS\\n\\nWe use 5-layer and 8-head Transformers with 512 attention dimensions and a dropout rate of 0.1 for all generative models in this work. They are trained by the AdamW optimizer (Loshchilov & Hutter, 2018) for 500,000 iterations with a batch size of 128, a learning rate of 1e-4, and a weight decay of 0.02. Exponentially moving average (EMA) technique (Polyak & Juditsky, 1992; Ho et al., 2020) with a decay factor of 0.9999 is utilized in the model parameters.\"}"}
{"id": "LtuRgL03pI", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We adopt OpenShape \\\\textit{pointbert-vitg14-rgb} (Liu et al., 2023b) to extract 3D object semantic features $f \\\\in \\\\mathbb{R}^{1280}$. It is a recently introduced 3D RGB point cloud encoder that aligns with the pretrained CLIP ViT-bigG/14 multi-modal features (Cherti et al., 2023), enabling the simultaneous representation of visual appearances and geometric shapes. The codebook $Z$ has a size of 64 and a dimension of 512. We use 4 ordered indices to quantize OpenShape features. VQ-VAE is trained on over 4,000 3D objects found in the filtered 3D-FRONT scenes (Fu et al., 2021a;b). We use the frozen text encoder in CLIP ViT-B/32 (Radford et al., 2021) to extract instruction features for all experiments. Regarding the loss weights $\\\\lambda_f$ and $\\\\lambda_e$ in Equation 6, we do not tune and simply fix them as 1 and 10 respectively to ensure that the three terms in the loss are of comparable numerical magnitudes.\\n\\nCode for both training and evaluation can be found in https://chenguolin.github.io/projects/InstructScene.\\n\\n**B.3 EVALUATION DETAILS**\\n\\nBlender Rendering\\nAfter retrieving objects from a 3D database (Fu et al., 2021b), we use Blender (Community, 2018) with the CYCLES engine to render high-quality images for 3D scenes. Our rendering script is adapted from the one available at https://github.com/allenai/objaverse-rendering/blob/main/scripts/blender_script.py. The images for evaluation are rendered from a top-down perspective in $256 \\\\times 256$ resolutions. We maintain a camera distance of 1.2, a filter width of 0.1, and use the RGB color mode. Other hyperparameters are set in accordance with the referenced script. Sizes of floor plans are adapted across scenes to include all objects, and their textures are fixed to ensure the choice does not introduce any bias in evaluations.\\n\\nComputation of Metrics\\nFID, FID\\\\textit{CLIP} and KID scores are computed by the clean-fid library (Parmar et al., 2022). Lower scores derived from these metrics indicate a higher degree of similarity between the learned distributions and real ones. Following Paschalidou et al. (2021), we fine-tuned an AlexNet (Krizhevsky et al., 2012) that had been pretrained on ImageNet to classify the rendered images of synthesized scenes as well as those of ground-truth scenes. The scene classification accuracy (SCA) that approaches 50% signifies better generation performance.\\n\\n**C ADDITIONAL RESULTS**\\n\\n**C.1 INSTRUCTION-DRIVEN SCENE SYNTHESIS**\\nWe present visualizations of instruction-driven synthesized bedrooms, living rooms, and dining rooms in Figure 5, 6 and 7. Besides the quantitative evaluations shown in Table 1, these qualitative visualizations also evident the superiority of our method over previous state-of-the-art approaches in terms of adherence to instructions and generative quality.\\n\\n**C.2 ZERO-SHOT APPLICATIONS**\\nWe present visualizations illustrating various zero-shot instruction-driven applications, including stylization, re-arrangement, completion, and unconditional 3D scene synthesis in Figure 8, 9, 10 and 11 respectively. We find that the autoregressive model ATISS tends to generate redundant objects, resulting in chaotic synthesized scenes. DiffuScene encounters challenges in accurately modeling object semantic features, often yielding objects that lack coherence in terms of style or pairing, thereby diminishing the aesthetic appeal of the synthesized scenes. Moreover, both of these baseline models frequently struggle to follow the provided instructions during conditional generation. In contrast, our approach demonstrates a notable capability to generate highly realistic 3D scenes that concurrently adhere to the provided instructions.\\n\\n3 https://github.com/Colin97/OpenShape_code\\n4 https://github.com/openai/clip\\n5 https://github.com/GaParmar/clean-fid\"}"}
{"id": "LtuRgL03pI", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Add a corner side table with a round top to the left of a black and silver pendant lamp with lights. Place an L-shaped sofa behind a black marble desk. Then, position a lounge chair to the right of a black marble desk. Put a TV stand in front of a grey sofa. Finally, add a green upholstered dining chair to the left of a grey TV stand.\\n\\n(a) Instructions\\n\\n(b) Ours w/o semantic features\\n\\n(c) Ours\\n\\nFigure 16: Instruction-driven scene synthesis results of INSTRUCT and its degraded version, which is not encoded with semantic features.\"}"}
{"id": "LtuRgL03pI", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nFigure 17: Unconditional scene synthesis results of a degraded version of INSTRUCT-SCENE, which is not encoded with semantic features.\"}"}
{"id": "LtuRgL03pI", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nGenerative Models for Graphs\\nThere have been lots of endeavors on generative models for undirected graphs, molecules and scene graphs by autoregressive models (You et al., 2018; Garg et al., 2021), VAEs (Simonovsky & Komodakis, 2018; Verma et al., 2022), GANs (De Cao & Kipf, 2018; Martinkus et al., 2022) and diffusion models (Niu et al., 2020; Jo et al., 2022; Vignac et al., 2023; Kong et al., 2023). Longland et al. (2022) and Lo et al. (2023) employ VAE and GAN respectively for text-driven undirected simple graph generation without any semantics. In contrast, we present a pioneering effort to generate holistic semantic graphs with expressive instructions.\\n\\n3 PRELIMINARY\\n\\nDiffusion generative models (Sohl-Dickstein et al., 2015) consist of a non-parametric forward process and a learnable reverse process. The forward process progressively corrupts a data point from \\\\( q(x_0) \\\\) to a sequence of increasingly noisy latent variables:\\n\\n\\\\[\\nq(x_1:T | x_0) = \\\\prod_{t=1}^{T} q(x_t | x_{t-1}).\\n\\\\]\\n\\nA neural network is trained to reverse the process by denoising them iteratively:\\n\\n\\\\[\\np_\\\\psi(x_0:T | c) = p(x_T) \\\\prod_{t=1}^{T} p_\\\\psi(x_{t-1} | x_t, c),\\n\\\\]\\n\\nwhere \\\\( c \\\\) is an optional condition to guide the reverse process as needed. These two processes are supposed to admit \\\\( p(x_T) \\\\approx q(x_T | x_0) \\\\) for a sufficiently large \\\\( T \\\\). The generative model is optimized by minimizing a variational upper bound on \\\\( \\\\mathbb{E}_{q(x_0)}[-\\\\log p_\\\\psi(x_0)] \\\\):\\n\\n\\\\[\\nL_{vb} = \\\\mathbb{E}_{q(x_0)}[D_{KL}[q(x_T | x_0) \\\\parallel p(x_T)]] + \\\\sum_{t=2}^{T} L_{t-1} - \\\\mathbb{E}_{q(x_1 | x_0)}[\\\\log p_\\\\psi(x_0 | x_1, c)],\\n\\\\]\\n\\nwhere \\\\( L_{t-1} = D_{KL}[q(x_{t-1} | x_t, x_0) \\\\parallel p_\\\\psi(x_{t-1} | x_t, c)] \\\\) and \\\\( L_T \\\\) is constant during training so can be ignored. \\\\( D_{KL}[\\\\cdot \\\\parallel \\\\cdot] \\\\) indicates the KL divergence between two distributions.\\n\\n4 METHOD\\n\\n4.1 PROBLEM STATEMENT\\n\\nDenote \\\\( S := \\\\{S_1, \\\\ldots, S_M\\\\} \\\\) as a collection of indoor scenes. Each scene \\\\( S_i \\\\) is composed of multiple objects \\\\( O_i := \\\\{o_{ij}\\\\}_{j=1}^{N_i} \\\\) with distinct attributes \\\\( o_{ij} := \\\\{c_{ij}, t_{ij}, s_{ij}, r_{ij}, f_{ij}\\\\} \\\\), including category \\\\( c_{ij} \\\\in \\\\{1, \\\\ldots, K_c\\\\} \\\\), where \\\\( K_c \\\\) is the number of object classes in \\\\( S \\\\), location \\\\( t_{ij} \\\\in \\\\mathbb{R}^3 \\\\), axis-aligned size \\\\( s_{ij} \\\\in \\\\mathbb{R}^3 \\\\), orientation \\\\( r_{ij} \\\\in \\\\mathbb{R} \\\\) and semantic feature \\\\( f_{ij} \\\\in \\\\mathbb{R}^d \\\\), where \\\\( d \\\\) is the dimension of the feature vector. To set up a 3D scene, one can generate each 3D object or retrieve it from a database by \\\\( c \\\\) and \\\\( f \\\\). They are then resized and transformed to the same scene coordinate by corresponding \\\\( t \\\\), \\\\( s \\\\) and \\\\( r \\\\). To simplify the process, we opt to retrieve 3D objects from a high-quality dataset, and leave the generative models of each object conditioned on \\\\( c \\\\) and \\\\( f \\\\) for future work.\\n\\nGiven instructions \\\\( y \\\\), our goal is to learn the conditional scene distribution \\\\( q(S | y) \\\\). Rather than direct modeling (Paschalidou et al., 2021; Tang et al., 2023), we employ well-structured and informative graphs to serve as general and semantic latents. Each graph \\\\( G_i \\\\) contains a node set \\\\( V_i := \\\\{v_{ij}\\\\}_{j=1}^{N_i} \\\\) and a directed edge set \\\\( E_i := \\\\{e_{ijk}\\\\}_{v_{ij}, v_{ik} \\\\in V_i} \\\\). A node \\\\( v_{ij} \\\\) functions as a high-level representation of an object \\\\( o_{ij} \\\\), and a directed edge \\\\( e_{ijk} \\\\) explicitly conveys the relations between objects.\\n\\nTo this end, we propose a generative framework, \\\\( I^2 \\\\) SCENE, that consists of two components: (1) semantic graph prior \\\\( p_\\\\phi(G | y) \\\\) (Sec. 4.2) that jointly models high-level object and relation distributions conditioned on \\\\( y \\\\); (2) layout decoder \\\\( p_\\\\theta(S | G) \\\\) (Sec. 4.3) that produces precise layout configurations with semantic graphs. Since \\\\( G \\\\) is deterministic by corresponding \\\\( S \\\\), the two networks together yield an instruction-driven generative model for 3D indoor scenes:\\n\\n\\\\[\\np_\\\\phi,\\\\theta(S | y) = p_\\\\phi,\\\\theta(S, G | y) = p_\\\\phi(G | y) p_\\\\theta(S | G).\\n\\\\]\"}"}
{"id": "LtuRgL03pI", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2.1 Feature Quantization\\nHigh-dimensional features, such as those with $d = 1280$ in OpenCLIP ViT-bigG/14 (Liu et al., 2023b), are too complicated to model. We circumvent this drawback by introducing a vector-quantized variational autoencoder for feature vectors, coined as $f_{VQ-VAE}$. The intuition behind it is that there are general intrinsic characteristics shared among objects, encompassing attributes like colors, materials and basic geometric shapes. Indexing semantic features from a codebook could dramatically reduce the cost of operating in a continuous space.\\n\\nFormally, $f_{VQ-VAE}$ contains a pair of encoder $E$ and decoder $D$, along with a codebook $Z \\\\in \\\\mathbb{R}^{K_f \\\\times d_Z}$, where $K_f$ and $d_Z$ are its size and dimension respectively. To concurrently capture object visual appearances and geometric shapes, we employ a multimodal-aligned point cloud encoder, OpenShape (Liu et al., 2023b), to extract object semantic features. The diagram for $f_{VQ-VAE}$ is presented in Figure 2(a). It is trained to maximize the evidence lower bound (ELBO):\\n\\n$$E_{z \\\\sim p_E(z|f)} [\\\\log p_D(f|z) - \\\\beta D KL(p_E(z|f) \\\\parallel p(z))]$$\\n\\nwhere $z \\\\in \\\\mathbb{R}^{n_f \\\\times d_Z}$ consists of $n_f$ vectors indexed by a sequence of scalars $f := [f_m]_{m=1}^{n_f}$, where each scalar $f_m \\\\in \\\\{1, \\\\ldots, K_c\\\\}$. Since the quantization operation is non-differentiable, gumbel-softmax relaxation (Jang et al., 2016; Ramesh et al., 2021) is adopted to optimize the ELBO.\\n\\n4.2.2 Discrete Semantic Graph Diffusion\\nAfter the feature quantization, all attributes in a semantic graph are categorical, $G_i := (C_i, F_i, E_i)$, where $C_i := \\\\{1, \\\\ldots, K_c\\\\}^{N_i}$, $E_i := \\\\{1, \\\\ldots, K_e\\\\}^{N_i \\\\times N_i}$ and $F_i := \\\\{1, \\\\ldots, K_f\\\\}^{N_i \\\\times n_f}$. While it is possible to embed discrete variables in continuous spaces using one-hot encodings, it diminishes the sparsity inherent in the original data and imposes a substantial burden on network optimization. Instead, we propose to model the semantic graph prior through discrete diffusion models.\\n\\nFor a scalar discrete random variable with $K$ categories $x \\\\in \\\\{1, \\\\ldots, K\\\\}$, diffusion noise is defined by a series of transition matrices $Q \\\\in \\\\mathbb{R}^{K \\\\times K}$. The forward process at timestep $t$ is expressed as $q(x_t| x_{t-1}) := x_t^\\\\top Q_t x_{t-1}$, where $x_t \\\\in \\\\mathbb{R}^K$ is the column one-hot encoding for $x_t$ and $[Q_t]_{mn} := q(x_t = m| x_{t-1} = n)$ is the probability that $x_{t-1}$ transits to the category $m$ from $n$. The probabilistic distribution of $x_t$ can be directly derived from $x_0$:\\n\\n$$q(x_t| x_0) := x_t^\\\\top \\\\bar{Q}_t x_0$$\\n\\nwhere $\\\\bar{Q}_t := Q_t \\\\cdots Q_1$.\\n\\nInstead of commonly used Gaussian or uniform transitions for graph generation (Niu et al., 2020; Hoogeboom et al., 2021; Jo et al., 2022; Vignac et al., 2023), we propose to diffuse semantic graphs by independently masking graph attributes (i.e., object class $c$, quantized feature indices $f$ and relation $e$) by introducing an absorbing state $[MASK]$ (Austin et al., 2021; Gu et al., 2022) to each uniform transition matrix. For object class $c$, its transition matrix is defined as:\\n\\n$$Q_C(t) = \\\\begin{pmatrix} \\\\alpha_c(t) + \\\\beta_c(t) & \\\\beta_c(t) & \\\\cdots & \\\\beta_c(t) \\\\\\\\ \\\\beta_c(t) & \\\\alpha_c(t) + \\\\beta_c(t) & \\\\cdots & \\\\beta_c(t) \\\\\\\\ \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\ \\\\beta_c(t) & \\\\beta_c(t) & \\\\cdots & \\\\alpha_c(t) + \\\\beta_c(t) \\\\\\\\ \\\\gamma_c(t) & \\\\gamma_c(t) & \\\\cdots & \\\\gamma_c(t) \\\\\\\\ \\\\end{pmatrix}$$\\n\\n(4)\\n\\nwhere $\\\\alpha_c(t), \\\\beta_c(t)$ and $\\\\gamma_c(t)$ are parameters that control the transitions in the graph.\"}"}
{"id": "LtuRgL03pI", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nby which\\n\\nc\\n\\nt has a probability of \\\\( \\\\gamma_c \\\\) to be masked, a probability of \\\\( \\\\alpha_c \\\\) to maintain the same, leaving a\\n\\nchance of \\\\( 1 - \\\\gamma_c - \\\\alpha_c \\\\) for uniform sampling.\\n\\n[MASK] will always stay in its own state. Transition\\n\\nmatrices for \\\\( f \\\\) and \\\\( e \\\\), denoted as \\\\( Q_F_t \\\\) and \\\\( Q_E_t \\\\) respectively, exhibit analogous structures. Schedules\\n\\nof \\\\( (\\\\alpha_t, \\\\beta_t, \\\\gamma_t) \\\\) are designed to admit that the initial states for semantic graphs are all masked.\\n\\nSince the number of objects varies across different scenes, semantic graphs are padded by empty\\n\\nstates to maintain a consistent number of \\\\( N \\\\) objects. One-hot encodings for scalar variables\\n\\nc, \\\\( f \\\\) and \\\\( e \\\\) in a scene are denoted as \\\\( C \\\\in \\\\mathbb{R}^{N \\\\times (K_c + 2)} \\\\), \\\\( F \\\\in \\\\mathbb{R}^{N \\\\times n_f \\\\times (K_f + 2)} \\\\) and\\n\\n\\\\( E \\\\in \\\\mathbb{R}^{N \\\\times N \\\\times (K_e + 2)} \\\\) respectively. Here \\\\( \"+2 \\\\) accounts for the two extra states (i.e., empty state and mask state) for each\\n\\nvariable. A one-hot encoded semantic graph \\\\( G_0 := (C_0, F_0, E_0) \\\\) at timestep \\\\( t \\\\) is formulated as\\n\\n\\\\[\\nq(G_t | G_0) = (\\\\bar{Q}_C_t C_0, \\\\bar{Q}_F_t F_0, \\\\bar{Q}_E_t E_0)\\n\\\\]\\n\\n(5)\\n\\nThe process for learning the graph prior is illustrated in Figure 2(b). The independent diffusion with\\n\\nmask states offers two significant advantages:\\n\\n\u2022 Perturbed states for one variable (e.g., \\\\( C \\\\)) could be recovered by incorporating information\\n\\n  from uncorrupted portions of the other variables (e.g., \\\\( F \\\\) and \\\\( E \\\\)), compelling the semantic\\n\\n  graph prior to learning from the interactions among different scene attributes.\\n\\n\u2022 The introduction of mask states facilitates the distinction between corrupted states and\\n\\n  clean ones, thus simplifying the denoising task.\\n\\nThese benefits are critical especially for intricate semantic graphs and diverse downstream generative\\n\\ntasks, compared with simple graph generative tasks (Niu et al., 2020; Jo et al., 2022; Vignac et al.,\\n\\n2023). Ablation study on the choice of \\\\( Q \\\\) is provided in Sec. 5.5.2.\\n\\nOutput of the graph prior network is re-parameterized to produce the clean scene graphs\\n\\n\\\\( \\\\hat{G}_0 \\\\), which\\n\\nis then diffused to get the predicted posterior for computing the variational bound in Equation 1:\\n\\n\\\\[\\np_\\\\phi(G_{t-1} | G_t, y) \\\\propto P(\\\\hat{G}_0) q(G_{t-1} | G_t, \\\\hat{G}_0) p_\\\\phi(\\\\hat{G}_0 | G_t, y)\\n\\\\]\\n\\n(6)\\n\\nwhere \\\\( \\\\lambda_e, \\\\lambda_f \\\\in \\\\mathbb{R}_+ \\\\) are hyperparameters to adjust the relative importance of three components in\\n\\nthe semantic graph.\\n\\n4.3 3D LAYOUT DECODER\\n\\nInstantiating 3D scenes becomes easy with semantic graph prior. Denote layout configurations of\\n\\n\\\\( S_i \\\\) as \\\\( \\\\{l_{ij}\\\\}_{j=1}^N \\\\), where \\\\( l_{ij} := o_{ij} - v_{ij} = \\\\{t_{ij}, s_{ij}, r_{ij}\\\\} \\\\). \\\\( SO(2) \\\\) rotation is parameterize by\\n\\n\\\\[\\n[\\\\cos(r), \\\\sin(r)]^T\\n\\\\]\\n\\nto continuously represent \\\\( r \\\\) (Zhou et al., 2019b). Consequently, the layout of \\\\( S_i \\\\) can be expressed as\\n\\n2D matrices \\\\( L_i \\\\in \\\\mathbb{R}^{N_i \\\\times 8} \\\\). Note that \\\\( S = (L, G) \\\\), so generating indoor scenes\\n\\n\\\\( p_\\\\theta(S | G) \\\\) is equivalent\\nto learning the conditional distributions of layout configurations \\\\( p_\\\\theta(L | G) \\\\).\\n\\nA diffusion model with variance-preserving Gaussian kernels (Ho et al., 2020; Song et al., 2020)\\nis adopted to learn \\\\( p_\\\\theta(L | G) \\\\). Its forward process is\\n\\n\\\\[\\nq(L_t | L_0) := \\\\mathcal{N}(L_t; \\\\sqrt{\\\\bar{\\\\alpha}_t} L_0, (1 - \\\\bar{\\\\alpha}_t) I)\\n\\\\]\\n\\nThe\\n\\nreverse process is modeled as\\n\\n\\\\[\\np_\\\\theta(L_{t-1} | L_t, G) := \\\\mathcal{N}(L_{t-1}; \\\\mu_\\\\theta(L_t, t, G); \\\\Sigma_\\\\theta(L_t, t, G))\\n\\\\]\\n\\nFollowing\\n\\nHo et al. (2020), the variational bound in Equation 1 for the decoder\\n\\n\\\\( p_\\\\theta(L | G) \\\\) is reweighted and\\n\\nsimplified:\\n\\n\\\\[\\nL_{simple} := \\\\mathbb{E}_{L_0, t, \\\\epsilon} \\\\| \\\\epsilon - \\\\epsilon_\\\\theta(L_t, t, G) \\\\|_2\\n\\\\]\\n\\n(7)\\n\\nwhere \\\\( t \\\\) is sampled from a uniform distribution \\\\( U(1, T) \\\\) and \\\\( \\\\epsilon \\\\) is sampled from a standard normal\\n\\ndistribution \\\\( \\\\mathcal{N}(0, I) \\\\). Diagram of the layout decoder is depicted in Figure 3(a). Intuitively, the\\n\\nnetwork is trained to predict noise \\\\( \\\\epsilon \\\\) in the corrupted data \\\\( L_t \\\\).\\n\\n4.4 MODEL ARCHITECTURE\\n\\nWe use the general-purpose Transformer (Vaswani et al., 2017) for all models across tasks.\"}"}
{"id": "LtuRgL03pI", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3: (a) 3D Layout Decoder. Gaussian noises are sampled to attach at every node of semantic graphs; A graph Transformer processes these graphs iteratively to remove noises and generate layout configurations. (b) Graph Transformer. A graph Transformer consists of a stack of $M$ blocks, each comprising graph attention, MLP and optional cross-attention modules; AdaLN and multi-head scheme are not depicted for concision.\\n\\nVanilla Transformer\\nAs illustrated in Figure 2(a), $n_f$ learnable tokens are employed with a stack of cross-attentions to extract information from object features $f$ in the encoder $E$. Regarding the decoder $D$, $n_f$ vectors retrieved from the codebook $Z$ are fed to another Transformer, and an average pooling on the top of it is applied to aggregate information.\\n\\nGraph Transformer\\nThe prior and decoder share the same model architecture as shown in Figure 3(b). Since relation $e_{jk}$ can be determined by $e_{kj}$, only the upper triangular part of the relation matrix is necessary. Object categories and features together form input tokens for Transformers. Message passing on graphs is operated via node self-attention and node-edge fusion with FiLM (Perez et al., 2018), which linearly modulates edge embeddings and node attention matrices before softmax (Dwivedi & Bresson, 2021; Vignac et al., 2023). Timestep for diffusion $t$ is injected by AdaLN (Ba et al., 2016; Dhariwal & Nichol, 2021). In the prior $p_\\\\phi(G|y)$, instructions are embedded by a frozen text encoder and consistently influence network outputs by cross-attention mechanisms. Layout decoder $p_\\\\theta(S|G)$ is conditioned on semantic graphs by appending sampled Gaussian noises on node embeddings, which are then iteratively denoised to produce layout attributes.\\n\\nPermutation Non-invariance\\nAlthough $G$ should ideally remain invariant to node permutations, invariant diffusion models could encounter learning challenges for multi-mode modeling. Thus, each node feature is added with positional encodings (Vaswani et al., 2017; Tang et al., 2023; Lei et al., 2023) before the permutation-equivariant Transformer. Exchangeability for graph prior distributions is strived by random permutation augmentation during the training process. Ablation on the permutation non-invariance is provided in Sec. 5.5.2.\\n\\n5 EXPERIMENTS\\n5.1 SCENE-INSTRUCTION PAIR DATASET\\nAll experiments are conducted on 3D-FRONT (Fu et al., 2021a), a professionally designed collection of synthetic indoor scenes. However, it does not contain any descriptions of room layouts or object appearances. To construct a high-quality scene-instruction paired dataset, we initially extract view-dependent spatial relations with predefined rules. The dataset is further enhanced by captioning objects with BLIP (Li et al., 2022). To ensure the accuracy of descriptions, the generated captions are filtered by ChatGPT (Ouyang et al., 2022; OpenAI, 2023) with object ground-truth categories. The final instructions are derived from randomly selected relation triplets. Details on dataset curation can be found in Appendix A.\\n\\n5.2 EXPERIMENTAL SETTINGS\\nBaselines\\nWe compare our method with two state-of-the-art approaches for 3D scene generative tasks: (1) ATISS (Paschalidou et al., 2021), a Transformer-based auto-regressive network that regards scenes as sets of unordered objects, and generates objects and their attributes sequentially. (2) DiffuScene (Tang et al., 2023), a diffusion model with Gaussian kernels that treats object attributes in one scene as a 2D matrix after padding them to a fixed size. Both of these methods can be conditioned on text prompts by cross-attention with a pretrained text encoder. Our preliminary\"}"}
{"id": "LtuRgL03pI", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Quantitative evaluations for instruction-driven synthesis by ATISS (Paschalidou et al., 2021), DiffuScene (Tang et al., 2023) and our method on three room types. Higher iRecall, lower FID, FID_{CLIP} and KID indicate better synthesis quality. For SCA, a score closer to 50% is better. Standard deviation values are provided as subscripts.\\n\\n| Room Type   | Method   | iRecall (\u00b1SD) | FID (\u00b1SD) | FID_{CLIP} (\u00b1SD) | KID (\u00b1SD) | SCA (\u00b1SD) |\\n|-------------|----------|---------------|-----------|------------------|-----------|-----------|\\n| Bedroom     | ATISS    | 48.13 \u00b1 2.50  | 119.73 \u00b1 1.55 | 6.95 \u00b1 0.06     | 0.39 \u00b1 0.02 | 59.17 \u00b1 1.39 |\\n|             | DiffuScene | 56.43 \u00b1 2.07  | 123.09 \u00b1 0.79 | 7.13 \u00b1 0.16     | 0.39 \u00b1 0.01 | 60.49 \u00b1 2.96 |\\n|             | Ours     | 73.64 \u00b1 1.37  | 114.78 \u00b1 1.19 | 6.65 \u00b1 0.18     | 0.32 \u00b1 0.03 | 56.02 \u00b1 1.43 |\\n| Living room | ATISS    | 29.50 \u00b1 3.67  | 117.67 \u00b1 2.32 | 6.08 \u00b1 0.13     | 17.60 \u00b1 2.65 | 69.38 \u00b1 3.38 |\\n|             | DiffuScene | 31.15 \u00b1 2.49  | 122.20 \u00b1 1.09 | 6.10 \u00b1 0.11     | 16.49 \u00b1 1.24 | 72.92 \u00b1 1.29 |\\n|             | Ours     | 56.81 \u00b1 2.85  | 110.39 \u00b1 0.78 | 5.37 \u00b1 0.07     | 8.16 \u00b1 0.56  | 65.42 \u00b1 2.52 |\\n| Dining room | ATISS    | 37.58 \u00b1 1.99  | 137.10 \u00b1 0.34 | 8.49 \u00b1 0.23     | 23.60 \u00b1 2.52 | 67.61 \u00b1 3.23 |\\n|             | DiffuScene | 37.87 \u00b1 2.76  | 145.48 \u00b1 1.36 | 8.63 \u00b1 0.31     | 24.08 \u00b1 1.90 | 70.57 \u00b1 2.14 |\\n|             | Ours     | 61.23 \u00b1 1.67  | 129.76 \u00b1 1.61 | 7.67 \u00b1 0.18     | 13.24 \u00b1 1.79 | 64.20 \u00b1 1.90 |\\n\\nExperiments suggest that both baselines encounter difficulties in modeling high-dimensional semantic feature distributions, consequently impacting their performance in generating other attributes. Therefore, we augment them to generate quantized features. Further implementation details about baselines and our method are provided in Appendix B.1 and B.2.\\n\\nEvaluation Metrics\\nTo assess the controllability of layouts, we use a metric named \u201cinstruction recall\u201d (iRecall), which quantifies the proportion of the required triplets \u201c(subject, relation, object)\u201d occurring in synthesized scenes to all provided in instructions. It is a stringent metric that takes into account all three elements in a layout relation simultaneously. Following previous works (Paschalidou et al., 2021; Liu et al., 2023a; Tang et al., 2023), we also report Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017), FID_{CLIP} (Kynk\u00e4nen et al., 2022), which computes FID scores by CLIP features (Radford et al., 2021), Kernel Inception Distance (KID) (B\u00edrko et al., 2018), scene classification accuracy (SCA). These metrics evaluate the overall quality of synthesized scenes and rely on rendered images. We use Blender (Community, 2018) to produce high-quality images for both synthesized and real scenes. For more details on evaluation, please refer to Appendix B.3.\\n\\n5.3 INSTRUCTION-DRIVEN SCENE SYNTHESIS\\nTable 1 presents the quantitative evaluations for synthesizing 3D scenes with instructions. We report the average scores of five runs with different random seeds. As demonstrated, even with the enhancement of quantized semantic features, two baseline methods continue to demonstrate inferior performance compared to ours. ATISS outperforms DiffuScene in terms of generation fidelity, owing to its capacity to model in discrete spaces. DiffuScene shows better controllability to ATISS because it affords global visibility of samples during generation. Our proposed INSPECTSENE exhibits the best of both worlds. Remarkably, we achieve a substantial advancement in controllability, measured in iRecall, for scene generative models, surpassing current state-of-the-art approaches by about 15%\u223c25% across various room types, all while maintaining high fidelity. It is noteworthy that INSPECTSENE excels in handling more complex scenes, such as living and dining rooms, which typically comprise an average of 20 objects, in contrast to bedrooms, which have only 8 objects on average, revealing the benefits of modeling intricate 3D scenes associated with the semantic graph prior. Qualitative visualizations are provided in Appendix C.1.\\n\\n5.4 ZERO-SHOT APPLICATIONS\\nThanks to the discrete design and mask modeling, the learned semantic graph prior is capable of diverse downstream tasks without any fine-tuning. We investigate four zero-shot tasks, including stylization, re-arrangement, completion, and unconditional generation. The first three tasks can be regarded as conditional synthesis guided by both instructions and partial scene attributes. Stylization and re-arrangement task can be formulated as $p(\\\\phi(f|c,t,s,r,y))$ and $p(\\\\phi,\\\\theta(t,s,r|c,f,y))$ respectively. In the completion task, we intend to add new objects $\\\\{o_i\\\\}$ to a partial scene $S_i$ with 7...\"}"}
{"id": "LtuRgL03pI", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Quantitative evaluations for zero-shot generative applications on three room types. \\\"Uncond.\\\" stands for unconditional scene synthesis.\\n\\n| Zero-shot Applications | Stylization | Re-arrangement | Completion | Uncond. |\\n|------------------------|-------------|----------------|------------|---------|\\n|                        | \u2191(1 - e^{-3}) | \u2193 | \u2191 | \u2193 |\\n| FID                    | iRecall % | FID | iRecall % | FID |\\n| Bedroom                | 3.44      | 123.91 | 61.22    | 107.67 |\\n| ATISS                  | 1.08      | 127.35 | 68.57    | 106.15 |\\n| DiffuScene             | 6.34      | 122.73 | 79.59    | 105.27 |\\n| Ours                   | 134.51    | 134.51 | 134.51   | 134.51 |\\n| Living room            | -3.57     | 110.85 | 31.97    | 117.97 |\\n| ATISS                  | -2.69     | 112.80 | 41.50    | 115.30 |\\n| DiffuScene             | 0.28      | 109.39 | 56.12    | 106.85 |\\n| Ours                   | 129.23    | 129.23 | 129.23   | 129.23 |\\n| Dining room            | -1.11     | 131.14 | 36.06    | 134.54 |\\n| ATISS                  | -2.98     | 135.20 | 46.84    | 133.73 |\\n| DiffuScene             | 1.69      | 128.78 | 62.08    | 125.07 |\\n| Ours                   | 147.52    | 147.52 | 147.52   | 147.52 |\\n\\nInstructions. By filling the partial scene attributes with [MASK] tokens, we treat them as intermediate states during discrete graph denoising, allowing for a straightforward adaptation of the learned semantic graph prior to these tasks in a zero-shot manner. Unconditional synthesis is implemented by simply setting text features as zeros. To assess controllability in the stylization task, we define $\\\\Delta := \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\cos \\\\text{Sim}(f_i, d_{\\\\text{style}i}) - \\\\cos \\\\text{Sim}(f_i, d_{\\\\text{class}i})$, where $d_{\\\\text{style}i}$ represents the CLIP text feature of object class name with the desired style, and $d_{\\\\text{class}i}$ is the CLIP text feature with only class information. $\\\\cos \\\\text{Sim}(\\\\cdot, \\\\cdot)$ calculates the cosine similarity between two vectors.\\n\\nEvaluations on zero-shot applications are reported in Table 2. Our method consistently outperforms two strong baselines in both controllability and fidelity. While ATISS, as an auto-regressive model, is a natural fit for the completion task, its unidirectional dependency chain limits its effectiveness for tasks requiring global scene modeling, such as re-arrangement. DiffuScene can adapt to these tasks by replacing the known parts with the noised corresponding scene attributes during sampling, similar to image in-painting (Meng et al., 2021; Nichol et al., 2022). However, the known attributes are greatly corrupted in the early steps, which could misguide the denoising direction, and therefore necessitate fine-tuning. Additionally, DiffuScene also faces challenges in searching for semantic features in a continuous space for stylization. In contrast, I\\\\textsc{Structure} globally models scene attributes and treats partial scene attributes as intermediate discrete states during training. These designs effectively eliminate the training-test gap, rendering it highly versatile for a wide range of downstream tasks. Visualizations of zero-shot applications are available in Appendix C.2.\\n\\n5.5 ABLATION STUDIES\\n\\n5.5.1 CONFIGURATIONS FOR DIFFUSION MODELS\\n\\nDiffusion Timesteps\\n\\nAlthough containing two diffusion models, our method could achieve better efficiency by reducing the steps of reverse processes without a noticeable decline in performance. This stems from the fact that each stage in I\\\\textsc{Structure} tackles an easier denoising task compared to the single-stage DiffuScene. Following the original setting of Tang et al. (2023), the timestep value ($T$) for DiffuScene is set to 1000. While for I\\\\textsc{Structure}, we find $T=100$ and 10 is sufficient for $p_{\\\\psi}(G|y)$ and $p_{\\\\theta}(S|G)$ respectively. Evaluation results with different timesteps are presented in Figure 4(a), with values averaged on three room types. The plotted timesteps for our method are \\\"100+1000\\\", \\\"100+400\\\", \\\"100+100\\\", \\\"100+10\\\", \\\"50+10\\\" and \\\"25+10\\\", where the first number represents $T$ for the prior and the latter is for the decoder.\\n\\nClassifier-Free Guidance\\n\\nClassifier-free guidance (CFG) (Ho & Salimans, 2021) is a widely used technique to trade off controllability with diversity. We do not adopt it in previous experiments for a fair comparison, as the sequential attribute decoding hinders ATISS from realizing the benefits offered by CFG. To assess its effectiveness for diffusion models, we randomly remove instruction conditions on 20% of samples during training, inducing an unconditional generation. At inference, CFG is implemented by adjusting conditional log-likelihoods away from unconditional counterparts:\\n\\n$$\\\\tilde{p}_{\\\\psi}(\\\\hat{G}_0|G_t, y) := (1 + s) \\\\cdot p_{\\\\psi}(\\\\hat{G}_0|G_t, y) - s \\\\cdot p_{\\\\psi}(\\\\hat{G}_0|G_t),$$\\n\\n(8)\"}"}
{"id": "LtuRgL03pI", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Ablation studies on configurations for diffusion models, including diffusion timesteps and classifier-free guidance scales.\\n\\nTable 3: Ablation studies on different strategies to learn semantic graph prior $\\\\phi(G|y)$. \u201cPerm. Invar.\u201d means permutation-invariant graph modeling.\\n\\n| Graph Prior          | Ours | Gaussian Joint Mask | Uniform |\\n|----------------------|------|---------------------|---------|\\n| $\\\\uparrow$iRecall $\\\\%$ | 73.64\u00b11.37 | 34.18\u00b12.53          | 34.21\u00b12.79 |\\n| $\\\\downarrow$FID      | 114.78\u00b11.19 | 128.98\u00b10.97         | 130.86\u00b12.76 |\\n| CLIP                 | 6.65\u00b10.18  | 7.30\u00b10.03           | 7.59\u00b10.17  |\\n| $\\\\downarrow$KID      | $\\\\times1e^{-3}$ 0.32\u00b10.03 | 2.63\u00b10.73   | 4.82\u00b11.69  |\\n| SCA                  | 56.02\u00b10.91  | 57.10\u00b13.22          | 60.37\u00b13.13 |\\n| $\\\\uparrow$          | 76.79\u00b13.14  | 76.79\u00b13.14          | 76.79\u00b13.14 |\\n\\nwhere $s$ is a hyperparameter to control the scale of CFG. Performance for diffusion-based models with different CFG scales are plotted in Figure 4(b), where values are averaged over three room types. Within an appropriate range of scales, CFG can effectively enhance the controllability for instructive-driven 3D scene synthesis, while large scales can lead to a performance decline. Though DiffuScene also benefits from CFG, our method still significantly outperforms it in both metrics.\\n\\n5.5.2 LEARNING SEMANTIC GRAPH PRIOR\\n\\nWe explore different strategies to learn the proposed semantic graph prior. All experiments are conducted on the bedroom dataset. Quantitative results are presented in Table 3.\\n\\nTransition Matrices for Learning Graph Prior\\n\\nWe investigate the effects of different transition matrices for learning the proposed semantic graph prior, including: (1) Embed all categorical variables into their one-hot encodings and diffuse them by Gaussian kernels, which is similar to Niu et al. (2020) and Jo et al. (2022); (2) Jointly masking $F$ and $E$ along with nodes $C$ in a graph, so only the attributes of other objects can be utilized for recovery; (3) Adopt uniform transition matrices without mask states, which is similar to Vignac et al. (2023). Evaluations on both controllability and fidelity reveal the advantages of our independent mask strategy.\\n\\nPermutation Non-invariance\\n\\nUnlike previous studies on graph generation (Niu et al., 2020; Jo et al., 2022; Vignac et al., 2023), we depart from the convention of permutation-invariant modeling to ease the learning process of semantic graph prior. We strive to preserve exchangeable graph distributions by randomly shuffling object orders during training. Performance for invariant graph prior is provided in the last column of Table 3. Its performance declines due to the unnecessary imposition of invariance in scene synthesis.\\n\\n6 CONCLUSION\\n\\nBy integrating a semantic graph prior and a layout decoder, we propose a novel generative framework, INSTRUCTSCENE, that significantly improves the controllability and fidelity of 3D indoor scene synthesis, providing a user-friendly interface through instructions in natural languages. Benefits from the design of semantic graph prior, our method can also apply to diverse applications without any fine-tuning. The controllability and versatility positions INSTRUCTSCENE as a promising tool. We hope this work could help in practical scenarios, such as facilitating interior design, delivering immersive metaverse experiences, simulations for embodied agents, developing cutting-edge VR/AR applications, etc. We discuss the limitations of our method and future work in Appendix D.\"}"}
{"id": "LtuRgL03pI", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Several large pretrained models are incorporated in this work, including OpenShape (Liu et al., 2023b) for object semantic feature extraction, CLIP (Radford et al., 2021) for text feature extraction, BLIP (Li et al., 2022) for object captioning and ChatGPT (Ouyang et al., 2022; OpenAI, 2023) for caption refinement. Most of these models are trained on large-scale datasets collected from the web, lacking rigorous content filtering, thereby potentially encompassing harmful material. We curate the dataset and train our method based on these models, thus may inherit these imperfections. Given that our generative framework is trained only on indoor scene datasets, it exhibits less probability of propagating negative consequences compared to the synthesis and editing methods on human faces and natural images. Nevertheless, we will still explicitly specify permissible applications of our system through appropriate licensing to mitigate potential adverse societal impacts.\\n\\nTo ensure the reproducibility of our method, we include the details of dataset processing in Appendix A, including the rule-based spatial relation definitions (A.1) and the used prompt and hyper-parameters for ChatGPT to refine object descriptions (A.2). Implementation details are also provided in Appendix B, including baseline reproductions (B.1), model hyperparameter disclosure (B.2) and evaluation metric computations (B.3). Our instruction-scene pair dataset and code for both training and evaluation can be found in https://chenguolin.github.io/projects/InstructScene.\\n\\nThis work is supported by National Key R&D Program of China (2022ZD0160305).\\n\\nREFERENCES\\n\\n1. Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems (NeurIPS), 34:17981\u201317993, 2021.\\n2. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n3. Miko\u0142aj Binkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. In International Conference on Learning Representations (ICLR), 2018.\\n4. Angel Chang, Manolis Savva, and Christopher D Manning. Learning spatial knowledge for text to 3d scene generation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2028\u20132038, 2014.\\n5. Angel Chang, Will Monroe, Manolis Savva, Christopher Potts, and Christopher D Manning. Text to 3d scene generation with rich lexical grounding. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), pp. 53\u201362, 2015a.\\n6. Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015b.\\n7. Angel X Chang, Mihail Eric, Manolis Savva, and Christopher D Manning. Sceneseer: 3d scene design with natural language. arXiv preprint arXiv:1703.00050, 2017.\\n8. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gorordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2818\u20132829, 2023.\"}"}
