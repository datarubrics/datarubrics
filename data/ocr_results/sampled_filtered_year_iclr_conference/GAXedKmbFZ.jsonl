{"id": "GAXedKmbFZ", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nISCO-BENCH: A CONTEXT-AWARE EVALUATION BENCHMARK FOR LANGUAGE MODELLING\\n\\nAnonymous authors\\n\\nPaper under double-blind review\\n\\nABSTRACT\\n\\nModeling large contexts, especially linguistic phenomena that span beyond individual sentences, is a fundamental yet challenging aspect of natural language processing (NLP). However, existing evaluation benchmarks primarily focus on the evaluation of inter-sentence properties and overlook critical discourse phenomena that cross sentences. To bridge the gap, we propose Disco-Bench, a benchmark that can evaluate intra-sentence contextual properties across a diverse set of NLP tasks, covering understanding, translation, and generation. Disco-Bench consists of 9 document-level testsets in the literature domain, which contain rich discourse phenomena (e.g. cohesion and coherence) in Chinese and/or English. For linguistic analysis, we also design a diagnostic test suite to probe the extent to which the evaluated models have internalized contextual information. We totally evaluate 20 general-purpose and domain-specific models based on advanced pretraining architectures and large language models (LLMs). Our results show that (1) our evaluation benchmark is both challenging and necessary; (2) fine-grained pretraining with literary document-level training data consistently enhances the modeling of discourse information. We will release the datasets, pretrained models, and leaderboard, which we hope can significantly facilitate research in this field.\\n\\n1 INTRODUCTION\\n\\nFigure 1: An example with intra-sentence contextual properties.\\n\\nTo evaluate the general performance of models, previous work proposed a variety of benchmarks, covering different tasks and languages such as GLUE (Wang et al., 2018), CLUE (Xu et al., 2020) and XGLUE (Liang et al., 2020). However, existing benchmarks pay little attention to intra-sentence contextual properties such as discourse, which are fundamental and challenging problems in natural language processing (NLP) (Kevitt et al., 1992). A text generally consists of meaningful, unified, and purposive groups of sentences, which are organized as a whole (Cook, 1989). As shown in Figure 1, the discourse property manifests in two ways: (1) cohesion, where the dependency between words or phrases makes them logically and consistently connected; (2) coherence, where the structural relation between segments or sentences enables them semantically and meaningfully composed.\\n\\nTo bridge the gap, we introduce a novel benchmark for the target evaluation on the context-aware modeling. Our Disco-Bench comprises three datasets:\\n\\n\u2022 Disco-Bench Benchmark: It consists of nine Chinese/English context-aware tasks covering a broad range of NLP tasks (understanding, translation, and generation), data quantities (from 26.4K to 2.4M), and difficulties. Besides, most task datasets are newly created in this work.\\n\\n\u2022 Disco-Bench Diagnostic Dataset: To understand the discourse information learned by models, we propose a dataset of hand-crafted 1,294 examples for probing trained models. Each instance is a contrastive pair, where the correct candidate is the original instance in the benchmark and the incorrect one is a perturbation by modifying discourse devices in the correct candidates.\\n\\n1\"}"}
{"id": "GAXedKmbFZ", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: An overview of our context-aware evaluation benchmark, covering language understanding, translation and generation. All datasets consist of document-level texts in the literature domain, which are rich in discourse phenomena. Eight of them are newly created by us and one is expanded based on existing corpus (i.e. MRC). It covers three languages: English (en), Modern Chinese (mzh/zh) and Classical Chinese (czh). We report commonly-used evaluation metrics. \\\"#\\\" means the number of instances (e.g. sentences, pairs or documents). \\\"Test\\\" represents both validation and testing sets.\\n\\n| Task | Metric | Dataset | Language |\\n|------|--------|---------|----------|\\n| Understanding Task | SI F1, EM | 48.0K 17.5K novel zh | novel zh |\\n| | ZPR F1, P, R | 2.2M 8.1K mixed zh | mixed zh |\\n| | MRC Acc. | 26.4K 6.5K composition mzh, czh | composition mzh, czh |\\n| Translation Task | NT d-BLEU, BLEU, TER, MET. COM. | 1.9M 1.3K novel zh \u2192 en | novel zh \u2192 en |\\n| | CCT 778.1K 5.3K dianji czh \u2192 mzh | dianji czh \u2192 mzh |\\n| | PT 47.1K 2.7K poetry zh \u2192 en | poetry zh \u2192 en |\\n| Generation Task | TE BLEU, PPL | 2.4M 10K book en | book en |\\n| | TI PPL, Dist, BERTscore | 233K 10K book zh | book zh |\\n| | TC | 233K 10K book zh | book zh |\\n\\nDisco-Bench Training Dataset: We introduce a large-scale (400G), long-text data in Chinese and English, which is in the same literature domain with the benchmark. The training data enables fine-grained pretraining to better model context-aware information required by the benchmark.\\n\\nTo better understand challenges posed by Disco-Bench, we conduct experiments on a variety of state-of-the-art models, including standard Transformer, pretrained models as well as large language models (LLMs). We found that these tasks display different levels of difficulty, resulting in different behaviors and performances across models. Furthermore, the fine-grained pretraining based on the context-rich Disco-Bench training data improves performances particularly on cohesive translation and coherent generation. However, the best models still achieve a fairly low absolute score, highlighting the difficulty of modeling discourse. There are three main contributions in this work:\\n\\n- Challenging Tasks: We propose a diverse set of context-aware tasks to evaluate monolingual and cross-lingual models' ability to understand, translate and generate texts.\\n- Considerable Resources: We build and release a variety of context-aware resources, including benchmarking datasets, diagnostic test suite and large-scale pretraining corpus.\\n- Comprehensive Comparisons: We systematically compare many advanced pretraining methods on the benchmark, and identify current challenges in context modelling for future exploration.\\n\\nTo comprehensively evaluate the target models, Disco-Bench covers three types of NLP tasks, including language understanding, translation and generation. We design the benchmarking tasks using the following criteria: (1) our tasks should measure the ability of models to handle contextual information, thus we define related tasks at different levels of difficulty; (2) our datasets should contain rich discourse phenomena, thus we build document-level datasets with whole contexts extracted from literary texts. As shown in Table 1, we introduce 9 tasks containing corresponding datasets in Chinese and/or English: eight of which are newly created, and one is expanded based on existing data.\"}"}
{"id": "GAXedKmbFZ", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Illustration of the proposed understating tasks in terms discourse properties and task definition. As seen, SI needs to recognize named entity and resolve coreference. While ZPR demands the further ability to tackle zero anaphora and gender identification. MRC is the hardest because it should fully understand coherence (e.g. discourse structure based on temporal relation) apart from cohesion in previous tasks. English translations of example sentences are listed in Appendix \u00a7A.1.\\n\\nas a host of other long-range language phenomena that have not even been adequately characterized much less conquered (Bates, 1995). As shown in Figure 2, we classify tasks into three difficulty levels according to the length of contexts and the amount of knowledge, required for discourse modeling.\\n\\nSI (Speaker Identification)\\nGiven a paragraph that may contain an utterance and the surrounding context, SI aims to identify the corresponding speaker(s) for the utterance or the content within quotation marks if no speaker exists. To archive this goal, models need to examine the existence of quotes, recognize named entities or phrases that can serve as speakers, and resolve coreference. We construct the dataset with 66K instances from eighteen Chinese novels. Unlike previous SI datasets like P&P (He et al., 2013) where all speakers are entities, speakers in our dataset can also be phrases, pronouns, or multi-entities. We employ macro-averaged F1 and exact match (EM) as the evaluation metrics, following standard extractive machine reading comprehension (Rajpurkar et al., 2016).\\n\\nZPR (Zero Pronoun Recovery)\\nZPR aims to recover omitted pronouns in terms of position and form, according to its anaphora information in the given sentence (Yang & Xue, 2010; Zhang et al., 2019b; Song et al., 2020). Figure 2 shows an example, where the omitted pronoun \u201c\u5979 (She)\u201d can be recovered according to its anaphora \u201c\u83f2\u6bd4 (Phoebe)\u201d. The BaiduKnows is a widely-used Chinese ZPR corpus, which contains only 5K human-annotated sentences extracted from a Q&A forum (Zhang et al., 2019b). The insufficient data limits the investigation of model performance on ZPR. Inspired by Wang et al. (2016), we automatically built a large-scale training set from Chinese-English movie subtitles using word alignments. For testset, we hire experts to manually annotate 8K sentences covering five domains and the label set contains 30 Chinese pronouns. Different from previous benchmarks like CLUEWSC2020 which mainly focus on anaphora resolution (explicit pronouns) (Kong & Zhou, 2010; Mitkov, 2014), while ZPR considers implicit pronouns which are complementary to each other. We use micro F1, precision and recall as the evaluation metrics.\\n\\nMRC (Machine Reading Comprehension)\\nThe goal of MRC is to answer questions based on the understanding of its meaning given an unstructured text (Liu et al., 2019a; Zeng et al., 2020). We collected the Haihua2021 corpus, which contains 8K articles extracted from reading comprehension tests in primary/high school examinations.\\n\\n1 Each article is followed by at least one question with 2 \u223c 5 choices and one correct answer. We manually create 2K articles as an additional supplement. Different from previous benchmarks based on Wikipedia texts (Cui et al., 2019) or Chinese idioms (Zheng et al., 2019), ours is in the literary domain (i.e. modern/ancient composition and poetry) that contains\"}"}
{"id": "GAXedKmbFZ", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In EMNLP, 2016.\\n\\nEhud Reiter and Robert Dale. Building applied natural language generation systems. Natural Language Engineering, 3(1):57\u201387, 1997.\\n\\nYunfan Shao, Zhichao Geng, Yitao Liu, Junqi Dai, Fei Yang, Li Zhe, Hujun Bao, and Xipeng Qiu. Cpt: A pre-trained unbalanced transformer for both chinese language understanding and generation. arXiv preprint arXiv:2109.05729, 2021.\\n\\nLinfeng Song, Kun Xu, Yue Zhang, Jianshu Chen, and Dong Yu. ZPR2: Joint zero pronoun recovery and resolution using multi-task learning and BERT. In ACL, 2020.\\n\\nKai Sun, Dian Yu, Dong Yu, and Claire Cardie. Investigating prior knowledge for challenging chinese machine reading comprehension. TACL, 2020.\\n\\nHuishuang Tian, Kexin Yang, Dayiheng Liu, and Jiancheng Lv. Anchibert: A pre-trained model for ancient chinese language understanding and generation. In 2021 IEEE International Joint Conference on Neural Networks, 2021.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\\n\\nElena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov. Context-aware neural machine translation learns anaphora resolution. In ACL, 2018.\\n\\nElena Voita, Rico Sennrich, and Ivan Titov. When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion. In ACL, 2019.\\n\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In EMNLP, 2018.\\n\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In NeurIPS, 2019.\\n\\nLongyue Wang, Zhaopeng Tu, Xiaojun Zhang, Hang Li, Andy Way, and Qun Liu. A novel approach for dropped pronoun translation. In NAACL, 2016.\\n\\nLongyue Wang, Zhaopeng Tu, Andy Way, and Qun Liu. Exploiting cross-sentence context for neural machine translation. In EMNLP, 2017.\\n\\nWenxuan Wang, Wenxiang Jiao, Yongchang Hao, Xing Wang, Shuming Shi, Zhaopeng Tu, and Michael Lyu. Understanding and improving sequence-to-sequence pretraining for neural machine translation. In ACL, 2022.\\n\\nLeo Wanner. Lexical choice in text generation and machine translation. Machine Translation, 11(1):3\u201335, 1996.\\n\\nBilly TM Wong and Chunyu Kit. Extending machine translation evaluation metrics with lexical cohesion to document level. In EMNLP, 2012.\\n\\nLiang Xu, Xuanwei Zhang, and Qianqian Dong. Cluecorpus2020: A large-scale chinese corpus for pre-training language model. ArXiv, abs/2003.01355, 2020.\\n\\nYaqin Yang and Nianwen Xue. Chasing the ghost: recovering empty categories in the chinese treebank. In COLING, 2010.\\n\\nYaqin Yang, Yalin Liu, and Nianwen Xue. Recovering dropped pronouns from chinese text messages. In ACL-IJCNLP, 2015.\\n\\nChangchang Zeng, Shaobo Li, Qin Li, Jie Hu, and Jianjun Hu. A survey on machine reading comprehension\u2014tasks, evaluation metrics and benchmark datasets. Applied Sciences, 10(21):7640, 2020.\"}"}
{"id": "GAXedKmbFZ", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2019a.\\n\\nWeinan Zhang, Ting Liu, Qingyu Yin, and Yu Zhang. Neural recovery machine for Chinese dropped pronoun. In Frontiers of Computer Science, 2019b.\\n\\nZhuosheng Zhang, Hanqing Zhang, Keming Chen, Yuhang Guo, Jingyun Hua, Yulong Wang, and Ming Zhou. Mengzi: Towards lightweight yet ingenious pre-trained models for chinese. arXiv preprint arXiv:2110.06696, 2021.\\n\\nZhe Zhao, Hui Chen, Jinbin Zhang, Wayne Xin Zhao, Tao Liu, Wei Lu, Xi Chen, Haotang Deng, Qi Ju, and Xiaoyong Du. Uer: An open-source toolkit for pre-training models. In EMNLP, 2019.\\n\\nChujie Zheng, Minlie Huang, and Aixin Sun. ChID: A large-scale Chinese IDiom dataset for cloze test. In ACL, 2019.\\n\\nWanrong Zhu, Zhiting Hu, and Eric Xing. Text infilling. arXiv preprint arXiv:1901.00158, 2019.\"}"}
{"id": "GAXedKmbFZ", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: English translations of examples in Figure 2, 3, and 4. Some are literal translations in order to map discourse phenomena into the English language.\\n\\n| Task Discourse Context | Task Description |\\n|------------------------|------------------|\\n| Figure 2               |                  |\\n| SI Xing Jiu'an followed Mu Qing into the car and sat in the co-pilot position. | \u201cAre you in a bad mood?\u201d Mu Qing asked. |\\n|                        | \u201cUm, yes.\u201d |\\n|                        | Inp: \u201cUm, yes.\u201d |\\n|                        | Out: Speaker=Xing Jiu'an |\\n| ZPR A                  | Phoebe would love to buy a TV. |\\n|                        | Joey won\u2019t let \u2205 buy \u2205? |\\n|                        | A: Yes. |\\n|                        | Inp: B: Joey won\u2019t let \u2205 buy \u2205? |\\n|                        | Out: B: Joey won\u2019t let her buy it? |\\n| MRC                    | The little princess climbed out of the castle window while her mother was sleeping. |\\n|                        | She climbed down the south wall and slipped out. |\\n|                        | Finally \u2205 walked into the forest without telegraph poles. |\\n|                        | Inp: Where did the little princess go after she escaped? (A) South Wall; (B) Forest; (C) Castle; (D) Mountain. |\\n|                        | Out: Answer=(B) Forest |\\n| Figure 3               |                  |\\n| NT King Ding sat on the side, smiling as he looked at Qing Shuang\u2019s astounded thoughts. | \u2205 mind had already flown to a faraway place. |\\n|                        | Inp: \u2205 mind had already flown to a faraway place. |\\n|                        | Out: \u2013 |\\n| CCT \u00a9                  | , when she is playing Xiao, not only can her beautiful face remain as usual, but also her charm increases. Why? \u00a9 \u2205 is playing, \u2205 fingers press the holes on the flute, and in this way, \u2205 tender and slim fingers will seem to be slimmer and fairer. \u00a9, when shrinking \u2205 month to blow, \u2205 mouth appears to be smaller. |\\n|                        | Inp: \u00a9, when shrinking \u2205 month to blow, \u2205 mouth appears to be smaller. |\\n|                        | Out: Besides, when shrinking her month to blow, her mouth appears to be smaller. |\\n| PT                    | I ask your lad beneath a tree. |\\n|                        | \u201cMy master\u2019s gone for herbs,\u201d says he, \u201cAmid the hills I know not where, For clouds have veiled them here and there.\u201d |\\n|                        | Inp: I ask your lad beneath a tree. |\\n|                        | Out: \u2013 |\\n| Figure 4               |                  |\\n| TE \u2013 \u2013 \u2013              | Mu Xiaoxiao looked at his back aggrieved, why did it suddenly change like this? |\\n|                        | She was inexplicably trained for a while, which made her feel bad. |\\n|                        | When she got to class S, she was lying on the table and was sullen. |\\n|                        | Inp: Mu Xiaoxiao looked at his back aggrieved, why did it suddenly change like this? [x] [x] [x] ... When she got to class S, she was lying on the table and was sullen. |\\n|                        | Out: She was inexplicably trained for a while, which made her feel bad. |\\n| TC                    | Chen Xu was hungry and cold. He used a small gas stove to cook a pot of noodles. |\\n|                        | The two gathered around the pot and devoured everything. |\\n|                        | After they ate the noodles, they felt alive. |\\n|                        | Inp: Chen Xu was hungry and cold. [x] [x] [x] ... |\\n|                        | Out: The two gathered around the pot and devoured everything. After they ate the noodles, they felt alive. |\\n\\nA.2 Details of Diagnostic Test Suite\\n\\nDefinition and Annotation\\n\\nAs shown in Table 7, we define 6 properties in our test suite:\\n\\n- Repetition: means the repeating of certain words or phrases. We mainly annotate nouns repetition in 4\u223c5 neighbouring sentences.\"}"}
{"id": "GAXedKmbFZ", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Synonyms means related words that having the same connotations, implications, or reference in two sentences. In our test suite, this phenomenon include nouns and adjectives synonyms in 4 \u223c 5 neighbouring sentences.\\n\\nEllipsis means the omission of one or more words that are obviously understood but that must be supplied to make a construction grammatically complete. This omission often happens after wh-words in English and in subject elements in Chinese.\\n\\nSubstitution occurs when one item within a text or discourse is replaced by another. In English, such nouns are often replaced by \\\"one\\\" or \\\"some\\\", and verbs are replaced by \\\"do\\\" or \\\"did\\\". In Chinese, this often happens around quantifier or temporal adverbial.\\n\\nReference is a relationship between objects in which one object designates, or acts as a means by which to connect to or link to, another object.\\n\\nConjunction expresses a logical semantic relationship between two sentences rather than between words or structures. We mainly annotate additive, adversative, causal, and temporal.\\n\\nTable 7: Chinese Examples of cohesion phenomena in our test suite.\\n\\n| Type       | Example                                                                 | Contrastive Instance |\\n|------------|-------------------------------------------------------------------------|----------------------|\\n| Repetition | \u4f51\\\u54e5\\\u72ec\\\u81ea\\n\u8fd4\\\u8eab\\n\u53c8\\n\u6765\\n\u5230\\n\u62cd\\n\u5356\\n\u884c\\n\u79bb\\n\u5f00\\n\u62cd\\n\u5356\\n\u884c\\n\u540e\\n\u4f51\\\u54e5\\\u8054\\n\u7cfb\\n\u4e86\\n\u97e9\\n\u5bb6\\n\u516c\\n\u5b50\\n\u7b49\\n\u4eba\\n... (Youge went alone and returned to the auction house... After leaving the auction house, Youge contacted the son of the Han family and others...) |\\n| Synonyms   | \u9ad8\\n\u624b\\n\u4e0d\u4e00\\n\u5b9a\\n\u8981\\n\u5f88\\n\u82f1\\n\u4fca\\n... \u4f60\u4e0d\\n\u8981\\n\u770b\\n\u5230\\n\u4e00\u4e2a\\n\u957f\\n\u5f97\\n\u5e05\\n\u7684\\n\u6cd5\\n\u5e08\\n\u5c31\\n\u8bf4\\n\u662f\\n... (A master does not necessarily have to be very handsome... Don't say a good-looking wizard is... when you see one.) |\\n| Ellipsis   | \u5218\\n\u7532\\n\u95ee\\n\u9053;\\n\u201c\u4e0d\\n\u77e5\\n\u4f60\\n\u4eec\\n\u8981\\n\u4e70\\n\u591a\\n\u5c11\\n\u4ea9\\n\u6c34\\n\u7530\\n\u5462\\n?\u201d ...\\n\u8fde\\n\u82b3\\n\u6d32\\n\u5c31\\n\u7b11\u7b11,\\n\u8bf4\\n\u9053;\\n\u201c\u5927\\n\u6982\\n\u4e24\\n\u5343\\n\u6765\\n\u4ea9\\n\u2205\\n\u5427\\n!\u201d ...\\n( Liu Jia asked, \u201cI don\u2019t know how many acres of paddy fields you want to buy?\u201d ... Lian Fangzhou just smiled and said, \u201cAbout two thousand acres of \u2205!\u201d) |\\n| Substitution | \u5468\\n\u4e8c\\n\u665a\\n\u4e5d\\n\u65f6\\n\u6211\\n\u4eec\\n\u89c1\\n\u5230\\n\u4e86\\n\u8fc8\\n\u514b\\n\u3002\u5f53\\n\u65f6\\n\u6211\\n\u4eec\\n\u9080\\n\u8bf7\\n\u4ed6\\n\u51fa\\n\u5e2d\\n\u90a3\\n\u4e2a\\n\u665a\\n\u4f1a\\n\u3002 (We met Mike at nine o'clock on Tuesday evening. At that time, we invited him to the party.) |\\n| Reference  | \u4e0d\\n\u8fc7\\n\u5927\\n\u536b\\n\u5374\\n\u662f\\n\u6bdb\\n\u9aa8\\n\u609a\\n\u7136...\\n\u4ed6\\n\u7acb\\n\u5373\\n\u505c\\n\u6b62\\n\u4e86\\n\u60f3\\n\u8bf4\\n\u51fa\\n\u66f4\\n\u591a\\n... (However, David was horrified... He immediately stopped wanting to say more...) |\\n| Conjunction| \u9648\\n\u65ed\\n\u5fc3\\n\u91cc\\n\u6709\\n\u4e9b\\n\u7591\\n, ... \u4e0d\\n\u8fc7\\n, \u8fd9\\n\u65f6\\n\u5019\\n\u51fa\\n\u8a00\\n\u9876\\n\u649e\\n, \u663e\\n\u7136\\n\u662f\\n\u4e0d\\n\u660e\u667a\\n\u7684\\n\u3002 (Chen Xu was somewhat doubtful, ... however, it was obviously unwise to contradict at this time.) |\"}"}
{"id": "GAXedKmbFZ", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The illustration of the proposed translation tasks in terms of discourse properties and task definition. As seen, a variety of elements may be omitted in the Chinese input but should be recalled in English translation. NT mainly deals with zero pronouns while CCT needs to further tackle omitted connective words that are the marker of discourse structure. PT is the most difficult task because even prepositions could be further omitted. English translation is in Appendix \u00a7A.1.\\n\\n2.2 **Language Translation**\\n\\nLanguage translation is a sequence-to-sequence generation task to translate text from one language to another. Context information is important for document-level translation to produce cohesive and coherent translations (Wang et al., 2017; Bawden et al., 2018). As shown in Figure 3, we design three translation tasks of increasing hardness, which differ in the conciseness of source sentences in Chinese. The more concise the Chinese text, the more discourse information is needed for translation. We report BLEU, TER, METEOR and COMET for measuring models' translation quality.\\n\\n**NT (Novel Translation)**\\n\\nThe significant challenges for translating novels are entity consistency, anaphora resolution, and lexical choice (Matusov, 2019). We build a document-level Chinese-English corpus, which is extracted from web fictions. Specifically, we crawl 45,134 chapters in 152 books from web fiction websites, covering 14 genres such as fantasy science and romance. We manually align them at both document and sentence levels. Different from previous document-level MT datasets such as LDC and OpenSubtitle from the news and movie subtitle domains, ours is the first literature-domain MT corpus containing richer linguistic phenomena especially in discourse.\\n\\n**CCT (Classical Chinese Translation)**\\n\\nClassical Chinese is a traditional style of written Chinese used in China until the early 20th century, making it different from any modern spoken form of Chinese. Compared with modern Chinese as in novel translation, classical Chinese texts are extremely concise and compact by often dropping subjects and objects when a reference to them is understood, which require discourse information for information recovery. We construct a document-level...\"}"}
{"id": "GAXedKmbFZ", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Classical-Modern Chinese translation dataset, extracted from Chinese classics across history branch. Different from the NiuTrans corpus that has no context, ours maintain the original context.\\n\\nPT (Poetry Translation) Poetry translation is regarded as one of the hardest tasks in computational linguistics, or even artificial intelligence in general (Genzel et al., 2010; Ghazvininejad et al., 2018). Chinese poetry is even more concise than classic Chinese with implicit coherence, which is generally reflected through situational context and contextual context. For example, Chinese poetry does not use any cohesive means, but the semantic is still clear. We build a document-level Chinese Poetry to Modern English translation corpus, covering different types of Chinese poetry (e.g. Shi, Ci, Qu, and Fu) translated by famous translators.\\n\\n2.3 LANGUAGE GENERATION\\n\\nLanguage generation is a sequence generation task to produce text based on a given context (Reiter & Dale, 1997). Generating long and coherent text is an important but challenging task, particularly on lexical cohesion (Wanner, 1996; Guan et al., 2021). As shown in Figure 4, we design three representative generation tasks that differ in degrees of freedom. The more open-ended the generation task, the more difficult to generate accurate cohesive devices and discourse structure.\\n\\nTE (Text Expansion) We define a new task: given a predefined text, the goal of TE is to insert appropriate words, phrases, or clauses for adding more details and deepening the meaning, while retaining coherence and cohesiveness. We use a semi-automatic generation method to obtain large-scale training data. Specifically, we use the Stanford Parser to produce the syntactic tree of a text, and then manually design some rules to delete the modifier words and phrases in the text. We use the remaining words as the input and predict the dropped modifier. Since some delete operations may produce ill-formed text, we filter out the training instances if the remaining text has a large perplexity measured by a language model. In order to retain the coherence and meaning of the source document, the expanded parts in the target text tends to be modifier phrases or clauses. We use BLEU and PPL metrics to measure the lexical and semantic similarities and fluency.\"}"}
{"id": "GAXedKmbFZ", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nTI (Text Infilling)\\nIt aims to predict a text snippet given its surrounding context (Zhu et al., 2019).\\nTo evaluate the discourse-level model capability, we focus on the sentence infilling task that predicts a missing bridge sentence \\\\( x_0 \\\\) given two preceding sentences \\\\( x_{-2} \\\\) and \\\\( x_{-1} \\\\) and two subsequent sentences \\\\( x_1 \\\\) and \\\\( x_2 \\\\) (Huang et al., 2020; Cai et al., 2020). We build a new TI dataset by extracting consecutive 5-sentence paragraphs from Chinese web fictions used in the NT task. To evaluate different models, we take the following automatic metrics: Perplexity (PPL), BLEU (Papineni et al., 2002), BERTscore (Zhang et al., 2019a) and diversity scores (Dist-2/4) (Li et al., 2016). We report degree of diversity by calculating the ratio of distinct 2-grams/4-grams in generated text.\\n\\nTC (Text Completion)\\nThe task is to predict a writing continuation given a preceding prompt.\\nWe focus on multi-sentence paragraph completion for a target evaluation of discourse modeling, which completes a multi-sentence paragraph \\\\( x_s \\\\) given its leading sentence \\\\( x_{s-1} \\\\). We use the same data collected for the TI task to construct the TC dataset. Specifically, given a sentence \\\\( x_{-2} \\\\), we aim to predict the concatenation of \\\\( x_{-1}, x_0, x_1, \\\\) and \\\\( x_2 \\\\). We use the same metrics as TI task.\\n\\n2.4 HUMAN VALUATION ON BENCHMARK QUALITY\\n\\nTable 2: Human evaluation on the benchmark quality. We also report the inter-annotator agreement (in bracket) for the translation and generation tasks.\\n\\n| Task          | Expert Evaluation | Agreement |\\n|---------------|-------------------|-----------|\\n| SI            |                   | 0.76      |\\n| ZPR           |                   | 0.91      |\\n| MRC           |                   | 0.97      |\\n| Fluency Adequacy |                |           |\\n| NT            | 4.9 (0.60)        | 4.7 (0.78) |\\n| CCT           | 4.9 (0.65)        | 4.9 (0.55) |\\n| PT            | 4.7 (0.63)        | 4.4 (0.69) |\\n| TE            | 4.0 (0.51)        | 4.1 (0.51) |\\n| TI            | 4.3 (0.63)        | 4.4 (0.55) |\\n| TC            | 4.3 (0.63)        | 4.4 (0.55) |\\n\\nWe assess the quality of our benchmark, as listed in Table 2. For the language understanding testsets that require human annotations, we follow Mitani et al. (2017) to calculate the inter-annotator agreement via Cohen's kappa (0 \\\\( \\\\sim \\\\) 1). The annotators reach high agreement on the testsets of understanding tasks, especially on the MRC testset, which annotates the correct answer from 2 \\\\( \\\\sim \\\\) 4 choices.\\n\\nFor translation and generation testsets, we randomly choose 100 instances for each task, and ask two human annotators to assess their quality in terms of fluency (1 \\\\( \\\\sim \\\\) 5) and adequacy/coherence (1 \\\\( \\\\sim \\\\) 5). We follow Kreutzer et al. (2018); Popovic (2021) to calculate inter-annotator agreement via Krippendorff's \\\\( \\\\alpha \\\\) (0 \\\\( \\\\sim \\\\) 1) (Krippendorff, 2013). All outputs are fluent and highly correlated with the input sentences (i.e. \\\\( > 4 \\\\)) with reasonable agreement, showing that our benchmark has high quality.\\n\\n3 DISCO-BENCH DIAGNOSTIC TEST SUITE\\nThe general-purpose automatic metrics (e.g. BLEU and PPL) may be not sufficient to distinguish model performance in terms of discourse knowledge (Wong & Kit, 2012; M\u00fcller et al., 2018; Voita et al., 2018; 2019; Lin et al., 2011). To better measure the ability of models on discourse modeling, we handcraft a discourse-aware test suite that is complementary to general evaluation.\\n\\nDefinition and Annotation\\nWe adapt the idea of contrastive testing in our approach (Bawden et al., 2018; Voita et al., 2019; Cai & Xiong, 2020; He et al., 2022). We craft a test suite that encompasses 6 cohesion properties (i.e. Repetition, Synonyms, Ellipsis, Substitution, Conjunction) for both English and Chinese languages. The detailed definition and examples are listed in Appendix \u00a7A.2.\\n\\nContrastive Testing\\nTable 3 provides examples of how we formulate contrastive pairs for different tasks. Each instance in our methodology comprises a contrastive pair, consisting of a correct and an incorrect input/hypothesis based on cohesion properties. The original content from the test set serves as the correct candidate, while we introduce variations by altering its discourse devices, creating the incorrect candidates. We select one representative task from each type of Disco-Bench Benchmark. Accordingly, we adopt diverse strategies which vary based on the location of modification:\\n\\n- **MRC (Understanding)**: To generate an incorrect candidate, we introduce noise into the input, transforming it from \\\\( x \\\\) to \\\\( x' \\\\), while keeping the hypothesis \\\\( y \\\\) constant. Thus, each instance contains 6\"}"}
{"id": "GAXedKmbFZ", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: The illustration of the proposed test suite. We design each contrastive instance with correct and incorrect discourse markers in terms of cohesion and coherence. Tested systems are asked to rank candidates according to their model scores.\\n\\n| Type          | Input                                                                 | Hypothesis                                                                 |\\n|---------------|------------------------------------------------------------------------|-----------------------------------------------------------------------------|\\n| **Understanding Task: MRC (Machine Reading Comprehension)** | **Context**: \u5c0f\u516c\u4e3b\u722c\u51fa\u57ce\u5821\u3002 (**The little princess escaped from the castle.**) | **Correct**: \u6700\u540e\u5979\u8eb2\u8fdb\u4e86\u68ee\u6797\u3002 (**In the end she hid in the forest.**) |\\n|               | **Incorrect**: \u7136\u800c\u5979\u8eb2\u8fdb\u4e86\u68ee\u6797\u3002 (**However, she hid in the forest.**)      |                                                                            |\\n| **Translation Task: NT (Novel Translation)**               | **Refe. Context**: \u5b9a\u738b\u542b\u7b11\u770b\u7740\u6e05\u971c\u3002 (**King Ding looked at Qingshuang with a smile.**) | **Current**: \u4ed6\u89c9\u5f97\u6e05\u971c\u5f88\u6ed1\u7a3d\u3002 (**He thinks Qingshuang is funny.**) |\\n|               | **Correct**: \u4ed6\u89c9\u5f97 Qingshuang \u662f\u6ed1\u7a3d\u3002 (**He thinks Qingshuang is funny.**) |                                                                            |\\n|               | **Incorrect**: \u5979 think the Qing-shuang is funny. (**She think the Qing-shuang is funny.**) |                                                                            |\\n| **Generation Task: TC (Text Completion)**                 | **Repe. Context**: \u53f6\u8fdc\u7684\u53f3\u81c2\u878d\u5408\u4e86\u6d2a\u8352\u9f99\u9aa8\u3002 (**Ye Yuan's right arm fused with the primordial dragon bone.**) | **Correct**: \u4f46\u53f6\u8fdc\u611f\u89c9\u81ea\u5df1\u7684\u53f3\u81c2\u5feb\u8981\u65ad\u4e86\u3002 (**But Ye Yuan felt as if his right arm was about to break.**) |\\n|               | **Incorrect**: \u4f46\u53f6\u8fdc\u611f\u89c9\u81ea\u5df1\u7684\u5de6\u624b\u81c2\u8981\u65ad\u4e86\u3002 (**But Ye Yuan felt as if his left hand was about to break.**) |                                                                            |\\n|               | **This one punch's power, actually is too strong!** (**The power of this punch is too strong!**) |                                                                            |\\n\\nWe then calculate the probability of the golden label by inputting these into the relevant models.\\n\\n- **NT (Translation)**: We introduce noise into the target translation to generate an incorrect candidate, transitioning $y$ to $y'$, while the source input $x$ remains unaltered. Each instance hence contains a correct $(x, y)$ and an incorrect $(x, y')$ candidate. Given the input and hypothesis, we calculate the probability of the hypothesis sequence using a forced-decoding method.\\n\\n- **TC (Generation)**: Similar to the MRC task, we introduce noise into the input while the hypothesis remains unchanged. By combining the input and hypothesis, we directly calculate the probability of the entire sequence.\\n\\nIn conclusion, we have annotated a total of 250 instances for the MRC task, 500 for the NT task, and 250 for the TC task, each marked with 6 different types of cohesion. Given each instance, we assess different models on their ability to rank the correct candidate higher than the incorrect one.\"}"}
{"id": "GAXedKmbFZ", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We evaluate the following public pretrained models on Disco-Bench Benchmark and Test Suite:\\n\\n- **BERT (base)**: we use the base model (12 layer encoder, hidden size 768, vocabulary size 21128) published by Devlin et al. (2019), which was pretrained on Chinese Wikipedia dump of about 0.4 billion tokens using the losses of mask language model (MLM) and next sentence prediction.\\n\\n- **RoBERTa (base)**: Cui et al. (2020) a model with the same architecture of BERT (base) except it uses whole word masking and is trained on additional 5 billion tokens with only MLM pretrained task. This model uses BERT (base) as the initial weight.\\n\\n- **RoBERTa (large)**: Cui et al. (2020) the large model size of RoBERTa model (24 layer encoder, hidden size 1024, vocabulary size 21128). This model has the same training procedure of RoBERTa-wwm-ext (base). This model is trained from scratch.\\n\\n- **AnchiBERT**: Tian et al. (2021) a model continues pretraining based on the BERT (base) model with the 39.5M anchient Chinese tokens. It uses the same tokenizer and other techniques as BERT-base.\\n\\n- **MengziBERT**: Zhang et al. (2021) a model initial on the RoBERTa (base) (Liu et al., 2019b) with special-designed objectives.\\n\\n- **BART (large)**: Shao et al. (2021) train a large model (12 layer encoder and decoder, hidden size 1024, vocabulary size 21128) with denoising auto-encoding (DAE) objective. This model is trained on the open source large-scale raw text, Chinese Wikipedia, and a part of WuDaoCorpus. The training data contains 200GB cleaned text ranging from different domains.\\n\\n- **mBART (CC25)**: Pires et al. (2019) use a large model (12 layer encoder and decoder, hidden size 1024, vocabulary size 250,000), trained with 25 language web corpus. This model is trained from scratch.\\n\\n- **GPT2**: Zhao et al. (2019) train a 12-layer decoder-only Transformers and its vocabulary is size 21,128. This model is trained with the CLUECorpusSmall corpus.\\n\\n- **GPT-3.5 & GPT-4**: ChatGPTis an intelligent chatting machine developed by OpenAI upon the InstructGPT (Ouyang et al., 2022), which is trained to follow an instruction in a prompt and provide a detailed response. All corresponding results were obtained from ChatGPT API in June 2023.\\n\\nThe fine-tuning hyper-parameters are detailed in Table 8. Table 9 showcases the ChatGPT's prompts used for the Disco-Bench Benchmark tasks.\\n\\n**Table 8: A summary of hyper-parameter for fine-tuning downstream tasks.**\\n\\n| Task | Batch Size | Max Length | Epoch | Learning Rate |\\n|------|------------|------------|-------|---------------|\\n| SI   | 64         | 512        | 5     | 3e-5          |\\n| ZPR  | 5          | 512        | 40    | 5e-6          |\\n| MRC  | 6          | 512        | 10    | 2e-5          |\\n| NT   | 3K token   | 1024       | 30K step | 1e-4         |\\n| ACT  | 3K token   | 1024       | 30K step | 1e-4         |\\n| PT   | 3K token   | 1024       | 30K step | 1e-5         |\\n| TE   | 32         | 512        | 3     | 2e-4          |\\n| TI   | 24         | 64         | 3     | 2e-5          |\\n| TC   | 24         | 512        | 8     | 2e-5          |\\n\\n**Table Notes:**\\n\\n7 [https://huggingface.co/bert-base-chinese](https://huggingface.co/bert-base-chinese).\\n8 [https://huggingface.co/hfl/chinese-roberta-wwm-ext/tree/main](https://huggingface.co/hfl/chinese-roberta-wwm-ext/tree/main).\\n9 [https://huggingface.co/hfl/chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext).\\n10 [https://github.com/ttzHome/AnchiBERT](https://github.com/ttzHome/AnchiBERT).\\n11 [https://huggingface.co/Langboat/mengzi-bert-base](https://huggingface.co/Langboat/mengzi-bert-base).\\n12 [https://huggingface.co/fnlp/bart-base-chinese](https://huggingface.co/fnlp/bart-base-chinese).\\n13 [https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.cc25.v2.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.cc25.v2.tar.gz).\\n14 [https://github.com/CLUEbenchmark/CLUECorpus2020](https://github.com/CLUEbenchmark/CLUECorpus2020).\\n15 [https://platform.openai.com](https://platform.openai.com).\"}"}
{"id": "GAXedKmbFZ", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: The prompt for evaluating ChatGPT.\\n\\nC represents the context for machine reading, SRC and T GT denote source and target languages, respectively. \\n\\nD represents a document contains several sentences. T1 . . . Tm refer to the translation candidates, where only one of them is a positive translation and the others are negative due to the modification of discourse-specific words.\\n\\nTask Prompt\\n\\nDisco-Bench Benchmark\\n\\nSI In this cloze reading comprehension task, I will input a passage of text and a sentence, and you will need to find relevant information from the text and determine the speaker of the sentence. Passage: P, Question: Q, Speaker: ZPR\\n\\nThe zero-anaphora recovery task is to restore the expression of omitted pronouns in terms of position and form based on the anaphoric information in the sentence. Please restore the original sentence with <> as the marker. If there is no zero-anaphora phenomenon, output \\\"none.\\\"\\n\\nMRC Answer the following multiple-choice questions. Choose A, B, C, or D as the final answer. \\\"Content\\\": C, \\\"Question\\\": Q, \\\"Choices\\\": [C1, C2, C3, C4], \\\"Answer\\\": \\n\\nNT Translate the given Chinese into English.\\n\\nCCT Translate this ancient text into modern Chinese.\\n\\nPT Translate the given Chinese into English.\\n\\nTE given a predefined text, the goal of TE is to insert appropriate words, phrases, or clauses for adding more details and deepening the meaning, while retaining coherence and cohesiveness.\\n\\nTI The purpose of the text filling task is to predict text fragments based on context. The input includes the two sentences before and after the target sentence. Please output the target sentence. S\u22122, S\u22121, S1, S2\\n\\nTC Based on the given context, the text completion task requires outputting the next four sentences. S\u22122, S\u22121\\n\\nDisco-Bench Cohesion Test Suit\\n\\nMRC Output the model's confidence for the answer based on the content and corresponding answer of the following multiple-choice reading comprehension. Answer the confidence for the following multiple-choice questions. Choose A, B, C, or D as the final answer. \\\"Content\\\": C, \\\"Question\\\": Q, \\\"Choices\\\": [C1, C2, C3, C4], \\\"Answer\\\": Cx, \\\"Confidence\\\": \\n\\nNT According to the Chinese text, which of the following is the correct English translation? Please output the correct translation's corresponding number. Chinese: D English: [T1, T2, ..., Tm]. Correct translation number: \\n\\nTC Given the Chinese text, please evaluate the following sentences based on cohesion and fluency, and output the corresponding number of the optimal sentences: [S1, S2, ..., Sm].\\n\\nA.4\"}"}
{"id": "GAXedKmbFZ", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Statistics of data for Disco-Bench pretraining. All data are extracted from literature texts with discourse context. We count number of characters in Chinese and number of words in English.\\n\\n| Category    | Genre       | Size   | Description          |\\n|-------------|-------------|--------|----------------------|\\n|             |             | # Doc. | # Sentence          |\\n| Chinese     | Electronic Novel | 91,620,211 | 1,169,127,191 |\\n|             | Modernist Classical | 38,495,887 | 490,733,235 |\\n|             | Book        | 324,912 | 4,141,874 |\\n|             | Ancient Poetry | 378,323 | 1,495,466 |\\n|             | Couplet    | 8,979,186 | 8,979,186 |\\n|             | Classical   | 1,011 | 1,947,136 |\\n|             | Others      | 452,715 | 4,952,039 |\\n|             |             | 140,327,150 | 1,717,564,050 |\\n| English     | Electronic Novel | 33,156,134 | 422,757,234 |\\n|             | Modernist Classical | 3,104,507 | 39,593,119 |\\n|             | Book        | 324,912 | 4,162,821 |\\n|             | Ancient Poetry | 2,269 | 21,456 |\\n|             | Others      | 3,088,688 | 110,268,328 |\\n|             |             | 39,844,197 | 650,330,076 |\\n\\nTable 11: Summary of pretrained models varying in model architecture, parameter scale, training data, and targeted task (i.e. understanding, translation, and generation). \\n\\n| #1 | Model Language | Size | Task | Corpus |\\n|----|----------------|------|------|--------|\\n|    | BERT (base) zh | 110M | U, T | 1.5GB Wiki |\\n|    | RoBERTa (base) zh | 110M | U, T | 15GB Wiki, EXT Corpus |\\n|    | RoBERTa (large) zh | 340M | U, T | 15GB Wiki, EXT Corpus |\\n|    | AnchiBERT (base) zh | 102M | U, T | Classical Chinese |\\n|    | MengziBERT (base) zh | 103M | U, T | 300GB Wiki, Common Crawl |\\n|    | BART (large) zh, en | 406M | U, T, G | 200GB Wiki, WuDao Corpus |\\n|    | mBART (CC25) zh, en, etc. | 610M | T | 1.4TB Common Crawl |\\n|    | GPT2 (base) zh | 102M | G | 14GB CLEU Corpus |\\n|    | GPT2 (large) en | 762M | G | 40GB Web Text |\\n|    | T5 (base) zh | 231M | G | 14GB CLEU Corpus |\\n|    | T5 (large) en | 770M | G | 745GB C4 |\\n|    | Disco-Bench (family) zh, en | \u2013 | U, T, G | 400GB Literature |\\n\\nFine-grained Pretraining with Disco-Bench Training Data\\nThe pretraining hyper-parameters details of the Disco-Bench models can be found in Table 12.\"}"}
{"id": "GAXedKmbFZ", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12: The summary of hyper-parameters used for Disco-Bench pretrained models.\\n\\n| Model     | Tokenization | Optimizer | Masking | Vocabulary Size | Learning Rate | Batch Size | Training Step | Max Length | Layer | Head | Total Param. |\\n|-----------|--------------|-----------|---------|----------------|--------------|------------|---------------|------------|-------|------|--------------|\\n| RoBERTa   | BERTtok.     | Adam      | word    | 21128         | 3e-4         | 4K         | 1M            | 512       | 12/24 | 12/16 | 110m/340m   |\\n| GPT2      | BERTtok.     | Adam      | word    | 21131         | 3e-4         | 4K         | 1M            | 1024      | 20    | 36    | 737M         |\\n| BART      | BERTtok.     | Adam      | word    | 21128         | 3e-4         | 4K         | 1M            | 512       | 24    | 16    | 406M         |\\n| mBART     | SentPiece    | Adam      | word    | 250000        | 3e-4         | 4K         | 1M            | 1024      | 12/24 | 12/16 | 669M         |\\n\\nA.5 RESULTS ON ADDITIONAL EVALUATION METRICS\\n\\nA single automatic evaluation metric might not provide a comprehensive depiction of a model's performance. We report the results on several additional evaluation metrics.\\n\\nUnderstanding Tasks\\n\\nTable 13 presents additional evaluation metrics for understanding tasks, including Exact Match (whether the system's response exactly matches the correct answer) for SI and both Precision (how many of the predicted positive responses were actually positive) and Recall (how many of the actual positive responses were correctly identified by the system) for ZPR. The performance of the Disco-Bench pretrained RoBERTa (large) model according to additional metrics is consistently superior and comparable to the other models. This corroborates our conclusions drawn from the main evaluation metrics. Notably, the existing pretrained RoBERTa (large) model shows the highest Precision at 39.3 on the ZPR task.\\n\\nTable 13: More results on understanding tasks using additional evaluation metrics, including Exact Match, Precision, and Recall. This is complementary to Table 4.\\n\\n| Model          | SI  | ZPR |\\n|----------------|-----|-----|\\n|                | Exact Match | \u2191  | Precision | \u2191  | Recall | \u2191  |\\n| Plain Models   |     |     |     |     |     |     |\\n| Transformer (base) | 0.3 | 10.2 | 11.5 |\\n| Transformer (big) | 0.1 | 10.5 | 11.9 |\\n| Existing Pretrained Models |     |     |     |     |     |     |\\n| BERT (base)    | 81.9 | 26.1 | 31.0 |\\n| AnchiBERT      | 76.9 | 22.1 | 24.6 |\\n| MengziBERT     | 84.0 | 36.6 | 29.6 |\\n| RoBERTa (base) | 83.4 | 29.0 | 29.9 |\\n| RoBERTa (large)| 85.9 |     |     |     |     |     |\\n| BART (large)   | 83.7 | 38.3 | 30.2 |\\n| Disco-Bench Pretrained Models |     |     |     |     |     |     |\\n| RoBERTa (base) | 85.2 | 32.0 | 30.6 |\\n| RoBERTa (large)| 87.2 | 38.7 | 30.8 |\\n| BART (large)   | 84.6 | 39.0 | 30.5 |\\n\\nTranslation Tasks\\n\\nTable 14 provides supplementary evaluation metrics for translation tasks, comprising TER (measuring the number of edits required to change a system's output into one of the references), METEOR (considering precision and recall, synonymy, stemming, and phrase-level matches to create an F-score-like composite of these factors), and COMET (a learned metric trained on human translation ranking data, which captures more nuanced, semantic comparisons and is less reliant on surface-level text matches). Notably, there are no resources available for Classical Chinese in the METEOR evaluation.\\n\\nWhen observing the performance across NT and PT tasks, the\"}"}
{"id": "GAXedKmbFZ", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4: Performance of baseline models on Disco-Bench benchmark. A similar table is presented on the online platform.\\n\\n| Model Understanding Translation Generation | SI  | ZPR  | MRC  | NT  | CCT  | PT  | TE  | BERTscore |\\n|------------------------------------------|-----|------|------|-----|------|-----|-----|-----------|\\n| Plain Models                             |     |      |      |     |      |     |     |           |\\n| Transformer (base)                       | 9.1 | 10.8 | 38.2 | 22.1| 32.5 | 4.3 | 24.9| 58.1      |\\n| Transformer (big)                        | 4.4 | 11.1 | 38.7 | 22.5| 33.5 | 4.3 | 29.6| 58.5      |\\n| Existing Pretrained Models               |     |      |      |     |      |     |     |           |\\n| BERT (base)                              | 85.1| 24.5 | 51.6 | 22.8| 42.5 | 6.1 | -  | -         |\\n| AnchiBERT (base)                         | 81.3| 23.2 | 46.3 | 22.1| 42.6 | 6.1 | -  | -         |\\n| MengziBERT (base)                        | 86.9| 31.2 | 51.0 | 21.2| 42.3 | 5.5 | -  | -         |\\n| RoBERTa (base)                           | 86.3| 28.5 | 51.0 | 21.9| 42.3 | 5.8 | -  | -         |\\n| RoBERTa (large)                          | 88.7| 33.0 | 55.9 | 20.8| 44.2 | 5.7 | -  | -         |\\n| GPT-2                                    | -   | -    | -    | -   | -    | -   | 30.0| 59.4      |\\n| BART (large)                             | 86.5| 32.8 | 50.2 | 21.7| 43.3 | 7.1 | 33.8| 62.2      |\\n| mBART (CC25)                             | -   | -    | -    | 24.0| -    | 12.6| -  | -         |\\n| Disco-Bench Pretrained Models            |     |      |      |     |      |     |     |           |\\n| RoBERTa (base)                           | 87.7| 31.2 | 50.0 | 22.8| 46.6 | 6.6 | -  | -         |\\n| RoBERTa (large)                          | 89.6| 34.3 | 56.7 | 21.6| 44.0 | 7.2 | -  | -         |\\n| GPT-2                                    | -   | -    | -    | -   | -    | -   | 32.5| 59.7      |\\n| BART (large)                             | 86.6| 33.5 | 50.3 | 23.2| 43.8 | 7.1 | 36.2| 62.4      |\\n| mBART (CC25)                             | -   | -    | -    | 24.3| -    | 13.9| -  | -         |\\n| Large Language Models                    |     |      |      |     |      |     |     |           |\\n| GPT-3.5                                  | 78.7| 13.5 | 48.6 | 22.5| 22.2 | 8.1 | 24.2| 59.7      |\\n| GPT-4                                    | 84.9| 9.7 | 63.2 | 24.0| 27.6 | 9.1 | 27.1| 60.4      |\\n\\nchoose the hyper-parameters based on the performance on the validation set for each model. We fine-tune each model twice and report the averaged test results. We use few-shot for testing ChatGPT. The fine-tuning hyper-parameters and ChaGPT's instructions are detailed in Appendix \u00a7A.3.\\n\\nDisco-Bench Pretrained Models\\n\\nWe present an extensive Disco-Bench training dataset (400GB), consisting of both Chinese and English texts, designed to align with the benchmark's literature domain. The frequencies and types of discourse phenomena vary in different domains (Yang et al., 2015), leading to differences in model behavior and quality across domains. However, most existing pretrained models are trained on non-literature data (e.g. Wikipedia). To fill the gap, we follow Wang et al. (2022) to train the existing pretraining models (coarse-grained pretraining) on our Disco-Bench training data (fine-grained pretraining) to enhance context modelling. Specifically, we use the existing pretrained models for weight initialization, and further train the models on the Disco-Bench training data with the same loss. More details on data and training settings are described in Appendix \u00a7A.4.\\n\\n4.2 Results\\n\\nTable 4 lists the results on the proposed benchmarks, using main evaluation metrics (results on additional evaluation metrics are detailed in Appendix \u00a7A.5). Concerning the existing pretrained models, pretraining improves performance over plain models in all tasks, which is consistent with previous studies. These results validate that the proposed benchmarks are reasonable. We evaluated the encoder-only architecture on tasks involving comprehension and translation. We also assessed the decoder-only architecture on tasks requiring generation, and the encoder-decoder architecture on all tasks. The reason some architectures were not tested on certain tasks is due to our preliminary experiences showing subpar performance in those particular tasks.\\n\\nAmong the BERT variants with the base setting, AncientBERT trained on small-scale classical Chinese data outperforms other models on CCT and PT, demonstrating the necessity of bridging the domain gap. Enlarging the model capacity usually improves performance (e.g. RoBERTa from base to large setting). The GPT-2 model exhibits superior performance on TE and TI tasks compared...\"}"}
{"id": "GAXedKmbFZ", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Results of selected models on Disco-Bench cohesion test suite. We assess models on their ability to rank the correct candidate higher than the incorrect one according to model score. We report overall accuracy (%).\\n\\n| Type      | Models          | Rep. Syn. | Con. | Ref. | Sub. | Ell. |\\n|-----------|-----------------|-----------|------|------|------|------|\\n| Understanding (MRC) | RoBERTa (large) | 66.7      | 61.4 | 68.0 | 64.0 | 69.8 |\\n|           | + Disco-Bench Pretrain | 68.8      | 66.3 | 63.4 | 58.3 | 59.5 |\\n|           | GPT-3.5         | 27.1      | 38.6 | 33.5 | 25.8 | 49.2 |\\n|           | GPT-4           | 31.3      | 24.1 | 21.0 | 21.6 | 39.7 |\\n| Translation (NT) | mBART (CC25)   | 94.0      | 85.3 | 92.7 | 95.9 | 83.3 |\\n|           | + Disco-Bench Pretrain | 96.0      | 88.2 | 95.0 | 96.7 | 86.7 |\\n|           | GPT-3.5         | 32.0      | 59.4 | 24.4 | 26.0 | 44.8 |\\n|           | GPT-4           | 62.0      | 85.3 | 45.1 | 71.6 | 58.6 |\\n| Generation (TC) | BART (large)   | 89.5      | 60.0 | 91.4 | 81.9 | 50.0 |\\n|           | + Disco-Bench Pretrain | 90.8      | 84.0 | 94.3 | 84.5 | 56.0 |\\n|           | GPT-3.5         | 26.3      | 16.0 | 11.4 | 10.3 | 25.0 |\\n|           | GPT-4           | 60.5      | 52.0 | 11.4 | 50.9 | 37.5 |\\n\\nto the plain Transformer model, but its performance is inferior on the TC task. The BART model excels in all generation tasks, underscoring the efficacy of the encoder-decoder architecture in such tasks. Pre-training with multilingual data, such as in the mBART model, can yield a more substantial improvement in translation quality than BART, particularly evident in NT and PT tasks.\\n\\nClearly, fine-grained pretraining on Disco-Bench training data outperforms their coarse-grained counterparts, demonstrating the effectiveness and necessity of modeling discourse information. The RoBERTa models work better on language understanding tasks, and the BART variants produce superior performances on the language translation and generation tasks. Although ChatGPT has shown substantial proficiency in long-text NLP tasks, it does not quite measure up to the performance of Disco-Bench's pretrained models across the majority of Disco-Bench tasks. These results underline the challenge and the necessity of our proposed benchmark.\\n\\n4.3 RESULTS ON DIAGNOSTIC TEST SUITE\\nWe evaluate three existing pretraining models on the diagnostic dataset: RoBERTa (large), BART (large), and mBART (CC25), each of which has exhibited superior performance on their respective representative tasks. \\\"+ Disco-Bench Pretrain\\\" donates fine-grained pretraining on Disco-Bench data specific to each model. Subsequently, every model is fine-tuned using the training data derived from the corresponding downstream task.\\n\\nTable 5 records the model's ability to rank a correct candidate higher than an incorrect one, revealing an overall accuracy percentage. Disco-Bench pretrained models generally improve the cohesion accuracies over their coarse-grained counterparts, which reconfirms our claim that fine-grained pretraining on Disco-Bench data helps model discourse information. Although the numbers are not comparable across tasks, we find that pretraining models on the understanding tasks generally perform worse on discourse modeling. One possible reason is that the understanding tasks are mostly classification tasks, whose signals may not be sufficient to guide models to learn discourse information. The results on GPT-3.5 and GPT-4 reveal a significant performance gap between LLMs and those pretrained with Disco-Bench data, emphasizing the challenge of capturing discourse information.\\n\\n5 CONCLUSION\\nThis paper introduces a benchmark for Chinese and/or English that can evaluate intra-sentence properties across various NLP tasks, covering understanding, translation, and generation. We also propose a diagnostic test suite that can examine whether the target models learn discourse knowledge for in-depth linguistic analysis. Extensive experiments demonstrate that fine-grained pretraining based on document-level training data consistently improves the modeling of discourse information. We offer the datasets, pretrained models, and leaderboards to facilitate research in this field.\"}"}
{"id": "GAXedKmbFZ", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"REFERENCES\\n\\nMadeleine Bates. Models of natural language understanding. *Proceedings of the National Academy of Sciences*, 92(22):9977\u20139982, 1995.\\n\\nRachel Bawden, Rico Sennrich, Alexandra Birch, and Barry Haddow. Evaluating discourse phenomena in neural machine translation. In *NAACL*, 2018.\\n\\nDeng Cai, Yizhe Zhang, Yichen Huang, Wai Lam, and Bill Dolan. Narrative incoherence detection. *arXiv preprint arXiv:2012.11157*, 2020.\\n\\nXinyi Cai and Deyi Xiong. A test suite for evaluating discourse phenomena in document-level neural machine translation. In *Proceedings of the Second International Workshop of Discourse Processing*, pp. 13\u201317, 2020.\\n\\nMingda Chen, Zewei Chu, and Kevin Gimpel. Evaluation benchmarks and learning criteria for discourse-aware sentence representations. In *EMNLP-IJCNLP*, 2019.\\n\\nAlexis Conneau and Douwe Kiela. Senteval: An evaluation toolkit for universal sentence representations. In *LREC*, 2018.\\n\\nGuy Cook. *Discourse*. Oxford University Press, 1989.\\n\\nYiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guoping Hu. A span-extraction dataset for Chinese machine reading comprehension. In *EMNLP*, 2019.\\n\\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. Revisiting pre-trained models for Chinese natural language processing. In *EMNLP: Findings*, 2020.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In *NAACL*, 2019.\\n\\nDmitriy Genzel, Jakob Uszkoreit, and Franz Josef Och. \u201cpoetic\u201d statistical machine translation: Rhyme and meter. In *EMNLP*, 2010.\\n\\nMarjan Ghazvininejad, Yejin Choi, and Kevin Knight. Neural poetry translation. In *NAACL*, 2018.\\n\\nJian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wenbiao Ding, and Minlie Huang. Long text generation by modeling sentence-level and discourse-level coherence. In *ACL*, 2021.\\n\\nJian Guan, Zhuoer Feng, Yamei Chen, Ruilin He, Xiaoxi Mao, Changjie Fan, and Minlie Huang. LOT: A story-centric benchmark for evaluating Chinese long text understanding and generation. *TACL*, 2022.\\n\\nHua He, Denilson Barbosa, and Grzegorz Kondrak. Identification of speakers in novels. In *ACL*, 2013.\\n\\nJie He, Wanqiu Long, and Deyi Xiong. Evaluating discourse cohesion in pre-trained language models. In *Proceedings of the 3rd Workshop on Computational Approaches to Discourse*, pp. 28\u201334, 2022.\\n\\nYichen Huang, Yizhe Zhang, Oussama Elachqar, and Yu Cheng. INSET: Sentence infilling with INter-SEntential transformer. In *ACL*, 2020.\\n\\nPaul Mc Kevitt, Derek Partridge, and Yorick Wilks. Approaches to natural language discourse processing. *Artificial Intelligence Review*, 6(4):333\u2013364, 1992.\\n\\nFang Kong and Guodong Zhou. A tree kernel-based unified framework for Chinese zero anaphora resolution. In *EMNLP*, 2010.\\n\\nJulia Kreutzer, Joshua Uyheng, and Stefan Riezler. Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning. In Iryna Gurevych and Yusuke Miyao (eds.), *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics*, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pp. 1777\u20131788. Association for Computational Linguistics, 2018. doi: 10.18653/v1/P18-1165. URL https://aclanthology.org/P18-1165/.\"}"}
{"id": "GAXedKmbFZ", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nKlaus Krippendorff. Content analysis: An introduction to its methodology. Sage publications, 2013.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In ACL, 2020.\\n\\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In NAACL, 2016.\\n\\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Bruce Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Rangan Majumder, and Ming Zhou. XGLUE: A new benchmark dataset for cross-lingual pre-training, understanding and generation. CoRR, 2020.\\n\\nZiheng Lin, Hwee Tou Ng, and Min-Yen Kan. Automatically evaluating text coherence using discourse relations. In ACL, 2011.\\n\\nShanshan Liu, Xin Zhang, Sheng Zhang, Hui Wang, and Weiming Zhang. Neural machine reading comprehension: Methods and trends. Applied Sciences, 9(18):3698, 2019a.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019b.\\n\\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726\u2013742, 2020.\\n\\nEvgeny Matusov. The challenges of using neural machine translation for literature. In Proceedings of the qualities of literary machine translation, 2019.\\n\\nAya A Mitani, Phoebe E Freer, and Kerrie P Nelson. Summary measures of agreement and association between many raters' ordinal classifications. Annals of epidemiology, 27(10):677\u2013685, 2017.\\n\\nRuslan Mitkov. Anaphora resolution. Routledge, 2014.\\n\\nMathias M\u00fcller, Annette Rios, Elena Voita, and Rico Sennrich. A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation. In WMT, 2018.\\n\\nMyle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In WMT, 2018.\\n\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, et al. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, 2022.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: A Method for Automatic Evaluation of Machine Translation. In ACL, 2002.\\n\\nTelmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual bert? In ACL, 2019.\\n\\nMaja Popovic. Agree to disagree: Analysis of inter-annotator disagreements in human evaluation of machine translation output. In Arianna Bisazza and Omri Abend (eds.), Proceedings of the 25th Conference on Computational Natural Language Learning, CoNLL 2021, Online, November 10-11, 2021, pp. 234\u2013243. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.conll-1.18. URL https://doi.org/10.18653/v1/2021.conll-1.18.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1\u201367, 2020.\"}"}
{"id": "GAXedKmbFZ", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nDisco-Bench pretrained mBART model outshines all others across all three metrics, reinforcing its top-ranking performance as indicated by the BLEU scores. However, the metrics TER and COMET display inconsistent performances when applied to the CCT task, thereby illustrating the inherent challenges in evaluating such tasks.\\n\\nTable 14: More results on translation tasks using additional evaluation metrics, including TER, METEOR and COMET. This is complementary to Table 4.\\n\\n| Model          | CCT TE  | PT TE  | CCT TI  | PT TI  | CCT TC  | PT TC  |\\n|----------------|---------|--------|---------|--------|---------|--------|\\n|                |         |        |         |        |         |        |\\n| Transformer (base) | 74.3    | 20.6   | 0.74    | 98.5   | 0.65    | 114.1  |\\n| Transformer (big)  | 73.3    | 20.9   | 0.75    | 98.4   | 0.65    | 112.9  |\\n| Existing Pretrained Models\\n| BERT (base)     | 73.7    | 21.1   | 0.74    | 95.8   | 0.65    | 105.9  |\\n| AnchiBERT       | 74.1    | 20.7   | 0.74    | 95.9   | 0.67    | 100.1  |\\n| MengziBERT      | 76.5    | 20.5   | 0.74    | 96.0   | 0.67    | 105.5  |\\n| RoBERTa (base)  | 74.1    | 20.5   | 0.75    | 96.2   | 0.65    | 104.7  |\\n| RoBERTa (large) | 75.1    | 19.6   | 0.72    | 94.8   | 0.68    | 99.6   |\\n| BART (large)    | 75.6    | 21.1   | 0.74    | 96.5   | 0.65    | 100.8  |\\n| mBART (CC25)    | 71.9    | 22.2   | 0.77    | -      | -       | 88.2   |\\n| Disco-Bench Pretrained Models\\n| RoBERTa (base)  | 73.6    | 21.0   | 0.75    | -      | -       | 91.5   |\\n| RoBERTa (large) | 74.6    | 20.5   | 0.75    | 95.5   | 0.67    | 102.0  |\\n| BART (large)    | 72.0    | 21.2   | 0.76    | 96.7   | 0.70    | 100.0  |\\n| mBART (large)   | 70.8    | 22.8   | 0.78    | -      | -       | 84.6   |\\n\\nGeneration Tasks\\n\\nTable 15 introduces additional evaluation metrics for generation tasks, comprising PPL (perplexity is a measurement of how well a probability distribution or probability model predicts a sample), BLEU (evaluating the quality of text which has been machine-generated based on reference), and Dist-n (calculating the number of unique n-grams divided by the total number of n-grams in the generated text). As seen these metrics exhibit varying performances, highlighting the complexities and challenges associated with the automatic evaluation of generation tasks. Dist-2 and Dist-4 exhibit consistent performance in line with the primary metric, BERTscore. Conversely, the performances of PPL and BLEU metrics are notably unstable.\\n\\nTable 15: More results on generation tasks using additional evaluation metrics, including BLEU, PPL, Dist-2 and Dist-4. This is complementary to Table 4.\\n\\n| Model          | TE PPL | TI PPL | TC PPL | TE BLEU | TI BLEU | TC BLEU |\\n|----------------|--------|--------|--------|---------|---------|---------|\\n|----------------|--------|--------|--------|---------|---------|---------|\\n| Existing Pretrained Models\\n| BART (large)  | 63.1   | 3.7    | 8.4    | 0.20    | 0.63    | 2.7     |\\n| GPT-2          | 70.1   | 1.6    | 11.2   | 0.18    | 0.54    | 2.1     |\\n| Disco-Bench Pretrained Models\\n| BART (large)  | 49.2   | 3.7    | 8.8    | 0.19    | 0.65    | 2.9     |\\n| GPT-2          | 67.5   | 2.2    | 11.5   | 0.27    | 0.84    | 4.7     |\\n\\nA.6 RELATED WORK\\n\\nEvaluation benchmarks are important for developing deep learning models, which enable comparison between different models and probe models for understanding of specific linguistic phenomena. Conneau & Kiela (2018) collected SentEval containing several sentence-level classification tasks. We use GPT2 language model to compute PPL. For TI and TE tasks, we use 'IDEA-CCNL/Wenzhong-GPT2-110M'.\\n\\nWe refer to the original paper for the complete details and results.\"}"}
{"id": "GAXedKmbFZ", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024 to test the representational power of models. Closely related to this work, DiscoEval (Chen et al., 2019) extended these tasks to evaluate discourse-related knowledge in pretrained models. DiscoEval only evaluates sentence encoder with language understanding tasks in English. In contrast, we extend the tasks to a broader range of NLP tasks, which can evaluate different types of models (e.g., encoder-based BERT, decoder-based GPT, and encoder-decoder based mBART). In addition, our benchmarks cover both Chinese and English.\\n\\nGLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) included a wider variety of natural language understanding tasks, further examining the capabilities of the models and making the results comparable for multi-task learning. Followed researchers extend the benchmarks to other languages, such as CLUE (Xu et al., 2020) and LOT (Guan et al., 2022) in Chinese, and XGLUE (Liang et al., 2020) in multiple languages. While these works focus on evaluating inter-sentence information, our benchmark evaluates intra-sentence discourse phenomena that cross sentences.\\n\\nLOT (Guan et al., 2022) evaluates models' abilities to model long text but ignores discourse information.\"}"}
