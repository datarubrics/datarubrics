{"id": "TWTTKlwrUP0", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the past decade, there has been exponentially growing interest in the use of observational data collected as a part of routine healthcare practice to determine the effect of a treatment with causal inference models. Validation of these models, however, has been a challenge because the ground truth is unknown: only one treatment-outcome pair for each person can be observed. There have been multiple efforts to fill this void using synthetic data where the ground truth can be generated. However, to date, these datasets have been severely limited in their utility either by being modeled after small non-representative patient populations, being dissimilar to real target populations, or only providing known effects for two cohorts (treated vs control). In this work, we produced a large-scale and realistic synthetic dataset that supports multiple hypertension treatments, by modeling after a nationwide cohort of more than 250,000 hypertension patients' multi-year history of diagnoses, medications, and laboratory values. We designed a data generation process by combining an adapted ADS-GAN model for fictitious patient information generation and a neural network for treatment outcome generation. Wasserstein distance of 0.35 demonstrates that our synthetic data follows a nearly identical joint distribution to the patient cohort used to generate the data. Our dataset provides ground truth effects for about 30 hypertension treatments on blood pressure outcomes. Patient privacy was a primary concern for this study; the $\\\\epsilon$-identifiability metric, which estimates the probability of actual patients being identified, is 0.008%, ensuring that our synthetic data cannot be used to identify any actual patients. To demonstrate its usage, we tested the bias in causal effect estimation of five well-established models using this dataset. The approach we used can be readily extended to other types of diseases in the clinical domain, and to datasets in other domains as well.\"}"}
{"id": "TWTTKlwrUP0", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Douglas Almond, Kenneth Y. Chay, and David S. Lee. The costs of low birth weight. *The Quarterly Journal of Economics*, 120(3):1031\u20131083, 2005. URL https://EconPapers.repec.org/RePEc:oup:qjecon:v:120:y:2005:i:3:p:1031-1083.\\n\\nM. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In *International Conference on Machine Learning*, 2017.\\n\\nKeith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Paul Oka, Miruna Oprescu, and Vasilis Syrgkanis. EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation. https://github.com/microsoft/EconML, 2019. Version 0.x.\\n\\nAnat Reiner Benaim, Ronit Almog, Yuri Gorelik, Irit Hochberg, Laila Nassar, Tanya Mashiach, Mogher Khamaisi, Yael Lurie, Zaher S Azzam, Johad Khoury, et al. Analyzing medical research results based on synthetic data and their relation to real data results: systematic comparison from five observational studies. *JMIR medical informatics*, 8(2):e16492, 2020.\\n\\nMary E Charlson, Peter Pompei, Kathy L Ales, and C Ronald MacKenzie. A new method of classifying prognostic comorbidity in longitudinal studies: development and validation. *Journal of chronic diseases*, 40(5):373\u2013383, 1987.\\n\\nEdward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F. Stewart, and Jimeng Sun. Generating multi-label discrete patient records using generative adversarial networks, 2018.\\n\\nXin Du, Lei Sun, Wouter Duivesteijn, Alexander Nikolaev, and Mykola Pechenizkiy. Adversarial balancing-based representation learning for causal effect inference with observational data. *Data Mining and Knowledge Discovery*, pp. 1\u201326, 2021.\\n\\nKhaled El Emam, David L. Buckeridge, Robyn Tamblyn, Angelica Neisa, Elizabeth Jonker, and Aman Verma. The re-identification risk of canadians from longitudinal demographics. *BMC Medical Informatics Dec. Mak.*, 11:46, 2011. doi: 10.1186/1472-6947-11-46. URL https://doi.org/10.1186/1472-6947-11-46.\\n\\nDylan J. Foster and Vasilis Syrgkanis. Orthogonal statistical learning, 2020.\\n\\nJessica M Franklin, Sebastian Schneeweiss, Jennifer M Polinski, and Jeremy A Rassen. Plasmode simulation for the evaluation of pharmacoepidemiologic methods in complex healthcare databases. *Computational statistics & data analysis*, 72:219\u2013226, 2014.\\n\\nAndre Goncalves, Priyadip Ray, Braden Soper, Jennifer Stevens, Linda Coyle, and Ana Paula Sales. Generation and evaluation of synthetic patient data. *BMC medical research methodology*, 20(1):1\u201340, 2020.\\n\\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), *Advances in Neural Information Processing Systems*, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf.\\n\\nJ Henry, Yuriy Pylypchuk, Talisha Searcy, and Vaishali Patel. Adoption of electronic health record systems among us non-federal acute care hospitals: 2008\u20132015. *ONC data brief*, 35:1\u20139, 2016.\\n\\nJennifer L. Hill. Bayesian nonparametric modeling for causal inference. *Journal of Computational and Graphical Statistics*, 20(1):217\u2013240, 01 2011. doi: 10.1198/jcgs.2010.08162. URL https://doi.org/10.1198/jcgs.2010.08162.\\n\\nJames Jordon, Jinsung Yoon, and M. Schaar. Pate-gan: Generating synthetic data with differential privacy guarantees. In *ICLR*, 2019.\\n\\nEhud Karavani, Tal El-Hay, Yishai Shimoni, and Chen Yanover. Comment: Causal inference controversies: Where should we aim? *Statistical Science*, 34(1):86\u201389, 2019.\"}"}
{"id": "TWTTKlwrUP0", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Robert LaLonde. Evaluating the econometric evaluations of training programs with experimental data. American Economic Review, 76(4):604\u201320, 1986. URL https://EconPapers.repec.org/RePEc:aea:aecrev:v:76:y:1986:i:4:p:604-20.\\n\\nMichael J. Lopez and Roee Gutman. Estimation of causal effects with multiple treatments: A review and new ideas. Statistical Science, 32(3), Aug 2017. ISSN 0883-4237. doi: 10.1214/17-sts612. URL http://dx.doi.org/10.1214/17-STS612.\\n\\nChristos Louizos, Uri Shalit, Joris Mooij, David Sontag, Richard Zemel, and Max Welling. Causal effect inference with deep latent-variable models, 2017.\\n\\nBradley Malin and Latanya Sweeney. How (not) to protect genomic data privacy in a distributed network: Using trail re-identification to evaluate and design anonymity protection systems. J. of Biomedical Informatics, 37(3):179\u2013192, June 2004. ISSN 1532-0464. doi: 10.1016/j.jbi.2004.04.005. URL https://doi.org/10.1016/j.jbi.2004.04.005.\\n\\nBrady Neal, Chin-Wei Huang, and Sunand Raghupathi. Realcause: Realistic causal inference benchmarking. arXiv preprint arXiv:2011.15007, 2020.\\n\\nPaul R. Rosenbaum and Donald B. Rubin. The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1):41\u201355, 04 1983. ISSN 0006-3444. doi: 10.1093/biomet/70.1.41. URL https://doi.org/10.1093/biomet/70.1.41.\\n\\nFlorian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 815\u2013823, 2015.\\n\\nMegan S Schuler and Sherri Rose. Targeted maximum likelihood estimation for causal inference in observational studies. American journal of epidemiology, 185(1):65\u201373, 2017.\\n\\nUri Shalit, Fredrik D. Johansson, and David Sontag. Estimating individual treatment effect: Generalization bounds and algorithms. In Proceedings of the 34th International Conference on Machine Learning, pp. 3076\u20133085, 2017.\\n\\nAmit Sharma and Emre Kiciman. Dowhy: An end-to-end library for causal inference, 2020.\\n\\nLatanya Sweeney. Weaving technology and policy together to maintain confidentiality. The Journal of Law, Medicine & Ethics, 25(2-3):98\u2013110, 1997. doi: 10.1111/j.1748-720X.1997.tb01885.x. URL https://doi.org/10.1111/j.1748-720X.1997.tb01885.x. PMID: 11066504.\\n\\nAllan Tucker, Zhenchen Wang, Ylenia Rotalinti, and Puja Myles. Generating high-fidelity synthetic patient data for assessing machine learning healthcare software. NPJ digital medicine, 3(1):1\u201313, 2020.\\n\\nC. Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.\\n\\nJason Walonoski, Mark Kramer, Joseph Nichols, Andre Quina, Chris Moesel, Dylan Hall, Carlton Duffett, Kudakwashe Dube, Thomas Gallagher, and Scott McLachlan. Synthea: An approach, method, and software mechanism for generating synthetic patients and the synthetic electronic health care record. Journal of the American Medical Informatics Association, 25(3):230\u2013238, 2018.\\n\\nZhenchen Wang, Puja Myles, and Allan Tucker. Generating and evaluating cross-sectional synthetic electronic healthcare data: Preserving data utility and patient privacy. Computational Intelligence, 37(2):819\u2013851, 2021.\\n\\nLiyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Zhou. Differentially private generative adversarial network, 2018.\"}"}
{"id": "TWTTKlwrUP0", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "TWTTKlwrUP0", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Detailed Description of Data Preprocessing\\n\\nHere we provide a description of the preprocessing procedure to clean and transform the patient claim data described in Table 1. The goal of this step is to prepare the data ready for subsequent steps.\\n\\nWe first one-hot encode all the categorical variables in the table, such as gender and race. For each lab test there are two variables: lab values and lab dates. Since all the lab dates were after the 'date-' date, which was the date when the patient's systolic blood pressure was measured before treatment, we convert all the lab dates to the number of days since 'date-', excluding holidays and weekends. The reason for excluding these dates in the conversion is that we eventually reverse the exact process to convert the synthetic lab dates in number of days back to the original date format, we make sure that such synthetic dates do not fall on holidays and weekends. Note that the exclusion of holidays and weekends does not apply to the prescription dates, assuming prescriptions can be filled on any day. When we transform the 'date-' variable itself, we arbitrarily pick a date proceeding all the 'date-' values, and convert the 'date-' values to the number of days since this picked date, excluding all holidays and weekends.\\n\\nThere are many missing lab dates and values, as expected. If doctors order a certain lab test for a small portion of patients, the lab test and dates for the majority of the patients would be missing. We fill the missing values with the mean of the values that are present. However, this induces a data distribution that significantly deviates from Gaussian distribution: the probability of the mean is much higher than the probability of other values. Our experiments showed that learning such a distribution was difficult. To address this problem, we created a binary indicator for each lab test, which is set to 1 if the lab test is not missing, and 0 otherwise. The intuition is that it is easier for the model to learn not only the distribution of the indicator, but also the join distribution of the indicator and its corresponding lab results, thus enhancing the synthetic data's ability to more faithfully reflect the original data distribution. Another benefit of doing this is to directly preserve the information on whether a lab test was ordered for each patient. Our experiments show that this approach works well in practice. The final step of data preprocessing is to standardize all the values to the $[0, 1]$ range.\\n\\nA.2 Fundamental Assumptions for Treatment Effect Estimations\\n\\nWe use $t \\\\in \\\\{t_1, t_2, \\\\ldots\\\\} = X_C$ to denote treatments, $Y_i(t_i) \\\\in \\\\mathbb{R}$ to denote the potential outcome if the patient is treated with the treatment $t_i \\\\in X_C$, and $x_i$ to denote the variables for patient $i$.\\n\\nSUTVA (Stable Unit Treatment Value Assumption)\\nPotential outcomes for one individual are unaffected by the treatment of others.\\n\\nUnconfoundedness\\nThe treatment distribution is conditional independent of the potential outcomes given covariates, i.e. $(\\\\forall i) (t_i \\\\perp \\\\perp (Y_i(t_1), Y_i(t_2), \\\\ldots, Y_i(t | X_C | x_i)))$.\\n\\nPositivity\\nEvery individual has a non-zero probability to receive each one of the treatments, for a given level of covariates, i.e. $(\\\\forall i) (\\\\forall s) (s \\\\in T \\\\land 0 < P(t_i = s | x_i) < 1)$.\\n\\nA.3 Results of Evaluating Different Causal Inference Models\\n\\n13\"}"}
{"id": "TWTTKlwrUP0", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We here introduce a contrastive loss (triplet ranking loss, Schroff et al. (2015)) item, which is defined as\\n\\\\[ U_{\\\\text{con}}(x, x', z) = \\\\max(0, U(x, G(x, z)) - U(x', G(x, z))) \\\\] (2)\\n\\nThen, the final identifiability loss function \\\\( L_I \\\\) is\\n\\\\[ L_I = \\\\mathbb{E}_{x \\\\sim P_X, z \\\\sim P_z} [ -U(x, G(x, z))] + \\\\beta \\\\mathbb{E}_{x, x' \\\\sim P_X} [U_{\\\\text{con}}(x, x', z)] \\\\] (3)\\n\\nSimilar to (Yoon et al. (2020)), this loss also assumes that \\\\( G(x, z) \\\\) is the closest data point to \\\\( x \\\\).\\nHowever, a penalty on the loss function will be imposed if this assumption is violated when the generated sample \\\\( G(x, z) \\\\) is closer to \\\\( x' \\\\), a randomly drawn sample from dataset \\\\( D \\\\), than to \\\\( x \\\\). The strength of the penalty term is controlled by \\\\( \\\\beta \\\\).\\n\\nIn the final optimization problem, we minimize \\\\( G \\\\) and maximize \\\\( D \\\\) simultaneously, written as\\n\\\\[ G^*, D^* = \\\\arg \\\\min_G \\\\max_D [L_D + \\\\lambda L_I] \\\\] (4)\\n\\nwhere \\\\( \\\\lambda \\\\) is a hyper-parameter that controls the trade-off between the two objectives. Once trained, the adapted ADS-GAN model can be used to produce synthetic data set \\\\( \\\\hat{D} \\\\).\\n\\n4.3 STEP 3: DATA GENERATION MODEL AND CAPTURED CAUSAL EFFECTS\\n\\nWe need to determine a data generation model and proper treatment effects to use in the data generation process to produce the potential outcomes of the synthetic data, i.e., the factuals and counterfactuals. Many researchers use arbitrary functions and arbitrary treatment effects. For example, Schuler & Rose (2017) used a linear function as the data generation process and set the treatment effects arbitrarily. To improve upon such an approach, in this work we train a neural network model from the original dataset aiming to capture the mapping from patient covariates to outcomes as well as the treatment effects. The captured treatment effects are not necessarily the true effects, but serve as the ground truth in the synthetic data when the data is used to evaluate causal inference models because the patient outcomes in the synthetic data are generated from these causal effects.\\n\\nWe first partition the domain of observed variables \\\\( X \\\\subseteq \\\\mathbb{R}^d \\\\) into the covariate domain \\\\( X_C \\\\subseteq \\\\mathbb{R}^{d_c} \\\\), the treatment domain \\\\( X_T \\\\subseteq \\\\mathbb{R}^{d_t} \\\\) and the outcome domain \\\\( X_o \\\\subseteq \\\\mathbb{R} \\\\), so that \\\\( d \\\\geq d_c + d_t + 1 \\\\). The covariates are all the patient variables excluding drugs, prior drugs, zip code, and lab+. Treatments are the drugs. Outcome is the difference between lab+ and lab-.\\n\\nGiven a collection of \\\\( N \\\\) observations, we use \\\\( Y_i(t_i) \\\\in \\\\mathbb{R} \\\\) to denote the potential outcome of the \\\\( i \\\\)-th individual, if treated with the treatment \\\\( t_i \\\\in X_C \\\\). We assume that \\\\( (Y_i(t_i), t_i, x_i) \\\\in \\\\mathbb{R} \\\\times X_C \\\\times X_C \\\\) are independent and identically distributed, and that the three fundamental assumptions for causal inference (Rosenbaum & Rubin (1983)) in Appendix A.2 are satisfied. Following (Lopez & Gutman (2017); Shalit et al. (2017)), given \\\\( x \\\\in X_C \\\\) and \\\\( t_i, t_0 \\\\in X_C \\\\), where \\\\( t_0 \\\\) is the placebo, the individual-level treatment effect (ITE) of \\\\( t_i \\\\) can be defined as\\n\\\\[ \\\\tau_{t_i}(x) := \\\\mathbb{E}[Y(t_i) - Y(t_0)|x] \\\\] (5)\\n\\nHence, the population average treatment effect for treatment \\\\( t_i \\\\) can be defined as\\n\\\\[ ATE_{t_i} = \\\\mathbb{E}[Y(t_i) - Y(t_0)] = \\\\int_{X_C} \\\\tau_{t_i}(x) p(x) \\\\, dx \\\\] (6)\\n\\nSo the data generation process can be modeled as \\\\( Y = \\\\Omega(x, t) \\\\), where \\\\( \\\\Omega: X_C \\\\times T \\\\rightarrow \\\\mathbb{R} \\\\). The true form of \\\\( \\\\Omega \\\\) is unknown and can be complicated. Here we make a simplifying assumption that the representation learned from the covariate domain is separated from the representation learned from the treatment domain. Specifically, let \\\\( \\\\Phi: X_C \\\\rightarrow \\\\mathbb{R} \\\\) be a representation function and \\\\( R \\\\) be the representation space. We define \\\\( Q: R \\\\times T \\\\rightarrow \\\\mathbb{R} \\\\) so that \\\\( \\\\Omega(x, t_i) = Q(\\\\Phi(x), t_i) \\\\). Also, we denote with \\\\( \\\\tau_Q(t_i)(x) \\\\) the treatment effect estimate of \\\\( Q \\\\).\\n\\nWith simplified \\\\( \\\\Omega \\\\), we propose a neural network architecture shown in Figure 1 that is able to capture \\\\( \\\\Omega \\\\), \\\\( \\\\Phi \\\\), and at the same time, calculate the treatment effects. For the covariate domain \\\\( X_C \\\\), the network is a fully connected feed-forward neural network with Relu as the activation function for all the neurons. For the treatment domain \\\\( X_T \\\\), the inputs are encoded treatments directly connected to a neuron with a linear activation. The loss function is the standard mean square error (MSE). We apply a dropout to all the layers and apply a gentle L2 regularization to all the weights of the neural network.\"}"}
{"id": "TWTTKlwrUP0", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The model $\\\\Omega$ is trained on the original dataset described in section 3, where we have one factual for each observation. Due to the separation of the covariate domain and treatment domain, and with the particular architecture of the ANN shown in Figure 1, the neural network weights for treatment connections can be interpreted as the causal treatment effects. Since there is no interaction between the covariates and treatments, the individual treatment effects and population average treatment effects are the same. Indeed, suppose $w_i$ is the weight for treatment $t_i$, one can show that $w_i$ is indeed the $\\\\tau_{t_i}$ in Equation 5 and the ATE$_{t_i}$ in Equation 6.\\n\\n4.4 STEPPING GENERATION OF FACTUALS AND COUNTERFACTUALS ON SYNTHETIC DATA\\n\\nThe domain of variables and all its partitions are the same for the real dataset $D$ and the synthetic dataset $\\\\hat{D} = \\\\{\\\\hat{x}_i: \\\\hat{x}_i = G(x_i, z), x_i \\\\in D, z \\\\sim P_Z\\\\}$. Hence, the neural network trained on the original dataset in Step 4.3 can be fed with the synthetic patient variables generated in Step 4.2. The neural network outputs are served as the treatment outcomes for the synthetic data.\\n\\nOnce trained, this neural network is capable of generating all factual and counterfactual treatment outcomes for the synthetic data. For any synthetic patient with covariate $\\\\hat{x}_j \\\\in X_c$, the potential outcome of any treatment $t_i \\\\in X_C$ can be generated as $\\\\hat{Y}_j(t_i) = \\\\Omega(\\\\hat{x}_j, t_i) = \\\\Omega(\\\\Phi(\\\\hat{x}_j), t_i)$.\\n\\nHowever, instead of generating the potential outcomes of all possible treatments in $X_C$, in this work we only generate two potential outcomes for each patient: the factual outcome corresponding to the treatment produced by the ADS-GAN model, and the counterfactual outcome if the patient had not received any treatment. The reason that we do not want to produce all the counterfactual outcomes is that we want to limit the treatment for each patient only to the one generated by the ADS-GAN model, in order to preserve the treatment assignment mechanism learned from the original dataset.\\n\\nThere is a distinction between the assumptions made in Section 4.3 in determining the treatment effects and the assumptions that our synthetic dataset actually satisfies. Specifically, our synthetic dataset satisfies the SUTVA and unconfoundedness assumption, as we did not model the interactions between patients and we provided all the patient variables in the dataset used to generate the outcomes. Whether it satisfies the positivity assumption, however, depends on the original dataset because the patient assignment mechanism of the synthetic data is learned from the original dataset.\\n\\n5 RESULTS\\n\\nIn this section we evaluate the quality of our synthetic dataset. We show that there is strong similarity in both marginal and joint data distributions between the original and synthetic dataset, and that patient privacy is preserved. We evaluate several causal inference models using our dataset to demonstrate the usage of it.\\n\\n5.1 ANALYSIS OF SYNTHETIC AND ORIGINAL DATA DISTRIBUTIONS\\n\\nWe first show how well the generated synthetic data preserves the joint distribution of the original data. We trained the adapted ADS-GAN model for 10,000 epochs and the outcome neural network for 20,000 epochs to generate the synthetic dataset. We calculated the Wasserstein distance (Villani (2008)) between the joint distribution of this synthetic data and that of the original data to be 0.35. To put this value in the correct perspective, we measured the Wasserstein distance between the original dataset and a randomly generated dataset of the same dimensions. This serves as the baseline scenario. In addition, we randomly split the original dataset into two datasets and measured the Wasserstein distance between them, which is essentially the Wasserstein distance between the dataset and itself and serves as the best case scenario. Our results show that the Wasserstein distance between the synthetic and original data distribution (0.35) is almost as good as that in the best case scenario (0.17), and far more lower than the value in the baseline scenario (8.6).\"}"}
{"id": "TWTTKlwrUP0", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Heatmaps of correlation matrices of patient variables for the original (left) and synthetic data (right), respectively.\\n\\nFigure 3: Comparison of marginal distribution of lab values between original and synthetic data. The three horizontal dotted lines in each violin plot from top to the bottom represent the third quartile, median, and the first quartile respectively.\\n\\nWe then compare the joint distributions visually. In Fig. 2, the correlation among all patient attributes in the original (synthetic) dataset is visualized by the heatmap on the left (right). In the heatmap, the brighter the color of a pixel is, the more correlated the two variables are with each other. The diagonal is the brightest in the map, as each pixel on the diagonal represents the correlation between a variable and itself. The two heatmaps show almost identical patterns, indicating the joint distribution of the original data is well preserved in the synthetic dataset.\\n\\nIn Fig. 3, we compare qualitatively the marginal distributions of individual variables of the generated synthetic data (orange) with the related ones from the original data (blue). The figure shows strong similarity between the original and synthetic dataset in both basic statistical summaries (e.g., median and quartiles) and overall shape of these distributions.\\n\\n5.2 Identifiability of Synthetic Data\\n\\nSince the synthetic dataset we generated in this study is meant to be made public, patient privacy has to be preserved to ensure that no actual patients in the original dataset source can be identified through the synthetic dataset. All the synthetic samples in our dataset are conceptually drawn from a distribution, so no single piece of information about any actual patients is directly carried over to our dataset. We further calculate the $\\\\epsilon$-identifiability as defined in Definition 2 to be 0.008%, indicating that the risk of any actual patient being identified is statistically zero.\\n\\n5.3 Results of Testing Causal Inference Algorithms on the Dataset\\n\\nIn this section, we evaluate the bias in causal treatment effect estimate of five well established models on our synthetic dataset: two models in the doubly robust (DR) family, one propensity score stratification, one propensity matching, and one inverse probability treatment weighting (IPTW) model.\\n\\nDoubly robust approaches adopt an outcome regression model to estimate the treatment outcome and a propensity model to estimate the probability of a patient being assigned with a treatment. In both the DR models we tested, random forest is used as the outcome regression model. The difference between the two DR models is in the propensity model, which can be logistic regression (DR-LR) or random forest (DR-RF) classification. We use Microsoft DoWhy (Sharma & Kiciman 8...\"}"}
{"id": "TWTTKlwrUP0", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nWhen calculating the causal effect of a treatment, we remove the counter-factuals of this treatment from the dataset to prevent the problem becoming trivial.\\n\\nWe adopted four metrics to evaluate the models: the Spearman\u2019s rank correlation coefficient to measure how well the models preserve the rank of the drugs by their treatment effects, Kendall rank coefficient similar to Spearman\u2019s coefficient but based on concordant and discordant pairs, correlation between the estimated effects and the ground truth, and finally the $R^2$ to measure how much variance of the ground truth can be explained by the estimate.\\n\\nTo estimate how models perform in a real-world setting, we generated an additional dataset consisting of all patient variables of the original dataset and patient outcomes generated from the trained outcome neural network with patient variables and treatments of the original dataset as its inputs. We call this dataset the hybrid dataset because part of the data comes from the original dataset and part of the data is generated. We run the five causal inference models on the two datasets and reported all the results in Appendix A.3.\\n\\nThe results on the hybrid dataset show that the algorithms can be grouped into two categories in terms of their performance: the propensity stratification, propensity matching, and two doubly robust models are in the first group and the IPTW model is in the second group. The models in the first group produced much better results than the IPTW model. The results on the synthetic dataset show a similar pattern. The variance of the performance metrics of the models in the first group is larger than that on the hybrid dataset, but all these models perform much better than the IPTW model.\\n\\nInvestigating why some models perform better than others on the two datasets is out of scope of this work. Here we show that the synthetic data preserves the relative performance of different models that would be achieved in a more realistic setting, simulated by the hybrid dataset.\\n\\n### DISCUSSION\\n\\nIn general, inclusion and exclusion criteria applied to the data may introduce selection bias. Our work was designed with a target trial in mind in which patients are recruited at an initial qualifying measurement and then followed up after treatment assignments. We believe this minimizes the impact of selection bias from conditioning on the inclusion and exclusion criteria in our original data. In this work, we only produced one dataset for hypertension and evaluated five causal inference models. We leave it to future work to produce synthetic datasets for other diseases and evaluate and compare other causal inference models. Because hypertension affects almost half of adults in the United States, a synthetic dataset on hypertension is of significant value by itself. For simplicity, in this study we did not consider treatment modifiers, i.e., interactions between treatments and patient variables. It is an interesting and important topic which we plan to address in the future.\\n\\n### CONCLUSION\\n\\nTo validate machine learning models, researchers have traditionally relied on labeled data, i.e., ground truth. Due to the fundamental problem of causal inference, however, the lack of realistic clinical data with ground truth makes it difficult to evaluate causal inference models. In this work, we produced a large-scale and realistic synthetic data by adapting an ADS-GAN model to generate patient variables and using a neural network to produce patient outcomes. The data we generated supports multiple treatments with known treatment effects. We demonstrated that this synthetic dataset preserves patient privacy and has strong similarity to the original dataset it is modeled after. We believe that it will facilitate the evaluation, understanding and improvement of causal inference models, especially with respect to how they perform in real-world scenarios.\\n\\n### REPRODUCIBILITY STATEMENT\\n\\nTo contribute to reproducibility, we provide in the supplementary material the code for replicating experiments. In section 3 we provide detailed description of the original real-world dataset and related inclusion and exclusion criteria. In section A.1 we provide a detailed description of the preprocessing procedure to make the generation process transparent and reproducible. Finally, we report the the link to download our synthetic dataset in the final paper.\"}"}
{"id": "TWTTKlwrUP0", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"lenge due to three reasons. First, the ground truth of treatment effects in a realistic setting is un-\\nknown. In real world, we can not compute the treatment effect by directly comparing the potential\\noutcomes of different treatments because of the fundamental problem of causal inference:\\nfor a given patient and treatment, we can only observe the factual, defined as the patient outcome for the\\ngiven treatment, but not the counterfactual, defined as the patient outcome if the treatment had been\\ndifferent. Second, legal and ethical issues around un-consented patient data and privacy created a\\nsignificant barrier in accessing routinely EHRs by the machine learning community. In order to mit-\\ngate the legal and ethical risks of sharing sensitive information, de-identification of patient records\\nis a commonly used practice. However, previous work has shown that de-identification is not suffi-\\ncient for avoiding re-identification through linkage with other identifiable datasets Sweeney (1997);\\nEmam et al. (2011); Malin & Sweeney (2004). Third, most publicly available datasets support either\\nbinary or very few treatments, while there has been growing literature developing techniques with\\nmultiple treatments in recent years (Lopez & Gutman (2017)).\\n\\nTo address these challenges, in this work we generated a large-scale and realistic patient dataset that\\nmimics real patient data distributions, supports multiple treatments, and provides ground truth for\\nthe effects of these treatments. The datasets we generated are synthetic patients with hypertension\\nmodeled on a massive nationwide cohort patients' history of diagnoses, medications, and laboratory\\nvalues. We designed a data generation process by adapting an Anonymization Through Data Synthe-\\nsis Using Generative Adversarial Networks (ADS-GAN by Yoon et al. (2020)) model for fictitious\\npatient information generation and using a neural network for treatment outcome generation. The\\nsynthetic dataset demonstrates strong similarity to the original dataset as measured by the Wasser-\\nstein distance. In addition, we ensure that the original patients' privacy is preserved so that our\\ndataset can be made available to the research community to evaluate causal inference models.\\n\\nWe demonstrated the use of the synthetic data by applying our dataset to evaluate five models:\\none inverse probability treatment weighting (IPTW) model, one propensity matching model, one\\npropensity score stratification model all introduced by Rosenbaum & Rubin (1983), and two models\\nin the doubly robust family (Foster & Syrgkanis (2020)).\\n\\nTo our knowledge, this is the first large scale dataset that mimics real data joint distributions with\\nmultiple treatments and known causal effects. The approach we used can be readily extended to\\nother types of diseases in the clinical domain, and to datasets in other domains as well.\\n\\nThe rest of this paper is organized as follows. In Section 2, we discuss related works. The details of\\nour method are presented in Section 4. In Section 5, we discuss the evaluation metrics for the quality\\nof the data and results of evaluating several established causal inference models with the data. We\\ndiscuss the limitations of the work in Section 6 and conclude the paper in Section 7.\\n\\n2 RELATED WORK\\n\\nOur work is related to several existing works on publicly available databases, fictitious patient record\\ncreations, and data generation processes.\\n\\n2.1 PUBLICLY AVAILABLE DATASETS\\n\\nFirst used in (Hill (2011)), the Infant Health and Development Program (IHDP) is a randomized con-\\ntrolled study designed to evaluate the effect of home visit from specialist doctors on the cognitive\\ntest scores of premature infants. It contains 747 subjects and 25 variables that describe the charac-\\nteristics of the infants and their mothers. The Jobs dataset by LaLonde (1986) is a benchmark used\\nby the causal inference community, where the treatment is job training and the outcomes are income\\nand employment status after training. The Twins dataset, originally used for evaluating causal infer-\\ncence in (Louizos et al. (2017); Yao et al. (2018)), consists of samples from twin births in the U.S.\\nbetween the years 1989 and 1991 provided in (Almond et al. (2005)). The Annual Atlantic Causal\\nInference Conference (ACIC) data challenge provides an opportunity to compare causal inference\\nmethodologies across a variety of data generating processes.\\n\\nSome of the above mentioned datasets do not provide true causal effects. Others are small in size\\nso the models validated on such datasets may perform very differently in a more general real-world\\nsetting. All the datasets created for ACIC challenge were designed specifically for competitions. The\\ncovariates in the data are either drawn from publicly available databases, or simulated. In the former\"}"}
{"id": "TWTTKlwrUP0", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"case, the datasets are limited by small populations (Du et al. (2021)) and arbitrarily designed data generation processes, which did not aim to capture any real-world causal relationships (Karavani et al. (2019)). In the latter case, the distribution of the data is not realistic, i.e., dissimilar to the distribution of any real dataset.\\n\\n2.2 EHR DATA GENERATION\\nWalonoski et al. (2018) generated synthetic EHRs based on publicly available information. The focus of their work is on generating the life cycle of a patient and how a disease evolves over time. Goncalves et al. (2020) evaluated three synthetic data generation models\u2014probabilistic models, classification-based imputation models, and generative adversarial neural networks\u2014in generating realistic EHR data. Tucker et al. (2020) used a Bayesian network model to generate synthetic data based on the Clinical Practice Research Datalink (CPRD) in the UK. Benaim et al. (2020) evaluated synthetic data produced from 5 contemporary studies using MDClone. Wang et al. (2021) proposed a framework to generate and evaluate synthetic health care data, and the key requirements of synthetic data for multiple purposes. All of these works focus on EHR data generation producing patient variables but without ground truth for causal effects. In contrast, the focus of our work is not only on generating patient variables, but producing ground truth for causal effects as well.\\n\\n2.3 POTENTIAL OUTCOME GENERATION\\nTo validate their models, many researchers such as (Schuler & Rose (2017)) created synthetic co-variates and produced potential outcomes with a designed data generation process. Such datasets are not designed to approximate any real data distributions. Franklin et al. (2014); Neal et al. (2020) generated potential outcomes from covariates with known causal effects, but without any regard to patient privacy. We addressed the critical issue of patient privacy concerns so our data can be made available for the research community to evaluate their models.\\n\\n3 PATIENT CLAIM DATA AND INCLUSION EXCLUSION CRITERIA\\nTo make our synthetic data realistic, we generate the data based on a real-world patient claim database from a large insurance company in the United States. This database contains 5 billion insurance claims (diagnoses, procedures, and drug prescriptions or refills) and lab test results from 56.4 million patients who subscribed to the company's service within a 5-year time period between December 2014 and December 2020. From this database, we extracted a subset of patients affected by hypertension. We chose hypertension because there are a large number of related claims, making it easier to learn the data distribution. In addition, since it is a condition affecting nearly half of adults in the United States (116 million, or 47%), our generated dataset can be directly used for clinical researchers to develop and evaluate their models for this important disease. Patients are included in the dataset if they have a medical claim indicating hypertension (ICD code I10, I11.9, I12.9, and I13.10) or currently treated with anti-hypertensive medications. We exclude patients from the dataset if they are age < 18 or age > 85, affected by white coat hypertension, secondary hypertension, malignant cancers, dementia, or are pregnant. After applying the above mentioned inclusion and exclusion criteria, we have about 1.6 million patients included in this study. We further exclude patients treated with a combination of drugs rather than a single drug. We then rank the drugs by the number of patients treated with each drug, and only keep patients treated with one of the 28 most popular drugs. These filtering steps produce about 262,000 patients in the study. The distribution of this dataset is then learned and used to generate synthetic patients, viewed as samples drawn from the learned distribution.\\n\\nThe patients' diagnoses and treatment history and how their conditions evolve over time are captured by trajectory data consisting of labs, diagnoses and their corresponding dates. For the convenience of data processing and analysis, we convert the trajectory data into tabular data with rows representing different patients (samples) and columns representing patient features (variables) including patient demographics, diagnoses, medications and labs. In Table 1, we list and briefly describe these 60 patient variables:\\n\\n- 2 variables (F1) describing the systolic blood pressure before the treatment and the date it was measured,\\n- 2 variables (F2) describing the same metric but after the treatment,\"}"}
{"id": "TWTTKlwrUP0", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\n| Family var. | Var. names | Description |\\n|-------------|------------|-------------|\\n| F1          | date-, lab- | First lab result and date |\\n| F2          | date+, lab+ | Second lab result and date |\\n| F3          | drugs, prior | Drugs' info |\\n| F4          | age, gndr, cd, race, ethncty | Age/Gender/Ethnicity |\\n| F5          | lab measurement results and date | 11 lab measurements and date |\\n| F6          | safety morbs, morbs prior | Current and previous comorbidities |\\n| F7          | zip cd, total pop, p female, median income etc | Zip code and related statistics |\\n| F8          | trajectory index, mcid | Meta-information |\\n\\nTable 1: The patient claim dataset on hypertension contains 1,618,363 observations and 60 variables. Above the family of such variables are listed and briefly described.\\n\\nBasic information (age, gender, ethnicity), 30 variables (F5) indicating laboratory measurements, 2 variables (F6) indicating the presence or absence of comorbid conditions defined by the Charlson Comorbidity Index (Charlson et al. (1987)), 15 variables (F7) describing the patient's zip code, the racial makeup and income levels in the patient's zip code tabulation area (ZCTA), 2 variables (F8) indicating meta information.\\n\\nIn this study, we are interested in the causal effects of anti-hypertensive drugs (current drugs of F3) on patient outcomes, measured as the difference between the first (F1) and second lab results (F2).\\n\\n4 APPROACH\\n\\nTo generate the synthetic data, we first generate the patient variables using an adapted ADS-GAN model, then generate the treatment outcomes using a neural network. Our approach can be conceptually decomposed into four steps described below.\\n\\n4.1 STEP 1: DATA PREPROCESSING\\n\\nSince we generate the synthetic data from the patient claim data extracted in Section 3, in this step we preprocess the patient claim data and prepare it for subsequent steps. Described in Table 1, this dataset are of mixed data types: integers (e.g., age), floats (e.g., lab values), categorical values (e.g., drugs), and dates. Further, the values and dates of a lab test are missing for some patients if the lab test was not ordered by the doctors for these patients. In this step, we clean, one-hot encode, transform and standardize the data so that all the variable are transformed into numerical values in [0, 1] range. We also add a binary feature for each lab test to indicate missing lab values and dates. The resulted dataset has 221 features and we call it the original dataset, to be distinguished from the synthetic dataset. The details of this step are described in Appendix A.1.\\n\\n4.2 STEP 2: GENERATION OF OBSERVED VARIABLES USING ADS-GAN\\n\\nIn this step we generate synthetic patients characterized by the same variables as listed in Table 1. We want to achieve two goals in this step: to make the synthetic data as realistic as possible and to make sure the probability of identifying any actual patients in the original dataset from the synthetic dataset is very low. We quantitatively define the identifiability in Definition 2, and the realisticity as the Wasserstein distance (Gulrajani et al. (2017)) between the distribution of the synthetic dataset and that of the real dataset it is modeled after.\\n\\nThere is a trade-off between identifiability and realisticity of generated data. Frameworks like MedGan (Choi et al. (2018)) and WGAN-GP (Arjovsky et al. (2017)) do not explicitly define and allow to control the identifiability levels. Therefore, we evaluated generative models that allow to explicitly control such trade-off, e.g., the ADS-GAN (Yoon et al. (2020)), PATE-GAN (Jordon et al. (2019)) and DP-GAN (Xie et al. (2018)). ADS-GAN proved to consistently outperform the benchmarks across the entire range of identifiability levels on both the MAGGIC and the three UNOS transplant datasets. It is also based on a measurable definition for identifiability. Another advantage of ADS-GAN is the use of Wasserstein distance to measure the similarity between two high\"}"}
{"id": "TWTTKlwrUP0", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\ndimensional joint distributions. We finally selected ADS-GAN and adapted it to generate the patient variables in our study.\\n\\nLet us denote the original dataset obtained in Section 4.1 as \\n\\\\[ D = \\\\{ x_i \\\\}_{i=1}^N, \\\\]\\nwhere \\n\\\\[ x_i \\\\in X \\\\subseteq \\\\mathbb{R}^d \\\\]\\nand \\n\\\\[ x_i = (x_{i1}, x_{i2}, \\\\ldots, x_{id}) \\\\]\\nwith \\n\\\\[ x_{ij} \\\\in X_{ij} \\\\subseteq \\\\mathbb{R} \\\\]\\nrepresenting the \\\\( j \\\\)-th feature of a patient.\\n\\n\\\\( N \\\\) is the number of samples and \\\\( d \\\\) is the number of features of each sample.\\n\\nLet \\n\\\\[ P_X \\\\]\\ndenote the underlying distribution from which each \\n\\\\[ x_i \\\\]\\nis drawn and let \\n\\\\[ z \\\\sim P_Z \\\\]\\nbe drawn from a multi-variate Gaussian distribution. The framework of ADS-GAN is to train a generator \\n\\\\[ G : X \\\\times Z \\\\to X \\\\]\\nand a discriminator \\n\\\\[ D : X \\\\to \\\\mathbb{R} \\\\]\\nin an adversarial fashion: the generator \\n\\\\[ G \\\\]\\nwhich produces synthetic patient variables \\n\\\\[ \\\\hat{x}_i = G(x, z) \\\\]\\nwhere \\n\\\\[ \\\\hat{x}_i \\\\in \\\\hat{X} \\\\subseteq \\\\mathbb{R}^d \\\\]\\nenherits that the synthetic dataset \\n\\\\[ \\\\hat{D} = \\\\{ \\\\hat{x}_i \\\\} \\\\]\\nare not too close to \\n\\\\[ D \\\\]\\nas measured by the \\\\( \\\\epsilon \\\\)-identifiability defined below; on the other hand, the discriminator \\n\\\\[ D \\\\]\\nwhich measures the distance between two distributions ensures that the distribution of generated variables \\n\\\\[ P_{\\\\hat{X}} \\\\]\\nis indistinguishable from the distribution of real variables \\n\\\\[ P_X \\\\].\\n\\n**Definition 1.**\\nWe define the weighted Euclidean distance \\n\\\\[ U(x_i, x_j) = \\\\| w(x_i - x_j) \\\\| \\\\]\\nwhere \\n\\\\[ w \\\\]\\nis a weight vector. Then \\n\\\\[ r_i = \\\\min_{x_j \\\\in D/\\\\hat{x}_i} U(x_i, x_j) \\\\]\\nwhere \\n\\\\[ D/\\\\hat{x}_i \\\\]\\nrepresents the dataset \\n\\\\[ D \\\\]\\nwithout \\\\( \\\\hat{x}_i \\\\). From the definition, \\n\\\\[ r_i \\\\]\\nis the weighted minimum distance between \\\\( x_i \\\\) and any other observations in \\n\\\\[ D \\\\].\\n\\nWe similarly define \\n\\\\[ \\\\hat{r}_i = \\\\min_{\\\\hat{x}_j \\\\in \\\\hat{D}} U(x_i, \\\\hat{x}_j) \\\\]\\nwhich is the weighted minimum distance between \\\\( x_i \\\\) and the synthetic samples in \\n\\\\[ \\\\hat{D} \\\\].\\n\\n**Definition 2.**\\nThe \\\\( \\\\epsilon \\\\)-identifiability of dataset \\n\\\\[ D \\\\]\\nfrom \\n\\\\[ \\\\hat{D} \\\\]\\nis defined as \\n\\\\[ \\\\epsilon = I(D, \\\\hat{D}) = \\\\frac{1}{N} \\\\sum_i \\\\left[ 1(\\\\hat{r}_i > r_i) \\\\right] \\\\]\\nwhere \\n\\\\[ 1 \\\\]\\nis an indicator function.\\n\\nTo calculate the weight vector \\n\\\\[ w \\\\],\\nwe first calculate the discrete entropy of the \\\\( j \\\\)-th feature, i.e.\\n\\\\[ H(X_{ij}) = -\\\\sum_{x_{ij} \\\\in X_{ij}} P(X_{ij} = x_{ij}) \\\\log[P(X_{ij} = x_{ij})] \\\\]\\nThe weight for a feature is then calculated as the inverse of \\n\\\\[ H(X_{ij}) \\\\]. In reality, if a patient can be re-identified, the re-identification is more likely through rare characteristics or medical conditions of a patient. Calculating the weight this way ensures that the rare features of a patient are given more weight, correctly reflecting the risk of re-identification associated with different features.\\n\\nWe base the discriminator \\n\\\\[ D \\\\]\\non Wasserstein GAN with gradient penalty (Gulrajani et al. (2017)) (WGAN-GP), which adopts Wasserstein distance between \\n\\\\[ P_{\\\\hat{X}} \\\\] and \\n\\\\[ P_X \\\\], and defines the loss \\n\\\\[ L_D \\\\]\\nfor the discriminator \\n\\\\[ D \\\\] as\\n\\\\[ L_D = E_{x \\\\sim P_X, \\\\hat{x} \\\\sim P_{\\\\hat{X}}} [D(x) - D(\\\\hat{x}) - \\\\mu(\\\\|\\\\nabla \\\\tilde{x} D(\\\\tilde{x})\\\\|_2 - 1)^2] \\\\]\\nwhere \\n\\\\[ \\\\tilde{x} \\\\]\\nbelongs to a random interpolation distribution between \\n\\\\[ P_X \\\\] and \\n\\\\[ P_{\\\\hat{X}} \\\\] and \\n\\\\[ \\\\mu \\\\]\\nis a further hyper-parameter that we set to 10 based on previous work (Gulrajani et al. (2017)). We implement both the generator and the discriminator using multi-layer perceptrons.\\n\\nTo train the generator \\n\\\\[ G \\\\],\\nwe need to compute the \\\\( \\\\epsilon \\\\)-identifiability by computing \\n\\\\[ r_i \\\\] and \\n\\\\[ \\\\hat{r}_i \\\\] for every sample, which is computationally expensive. To solve the problem, Yoon et al. (2020) made a simplifying assumption that \\n\\\\[ G(x, z) \\\\]\\nis the closest data point to \\n\\\\[ x \\\\]. However, this assumption can be violated during the training of the network that maximizes the distance between \\n\\\\[ G(x, z) \\\\] and \\n\\\\[ x \\\\].\"}"}
{"id": "TWTTKlwrUP0", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Model evaluation on hybrid data. The results show that the first four models perform much better than the IPTW model.\\n\\n| Model Type               | Spearman Correlation | Kendall Tau Correlation | R\u00b2 Score |\\n|--------------------------|----------------------|-------------------------|---------|\\n| Propensity Stratification| 0.99                 | 0.92                    | 0.99    |\\n| Propensity Matching      | 0.92                 | 0.8                     | 0.98    |\\n| Doubly Robust - RF       | 0.95                 | 0.93                    | 0.99    |\\n| Doubly Robust - LR       | 0.99                 | 0.96                    | 0.99    |\\n| IPTW                     | 0.59                 | 0.48                    | -0.46   |\\n|                          |                      |                         | -5.9    |\\n\\nTable 3: Model evaluation on synthetic data. The results show a pattern similar to the table for the hybrid dataset: the first four models perform much better than the IPTW model. The performance variance of the first four models is larger than that on the hybrid dataset.\\n\\n| Model Type               | Spearman Correlation | Kendall Tau Correlation | R\u00b2 Score |\\n|--------------------------|----------------------|-------------------------|---------|\\n| Propensity Stratification| 0.81                 | 0.68                    | 0.87    |\\n| Propensity Matching      | 0.52                 | 0.37                    | 0.47    |\\n| Doubly Robust - RF       | 0.92                 | 0.78                    | 0.97    |\\n| Doubly Robust - LR       | 0.74                 | 0.63                    | 0.88    |\\n| IPTW                     | 0.09                 | 0.04                    | 0.06    |\\n|                          |                      |                         | -34     |\"}"}
