{"id": "DQou0RiwkR0", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multichannel speech enhancement (SE) systems separate the target speech from background noise by performing spatial and spectral filtering. The development of multichannel SE has a long history in the signal processing field, where one crucial step is to exploit spatial separability of sound sources by aligning the microphone signals in response to the target speech source prior to further filtering processes. This is similar to the human listening behavior of facing toward the speaker for better perception of the speech. However, most existing deep learning-based multichannel SE works have yet to effectively incorporate or emphasize this spatial alignment aspect in the network design; some of them rely on integrating conventional model-based beamformer units to extract useful spatial features implicitly while others just let the network figure everything out by itself. However, the beamformer operation could be computationally expensive and numerically unstable when trained with the network while without it the model lacks guidance on learning meaningful spatial features. In this paper, we highlight this important but often overlooked step in deep learning-based multichannel SE, i.e., signal alignment, by introducing an Align-and-Filter network (AFnet) featuring a two-stage sequential masking design. The AFnet aims at estimating two sets of masks, the alignment masks and filtering masks, to carry out temporal alignment and spectral filtering processes. During training, we propose to supervise the learning of alignment masks by predicting the relative transfer functions (RTFs) of various speech source locations followed by learning the filtering masks for signal reconstruction. During inference, the AFnet sequentially multiplies the estimated alignment and filtering masks with the microphone signals, performing the \u201calign-then-filter\u201d process similar to the human listening behavior. Due to the incorporation of RTF supervision, the AFnet explicitly learns interpretable spatial features without integrating traditional beamformer operations.\\n\\nIntroduction\\n\\nSpeech enhancement (SE) systems can be categorized into single-channel (single microphone) and multichannel (multiple microphones) schemes. An important aspect of multichannel SE against single-channel SE is the exploitation of spatial separability, as known as spatial filtering or beam-forming, enabled by the difference between the amplitudes and times of arrival of the received microphone signals due to the different acoustic paths the sound waveform travels to the microphones. In many signal processing beamforming methods (Gannot et al., 2001; Cohen, 2004; Krueger et al., 2010; Koldovsk`y et al., 2015), a key step is to align the microphone signals in response to the target signal source before any further filtering processes. This step, by steering the array toward the location of the target signal, aims to compensate for the difference of the amplitudes and time delays (or correspondingly the magnitudes and phases in the frequency domain) of the microphone signals with respect to the target source. Ideally, after the alignment step, each microphone should contain the same target speech component with no difference in amplitude and time delay (or magnitude and phase). For a linear array in the far-field, anechoic setting, perfectly steering the microphone array makes it as if the target signal comes from the broadside, which renders the speech extraction task easier in the later filtering stage. Such process is similar to the human listening behavior of facing toward the speaker for better perception of the speech. Thus, an efficient SE system can first align its microphone signals in response to the target speech, followed by spectral processing for fine-tuning.\"}"}
{"id": "DQou0RiwkR0", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023 and enhancement. Surprisingly, though well-known in signal processing approaches, there are only few deep learning SE systems designed mainly based on this observation. Instead, several recent works (Wang et al., 2020; 2021) have been utilizing conventional beamformer units such as the minimum-variance-distortionless-response (MVDR) beamformer (Capon, 1969) to extract spatial characteristics implicitly in deep learning methods. However, matrix inversion or eigendecomposition are often required which in turn increase the computation burden and numerical instability.\\n\\nIn this paper, we revisit this important but often overlooked alignment aspect from conventional signal processing algorithms and recognize its importance for efficient deep learning multichannel SE network design that requires no intermediate beamformer units such as MVDR to extract meaningful spatial features. Specifically, we propose the Align-and-Filter network (AFnet) shown in Figure 1 for exploiting spatial separability within multichannel data with the following main contributions:\\n\\n\u2022 The AFnet features a two-stage sequential masking design, i.e., Align Net and Filter Net, where two sets of masks, alignment and filtering masks, are estimated and multiplied with the microphone signals to perform the \u201calign-then-filter\u201d process mimicking the human listening behavior.\\n\\n\u2022 During the training stage, we propose to supervise the learning of the alignment masks by estimating the relative transfer functions (RTFs) (Gannot et al., 2001; Cohen, 2004) for speech sources coming from various locations, prior to learning the filtering masks for final enhancement.\\n\\n\u2022 During inference, the AFnet is able to first align the microphone signals with respect to a speech source coming from an unknown direction due to supervised learning of alignment masks. Subsequently, the model performs filtering on the roughly aligned signals to achieve denoising.\\n\\n\u2022 It is demonstrated that the RTF supervision incorporated with the sequential masking mechanism is the key to effectively learn useful, interpretable spatial characteristics. On situations where the target speech may come from arbitrary positions, the \u201calign-then-filter\u201d mechanism consistently improves the SE performance by more efficiently exploiting spatial separability of sound sources.\\n\\n2 RELATED WORK\\n\\nMultichannel SE has been a well known topic in signal processing for decades. Aside from leveraging spectral characteristics, multichannel SE can exploit positional information to perform spatial filtering, or beamforming, that allows extracting the target speech from noise based on spatial separability. Conventional beamforming approaches rely on the so-called \u201csteering vector\u201d which carries positional information about the target speech (Doclo et al., 2015; Trees, 2004), e.g., the MVDR beamformer (Capon, 1969) and its variants (Frost, 1972; Griffiths & Jim, 1982). It is an important step in beamforming that the microphone array is steered toward the target signal location prior to further filtering processes. To this end, certain knowledge about the acoustic paths between the target signal and the microphones have to be known. Many signal processing-based multichannel SE systems utilize the ratio of the acoustic transfer functions, i.e., the relative transfer function (RTF) (Gannot et al., 2001; Cohen, 2004; Krueger et al., 2010; Koldovsk`y et al., 2015) for improving the denoising process.\\n\\nIn the past decade, deep learning approaches have remarkably changed the way of developing SE systems. Along with the success of deep neural networks (DNNs) on single-channel SE (Lu et al., 2013; Williamson et al., 2015; Pascual et al., 2017; Luo & Mesgarani, 2019; Kim et al., 2020; Zheng et al., 2021; Hu et al., 2020), several multichannel SE systems have also been proposed, e.g., Erdogan et al. (2016); Variani et al. (2016); Sainath et al. (2017); Wang & Wang (2018); Bu et al. (2019); Koyama & Raj (2019); Luo et al. (2020); Tolooshams et al. (2020); Koyama & Raj (2020); Wang et al. (2021); Zhang et al. (2021); Kim et al. (2022); Li et al. (2022). However, the utilization of such important RTF information is often overlooked in the model design of DNN-based multichannel SE. Although several SE works have incorporated RTFs (Wang & Wang, 2018; Zhang et al., 2021) the RTF estimation is mostly used as an intermediate step to assist the MVDR beamformer only. We postulate that the overlook may be due to the lack of sufficient spatial variety of the speech sources in popular datasets such as CHiME-3 Barker et al. (2015), where the benefit of utilizing RTFs could be only marginal. However, many practical situations can have the target speech coming from arbitrary directions and thus a deep dive into the RTF spatial alignment aspect is still of great importance.\\n\\n3 PROPOSED METHOD\\n\\nWe consider an acoustic scenario with one desired speech source and several interfering noise signals in a reverberant environment. The SE system will be developed in the time-frequency domain using\"}"}
{"id": "DQou0RiwkR0", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\ndConv\\\\((M, C_1)\\\\)\\ndConv\\\\((C_1, C_2)\\\\)\\ndConv\\\\((C_2, C_3)\\\\)\\ndConv\\\\((C_3, C_4)\\\\)\\ndConv\\\\((C_4, C_4)\\\\)\\ndConv\\\\((2C_1, C_1)\\\\)\\ndConv\\\\((2C_2, C_1)\\\\)\\ndConv\\\\((2C_3, C_2)\\\\)\\ndConv\\\\((2C_4, C_3)\\\\)\\n\\n*Figure 1: The proposed AFnet for exploiting spatial separability for multichannel SE. During training, the Align Net estimates the alignment masks by predicting the speech RTFs. The Filter Net estimates the filtering masks to reconstruct the clean speech by linearly combining the aligned signals after the Align Net. The RTF supervision and the two-stage sequential masking design contribute to the improved multichannel SE performance.*\\n\\nThe short-time Fourier transform (STFT) (Parchami et al., 2016) where the time-domain waveforms are transformed to complex-valued STFT representations. Let \\\\( f, t \\\\) stand for the frequency index and time frame index (with a total of \\\\( F \\\\) frequency bins and \\\\( T \\\\) time frames), we consider an additive noise model where the \\\\( i \\\\)-th microphone noisy signal STFT \\\\( X_i \\\\in \\\\mathbb{C}^{F \\\\times T} \\\\) of an \\\\( N \\\\)-element local microphone array can be written as (Doclo et al., 2015):\\n\\n\\\\[\\nX_i(f, t) = H_i(f, t) S_0(f, t) + V_i(f, t), \\\\quad (1)\\n\\\\]\\n\\nwhere \\\\( S_i(f, t) \\\\) is the speech component received by microphone \\\\( i \\\\), \\\\( H_i(f, t) \\\\) is the (potentially time-varying) acoustic transfer function between microphone \\\\( i \\\\) and the location of the far-field speech source signal \\\\( S_0(f, t) \\\\), and \\\\( V_i(f, t) \\\\) is the noise component captured by microphone \\\\( i \\\\). Typically, the goal of multichannel SE is to recover the speech component \\\\( S = S_r \\\\in \\\\mathbb{C}^{F \\\\times T} \\\\) of a selected reference microphone \\\\( r \\\\in \\\\{1, \\\\ldots, N\\\\} \\\\) given the noisy microphone signals \\\\( X_1, \\\\ldots, X_N \\\\). Multichannel SE systems typically perform the \u201cfilter-and-sum\u201d operation, or more generally known as \u201cbeam-forming\u201d \u2013 linearly combining the signals of different microphones to extract the target signal from background noise. In the STFT domain, the estimation process can be expressed as:\\n\\n\\\\[\\n\\\\hat{S} = \\\\sum_{i=1}^{N} W_i \\\\odot X_i, \\\\quad (2)\\n\\\\]\\n\\nwhere \\\\( W_i \\\\in \\\\mathbb{C}^{F \\\\times T} \\\\) is the corresponding set of filter weights for microphone \\\\( i \\\\), \\\\( \\\\hat{S} \\\\) is the enhanced signal, and \\\\( \\\\odot \\\\) denotes element-wise complex multiplication.\\n\\n3.1 Two-Stage Align-and-Filter Framework with Sequential Masking\\n\\nInspired by the alignment concept in signal processing-based algorithms described in Section 1 and Appendix A, we introduce a two-stage Align-and-Filter framework for deep learning multichannel SE. This framework consists of two stages: alignment and filtering, each with its own set of operations and weights. The alignment stage is designed to estimate the weights for aligning the signals, while the filtering stage is responsible for reconstructing the clean speech. The sequential masking design contributes to the improved performance of the multichannel SE system.\"}"}
{"id": "DQou0RiwkR0", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SE in Figure 1, where two modules are utilized to carry out alignment and filtering purposes respectively. The first module (Align Net) aims at estimating a set of alignment masks for steering the input microphone signals toward the target speech location. The second module (Filter Net) focuses on cleaning up the interference in the aligned signals by estimating another set of filtering masks and linearly combines all channel outputs. Overall, the enhancement process can be expressed as:\\n\\n$$\\\\hat{S} = \\\\bigotimes_{i=1}^{N} (F_i \\\\odot A_i) \\\\odot X_i,$$\\n\\nwhere $A_i \\\\in \\\\mathbb{C}^{F \\\\times T}$ is the alignment mask and $F_i \\\\in \\\\mathbb{C}^{F \\\\times T}$ is the filtering mask of the $i$-th microphone. This corresponds to having $W_i = F_i \\\\odot A_i$ for the equivalent beamforming filter weights applied on $X_i$ in (2). Such a sequential masking scheme also introduces signal level skip connections on top of the two modules for improved optimization.\\n\\n### 3.2 RTF-AWARE TRAINING\\n\\nThe spatial relationship between the far-field target source and the microphones along with the acoustic environment characteristics determine the inter-channel level difference (ILD) and inter-channel time difference (ITD) of the microphone signals, which are important cues for sound source localization. In the STFT domain, the ILD and ITD correspond to the magnitude and phase of the RTF respectively, which is the ratio between the acoustic transfer functions of two microphones with regards to the sound source (Li et al., 2016). Formally, we can express the RTFs $\\\\tilde{H}_i \\\\in \\\\mathbb{C}^{F \\\\times T}$ as:\\n\\n$$\\\\tilde{H}_i(f, t) \\\\triangleq H_r(f, t) H_i(f, t) = H_r(f, t) S_0(f, t) H_i(f, t) S_i(f, t),$$\\n\\nwhere $S_0(f, t)$ is the speech component of the reference microphone and $\\\\tilde{V}_i(f, t) = [H_r(f, t)/H_i(f, t)] V_i(f, t)$ is the noise component. It can be seen that multiplying the RTFs with the respective input STFT leads to aligning all the microphones with respect to the clean speech component at the reference channel as each $Z_i(f, t)$ contains $S_r(f, t)$. In the case of far-field, anechoic setting, this means the microphone array has steered to the direction of the signal source. For the case of linear arrays, it can be viewed as the signal is coming from the broadside after steered.\\n\\nWe incorporate RTFs during the model learning phase via an RTF-aware training protocol consisting of two phases: First adjusting only the Align Net parameters so that the alignment mask $A_i$ approximates the RTFs $\\\\tilde{H}_i$ by minimizing the following RTF alignment loss:\\n\\n$$L_{\\\\text{rtf}} = \\\\frac{1}{\\\\text{NF} \\\\times \\\\text{F} \\\\times \\\\text{T}} \\\\sum_{i,f,t} |A_i(f, t) - \\\\tilde{H}_i(f, t)|^2,$$\\n\\nwhere we have chosen to use the mean squared error (MSE) loss. Note that truncation of the magnitude of the ground-truth RTFs $\\\\tilde{H}_i(f, t)$ to a maximum value of, e.g., $10$, can be applied for avoiding training instability, and the alignment mask corresponding to the reference microphone is always all ones and is possible not to be estimated. Optimizing for this loss function guides the network to learn to steer the microphone signals toward the target speech, making the model aware of the spatial information of the target source. The second phase performs parameter learning of the whole model (i.e., jointly adjusting both Align Net and Filter Net parameters) to reconstruct the clean speech. To this end, we minimize the signal reconstruction loss:\\n\\n$$L_{\\\\text{rec}} = \\\\sum_{f,t} (1 - \\\\beta) (|\\\\hat{S}(f, t)|_c - |S(f, t)|_c)^2 + \\\\beta |||\\\\hat{S}(f, t)|_c e^{j \\\\angle \\\\hat{S}(f,t)} - |S(f, t)|_c e^{j \\\\angle S(f,t)}||_2^2,$$\\n\\nwhere we have chosen to use the combined power-law compressed MSE loss first appeared in (Wilson et al., 2018). In this work we use $\\\\beta = 0$ and $c = 0.3$ as suggested by a consolidated study in (Braun & Tashev, 2021).\"}"}
{"id": "DQou0RiwkR0", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"K. Wilson, M. Chinen, J. Thorpe, B. Patton, J. Hershey, R. A. Saurous, J. Skoglund, and R. F. Lyon. Exploring tradeoffs in models for low-latency speech enhancement. In Proceedings of International Workshop on Acoustic Signal Enhancement (IWAENC), pp. 366\u2013370, 2018.\\n\\nX. Xia and B. Kulis. W-Net: A deep model for fully unsupervised image segmentation. arXiv preprint arXiv:1711.08506, 2017.\\n\\nZ. Zhang, Y. Xu, M. Yu, S.-X. Zhang, L. Chen, and D. Yu. ADL-MVDR: All deep learning MVDR beamformer for target speech separation. In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 6089\u20136093, 2021.\\n\\nC. Zheng, X. Peng, Y. Zhang, S. Srinivasan, and Y. Lu. Interactive speech and noise modeling for speech enhancement. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 14549\u201314557, 2021.\"}"}
{"id": "DQou0RiwkR0", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A conventional signal processing beamforming algorithms based on align-then-filter process\\n\\nTo support the motivation behind the proposed AFnet which is designed based on the align-then-filter concept well-known in signal processing, we review several spatial filtering algorithms based on using this design principle.\\n\\nThe intuition for performing alignment followed by filtering is relatively simple. Consider the RTF aligned signal expression in (5). We can see that in the aligned signals each $Z_i(f, t)$ contains $S_r(f, t)$. Since all the $S_r(f, t)$ among the $N$ channels are in-phase (and actually are identical if magnitudes are also aligned in this case), summing up all the $Z_i(f, t)$ over the $N$ channels will not be destructive to the speech component. However, the noise components $\\\\tilde{V}_i(f, t)$ are usually not in-phase (unless coming from the same direction as the speech source), destructive combination can happen by summing them up. Consequently, combining all the $Z_i(f, t)$ across the $N$ channels will boost the SNR as the speech component is preserved while the noise components are suppressed.\\n\\nThe above process is actually the classic \\\"delay-and-sum (DS)\\\" beamformer Trees (2004); Doclo et al. (2015). In a DS beamformer, a steering vector containing the time delay information between the microphone sensors with respect to the target speech is first applied to the input multichannel signals to compensate the time delay (or equivalently the phase difference) of the speech components in all channels. This step is called \\\"alignment.\\\" Then a summation operation over the aligned channels is performed to suppress noise for boosting SNR. This step is called \\\"filtering.\\\" Mathematically, the enhancement process of the DS beamformer can be expressed as:\\n\\n$$\\\\hat{S}(f, t) = h^T(f, t)x(f, t),$$\\n\\nfor all $f, t$, where $\\\\tilde{h}(f, t) = [\\\\tilde{H}_1(f, t), \\\\ldots, \\\\tilde{H}_N(f, t)]^T$ is the steering vector consisting of RTFs (potentially only the phase components) and $x(f, t) = [X_1(f, t), \\\\ldots, X_N(f, t)]^T$ is the vector of microphone signals.\\n\\nAnother famous beamformer utilizing the align-then-filter concept is the \\\"generalized sidelobe canceller (GSC)\\\" (Griffiths & Jim, 1982; Frost, 1972). In a typical GSC, the input noisy signals are first aligned by the steering vector. Then, two branches of processing follow, where one branch sums up all the aligned microphone signals to perform a DS beamforming, while the other branch introduces a \\\"blocking matrix\\\" to extract noise references while blocking out the target speech component. Then, the noise-only output of the blocking matrix will go through an adaptive filtering algorithm to estimate the residual noise component in the DS beamforming output signal in an adaptive manner. Note that the alignment step plays an important role here. First, it leads to the DS beamformer for boosting SNR. In addition, it is beneficial for the blocking matrix stage to extract noise-only references. For example, a simple blocking mechanism can be just subtracting all other $Z_i(f, t)$, $i \\\\neq r$ from the reference channel $Z_r(f, t)$ so that the $S_r(f, t)$ will be cancelled out and only the noise remains. The GSC is obviously another \\\"align-then-filter\\\" example with a more sophisticated filtering stage.\\n\\nFinally, the popular \\\"minimum-variance-distortionless-response (MVDR)\\\" beamformer (Capon, 1969; Doclo et al., 2015) can also belong to the align-then-filter framework. The derivation of the MVDR filter weights $w = [W_1(f, t), \\\\ldots, W_N(f, t)]^T \\\\in \\\\mathbb{C}^N$ which applies to the noisy signals to obtain the enhanced speech as $\\\\hat{S}(f, t) = w^H(f, t)x(f, t)$ starts from considering the following optimization problem:\\n\\n$$\\\\min_w w^H\\\\Phi_v(f, t)w,$$\\n\\nsubject to\\n\\n$$w^H\\\\bar{h}(f, t) = 1,$$\\n\\nfor all $f, t$, where $\\\\Phi_v(f, t) = \\\\mathbb{E}[v(f, t)v(f, t)^H] \\\\in \\\\mathbb{C}^{N \\\\times N}$ is the power spectral density of the vector of noise signals $v(f, t) = [V_1(f, t), \\\\ldots, V_N(f, t)]^T$ and $\\\\bar{h}(f, t) = [\\\\tilde{H}^{-1}_1(f, t), \\\\ldots, \\\\tilde{H}^{-1}_N(f, t)]^T$ is the (reciprocal) vector of RTFs. Solving the above we obtain the MVDR filter as:\\n\\n$$w(f, t) = \\\\Phi_v^{-1}(f, t)v(f, t)\\\\bar{h}(f, t)\\\\bar{h}^H(f, t)\\\\Phi_v^{-1}(f, t)v(f, t)\\\\bar{h}(f, t),$$\\n\\nfor all $f, t$. We see that the RTFs which are related to signal alignment are involved in the computation of the MVDR filter weights.\"}"}
{"id": "DQou0RiwkR0", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"From the above, we can see that the RTF information plays an essential role in many conventional beamforming algorithms. This has motivated us to devise the AFnet following the alignment principle. Note that the signal processing-based beamforming algorithms typically rely on modeling assumptions of signal or noise statistics to perform denoising, and therefore the performance is limited due to mismatch of real-world observations and modeling assumptions. Our approach is based on data-driven, deep learning techniques to overcome such limitations while taking advantage of the RTFs.\\n\\n**APPENDIX B IMPLEMENTATION DETAILS**\\n\\n**B.1 RTF-AWARE TRAINING ALGORITHMS**\\n\\nThe main RTF-incorporated training scheme is summarized in Algorithm 1, which performs two-step training consisting of i) training the Align Net for the RTF loss followed by ii) training both Align and Filter Nets jointly for the signal reconstruction loss.\\n\\n**Algorithm 1**\\n\\n**RTF-Aware Training (Two-Step)**\\n\\ninputs \\\\( \\\\text{Model}(X_i \\\\in \\\\mathbb{C}^F \\\\times T, \\\\forall i = 1, \\\\ldots, N; \\\\text{Align Net, Filter Net}) \\\\)\\n\\nInitialize Align Net, Filter Net\\n\\nfor number of training iterations do\\n\\nSample a minibatch of new input spectrograms \\\\( X_i, \\\\forall i = 1, \\\\ldots, N \\\\)\\n\\nUpdate Align Net by minimizing \\\\( L_{rtf} \\\\)\\n\\nend for\\n\\nfor number of training iterations do\\n\\nSample a minibatch of new input spectrograms \\\\( X_i, \\\\forall i = 1, \\\\ldots, N \\\\)\\n\\nUpdate Align Net and Filter Net by minimizing \\\\( L_{rec} \\\\)\\n\\nend for\\n\\nAnother way of RTF-aware training for the AFnet is by combining the RTF loss and signal reconstruction loss together and jointly train the entire AFnet from scratch. In this sense, the RTF loss serves as a regularization term weighted by a scaler \\\\( \\\\lambda > 0 \\\\). The training scheme is depicted in Algorithm 2. Later in Section C.1 we will show that this is less effective than Algorithm 1.\\n\\n**Algorithm 2**\\n\\n**RTF-Aware Training (via Regularization)**\\n\\ninputs \\\\( \\\\text{Model}(X_i \\\\in \\\\mathbb{C}^F \\\\times T, \\\\forall i = 1, \\\\ldots, N; \\\\text{Align Net, Filter Net}) \\\\)\\n\\nInitialize Align Net, Filter Net\\n\\nfor number of training iterations do\\n\\nSample a minibatch of new input spectrograms \\\\( X_i, \\\\forall i = 1, \\\\ldots, N \\\\)\\n\\nUpdate Align Net and Filter Net by minimizing \\\\( L_{rec} + \\\\lambda L_{rtf} \\\\)\\n\\nend for\\n\\n**B.2 PRIMARY DATASET**\\n\\nCorpus: We use Audio-Visual Speech Dataset (AVSpeech) (Ephrat et al., 2018) available online. AVSpeech is a large collection of video clips of single speakers talking with no audio background interference. The audio files of the AVSpeech dataset serve as proper target speech files due to their variety (diversity of language, gender, age, etc.). The dataset can be found at: https://looking-to-listen.github.io/avspeech/.\\n\\nNoise data: We collected 8 types of noise profiles from YouTube which are commonly seen sound sources in daily life. The noise types include blender, vacuum, washer, baby cry (for training) and dog barking, kids playing sound, hair dryer, food sizzling (for testing). Each type of noise profile contains recordings from different sound sources of the same type.\"}"}
{"id": "DQou0RiwkR0", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Data pre-processing:\\nAll the raw audio data are first converted to 16 kHz mono .wav files before feeding to the room acoustic simulator to generate the multichannel data samples for experimentation (the AVSpeech audio originally may contain 1 or 2 channels of audio). For spectral processing, we use the Hann window with 512-point FFT and a hop size of 256 for the STFT. By conjugate symmetry of the FFT, only half of the frequency bins (i.e., 257) are actually needed for the network to process the spectrograms.\\n\\nRoom acoustic simulator:\\nPyroomacoustics is utilized to generate multichannel audio which can be found at https://github.com/LCAV/pyroomacoustics/tree/pypi-release. We create a room of size $8 \\\\times 8 \\\\times 3$ (length $\\\\times$ width $\\\\times$ height in meters). Reverberation time is randomly chosen from $\\\\{0.16, 0.32, 0.48, 0.64\\\\}$ second. The RIRs are estimated based on the image source method. 8 microphones are placed to form a planar array (4 columns, 2 rows) vertically, where the distance between adjacent microphones is set to 5 cm. The microphone array center is placed at the center of the room. Speech and noise signals are randomly positioned at a distance of $\\\\{1, 2, 3, 4\\\\}$ meters from the microphone array center. In the experiments, the upper leftmost and rightmost microphones are used for the 2-mic scheme, the microphones at the four corners are used for the 4-mic scheme, and all microphones are used for the 8-mic scheme.\\n\\nReal-world measured RIRs:\\nTo further validate the proposed approach's generalization capabilities to more realistic scenarios, we also conduct experiments on multichannel audio data generated using real-world measured acoustic RIRs. A popular multichannel impulse response dataset is from the work by Hadad et al. (2014), where the impulse responses of several 8-channel microphone arrays with respect to various target source directions (from -90 to 90 degrees) are provided. We utilize the 3-3-3-8-3-3-3 array impulse responses of the dataset including reverberation time of 0.16, 0.36, and 0.61 second cases (https://www.iks.rwth-aachen.de/en/research/tools-downloads/databases/multi-channel-impulse-response-database/), together with the AVSpeech utterances and the YouTube noise profiles to generate the multichannel noisy data for experiments. The experimental results and related discussion are presented in Appendix C.7.\\n\\nMixing speech with noise:\\nWe mix the multichannel speech files with noise profiles to generate noisy-clean data pairs. Each type of noise is multiplied with a scale randomly chosen from $\\\\{0, 0.5, 1, 1.5, 2\\\\}$ before added up, and the combined noise is then added to the clean speech according to a specific SNR level randomly selected from $\\\\{-10, -6, -3, 0, 3, 6, 10\\\\}$ dB. In this way, we are adopting the supervised learning SE scheme of mix-and-separate, where the clean and noise signals are separately collected and subsequently mixed together to become the noisy signals for training purposes. This mix-and-separate scheme by assuming additive noise model is still being used in the majority of SE works (not only signal processing- but also deep learning-based approaches). To apply it to real data, one may collect the clean and noise recordings using the target device in real-world environments and mix them up to generate noisy mixtures for training.\\n\\nNetwork architectures:\\n- **AFnet:** the implementation of AFnet (Figure 1) is based on the deep complex U-Net model at: https://github.com/sweetcocoa/DeepComplexUNetPyTorch, for both the Align Net and Filter Net parts. The number of layers and the number of features in each layer are modified to the one shown in Figure 1, where $C_1, C_2, C_3, C_4 = 32, 64, 64, 64$.\\n- **Conv-TasNet:** the experimental results of Conv-TasNet is based on the implementation from https://github.com/kaituoxu/Conv-TasNet using their own model settings instead of the original Conv-TasNet paper. Note that although the model hyperparameters were designed based on 8 kHz sampling rate, we directly use them for 16 kHz sampling rate of our data without further modifications. Hyperparameters: $N = 256$, $L = 20$, $B = 256$, $H = 512$, $P = 3$, $X = 8$, $R = 4$, Norm=gLN, Noncausal.\\n- **DCUnet:** the experimental results of DCUnet is based on the implementation of the code at https://github.com/mhlevgen/DCUNetTorchSound. We choose the Large-DCUnet-20 model which realizes the 20-layer model in the original DCUnet paper. Note that although in the original system a 1024-point FFT was used, here we use 512-point FFT instead.\"}"}
{"id": "DQou0RiwkR0", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: SE comparison with SOTA methods on the primary dataset.\\n\\n| Methods                | # Params | PESQ | STOI | SSNR |\\n|------------------------|----------|------|------|------|\\n| Noisy                  |          | 1.21 | 0.577| -1.70|\\n| Conv-TasNet            | 8.7M     | 1.47 | 0.636| 3.00 |\\n| DCUnet                 | 7.6M     | 1.49 | 0.660| 2.91 |\\n| FaSNet (2-mic)         | 2.8M     | 1.55 | 0.663| 3.70 |\\n| FaSNet (4-mic)         |          | 1.57 | 0.667| 3.64 |\\n| FaSNet (8-mic)         |          | 1.64 | 0.683| 3.66 |\\n| AFnet (2-mic)          | 2.6M     | 1.84 | 0.728| 5.06 |\\n| AFnet (4-mic)          |          | 1.92 | 0.739| 5.26 |\\n| AFnet (8-mic)          |          | 1.99 | 0.753| 5.35 |\\n\\nTable 3: SE comparison with SOTA methods on the CHiME-3 dataset.\\n\\n| Methods                | # Params | PESQ | STOI |\\n|------------------------|----------|------|------|\\n| Noisy                  |          | 1.27 | 0.870|\\n| Neural BF              |          |      |      |\\n| MVDR                   | 0.5M     |      | 0.952|\\n| GC                     |          |      |      |\\n| rSDFCN                 | 2.1M     | 2.15 | 0.937|\\n| CA Dense U-Net         | >20M     | 2.44 |      |\\n| IC Conv-TasNet         | 1.7M     | 2.67 | 0.973|\\n| AFnet w/o RTF Loss (ours) | 2.6M | 2.68 | 0.972|\\n| AFnet (ours)           |          | 2.72 | 0.972|\\n\\nFigure 4: SE comparison of different RTF alignment schemes for AFnet training.\\n\\n4.2.6 A RE INTERMEDIATE ALIGNMENT MASKS NECESSARY?\\n\\nGiven the results in Figure 4 where the phase importance is recognized, one would ask if the intermediate alignment mask design of AFnet is necessary, and if we can directly apply phase alignment at the output filtering masks? To answer this question, we train the W-Net for predicting the filter weights (i.e., the filtering masks) to minimize $L_{\\\\text{rec}} + \\\\lambda L_{\\\\text{rtf, phase}}$, where $L_{\\\\text{rec}}$ is the same signal reconstruction loss as (7) and $L_{\\\\text{rtf, phase}}$ is the RTF alignment loss similar to (6) but imposed on the filtering masks given by the W-Net for the phase portion only and regularized by $\\\\lambda > 0$. Several values of $\\\\lambda$ are considered and the results are compared to AFnet in Table 4. We can see that over a wide range of $\\\\lambda$ the W-Net is still not able to achieve as good performance as AFnet which utilizes the alignment masks with RTF supervision. This implies that the intermediate masking mechanism for performing alignment is critical for achieving improved SE with two-stage network design.\\n\\nTable 4: SE comparison of AFnet and W-Net with RTF phase regularization on the filtering masks.\\n\\n| Method                                    | PESQ 2-mic | PESQ 4-mic | PESQ 8-mic | STOI 2-mic | STOI 4-mic | STOI 8-mic | SSNR 2-mic | SSNR 4-mic | SSNR 8-mic |\\n|-------------------------------------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|\\n| W-Net for filter weights, no phase reg.  | 1.74       | 1.61       | 1.67       | 0.703      | 0.679      | 0.688      | 4.80       | 4.07       | 4.39       |\\n| W-Net for filter weights, phase reg. $\\\\lambda = 0.0001$ | 1.76       | 1.61       | 1.65       | 0.713      | 0.686      | 0.692      | 4.95       | 4.21       | 4.42       |\\n| W-Net for filter weights, phase reg. $\\\\lambda = 0.001$  | 1.63       | 1.60       | 1.68       | 0.688      | 0.686      | 0.702      | 4.36       | 4.14       | 4.60       |\\n| W-Net for filter weights, phase reg. $\\\\lambda = 0.01$   | 1.71       | 1.59       | 1.61       | 0.705      | 0.683      | 0.684      | 4.55       | 4.14       | 4.30       |\\n| W-Net for filter weights, phase reg. $\\\\lambda = 0.1$    | 1.73       | 1.66       | 1.74       | 0.708      | 0.701      | 0.710      | 4.09       | 3.54       | 3.87       |\\n| AFnet                                                   | 1.84       | 1.92       | 1.99       | 0.728      | 0.739      | 0.753      | 5.06       | 5.26       | 5.35       |\\n\\nAdditional results on a variety of testing scenarios, e.g., SNR conditions, room configurations, training protocols, time-varying RIR and realistic RIR data are provided in Appendix C. Comparison of Align Net with signal processing-based algorithms for RTF estimation is presented in Appendix D.\\n\\n5 CONCLUSION\\n\\nIn this paper, we presented the AFnet, a deep learning-based multichannel SE approach that performs signal alignment followed by filtering for extracting the target speech from noisy observations. We showed that leveraging spatial information inherent in the RTFs for signal alignment purposes of the two-stage, sequential masking network during training is the key to remarkable improvements. Our findings suggest that RTF alignment, especially the phase estimation, plays a crucial role in deep learning multichannel SE for target speech that may come from arbitrary locations.\"}"}
{"id": "DQou0RiwkR0", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nREFERENCES\\n\\nJ. Barker, R. Marxer, E. Vincent, and S. Watanabe. The third 'CHiME' speech separation and recognition challenge: Dataset, task and baselines. In Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pp. 504\u2013511, 2015.\\n\\nA. Benichoux, L. S. R. Simon, E. Vincent, and R. Gribonval. Convex regularizations for the simultaneous recording of room impulse responses. IEEE Transactions on Signal Processing, 62(8):1976\u20131986, 2014.\\n\\nS. Braun and I. Tashev. A consolidated view of loss functions for supervised deep learning-based speech enhancement. In Proceedings of International Conference on Telecommunications and Signal Processing (TSP), pp. 72\u201376, 2021.\\n\\nS. Bu, Y. Zhao, and M.-Y. Hwang. A novel method to correct steering vectors in MVDR beamformer for noise robust ASR. In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), pp. 4280\u20134284, 2019.\\n\\nJ. Capon. High-resolution frequency-wavenumber spectrum analysis. Proceedings of the IEEE, 57(8):1408\u20131418, 1969.\\n\\nH.-S. Choi, J.-H. Kim, J. Huh, A. Kim, J.-W. Ha, and K. Lee. Phase-aware speech enhancement with deep complex U-Net. In Proceedings of International Conference on Learning Representations (ICLR), 2018.\\n\\nI. Cohen. Relative transfer function identification using speech signals. IEEE Transactions on Speech and Audio Processing, 12(5):451\u2013459, 2004.\\n\\nS. Doclo, W. Kellermann, S. Makino, and S. E. Nordholm. Multichannel signal enhancement algorithms for assisted listening devices: Exploiting spatial diversity using multiple microphones. IEEE Signal Processing Magazine, 32(2):18\u201330, 2015.\\n\\nA. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W. T. Freeman, and M. Rubinstein. Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation. ACM Transactions on Graphics, 37(4):109:1\u2013109:11, 2018.\\n\\nH. Erdogan, J. R. Hershey, S. Watanabe, M. Mandel, and J. Le Roux. Improved MVDR beamforming using single-channel mask prediction networks. In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), pp. 1981\u20131985, 2016.\\n\\nO. L. Frost. An algorithm for linearly constrained adaptive array processing. Proceedings of the IEEE, 60(8):926\u2013935, 1972.\\n\\nS. Gannot and I. Cohen. Speech enhancement based on the general transfer function GSC and postfiltering. IEEE Transactions on Speech and Audio Processing, 12(6):561\u2013571, 2004.\\n\\nS. Gannot, D. Burshtein, and E. Weinstein. Signal enhancement using beamforming and nonstationarity with applications to speech. IEEE Transactions on Signal Processing, 49(8):1614\u20131626, 2001.\\n\\nR. Giri, B. D. Rao, F. Mustiere, and T. Zhang. Dynamic relative impulse response estimation using structured sparse Bayesian learning. In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 514\u2013518, 2016.\\n\\nR. Giri, T. A. Srikrishnan, B. D. Rao, and T. Zhang. Empirical bayes based relative impulse response estimation. The Journal of the Acoustical Society of America, 143(6):3922\u20133933, 2018.\\n\\nL. Griffiths and C. Jim. An alternative approach to linearly constrained adaptive beamforming. IEEE Transactions on Antennas and Propagation, 30(1):27\u201334, 1982.\\n\\nE. Hadad, F. Heese, P. Vary, and S. Gannot. Multichannel audio database in various acoustic environments. In International Workshop on Acoustic Signal Enhancement (IWAENC), pp. 313\u2013317, 2014.\"}"}
{"id": "DQou0RiwkR0", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"J. H. L. Hansen and B. L. Pellom. An effective quality evaluation protocol for speech enhancement algorithms. In Proceedings of International Conference on Speech and Language Processing (ICSLP), volume 7, pp. 2819\u20132822, 1998.\\n\\nY. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu, B. Zhang, and L. Xie. DCCRN: Deep complex convolution recurrent network for phase-aware speech enhancement. In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), pp. 2472\u20132476, 2020.\\n\\nITU-T Recommendation P.862.2. Wideband extension to recommendation P.862 for the assessment of wideband telephone networks and speech codecs. International Telecommunication Union, 2005.\\n\\nH. Kim, K. Kang, and J. W. Shin. Factorized MVDR deep beamforming for multi-channel speech enhancement. IEEE Signal Processing Letters, 29:1898\u20131902, 2022.\\n\\nJ. Kim, M. El-Khamy, and J. Lee. T-GSA: Transformer with Gaussian-weighted self-attention for speech enhancement. In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 6649\u20136653, 2020.\\n\\nZ. Koldovsky, P. Tichavsky, and D. Botka. Noise reduction in dual-microphone mobile phones using a bank of pre-measured target-cancellation filters. In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 679\u2013683, 2013.\\n\\nZ. Koldovsky, J. Malek, and S. Gannot. Spatial source subtraction based on incomplete measurements of relative transfer function. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(8):1335\u20131347, 2015.\\n\\nY. Koyama and B. Raj. W-Net BF: DNN-based beamformer using joint training approach. arXiv preprint arXiv:1910.14262, 2019.\\n\\nY. Koyama and B. Raj. Exploring optimal DNN architecture for end-to-end beamformers based on time-frequency references. arXiv preprint arXiv:2005.12683, 2020.\\n\\nA. Krueger, E. Warsitz, and R. Haeb-Umbach. Speech enhancement with a GSC-like structure employing eigenvector-based transfer function ratios estimation. IEEE Transactions on Audio, Speech, and Language Processing, 19(1):206\u2013219, 2010.\\n\\nB. Laufer, R. Talmon, and S. Gannot. Relative transfer function modeling for supervised source localization. In IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), pp. 1\u20134, 2013.\\n\\nD. Lee, S. Kim, and J.-W. Choi. Inter-channel Conv-TasNet for multichannel speech enhancement. arXiv preprint arXiv:2111.04312, 2021.\\n\\nJ. Li, Y. Zhu, D. Luo, Y. Liu, G. Cui, and Z. Li. The PCG-AIID system for L3DAS22 Challenge: MIMO and MISO convolutional recurrent network for multi channel speech enhancement and speech recognition. In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 9211\u20139215, 2022.\\n\\nX. Li, L. Girin, R. Horaud, and S. Gannot. Estimation of the direct-path relative transfer function for supervised sound-source localization. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 24(11):2171\u20132186, 2016.\\n\\nY. Lin, J. Chen, Y. Kim, and D. D. Lee. Blind channel identification for speech dereverberation using l_1-norm sparse learning. Advances in Neural Information Processing Systems (NIPS), 20, 2007.\\n\\nC.-L. Liu, S.-W. Fu, Y.-J. Li, J.-W. Huang, H.-M. Wang, and Y. Tsao. Multichannel speech enhancement by raw waveform-mapping using fully convolutional networks. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:1888\u20131900, 2020.\"}"}
{"id": "DQou0RiwkR0", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"X. Lu, Y. Tsao, S. Matsuda, and C. Hori. Speech enhancement based on deep denoising autoencoder. In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), volume 2013, pp. 436\u2013440, 2013.\\n\\nY. Luo and N. Mesgarani. Conv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 27(8):1256\u20131266, 2019.\\n\\nY. Luo, Z. Chen, N. Mesgarani, and T. Yoshioka. End-to-end microphone permutation and number invariant multi-channel speech separation. In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 6394\u20136398, 2020.\\n\\nM. Parchami, W.-P. Zhu, B. Champagne, and E. Plourde. Recent developments in speech enhancement in the short-time Fourier transform domain. IEEE Circuits and Systems Magazine, 16(3):45\u201377, 2016.\\n\\nS. Pascual, A. Bonafonte, and J. Serra. SEGAN: Speech enhancement generative adversarial network. In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), pp. 3642\u20133646, 2017.\\n\\nT. N. Sainath, R. J. Weiss, K. W. Wilson, B. Li, A. Narayanan, E. Variani, M. Bacchiani, I. Shafran, A. Senior, K. Chin, A. Misra, and C. Kim. Multichannel signal processing with deep neural networks for automatic speech recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25(5):965\u2013979, 2017.\\n\\nR. Scheibler, E. Bezzam, and I. Dokmani \u00b4c. Pyroomacoustics: A python package for audio room simulation and array processing algorithms. In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 351\u2013355, 2018.\\n\\nT. A. Srikrishnan, B. D. Rao, R. Giri, and T. Zhang. Improved noise characterization for relative impulse response estimation. In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 411\u2013415, 2018.\\n\\nC. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen. An algorithm for intelligibility prediction of time-frequency weighted noisy speech. IEEE Transactions on Audio, Speech, and Language Processing, 19(7):2125\u20132136, 2011.\\n\\nB. Tolooshams, R. Giri, A. H. Song, U. Isik, and A. Krishnaswamy. Channel-attention dense U-Net for multichannel speech enhancement. In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 836\u2013840, 2020.\\n\\nH. L. Van Trees. Optimum Array Processing: Part IV of Detection, Estimation, and Modulation Theory. Honoken, NJ, USA: John Wiley & Sons, 2004.\\n\\nE. Variani, T. N Sainath, I. Shafran, and M. Bacchiani. Complex linear projection (CLP): A discriminative approach to joint feature extraction and acoustic modeling. In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), pp. 808\u2013812, 2016.\\n\\nZ.-Q. Wang and D. Wang. Mask weighted STFT ratios for relative transfer function estimation and its application to robust ASR. In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 5619\u20135623. IEEE, 2018.\\n\\nZ.-Q. Wang, P. Wang, and D. Wang. Complex spectral mapping for single-and multi-channel speech enhancement and robust ASR. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:1778\u20131787, 2020.\\n\\nZ.-Q. Wang, H. Erdogan, S. Wisdom, K. Wilson, D. Raj, S. Watanabe, Z. Chen, and J. R. Hershey. Sequential multi-frame neural beamforming for speech separation and enhancement. In IEEE Spoken Language Technology Workshop (SLT), pp. 905\u2013911, 2021.\\n\\nD. S. Williamson, Y. Wang, and D. Wang. Complex ratio masking for monaural speech separation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 24(3):483\u2013492, 2015.\"}"}
{"id": "DQou0RiwkR0", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3 AFNET BASED ON COMPLEX W-NET ARCHITECTURE\\n\\nWe introduce the AFnet (Figure 1) for multichannel SE based on implementing a W-Net (Xia & Kulis, 2017) architecture suitable for realizing the two-stage framework. For the SE application here, extension to complex operations can be more suitable when working in the STFT domain. Choi et al. (2018) have proposed to utilize complex-valued U-Net for the single-channel SE task. We follow this idea and extend the W-Net by incorporating complex operations for multichannel SE. In the current work, main modifications to the W-Net include: Convolutional layers of W-Net are all replaced to complex convolutional layers. For the activation function, complex leaky ReLU, i.e., an activation function which applies leaky ReLU on both real and imaginary values, are utilized. The number of feature maps in each layer are also modified. Note that in Figure 1, \\\"dConv (C_{in}, C_{out})\\\" stands for \\\"double Convolutions\\\" with C_{in} input channels and C_{out} output channels. Each dConv unit consists of two stacks of \\\"(complex) 3 \u00d7 3 convolution \u2192 batch normalization \u2192 leaky ReLU.\\\" The convolution layer of the first stack takes C_{in} channels and outputs C_{out} channels; the convolution layer of the second stack takes C_{out} channels and outputs C_{out} channels.\\n\\n3.4 DIFFERENTIATION FROM EXISTING METHODS\\n\\nExisting works such as (Wang et al., 2021; Zhang et al., 2021) leverage the conventional beamformer modules for incorporating spatial features implicitly. However, matrix inversion or eigendecomposition are often required and additional network modules might otherwise be needed to replace such operations. Other works (Koyama & Raj, 2019; 2020; Li et al., 2022) concatenate two DNN modules and jointly train both toward the enhanced speech. However, without utilizing any beamforming units within the model it is hard to interpret if the network actually learns to exploit spatial features. Differently, our method introduces a simple skip connection of alignment masks trained via supervised learning of the RTFs to explicitly incorporate meaningful spatial characteristics.\\n\\n4 EXPERIMENTS\\n\\n4.1 DATASETS AND EXPERIMENTAL SETUP\\n\\n4.1.1 PRIMARY DATASET\\n\\nSpeech corpus: We leverage the AVSpeech dataset which is a large collection of video clips of single speakers talking with no audio background interference introduced in Ephrat et al. (2018). The dataset is based on public instructional YouTube videos, from which short, 3-10 second clips were automatically extracted, where the only audible sound in the soundtrack belongs to a single speaking person. We downloaded 8308 clips from the training set and another 1199 clips from the test set for our experiments, using only the audio portion (converted to 16 kHz .wav files).\\n\\nRoom impulse response (RIR) data: We mix the speech files with noise files of several types also downloaded from YouTube and consider different reverberation conditions for generating the noisy data using simulated impulse responses via Pyroomacoustics (Scheibler et al., 2018), which is a Python-based acoustic simulator for generating different acoustic scenes. We create a room of size 8 \u00d7 8 \u00d7 3 (length \u00d7 width \u00d7 height in meters). Reverberation time is randomly chosen from {0.16, 0.32, 0.48, 0.64} second. The simulating approach for estimating RIRs is based on the image source method. Speech and noise signals are randomly positioned at a distance of {1, 2, 3, 4} meters from the microphone array with arbitrary directions of arrival. The microphones form a 2-by-4 planar array placed vertically at the center of the room, and the spacing between two adjacent microphones is 5 cm. 4 types of commonly seen interference (blender, vacuum, washer, baby cry) are used for training and another 4 types (dog barking, kids playing sound, hair dryer, food sizzling) are used for testing. Each type of noise is multiplied with a scale randomly chosen from {0, 0.5, 1, 1.5, 2} before added up, and the combined noise is then added to the clean speech according to a specific SNR level randomly selected from {-10, -6, -3, 0, 3, 6, 10} dB.\\n\\n4.1.2 AUXILIARY DATASET\\n\\nThe CHiME-3 dataset (Barker et al., 2015), made available as part of a speech separation and recognition challenge, is used for training and evaluating SE performance. The dataset is a 6-channel microphone recording of talkers speaking in a noisy environment, sampled at 16 kHz. It consists of 7,138 and 1,320 simulated utterances with an average length of 3 seconds for training and testing, respectively. We take this dataset only as auxiliary due to limited positional variety of the target speech.\"}"}
{"id": "DQou0RiwkR0", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nAs such, the dataset is less ideal for exploration of spatial characteristics and signal alignment purposes.\\n\\n4.1.3 MODEL SETTINGS AND EVALUATION METRICS\\n\\nIn this paper, we set the number of channels of the AFnet (Figure 1) as:\\n\\n\\\\[ C_1, C_2, C_3, C_4 = 32, 64, 64, 64 \\\\]\\n\\nresulting in a model size of approximately 2.6M parameters. The slope of leaky ReLU is set to 0.01.\\n\\nRegarding the STFT processing of the input signal, for the primary dataset we use the Hann window with 512-point fast Fourier transform (FFT) and for CHiME-3 with 1024-point FFT, both using a hop size of 256.\\n\\nDuring training, 4-second long segments are randomly cropped from the training data samples while during testing the whole utterances are used. The RTFs are obtained by computing the ratios between the clean speech signals at all microphones according to (4). We use the following three commonly seen SE metrics for evaluation purposes.\\n\\n- **PESQ**: Perceptual Evaluation of Speech Quality (ITU-T Recommendation P.862.2, 2005).\\n- **STOI**: Short-Time Objective Intelligibility (Taal et al., 2011).\\n- **SSNR**: Segmental Signal-to-Noise Ratio (Hansen & Pellom, 1998) (segment length = 30 msec, 75% overlap, \\\\( \\\\text{SNR}_{\\\\text{min}} = -10 \\\\) dB, \\\\( \\\\text{SNR}_{\\\\text{max}} = 35 \\\\) dB). For all the metrics, the higher the score, the better the performance.\\n\\nMore details of experiential settings can be found in Appendix B.\\n\\n4.2 RESULTS\\n\\n4.2.1 ALIGNED VS. UNALIGNED SIGNAL ENHANCEMENT\\n\\nTo get an understanding of the importance of alignment, we first conduct an experiment to compare the difference between enhancement outcomes of aligned and unaligned input signals with the primary dataset.\\n\\nTo this end, two schemes are considered, where in the first scheme we manually align the noisy microphone signals by multiplying them with the corresponding ground truth speech RTFs, while in the other scheme we directly use the original unaligned signals.\\n\\nIn each scheme, a single U-Net model, or equivalently, the Filter Net (the second half of AFnet), is trained for the signal reconstruction loss \\\\( L_{rec} \\\\) and tested on the corresponding aligned or unaligned data. The performance numbers of the two cases are presented in Table 1.\\n\\nIt can be seen that when the signals are aligned, the Filter Net model can do a much better job of separating the target signal from background noise and achieves significantly better scores than unaligned results in all three metrics. This observation is consistent across all three number of microphone settings. The results suggest that for a spatially diverse dataset where the target speech may come from various locations, having a signal alignment module in front of the signal filtering module can be beneficial.\\n\\nWe also present the results for the CHiME-3 dataset in Table 1. We can see that the margin between the results of using the aligned and unaligned signals is much smaller here. It could be attributed to that most of the CHiME-3 recording data samples have the speaker almost speaking from the front side with respect to the microphones which results in less variety in directional angles (the common situation when the speaker was asked for holding a tablet to speak the sentence listed in the tablet, and the speech data were recorded through 6 embedded microphones in the same tablet. See the recording setup and recording demonstration picture in CHiME-3 official website) and thus the less spatial diversity. Moreover, as the speech sources mostly come from the front side, they are in a sense already roughly aligned. Thus, there is the less benefit brought by performing the alignment, meaning that the requirement of aligning signals is bypassed to some extent.\"}"}
{"id": "DQou0RiwkR0", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Cases i) and ii) represent the typical approach which uses a DNN to directly estimate the beamformer weights. Cases iii) and iv) are considered to observe the role of RTF alignment in the AFnet.\\n\\nFigure 2 presents the enhancement results. One can see that the typical approaches of U-Net and W-Net for direct filter weights estimation do not seem to actually benefit from the multichannel input data, as the performance is not necessarily improving with increased number of microphones. In fact, the W-Net even performs worse than the single U-Net model for the cases with more microphones (4-mic, 8-mic) though doubling the model size, with its best result surprisingly reached for the 2-mic case. This observation implies that to further take advantage of audio data with more microphone channels, simply stacking U-Net models may not straightforwardly work well. This might be due to the potential optimization difficulty of having a deeper model and more complicated multichannel input data to extract useful information. On the other hand, the AFnet seems to mitigate these issues by introducing the sequential masking mechanism which also provides certain input signal level skip connections, as we can see that even without RTF supervision it is able to perform much better than the W-Net. Nevertheless, the performance of AFnet without RTF supervision does not necessarily improve with increased number of microphones. This may be a result of lacking guidance on learning meaningful spatial features. Finally we see that with the RTF loss incorporated for explicitly leveraging spatial separability, the AFnet is able to obtain better performance as the number of microphones increases, and achieves the best performance overall.\\n\\n4.2.3 Visualizing the alignment outcomes\\n\\nTo further observe the effect of RTF supervision, we visualize the distributions of the alignment masks in Figure 3 from AFnet trained with RTF loss and that without RTF loss. To obtain these plots, we take noisy signal samples from the test set in which the speech source in each sample is located at one of three pre-defined positions in the room. We feed the taken noisy samples to the trained AFnet models (both with and without RTF supervision) and obtain the alignment masks for each sample. The distribution of the principal components (PCs) of the obtained masks are plotted. It can be seen that with RTF-aware training, the distributions show clearer clusters of speech sources corresponding to the three different positions than with normal training. In addition, with increased number of microphones the clusters also become more separate. The results together with improved enhanced speech quality provide evidence of the fact that the system is actually exploiting spatial separability of the audio sources for achieving better enhancement performance.\\n\\n4.2.4 Comparison to SOTA methods\\n\\nTo demonstrate the superiority of our method, we compare the AFnet with several SOTA architectures, including:\\n\\n- **Conv-TasNet** (Luo & Mesgarani, 2019): a single-channel SE system performing masking-based separation method in the learned transform domain via learnable encoder and decoder.\\n- **DCUnet** (Choi et al., 2018): a phase-aware single-channel SE approach utilizing complex-valued operations with the U-Net architecture.\\n- **FaSNet** (Luo et al., 2020): a time-domain, multi-channel SE system exploiting cross-channel correlation based on filter-and-sum method.\"}"}
{"id": "DQou0RiwkR0", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Visualization of the distributions of alignment masks. The RTF-aware training cases (Upper Row) show obvious clusters of signals coming from the three different locations (loc 1, loc 2, loc 3) as the number of microphone increases, while the normal training cases (Lower Row) do not present clear boundaries.\\n\\nWe train the SOTA models by ourselves. For fairness, all models are trained by minimizing the reconstruction loss $L_{\\\\text{rec}}$ of the combined power-law compressed MSE of (7), using 512-point FFT with a hop size of 256 via the Hann window. The results are presented in Table 2. It can be seen that the single-channel methods do not perform well even with a larger model size. The FaSNet, with a lighter model, performs slightly better than the two single-channel approaches when the number of microphones is larger. Finally, our proposed AFnet outperforms all the SOTA approaches by significant amount in all the metrics used, while being of the smallest model size. Note that the performance of AFnet lies in between the aligned and unaligned signal cases in Table 1, which is reasonable as the alignment is never perfect but only to certain degree.\\n\\nAlthough a less ideal dataset for our study on spatial characteristics, we still benchmark the AFnet with several SOTAs on the CHiME-3 dataset, including: Neural BF (Erdogan et al., 2016), MVDR GC (Bu et al., 2019), rSDFCN (Liu et al., 2020), CA Dense U-Net (Tolooshams et al., 2020), and IC Conv-TasNet (Lee et al., 2021). In Table 3 we report the PESQ and STOI taken from the corresponding papers and the missing entries in the table indicate that the metric is not reported in the reference paper (we do not report SSNR as most papers do not report it). We present both results of the AFnet trained with and without RTF supervision (with the 5-th channel selected as the reference microphone). One can see that the two-stage, sequential masking design of the AFnet is efficient in that it achieves comparable or better performance to other SOTA approaches while maintaining a small model size. Notably, the RTF supervision helps improve the AFnet performance, while the gains are minor in this case due to the less spatial diversity of the speech data. Nevertheless, the results still show the benefit of incorporating signal alignment into the network design, even though on a dataset with limited positional variety of the speech target.\\n\\n4.2.5 Importance of Phase Alignment in RTF Estimation\\n\\nThe Align Net trained with the RTF loss performs both temporal alignment as well as level adjustment. It would be interesting to look at how the magnitude and phase components of the RTFs, which correspond to ILD and ITD in time domain, affect the enhancement performance of the AFnet. In Figure 4 we compare four different RTF alignment schemes \u2013 no alignment, aligning RTF magnitude only, aligning RTF phase only, and aligning everything for the RTF loss $L_{\\\\text{rtf}}$. From the results it can be seen that phase alignment is the key to obtaining improved performance over the alignment-agnostic system. This is reasonable as aligning phase in the frequency domain corresponds to compensating the delay in the time domain, which is crucial for any beamforming algorithms that rely on spatial separability. The results suggest that the AFnet indeed exploits spatial information for improved enhancement performance.\"}"}
{"id": "DQou0RiwkR0", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FaSNet: the experimental results of FaSNet is based on the implementation of the code at: https://github.com/yluo42/TAC. We choose to use the FaSNet TAC model where TAC stands for transform-average-concatenate. Hyperparameters: enc_dim=64, feature_dim=64, hidden_dim=128, layer=4, segment_size=50, nspk=1, win_len=4, context_len=16, sr=16000.\\n\\nModel optimization: All experiments were run on one NVIDIA Tesla V100 GPU of 32 GB CUDA memory.\\n\\nAFnet training: the AFnet is trained via the RTF-aware training scheme. The Adam optimizer is used for minimizing $L_{rtf}$ with a total of 80 epochs, where for the first 50 epochs a learning rate of 0.001 is used and for the rest 30 epochs it is decreased to 0.0001. For minimizing $L_{rec}$ we train the network for another 55 epochs with Adam, where for the first 50 epochs a learning rate of 0.001 is used and for the rest 5 epochs it is decreased to 0.0001. The total number of epochs is 80+55=135. A batch size of 4 is used.\\n\\nAFnet training (w/o RTF supervision): the AFnet trained without RTF loss is just minimizing $L_{rec}$. In this case, the Adam optimizer with a learning rate of 0.001 is used, which is decreased to 0.0001 at the 50-th epoch, and with a total of 80 epochs. A batch size of 4 is used.\\n\\nConv-TasNet, DCUnet, and FaSNet training: for fair comparison purposes, the training of the SOTA methods is same by minimizing the reconstruction loss $L_{rec}$ using 512-point FFT with a hop size of 256 via the Hann window (same STFT setting as AFnet). For the time-domain approaches (Conv-TasNet and FaSnet), the waveform of the model output is converted to STFTs for computing the loss. In all cases, the Adam optimizer with a learning rate of 0.001 is used, which is decreased to 0.0001 at the 50-th epoch, and with a total of 80 epochs. A batch size of 4 is used.\\n\\nB.3 CHiME-3 DATASET\\n\\nDataset: The publicly available CHiME-3 dataset (Barker et al., 2015), made available as part of a speech separation and recognition challenge, is used for training and evaluating SE performance. The dataset is a 6-channel microphone recording of talkers speaking in a noisy environment, sampled at 16 kHz. It consists of 7,138 and 1,320 simulated utterances with an average length of 3 seconds for training and testing, respectively.\\n\\nSOTAs to compare:\\n\\n- Neural BF (Erdogan et al., 2016): An MVDR beamforming with mask estimation through bidirectional-LSTM.\\n- MVDR GC (Bu et al., 2019): An MVDR beamforming using a neural network-based method to identify and correct phase errors in the steering vector.\\n- rSDFCN (Liu et al., 2020): A time-domain, fully convolutional network (FCN) with sinc and dilated convolutional layers for multichannel SE.\\n- CA Dense U-Net (Tolooshams et al., 2020): A time-frequency domain multichannel SE model that combines the merits of DenseNet, U-Net, and channel attention (CA) mechanism.\\n- IC Conv-TasNet (Lee et al., 2021): The Inter-Channel (IC) Conv-TasNet is the extension of the time-domain Conv-TasNet for single-channel SE to the multichannel SE case.\\n\\nSpectral processing: We use the Hann window with 1024-point FFT and a hop size of 256 for the STFT. By conjugate symmetry of the FFT, only half of the frequency bins (i.e., 513) are actually needed for the network to process the spectrograms.\\n\\nModel optimization: The AFnet training was run on one NVIDIA Tesla V100 GPU of 32 GB CUDA memory using the same model optimization scheme discussed in Section B.2.\"}"}
{"id": "DQou0RiwkR0", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.4 Evaluation Metrics\\n\\n**PESQ:** a speech quality measure using the wide-band version recommended in ITU-T P.862.2. It basically models the mean opinion scores (MOS) that cover a scale from 1 (bad) to 5 (excellent). We use the Python-based PESQ implementation from: [https://github.com/ludlows/python-pesq](https://github.com/ludlows/python-pesq).\\n\\n**STOI:** a function that well represents the average intelligibility of the degraded speech. It provides a value from 0 to 1, which can be interpreted as the percentage of correctly recognized words by normal-hearing people. We use the Python-based STOI implementation from: [https://github.com/mpariente/pystoi](https://github.com/mpariente/pystoi).\\n\\n**SSNR:** an SNR measure, instead of working on the whole signal, that calculates the average of the SNR values of short segments (segment length = 30 msec, 75% overlap, $\\\\text{SNR}_{\\\\text{min}} = -10$ dB, $\\\\text{SNR}_{\\\\text{max}} = 35$ dB). We use the Python-based SSNR implementation from: [https://github.com/schmiph2/pysepm](https://github.com/schmiph2/pysepm).\\n\\n### Appendix C Additional Experimental Results\\n\\n#### C.1 Comparison of RTF-aware Training Schemes (Two-Step vs. Regularized)\\n\\nIn this section we try to answer whether the two-step RTF-aware training scheme (Algorithm 1) or the regularized training scheme (Algorithm 2) is better. To this end, we train the AFnet model with several values of $\\\\lambda$ using Algorithm 2 and the results are compared to AFnet trained with Algorithm 1 in Table 5. It can be seen that both training schemes help improve over the RTF-agnostic results (i.e., $\\\\lambda = 0$) as they both incorporate RTF information for explicitly learning spatial characteristics. However, the two-step approach performs consistently better than the regularized approach. The results suggest that learning to align prior to learning to denoise during the training stage is more beneficial.\\n\\n| Method          | PESQ 2-mic | PESQ 4-mic | PESQ 8-mic | STOI 2-mic | STOI 4-mic | STOI 8-mic | SSNR 2-mic | SSNR 4-mic | SSNR 8-mic |\\n|-----------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|\\n| Algorithm 1     | 1.84       | 1.92       | 1.99       | 0.728      | 0.739      | 0.753      | 5.06       | 5.26       | 5.35       |\\n| Algorithm 2, $\\\\lambda = 0$ | 1.80       | 1.72       | 1.88       | 0.720      | 0.711      | 0.734      | 4.99       | 4.66       | 5.05       |\\n| Algorithm 2, $\\\\lambda = 0.0001$ | 1.77       | 1.77       | 1.88       | 0.718      | 0.714      | 0.735      | 4.80       | 4.76       | 5.10       |\\n| Algorithm 2, $\\\\lambda = 0.01$ | 1.80       | 1.73       | 1.88       | 0.721      | 0.714      | 0.735      | 4.98       | 4.64       | 5.10       |\\n| Algorithm 2, $\\\\lambda = 0.1$ | 1.78       | 1.82       | 1.84       | 0.718      | 0.729      | 0.726      | 4.91       | 4.94       | 4.98       |\\n\\n#### C.2 Fixing vs. Unfixing Align Net While Minimizing Reconstruction Loss\\n\\nWe compare two additional training schemes of the AFnet \u2013 fixing or unfixing the Align Net parameters after it has been trained via the RTF loss $L_{\\\\text{rtf}}$, when we train the full AFnet for the reconstruction loss $L_{\\\\text{rec}}$. Table 6 presents the results. It can be seen that adjusting the Align Net parameters together with the Filter Net parameters for optimizing the reconstruction loss is crucial for achieving better performance compared to fixing the Align Net after the RTF loss minimization phase. This indicates that both stages may share the task of performing spatial and spectral denoising jointly, where the first stage shares more loading on the spatial denoising part while the second stage is more on the spectral filtering portion. Dividing the SE task into the two subtasks makes the network learn to more efficiently denoise the speech.\"}"}
{"id": "DQou0RiwkR0", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: SE performance comparison between using fixed (F) and unfixed (U) Align Net for training the AFnet after the Align Net has been trained with the RTF loss.\\n\\n|   | Mic | PESQ | STOI | SSNR |\\n|---|-----|------|------|------|\\n|   | F   | U    | F    | U    |\\n| 2 | 1.77| 1.84 | 0.712| 0.728|\\n| 4 | 1.81| 1.92 | 0.721| 0.739|\\n| 8 | 1.84| 1.99 | 0.728| 0.753|\\n\\nC.3 Visualizing the dataset spatial diversity\\n\\nGiven the ability of AFnet to exploit spatial separability, it is also interesting to utilize the AFnet for inspecting the spatial diversity of the dataset. Such information can be provided by observing the alignment masks of the data points. In Figure 5 we compare the distributions of the intermediate alignment masks learned by the AFnet models trained on the primary dataset (high spatial diversity) and the CHiME-3 dataset (low spatial variety). Here, we plot the distributions of the alignment masks estimated by the trained models for all the test data samples of the two datasets. It can be seen that for the primary dataset, the RTF-aware training leads to a more widespread distribution than that of the normal training case, corresponding to the greater spatial diversity of the target speech source positions. In contrast, in the CHiME-3 dataset the RTF-aware training actually results in a more aggregated cluster than the normal training scheme, indicating that the target speech sources are coming from similar directions. This visualization of the alignment masks provides another angle to inspect the data properties from the spatial diversity aspect for exploring SE performance.\\n\\nFigure 5: Visualization of the distributions of alignment masks. (Left): the estimated masks for the primary dataset using AVSpeech audio with Pyroomacoustics. (Right): the estimated masks for the CHiME-3 dataset.\\n\\nC.4 Enhancement performance vs. input SNR condition comparison\\n\\nTo look more into the testing results, we present the PESQ, STOI, and SSNR performance numbers vs. input SNR conditions in Figure 6 for the three schemes:\\n\\ni) AFnet: the two-stage AFnet model trained with the RTF-aware training scheme\\n\\nii) AFnet w/o RTF loss: the two-stage AFnet model trained with normal training scheme without incorporating RTFs\\n\\niii) W-Net for filter weights: using a W-Net (same model as the AFnet, but the output of the first U-Net block goes directly to the next U-Net block without multiplying the input signals) to directly estimate the sets of filter weights $W_i$ in typical multichannel SE\\n\\nWe take 160 utterances for each specified input SNR value from the test set and obtain the corresponding performance number of each input SNR value. The results for the 8-mic systems are presented. From the results we can see that the proposed AFnet consistently improves the baseline method over a wide range of input SNR conditions.\"}"}
{"id": "DQou0RiwkR0", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.5 GENERALIZATION TO OTHER ROOM CONFIGURATIONS\\n\\nTo further test the generalization of the proposed AFnet to unseen room configurations, we utilize the Pyroomacoustic simulator to generate test data randomly sampled from three different room sizes: (8 \u00d7 5 \u00d7 3), (7 \u00d7 6 \u00d7 4), and (8 \u00d7 6 \u00d7 3) in terms of (length \u00d7 width \u00d7 height) in meters, which are different from the training room (i.e., (8 \u00d7 8 \u00d7 3)). Figure 7 presents the SE results under the unseen room configurations. We can see that the proposed AFnet generalizes well to unseen room schemes as it still improves over the typical direct filter weight estimation approaches in most cases.\\n\\nMoreover, the performance of AFnet consistently improves as the number of microphones increases.\\n\\nC.6 SE UNDER TIME-VARYING RIR SCENARIOS\\n\\nThe above results have been tested on a relatively static environment where the target speech position of an utterance, once given, is not changing. However, in most real-world scenarios the speaker may naturally move around and therefore the RIRs will not always be time-invariant. To test the proposed AFnet under such circumstances, we leverage the Pyroomacoustic simulator to generate another set of test data (same amount of data as the static RIR case in the previous sections) in which during an utterance the RIRs are changed from one set to another to emulate speaker movements. We evaluate the algorithms on such time-varying RIR generated multichannel dataset and the results are presented in Figure 8. Note that the testing is performed on the same models that have been trained only with the data generated with static RIRs before. From the results we can see that the proposed AFnet has a better tracking ability to tackle time-varying target speech locations, even though the model has not observed any changing RIR data during the training stage. This indicates that the RTF supervision of the alignment masks is also crucial for SE under time-varying RIR environments \u2013 by incorporating phase alignment into the model learning, the positional information of the target speech can be better captured for improved SE even under the moving target scenario.\\n\\nC.7 RESULTS ON REAL-WORLD MEASURED RIR GENERATED DATA\\n\\nThe results presented in previous sections on the primary dataset are based on the multichannel audio data synthesized by using the image source method via the Pyroomacoustics library, which is useful for methodological study but also has its limitation to represent real data. To further validate the proposed approach\u2019s generalization capabilities to more realistic scenarios, we also conduct...\"}"}
{"id": "DQou0RiwkR0", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: SE performance under time-varying RIR scenarios.\\n\\nExperiments on multichannel audio data generated using real-world measured acoustic RIRs taken from the work of Hadad et al. (2014), where the impulse responses of several 8-channel linear microphone arrays with respect to various target source directions of arrival (DoAs) (from -90 to 90 degrees) are provided. We utilize the 3-3-3-8-3-3-3 array impulse responses of the dataset including reverberation time of 0.16, 0.36, and 0.61 second cases, together with the AVSpeech utterances and the YouTube noise profiles to generate the multichannel noisy data for experiments. To observe the relation of the AFnet performance with the speech source spatial diversity, we generate a spatially diverse dataset where the speech DoA ranges from -90 to 90 degrees, and another spatially constant dataset where the speech comes from only one direction.\\n\\nTable 7 and Table 8 present the results for the two datasets respectively. One can see that for the spatially diverse dataset of Table 7, the AFnet clearly benefits from the sequentially masking mechanism and the RTF loss supervision, while for the spatially constant dataset of Table 8 the three models perform comparably with the RTF loss marginally improves AFnet. These results show that a model (such as the W-Net) that performs well on a dataset lacking spatial variety of the target speech may not generalize well to a spatially diverse dataset, unless further attention is paid to exploiting spatial characteristics of the speech sources. The RTF information leveraged by the two-stage sequential masking design of the AFnet is shown to take advantage of the spatial separability for enhanced SE performance on spatially diverse datasets.\\n\\nTable 7: Results on spatially diverse multichannel data generated using measured acoustic RIRs where the target speech DoA range is [-90, 90] degrees.\\n\\n| Methods                      | PESQ | STOI   | SSNR  |\\n|------------------------------|------|--------|-------|\\n| Noisy                        | 1.40 | 0.598  | -0.91 |\\n| W-Net for filter weights     | 1.89 | 0.693  | 3.20  |\\n| AFnet w/o RTF Loss           | 2.06 | 0.728  | 3.83  |\\n| AFnet                        | 2.22 | 0.759  | 4.16  |\\n\\nTable 8: Results on spatially constant multichannel data generated using measured acoustic RIRs where the target speech DoA is fixed to one direction.\\n\\n| Methods                      | PESQ | STOI   | SSNR  |\\n|------------------------------|------|--------|-------|\\n| Noisy                        | 1.42 | 0.609  | -0.51 |\\n| W-Net for filter weights     | 1.93 | 0.708  | 3.81  |\\n| AFnet w/o RTF Loss           | 1.93 | 0.702  | 3.95  |\\n| AFnet                        | 1.99 | 0.717  | 3.40  |\\n\\nSimilar to what have been done in Figure 3 for the simulated RIR case, we also visualize the estimated alignment masks for the spatially diverse dataset of the realistic RIR case. The results in Figure 9 once again show that the RTF-aware training results in clear clusters of the speech sources coming from difference directions as compared to the normal training case, confirming that the network is more aware of the spatial separability of the sound sources.\\n\\nAppendix D Alignment Net vs. Conventional Signal Processing Approaches for RTF Estimation\\n\\nIn this section, we study the effectiveness of the deep learning-based Align Net for estimating the RTFs compared to conventional model-based signal processing methods in the presence of noise. We show that the Align Net performs reasonably well in the low SNR regime where signal processing approaches fail to, justifying its usage of alignment purposes for later enhancement processing in AFnet.\"}"}
{"id": "DQou0RiwkR0", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Visualization of the distributions of alignment masks for the spatially diverse data generated using real-world measured acoustic RIRs. Again, the RTF-aware training case (Left) shows obvious clusters of signals coming from the three different locations (loc 1, loc 2, loc 3), while the normal training case (Right) does not present clear boundaries.\\n\\nD.1 RTF ESTIMATION PROBLEM\\n\\nRTFs or their time-domain counterparts, relative impulse responses (ReIRs) are important tools in several multichannel audio processing tasks (Gannot & Cohen, 2004; Laufer et al., 2013). The estimation of RTFs has been studied for a while in the audio signal processing field (Gannot et al., 2001; Koldovsk`y et al., 2015; Giri et al., 2016; 2018; Srikrishnan et al., 2018), where the task is to estimate the correlation between the target source components received by the two microphones, either in terms of RTFs or ReIRs, given the noisy recordings. Following the previous works, we will discuss the scenario with two microphones (i.e., left and right microphones) while the idea may be generalized to more microphones.\\n\\nConsider a two-channel noisy recording of a target speech in a noisy environment, whose position is fixed for a certain time interval. This situation can be represented as:\\n\\n\\\\[\\nx_L(n) = h_L(n) * s_L(n) + v_L(n),\\n\\\\]\\n\\n\\\\[\\nx_R(n) = h_R(n) * s_L(n) + v_R(n),\\n\\\\]\\n\\n(11)\\n\\nwhere \\\\( n \\\\) is the time sample index taking values \\\\( 1, \\\\ldots, N \\\\); \\\\( * \\\\) denotes the convolution; \\\\( x_L \\\\) and \\\\( x_R \\\\) are, respectively, the signals from the left (\\\\( L \\\\)) and right (\\\\( R \\\\)) microphones; \\\\( h_L \\\\) and \\\\( h_R \\\\) are the impulse responses between the target and the two microphones; \\\\( s \\\\) is the far-field target speech; and \\\\( v_L \\\\) and \\\\( v_R \\\\) are the noise components.\\n\\nLet \\\\( h_{rel} \\\\) represent the ReIR between the speech signal components arriving at the two microphones, we have following relation:\\n\\n\\\\[\\nx_L(n) = h_{rel}(n) * x_R(n) + \\\\left[ v_L(n) - h_{rel}(n) * v_R(n) \\\\right],\\n\\\\]\\n\\n(12)\\n\\nThe main goal is to estimate \\\\( h_{rel} \\\\) given \\\\( x_L \\\\) and \\\\( x_R \\\\). The issue is in the presence of noise \\\\( v \\\\) the estimation becomes challenging. The oracle ReIR is given as \\\\( h_{rel} = h_L * h_{R}^{-1} \\\\), where \\\\( h_{R}^{-1} \\\\) denotes the filter inverse to \\\\( h_R \\\\). To ensure that the solution is causal, a fixed delay of a few milliseconds can be introduced (Lin et al., 2007; Koldovsk `y et al., 2013), i.e., \\\\( h_{rel} = h_L * h_{R}^{-1} * \\\\delta(n - D) \\\\), where \\\\( \\\\delta(\\\\cdot) \\\\) is the unit impulse function and \\\\( D \\\\) is the delay in samples. In the STFT domain, (12) can be equivalent to:\\n\\n\\\\[\\nX_L(f, t) = H_{rel}(f) X_R(f, t) + V(f, t),\\n\\\\]\\n\\n(13)\\n\\nwhere the oracle RTF is given by:\\n\\n\\\\[\\nH_{rel}(f) = H_L(f) H_R(f).\\n\\\\]\\n\\n(14)\"}"}
{"id": "DQou0RiwkR0", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The goal again is to estimate $H_{rel}$ given the microphone signals $X_L$ and $X_R$. Note that under the assumption that the source position is fixed, we have the transfer functions independent of the time (thus omitting the index $t$).\\n\\n**D.2 METHODS**\\n\\n**Signal processing-based methods**\\n\\n- **Frequency-domain approaches:** The RTF estimation problem can be addressed in the frequency domain by utilizing signal and noise statistics, i.e., using the power spectral density (PSD) entities. To deal with noise, Gannot et al. (2001) propose the non-stationarity based frequency domain (NSFD) method relying on the assumption that noise signals are stationary, or less dynamic, when compared to the target speech signal. To be more exact, let $\\\\Phi_{AB}(f, t)$ denote the (cross)-PSD between $A$ and $B$ during the $t$-th frame, we have:\\n\\n$$\\\\Phi_{X_L X_R}(f, t) = H_{rel}(f)\\\\Phi_{X_R X_R}(f, t) + \\\\Phi_{V X_R}(f, t).$$\\n\\n(15)\\n\\nAssuming over $t = 1, \\\\ldots, P$ frames the the noise is stationary, we can write $\\\\Phi_{V X_R}(f, t) = \\\\Phi_{V X_R}(f)$ and solve the overdetermined set of equations:\\n\\n$$\\\\begin{bmatrix} \\\\Phi_{X_L X_R}(f, 1) & \\\\cdots & \\\\Phi_{X_L X_R}(f, P) \\\\end{bmatrix} = \\\\begin{bmatrix} \\\\Phi_{X_R X_R}(f, 1) & 1 \\\\cdots & \\\\Phi_{X_R X_R}(f, P) & 1 \\\\end{bmatrix} H_{rel}(f)\\\\Phi_{V X_R}(f).$$\\n\\n(16)\\n\\nIn practice the PSDs in the above set of equations are replaced by their sample estimates.\\n\\n- **Time-domain approaches:** The ReIR estimation problem can be formulated in the time domain as:\\n\\n$$h_{rel} = \\\\arg \\\\min_{h \\\\in \\\\mathbb{R}^M} L(x_L, X_R h),$$\\n\\nwhere $x_L = [x_L(1-D), \\\\ldots, x_L(N-D)]^T$ and $D$ is an integer delay for causality, and $X_R$ is the convolution matrix of dimensions $N \\\\times M$ constructed from $x_R$. When the cost function $L(\\\\cdot, \\\\cdot)$ is the squared Euclidean distance, the problem can be solved by least squares (noise-free) or regularized least squares (noisy) methods. To better handle the noise, prior knowledge about the ReIR can be leveraged to improve the estimation, e.g., utilizing the structural sparsity of ReIRs. One popular approach is the weighted $\\\\ell_1$ approach (Benichoux et al., 2014; Koldovsk\u00fd et al., 2015; Giri et al., 2018):\\n\\n$$h_{rel} = \\\\arg \\\\min_{h \\\\in \\\\mathbb{R}^M} \\\\|x_L - X_R h\\\\|_2^2 + \\\\lambda \\\\|w \\\\odot h\\\\|_1,$$\\n\\n(17)\\n\\nwhere $w = [w_1, \\\\ldots, w_M]^T$ is a vector of non-negative weights and $\\\\odot$ denotes the Hadamard product. To mimic the expected structure of a ReIR, the weights are chosen as follows:\\n\\n$$w_i = k_1 e^{k_2 |i-D|} k_3, i = 1, \\\\ldots, M,$$\\n\\n(18)\\n\\nwhere $k_1$, $k_2$, and $k_3$ are positive constants and $D$ is the integer delay.\\n\\n**Deep learning-based approach (proposed Align Net):**\\n\\nIn the proposed AFnet, the first half Align Net is utilized to perform signal alignment by estimating RTFs, which can be viewed as a deep learning-based RTF estimation method. By exploiting the power of complex-valued deep networks for predicting the RTFs, the data-driven Align Net approach can be shown to outperform conventional model-based signal processing methods under noisy environments.\\n\\n**D.3 EXPERIMENTS**\\n\\n**Settings:** We generate 2-mic noisy data using Pyroomacoustics library for conducting the RTF estimation experiments. We create a room of size $8 \\\\times 8 \\\\times 3$ (length $\\\\times$ width $\\\\times$ height in meters). Reverberation time is randomly chosen from $\\\\{0.16, 0.32, 0.48, 0.64\\\\}$ second. The simulating approach for estimating RIRs is based on the image source method. The two microphones are placed with a distance of 15 cm in between, where the center is placed at a distance of 1 meter to one of the walls, with a height of 1.525 meters, and the array is positioned horizontally to the wall with equal distance to the two sides. Speech and noise signals are randomly positioned at a distance of $\\\\{1, 2, 3, 4\\\\}$ meters from the microphone array. 649 files from the training set of AVSpeech dataset are used for training the Align Net and 160 files from the testing set are used for evaluating the competing.\"}"}
{"id": "DQou0RiwkR0", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"algorithms (i.e., Align Net, time-domain weighted \\\\( \\\\ell_1 \\\\) approach, and NSFD approach). 4 types of commonly seen interference (blender, vacuum, washer, baby cry) are used for training and another 4 types (dog barking, kids playing sound, hair dryer, food sizzling) are used for testing. Each type of noise is multiplied with a scale randomly chosen from \\\\( \\\\{0, 0.5, 1, 1.5, 2\\\\} \\\\) before added up, and the combined noise is then added to the clean speech according to a specific SNR level randomly selected from \\\\( \\\\{-10, -6, -3, 0, 3, 6, 10\\\\} \\\\) dB for training and \\\\( \\\\{0, 5, 10, 15\\\\} \\\\) dB for testing.\\n\\nFor the frequency-domain approaches (Align Net and NSFD) we use the Hann window with 512-point FFT and a hop size of 256 for the STFT. For Align Net we use \\\\( C_1, C_2, C_3, C_4 = 32, 64, 64, 64 \\\\) and train for the RTF loss \\\\( L_{rtf} \\\\). For NSDF we use 250 frames as a processing block and smoothing over 5 frames for computing the PSD entities. For the time-domain weighted \\\\( \\\\ell_1 \\\\) we use \\\\( k_1 = 0.1, k_2 = 0.11 \\\\) and \\\\( k_3 = 0.3 \\\\) with \\\\( \\\\lambda = 0.0001 \\\\); and a ReIR length of \\\\( M = 1024 \\\\) for each processing block of 2048 samples.\\n\\nEvaluation metric: To quantitatively evaluate the competing algorithms, we use a well-known and widely used performance metric called the Attenuation Rate (ATR) (Koldovsk\u00fd et al., 2015), which can be evaluated as the ratio between \\\\( \\\\text{SNR}_{in} \\\\) and \\\\( \\\\text{SNR}_{out} \\\\) in dB scale, where:\\n\\n\\\\[\\n\\\\text{SNR}_{in} = \\\\frac{\\\\sum_{i \\\\in \\\\{L,R\\\\}} \\\\sum_{n} |h_i(n) \\\\ast s(n)|^2}{\\\\sum_{i \\\\in \\\\{L,R\\\\}} \\\\sum_{n} |v_i(n)|^2} \\\\tag{19}\\n\\\\]\\n\\nand\\n\\n\\\\[\\n\\\\text{SNR}_{out} = \\\\frac{\\\\sum_{n} |h_{rel}(n) \\\\ast s_R(n) - s_L(n)|^2}{\\\\sum_{n} |h_{rel}(n) \\\\ast v_R(n) - v_L(n)|^2} \\\\tag{20}\\n\\\\]\\n\\nThe numerator of \\\\( \\\\text{SNR}_{out} \\\\) measures the leakage of the target signal whereas the denominator measures the attenuation of the noise signal. The more negative the value (in dB) of ATR is, the better the evaluated algorithm performs.\\n\\nResults: The ATR results are presented in Table 9. It can be seen that for all the methods the performance degrades as SNR decreases, since it becomes more challenging to estimate the RTF when noise is stronger. However, the proposed Align Net significantly outperforms the conventional model-based approaches over all SNR settings, indicating the effectiveness of the data-driven approach for the RTF estimation task against existing methods. In future study, improving the design of Align Net for better RTF estimation as well as its applications to other audio processing tasks (e.g., sound localization, speech separation, etc.) can be interesting topics to explore.\\n\\n| Methods              | Input SNR | 0 dB | 5 dB | 10 dB | 15 dB |\\n|----------------------|-----------|------|------|-------|-------|\\n| Unprocessed          |           | -1.33| -1.34| -1.20 | -1.64 |\\n| NSFD                 |           | -2.79| -3.79| -5.14 | -5.78 |\\n| Weighted \\\\( \\\\ell_1 \\\\)|           | -2.15| -3.99| -5.86 | -6.64 |\\n| Align Net (proposed) |           | -5.00| -6.27| -7.79 | -8.63 |\"}"}
