{"id": "4iQuByhNie", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C-TEXT-CONTEXTUAL-PHRASE GENERATION\\n\\nAnonymous authors\\nPaper under double-blind review\\n\\nABSTRACT\\n\\nNamed Entity Recognition (NER) has seen significant progress in recent years, with numerous state-of-the-art (SOTA) models achieving high performance. However, very few studies have focused on the generation of entities' context. In this paper, we introduce C-CONTEXT-NER, a task that aims to generate the relevant context for entities in a sentence, where the context is a phrase describing the entity but not necessarily present in the sentence. To facilitate research in this task, we also present the EDGAR10-Q dataset, which consists of annual and quarterly reports from the top 1500 publicly traded companies. The dataset is the largest of its kind, containing 1M sentences, 2.8M entities, and an average of 35 tokens per sentence, making it a challenging dataset. We propose a baseline approach that combines a phrase generation algorithm with inferencing using a 220M language model, achieving a ROUGE-L score of 27% on the test split. Additionally, we perform a one-shot inference with ChatGPT, which obtains a 30% ROUGE-L, highlighting the difficulty of the dataset. We also evaluate models such as T5 and BART, which achieve a maximum ROUGE-L of 49% after supervised finetuning on EDGAR10-Q. We also find that T5-large, when pre-finetuned on EDGAR10-Q, achieve SOTA results on downstream finance tasks such as Headline, FPB, and FiQA SA, outperforming vanilla version by 10.81 points. To our surprise, this 66x smaller pre-finetuned model also surpasses the finance-specific LLM BloombergGPT-50B by 15 points. We hope that our dataset and generated artifacts will encourage further research in this direction, leading to the development of more sophisticated language models for financial text analysis.\\n\\nINTRODUCTION\\n\\nRecent advancements in Named Entity Recognition (NER) have led to impressive results through the development of various large-scale pretrained models Zhang et al. (2023); Ma et al. (2023); Zhang et al. (2022); Yuan et al. (2021); Wang et al. (2021b;a). While existing research has primarily focused on NER task performance Zhang & Zhang (2022); Wang et al. (2014); Francis et al. (2019); Alexander & de Vries (2021); Wu et al. (2022); Wang & Wang (2022); Varshney et al. (2022); Shrimal et al. (2022), limited attention has been given to exploring contextual information associated with the identified entities within sentences. To illustrate this challenge, consider the sentence \\\"The rent due today is $500.\\\" In this example, the question \\\"What is $500?\\\" can be answered as \\\"rent due today.\\\" This task becomes particularly challenging when sentences are lengthy, lack explicit contextual cues, or contain multiple entities. For instance, referring to the second sentence in Table 1, discerning the context of \\\"2.2 Million\\\" as \\\"Valuation allowance for loan servicing rights with impairments\\\" solely based on the sentence becomes difficult (Asking the question \\\"What is 2.2 million?\\\" using the sentence as the context). Notably, the exact phrase is not directly present in the sentence; however, it holds significant relevance in accurately describing the entity. Estimating the context of the entity is very difficult for a non-expert human user as well (\u00a7. 6.4.) We pose the following question \\\"How do you train models that could capture the intrinsic value of an entity in a sentence?\\\"\\n\\nWe propose C-CONTEXT-NER, a task which involves generating a concept description of the entity given a sentence, regardless of whether the concept is explicitly present in the sentence. We also note that the dataset, the script to generate it, baseline approach, ChatGPT evaluations, and finetuned models are freely available at https://anonymous.4open.science/r/edgar10q-dataset-144D/README.md.\"}"}
{"id": "4iQuByhNie", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As of June 30, 2019, the department store loans discussed above were 90 days or greater past due, as were $4.5 million of residential loans and a $36.2 million infrastructure loan with a carrying value of $29.2 million, net of a $7.0 million non accretable difference.\\n\\nThere were impairments of $0.8 million for the three months ended June 30, 2020 and $2.2 million for the six months ended June 30, 2020.\\n\\nTable 1: Example sentences from the dataset with sampled entities (Column 2) and their context (Column 4). As can be seen from Column 2, the entities do not convey the entire picture by themselves and generating their relevant context becomes an important task to address.\\n\\nintroduce the EDGAR10-Q dataset\\n\\nTable 2: Responses of the baseline approach and ChatGPT for sample sentences present in Table 1\\n\\nThe responses highlight the difficulty to generate relevant context associated with entities.\\n\\nWe conduct various experiments in algorithmic (rule based), one-shot and supervised learning settings. We introduce a baseline method that leverages syntactic trees of the sentence to generate questions and find relevant phrases in the sentences (\u00a74). The baseline yields an overall result of 27.59 ROUGE-L score. We also conduct zero shot, one shot, few shot evaluations on ChatGPT Brown et al. (2020) to get max 36.81% score. Responses of the baseline approach and ChatGPT are illustrated in Table 2. We also train T5's Raffel et al. (2020) different variants (T5, Tk-InstructWang et al. (2022b), and Flan T5Chung et al. (2022)) and BART model Lewis et al. (2020) in a supervised manner to get 49% as the highest ROUGE-L score (\u00a75.2). The low scores are identified as an area for further research to enhance the learning capabilities for such complex tasks.\\n\\nWe examine the effects of the generated artifacts using the dataset. Our findings reveal that T5 pre-finetuned on EDGAR10-Q outperforms vanilla T5 by 10.81 points and surpasses BloombergGPT 50B by 15.81 points on various downstream finance datasets (\u00a76.3). Additionally, we provide a comparison between CONTEXT-NER and OpenIE to highlight the distinctions between these tasks.\\n\\nWe explore the effect of instruction tuning on the T5 model using the EDGAR10-Q dataset, resulting in a performance improvement of 2% points. Lastly, our human evaluation case study, involving non-experts, yields a best score of 36%, further underscoring the need for continued research in this domain.\\n\\nContributions:\\n(a) we introduce the task CONTEXT-NER, to generate contextual phrases for entities in sentences and associated EDGAR10-Q dataset created from financial reports; (c) we evaluate the dataset using the following methods: (c.1) we introduce a baseline approach which achieves a 27%\"}"}
{"id": "4iQuByhNie", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ROUGE-L; (c.2) we evaluate the dataset in a one-shot setting via ChatGPT achieving 30% ROUGE-L; (c.3) we train different generative models in a supervised manner to get \\\\( \\\\sim 50\\\\% \\\\) performance; (d) we perform a detailed analysis on following lines of enquiry (d.1) effect of pre-finetuning using EDGAR10-Q to achieve SOTA on several finance downstream tasks (d.2) qualitative comparison of CONTEXT-NER with OpenIE (d.3) explore the effect of instruction tuning for EDGAR10-Q.\\n\\nConsider the example from Table 1: \u201cImpairments of \\\\$0.8 million for the three months ended June 30, 2020 and \\\\$2.2 million for the six months ended June 30, 2020.\u201d Estimating the meaning of 2.2 million as \u201cValuation allowance for loan servicing rights with impairments\u201d remains challenging for non-experts (Asking the question \u201cWhat is 2.2 million?\u201d, using the sentence as the context) We also see that from Table 2, ChatGPT and baselines struggle, emphasizing the necessity for research. However, subject matter experts who created gold labels could understand it. This led to the question: How to transfer domain expert knowledge to an LLM?\\n\\nIn order to capture intrinsic entity value in sentences, we introduce ContextNER task and its associated artifacts (large benchmark dataset and fine-tuned models released to the community). Our objective is to bridge the gap between experts and models by infusing domain knowledge into the models that goes beyond what is explicitly present in the sentence. In our domain-specific document corpora, generating pertinent contexts linked to various financial Named Entities holds substantial value for professionals making critical decisions based on such reports. Through the creation of this task, we\u2019ve recognized the following beneficial use cases:\\n\\n- **Credit Companies (Risk Assessment):** Credit companies could leverage the contextual understanding of entities to perform more accurate risk assessments of other companies. This would enable them to evaluate the financial health and stability of businesses more effectively, leading to better-informed credit decisions.\\n\\n- **Hedge Funds (Investment Decisions):** By analyzing the contextual relationships between companies and other market factors, hedge funds could refine their strategies, resulting in more favorable investment outcomes.\\n\\n- **Finance Journalism (Accurate Reporting):** With contextual understanding, finance journalists would be empowered to extract precise and up-to-date information from Edgar reports, enabling them to produce more accurate and insightful articles and reports.\\n\\n- **Regulatory Compliance (Efficient Reporting):** Understanding the intricacies of entities in the reports would enable compliance to fulfill reporting obligations more efficiently, ensuring adherence to regulatory requirements.\\n\\n### EDGAR10-Q Dataset\\n\\n| Entity Types          | Counts     |\\n|-----------------------|------------|\\n| Floating Values (monetary and percent) | 2.1M       |\\n| number of Assets (Shares and Integers) | 425.8K     |\\n| Ordinal Values        | 16.8K      |\\n| Dates                 | 195K       |\\n\\nTable 3: Distribution of different types of entities in the dataset.\\n\\n**Dataset Creation:** The EDGAR10-Q dataset was created by scraping quarterly (10-Q) and annual (10-K) reports from the years 2019, 2020, and 2021. Given the crucial role these meticulously prepared reports play in assessing the financial health of organizations, great care is taken in their curation to ensure accuracy and quality, leaving no room for oversight. To ensure standardization, all SEC filings undergo a tagging process where entities within sentences are labeled with corresponding Named Entity Recognition (NER) context labels, serving as high-quality gold labels for the dataset. We refer the reader to \u00a7A for more details.\\n\\n**Dataset Description:** Table 3 shows the four types of entities, namely money, time (duration), percent, and cardinal values (pure, shares, and integer) present in the dataset. We follow their convention for the entity recognition. Stanford NER tagger [http://nlp.stanford.edu:8080/ner/](http://nlp.stanford.edu:8080/ner/) recognizes the aforementioned types as named entities.\"}"}
{"id": "4iQuByhNie", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "4iQuByhNie", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "4iQuByhNie", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nThomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pierric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38\u201345, 2020.\\n\\nLinzhi Wu, Pengjun Xie, Jie Zhou, Meishan Zhang, Chunping Ma, Guangwei Xu, and Min Zhang. Robust self-augmentation for named entity recognition with meta reweighting. CoRR, 2022.\\n\\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023.\\n\\nZheng Yuan, Yijia Liu, Chuanqi Tan, Songfang Huang, and Fei Huang. Improving biomedical pretrained language models with knowledge. In Proceedings of the 20th Workshop on Biomedical Language Processing, pp. 180\u2013190, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.bionlp-1.20. URL https://aclanthology.org/2021.bionlp-1.20.\\n\\nMozhi Zhang, Hang Yan, Yaqian Zhou, and Xipeng Qiu. Promptner: A prompting method for few-shot named entity recognition via k nearest neighbor search. ArXiv, abs/2305.12217, 2023.\\n\\nSheng Zhang, Hao Cheng, Jianfeng Gao, and Hoifung Poon. Optimizing bi-encoder for named entity recognition via contrastive learning. ArXiv, abs/2208.14565, 2022.\\n\\nYuzhe Zhang and Hong Zhang. Finbert-mrc: financial named entity recognition using bert under the machine reading comprehension paradigm. arXiv preprint arXiv:2205.15485, 2022.\\n\\nZexuan Zhong and Danqi Chen. A frustratingly easy approach for entity and relation extraction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 50\u201361, 2021.\\n\\nQile Zhu, Xiaolin Li, Ana Conesa, and C\u00e9cile Pereira. Gram-cnn: a deep learning approach with local context for named entity recognition in biomedical text. Bioinformatics, 34(9):1547\u20131554, 2018.\"}"}
{"id": "4iQuByhNie", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The process to extract data is described below:\\n\\nThe code starts from the function called driver_writer_func, which takes five arguments: company_name, cik, start_date, base_folder, and dest_folder. The function performs the following steps:\\n\\n1. Deletes the base_folder directory and creates a new one.\\n2. Creates a dest_folder directory if it does not already exist.\\n3. Calls a function get_all_submissions with arguments cik, start_date, base_folder, and company_name to retrieve financial documents for the company.\\n4. Parses the documents using IE_Parser and structures the resulting data in a tabular format.\\n5. Filters the data based on certain criteria such as text length and data type.\\n6. Parses the text data to extract entities, phrases, and questions and answers using various functions such as sent_parse, sentence_entity_flair, phrase_extraction, and qa_model (They are part of baseline extraction method and are explained in \u00a7).\\n7. Writes the resulting data to a CSV file in the dest_folder directory with the name company_name.csv.\\n\\nget_all_submissions:\\ntakes four arguments - cik (an integer), start_date, base_folder (a string), and company_name (also a string). It first checks if the company_name exists in a file called done_comps, and if so, prints a message saying all files of the company have already been downloaded and returns None. Next, it reads the contents of a file called DONE_LIST if it exists, and assigns it to the variable done_subs. Then, it converts cik to a string data type, calls the function get_accession_numbers with cik, '10-K', and start_date as arguments, and assigns the result to the variable subs_10k. Similarly, it calls get_accession_numbers with cik, '10-Q', and start_date as arguments and assigns the result to the variable subs_10q. It then concatenates these two lists (subs_10k and subs_10q) into a new list called subms. The function then logs the number of submissions made after start_date by cik. For each submission in the subms list, the function extracts the name and url of the JSON file associated with it. It then loads the JSON data into a dictionary called subm_json using the json.loads() method. From this dictionary, it extracts the list of files associated with the submission and filters out those with a .txt file extension. It then selects the first .txt file and extracts its file name and url. Next, it calls the function get_meta_data with the contents of the text file as an argument to extract metadata from the file. If successful, it assigns the submission type based on the extracted metadata. If the submission type folder doesn't exist in the base_folder, it creates the necessary directory structure. It then writes the contents of the text file to a file in the appropriate directory in the base_folder, and appends the name of the submission to a file called DONE_LIST. Finally, it appends the company_name to a file called DONE_COMP.\\n\\nget_accession_numbers:\\naccepts three parameters: cik (a string), type (a string) and start_date (a datetime object). It returns a list of accession numbers for a company with the specified Central Index Key (CIK) that have been filed with the Securities and Exchange Commission (SEC) after a specified date and of a specified type. The function starts by constructing a URL based on the parameters passed in. The URL is used to fetch an HTML page containing a table of filing information. The function then processes the HTML page using BeautifulSoup to extract the relevant table, convert it to a pandas DataFrame, and filter the rows to those with filing dates greater than the specified start_date. It then extracts the accession numbers from the filtered table, cleaning them up and returning them as a list.\\n\\nget_meta_data:\\naccepts a string subm_details_text that contains the content of a submission details text file in the EDGAR database. It returns a dictionary containing the metadata for the submission. The function starts by initializing an empty dictionary called meta_data and two lists called running_titles and running_indents. It then loops over the rows of the subm_details_text string,\"}"}
{"id": "4iQuByhNie", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"splitting each row into segments using the colon (:) as a separator. If a row contains a single segment, the function assumes that it is a heading and adds it to running_titles along with its indentation level, which is calculated by counting the number of tabs in the row. If a row contains two segments, the function assumes that it is a key-value pair and adds it to meta_data using the deep_set function to create nested dictionaries for the various levels of headings. The deep_set function sets the value of a nested dictionary by walking the dictionary hierarchy according to the list of headings passed in and creating new dictionaries as needed. Finally, the function returns the meta_data dictionary.\\n\\nFigure 3 shows one instance of the raw dataset. The complete dataset is present in the GitHub repository. Each column is described below:\\n\\n1. paragraph: It contains the input string and the sentences surrounding it.\\n2. value: The numerical value of the entity whose context is going to be extracted from the sentence.\\n3. label: A list of phrases, which describe the entity. In this case, the phrases are: 'Concentration risk, percentage'.\\n4. name: A string representing the name of the value, in this case, is 'us-gaap_ConcentrationRiskPercentage1'.\\n5. type: Description of the data type of the value, in this case, is 'percentItemType'.\\n6. sent: The sentence that contains the entity.\\n7. entity: Entity extracted using NER library '52%'.\\n8. entity_type_ext: The data type of the entity extracted using the NER library, which is 'PERCENT'.\\n9. sentence: Cleaned version of sent.\\n10. phrases: Phrases extracted from the phrase generation algorithm, including 'more % of total vendor trade receivables', 'more %', and 'total vendor trade receivables'.\\n11. qa_temp: List of questions that are formed using the phrases.\\n12. key: The phrase whose question gave the correct answer.\\n13. score: The confidence score given by the answer, that is 0.8616288900375366 in this case.\"}"}
{"id": "4iQuByhNie", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 2: Phrase Generation\\n\\nPseudocode\\n\\nInput: Sentence\\nOutput: List of Phrases\\n\\n1. Function `simple_noun_phrase_extractor(Sentence)`:\\n   2. `doc = sequence_of_token(Sentence), phrase_list = []`\\n   3. for `token in Doc`:\\n   4.     `phrase = ' '`\\n   5.     if `token.head.pos in [Noun, Pronoun] and token.dep in [Object, Subject]`:\\n   6.         for `subtoken in token.children`:\\n   7.             if `subtoken.pos is Adj or subtoken.dep is Comp`:\\n   8.                 `phrase += subtoken.text + ' '`\\n   9.             if `len(phrase) is not 0`:\\n   10.                `phrase += token.text`\\n   11.               if `len(phrase) is not 0 and phrase does not have entities`:\\n   12.                  `phrase_list.append(phrase)`\\n   13.     return `phrase_list`\\n\\n14. Function `complex_noun_phrase_extractor(Sentence)`:\\n   15.     `doc = sequence_of_token(Sentence)`\\n   16.     `phrase_list = []`\\n   17.     for `token in Doc`:\\n   18.         if `token.pos is Preposition`:\\n   19.             `phrase = ' '`\\n   20.             if `token.head.pos in [Noun, Pronoun]`:\\n   21.                 for `subtoken in token.head.children`:\\n   22.                     if `subtoken.pos is Adj or subtoken.dep is Comp`:\\n   23.                         `phrase += subtoken.text + ' '`\\n   24.                     `phrase += token.head.text + ' ' + token.text`\\n   25.                 for `right_tok in token.rights`:\\n   26.                     if `right_tok in [Noun, Pronoun]`:\\n   27.                         for `subtoken in right_tok.children`:\\n   28.                             if `subtoken.pos is Adj or subtoken.dep is Comp`:\\n   29.                                 `phrase += subtoken.text + ' '`\\n   30.                                 `phrase += ' ' + right_tok.text`\\n   31.                 if `len(phrase) > 1 and phrase does not have entities`:\\n   32.                     `phrase_list.append(phrase)`\\n   33.     return `phrase_list`\\n\\n14. question: The question that gave the correct answer, that is, 'What is total vendor trade receivables?' in this case.\\n\\n15. answer: String that represents the MRC output of the BERT model used in the baseline approach.\\n\\nB.2 SUPERVISED MODELING DATA\\n\\nSupervised modeling data consisted of the entity concatenated with the sentence. The output is one of phrases from the labels. In this case, the input is: 52%. As of September 25, 2021, the Company had three vendors that individually represented 108 or more of total vendor non-trade receivables, which accounted for 52%, 11%, and 11%. The output for this sentence is: Concentration risk.\\n\\nSupervised Training setup:\\nWe finetune generative models (T5 Base, T5 Large (Raffel et al., 2020), BART Base (Lewis et al., 2020), Flan-T5 Large (Chung et al., 2022), Tk-Instruct Large (Wang et al., 2022b)) on the train split of the dataset.\\n\\nHyper parameters: Train Batch Size: 8, Gradient Accumulation Steps: 8, Max Source Length: 512, Max Target Length: 128, Number of Epochs: 2, Warmup Steps: 100, Learning Rate: $5 \\\\times 10^{-5}$.\"}"}
{"id": "4iQuByhNie", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 In October 2019, the Company increased the borrowing capacity on the revolving credit loan by $33,000 increasing the available credit facility from $60,000 to $93,000.\\n\\n\u2022 If the loan is paid during months 13-24 or 25-36 and then a penalty of 2% and 1%, respectively, of the loan balance will be charged on the date of repayment.\\n\\n\u2022 The weighted-average remaining lease term and discount rate related to the Company\u2019s lease liabilities as of September 26, 2020 were 10.3 years and 2.0%, respectively.\\n\\nThis paper presents a simple, yet efficient technique to extract entities and their descriptions from sentences. As shown in Figure 1, it starts with data cleaning and entity extractions. A noun phrase (NP) (Stuart et al., 2013) includes a noun, a person, place, or thing, and the modifier that distinguishes it. Open IE is predicated on the idea that the relation (which is action verbs in most cases) is the central element of the study, from which all other considerations flow. However, in many cases, the verb is not helpful, particularly in financial data. Consider the sentence: \u201cDeferred revenue for 2020 is $20 billion.\u201d Like most financial records are of the form \u201cis, was, be,\u201d etc., the verb \u201cis\u201d in this sentence is an auxiliary verb and does not describe any particular event or give any information about the entity.\\n\\nWe extract two types of phrases from the sentences, namely simple and complex. In simple phrase extraction, each sentence comprises subject-object and verb connecting them where Subject or Object is usually a noun or pronoun. After searching for a noun and pronoun, we check for any noun compound or adjective. On the other hand, for complex phrase extraction we first start with preposition extraction. We then follow similar steps as in simple phrase extraction to look for phrases in both left and right of the preposition. It has to be noted that simple phrases are not always found on both sides of the proposition. Algorithm 2 further summarizes the process of simple and complex phrase extraction from the sentences.\\n\\nNow we demonstrate the extraction of simple and complex noun phrases for the sentence, \u2018In connection with the refinance we reduced the loan amount by $6.8 million.\u2019 The syntactic tree for the above sentence is shown in Figure 4. We search if the token\u2019s POS tag is a noun or pronoun as we are looking just for noun phrases. We also ensure that phrase lies either in the Subject or Object of\"}"}
{"id": "4iQuByhNie", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the sentence to ensure we are skipping the relations. In this case, we got the first word of the phrase. After that, we iterate the node to see its children named subtoken in Algorithm 2. We search for subtoken\u2019s dependency relation with the token as a compound relation, or we search if the subtoken is an adjective. The intuition behind this is that if the subtoken and token have a compound relationship, they form a meaningful noun phrase. In this case, \u201camount\u201d has a compound relationship with its subtoken \u201cloan\u201d so they together form \u201cloan amount\u201d as the meaningful noun phrase. Similar logic is followed for searching adjectives. Complex NPs are identified as series of noun phrases with a preposition separating them, so we start by identifying them. In this example, the preposition identified was \u201cin\u201d. Then we iterate both up and down the node to find noun phrases that follow the same method mentioned above. The noun phrases identified from the top were \u201cconnection\u201d and the bottom was \u201crefinance\u201d. The entire complex NP was formed as NP from top + preposition in the middle and + NP from below. The resultant was \u201cconnection with refinance\u201d.\\n\\nThis paper presents a zero-shot technique as we leverage the phrase generation to generate meaningful questions without further training of the machine reading comprehension (MRC) model. This allows our technique to be domain agnostic and thus can be easily expanded to newer domains. The process to leverage noun phrases to generate the questions and further using the MRC model to associate entities with their corresponding descriptions is described below:\\n\\n- Each paragraph in the document is broken down into sentences. For each sentence, the following are extracted: Phrases (using simple and complex noun phrases described in Algorithm 2) and Entities using the Flair NER Model.\\n- On the basis of the entity type and the noun phrases, the questions are framed accordingly. For instance, if the entity found out was of type date, then the question would be \u201cwhen is\u201d + NP?. In our example, the question for the first sentence for \u00a7C would be \u201chow much is borrowing capacity on revolving credit loan?\u201d. In instances where the entity type is of integer, float, or percent where appending \u201cwhen is\u201d or \u201chow much is\u201d does not give an advantage. For such cases, to keep the question generic we append \u201cwhat is\u201d to the noun phrase. For example, the question for second sentence for \u00a7C is, \u201cWhat is the loan balance?\u201d was created based on the entity type of 2% and 1%.\\n- Once these questions are generated, they are fed into the MRC Model, and its answer is checked if it contains the entity. To give an example, in the 1st sentence, the following questions are created, and the model returns their corresponding answers and their confidence values:\\n  - \u201cHow much is borrowing capacity on revolving credit loan?\u201d answer: \u201c$33,000\u201d, confidence score: 0.946\\n  - \u201cHow much is borrowing capacity?\u201d answer: \u201c$33,000\u201d, confidence score: 0.824\\n  - \u201cHow much is revolving credit loan?\u201d answer: \u201c$33,000\u201d, confidence score: 0.856\\n  - \u201cHow much is available credit facility?\u201d answer: \u201c$60,000 to $93,000\u201d, confidence score: 0.5762\\nIf there are multiple questions whose answer has the entity, we select the question whose answer is of the highest confidence value. In the above example, \u201cborrowing capacity on revolving credit loan\u201d is chosen as the key for $30,000, and \u201crevolving credit loan\u201d is chosen as the key for both $60,000 to $93,000.\\n- If the entity is not present in the response of the MRC model, the question is discarded. In the 2nd Sentence of Table 1, the following questions are created:\\n  - \u201cWhat is penalty of %?\u201d\\n  - \u201cWhat is loan balance?\u201d None of them are returning \u201c13-24 or 25-36\u201d, so the phrases \u201cpenalty of %\u201d and \u201cloan balance\u201d are discarded.\\n- There are instances where none of the generated questions returned an answer with the target entity or returned responses with a different entity as shown above. For those cases, we create the question \u201cwhat is\u201d entity?. Here, its response would be considered as the key.\"}"}
{"id": "4iQuByhNie", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dataset\\n\\n| Dataset     | EDGAR-T5 | Vanilla T5 |\\n|-------------|----------|------------|\\n| BloombergGPT 50B | 75.07    | 74.89      |\\n| FiQA        | 80.42    | 51.07      |\\n| Headline    | 90.55    | 82.20      |\\n\\nTable 11: Comparison of EDGAR-T5 and Vanilla T5 on different finance-related tasks. Both models are 770M in size. BloombergGPT 50B is used as the baseline. Scores are weighted F1 as shown in BloombergGPT 50B.\\n\\nDatasets\\n\\n| Dataset     | EDGAR-T5 | Vanilla T5 |\\n|-------------|----------|------------|\\n| Boolq       | 32.94    | 37.41      |\\n| CB          | 89.85    | 94.20      |\\n| COPA        | 63.20    | 86.40      |\\n| RTE         | 86.64    | 94.70      |\\n| WIC         | 58.71    | 59.80      |\\n\\nTable 12: Comparison of EDGAR-T5 and Vanilla T5 on downstream NLP tasks. Both models are 770M in size. Scores denote F1 score.\\n\\nFollowing works from instruction tuning Wang et al. (2022b); Gupta et al. (2023), we add instructions on the train data and instruction tune Tk-Instruct. Figure 2 showcases the performance increase across the entire test set. Across each sentence category, there is an increase of roughly 2%, highlighting that instruction-tuned models with instruction data work well. The improvement is significant in sentences with 5+ categories where there is an increase of absolute 4%.\\n\\nTo study the impact of EDGAR10-Q in the real world, we compare the effect of a model pre-finetuned on EDGAR10-Q vs a vanilla T5 model. We call T5 pre-finetuned on the dataset as EDGAR-T5. Both EDGAR-T5 and vanilla T5 are then finetuned on three finance datasets; FiQASinha & Khandait (2021), FPBMalo et al. (2014), and Headline Maia et al. (2018) datasets.\\n\\nAll the hyperparameters for vanilla T5 and Edgar-T5 were the same for a fair comparison. We use BloombergGPT-50B Wu et al. (2023) 10 shot score as the baseline for these tasks. The splits used in downstream datasets and weighted F1 score were kept exactly the same as BloombergGPT for a fair comparison.\\n\\nAs shown in Table 11, EDGAR-T5 outperforms both vanilla T5 and BloombergGPT on all three downstream tasks and establishes SOTA results on all three of the tasks.\\n\\nThe experiments suggest that the EDGAR10-Q dataset has led to an increase in the model's inherent ability for financial tasks. We also conducted a similar study to compare the performance of the models on general domain downstream NLP tasks. Table 12 shows the F1 score of the models across BoolqClark et al. (2019) , CBDe Marneffe et al. (2019), COPAGordon et al. (2012), RTEPilehvar & Camacho-Collados (2019) and WICPilehvar & Camacho-Collados (2019) datasets. Same hyperparameters were used for training of both the models. Performance gains were observed across all five datasets demonstrating the artifact's usability in general domain as well. We release all the finetuned models to the community for future use.\\n\\nHyperparameters are available in appendix.\"}"}
{"id": "4iQuByhNie", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We also present a study of human evaluation on a small sample of the dataset (randomly sampled 100 examples). To account for human bias, we ask two graduate students evaluate the samples. We get a Rouge-L F1 score of 34.69% (average) - indicating the difficulty of the task for non-experts. The exact match score is 8.1% while the no-match score is 36% - both further validating the task's difficulty.\\n\\nSeveral approaches have been developed for State-of-the-art NER detection Chawla et al. (2021); Li et al. (2019a); Luoma & Pyysalo (2020); Du et al. (2010); Zhu et al. (2018). Multiple approaches have been developed around various aspects of NER Moon et al. (2018); Amalvy et al. (2023); Li et al. (2020); Kocaman & Talby (2021); Zhong & Chen (2021); Zhang et al. (2022); Shon et al. (2022); Wang et al. (2022a). Etzioni et al. introduced a schemaless approach for extracting facts from text, focusing on relation extraction using OpenIE. However, this approach assumes relations between two entities, which poses challenges for financial data. Levy et al. used a zero-shot approach to train MRC model on templatized questions and inferred it on unseen relations. Li et al. formalizes relation extraction as multi-turn question answering. Miwa et al. jointly extracted entities and relations using neural networks, but performance suffers on unseen relations. Sun et al. build on the previously mentioned framework and uses a joint learning framework and a flexible global loss function to capture the interactions of the entities and their relationships. McCann et al. introduced decaNLP, addressing multiple challenges including relation extraction and question answering. Various frameworks like Stanford CoreNLP's NER, Spacy, NLTK, and Flair are available for entity extraction. We aim to extract entities and their contexts, a more complex scenario than relation-based approaches.\\n\\nIn this paper, we introduced the Context-NER task, which aims to bridge the gap between existing NER tasks by extracting relevant phrases for entities. We also presented the EDGAR10-Q dataset, which is a large and complex finance dataset, providing a valuable resource for research in this domain. Through our baseline approach, we demonstrated the feasibility of solving the Context-NER task and conducted extensive experiments to evaluate our method's performance. Our comparison with GPT-3 showcased the challenges posed by the dataset. Additionally, we explored a supervised setting by finetuning pre-trained language models, which showed promising results. We believe that the introduction of the EDGAR10-Q dataset and our study will encourage further investigation and advancements in this field.\\n\\nTo advance our work, there are several promising directions for future research. Due to limited computational resources, we were unable to finetune large models (greater than 1B) on the EDGAR10-Q dataset. Elaborate experimentation could be conducted using other instruction-based or chain-of-thought reasoning on the EDGAR10-Q dataset. Future work should consider leveraging more powerful models to potentially achieve higher scores on this dataset. Furthermore, our evaluation set for ChatGPT was smaller than the actual test set due to budget constraints. Lastly, expanding the dataset to include reports from different markets, non-English languages, and including more recent years would enable researchers to explore the generalizability and temporal dynamics of the task.\\n\\nWe have verified that all licenses of source documents used in this document allow their use, modification, and redistribution in a research context. There were no real-life names in the data set. No particular sociopolitical bias is emphasized or reduced specifically by our methods.\"}"}
{"id": "4iQuByhNie", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"REFERENCES\\n\\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf. Flair: An easy-to-use framework for state-of-the-art nlp. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 54\u201359, 2019.\\n\\nDaria Alexander and Arjen P de Vries. \\\"this research is funded by...\\\": Named entity recognition of financial information in research papers. 2021.\\n\\nArthur Amalvy, Vincent Labatut, and Richard Dufour. The role of global and local context in named entity recognition. In 61st Annual Meeting of the Association for Computational Linguistics, 2023.\\n\\nGabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D Manning. Leveraging linguistic structure for open domain information extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 344\u2013354, 2015.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\nAvi Chawla, Nidhi Mulay, Vikas Bishnoi, and Gaurav Dhama. Improving the performance of transformer context encoders for ner. In 2021 IEEE 24th International Conference on Information Fusion (FUSION), pp. 1\u20138. IEEE, 2021.\\n\\nHyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed Huisin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416, 2022.\\n\\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924\u20132936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300.\\n\\nMarie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pp. 107\u2013124, 2019.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\nJunwu Du, Zhimin Zhang, Jun Yan, Yan Cui, and Zheng Chen. Using search session context for named entity recognition in query. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pp. 765\u2013766, 2010.\\n\\nOren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. Open information extraction from the web. Communications of the ACM, 51(12):68\u201374, 2008.\\n\\nSumam Francis, Jordy Van Landeghem, and Marie-Francine Moens. Transfer learning for named entity recognition in financial and biomedical documents. Information, 10(8):248, 2019.\\n\\nYao Fu, Chuanqi Tan, Mosha Chen, Songfang Huang, and Fei Huang. Nested named entity recognition with partially-observed treecrfs. In AAAI Conference on Artificial Intelligence, 2020.\\n\\nAbbas Ghaddar and Philippe Langlais. Wikicoref: An english coreference-annotated corpus of wikipedia articles. In International Conference on Language Resources and Evaluation, 2016.\"}"}
{"id": "4iQuByhNie", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nAndrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012)*, pp. 394\u2013398, Montr\u00e9al, Canada, 7-8 June 2012. Association for Computational Linguistics. URL https://aclanthology.org/S12-1052.\\n\\nJean-Philippe Thiran Guillaume Jaume, Hazim Kemal Ekenel. Funsd: A dataset for form understanding in noisy scanned documents. In Accepted to ICDAR-OST, 2019.\\n\\nHimanshu Gupta, Saurabh Arjun Sawant, Swaroop Mishra, Mutsumi Nakamura, Arindam Mitra, Santosh Mashetty, and Chitta Baral. Instruction tuned models are quick learners, 2023.\\n\\nRaphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pp. 541\u2013550, 2011.\\n\\nMatthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. spacy: Industrial-strength natural language processing in python, 2020. URL https://doi.org/10.5281/zenodo.1212303.\\n\\nVeysel Kocaman and David Talby. Biomedical named entity recognition at scale. In Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event, January 10\u201315, 2021, Proceedings, Part I, pp. 635\u2013646. Springer, 2021.\\n\\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via reading comprehension. *arXiv preprint arXiv:1706.04115*, 2017.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871\u20137880, 2020.\\n\\nPeng-Hsuan Li, Tsu-Jui Fu, and Wei-Yun Ma. Why attention? analyzing and remedying bilstm deficiency in modeling cross-context for ner. *arXiv e-prints*, pp. arXiv\u20131910, 2019a.\\n\\nXiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, and Jiwei Li. Entity-relation extraction as multi-turn question answering. *arXiv preprint arXiv:1905.05529*, 2019b.\\n\\nXiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, and Jiwei Li. Dice loss for data-imbalanced nlp tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 465\u2013476, 2020.\\n\\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74\u201381, 2004.\\n\\nAngli Liu, Stephen Soderland, Jonathan Bragg, Christopher H Lin, Xiao Ling, and Daniel S Weld. Effective crowd annotation for relation extraction. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 897\u2013906, 2016.\\n\\nEdward Loper and Steven Bird. Nltk: The natural language toolkit. *arXiv preprint cs/0205028*, 2002.\\n\\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), 2018.\\n\\nJouni Luoma and Sampo Pyysalo. Exploring cross-sentence contexts for named entity recognition with bert. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 904\u2013914, 2020.\"}"}
{"id": "4iQuByhNie", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Systematic comparison of EDGAR 10-Q with other benchmark NER datasets.\\n\\n| Dataset Name       | # of Entities | # of Sent. | Avg. S. Len | # of Sent. | Avg. S. Len |\\n|--------------------|---------------|------------|-------------|------------|-------------|\\n| funsd Guillaume Jaume (2019) | 200            | NA         | 31485       | 9743       |\\n| wikicoref Ghaddar & Langlais (2016) | 30             | 2229       | 59652       | 3557       |\\n| scierc Luan et al. (2018) | 500            | NA         | NA          | 8089       |\\n| med ment. Patil (2020) | 4392           | 42602      | 1176058     | 352496     |\\n| genia Fu et al. (2020) | 2000           | 18545      | 436967      | 96582      |\\n| CONLL 2003 Sang & Meulder (2003) | 1393           | 22137      | 301418      | 35089      |\\n| EDGAR 10-Q         | 18752          | 1009712    | 77400425    | 2780969    |\\n\\nTable 6: Train and test split statistics of the dataset for supervised learning experiments.\\n\\nThe average length of the label is 4.55 tokens highlighting that sufficiently long context phrases are used to describe the entity. The train and test set are of roughly equal difficulty in terms of sentences.\\n\\nTable 4 further elucidate data richness through paragraph and sentence level statistics. The average length of the label is 4.55 tokens highlighting that sufficiently long context phrases are used to describe the entity. The train and test set are of roughly equal difficulty in terms of sentences.\\n\\n| Statistic                  | Values |\\n|---------------------------|--------|\\n| Entities / sentence      | 1.78   |\\n| Words / paragraph        | 113.14 |\\n| Avg tokens / label       | 4.55   |\\n| Words / sentence         | 35.88  |\\n\\nTable 4: Relevant sentence and paragraph wise statistics of the dataset that highlight the task's difficulty.\\n\\nTable 5 compares this dataset with benchmark NER datasets and contains nearly 18.7K documents comprising 1M sentences. As it can be inferred from the table, the EDGAR-10Q dataset has nearly 650x more documents and nearly 780x more entities in comparison to the popular NER benchmark dataset - wikicoref Ghaddar & Langlais (2016). We observe that the EDGAR10-Q is the largest and richest in multiple parameters and a first-of-its-kind dataset in the financial domain. Table 6 highlights the train test split of the dataset.\\n\\nThe dataset is also divided according to the different number of entities present in a sentence. We see that as the number of entities increases, the average sentence length increase as well. Since this is a real-world dataset, sentences with 1 entity are most prevalent and comprise 49% of the dataset while sentences with 5+ entities consist of 2% of the dataset (more details present in Table 20 in \u00a7F).\\n\\nWe present a simple, yet efficient method to extract entities' descriptions from sentences, as shown in Figure 1. OpenIE is predicated on the idea that the relation (which is action verbs in most cases) is the central element of the extraction process, from which all other considerations flow. However, in many cases, the verb is not helpful, particularly in financial data. Consider the sentence: \\\"Deferred revenue for 2020 is $20 billion.\\\" Like most financial records are of the form \\\"is, was, be,\\\" etc., the verb \\\"is\\\" in this sentence is an auxiliary verb and does not describe any particular event or give any information about the entity. Our approach consists of a phrase generation algorithm that is in turn used for the creation of questions and fed into a transformer model for machine reading comprehension.\\n\\nAlgorithm 1: Phrase Generation\\n\\nInput: Sentence\\nOutput: List of Phrases\\n\\n1. Function noun_phrase(Sentence):\\n   - doc = sequence_of_token(Sentence)\\n   - phrase_list = []\\n   - for token in Doc:\\n     - ph = ' '  \\n     - if token.head.pos in [Noun, Pronoun] and token.dep in [Object, Subject]:\\n       - for subtoken in token.children:\\n         - if subtoken.pos is Adj or subtoken.dep is Comp:\\n           - ph += subtoken.text + ' '  \\n     - if len(ph) is not 0:\\n       - ph += token.text\\n       - if len(ph) is not 0 and ph does not have entities:\\n         - phrase_list.append(ph)\\n   - return phrase_list\"}"}
{"id": "4iQuByhNie", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1 PHRASE EXTRACTION\\n\\nA noun phrase (NP) is defined as a phrase containing a noun, a person, place, or thing, and the modifier that distinguishes it. We extract two types of phrases from the sentences, namely simple and complex.\\n\\nIn simple phrase extraction, each sentence comprises subject-object and verb connecting them where the subject or object is usually a noun or pronoun. After searching for a noun and pronoun, we check for any noun compound or adjective. On the other hand, for complex phrase extraction, we first start with preposition extraction. It has to be noted that simple phrases are not always found on both sides of the proposition. We then follow similar steps as in simple phrase extraction to look for phrases in both the left and right of the preposition. Consider an example,\\n\\nIn connection with the refinance we reduced the loan amount by $6.8 million. From our algorithm, the phrases extracted from this sentence are loan amount and connection with refinance.\\n\\nWe use Spacy library for POS tags of the word which were leveraged in Algorithm 1. We refer the reader to \u00a7C where the approach is described in more detail along with examples and flowchart (Algo. 2 and Fig. 4).\\n\\n4.2 MACHINE READING COMPREHENSION FOR BASELINE\\n\\nPhrases and entities are extracted on a sentence level for each paragraph. Based on the type of entity and the noun phrases, the questions are framed accordingly in a rule-based fashion. For instance, if the entity found out was of type date, then the question would be \u201cwhen is\u201d + NP?. Once these questions are generated, they are fed into the MRC Model, and the answers are checked for the relevant entity. If there are multiple questions with the same answer, we select the one with the highest confidence score.\\n\\nEvaluation Methods\\n\\n| Method               | Precision | Recall  | F1     |\\n|----------------------|-----------|---------|--------|\\n| Baseline             | 34.59     | 27.06   | 27.59  |\\n| ChatGPT zero shot    | 27.53     | 42.31   | 27.53  |\\n| ChatGPT one shot     | 33.71     | 41.10   | 34.10  |\\n| ChatGPT few shot     | 38.62     | 41.68   | 36.81  |\\n\\nTable 7: Scores of different baseline approaches on test set of EDGAR 10-Q dataset. Baseline and the ChatGPT baseline give nearly the same performance.\\n\\nThere are instances where none of the generated questions returned an answer with the target entity or returned responses with a different entity. For those cases, we create the question \u201cwhat is\u201d entity? and its response would be considered as the relevant phrase. In case these questions return different entities as responses, all cases to identify the noun phrase fail and the algorithm does not return a response. A detailed description of the MRC model is present in \u00a7C.2. Since this method does not require any finetuning, the baseline is directly evaluated on the test set.\\n\\nSpacy POS Tagging Library link: https://spacy.io/usage/linguistic-features\"}"}
{"id": "4iQuByhNie", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We establish three ChatGPT baselines that evaluate the test set in a zero-shot, one-shot and few-shot setting. This method is evaluated on 10% of the test set due to budget constraints. The prompts used for the experiment are present in \u00a7D.\\n\\n### Experiments and Results\\n\\n#### Experimental Setup\\n\\n**Baseline Model Setup:** We run all our experiments using the BERT base model Devlin et al. (2018). All experiments are done with Nvidia V100 16GB GPU.\\n\\n**ChatGPT Setup:** We evaluate ChatGPT (gpt-3.5-turbo, max tokens = 256, top p = 1, frequency penalty = 0, presence penalty = 0) in zero, one and few shot setting.\\n\\n**Performance Evaluation metrics:** ROUGE-L score uses the longest common subsequence matching between the baseline and GPT-3 responses to compare output quality. We report precision, recall, and the F1 measure against the ROUGE-L Lin (2004) score. We also report the Exact Match Rajpurkar et al. (2016) which measures the ratio of the instances for which a model's response has a ROUGE-l score of 1 with a gold label. We report No match where the generated output and gold label 0 ROUGE-L score.\\n\\n#### Results\\n\\n| Evaluation Methods | Exact Match Score | No Match Score |\\n|-------------------|-------------------|---------------|\\n| Baseline          | 5.77              | 47.96         |\\n| ChatGPT (zero-shot) | 4.97              | 26.95         |\\n| ChatGPT (one-shot)  | 3.67              | 26.37         |\\n| ChatGPT (few-shot)   | 8.18              | 28.61         |\\n| Bart Base          | 20.88             | 23.74         |\\n| T5 Base            | 19.31             | 24.09         |\\n| T5 Large           | 20.92             | 23.24         |\\n| Flan T5 Base       | 19.64             | 24.24         |\\n| Tk Inst. Base      | 19.44             | 24.23         |\\n| Tk Inst. w. Inst.  | 21.45             | 22.83         |\\n\\nTable 9: Exact Match and No Match scores for LLMs fine-tuned on EDGAR 10-Q dataset. On comparison with Baseline and ChatGPT results, we see that finetuning improves upon result quality.\\n\\n**Baseline Scores:** Table 7 shows the baseline results where the overall F1 27.59%. Table 14 in \u00a7E gives the detailed results for the baseline.\\n\\n**ChatGPT Scores:** Table 7 also shows ChatGPT scores on the test set in zero-shot, one-shot and few-shot settings. As more examples are given to ChatGPT, the overall F1 increases. Precision of ChatGPT zero shot is lower than baseline, but recall is much higher, resulting in a higher overall F1. Detailed one-shot experiments on ChatGPT one-shot are present in Table 15.\\n\\n**Supervised Training Results:** Table 8 shows the results when the generative models are fine-tuned with the train split and evaluated on test split. The overall supervised training performance is much higher as compared to baselines, where base models (T5, Flan, and BART Base) perform much better than ChatGPT (47.67%, 6...)\"}"}
{"id": "4iQuByhNie", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Instances of Exact Match\\n\\nS1\\nPremium receivables are reported net of an allowance for doubtful accounts of $250 and $237 at September 30, 2020 and December 31, 2019, respectively.\\n\\nS2\\nThe fair value of the collateral received at the time of the transactions amounted to $1,019 and $351 at September 30, 2020 and December 31, 2019, respectively.\\n\\nInstances of No Match\\n\\nS3\\nDuring the nine months ended September 30, 2020, we granted approximately 0.3 restricted stock units that are contingent upon us achieving earnings targets over the three year period from 2020 to 2022.\\n\\nS4\\nCertain selling equity holders elected to receive deferred, variable earn out consideration with an estimated value of $21,500 over the rollover period of three years.\\n\\nTable 10: Instances of exact match and no match by the baseline approach. Column Baseline denotes the responses generated by the baseline approach.\\n\\n47.77%, and 49.01% respectively vs. 27.59%). We see that there is no significant improvement in performance as the number of parameters increases. T5 and Tk-Instruct Large (49.23% and 47.70%) give nearly the same F1 scores as BART (49.01%).\\n\\nExact and No Match:\\nTable 9 gives the summarized results for exact and no match scores of all the approaches on EDGAR10-Q dataset. The results of ChatGPT and Baseline are consistently low, as shown in Table 16. We infer this is because of complex hidden contexts and the sentence structures of the dataset. ChatGPT's score is consistently lower than the baseline as the recall of ChatGPT is consistently higher due to which obtaining an exact match is difficult. Table 17 shows the results of supervised learning where consistently higher scores are obtained. Instruction-tuned variants of T5 (Flan and Tk-Instruct) perform the best out of the models but the overall score is still low.\\n\\nNo Match:\\nThe baseline results are consistently worse than ChatGPT, as shown in Table 18. The no-match score for the baseline is more than twice as compared to ChatGPT (47.96% vs. 19%). As shown in Table 19, the no-match score for supervised learning models is also around 20%. This could again be attributed to the recall scores, as all the supervised models and ChatGPT had recall scores of around 48.\\n\\nTable 23 gives a few examples of both exact and no matches by the Baseline method.\\n\\n6 A NALYSIS\\nIn this section, we compare our approach with traditional OpenIE approaches and highlight the differences between them. We observe the effect of instruction tuning on the dataset and compare its performance. We explore the effects of the dataset with respect to different downstream tasks by using the models pre-finetuned on EDGAR on different downstream tasks.\\n\\n6.1 C ONTEXT\\n- NER VS. O PENIE\\nTraditionally, information extraction approaches from textual documents assume pre-specified relations for a given domain and employ crowd-sourcing or distant supervision approaches Hoffmann et al. (2011); Liu et al. (2016) to collect examples and train models for each type of relation. However, these approaches have a limitation in that they cannot extract unseen relations that were not observed or specified during training, rendering them impractical. In contrast, Open Information Extraction (OpenIE) Etzioni et al. (2008) does not rely on pre-defined relations but extracts them on-the-fly as they are encountered. To compare our methods with existing OpenIE models, we evaluated Stanford's OpenIE and AllenNLP OpenIE models Stanovsky et al. (2018) on a subset of the EDGAR10-Q.\"}"}
{"id": "4iQuByhNie", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nBased on the sentence given below, answer the following question:\\n\\nSentence: During the nine months ended September 30, 2020, we granted approximately 0.3 restricted stock units that are contingent upon us achieving earnings targets over the three year period from 2020 to 2022.\\n\\nQuestion: What is 0.3?\\n\\nResponse: In the given sentence, \u201c0.3\u201d refers to the number of restricted stock units that were granted during the nine months ended September 30, 2020.\\n\\nSentence: Certain selling equity holders elected to receive deferred, variable earn out consideration with an estimated value of $21,500 over the rollover period of three years.\\n\\nQuestion: What is 21,500?\\n\\nResponse: The amount $21,500 represents the estimated value of deferred, variable earn out consideration that certain selling equity holders elected to receive. This amount will be distributed over a period of three years during the rollover period.\\n\\nHANNOTATION PROCESS OF THE DATASET\\n\\nThe gold labels in our dataset are meticulously created by human annotators who are domain experts and possess a comprehensive understanding of the organizations involved. The annotation process follows guidelines from the SEC (U.S. Securities and Exchange Commission) for filing reports. As highlighted on the SEC website https://www.sec.gov/oiea/investor-alerts-and-bulletins/how-read-10-k10-q?_gl=1*32qw6m_*gcl_au*MTc5ODMzNDg2Ni4xNjk5MDYzOTU1, \u201cU.S. companies are required to present their financial statements according to a set of accounting standards, conventions and rules known as Generally Accepted Accounting Principles, or GAAP. An independent accountant audits the company's financial statements. For large companies, the independent accountant also reports on a company's internal controls over financial reporting.\u201d\\n\\nImportantly, the gold labels are not automatically extracted from the annual reports; rather, they are carefully annotated by human experts following the guidelines and procedures outlined in the SEC regulations. Given the critical nature of these reports in depicting an organization's financial well-being, they are prepared with utmost care, often under the oversight of the management team. As emphasized on the same SEC website, \u201cLaws and regulations prohibit companies from making materially false or misleading statements. Likewise, companies are prohibited from omitting material information that is needed to make the disclosure not misleading. In addition, a company's CFO and CEO must certify to the accuracy of the 10-K and 10-Q.\u201d\\n\\nTo ensure consistency, all instances in the dataset adhere to the guidelines provided by the SEC filing section for public organizations. These guidelines are outlined in the \u201cPrepare filing documents\u201d section, and we have included a reference to this information in our revised paper. For convenience and reference, the specific SEC guidelines can be found at https://www.sec.gov/edgar/filer-information/how-do-i.\\n\\nI.1 EFFECT OF CONTINUAL PRETRAINING\\n\\nWe did continual pre training with 100K samples of the dataset (we call this mode Cont. PT EDGAR T5) and then later fine tuned with the same downstream financial tasks described in the paper. The results presented in Table 21 were better than vanilla T5 but lesser than EDGAR T5.\\n\\nI.2 BASELINE WITH FINBERT\\n\\nIn this experiment we use FinBERT instead of vanilla BERT for our experimentation. The results are given in Table 22:\"}"}
{"id": "4iQuByhNie", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 21: Comparison of EDGAR-T5 and Vanilla T5 and Continual Pretrained EDGAR T5\\n\\n| # of Ent. | FinBERT Baseline Scores |\\n|----------|-------------------------|\\n|          | P  | R  | F1  |\\n| 1        | 36.59 | 27.12 | 28.21 |\\n| 2        | 36.34 | 29.56 | 29.90 |\\n| 3        | 32.54 | 25.92 | 26.49 |\\n| 4        | 29.32 | 24.62 | 24.53 |\\n| 5+       | 23.89 | 19.98 | 19.82 |\\n| Overall  | 35.89 | 26.56 | 27.28 |\\n\\nTable 22: Score of FinBERT baseline approach on the test set showing precision, recall and F1\\n\\nTable 23: Instances of exact match and no match by the baseline approach. Column Baseline denotes the responses generated by the baseline approach.\\n\\nTable 24: Responses of different OpenIE approaches on EDGAR 10-Q examples in Table 24.\"}"}
{"id": "4iQuByhNie", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Phrases extracted\\n\\nQuestion                      Answer\\nborrowing capacity,          available credit facility\\nWhat is borrowing capacity    on evolving credit loan?\\nHow much is available credit  facility?\\n$60,000 to $93,000\\n\\nborrowing capacity            on revolving credit loan\\nWhat is available credit      facility?\\n$33,000\\n\\npenalty of %, loan balance    What is 13-24 or 25-36?\\nWhat 13-24 or 25-36 is?\\nloan is paid during months\\ndate of repayment\\nWhat is 2% and 1%?\\nWhat is 2% and 1%?\\n2% (Wrong Answer)\\n\\nTable 13: Illustration of the baseline approach based on sentences in \u00a7C\\n\\nIn the 2nd sentence of the Table, none of the questions returned relevant answers, so the following questions were created:\\n\u2013 \u201cWhat is 13-24 or 25-36?\\\"\\n\u2013 \u201cWhat is 2% and 1%?\\\"\\n\\nIn the above cases, where questions are formed based on entities, the answers are checked if they have given any other entity as the answer. For instance, the questions, \u201cwhat is 2% and 1%?\u201d, return \u201c2\u201d as the answer to the second sentence. If the cases mentioned above hold, then the response is discarded. Here all the cases to identify the noun phrase associated with the entity fail, so no answer is returned.\\n\\nIf they do not fail, then the response is also considered a viable answer. For instance, in the 2nd sentence, the question was framed: \u201cWhat is 13-24 or 25-36?\u201d, which returned \u201cloan is paid during months\u201d as the answer.\\n\\nUsing the rules stated above, the entity and its associated noun phrases are identified. The last two columns of Table 13 show the questions which were generated and their responses from the MRC model. Inspired by the success of the pre-trained transformer model, we employ distilled BERT (Sanh et al., 2019) by Hugging Face (Wolf et al., 2020) trained on SQuAD dataset (Rajpurkar et al., 2016) as the MRC model for our zero-shot question answering.\\n\\nC.3 BASELINE METHOD CODE:\\nsent_parse: This function takes a row as input, which has a column named \u201cparagraph\u201d, and tokenizes the paragraph into sentences using the sent_tokenize function from the nltk library. It then iterates through each sentence and checks if the value in the row is present in the sentence. If it is, the function returns the sentence. If not, the function does nothing.\\nsentence_entity_flair(sentence, entity, entity_type): This function takes a sentence, an entity, and an entity type as input. It first removes words between parentheses that do not contain digits, as well as any forward slashes. It then removes any brackets surrounding a dollar amount. The function then uses an entity_tagger function to identify entities in the sentence, and iterates through each identified entity. Depending on the entity label and the entity type provided, the function checks if the entity matches the given entity. If it does, the function creates a list containing the entity, its label, and the original sentence, and returns it. If no matching entity is found, the function returns a list containing the original entity, a label of \u201cnone\u201d, and the original sentence.\\npreposition_phrase_extraction: This function takes a text as input and uses the nlp function from the spacy library to parse the text. It then iterates through each token in the parsed text, and if the token is an adposition (preposition), it checks if its headword is a noun or pronoun. If it is, the function creates a phrase by appending any adjectives or compound dependencies of the head noun, the head noun itself, and any nouns or proper nouns to the right of the preposition, along with the preposition. The function then returns a list of all phrases found.\\n\\n6 Hugging Face\u2019s Model Link: https://huggingface.co/transformers/v2.8.0/usage.html.\"}"}
{"id": "4iQuByhNie", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"noun_phrase_extraction: This function takes a text as input and uses the nlp function from the spacy library to parse the text. It then iterates through each token in the parsed text, and if the token is a noun or proper noun and its dependency is either \\\"dobj,\\\" \\\"pobj,\\\" \\\"nsubj,\\\" or \\\"nsubjpass,\\\" the function creates a phrase by appending any adjectives or compound dependencies of the noun and the noun itself. The function then returns a list of all phrases found.\\n\\nphrase_extraction: This function takes a text as input and uses the entity_tagger function to identify entities in the text. It then uses the preposition_phrase_extraction and noun_phrase_extraction functions to extract phrases from the text. For each extracted phrase, the function checks if it is present in any of the identified entities. If it is not, the function appends the phrase to a list of phrases to return. The function then returns the list of phrases.\\n\\nD.1 INSTRUCTIONS FOR HUMAN EVALUATIONS\\n\\nInstructions to Human Annotators\\n\\nYou are given an entity and a sentence that contains the entity. Your job is to generate a phrase that describes the meaning of the entity in the sentence. The phrase may not be present in the sentence and you may have to come up with phrase using your prior knowledge. An example is given to help you out:\\n\\nExample: Input:\\n$15. Issuance of common stock in May 2019 public offering at $243.00 per share, net issuance costs of $15.\\n\\nOutput:\\nCommon stock public offering issuance cost\\n\\nD.2 CHATGPT INSTRUCTIONS\\n\\nZero shot\\nDefinition: You are given a \\\"key term\\\" and a sentence. Based on the information in the sentence, output a brief description of the role of the \\\"key term\\\" in the context of the sentence. The output should be a brief relevant phrase describing the \\\"key term\\\" within a given sentence, regardless of whether the phrase is explicitly present in the sentence.\\n\\nOne shot\\nExample: Input:\\n$15. Issuance of common stock in May 2019 public offering at $243.00 per share, net issuance costs of $15.\\n\\nOutput:\\nCommon stock public offering issuance cost\\n\\nFew shot\\nExample 1:\\nInput:\\n$15. Issuance of common stock in May 2019 public offering at $243.00 per share, net issuance costs of $15.\\n\\nOutput:\\nCommon stock public offering issuance cost\\n\\nExample 2:\\nInput:\\n$1.8 billion. As of December 31, 2021 and 2020, the net carrying value of real estate collateralizing our mortgages payable totaled $1.8 billion.\\n\\nOutput:\\nNet carrying value of real estate collateralizing the mortgages payable\\n\\nExample 3:\\nInput:\\n75,305,400. $0.00001 par value\u201476,420,805 and zero shares authorized as of December 31, 2020 and September 30, 2021, respectively; 75,305,400 and zero shares issued and outstanding as of December 31, 2020 and September 30, 2021, respectively; and aggregate liquidation preference, $464,036 and zero as of December 31, 2020 and September 30, 2021, respectively\\n\\nOutput:\\nShares outstanding\\n\\nFigure 5 shows the precision and recall scores of instruction tuning Tk-Instruct on EDGAR10-Q. Similar trends are observed for precision and recall as for F1 score.\"}"}
{"id": "4iQuByhNie", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 14: Score of baseline approach on the test set showing precision, recall and F1.\\n\\n| # of Ent. | P     | R     | F1   |\\n|----------|-------|-------|------|\\n| 1        | 36.69 | 27.04 | 28.19|\\n| 2        | 36.57 | 29.53 | 29.66|\\n| 3        | 32.67 | 25.97 | 26.48|\\n| 4        | 29.59 | 24.11 | 24.27|\\n| 5+       | 23.82 | 19.35 | 19.56|\\n| Overall  | 34.59 | 27.06 | 27.59|\\n\\nTable 15: Details of ChatGPT performance on a smaller test set. The table shows the smaller test's statistics and ChatGPT's precision, recall, and F1.\\n\\n| # of Ent. | # of Sent. | Avg. S. Len. | P     | R     | F1   |\\n|----------|------------|--------------|-------|-------|------|\\n| 1        | 38919      |              | 29.68 | 25.13 | 42.19|\\n| 2        | 24717      |              | 33.61 | 26.25 | 50.99|\\n| 3        | 8131       |              | 39.03 | 26.60 | 48.86|\\n| 4        | 3341       |              | 50.28 | 26.56 | 50.37|\\n| 5+       | 1050       |              | 75.99 | 20.47 | 43.53|\\n| Overall  | 76158      |              | 33.49 | 25.72 | 47.49|\\n\\nTable 16: Exact match of baseline approach and ChatGPT.\\n\\n| # of Ent. | Baseline | ChatGPT |\\n|----------|----------|---------|\\n| 1        | 4.88     | 0.84    |\\n| 2        | 6.58     | 1.28    |\\n| 3        | 5.97     | 1.29    |\\n| 4        | 5.44     | 1.84    |\\n| 5+       | 5.46     | 0.81    |\\n| Overall  | 5.77     | 1.18    |\\n\\nTable 17: Exact match scores of supervised learning models. TkInst w. Inst denotes instruction tuning TkInstruct.\\n\\n| # of Ent. | Bart | Base | T5 | Base | Large | Flan T5 | Base | Tk Inst | Tk Inst w. Inst |\\n|-----------|------|------|----|------|-------|---------|------|---------|-----------------|\\n| 1         | 17.46| 15.82| 17.03| 16.05| 16.02 | 17.35  |      |         |                 |\\n| 2         | 23.31| 22.12| 23.52| 22.56| 22.23 | 24.04  |      |         |                 |\\n| 3         | 21.40| 19.29| 21.42| 19.97| 19.66 | 22.20  |      |         |                 |\\n| 4         | 21.76| 20.80| 23.39| 20.74| 20.66 | 23.61  |      |         |                 |\\n| 5+        | 21.43| 18.38| 20.42| 18.21| 17.90 | 22.09  |      |         |                 |\\n| Overall   | 20.88| 19.31| 20.92| 19.64| 19.44 | 21.45  |      |         |                 |\\n\\nTable 18: No match of baseline approach and ChatGPT.\\n\\n| # of Ent. | Bart | Base | T5 | Base | Large | Flan T5 | Base | Tk Inst | Tk Inst w. Inst |\\n|-----------|------|------|----|------|-------|---------|------|---------|-----------------|\\n| 1         | 25.44| 25.46| 24.65| 25.31| 25.44 | 24.44  |      |         |                 |\\n| 2         | 23.39| 23.68| 22.73| 23.60| 23.62 | 22.91  |      |         |                 |\\n| 3         | 21.39| 22.17| 21.29| 22.50| 22.53 | 20.17  |      |         |                 |\\n| 4         | 23.18| 23.15| 21.94| 23.40| 23.05 | 21.51  |      |         |                 |\\n| 5+        | 24.88| 26.74| 27.07| 29.51| 28.99 | 23.83  |      |         |                 |\\n| Overall   | 23.74| 24.09| 23.24| 24.24| 24.23 | 22.83  |      |         |                 |\\n\\nTable 19: No match scores of supervised learning models. TkInst w. Inst denotes instruction tuning TkInstruct.\\n\\nE.1 Detailed Baseline Results\\n\\nTable 14 gives the detailed baseline results with entity-wise scores. Precision uniformly decreases from 36% to 23% while recall is ranging from 19% to 30%, leading to an overall F1 score in the range from 20% to 30% for each category of the baseline model. F1 shows a linearly decreasing trend with an increase in the number of entities (the exception being of 2 entity sentences higher than 1).\\n\\nE.2 Fine-Tuning Results\\n\\nFigure 5 shows the precision and recall results of further instruction tuning.\"}"}
{"id": "4iQuByhNie", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: ROUGE-L precision recall scores for showing the effect of instruction tuning Tk Instruct vs conventional finetuning. X-axis denotes scores on different numbers of entities and overall score.\\n\\nTable 20: We present some additional descriptive statistics about the EDGAR-10Q dataset here. Columns 1,2 and 3 present a list of top 20 most common bi-grams, tri-grams and 4-grams respectively (along with their counts).\\n\\nIn this section we conduct a case study on format conversion where we find that ChatGPT fails to do this task on converting this to a QA task (\u201cWhat is entity?\u201d keeping sentences as the context).\"}"}
