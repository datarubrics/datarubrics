{"id": "iaYcJKpY2B_", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER.\\n\\nWe show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.\\n\\nINTRODUCTION\\nCreating a program has typically involved a human entering code by hand. The goal of program synthesis is to automate the coding process, and generate a computer program that satisfies the user's specified intent. Some have called it the holy grail of computer science (Manna & Waldinger, 1971; Gulwani et al., 2017). Successful program synthesis would not only improve the productivity of experienced programmers but also make programming accessible to a wider audience.\\n\\nTwo key challenges arise when striving to achieve program synthesis: (1) the intractability of the search space, and (2) the difficulty of properly specifying user intent. To maintain an expressive search space, one needs a large search space, which poses challenges in efficient search. Previous work (Joshi et al., 2002; Panchekha et al., 2015; Cheung et al., 2013) leverages domain-specific language to restrict the search space; however, this limits the applicability of synthesized programs. On the contrary, while being widely applicable, general-purpose programming languages (e.g., C, Python) introduce an even larger search space for possible programs. To navigate through the enormous program space, we formulate the task as language modeling, learning a conditional distribution of the next token given preceding tokens and leverage transformers (Vaswani et al., 2017) and large-scale self-supervised pre-training. This approach has seen success across modalities (Devlin et al., 2019; Lewis et al., 2020; Dosovitskiy et al., 2021). Likewise, prior works have developed pre-trained language models for programming language understanding (Kanade et al., 2020; Feng et al., 2020).\\n\\nTo realize program synthesis successfully, users must employ some means to communicate their intent to the models such as a logical expression (which specifies a logical relation between inputs...\"}"}
{"id": "iaYcJKpY2B_", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023.\\n\\nOn the one hand, a complete formal specification enjoys the exact specifications of user intent but may require domain expertise and effort from users to translate the intent to such a form. On the other hand, specification merely based on input-output examples is less costly but may under-specify the intent, leading to inaccurate solutions. Previous work has benefited from various methods and their combinations as the input to program synthesis models, including pseudo-code (Kulal et al., 2019), a part of a program and its documentation (Chen et al., 2021), or natural language paragraph with input-output examples (Hendrycks et al., 2021). However, we argue that a truly user-friendly form of intent is natural language text.\\n\\nTo overcome these challenges, we propose a multi-turn program synthesis approach, where a user communicates with the synthesis system by progressively providing specifications in natural language while receiving responses from the system in the form of synthesized subprograms, such that the user together with the system complete the program in multiple steps. The following two considerations motivate this approach.\\n\\nFirst, we speculate that factorizing a potentially long and complicated specification into multiple steps would ease the understanding by a model and hence enhance program synthesis. In the multi-turn approach, a model can focus on the specification associated with one subprogram and avoid arduously tracking the complicated dependency among subprograms. This effectively reduces the search space besides the convenience of specifying user intent. Indeed, our speculations are confirmed in our experiments with higher quality synthesized programs through the multi-turn approach.\\n\\nSecond, code exhibits a weak pattern of interleaved natural and programming language, which may be exploitable. Such a pattern is formed by programmers who explain the functionality of a program with comments. With the language modeling objective, we hypothesize that the interleaving pattern provides a supervision signal for the model to generate programs given natural language descriptions over multiple turns. The signal is highly noisy or weak, because only a subset of data would exhibit such a pattern, comments may be inaccurate or uninformative, and some of them may even be placed at an irrelevant position. However, up-scaling the model and data size might overcome such weak supervision, allowing the model to develop multi-turn program synthesis capacity. This enables user intent to be expressed in multiple turns, that is, the intent can be decomposed and fulfilled part by part while each turn can easily be expressed in natural language.\\n\\nIn this work, we develop a multi-turn programming benchmark to measure the models' capacity for multi-turn program synthesis. To solve a problem in the benchmark, a model needs to synthesize a program in multiple steps with a user who specifies the intent in each turn in natural language. Please refer to Figure 1 for an example where the model synthesizes a program to extract the user name of an email address. Performance on the benchmark is measured by pass rate on expert-written test cases. To the best of our knowledge, this is the first multi-turn program synthesis benchmark, which allows quantitative analysis of multi-turn program synthesis. With the emergence of multi-turn program synthesis capacity in large language models that benefits problem-solving, we believe this benchmark will foster future research in program synthesis.\\n\\nOur Contributions\\nOur work shares the basic idea of adopting language models for program synthesis with the recent and concurrent efforts (Chen et al., 2021; Austin et al., 2021; Li et al., 2022) with a single-turn user intent specification. In addition, we contribute with respect to four aspects:\\n\\n- We study multi-turn program synthesis emerging in autoregressive models under scaling laws.\\n- We leverage this capacity to introduce a multi-turn program synthesis paradigm.\\n- We investigate its properties quantitatively with a novel multi-turn programming benchmark.\\n- We will open source model checkpoints and the custom training library: JAXFORMER.\\n\\nFor program synthesis, no large-scale models competitive with Codex are available as open-source. This hinders progress, given that the expensive compute resources required to train these models are only accessible to a limited number of institutions. Our open source contribution allows a wide range of researchers to study and advance these models, which may greatly facilitate research progress.\\n\\n1 Benchmark: https://github.com/salesforce/CodeGen/tree/main/benchmark\\n2 Checkpoints: https://github.com/salesforce/CodeGen\\n3 Training: https://github.com/salesforce/jaxformer\"}"}
{"id": "iaYcJKpY2B_", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Evaluation results on the Multi-Turn Programming Benchmark. The multi-turn program synthesis performance varies as a function of model size (columns) and code data size (rows).\\n\\n| Model              | 350M | 2.7B | 6.1B | 16.1B | 350M | 2.7B | 6.1B | 16.1B |\\n|--------------------|------|------|------|-------|------|------|------|-------|\\n| THEPILE GPT-N & GPT-J | 0.79 | 8.17 | 18.86|       |      |      |      |       |\\n| THEPILE CODE GEN -NL | 0.23 | 15.31| 19.37| 30.33 |      |      |      |       |\\n| BIGQUERY CODE GEN -MULTI | 4.09 | 20.82| 25.51| 26.27 |      |      |      |       |\\n| BIGPYTHON CODE GEN -MONO | 16.98| 38.72| 43.52| 47.34 |      |      |      |       |\\n\\nTable 4: Comparison between multi- and concatenated single-turn specifications on perplexity (PPL) and program synthesis performance (as measured by pass rate) under CODEGEN-MONO models.\\n\\n4.2 EXECUTION ENVIRONMENT AND SOLUTION EVALUATION\\n\\nFor execution, the history of pairs of prompts and generated completions is concatenated into a self-contained program (see Figure 1 for an example). The program is then executed in an isolated Python environment following the single-turn HumanEval benchmark (Chen et al., 2021). However, the problems in HumanEval are constructed in such a way that a known function signature is completed, thus invocation of the generated code under a set of functional unit tests is trivial. In our multi-turn case, no such entry point (or return value) is guaranteed to be generated. To circumvent the issue of a missing return signature (or value), the last prompt of the multi-turn problems in MTPB is always specified to print out the resulting state to the terminal. Then, the benchmark execution environment overloads the Python `print(args)` function and stores `args` on a stack. If the sampled code for the last prompt of a problem does not include the `print()` statement, which is a valid convention to print on the terminal in Python or specifically Jupyter notebooks, then the AST of the generated code will be mutated to inject an invocation of `print()`. Finally, a type-relaxed equivalence check (e.g., an implicit conversion between lists and tuples) of `args` against the predefined gold output of the problem is performed to determine test failure or success.\\n\\n4.3 MULTI-STEP PROGRAMMING CAPACITY SCALES WITH MODEL SIZE AND DATA SIZE\\n\\nIn this analysis, we investigate how the model size and data size affect the program synthesis capacity in a multi-turn paradigm. In the MTPB, each problem has 5 test cases and we sample 40 samples for each test case with each model, based on which the pass rate is computed for each problem. The MTPB evaluation results (average pass rate) for our CODEGEN models, baselines, and OpenAI Codex models are shown in Table 3. Clearly, the performance on the MTPB improves as a function of the model size and data size. This suggests that the capacity of multi-step program synthesis scales as a function of the model size and data size. The models are simply trained with an autoregressive language modeling objective. While the model and the data scale up, multi-turn program synthesis capacity emerges, that is, the capacity to synthesize programs in a multi-turn fashion.\\n\\n---\\n\\n8 Accessed on November 10th, 2022.\"}"}
{"id": "iaYcJKpY2B_", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Difference in average pass-rate of problems in single-turn and multi-turn formulation over levels of problem difficulty. The improvement is sizable for most model sizes and difficulty levels, except for easy problems with larger models.\\n\\nWe hypothesize that multi-turn factorization enhances the model's understanding of user intent specifications, which in turn lead to higher program synthesis capacity. To test this hypothesis, we form a single-turn counterpart of multi-turn specifications by concatenating each specification into a single turn. As discussed in Section 3.2, we adopt the prompt perplexity as a proxy for user intent understanding. Thus, we compare the perplexity of the multi-turn prompts and that of the concatenated single-turn prompts under the four CODEGEN-MONO models.\\n\\nThe average perplexity (see Appendix E for the calculation details) over all the problems in the MTPB is displayed in the left panel of Table 4. For all models, the single-turn specification has a higher average perplexity than the multi-turn specification. It implies that the multi-turn user specifications can be better understood by the models. We notice that the average perplexity for both multi-turn and single-turn intent specifications under larger models is slightly lower than that under smaller models, indicating that the larger ones understand the user intent better than the smaller ones.\\n\\nWe compare the program synthesis pass rate with the multi-turn prompts to that with the concatenated single-turn prompts. The results are shown in the right panel of Table 4. Multi-turn specifications lead to close to or more than 10 percentage points over single-turn specifications for all model sizes. Together with the perplexity analysis above, it appears that factorizing a user specification into multiple steps and leveraging the emerged capacity of large language models allow them to digest the specification more easily and synthesize programs more successfully.\\n\\nFurthermore, we categorize the problems by difficulty level based on their average pass rates (\u201chard\u201d with less than 30%, \u201ceasy\u201d with larger than 70%), and examine the interaction effect between difficulty level and model size on the improvement by multi-turn factorization. See the results in Figure 2.\\n\\nAcross almost all model sizes and difficulty levels, multi-turn prompts lead to significant improvement over single-turn prompts and most improvements are nearly or higher than 10 percentage points. Interestingly, the larger models (6.1B and 16.1B) are invariant to multi-turn factorization for easy problems (see the two short bars, 0.19% and -0.25%, in Figure 2). This implies that when the problems can be easily understood by the model (due to the combined effect of easiness of the problems and the high capacity of larger models), it is not necessary or beneficial to factorize the specifications. This is in fact consistent with our motivating assumption that factorizing complicated specifications would ease problem understanding and improve program synthesis.\\n\\n4.5 QUALITATIVE EXAMPLES\\n\\nTo further understand the differences in model behavior over model sizes, we examine cases where large models have contrasting performances to smaller models. We specifically select problems for which CODEGEN-MONO 16.1B and CODEGEN-MONO 2.7B show a significant discrepancy in performance. On problems where CODEGEN-MONO 16.1B performed significantly worse compared to CODEGEN-MONO 2.7B, we observe that the larger model becomes inflexible due to taking the prompt literally. For example, initializing a number always results in an integer, despite the prompt asking to cast into a string (Figure 3), or the \u201creturn\u201d keyword in a prompt triggers a function definition while the intent is to directly generate an executable program (Figure 4). However in general, larger-scale models overcome mistakes due to prompt misinterpretation by smaller models, including assigning multiple variables at the same time (Figure 5) or understanding the concept of any comparison (Figure 6).\"}"}
{"id": "iaYcJKpY2B_", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"While program synthesis has a long history, two inherent challenges remain unsolved: (1) intractability of the program space and (2) difficulty in accurately expressing user intent (Manna & Waldinger, 1971; Gulwani et al., 2017). A large body of prior research attempted to address (1) by exploring methods like stochastic search techniques (Parisotto et al., 2017; Schkufza et al., 2013) and deductive top-down search (Gulwani, 2011; Polozov & Gulwani, 2015). However, the scalability of these approaches is still limited. User intent can be expressed with various methods: formal logical specifications, input-output examples, and natural language descriptions. Complete and formal specifications require too much effort, while informal ones like input-output examples often under-specify problems (Gulwani, 2011). Well-learned conditional distribution and language understanding capacity owing to the large-scale model and data allows for efficient solutions for these two challenges. Several works investigate converting conversational intents into programmable representations, such as SQL (Yu et al., 2019a;b) or dataflow graph (Andreas et al., 2020). Our proposed benchmark requires the generation of Python, which is more general and complex.\\n\\nLarge Language Models\\nTransformers capture dependency among sequence elements through attention mechanism (Bahdanau et al., 2014) and are highly scalable. It has been successfully applied to natural language processing (Devlin et al., 2019; Lewis et al., 2020; Raffel et al., 2020), computer vision (Dosovitskiy et al., 2021), and many other areas (Oord et al., 2018; Jumper et al., 2021). Prior works, such as CuBERT (Kanade et al., 2020), CodeBERT (Feng et al., 2020), PyMT5 (Clement et al., 2020), and CodeT5 (Wang et al., 2021), have applied transformers towards code understanding but these mostly focus on code retrieval, classification, and program repair. Several recent and concurrent efforts explore using large language models for program synthesis (Chen et al., 2021; Austin et al., 2021; Li et al., 2022; Fried et al., 2022) and its effectiveness (Vaithilingam et al., 2022). While they focus on generating code in a single turn, we propose to factorize the specifications into multiple turns and demonstrate that it is highly effective to improve synthesis quality. It is worth pointing out that Austin et al. (2021) explored refining the code in multiple iterations, but it is essentially a single-turn approach since a complete program is produced in every single turn. Prompting pre-trained language models with intermediate information to improve task performance has attracted interest (Nye et al., 2021; Wei et al., 2022). Our proposed MTPB also allows the model to leverage past turns as context.\\n\\nBenchmarks for Program Synthesis\\nTo quantitatively evaluate program synthesis models, several benchmarks have been proposed with different input forms. A popular input forms include preceding code in the same line (Raychev et al., 2016), pseudo-code (Kulal et al., 2019), a docstring and function signature (Chen et al., 2021), or problem description (Hendrycks et al., 2021). In most of those cases, only directly relevant input information is given to the model. In contrast, a few previous works instantiate benchmarks that measure the ability to generate programs given surrounding program context beyond the target program, such as variables and other methods (Iyer et al., 2018) or alternating \u201ccells\u201d of preceding code and text blocks (Agashe et al., 2019), while the primary focus is to generate the target program itself. We propose a new benchmark that requires a progressive generation of subprograms through multi-turn prompts.\\n\\nCONCLUSION\\nWe study program synthesis with large causal language models trained on large corpora of code data. The capacity to understand long context and generate coherent responses emerges from the simple language modeling as the model size and data size scale up. Leveraging this capacity and observing that better user intent understanding leads to better program synthesis, we propose a multi-step program synthesis approach in which program synthesis is achieved through a multi-turn specification and code generation. Moreover, we develop the Multi-Turn Programming Benchmark (MTPB) to investigate our models' capacity on synthesizing programs in such a multi-step paradigm. Our experiments show that the multi-step program synthesis capacity scales as a function of the model size and data size. The intent specifications, which are specified in multiple steps, are digested more easily by the models and lead to more accurate program synthesis. We open-source the training code and the model checkpoints to facilitate future research and practical applications in this area.\"}"}
{"id": "iaYcJKpY2B_", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"All variants of CODEGEN are firstly pre-trained on the Pile, which includes a small portion of profane language. Focusing on the GitHub data that best aligns our expected use case of program synthesis, Gao et al. (2020) report that 0.1% of the data contained profane language, and has sentiment biases against gender and certain religious groups. Thus, while we did not observe in our samples, CODEGEN may generate such content as well. In addition to risks on natural language outputs (e.g., docstrings), generated programs may include vulnerabilities and safety concerns, which are not remedied in this work. Models should not be used in applications until being treated for these risks.\"}"}
{"id": "iaYcJKpY2B_", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To evaluate the emergence of multi-turn programming capabilities under scaling laws, we adopt standard transformer-based autoregressive language models, varying (1) the number of model parameters (350M, 2.7B, 6.1B, 16.1B) and (2) the number of tokens of programming languages in the training corpora. For scaling the training, a custom library JAXFORMER for TPU-v4 hardware was developed and will be released as open-source, including the trained model weights.\\n\\n2.1 DATASETS\\n\\nThe family of CODEGEN models is trained sequentially on three datasets: THEPILE, BIGQUERY, and BIGPYTHON.\\n\\nThe natural language dataset THEPILE is an 825.18 GiB English text corpus collected by Gao et al. (2020) for language modeling (MIT license). The dataset is constructed from 22 diverse high-quality subsets, one of which is programming language data collected from GitHub repositories with >100 stars that constitute 7.6% of the dataset. Since the majority of THEPILE is English text, the resulting models are called as natural language CODEGEN models (CODEGEN-NL).\\n\\nThe multi-lingual dataset BIGQUERY is a subset of Google's publicly available BigQuery dataset, which consists of code (under open-source license) in multiple programming languages. For the multi-lingual training, the following 6 programming languages are chosen: C, C++, Go, Java, JavaScript, and Python. Thus, we refer to models trained on the BIGQUERY as multi-lingual CODEGEN models (CODEGEN-MULTI).\\n\\nThe mono-lingual dataset BIGPYTHON contains a large amount of data in the programming language, Python. We have compiled public, non-personal information from GitHub consisting of permissively licensed Python code in October 2021. Consequently, we refer to models trained on BIGPYTHON as mono-lingual CODEGEN models (CODEGEN-MONO).\\n\\nThe pre-processing follows: (1) filtering, (2) deduplication, (3) tokenization, (4) shuffling, and (5) concatenation. For details on THEPILE, we refer to Gao et al. (2020). For BIGQUERY and BIGPYTHON, we refer to Appendix A. Table 5 summarizes the statistics of the training corpora.\\n\\n2.2 MODELS\\n\\nThe CODEGEN models are in the form of autoregressive transformers with next-token prediction language modeling as the learning objective trained on a natural language corpus and programming language data curated from GitHub. The models are trained in various sizes with 350M, 2.7B, 6.1B, and 16.1B parameters. The first three configurations allow for direct comparison with open-sourced large language models trained on text corpus, GPT-NEO (350M, 2.7B) (Black et al., 2021) and GPT-J (6B) (Wang & Komatsuzaki, 2021). See Table 6 in Appendix A for model specifications.\\n\\nThe CODEGEN models are trained in a sequential nature over datasets. CODEGEN-NL is first trained on THEPILE. CODEGEN-MULTI is initialized from CODEGEN-NL and trained on BIGQUERY. Finally CODEGEN-MONO is initialized from CODEGEN-MULTI and trained on BIGPYTHON.\\n\\nThe emergence of program synthesis conditional on descriptions in natural language may stem from the size of the models and data, training objective, and nature of the training data itself. This is called emergence since we do not explicitly train the model on comment-code pairs. Similar phenomena are observed in a wide range of natural language tasks where a large-scale unsupervised language model can solve unseen tasks in a zero-shot fashion (Brown et al., 2020). The emergence phenomena or surprising zero-shot generalization is often attributed to the large scale of the model and the data.\\n\\nWhile our focus is not to reveal the underlying mechanism on why program synthesis capacity emerges from simple language modeling, we make an attempt to provide an explanation given the nature of our modeling approach and the training data. The data consists of regular code from GitHub (without manual selection), for which some data exhibits a pattern of interleaved natural and programming language, which we believe provides a noisy supervision signal for the program synthesis capacity due to the next-token prediction training objective. However, we emphasize that such a data pattern is highly noisy and weak, because only a subset of data exhibits such a pattern, e.g., comments may be inaccurate or uninformative, and some of them may even be placed at an irrelevant.\"}"}
{"id": "iaYcJKpY2B_", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Evaluation results on the HumanEval benchmark. Each pass@$k$ (where $k \\\\in \\\\{1, 10, 100\\\\}$) for each model is computed with three sampling temperatures ($t \\\\in \\\\{0.2, 0.6, 0.8\\\\}$) and the highest one among the three are displayed, which follows the evaluation procedure in Chen et al. (2021). Results for the model marked with $^\\\\ast$ are from Chen et al. (2022).\\n\\n| Model       | 350M   | 2.7B   | 6.1B   | 16.1B  |\\n|-------------|--------|--------|--------|--------|\\n| GPT-N EO    | 0.85   | 6.41   | 21.37  | 33.5   |\\n| GPT-J       | 11.62  | 21.36  | 59.50  | 39.0   |\\n| CODEX       | 13.17  | 21.36  | 59.50  | 39.0   |\\n| CODEX GEN-NL| 2.12   | 6.70   | 22.84  | 39.0   |\\n| CODEX GEN-MULTI | 6.67 | 14.51  | 38.56  | 47.0   |\\n| CODEX GEN-MONO | 6.67 | 14.51  | 38.56  | 47.0   |\\n\\nThe scaling of such LLMs requires data and model parallelism. To address these requirements, a training library JAXFORMER (https://github.com/salesforce/jaxformer) was developed for efficient training on Google's TPU-v4 hardware. We refer to Appendix A for further details on the technical implementation and sharding schemes.\\n\\n3 Single-Turn Evaluation\\n\\nWe first evaluate our CODEXGEN using an existing program synthesis benchmark: HumanEval (MIT license) (Chen et al., 2021). HumanEval contains 164 hand-written Python programming problems. Each problem provides a prompt with descriptions of the function to be generated, function signature, and example test cases in the form of assertions. The model needs to complete a function given the prompt such that it can pass all provided test cases, thus measuring the performance by functional correctness. Since a user intent is specified in a single prompt and provided to the model once, we regard the evaluation on HumanEval as a single-turn evaluation, to distinguish it from the multi-turn evaluation which we introduce in the next section. Following Chen et al. (2021), we recruit nucleus sampling (Holtzman et al., 2020) with top-$p$ where $p = 0.95$.\\n\\n3.1 Human Performance Scales as a Function of Model Size and Data Size\\n\\nWe compare our models to the Codex models (Chen et al., 2021), which demonstrate the state-of-the-art performance on HumanEval. Moreover, our models are compared to open-sourced large language models, GPT-N EO (Black et al., 2021) and GPT-J (Wang & Komatsuzaki, 2021). These are trained on THEPILE (Gao et al., 2020), and thus similar to our CODEXGEN-NL models, in terms of training data and model size. All models are evaluated with temperature $t \\\\in \\\\{0.2, 0.6, 0.8\\\\}$, and we compute pass@$k$ where $k \\\\in \\\\{1, 10, 100\\\\}$ for each model. For direct comparison to the results by Chen et al. (2021), we choose the temperature that yields the best-performing pass@$k$ for each.\"}"}
{"id": "iaYcJKpY2B_", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"k. The results of our models and baselines are summarized in Table 1. Our C\\\\textsc{ODE\\\\textsc{GEN}}-\\\\textsc{NL} models (350M, 2.7B, 6.1B) outperform or perform on par with the respective GPT-N \\\\textsc{EO} and GPT-J models. Further training C\\\\textsc{ODE\\\\textsc{GEN}}-\\\\textsc{NL} on multilingual programming language data (B\\\\textsc{IG\\\\textsc{U\\\\textsc{ERY}}}) leads to C\\\\textsc{ODE\\\\textsc{GEN}}-\\\\textsc{MULTI}. The multilingual C\\\\textsc{ODE\\\\textsc{GEN}} models outperform the models trained on T\\\\textsc{HE\\\\textsc{PILE}} (GPT-N \\\\textsc{EO}, GPT-J, C\\\\textsc{ODE\\\\textsc{GEN}}-\\\\textsc{NL}) by a large margin. We then finetune C\\\\textsc{ODE\\\\textsc{GEN}}-\\\\textsc{MULTI} on a Python-only dataset (B\\\\textsc{IG\\\\textsc{PYTHON}}), resulting in C\\\\textsc{ODE\\\\textsc{GEN}}-\\\\textsc{MONO}. The program synthesis capacity is improved substantially. Therefore, the Python program synthesis capacity enhances as the amount of Python training data increases. For almost all models, as expected, increasing the size of the model improves overall performance.\\n\\nOur Python-monolingual C\\\\textsc{ODE\\\\textsc{GEN}} models have competitive or improved performance, compared to the current state-of-the-art models, Codex. C\\\\textsc{ODE\\\\textsc{GEN}}-\\\\textsc{MONO} 2.7B underperforms C\\\\textsc{ODEX} 2.5B when $k = 100$ but outperforms it when $k \\\\in \\\\{1, 10\\\\}$. While it is only half the size, our C\\\\textsc{ODE\\\\textsc{GEN}}-\\\\textsc{MONO} 6.1B demonstrates pass@$k$ scores approaching those of the best-performing Codex, C\\\\textsc{ODEX} 12B. Our largest model C\\\\textsc{ODE\\\\textsc{GEN}}-\\\\textsc{MONO} 16.1B is competitive or outperforms it depending on $k$.\\n\\n3.2 Better User Intent Understanding Yields Better Synthesized Programs\\nThe success of a program synthesis system highly depends on how well it understands user intent. When the system is based on a language model, the perplexity of problem prompts provides a proxy for the system's understanding of user intent specifications. A low perplexity of an intent specification under a model indicates that this intent specification is compatible with the knowledge learned by the model from the training data. We investigate whether better prompt understanding, with lower prompt perplexity as a proxy, leads to more functionally accurate programs.\\n\\nWe partition all problems into pass versus non-pass ones. A pass problem is one that at least one sample from 200 samples passes all test cases, while for a non-pass problem none of the 200 samples pass all test cases. We compute the average perplexity of the problem prompts of the pass problems and that of the non-pass ones, based on samples from C\\\\textsc{ODE\\\\textsc{GEN}}-\\\\textsc{MONO} models. The results are displayed in Table 2 (see Appendix F for the results on C\\\\textsc{ODE\\\\textsc{GEN}}-\\\\textsc{NL} and C\\\\textsc{ODE\\\\textsc{GEN}}-\\\\textsc{MULTI}). The prompts of the pass problems have lower perplexity than those of the non-pass ones. This finding implies that program synthesis is more likely to be successful when the user intent specification is understood better by the model. Indeed, some training data contains interleaved sequences of natural language comments and programs, where the comments describe the functionality of the following program. We thus speculate that user intent specifications similar to such a pattern would be better understood by the model, and hence lead to better program synthesis. Inspired by this pattern, we propose to specify user intent in multiple turns such that the model focuses on a partial problem at a time, which would make user intent understanding by the model easier.\\n\\n4 Multi-Turn Evaluation\\nIn this section, we propose and study a multi-step program synthesis paradigm where program synthesis is decomposed into multiple steps and the system synthesizes a subprogram in each step. To examine such a paradigm, we first develop a Multi-Turn Programming Benchmark (MTPB). MTPB consists of 115 problems written by experts, each of which includes a multi-step descriptions in natural language (prompt). To solve a problem, a model needs to synthesize functionally correct subprograms (1) following the description at the current step and (2) considering descriptions and synthesized subprograms at previous steps (e.g., correct backreference of functions and/or variables defined in the previous steps). An illustrative example is shown in Figure 1.\"}"}
{"id": "iaYcJKpY2B_", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# Import re and define a regular expression that matches an email address.\\n\\n```python\\nimport re\\nemail_regex = re.compile(r\\\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9\\\\-\\\\._]+)\\\"\\n)\\n\\n# Search for an email address in \\\"... abc.xyz@example.com ...\\\" and store the first match to a variable \\\"address\\\".\\naddress = email_regex.search(\\\"... abc.xyz@example.com ...\\\").group(0)\\n\\n# Remove the substring starting from the @ symbol from \\\"address\\\".\\naddress = address[:address.find('@')]\\n\\n# Replace non-alphabetical symbols with a whitespace in \\\"address\\\".\\naddress = re.sub(r\\\"[^a-zA-Z]+\\\", \\\" \\\", address)\\n\\n# Print out \\\"address\\\".\\nprint(address)\\n```\\n\\nFigure 1: An illustrative example for the Multi-Turn Programming Benchmark, performing the task of extracting the user name of an email address.\\n\\nEach problem consists of prompts $p_i$ and unit tests, where some prompts include templates (i.e. `{input}`) that are filled with test case inputs before it is fed to the model. In the displayed example, the input is a string containing `abc.xyz@example.com`, which replaces `{input}` in $p_2$, and the expected output is `abc xyz`.\\n\\nOur model conditions on the concatenation of interleaved past prompts and generated responses. Generated responses from each turn are concatenated and executed, where the output is compared to the answer.\\n\\n4.1 BENCHMARK CONSTRUCTION\\n\\nWe (4 authors) start by defining 115 problems requiring a diverse range of programming knowledge, including math, array operations, string manipulations, algorithms, data science, and problems that require other knowledge, such that the number of problems in each category is roughly balanced.\\n\\nFor each problem, we construct a triplet consisting of multi-turn prompts $P$, test case inputs $I$, and test case outputs $O$. Multi-turn prompts are designed following the two constraints: (1) the problem is decomposed into 3 or more turns, (2) a single turn cannot be attributed to solving the problem. For example, implementing a linear regression model could be phrased as \\\"Perform linear regression on $x$ and $y\\\". Since the main task is fully expressed in this prompt, understanding this prompt is sufficient to perform the task. We avoid such cases via manual inspection and distribute problem-solving over turns. Together with the prompts, we task the problem author to prepare 5 sets of test case inputs $I$ and outputs $O$ to evaluate model outputs with functional correctness. To reduce wrongly rewarding false positive solutions that give meaningless programs but pass the tests, we examine and revise such cases to ensure the test quality.\\n\\nUnlike HumanEval for which models are expected to complete a partially defined function, MTPB problems only provide the prompts, thereby models have to generate the solution from scratch. While the free-form generation may allow for more potential solutions, the lack of an entry point to provide test case inputs makes it challenging to test the generated code on diverse test cases. To overcome this challenge, we instead embed test case inputs within prompts. Specifically, prompts are written with Python's formatted string where input values are substituted for the variable name when a specific test case is applied to the problem. For example, a prompt, \\\"Define a string named 's'\\n\\nProblem writing was performed in a closed book format, i.e. we are not allowed to consult with online resources while writing the problems.\\n\\nSee Appendix D for a complete listing.\\n\\nTo guide sampling in Python, we prefix the prompt with:\\n\\n```\\n# Import libraries.\\n import numpy as np\\n```\\n\\nhttps://docs.python.org/3/reference/lexical_analysis.html#f-strings\"}"}
{"id": "iaYcJKpY2B_", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Hyper-parameters for model specification and optimization for the family of C\\\\textsc{ODE}\\\\textsc{Gen} models.\\n\\n| Model | Dataset | Hyper-parameter | 350M | 2.7B | 6.1B | 16.1B |\\n|-------|---------|-----------------|------|------|------|-------|\\n|       |         | Number of layers | 20   | 32   | 33   | 34    |\\n|       |         | Number of heads  | 16   | 32   | 16   | 24    |\\n|       |         | Dimensions per head | 64  | 80   | 256  | 256   |\\n|       |         | Context length   | 2,048| 2,048| 2,048| 2,048 |\\n|       |         | Batch size       | 500k | 1M   | 2M   | 2M    |\\n|       |         | Weight decay     | 0.1  | 0.1  | 0.1  | 0.1   |\\n|       |         | Learning rate    | \\\\(3.0 \\\\times 10^{-4}\\\\) | \\\\(1.6 \\\\times 10^{-4}\\\\) | \\\\(1.2 \\\\times 10^{-4}\\\\) | \\\\(9.9 \\\\times 10^{-5}\\\\) |\\n|       |         | Warm-up steps    | 3k   | 3k   | 3k   | 3k    |\\n|       |         | Warm-up / Total steps | 350k| 350k| 350k| 350k |\\n\\nA.3 Training\\n\\nThe scaling of large language models requires data and model parallelism. Google's TPU-v4 hardware with a high-speed toroidal mesh interconnect naturally allows for efficient parallelism. To efficiently utilize the hardware, the training of the models is implemented in JAX (Bradbury et al., 2018). For parallel evaluation in JAX the \\\\texttt{pjit()} operator is adopted. The operator enables a paradigm named single-program, multiple-data (SPMD) code, which refers to a parallelism technique where the same computation is run on different input data in parallel on different devices.\\n\\nSpecifically, \\\\texttt{pjit()} is the API exposed for the XLA SPMD partitioner in JAX, which allows a given function to be evaluated in parallel with equivalent semantics over a logical mesh of compute.\\n\\nOur library JAX\\\\textsc{former} recruits a designated coordinator node to orchestrate the cluster of TPU-VMs with a custom TCP/IP protocol. For data parallelism, the coordinator partitions a batch and distributes the partitions to the individual TPU-VMs. For model parallelism, two schemes for the sharding of model parameters are supported: (1) Intra-TPU-VM, where parameters are sharded across MXU cores inside a physical TPU-v4 board and replicated across boards following Shoeybi et al. (2019); Wang & Komatsuzaki (2021); (2) Inter-TPU-VM, where parameters are sharded across TPU-v4 boards and activations are replicated following Rajbhandari et al. (2020).\\n\\nBoth intra-TPU-VM and inter-TPU-VM sharding schemes are implemented based on our specific \\\\texttt{pjit()} a logical mesh specification \\\\((r, p, c)\\\\) with \\\\(r\\\\) replicas of the parameters, \\\\(p\\\\) partitions of the parameters, and \\\\(c\\\\) logical cores per board over \\\\(n_b\\\\) TPU boards with each \\\\(n_c\\\\) logical cores such that \\\\(d \\\\times p = n_b\\\\) and \\\\(r \\\\times p \\\\times c = n_b \\\\times n_c\\\\).\\n\\nThe intra-TPU-VM scheme is adopted for models of size of less or equal to 6B parameters, the total amount of model and optimizer parameters fit into the combined HBM memory of a single TPU-v4 board. For instance, a TPU-v4-512 slice with \\\\(n_b = 64\\\\) and \\\\(n_c = 4\\\\) would be configured as \\\\((r, p, c) = (64, 1, 4)\\\\). That is, the parameters are being replicated across \\\\(r = 64\\\\) boards with \\\\(p = 1\\\\) total inter-board partitions and intra-board parallelism across \\\\(c = 4\\\\) logical chips. In this configuration, the mean gradient is accumulated across boards via \\\\texttt{with_sharding_constraint()} effectively emulating the behavior of the \\\\texttt{xmap()} operator.\\n\\n---\\n\\n9. https://jax.readthedocs.io/en/latest/_modules/jax/experimental/pjit.html\\n10. https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html\\n11. https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms\\n12. Specifically, 4 TPU-v4 chips (i.e., 8 physical which amount 4 logical or virtual MXU cores).\\n13. https://jax.readthedocs.io/en/latest/_autosummary/jax.experimental.maps.xmap.html\"}"}
{"id": "iaYcJKpY2B_", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The inter-TPU-VM scheme is adopted for models exceeding the size of 6B parameters for which the model and optimizer parameters have to be sharded across TPU-v4 boards. For instance, a TPU-v4-512 slice with $n_b = 64$ and $n_c = 4$ would be configured as $(r, p, c) = (1, 64, 4)$. For larger slices such as TPU-v4-1024 with $n_b = 128$, one may introduce redundancy in the parameter sharding, e.g., $(r, p, c) = (2, 64, 4)$. In this configuration, the activations are replicated across boards with_sharding_constraint(). Moreover, $(r, p, c)$ allows for backwards compatibility for the logical hardware layout transition from TPU-v3 with $c = 8$ to TPU-v4 with $c = 4$ by adjusting $p$ without the need for re-sharding.\\n\\nFor the optimization, Table 6 summarizes the hyper-parameters. We adopt the Adam (Kingma & Ba, 2015) optimizer with $(\\\\beta_1, \\\\beta_2, \\\\epsilon) = (0.9, 0.999, 1e^{-08})$ and global gradient norm clipping (Pascanu et al., 2013) of 1.0. The learning rate function over time follows GPT-3 (Brown et al., 2020) with warm-up steps and cosine annealing. In summary, we mainly adopted the GPT-3 reference configurations with minor variations accounting for TPU optimizations. We did not have the compute capacity to optimize these hyper-parameters further.\\n\\nWe use the unbiased estimator proposed in Chen et al. (2021) to compute pass@k. For each task, $n \\\\geq k$ samples are sampled. In particular, we use $n = 200$ and $k \\\\leq 100$. Suppose $c$ is the number of correct samples, among the $n$ samples, which pass all the unit tests. Then the unbiased estimator is defined as follows:\\n\\n$$\\\\text{pass}@k = \\\\text{Problems}^{-1} - \\\\frac{n - c}{k} \\\\frac{n}{k}$$\\n\\nDirectly computing this estimator is numerically unstable. We use the numerically stable numpy implementation introduced by Chen et al. (2021).\\n\\nWe perform the following type-relaxation before assessing the equivalence between model outputs and the expected outputs.\\n\\n\u2022 Convert numpy arrays into correspondingly typed lists of standard types (e.g. np.int32 will be cast to int).\\n\u2022 pandas series are converted and compared in numpy array format.\\n\u2022 For the rest, model outputs are cast into the type of gold standard outputs.\\n\u2022 Floating numbers are compared with $\\\\epsilon = 1e^{-6}$ as the tolerance threshold.\"}"}
{"id": "iaYcJKpY2B_", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Problem Name       | Problem Description                              | Category   |\\n|-------------------|--------------------------------------------------|------------|\\n| Sandwich string   | Append a string in the middle of another string. | string     |\\n| Normalize integer list | Normalize a list of positive integers and print formatted percentages. | math       |\\n| Convert time      | Convert units of time.                           | math       |\\n| Squared Fibonacci | Print the squared Fibonacci numbers.             | math       |\\n| Compare counts    | Compare the count of positive and negative numbers in a given list. | array      |\\n| Pandas mean       | Construct and compute the mean of a pandas DataFrame. | D.S.       |\\n| Fizz buzz         | Solve the fizz buzz problem.                    | Algo.      |\\n| Bi-grams          | Print the bi-grams of a sentence.                | string     |\\n| Top note          | Print the name with top note out of a dictionary. | dict       |\\n| Hex to binary     | Convert hex to binary and reverse.               | math       |\\n| Invert dict       | Detect an inversion of a given dictionary.       | dict       |\\n| Class definition  | Create a POJO class.                            | class      |\\n| Longest number    | Print the longest number.                        | math       |\\n| Linear regression | Fit linear regression model with specified function and sk-learn. | D.S.       |\\n| Encrypt and decrypt | Rotate alphabet for encryption, then reverse the operation. | Algo.      |\\n| Dedup custom objects | Implement a class with __hash__ and obtain a count unique objects. | class      |\\n| Drunken python    | Convert between integer and string without using built-in functions. | string     |\\n| Morse code        | Encode a string into morse code given its conversion rule. | Algo.      |\\n| Two-sum           | Implement the two-sum problem on a given input pair. | Algo.      |\\n| k-means           | Implement and run k-means on sampled points.     | D.S.       |\\n| Even odd sum      | Print the sum of even and odd numbers in a list. | math       |\\n| Shift zeros       | Move all the zeros in a list to the right.       | array      |\\n| Bootstrap 95% CI  | Calculate the bootstrap 95% confidence interval of an array. | D.S.       |\\n| Sum even digits   | Sum the even digits between two numbers.          | math       |\\n| Min-max diff      | Compute the difference between max and min numbers in a list. | array      |\\n| Distinct chars    | Print the sorted, case-insensitive unique characters of a string. | string     |\\n| Longer string     | Compare and print the longer string given two strings. | string     |\\n| Sum float digits  | Sum numbers before and after the decimal point of a float. | math       |\\n| Count vowels      | Count the number of vowels in a string.          | string     |\\n| Factorial         | Compute the factorial of n.                      | math       |\\n| Max edge triangle | Finds the maximum range of a triangle's third edge. | math       |\\n| Factorial & remainder | Compute the factorial and its remainder when divided. | math       |\\n| Sum polygon angles| Sum the angles in a polygon.                      | math       |\\n| Sum string numbers| Add together two numbers represented in string.   | string     |\\n| Min-max sum       | Sum the range from the minimum to the maximum of a list. | array      |\\n| Vowel overlap     | Find the number of overlapped vowels of two words. | string     |\\n| Sum negative      | Calculate the sum of negative numbers in a list. | math       |\\n| Load dataset      | Load from a file and print statistics.            | D.S.       |\\n| Char length list  | Return a list of non-punctuation character lengths from words. | string     |\\n| Hex to RGB        | Convert a six hexadecimal digit string into list of RGB values. | math       |\\n| Majority vote     | Check if a certain element is the majority of a given list. | array      |\\n| Week later        | Print the formatted date of a week later given a date. | string     |\\n| Sorted word weights| Check if the list of word weights (sum of ASCII values) are sorted. | math       |\\n| Create Palindrome | Sum pairs of adjacent digits until the number is palindrome. | string     |\\n| Simulate Backspace| Apply the backspace characters in a string and print the modified. | string     |\\n| Data manipulation | Manipulate a pandas DataFrame and split into train and test set. | D.S.       |\\n| Sum non-overlap   | Sum the integers in a (min, max) range that don't appear in a list. | array      |\\n| Detect digits     | Find if a string contains digits.                 | array      |\\n| Cascading functions | Sequentially invoke function objects in a given list. | math       |\\n| Pluralize duplicates | Pluralize duplicated words in a list.            | dict       |\\n| Highest altitude  | Given relative altitudes, find the highest altitude | array      |\\n| Truncate words    | Truncate a sentence so that it contains k words   | array      |\\n| Single element    | Find the elements that appear one time in an array | array      |\\n| Remove elements   | Remove all the occurrences of an element in an array | array      |\\n| Check array sum   | Check whether the sum of an array is equal to a given value | array      |\\n\\nTable 7: Problems in MTPB, showing the problem 1 to 55. D.S. and Algo. refers to data science and algorithm.\"}"}
{"id": "iaYcJKpY2B_", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Problem Name                  | Problem Description | Category |\\n|------------------------------|--------------------|----------|\\n| Merge sorted lists           | Merge two sorted lists into one | Algo.    |\\n| Maximum subarray             | Find the max contiguous subarray and return the sum | Algo.    |\\n| Max square root integer      | Find the largest integer but smaller than the square root | Algo.    |\\n| Longest word                 | Find the longest word in a word list | Algo.    |\\n| Sum unique elements          | Sum all the unique numbers in a list | Algo.    |\\n| Diagonal sum                 | Compute the diagonal sum of a matrix | D.S.     |\\n| Matrix condition number      | Check condition number of a matrix is less than a threshold | D.S.     |\\n| Matrix multiplication sum    | Compute matrix multiplication sum of two matrices | D.S.     |\\n| Matrix determinant           | Compare two matrix determinants | D.S.     |\\n| Log-sum-exp                  | Compute the log of sum exponential input | D.S.     |\\n| K nearest points             | Find the k nearest points to the origin array | Algo.    |\\n| Longest common prefix        | Find the longest common prefix of two strings | Algo.    |\\n| Duplicate elements           | Find duplicates in a list array | Algo.    |\\n| First unique character       | Find the first non-repeating character in a string | Algo.    |\\n| Uncommon words               | Find uncommon words in two sentences | Algo.    |\\n| Average words length         | Compute the average word length of a sentence | Algo.    |\\n| Compare char freq            | Compare the character frequencies in two strings | string   |\\n| Reverse string               | Reverse a string | string   |\\n| Square Sum diff              | Difference between the square of sum and the sum of squares | math    |\\n| Cosine sim                   | Compute the cosine similarity between two vectors | math    |\\n| Vector distance              | Compare vector distances to the origin | math    |\\n| Smallest standard dev.       | Find the smaller standard deviation given two lists | D.S.     |\\n| Smallest means               | Find the smaller mean given two lists | D.S.     |\\n| Coefficient of variation     | Compute coefficient of variation given a list | D.S.     |\\n| L1 norm                      | Compute the L1 norm given a list | D.S.     |\\n| Z-statistic                  | Compute z-statistic given a list | D.S.     |\\n| Move negatives               | Move all negative elements in a list to the end | array    |\\n| Remove alphabets             | Remove alphabetical characters in a string | string   |\\n| Largest norm                 | Find the largest norm among n-dimensional points | D.S.     |\\n| F1 score                     | Given two arrays (pred, gold), calculate the F1 score | D.S.     |\\n| Add Space                    | Add spaces before capital letters | string   |\\n| Remove outlier               | Remove data points in the tail (2sigma) of normal distribution | D.S.     |\\n| Convert to categorical       | Convert values into categorical variables | D.S.     |\\n| Group by key                 | Group items in an array using a provided function | array    |\\n| Max stock profit             | Given an array of \u201cprices\u201d, find the max profit | array    |\\n| Sum positions                | Sum of all position indices where a value appear | array    |\\n| Find missing num             | Find a missing number given a list and a max number | array    |\\n| Common num in matrix         | Common numbers among rows in a matrix | array    |\\n| Sum Collatz                  | Obtain the sum of Collatz sequence starting from given number | Algo.    |\\n| Cup swap                     | Name the location of a \u201cball\u201d after cup swapping | Algo.    |\\n| Reverse digits               | Reverse digits in a number with a stack | Algo.    |\\n| Calculate arrows             | Calculate arrowheads left and right | Algo.    |\\n| Check interval num           | Check if the interval (max-min) is included in a list | Algo.    |\\n| Length encoding              | Encode a string by converting repeated chars with counts | string   |\\n| Convert email                | Use regex to match email addresses and remove special chars | string   |\\n| Second largest               | Print out the second largest element in an array | array    |\\n| Largest prefix sum           | Return the largest prefix sum in an array | array    |\\n| Closest element to zero      | Find the element which is the closest to 0 and print the distance | array    |\\n| Consecutive unique char      | Find the max length contiguous subarray with unique characters | string   |\\n| Highest frequency char       | Obtain the frequency of the most frequent character | string   |\\n| Longest palindrome           | Find the length of longest palindrome substring | string   |\\n| Count primes                 | Calculate prime numbers in a range | Algo.    |\\n| Rotate array                 | Rotate an array to the right k steps | Algo.    |\\n| Partition equal sets         | Check if an array can be split into two sets with equal sums | Algo.    |\\n| Square root integer          | Compute the integer part of square root | math    |\\n| Plus 1                       | Return the digits after an integer is added by 1 | math    |\\n| Check square sum             | Check whether one integer is a sum of two square numbers | math    |\\n| Compare standard dev.        | Determine whether standard deviation is less than 1 | D.S.     |\\n| Matrix size                  | Calculate the sum of row and column numbers | D.S.     |\\n| Diff mean and median         | Calculate the difference between mean and median for an array | D.S.     |\"}"}
{"id": "iaYcJKpY2B_", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.\\n\\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1536\u20131547, 2020.\\n\\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999, 2022.\\n\\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\\n\\nSumit Gulwani. Automating string processing in spreadsheets using input-output examples. ACM Sigplan Notices, 46(1):317\u2013330, 2011.\\n\\nSumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. Program synthesis. Foundations and Trends\u00ae in Programming Languages, 4(1-2):1\u2013119, 2017.\\n\\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with APPS. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=sD93GOzH3i5.\\n\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In ICLR, 2020. URL https://openreview.net/forum?id=rygGQyrFvH.\\n\\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Mapping language to code in programmatic context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1643\u20131652, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1192. URL https://aclanthology.org/D18-1192.\\n\\nRajeev Joshi, Greg Nelson, and Keith Randall. Denali: A goal-directed superoptimizer. ACM SIGPLAN Notices, 37(5):304\u2013314, 2002.\\n\\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021.\\n\\nAditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating contextual embedding of source code. In International Conference on Machine Learning, pp. 5110\u20135121. PMLR, 2020.\\n\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster), 2015. URL http://arxiv.org/abs/1412.6980.\\n\\nSumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy S Liang. Spoc: Search-based pseudocode to code. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871\u20137880, 2020.\"}"}
{"id": "iaYcJKpY2B_", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "iaYcJKpY2B_", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, 2021.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\\n\\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter Lasecki, and Dragomir Radev. CoSQL: A conversational text-to-SQL challenge towards cross-domain natural language interfaces to databases. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1962\u20131979, Hong Kong, China, November 2019a. Association for Computational Linguistics. doi: 10.18653/v1/D19-1204. URL https://aclanthology.org/D19-1204.\\n\\nTao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, and Dragomir Radev. SParC: Cross-domain semantic parsing in context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4511\u20134523, Florence, Italy, July 2019b. Association for Computational Linguistics. doi: 10.18653/v1/P19-1443. URL https://aclanthology.org/P19-1443.\"}"}
{"id": "iaYcJKpY2B_", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To evaluate the emergence of multi-turn program synthesis capabilities under scaling laws, we adopt standard transformer-based autoregressive language models, varying (1) the number of model parameters (350M, 2.7B, 6.1B, 16.1B) and (2) the number of tokens of programming languages in the training corpora. For scaling the models, a custom library JAXFORMER for training large language models on TPU-v4 hardware was developed and will be released as open source, including the trained model weights.\\n\\n### A.1 Datasets\\n\\n| Dataset | Language | Raw Size | Final Size | Final Tokens |\\n|---------|----------|----------|------------|--------------|\\n| TPILE   | Natural Language | 825.18 GiB | 1159.04 GiB | 354.7B |\\n| Code    | C++      | 95.16 GiB  | 95.16 GiB  | 31.6B   |\\n|         | Go       | 256.4 GiB  | 21.4 GiB   | 9.6B    |\\n|         | Java     | 335.1 GiB  | 120.3 GiB  | 35.4B   |\\n|         | JavaScript | 1282.3 GiB | 24.7 GiB  | 9.7B    |\\n|         | Python   | 196.8 GiB  | 55.9 GiB   | 19.3B   |\\n|         | PYTHON   | 5558.1 GiB | 217.3 GiB  | 71.7B   |\\n\\nTable 5: Approximate statistics for training corpora along the pre-processing steps. For each dataset, the pre-processing shares the following steps: (1) filtering, (2) deduplication, (3) tokenization, (4) shuffling, and (5) concatenation. For details on TPILE, we refer to Gao et al. (2020). For BQUERY and BPYTHON, in (1) files are filtered by file extension, and files with average lines length of <100 characters, a maximum line length of 1,000, and >90% of the characters being decimal or hexadecimal digits are removed. For (2), exact duplicates based on their SHA-256 hash are removed, which amounts to a substantial portion of the raw data due to forks and copies of repositories. For (3), the BPE vocabulary of GPT-2 is extended by special tokens representing repeating tokens of tabs and white spaces. In the multi-lingual setting of BQUERY, a prefix is prepended to indicate the name of the programming language. For (4), each year of data is randomly shuffled. For (5), sequences are concatenated to fill the context length of 2,048 tokens with a special token as a separator. Table 5 summarizes the statistics of the training corpora.\\n\\nCODEGEN-NL models are randomly initialized and trained on TPILE. CODEGEN-MULTI models are initialized from CODEGEN-NL and then trained on BQUERY. CODEGEN-MONO models are initialized from CODEGEN-MULTI and then trained on BPYTHON.\\n\\n### A.2 Models\\n\\nOur models are autoregressive transformers with the regular next-token prediction language modeling as the learning objective. The family of CODEGEN models is trained in various sizes with 350M, 2.7B, 6.1B, and 16.1B parameters. The first three configurations allow for direct comparison with open-sourced large language models trained on text corpus, GPT-NEO (350M, 2.7B) (Black et al., 2021) and GPT-J (6B) (Wang & Komatsuzaki, 2021). See Table 6 in Appendix A for model specifications.\\n\\nThe architecture follows a standard transformer decoder with left-to-right causal masking. For the positional encoding, we adopt rotary position embedding (Su et al., 2021). For the forward pass, we execute the self-attention and feed-forward circuits in parallel for improved communication overhead following Wang & Komatsuzaki (2021), that is, $x_{t+1} = x_t + \\\\text{attn}(\\\\ln(x_t + \\\\text{attn}(\\\\ln(x_t)))) + \\\\text{mlp}(\\\\ln(x_t))$ is altered to $x_{t+1} = x_t + \\\\text{attn}(\\\\ln(x_t)) + \\\\text{mlp}(\\\\ln(x_t))$ for which the computation of self-attention, $\\\\text{attn}()$, and feed-forward, $\\\\text{mlp}()$, with layer-norm, $\\\\ln()$, is simultaneous. The architecture and hyper-parameter choices were optimized specifically for the hardware layout of TPU-v4.\"}"}
{"id": "iaYcJKpY2B_", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Suppose \\\\( \\\\{ p_i \\\\}_{i=1}^n \\\\) is the set of prompts for a given problem, and \\\\( \\\\{ s_i \\\\}_{i=1}^n \\\\) are the \\\\( n \\\\) sub-programs synthesized by a model \\\\( P_\\\\theta \\\\). Suppose \\\\( c_{i-1} = [p_1; s_1; \\\\ldots; p_{i-1}; s_{i-1}] \\\\) where \\\\([\\\\cdot; \\\\cdot]\\\\) indicates concatenation, the conditional probability of \\\\( p_i \\\\) is \\\\( \\\\text{Prob}_i = P_\\\\theta(p_i | c_{i-1}) \\\\), and then the perplexity for the multi-turn prompts is computed as\\n\\\\[\\nPPL_{\\\\text{Multi-turn}} = \\\\exp \\\\left( - \\\\frac{1}{m} \\\\sum_{i=1}^n \\\\log \\\\text{Prob}_i \\\\right), \\\\tag{2} \\\\]\\nwhere \\\\( m \\\\) is the total number of tokens of all prompts \\\\( \\\\{p_i\\\\}_{i=1}^n \\\\). Suppose \\\\( c = [p_1; s_1; \\\\ldots; p_n; s_n] \\\\), then its probability is \\\\( \\\\text{Prob} = P_\\\\theta(c) \\\\), and the perplexity for the single-turn prompts is computed as\\n\\\\[\\nPPL_{\\\\text{Single-turn}} = \\\\exp \\\\left( - \\\\frac{1}{m} \\\\log \\\\text{Prob} \\\\right). \\\\tag{3} \\\\]\\n\\n### Table 9: Average prompt perplexity of CODEGEN-NL models on pass and non-pass problems.\\n\\n| Model          | Pass \\\\( \\\\downarrow \\\\) | Non-Pass \\\\( \\\\downarrow \\\\) |\\n|----------------|------------------------|---------------------------|\\n| CODEGEN-NL 350M | 4.53                    | 4.96                      |\\n| CODEGEN-NL 2.7B | 3.25                    | 3.87                      |\\n| CODEGEN-NL 6.1B | 2.78                    | 3.65                      |\\n| CODEGEN-NL 16.1B | 2.78                    | 3.82                      |\\n\\n### Table 10: Average prompt perplexity of CODEGEN-MULTI models on pass and non-pass problems.\\n\\n| Model          | Pass \\\\( \\\\downarrow \\\\) | Non-Pass \\\\( \\\\downarrow \\\\) |\\n|----------------|------------------------|---------------------------|\\n| CODEGEN-MULTI 350M | 4.78                    | 5.34                      |\\n| CODEGEN-MULTI 2.7B | 3.82                    | 4.63                      |\\n| CODEGEN-MULTI 6.1B | 3.82                    | 4.82                      |\\n| CODEGEN-MULTI 16.1B | 3.82                    | 4.82                      |\\n\\n### Table 11: Pass rates on Mostly Basic Python Problems (MBPP).\\n\\n| Model          | Pass \\\\( \\\\downarrow \\\\) \\\\( \\\\% \\\\) | Pass \\\\( \\\\downarrow \\\\) \\\\( \\\\% \\\\) | Pass \\\\( \\\\downarrow \\\\) \\\\( \\\\% \\\\) |\\n|----------------|-----------------|-----------------|-----------------|\\n| CODEGEN-NL 350M | 0.96            | 6.37            | 19.91           |\\n| CODEGEN-NL 2.7B | 5.34            | 24.18           | 46.37           |\\n| CODEGEN-NL 6.1B | 8.15            | 31.21           | 55.27           |\\n| CODEGEN-NL 16.1B | 10.92           | 38.43           | 62.76           |\\n| CODEGEN-MULTI 350M | 7.46            | 24.18           | 46.37           |\\n| CODEGEN-MULTI 2.7B | 18.06           | 45.80           | 65.34           |\\n| CODEGEN-MULTI 6.1B | 18.35           | 47.27           | 67.92           |\\n| CODEGEN-MULTI 16.1B | 20.94           | 51.61           | 70.02           |\\n| CODEGEN-MONO 350M | 14.59           | 41.49           | 63.00           |\\n| CODEGEN-MONO 2.7B | 27.31           | 59.19           | 74.24           |\\n| CODEGEN-MONO 6.1B | 32.48           | 64.20           | 76.81           |\\n| CODEGEN-MONO 16.1B | 35.28           | 67.32           | 80.09           |\\n| I.N.CODER 6B    | 21.30           | 46.50           | 66.20           |\\n| code-cushman-001 | 45.90           | 66.90           | 79.90           |\\n| code-davinci-001 | 51.80           | 72.80           | 84.10           |\\n| code-davinci-002 | 58.10           | 76.70           | 84.50           |\\n\\nWe also evaluated our models on Mostly Basic Python Problems (MBPP) (Austin et al., 2021). The results are displayed in Table 11. Following Chen et al. (2022), we sampled programs from the\"}"}
{"id": "iaYcJKpY2B_", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"sanitized MBPP for all of our models, with $n = 100$ and temperature $0.8$. The last four rows are from the aforementioned paper. In general we observe the consistent trend of improving the performance over different versions (NL, Multi, Mono), with our largest CODEGEN-MONO16.1B approaching the results from code-cushman-001. While we do not know whether any of OpenAI models is the \u201cCodex 12B\u201d reported in Chen et al. (2021), we believe our model achieves reasonable results on MBPP as well. We also note that our CODEGEN-MONO6.1B significantly outperformed INCODER6B.\\n\\nFigure 3: Generated samples for \u201cReverse digits\u201d problem. While being simplistic, we often observe that CODEGEN-MONO16.1B assumes the type when assigning a number to a variable. Here on the right, despite being explicitly specified to initialize a number as a string, the larger model fails to do so. This causes an error in the following turn when the code attempts to iterate over num.\"}"}
{"id": "iaYcJKpY2B_", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"import numpy as np\\n\\n# Assign the matrix \\\\[\\n\\\\begin{bmatrix}\\n3 & 2 \\\\\\\\\\n2 & 3 \\\\\\\\\\n\\\\end{bmatrix}\\n\\\\]\\n# Assign the number \\\"1\\\" to a variable named \\\"t\\\".\\nmy_matrix = \\n\\nt = 1\\n\\n# Compute the condition number of my_matrix and store as result.\\nresult = np.linalg.cond(my_matrix)\\n\\n# Check whether the result is smaller than t. If yes, return \\\"True\\\", otherwise return \\\"False\\\".\\nif result < t:\\n    print(\\\"True\\\")\\nelse:\\n    print(\\\"False\\\")\\n\\nFigure 4: Generated samples for \\\"Matrix condition number\\\" problem. Both models generate programs in the same scope until the last turn, where C\\\\textsc{ODE}GEN-MONO 16.1B generates in the form of function definition. Noticeably, the larger model generates return keyword that appears in the prompt, but it is only valid within a function.\"}"}
{"id": "iaYcJKpY2B_", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"```python\\n# Import libraries.\\nimport numpy as np\\n\\n# Define a variable \\\"pos\\\" with \\\"B\\\", \\\"swap\\\" with ['AC', 'CA'].\\npos = ['B','swap']\\n\\n# Write a function \\\"move\\\" that takes two strings x and y as input, and replace any appearance of x in y with an empty string, then return y.\\ndef move(x, y):\\n    return y.replace(x, '')\\n\\n# For each element in \\\"swap\\\", if it contains \\\"pos\\\", call \\\"move\\\" on \\\"pos\\\" and the current element and store the result to \\\"pos\\\".\\ndef swap(swap):\\n    for i in range(len(swap)):\\n        if swap[i] == 'pos':\\n            swap[i] = move(pos[0], pos[1])\\n    return swap\\n\\n# Print out \\\"pos\\\".\\nprint(pos)\\n```\\n\\nFigure 5: Generated samples for \\\"Cup swap\\\" problem. Here, CODEGEN-MONO 2.7B mistakenly treats \\\"swap\\\" as a part of a value to be assigned, while CODEGEN-MONO 16.1B parses the intent correctly and assigns two variables in parallel.\"}"}
{"id": "iaYcJKpY2B_", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# Import libraries.\\nimport numpy as np\\n\\n# Initialize the variable named lst1 with a list ['abc', 'abc10'].\\nlst1 = ['abc', 'abc10']\\n\\n# Create a function called num_in_str() to check whether a string contains a number.\\ndef num_in_str(string):\\n    for i in string:\\n        if i.isnumeric():\\n            return True\\n        else:\\n            return False\\n\\n# Call the function num_in_str() to find strings in lst1 that have numbers and assign them to a list named lst2\\nlst2 = []\\nfor i in lst1:\\n    if num_in_str(i):\\n        lst2.append(i)\\n\\n# Print out lst2\\nprint(lst2)\"}"}
{"id": "iaYcJKpY2B_", "page_num": 24, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":null}"}
{"id": "iaYcJKpY2B_", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 10: Prompt Length versus Generated Program Length.\\n\\nToken count relationship between prompts and generated programs from CodeGen-16B-Mono:\\n\\n\\\\[ y = x \\\\]\"}"}
