{"id": "nkaba3ND7B5", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"AUBONOMOUS REINFORCEMENT LEARNING: FORMALISM AND BENCHMARKING\\n\\nArchit Sharma \u22171\\nKelvin Xu \u22172\\nNikhil Sardana 1\\nAbhishek Gupta 3\\nKarol Hausman 4\\nSergey Levine 2\\nChelsea Finn 1\\n\\n1 Stanford University\\n2 University of California, Berkeley\\n3 MIT\\n4 Google Brain\\n\\nABSTRACT\\n\\nReinforcement learning (RL) provides a naturalistic framing for learning through trial and error, which is appealing both because of its simplicity and effectiveness and because of its resemblance to how humans and animals acquire skills through experience. However, real-world embodied learning, such as that performed by humans and animals, is situated in a continual, non-episodic world, whereas common benchmark tasks in RL are episodic, with the environment resetting between trials to provide the agent with multiple attempts. This discrepancy presents a major challenge when attempting to take RL algorithms developed for episodic simulated environments and run them on real-world platforms, such as robots. In this paper, we aim to address this discrepancy by laying out a framework for Autonomous Reinforcement Learning (ARL): reinforcement learning where the agent not only learns through its own experience, but also contends with lack of human supervision to reset between trials. We introduce a simulated benchmark EARL around this framework, containing a set of diverse and challenging simulated tasks reflective of the hurdles introduced to learning when only a minimal reliance on extrinsic intervention can be assumed. We show that standard approaches to episodic RL and existing approaches struggle as interventions are minimized, underscoring the need for developing new algorithms for reinforcement learning with a greater focus on autonomy.\\n\\n1 INTRODUCTION\\n\\nOne of the appeals of reinforcement learning is that it provides a naturalistic account for how complex behavior could emerge autonomously from trial and error, similar to how humans and animals can acquire skills through experience in the real world. Real world learning however, is situated in a continual, non-episodic world, whereas commonly benchmarks in RL often assume access to an oracle reset mechanism that provides agents the ability to make multiple attempts. This presents a major challenge with attempting to deploy RL algorithms in environments where such an assumption would necessitate costly human intervention or manual engineering. Consider an example of a robot learning to clean and organize a home. We would ideally like the robot to be able to autonomously explore the house, understand cleaning implements, identify good strategies on its own throughout this process, and adapt when the home changes. This vision of robotic learning in real world, continual, non-episodic manner is in stark contrast to typical experimentation in reinforcement learning in the literature (Levine et al., 2016; Chebotar et al., 2017; Yahya et al., 2017; Ghadirzadeh et al., 2017; Zeng et al., 2020), where agents must be consistently reset to a set of initial conditions through human effort or engineering so that they may try again. Autonomy, especially for data hungry approaches such as reinforcement learning, is an enabler of broad applicability, but is rarely an explicit consideration. In this work, we propose to bridge this specific gap by providing a formalism and set of benchmark tasks that considers the challenges faced by agents situated in non-episodic environment, rather than treating them as being abstracted away by an oracle reset.\\n\\n\u2217 Equal contribution. Corresponding authors: architsh@stanford.edu, kelvinxu@berkeley.edu\\n1 Code and related information for EARL can be found at architsharma97.github.io/earl\\n\\n1\"}"}
{"id": "nkaba3ND7B5", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our specific aim is to place a greater focus on developing algorithms under assumptions that more closely resemble autonomous real world robotic deployment. While the episodic setting naturally captures the notion of completing a \u201ctask\u201d, it hides the costs of assuming oracle resets, which when removed can cause algorithms developed in the episodic setting to perform poorly (Sec. 6.1). Moreover, while prior work has examined settings such as RL without resets (Eysenbach et al., 2017; Xu et al., 2020; Zhu et al., 2020; Gupta et al., 2021), ecological RL (Co-Reyes et al., 2020), or RL amidst non-stationarity (Xie et al., 2020) in isolated scenarios, these settings are not well-represented in existing benchmarks. As a result, there is not a consistent formal framework for evaluating autonomy in reinforcement learning and there is limited work in this direction compared to the vast literature on reinforcement learning. By establishing this framework and benchmark, we aim to solidify the importance of algorithms that operate with greater autonomy in RL.\\n\\nBefore we can formulate a set of benchmarks for this type of autonomous reinforcement learning, we first formally define the problem we are solving. As we will discuss in Section 4, we can formulate two distinct problem settings where autonomous learning presents a major challenge. The first is a setting where the agent first trains in a non-episodic environment, and is then \u201cdeployed\u201d into an episodic test environment. In this setting, which is most commonly studied in prior works on \u201creset-free\u201d learning (Han et al., 2015; Zhu et al., 2020; Sharma et al., 2021), the goal is to learn the best possible episodic policy after a period of non-episodic training. For instance, in the case of a home cleaning robot, this would correspond to evaluating its ability to clean a messy home. The second setting is a continued learning setting: like the first setting, the goal is to learn in a non-episodic environment, but there is no distinct \u201cdeployment\u201d phase, and instead the agent must minimize regret over the duration of the training process. In the previous setting of the home cleaning robot, this would evaluate the persistent cleanliness of the home. We discuss in our autonomous RL problem definition how these settings present a number of unique challenges, such as challenging exploration.\\n\\nThe main contributions of our work consist of a benchmark for autonomous RL (ARL), as well as formal definitions of two distinct ARL settings. Our benchmarks combine components from previously proposed environments (Coumans & Bai, 2016; Gupta et al., 2019; Yu et al., 2020; Gupta et al., 2021; Sharma et al., 2021), but reformulate the learning tasks to reflect ARL constraints, such as the absence of explicitly available resets. Our formalization of ARL relates it to the standard RL problem statement, provides a concrete and general definition, and provides a number of instantiations that describe how common ingredients of ARL, such as irreversible states, interventions, and other components, fit into the general framework. We additionally evaluate a range of previously proposed algorithms on our benchmark, focusing on methods that explicitly tackle reset-free learning and other related scenarios. We find that both standard RL methods and methods designed for reset-free learning struggle to solve the problems in the benchmark and often get stuck in parts of the state space, underscoring the need for algorithms that can learn with greater autonomy and suggesting a path towards the development of such methods.\\n\\nPrior work has proposed a number of benchmarks for reinforcement learning, which are often either explicitly episodic (Todorov et al., 2012; Beattie et al., 2016; Chevalier-Boisvert et al., 2018), or consist of games that are implicitly episodic after the player dies or completes the game (Bellemare et al., 2013; Silver et al., 2016). In addition, RL benchmarks have been proposed in the episodic setting for studying a number of orthogonal questions, such multi-task learning (Bellemare et al., 2013; Yu et al., 2020), sequential task learning (Wo\u0142czyk et al., 2021), generalization (Cobbe et al., 2020), and multi-agent learning (Samvelyan et al., 2019; Wang et al., 2020). These benchmarks differ from our own in that we propose to study the challenge of autonomy. Among recent benchmarks, the closest to our own is Jelly Bean World (Platanios et al., 2020), which consists of a set of procedural generated gridworld tasks. While this benchmark also considers the non-episodic setting, our work is inspired by the challenge of autonomous learning in robotics, and hence considers an array of manipulation and locomotion tasks. In addition, our work aims to establish a conceptual framework for evaluating prior algorithms in light of the requirement for persistent autonomy.\\n\\nEnabling embodied agents to learn continually with minimal interventions is a motivation shared by several subtopics of reinforcement learning research. The setting we study in our work shares conceptual similarities with prior work in continual and lifelong learning (Schmidhuber, 1987; Thrun & Mitchell, 1995; Parisi et al., 2019; Hadsell et al., 2020). In context of reinforcement learning, this work has studied the problem of episodic learning in sequential MDPs (Khetarpal et al., 2020; Rusu et al., 2020).\"}"}
{"id": "nkaba3ND7B5", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a surrogate reward function $\\\\tilde{r}_t: S \\\\times A \\\\rightarrow \\\\mathbb{R}$. At every time step $t$, $A$ outputs $a_t \\\\sim \\\\pi$ and $\\\\pi_t$ for evaluation, where $\\\\pi_t$ optimizes $\\\\tilde{r}_t$ over the transitions seen till time $t - 1$. Prior works on reset-free reinforcement learning can be encapsulated within this framework as different choices for surrogate reward function $\\\\tilde{r}_t$. Some pertinent examples: Assuming $r_\\\\rho$ is some reward function designed that shifts the agent's state distribution towards initial state distribution $\\\\rho$, alternating $\\\\tilde{r}_t = r$ and $\\\\tilde{r}_t = r_\\\\rho$ for a fixed number of environment steps recovers the forward-backward reinforcement learning algorithms proposed by Han et al. (2015); Eysenbach et al. (2017). Similarly, R3L (Zhu et al., 2020) can be understood as alternating between a perturbation controller optimizing a state novelty reward and the forward controller optimizing the task reward $r$. Recent work on using multi-task learning for reset-free reinforcement learning (Gupta et al., 2021) can be understood as choosing $\\\\tilde{r}_t(s_t, a_t) = \\\\sum_{k=1}^{K} r_k(s_t, a_t) I[s_t \\\\in S_k]$ such that $S_1, ..., S_K$ is a partition of the state space $S$ and only reward function $r_k$ is active in the subset $S_k$. Assuming the goal-conditioned autonomous reinforcement learning framework, the recently proposed algorithm VaPRL (Sharma et al., 2021) can be understood as creating a curriculum of goals $g_t$ such that at every step, the action $a_t \\\\sim \\\\pi(\\\\cdot | s_t, g_t)$. The curriculum simplifies the task for the agent, bootstrapping on the success of easier tasks to efficiently improve $J_D(\\\\pi)$.\\n\\n### Environment Descriptions and Reward Functions\\n\\n**Tabletop-Organization.** The Tabletop-Organization task is a diagnostics object manipulation task proposed by Sharma et al. (2021). The observation space is a 12 dimensional vector consisting of the object position, gripper position, gripper state, and the current goal. The action space is 3-D action that consists of a 2-D position delta and an automated gripper that can attach to the mug if the gripper is close enough to the object. The reward function is a sparse indicator function, $r(s, g) = 1(\\\\|s - g\\\\|_2 \\\\leq 0.2)$. The agent is provided with 12 forward and 12 backward demonstrations, 3 for each of the 4 goal positions.\\n\\n**Sawyer-Door.** The Sawyer Door environment consists of a Sawyer robot with a 12 dimension observation space consisting of 3-D end effector position, 3-D door position, gripper state and desired goal. The action space is a 4-D action space that consisting of a 3-D end effector control and normalized gripper torque. Let $s_d$ be dimensions of the observation corresponding to the door state, and $g$ be the corresponding goal. The reward function is a sparse indicator function $r(s, g) = 1(\\\\|s_d - g\\\\|_2 \\\\leq 0.08)$. The agent is provided with a 5 forward and 5 backward demos.\\n\\n**Sawyer-Peg.** The Sawyer-Peg environment shares observation and action space as the Sawyer-Door environment. Let $s_p$ be the state of the peg and $g$ be the corresponding goal. The reward function is a sparse indicator function $r(s, g) = 1(\\\\|s - g\\\\|_2 \\\\leq 0.05)$. The agent is provided with 10 forward and 10 backward demonstrations for this task.\\n\\n**Franka-Kitchen.** The Franka-Kitchen environment consists of a 9-DoF Franka robot with an array of objects (microwave, two distinct burner, door) represented by a 14 dimensional vector. The reward function is composed of a dense reward that is a sum of the euclidean distance between the goal position of the arm and the current state plus shaped reward per object as described in Gupta et al. (2019). No demonstrations are provided for this task.\\n\\n**DHand-LightBulb.** The DHand is a 4 fingered robot (16-DoF) mounted on a 6-DoF Sawyer Arm. The observation space of the DHand consists of a 30 dimensional observation and corresponding goal state. The observation is composed of a 16 dimensional hand position, 7 dimensional arm position, 3 dimension object position, 3 dimensional euler angle and a 6 dimensional vector representing the dimensional wise distance to between objects in the environment. The action space is a position delta over the combined 22 DoF of the robot. No demonstrations are provided for these tasks.\\n\\n**Minitaur-Pen.** The Minitaur pen's observation space is the joint positions of its 8 links, their corresponding velocity, current torque, quaternion of its base position, and goal location in the pen. The action space is a position delta over the combined 8 dimensions of the robot. Let $s_b$ be the 2-D position of the agent, and $g$ be the corresponding goal. Let $s_t$ be the current torques on the agent, and $s_v$ be their velocities. The reward for the agent is a dense reward $r(s, a) = -2.0 \\\\cdot \\\\|s_b - g\\\\| + 0.02 \\\\cdot \\\\|s_v \\\\cdot s_t\\\\|$. No demonstrations are provided for these tasks.\\n\\nThis can also be captured in a multi-task reinforcement learning framework, where $\\\\pi_e, \\\\pi_t$ are the same policy with different task variables as input.\"}"}
{"id": "nkaba3ND7B5", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use soft actor-critic (Haarnoja et al., 2018b) as the base algorithm for our experiments in this paper. When available, the demonstrations are added to the replay buffer at the beginning of training. Further details on the parameters of the environments and algorithms are reported in Tables A.5. The implementations will be open-sourced and implementation details can be found there.\\n\\n| Hyperparameter  | Value                      |\\n|-----------------|----------------------------|\\n| Actor-critic architecture | fully connected(256, 256) |\\n| Nonlinearity    | ReLU                       |\\n| RND architecture| fully connected(256, 256, 512) |\\n| RND Gamma       | 0.99                       |\\n| Optimizer       | Adam                       |\\n| Learning rate   | 3e-4                       |\\n| $\\\\gamma$        | 0.99                       |\\n| Target update   | $\\\\tau$ 0.005               |\\n| Target update period | 1                        |\\n| Batch size      | 256                        |\\n| Classifier batch size | 128                    |\\n| Initial collect steps | 10                       |\\n| Collect steps per iteration | 3                        |\\n| Reward scale    | 1                          |\\n| Min log std     | -20                        |\\n| Max log std     | 2                          |\\n\\nTable 3: Shared algorithm parameters.\\n\\n| Environment           | Training Horizon ($H_T$) | Evaluation Horizon ($H_E$) | Replay Buffer Capacity     |\\n|-----------------------|--------------------------|---------------------------|----------------------------|\\n| Tabletop-Organization | 200,000                  | 200                       | 20,000,000                 |\\n| Sawyer-Door           | 200,000                  | 300                       | 20,000,000                 |\\n| Sawyer-Peg            | 100,000                  | 200                       | 20,000,000                 |\\n| Franka-Kitchen        | 100,000                  | 400                       | 10,000,000                 |\\n| DHand-Lightbulb       | 400,000                  | 400                       | 10,000,000                 |\\n| Minitaur-Pen          | 100,000                  | 1000                      | 10,000,000                 |\\n\\nTable 4: Environment specific parameters, including the training horizon (i.e. how frequently an intervention is provided), the evaluation horizon, and the replay buffer capacity.\\n\\nIn this section, we plot deployed policy evaluation and continuing policy evaluation curves for different algorithms and different environments:\"}"}
{"id": "nkaba3ND7B5", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 5: Deployed Policy Evaluation and Continuing Policy Evaluation per environment. Results and averaged over 5 seeds. Shaded regions denote 95% confidence bounds.\"}"}
{"id": "nkaba3ND7B5", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "nkaba3ND7B5", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "nkaba3ND7B5", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "nkaba3ND7B5", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This framework can be readily extended to goal-conditioned reinforcement learning (Kaelbling, 1993; Schaul et al., 2015), which is also an area of study covered in some prior works on reset-free reinforcement learning (Sharma et al., 2021). Assuming a goal space $G$ and task-distribution $p(g): G \\\\rightarrow R_{\\\\geq 0}$, assume that the algorithm $A:\\\\{(s_i, a_i, s_{i+1})\\\\}_{t=0}^{\\\\infty} \\\\rightarrow (a_t, \\\\pi_t)$, where $a_t \\\\in A$ and $\\\\pi_t: S \\\\times A \\\\times G \\\\rightarrow R_{\\\\geq 0}$. Equation 1 can be redefined as follows:\\n\\n$$J_D(\\\\pi) = \\\\mathbb{E}_{g \\\\sim p(g), s_0 \\\\sim \\\\rho, a_t \\\\sim \\\\pi(\\\\cdot | s_t, g), s_{t+1} \\\\sim p(\\\\cdot | s_t, a_t)} \\\\left[ \\\\sum_{t=0}^{\\\\infty} \\\\gamma^t r(s_t, a_t, g) \\\\right]$$\\n\\nAdditionally, we will assume that the algorithm has access to a set of samples $g_i \\\\sim p(g)$ from the goal distribution. The definitions for deployed policy evaluation and continuing policy evaluation remains the same.\\n\\nWe expand on the discussion of reversibility in 4.2. We also discuss how we can deal with non-ergodic MDPs by augmenting the action space in MDP $M_T$ with calls for extrinsic interventions.\\n\\n**Definition 3 (Ergodic MDPs).** A MDP is considered ergodic if for all states $s, a, s' \\\\in S$, there exists a policy $\\\\pi$ such that $\\\\pi(s') > 0$, where $\\\\pi(s) = (1 - \\\\gamma) \\\\sum_{t=0}^{\\\\infty} \\\\gamma^t p(s_t = s | s_0 = a, \\\\pi)$ denotes discounted state distribution induced by the policy $\\\\pi$ starting from the state $s_0 = a$ (Moldovan & Abbeel, 2012). A policy that assigns a non-zero probability to all actions on its support ensures that all states in $S$ are visited in the limit for ergodic MDPs, satisfying the condition above.\\n\\nWe adapt the ARL framework to develop learning algorithms for non-ergodic MDPs. In particular, we introduce a mechanism below for the agent to call for an intervention in the MDP $M_T$. Our goal here is to show that our described ARL framework is general enough to include cases where a human is asked for help in irreversible states. Consider an augmented state space $S^+ = S \\\\cup R_{\\\\geq 0}$ and an augmented action space $A^+ = A \\\\cup A_h$, where $A_h$ denotes the action of asking for help via human intervention. A state $s^+ \\\\in S$ can be written as a state $s \\\\in S$, and $h \\\\in R_{\\\\geq 0}$ which denotes the remaining budget for interventions into the training. The budget $h$ is initialized to $h_{\\\\text{max}}$ when the training begins. The intervention can be requested by the agent itself using an action $a \\\\in A_h$ or it can enforced by the environment (for example, if the agent reaches certain detectable irreversible states in the environment and requests a human to bring it back into the reversible set of states). For an action $a \\\\in A_h$, the environment transitions from $(s, h) \\\\rightarrow (s', h - c(s, a))$, where the next state $s'$ depends on the nature of the requested intervention $a$ and $c(s, a): S \\\\times A_h \\\\rightarrow R_{\\\\geq 0}$ denotes the cost of intervention. A similar transition occurs when the environment enforces the human intervention. When the cost exceeds the remaining budget, that is $h - c(s, a) \\\\leq 0$, the environment transitions into an absorbing state $s_\\\\emptyset$, and the training is terminated. We retain the definitions introduced in Sec 4.1.1 and 4.1.2 for evaluation protocols.\\n\\nIn this way, we see that we can actually encompass the setting of non-ergodic MDPs as well, with some reparameterization of the MDP.\\n\\nIn this section, we connect the settings in prior works to our proposed autonomous reinforcement learning formalism. First, consider the typical reinforcement learning approach: Algorithm $A$ assigns the rewards to transitions using $r(s, a)$, learns a policy $\\\\pi$ and outputs $\\\\pi_t = \\\\pi$ and $a_t \\\\sim \\\\pi$ (possibly adding noise to the action). The algorithm exclusively optimizes the reward function $r$ throughout training.\\n\\nReconsider the door closing example: the agent needs to practice closing the door repeatedly, which requires opening the door repeatedly. However, if we optimize $\\\\pi$ to maximize $r$ over its lifetime, it will never be incentivized to open the door to practice closing it again. In theory, assuming an ergodic MDP and that the exploratory actions have support over all actions, the agent will open the door given enough time, and the agent will practice closing it again. However, in practice, this can be quite an inefficient strategy to rely on and thus, prior reset-free reinforcement learning algorithms consider other strategies for exploration. To understand current work, we introduce the notion of...\"}"}
{"id": "nkaba3ND7B5", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The DHand-Lightbulb environment consists of a 22-DoF 4 fingered hand, mounted on a 6 DoF Sawyer robot. The environment is based on one originally proposed by Gupta et al. (2021). The task in this domain is for the robot to grasp and pick up a lightbulb to a specific location. The high-dimensional action space makes the task extremely challenging. In the deployment setting, the bulb can be initialized anywhere on the table, testing the agent on a wide initial state distribution.\\n\\nThe Minitaur-Pen task consists of an 8-DoF Minitaur robot (Coumans & Bai, 2016) confined to a pen environment. The goal of the agent is to navigate to a set of goal locations in the pen. The task is designed to mimic the setup of leaving a robot to learn to navigate within an enclosed setting in an autonomous fashion. This task is different from the other tasks given it is a locomotion task, as opposed to the other tasks being manipulation tasks.\\n\\n### 6 Benchmarking and Analysis\\n\\nThe aim of this section is to understand the challenges in autonomous reinforcement learning and to evaluate the performance and shortcomings of current autonomous RL algorithms. In Section 6.1, we first evaluate standard episodic RL algorithms in ARL settings as they are required to operate with increased autonomy, underscoring the need for a greater focus on autonomy in RL algorithms. We then evaluate prior autonomous learning algorithms on EARL in Section 6.2. While these algorithms do improve upon episodic RL methods, they fail to make progress on more challenging tasks compared to methods provided with oracle resets leaving a large gap for improvement. Lastly, in Section 6.3, we investigate the learning of existing algorithms, providing a hypothesis for their inadequate performance. We also find that when autonomous RL does succeed, it tends to find more robust policies, suggesting an intriguing connection between autonomy and robustness.\\n\\n#### 6.1 Going from Standard RL to Autonomous RL\\n\\n![Figure 2: Performance of standard RL at with varying levels of autonomy, ranging from resets provided every 1000 to 200000 steps. Performance degrades substantially as environment resets become infrequent.](image)\\n\\nIn this section, we evaluate standard RL methods to understand how their performance changes when they are applied naively to ARL problems. To create a continuum, we will vary the level of \u201cautonomy\u201d (i.e., frequency of resets), corresponding to $\\\\epsilon$ as defined in Section 4.2. For these experiments only, we use the simple cheetah and fish environments from the DeepMind Control Suite (Tassa et al., 2018). We use soft actor-critic (SAC) (Haarnoja et al., 2018a) as a representative standard RL algorithm. We consider different training environments with increasing number of steps between resets, ranging from 1000 to 200,000 steps. Figure 2 shows the performance of the learned policy as the training progresses, where the return is measured by running the policy for 1000 steps. The cheetah environment is an infinite-horizon running environment, so changing the training horizon should not affect the performance theoretically. However, we find that the performance degrades drastically as the training horizon is increased, as shown in Fig 2 (left). We attribute this problem to a combination of function approximation and temporal difference learning. Increasing the episode length destabilizes the learning as the effective bootstrapping length increases: the $Q$-value function $Q_\\\\pi(s_0, a_0)$ bootstraps on the value of $Q_\\\\pi(s_1, a_1)$, which bootstraps on $Q_\\\\pi(s_2, a_2)$ and so on till $Q_\\\\pi(s_{100,000}, a_{100,000})$. To break this chain, we consider a biased TD update:\\n\\n$$Q_\\\\pi(s_t, a_t) \\\\leftarrow r(s_t, a_t) + \\\\gamma Q_\\\\pi(s_{t+1}, a_{t+1}) \\\\text{ if } t \\\\text{ is not a multiple of 1000, else}$$\"}"}
{"id": "nkaba3ND7B5", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Average return of the final deployed policy. Performance is averaged over 5 random seeds. The mean and the standard error are reported, with the best performing entry in bold. For sparse reward domains (TO, SD, SP), 1.0 indicates the maximum performance and 0.0 indicates minimum performance.\\n\\n| Method     | TO   | SD   | SP   | FK   | DL   | MP   |\\n|------------|------|------|------|------|------|------|\\n| na\u00efve RL   | 0.32 | 0.00 | 0.00 | -2705.21 | 167.10 | 239.30 | 8.85 |\\n| FBRL       | 0.94 | 1.00 | 0.00 | -2733.15 | 324.10 | 242.38 | 8.84 |\\n| R3L        | 0.96 | 0.54 | 0.00 | -2639.28 | 233.28 | 728.54 | 122.86 |\\n| VaPRL      | 0.98 | 0.94 | 0.02 | -         | -      | -      | -    |\\n| oracle RL  | 0.80 | 1.00 | 1.00 | 1203.88 | 203.86 | 2028.75 | 35.95 |\\n\\nTable 2: Average reward accumulated over the training lifetime in accordance to continuing policy evaluation. Performance is averaged over 5 random seeds. The mean and the standard error (brackets) are reported, with the best performing entry in bold.\\n\\n| Method     | TO   | SD   | SP   | FK   | DL   | MP   |\\n|------------|------|------|------|------|------|------|\\n| na\u00efve RL   | 0.012 | 0.373 | <0.001 | -4.944 | 0.440 | -0.734 | 0.024 |\\n| FBRL       | 0.005 | 0.329 | 0.003 | -8.754 | 0.405 | -0.747 | 0.014 |\\n| R3L        | 0.001 | 0.369 | <0.001 | -6.577 | 0.309 | 0.091 | 0.026 |\\n| VaPRL      | 0.009 | 0.574 | <0.001 | -         | -      | -      | -    |\\n\\nQ_\u03c0(s_t, a_t) \u2190 r(s_t, a_t). This is inspired by practical implementations of SAC (Haarnoja et al., 2018b), where the Q-value function regresses to r(s, a) for terminal transitions to stabilize the training. This effectively fixes the problem for cheetah, as shown in Figure 2 (middle). However, this solution does not translate in general, as can be seen observed in the fish environment, where the performance continues to degrade with increasing training horizon, shown in Fig 2 (right). The primary difference between cheetah and fish is that the latter is a goal-reaching domain. The cheetah can continue improving its gait on the infinite plane without resets, whereas the fish needs to undo the task to practice goal reaching again, creating a non-trivial exploration problem.\"}"}
{"id": "nkaba3ND7B5", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Amongst the autonomous RL algorithms, VaPRL does best when the demonstrations are given and R3L does well on domains when no demonstrations are provided. Nonetheless, this leaves substantial room for improvement for future works evaluating on this benchmark. More detailed learning curves are shown in Section A.6. In the continuing setting, we find that na\u00efve RL performs well on certain domains (best on 2 out of 6 domains). This is unsurprising, as na\u00efve RL is incentivized to occupy the final \u201cgoal\u201d position, and continue to accumulate over the course of its lifetime, whereas other algorithms are explicitly incentivized to explore. Perhaps surprisingly, we find that VaPRL on sawyer-door and R3L on dhand-lightbulb and minitaur does better than na\u00efve RL, suggesting that optimizing for deployed performance can also improve the continuing performance.\\n\\nOverall, we find that performance in the continuing setting does not necessarily translate to improved performance in the deployed policy evaluation, emphasizing the differences between these two evaluation schemes.\\n\\n6.3 Analyzing Autonomous RL Algorithms\\n\\nFigure 3: Comparing the distribution of states visited with resets (blue) and without resets (brown). Heatmaps visualize the difference between state visitations for oracle RL and FBRL, thresholded to highlight states with large differences. Resets enable the agent to stay around the initial state distribution and the goal distribution, whereas the agents operating autonomously skew farther away, posing an exploration challenge.\\n\\nA hypothesis to account for the relative underperformance of autonomous RL algorithms compared to oracle RL is that environment resets constrain the state distribution visited by the agent close to the initial and the goal states. When operating autonomously for long periods of time, the agent can skew far away from the goal states, creating a hard exploration challenge. To test this hypothesis, we compare the state distribution when using oracle RL (in blue) versus FBRL (in brown) in Figure 3.\\n\\nWe visualize the $(x,y)$ positions visited by the gripper for tabletop-organization, the $(x,y)$ positions of the peg for sawyer-peg and the $x,y$ positions of the minitaur for minitaur-pen. As seen in the figure, autonomous operation skews the gripper towards the corners in tabletop-organization, the peg is stuck around the goal box and minitaur can completely go away from the goal distribution. However, when autonomous algorithms are able to solve the task, the learned policies can be more robust as they are faced with a tougher exploration challenge during training. We visualize this in Figure 4, where we test the final policies learned by oracle RL, FBRL and VaPRL on tabletop-organization starting from a uniform state distribution instead of the default ones. We observe that the policies learned by VaPRL and FBRL depreciate by 2% and 14.3% respectively, which is much smaller than the 37.4% depreciation of the policy learned by oracle RL, suggesting that autonomous RL can lead to more robust policies.\\n\\n7 Conclusion\\n\\nWe proposed a formalism and benchmark for autonomous reinforcement learning, including an evaluation of prior state-of-the-art algorithms with explicit emphasis on autonomy. We present two distinct evaluation settings, which represent different practical use cases for autonomous learning. The main conclusion from our experiments is that existing algorithms generally do not perform well in scenarios that demand autonomy during learning. We also find that exploration challenges, while present in the episodic setting, are greatly exacerbated in the autonomous setting.\"}"}
{"id": "nkaba3ND7B5", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"While our work focuses on predominantly autonomous settings, there may be task-specific trade-offs between learning speed and the cost of human interventions, and it may indeed be beneficial to provide some human supervision to curtail total training time. How to best provide this supervision (rewards and goal setting, demonstrations, resets etc) while minimizing human cost provides a number of interesting directions for future work. However, we believe that there is a lot of room to improve the autonomous learning algorithms and our work attempts to highlight the importance and challenge of doing so.\\n\\nREFERENCES\\n\\nCharles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K\\\"uttler, Andrew Lefrancq, Simon Green, Victor Vald\\'es, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig. Petersen. Deepmind lab. arXiv:1612.03801 [cs.AI], 2016.\\n\\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, 2013.\\n\\nV\u00b4\u0131ctor Campos, Alexander Trott, Caiming Xiong, Richard Socher, Xavier Gir \u00b4o-i Nieto, and Jordi Torres. Explore, discover and learn: Unsupervised discovery of state-covering skills. In International Conference on Machine Learning, pp. 1317\u20131327. PMLR, 2020.\\n\\nYash Chandak, Georgios Theocharous, Shiv Shankar, Martha White, Sridhar Mahadevan, and Philip Thomas. Optimizing for the future in non-stationary mdps. In International Conference on Machine Learning, pp. 1414\u20131425. PMLR, 2020.\\n\\nYevgen Chebotar, Karol Hausman, Marvin Zhang, Gaurav Sukhatme, Stefan Schaal, and Sergey Levine. Combining model-based and model-free updates for trajectory-centric reinforcement learning. In International conference on machine learning, pp. 703\u2013711. PMLR, 2017.\\n\\nMaxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for openai gym. https://github.com/maximecb/gym-minigrid, 2018.\\n\\nJohn D Co-Reyes, Suvansh Sanjeev, Glen Berseth, Abhishek Gupta, and Sergey Levine. Ecological reinforcement learning. arXiv preprint arXiv:2006.12478, 2020.\\n\\nKarl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. In International conference on machine learning, pp. 2048\u20132056. PMLR, 2020.\\n\\nErwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016.\\n\\nBenjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine. Leave no trace: Learning to reset for safe and autonomous reinforcement learning. arXiv preprint arXiv:1711.06782, 2017.\\n\\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.\\n\\nChrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.\\n\\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning. PMLR, 2017.\\n\\nAli Ghadirzadeh, Atsuto Maki, Danica Kragic, and M\u02daarten Bj\u00a8orkman. Deep predictive policy training using reinforcement learning. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 2351\u20132358. IEEE, 2017.\\n\\nKarol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv preprint arXiv:1611.07507, 2016.\"}"}
{"id": "nkaba3ND7B5", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Two evaluation schemes in autonomous RL. First, the deployment setting (top row, (1)), where we are interested in obtaining a policy during a training phase, $\\\\pi$, that performs well when deployed from a state $s_0 \\\\sim \\\\rho$.\\n\\nSecond, the continuing setting (bottom row, (2)), where a floor cleaning robot is tasked with keeping a floor clean and is only evaluated on its cumulative performance (Eq. 2) over the agent's lifetime.\\n\\nAs we will discuss, learning autonomously without access to oracle resets is a hard problem even when the task-distribution and dynamics are stationary. In a similar vein, unsupervised RL (Gregor et al., 2016; Pong et al., 2019; Eysenbach et al., 2018; Sharma et al., 2019; Campos et al., 2020) also enables skill acquisition in the absence of rewards, reducing human intervention required for designing reward functions. These works are complimentary to our proposal and form interesting future work.\\n\\nReset-free RL has been studied by previous works with a focus on safety (Eysenbach et al., 2017), automated and unattended learning in the real world (Han et al., 2015; Zhu et al., 2020; Gupta et al., 2021), skill discovery (Xu et al., 2020; Lu et al., 2020), and providing a curriculum (Sharma et al., 2021). Strategies to learn reset-free behavior include directly learning a backward reset controller (Eysenbach et al., 2017), learning a set of auxiliary tasks that can serve as an approximate reset (Ha et al., 2020; Gupta et al., 2021), or using a novelty seeking reset controller (Zhu et al., 2020). Complementary to this literature, we aim to develop a set of benchmarks and a framework that allows for this class of algorithms to be studied in a unified way. Instead of proposing new algorithms, our work is focused on developing a set of unified tasks that emphasize and allow us to study algorithms through the lens of autonomy.\\n\\n### 3 Preliminaries\\n\\nConsider a Markov Decision Process (MDP) $M \\\\equiv (S, A, p, r, \\\\rho, \\\\gamma)$ (Sutton & Barto, 2018). Here, $S$ denotes the state space, $A$ denotes the action space, $p: S \\\\times A \\\\times S \\\\rightarrow \\\\mathbb{R}_{\\\\geq 0}$ denotes the transition dynamics, $r: S \\\\times A \\\\rightarrow \\\\mathbb{R}$ denotes the reward function, $\\\\rho: S \\\\rightarrow \\\\mathbb{R}_{\\\\geq 0}$ denotes the initial state distribution and $\\\\gamma \\\\in [0, 1)$ denotes the discount factor. The objective in reinforcement learning is to maximize $J(\\\\pi) = \\\\mathbb{E}\\\\left[\\\\sum_{t=0}^{\\\\infty} \\\\gamma^t r(s_t, a_t)\\\\right]$ with respect to the policy $\\\\pi$, where $s_0 \\\\sim \\\\rho(\\\\cdot)$, $a_t \\\\sim \\\\pi(\\\\cdot | s_t)$ and $s_{t+1} \\\\sim p(\\\\cdot | s_t, a_t)$. Importantly, the RL framework assumes the ability to sample $s_0 \\\\sim \\\\rho$ arbitrarily. Typical implementations of reinforcement learning algorithms carry out thousands or millions of these trials, implicitly requiring the environment to provide a mechanism to be \u201creset\u201d to a state $s_0 \\\\sim \\\\rho$ for every trial.\\n\\n### 4 Autonomous Reinforcement Learning\\n\\nIn this section, we develop a framework for autonomous reinforcement learning (ARL) that formalizes reinforcement learning in settings without extrinsic interventions. We first define a non-episodic training environment where the agent can autonomously interact with its environment in Section 4.1, building on the formalism of standard reinforcement learning. We introduce two distinct evaluation settings as visualized in Figure 1: Section 4.1.1 discusses the deployment setting where the agent will be deployed in a test environment after training, and the goal is to maximize this \u201cdeployed\u201d performance. Section 4.1.2 discusses the continuing setting, where the agent has no separate deployment phase and aims to maximize the reward accumulated over its lifetime. In its most general form, the latter corresponds closely to standard RL, while the former can be interpreted as a kind of transfer learning. As we will discuss in Section 4.2, this general framework can be instantiated such...\"}"}
{"id": "nkaba3ND7B5", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"that, for different choices of the underlying MDP, we can model different realistic autonomous RL scenarios such as settings where a robot must learn to reset itself between trials or settings with non-reversible dynamics. Finally, Section 4.3 considers algorithm design for autonomous RL, discussing the challenges in autonomous operation while also contrasting the evaluation protocols.\\n\\n4.1 General Setup\\n\\nOur goal is to formalize a problem setting for autonomous reinforcement learning that encapsulates realistic autonomous learning scenarios. We define the setup in terms of a training MDP \\\\( M_T \\\\equiv (S, A, p, r, \\\\rho) \\\\), where the environment initializes to \\\\( s_0 \\\\sim \\\\rho \\\\), and then the agent interacts with the environment autonomously from then on. Note, this lack of episodic resets in our setup departs not only from the standard RL setting, but from other continual reinforcement learning settings e.g. Wo\u0142czyk et al. (2021), where resets are provided between tasks. Symbols retain their meaning from Section 3. In this setting, a learning algorithm \\\\( A \\\\) can be defined as a function \\\\( A: \\\\{s_i, a_i, s_{i+1}, r_i\\\\}_{t-1}^t \\\\rightarrow (a_t, \\\\pi_t) \\\\), which maps the transitions collected in the environment until the time \\\\( t \\\\) (e.g., a replay buffer), to a (potentially exploratory) action \\\\( a_t \\\\in A \\\\) applied in the environment and its best guess at the optimal policy \\\\( \\\\pi_t: S \\\\times A \\\\rightarrow R \\\\geq 0 \\\\) used for evaluation at time \\\\( t \\\\). We note that the assumption of a reward function implicitly requires human engineering, but in principle could be relaxed by methods that learn reward functions from data. In addition, we note that \\\\( a_t \\\\) does not need to come from \\\\( \\\\pi_t \\\\), which is already implicit in most reinforcement learning algorithms: \\\\( Q \\\\)-learning (Sutton & Barto, 2018) methods such as DQN (Mnih et al., 2015), DDPG (Lillicrap et al., 2015) use an \\\\( \\\\epsilon \\\\)-greedy policy as an exploration policy on top of the greedy policy for evaluation. However, our setting necessitates more concerted exploration, and the exploratory action may come from an entirely different policy. Note that the initial state distribution is sampled \\\\( (s_0 \\\\sim \\\\rho) \\\\) exactly once to begin training, and then the algorithm \\\\( A \\\\) is run until \\\\( t \\\\rightarrow \\\\infty \\\\), generating the sequence \\\\( s_0, a_0, s_1, a_1, ... \\\\) in the MDP \\\\( M_T \\\\). This is the primary difference compared to the episodic setting described in Section 3, which can sample the initial state distribution repeatedly.\\n\\n4.1.1 ARL Deployment Setting\\n\\nConsider the problem where a robot has to learn how to close a door. Traditional reinforcement learning algorithms require several trials, repeatedly requiring interventions to open the door between trials. The desire is that the robot autonomously interacts with the door, learning to open it if it is required to practice closing the door. The output policy of the training procedure is evaluated in its deployment setting, in this case on its ability to close the door. Formally, the evaluation objective \\\\( J_D \\\\) for a policy \\\\( \\\\pi \\\\) in the deployment setting can be written as:\\n\\n\\\\[\\nJ_D(\\\\pi) = E_{s_0 \\\\sim \\\\rho, a_j \\\\sim \\\\pi(\\\\cdot|s_j), s_{j+1} \\\\sim p(\\\\cdot|s_j, a_j)} \\\\left[ \\\\sum_{j=0}^{\\\\infty} \\\\gamma^j r(s_j, a_j) \\\\right].\\n\\\\]\\n\\nDefinition 1 (Deployed Policy Evaluation). For an algorithm \\\\( A: \\\\{s_i, a_i, s_{i+1}\\\\}_{t-1}^t \\\\rightarrow (a_t, \\\\pi_t) \\\\), deployed policy evaluation \\\\( D(A) \\\\) is given by\\n\\n\\\\[\\nD(A) = \\\\sum_{t=0}^{\\\\infty} (J_D(\\\\pi_*) - J_D(\\\\pi_t)),\\n\\\\]\\n\\nwhere \\\\( J_D(\\\\pi) \\\\) is defined in Eq 1 and \\\\( \\\\pi_* \\\\in \\\\arg \\\\max \\\\pi J_D(\\\\pi) \\\\).\\n\\nThe evaluation objective \\\\( J_D(\\\\pi) \\\\) is identical to the one defined in Section 3 on MDP \\\\( M \\\\) (deployment environment). The policy evaluation is \u201chypothetical\u201d, the environment rollouts used for evaluating policies are not used in training. Even though the evaluation trajectories are rolled out from the initial state, there are no interventions in training. Concretely, the algorithmic goal in this setting can be stated as \\\\( \\\\min A D(A) \\\\). In essence, the policy outputs \\\\( \\\\pi_t \\\\) from the autonomous algorithm \\\\( A \\\\) should match the oracle deployment performance, i.e. \\\\( J_D(\\\\pi_*) \\\\), as quickly as possible. Note that \\\\( J_D(\\\\pi_*) \\\\) is a constant that can be ignored when comparing two algorithms, i.e. we only need to know \\\\( J_D(\\\\pi_t) \\\\) for a given algorithm in practice.\\n\\n4.1.2 ARL Continuing Setting\\n\\nFor some applications, the agent's experience cannot be separated into training and deployment phases. Agents may have to learn and improve in the environment that they are \u201cdeployed\u201d into, and thus these algorithms need to be evaluated on their performance during the agent's lifetime. For example, a robot tasked with keeping the home clean learns and improves on the job as it adapts to the home in which it is deployed. To this end, consider the following definition:\"}"}
{"id": "nkaba3ND7B5", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Definition 2 (Continuing Policy Evaluation)\\n\\nFor an algorithm $A$:\\n\\n$\\\\{s_i, a_i, s_{i+1}\\\\}_{t-1} \\\\xrightarrow{i=0} (a_t, \\\\pi_t)$,\\n\\ncontinuing policy evaluation $C(A)$ can be defined as:\\n\\n$$C(A) = \\\\lim_{h \\\\to \\\\infty} \\\\frac{1}{h} E_{s_0 \\\\sim \\\\rho, a_t \\\\sim A} \\\\left[ h \\\\sum_{t=0}^{\\\\infty} r(s_t, a_t) \\\\right]$$\\n\\nHere, $a_t$ is the action taken by the algorithm $A$ based on the transitions collected in the environment until time $t$, measuring the performance under reward $r$. The optimization objective can be stated as $\\\\max A C(A)$. Note that $\\\\pi_t$ is not used in computing $C(A)$. In practice, this amounts to measuring the reward collected by the agent in the MDP $M_T$ during its lifetime.\\n\\n4.2 How Specific ARL Problems Fit Into Our Framework\\n\\nThe framework can easily be adapted to model possible autonomous reinforcement learning scenarios that may be encountered:\\n\\nIntermittent interventions. By default, the agent collects experience in the environment with fully autonomous interaction in the MDP $M_T$. However, we can model the occasional intervention with transition dynamics defined as $\\\\tilde{p}(\\\\cdot|s,a) = (1-\\\\epsilon)p(\\\\cdot|s,a) + \\\\epsilon \\\\rho(\\\\cdot)$, where the next state is sampled with $1-\\\\epsilon$ probability from the environment dynamics or with $\\\\epsilon$ probability from the initial state distribution via intervention for some $\\\\epsilon \\\\in [0,1]$. A low $\\\\epsilon$ represents very occasional interventions through the training in MDP $M_T$. In fact, the framework described in Section 3, which is predominantly assumed by reinforcement learning algorithms, can be understood to have a large $\\\\epsilon$.\\n\\nTo contextualize $\\\\epsilon$, the agent should expect to get an intervention after $1/\\\\epsilon$ steps in the environment. Current episodic settings typically provide an environment reset every 100 to 1000 steps, corresponding to $\\\\epsilon \\\\in (1e^{-3}, 1e^{-2})$ and an autonomous operation time of typically a few seconds to few minutes depending on the environment. While full autonomy would be desirable (that is, $\\\\epsilon = 0$), intervening every few hours to few days may be reasonable to arrange, which corresponds to environment resets every 100,000 to 1,000,000 steps or $\\\\epsilon \\\\in (1e^{-6}, 1e^{-5})$. We evaluate the dependence of algorithms designed for episodic reinforcement learning on the reset frequency in Section 6.1.\\n\\nIrreversible states. An important consideration for developing autonomous algorithms is the \\\"reversibility\\\" of the underlying MDP $M$. Informally, if the agent can reverse any transition in the environment, the agent is guaranteed to not get stuck in the environment. As an example, a static robot arm can be setup such that there always exists an action sequence to open or close the door. However, the robot arm can push the object out of its reach such that no action sequence can retrieve it. Formally, we require MDPs to be ergodic for them to be considered reversible (Moldovan & Abbeel, 2012). In the case of non-ergodic MDPs, we adapt the ARL framework to enable the agent to request extrinsic interventions, which we discuss in Appendix A.\\n\\n4.3 Discussion of the ARL Formalism\\n\\nThe ARL framework provides two evaluation protocols for autonomous RL algorithms. Algorithms can typically optimize only one of the two evaluation metrics. Which evaluation protocol should the designer optimize for?\\n\\nIn a sense, the need for two evaluation protocols arises from task specific constraints, which themselves can be sometimes relaxed depending on the specific trade-off between the cost of real world training and the cost of intervention. The continuing policy evaluation represents the oracle metric one should strive to optimize when continually operational agents are deployed into dynamic environments. The need for deployment policy evaluation arises from two implicit practical constraints: (a) requirement of a large number of trials to solve desired tasks and (b) absence of interventions to enable those trials. If either of these can be easily relaxed, then one could consider optimizing for continuing policy evaluation. For example, if the agent can learn in a few trials because it was meta-trained for quick adaptation (Finn et al., 2017), providing a few interventions for those trials may be reasonable. Similarly, if the interventions are easy to obtain during deployment without incurring significant human cost, perhaps through scripted behaviors or enabled by the deployment setting (for example, sorting trash in a facility), the agent can repeatedly try the task and learn while deployed. However, if these constraints cannot be relaxed at deployment, one should consider optimizing for the deployment policy evaluation since this incentivizes the agents to learn targeted behaviors by setting up its own practice problems.\\n\\nWe can also consider the more commonly used setting of expected discounted sum of rewards as the objective. To ensure that future rewards are relevant, the discount factors would need to be much larger than values typically used.\"}"}
{"id": "nkaba3ND7B5", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we introduce the set of environments in our proposed benchmark, Environments for Autonomous Reinforcement Learning (EARL). We first discuss the factors in our design criteria and provide a description of how each environment fits into our overall benchmark philosophy, before presenting the results and analysis. For detailed descriptions of each environment, see Appendix A.\\n\\n5.1 Benchmark Design\\n\\nRepresentative Autonomous Settings. We include a broad array of tasks that reflect the types of autonomous learning scenarios agents may encounter in the real world. This includes different problems in manipulation and locomotion, and tasks with multiple object interactions for which it would be challenging to instrument resets. We also ensure that both the continuing and deployment evaluation protocols of ARL are realistic representative evaluations.\\n\\nDirected Exploration. In the autonomous setting, the necessity to practice a task again, potentially from different initial states, gives rise to the need for agents to learn rich reset behaviors. For example, in the instance of a robot learning to interact with multiple objects in a kitchen, the robot must also learn to implicitly or explicitly compose different reset behaviors.\\n\\nRewards and Demonstrations. One final design aspect for our benchmark is the choice of reward functions. Dense rewards are a natural choice in certain domains (e.g., locomotion), but designing and providing dense rewards in real world manipulation domains can be exceptionally challenging. Sparse rewards are easier to specify in such scenarios, but this often makes exploration impractical. As a result, prior work has often leveraged demonstrations (e.g., (Gupta et al., 2019)), especially in real world experimentation. To reflect practical usage of RL in real world manipulation settings, we include a small number of demonstrations for the sparse-reward manipulation tasks.\\n\\n5.2 Environment Descriptions\\n\\nTabletop-Organization (TO). The Tabletop-Organization task is a diagnostic object manipulation environment proposed by Sharma et al. (2021). The agent consists of a gripper modeled as a point-mass, which can grasp objects that are close to it. The agent's goal is to bring a mug to four different locations designated by a goal coaster. The agent's reward function is a sparse indicator function when the mug is placed at the goal location. Limited demonstrations are provided to the agent.\\n\\nSawyer-Door (SD). The Sawyer-Door task, from the MetaWorld benchmark (Yu et al., 2020) consists of a Sawyer robot arm who's goal is to close the door whenever it is in an open position. The task reward is a sparse indicator function based on the angle of the door. Repeatedly practicing this task implicitly requires the agent to learn to open the door. Limited demonstrations for opening and closing the door are provided.\\n\\nSawyer-Peg (SP). The Sawyer-Peg task (Yu et al., 2020) consists of a Sawyer robot required to insert a peg into a designed goal location. The task reward is a sparse indicator function for when the peg is in the goal location. In the deployment setting, the agent must learn to insert the peg starting on the table. Limited demonstrations for inserting and removing the peg are provided.\\n\\nFranka-Kitchen (FK). The Franka-Kitchen (Gupta et al., 2019) is a domain where a 9-DoF robot, situated in a kitchen environment, is required to solve tasks consisting of compound object interactions. The environment consists of a microwave, a hinged cabinet, a burner, and a slide cabinet. One example task is to open the microwave, door and burner. This domain presents a number of distinct challenges for ARL. First, the compound nature of each task results in a challenging long horizon problem, which introduces exploration and credit assignment challenges. Second, while generalization is important in solving the environment, combining reset behaviors are equally important given the compositional nature of the task.\"}"}
