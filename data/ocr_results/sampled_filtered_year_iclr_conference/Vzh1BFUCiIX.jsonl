{"id": "Vzh1BFUCiIX", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Vzh1BFUCiIX", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Vzh1BFUCiIX", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Vzh1BFUCiIX", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Vzh1BFUCiIX", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Choosing which tasks to transfer from\\n\\nOur experiments in \u00a72.2 attempted to empirically select a set of tasks to transfer from. Along these lines, Ruder & Plank (2017) use a Bayesian Optimization method with similarity measures to automatically select relevant data from different domains. Similarly, Guo et al. (2019) use multi-armed bandits to select tasks and a Gaussian Process to control the mixing rates for the selected tasks. Another strand of recent work selects appropriate transfer languages based on manually defined features (Lin et al., 2019; Sun et al., 2021). Aside from the NLP domain, Fifty et al. (2021) proposed a method to select which tasks to transfer to based on task gradients. All of the aforementioned works select data tailored to a downstream task of interest. If a general pre-trained model was attempted to be trained in a similar fashion, computational bottlenecks similar to those motivating \u00a72.1 and \u00a72.2 would arise.\\n\\nPre-trained Transformers\\n\\nTransformer models (Vaswani et al., 2017) such as T5 (Raffel et al., 2020), BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020) rely on large unlabeled corpus for self-supervised learning. Given the wild success of the pre-train-finetune paradigm, the search for suitable pre-training tasks has also become an active area of research (Lewis et al., 2019; Lan et al., 2019; Chang et al., 2020; Zhang et al., 2019; Lourie et al., 2021). While there has been evidence that supplementary pre-training tasks can help improve performance, this work is the first massive-scale multi-task pre-trained model.\\n\\nScaling Laws\\n\\nScaling laws for Transformers have attracted much attention recently, especially pertaining to model size (Kaplan et al., 2020; Zhai et al., 2021; Tay et al., 2021a). In Kaplan et al. (2020), the authors further investigate scaling with respect to dataset size (on the same pre-training corpus). To this end, this work can be interpreted as an attempt of scaling up with respect to the number of high quality, diverse labeled tasks that can be used for pre-training.\\n\\n5 EPILOGUE\\n\\nLimitations\\n\\nDespite our best efforts to evaluate on as many representative tasks as possible while also maintaining a balance among task partitions for a given set of transfer learning experiments, any study that explicitly abstracts datasets into \u201ctask families\u201d is highly dependent on nuances pertaining to the nature, domain, and expressiveness of the task family\u2019s representative datasets. For this paper, the subsets were constructed so as to include a diverse set of datasets to evaluate on, and we tried to partition task-families to be as mutually exclusive as possible. However, it must be acknowledged that no dataset is perfectly isolated, and any set of them only a proxy for a larger \u201ctask family\u201d. On a separate note, lexical metrics like BLEU/ROUGE are useful but do not paint the full picture of how well a model truly performs on text-generation tasks.\\n\\nFuture Work\\n\\nWe believe that a multilingual version of ET5 would be a natural extension of this work. Such a model will require extra care with regard to balancing not only task families, but also task languages. A multilingual version of EMIX could provide a more robust foundation for the analysis of task families in existing works that analyze how multilingual NLP models transfer amongst different languages (Kudugunta et al., 2019; Hu et al., 2020; Wang et al., 2021). For example, it would be interesting to understand whether our results in \u00a72.1 hold across different languages (and language families), and to explore cross-lingual cross-task generalization. We also hypothesize that modeling innovations that introduce inductive biases designed to exploit multi-task learning setups (Ha et al., 2016; Tay et al., 2021b) can push the boundary of the strong performance displayed by ET5. Other solutions like gradient manipulation (Yu et al., 2020; Wang et al., 2021) might also further improve extreme multi-task scaling, albeit at the cost of more complex implementations.\\n\\nConclusion\\n\\nThis paper explores how supervised multi-task learning at a massive scale can be used to improve existing self-supervised pre-training strategies for NLP models, and does so by introducing EMIX (\u00a72) and ET5 (\u00a73). Our experiments showed that while negative transfer is common when fine-tuning on diverse tasks (\u00a72.1), scaling up the number of tasks to include in a multi-task pre-training setup enables strong downstream performance (\u00a73.2) with better sample-efficiency (\u00a72.6). We hope that this paper motivates future research on how existing labeled datasets can be used to further improve NLP models within the pre-train/fine-tune paradigm.\"}"}
{"id": "Vzh1BFUCiIX", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGEMENTS\\n\\nThe authors would like to thank Mostafa Dehghani and Adhi Kuncoro for valuable comments and insights. We would also like to thank the authors of Mesh Tensorflow (Shazeer et al., 2018) and T5 (Raffel et al., 2020), as their high-quality code and paper enabled this work.\\n\\nAUTHOR CONTRIBUTIONS\\n\\nVamsi co-led the project and was primarily responsible for the design, implementation, and engineering effort behind it. Vamsi proposed and ran key experiments including (but not limited to): task-family transfer analysis, early proof-of-concepts for extreme task-scaling and EXT5, task-scaling analysis, etc. Vamsi also wrote most of the paper, and was (jointly) responsible for the overall framing of the paper.\\n\\nYi served as the co-lead of the paper and proposed the initial idea. Yi was responsible for most of the downstream EXT5 experiments, including SuperGLUE, Rainbow, CBQA, and Machine translation, along with large-scale pre-training of EXT5. Yi also wrote large portions of the paper.\\n\\nTal was (jointly) responsible for the overall framing of the paper, and wrote large portions of it. Tal contributed the Vitamin C task to the EXT MIX mixture, along with running out-of-mixture experiments for Named Entity Recognition.\\n\\nJinfeng contributed the GEM benchmark to our mixture and was heavily involved in running experiments.\\n\\nSteven contributed the DialoGLUE tasks to EXT MIX, helped run a substantial number of experiments and contributed substantially to discussions around the paper's framing.\\n\\nSanket was responsible for and ran experiments on GEM, and contributed the Yahoo Answers and Argument mining tasks.\\n\\nHonglei contributed the MsMarco task to EXT MIX and helped with benchmarking EXT5 on MsMarco. Honglei also contributed scripts and pipelines for obtaining reranking results on MsMarco.\\n\\nVinh contributed NewsQuiz, AgreeSum, TweetQa and TweetEval to EXT MIX and contributed substantially to paper writing and the framing of the paper.\\n\\nDara contributed GPT DeepFake tasks and low-resource tasks (which were not used in the end).\\n\\nJianmo contributed the DocNLI task and helped with running experiments for out-of-mixture tasks.\\n\\nJai contributed the Race and MultiRC Eraser tasks to the EXT MIX and helped edit the paper.\\n\\nKai contributed several retrieval tasks to EXT MIX, which were not included eventually.\\n\\nSebastian helped substantially with the paper narrative, writing of the paper and brainstorming.\\n\\nDonald (along with Yi and Vamsi) was heavily involved in framing the early vision of the project. Donald was also deeply involved in brainstorming sessions, and provided critical feedback that helped to steer the project in the right direction.\\n\\nAll authors contributed to brainstorming and discussion sessions.\\n\\nETHICS STATEMENT\\n\\nLarge language models have been shown to capture certain biases about the data they have been pre-trained on (Bender et al., 2020). While a comprehensive analysis of such biases is outside of the scope of this work, it is a compelling direction to investigate to what extent the inclusion of supervised data during pre-training can help mitigate such biases. An alternative consideration is the addition of diverse values-targeted data (Solaiman & Dennison, 2021) during pre-training in order to instill beneficial biases in a model.\\n\\nAnother challenge when training large models is their energy consumption and environmental impact (Strubell et al., 2019). To ablate different task combinations, we performed experiments using 10\"}"}
{"id": "Vzh1BFUCiIX", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the more computationally efficient fine-tuning setup. We have shown that E\\\\textsuperscript{XM} leads to more sample-efficient pre-training compared to standard self-supervision, which we hope will save compute in future experiments.\\n\\n\\\\textbf{REPRODUCIBILITY STATEMENT}\\n\\nAll of the modeling and training code used for E\\\\textsuperscript{XM} and its variants is already open-sourced as a part of the Mesh Tensorflow\\\\textsuperscript{1} (Shazeer et al., 2018) and T5\\\\textsuperscript{2} (Raffel et al., 2020) Libraries. Additionally, E\\\\textsuperscript{XM} is composed of datasets that are already publicly available.\\n\\n\\\\textbf{REFERENCES}\\n\\nAmazon reviews dataset. https://s3.amazonaws.com/amazon-reviews-pds/readme.html.\\n\\nArmen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. Muppet: Massive multi-task representations with pre-finetuning, 2021.\\n\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. Ms marco: A human generated machine reading comprehension dataset, 2018.\\n\\nFrancesco Barbieri, Jose Camacho-Collados, Francesco Ronzano, Luis Espinosa-Anke, Miguel Ballesteros, Valerio Basile, Viviana Patti, and Horacio Saggion. Semeval 2018 task 2: Multilingual emoji prediction. In Proceedings of The 12th International Workshop on Semantic Evaluation, pp. 24\u201333, 2018.\\n\\nFrancesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. TweetEval: Unified benchmark and comparative evaluation for tweet classification. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1644\u20131650, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.148. URL https://aclanthology.org/2020.findings-emnlp.148.\\n\\nValerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana Patti, Francisco Manuel Rangel Pardo, Paolo Rosso, and Manuela Sanguinetti. SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter. In Proceedings of the 13th International Workshop on Semantic Evaluation, pp. 54\u201363, Minneapolis, Minnesota, USA, 2019. Association for Computational Linguistics. doi: 10.18653/v1/S19-2007. URL https://www.aclweb.org/anthology/S19-2007.\\n\\nJonathan Baxter. A model of inductive bias learning. Journal of artificial intelligence research, 12:149\u2013198, 2000.\\n\\nEmily Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: can language models be too big? In Proceedings of the ACM/IEEE Joint Conference on Digital Libraries, volume 1, pp. 271\u2013278. Association for Computing Machinery, 2020. ISBN 9781450375856. doi: 10.1145/3442188.3445922.\\n\\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1533\u20131544, Seattle, Washington, USA, 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1160.\\n\\nChandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen tau Yih, and Yejin Choi. Abductive commonsense reasoning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=Byg1v1HKDB.\"}"}
{"id": "Vzh1BFUCiIX", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Vzh1BFUCiIX", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Vzh1BFUCiIX", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Vzh1BFUCiIX", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Vzh1BFUCiIX", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. ArXiv, abs/1907.10641, 2020.\\n\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99\u2013106, August 2021. ISSN 0001-0782. doi: 10.1145/3474381. URL https://doi.org/10.1145/3474381.\\n\\nVictor Sanh, Thomas Wolf, and Sebastian Ruder. A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks. In Proceedings of AAAI 2019, 2019.\\n\\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4463\u20134473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL https://aclanthology.org/D19-1454.\\n\\nJ. H. Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. 1987.\\n\\nTal Schuster, Adam Fisch, and Regina Barzilay. Get your vitamin C! robust fact verification with contrastive evidence. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 624\u2013643, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.52. URL https://aclanthology.org/2021.naacl-main.52.\\n\\nAbigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1073\u20131083, Vancouver, Canada, 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1099. URL https://aclanthology.org/P17-1099.\\n\\nNoam Shazeer. Glu variants improve transformer, 2020.\\n\\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake A. Hechtman. Mesh-tensorflow: Deep learning for supercomputers. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pp. 10435\u201310444, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/3a37abdeefe1dab1b30f7c5c7e581b93-Abstract.html.\\n\\nSamuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V. Le. Don't decay the learning rate, increase the batch size. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=B1Yy1BxCZ.\\n\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631\u20131642, Seattle, Washington, USA, 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1170.\\n\\nIrene Solaiman and Christy Dennison. Process for adapting language models to society (pLAMS) with values-targeted datasets. arXiv preprint arXiv:2106.10328, 2021.\\n\\nChristian Stab, Tristan Miller, Pranav Rai, Benjamin Schiller, and Iryna Gurevych. Ukp sentential argument mining corpus, 2018. URL https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2345.\\n\\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243, 2019.\"}"}
{"id": "Vzh1BFUCiIX", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Instead of pre-training on ELMIX, another way to leverage multi-task learning is as an intermediate step between pre-training and fine-tuning. This is referred to as pre-finetuning by Aghajanyan et al. (2021). We conduct controlled experiments to compare pre-training with pre-finetuning. We begin with a standard T5 base checkpoint and pre-finetune it with ELMIX. After this phase, we fine-tune on SuperGLUE.\\n\\nTable 4: Comparison of Pre-finetuning and Multi-task Pre-training on ELMIX.\\n\\n| Method                        | Compute | SuperGLUE |\\n|-------------------------------|---------|-----------|\\n| Vanilla                      | 1.2M    | 76.1      |\\n| Pre-finetuning (200k)         | 1.4M    | 78.1      |\\n| Multi-task Pre-training       | 1.2M    | 79.9      |\\n\\nTable 4 compares pre-finetuning and our proposed multi-task pre-training. We also report the total compute (in total number of tokens processed) by the model in both schemes. The results show that multi-task pre-training is significantly superior to pre-finetuning. A potential hypothesis is that multi-task pre-training narrows the gap between pre-training and finetuning data distributions, as the pre-training stage more closely resembles fine-tuning. Conversely, segregating pre-training and pre-finetuning into two different stages may induce catastrophic forgetting of the pre-training task. Hence, in ELM5, we opt for multi-task pre-training over pre-finetuning.\\n\\nIn this section, we explore how the ratio of C4 span denoising examples to ELMIX examples during massive multi-task pre-training affects performance. As mentioned later in \u00a73, this is controlled by a hyperparameter $R$, where a pre-training batch will have approximately $R$ times as many C4 examples compared to ELMIX.\\n\\nFrom our results in Figure 3, we find that despite ELMIX improving downstream performance when mixed with self-supervised C4 pre-training at many rates, a model trained with $R=0$ suffers greatly in comparison. This result is significant, as it shows that while ELMIX improves the pre-training process, self-supervised training over a large unstructured corpus is still crucial.\\n\\nIn this section, we explore how model performance changes as the number of tasks included in a massive multi-task pre-training setup is scaled up. We choose random sets of 30, 55, and 80 tasks (each a superset of the last), pre-train a BASE-sized model for 524k steps, and fine-tune them on SuperGLUE. We train our models with batch sizes of 128 and 512 and $R=2$ (the ratio of C4 to ELMIX examples) as this configuration worked best for our BASE-sized models (\u00a72.4). We repeat this over three random seeds (for random subset selection), and report average scores in Figure 4.\\n\\nOverall, with large batches, we can see that increasing the number of tasks being mixed generally helps downstream performance. This reinforces our intuition that task scaling indeed helps. With small batches, there is less of an upward trend, signifying that large batches are essential for a large number of tasks. This is intuitive, given that multi-task learning may cause gradients to be noisy (Yu et al., 2020). Another explanation as to why this happens is that large-batch training can offer benefits even for single-task models (Smith et al., 2018) \u2014 a trend formalized by McCandlish et al. (2018).\"}"}
{"id": "Vzh1BFUCiIX", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.6 Improving Sample Efficiency with Extreme\\n\\nWe hypothesize that extreme multi-task scaling also improves the sample efficiency of pre-training. To test this, we exclude SuperGLUE from ExMIX, pre-train a large model for 200k steps, and fine-tune it on SuperGLUE at several intervals during early pre-training stages. We find that ExMIX pre-training is significantly more sample-efficient than vanilla self-supervised pre-training. Note that at only 20k pre-training steps, our ExT5 model already achieves 75.8 SuperGLUE score, which outperforms a fully pre-trained BERT large model by about +4% (Wang et al., 2019a).\\n\\n3 The EXT5 Model\\n\\nTo overcome the challenges of multi-task co-training at scale, i.e. negative transfer and catastrophic forgetting explored in \u00a72.1, the rest of this paper revisits the multi-task pre-training paradigm introduced by Raffel et al. (2020) via extreme multi-task scaling. This section introduces ExT5: a pre-trained sequence-to-sequence Transformer encoder-decoder model (Vaswani et al., 2017) based on the popular T5 framework.\\n\\n3.1 Training ExT5\\n\\nPre-training\\nWe pre-train on a mixture of C4 and ExMIX (\u00a72), and combine them with a hyper-parameter $R$ that is the ratio at which C4 examples are sampled with respect to ExMIX examples. The C4 objective we use is the same as that used by Raffel et al. (2020), and every task optimizes the standard sequence-to-sequence cross-entropy loss. We pre-train ExT5 on the same number of steps as T5, and ExT5 sees an identical number of tokens to the released T5 models. Concretely, we pre-train our models for 1M total steps with a batch size of 2048 and sequence length 512, resulting in a total of approximately 1T tokens seen by the model during pre-training (both unsupervised and supervised inclusive). We use the T5.1.1 architecture (Shazeer, 2020) for all of our experiments \u2014 which uses GEGLU-activated layers instead of ReLU in classic Transformer models (Vaswani et al., 2017). For optimization, we use Adafactor with an inverse square root learning rate schedule that kicks in after a constant phase of 0.01 for 10k steps. ExT5 also uses the same tokenizer as T5.\\n\\nFine-tuning\\nWe follow the same fine-tuning procedure for T5 and ExT5 for fair comparison, although we found that ExT5 generally benefitted from a smaller learning rate while fine-tuning ($10^{-4}$ worked well for ExT5 vs $10^{-3}$ for T5 variants). Fine-grained details can be found in Appendix B.\\n\\n3.2 Experimental Setup\\n\\nOur experiments consider both within-mixture and out-of-mixture tasks (i.e., whether a task is included in ExMIX). Within-mixture tasks measure the amount the task benefits from multi-task pre-training and extreme task scaling. Similar to the co-trained models in Raffel et al. (2020), we continue to fine-tune on the target task from a pre-trained ExT5 checkpoint. For out-of-mixture tasks, we consider possibly new unseen tasks or collections that were not included in the ExMIX mixture to test the effect of generalizing to unseen tasks. For the sake of brevity, the fine-grained details of these experimental setups can be found in the Appendix.\\n\\n3.3 Experimental Results\\n\\nWithin-Mixture Results\\nWe report results on SuperGLUE (Table 5), GEM (Table 6), Rainbow (Table 7), MsMarco (Table 8) and CBQA datasets (Table 9). On the whole, we observe that ExT5 consistently outperforms strong T5 baselines across a range of model sizes. On SuperGLUE, we achieve $+5\\\\%$, $+2.3\\\\%$ and $+6.3\\\\%$.\"}"}
{"id": "Vzh1BFUCiIX", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"+0.7% gain on BASE, LARGE and XL respectively. On GEM, ExT5 outperforms T5 on 6 out of 9 collections while remaining on-par on the other 3 collections. Notably, the gain on datasets such as WebNLG are approximately +11% ROUGE for the large model and generally range from +1% to +6% on different collections. On Rainbow, ExT5 outperforms our own run of T5 by +0.7% on average and +4.6% improvement over the best multi-task (sequential) setup in Lourie et al. (2021).\\n\\nFinally, on question answering and ranking, ExT5 substantially outperforms T5 at two different sizes.\\n\\n| Model  | Metric | WebNLG | DART | SGD | E2E | CG | ToTTo | WiA-A | WiA-T | WLE |\\n|-------|--------|--------|------|-----|-----|----|-------|-------|-------|-----|\\n| T5.1.1 BASE     | METEOR | 0.323  | 0.364 | 0.325 | 0.383 | 0.201  | 0.366 | 0.302 | 0.368 | 0.189 |\\n| ExT5 BASE       | METEOR | 0.349  | 0.367 | 0.330 | 0.382 | 0.206  | 0.368 | 0.306 | 0.367 | 0.192 |\\n| T5.1.1 LARGE    | METEOR | 0.344  | 0.363 | 0.324 | 0.382 | 0.202  | 0.368 | 0.301 | 0.362 | 0.196 |\\n| ExT5 LARGE      | METEOR | 0.365  | 0.376 | 0.330 | 0.381 | 0.214  | 0.369 | 0.300 | 0.358 | 0.204 |\\n\\nTable 5: Comparisons of T5 and ExT5 on SuperGLUE validation sets.\\n\\n| Model  | \u03b1 | NLI | CosmosQA | HellaSwag | PIQA | SocialIQA | Winogrande | A VG |\\n|-------|---|-----|----------|-----------|------|------------|------------|------|\\n| T5.1.1 LARGE (multitask) | | 78.40 | 81.10 | 81.30 | 80.70 | 74.80 | 72.10 | 78.07 |\\n| T5.1.1 LARGE (sequential) | | 79.50 | 83.20 | 83.00 | 82.20 | 75.50 | 78.70 | 80.35 |\\n| T5.1.1 LARGE                 | | 82.51 | 85.59 | 88.57 | 85.53 | 78.51 | 79.79 | 83.42 |\\n| ExT5 LARGE                 | | 82.25 | 85.86 | 88.99 | 85.04 | 79.73 | 82.53 | 84.07 |\\n| % Gain | | -0.3% | +0.3% | +0.5% | -0.6% | +1.6% | +3.4% | +0.8% |\\n\\nTable 7: Results on the Rainbow Commonsense Reasoning benchmark validation sets. Results with \u2020 are from Lourie et al. (2021).\\n\\n| Model  | \u03b1 | NQ | WQ | TQA |\\n|-------|---|----|----|-----|\\n| T5.1.1 LARGE | | 27.3 | 29.5 | 28.5 |\\n| ExT5 LARGE | | 28.6 | 30.5 | 30.7 |\\n| % Gain | | +4.8% | +3.4% | +7.7% |\\n\\n| Model  | \u03b1 | NQ | WQ | TQA |\\n|-------|---|----|----|-----|\\n| T5.1.1 XL | | 29.5 | 32.4 | 36.0 |\\n| ExT5 XL | | 30.6 | 35.2 | 37.0 |\\n| % Gain | | +3.7% | +8.6% | +2.8% |\\n\\nTable 8: Results on MSMarco.\\n\\n| Model  | \u03b1 | MRR@10 |\\n|-------|---|--------|\\n| T5.1.1 LARGE (Nogueira et al., 2020) | | 0.393 |\\n| ExT5 LARGE | | 0.402 |\\n| % Gain | | +2.3% |\\n\\n| Model  | \u03b1 | MRR@10 |\\n|-------|---|--------|\\n| T5.1.1 XL (Nogueira et al., 2020) | | 0.398 |\\n| ExT5 XL | | 0.403 |\\n| % Gain | | +1.3% |\\n\\nTable 9: Results on CBQA dev sets.\"}"}
{"id": "Vzh1BFUCiIX", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We are also interested in evaluating ExT5 on tasks outside of ExMIX, and hypothesize that the extreme multi-task pre-training of ExT5 will lead to better performance on new unseen settings. Concretely, we fine-tune and evaluate on Machine Translation: translating sentences from English to other languages (Bojar et al., 2014; 2015; 2016); Reasoning: answering scientific questions on ARC (Clark et al., 2018); and Named Entity Recognition: extracting all entities from sentences on the CoNLL-2003 NER dataset (Tjong Kim Sang & De Meulder, 2003).\\n\\n| Model           | EnDe | EnFr | EnRo | Dev  | Test | % Gain \u00b1 |\\n|-----------------|------|------|------|------|------|----------|\\n| Raffel et al. (2020) | 26.98| 39.82| 27.65| -    | -    | -        |\\n| T5.1.1 BASE      | 28.30| 41.01| 28.11| 26.45| 92.55| 85.75    |\\n| ExT5 BASE        | 28.32| 41.89| 28.38| 36.35| 92.68| 86.53    |\\n| % Gain \u00b1         | 0%   | +2.1%| +1.0%| +37.4%| 0.13%| +0.91%   |\\n| T5.1.1 LARGE     | 28.68| 41.34| 29.01| 55.80| 92.80| 86.56    |\\n| ExT5 LARGE       | 28.98| 42.71| 29.49| 63.99| 93.63| 87.34    |\\n| % Gain \u00b1         | +1.0%| +3.3%| +1.7%| +14.7%| 0.90%| +0.90%   |\\n\\nTable 10: Experimental results on tasks that are not in ExMIX. For ARC, we report test scores on the challenge set with retrieval. For NER, we report accuracy on a sentence level (see Appendix B.2). Table 10 summarizes the results on the out-of-mixture tasks. Across all tasks, we see that ExT5 outperforms upon T5 baselines. The largest improvement is on the ARC scientific reasoning task, perhaps due to the large amount of QA tasks in ExMIX. Though, the trend is consistent also with the NER and MT tasks that do not have any similar dataset in ExMIX. This suggests that the representations learned by ExT5 are more general adaptable to a new objective, even when the output is in a new language. This improved generalization of ExT5 is very encouraging from a practical standpoint, since pre-training again with ExMIX \u222a {t} for any new target task t would be very expensive. Instead, we see that the extreme multi-task pre-training of ExT5 already provides improved results. Therefore, it might only be worth repeating pre-training when the collection of training datasets grows by a significant amount (see \u00a72.5).\\n\\n4 RELATED WORK\\nImproving NLP models with Multi-task Learning\\nCollobert & Weston (2008) leverage multi-task learning for relatively simple tasks like Part-of-Speech tagging. Phang et al. (2019) use an intermediate fine-tuning stage using four tasks with large datasets for Natural Language Understanding. Similarly, Liu et al. (2019a) proposed MT-DNN, which uses a setup at a scale of around 30 tasks and up to 440M parameters. Most recently, Aghajanyan et al. (2021) use around 50 tasks and models of sizes up to 440M parameters. Gururangan et al. (2020) take an alternative approach, which is to continue pre-training a model but use domain-specific data as an intermediate step. McCann et al. (2018) proposed a unified framework similar to that of T5. Recently, Wei et al. (2021) also illustrated how a multi-task learning stage can greatly improve the zero-shot prompting performance of large language models at the scale of ~137B parameters. Efforts have also been made to tailor pre-training objectives to specific tasks, e.g., question answering (Ram et al., 2021; Jia et al., 2021), dialogue (Li et al., 2020), and span selection tasks (Joshi et al., 2020).\\n\\nRelationships amongst different tasks\\nBingel & S\u00f8gaard (2017) conducted a study similar to ours in \u00a72.1 but for more traditional NLP tasks like chunking, CCG tagging, POS tagging, etc. More recently, Vu et al. (2020) conducted an in-depth study of relationships between various classification/regression, question-answering, and sequence-labeling tasks, and proposed a task-embedding framework to predict such relationships. Khashabi et al. (2020) also conducted similar experiments but specific to question-answering datasets/formats, resulting in a strong QA model known as UnifiedQA that is also based on the T5 framework. Outside of NLP, Zhang & Yeung (2010) introduced a convex optimization objective for learning task relationships, and Li et al. (2018) explore and exploit task relationships on a variety of diverse datasets.\"}"}
{"id": "Vzh1BFUCiIX", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Vzh1BFUCiIX", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Vzh1BFUCiIX", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rui Zhang and Joel Tetreault. This email could save your life: Introducing the task of email subject line generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 446\u2013456, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1043. URL https://aclanthology.org/P19-1043.\\n\\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint 1810.12885, 2018.\\n\\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 649\u2013657, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html.\\n\\nYian Zhang, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman. When do you need billions of words of pretraining data? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1112\u20131125, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.90. URL https://aclanthology.org/2021.acl-long.90.\\n\\nYu Zhang and Dit-Yan Yeung. A convex formulation for learning task relationships in multi-task learning. In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, UAI\u201910, pp. 733\u2013742, Arlington, Virginia, USA, 2010. AUAI Press. ISBN 9780974903965.\\n\\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced language representation with informative entities. arXiv preprint arXiv:1905.07129, 2019.\"}"}
{"id": "Vzh1BFUCiIX", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 11: All of the training datasets used to construct ExMix.\\n\\n| Description                | No. | Train Datasets | Citation                                                                 |\\n|----------------------------|-----|----------------|--------------------------------------------------------------------------|\\n| GLUE                       | 7   | 949,101        | Wang et al. (2019b)                                                      |\\n| SuperGLUE                  | 8   | 185,673        | Wang et al. (2019a)                                                      |\\n| KILT                       | 9   | 3,129,859      | Petroni et al. (2021)                                                    |\\n| Rainbow                    | 6   | 324,742        | Lourie et al. (2021)                                                     |\\n| GEM (en)                   | 8   | 1,067,955      | Gehrmann et al. (2021)                                                   |\\n| DialoGLUE                  | 6   | 76,122         | Mehri et al. (2020)                                                      |\\n| TweetEval                  | 8   | 120,104        | Barbieri et al. (2020)                                                   |\\n| CNN/Dailymail              | 1   | 287,113        | See et al. (2017)                                                        |\\n| XSum                       | 1   | 203,577        | Narayan et al. (2018)                                                    |\\n| Multi-News                 | 1   | 44,972         | Fabbri et al. (2019)                                                     |\\n| AESLC                      | 1   | 14,436         | Zhang & Tetreault (2019)                                                 |\\n| Gigaword                   | 1   | 3,803,957      | Rush et al. (2015)                                                       |\\n| SamSum                     | 1   | 14,372         | Gliwa et al. (2019)                                                      |\\n| ANLI                       | 1   | 162,865        | Nie et al. (2020)                                                        |\\n| ESNLI                      | 1   | 549,367        | DeYoung et al. (2020)                                                    |\\n| AgreeSum                   | 1   | 7,750          | Pang et al. (2021)                                                       |\\n| DocNLI                     | 1   | 942,314        | Yin et al. (2021)                                                        |\\n| Vitamin C                  | 1   | 370,653        | Schuster et al. (2021)                                                   |\\n| Web Questions              | 1   | 3778           | Berant et al. (2013)                                                     |\\n| SQuAD                      | 1   | 87,599         | Rajpurkar et al. (2016)                                                  |\\n| QuAC                       | 1   | 83,568         | Choi et al. (2018)                                                       |\\n| DROP                       | 1   | 77,409         | Dua et al. (2019)                                                        |\\n| RACE                       | 4   | 113,013        | Lai et al. (2017)                                                        |\\n| Eraser MultiRC             | 1   | 24,029         | DeYoung et al. (2020)                                                    |\\n| TweetQA                    | 1   | 10692          | Xiong et al. (2019)                                                      |\\n| NewsQuizQA                 | 1   | 16,160         | Lelkes et al. (2021)                                                     |\\n| Amazon Reviews             | 1   | 100,000        | -                                                                        |\\n| GoEmotions                 | 1   | 43.410         | Demszky et al. (2020)                                                    |\\n| IMDb Reviews               | 1   | 25,000         | Maas et al. (2011)                                                       |\\n| Sentiment140               | 1   | 1,600,000      | Go et al. (2009)                                                         |\\n| Yelp Reviews               | 1   | 560,000        | Zhang et al. (2015)                                                      |\\n| AGNews                     | 1   | 120,000        | Zhang et al. (2015)                                                      |\\n| TreqQC                     | 1   | 5000           | Hovy et al. (2001)                                                       |\\n| Civil Comments             | 1   | 1,804,874      | Borkan et al. (2019)                                                     |\\n| Wiki Toxicity              | 1   | 159,571        | Wulczyn et al. (2017)                                                    |\\n| Yahoo! Answers             | 1   | 140,000        | Zhang et al. (2015)                                                      |\\n| UKP Arg. Mining            | 1   | 18,341         | Stab et al. (2018)                                                       |\\n| Parsing to FunQL           | 3   | 5,565          | Guo et al. (2020)                                                        |\\n| Parsing to interm. repr.   | 4   | 117,318        | Herzig et al. (2021)                                                     |\\n| COGS                       | 1   | 24,155         | Kim & Linzen (2020)                                                      |\\n| GPT Deepfake detection     | 8   | 500,000        | Radford et al. (2019)                                                    |\\n| StylePTB                   | 4   | 53,546         | Lyu et al. (2021)                                                        |\\n| Shakespearizing English    | 2   | 36,790         | Jhamtani et al. (2017)                                                   |\\n| MS-MARCO                   | 1   | 100,000        | Bajaj et al. (2018)                                                      |\\n| Total                      |     | 107            |                                                                           |\"}"}
{"id": "Vzh1BFUCiIX", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ToTTo (Parikh et al., 2020), Wiki-Auto (Jiang et al., 2020), Turk Corpus (Xu et al., 2016) \u2013 DialoGLUE: Banking77 (Casanueva et al., 2020), HWU64 (Liu et al., 2019b), CLINC150 (Larson et al., 2019), SGD (Rastogi et al., 2020), TOP (Gupta et al., 2018).\\n\\nTweetEval: Emotion Recognition (Mohammad et al., 2018), Emoji Prediction (Barbieri et al., 2018), Irony Detection (Van Hee et al., 2018), Hate Speech Detection (Basile et al., 2019), Offensive Language Identification (Zampieri et al., 2019), Sentiment Analysis (Rosenthal et al., 2017), Stance Detection (Mohammad et al., 2016).\\n\\nExperimental Details\\n\\nThis section describes the experimental details.\\n\\nB.1 Implementation Details\\n\\nOur models were trained using Mesh Tensorflow (Shazeer et al., 2018) using the T5 library (Raffel et al., 2020).\\n\\nB.2 Dataset Experimental Setup\\n\\nThis section reports the dataset and experimental setup on each individual target task/dataset.\\n\\nSuperGLUE\\n\\nWe finetune on the entire SuperGLUE as a mixture with proportionate sampling in a similar fashion to (Raffel et al., 2020). We finetune for a total of 200k steps with a batch size of 128. When selecting checkpoints on SuperGLUE, we follow the same convention as Raffel et al. (2020) in selecting the best checkpoint for each task for a fair comparison to models that are fine-tuned on the individual tasks instead of co-training on all of them.\\n\\nGEM\\n\\nWe report test set results on all datasets except CommonGen and ToTTo, on which we report validation scores. We sweep over learning rates of $10^{-3}$, $5 \\\\times 10^{-4}$ and $10^{-4}$. All results are computed using GEM-metrics. For each dataset, we select the best model checkpoint using the average of BLEU, ROUGE-1, ROUGE-2 and ROUGE-L scores on the validation set. We use the greedy decoding strategy to be consistent with the original GEM paper (Gehrmann et al., 2021).\\n\\nCBQA\\n\\nWe report validation set results, and sweep over learning rates of $10^{-3}$ and $10^{-4}$.\\n\\nRainbow\\n\\nWe multi-task co-train on all datasets, and sweep over learning rates of $10^{-3}$ and $10^{-4}$.\\n\\nWMT Machine Translation\\n\\nWe finetune our models on three collections of WMT, namely EnDe, EnFr and EnRo. We use a constant learning rate of $10^{-3}$ and dropout of 0.1. We train with a batch size of 4096 for a maximum of 400k steps and report peak validation BLEU score. We use a beam size of 4 and a length penalty of 0.6.\\n\\nARC\\n\\nWe report scores on the Challenge set, and train with a batch size of 32 and sweep over learning rates of $10^{-3}$ and $10^{-4}$.\\n\\nCoNLL-03 NER\\n\\nWe convert NER to seq2seq by writing the target as the ordered sequence of tags and entities (for example \u201cWhen Alice visited New York\u201d \u2192 \u201c[PER] Alice [LOC] New York\u201d). Accuracy is measured on a sentence level, considering a prediction to be correct only if it exactly matches the reference sequence.\\n\\nDetailed Results\\n\\nMany of our experiments in \u00a72 used the average SuperGLUE score of a model for evaluation. We report the full results on all datasets below.\"}"}
{"id": "Vzh1BFUCiIX", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method                        | BoolQ | CB   | Copa | MultiRC | ReC   | RTE   | WiC  | WSC  |\\n|------------------------------|-------|------|------|---------|-------|-------|------|------|\\n| Vanilla                      | 82.3  | 91.7 | 60.0 | 76.9    | 80.9  | 84.5  | 69.3 | 81.7 |\\n| Pre-finetuning               | 82.2  | 85.1 | 74.0 | 79.8    | 79.2  | 87.7  | 69.6 | 82.7 |\\n| Multi-task pre-training      | 82.6  | 98.7 | 73.0 | 79.5    | 80.8  | 87.0  | 71.3 | 83.7 |\\n| E                            | 81.3  | 97.3 | 67.7 | 77.0    | 76.5  | 82.7  | 69.5 | 83.3 |\\n| X                            | 82.6  | 98.7 | 73.0 | 79.5    | 80.8  | 87.0  | 71.3 | 83.7 |\\n| Random-55                    | 81.3  | 97.3 | 67.7 | 77.0    | 76.5  | 82.7  | 69.5 | 83.3 |\\n| T5                           | 82.6  | 98.7 | 73.0 | 79.5    | 80.8  | 87.0  | 71.3 | 83.7 |\\n\\nTable 12: Full SuperGLUE results from \u00a72.2\\n\\n| Method                        | BoolQ | CB   | Copa | MultiRC | ReC   | RTE   | WiC  | WSC  |\\n|------------------------------|-------|------|------|---------|-------|-------|------|------|\\n| # Tasks                      |       |      |      |         |       |       |      |      |\\n| Batch Size = 128             |       |      |      |         |       |       |      |      |\\n| 0                            | 79.3  | 92.4 | 72.0 | 74.2    | 74.9  | 79.8  | 70.2 | 81.7 |\\n| 30 (random)                  | 78.7  | 95.7 | 66.0 | 72.6    | 72.8  | 77.6  | 68.4 | 82.4 |\\n| 55 (random)                  | 79.4  | 93.2 | 74.3 | 73.6    | 74.1  | 80.7  | 68.8 | 82.1 |\\n| 80 (random)                  | 80.0  | 92.5 | 70.0 | 74.3    | 73.9  | 80.0  | 68.1 | 82.4 |\\n| 107                          | 79.3  | 95.0 | 74.0 | 74.0    | 73.5  | 79.4  | 70.7 | 77.9 |\\n| Batch Size = 512             |       |      |      |         |       |       |      |      |\\n| 0                            | 82.3  | 91.7 | 60.0 | 76.9    | 80.9  | 84.5  | 69.3 | 81.7 |\\n| 30 (random)                  | 80.6  | 93.6 | 67.7 | 74.6    | 75.7  | 81.2  | 68.6 | 83.3 |\\n| 55 (random)                  | 81.3  | 97.3 | 67.7 | 77.0    | 76.5  | 82.7  | 69.5 | 83.3 |\\n| 80 (random)                  | 82.1  | 94.4 | 71.7 | 76.8    | 77.0  | 84.7  | 69.2 | 83.0 |\\n| 107                          | 82.6  | 98.7 | 73.0 | 79.5    | 80.8  | 87.0  | 71.3 | 83.7 |\\n\\nTable 13: Full SuperGLUE results from \u00a72.3\\n\\n| Method                        | BoolQ | CB   | Copa | MultiRC | ReC   | RTE   | WiC  | WSC  |\\n|------------------------------|-------|------|------|---------|-------|-------|------|------|\\n| # Pre-train steps            |       |      |      |         |       |       |      |      |\\n| T5.1.1                       |       |      |      |         |       |       |      |      |\\n| 20k                          | 77.9  | 93.0 | 69.0 | 73.0    | 73.3  | 77.6  | 69.6 | 79.8 |\\n| 50k                          | 82.3  | 100 | 74.0 | 76.8    | 79.9  | 82.3  | 70.4 | 83.7 |\\n| 100k                         | 83.7  | 95.0 | 82.0 | 80.0    | 83.8  | 87.0  | 73.7 | 84.6 |\\n| 200k                         | 85.7  | 100 | 85.0 | 81.8    | 85.2  | 87.7  | 73.5 | 88.5 |\\n| ExT5                         |       |      |      |         |       |       |      |      |\\n| 20k                          | 80.3  | 95.0 | 70.0 | 74.4    | 72.9  | 82.7  | 68.5 | 81.7 |\\n| 50k                          | 83.1  | 97.4 | 78.0 | 79.2    | 79.6  | 88.1  | 73.4 | 87.5 |\\n| 100k                         | 85.3  | 100 | 81.0 | 81.6    | 83.7  | 89.2  | 73.2 | 90.4 |\\n| 200k                         | 86.5  | 98.7 | 86.0 | 83.2    | 85.4  | 91.7  | 73.4 | 93.3 |\\n\\nTable 14: Full SuperGLUE results from \u00a72.4\\n\\n| Method                        | BoolQ | CB   | Copa | MultiRC | ReC   | RTE   | WiC  | WSC  |\\n|------------------------------|-------|------|------|---------|-------|-------|------|------|\\n| # Pre-train steps            |       |      |      |         |       |       |      |      |\\n| T5.1.2                       |       |      |      |         |       |       |      |      |\\n| 20k                          | 77.9  | 93.0 | 69.0 | 73.0    | 73.3  | 77.6  | 69.6 | 79.8 |\\n| 50k                          | 82.3  | 100 | 74.0 | 76.8    | 79.9  | 82.3  | 70.4 | 83.7 |\\n| 100k                         | 83.7  | 95.0 | 82.0 | 80.0    | 83.8  | 87.0  | 73.7 | 84.6 |\\n| 200k                         | 85.7  | 100 | 85.0 | 81.8    | 85.2  | 87.7  | 73.5 | 88.5 |\\n| ExT5                         |       |      |      |         |       |       |      |      |\\n| 20k                          | 80.3  | 95.0 | 70.0 | 74.4    | 72.9  | 82.7  | 68.5 | 81.7 |\\n| 50k                          | 83.1  | 97.4 | 78.0 | 79.2    | 79.6  | 88.1  | 73.4 | 87.5 |\\n| 100k                         | 85.3  | 100 | 81.0 | 81.6    | 83.7  | 89.2  | 73.2 | 90.4 |\\n| 200k                         | 86.5  | 98.7 | 86.0 | 83.2    | 85.4  | 91.7  | 73.4 | 93.3 |\\n\\nTable 15: Full SuperGLUE results from \u00a72.5\\n\\n| Method                        | BoolQ | CB   | Copa | MultiRC | ReC   | RTE   | WiC  | WSC  |\\n|------------------------------|-------|------|------|---------|-------|-------|------|------|\\n| # Pre-train steps            |       |      |      |         |       |       |      |      |\\n| T5.1.1                       |       |      |      |         |       |       |      |      |\\n| 20k                          | 77.9  | 93.0 | 69.0 | 73.0    | 73.3  | 77.6  | 69.6 | 79.8 |\\n| 50k                          | 82.3  | 100 | 74.0 | 76.8    | 79.9  | 82.3  | 70.4 | 83.7 |\\n| 100k                         | 83.7  | 95.0 | 82.0 | 80.0    | 83.8  | 87.0  | 73.7 | 84.6 |\\n| 200k                         | 85.7  | 100 | 85.0 | 81.8    | 85.2  | 87.7  | 73.5 | 88.5 |\\n| ExT5                         |       |      |      |         |       |       |      |      |\\n| 20k                          | 80.3  | 95.0 | 70.0 | 74.4    | 72.9  | 82.7  | 68.5 | 81.7 |\\n| 50k                          | 83.1  | 97.4 | 78.0 | 79.2    | 79.6  | 88.1  | 73.4 | 87.5 |\\n| 100k                         | 85.3  | 100 | 81.0 | 81.6    | 83.7  | 89.2  | 73.2 | 90.4 |\\n| 200k                         | 86.5  | 98.7 | 86.0 | 83.2    | 85.4  | 91.7  | 73.4 | 93.3 |\\n\\nTable 16: Full SuperGLUE results from \u00a72.6\"}"}
{"id": "Vzh1BFUCiIX", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nDespite the recent success of multi-task learning and transfer learning for natural language processing (NLP), few works have systematically studied the effect of scaling up the number of tasks during pre-training. Towards this goal, this paper introduces ExtremeMix (ExMix): a massive collection of 10^7 supervised NLP tasks across diverse domains and task-families. Using ExMix, we study the effect of multi-task pre-training at the largest scale to date, and analyze co-training transfer amongst common families of tasks. Through this analysis, we show that manually curating an ideal set of tasks for multi-task pre-training is not straightforward, and that multi-task scaling can vastly improve models on its own. Finally, we propose ExT5: a model pre-trained using a multi-task objective of self-supervised span denoising and supervised ExMix. Via extensive experiments, we show that ExT5 outperforms strong T5 baselines on SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of ExMix. ExT5 also significantly improves sample efficiency while pre-training.\\n\\nIntroduction\\n\\nTransfer learning (Schmidhuber, 1987; Pratt et al., 1991; Caruana et al., 1995) has been the cornerstone of recent progress in natural language processing (Ruder et al., 2019; Devlin et al., 2019; Raffel et al., 2020). While self-supervised pre-training has been shown to be highly effective at exploiting large amounts of unlabeled data without relying on human annotation, there is still much to explore regarding transfer learning in a multi-task co-training setup.\\n\\nPrior seminal works like T5 (Raffel et al., 2020) and MT-DNN (Liu et al., 2019a) have demonstrated a degree of promise in the paradigm of multi-task co-training (Caruana, 1997). However, the challenge of catastrophic forgetting remains. Tasks often have to be carefully selected in order to demonstrate positive affinity with regards to downstream transfer. In many cases, it is not unreasonable to expect negative transfer (Rosenstein et al., 2005; Vu et al., 2020). This makes the process of empirically curating a set of tasks to include in a transfer learning setup both computationally prohibitive and specific to downstream tasks.\\n\\nWhile standard pre-training typically employs a variant of the self-supervised language modeling objective (Raffel et al., 2020), certain types of skills such as commonsense knowledge are only acquired at a slow rate even using massive amounts of unlabeled data (Zhang et al., 2021). As ever larger models are trained, the development of much more sample-efficient pre-training settings becomes thus more important, and could be addressed via multi-task learning.\\n\\nFor the first time, we explore and propose Extreme Multi-task Scaling\u2014a new paradigm for multi-task pre-training. Compared to the largest prior work (Aghajanyan et al., 2021), our study doubles the number of tasks and focuses on multi-task pre-training rather than fine-tuning, which enables a direct comparison to standard pre-training. Our proposal is based on the insight that despite negative transfer being common during fine-tuning, a massive and diverse collection of pre-training tasks is generally preferable to an expensive search for the best combination of pre-training tasks.\"}"}
{"id": "Vzh1BFUCiIX", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To this end, we introduce **EXMIX**: a massive collection of 107 supervised NLP tasks to be included in a multi-task pre-training setup. We process all tasks in an encoder-decoder friendly format to readily support the sharing of all parameters across all tasks. We postulate that an **ensembling** effect across as many tasks, distributions and domains as possible results in a consistently net-positive outcome. This echoes early multi-task learning results (Caruana, 1997; Baxter, 2000) indicating that a bias that is learned on sufficiently many tasks is likely to generalize to unseen tasks drawn from the same environment. Moreover, our experiments verify that our **EXMIX** mixture outperforms a best-effort mixture of manually curated tasks. Finally, we propose **EX-T5**: a T5 model (Raffel et al., 2020) pre-trained on a mixture of supervised **EXMIX** and self-supervised C4 span denoising. **EX-T5** outperforms state-of-the-art T5 models on well-established benchmarks such as SuperGLUE (Wang et al., 2019a), GEM (Gehrmann et al., 2021), and Rainbow (Lourie et al., 2021); as well as Closed-Book QA (Roberts et al., 2020) tasks. Notably, our experimental findings also suggest that including **EXMIX** may reduce the number of pre-training steps required to achieve strong performance, bringing about substantial sample efficiency benefits.\\n\\nTo summarize, this paper contributes the following:\\n\\n- We propose **EXMIX** (\u00a72): a collection of 107 supervised NLP tasks for **Extreme Multi-task Scaling**, formatted for encoder-decoder training. **EXMIX** has approximately twice as many tasks as the largest prior study to date (Aghajanyan et al., 2021), totaling 18M labeled examples across diverse task families.\\n- Given this large collection of tasks, we conduct rigorous empirical studies evaluating transfer between common task families (\u00a72.1). Our experiments show that curating a pre-training mixture based on fine-tuning transfer is not straightforward (\u00a72.2). Hence, efficiently searching for the best subset of tasks to include in a multi-task pre-training setup is challenging and prohibitive.\\n- Using **EXMIX**, we pre-train a model alongside the C4 span-denoising objective introduced by Raffel et al. (2020), resulting in a new pre-trained model which we call **EX-T5** (\u00a73). **EX-T5** outperforms state-of-the-art T5 on well-established benchmarks such as SuperGLUE, GEM, Rainbow, Closed Book Question Answering, and several other tasks that are outside of **EXMIX** (\u00a73.2), while also being more sample-efficient (\u00a72.6).\"}"}
{"id": "Vzh1BFUCiIX", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The pre-training process is expensive to run. Instead, we experiment with the less expensive co-training procedure (i.e., multi-task fine-tuning), using representative subsets of similar tasks. Later, in \u00a72.2, we explore whether these results can be used to inform the task selection for multi-task pre-training.\\n\\n| Task Family | Datasets |\\n|-------------|----------|\\n| Summarization | CNN/DailyMail, XSum, Wiki Lingua |\\n| Dialogue | Schema-guided dialogue, Wizard-of-Wikipedia, Dialoglue-TOP |\\n| NLI | ANLI, MNLI, \u03b1NLI |\\n| Classification | IMDb reviews, GoEmotions, Civil Comments |\\n| Semantic Parsing | ATIS to FunQL, GEO to FunQL, COGS |\\n| Commonsense | PhysicaliQA, SocialiQA, WinoGrande |\\n| Closed-Book QA | Natural Questions, Trivia QA, Hotpot QA |\\n| Reading Comprehension | SQuAD, BoolQ, TweetQA |\\n\\nTable 1: Representative datasets used for task-family transfer learning experiments (\u00a72.1).\\n\\nTo study transfer amongst task-families in $\\\\text{E}_\\\\text{X}M\\\\text{IX}$, we construct subsets (Table 1) of 3 tasks each that are partitioned along their task family. Using these subsets, we evaluate transfer among task families in a multi-task learning (co-training) setup. While other types of tasks are available in $\\\\text{E}_\\\\text{X}M\\\\text{IX}$, we did not include them because they were not diverse enough to be representative of a task family, and would scale the number of models needing to be trained at a quadratic rate.\\n\\nExperimental Setting\\n\\nWe fine-tune a model on each pair of task families (i.e., 6 datasets at a time). To ensure a fair balance of tasks, we sample tasks proportional within their family, but uniformly between task families. For example, while evaluating how classification tasks and NLI tasks transfer amongst each other, the sampling ratio of MNLI:ANLI will be proportional (approximately 2.4:1), but the overall ratio of NLI examples to classification examples will be 1:1. For reference, we also train a model on each individual task family using proportional example mixing (Sanh et al., 2019).\\n\\nIn total, this results in $F^2$ models trained, where $F$ is the number of task families. Our experiments use $F=8$ as shown in Table 1, resulting in 34 models trained in total. Each model is fine-tuned on top of the released T5.1.1 BASE checkpoint for 200k steps using a batch size of 128 and a constant learning rate of $10^{-3}$.\\n\\nObservations\\n\\nWe summarize our results in Table 2. We observe that although there exist particular task-family pairs that show positive transfer (e.g., co-training with NLI helps most other tasks), negative transfer is more common when training across task families compared to intra-family training. 21 out of the 56 inter-family relationships perform worse than intra-family models with the same data budget, which grows to 38 out of 56 for a fixed compute-budget. While the abundance of negative transfer among diverse task families is an expected result, interesting trends manifest in the individual relationships. For example, summarization tasks generally seem to hurt performance on most other task families; and CBQA tasks are highly sensitive to multi-task fine-tuning.\\n\\nWe also report correlations for intra-family datasets in Figure 2 using the same models as in Table 2. In most cases, we see positive correlations between datasets in the same family. In a few cases, however, we observe an opposite trend. For example, fine-tuned models that performed better on the GEM schema-guided dialog dataset achieved lower scores on KILT Wizard-of-Wikipedia.\\n\\nThis initial exploratory analysis highlights both the potential of $\\\\text{E}_\\\\text{X}M\\\\text{IX}$ as a tool to systematically study task relationships, as well as the potential challenges in leveraging multi-task learning naively on top of pre-trained representations.\\n\\n2.2 Can Fine-Tuning Task Relationships Help Curate a Pre-Training Mixture?\\n\\nOur observations in \u00a72.1 showed that multi-task co-training on top of existing pre-trained checkpoints is not straightforward, and often results in negative transfer. However, the uncovered task relationships might help efficiently search for an ideal subset of $\\\\text{E}_\\\\text{X}M\\\\text{IX}$ for multi-task pre-training.\\n\\nTo this end, we select a set of the most promising task families to be included in a multi-task pre-training setup, ranking task families by the average relative improvement they provide to other target families (the last column in Table 2). Specifically, we include NLI, commonsense, classification, and closed-book QA tasks from $\\\\text{E}_\\\\text{X}M\\\\text{IX}$ to form a mixture of 48 tasks to include in a multi-task pre-training setup.\"}"}
{"id": "Vzh1BFUCiIX", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Co-training transfer among task families. The entry at (row $i$, column $j$) of the table gives the task family $j$ that leads to a significant improvement in performance on family $i$, using models from Table 2. The last column denotes the average gain that a source family provides to other task families.\\n\\n| Task Family | $\\\\text{SUM DLG NLI CLS SEM CMNS CBQA RC}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text{GoE}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{COGS}$ | $\\\\text{SoQA}$ | $\\\\text{HoPo}$ | $\\\\text{MNLI}$ | $\\\\text{WinoGr}$ | $\\\\text{XSum}$ | $\\\\text{CNN-DM}$ | $\\\\text{SQuAD}$ | $\\\\text{PiQA}$ | $\\\\text{ANLI}$ | $\\\\text{ATIS}$ | $\\\\text{TOP}$ | $\\\\text{NQ}$ | $\\\\text{KILT WoW}$ | $\\\\text{GEM WLE}$ | $\\\\text{TweetQA}$ | $\\\\text{AbdNLI}$ | $\\\\text{IMDb}$ | $\\\\text"}
