{"id": "03RLpj-tc_", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "03RLpj-tc_", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alexander Frank Wells et al. Three dimensional nets and polyhedra. Wiley, 1977.\\n\\nJiaxiang Wu, Tao Shen, Haidong Lan, Yatao Bian, and Junzhou Huang. Se (3)-equivariant energy-based models for end-to-end protein folding. bioRxiv, 2021.\\n\\nTian Xie and Jeffrey C Grossman. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. Physical review letters, 120(14):145301, 2018.\\n\\nMinkai Xu, Shitong Luo, Yoshua Bengio, Jian Peng, and Jian Tang. Learning neural generative dynamics for molecular conformation generation. In International Conference on Learning Representations, 2021.\\n\\nMinkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In International Conference on Learning Representations, 2021.\\n\\nGuandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan. Pointflow: 3d point cloud generation with continuous normalizing flows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4541\u20134550, 2019.\\n\\nWenhui Yang, Edirisuriya M Dilanga Siriwardane, Rongzhi Dong, Yuxin Li, and Jianjun Hu. Crystal structure prediction of materials with high symmetry using differential evolution. arXiv preprint arXiv:2104.09764, 2021.\\n\\nYong Zhao, Mohammed Al-Fahdi, Ming Hu, Edirisuriya Siriwardane, Yuqi Song, Alireza Nasiri, and Jianjun Hu. High-throughput discovery of novel cubic crystal materials using deep generative neural networks. arXiv preprint arXiv:2102.01880, 2021.\\n\\nNils ER Zimmermann and Anubhav Jain. Local structure order parameters and site fingerprints for quantification of coordination environment and crystal structure similarity. RSC Advances, 10(10):6063\u20136081, 2020.\"}"}
{"id": "03RLpj-tc_", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We assume the loss in Equation 2 can be minimized to zero when the noises are small, meaning that\\n\\\\[ s_X(\\\\tilde{A}, \\\\tilde{X}, L|z) = \\\\min_{X} (X, \\\\tilde{X}) \\\\]\\nwhere \\\\( \\\\sigma_{X,j} \\\\in \\\\{ \\\\sigma_{X,j} \\\\}^L \\\\) and any noise smaller than \\\\( \\\\sigma_{X,J} \\\\) is considered as small.\\n\\nThe force term in the Langevin dynamics \\\\( \\\\alpha_j s_X(\\\\tilde{A}, \\\\tilde{X}, L|z; \\\\sigma_A,j, \\\\sigma_X,j) \\\\) can then be written as\\n\\\\[ = \\\\epsilon \\\\cdot \\\\frac{\\\\sigma_X^2}{\\\\sigma_X^2} \\\\frac{s_X(\\\\tilde{A}, \\\\tilde{X}, L|z)}{\\\\sigma_X,j} (\\\\forall j > J) \\\\]\\nIf we write \\\\( \\\\frac{\\\\epsilon}{\\\\sigma_X^2} = k \\\\), then,\\n\\\\[ = -k \\\\min_{X} (\\\\tilde{X}, X) \\\\]\\nIf the noises are small enough that atoms do not cross the periodic boundaries, then we have\\n\\\\[ \\\\min_{X} (X, \\\\tilde{X}) = X - \\\\tilde{X}. \\\\] Therefore,\\n\\\\[ = -k (\\\\tilde{X} - X) \\\\]\\n\\n**B.1 Prediction of Lattice Parameters**\\n\\nThere are infinitely many different ways of choosing the lattice for the same material. We compute the Niggli reduced lattice (Grosse-Kunstleve et al., 2004) with pymatgen (Ong et al., 2013), which is a unique lattice for any given material. Since the lattice matrix \\\\( L \\\\) is not rotation invariant, we instead predict the 6 lattice parameters, i.e. the lengths of the 3 lattice vectors and the angles between them. We normalize the lengths of lattice vectors with \\\\( \\\\frac{3}{\\\\sqrt{N}} \\\\), where \\\\( N \\\\) is the number of atoms, to ensure that the lengths for materials of different sizes are at the same scale.\\n\\n**B.2 Multi-Graph Construction**\\n\\nFor the encoder, we use CrystalNN (Pan et al., 2021) to determine edges between atoms and build a multi-graph representation. For the decoder, since it inputs a noisy structure generated on the fly, the multi-graph must also be built on the fly for both training and generation, and CrystalNN is too slow for that purpose. We use a KNN algorithm that considers periodicity to build the decoder graph where \\\\( K = 20 \\\\) in all of our experiments.\\n\\n**B.3 GNN Architecture**\\n\\nWe use DimeNet++ adapted for periodicity (Klicpera et al., 2020a;b) as the encoder, which is SE(3) invariant to the input structure. The decoder needs to output an vector per node that is SE(3) equivariant to the input structure. We use GemNet-dQ (Klicpera et al., 2021) as the decoder. We used implementations from the Open Catalysis Project (OCP) (Chanussot et al., 2021), but we reduced the size of hidden dimensions to 128 for faster training. The encoder has 2.2 million parameters and the decoder has 2.3 million parameters.\\n\\n**C.1 PROV-5**\\n\\nPerovskite is a class of materials that share a similar structure and have the general chemical formula \\\\( ABX_3 \\\\). The ideal perovskites have a cubic structure, where the site A atom sits at a corner position,\"}"}
{"id": "03RLpj-tc_", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the site B atom sits at a body centered position and site X atoms sit at face centered positions. Perovskite materials are known for their wide applications. We curate the Perov-5 dataset from an open database that was originally developed for water splitting (Castelli et al., 2012a;b). All 18928 materials in the original database are included. In the database, A, B can be any non-radioactive metal and X can be one or several elements from O, N, S, and F. Note that there can be multiple different X atoms in the same material. All materials in Perov-5 are relaxed using density functional theory (DFT), and their relaxed structure can deviate significantly from the ideal structures. A significant portion of the materials are not thermodynamically stable, i.e., they will decompose to nearby phases and cannot be synthesized.\\n\\nC.2 Carbon-24 includes various carbon structures obtained via ab initio random structure searching (AIRSS) (Pickard & Needs, 2006; 2011) performed at 10 GPa. The original dataset includes 101529 carbon structures, and we selected the 10% of the carbon structure with the lowest energy per atom to create Carbon-24. All 10153 structures in Carbon-24 are relaxed using DFT. The most stable structure is diamond at 10 GPa. All remaining structures are thermodynamically unstable but may be kinetically stable. Most of the structures cannot be synthesized.\\n\\nC.3 MP-20 includes almost all experimentally stable materials from the Materials Project (Jain et al., 2013) with unit cells including at most 20 atoms. We only include materials that are originally from ICSD (Belsky et al., 2002) to ensure the experimental stability, and these materials represent the majority of experimentally known materials with at most 20 atoms in unit cells. To ensure stability, we only select materials with energy above the hull smaller than 0.08 eV/atom and formation energy smaller than 2 eV/atom, following Ren et al. (2020). Differing from Ren et al. (2020), we do not constrain the number of unique elements per material. All materials in MP-20 are relaxed using DFT. Most materials are thermodynamically stable and have been synthesized.\\n\\nD.1 Reasons for the unsuitability of some metrics for specific datasets\\nIn Table 2, property statistics are computed by comparing the earth mover's distance between the property distribution of generated materials and ground truth materials. So, they are not meaningful for ground truth data. Materials in Perov-5 have the same structure, so it is not meaningful to require higher structure diversity. Materials in Carbon-24 have the same composition (carbon), so it is not meaningful to require higher composition diversity. In addition, all models have \\\\( \\\\sim 100\\\\% \\\\) composition validity, so it is not compared in the table.\\n\\nD.2 Composition validity checker\\nWe modified the charge neutrality checker from SMACT (Davies et al., 2019) because the original checker is not suitable for alloys. The checker is based on a list of possible charges for each element and it checks if the material can be charge neutral by enumerating all possible charge combinations. However, it does not consider that metal alloys can be mixed with almost any combination. As a result, for materials composed of all metal elements, we always assume the composition is valid in our validity checker. For the ground truth materials in MP-20, the original checker gives a composition validity of \\\\( \\\\sim 50\\\\% \\\\), which significantly underestimates the validity of MP-20 materials (because most of them are experimentally synthesizable and thus valid). Our checker gives a composition validity of \\\\( \\\\sim 90\\\\% \\\\), which is far more reasonable. We note again that these checkers are all empirical and the only high-fidelity evaluation of material stability requires QM simulations.\"}"}
{"id": "03RLpj-tc_", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Property optimization performance.\\n\\n| Method          | Perov-5 | Carbon-24 | MP-20 |\\n|-----------------|---------|-----------|-------|\\n|                 | SR5     | SR10      | SR15  |\\n| FTCP            | 0.06    | 0.11      | 0.16  |\\n| Cond-DFC-V AE   | 0.55    | 0.64      | 0.69  |\\n| CDV AE          | 0.52    | 0.65      | 0.79  |\\n\\nResults. The generated structures are shown in Figure 4 and the metrics are in Table 2. Our model achieves a higher validity than FTCP, Cond-DFC-V AE, and P-G-SchNet, while G-SchNet achieves a similar validity as ours. The lower structural validity in P-G-SchNet than G-SchNet is likely due to the difficulty of avoiding atom collisions during the autoregressive generation inside a finite periodic box. On the contrary, our G-SchNet baseline constructs the lattice box after the 3D positions of all atoms are generated, and the construction explicitly avoids introducing invalidity. Furthermore, our model also achieves higher COV-R and COV-P than all other models, except in MP-20 our COV-P is similar to G-SchNet and P-G-SchNet. These results indicate that our model generates both diverse (COV-R) and high quality (COV-P) materials. More detailed results on the choice of thresholds for COV-R and COV-P, as well as additional metrics can be found in Appendix G. Finally, our model also significantly outperforms all other models in the property statistics of density and energy, further confirming the high quality of generated materials. We observe that our method tends to generate more elements in a material than ground truth, which explains the lower performance in the statistics of # of elems. than G-SchNet. We hypothesize this is due to the non-Gaussian statistical structure of ground truth materials (details in Appendix D.3), and using a more complex prior, e.g., a flow-model-transformed Gaussian (Yang et al., 2019), might resolve this issue.\\n\\n5.3 Property Optimization\\n\\nSetup. The third task is to generate materials that optimize a specific property. Following Jin et al. (2018), we jointly train a property predictor $F$ parameterized by an MLP to predict properties of training materials from latent $z$. To optimize properties, we start with the latent representations of testing materials and apply gradient ascent in the latent space to improve the predicted property $F(\\\\cdot)$. After applying 5000 gradient steps with step sizes of $10^{-3}$, 10 materials are decoded from the latent trajectories every 500 steps. We use an independently trained property predictor to select the best one from the 10 decoded materials. Cond-DFC-V AE is a conditional V AE so we directly condition on the target property, sample 10 materials, and select the best one using the property predictor. For all methods, we generate 100 materials following the protocol above. We use the independent property predictor to predict the properties for evaluation. We report the success rate (SR) as the percentage of materials achieving 5, 10, and 15 percentiles of the target property distribution. Our task is to minimize formation energy per atom for all 3 datasets.\\n\\nResults. The performance is shown in Table 3. We significantly outperform FTCP, while having a similar performance as Cond-DFC-V AE in Perov-5 (Cond-DFC-V AE cannot work for Carbon-24 and MP-20). Both G-SchNet and P-G-SchNet are incapable of property optimization. We note that all models perform poorly on the Carbon-24 dataset, which might be explained by the complex and diverse 3D structures of carbon.\\n\\n6 Conclusions and Outlook\\n\\nWe have introduced a Crystal Diffusion Variational Autoencoder (CDV AE) to generate the periodic structure of stable materials and demonstrated that it significantly outperforms past methods on the tasks of reconstruction, generation, and property optimization. We note that the last two tasks are far more important for material design than reconstruction because they can be directly used to generate new materials whose properties can then be verified by QM simulations and experiments. We believe CDV AE opens up exciting opportunities for the inverse design of materials for various important applications. Meanwhile, our model is just a first step towards the grand challenge of material design. We provide our datasets and evaluation metrics to the broader machine learning community to collectively develop better methods for the task of material generation.\\n\\nVery recently the authors published an improved version for conditional generation (Gebauer et al., 2021) but the code has not been released yet.\"}"}
{"id": "03RLpj-tc_", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We have made the following efforts to ensure reproducibility: 1) We provide our code at https://github.com/txie-93/cdvae; 2) We provide our data and corresponding train/validation/test splits at https://github.com/txie-93/cdvae/tree/main/data; 3) We provide details on experimental configurations in Appendix D.\\n\\nACKNOWLEDGMENTS\\n\\nWe thank Peter Mikhael, Jason Yim, Rachel Wu, Bracha Laufer, Gabriele Corso, Felix Faltings, Bowen Jing, and the rest of the RB and TJ group members for their helpful comments and suggestions. The authors gratefully thank DARPA (HR00111920025), the consortium Machine Learning for Pharmaceutical Discovery and Synthesis (mlpds.mit.edu), and MIT-GIST collaboration for support.\\n\\nREFERENCES\\n\\nAlec Belsky, Mariette Hellenbrandt, Vicky Lynn Karen, and Peter Luksch. New developments in the inorganic crystal structure database (icsd): accessibility in support of materials research and design. Acta Crystallographica Section B: Structural Science, 58(3):364\u2013369, 2002. 1, 16\\n\\nKeith T Butler, Daniel W Davies, Hugh Cartwright, Olexandr Isayev, and Aron Walsh. Machine learning for molecular and materials science. Nature, 559(7715):547\u2013555, 2018. 1\\n\\nRuojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and Bharath Hariharan. Learning gradient fields for shape generation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16, pp. 364\u2013381. Springer, 2020. 4\\n\\nIvano E Castelli, David D Landis, Kristian S Thygesen, S\u00f8ren Dahl, Ib Chorkendorff, Thomas F Jaramillo, and Karsten W Jacobsen. New cubic perovskites for one-and two-photon water splitting using the computational materials repository. Energy & Environmental Science, 5(10):9034\u20139043, 2012a. 6, 16\\n\\nIvano E Castelli, Thomas Olsen, Soumendu Datta, David D Landis, S\u00f8ren Dahl, Kristian S Thygesen, and Karsten W Jacobsen. Computational screening of perovskite metal oxides for optimal solar light capture. Energy & Environmental Science, 5(2):5814\u20135819, 2012b. 6, 16\\n\\nLowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, et al. Open catalyst 2020 (oc20) dataset and community challenges. ACS Catalysis, 11(10):6059\u20136072, 2021. 2, 15\\n\\nChi Chen and Shyue Ping Ong. A universal graph deep learning interatomic potential for the periodic table. arXiv preprint arXiv:2202.02450, 2022. 3\\n\\nChi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, and Shyue Ping Ong. Graph networks as a universal machine learning framework for molecules and crystals. Chemistry of Materials, 31(9):3564\u20133572, 2019. 2\\n\\nCallum J Court, Batuhan Yildirim, Apoorv Jain, and Jacqueline M Cole. 3-d inorganic crystal structure generation and property prediction via representation learning. Journal of chemical information and modeling, 60(10):4518\u20134535, 2020. 2, 6, 7, 8\\n\\nYabo Dan, Yong Zhao, Xiang Li, Shaobo Li, Ming Hu, and Jianjun Hu. Generative adversarial networks (gan) based efficient sampling of chemical composition space for inverse design of inorganic materials. npj Computational Materials, 6(1):1\u20137, 2020. 3\\n\\nDaniel W Davies, Keith T Butler, Adam J Jackson, Jonathan M Skelton, Kazuki Morita, and Aron Walsh. Smact: Semiconducting materials by analogy and chemical theory. Journal of Open Source Software, 4(38):1361, 2019. 8, 16\\n\\nVolker L Deringer, Chris J Pickard, and Gabor Csanyi. Data-driven learning of total and local energies in elemental boron. Physical review letters, 120(15):156001, 2018. 3\"}"}
{"id": "03RLpj-tc_", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "03RLpj-tc_", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "03RLpj-tc_", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nGenerating the periodic structure of stable materials is a long-standing challenge for the material design community. This task is difficult because stable materials only exist in a low-dimensional subspace of all possible periodic arrangements of atoms: 1) the coordinates must lie in the local energy minimum defined by quantum mechanics, and 2) global stability also requires the structure to follow the complex, yet specific bonding preferences between different atom types. Existing methods fail to incorporate these factors and often lack proper invariances. We propose a Crystal Diffusion Variational Autoencoder (CDV AE) that captures the physical inductive bias of material stability. By learning from the data distribution of stable materials, the decoder generates materials in a diffusion process that moves atomic coordinates towards a lower energy state and updates atom types to satisfy bonding preferences between neighbors. Our model also explicitly encodes interactions across periodic boundaries and respects permutation, translation, rotation, and periodic invariances. We significantly outperform past methods in three tasks: 1) reconstructing the input structure, 2) generating valid, diverse, and realistic materials, and 3) generating materials that optimize a specific property. We also provide several standard datasets and evaluation metrics for the broader machine learning community.\\n\\nIntroduction\\n\\nSolid state materials, represented by the periodic arrangement of atoms in the 3D space, are the foundation of many key technologies including solar cells, batteries, and catalysis (Butler et al., 2018). Despite the rapid progress of molecular generative models and their significant impact on drug discovery, the problem of material generation has many unique challenges. Compared with small molecules, materials have more complex periodic 3D structures and cannot be adequately represented by a simple graph like molecular graphs (Figure 1). In addition, materials can be made up of more than 100 elements in the periodic table, while molecules are generally only made up of a small subset of atoms such as carbon, oxygen, and hydrogen. Finally, the data for training ML models for material design is limited. There are only $\\\\sim 200k$ experimentally known inorganic materials, collected by the ICSD (Belsky et al., 2002), in contrast to close to a billion molecules in ZINC (Irwin & Shoichet, 2005).\\n\\nFigure 1: The periodic structure of diamond. The left shows the infinite periodic structure, the middle shows a unit cell representing the periodic structure, and the right shows a multi-graph (Xie & Grossman, 2018) representation.\"}"}
{"id": "03RLpj-tc_", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"bonding preferences between different atom types (section 3.2). The issue of stability is unique to material generation because valency checkers assessing molecular stability are not applicable to materials. Moreover, we also have to encode the interactions crossing periodic boundaries (Figure 1, middle), and satisfy permutation, translation, rotation, and periodic invariances (section 3.1). Our goal is to learn representations that can learn features of stable materials from data, while adhering to the above invariance properties.\\n\\nWe address these challenges by learning a variational autoencoder (VAE) (Kingma & Welling, 2014) to generate stable 3D materials directly from a latent representation without intermediates like graphs. The key insight is to exploit the fact that all materials in the data distribution are stable, therefore if noise is added to the ground truth structure, denoising it back to its original structure will likely increase stability. We capture this insight by designing a noise conditional score network (NCSN) (Song & Ermon, 2019) as our decoder: 1) the decoder outputs gradients that drive the atom coordinates to the energy local minimum; 2) it also updates atom types based on the neighbors to capture the specific local bonding preferences (e.g., Si-O is preferred over Si-Si and O-O in SiO$_2$). During generation, materials are generated using Langevin dynamics that gradually deforms an initial random structure to a stable structure. To capture the necessary invariances and encode the interactions crossing periodic boundaries, we use SE(3) equivariant graph neural networks adapted with periodicity (PGNNs) for both the encoder and decoder of our VAE.\\n\\nOur theoretical analysis further reveals an intriguing connection between the gradient field learned by our decoder and an harmonic force field. De facto, the decoder utilizes the latter to estimate the forces on atoms when their coordinates deviate from the equilibrium positions. Consequently, this formulation provides an important physical inductive bias for generating stable materials.\\n\\nIn this work, we propose Crystal Diffusion Variational AutoEncoder (CDVAE) to generate stable materials by learning from the data distribution of known materials. Our main contributions include:\\n\\n\u2022 We curate 3 standard datasets from QM simulations and create a set of physically meaningful tasks and metrics for the problem of material generation.\\n\u2022 We incorporate stability as an inductive bias by designing a noise conditional score network as the decoder of our VAE, which allows us to generate significantly more realistic materials.\\n\u2022 We encode permutation, translation, rotation, and periodic invariances, as well as interactions crossing periodic boundaries with SE(3) equivariant GNNs adapted with periodicity.\\n\u2022 Empirically, our model significantly outperforms past methods in tasks including reconstructing an input structure, generating valid, diverse, and realistic materials, and generating materials that optimize specific properties.\\n\\nRELATED WORK\\n\\nMaterial graph representation learning. Graph neural networks have made major impacts in material property prediction. They were first applied to the representation learning of periodic materials by Xie & Grossman (2018) and later enhanced by many studies including Sch\u00fctt et al. (2018); Chen et al. (2019). The Open Catalyst Project (OCP) provides a platform for comparing different architectures by predicting energies and forces from the periodic structure of catalytic surfaces (Chanussot et al., 2021). Our encoder and decoder PGNNs directly use GNN architectures developed for the OCP (Klicpera et al., 2020b; 2021; Shuaibi et al., 2021; Godwin et al., 2021), which are also closely related to SE(3) equivariant networks (Thomas et al., 2018; Fuchs et al., 2020).\\n\\nQuantum mechanical search of stable materials. Predicting the structure of unknown materials requires very expensive random search and QM simulations, and is considered a grand challenge in materials discovery (Oganov et al., 2019). State-of-the-art methods include random sampling (Pickard & Needs, 2011), evolutionary algorithms (Wang et al., 2012; Glass et al., 2006), substituting elements in known materials (Hautier et al., 2011), etc., but they generally have low success rates and require extensive computation even on relatively small problems.\\n\\nMaterial generative models. Past material generative models mainly focus on two different approaches, and neither incorporate stability as an inductive bias. The first approach treats materials as 3D voxel images, but the process of decoding images back to atom types and coordinates often results in low validity, and the models are not rotationally invariant (Hoffmann et al., 2019; Noh et al., 2019; Court et al., 2020; Long et al., 2021). The second directly encodes atom coordinates, but this approach is challenging due to the high-dimensional space of atomic configurations.\"}"}
{"id": "03RLpj-tc_", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"types, and lattices as vectors (Ren et al., 2020; Kim et al., 2020; Zhao et al., 2021), but the models are generally not invariant to any Euclidean transformations. Another related method is to train a force field from QM forces and then apply the learned force field to generate stable materials by minimizing energy (Deringer et al., 2018; Chen & Ong, 2022). This method is conceptually similar to our decoder, but it requires additional force data which is expensive to obtain. Remotely related works include generating contact maps from chemical compositions (Hu et al., 2021; Yang et al., 2021) and building generative models only for chemical compositions (Sawada et al., 2019; Pathak et al., 2020; Dan et al., 2020).\\n\\nMolecular conformer generation and protein folding. Our decoder that generates the 3D atomic structures via a diffusion process is closely related to the diffusion models used for molecular conformer generation (Shi et al., 2021; Xu et al., 2021b). The key difference is that our model does not rely on intermediate representations like molecular graphs. G-SchNet (Gebauer et al., 2019) is more closely related to our method because it directly generates 3D molecules atom-by-atom without relying on a graph. Another closely related work is E-NFs (Satorras et al., 2021) that use a flow model to generate 3D molecules. In addition, score-based and energy-based models have also been used for molecular graph generation (Liu et al., 2021) and protein folding (Wu et al., 2021). Flow models have also been used for molecular graph generation (Shi et al., 2020; Luo et al., 2021). However, these generative models do not incorporate periodicity, which makes them unsuitable for materials.\\n\\n3 P RELIMINARIES\\n\\n3.1 PERIODIC STRUCTURE OF MATERIALS\\n\\nAny material structure can be represented as the periodic arrangement of atoms in the 3D space. As illustrated in Figure 1, we can always find a repeating unit, i.e. a unit cell, to describe the infinite periodic structure of a material. A unit cell that includes $N$ atoms can be fully described by 3 lists: 1) atom types $A = (a_0, ..., a_N) \\\\in A^N$, where $A$ denotes the set of all chemical elements; 2) atom coordinates $X = (x_0, ..., x_N) \\\\in \\\\mathbb{R}^{N \\\\times 3}$; and 3) periodic lattice $L = (l_1, l_2, l_3) \\\\in \\\\mathbb{R}^{3 \\\\times 3}$. The periodic lattice defines the periodic translation symmetry of the material. Given $M = (A, X, L)$, the infinite periodic structure can be represented as,\\n\\n$$\\\\{ (a'_i, x'_i) | a'_i = a_i, x'_i = x_i + k_1 l_1 + k_2 l_2 + k_3 l_3, k_1, k_2, k_3 \\\\in \\\\mathbb{Z} \\\\},$$\\n\\nwhere $k_1, k_2, k_3$ are any integers that translate the unit cell using $L$ to tile the entire 3D space.\\n\\nThe chemical composition of a material denotes the ratio of different elements that the material is composed of. Given the atom types of a material with $N$ atoms $A \\\\in A_N$, the composition can be represented as $c \\\\in \\\\mathbb{R} | A |$, where $c_i > 0$ denotes the percentage of atom type $i$ and $\\\\sum_i c_i = 1$. For example, the composition of diamond in Figure 1 has $c_6 = 1$ and $c_i = 0$ for $i \\\\neq 6$ because 6 is the atomic number of carbon.\\n\\nInvariances for materials. The structure of a material does not change under several invariances. 1) Permutation invariance. Exchanging the indices of any pair of atoms will not change the material. 2) Translation invariance. Translating the atom coordinates $X$ by an arbitrary vector will not change the material. 3) Rotation invariance. Rotating $X$ and $L$ together by an arbitrary rotation matrix will not change the material. 4) Periodic invariance. There are infinite different ways of choosing unit cells with different shapes and sizes, e.g., obtaining a bigger unit cell as an integer multiplier of a smaller unit cell using integer translations. The material will again not change given different choices of unit cells.\\n\\nMulti-graph representation for materials. Materials can be represented as a directed multi-graph $G = \\\\{V, E\\\\}$ to encode the periodic structures following (Wells et al., 1977; O'Keeffe & Hyde, 1980; Xie & Grossman, 2018), where $V = \\\\{v_1, ..., v_N\\\\}$ is the set of nodes representing atoms and $E = \\\\{e_{ij}, (k_1, k_2, k_3) | i, j \\\\in \\\\{1, ..., N\\\\}, k_1, k_2, k_3 \\\\in \\\\mathbb{Z} \\\\}$ is the set of edges representing bonds. $e_{ij}, (k_1, k_2, k_3)$ denotes a directed edge from node $i$ at the original unit cell to node $j$ at the cell translated by $k_1 l_1 + k_2 l_2 + k_3 l_3$ (in Figure 1 right, $(k_1, k_2, k_3)$ are labeled on top of edges). For materials, there is no unique way to define edges (bonds) and the edges are often computed using K-nearest neighbor (KNN) approaches under periodicity or more advanced methods such as CrystalNN (Pan et al., 2021). Given this directed multi-graph, message-passing neural networks and SE(3)-equivariant networks can be used for the representation learning of materials.\"}"}
{"id": "03RLpj-tc_", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Overview of the proposed CDV AE approach.\\n\\n3.2 PROBLEM DEFINITION AND ITS PHYSICAL ORIGIN\\n\\nOur goal is to generate novel, stable materials $M = (A, X, L) \\\\in A \\\\times \\\\mathbb{R}^N \\\\times \\\\mathbb{R}^3 \\\\times \\\\mathbb{R}^3 \\\\times \\\\mathbb{R}^3$. The space of stable materials is a subspace in $A \\\\times \\\\mathbb{R}^N \\\\times \\\\mathbb{R}^3 \\\\times \\\\mathbb{R}^3 \\\\times \\\\mathbb{R}^3$ that satisfies the following constraints. 1) The materials lie in the local minimum of the energy landscape defined by quantum mechanics, with respect to the atom coordinates and lattice, i.e. $\\\\frac{\\\\partial E}{\\\\partial X} = 0$ and $\\\\frac{\\\\partial E}{\\\\partial L} = 0$. 2) The material is globally stable and thus cannot decompose into nearby phases. Global stability is strongly related to bonding preferences between neighboring atoms. For example, in SiO$_2$, each Si is surrounded by 4 O and each O is surrounded by 2 Si. This configuration is caused by the stronger bonding preferences between Si-O than Si-Si and O-O.\\n\\nGenerally, finding novel, stable materials requires very expensive random search and quantum mechanical simulations. To bypass this challenge, we aim to learn a generative model $p(M)$ from the empirical distribution of experimentally observed stable materials. A successful generative model will be able to generate novel materials that satisfy the above constraints, which can then be verified using quantum mechanical simulations.\\n\\n3.3 DIFFUSION MODELS\\n\\nDiffusion models are a new class of generative models that have recently shown great success in generating high-quality images (Dhariwal & Nichol, 2021), point clouds (Cai et al., 2020; Luo & Hu, 2021), and molecular conformations (Shi et al., 2021). There are several different types of diffusion models including diffusion probabilistic models (Sohl-Dickstein et al., 2015), noise-conditioned score networks (NCSN) (Song & Ermon, 2019), and denoising diffusion probabilistic models (DDPM) (Ho et al., 2020). We follow ideas from the NCSN (Song & Ermon, 2019) and learn a score network $s_\\\\theta(x)$ to approximate the gradient of a probability density $\\\\nabla_x p(x)$ at different noise levels. Let $\\\\{\\\\sigma_i\\\\}_{L_i=1}^{L}$ be a sequence of positive scalars that satisfies $\\\\sigma_1/\\\\sigma_2 = \\\\ldots = \\\\sigma_{L-1}/\\\\sigma_L > 1$. We define the data distribution perturbed by Gaussian noise $\\\\sigma$ as $q_\\\\sigma(x) = \\\\int p_{\\\\text{data}}(t) N(x|t,\\\\sigma^2 I) \\\\, dt$.\\n\\nThe goal of NCSN is to learn a score network to jointly estimate the scores of all perturbed data distributions, i.e. $\\\\forall \\\\sigma \\\\in \\\\{\\\\sigma_i\\\\}_{L_i=1}^{L}$: $s_\\\\theta(x,\\\\sigma) \\\\approx \\\\nabla_x q_\\\\sigma(x)$. During generation, NCSN uses an annealed Langevin dynamics algorithm to produce samples following the gradient estimated by the score network with a gradually reduced noise level.\\n\\n4 PROPOSED METHOD\\n\\nOur approach generates new materials via a two-step process: 1) We sample a $z$ from the latent space and use it to predict 3 aggregated properties of a material: composition ($c$), lattice ($L$), and number of atoms ($N$), which are then used to randomly initialize a material structure $\\\\tilde{M} = (\\\\tilde{A}, \\\\tilde{X}, L)$. 2) We perform Langevin dynamics to simultaneously denoise $\\\\tilde{X}$ and $\\\\tilde{A}$ conditioned on $z$ to improve both the local and global stability of $\\\\tilde{M}$ and generate the final structure of the new material.\\n\\nTo train our model, we optimize 3 networks concurrently using stable materials $M = (A, X, L)$ sampled from the data distribution. 1) A periodic GNN encoder $\\\\text{PGNN}^E_{\\\\text{NC}}(M)$ that encodes $M$ into a latent representation $z$. 2) A property predictor $\\\\text{MLP}^A_{\\\\text{GG}}(z)$ that predicts the $c$, $L$, and $N$ of $M$ from $z$. 3) A periodic GNN decoder $\\\\text{PGNN}^D_{\\\\text{EC}}(\\\\tilde{M}|z)$ that denoises both $\\\\tilde{X}$ and $\\\\tilde{A}$ conditioned on $z$. 4\"}"}
{"id": "03RLpj-tc_", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For 3), the noisy structure $\\\\tilde{M} = (\\\\tilde{A}, \\\\tilde{X}, L)$ is obtained by adding different levels of noise to $X$ and $A$. The noise schedules are defined by the predicted aggregated properties, with the motivation of simplifying the task for our decoder from denoising an arbitrary random structure from over $\\\\sim 100$ elements to a constrained random structure from predicted properties. We train all three networks together by minimizing a combined loss including the aggregated property loss $L_{AGG}$, decoder denoising loss $L_{DEC}$, and a KL divergence loss $L_{KL}$ for the VAE.\\n\\nTo capture the interactions across periodic boundaries, we employ a multi-graph representation (section 3.1) for both $M$ and $\\\\tilde{M}$. We also use SE(3) equivariant GNNs adapted with periodicity as both the encoder and the decoder to ensure the permutation, translation, rotation, and periodic invariances of our model. The CDV AE is summarized in Figure 2 and we explain the individual components of our method below. The implementation details can be found in Appendix B.\\n\\n**Periodic material encoder.**\\n\\n$\\\\text{PGNN}^{E}_{NC}(M)$ encodes a material $M$ as a latent representation $z \\\\in \\\\mathbb{R}^D$ following the reparameterization trick in VAE (Kingma & Welling, 2014). We use the multi-graph representation (refer to section 3.1) to encode $M$, and $\\\\text{PGNN}^{E}_{NC}$ can be parameterized with an SE(3) invariant graph neural network.\\n\\n**Prediction of aggregated properties.**\\n\\n$\\\\text{MLP}^{AGG}(z)$ predicts 3 aggregated properties of the encoded material from its latent representation $z$. It is parameterized by 3 separate multilayer perceptrons (MLPs). 1) Composition $c \\\\in \\\\mathbb{R}^{|A|}$ is predicted by minimizing the cross entropy between the ground truth composition and predicted composition, i.e. $- \\\\sum p_i \\\\log c_i$. 2) Lattice $L \\\\in \\\\mathbb{R}^{3 \\\\times 3}$ is reduced to 6 unique, rotation invariant parameters with the Niggli algorithm (Grosse-Kunstleve et al., 2004), i.e., the lengths of the 3 lattice vectors, the angles between them, and the values are predicted with an MLP after being normalized to the same scale (Appendix B.1) with an $L_2$ loss. 3) Number of atoms $N \\\\in \\\\{1, 2, \\\\ldots\\\\}$ is predicted with a softmax classification loss from the set of possible number of atoms. $L_{AGG}$ is a weighted sum of the above 3 losses.\\n\\n**Conditional score matching decoder.**\\n\\n$\\\\text{PGNN}^{D}_{EC}(\\\\tilde{M} | z)$ is a PGNN that inputs a noisy material $\\\\tilde{M}$ with type noises $\\\\sigma_A$, coordinate noises $\\\\sigma_X$, as well as a latent $z$, and outputs 1) a score $s_X(\\\\tilde{M} | z; \\\\sigma_A, \\\\sigma_X) \\\\in \\\\mathbb{R}^{N \\\\times 3}$ to denoise the coordinate for each atom towards its ground truth value, and 2) a probability distribution of the true atom types $p_A(\\\\tilde{M} | z; \\\\sigma_A, \\\\sigma_X) \\\\in \\\\mathbb{R}^{N \\\\times |A|}$. We use a SE(3) graph network to ensure the equivariance of $s_X$ with respect to the rotation of $\\\\tilde{M}$. To obtain the noisy structures $\\\\tilde{M}$, we sample $\\\\sigma_A$ and $\\\\sigma_X$ from two geometric sequences of the same length: $\\\\{\\\\sigma_A, j\\\\}_{L_{j=1}}$, $\\\\{\\\\sigma_X, j\\\\}_{L_{j=1}}$, and add the noises with the following methods. For type noises, we use the type distribution defined by the predicted composition $c$ to linearly perturb true type distribution $\\\\tilde{A} \\\\sim (1 + \\\\sigma_A p_A + \\\\sigma_A 1 + \\\\sigma_A p_c)$, where $p_A,ij = 1$ if atom $i$ has the true atom type $j$ and $p_A,ij = 0$ for all other $j$s, and $p_c$ is the predicted composition. For coordinate noises, we add Gaussian noises to the true coordinates $\\\\tilde{X} \\\\sim N(X, \\\\sigma_X I)$.\\n\\n$\\\\text{PGNN}^{D}_{EC}$ is parameterized by a SE(3) equivariant PGNN that inputs a multi-graph representation (section 3.1) of the noisy material structure and the latent representation. The node embedding for node $i$ is obtained by the concatenation of the element embedding of $\\\\tilde{a}_i$ and the latent representation $z$, followed by a MLP, $h_0i = \\\\text{MLP}(e_a(\\\\tilde{a}_i) \\\\parallel z)$, where $\\\\parallel$ denotes concatenation of two vectors and $e_a$ is a learned embedding for elements. After $K$ message-passing layers, $\\\\text{PGNN}^{D}_{EC}$ outputs a vector per node that is equivariant to the rotation of $\\\\tilde{M}$. These vectors are used to predict the scores, and we follow Song & Ermon (2019); Shi et al. (2021) to parameterize the score network with noise scaling: $s_X(\\\\tilde{M} | z; \\\\sigma_A, \\\\sigma_X) = s_X(\\\\tilde{M} | z) / \\\\sigma_X$. The node representations $h_K$ are used to predict the distribution of true atom types, and the type predictor is the same at all noise levels: $p_A(\\\\tilde{M} | z; \\\\sigma_A, \\\\sigma_X) = p_A(\\\\tilde{M} | z)$, $p_A(\\\\tilde{M} | z)_{ij} = \\\\text{softmax}(\\\\text{MLP}(h_Ki))$.\\n\\n**Periodicity influences denoising target.**\\n\\nDue to periodicity, a specific atom $i$ may move out of the unit cell defined by $L$ when the noise is sufficiently large. This leads to two different ways to define the scores for node $i$.\\n\\n1) Ignore periodicity and define the target score as $x_i - \\\\tilde{x}_i$; or\\n2) Define the target score as the shortest possible displacement between $x_i$ and $\\\\tilde{x}_i$ considering periodicity, i.e. $d_{min}(x_i, \\\\tilde{x}_i) = \\\\min_{k_1, k_2, k_3} (x_i - \\\\tilde{x}_i + k_1 l_1 + k_2 l_2 + k_3 l_3)$.\\n\\nWe choose 2) because the scores are the same given two different $\\\\tilde{X}$ that are periodically equivalent, which is mathematically grounded for periodic structures, and empirically results in much more stable training.\"}"}
{"id": "03RLpj-tc_", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The training loss for the decoder $L_D$ can be written as,\\n\\n$$L_D = \\\\frac{1}{2} \\\\sum_{j=1}^{L} \\\\left[ \\\\| E_{q,\\\\text{data}}(M) - E_{q,\\\\sigma_A,j,\\\\sigma_X,j}(\\\\tilde{M}|M) \\\\|_2^2 + \\\\lambda_a \\\\sigma_{A,j} L_a(p_{A}(\\\\tilde{M}|z), p_A) \\\\right],$$\\n\\nwhere $\\\\lambda_a$ denotes a coefficient for balancing the coordinate and type losses, $L_a$ denotes the cross entropy loss over atom types, $p_A$ denotes the true atom type distribution. Note that to simplify the equation, we follow the loss coefficients in Song & Ermon (2019) for different $\\\\sigma_X,j$ and $\\\\sigma_A,j$ and factor them into Equation 2.\\n\\n**Algorithm 1**\\n\\n**Material Generation via Annealed Langevin Dynamics**\\n\\n1: Input: latent representation $z$, type and coordinate noise levels $\\\\{\\\\sigma_A\\\\}$, $\\\\{\\\\sigma_X\\\\}$, step size $\\\\epsilon$, number of sampling steps $T$\\n\\n2: Predict aggregated properties $c$, $L$, $N$ from $z$.\\n\\n3: Uniformly initialize $X_0$ within the unit cell by $L$.\\n\\n4: Randomly initialize $A_0$ with $c$.\\n\\n5: for $j \\\\leftarrow 1$ to $L$ do\\n\\n6: $\\\\alpha_j \\\\leftarrow \\\\epsilon \\\\cdot \\\\sigma_{X,j}^2 / \\\\sigma_{X,L}^2$\\n\\n7: for $t \\\\leftarrow 1$ to $T$ do\\n\\n8: $s_{X,t} \\\\leftarrow s_{X}(A_{t-1}, X_{t-1}, L|z; \\\\sigma_A,j, \\\\sigma_X,j)$\\n\\n9: $p_{A,t} \\\\leftarrow p_{A}(A_{t-1}, X_{t-1}, L|z; \\\\sigma_A,j, \\\\sigma_X,j)$\\n\\n10: Draw $X^t \\\\sim N(0, I)$\\n\\n11: $X_t^\\\\prime \\\\leftarrow X_t - 1 + \\\\alpha_j s_{X,t} + \\\\sqrt{2} \\\\alpha_j X^t$\\n\\n12: $X_t \\\\leftarrow \\\\text{back to cell}(X_t^\\\\prime, L)$\\n\\n13: $A_t = \\\\arg\\\\max_{A_{t-1}} p_{A,t}$\\n\\n14: $X_T, A_T \\\\leftarrow X_T, A_T$\\n\\nMaterial generation with Langevin dynamics. After training the model, we can generate the periodic structure of material given a latent representation $z$. First, we use $z$ to predict the aggregated properties: 1) composition $c$, 2) lattice $L$, and 3) the number of atoms $N$. Then, we randomly initialize an initial periodic structure $(A_0, X_0, L)$ with the aggregated properties and perform an annealed Langevin dynamics (Song & Ermon, 2019) using the decoder, simultaneously updating the atom types and coordinates. During the coordinate update, we map the coordinates back to the unit cell at each step if atoms move out of the cell. The algorithm is summarized in Algorithm 1.\\n\\n**Connection between the gradient field and a harmonic force field.** The gradient field $s_{X}(\\\\tilde{M}|z)$ is used to update atom coordinates in Langevin dynamics via the force term, $\\\\alpha_j s_{X,t}$. In Appendix A, we show that $\\\\alpha_j s_{X,t}$ is mathematically equivalent to a harmonic force field $F(\\\\tilde{X}) = -k(\\\\tilde{X} - X)$ when the noises are small, where $X$ is the equilibrium position of the atoms and $k$ is a force constant. Harmonic force field, i.e. spring-like force field, is a simple yet general physical model that approximates the forces on atoms when they are close to their equilibrium locations. This indicates that our learned gradient field utilizes the harmonic approximation to approximate QM forces without any explicit force data and generates stable materials with this physically motivated inductive bias.\\n\\n## 5 Experiments\\n\\nWe evaluate multiple aspects of material generation that are related to real-world material discovery process. Past studies in this field used very different tasks and metrics, making it difficult to compare different methods. Building upon past studies (Court et al., 2020; Ren et al., 2020), we create a set of standard tasks, datasets, and metrics to evaluate and compare models for material generation. Experiment details can be found in Appendix D.\\n\\n**Tasks.** We focus on 3 tasks for material generation. 1) **Reconstruction** evaluates the ability of the model to reconstruct the original material from its latent representation $z$. 2) **Generation** evaluates the validity, property statistics, and diversity of material structures generated by the model. 3) **Property optimization** evaluates the model's ability to generate materials that are optimized for a specific property.\\n\\n**Datasets.** We curated 3 datasets representing different types of material distributions. 1) **Perov** (Castelli et al., 2012a;b) includes 18928 perovskite materials that share the same structure but differ in composition. There are 56 elements and all materials have 5 atoms in the unit cell. 2) **Carbon-24** (Pickard, 2020) includes 10153 materials that are all made up of carbon atoms but differ in structures. There is 1 element and the materials have 6 - 24 atoms in the unit cells. 3) **MP-20** (Jain et al., 2013) includes 45231 materials that differ in both structure and composition. There are 59 elements and all materials have 5 - 17 atoms in the unit cells.\\n\\nIn fact, this is also true for the original formulation of NCSN (Song & Ermon, 2019).\"}"}
{"id": "03RLpj-tc_", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Reconstructed structures of randomly selected materials in the test set. Note our model reconstructs rotated (translated) version of the original material due to the SE(3) invariance.\\n\\nTable 1: Reconstruction performance.\\n\\n| Method               | Match rate (%) | \u2191RMSE | \u2193RMSE |\\n|----------------------|----------------|-------|-------|\\n| Perov-5 Carbon-24 MP-20 | 99.34          | 62.28 | 69.89 |\\n| FTCP                 | 99.34          | 0.0259| 0.2563|\\n| Cond-DFC-VAE        | 51.65          | \u2013     | \u2013     |\\n| CDVAE               | 97.52          | 55.22 | 45.43 |\\n|                     |                | 0.0156| 0.1251|\\n\\n89 elements and the materials have 1 - 20 atoms in the unit cells. We use a 60-20-20 random split for all of our experiments. Details regarding dataset curation can be found at Appendix C.\\n\\nStability of materials in datasets.\\n\\nStructures in all 3 datasets are obtained from QM simulations and all structures are at local energy minima. Most materials in Perov-5 and Carbon-24 are hypothetical, i.e. they may not have global stability (section 3.2) and likely cannot be synthesized. MP-20 is a realistic dataset that includes most experimentally known inorganic materials with at most 20 atoms in the unit cell, most of which are globally stable. A model achieving good performance in MP-20 has the potential to generate novel materials that can be experimentally synthesized.\\n\\nBaselines.\\n\\nWe compare CDVAE with the following 4 baselines, which include the latest coordinate-based, voxel-based, and 3D molecule generation methods.\\n\\nFTCP (Ren et al., 2020) is a crystal representation that concatenates real-space properties (atom positions, atom types, etc.) and Fourier-transformed momentum-space properties (diffraction pattern). A 1D CNN-V AE is trained over this representation for crystal generation.\\n\\nCond-DFC-V AE (Court et al., 2020) encodes and generates crystals with 3D density maps, while employing several modifications over the previous Voxel-V AE (Hoffmann et al., 2019) method. However, the effectiveness is only demonstrated for cubic systems, limiting its usage to the Perov-5 dataset.\\n\\nG-SchNet (Gebauer et al., 2019) is an auto-regressive model that generates 3D molecules by performing atom-by-atom completion using SchNet (Sch\u00fctt et al., 2018). Since G-SchNet is unaware of periodicity and cannot generate the lattice \\\\( \\\\mathcal{L} \\\\). We adapt G-SchNet to our material generation tasks by constructing the smallest oriented bounding box with PCA such that the introduced periodicity does not cause structural invalidity.\\n\\nP-G-SchNet is our modified G-SchNet that incorporates periodicity. During training, the SchNet encoder inputs the partial periodic structure to predict next atoms. During generation, we first randomly sample a lattice \\\\( \\\\mathcal{L} \\\\) from training data and autoregressively generate the periodic structure.\\n\\n5.1 MATERIAL RECONSTRUCTION\\n\\nSetup.\\n\\nThe first task is to reconstruct the material from its latent representation. We evaluate reconstruction performance by matching the generated structure and the input structure for all materials in the test set. We use StructureMatcher from pymatgen (Ong et al., 2013), which finds the best match between two structures considering all invariances of materials. The match rate is the percentage of materials satisfying the criteria \\\\( s_{tol}=0.5 \\\\), \\\\( \\\\angle_{tol}=10 \\\\), \\\\( l_{tol}=0.3 \\\\). The RMSE is averaged over all matched materials. Because the inter-atomic distances can vary significantly for different materials, the RMSE is normalized by \\\\( \\\\sqrt[3]{V/N} \\\\), roughly the average atom radius per material. Note G-SchNet is not a VAE so we do not evaluate its reconstruction performance.\\n\\nResults.\\n\\nThe reconstructed structures are shown in Figure 3 and the metrics are in Table 1. Since our model is SE(3) invariant, the generated structures may be a translated (or rotated) version of the ground truth structure. Our model has a lower RMSE than all other models, indicating its stronger capability to reconstruct the original stable structures. FTCP has a higher match rate than our model.\"}"}
{"id": "03RLpj-tc_", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2: Generation performance\\n\\n| Method      | Data          | Perov-5 | Carbon-24 | MP-20 |\\n|-------------|---------------|---------|-----------|-------|\\n| G-SchNet    |               | 0.37    | 0.25      | 0.2755|\\n| Cond-DFC-VAE|               | 1.388   | 0.4552    |       |\\n| FTCP        |               |         |           |       |\\n| CDV AE      |               | 99.80   | 83.08     | 0.1407|\\n\\n### Validity (%)\\n\\n| Ca          | 100.0 | 98.59 | 99.45 | 98.46 | 0.1258 | 0.0264 |\\n|-------------|-------|-------|-------|-------|--------|--------|\\n| Yb          | 73.60 | 82.95 | 73.92 | 10.13 | 2.268  | 4.111  |\\n| Zr          | #     | #     | #     | #     | #      |        |\\n| Al          | #     | #     | #     | #     | #      |        |\\n| V           | #     | #     | #     | #     | #      |        |\\n| Rh          | #     | #     | #     | #     | #      |        |\\n| N           | #     | #     | #     | #     | #      |        |\\n| Au          | #     | #     | #     | #     | #      |        |\\n| K           | #     | #     | #     | #     | #      |        |\\n| Er          | #     | #     | #     | #     | #      |        |\\n| Au          | #     | #     | #     | #     | #      |        |\\n| H           | #     | #     | #     | #     | #      |        |\\n| C           | #     | #     | #     | #     | #      |        |\\n| B           | #     | #     | #     | #     | #      |        |\\n| K           | #     | #     | #     | #     | #      |        |\\n| Fe          | #     | #     | #     | #     | #      |        |\\n| Pb          | #     | #     | #     | #     | #      |        |\\n| Nb          | #     | #     | #     | #     | #      |        |\\n| Fe          | #     | #     | #     | #     | #      |        |\\n| V           | #     | #     | #     | #     | #      |        |\\n| 77.51       | 76.40 | 41.93 |       | 1.55  | 48.37  | 4.72   |\\n| M          | 99.65 | 75.96 | 38.33  |       |        |        |\\n| 100.0       | 86.70 | 99.15 | 99.49  | 0.6875| 0.2778 |        |\\n| 99.94       | \u2013      | 0.00  | 0.00   | 0.9427| 1.320  | \u2013      |\\n| K          | #     | #     | #     | #     | #      |        |\\n| Er          | #     | #     | #     | #     | #      |        |\\n| C           | #     | #     | #     | #     | #      |        |\\n| Al          | #     | #     | #     | #     | #      |        |\\n\\n### Property statistics\\n\\n- Percentage of ground truth materials being correctly predicted, and COV-R measures the percentage of generated materials and ground truth materials in the test set. Intuitively, COV-R measures the diversity of generated materials.\\n- SMACT computes the earth mover's distance (EMD) between the property distribution of generated materials and a set of ground truth properties.\\n- Coverage (COV) measures the diversity of generated materials.\\n- COV-P measures the percentage of predicted materials having high quality (details in Appendix G).\"}"}
{"id": "03RLpj-tc_", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"During the generation, we use $\\\\epsilon$ of 0.6 if the validation loss does not improve after 30 epochs. The minimum learning rate is 0.0001.\\n\\nDuring the training, we use an initial learning rate of 0.001 and reduce the learning rate by a factor level.\\n\\nFor the noise levels in $\\\\{\\\\beta, \\\\sigma\\\\}$ loss. For Perov-5, MP-20, we use $10$.\\n\\nWe aim to keep each loss term at a similar scale. For all three datasets, we use $\\\\lambda$.\\n\\n$\\\\beta$ We tune $\\\\beta$.\\n\\n$\\\\lambda$ $\\\\lambda$ between $0$ and $1$.\\n\\nFor all three datasets and select the model with best validation\\n\\n$\\\\beta$ $\\\\beta$.\\n\\n$\\\\lambda$ $\\\\lambda$.\\n\\nWe summarize the speed for generating 10,000 materials for all models in Table 4. FTCP is significantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce sampling\\n\\nDFC-V AE is faster than our method in Perov-5, but has a lower quality than our method and only\\n\\nsignificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-\\n\\nWe also note that we did not optimize sampling speed in current work. It is possible to reduce"}
{"id": "03RLpj-tc_", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Time used for generating 10,000 materials on a single RTX 2080 Ti GPU.\\n\\n| Material  | FTCP Cond-DFC-V AE | G-SchNet | P-G-SchNet | CDV AE |\\n|-----------|--------------------|----------|------------|--------|\\n| Perov-5   | 0.5 h              | 2.0 h    | 2.0 h      | 3.1 h  |\\n| Carbon-24 | \u2013                  | 6.2 h    | 6.3 h      | 5.3 h  |\\n| MP-20     | \u2013                  | 6.3 h    | 6.3 h      | 5.8 h  |\\n\\nGoverage Metrics for Material Generation\\n\\nInspired by Xu et al. (2021a); Ganea et al. (2021), we define six metrics to compare two ensembles of materials: materials generated by a method \\\\( \\\\{M_k\\\\}_{k=1}^{K} \\\\), and ground truth materials in test data \\\\( \\\\{M^*_l\\\\}_{l=1}^{L} \\\\).\\n\\nWe use the Euclidean distance of the CrystalNN fingerprint (Zimmermann & Jain, 2020) and normalized Magpie fingerprint (Ward et al., 2016) to define the structure distance and composition distance between generated and ground truth materials, respectively. They can be written as:\\n\\n\\\\[\\nD_{\\\\text{struc}}(M_k, M^*_l) \\\\quad \\\\text{and} \\\\quad D_{\\\\text{comp}}(M_k, M^*_l).\\n\\\\]\\n\\nWe further define the thresholds for the structure and composition distance as \\\\( \\\\delta_{\\\\text{struc}} \\\\) and \\\\( \\\\delta_{\\\\text{comp}} \\\\), respectively.\\n\\nFollowing the established classification metrics of Precision and Recall, we define the coverage metrics as:\\n\\n\\\\[\\n\\\\text{COV-R (Recall)} = \\\\frac{1}{L} \\\\left| \\\\left\\\\{ l \\\\in [1..L] : \\\\exists k \\\\in [1..K], D_{\\\\text{struc}}(M_k, M^*_l) < \\\\delta_{\\\\text{struc}}, D_{\\\\text{comp}}(M_k, M^*_l) < \\\\delta_{\\\\text{comp}} \\\\right\\\\} \\\\right|.\\n\\\\]\\n\\n\\\\[\\n\\\\text{AMSD-R (Recall)} = \\\\frac{1}{L} \\\\sum_{l=1}^{L} \\\\min_{k=1}^{K} D_{\\\\text{struc}}(M_k, M^*_l).\\n\\\\]\\n\\n\\\\[\\n\\\\text{AMCD-R (Recall)} = \\\\frac{1}{L} \\\\sum_{l=1}^{L} \\\\min_{k=1}^{K} D_{\\\\text{comp}}(M_k, M^*_l).\\n\\\\]\\n\\nwhere COV is \u201cCoverage\u201d, AMSD is \u201cAverage Minimum Structure Distance\u201d, AMCD is \u201cAverage Minimum Composition Distance\u201d, and COV-P (precision), AMSD-P (precision), AMCD-P (precision) are defined as in above equations, but with the generated and ground truth material sets swapped. The recall metrics measure how many ground truth materials are correctly predicted, while the precision metrics measure how many generated materials are of high quality (more discussions can be found in Ganea et al. (2021)).\\n\\nWe note several points on why we define the metrics in their current forms. 1) COV requires both structure and composition distances to be within the thresholds, because generating materials that are structurally close to one ground truth material and compositionally close to another is not meaningful. As a result, AMSD and AMCD are less useful than COV. 2) We use fingerprint distance, rather than RMSE from StructureMatcher (Ong et al., 2013), because the material space is too large for the models to generate enough materials to exactly match the ground truth materials. StructureMatcher first requires the compositions of two materials to exactly match, which will cause all models to have close-to-zero coverage.\\n\\nFor Perov-5 and Carbon-24, we choose \\\\( \\\\delta_{\\\\text{struc}} = 0.2 \\\\), \\\\( \\\\delta_{\\\\text{comp}} = 4 \\\\). For MP-20, we choose \\\\( \\\\delta_{\\\\text{struc}} = 0.4 \\\\), \\\\( \\\\delta_{\\\\text{comp}} = 10 \\\\). In Figure 6, Figure 7, Figure 8, we show how both COV-R and COV-P change by varying \\\\( \\\\delta_{\\\\text{struc}} \\\\) and \\\\( \\\\delta_{\\\\text{comp}} \\\\) in all three datasets.\"}"}
{"id": "03RLpj-tc_", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: Full coverage metrics for the generation task.\\n\\n| Method       | Data | COV-R | AMS-R | AMCD-R | COV-P | AMS-P | AMCD-P |\\n|--------------|------|-------|-------|--------|-------|-------|--------|\\n| FTCP Perov-5 | 0.00 | 0.7447| 7.212 | 0.00   | 0.3582| 3.390 |\\n| Carbon-24    | 0.00 | 1.181 | 0.00  | 0.00   | 0.8822| 24.16 |\\n| MP-20        | 4.72 | 0.6542| 9.271 | 0.09   | 0.1954| 4.378 |\\n| Cond-DFC-VAE | 73.92| 0.1508| 2.773 | 10.13  | 0.3162| 4.257 |\\n| G-SchNet Perov-5 | 0.18  | 0.5962| 1.006 | 0.23   | 0.4259| 1.3163|\\n| Carbon-24    | 0.00 | 0.5887| 0.00  | 0.00   | 0.5970| 0.00  |\\n| MP-20        | 38.33| 0.5365| 3.233 | 99.57  | 0.2026| 3.601 |\\n| P-G-SchNet Perov-5 | 0.37  | 0.5510| 1.0264| 0.25   | 0.3967| 1.316 |\\n| Carbon-24    | 0.00 | 0.6308| 0.00  | 0.00   | 0.8166| 0.00  |\\n| MP-20        | 41.93| 0.5327| 3.274 | 99.74  | 0.1985| 3.567 |\\n| CDV AE Perov-5 | 99.45 | 0.0482| 0.6969| 98.46  | 0.0593| 1.272 |\\n| Carbon-24    | 99.80| 0.0489| 0.00  | 83.08  | 0.1343| 0.00  |\\n| MP-20        | 99.15| 0.1549| 3.621 | 99.49  | 0.1883| 4.014 |\\n\\nFigure 6: Change of COV-R and COV-P by varying $\\\\delta_{\\\\text{struc}}$ and $\\\\delta_{\\\\text{comp}}$ for Perov-5. Dashed line denotes the current chosen thresholds.\"}"}
{"id": "03RLpj-tc_", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 7: Change of COV-R and COV-P by varying $\\\\delta_{\\\\text{struc}}$ and $\\\\delta_{\\\\text{comp}}$. for Carbon-24. Dashed line denotes the current chosen thresholds.\\n\\nFigure 8: Change of COV-R and COV-P by varying $\\\\delta_{\\\\text{struc}}$ and $\\\\delta_{\\\\text{comp}}$. for MP-20. Dashed line denotes the current chosen thresholds.\"}"}
