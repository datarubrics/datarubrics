{"id": "8svLJL54sj8", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: Leave-one-out log regret mean and standard deviation results on ImageNet ResNet50, LM1B Transformer 2048, WMT XFormer 64 and Uniref50 Transformer 128. All methods were repeated 5 times with different random seeds to initialize their models. In LM1B Transformer 2048, H* NLL and H* KL disappeared around 60 to 80 BO iterations because they reached 0 regret.\\n\\nDifficult because few similar task datasets are present in their training data. On all of these 4 difficult tasks, HyperBO identified good hyperparameters much sooner than its competitors.\\n\\nE.3 RESULTS ON ONLINE OPTIMIZER HYPERPARAMETER TUNING TASKS\\n\\nFinally, we look into the online BO setting where we optimize over the full hypercube. In the online setting, some combinations of hyperparameters may be infeasible to evaluate. For example, an overly big learning rate may lead to divergence in gradients, in which case we do not obtain a valid model.\\n\\nTo address this, we pre-process the function values to $[-2, 2)$ such that infeasible evaluations map to $-2$, while bad evaluations approach asymptotically to $-2$. More precisely, for each subdataset $D_{fi}$, we applied for each successful $y \\\\in \\\\{y_j(i)\\\\}_{M_{ij}=1}$ the following mapping:\\n\\n$$y \\\\leftarrow \\\\text{softplus}(y - y_{\\\\text{median}}) \\\\cdot 4 - 2$$\\n\\nwhere $y_{\\\\text{median}}$ is the median of $\\\\{y_j(i)\\\\}_{M_{ij}=1}$. \\n\\n20\"}"}
{"id": "8svLJL54sj8", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We noticed that for some methods, e.g. STBO and MIMO, it is very difficult for them to recover from initial observations. For example, STBO may overfit to the initial bad value and believe there are bad options to set \\\\( \\\\lambda \\\\), which corresponds to Eq. 8 with \\\\( \\\\lambda = 10 \\\\).\\n\\nBesides NLL and KL, which are already described in details in \u00a73, we also include NLL+KL, which might be preferred for UCB with coefficient 2, 3 or 4.\\n\\nIn this section, we set HyperBO variants and STBO to share exactly the same GP-UCB acquisition function as STBOH, MIMO and RFGP. The UCB coefficient for all methods is 1.\\n\\nIn Fig. 12, we include the online tuning results for selected tasks due to limited compute resources. We show results of the top 6 methods, and we highlight the lowest error values in the entire search space. Nevertheless, in 7 out of 9 tasks, HyperBO methods performed the best among all methods being compared.\\n\\nIn Table 4, the mean and standard error of best validation error rates are given. The variants outperform the baselines. For EI and PI, the KL objective gives better performance, but NLL or NLL+KL might be preferred for UCB with coefficient 2, 3 or 4.\\n\\nUnder review as a conference paper at ICLR 2022.\"}"}
{"id": "8svLJL54sj8", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12: Results of running BO methods in the online setting on 9 different tasks. The image based tasks all use best validation error rate as objective while the text based tasks (LM1B, Uniref50 and WMT) use best validation ce loss. HyperBO methods achieved better results in 7 out of 9 tasks. Our goal is to verify that HyperBO maintains good performance across different choices of acquisition functions. To do so, we investigated in the performance of HyperBO variants under different objectives. We avoid over cluttering the figures by only including STBOH as baseline, since it is roughly the best baseline according to the main results in Fig. 6.\\n\\nAs shown in Figure 14, HyperBO with EI and PI as acquisition functions perform relatively better than HyperBO with UCB variants. However, HyperBO with UCB3 can still be very competitive when it is coupled with NLL objective. Overall, HyperBO with all of the 5 acquisition function options outperforms the best performing baselines.\\n\\nE.6 MAF\\n\\nIMPLEMENTATION DETAILS\\n\\nWe compared to (Volpp et al., 2020) using the code and default hyperparameters provided by the authors. This code assumes that each task is additionally accompanied by the optimal set of hyperparameters for the GP used to model the task (including the task used for evaluation). Following the MAF approach, we learned these hyperparameters using the GP library, and provided them to https://github.com/boschresearch/MetaBO.\"}"}
{"id": "8svLJL54sj8", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13: We compare the performance of 3 different objective functions in HyperBO under 5 settings of acquisition functions. For EI and PI, using KL as the objective for HyperBO is slightly better than NLL or NLL+KL. However, different conclusions can be drawn for UCB2, UCB3 and UCB4. Nevertheless, all HyperBO variants still outperform the best alternatives.\\n\\nThe MAF algorithm. Given that MAF takes significantly longer to run than HyperBO, each subdataset was evaluated using only one random seed.\\n\\nEach neural acquisition function was trained for a total of 1000 iterations. As was done in (Volpp et al., 2020), we selected the optimal training iteration for the neural acquisition function by cross-validation on the transfer learning tasks; in this case, we randomly sampled 3 transfer learning task, and chose the training iteration with the lowest average simple regret.\\n\\nFinally, to reuse the MAF code, we also had to ensure that (a) each subtask had the same number of evaluation points, and (b) that there were no duplicated hyperparameters. For this reason, we first removed all duplicate hyperparameters within each subdataset, then capped each subdataset to the first 1559 points (the size of the smallest sub-dataset).\"}"}
{"id": "8svLJL54sj8", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: (a) The performance profile for outperforming the median of best error rates at the 100th BO iteration. (b) BO convergence of all methods: the median and 20/80 percentiles of the regrets on error rates over 115 BO runs: 23 tasks and each with 5 repeats of different random seeds. (c) A violin plot on the vertical slices of (b) at the 100th iteration; the white dot is the median and the black line is the 20/80 percentile. Overall, HyperBO methods H* NLL and H* KL are able to achieve the lowest regret on error rate on the majority of tasks with fewer iterations.\\n\\n4.3 RESULTS ON OFFLINE OPTIMIZER HYPERPARAMETER TUNING TASKS\\n\\nMany tasks in \u00a74.1 can use up a lot of compute resources and time, which makes it infeasible to perform a wide variety of experiments to analyze the characteristics of BO methods. Hence we adopt an offline approximation, which runs BO only on the finite set of points that each tuning sub-dataset contains. More details and analyses are available in Appendix E.2.\\n\\nHolding out relevant tasks. Fig. 2 (a) shows the performance profiles, the fraction of all test tasks that each method from \u00a74.2 is able to outperform a baseline criterion at each BO iteration. We can see that MIMO is able to outperform other methods in the beginning 20 BO iterations, but its leading position soon gets surpassed by HyperBO (H* NLL and H* KL). Fig. 2 (b,c) illustrates the BO convergence curves of all competing methods, together with the vertical slice at the 100th iterations. RFGP and STBO are both falling much behind Rand. STBO trains the GP on the data that the GP suggests to query. Optimizing the marginal data likelihood on at most 100 datapoints in fact may not lead to a better model than random initialization (see Tab. 5 in \u00a7F). Surprisingly, the contextual information learned by RFGP did not generalize to a new task. On the other hand, MIMO is able to obtain a slightly better error rate than STBO. Overall, learning the GP prior through data as with HyperBO outperforms other meta BO methods, and is a more principled and effective way to obtain the GP prior when compared with hand-tuning.\\n\\nFigure 3: Medians and 20/80 percentiles of regrets on best validation error rates for methods that uses models trained on 3 to 23 training tasks.\\n\\nEffect of number of training tasks. We now investigate the impact of the number of training tasks on the performance of meta BO methods. In Fig 3 we show the BO simple regrets on tasks from Table 1 (except ImageNet ResNet50 2048) that use meta BO models trained on different number of training tasks. To analyze the performance of all methods on less-related tasks, we first remove training tasks that have the same task dataset as our current tuning task for testing, and then remove randomly selected training datasets from the rest. HyperBO variants reduced the simple regret as more training tasks are given. Interestingly, H* NLL and H* KL are already slightly better than Rand and STBOH when they started off with only 3 training tasks. There are reasonable fluctuations in the results but overall the trend of regret is going down as the number of training tasks increases. MIMO also reduced regret when the number of tasks increased from 8 to 18. RFGP, however, fails to learn from training tasks possibly because it did not learn good task embeddings for GP regression models.\"}"}
{"id": "8svLJL54sj8", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nFigure 5: Results of running BO methods in the online setting on 3 different tasks. The image based tasks use best validation error rate as objective while the text based ones including Uniref50 use best validation CE loss. In all 3 tasks, HyperBO methods achieved better results.\\n\\nFigure 4: Medians and 20/80 percentiles of simple regrets for methods that uses models trained on 0.2% to 90% of data in each task.\\n\\nEffect of number of data points in training tasks. One remaining question is, how does $M_i$ in \u00a72, the number of data points in each training task, affect the performance of meta BO methods. We analyze the impact of $M_i$ by removing a portion of all data that we have access to for each task. In particular, we set the percentage of remaining data to be 0.2%, 0.5%, 1%, 3%, 5%, 10%, 30%, 50%, 70%, 90%. Remaining datapoints are selected uniformly randomly, which breaks the structure of matching data. Hence we do not include $H^* KL$ in this comparison, as $H^* KL$ only makes use of matching data.\\n\\nFig. 4 shows how the simple regret changes as the fraction of training data grows. Below 10% training data, we observe clear trend that more data lead to lower regret for both $H^* NLL$ and MIMO, and relatively no change for RFGP. We also found that the performance of HyperBO ($H^* NLL$) does not change much as the fraction of training data increases from 5% to 90%. However, MIMO and RFGP suffers significantly from more data as the fraction of training data increases from 5% to 50%. It is not entirely clear why MIMO and RFGP have such behaviors. One conjecture is that neural network based Bayesian linear regression models may get too confident once the amount of data reaches a certain threshold. This means much less exploration if those models are used for BO.\\n\\n4.4 RESULTS ON ONLINE OPTIMIZER HYPERPARAMETER TUNING TASKS\\n\\nWe now evaluate HyperBO methods, $H^* NLL$ and $H^* NLLKL$ (NLL plus KL divergence on matching datapoints as the objective), in the online setting, where we optimize over the full hypercube and some hyperparameters may be infeasible to evaluate. See full results at \u00a7E.3. Fig. 5 shows that it can be difficult for STBO or MIMO to recover from a \u201cbad\u201d datapoint, but HyperBO methods are robust and performed the best among all methods being compared.\\n\\n5 DISCUSSION AND CONCLUSION\\n\\nWhile we focused on obtaining a better prior in BO in this work, the following directions are orthogonal to what we studied: different search spaces across tasks, batch evaluation, high-dimensional or large scale data, etc. However, it should be straightforward to combine their solutions with HyperBO.\\n\\nPlease find more discussions at \u00a7F together with implications of our assumptions.\\n\\nHyperBO is a novel meta BO approach that supports practical applications that involve continuous inputs queried at possibly non-aligned locations across tasks. HyperBO uses a simple yet effective idea that is easy to implement and efficient to run. We evaluated HyperBO on real-world big model optimizer tuning tasks, and the results demonstrated its superior performance over state-of-the-art competing methods.\"}"}
{"id": "8svLJL54sj8", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"R\u00e9mi Bardenet, M\u00e1ty\u00e1s Brendel, Bal\u00e1zs K\u00e9gl, and Michele Sebag. Collaborative hyperparameter tuning. In *ICML*, 2013.\\n\\nJames Bergstra, R\u00e9mi Bardenet, Yoshua Bengio, and Bal\u00e1zs K\u00e9gl. Algorithms for hyper-parameter optimization. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger (eds.), *Advances in Neural Information Processing Systems*, 2011.\\n\\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\\n\\nPavel Brazdil, Jo\u00e3\u00f4 Gama, and Bob Henery. Characterizing the applicability of classification algorithms using meta-level learning. In *ECML*, 1994.\\n\\nYutian Chen, Matthew W Hoffman, Sergio G\u00f3mez Colmenarejo, Misha Denil, Timothy P Lillicrap, Matt Botvinick, and Nando de Freitas. Learning to learn without gradient descent by gradient descent. In *ICML*, 2017.\\n\\nMatthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, and Frank Hutter. Efficient and robust automated machine learning. In *NeurIPS*, 2015.\\n\\nJustin M. Gilmer, George E. Dahl, and Zachary Nado. init2winit: a jax codebase for initialization, optimization, and tuning research, 2021. URL http://github.com/google/init2winit.\\n\\nNikolaus Hansen, Anne Auger, Raymond Ros, Olaf Mersmann, Tea Tu\u0161ar, and Dimo Brockhoff. Coco: A platform for comparing continuous optimizers in a black-box setting. *Optimization Methods and Software*, 36(1):114\u2013144, 2021. URL https://arxiv.org/pdf/1603.08785.pdf.\\n\\nMarton Havasi, Rodolphe Jenatton, Stanislav Fort, Jeremiah Zhe Liu, Jasper Snoek, Balaji Lakshminarayanan, Andrew M Dai, and Dustin Tran. Training independent subnetworks for robust prediction. *arXiv preprint arXiv:2010.06610*, 2020.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pp. 770\u2013778, 2016.\\n\\nTarun Kathuria, Amit Deshpande, and Pushmeet Kohli. Batched gaussian process bandit optimization via determinantal point processes. In *NeurIPS*, 2016.\\n\\nCharles Kemp and Joshua B Tenenbaum. The discovery of structural form. *Proceedings of the National Academy of Sciences*, 105(31):10687\u201310692, 2008.\\n\\nBeomjoon Kim, Leslie Pack Kaelbling, and Tom\u00e1s Lozano-P\u00e9rez. Learning to guide task and motion planning using score-space representation. In *ICRA*, 2017.\\n\\nBeomjoon Kim, Zi Wang, Leslie Pack Kaelbling, and Tom\u00e1s Lozano-P\u00e9rez. Learning to guide task and motion planning using score-space representation. *The International Journal of Robotics Research*, 38(7):793\u2013812, 2019.\\n\\nAndreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In *NeurIPS*, 2011.\\n\\nJeremiah Zhe Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss, and Balaji Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. *arXiv preprint arXiv:2006.10108*, 2020.\\n\\nGustavo Malkomes and Roman Garnett. Automating Bayesian optimization with Bayesian optimization. *Advances in Neural Information Processing Systems*, 31:5984\u20135994, 2018.\\n\\nZachary Nado, Justin Gilmer, Christopher J. Shallue, Rohan Anil, and George E. Dahl. A large batch optimizer reality check: Traditional, generic optimizers suffice across batch sizes. *CoRR*, abs/2102.06356, 2021. URL https://arxiv.org/abs/2102.06356.\"}"}
{"id": "8svLJL54sj8", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nYurii E. Nesterov. A method for solving the convex programming problem with convergence rate $O(1/k^2)$. In Dokl. akad. nauk Sssr, volume 269, pp. 543\u2013547, 1983.\\n\\nJorge Nocedal. Updating quasi-newton matrices with limited storage. Mathematics of computation, 35(151):773\u2013782, 1980.\\n\\nChangYong Oh, Efstratios Gavves, and Max Welling. Bock: Bayesian optimization with cylindrical kernels. In ICML, 2018.\\n\\nAnne Onomous. Anonymized, 2021. URL http://github.com/ANONYMOUS.\\n\\nValerio Perrone, Rodolphe Jenatton, Matthias Seeger, and C\u00e9dric Archambeau. Scalable hyperparameter transfer learning. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 6846\u20136856, 2018.\\n\\nMatthias Poloczek, Jialei Wang, and Peter I. Frazier. Warm starting Bayesian optimization. In Winter Simulation Conference (WSC). IEEE, 2016.\\n\\nMatthias Poloczek, Jialei Wang, and Peter Frazier. Multi-information source optimization. In NeurIPS, 2017.\\n\\nAli Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines. In NeurIPS, 2007.\\n\\nCarl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning. The MIT Press, 2006.\\n\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015.\\n\\nJasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine learning algorithms. In NeurIPS, 2012.\\n\\nJasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable Bayesian optimization using deep neural networks. In International conference on machine learning, pp. 2171\u20132180. PMLR, 2015.\\n\\nJost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization with robust Bayesian neural networks. Advances in neural information processing systems, 29:4134\u20134142, 2016.\\n\\nIlya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In ICML, 2013.\\n\\nKevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In NeurIPS, 2013.\\n\\nRyan Turner, David Eriksson, M. McCourt, Juha Kiili, Eero Laaksonen, Zhen Xu, and I. Guyon. Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the black-box optimization challenge 2020. ArXiv, abs/2104.10201, 2021.\\n\\nMichael Volpp, Lukas P Fr\u00f6hlich, Kirsten Fischer, Andreas Doerr, Stefan Falkner, Frank Hutter, and Christian Daniel. Meta-learning acquisition functions for transfer learning in Bayesian optimization. In International Conference on Learning Representations (ICLR), 2020.\\n\\nZi Wang, Chengtao Li, Stefanie Jegelka, and Pushmeet Kohli. Batched high-dimensional Bayesian optimization via structural kernel learning. In ICML, 2017.\\n\\nZi Wang, Clement Gehring, Pushmeet Kohli, and Stefanie Jegelka. Batched large-scale Bayesian optimization in high-dimensional spaces. In AISTATS, 2018a.\\n\\nZi Wang, Beomjoon Kim, and Leslie Pack Kaelbling. Regret bounds for meta Bayesian optimization with an unknown Gaussian process prior. In NeurIPS, 2018b.\"}"}
{"id": "8svLJL54sj8", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ABSTRACT\\nThe performance of deep neural networks can be highly sensitive to the choice of a variety of meta-parameters, such as optimizer parameters and model hyperparameters. Tuning these well, however, often requires extensive and costly experimentation. Bayesian optimization (BO) is a principled approach to solve such expensive hyperparameter tuning problems efficiently. Key to the performance of BO is specifying and refining a distribution over functions, which is used to reason about the optima of the underlying function being optimized. In this work, we consider the scenario where we have data from similar functions that allows us to specify a tighter distribution a priori. Specifically, we focus on the common but potentially costly task of tuning optimizer parameters for training neural networks. Building on the meta BO method from Wang et al. (2018b), we develop practical improvements that (a) boost its performance by leveraging tuning results on multiple tasks without requiring observations for the same meta-parameter points across all tasks, and (b) retain its regret bound for a special case of our method. As a result, we provide a coherent BO solution for iterative optimization of continuous optimizer parameters. To verify our approach in realistic model training setups, we collected a large multi-task hyperparameter tuning dataset by training tens of thousands of configurations of near-state-of-the-art models on popular image and text datasets, as well as a protein sequence dataset. Our results show that on average, our method is able to locate good hyperparameters at least 3 times more efficiently than the best competing methods.\\n\\n1 INTRODUCTION\\nThe careful tuning of a variety of meta-parameters, such as optimizer parameters and model hyperparameters has become a basic necessity for deep learning (Bergstra et al., 2011; Feurer et al., 2015). Such tuning requires extensive experimentation, retraining models repeatedly with different configurations, and can be challenging at realistic budgets because the tuning landscape is typically non-stationary, noisy and ill-behaved. Tuning has become sufficiently costly that finding more efficient and effective tuning procedures has the potential to save a substantial amount of resources or, alternatively, improve the accuracy of the final models at a given budget.\\n\\nSome hyperparameters might be common across a large number of tuning problems, such as those pertaining to an optimization algorithm. For example, Adam is used across many deep learning applications and has four parameters that require careful tuning (Nado et al., 2021). Thus, if we have access to the performance of different optimizer-specific hyperparameters on different model training tasks, we may be able to transfer the knowledge among those tasks. This kind of meta-level learning is common among practitioners themselves: when faced with a new tuning problem, one might first try reusing hyperparameter settings that worked well on another problem. The underlying assumption is that hyperparameters should perform similarly across tasks. In this work, we aim to formalize this assumption and automate optimizer hyperparameter tuning by leveraging knowledge from previous experiments. Although our experiments consider optimizer parameter tuning as a practically important sub-problem of hyperparameter tuning for deep neural networks, our method applies to any hyperparameters that are common across multiple tasks.\"}"}
{"id": "8svLJL54sj8", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bayesian optimization (BO) has become a popular methodology for optimizing the hyperparameters of machine learning models (Snoek et al., 2012; Bergstra et al., 2011) and represents the state-of-the-art (Turner et al., 2021). BO involves specifying a probabilistic model over the function to be optimized and using this to reason about the location of the optimum. The optimization proceeds by iteratively updating the model with new data and using the posterior distribution to reason about where to next evaluate, trading off exploration and exploitation. The model is typically specified with only a-priori assumptions of smoothness, for example using a Gaussian process (GP) with a smooth covariance function. Even if the model is well-specified, BO can be slow to converge due to the generality of the prior assumptions. This would seem wasteful for problems that are repeated often or share considerable structure with previous experiments.\\n\\nOne natural option is to cast our problem as meta Bayesian optimization, where the goal is to learn to optimize a function by generalizing from past experience with other similar functions. Indeed, several BO methods exist in the literature, but they are unsuitable for our scenario where we envision potentially thousands of related tasks within e.g. the context of a hyperparameter tuning service. Existing meta BO methods either scale cubically in the number of evaluations and tasks (Swersky et al., 2013; Bardenet et al., 2013) (See \u00a74.3 for more details), impose a restrictive set of assumptions on the available data (Wang et al., 2018b; Swersky et al., 2013) to obtain efficient solutions, or make assumptions on the availability of GP parameters (Volpp et al., 2020) or descriptive task-level features (Brazdil et al., 1994; Bardenet et al., 2013; Yogatama & Mann, 2014).\\n\\nTo address these issues, we introduce HyperBO: a meta BO method that builds upon Wang et al. (2018b) with a relatively simple assumption: all the related functions being optimized are samples from the same GP prior distribution over functions. Concretely, HyperBO assumes the functions are conditionally independent given the hyperparameters, mean and covariance function of the GP. Compared to Wang et al. (2018b), HyperBO does not impose any strict conditions on data or model structures, and a special case of HyperBO retains strong regret bounds. From a computational perspective, HyperBO scales linearly in the number of tasks during training, and does not depend on the number of tasks when deployed. By not imposing assumptions about the data collection conditions, it can be used with large offline datasets or a few related optimization trajectories.\\n\\nTo evaluate HyperBO, we collected a large multi-task hyperparameter tuning dataset by training tens of thousands of configurations of near-state-of-the-art models on popular image and text datasets, as well as on a protein sequence dataset. We compare HyperBO to several hyperparameter tuning baselines in the sequential BO setting. Our results show that optimizers that use hyperparameters suggested by our method are able to obtain better performing models requiring at least 3 times fewer function evaluations than other baselines.\\n\\nOur main contributions are two-fold: (1) a practical meta BO approach that makes minimal assumptions; and (2) a large multi-task hyperparameter tuning dataset that not only benefits our method but also serves as an ideal benchmark to test future multi-task or meta-learning BO methods.\\n\\nRelated work\\nThere is a rich literature of innovative methodologies to improve the efficiency of BO given related tasks or additional context. Here we discuss the most closely related work and explain why these don\u2019t solve the specific scenario which we envision. Specifically, our goal is a methodology that is scalable enough to share information across thousands of tasks, each with potentially hundreds of observations, such as in the context of a large BO service or library. In this work we use the term meta-BO to refer to the class of BO methods that use data from existing tasks to optimize a new task. Multi-task BO (Swersky et al., 2013; Poloczek et al., 2017; Yogatama & Mann, 2014) and transfer learning BO using contextual GPs (Krause & Ong, 2011; Bardenet et al., 2013; Poloczek et al., 2016) are both meta BO approaches. Some meta BO methods have also been studied for hyperparameter tuning tasks in machine learning (Feurer et al., 2015).\\n\\nHyperBO assumes all tasks are independent (after conditioning on the GP), whereas both multi-task and contextual BO rely heavily on the assumption that tasks are related. Thus the latter approaches typically scale cubically in both the number of tasks and observations in each task, meaning that they cannot gracefully scale across both without heavy approximations. When assuming that all inputs are equal across tasks, multi-task BO can be sped up using a Kronecker decomposition of the kernel.\"}"}
{"id": "8svLJL54sj8", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\nto a task kernel and an input kernel which can be inverted separately; a similar assumption is made\\nby Wang et al. (2018b). In comparison, HyperBO scales linearly in the number of tasks (see \u00a7B).\\n\\nEnd-to-end learning (Chen et al., 2017; Volpp et al., 2020) is another popular meta BO approach for\\nhyperparameter tuning that learns a strategy to suggest new query points based on past history of\\nBO. One limitation of such approaches is that the total number of BO iterations must be determined\\na-priori. Furthermore, by nature of using a highly parameterized model to train the strategy, we lose\\nthe interpretability of intermediate steps that GPs and acquisition functions provide.\\n\\nHyperBO builds upon Wang et al. (2018b) and Kim et al. (2017; 2019). We resolve their issues with\\noptimizing over a continuous space rather than a discrete set and limitations on using the same set of\\ninputs across tasks. Kim et al. (2017; 2019) estimated a multivariate Gaussian that models values for\\nsearch strategies in robot manipulation tasks, and thus only considered discrete inputs. Wang et al.\\n(2018b) provided regret bounds for Kim et al. (2017; 2019), which was identified as meta BO without\\nthe knowledge of the GP prior. For both finite discrete search spaces and continuous ones, Wang et al.\\n(2018b) requires observations on the same set of inputs across tasks, which is an assumption that\\nis not required for HyperBO; HyperBO still inherits the same regret bound as Wang et al. (2018b)\\nfor the special case where the same-inputs assumption is satisfied. Similar ideas also appeared in\\nPerrone et al. (2018), which can be viewed as a special case of HyperBO or Wang et al. (2018b).\\n\\nProblem Formulation\\n\\nWe consider the standard black-box function optimization scenario: given a real-valued function\\n\\\\( f \\\\) defined over a compact, hyper-rectangular space \\\\( X \\\\subset \\\\mathbb{R}^d \\\\), and given observations of similar functions \\\\( f_1, \\\\ldots, f_N \\\\), we seek an \\\\( x \\\\in X \\\\) optimizing \\\\( f \\\\). We inherit our problem formulation from Wang et al. (2018b), but we relax impractical assumptions on data availability (we do not require all observations to be made on the same inputs across tasks) and model restrictions.\\n\\nAssumptions and the goal.\\n\\nConcretely, we assume that there exists a Gaussian process \\\\( \\\\text{GP}(\\\\mu, k) \\\\) with unknown mean function \\\\( \\\\mu : X \\\\to \\\\mathbb{R} \\\\) and kernel \\\\( k : X \\\\times X \\\\to \\\\mathbb{R} \\\\). Let \\\\( N \\\\) be the number of tasks and let \\\\( M_i \\\\) be the number of observations we have for the \\\\( i \\\\)-th task. Conditioned on independent function samples \\\\( f_i \\\\sim \\\\text{GP}(\\\\mu, k) \\\\) and inputs \\\\( x_i \\\\in X, i \\\\in [N], j \\\\in [M_i] \\\\), we observe evaluations \\\\( y_i(j) \\\\sim \\\\mathcal{N}(f_i(x_i(j)), \\\\sigma^2) \\\\) perturbed by i.i.d. additive Gaussian noise \\\\( \\\\mathcal{N}(0, \\\\sigma^2) \\\\). Taken together, the collection of sub-datasets \\\\( D_f_i = \\\\{(x_i(j), y_i(j))\\\\}_{j=1}^{M_i} \\\\) define a dataset \\\\( D_N = \\\\{D_f_i\\\\}_{i=1}^N \\\\). Finally, our goal is to maximize a new function independently sampled from the same GP, \\\\( f \\\\sim \\\\text{GP}(\\\\mu, k) \\\\); that is, solve \\\\( \\\\arg \\\\max_{x \\\\in X} f(x) \\\\) given dataset \\\\( D_N \\\\) but unknown functions \\\\( \\\\mu, k \\\\) and unknown parameter \\\\( \\\\sigma^2 \\\\).\\n\\nExample.\\n\\nIn our optimizer hyperparameter tuning application, a task corresponds to finding the best optimizer hyperparameters to train a given model on a particular dataset, e.g. training a ResNet (He et al., 2016) on ImageNet (Russakovsky et al., 2015). Notice that we do not assume that the mean function \\\\( \\\\mu \\\\), kernel \\\\( k \\\\) and noise variance \\\\( \\\\sigma^2 \\\\) are given. This is consistent with the reality of solving real-world black-box optimization problems including hyperparameter tuning. We must learn those unknown functions and parameters from data. However, in practice, searching in functional spaces to find the right mean \\\\( \\\\mu \\\\) or kernel \\\\( k \\\\) is a daunting task. Hence for practical concerns, a well defined search space for functions is required. More details on this can be found at \u00a73.1.\\n\\nMetrics.\\n\\nFor simplicity, throughout this paper, we focus on the setting where the target function \\\\( f \\\\) can only be optimized by iteratively choosing where to evaluate, and defer batch evaluation setups to Sec. F. As we run BO on the target function \\\\( f \\\\) for \\\\( T \\\\) iterations, we accumulate a set of observations \\\\( D_f = \\\\{(x_t, y_t)\\\\}_{t=1}^{T} \\\\), \\\\( y_t \\\\sim \\\\mathcal{N}(f(x_t), \\\\sigma^2) \\\\). We evaluate the quality of the optimization using the simple regret metric:\\n\\n\\\\[\\nR_T = \\\\max_{x \\\\in X} f(x) - f(\\\\hat{x})\\n\\\\]\\n\\nwhere \\\\( \\\\hat{x} \\\\) is the final recommendation at the end of the optimization process. There are various ways of setting \\\\( \\\\hat{x} \\\\) based on the observations \\\\( D_f \\\\); we use the input that achieved the best evaluation:\\n\\n\\\\[\\n\\\\hat{x} = x_{\\\\tau}; \\\\quad \\\\tau = \\\\arg \\\\max_{t \\\\in [T]} y_t.\\n\\\\]\\n\\nBayesian viewpoint.\\n\\nAs mentioned above, the observed functions \\\\( f_1, \\\\ldots, f_N \\\\) and the evaluation target \\\\( f \\\\) are assumed to be independent draws from the same GP. This assumption is consistent with a hierarchical Bayes interpretation (Fig. 1), where all observed functions are independent conditioned on the GP. Notice that for BO, each selected input \\\\( x_i(j) \\\\) depends on all previous observations. But we only describe the generative model of a hierarchical GP for simplicity.\"}"}
{"id": "8svLJL54sj8", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Graphical model for a hierarchical Gaussian process.\\n\\n\u03b8\\n\\nMore specifically, we assume that underlying functions of hyperparameter optimization tasks are defined by a parameter\\n\\n\u03b8 \u223c p(\u03b8; \u03b1);\\n\\nmean and kernel functions \u00b5 and k are defined by deterministic functions parameterized by \u03b8. The independent function samples \\\\{f_i\\\\}_{i \\\\in [N]} are draws from \\\\(GP(\u00b5, k)\\\\). We then learn function \\\\(f\\\\) from observations \\\\(D_N\\\\) on all other conditionally i.i.d. function samples \\\\(f_1, \\\\ldots, f_N\\\\). We forgo a fully Bayesian approach that samples from the posterior over \\\\(\u03b8\\\\) at every BO iteration. Our method can be viewed as a type-II maximum likelihood approximation of such a Bayesian solution.\\n\\nNotations. Let \\\\([n]\\\\) denote \\\\{1, \\\\ldots, n\\\\}, \\\\forall n \\\\in \\\\mathbb{Z}^+. For conciseness, we write the evaluation of a function \\\\(f\\\\) on matrix \\\\(x\\\\) as \\\\(\u00b5(x) := [\u00b5(x_i)]_{i=1}^n\\\\). Similarly, for two matrices (i.e., the concatenation of input vectors) \\\\(x, x'\\\\), we write the corresponding kernel matrix as \\\\(k(x, x') := [k(x_i, x'_j)]_{i \\\\in [n], j \\\\in [n']}\\\\), and shorten \\\\(k(x) := k(x, x)\\\\). We denote a (multivariate) Gaussian distribution with mean \\\\(u\\\\) and variance \\\\(\u03a3\\\\) by \\\\(N(u, \u03a3)\\\\), and a Gaussian process (GP) with mean function \\\\(\u00b5\\\\) and covariance function \\\\(k\\\\) by \\\\(GP(\u00b5, k)\\\\). Let \\\\(\u03c3^2\\\\) be the noise variance in observations. Given a set of observations \\\\(D = \\\\{(x_t, y_t)\\\\}_{t=1}^T\\\\), \\\\(y \\\\sim N(f(x)^T), \u03c3^2I\\\\), \\\\(x^T = [x_t]^T\\\\) and \\\\(f \\\\sim GP(\u00b5, k)\\\\), we denote the corresponding conditional GP distribution as \\\\(GP(\u00b5, k|D)\\\\). Let \\\\(\u03c8(x) = k(x, x^T)(k(x^T) + \u03c3^2I)^{-1}\\\\).\\n\\nRecall that the conditional distribution, \\\\(GP(\u00b5, k|D) = GP(\u00b5_D, k_D)\\\\), is given for any \\\\(x, x' \\\\in X\\\\) as\\n\\n\\\\[\\n\u00b5_D(x) = \u00b5(x) + \u03c8(x)(y^T \u2212 \u00b5(x^T)),\\n\\\\]\\n\\n\\\\[\\nk_D(x, x') = k(x, x') \u2212 \u03c8(x)k(x^T, x'),\\n\\\\]\\n\\n(1)\\n\\nAlgorithm 1\\n\\n```plaintext\\n1: function HYPERBO (f, D_N)\\n2: \\\\(GP(\u02c6\u00b5, \u02c6k) \\\\leftarrow \\\\text{TRAIN}GP(D_N)\\\\)\\n3: \\\\(D_f \\\\leftarrow \\\\emptyset\\\\)\\n4: for \\\\(t = 1, \\\\ldots, T\\\\) do\\n5: \\\\(x_t \\\\leftarrow \\\\text{arg max}_{x \\\\in X} \\\\alpha(x; GP(\u02c6\u00b5, \u02c6k|D_f))\\\\)\\n6: \\\\(y_t \\\\leftarrow \\\\text{OBSERVE}(f(x_t))\\\\)\\n7: \\\\(D_f \\\\leftarrow D_f \\\\cup \\\\{(x_t, y_t)\\\\}\\\\)\\n8: end for\\n9: return \\\\(D_f\\\\)\\n10: end function\\n```\\n\\nAs shown in Alg. 1, our approach trains the GP hyperparameters on a representative set of datasets and fixes them for the duration of the optimization procedure; we refer to this approach as HyperBO. HyperBO runs in two steps. First, we learn a GP model \\\\(GP(\u02c6\u00b5, \u02c6k)\\\\) to approximate the ground-truth (unknown) GP that generated the dataset \\\\(D_N\\\\). Then, we do standard BO to optimize a new function \\\\(f\\\\) with the learned \\\\(GP(\u02c6\u00b5, \u02c6k)\\\\). The initial learning process (line 2) is the critical difference between HyperBO and standard BO algorithms, as well as the key contribution of this paper.\\n\\nBased on the Bayesian graphical model interpretation (Fig. 1), our goal is to obtain a point estimate \\\\(\u02c6\u03b8\\\\) for the parameter \\\\(\u03b8\\\\). Given this estimate, we can then estimate the mean function \\\\(\u02c6\u00b5\\\\) and the kernel \\\\(\u02c6k\\\\), which defines our learned model \\\\(GP(\u02c6\u00b5, \u02c6k)\\\\). During the BO iterations (Alg. 1, lines 4-8), we update the conditional GP, but do not re-estimate the GP mean and kernel. By separating the data for the conditional GP update and GP parameter training, we minimize the computational cost while still maintaining good performance both theoretically and empirically. Moreover, we avoid the BO chicken-and-egg dilemma (Wang et al., 2018b) where the search strategy is trained on data collected in the BO process and the data points are selected by the search strategy.\\n\\nNext, we introduce our GP training strategy based on two types of objectives: marginal data likelihood (\u00a7 3.1) and distance between estimates and model predictions (\u00a7 3.2). In the appendix, \u00a7B reveals the complexity of HyperBO that is linear in the number of tasks and \u00a7 3.3 shows that a special case of HyperBO retains strong regret bounds (Wang et al., 2018b).\\n\\n3.1 MARGINAL LIKELIHOOD\\n\\nA straightforward way to train a GP is by optimizing the log marginal likelihood over the GP\u2019s hyperparameters. This is also known as type II maximum likelihood approximation (Rasmussen 4).\"}"}
{"id": "8svLJL54sj8", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022 & Williams, 2006). In our case, we derive the data likelihood for the observations from multiple functions that are assumed to be given, which is a key difference to regular GP or BO setups. The log marginal likelihood for our method is\\n\\n$$\\\\log p(D_{\\\\mathbf{N}}|\\\\mu, k, \\\\sigma^2) = \\\\sum_{i=1}^{N} \\\\left(-\\\\frac{1}{2} \\\\bar{y}_i^\\\\top (\\\\mathbf{K}^{-1} \\\\bar{y}_i - \\\\frac{1}{2} \\\\log |\\\\mathbf{K}| - M_i^2 \\\\log 2\\\\pi)\\\\right),$$\\n\\nwhere\\n\\n$$\\\\bar{y}_i = y_i - \\\\mu(x_i), \\\\quad \\\\mathbf{K} = k(x_i) + \\\\sigma^2 \\\\mathbf{I}, \\\\quad x_i = [x_i^j]_{M_i j=1}$$\\n\\nOur solution to the choice of mean function, kernel function and noise variance then becomes\\n\\n$$\\\\hat{\\\\mu}, \\\\hat{k}, \\\\hat{\\\\sigma}^2 = \\\\arg \\\\max_{\\\\mu, k, \\\\sigma^2} \\\\log p(D_{\\\\mathbf{N}}|\\\\mu, k, \\\\sigma^2).$$\\n\\nFor mean function $\\\\mu$ and kernel $k$, this optimization is done in functional space. While methods exist to search for functional structures (Kemp & Tenenbaum, 2008; Malkomes & Garnett, 2018), one may opt for a simple search strategy within a group of functional structures (e.g. mean $\\\\mu \\\\in \\\\{\\\\text{linear}, \\\\text{constant}\\\\}$ and kernel $k \\\\in \\\\{\\\\text{exponentiated quadratic}, \\\\text{Mat\u00e9rn}\\\\}$). For all combinations of mean/kernel structures or functional classes, we then optimize the parameterization of them and noise variance to eventually solve Eq. 3. Details of how we defined the search space can be found in \u00a74.\\n\\n### 3.2 Distance between estimates and model predictions\\n\\nAlthough the marginal likelihood is a straightforward objective to optimize, it may not be straightforward to interpret how high of a likelihood is high enough for us to stop our search for a decent model. Nevertheless, we may be able to directly estimate the sample mean and covariance, and the distance between those estimates and model predictions could be a good indicator of how good the model is. We will show in \u00a73.3 that a distance objective may lead us to better theoretical properties.\\n\\nHere we consider a special case of dataset $D_{\\\\mathbf{N}}$ where part of it has matching inputs across some sampled functions. More formally, suppose we have a matching dataset $D'_{\\\\mathbf{N}} = \\\\{(x_j, y_j)\\\\}_{M_j=1}^{M}$ where $M$ is a positive integer, $x_j \\\\in \\\\mathbf{X}$, $y_j = [y_i^j]_{N_i=1}^{N} \\\\in \\\\mathbb{R}^N$ and $y_i^j \\\\sim N(f(x_j), \\\\sigma^2)$. Empirically, dataset $D'_{\\\\mathbf{N}}$ can be constructed by querying a set of functions $f_1, \\\\cdots, f_N$ at the same set of input locations $x = [x_j]_{M_j=1}^{M} \\\\in \\\\mathbb{R}^{M \\\\times d}$ to obtain an observation matrix $y = [y_j]_{M_j=1}^{M} \\\\in \\\\mathbb{R}^{M \\\\times N}$.\\n\\nBy definition of a GP, the vector of all function queries $f(x)$ is distributed according to a multivariate Gaussian distribution $N(\\\\mu(x), k(x))$. With our observation model, we get the distribution for observations $y \\\\sim N(\\\\mu(x), k(x) + \\\\mathbf{I} \\\\sigma^2)$ for some unknown mean function $\\\\mu$ and kernel $k$.\\n\\nHowever, given that we have access to all observations $y$, we can estimate the mean on inputs $x$ as\\n\\n$$\\\\tilde{\\\\mu} = \\\\frac{1}{N} y_1 \\\\in \\\\mathbb{R}^M$$\\n\\nand estimated covariance as\\n\\n$$\\\\tilde{\\\\mathbf{K}} = \\\\frac{1}{N} (y - \\\\tilde{\\\\mu} 1^\\\\top_N) (y - \\\\tilde{\\\\mu} 1^\\\\top_N)\\\\top \\\\in \\\\mathbb{R}^{M \\\\times M};$$\\n\\nhere $1^N$ is a column vector of size $N$ filled with 1s. We use a biased estimate of covariance to be consistent with the corresponding maximum likelihood estimator in Eq. 3.\\n\\nNotice that the estimated covariance includes in diagonal terms the variance of the observation noise.\\n\\nFor any distance function between the estimate $N(\\\\tilde{\\\\mu}, \\\\tilde{\\\\mathbf{K}})$ and model prediction $N(\\\\mu(x), k(x) + \\\\mathbf{I} \\\\sigma^2)$, we obtain an objective to minimize,\\n\\n$$D(N(\\\\tilde{\\\\mu}, \\\\tilde{\\\\mathbf{K}}), N(\\\\mu(x), k(x) + \\\\mathbf{I} \\\\sigma^2)).$$\\n\\nWhile there are different measures of distributional discrepancy, we adopt the KL divergence. Let $\\\\mu = \\\\mu(x)$ and $\\\\mathbf{K} = k(x) + \\\\mathbf{I} \\\\sigma^2$. The KL divergence is defined as\\n\\n$$D_{KL}(N(\\\\tilde{\\\\mu}, \\\\tilde{\\\\mathbf{K}}), N(\\\\mu(x), \\\\mathbf{K})) = \\\\frac{1}{2} \\\\left(\\\\text{tr}(\\\\mathbf{K}^{-1} \\\\tilde{\\\\mathbf{K}}) + (\\\\mu - \\\\tilde{\\\\mu})^\\\\top \\\\mathbf{K}^{-1} (\\\\mu - \\\\tilde{\\\\mu}) + \\\\ln \\\\frac{|\\\\mathbf{K}|}{|\\\\tilde{\\\\mathbf{K}}|} - M\\\\right),$$\\n\\nand we can estimate the mean, kernel and noise variance by minimizing $D_{KL}$. While it is difficult to gauge how much a probability density is enough to obtain a good model, Eq. 4 is a \u201cdistance\u201d that goes to 0 as the difference between two distributions reduces. One may choose to do early stopping or model selection based on how close Eq. 4 is to 0. Through information theory, we also know that the KL divergence in Eq. 4 describes the number of extra bits (or nats) to encode the multivariate normal $N(\\\\tilde{\\\\mu}, \\\\tilde{\\\\mathbf{K}})$.\\n\\nOverall we found the KL divergence in Eq. 4 relatively more interpretable than the marginal likelihood in Eq. 3.\"}"}
{"id": "8svLJL54sj8", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The KL divergence in Eq. 4 introduces a different optimization landscape than the marginal likelihood in Eq. 3. The KL divergence also makes use of the matching dataset $D'$ in a way that the marginal likelihood cannot. In fact, all matching inputs in the marginal likelihood in Eq. 3 are implicit: all inputs are passed in to mean/kernel functions, and so there is no way that Eq. 3 can be informed that some inputs are the same across tasks. As shown in \u00a74, the KL divergence in Eq. 4 interestingly led to better results in our experiments.\\n\\n### 3.3 Theoretical Analyses\\n\\nWhile it is nontrivial to prove regret bounds for general scenarios without strict assumptions, it is straightforward to show a regret bound for our method with objective $D_{KL}$ of Eq. 4 in the matching dataset case where BO is running on a finite set of inputs (Wang et al., 2018b).\\n\\n**Theorem 1.** Let $N \\\\geq \\\\frac{4 \\\\log_6 \\\\delta}{T} + 2$. With probability at least $1 - \\\\delta$, simple regret in $T$ iterations of Alg. 1 with special cases of either GP-UCB or PI satisfies\\n\\n$$R_T < O\\\\left(\\\\sqrt{\\\\frac{1}{N} - \\\\frac{T}{\\\\log 1 - \\\\delta}}\\\\right).$$\\n\\n(5)\\n\\nMore details can be found at \u00a7D. Theorem 1 shows that the regret bound has a linear dependency on the observation noise $\\\\sigma$. This is expected because in practice, we select the best observation rather than best function value (before observing a noisy version of it) to compute the simple regret. Another reason is that we learn the noise parameter $\\\\sigma$ jointly with the kernel, as shown by Eq. 4. Hence when computing acquisition functions, the noise $\\\\sigma$ is always included in the predicted variance.\\n\\nIntuitively, the more sub-datasets we have in the dataset, the larger $N$ is, the better we are able to estimate the GP model, and the closer the regret bound is to the case where the GP model is assumed known. Interestingly, the number of BO iterations $T$ makes the regret smaller in the second term but larger in the first term in Eq. 5. Usually as we get more observations, we get more information about the maximizer, and we are able to optimize the function better. However, as we get more observations on the new function, GP conditional predictions have more freedom to deviate from the ground truth (see Lemma 1 of Wang et al. (2018b)). As a result, we get less and less confident about our predictions, which is eventually reflected in a looser regret upper bound.\\n\\nIt is tempting to prove similar bounds for more general settings where inputs are not the same across all sub-datasets and BO happens in continuous space. Though the only prerequisite is to show that the difference between the learned mean/kernel and the ground truth mean/kernel is small, this prerequisite is as difficult as showing we can find a model that has bounded generalization error across the entire continuous input space of an arbitrary function. Instead of making unrealistic assumptions just to satisfy such prerequisite, we leave the regret bound for general settings as an open question.\\n\\n### 4 Experiments\\n\\nOur goal in this paper is to provide a practical approach for hyperparameter optimization when we are given data on a range of tasks over the same search space. To analyze the effectiveness of our proposal, we take the optimizer hyperparameter tuning problem in deep learning as a case study. Our implementation of HyperBO is based on JAX (Bradbury et al., 2018).\"}"}
{"id": "8svLJL54sj8", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"momentum. Each task is defined by a task dataset (e.g. ImageNet), a specific neural network model (e.g. ResNet50), and a batch size. Tab. 1 shows all the tasks that we consider in the tuning dataset. We used an existing code base (Gilmer et al., 2021) for neural network model training. The dataset used roughly 12,000 machine-days of computation for approximately 50,000 hyperparameter evaluations.\\n\\nFor each task, we trained the model on the task dataset repeatedly using Nesterov momentum (Nesterov, 1983; Sutskever et al., 2013), with the task's minibatch size, with different hyperparameter settings drawn from the 4-dimensional search space detailed in Tab. 2. We tuned the base learning rate, $\\\\eta$, on a log scale, the momentum, $\\\\beta$, with $1 - \\\\beta$ on a log scale, and the polynomial learning rate decay schedule power $p$ and decay steps fraction $\\\\lambda$. We used a polynomial decay schedule with the following form:\\n\\n$$\\n\\\\eta_{\\\\tau} = \\\\eta_{1000} + (\\\\eta - \\\\eta_{1000}) (1 - \\\\min(\\\\tau, \\\\lambda T))^{\\\\lambda p}\\n$$\\n\\nwhere $\\\\tau$ is the training step and $T$ is the total number of training steps for the task.\\n\\n| Task Dataset | Model | Batch Sizes |\\n|--------------|-------|-------------|\\n| CIFAR10      | Wide ResNet | {256, 2048} |\\n| CIFAR100     | Wide ResNet | {256, 2048} |\\n| Fashion MNIST| Max pool CNN| ReLU | {256, 2048} |\\n| Fashion MNIST| Max pool CNN| tanh | {256, 2048} |\\n| Fashion MNIST| Simple CNN  |              | {256, 2048} |\\n| ImageNet     | ResNet50   | {512, 1024, 2048} |\\n| LM1B         | Transformer| {2048}       |\\n| MNIST        | Max pool CNN| ReLU | {256, 2048} |\\n| MNIST        | Max pool CNN| tanh | {256, 2048} |\\n| MNIST        | Simple CNN  |              | {256, 2048} |\\n| SVHN (no extra) | Wide ResNet | {256, 1024} |\\n| WMT15 German-English | xformer | {64} |\\n| uniref50     | Transformer| {128}        |\\n\\nTable 2: 4-dimensional input search space (see text for more details)\\n\\n| Hyperparameter | Range                      | Scaling |\\n|----------------|----------------------------|---------|\\n| $\\\\eta$         | $[10^{-5}, 10]$         | Log     |\\n| $p$            | $[0, 2]$                   | Linear  |\\n| $1 - \\\\beta$    | $[10^{-3}, 1]$            | Log     |\\n| $\\\\lambda$      | $[0, 0.99]$                | Linear  |\\n\\nWe collected two types of data: matched and unmatched data. Matched data used the same set of uniformly-sampled hyperparameter points across all tasks and unmatched data sampled new points for each task. All other training pipeline hyperparameters were fixed to hand-selected, task-specific default values. All of our tasks are classification problems, so they all used the same training loss, although occasionally task-specific regularization terms were added. For each trial (training run for a single hyperparameter point), we recorded validation error (both cross entropy error and misclassification rate). In many cases, poor optimizer hyperparameter choices can cause training to diverge. We detected divergent training when the training cost became NaN and then marked the trial but did not discard it. Please see the Appendix, supplementary material, and code (Onomous, 2021) for additional details about the tasks and training procedure. The different tuning tasks vary in difficulty and numbers of data points, but generally there are roughly 500 matched datapoints and 1500 unmatched datapoints per tuning task. For unmatched data only, we attempted to generate roughly similar numbers of non-divergent points across tasks, so tasks with a higher probability of sampling a hyperparameter point that causes training to diverge will tend to have more trials.\\n\\n4.2 DESCRIPTION OF ALL COMPARED METHODS\\n\\nOur method HyperBO has several variants including using different acquisition functions and different objectives. In \u00a74, unless otherwise mentioned, we used a thresholded probability of improvement (PI) as the acquisition function. We set PI in line 5 of Alg. 1 as $\\\\hat{\\\\mu}_{Df}(x) - \\\\max_{t}(y_t + 0.1)\\\\hat{\\\\sigma}_{Df}(x)$. We empirically evaluated a variety of acquisition functions, but found PI thresholded at 0.1 to be surprisingly effective. Because we model the observations as log error rate, this actually trades off exploration and exploitation - i.e. with larger error rates this seeks relatively more substantial improvements than with small error rates. The list of 5 different acquisition functions we tested is as follows: PI with 0.1 threshold, expected improvement and UCB with 2, 3, 4 coefficients. See their comparisons at \u00a7E.5.\\n\\nWe use $H^*_{NLL}$ to denote HyperBO with negative log marginal likelihood as the objective and $H^*_{KL}$ to denote HyperBO with KL divergence on matching datapoints as objective. Both objectives are optimized via L-BFGS (Nocedal, 1980) with $\\\\mu(x) = \\\\theta_0^{\\\\top} \\\\tanh(\\\\theta_1^{\\\\top} x)_{k}(x,x) = \\\\text{Mat\u00e9rn32}(\\\\tanh(\\\\theta_1^{\\\\top} x), \\\\tanh(\\\\theta_1^{\\\\top} x'))$, $\\\\theta_1 \\\\in \\\\mathbb{R}^{4 \\\\times 8}$. These two settings of HyperBO are relatively representative of the performance of variants of HyperBO. See comparisons over objectives at \u00a7E.4.\\n\\nOur baselines include (a) Rand: Random search in the corresponding scaled space in Tab. 2. (b) STBO: Single-task BO where in every BO iteration, STBO optimizes the GP hyperparameters via marginal likelihood on data of the test task. This implementation corresponds to the basic off-the-shelf BO setups. (c) STBOH: Single-task GP-UCB with a hand-tuned prior on hyper-parameters including\"}"}
{"id": "8svLJL54sj8", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Bayesian viewpoint of \u00a72, we assume that the overall setting of the hyperparameter optimization task is defined by a parameter $\\\\theta \\\\sim p(\\\\theta; \\\\alpha)$; mean and kernel functions $\\\\mu$ and $k$ are drawn from $p(\\\\mu, k|\\\\theta)$. The independent function samples $\\\\{f_i\\\\}_{i \\\\in [N]}$ are themselves draws from $\\\\text{GP}(\\\\mu, k)$. The generative story is as follows:\\n\\n- Draw GP parameter $\\\\theta$ from $p(\\\\theta; \\\\alpha)$ and observation noise parameter $\\\\sigma$ from $p(\\\\sigma; \\\\alpha)$.\\n- Draw mean function $\\\\mu$ and kernel function $k$ from $p(\\\\mu, k|\\\\theta)$.\\n- For each task $i$ from 1 to $N$,\\n  - Draw a function $f_i$ from $\\\\text{GP}(\\\\mu, k)$.\\n  - For each data point $j$ from 1 to $M_i$,\\n    - Given input $x(i)_j$, we draw the observation $y(i)_j \\\\sim N(f(i)(x(i)_j), \\\\sigma^2)$.\\n\\nWe simplify this hierarchical setting by defining $p(\\\\mu, k|\\\\theta)$ to be a sum of Dirac delta functions: both mean function $\\\\mu$ and kernel $k$ are deterministic functions parameterized by $\\\\theta$. Thus, we can infer GP parameter $\\\\theta$ and noise $\\\\sigma$ from their posterior $p(\\\\theta, \\\\sigma|D_N \\\\cup D_f; \\\\alpha)$ and obtain an informed prediction for the target function $p(f|D_N \\\\cup D_f) = \\\\int_\\\\theta p(f|\\\\theta) \\\\int_\\\\sigma p(\\\\theta, \\\\sigma|D_N \\\\cup D_f; \\\\alpha)$.\\n\\nIn other words, we learn function $f$ from observations on all other conditionally i.i.d. function samples $f_1, \\\\cdots, f_N$. We forgo a fully Bayesian approach that samples from the posterior over $\\\\theta$ at every BO iteration, although our method, HyperBO, can be viewed as a type-II maximum likelihood approximation of such a Bayesian solution.\\n\\n### Computational Complexity\\n\\nThe marginal likelihood in Eq. 2 naturally decomposes into a sum of GP data likelihood terms on each sub-dataset $D_f_i$. The time complexity to compute Eq. 2 is $O(M^3N)$, where $N$ is the number of sub-datasets and $M = \\\\max_{i=1}^N M_i$ is the maximum number of data points for these sub-datasets.\\n\\nNotice that our method scales linearly in the number of tasks, $N$, in contrast to the cubic $O(M^3N^3)$ scaling of multi-task or contextual BO methods (Swersky et al., 2013; Bardenet et al., 2013; Poloczek et al., 2016; Yogatama & Mann, 2014). The only cubic cost of HyperBO is on the number of data points in sub-datasets.\\n\\nTo train a GP with $K$ optimization steps on Eq. 2, the time complexity is $O(M^3NK)$. The distance regularizers introduced in \u00a73.2 requires estimating mean and covariance, which takes $O(M^2N)$ for matrix multiplication. The KL divergence in Eq. 4 has complexity $O(M^3)$ to compute and $O(M^3K)$ to optimize.\\n\\nIf there is any better probabilistic model than a GP to fit the data with less compute time, we can easily swap it in and reduce the $O(M^3)$ complexity that the GP contributed to the $O(M^3N)$ complexity of Eq. 2. For example, if we approximate a GP with a linear model on $V$ random features (Rahimi et al., 2007), the complexity of Eq. 2 becomes $O(V^3N)$. Another example is to train Eq. 2 with stochastic optimization methods, where the complexity of Eq. 2 on the full dataset can be reduced to $O(B^2MN)$, where $B$ is the mini-batch size. Running stochastic optimization will then take $O(B^2MNK)$, where $K$ is the number of optimization epochs.\"}"}
{"id": "8svLJL54sj8", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"negativity. In practice, we may also choose to add small epsilon values to the diagonal terms of both\\nD task in Eq. 6. However, pseudo KL divergence\\nminimize this pseudo KL divergence\\nKL divergence\\nD pseudo-inverse of the non-degenerate one be\\nLet the degenerate Gaussian be\\np\\nD possible to derive a pseudo KL divergence\\nN\\nK number of matching data points,\\nA given that there exists a full rank matrix\\nIf\\nN the optimization objective stays the same as reflected by the derivation below.\\nHere the variables we care about,\\n\u00b5,k,\u03c3\\nsimply remove the constants and do the following\\nbetween two Gaussians in the non-degenerate case. In practice, when we minimize Eq. 4, we can\\nKL divergence for a degenerate multivariate Gaussian\\nNLL and KL, interpreted as MAP with a data-dependent prior.\\nnumber of training tasks. In the end of this section, we introduce a new objective function, combining\\nfor most of our matching data settings in \u00a74.1: the number of matching data points is greater than the\\nIn \u00a73, we presented NLL and KL divergence as objectives. Below we derive the KL divergence\\nUnder review as a conference paper at ICLR 2022\\n\\n\\\\[ \\\\text{KL} = \\\\int q(x) \\\\log \\\\left( \\\\frac{q(x)}{p(x)} \\\\right) dx \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\sum_{i=1}^{d} \\\\left( \\\\mu_i^2 + \\\\sigma_i^2 - \\\\mu_i^2 \\\\sigma_i^2 - 1 \\\\right) \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1} \\\\mu + R \\\\]\\n\\n\\\\[ \\\\text{KL} = -\\\\frac{1}{2} \\\\log |A| + \\\\text{tr}(M) + \\\\mu^T \\\\mu - \\\\mu^T A^{-1"}
{"id": "8svLJL54sj8", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Combining marginal likelihood and distance metrics\\n\\nIt is also possible to additively combine the KL divergence with the negative log marginal likelihood objective, and treat this distance as a regularizer. In the case of KL divergence, it is equivalent to adding a data-dependent prior on the GP itself:\\n\\n$$\\\\mu, K \\\\sim \\\\exp\\\\left(-\\\\lambda D_{KL}(N(\\\\hat{\\\\mu}, \\\\hat{K}), N(\\\\mu, K))\\\\right)$$\\n\\nfor some normalization constant $Z$, and the posterior is\\n\\n$$p(\\\\mu,k,\\\\sigma^2 | D_{N}) \\\\propto p(D_{N} | \\\\mu,k,\\\\sigma^2) \\\\exp\\\\left(-\\\\lambda D_{KL}(N(\\\\hat{\\\\mu}, \\\\hat{K}), N(\\\\mu, K))\\\\right).$$\\n\\n(8)\\n\\nWe can then obtain an MAP estimation for the unknown functions and variables $\\\\mu,k,\\\\sigma^2$.\\n\\n**Details of regret bounds**\\n\\nTheorem 1 is a direct result of Theorem 16 in Wang et al. (2018b). The only subtle difference is that we adopted a biased estimate of the covariance matrix, a factor of $N^{-1}$ different from the unbiased estimate. But this can be corrected in the acquisition functions. We provide more details below.\\n\\n**Proposition 2.** For any $M,d,N \\\\in \\\\mathbb{Z}^+$, $x \\\\in \\\\mathbb{R}^{M \\\\times d}$, $\\\\mu \\\\in \\\\mathbb{R}^M$, $V \\\\in \\\\mathbb{R}^{N \\\\times M}$ and $K = V^T V$, there exists a Gaussian process $GP(\\\\hat{\\\\mu}, \\\\hat{k})$ such that $D_{KL}(N(\\\\mu, K), N(\\\\hat{\\\\mu}(x), \\\\hat{k}(x))) \\\\equiv 0$.\\n\\nProposition 2 is easy to show. We can train a simple memory based model for mean function $\\\\hat{\\\\mu}$ and kernel $\\\\hat{k}$. The model stores each element of vector $\\\\mu$ and matrix $K$ at the corresponding locations in input $x$. When making a prediction on a new input $x' \\\\in \\\\mathbb{R}^d$, the model simply retrieves the values of the closest element in $x$.\\n\\nGiven Proposition 2, a regret bound follows (Wang et al., 2018b). By Proposition 2, we are able to obtain a $GP(\\\\mu,k)$ where $\\\\mu(x)$ and $k(x)$ are equal to the sample mean and covariance on a matching dataset $(x, y)$, following the notations in \u00a73.2. Hence, our problem setup is consistent with the case with discrete input space in Wang et al. (2018b). The following theorem is a rewrite of Theorem 16 in Wang et al. (2018b), taking into account our biased estimators.\\n\\n**Theorem 3.** Assume there exist constant $c \\\\geq \\\\max_{x \\\\in X} k(x)$ and a training dataset is available whose size is $N \\\\geq 4 \\\\log_6 \\\\delta + T + 2$. Define\\n\\n$$\\\\iota_t = \\\\sqrt{\\\\frac{6}{N - 3 + t + 2} \\\\log_6 \\\\delta + 2 \\\\log_6 \\\\delta},$$\\n\\n$$b_t = \\\\frac{1}{N - t \\\\log_6 \\\\delta},$$\\n\\nfor any $t \\\\in [T]$, and $\\\\rho_T = \\\\max_{A \\\\subseteq X, |A| = T/2} \\\\log |I + \\\\sigma^{-2} k(A)|$. Then, with probability at least $1 - \\\\delta$,\\n\\n$$\\\\text{best-sample simple regret in } T \\\\text{ iterations of meta BO with GP-UCB that uses } \\\\zeta_t = \\\\frac{6N}{(N - 3 + t + 2) \\\\log_6 \\\\delta + 2 \\\\log_6 \\\\delta} + \\\\frac{2N \\\\log(3\\\\delta)}{1}$$\\n\\nas its hyperparameter in $\\\\alpha_{GP-UCB}$ satisfies\\n\\n$$r_{GP-UCB}^T \\\\leq \\\\eta_{GP-UCB} \\\\sqrt{2c\\\\rho_T T \\\\log(1 + c\\\\sigma^{-2}) + \\\\sigma^2 - (2 \\\\log(3\\\\delta))^2 \\\\sigma^2}.$$\\n\\n(9)\\n\\nWith probability at least $1 - \\\\delta$, the best-sample simple regret in $T$ iterations of meta BO with PI that uses $\\\\hat{f}_* \\\\geq \\\\max_{x \\\\in X} f(x)$ as its target value satisfies\\n\\n$$r_{PI}^T < \\\\eta_{PI} \\\\sqrt{2c\\\\rho_T T \\\\log(1 + c\\\\sigma^{-2}) + \\\\sigma^2 - (2 \\\\log(3\\\\delta))^2 \\\\sigma^2}.$$\\n\\nwhere $\\\\eta_{PI} = \\\\frac{\\\\hat{f}_* - \\\\mu_{\\\\tau - 1}(x^*) + \\\\sigma_2}{\\\\sqrt{k_{\\\\tau - 1}(x^*)} + 2 \\\\sigma_2}$. \\n\\n$\\\\tau = \\\\arg\\\\min_{t \\\\in [T]} k_{t - 1}(x_t)$.\"}"}
{"id": "8svLJL54sj8", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The proof can be found in Wang et al. (2018b). Theorem 1 is a condensed version of Theorem 3. While Theorem 3 provides us with some understanding of HyperBO in a specific setting, in practice, we need to query in a continuous input space that goes beyond the finite set of points present in the training dataset. It may or may not be possible to obtain data on a wide range of tasks to ensure $N \\\\geq 4 \\\\log_6 \\\\delta + T + 2$. In fact, in all of our experiment, this criterion on number of tasks is not satisfied. However, we still obtained good performance.\\n\\n### EXPERIMENT DETAILS AND MORE RESULTS\\n\\nIn this section, we provide more detailed setups and empirical results on the impact of objective functions and acquisition functions in HyperBO. All experiment setups are the same as \u00a7E.2.1: offline and holding out related tasks.\\n\\n#### E.1 BASELINES\\n\\nOur first set of baselines include those that do not use information from training tasks:\\n\\n- **Rand**: Random search in the corresponding scaled space in Tab. 2.\\n- **STBO**: Single task BO with a constant mean function, Matern32 kernel and PI acquisition function (same as above). Every BO iteration, STBO optimizes the GP hyperparameters via marginal likelihood on data of the test task. This implementation corresponds to the basic off-the-shelf BO setups.\\n- **STBOH**: Single task GP-UCB (coefficient=1.8) with constant mean, Matern52 kernel and hand-tuned prior on hyper-parameters including UCB coefficient. Specifically, log amplitude follows Normal(-1, 1), log length scale (one per input parameter) follows Normal(0,1), and log observation noise variance follows Normal(-6, 3). The hyperparameters are post-processed by tensorflow-probability's `SoftClip` bijector to constrain the values between 1-st and 99-th quantiles. These prior distributions were manually tuned to obtain reasonable convergence rates on 24 analytic functions in COCO (Hansen et al., 2021). The GP parameters are then optimized via maximum marginal likelihood every BO iteration.\\n\\nFor multi-task BO baselines, we included scalable methods that replace the GP with a regression model that can be trained using SGD and thus scales linearly in the number of observations. Following the multi-task setup of Springenberg et al. (2016), we jointly trained a 5-dimensional embedding of each task, which was then added to the input of the following two models.\\n\\n- **MIMO**: We trained an ensemble of feedforward neural networks with shared subnetworks (Havasi et al., 2020). We used 1 shared dense layer of size 10 and 2 unshared layers of size 10. We used tanh activation based on (Snoek et al., 2015, Figure 2). The network has one output unit with linear activation and another with $\\\\text{softmax}(10^{-4}, 1)$ activation, corresponding respectively to the mean and standard deviation parameters of a normal distribution. We trained for 1000 epochs using the Adam optimizer with learning rate $10^{-4}$ and batch size 64.\\n- **RFGP**: We used the open-source implementation of approximate GP by Liu et al. (2020). We trained for 1000 epochs using the Adam optimizer with learning rate $10^{-3}$ and batch size 64.\\n\\nAll methods share the same input and output warping. The input warping is done according to the scaling function in Tab. 2:\\n\\n$$\\\\eta \\\\rightarrow \\\\log \\\\eta, \\\\quad 1 - \\\\beta \\\\rightarrow \\\\log(1 - \\\\beta)$$\\n\\nThe output warping is done for the best validation error rate $r \\\\rightarrow -\\\\log(r + 10^{-10})$.\\n\\n### E.2 OFFLINE BO EXPERIMENTS\\n\\nIn all offline experiments, we ran offline BO on the data from the test task starting from zero initial data from this task. Each method was repeated 5 times with different random seeds to initialize its model. We ran all methods without de-duplication to best simulate online BO. We evaluate on regret on error rate which denotes the simple regret on the finite set of data points in each tuning sub-dataset.\"}"}
{"id": "8svLJL54sj8", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nFigure 6: Performance profiles for outperforming the median of best error rates at the (a) 25th BO iteration, (b) 50th BO iteration and (c) 100th BO iteration.\\n\\nFigure 7: The left most is a summary of the BO convergence of all methods: the median and 20/80 percentiles of the regrets on error rates over 115 BO runs: 23 tasks and each with 5 repeats of different random seeds. We also show violin plots on its two vertical slices at 50th and 100th iteration, where the white dot is the median and the black line is the 20/80 percentile. Overall, HyperBO methods H* NLL and H* KL are able to achieve the lowest regret on error rate on the majority of tasks.\\n\\nE.2.1 HOLDING OUT RELEVANT TASKS\\n\\nWe first conducted experiments in a setting where a new task dataset is presented, and a BO method is trying to tune the optimizer hyperparameters for a selected model on that task dataset. A training dataset for meta BO is composed of at most 18 tuning sub-datasets on training tasks that do not involve the same task dataset as the test task. All methods then proceed to solve the test task on the new task dataset. Fig. 6 shows performance profiles of the BO methods described in \u00a74.2. The performance profiles show the fraction of all test tasks that each method is able to outperform a baseline criterion at each BO iteration.\\n\\nWe chose the criterion to be the median of best error rates achieved by all methods at 3 different BO iterations: 25th, 50th or 100th. The larger the fraction of tasks at each BO iteration, the better the method is. From all 3 criteria, we can see that MIMO is able to outperform other methods in the beginning 10 to 20 BO iterations, but its leading position soon gets surpassed by HyperBO (H* NLL and H* KL) and STBOH. HyperBO methods are gaining a similar if not larger fraction than the best alternative, STBOH, throughout BO iterations. Fig. 6 (c) has the most stringent performance criterion, and it shows that HyperBO with the KL objective outperforms HyperBO with the NLL objective in this set of experiments with a small margin. And both methods in HyperBO are doing considerably better than others.\\n\\nFig. 7 illustrates the BO convergence curves of all competing methods, together with the vertical slices at the 50th and 100th iterations. RFGP and STBO are both falling much behind Random search. STBO trains the GP on the data that the GP suggests to query, which creates a loop that could be harmful for data acquisition. Optimizing the marginal data likelihood on at most 100 datapoints in fact may not lead to a better model than random initialization (see Tab. 5 in \u00a7F). Surprisingly, RFGP, though equipped with the tuning dataset and initially reached some good values, performed similarly to STBO in the end. Clearly, the contextual information learned by RFGP did not generalize to a new task. On the other hand, MIMO is able to obtain a slightly better error rate than STBOH.\\n\\nFig. 6 and Fig. 7 both show that learning the GP prior through data like what HyperBO does is performing much better than other meta BO methods, and it is a more principled and effective approach.\\n\\n---\\n\\nWe show performance relative to a baseline because of varying scales across tasks.\"}"}
{"id": "8svLJL54sj8", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We now investigate the impact of number of training tasks on the performance of meta BO methods. With only 3 training tasks, there are reasonable fluctuations in the results but overall the trend of Table 3: The mean and standard error of best validation error rates (change much as the fraction of training data increases from 5\\\\% relatively no change for RFGP. We also found that the performance of HyperBO (H* NLL) does not change much as the fraction of training data increases from 5\\\\%.\\n\\nFig. 9 shows how the simple regret changes as the fraction of training data grows. Below this comparison, as H* KL only makes use of matching data. Uniformly randomly, which breaks the structure of matching data. Hence we do not include H* KL in our current tuning task for testing, and then remove randomly selected training datasets from the rest.\\n\\nOne remaining question is, how does the number of tasks increased from 8 to 18. RFGP, however, fails to learn from training tasks possibly because it did not learn good task embeddings for GP regression models.\\n\\nIn Fig 8 we show the BO simple regrets on tasks from Table 1 (except ImageNet ResNet50 2048) of all data that we have access to for each task. In particular, we set the percentage of remaining M affects the performance of meta BO methods. We analyze the impact of M on over 2\\\\% data to be of all data that we have access to for each task. In particular, we set the percentage of remaining M of all data that we have access to for each task. In particular, we set the percentage of remaining M that affect the performance of meta BO methods. We analyze the impact of M on over 2\\\\% data to be of all data that we have access to for each task. In particular, we set the percentage of remaining M.\\n\\nIn \u00a72, the number of data points in each training tasks, i.e., 10% and 50%. We found that the performance of HyperBO variants were able to reduce the simple regret as more training tasks are given. Interestingly, both H* NLL and H* KL are already slightly better than Rand and STBOH when they started off.\\n\\nIn Table 4 we show the hyperparameter tuning performance of different meta BO models trained on different number of training tasks. To analyze the performance of different meta BO models, we use the test task. We show results of the top 5 methods, and we highlight the lowest error rates in bold. In Table 5 we show the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the approach to obtain the GP prior than hand-tuning. As a reference, we include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test task. We include Tab. 3 which shows the error rates obtained by the top 5 methods in 100 BO iterations for each test"}
{"id": "8svLJL54sj8", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Aggregated BO results on 23 tasks (all in Table 1 except ImageNet ResNet50 2048 because of insufficient data) that uses models trained on 3 to 23 training tasks. Note that the models are never trained on the data from the test task that we run BO on. If the number of training tasks is less than 23, we first remove the tasks that involve the same task dataset as the test task and then remove others randomly until we reach the designated number of training tasks. The top left shows the median and 20/80 percentiles of regret on best validation error rate for each method. The rest are violin plots showing the regret for MIMO, H* NLL and H* KL, where white dots indicate the median and black lines the 20/80 percentiles.\\n\\nsuffers significantly from more data as the fraction of training data increases from 5% to 50%. It is not entirely clear why MIMO and RFGP have such behaviors. One conjecture is that neural network based Bayesian linear regression models may get too confident once the amount of data reaches a certain threshold. This means much less exploration if those models are used for BO.\\n\\nE.2.4 TRAINING ON ALL BUT ONE TASK\\nWe also studied the case where meta BO approaches have access to both training tasks that do not use the same task dataset and training tasks that use the same task dataset but different model configurations. This is especially common when we do architecture search: we aim to find the best model and we are tuning the optimizer hyperparameters for a new machine learning model given tuning data of the same task dataset on some other models.\\n\\nFor this section only, we added a new baseline, MAF: We refer to the meta BO method from Volpp et al. (2020) as MAF (Meta Acquisition Function) to avoid confusion. MAF used reinforcement learning to learn an acquisition function modeled by a neural network over a set of transfer learning tasks. All MAF results were generated using the code from Volpp et al. (2020). See App. E.6 for experimental details. As MAF takes significantly longer to run than HyperBO and other methods, we only include its results for this section.\\n\\nWe carried out a series of leave-one-out experiments, where we picked one task as the BO test function and let meta BO methods train on the remaining tasks. In Fig. 10, we aggregated results from all 23 tasks to show the trend of how each method performs. The conclusions are similar to those from \u00a7E.2.1. As expected, STBO without any tricks to avoid pitfalls of vanilla BO did not show very good results. We inspected its learned GP which mimicked a\"}"}
{"id": "8svLJL54sj8", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Aggregated BO results on 23 tasks (all in Table 1 except ImageNet ResNet50 2048 because of insufficient data) that uses models trained on 0.2% to 90% of data in each task. Note that the models are never trained on the data from the test task that we run BO on. The top left is the median and 20/80 percentiles of simple regret in log scale. The rest of the figures are simple regret violin plots for MIMO and H* NLL.\\n\\nFigure 10: Aggregated leave-one-out BO convergence results on 23 tasks, each with 5 repeats using different random seeds. The left most is the median and 20/80 percentiles of the regrets on error rates. We also show violin plots on its two vertical slices at 50th and 100th iteration, where the white dot is the median and the black line is the 20/80 percentile.\\n\\nDirac function that is flat almost everywhere except some locations, and hence it got very confident that it landed at a good spot and lost its ability to explore.\\n\\nSTBOH, on the other hand, achieved very competitive results. This is because it used hand-tuned priors on all of its GP parameters, although they were tuned on somewhat different problems than the ones we consider. As part of the goals of meta learning, we would like to show that it is possible for meta BO methods to exceed or at least match STBOH.\\n\\nBoth HyperBO variants obtained better results than the hand-tuned STBOH. Especially in the beginning few BO iterations, it is able to locate much better hyperparameters than all other methods.\\n\\nTab. 4 presents mean and standard error of the best validation error rates achieved in 100 BO iterations on the 23 tasks. HyperBO and its variants were able to achieve the best performance on 20 out of 23 tasks. In Fig. 11, we show the optimization curves of 4 individual tasks that are considered most...\"}"}
{"id": "8svLJL54sj8", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 14: We compare the performance of 5 different acquisition functions under 3 settings of objectives in HyperBO. Overall, PI and EI outperform UCB with different coefficient values. But HyperBO with UCB variants still outperforms STBOH, which is roughly the best baseline according to the main results in Fig. 6.\\n\\n**DISCUSSION**\\n\\nIn this work, we focused on the question of how to make use of multi-task data to enable better Bayesian optimization. For the convenience of our investigation, we made simplifications such as sequential evaluations and a shared search space across tasks, although these are mostly unnecessary (see below). Our method also relies on an important assumption: functions of all tasks are i.i.d. samples from the same GP. In this section, we explore how reasonable the i.i.d. assumption is and discuss extensions to our work that would enable even more flexible uses.\\n\\n**Assumption on i.i.d. GP samples.** To get a better idea on how much our assumptions helped on training the GP, we compare NLLs associated with 23 tasks in \u00a74.1 with models obtained via 3 scenarios:\"}"}
{"id": "8svLJL54sj8", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We also computed the (pseudo) KL divergence across all matching datasets in the last columns of Table 5. See Appendix C for a comprehensive analysis on pseudo KL divergence for degenerate multivariate Gaussians. Note that pseudo KL divergence can be negative. Here we use pseudo KL divergence if required by the matching dataset. Again, single-task training leads to unstable (pseudo) KL values, sometimes even higher than without training (No training). Comparing NLLs of the test tasks using models without training and trained via marginal likelihood leads to much more stable and lower values for KL. This indicates that the model learned to predict the assumption with ours on i.i.d. and so lower NLLs of model will contribute to matching the assumption of typical BO methods. By enhancing the assumption with ours on i.i.d. and so lower NLLs of model will contribute to matching the assumption of typical BO methods, the test function should look like a sample from our model. Hence, by the assumption of typical BO methods, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from our model, the test function should look like a sample from"}
{"id": "8svLJL54sj8", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022.\\n\\nSimilarly to the sample mean/covariance estimate, which is known to help better selection of BO query points by Theorem 1.\\n\\nBatch evaluation.\\n\\nFor simplicity of this paper, we did not consider batch evaluation but rather only focused on the prior selection dimension of the challenges in BO. However, it is straightforward to adopt any batch BO methods in conjunction with HyperBO to support obtaining observations in parallel. For example, we can directly use batch methods in Snoek et al. (2012), Kathuria et al. (2016), or Wang et al. (2017) to replace line 5 of Alg. 1.\\n\\nHigh-dimensional and large scale data.\\n\\nSimilar to batch BO, our method can also be naturally combined with most high-dimensional and large scale BO methods to offer more capabilities. For these cases, typically a probabilistic model different from vanilla GPs may be adopted. In line 2 of Alg. 1, it is straightforward to adapt our method to optimize the cumulative marginal likelihood in Eq. 3 instead for the new model. Our meta-learning idea in this paper in fact also brings benefit to high-dimensional and large scale BO methods so that they can better identify their critical special structures, e.g. low-dimensional embedding Wang et al. (2016), cylindrical kernels Oh et al. (2018) or additive Mondrian kernels Wang et al. (2018a).\\n\\nDifferent search spaces.\\n\\nRoughly speaking, there could be two circumstances for difference search spaces. Case I is that tasks share the same search variables, but the search ranges for some variables are different. For example, we may have each function $f_i: X_i \\\\rightarrow \\\\mathbb{R}, i \\\\in \\\\mathbb{N}$ and $X_i = \\\\prod_{d=1}^{d} [l_{ij}, h_{ij}] \\\\subset \\\\mathbb{R}$. In this case, our solution still applies by simply setting a union search space as $X = \\\\bigcup_{i=1}^{N} X_i$ for learning and use the designated search space of new tasks for optimization.\\n\\nCase II is more complicated: the search space for each function $f_i$ is $X_i \\\\subset \\\\mathbb{R}^{d_i}$ and each dimension of $X_i$ may have a different meaning than another search space $X_j (i \\\\neq j)$. This paper does not have a solution for this scenario. Further research will be needed to reduce Case II to Case I which can be then immediately combined with HyperBO.\"}"}
