{"id": "0V5TVt9bk0", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A total of eleven experts, each with professional skills and extensive experience in photography, are invited to participate in the subjective labeling experiment of Q-Bench. The subjective experiment takes place in a laboratory environment with standard indoor lighting. A Dell-4K monitor, which supports a resolution of $3840 \\\\times 2160$, is used for displaying the interfaces. The screenshots of interfaces can be referred to in Fig. 5. Each expert annotates up to 30 images a day to avoid fatigue, and every annotation is carefully reviewed by at least three other experts before acceptance. In this way, we ensure the accuracy and rigor of the Q-Bench labels to the greatest extent possible. This, in turn, makes the performance testing capability of Q-Bench more precise and meaningful.\"}"}
{"id": "0V5TVt9bk0", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: The illustration of the annotation interfaces for the **LLVisionQA** dataset (questions, answers) on **Perception** ability, and the **LLDescribe** dataset (text description) on **Description** ability.\\n\\nA.2.1 EVALUATION DETAILS FOR PERCEPTION ABILITY\\n\\n[Special Note] Multi-choice Question vs Close-Set Inference for Kosmos-2: While Kosmos-2 performs generally well on the description and assessment tasks, we notice that it is hardly capable of answering a multi-choice question with the general prompt form applicable for other methods, as follows:\\n\\n**Question:** How is the clarity of the image?\\n\\n[IMAGE TOKEN] (Image)\\n\\nChoose between one of the following options: A. High (Correct) B. Medium (Wrong) C. Low (Wrong)\\n\\nFor most situations (86%) in our primary sample test with the prompts above, Kosmos-2 will directly append a new candidate (e.g., D. Excellent or D. Very Low) answer instead of choosing one option among them, denoted as prompt failure. This might be because the language model of Kosmos-2 has smaller capacity (1B) than other MLLMs that are based on LLaMA/MPT (7B/13B).\\n\\nConsidering that the prompt failure is actually not directly related with low-level perception, we try different prompt engineering techniques to reduce the prompt failure rate, and finalize with a simple modification which can limit the prompt failure to less than 10% in our sample set, as follows:\\n\\n**Question:** How is the clarity of the image?\\n\\n[IMAGE TOKEN] (Image)\\n\\nChoose between one of the following options: A. High (Correct) B. Medium (Wrong) C. Low (Wrong)\\n\\n#Answer:\\n\\nNevertheless, we are still not able to eliminate the prompt failures for Kosmos-2. Henceforth, to systematically remove the negative effect of prompt failures on multi-choice questions for Kosmos-2...\"}"}
{"id": "0V5TVt9bk0", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Perplexity-based close-set evaluation compared with normal evaluation on LLVisionQA; after eliminating the prompt failures, the results of Kosmos-2 significantly improved.\\n\\n| Sub-categories | Question Types | Quadrants of Low-level Concerns | Overall |\\n|----------------|----------------|---------------------------------|---------|\\n| Model (variant) | Yes-or-No       |                                 |         |\\n|                 | What            |                                 |         |\\n|                 | How             |                                 |         |\\n|                 | Distortion      |                                 |         |\\n|                 | Other           |                                 |         |\\n|                 | In-context      |                                 |         |\\n|                 | Distortion      |                                 |         |\\n|                 | Other           |                                 |         |\\n\\n|                           | random guess    | Kosmos-2 (normal)               | Kosmos-2 (close-set) |\\n|---------------------------|-----------------|---------------------------------|---------------------|\\n|                           | 50.00%          | 58.20%                          | 61.48%              |\\n|                           | 28.18%          | 29.13%                          | 37.13%              |\\n|                           | 33.30%          | 34.22%                          | 40.76%              |\\n|                           | 37.54%          | 38.10%                          | 40.04%              |\\n|                           | 38.49%          | 44.30%                          | 50.88%              |\\n|                           | 38.70%          | 40.93%                          | 45.30%              |\\n|                           | 36.50%          | 44.20%                          | 58.15%              |\\n|                           | 37.87%          | 41.47%                          | 47.26%              |\\n\\nFigure 6: Image of example (1).\\nFigure 7: Image of example (2).\\nFigure 8: Image of example (3).\\n\\n2, we conduct a choice-free special setting for it, i.e. close-set inference, via ranking the perplexity of different answers and choose the answer with minimum generative loss:\\n\\nHow is the clarity of the image? [IMAGE TOKEN] #Answer: High \u2192 loss:7.43 \u2192 \u2713 Choose this.\\nHow is the clarity of the image? [IMAGE TOKEN] #Answer: Medium \u2192 loss:7.56 \u2192 \u2717\\nHow is the clarity of the image? [IMAGE TOKEN] #Answer: Low \u2192 loss:7.92 \u2192 \u2717\\n\\nAs shown in Tab. 5, perplexity-based close-set inference can notably improve results of Kosmos-2.\\n\\nConsidering that it is still the MLLM with fewest parameters among the ten models, its results are decent at its model size. More importantly, they validate that our observation on the prompt failure is reasonable, and we will further delve deeper into this problem of MLLMs in our extended works.\\n\\nSettings for GPT Evaluation:\\nGiven GPT's inherent variability, identical prompts can yield non-definitive responses. To address the impact of such situations on our evaluation, we've implemented a 5-round voting strategy. Under this approach, we pose the same prompt as defined in the following templates five times, taking the popular votes of GPT's answers to determine the final outcome. Our human analysis on a sample set confirms that the 5-round voting strategy improves GPT evaluation accuracy from 93.2% to 98.4%, reducing errors to only 1/4 compared with the single-round evaluation.\\n\\nPrompt Templates for GPT Evaluation:\\n\\n#System: You are a helpful assistant that grades answers related to image quality and aesthetics. There are a lot of special terms or keywords related to image processing and photography. You will pay attention to the context of 'quality evaluation' when grading.\\n\\n#User: Assuming you are a grader, you will now be provided with a question [question] and a set of options [options] with option [options[0]] being the correct answer. Additionally, there will be an answer [answer] provided by a respondent. Please determine whether the respondent's answer is correct considering the context of the question. Even if the word choice is not completely the same, you can decide based on the given options and see whether the one in the answer is close enough to the given correct answer, The result is 1 if the answer is correct and else the result is 0. Please only provide the result in the following format: Result:\\n\\nExamples for GPT Evaluation:\\n\\n(1) \\\"Rephrased\\\" Answers.\\n(Fig. 6)\\nQuestion: Which is the brightest part in this image?\\nChoices: ['Capital letters E and S', 'ST', '18', '56']\\nMLLM Answer: 15\"}"}
{"id": "0V5TVt9bk0", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5-Round GPT Answers:\\n\\n| Score | Score | Score | Score | Score |\\n|-------|-------|-------|-------|-------|\\n| 1     | 1     | 1     | 1     | 1     |\\n\\n\u2192 Final Correctness after Voting:\\n\\n\u2713\\n\\n2) \\\"Distracted\\\" Answers.\\n\\n(Fig. 7)\\n\\nQuestion: How's the level of blur in the image?\\n\\nChoices: ['Some blur', 'Not blurry at all', 'Very blurry']\\n\\nMLLM Answer: The image has some blur, but it is not very blurry.\\n\\n5-Round GPT Answers:\\n\\n| Score | Score | Score | Score | Score |\\n|-------|-------|-------|-------|-------|\\n| 1     | 1     | 1     | 1     | 1     |\\n\\n\u2192 Final Correctness after Voting:\\n\\n\u2713\\n\\n3) \\\"Both\\\" Answers.\\n\\n(Fig. 8)\\n\\nQuestion: Is the little boy emphasized in the center of the composition of the image?\\n\\nChoices: ['No', 'Yes']\\n\\nMLLM Answer: A. No, the little boy is not emphasized in the center of the composition of the image. B. Yes,\\n\\n5-Round GPT Answers:\\n\\n| Score | Score | Score | Score | Score |\\n|-------|-------|-------|-------|-------|\\n| 1     | 0     | 0     | 1     | 0     |\\n\\n\u2192 Final Correctness after Voting:\\n\\n\u2717\\n\\nA.2.2 E VALUATION DETAILS FOR DESCRIPTor ABILITY\\n\\nGeneral Description Prompt for MLLMs:\\n\\n#User: Describe the quality, aesthetics and other low-level appearance of the image in details.\\n\\nSettings for GPT Evaluation:\\n\\nGiven GPT's inherent variability, identical prompts can yield non-definitive responses. To address the impact of such situations on our evaluation, we've implemented a 5-round average pooling strategy. Under this approach, we pose the same prompt as defined in the following templates five times, taking the mean result of GPT's answers to determine the final outcome. This method effectively mitigates the unpredictability associated with GPT, ensuring a more accurate score.\\n\\nPrompt Templates for GPT Evaluation:\\n\\n#System: You are a helpful assistant.\\n\\nCompleteness.\\n\\n#User: Evaluate whether the description [MLLM DESC] completely includes the low-level visual information in the reference description [GOLDEN DESC].\\n\\nPlease rate score 2 for completely or almost completely including reference information, 0 for not including at all, 1 for including part of the information or similar description. Please only provide the result in the following format: Score:\\n\\nPreciseness.\\n\\n#The precision metric punishes controversial low-level descriptions that output description contrasts with the reference, e.g., blur for clear, high quality for low quality, colorful for monotonous, noisy for clean, bright for dark.\\n\\nEvaluate whether output [MLLM DESC] precisely reflects reference [GOLDEN DESC].\\n\\nPlease rate score 2 for totally no controversial low-level description, 1 for less controversial low-level description than matched description, and 0 for more controversial low-level description than matched description. Please only provide the result in the following format: Score:\\n\\nRelevance.\\n\\n#User: Evaluate whether the description [MLLM DESC] is relevant to the low-level visual information, which may include blur, noise, exposure, artifact, color, lighting, focus, composition, etc.\\n\\nPlease rate score 2 for completely relevant, 1 for partly relevant, and 0 for totally irrelevant.\"}"}
{"id": "0V5TVt9bk0", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the LLVisionQA dataset, we curate three question types, Yes-or-No, What, and How to simulate multiple query forms from humans. The details of the three question types are defined as follows.\\n\\nType 1: Yes-or-No Questions. The fundamental type of questions is Yes-or-No, i.e., judgments. Specifically, we notice that some MLLMs especially prefer to respond with yes rather than no. To reduce such biases in our benchmark, though designing questions with answers as yes is easier, we ensure that around 40% of all judgments are with correct answers as no, via querying on contrastive low-level attributes or non-existing low-level attributes. We further measure the bias levels of different MLLMs and present a further de-biased evaluation among them, as discussed in Sec. A.3.2.\\n\\nType 2: What Questions. Despite Yes-or-No judgments, the what questions are also a common type of queries in recent MLLM benchmarks such as Lu et al. (2023). In Q-bench, they classify low-level attributes in pictures (e.g., What distortion occurs in the image?), or associated context given specific low-level appearances (for in-context perception questions, e.g., Which object in the image is underexposed?). Unlike Yes-or-No questions, the What questions examine more comprehensive low-level attribute understanding of MLLMs, by requiring correct perception on multiple attributes.\\n\\nType 3: How Questions. Despite the two common types, we also include a special type, the How questions, to cover non-extreme appearances (Wu et al., 2023d) of low-level attribute dimensions into our benchmark, as an extension to Yes-or-No questions. As shown in Fig. 2, we can query How is the clarity of the image? for the image with both clear and blurry areas, and answer with Medium. With this special question type, we broaden the Q-bench into finer-grained low-level perception.\\n\\nAfter constructing the LLVisionQA dataset, we feed it to multiple MLLMs to evaluate their abilities on low-level visual perception. The input format to query MLLMs is exemplified as follows:\\n\\n#User: How is the clarity of the image? (Question)\\n\\n[IMAGE TOKEN] (Image)\\n\\nChoose between one of the following options: A. High (Correct) B. Medium (Wrong) C. Low (Wrong)\\n\\nThe correct and wrong answers are shuffled during the actual evaluation. Moreover, while traditional visual question answering (Antol et al., 2015; Marino et al., 2019) tasks typically employ traditional language metrics (BLEU-4, CIDEr) to compare performance, as observed by recent studies (Ye et al., 2023) and validated by us, most MLLMs cannot consistently provide outputs on instructed formats. Given the question above, different MLLMs may reply \u201cA. High\u201d, \u201cThe clarity of the image is high.\u201d, \u201cThe image is of high clarity.\u201d (all correct), which are difficult to be exhaustively...\"}"}
{"id": "0V5TVt9bk0", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the second task of Q-Bench, we evaluate the language description ability of MLLMs on low-level information. This task is a sibling task of image captioning (Chen et al., 2015; Young et al., 2014; Agrawal et al., 2019) that describes image content with natural language, with a specific concern on the low-level appearance of images. To evaluate this ability automatically, we first derive a golden low-level description dataset, denoted as LLDescribe (Sec. 2.3.1), including one long (average 40 words) golden description provided by experts for each of 499 images. With these golden text descriptions, we are able to measure the quality of output low-level descriptions from MLLMs with a single-modal GPT, under the three dimensions: completeness, preciseness, as well as relevance (Sec 2.3.2). The discussions of the golden descriptions and the evaluation process are as follows.\\n\\n2.3.1 Defining Golden Low-Level Descriptions for Images\\nFor the description ability, MLLMs should accurately and completely describe low-level visual information of images. Thus, the ground truths for these MLLMs are also built within a basic principle to cover as many low-level concerns as possible, so long as they are enumerated in Sec. 2.2.1 and occur in images. The resulting golden descriptions in LLDescribe have an average duration of 58 words, notably longer than common high-level image caption datasets (11 for Agrawal et al. (2019), 10 for Chen et al. (2015)). Similar to the LLVisionQA dataset for the perception task, the 499 images in LLDescribe dataset also include all 10 sources (as in Tab. 1) to cover images with diverse low-level appearances. The golden descriptions on different sources of images are depicted in Fig. 3.\\n\\n2.3.2 Evaluation with Single-Modal GPT\\nRecent studies (Zheng et al., 2023) have proved single-modal GPT (OpenAI, 2023) to be a reliable evaluation tool for pure language tasks. Via the LLDescribe dataset, we convert the multi-modality problem into a text-only setting, by matching the MLLM outputs with the golden descriptions with single-modal GPT under three dimensions: (1) Completeness. More matched information with the golden description is encouraged. (2) Preciseness. The controversial information with the golden description is punished. (3) Relevance. More proportions of MLLM outputs should be related to low-level information, instead of others. Each dimension is scored among [0,1,2]. Similar as Sec. 2.2.3, we repeat 5 rounds for each single evaluation and collect the weighted average as the final score. The detailed settings for GPT to evaluate the three dimensions are in Sec. A.2.2.\"}"}
{"id": "0V5TVt9bk0", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: The proposed softmax-based quality assessment strategy for MLLMs. Instead of directly decoding tokens from the position, the strategy extracts log probabilities (logits) of good and poor, and predicts quantifiable score via a softmax pooling between the two logits.\\n\\n2.4.1 WEAK MEASURABILITY OF MLLM OUTPUTS\\n\\nIn Q-Bench, we aim to fairly compare the assessment ability between different MLLMs on diverse low-level appearances. Henceforth, our principle is to define a unified, simplest instruction that is applicable for all MLLMs on all IQA datasets. Under this principle, we conduct toy experiments on LLVisionQA on Shikra and LLaVA-v1, with two simple instruction strategies: (A) Direct Instruction, in which the prompt is designed as simple as \u201cRate the quality of the image.\u201d The top-frequency answers are good (78%), and poor (20%), with other outputs almost negligible. (B) Numerical Instruction, in which we specifically instruct numerical ratings, with the prompt: \u201cScore the quality of the image from 1 to 5, with 1 as lowest and 5 as highest.\u201d Under the numerical strategy, the top-frequency answers are 5 (84%), 1 (9%), and 3 (5%); though within the score range, the frequencies of scores 2 and 4 are both less than 1%. The toy experiments imply the weak measurability of MLLM outputs, given that the answers are statistically 1) biased towards positive, 2) biased towards extreme, and 3) with only two effective scales. Therefore, it is necessary to explore extended strategies for MLLMs to provide truly quantifiable outputs for low-level assessment.\\n\\n2.4.2 SOFTMAX-BASED EVALUATION STRATEGY\\n\\nGiven the above observations, we design the softmax-based evaluation strategy (Fig. 4) to reduce the negative impacts of the biases and lack of scales. To start with, we design our strategy within the Direct Instruction, which is more general and less biased than the Numerical Instruction. The strategy is based on the observation that two top-frequency outputs, good and poor, can be considered as anchors for better and worse human perception, and the Direct Strategy can be approximated into a binary classification problem on the position, or technically, an argmax between the logits of good (x_good) and poor (x_poor) on this position. In our revised strategy, we modify the argmax into softmax to collect better quantifiable scores:\\n\\n\\\\[ q_{\\\\text{pred}} = \\\\frac{e^{x_{\\\\text{good}}} + e^{x_{\\\\text{poor}}}}{e^{x_{\\\\text{good}}} + e^{x_{\\\\text{poor}}}} \\\\]  \\n\\nThis simple and generally-applicable strategy enables us to collect quantifiable outputs \\\\( q_{\\\\text{pred}} \\\\) from MLLMs with higher correlation to human ratings, as verified in our experimental analysis (Tab. 9).\"}"}
{"id": "0V5TVt9bk0", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2: Results on the test subset for the low-level Perception ability of MLLMs.\\n\\n| Sub-categories Question Types | Quadrants of Low-level Concerns |\\n|------------------------------|---------------------------------|\\n| Overall                      | \u2191                               |\\n| Model (variant)              | Yes-or-No                       |\\n| What                         | \u2191                               |\\n| How                          | \u2191                               |\\n| Distortion                   | \u2191                               |\\n| Other                        | \u2191                               |\\n\\n| Random guess | LLaV A-v1.5 (Vicuna-v1.5-7B) | LLaV A-v1.5 (Vicuna-v1.5-13B) | InternLM-XComposer-VL (InternLM) | IDEFICS-Instruct (LLaMA-7B) | Qwen-VL (QwenLM) | Shikra (Vicuna-7B) | Otter-v1 (MPT-7B) | InstructBLIP (Flan-T5-XL) | InstructBLIP (Vicuna-7B) | VisualGLM-6B (GLM-6B) | mPLUG-Owl (LLaMA-7B) | LLaV A-v1 (Vicuna-13B) | MiniGPT-4 (Vicuna-13B) | GPT-4V (Close-Source Model) | Junior-level Human | Senior-level Human |\\n|--------------|-------------------------------|-------------------------------|---------------------------------|-------------------------------|-----------------|-------------------|-----------------|------------------------|------------------------|---------------------|-------------------|-------------------|-------------------|-------------------|------------------------|----------------|-----------------|\\n|              | 50.00%                        | 28.48%                        | 33.30%                         | 37.24%                        | 38.50%                      | 39.13%                        | 37.10%                      | 37.94%                  |                       |                    |                   | 64.60%                        | 64.96%                        | 64.86%                        | 54.12%                      | 53.55%                  | 66.59%                        | 58.90%                        | 73.76%                        | 60.07%                        | 64.35%                        | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |"}
{"id": "0V5TVt9bk0", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the prompt template, the [MLLM DESC] denotes the output description from MLLMs, and [GOLDEN DESC] denotes the golden description in the LLDescribe dataset.\\n\\nExamples for GPT Evaluation:\\n\\n(A) Completeness.\\n\\nUser Input:\\n\\n#User: Evaluate whether the description [The image is a large, clear, and detailed picture of a white airplane flying in the sky. The airplane appears to be a JAL (Japan Airlines) passenger jet, and it is flying at a relatively low altitude. The plane's white color and its distinctive red logo are clearly visible, making it easily discernable as a JAL aircraft. Overall, the image captures a clear, beautiful scene of the airplane in flight, with a blue sky as the backdrop.] completely includes the low-level visual information in the reference description [This image has poor lighting. The blue sky has vivid colors, but the overall tone is dark. The focus is on the main subject, the aircraft. Apart from the backlighting on the wings, the details of the aircraft are clear. The composition is slightly incomplete, and the clarity of the background sky is average. Overall, the image quality is good.].\\n\\nPlease rate score 2 for completely or almost completely including reference information, 0 for not including at all, 1 for including part of the information or similar description.\\n\\nPlease only provide the result in the following format: Score:\\n\\n5-Round GPT Answers:\\n\\n[\"Score: 1\",\"Score: 1\",\"Score: 1\",\"Score: 1\",\"Score: 1\"] \u2192 Final Score: 1.0\\n\\n(B) Preciseness.\\n\\nUser Input:\\n\\n#User: The precision metric punishes controversial low-level descriptions that output description outputs contrast with the referencce, e.g., blur for clear, high quality for low quality, colorful for monotonous, noisy for clean, bright for dark.\\n\\nEvaluate whether output [The image quality is good, with a clear and well-exposed scene. The lighting appears to be natural, as the sun is shining on the scene, which adds a warm and inviting atmosphere to the image. The composition is well-balanced, with the focus on the trees and the pathway, which creates a sense of depth and perspective. The aesthetics of the image are pleasing, with the use of natural light and the greenery of the trees contributing to a visually appealing scene. The overall feeling of the image is peaceful and serene, as the viewer is invited to walk down the pathway surrounded by the trees and the cityscape in the background.] precisely reflect reference [The overall clarity of this image is very low, with serious focusing issues and significant noise. The lighting is adequate, but the colors are monotone. The main subjects are trees and distant buildings, which are extremely blurry and lack any texture details. The composition is unbalanced, and the background is very blurry. Therefore, the quality of this image is very poor.],\\n\\nPlease rate score 2 for no controversial low-level description, 1 for less controversial low-level description than matched descrpition, and 0 for more controversial low-level description than matched description.\\n\\nPlease only provide the result in the following format: Score:\\n\\n5-Round GPT Answers:\\n\\n[\"Score: 0\",\"Score: 0\",\"Score: 0\",\"Score: 0\",\"Score: 0\"] \u2192 Final Score: 0.0\\n\\n(C) Relevance.\\n\\nUser Input:\\n\\n#User: Evaluate whether the description [The image is a low-level shot of a white dog walking through a dark forest. The dog appears to be somewhat blurry, suggesting a level of motion in the picture. The photo is not very detailed, and the colors in the image might be somewhat muted due to the darkness of the forest. Overall, the picture has a somewhat mysterious and moody atmosphere.] is relevant to the low-level visual information, which may include blur, noise, exposure, artifact, color, lighting, focus, composition, etc.\\n\\nScore:\"}"}
{"id": "0V5TVt9bk0", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\n\\nPytorch-style Pseudo Code for Softmax-based Strategy for IQA with MLLMs\\n\\n```python\\nfrom PIL import Image\\nfrom my_mllm_model import Model, Tokenizer, embed_image_and_text\\n\\nmodel, tokenizer = Model(), Tokenizer()\\nprompt = \\\"##User: Rate the quality of the image.\\n##Assistant: The quality of the image is\\\"\\n\\ngood_idx, poor_idx = tokenizer(['good', 'poor']).tolist()\\n\\nimage = Image.open('image_for_iqa.jpg')\\ninput_embeds = embed_image_and_text(image, prompt)\\noutput_logits = model(input_embeds=input_embeds).logits[0,-1]\\nq_pred = (output_logits[[good_idx, poor_idx]] / 100).softmax(0)[0]\\n```\\n\\nA.2.3 E\\n\\n**Evaluation Details for Assessment Ability**\\n\\nExample Pseudo Code for MLLMs on IQA:\\n\\nIn Algo. 1, we provide an example on how to evaluate image quality with MLLMs. The algorithm is simple with only 9 lines, and could be easily integrated with any new MLLMs (based on causal LLMs), so as to allow these models to quantitatively predict the quality of images.\\n\\nIQA Evaluation Strategy for CLIP-ViT-Large-14:\\n\\nIn Tab. 4, we compare the IQA performance of MLLMs with CLIP-ViT-Large-14, the visual backbone of the majority of MLLMs. Attempting to understand whether the new language part (LLM) can do better than the original language part of CLIP, we try to compare between CLIP and MLLMs in a relatively aligned setting. Firstly, noticing that most MLLMs will resize images into $224 \\\\times 224$ as their input sizes, we align this setting on CLIP, and ignore the strategies as proposed by (Wang et al., 2022). Secondly, same as the strategy on MLLMs, we also apply softmax pooling between good and poor, as in the CLIP's zero-shot classification format: a photo of good quality and a photo of poor quality. Besides the two alignments, similar as existing practices (Wang et al., 2022; Wu et al., 2023b; Zhang et al., 2023c), the quality scores of CLIP-ViT-Large-14 are obtained as follows:\\n\\n$$q_{pred, CLIP} = e^{\\\\text{CosineSimilarity}(f_{\\\\text{IMAGE}}, f_{\\\\text{a photo of good quality}})} + e^{\\\\text{CosineSimilarity}(f_{\\\\text{IMAGE}}, f_{\\\\text{a photo of poor quality}})}$$\\n\\nSpecial IQA Settings for Flan-T5-based InstructBLIP:\\n\\nFor InstructBLIP (Dai et al., 2023) (Flan-T5-XL), different from the majority of LLaMA-based (or MPT-based Otter-v1) MLLMs, the two top-frequency tokens are high (89%) and low (8%) instead of the common good $\\\\leftrightarrow$ poor. Henceforth, based on our motivation to only modify the argmax into softmax and follow the default top-frequency output tokens of MLLMs, we replace the probabilities of good $\\\\leftrightarrow$ poor into those of high $\\\\leftrightarrow$ low in Eq. 1 for T5, defined as follows:\\n\\n$$q_{pred,T5} = e^{x_{\\\\text{high SCORE TOKEN}}} + e^{x_{\\\\text{low SCORE TOKEN}}}$$\"}"}
{"id": "0V5TVt9bk0", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As validated in our experiments (Tab. 10, the high\u2194low pair generally predicts better than good\u2194poor on majority of databases. The better performance on MLLM-specific top-frequency tokens by side validates the effectiveness of our methodology for MLLMs on IQA.\\n\\nFurther Improving IQA Abilities of MLLMs with Synonym Ensemble:\\nThe quality assessment scores for the synonym ensemble strategy can be derived as:\\n\\n$$q_{pred} = \\\\sum_{t \\\\in P_t} x_{t} \\\\times \\\\text{SCORE}_{t} + \\\\sum_{t \\\\in N_t} x_{t} \\\\times \\\\text{SCORE}_{t}$$\\n\\n(4)\\n\\nwhere $P$ indicates the positive token set (from good, fine, high, etc.), while $N$ represents the negative token set (from poor, bad, low, etc.). The results of different $P$ and $N$ are listed in Tab. 11.\\n\\nSpecial Validation Protocol for CGIQA-6K:\\nThe CGIQA-6K (Zhang et al., 2023b) dataset contains two separate sub-sets which consist of 3,000 game images and 3,000 movie images respectively, with different instructions for human annotators during its subjective experiments. Therefore, we validate the MLLMs' assessment performance on the two sub-sets individually and average the results for the final exhibition. The results of NIQE and CLIP-ViT-Large-14 are also obtained under the same protocol for a fair comparison.\"}"}
{"id": "0V5TVt9bk0", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Results on the dev subset for the low-level perception ability of MLLMs. MLLMs with top-3 performance in each sub-category and the overall LLVisionQA is emphasized with boldface.\\n\\n| Sub-categories | Question Types | Quadrants of Low-level Concerns | Overall |\\n|----------------|----------------|---------------------------------|---------|\\n|                | Model (variant) | Yes-or-No                        |         |\\n|                |                | What                             |         |\\n|                |                | How                             |         |\\n|                |                | Distortion                       |         |\\n|                |                | Other                            |         |\\n\\n|                     | random guess  | LLaV A-v1.5 (Vicuna-v1.5-7B) | 66.36%  |\\n|                     |               | LLaV A-v1.5 (Vicuna-v1.5-13B) | 65.27%  |\\n|                     |               | InternLM-XComposer-VL (InternLM) | 69.45%  |\\n|                     |               | IDEFICS-Instruct (LLaMA-7B)   | 56.18%  |\\n|                     |               | Qwen-VL (QwenLM)              | 63.09%  |\\n|                     |               | Shikra (Vicuna-7B)            | 65.64%  |\\n|                     |               | Otter-v1 (MPT-7B)             | 57.09%  |\\n|                     |               | InstructBLIP (Flan-T5-XL)     | 67.64%  |\\n|                     |               | InstructBLIP (Vicuna-7B)      | 71.64%  |\\n|                     |               | VisualGLM-6B (GLM-6B)         | 60.18%  |\\n|                     |               | mPLUG-Owl (LLaMA-7B)          | 66.0%   |\\n|                     |               | LLaV A-v1 (Vicuna-13B)        | 54.00%  |\\n|                     |               | MiniGPT-4 (Vicuna-13B)        | 55.82%  |\\n\\nA.3 Extended Results\\n\\nA.3.1 Architectures of Different MLLMs\\nAs compared in Tab. 6, the 15 variants of MLLMs as evaluated in the Q-Bench are with varying vision and language architectures, as well as the alignment strategies between the two modalities. It can be noticed that all MLLMs are combined with a version of CLIP Radford et al. (2021) and a large language model, which are generally connected under one among three strategies: direct project layers (MLP or linear layer), Q-Former (a transformer to abstract visual features into LLM tokens), or cross-attention (use visual features as conditions for text generation).\\n\\nA.3.2 Extended Results for Perception\\nResults on the dev subset:\\nIn Tab. 7, we list the results on the dev subset of the LLVisionQA benchmark set for the low-level perception task. This subset is planned to be opened to public in the future. Therefore, the performance in it will only be taken as a reference. At present, all MLLMs as evaluated have not yet seen this subset, so it can be taken as a cross-validation with the test subset. From Tab. 7 and Tab. 2, we validate that MLLMs perform pretty similar between the two subsets, suggesting that LLVisionQA is a reliable and stable benchmark set for question answering on low-level vision.\\n\\nRadar Chart for Different MLLMs:\\nIn Fig. 9, we show the radar chart to compare the low-level perception abilities among different MLLMs. Despite the observations as revealed in Sec. 3.1, we also notice two extra fun facts: 1) Adding the content context does not degrade the performance of MLLMs. On the contrary, MLLMs can answer better on in-context questions. This result validates the aforementioned conjectures that appropriate higher-level contexts as prompts may help improve the preciseness of low-level visual perception; 2) MLLMs have strong capabilities of answering what questions, suggesting potential reasoning abilities. In the future, we will excavate more interesting characteristics of MLLMs and try to improve their perception accuracy through better guidance based on these characteristics.\\n\\n\"Yes or No?\": How Biased are MLLMs?\\nIn this section, we take a deeper analysis on the Yes-or-No judgment ability of MLLMs, that whether these models can get similar accuracy on questions that should be answered with Yes, as those should be replied as No. Sadly, we notice that all MLLMs have higher prediction accuracy on Yes-questions than No-questions, while some MLLMs are more very severe biased (e.g., IDEFICS-Instruct). Considering that our LLVisionQA dataset contains more (62%) Yes-questions than No-questions (38%) and may introduce biases while comparing different MLLMs, we further compute a de-biased accuracy for all these methods, as the mean value of the accuracies on two types of questions, and present the respective de-biased rank for all participating MLLMs, as listed in Tab 8.\"}"}
{"id": "0V5TVt9bk0", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Q-BENCH: A BENCHMARK FOR GENERAL-PURPOSE FOUNDATION MODELS ON LOW-LEVEL VISION\\n\\nHaoning Wu\\nZicheng Zhang\\nErli Zhang\\nChaofeng Chen\\nLiang Liao\\nAnnan Wang\\nChunyi Li\\nWenxiu Sun\\nQiong Yan\\nGuangtao Zhai\\nWeisi Lin\\n\\n1 S-Lab, Nanyang Technological University,\\n2 Shanghai Jiaotong University,\\n3 Sensetime\\n\\n\u2217 Equal contribution.\\n\u2020 Corresponding author. Project Page: https://q-future.github.io/Q-Bench.\\n\\nABSTRACT\\n\\nThe rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment.\\n\\na) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions.\\n\\nb) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions on 499 images, and a GPT-involved comparison pipeline between outputs of MLLMs and the golden descriptions.\\n\\nc) Besides these two tasks, we further measure their visual quality assessment ability to align with human opinion scores. Specifically, we design a softmax-based strategy that enables MLLMs to predict quantifiable quality scores, and evaluate them on various existing image quality assessment (IQA) datasets.\\n\\nOur evaluation across the three abilities confirms that MLLMs possess preliminary low-level visual skills. However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs towards these abilities. We hope that our benchmark can encourage the research community to delve deeper to discover and enhance these untapped potentials of MLLMs.\\n\\nINTRODUCTION\\n\\nThe emergent large language models (LLMs) such as ChatGPT and Bard, as well as their excellent open-source counterparts (e.g., LLaMA (Touvron et al., 2023), MPT (Team, 2023)), have served as powerful general-purpose assistants, which opens a new era for artificial intelligence (AI) from targeting specific tasks towards general intelligence. Following the advancements of LLMs, multi-modality large language models (MLLMs), as represented by LLaV A (Liu et al., 2023b), MiniGPT-4 (Zhu et al., 2023), InstructBLIP (Dai et al., 2023), and Otter (Li et al., 2023a), have brought exciting progresses on the vision field as well. They are capable of providing robust general-level abilities on visual perception/understanding and can even seamlessly dialog and interact with humans through natural language. While such abilities of MLLMs have been explored and validated on several vision-language tasks such as image captioning (Chen et al., 2015), visual question answering (Antol et al., 2015), cross-modality grounding (Peng et al., 2023), and traditional vision tasks such as image classification or segmentation (Lai et al., 2023), most attention is paid to the high-level perception and understanding of visual contents. Meanwhile, the ability of MLLMs remains not clear on low-level visual perception and understanding, which play significant roles in image quality assessment (IQA) (Hosu et al., 2020; Fang et al., 2020) and its associated tasks on perceiving visual distortions (noises, blurs) (Su et al., 2021; Wu et al., 2023d) and other low-level attributes (color, lighting, composition, style, etc) (Kong et al., 2016) that may relate to aesthetics and emotions of natural photos (Murray et al., 2012) and human preferences on emerging computer-\"}"}
{"id": "0V5TVt9bk0", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This picture is underexposed, with the two people in the middle completely black and the fish in the background blurred. Therefore, this is a very low-quality image.\\n\\nPlease describe the quality of the image in details.\\n\\nQuestions:\\n1. Is this image clear?\\n2. How is the brightness?\\n\\nAnswers:\\n1. No.\\n2. High.\\n\\nRate the quality of the image. The quality of the image is [SCORE_TOKEN].\\n\\nIn our work, we propose the first systematic benchmark to measure the low-level visual perception and understanding abilities of MLLMs. Our benchmark is constructed around a key question: How do MLLMs emulate human ability related to low-level visual perception and understanding?\\n\\nA simple answer is language, which is the fundamental property of MLLMs. Specifically, we define two emerging language abilities of MLLMs on low-level vision as follows:\\n\\n\u2022 Ability 1 (A1): Perception of Low-level Attributes. As shown in Fig. 1(a), like a human, an MLLM should be able to respond accurately to simple questions related to low-level attributes, e.g. answering 'No' for a blurry image when queried with 'Is this image clear?'\\n\\n\u2022 Ability 2 (A2): Description via Natural Language. As shown in Fig. 1(b), like a human, an MLLM should be able to describe the quality and other low-level information for an image with natural language. The descriptions should be both complete and accurate.\\n\\nTo systematically evaluate the low-level perception ability (A1) on various low-level attributes under diverse circumstances, we construct the LLVisionQA dataset, including 2,990 images from 10 diverse sources. Aligned with existing practices (Liu et al., 2023c; Lu et al., 2023), each image in LLVisionQA is equipped with a question, alongside a correct answer and false candidate answers. In LLVisionQA, we design three diverse types of questions: Yes-or-No questions, What questions, and How questions. Moreover, we divide low-level concerns into four quadrants, via two axes: (1) distortions (blur, noises, etc) vs other low-level attributes (color, lighting, composition, etc) (Guha et al., 2023).\"}"}
{"id": "0V5TVt9bk0", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"global perception (e.g., sharpness of the whole picture) vs local content-related in-context perception (e.g., whether the red flower is in focus) (Li et al., 2019). With three types of questions and four quadrants of concerns, the proposed LLVisionQA dataset provides a holistic, diverse, and balanced benchmark for the perception ability on low-level visual attributes of MLLMs.\\n\\nFor the description ability (A2), given that the output description is expected to be complex (without fixed formats), we propose the LLDescribe dataset by inviting experts to write long golden low-level descriptions (average 58 words per description) for 499 images, which serve as the reference texts for the single-modal GPT to evaluate MLLM output descriptions. The quality of MLLM descriptions is evaluated through three dimensions: completeness (punish missing information), preciseness (punish outputs controversial with reference), as well as relevance (punish outputs irrelevant to low-level attributes). With golden descriptions and the multi-dimensional evaluation process participated by GPT, we comprehensively evaluate the low-level description ability of MLLMs.\\n\\nBesides the two emerging language abilities, we also evaluate MLLMs on the traditional IQA task, a more abstract task that requires understanding on human opinions of low-level attributes, as follows:\\n\\n\u2022 Ability 3 (A3): Precise Assessment Aligned with Human Opinions. As depicted in Fig. 1(c), an MLLM should be able to predict quantifiable quality scores for images, which can be aligned with the human-rated mean opinion scores (MOS) on low-level visual appearances.\\n\\nFor the assessment ability (A3), we utilize plenty of existing IQA databases (Hosu et al., 2020; Lin et al., 2019; Li et al., 2023c) that focus on various low-level appearances of images, to benchmark MLLMs within conventional IQA settings. Specifically, we notice that MLLMs encounter difficulties in providing sufficiently quantifiable outputs, whether instructed to directly rate with texts or provide numerical outputs. To solve this challenge, we propose to extract the softmax pooling result on the logits of the two most frequent tokens (good and poor) under the response template of MLLMs (Fig 1(c)) as their quality predictions. Our studies prove that the proposed softmax-based strategy is generally better correlated with human perception than direct token outputs of MLLMs (via argmax), which bridges between these emergent MLLMs and the traditional IQA task settings.\\n\\nUnder this strategy, we evaluate all MLLMs on their precise assessment ability by measuring the correlations between their predictions and human opinion scores in various IQA databases.\\n\\nIn summary, we systematically explore the potential of MLLMs on three low-level visual abilities: perception, description, and assessment. The three realms compose into the proposed Q-Bench, a MLLM benchmark on low-level visual tasks. Our contributions can be summarized as three-fold:\\n\\n\u2022 We build a benchmark for MLLMs on low-level perception ability. To achieve this, we construct a first-of-its-kind balanced and comprehensive LLVisionQA dataset with 2,990 images with one low-level-related question-answer pair for each image. The LLVisionQA includes three question types and four quadrants of low-level concerns to ensure diversity.\\n\\n\u2022 We define a benchmark process to evaluate the low-level description ability of MLLMs, including an LLDescription dataset of 499 images with expert-labelled long golden quality descriptions, and a GPT-assisted evaluation to rate MLLM-descriptions in terms of completeness, preciseness, and relevance compared with golden descriptions.\\n\\n\u2022 To evaluate precise quality assessment ability, we propose a unified softmax-based quality prediction strategy for all MLLMs based on their probability outputs. With its effectiveness validated in our experiments, the proposed strategy sets up a bridge between general-purpose MLLMs and traditional IQA tasks that requires quantifiable scores as outputs.\\n\\n2 CONSTRUCTING THE Q-BENCH\\n\\n2.1 GENERAL PRINCIPLES\\nFocusing on Low-level Visual Abilities of MLLMs. Unlike existing MLLM benchmarks (Li et al., 2023b; Liu et al., 2023c; Lu et al., 2023) that aim at all-round abilities, the tasks in Q-Bench are constrained with two basic principles:\\n\\n(1) Requiring perception and/or understanding on low-level attributes of images;\\n(2) Not requiring reasoning (i.e. why) or outside knowledge (Marino et al., 2019). We adhere to the principles in designing the perception, description, and assessment tasks, making the proposed Q-bench a focused reflection on the low-level visual abilities of MLLMs.\"}"}
{"id": "0V5TVt9bk0", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Overview of the 10 diverse image source datasets in the Q-Bench, and the respective benchmark dataset size for each low-level ability among perception, description, and assessment.\\n\\n| Type            | Image Source Dataset                  | Sampled Size in LLVisionQA | Sampled Size in LLDescribe for Assessment Task | Full Dataset Size |\\n|-----------------|---------------------------------------|----------------------------|-----------------------------------------------|------------------|\\n| In-the-wild     | KONiQ-10K (Hosu et al., 2020)         | 600                        | 100                                           | 10,073           |\\n|                 | SPAQ (Fang et al., 2020)              | 800                        | 130                                           | 11,125           |\\n|                 | LIVE-FB (Ying et al., 2020)           | 300                        | 50                                            | 39,810           |\\n|                 | LIVE-itw (Ghadiyaram & Bovik, 2016)   | 300                        | 50                                            | 1,169            |\\n| Generated       | CGIQA-6K (Zhang et al., 2023b)        | 200                        | 30                                            | 6,000            |\\n|                 | AGIQA-3K (Li et al., 2023c)           | 198                        | 30                                            | 2,982            |\\n|                 | ImageRewardDB (Xu et al., 2023)       | 194                        | 29                                            | not included     |\\n| Artificially-distorted | KADID-10K (Lin et al., 2019)       | 81                         | 20                                            | 10,125           |\\n|                 | LIVEMultiDistortion (Jayaraman et al., 2012) | 15                         | 10                                            | not included     |\\n| Corrupted       | Corrupted COCO (Chen et al., 2015)   | 302                        | 50                                            | not included     |\\n\\nCovering Diverse Low-level Appearances. To cover diverse low-level appearances, we collect multi-sourced images for each task, as depicted in Tab. 1. Among all images in the perception and description tasks, two-thirds are in-the-wild images directly collected from social media posts, smartphones or professional photography. The rest one-third images are collected after various artificially distortions, or via generative processes (CGI, AIGC). Furthermore, we employ k-means clustering for the low-level attribute indicators to certify that the sub-sampled images retain high diversity. In the assessment task, full images of 7 IQA datasets within all three source types are evaluated through traditional IQA metrics. The diverse and multiple sources of images morph the Q-bench into a holistic and balanced benchmark to fairly evaluate low-level-related abilities.\\n\\n2.2 Benchmark on Low-Level Perception Ability\\n\\nIn the first task of Q-Bench, we evaluate the low-level perception ability of MLLMs to examine whether they can answer simple natural queries related to low-level attributes. For this purpose, we first collect 2,990 images (I), from multiple sources (see Table 1) with diverse low-level concerns. Then, we collect one low-level-related question (Q), one correct answer to the question (C), and 1-3 candidate false answers (F) for each image. The 2,990 (I,Q,C,F) tuples compose into the LLVisionQA dataset (as illustrated in Fig. 2), the first visual question answering (VQA) dataset in the low-level computer vision field. Specifically, the questions in LLVisionQA cover four quadrants of distinct low-level concerns (in Sec. 2.2.1) and three question types (in Sec. 2.2.2). After constructing the dataset, the (I,Q,C,F) are together fed into MLLMs for evaluation, while their outputs are further examined by GPT to judge correctness (in Sec. 2.2.3). The details are elaborated as follows.\\n\\n2.2.1 Quadrants for Low-level Visual Concerns\\n\\nAxis 1: Distortions vs Other Low-level Attributes. The primary axis differentiates two categories of low-level perceptual attributes: 1) technical distortions (Su et al., 2021), seen as the low-level characteristics that directly degrade the quality of images (Ying et al., 2020), and 2) aesthetic-related other low-level attributes (Kong et al., 2016; Hou et al., 2023) which are discernible to human perception and evoke varied emotions. Several studies (Talebi & Milanfar, 2018; Ying et al., 2020; Guha et al., 2020) follow this paradigm and categorize them through a relative golden standard, that whether the attributes directly improve or degrade picture quality (Yes \u2192 Distortions; No \u2192 Others).\\n\\nDespite this standard, we also enumerate common types of distortions vs other low-level attributes as extra guidance for constructing the LLVisionQA dataset, as listed in Sec. A.1.2.\\n\\nAxis 2: Global Perception vs Local In-context Perception. In recent research on low-level vision, it is observed that human perceptions of low-level visuals often intertwine with higher-level contextual comprehension (Li et al., 2019; Wang et al., 2021; Wu et al., 2023a). For instance, a clear sky might lack complex textures yet display exceptional clarity. Furthermore, localized low-level appearances can deviate from their overall counterparts, as observed by Wu et al. (2022); Ying et al. (2021). Acknowledging these differences, we curate local in-context perception (Fig. 2 right) questions, that require MLLMs to grasp the content or other context to answer correctly, while other questions are categorized as global perception (Fig. 2 left). (More analysis in Sec. A.1.2.)\"}"}
{"id": "0V5TVt9bk0", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 8: Judgment accuracies of MLLMs on questions with correct answers as Yes or No.\\n\\n| Model (variant)       | all correct answer: Yes | correct answer: No | mean de-biased \u2193 | random guess |\\n|-----------------------|-------------------------|--------------------|------------------|--------------|\\n| Shikra (Vicuna-7B)    | 66.91%                  | 71.79%             | 60.00%           | 65.90%       |\\n| LLaV A-v1 (Vicuna-13B)| 57.10%                  | 60.29%             | 51.66%           | 55.97%       |\\n| MiniGPT-4 (Vicuna-13B)| 57.56%                  | 70.00%             | 37.38%           | 53.69%       |\\n| LLaMA-Adapter-V2      | 67.12%                  | 68.80%             | 64.76%           | 66.78%       |\\n| InstructBLIP (Flan-T5-XL) | 68.67%             | 80.14%             | 50.23%           | 65.19%       |\\n| InstructBLIP (Vicuna-7B) | 71.40%             | 84.32%             | 50.47%           | 67.39%       |\\n| Otter-v1 (MPT-7B)     | 57.74%                  | 70.14%             | 37.38%           | 53.76%       |\\n| IDEFICS-Instruct (LLaMA-7B) | 59.74%             | 88.65%             | 13.09%           | 50.87%       |\\n| mPLUG-Owl (LLaMA-7B)  | 69.31%                  | 95.82%             | 26.67%           | 61.25%       |\\n\\n**Question**: Is there any blur in this image? A. Yes, B. No.\\n\\n- **Shikra**: The answer is yes.\\n- **Correct Answer**: A. Yes.\\n\\n**Question**: Which part of the human is cropped out of the image? A. His head; B. His leg; C. His hand.\\n\\n- **Shikra**: The answer is arm.\\n- **Correct Answer**: A. His head.\\n\\nWe hope this study on the biases and the de-biased results can provide a fairer comparison among them, as well as bring insights on the future improvements of MLLMs for low-level perception.\\n\\nQualitative examples of MLLM responses:\\n\\nIn Fig. 10, we show qualitative examples of MLLM responses on questions in the LLVisionQA dataset, that MLLMs are still unstable on basic low-level attributes such as blurs (Fig. 10(a)), and may fail on in-context questions that are easy to human (Fig. 10(b)). These unsatisfactory results suggest that we still need to improve the basic low-level perception ability of these models.\\n\\nA.3.3 EXTENDED RESULTS FOR DESCRIPTION\\n\\nBar Chart for Different MLLMs:\\n\\nIn Fig. 11, we show the bar chart to visualize MLLM capabilities on the three dimensions of low-level visual description. From the figure, we notice that current MLLMs still struggle on describing complete and accurate low-level information. As the relevance scores are generally higher (showing that most MLLMs can follow this abstract instruction well), the results suggest that the main bottleneck of MLLMs on enhancing their description ability is still the perception on low-level attributes.\"}"}
{"id": "0V5TVt9bk0", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: Bar chart for the ability, with scores in all dimensions normalized into $[0, 1]$.\\n\\nTable 9: Effectiveness of the proposed softmax probability-based strategy against the baseline argmax strategy, on multiple MLLMs and different IQA datasets. Metrics are SRCC/PLCC.\\n\\n| Dataset Type       | Model / Dataset | Strategy | KONiQ-10k SRCC | SPAQ SRCC | LIVE-FB SRCC | LIVE-itw SRCC | AGIQA-3K SRCC | KADID-10K SRCC | KADID-10K PLCC |\\n|-------------------|-----------------|----------|---------------|-----------|--------------|---------------|---------------|----------------|----------------|\\n| Shikra (Vicuna-7B) | argmax          |          | 0.178/0.201   | 0.277/0.281 | 0.152/0.169  | 0.248/0.267   | 0.513/0.562   | 0.245/0.246   |\\n| Shikra (Vicuna-7B) | softmax         |          | 0.314/0.307   | 0.327/0.337 | 0.237/0.241  | 0.322/0.336   | 0.640/0.661   | 0.324/0.332   |\\n| LLaV A-v1 (Vicuna-13B) | argmax     |          | 0.038/0.045   | 0.101/0.108 | 0.036/0.035  | 0.059/0.075   | 0.240/0.297   | 0.005/0.005   |\\n| LLaV A-v1 (Vicuna-13B) | softmax     |          | 0.462/0.457   | 0.442/0.462 | 0.264/0.280  | 0.404/0.417   | 0.626/0.684   | 0.349/0.372   |\\n| LLaMA-Adapter-V2 | argmax        |          | 0.218/0.237   | 0.417/0.423 | 0.222/0.257  | 0.205/0.239   | 0.545/0.579   | 0.228/0.229   |\\n| LLaMA-Adapter-V2 | softmax        |          | 0.354/0.363   | 0.464/0.506 | 0.275/0.329  | 0.298/0.360   | 0.604/0.666   | 0.412/0.425   |\\n| InstructBLIP (Vicuna-7B) | argmax   |          | 0.284/0.352   | 0.662/0.664 | 0.156/0.249  | 0.195/0.264   | 0.505/0.567   | 0.305/0.307   |\\n| InstructBLIP (Vicuna-7B) | softmax   |          | 0.359/0.437   | 0.683/0.689 | 0.200/0.283  | 0.253/0.367   | 0.629/0.663   | 0.337/0.382   |\\n| mPLUG-Owl (LLaMA-7B) | argmax |          | 0.111/0.154   | 0.463/0.469 | 0.081/0.123  | 0.169/0.237   | 0.410/0.466   | 0.203/0.204   |\\n| mPLUG-Owl (LLaMA-7B) | softmax |          | 0.409/0.427   | 0.634/0.644 | 0.241/0.271  | 0.437/0.487   | 0.687/0.711   | 0.466/0.486   |\\n\\nIn Fig. 12 and Fig. 12 we qualitatively compare among different MLLM low-level descriptions on an AI-generated image and a natural photograph. While most MLLMs can precisely describe their contents (which are actually not instructed in our user prompt), different MLLMs may have several divergences on their quality and related low-level attributes, especially on the AI-generated image. Some MLLMs describe it as clear, colorful, or aesthetically pleasing, which are typically incorrect; on the contrary, some correct descriptions are also seen, such as colors are subdued, not as clear as it could be, or not very detailed. This qualitative study validates our quantitative conclusion that current MLLMs generally cannot provide noise-free low-level visual descriptions of images in a stable manner. Moreover, we notice that even given the same prompt, different MLLMs tend to describe the image with diverse output styles and lengths, while the longer descriptions usually come with a larger percentage of descriptions on irrelevant information.\\n\\nA.3.4 Extended Results for Assessment\\n\\nRadar Chart for Different MLLMs:\\n\\nIn Fig. 14, we visualize the IQA performance of different MLLMs on seven IQA datasets. The visualization proves that MLLMs can notably positively correlate with human ratings, and could very highly align with human perception on relatively coarse situations (AGIQA-3K, SPAQ). As...\"}"}
{"id": "0V5TVt9bk0", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The image features a portrait of a woman, with her hair parted to the side. Her facial features are realistic and clear, and she appears to be looking to the side with a serious expression. The overall quality of the image is good, with no obvious flaws or distortions in the depiction.\"}"}
{"id": "0V5TVt9bk0", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The image presents a close-up view of a delicious dessert, which appears to be a piece of fruit or a scoop of ice cream with berries on top. The dessert is topped with whipped cream, giving it an appetizing and visually appealing appearance. In addition to the dessert, there are three red berries on top, adding a touch of color and natural sweetness to the dish. The overall quality of the image is good, with a clear representation of the dessert and the berries, making it an inviting and attractive scene for viewers.\"}"}
{"id": "0V5TVt9bk0", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"such results are obtained via no direct alignment with human perception and from the simplest prompts (\\\"Rate the quality of the image.\\\") they suggest the exciting underlying abilities of general intelligence to naturally understand \u201cquality\u201d via their vast training data. However, the performance are still yet to be accurate on finer-grained situations, such as LIVE-FB, which is constructed by more than 95% high-quality images (i.e., quality score > 50/100), or CGIQA-6K, made up entirely by relatively high-quality images collected from video games or movies. This suggests that MLLMs still need to improve the measurability on their predictions through well-designed fine-tuning.\\n\\nA Deep Dive Into the Probabilities:\\n\\n(A) Softmax vs Argmax:\\nIn the first part of the deep dive, we quantitatively evaluate the correlation with human perception on a simple argmax strategy between good \u2194 bad and our proposed softmax strategy. In Tab. 9, we confirm that for all MLLMs on all IQA datasets, the more measurable softmax strategy predicts better than the argmax strategy, which degenerates into only two scores, 0 and 1. Though the result is generally expected, the experiments validate that MLLMs have quantitative assessment ability hidden behind their word outputs, and prove the effectiveness of our softmax-based IQA strategy.\\n\\n(B) [For T5-based InstructBLIP] high \u2194 low vs good \u2194 poor:\\nWe further conduct a special study for InstructBLIP (Flan-T5-XL). With a different LLM as language backbone, even pre-trained with the same settings, the T5-version of InstructBLIP tends to predict more high \u2194 low than good \u2194 poor, different from its Vicuna-7B-based counterpart. The experimental results in Tab 10 validate that the more probable high \u2194 low tokens are more competitive in IQA than good \u2194 bad tokens, suggesting that top-frequency tokens are more quality-distinctive.\"}"}
{"id": "0V5TVt9bk0", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: Evaluation results on the synonym ensemble strategy for the (A3) Assessment ability on MLLMs with top-5 results in the default A3 leaderboard of the Q-Bench. After ensemble, the rankings among them are not changed. Metrics are SRCC/PLCC.\\n\\n| Dataset Type | In-the-wild | Generated | Artificial |\\n|--------------|-------------|-----------|------------|\\n| Prompt / Dataset | KONiQ-10k | SPAQ | LIVE-FB | LIVE-itw | CGIQA-6K | AGIQA-3K | KADID-10K |\\n| LLaV A-v1.5 (Vicuna-v1.5-13B) | good \u2194 poor | 0.448/0.460 | 0.563/0.584 | 0.310/0.339 | 0.445/0.481 | 0.285/0.297 | 0.664/0.754 | 0.390/0.400 | 0.444/0.473 |\\n| fine \u2194 bad | 0.449/0.487 | 0.583/0.597 | 0.316/0.360 | 0.466/0.513 | 0.349/0.365 | 0.650/0.749 | 0.425/0.437 | 0.463/0.501 |\\n| high \u2194 low | 0.456/0.482 | 0.529/0.553 | 0.286/0.306 | 0.489/0.513 | 0.276/0.284 | 0.683/0.752 | 0.316/0.331 | 0.434/0.460 |\\n| good + high \u2194 poor + low | 0.462/0.484 | 0.548/0.573 | 0.303/0.327 | 0.480/0.509 | 0.283/0.294 | 0.687/0.763 | 0.350/0.363 | 0.445/0.473 |\\n| good + fine \u2194 poor + bad | 0.463/0.483 | 0.579/0.596 | 0.321/0.356 | 0.467/0.505 | 0.326/0.339 | 0.670/0.762 | 0.420/0.426 | 0.464/0.495 |\\n| good + high + fine \u2194 poor + low + bad | 0.474/0.498 | 0.565/0.588 | 0.314/0.345 | 0.488/0.521 | 0.311/0.322 | 0.692/0.771 | 0.382/0.392 | 0.461/0.491 |\\n| LLaV A-v1.5 (Vicuna-v1.5-7B) | good \u2194 poor | 0.463/0.459 | 0.443/0.467 | 0.305/0.321 | 0.344/0.358 | 0.321/0.333 | 0.672/0.738 | 0.417/0.440 | 0.424/0.445 |\\n| fine \u2194 bad | 0.453/0.469 | 0.457/0.482 | 0.258/0.288 | 0.303/0.333 | 0.294/0.302 | 0.558/0.617 | 0.389/0.420 | 0.388/0.416 |\\n| high \u2194 low | 0.474/0.476 | 0.370/0.386 | 0.261/0.262 | 0.432/0.429 | 0.266/0.269 | 0.669/0.716 | 0.304/0.331 | 0.397/0.410 |\\n| good + high \u2194 poor + low | 0.491/0.491 | 0.416/0.436 | 0.293/0.300 | 0.696/0.751 | 0.413/0.416 | 0.298/0.304 | 0.359/0.389 | 0.424/0.441 |\\n| good + fine \u2194 poor + bad | 0.482/0.482 | 0.461/0.485 | 0.300/0.320 | 0.644/0.708 | 0.339/0.357 | 0.327/0.336 | 0.425/0.451 | 0.425/0.449 |\\n| good + high + fine \u2194 poor + low + bad | 0.512/0.513 | 0.443/0.465 | 0.303/0.315 | 0.408/0.415 | 0.318/0.324 | 0.697/0.752 | 0.392/0.421 | 0.439/0.458 |\\n| mPLUG-Owl (LLaMA-7B) | good \u2194 poor | 0.409/0.427 | 0.634/0.644 | 0.241/0.271 | 0.437/0.487 | 0.148/0.180 | 0.687/0.711 | 0.466/0.486 | 0.432/0.458 |\\n| fine \u2194 bad | 0.357/0.398 | 0.622/0.636 | 0.260/0.290 | 0.422/0.475 | 0.178/0.224 | 0.606/0.646 | 0.536/0.534 | 0.426/0.458 |\\n| high \u2194 low | 0.353/0.369 | 0.610/0.624 | 0.176/0.187 | 0.436/0.464 | 0.110/0.124 | 0.662/0.663 | 0.361/0.378 | 0.387/0.401 |\\n| Qwen-VL (QwenLM) | good \u2194 poor | 0.470/0.546 | 0.676/0.669 | 0.298/0.339 | 0.504/0.532 | 0.273/0.284 | 0.617/0.686 | 0.486/0.486 | 0.475/0.506 |\\n| fine \u2194 bad | 0.467/0.507 | 0.352/0.365 | 0.205/0.238 | 0.451/0.472 | 0.188/0.224 | 0.599/0.627 | 0.354/0.378 | 0.374/0.396 |\\n| high \u2194 low | 0.531/0.578 | 0.626/0.616 | 0.281/0.290 | 0.574/0.560 | 0.286/0.314 | 0.637/0.692 | 0.332/0.344 | 0.467/0.485 |\\n\\nA.3.5 Synonym Ensemble: Further Improving IQA Ability (A3) for MLLMs\\n\\nAs shown in Table 11, the synonym ensemble strategy (as proposed in Eq. 4) on top-5 methods (i.e. InternLM-XComposer-VL, QWen-VL, LLaV A-v1.5 (13B), mPLUG-Owl, and LLaV A-v1.5 (7B)) can in average lead to up to 2% accuracy improvement (in average 1.3%). We believe it is a useful boost to improve the performance of MLLMs on IQA task. Nevertheless, we also notice that different MLLMs perform best with different specific prompt combos. For example, the good+fine \u2194 poor+bad performs best on InternLM-XComposer-VL, but comes with reduced accuracy on QWen-VL compared with only good \u2194 poor. While good \u2194 poor is proved overall best single word pair for the evaluation and shows stable results across MLLMs, we decide to keep the current strategy in Q-Bench to evaluate MLLMs.\\n\\nB. STATEMENT ON DATA CONTAMINATION\\n\\nThe Q-bench contains three tasks, where the first two tasks, (A1) perception and (A2) description, are evaluated with our own datasets proposed with the paper. For these two tasks, the questions, answers, or low-level descriptions in the two datasets are not seen by any existing MLLMs. Half of LLVisionQA (i.e. the test subset) and full of LLDescribe labels are kept private, to avoid being added to the training sets of any MLLMs. We hope that this measure will allow Q-Bench to have long-term significance as an indicator of low-level visual abilities.\"}"}
{"id": "0V5TVt9bk0", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"While text knowledge about image quality assessment should have been injected to them (e.g., a blurry image is a low quality image) during their pure-language training stages, we think this should not be regarded as data contamination for IQA, because the images cannot be seen by a language model. Instead, they are important knowledge for MLLMs to better link particular visual attributes (blur) to human opinions (quality), which motivates us to explore MLLMs for these tasks.\\n\\nLIMITATIONS AND DISCUSSIONS\\n\\nIn Section A.3.2, we observed that MLLMs frequently respond with 'yes' to Yes-or-No questions. It's worth noting that the current LLVisionQA dataset is skewed, with 62% of its questions being Yes-questions and only 38% being No-questions. This imbalance could introduce biases when comparing various MLLMs. To fully address this, we aim to balance the dataset by preparing a reversed version for each question in our subsequent work, ensuring a less biased evaluation.\\n\\nFor the description task, we acknowledge that judging whether a description matches the gold description is a subjective process, which may not have an absolute standard. Even when evaluated by humans, the scores rated for the MLLM descriptions are subject to individual differences. Though we have employed the 5-round GPT-assisted evaluation protocol, which could be the most reliable and reproducible way at present, it may still unavoidably contain hallucinations (from GPT). We will continue to explore how to design a more reliable evaluation protocol for the low-level visual description task in our follow-up works.\\n\\nWhile the proposed Q-Bench has offered a comprehensive evaluation on the low-level visual capabilities of MLLMs, it does not provide direct guidance on enhancing these capabilities. As our next steps, we intend to progressively scale up the LLDescribe and LLVisionQA datasets to eventually allow a reliable low-level visual instruction tuning process that can further improve the low-level abilities for MLLMs.\"}"}
{"id": "0V5TVt9bk0", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Results on the low-level description ability of MLLMs.\\n\\n| Model / Dataset                  | Dimensions | Completeness | Precision | Relevance | Score |\\n|----------------------------------|------------|--------------|-----------|-----------|-------|\\n| LLaV A-v1.5 (Vicuna-v1.5-7B)     | 27.48%     | 54.74%       | 17.78%    | 0.90      | 30.51%|\\n| LLaV A-v1.5 (Vicuna-v1.5-13B)    | 27.68%     | 53.78%       | 18.55%    | 0.91      | 25.45%|\\n| InternLM-XComposer-VL (InternLM)| 19.94%     | 51.82%       | 28.24%    | 1.08      | 22.59%|\\n| IDEFICS-Instruct (LLaMA-7B)      | 28.91%     | 59.16%       | 11.93%    | 0.83      | 34.68%|\\n| Qwen-VL (QwenLM)                 | 26.34%     | 49.13%       | 24.53%    | 0.98      | 50.62%|\\n| Shikra (Vicuna-7B)               | 21.14%     | 68.33%       | 10.52%    | 0.89      | 30.33%|\\n| Otter-v1 (MPT-7B)                | 22.38%     | 59.36%       | 18.25%    | 0.96      | 40.68%|\\n| Kosmos-2                        | 8.76%      | 70.91%       | 20.33%    | 1.12      | 29.45%|\\n| InstructBLIP (Flan-T5-XL)        | 23.16%     | 66.44%       | 10.40%    | 0.87      | 34.85%|\\n| InstructBLIP (Vicuna-7B)         | 29.73%     | 61.47%       | 8.80%     | 0.79      | 27.84%|\\n| VisualGLM-6B (GLM-6B)            | 30.75%     | 56.64%       | 12.61%    | 0.82      | 38.64%|\\n| mPLUG-Owl (LLaMA-7B)             | 28.28%     | 37.69%       | 34.03%    | 1.06      | 26.75%|\\n| LLaMA-Adapter-V2                 | 30.44%     | 53.99%       | 15.57%    | 0.85      | 29.41%|\\n| LLaV A-v1 (Vicuna-13B)           | 34.10%     | 40.52%       | 25.39%    | 0.91      | 30.02%|\\n| MiniGPT-4 (Vicuna-13B)           | 34.01%     | 32.15%       | 33.85%    | 1.00      | 29.20%|\\n\\nTable 4: Main evaluation results on the zero-shot assessment ability of MLLMs, in comparison with NIQE and CLIP-ViT-Large-14, the visual backbone of most MLLMs. Metrics are SRCC/PLCC.\\n\\n| Dataset Type                  | Average | Model / Dataset                  | KONiQ-10k | SPAQ LIVE | LIVE-FB | LIVE-itw | CGIQA-6K | AGIQA-3K | KADID-10K |\\n|-------------------------------|---------|----------------------------------|-----------|-----------|---------|---------|---------|---------|-----------|\\n| In-the-wild                   |         | NIQE (Mittal et al., 2013)       | 0.316/0.377| 0.693     | 0.211/0.288| 0.480/0.451| 0.075/0.056| 0.562/0.517| 0.374/0.428|\\n| Generated                     |         | CLIP-ViT-Large-14                 | 0.468/0.505| 0.385/0.389| 0.218/0.237| 0.307/0.308| 0.285/0.290| 0.436/0.458| 0.376/0.388|\\n| Artificial                    |         | LLaV A-v1.5 (Vicuna-v1.5-7B)     | 0.468/0.459| 0.443/0.467| 0.305/0.321| 0.344/0.358| 0.321/0.333| 0.672/0.738| 0.417/0.440|\\n|                               |         | LLaV A-v1.5 (Vicuna-v1.5-13B)    | 0.448/0.460| 0.563/0.584| 0.310/0.339| 0.445/0.481| 0.285/0.297| 0.664/0.754| 0.390/0.400|\\n|                               |         | InternLM-XComposer-VL (InternLM)| 0.564/0.615| 0.730/0.750| 0.360/0.416| 0.612/0.676| 0.243/0.265| 0.732/0.775| 0.546/0.572|\\n|                               |         | IDEFICS-Instruct (LLaMA-7B)      | 0.375/0.400| 0.474/0.484| 0.235/0.240| 0.409/0.428| 0.244/0.227| 0.562/0.622| 0.370/0.373|\\n|                               |         | Qwen-VL (QwenLM)                 | 0.470/0.546| 0.676/0.669| 0.298/0.338| 0.504/0.532| 0.273/0.284| 0.617/0.686| 0.486/0.486|\\n|                               |         | Shikra (Vicuna-7B)               | 0.314/0.307| 0.320/0.337| 0.237/0.241| 0.322/0.336| 0.198/0.201| 0.640/0.661| 0.324/0.332|\\n|                               |         | Otter-v1 (MPT-7B)                | 0.406/0.406| 0.436/0.441| 0.143/0.142|-0.008/0.018| 0.254/0.264| 0.475/0.481| 0.557/0.577|\\n|                               |         | Kosmos-2                        | 0.255/0.281| 0.644/0.641| 0.196/0.195| 0.358/0.368| 0.210/0.225| 0.489/0.491| 0.359/0.365|\\n|                               |         | InstructBLIP (Flan-T5-XL)        | 0.334/0.362| 0.582/0.599| 0.248/0.267| 0.113/0.113| 0.167/0.188| 0.378/0.400| 0.211/0.179|\\n|                               |         | InstructBLIP (Vicuna-7B)         | 0.359/0.437| 0.683/0.689| 0.200/0.283| 0.253/0.367| 0.263/0.304| 0.629/0.663| 0.337/0.382|\\n|                               |         | VisualGLM-6B (GLM-6B)            | 0.247/0.234| 0.498/0.507| 0.146/0.154| 0.110/0.116| 0.209/0.183| 0.342/0.349| 0.127/0.131|\\n|                               |         | mPLUG-Owl (LLaMA-7B)             | 0.409/0.427| 0.634/0.644| 0.241/0.271| 0.437/0.487| 0.148/0.180| 0.687/0.711| 0.466/0.486|\\n|                               |         | LLaMA-Adapter-V2                 | 0.354/0.363| 0.464/0.506| 0.275/0.329| 0.298/0.360| 0.257/0.271| 0.604/0.666| 0.412/0.425|\\n\\nResults and Observations on Assessment\\n\\nTo measure the assessment ability, we evaluate the performance of 15 MLLMs on 7 IQA datasets that are with at least 1,000 images and 15 human ratings per image (itu, 2000). Primarily, we notice that the majority of MLLMs are notably better than NIQE on non-natural circumstances (CGI, AIGC, artificial distortions), showing their potential towards general-purpose evaluators on a broader range of low-level appearances. We also notice that without explicit alignment with human opinions during training, the most excellent MLLM, which is again InternLM-XComposer-VL, can already outperform CLIP-ViT-Large-14 by a large margin (20%), marking the dawn of MLLMs as robust quality evaluators. Furthermore, we also design a synonym ensemble (see Sec. A.2.3) strategy which can further generally improve IQA accuracy of MLLMs, whose results are analyzed in Sec. A.3.5. Despite their proficiency, current MLLMs are still less accurate in finer-grained situations (LIVE-FB, CGIQA-6K) for the assessment task, which could be enhanced in the future.\"}"}
{"id": "0V5TVt9bk0", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGMENTS\\n\\n100% of the annotated labels in the LLVisionQA and LLDescribe datasets (question-answers and long golden descriptions) are conducted by human experts. We sincerely thank their efforts.\\n\\nWe thank the anonymous reviewers on ICLR2024 Conference on providing valuable and constructive suggestions for us to improve this paper.\\n\\nThis study is supported under the RIE2020 Industry Alignment Fund \u2013 Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).\\n\\nREFERENCES\\n\\nRecommendation 500-10: Methodology for the subjective assessment of the quality of television pictures. ITU-R Rec. BT.500, 2000.\\n\\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In ICCV, 2019.\\n\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: Visual Question Answering. In ICCV, 2015.\\n\\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023.\\n\\nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm\u2019s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023.\\n\\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server, 2015.\\n\\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.\\n\\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 320\u2013335, 2022.\\n\\nYuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou Wang. Perceptual quality assessment of smartphone photography. In CVPR, 2020.\\n\\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023.\\n\\nDeepti Ghadiyaram and Alan C. Bovik. Massive online crowdsourced study of subjective and objective picture quality. IEEE, 25(1):372\u2013387, 2016.\\n\\nTanaya Guha, Vlad Hosu, Dietmar Saupe, Bastian Goldl\u00fccke, Naveen Kumar, Weisi Lin, Victor Martinez, Krishna Somandepalli, Shrikanth Narayanan, Wen-Huang Cheng, Kree McLaughlin, Hartwig Adam, John See, and Lai-Kuan Wong. Atqam/mast\u201920: Joint workshop on aesthetic and technical quality assessment of multimedia and media analytics for societal trends. In ACM MM, pp. 4758\u20134760, 2020.\\n\\nVlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe. Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment. IEEE TIP, 29:4041\u20134056, 2020.\"}"}
{"id": "0V5TVt9bk0", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Huggingface. Introducing idefics: An open reproduction of state-of-the-art visual language model, 2023. URL https://huggingface.co/blog/idefics.\\n\\nDinesh Jayaraman, Anish Mittal, Anush K. Moorthy, and Alan C. Bovik. Objective quality assessment of multiply distorted images. In ASILOMAR, pp. 1693\u20131697, 2012.\\n\\nShu Kong, Xiaohui Shen, Zhe Lin, Radomir Mech, and Charless Fowlkes. Photo aesthetics ranking network with attributes and content adaptation. In ECCV, 2016.\\n\\nXin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. arXiv preprint arXiv:2308.00692, 2023.\\n\\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023a.\\n\\nBohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023b.\\n\\nChunyi Li, Zicheng Zhang, Haoning Wu, Wei Sun, Xiongkuo Min, Xiaohong Liu, Guangtao Zhai, and Weisi Lin. Agiqa-3k: An open database for ai-generated image quality assessment, 2023c.\\n\\nDingquan Li, Tingting Jiang, Weisi Lin, and Ming Jiang. Which has better visual quality: The clear blue sky or a blurry animal? IEEE TMM, 21(5):1221\u20131234, 2019.\\n\\nHanhe Lin, Vlad Hosu, and Dietmar Saupe. Kadid-10k: A large-scale artificially distorted iqa database. In QoMEX, pp. 1\u20133, 2019.\\n\\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023a.\\n\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023b.\\n\\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2023c.\\n\\nJiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Baochen Sun, Carl Yang, and Jie Yang. Evaluation and mitigation of agnosia in multimodal large language models, 2023.\\n\\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nClaudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S. Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking robustness in object detection: Autonomous driving when winter is coming. arXiv preprint arXiv:1907.07484, 2019.\\n\\nAnish Mittal, Rajiv Soundararajan, and Alan C. Bovik. Making a \u201ccompletely blind\u201d image quality analyzer. IEEE Signal Processing Letters, 20(3):209\u2013212, 2013.\\n\\nNaila Murray, Luca Marchesotti, and Florent Perronnin. Ava: A large-scale database for aesthetic visual analysis. In CVPR, pp. 2408\u20132415, 2012.\\n\\nOpenAI. Gpt-4 technical report, 2023.\\n\\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. ArXiv, abs/2306, 2023.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.\"}"}
{"id": "0V5TVt9bk0", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shaolin Su, Vlad Hosu, Hanhe Lin, Yanning Zhang, and Dietmar Saupe. Koniq++: Boosting no-reference image quality assessment in the wild by jointly predicting image quality and defects. In The British Machine Vision Conference (BMVC), pp. 1\u201312, 2021.\\n\\nHossein Talebi and Peyman Milanfar. Nima: Neural image assessment. IEEE TIP, 2018.\\n\\nMosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.\\n\\nJianyi Wang, Kelvin C. K. Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images, 2022.\\n\\nYilin Wang, Junjie Ke, Hossein Talebi, Joong Gon Yim, Neil Birkbeck, Balu Adsumilli, Peyman Milanfar, and Feng Yang. Rich features for perceptual quality assessment of ugc videos. In CVPR, pp. 13435\u201313444, June 2021.\\n\\nHaoning Wu, Chaofeng Chen, Jingwen Hou, Liang Liao, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Fast-vqa: Efficient end-to-end video quality assessment with fragment sampling. In ECCV, 2022.\\n\\nHaoning Wu, Chaofeng Chen, Liang Liao, Jingwen Hou, Wenxiu Sun, Qiong Yan, Jinwei Gu, and Weisi Lin. Neighbourhood representative sampling for efficient end-to-end video quality assessment, 2023a.\\n\\nHaoning Wu, Liang Liao, Chaofeng Chen, Jingwen Hou, Erli Zhang, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring opinion-unaware video quality assessment with semantic affinity criterion. In International Conference on Multimedia and Expo (ICME), 2023b.\\n\\nHaoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In ICCV, 2023c.\\n\\nHaoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Towards explainable video quality assessment: A database and a language-prompted approach. In ACM MM, 2023d.\\n\\nJiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023.\\n\\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with multimodality, 2023.\\n\\nZhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Mahajan, Deepti Ghadiyaram, and Alan Bovik. From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality. In CVPR, 2020.\\n\\nZhenqiang Ying, Maniratnam Mandal, Deepti Ghadiyaram, and Alan Bovik. Patch-vq: 'patching up' the video quality problem. In CVPR, 2021.\\n\\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014.\\n\\nCheng Zhang, Shaolin Su, Yu Zhu, Qingsen Yan, Jinqiu Sun, and Yanning Zhang. Exploring and evaluating image restoration potential in dynamic scenes. In CVPR, pp. 2057\u20132066, 2022.\"}"}
