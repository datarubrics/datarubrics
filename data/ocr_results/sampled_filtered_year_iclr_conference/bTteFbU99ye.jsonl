{"id": "bTteFbU99ye", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "bTteFbU99ye", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "bTteFbU99ye", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In our previous experiments, our ground-truth model was given as a Transformer language model trained on 1.5M sequences from the OpenWebText corpus. Here we explore underestimation behaviour when the ground-truth distribution is given by a pretrained GPT2-medium model fine-tuned on 1.5M sequences from the OpenWebText corpus. We sample from $p_L$ with softmax $T = 1.00$.\\n\\nAnalogously to the experiment in Section 4.1, we train a randomly-initialized GPT2-medium model on 1M sequences sampled from the fine-tuned model. In the center of Figure 7, we visualize mean model estimation error for 50,000 test sequences as a function of true sequence probability. Similarly to our experiments in Section 4.4, we find that the LM underestimates the probability of the majority of sequences, and does so more severely for less probable sequences. Note that this LM obtains a test perplexity of 67.97.\\n\\n![Figure 7:](image)\\n\\nLeft: Joint histogram of sequence probability estimates for test sequences. Center: Mean model estimation error by true sequence probability for test sequences. Right: Mean model estimation error by true sequence probability for sequences randomly sampled from $\\\\Sigma^\\\\ast$.\\n\\nFinally, to ensure that our findings regarding the overestimation of ill-formed sequences hold, we compute model estimation error on random sequences sampled from $\\\\Sigma^\\\\ast$ (see A.1.4 for details on how these sequences are constructed). Figure 7 center visualizes mean model estimation error as a function of target sequence probability. We find that $p_M$ overestimates the majority of ill-formed sequences, indicating that these findings hold when the ground-truth distribution is defined using a pretrained model.\\n\\nIn Sections 4.1 to 4.4, we conducted all experiments on an artificial language defined by ancestral sampling scheme with $T = 0.85$. In Section 4.5, we saw that the underestimation findings held regardless of $T$. To provide further evidence that these results hold for other values of $T$, we conduct similar experiments as in Section 4.3 with an artificial language $p_L$ defined by an ancestral sampling scheme with $T = 1.00$. Specifically, we train GPT2-medium and a GPT2-large on a total of 30M sequences sampled from $p_L$, and we compute model estimation error on a set of withheld test sequences at each training iteration.\\n\\nIn Section 4.3, we explored how estimation error varies as a function of the amount of training data, finding that while increased data weakens estimation error, the underestimation behaviour persists. As an alternative way to explore how underestimation varies with increasing data, we fine-tune Huggingface's pre-trained GPT2-small, -medium and -large models on the set of 1M sequences used in Section 4.1. Computing estimation error on a set of unseen test sequences, we find, analogously to our experiments on models trained from scratch, that pre-trained models underestimate the probability of the majority of sequences sampled from the target distribution, and do so more severely for rarer sequences (we visualize this in Figure 9). Furthermore, in Figure 10, we increase the amount of...\"}"}
{"id": "bTteFbU99ye", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2022\\n\\nFigure 8: GPT2-medium and GPT2-large trained on 30M sequences sampled from $p_L$ with $T = 1.00$. Main plots: Mean estimation error on test sequences as a function of the number of sequences seen in training. Inset plots: Relative change in mean estimation error of test sequences as a function of the number of sequences seen in training. In both cases, each line denotes estimation behaviour for a 50th of the test sequences; darker lines represent less probable sequences.\\n\\nFigure 9: Test sequence probability estimates given by pretrained neural LMs fine-tuned on 1M sequences sampled from $p_L$. Three left-most figures: The joint histograms of sequence probability estimates. The dotted line denotes the cases in which the model's estimates perfectly align with the target probability; shading to the right of this line denotes underestimation. Right-most figure: Mean sequence estimation error by target sequence probability.\\n\\nFigure 10: Pre-trained GPT2-medium fine-tuned on 30M sequences sampled from $p_L$. Main plots: Mean estimation error on test sequences as a function of the number of sequences seen in fine-tuning. Inset plots: Relative change in mean estimation error of test sequences as a function of the number of sequences seen in fine-tuning. In both cases, each line denotes estimation behaviour for a 50th of the test sequences; darker lines represent less probable sequences.\\n\\nA.1.4 Alternative Perturbations\\n\\nIn Section 4.4, we study where the language model's underestimated probability mass went by computing model estimation error on perturbed sequences. We obtain a set of perturbed sequences by (i) sampling a sequence from $p_L$ and then (ii) recursively perturbing this sequence according to the perturbations provided in Table 3.\"}"}
{"id": "bTteFbU99ye", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: GPT2-medium estimation behaviour for 15M sequences in $\\\\Sigma^*$ across two dimensions: sequence rarity (x-axis) and degree of perturbation (y-axis). The heat map is shaded based on estimation error severity; blue areas indicate overestimation, whereas brown areas indicate underestimation. We also include example sequences from two zones in this sequence space.\\n\\nEstimation error varies as a function of the amount of training data. We train GPT2-medium, GPT2-large and an LSTM model in the online \\\"Ideal World\\\" setting (Nakkiran et al., 2020) by sampling, at the beginning of each training iteration, a fresh set of 500,000 sequences from $p_L$, and training $M$ on this sample. Doing so for 60 iterations, we obtain LMs which have been trained on 30 million sequences. We compute model estimation error on $D_{test}$ at the end of each iteration $i$. Figure 4 visualizes underestimation error throughout training for these LMs. We again split test sequences by their true probability, with darker lines denoting estimation trends for less probable target sequences. The estimation curves in Figure 4 suggest that while increasing the amount of data in training initially leads to lower estimation error, this reduction eventually asymptotes. In the insets of Figure 4, we visualize the relative change in mean estimation between epochs $i-1$ and $i$. Relative change in estimation error eventually fluctuates around 0 (minimal change) for the majority of the distribution. Comparing architectures, we find that the Transformer is significantly more efficient at reducing mean estimation error throughout the distribution.\\n\\n4.4 WHERE DID THE PROBABILITY MASS GO?\\n\\nIn the previous section, we saw that even when increasing the amount of training data, $p_M$ consistently underestimates the probabilities of sequences sampled from the tail of $p_L$. At the same time, we did not find that $p_M$ overestimated sequences in the head of $p_L$. Under the assumption that $p_M$ is a proper probability distribution, that is, $\\\\sum_{x \\\\in \\\\Sigma^*} p_M(x) = 1$, these findings suggest that there exists sequences in $\\\\Sigma^*$ whose probability is overestimated by the model. In this section, we investigate where this probability mass went.\\n\\nSwap two tokens in $x$.\\nDelete a token from $x$.\\nInsert a token from $\\\\Sigma$ at a position in $x$.\\nSubstitute a token in $x$ with a token from $\\\\Sigma$.\\n\\nTable 3: PERTURB($x$) randomly applies one of these perturbations to $x$.\\n\\nTo do so, we compute model estimation error on perturbed sequences from $p_L$\u2014sequences in $\\\\Sigma^*$ which are increasingly far away from the high-probability zones in $p_L$. We build a corpus of perturbed sequences by recursively applying 30 random perturbations to each sequence $x \\\\in D_{test}$. Formally, the set of sequences at perturbation step $i$ can be expressed as: $D_i^{perturbed} = \\\\{\\\\text{PERTURB}(x) | x \\\\in D_{i-1}^{perturbed}\\\\}$ where PERTURB($x$) is a function which returns a novel perturbed version of $x$, and $D_0^{perturbed} = D_{test}$. Sequence perturbation operations are shown in Table 3. While it is possible that these operations produce other well-formed strings, we expect this to be a relatively rare outcome. We score each of these 15M sequences under both the target generative model $p_L$ and the LM $p_M$. Note that we use as LM the GPT2-medium model from the previous section (trained on 30M sequences).\"}"}
{"id": "bTteFbU99ye", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Model estimation error on test sequences as a function of target sequence probability for three different artificial languages. Each line visualizes estimation error for a GPT2-medium model trained to model a language with a specific softmax temperature parameter $T$.\\n\\nFigure 5 visualizes GPT2-medium's mean estimation error for these 15M sequences across two dimensions. On the x-axis, we plot the target probability of the sequence under $p_L$, and on the y-axis, the number of perturbations performed on the sequence. For example, the bottom-left corner (1) of Figure 5 visualizes estimation behaviour for rare sequences sampled directly from $p_L$, whereas the top-left corner (2) visualizes estimation error sequences which are equally rare, but which have been perturbed up to 30 times.\\n\\nFigure 5 offers a nuanced characterization of underestimation behaviour. The brown area on the bottom of the figure re-states the underestimation findings of the previous section. When increasing the number of perturbations performed, however, we begin entering into a space of sequences which are at first well-estimated by $p_M$ (the white areas) but then are quickly overestimated by $p_M$ (the dark blue areas), confirming that there are indeed sequences in $\\\\Sigma^*$ which are overestimated by the language model. Furthermore, these findings suggest that the tail of rare events defined by the language model does not match the tail of the artificial language\u2014the rare events typical in $p_L$ are under-represented in $p_M$ in favour of other sequences in $\\\\Sigma^*$. See Section A.1.4 for experiments finding that random sequences from $\\\\Sigma^*$ are also overestimated by $p_M$.\\n\\n4.5 Modulating the Shape of the Target Distribution\\n\\nUp to this point, the target artificial language $p_L$ was given as the distribution induced by an ancestral sampling scheme with softmax $T = 0.85$ from the generative model $L$. In the previous section, we saw that $p_M$ placed excess probability mass on areas in $\\\\Sigma^*$ with low-probability under $p_L$. Here we modulate the shape of the sequence space defined by $p_L$ to investigate how estimation error varies with respect to systematic interventions in the target distribution. To adjust the way that $p_L$ allocates probability mass over $\\\\Sigma^*$, we control the entropy of the conditional distributions at each generation step $t$ by dividing the pre-softmax logits by a temperature value $T$.\\n\\nWe visualize the effects of $T$ on the shape of the distribution in the left of Figure 6. By increasing the value of $T$, we increase the entropy of the distributions over next tokens, which in turn, spreads probability mass across a larger number of sequences in $\\\\Sigma^*$. We define four artificial languages with varying $T$ and train GPT2-medium on an ancestral sample of 1M sequences from each of these artificial languages. Figure 6 visualizes model estimation error by true sequence probability for each model. We find that models trained on languages with increased entropy perform comparatively better than models trained on low entropy languages. Estimation error for models trained on languages with greater $T$ is less severe, and this holds throughout nearly all target sequence probabilities. These results indicate that neural LMs provide a more accurate approximation of target distributions which spread mass more uniformly across $\\\\Sigma^*$.\\n\\n5 Related Work\\n\\nThis paper contributes to recent work investigating the properties of the distributions defined by LMs. Prior studies have focused on exploring (Takahashi & Tanaka-Ishii, 2019; 2017) and developing...\"}"}
{"id": "bTteFbU99ye", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ing frameworks (Meister & Cotterell, 2021) to better understand whether the large-scale statistical\\ntendencies of natural language, such as Zipf's law (Zipf, 1949), are captured by LMs. We take a\\nmore fine-grained approach, proposing a methodology which draws off of instance-level evaluation\\nschemes (Zhong et al., 2021) and the experimental control afforded by artificial corpora (White &\\nCotterell, 2021; Papadimitriou & Jurafsky, 2020). Indeed, closely related to our work is Kulikov\\net al. (2021)'s, in which artificial corpora produced by generative models were used to explore mode\\nrecovery in neural language modeling. Our analysis exploring the overestimation of ill-formed\\nsequences extends previous findings on locally normalized conditional models assigning arbitrary\\nprobability mass to unlikely sequences (Andor et al., 2016; Goyal et al., 2019; Lafferty et al., 2001),\\nneural LMs assigning high likelihood to sequences with repetitions (Welleck et al., 2020b), the con-\\nsistency of decoding algorithms (Welleck et al., 2020a), and on machine translation models placing\\nsignificant probability mass on the empty sequence (Stahlberg & Byrne, 2019).\\n\\nWe additionally contribute to the body work seeking to characterize and adapt neural model per-\\ncformance on rare or novel examples and classes (Horn & Perona, 2017; Bengio, 2015). In the\\ncontext of language modeling, Lent et al. (2021) explored performance on under-resourced lan-\\nguages, whereas Oren et al. (2019) did so on under-represented domains in training corpora. Mc-\\ncoy et al. (2021) introduced analyses to assess sequential and syntactic novelty in LMs. Focusing\\non the word frequency distribution, Dudy & Bedrick (2020) found that LMs under-perform when\\nless frequent examples are encountered at test time. In the classification setting, various approaches\\nhave been proposed to help alleviate class imbalance in the data distribution, such as data aug-\\nmentation (Sagawa et al., 2020) or the transfer of knowledge from high-frequency classes to infre-\\nquent ones (Ouyang et al., 2016; Zhu et al., 2014; Chen et al., 2021). Prior to the current neural\\nparadigm (Bengio et al., 2003), multiple approaches have been proposed to deal with the heavy-tail,\\nsuch as smoothing and back-off approaches in statistical\\nn-grams (Chen & Goodman, 1999) and\\ntwo-stage Bayesian approaches (Goldwater et al., 2006).\\n\\nCONCLUSION\\n\\nEmerging as a result of a language user's ability to produce and comprehend novel expressions, the\\nheavy-tail of rare events is one of the fundamental features of distributions in natural language. In\\nthis work, we introduce a controlled methodology to evaluate instance-level LM performance on\\nthis set of individually rare but collectively frequent events. We use generative models trained on\\nnatural language corpora to define a set of artificial languages for which we can exactly compute the\\nprobability of sequences. Training LSTM and Transformer LMs on sequences sampled from these\\nartificial languages, our analysis compares the probability estimates given to sequences by the LMs\\nto the target probabilities of sequences under the artificial language.\\n\\nOur results indicate that neural LMs systematically under-represent sequences in the tail of the target\\ndistribution, even when increasing the amount of the training data. Investigating where this proba-\\nbility mass went, our perturbation experiments reveal that neural LMs do not tend to overestimate\\nthe head of the distribution, but rather overestimate the probability of sequences outside those typ-\\nical in the target distribution. Comparing model performance on target distributions with varying\\nproperties, we find that neural LMs tend to provide more accurate approximations of distributions\\nwith greater entropy. Interpreted together, these results indicate that autoregressive neural language\\nmodels have a tendency to spread probability mass too uniformly across the space of possible se-\\ncquences.\\n\\nFinally, we would like to acknowledge that we do not know the degree of structural difference\\nbetween our Transformer-generated ground-truth distributions and the distributions of actual natural\\nlanguages. It is likely that the distribution defined by our ground truth models is less structured\\nthan the distribution of a natural language. Therefore, it is possible that some systematic difference\\nbetween natural language distributions and our ground-truth distributions may affect our results to\\na certain degree. That being said, our experiments in Section 4.5 suggest that it may actually be\\neasier for neural LMs to learn less structured distributions, and we expect the task of recovering\\na ground-truth distribution to be made easier when the target distribution and LM are in the same\\nmodel class. Nevertheless, future work should seek to conduct similar experiments using ground-\\ntruth distributions with more explicit structure.\"}"}
{"id": "bTteFbU99ye", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"REFERENCES\\n\\nDaniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. Globally normalized transition-based neural networks, 2016.\\n\\nR. Harald Baayen. Word frequency distributions. 2001.\\n\\nR. Harald Baayen. Productivity in language production. Language and Cognitive Processes, 9(3):447\u2013469, 1994. doi: 10.1080/01690969408402127. URL https://doi.org/10.1080/01690969408402127.\\n\\nR Harald Baayen. 41. corpus linguistics in morphology: Morphological productivity. In Corpus linguistics, pp. 899\u2013919. De Gruyter Mouton, 2009.\\n\\nSamy Bengio. The battle against the long tail. In Talk on Workshop on Big Data and Statistical Machine Learning, volume 1, 2015.\\n\\nYoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language model. The journal of machine learning research, 3:1137\u20131155, 2003.\\n\\nTerra Blevins and Luke Zettlemoyer. Moving down the long tail of word sense disambiguation with gloss-informed biencoders. CoRR, abs/2005.02590, 2020. URL https://arxiv.org/abs/2005.02590.\\n\\nHoward Chen, Mengzhou Xia, and Danqi Chen. Non-parametric few-shot learning for word sense disambiguation. arXiv preprint arXiv:2104.12677, 2021.\\n\\nStanley F Chen and Joshua Goodman. An empirical study of smoothing techniques for language modeling. Computer Speech & Language, 13(4):359\u2013394, 1999.\\n\\nN. Chomsky. The Logical Structure of Linguistic Theory. Springer US, 1975, 1955. ISBN 9780306307607. URL https://books.google.ca/books?id=1D66ktXOITAC.\\n\\nPaula Czarnowska, Sebastian Ruder, Edouard Grave, Ryan Cotterell, and Ann Copestake. Don't forget the long tail! a comprehensive analysis of morphological generalization in bilingual lexicon induction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 974\u2013983, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1090. URL https://aclanthology.org/D19-1090.\\n\\nDaniel D'souza, Zach Nussbaum, Chirag Agarwal, and Sara Hooker. A tale of two long tails. arXiv preprint arXiv:2107.13098, 2021.\\n\\nShiran Dudy and Steven Bedrick. Are some words worth more than others?, 2020.\\n\\nJerry A Fodor. The language of thought, volume 5. Harvard university press, 1975.\\n\\nJerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3\u201371, 1988.\\n\\nAaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019.\\n\\nSharon Goldwater, Mark Johnson, and Thomas Griffiths. Interpolating between types and tokens by estimating power-law generators. In Y . Weiss, B. Sch\u00f6lkopf, and J. Platt (eds.), Advances in Neural Information Processing Systems, volume 18. MIT Press, 2006. URL https://proceedings.neurips.cc/paper/2005/file/4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf.\\n\\nSharon Goldwater, Thomas L. Griffiths, and Mark Johnson. Producing power-law distributions and damping word frequencies with two-stage language models. Journal of Machine Learning Research, 12(68):2335\u20132382, 2011. URL http://jmlr.org/papers/v12/goldwater11a.html.\"}"}
{"id": "bTteFbU99ye", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This method provides us with sequences which are increasingly far away from high-probability zones under $p_L$. However, it does so with initial sequences sampled directly from $p_L$, and as a result, produces strings which are edit-adjacent to the high-probability zones (under $p_L$) in $\\\\Sigma^*$. To ensure that our results hold in other low-probability subspaces, we conduct an analogous experiment on sequences randomly sampled from $\\\\Sigma^*$. We sample a sequence from $\\\\Sigma^*$ by first sampling a sequence length $l$ from a Poisson distribution with $\\\\lambda = 10$. Given this length, we sample $l$ tokens from $\\\\Sigma^*$ and concatenate all tokens to form a sequence. We then score the sequence under both $p_L$ and $p_M$, and compute model estimation error. We use as artificial language $p_L$ GPT2-medium with $T = 0.85$ and we use as language model $p_M$ a GPT2-medium model trained on 30M sequences ancestrally sampled from $p_L$. Figure 11 visualizes mean estimation error as a function of the ground-truth probability of the sequence. Similarly to all other perturbation experiments, we do indeed find that $p_M$ overestimates these sequences, regardless of their true sequence probability.\\n\\n### A.1.5 Estimation Error by Sequence Length\\n\\nAutoregressive neural language models decompose the joint distribution $p(x)$ over sequences into a series of conditional distributions $p(x_i|x_\\\\leq i)$. Generating a sequence of length $n$, then, involves estimating $n$ conditional distributions. Since the probability of a sequence is inversely correlated with its length, our findings that estimation error is worse for rarer sentences may be explained by compounding errors.\\n\\nTo test this claim, we ask whether the observed estimation error is worse than would be expected if it was due to error compounding. Specifically, in Figure 12, we plot the expected model estimation (black) and the mean observed error (blue) by sequence length. Expected estimation error for sequence length $n$ is computed by multiplying the average token level estimation error by the sequence length, i.e., $n\\\\bar{\\\\epsilon}$, where $\\\\bar{\\\\epsilon} = \\\\frac{1}{|D|} \\\\sum_{x \\\\in D} \\\\frac{1}{|x|} \\\\sum_{i=1}^{\\\\text{length}(x)} \\\\log \\\\frac{p_M(x_i|x_\\\\leq i)}{p_L(x_i|x_\\\\leq i)}$.\\n\\nObserved estimation error is computed as the mean estimation error for test sentences of length $n$. Note that the shaded areas around this curve denote the 95% bootstrapped confidence intervals for this mean. As shown in Figure 12, observed estimation error for both GPT2-small and GPT2-medium is more severe than expected estimation error as we increase sentence length. This suggests that estimation error for longer (and typically rarer) sequences is not solely due to error compounding.\"}"}
{"id": "bTteFbU99ye", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In section 2, we formally defined the LNRE zone as the range of values of $N$ for which there is non-zero probability of sampling a novel event at the $N+1$th draw. Here we introduce a frequentist estimator for this probability. This in turn allows us to empirically verify if a sample exists in the LNRE zone.\\n\\nA.2.1 Estimating the Potential Productivity\\n\\nSuppose we have a set of $N$ events $D = \\\\{x_1, ..., x_N\\\\}$ drawn from some generative process $\\\\phi$. Given $D$, we aim to obtain an empirical estimate for the potential productivity $\\\\hat{P}_N$: the amount of probability allocated to unseen events as a function of $N$. We can do so using the Good-Turing estimate for the probability of an event given its frequency (Good, 1953).\\n\\nSpecifically, let $f(x, D)$ be a function which returns the frequency of the event $x$ in $D$. Let $N_m$ denote the number of types (unique events) in $D$ for which $f(x, D) = m$. Good-Turing says that for large $N$,\\n\\n$$\\\\hat{P}(x | f(x, D) = m) = \\\\frac{(m+1)N}{Nm+1} \\\\quad (3)$$\\n\\nTo obtain an estimate for $P_N$, we set $m = 0$:\\n\\n$$\\\\hat{P}_N = \\\\frac{N_0}{N} \\\\quad (4)$$\\n\\nwhich states that the total amount of probability mass allocated to unseen events is equal to the proportion of events which occurred only once (hapax legomena) in $D$. This quantity is known as the potential productivity of a linguistic process (Baayen, 2009; 2001; 1994).\\n\\nA.2.2 The LNRE Zone in OpenWebText\\n\\nWe apply this method to a subset of OpenWebText, a popular language modeling corpus. In Figure 13, we plot the empirical estimate of $P_N$ as function of the sample size $N$ for $n$-grams sampled from a subset of OpenWebText. Particularly for $n$-grams with $n \\\\geq 3$, we find that there is significant probability of sampling a previously unseen event, even for $N > 10,000,000$.\\n\\nFigure 13: The probability of sampling a novel item (potential productivity $P$) as a function of sample size $N$. Many distributions in natural language are characterized by a potentially unbounded amount of novelty.\\n\\nA.3 Model Perplexity Values\\n\\nIn this section we report relevant perplexity values for all models used. For each model, we report mean perplexity across sentences drawn from (i) the validation set generated by the artificial language they attempt to model and (ii) the OpenWebText corpus. While we include the perplexity values for our models on sentences of English, this is not to claim that our ground-truth models are meant to define a distribution which closely resembles the distribution of English.\"}"}
{"id": "bTteFbU99ye", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4:\\n\\n| Model                        | PP (val) mean | PP (eng) mean |\\n|------------------------------|---------------|---------------|\\n| LSTM                         | 36.53         | 158.31        |\\n| GPT2-small                   | 26.66         | 129.84        |\\n| GPT2-medium                  | 25.42         | 132.56        |\\n| GPT2-small (pretrained)      | 21.02         | 53.79         |\\n| GPT2-medium (pretrained)     | 20.87         | 48.42         |\\n| GPT2-large (pretrained)      | 20.90         | 47.30         |\\n| LSTM (increased data)        | 35.01         | 149.02        |\\n| GPT2-medium (increased data) | 21.60         | 94.03         |\\n| GPT2-large (increased data)  | 21.32         | 91.28         |\\n| GPT2-medium (ground-truth model) | -           | 73.79         |\\n\\nTable 5: GPT2-medium PP on validation set generated by the artificial language the LM attempts to model (val), and on real sentences sampled from OpenWebText (eng) for models used in Section 4.5\\n\\nA.4 LANGUAGE SAMPLES\\n\\nhe was a complete east end player.\\n\\n\u221226.399\\n\\na former harvard university graduate told cnn that in recent weeks, u.s. intelligence officials have begun to gather evidence that trump's campaign colluded with russia to influence the election.\\n\\n\u221261.846\\n\\nin the current study, we examined whether participants in the study performed more or less \\\"active\\\" in weight loss (p = 0.05).\\n\\n\u221251.6566\\n\\nyou, the one that is the republican candidate, who is taking over the senate and government as a democrat and who is a bipartisan democrat, and you've got to be able to get that done.\\n\\n\u221292.703\\n\\nsince the 1970s, the city has been in the midst of a landmark urban pride.\\n\\n\u221244.9523\\n\\nTable 6: Sequences ancestrally sampled from the artificial language generated by GPT2-medium with softmax $T = 0.70$.\"}"}
{"id": "bTteFbU99ye", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u201cI have no doubt that\u2019s a big part of any kind of campaign machine,\u201d he told Al Jazeera.\\n\\nAccording to the New York Times, Susan Macmahon, 54, and her husband, Elizabeth Bailey, were in the vehicle with their son, aged between 12 and 19.\\n\\nThis is the reason I\u2019d like to take advantage of these ideas in an attempt to transform my life.\\n\\nTable 7: Sequences ancestrally sampled from the artificial language generated by GPT2-medium with softmax $T = 0.85$.\\n\\nTable 8: Sequences ancestrally sampled from the artificial language generated by GPT2-medium with softmax $T = 1.00$.\\n\\nTable 9: Sequences ancestrally sampled from the artificial language generated by GPT2-medium with softmax $T = 1.15$.18\"}"}
{"id": "bTteFbU99ye", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the LNRE zone, it is difficult to obtain accurate estimates of the probability of events using straightforward maximum likelihood estimation (MLE). Accounting for this enormous amount of novelty is thus a central challenge in language modeling.\\n\\nLanguage modeling\\n\\nA model $M$ of the language $L$ attempts to define a distribution $p_M$ which closely resembles the true distribution of the language $p_L$. In a locally-normalized autoregressive model, this distribution is defined by assigning probabilities to variable length sequences $x$ via a chain-rule decomposition:\\n\\n$$p(x) = \\\\prod_{i=1}^{|x|} p(x_i | x_1:i-1) = \\\\prod_{i=1}^{|x|} \\\\exp \\\\rho(x_1:i-1, x_i) \\\\sum_{x \\\\in \\\\Sigma} \\\\exp \\\\rho(x_1:i-1, x_i)$$\\n\\nwhere $\\\\Sigma$ is the vocabulary, and $\\\\rho(x_1:i-1, x_i; \\\\theta)$ is the non-negative score of token $x_i$ given the sequence prefix $x_1:i-1$, which is computed by a neural network with parameters $\\\\theta$.\\n\\nFor $p_M$ to perfectly approximate $p_L$, we expect $p_M(x) = p_L(x)$ for all $x \\\\in \\\\Sigma^*$, where $\\\\Sigma^*$ is the set of all strings of finite length (the Kleene closure of $\\\\Sigma$). In the LNRE zone, $p_M$ is defined over a support containing a very large set of sequences which have never occurred in a training corpus (or equivalently, have all occurred with frequency 0), and which take on a very wide array of differing probabilities. For example, while the sequences $x_1, x_2 \\\\in \\\\Sigma^*$ have likely never occurred in any sample of English, most would agree that $x_1$ is far more probable than $x_2$:\\n\\n1. The East pond in Parc Lafontaine was filled to the brim with Diet Coke.\\n2. Certain nak indicate liberationing among theorter codity voters vandalized.\\n\\nLM Evaluation\\n\\nFor a perfect LM of English, we would expect the estimated probabilities of the sequences $x_1$ and $x_2$ to match their probabilities under the true distribution $p_{\\\\text{English}}$. However, since $p_{\\\\text{English}}$ and its underlying generative process are unknown, it is not possible to explicitly evaluate how closely instance-level probability estimates align. As a proxy, the mean perplexity of the model on a holdout set of sequences $D$ is typically used, which measures whether the model, on average, assigns high likelihood to unseen instances. This measure does not, however, tell us whether instance-level estimates align with their true counterparts, nor is it necessarily indicative of performance on rare, idiosyncratic events in $D$. In this way, the lack of access to the ground-truth distribution severely complicates LM evaluation on the heavy-tail of rare sequences in language.\\n\\nThe following section introduces a methodology to overcome these limitations.\\n\\n### Table 1: Components of our instance-level evaluation scheme\\n\\n| Component                        | Notation | Description |\\n|----------------------------------|----------|-------------|\\n| Generative model                | $L$      | A LM trained on natural instance-level data. |\\n| Artificial language             | $p_L$    | The distribution over sequences induced by a sampling scheme from $L$. |\\n| Language model                  | $p_M$    | The distribution of a LM trained on sequences sampled from $p_L$. |\\n| Target probabilities            | $p_L(x)$ | The probability assigned by $p_L$ to the sequence $x$. |\\n| Model probabilities             | $p_M(x)$ | The probability assigned by $p_M$ to the sequence $x$. |\\n\\nWe propose evaluating language model performance on the heavy-tail of rare events via a known probability distribution over sequences. Specifically, we train a Transformer LM on sequences sampled from a corpus of natural language to define a generative model $L$. The distribution over sequences induced by a sampling scheme from $L$, denoted $p_L$, is then our artificial language. We expect a model $M$ of this artificial language to assign probabilities $p_M(x)$ to sequences $x$ which match $p_L(x)$ for $x \\\\in \\\\Sigma^*$. For an empirical validation of this claim on a sample of practical size from the OpenWebText corpus, see the Appendix.\"}"}
{"id": "bTteFbU99ye", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2022\\n\\nThe target probabilities $p_L(x)$ of $x$ under $p_L$. To characterize neural LM behaviour on rare events, we train Transformer and LSTM LMs on data sampled from $p_L$, and compare the instance-level probability estimates given by $p_M$ to target probabilities under $p_L$. We summarize the components of this methodology in Table 1, and overview it in greater detail in the following section.\\n\\n3.1 Artificial Languages as Target Distributions\\n\\n$\\\\log p_L(x)$\\n\\n\\\"we're very excited to have the opportunity to help them,\\\" he says.\\n\\n\u221229 3811\\n\\nso what's going to happen?\\n\\n\u221217 4128\\n\\nto me, the fisheries are in the midst of a global financial crisis.\\n\\n\u221241 4835\\n\\nTable 2: Sample of sequences drawn from our artificial language.\\n\\nTo define a generative model $L$, we train a randomly-initialized GPT2-medium on 1.5M sentences sampled from the OpenWebText corpus (Gokaslan & Cohen, 2019). We set the maximum sequence length to be 128 tokens. We additionally train a byte-pair-encoding (BPE) tokenizer on this data set with a standard GPT2 vocabulary size of 57,256 tokens. For simplicity, this tokenizer is used for all models.\\n\\nUsing this generative model $L$, we define the target distribution over sequences\u2014the artificial language\u2014as the distribution induced by an ancestral sampling scheme from $L$. Thus, we draw instances $x = (x_1, \\\\ldots, x_{|x|})$ from our language $p_L$ by recursively sampling from the conditional distribution over tokens at each time step:\\n\\n$x_t \\\\sim p_L(\\\\cdot|x_{<t})$\\n\\nwhere $x_1 = \\\\text{BOS}$ and $x_t = \\\\text{EOS}$. All experiments up until Section 4.5 are conducted on the distribution induced by ancestrally sampling from $L$ with softmax temperature $T = 0.85$.\\n\\nIn Section 4.5, we explore the effects of different values of $T$ when sampling from $p_L$. Table 2 shows three sequences sampled from this distribution.\\n\\n3.2 Sequence Probability Estimation in the LNRE Zone\\n\\nGiven an artificial language $p_L$, the task of $M$\u2014the language model\u2014is to define a distribution $p_M$ whose sequence-level probability estimates closely align with the sequence-level probabilities given by $p_L$. We refer to any deviation from this desiderata as model estimation error. To quantify the model estimation error for a sequence $x$, we take the difference between the sequence's log probability under $M$ and its true log probability under $L$:\\n\\n$error(x) = \\\\log p_M(x) - \\\\log p_L(x)$ (2)\\n\\nThis quantity is the log probability ratio, which measures, in log-space, the number of times more or less likely the sequence $x$ is under the language model $M$. Note that $error(x) < 0$ indicates that $M$ underestimates the probability of $x$, whereas $error(x) > 0$ indicates that $M$ overestimates the probability of $x$. In practice, we train $M$ on a set of sequences $D_{\\\\text{train}}$ sampled from $p_L$, and compute model estimation error on a separate set of sequences $D_{\\\\text{test}}$ sampled from $p_L$. In all cases, we compute the probability of a sequence $x$ as its chain rule decomposition:\\n\\n$p(x) = \\\\prod_{i=1}^{|x|} p(x_i|x_{<i})$\\n\\nwhere $x_0 = \\\\text{BOS}$ and $x_{|x|} = \\\\text{EOS}$. When computing the ground-truth sequence probabilities for $p_L$, we take into account any softmax tempering.\\n\\n3.3 Neural Language Models\\n\\nWe study the estimation performance of two neural LM architectures: the Transformer (Vaswani et al., 2017) and the LSTM (Melis et al., 2020). When training either architecture, we halve the learning rate if validation loss increases at the end of an epoch. For all model sizes, we use a batch size of 128 sequences. Models with the lowest cross-entropy loss on a withheld validation set are used in experiments unless otherwise mentioned.\"}"}
{"id": "bTteFbU99ye", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Test sequence probability estimates given by neural LMs.\\n\\nThree left-most figures: The joint histograms of sequence probability estimates. The dotted line denotes the cases in which the model's estimates perfectly align with the target probability; shading to the right of this line denotes underestimation.\\n\\nRight-most figure: Mean sequence estimation error by target sequence probability.\\n\\nWe use the Huggingface (Wolf et al., 2020) implementations of GPT2-small, GPT2-medium and GPT2-large (Radford et al., 2019) as representative Transformer LMs. We use Adam Optimization with $\\\\epsilon = 1 \\\\times 10^{-8}$ and learning rates $\\\\alpha = 5 \\\\times 10^{-5}$, $\\\\alpha = 4 \\\\times 10^{-5}$ and $\\\\alpha = 3 \\\\times 10^{-5}$ for GPT2-small, -medium and -large respectively. Since these models are in the same model class as our artificial target language $p_L$, we expect the task of recovering the ground-truth distribution to be relatively easy compared to the true problem faced in modeling natural language, where both the distribution and the underlying generative process are unknown. For the LSTM (Hochreiter & Schmidhuber, 1997), we follow the implementation of the baseline LM described in (Melis et al., 2020). We use 2 layers and adjust the hidden state and embedding dimension (2048 and 1024, respectively) to be such that the total number of parameters is approximately equal to GPT2-small (110M).\\n\\n4 RESULTS\\n\\n4.1 ESTIMATION ERROR WITH FIXED DATA\\n\\nWe begin by exploring model estimation error on a fixed training set $D_{train}$ of 1M sequences sampled from $p_L$. We first train LSTM and GPT models on $D_{train}$, early-stopping as described above. Following training, we sample a test set $D_{test}$ of 500,000 sequences from $p_L$, and score each sequence under both the model distribution $p_M$ and the true language distribution $p_L$. From this, we obtain a set of probability estimates:\\n\\n$S_{test} = \\\\{\\\\langle p_L(x), p_M(x) \\\\rangle | x \\\\in D_{test} \\\\}$\\n\\nIf the model $M$ perfectly models the language $L$, then for each $\\\\langle p_L(x), p_M(x) \\\\rangle \\\\in S_{test}$ we would expect $p_L(x) = p_M(x)$.\\n\\nFigures 2(A) and 2(B) visualize this relationship with the $x$- and $y$-axes denoting the true and model estimated sequence probabilities respectively, and a dashed line representing equality. To compare probability estimates, we represent the set $S_{test}$ in the form of a joint histogram over this coordinate space. Histogram bins are shaded based on the number of tuples which lie in the coordinate range they define. Importantly, any deviation of this histogram from the identity line indicates that the model distorts the shape of the distribution of the language.\\n\\nFigures 2(A) and 2(B) provide evidence for distributional distortion in the form of underestimation. The majority of probability tuples in $S_{test}$ lie to the right of the identity line, indicating that LSTM and GPT2 models consistently underestimate the probability of sequences sampled from $p_L$. Furthermore, the distance between the identity line and the probability tuples grows non-linearly as function of the true sequence probability, indicating that underestimation error is more severe for rarer sequences in the language. We validate these observations in the right-most plot of Figure 2, which shows mean estimation error decreasing non-linearly as a function of the target sequences probability.\\n\\nIn addition, comparing underestimation behaviour across model size, we find that while GPT2-medium performs slightly better than GPT2-small, these improvements are typically within the range defined by the bootstrapped 95% confidence intervals. See A.1.3 for evidence indicating that this underestimation behaviour also occurs in pre-trained models fine-tuned on $D_{train}$.\\n\\nTo compute this curve, we split the target sequence probability $p_L(x)$ range into $N$ equally sized bins (by probability range). We report the mean estimation error for each bin with $>10$ sequences. We additionally compute 95% confidence intervals with 10,000 bootstraps for each mean, resampling $n$ equal to the number of sequences in the given bin.\"}"}
{"id": "bTteFbU99ye", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To understand the training dynamics underlying the previously reported underestimation, we compute model probability estimates on subsets of $D_{train}$ and $D_{test}$ at the end of each training iteration $i$. Once computed, we sort each set of probability tuples $S(i)$ by their target sequence probabilities $p_L(x)$, and split the probability tuples into 50 equally-sized bins. We plot estimation curves in Figure 3: Each curve represents a 50th of the sequences, with darker curves denoting estimation error for sequences with lower target probabilities (rarer sequences). At any given point, then, the distance between estimation curves represents the degree to which estimation error is dependent on the target probability of the sequence.\\n\\nFigure 3 left visualizes underestimation error for sequences seen in training. Around the fifth epoch, estimation error for train sentences converges to zero, that is $p_L(x) \\\\approx p_M(x)$, indicating that GPT2-medium is able to almost perfectly recover the target probabilities of training sequences no matter their target probability. At the same time, this convergence happens almost simultaneously for all sequences, indicating that a complete reduction in error during training occurs throughout the entire range of target sequence probabilities.\\n\\nFigure 3 right visualizes GPT2-medium model's performance on a separate set of test sequences. First, unlike for $D_{train}$, estimation error for $D_{test}$ does not converge to zero, meaning that even when the model has perfectly recovered the target probability of train sequences, the target probabilities for test sequences remain underestimated. Second, in the case of $D_{train}$, the difference between estimation curves of different shades converges to zero, indicating that estimation performance becomes uniform across the distribution of train sequences. We do not see such behaviour in $D_{test}$. Instead, the error curves remain at a relatively consistent distance from one another, indicating that the discrepancy in estimation error at different parts of the distribution is unchanging for sequences not seen during training.\\n\\n4.3 Estimation Error by Amount of Training Data\\n\\nFigure 4: GPT2-medium, GPT2-large and LSTM trained on 30M sequences sampled from $p_L$.\\n\\nMain plots: Mean estimation error on test sequences as a function of the number of sequences seen in training.\\n\\nInset plots: Relative change in mean estimation error of test sequences as a function of the number of sequences seen in training. In both cases, each line denotes estimation behaviour for a 50th of the test sequences; darker lines represent less probable sequences.\\n\\nOur previous experiment trained language models on a set of 1M sequences. A plausible explanation for the model's underestimation behaviour on unseen test sequences is therefore that the language model has not seen enough samples from the target distribution. Here we explore how...\"}"}
{"id": "bTteFbU99ye", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"EVALUATING DISTRIBUTIONAL DISTORTION IN NEURAL LANGUAGE MODELING\\n\\nBenjamin LeBrun 1,2,\u2020\\nAlessandro Sordoni 3,*\\nTimothy J. O'Donnell 1,2,4,*\\n\\n1 McGill University\\n2 Mila \u2013 Quebec Artificial Intelligence Institute\\n3 Microsoft Research\\n4 Canada CIFAR AI Chair, Mila\\n\\nABSTRACT\\nA fundamental characteristic of natural language is the high rate at which speakers produce novel expressions. Because of this novelty, a heavy-tail of rare events accounts for a significant amount of the total probability mass of distributions in language (Baayen, 2001). Standard language modeling metrics such as perplexity quantify the performance of language models (LM) in aggregate. As a result, we have relatively little understanding of whether neural LMs accurately estimate the probability of sequences in this heavy-tail of rare events. To address this gap, we develop a controlled evaluation scheme which uses generative models trained on natural data as artificial languages from which we can exactly compute sequence probabilities. Training LMs on generations from these artificial languages, we compare the sequence-level probability estimates given by LMs to the true probabilities in the target language. Our experiments reveal that LSTM and Transformer language models (i) systematically underestimate the probability of sequences drawn from the target language, and (ii) do so more severely for less-probable sequences. Investigating where this probability mass went, (iii) we find that LMs tend to overestimate the probability of ill-formed (perturbed) sequences. In addition, we find that this underestimation behaviour (iv) is weakened, but not eliminated by greater amounts of training data, and (v) is exacerbated for target distributions with lower entropy.\\n\\n1 INTRODUCTION\\n\\nFigure 1: GPT2 sequence probability estimates plotted against the true sequence probabilities. Neural LMs underestimate the probability of sequences drawn from the language they are trained to model.\\n\\nNatural language is fundamentally creative\u2014speakers and listeners frequently produce and comprehend sentences which have never been produced before (Fodor, 1975; Fodor & Pylyshyn, 1988; Chomsky, 1975, 1955). As a side-effect of this property, distributions in natural language are characterized by a heavy-tail of individually improbable events which collectively account for a significant amount of the total probability mass of the distribution (Khmaladze, 1988; Baayen, 2001). Precisely approximating this large number of rare events is one of the foundational challenges for models of natural language (Good, 1953; Jelinek, 1980; Katz, 1987; Kneser & Ney, 1995; Wood et al., 2011; Goldwater et al., 2011).\\n\\nAutoregressive neural language models (Bengio et al., 2003; Mikolov et al., 2013; Radford et al., 2019) attempt to do so by decomposing the probability of an event (a sequence) into a series of conditional distributions, each parameterized by a shared neural network.\\n\\nRecently, a growing body work has sought to understand how these language models (LM) fit the distribution of a language beyond standard measures such as perplexity.\"}"}
{"id": "bTteFbU99ye", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"as perplexity. Meister & Cotterell (2021), for example, investigated the statistical tendencies of the distribution defined by neural LMs, whereas Kulikov et al. (2021) explored whether they adequately capture the modes of the distribution they attempt to model. At the same time, increased focus has been given to performance on rare or novel events in the data distribution, both for models of natural language (McCoy et al., 2021; Lent et al., 2021; Dudy & Bedrick, 2020; Oren et al., 2019) and neural models more generally (see, for example Sagawa et al., 2020; D\u2019souza et al., 2021; Chen et al., 2021; Blevins & Zettlemoyer, 2020; Czarnowska et al., 2019; Horn & Perona, 2017; Ouyang et al., 2016; Bengio, 2015; Zhu et al., 2014). Neither of these branches of work, however, has explored instance-level LM performance on rare sequences in the distribution. As a result, we have relatively little understanding of how neural LMs approximate sequences in the heavy-tail characteristic of natural language.\\n\\nIn this work, we introduce a controlled methodology to explore how LMs estimate the probability of sequences in the heavy-tail of the distribution. Our instance-level evaluation scheme explicitly compares the target probability distribution of the language to the distribution defined by the LM. Since the true distribution of any natural language is in practice unknown, we use a Transformer LM trained on natural data as a generative model to define target artificial languages for which we can exactly compute sequence probabilities. Training LSTM and Transformer LMs on sequences sampled from these target artificial languages, we compare the sequence-level probability estimates given by neural LMs to the target probabilities in the language. By controlling the entropy of the generative model\u2019s conditional distributions, we create a set of artificial languages with varying distributional properties, and analyze how LM estimation behaviour is modulated by the properties of the target distribution.\\n\\nOur experiments uncover the extent to which neural LMs provide a distorted fit of the language they are trained to model. We find that LSTM and Transformer LMs (i) systematically underestimate the probability of sequences drawn from the target language and (ii) do so more when such sequences are rare. Where did this underestimated probability mass go? We do not find that the underestimation is accompanied by overestimation in the head of distribution. Rather, we find that LMs tend to (iii) overestimate the probability of rare perturbed (ill-formed) sequences. Interpreted together, these findings indicate that on the one hand, neural LMs under-represent well-formed sequences in the tail of the language they attempt to model, and on the other hand, over-represent ill-formed sequences far away from high probability zones in sequence-space. In addition, we find that (iv) greater amounts of training data lessen underestimation but do not eliminate it and that (v) underestimation is exacerbated for target distributions with lower entropy.\\n\\nWe begin by briefly characterizing why distributions with a large number of rare events (LNRE) emerge in natural language, and why these events pose challenges for LMs. Furthermore, we motivate the need for instance-level evaluation when dealing with a large number of rare events.\\n\\n**Productivity**\\n\\nIn the context of language production, a language user has the ability to produce, at any given point in their linguistic lifespan, an utterance which they have never produced before. This creativity is the result of the generative property of productivity, which states that on the basis of finite linguistic experience, a language user can produce and comprehend an unbounded number of grammatically acceptable utterances (Chomsky, 1975, 1955). Productive processes induce a distribution which places non-zero probability on unseen events at all practical sample sizes. Because of this property, many of the distributions in natural language\u2014particularly the distribution over the sequences of a language\u2014are characterized by a heavy-tail of rare events.\\n\\n**LNRE Zone**\\n\\nTo make explicit the connection between productivity and a heavy-tail of rare events, let $P_N$ denote the probability of sampling a novel (previously unseen) event from some distribution after having sampled $N$ events. Then productivity as described above states that $P_N > 0$ for all sample sizes $N$ that occur in practice. The range of sample sizes $N$ for which it is the case that $P_N > 0$ is known as the LNRE zone (Khmaladze, 1988; Baayen, 2001). The LNRE zone for natural language appears to be very large, and it seems likely that $P_N > 0$ will remain greater than 0 for samples of natural language many orders of magnitude larger than all the data currently available.\"}"}
