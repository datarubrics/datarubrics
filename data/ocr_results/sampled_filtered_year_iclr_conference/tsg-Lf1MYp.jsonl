{"id": "tsg-Lf1MYp", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Safwan S. Halabi, Luciano M. Prevedello, Jayashree Kalpathy-Cramer, Artem B. Mamonov, Alexander Bilbily, Mark Cicero, Ian Pan, Lucas Ara\u00fajo Pereira, Rafael Teixeira Sousa, Nita-mar Abdala, Felipe Campos Kitamura, Hans H. Thodberg, Leon Chen, George Shih, Katherine Andriole, Marc D. Kohli, Bradley J. Erickson, and Adam E. Flanders. The rsna pediatric bone age machine learning challenge. *Radiology*, 290(3):498\u2013503, 2019. ISSN 0033-8419. doi: 10.1148/radiol.2018180736.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pp. 770\u2013778, 2016.\\n\\nRuining He and Julian J. McAuley. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In *Proc. the International Conference on World Wide Web (WWW)*, pp. 507\u2013517, 2016.\\n\\nDan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In *Proc. the International Conference on Learning Representations (ICLR)*, 2017.\\n\\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. *ArXiv*, abs/1503.02531, 2015.\\n\\nYen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data. In *Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)*, 2020.\\n\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*, 2014.\\n\\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In *Proc. the Advances in Neural Information Processing Systems (NeurIPS)*, 2017.\\n\\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In *Proc. the Advances in Neural Information Processing Systems (NeurIPS)*, 2018.\\n\\nShiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In *Proc. the International Conference on Learning Representations (ICLR)*, 2018.\\n\\nWeitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. *Advances in Neural Information Processing Systems*, 33, 2020.\\n\\nWeiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In *Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)*, pp. 212\u2013220, 2017.\\n\\nYan Luo, Yongkang Wong, Mohan S. Kankanhalli, and Qi Zhao. G-softmax: Improving intraclass compactness and interclass separability of features. *IEEE Trans. Neural Networks Learn. Syst.*, 31(2):685\u2013699, 2020.\\n\\nAhsan Mahmood, Junier Oliva, and Martin Andreas Styner. Multiscale score matching for out-of-distribution detection. In *Proc. the International Conference on Learning Representations (ICLR)*, 2020.\\n\\nJulian J. McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. Image-based recommendations on styles and substitutes. In *Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval*, pp. 43\u201352, 2015.\\n\\nTom\u00e1s Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In *Proc. the International Conference on Learning Representations (ICLR)*, 2013.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nAnh Mai Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proc. of the IEEE conference on computer vision and pattern recognition (CVPR), pp. 427\u2013436, 2015.\\n\\nLiudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: unbiased boosting with categorical features. arXiv preprint arXiv:1706.09516, 2017.\\n\\nStephan Rabanser, Stephan G\\\"unnemann, and Zachary Lipton. Failing loudly: An empirical study of methods for detecting dataset shift. Advances in Neural Information Processing Systems, 32:1396\u20131408, 2019.\\n\\nJie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark A DePristo, Joshua V Dillon, and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. arXiv preprint arXiv:1906.02845, 2019.\\n\\nVikash Sehwag, Mung Chiang, and Prateek Mittal. {SSD}: A unified framework for self-supervised outlier detection. In Proc. the International Conference on Learning Representations (ICLR), 2021.\\n\\nSeonguk Seo, Yumin Suh, Dongwan Kim, Geeho Kim, Jongwoo Han, and Bohyung Han. Learning to optimize domain specific normalization for domain generalization. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXII 16, pp. 68\u201383. Springer, 2020.\\n\\nAlireza Shafaei, Mark Schmidt, and James J. Little. A less biased evaluation of out-of-distribution sample detectors. pp. 3, 2019.\\n\\nEngkarat Techapanurak, Masanori Suganuma, and Takayuki Okatani. Hyperparameter-free out-of-distribution detection using cosine similarity. In Proc. of the Asian Conference on Computer Vision (ACCV), volume 12625, pp. 53\u201369, 2020.\\n\\nVikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In Proc. the International Conference on Machine Learning (ICML), pp. 6438\u20136447. PMLR, 2019.\\n\\nOvadia Yaniv, Fertig Emily, Ren Jie, Nado Zachary, Sculley David, Nowozin Sebastian, Dillon Joshua V, Lakshminarayanan Balaji, and Snoek Jasper. Can you trust your model\u2019s uncertainty? evaluating predictive uncertainty under dataset shift. In Proc. the Advances in Neural Information Processing Systems (NeurIPS), 2019.\\n\\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self-supervised learning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021.\\n\\nZhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial autoencoder. In Proc. of the IEEE conference on computer vision and pattern recognition (CVPR), 2017.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we describe the three post-hoc and task-agnostic OOD detection methods in detail, focusing mainly on their formulation and how each method assigns an OOD score to an input sample.\\n\\nA.1 Maximum of Softmax Probability (MSP)\\n\\nIn this method, the maximum of softmax probability is considered as confidence score (Hendrycks & Gimpel, 2017). Formally, we calculate the maximum softmax probability as follows:\\n\\n$$\\n\\\\text{SMSP}(x) = \\\\max_c \\\\exp(z(c))\\n$$\\n\\nwhere $C$ is the number of target classes, $c$ is the index of a class, and $z(j)$ denotes the $j$th attribute of the feature in the logit layer.\\n\\nA.2 ODIN\\n\\nODIN (Liang et al., 2018) utilized two well-established techniques, namely temperature scaling and input preprocessing to increase the difference between softmax scores of in-distribution and OOD samples. Temperature scaling was originally proposed in Hinton et al. (2015) to distill the knowledge in neural networks and was later adopted widely in classification tasks to calibrate confidence of prediction (Guo et al., 2017). In addition to temperature scaling, the input is preprocessed in order to increase the softmax score of the given input by adding small perturbations which are obtained by back-propagating the gradient of the loss with respect to the input. More specifically, ODIN is computed as follows:\"}"}
{"id": "tsg-Lf1MYp", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\n\\\\[ S(x; T) = \\\\max_c \\\\exp \\\\frac{z(c)}{T} \\\\]\\n\\n\\\\[ PC_j = 1 \\\\exp \\\\frac{\\\\tilde{z}(j)}{T}, \\\\]\\n\\nwhere \\\\( T \\\\in \\\\mathbb{R}^+ \\\\) is the temperature scaling parameter, \\\\( C \\\\) is the number of target classes, \\\\( c \\\\) is the index of class, and \\\\( z(j) \\\\) denotes the \\\\( j \\\\)th attribute of the logit layer features of input \\\\( x \\\\). During training, \\\\( T \\\\) is set to 1.\\n\\nFor OOD detection, the input is first pre-processed as follows:\\n\\n\\\\[ \\\\tilde{x} = x - \\\\varepsilon \\\\text{sign} \\\\left( -\\\\nabla x \\\\log S(x; T) \\\\right) \\\\]\\n\\nwhere \\\\( \\\\varepsilon \\\\) represents the magnitude of perturbation.\\n\\nNext, the network calculates the calibrated softmax score of the preprocessed input as follows:\\n\\n\\\\[ S_{ODIN}(x; T) = \\\\max_c \\\\exp \\\\frac{\\\\tilde{z}(c)}{T} \\\\]\\n\\n\\\\[ PC_j = 1 \\\\exp \\\\frac{\\\\tilde{z}(j)}{T}, \\\\]\\n\\nwhere \\\\( \\\\tilde{z}(j) \\\\) denotes the \\\\( j \\\\)th attribute of the logit layer features of the preprocessed input \\\\( \\\\tilde{x} \\\\).\\n\\nLastly, the modified softmax score is compared to a threshold value \\\\( \\\\delta \\\\). If the score is greater than the threshold, then the input is classified as ID sample and otherwise OOD. Originally, \\\\( T, \\\\varepsilon, \\\\) and \\\\( \\\\delta \\\\) are hyperparameters and are selected such that the false positive rate (FPR) at true positive rate (TPR) 95% is minimized on the validation OOD dataset. However, the performance saturates when \\\\( T \\\\) is greater than 1000 and therefore, in general, a large value of \\\\( T \\\\) is preferred. Following this, in this paper, we fix \\\\( T = 1000 \\\\) for our experiments.\\n\\nA.3 MAHALANOBIS DETECTOR\\n\\nTo obtain Mahalanobis distance-based OOD score (Lee et al., 2018) of a sample, we calculate the mahalanobis distance from the clusters of classes to the sample. Then, the distance from the closest class is chosen as the confidence score.\\n\\nSpecifically, the Mahalanobis score of an input \\\\( x \\\\) is defined as\\n\\n\\\\[ S_{mahala}(x) = X_l \\\\alpha_l \\\\max_c f_l \\\\theta(x) - \\\\mu_{c,l}^\\\\top \\\\Sigma_{l}^{-1} f_l \\\\theta(x) - \\\\mu_{c,l} \\\\]\\n\\nwhere \\\\( c \\\\) and \\\\( l \\\\) are the class and layer index, respectively, \\\\( f_l \\\\theta \\\\) is the \\\\( l \\\\)th layer's feature representation of an input \\\\( x \\\\), \\\\( \\\\mu_{c,l} \\\\) and \\\\( \\\\Sigma_l \\\\) are their class mean vector and tied covariance of the training data, correspondingly.\\n\\nNote that ODIN and Mahalanobis Detector assume the availability of OOD validation dataset. However, some recent works (Shafaei et al., 2019; Techapanurak et al., 2020) report that this assumption limits the OOD detection generalizability since a model is biased towards an OOD validation set. In response, this paper validate the performance of OOD methods in the version that does not require to tune with OOD validation dataset. We perform the performance of ODIN as Hsu et al. (2020). We do not perform Mahalanobis Detector ensembling over on all layers with the optimal linear combination which requires explicit OOD data. Instead, we perform two version that use only the penultimate layer of hidden representation and sum uniformly over all layers. Therefore, for all our experiments, we use modified OOD detection methods that do not require the OOD validation dataset.\\n\\nB ANALYSIS ON THE TEXT DATASET\\n\\nIn this section, we provide a detailed analysis of the text dataset. We use the Amazon Review (He & McAuley, 2016; McAuley et al., 2015) dataset. We consider the product category \u201cbook\u201d and conducted an analysis to see the impact of time on product reviews. We then performed an analysis to see the impact of time on the length of the reviews. Figure 6 presents a comparison between density plot of review length for each year from 2006 to 2014. We observe that as we move ahead in time,\"}"}
{"id": "tsg-Lf1MYp", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Abstract**\\n\\nDespite the impressive performance of deep networks in vision, language, and healthcare, unpredictable behaviors on samples from the distribution different than the training distribution cause severe problems in deployment. For better reliability of neural-network-based classifiers, we define a new task, natural attribute-based shift (NAtS) detection, to detect the samples shifted from the training distribution by some natural attribute such as age of subjects or brightness of images. Using the natural attributes present in existing datasets, we introduce benchmark datasets in vision, language, and medical domain for NAtS detection. Further, we conduct an extensive evaluation of prior representative out-of-distribution (OOD) detection methods on NAtS datasets and observe an inconsistency in their performance. To understand this, we provide an analysis on the relationship between the location of NAtS samples in the feature space and the performance of distance- and confidence-based OOD detection methods. Based on the analysis, we split NAtS samples into three categories and further suggest a simple modification to the training objective to obtain an improved OOD detection method that is capable of detecting samples from all NAtS categories.\\n\\n**Introduction**\\n\\nDeep learning has significantly improved the performance in various domains such as computer vision, natural language processing, and healthcare (Bojarski et al., 2016; Mikolov et al., 2013; Esteva et al., 2016). However, it has been reported that the deep classifiers make unreliable predictions on samples drawn from a different distribution than the training distribution (Amodei et al., 2016; Hendrycks & Gimpel, 2017; Nguyen et al., 2015). Detection of unreliable predictions is important to build a robust model but it is relatively hard when the test distribution is gradually shifting due to a natural attribute since it is difficult to determine whether the classifier fails when the shift is not significant. These shifts occur in the real-world as a result of a change in specific attribute. For example, a clinical text-based diagnosis classifier trained in 2021 will gradually encounter increasingly shifted samples as time flows, since writing styles change and new terms are introduced in time. Detection of such samples is a vital task especially in safety-critical systems, such as autonomous vehicle control or medical diagnosis, where wrong predictions can lead to dire consequences. To this end, we take a step forward by proposing a new task of detecting samples shifted by a natural attribute (e.g., age, time) that can easily be observed in the real-world setting. We refer to such shifts as Natural Attribute-based Shifts (NAtS), and the task of detecting them as NAtS detection.\\n\\nDetection of NAtS is both different from, and also more challenging than out-of-distribution (OOD) detection (Hendrycks & Gimpel, 2017; Liang et al., 2018; Lee et al., 2018; Chandramouli & Sageev, 2020), which typically evaluates the detection methods with a clearly distinguished in-distribution (ID) samples and OOD samples (e.g., CIFAR10 as ID and SVHN as OOD, which have disjoint labels). In contrast, we aim to detect samples from a natural attribute-based shift within the same label space. Since NAtS samples share more features with the ID than the typical OOD samples do, identifying the former is expected to be more challenging than the latter. Although OOD detection has some relevance to NAtS detection, comprehensive evaluation of the existing OOD detection methods on the natural attribute-based shift is an unexplored territory. Therefore, in this paper, we perform an extensive evaluation of representative OOD methods on NAtS samples.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022.\\n\\nThe diagnosis model can be properly configured, and the diagnosis model can perform in the optimal setting. In other cases, domain generalization can be preferred, such as when we expect the classifier to be deployed in a less controlled environment (e.g., online image classifier) for non-safety critical tasks.\\n\\nIn this paper, we formalize NAtS detection to enhance the reliability of real-world decision systems. Since there exists no standard dataset for this task, we create a new benchmark dataset in the vision, text, and medical domain by adjusting the natural attributes (e.g., age, time, and brightness) of the ID dataset. Then we conduct an extensive evaluation of representative confidence- and distance-based OOD methods on our datasets and observe that none of the methods perform consistently across all NAtS datasets.\\n\\nAfter a careful analysis on where NAtS samples reside in the feature space and its impact on the distance- and confidence-based OOD detection performance, we identify the root cause of the inconsistent performance. Following this observation, we define three general NAtS categories based on two criteria: the distance between NAtS samples and the decision boundary, the distance between NAtS samples, and the ID data. Finally, we conduct an additional experiment to demonstrate that a simple modification to the negative log-likelihood training objective can dramatically help the Mahalanobis detector (Lee et al., 2018), a distance-based OOD detection method, generalize to all NAtS categories. We also compare our results with various baselines and show that our proposed modification outperforms the baselines and is effective across the three NAtS datasets.\\n\\nIn summary, the contributions of this paper are as follows:\\n\\n- We define a new task, NAtS detection, which aims to detect the samples from a distribution shifted by some natural attribute. We create a new benchmark dataset and provide them to encourage further research on evaluating NAtS detection.\\n\\n- To the best of our knowledge, this is the first work to conduct a comprehensive evaluation of the OOD detection methods on shifts based on natural attributes, and discover that none of the OOD methods perform consistently across all NAtS scenarios.\\n\\n- We provide novel analysis based on the location of shifted samples in the feature space and the performance of existing OOD detection methods. Based on the analysis, we split NAtS samples into three categories.\\n\\n- We demonstrate that a simple yet effective modification to the training objective for deep classifiers enables consistent OOD detection performances for all NAtS categories.\\n\\nWe now formalize a new task, NAtS detection, which aims to enhance the reliability of real-world decision systems by detecting samples from NAtS. We address this task in the classification problems. Let $D_I = \\\\{X, Y\\\\}$ denote the in-distribution data, which is composed of $N$ training samples with inputs $X = \\\\{x_1, ..., x_N\\\\}$ and labels $Y = \\\\{y_1, ..., y_N\\\\}$. Specifically, $x_i \\\\in \\\\mathbb{R}^d$ represents a $d$-dimensional input vector, and $y_i \\\\in \\\\mathcal{K}$ represents its corresponding label where $\\\\mathcal{K} = \\\\{1, ..., K\\\\}$ is a set of class labels. The discriminative model $f_\\\\theta: X \\\\rightarrow Y$ learns with ID dataset $D_I$ to assign label $y_i$ for each $x_i$.\\n\\nIn the NAtS detection setting, we assume that an in-distribution sample consists of attributes, and some of the attributes can be shifted in the test time due to natural causes such as time, age, or brightness. When a particular attribute $A$ (e.g., age), which has a value of $a$ (e.g., 16), is shifted by the degree of $\\\\delta$, the shifted distribution can be denoted as $D_A = a + \\\\delta_S = \\\\{X', Y'\\\\}$. $X' = \\\\{x'_1, ..., x'_M\\\\}$ and $Y' = \\\\{y'_1, ..., y'_M\\\\}$ represents the $M$ shifted samples and labels respectively. Importantly, in the NAtS setting, although the test distribution is changed from the ID, the label space is preserved as $\\\\mathcal{K}$, which is the set of class labels in $D_I$. In the test time, the model $f_\\\\theta$ might encounter the sample $x'$ from a shifted data $D_A$, and it should be able to identify that the attribute-shifted sample is not from the ID.\\n\\nIn this section, we describe three benchmark datasets which have a controllable attribute for simulating realistic distribution shifts. Since there exists no standard dataset for NAtS detection, we create new benchmark datasets using existing datasets by adjusting natural attributes in order to reflect real-world scenarios. We carefully select datasets from vision, language, and medical domains containing natural attributes (e.g., year, age, and brightness), which allows us to naturally split the...\"}"}
{"id": "tsg-Lf1MYp", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Facial images from the UTKFace dataset to show the variation with age. X-ray images with different levels of brightness created from the RSNA Bone Age dataset.\\n\\nWe use the UTKFace dataset (Zhang et al., 2017) which consists of over 20,000 face images with annotations of age, gender, and ethnicity. As shown in Figure 1, we can visually observe that the facial images vary with age. Therefore, we set the 26-year-old age as $D_I$. For creating the NAtS dataset, we vary the age of UTKFace dataset. To obtain an equal number of samples in the NAtS dataset, the age groups that have less than 200 images are merged into one group until it has 200 samples. Finally, 15 groups $D_{age}$ are produced for the NAtS datasets, varying the ages from 25 to 1 (i.e., $D_{age}=25$, $D_{age}=24$, \\\\ldots, $D_{age}=1$).\\n\\nWe use the Amazon Review dataset (He & McAuley, 2016; McAuley et al., 2015) which contains product reviews from Amazon. We consider the product category \u201cbook\u201d and group its reviews based on the year to reflect the distributional shift across time. We obtain 9 groups with each group containing reviews from the year between 2005 and 2014. Then, the group with 24,000 reviews posted in 2005 is set as $D_I$, and the groups with reviews after 2005 as $D_{year}$ (i.e., $D_{year}=2006$, $D_{year}=2007$, \\\\ldots, $D_{year}=2014$). Each $D_{year}$ group contains 1500 positive reviews and 1500 negative reviews. We observed that as we move ahead in time, the average length of a review gets shorter and it uses more adjectives than previous years. Due to the space constraint, we provide a detailed analysis of the dataset in the Section B of the Appendix.\\n\\nWe use the RSNA Bone Age dataset (Halabi et al., 2019), a real-world dataset that contains left-hand X-ray images of the patient, along with their gender and age (0 to 20 years). We consider patients in the age group of 10 to 12 years for our dataset. To reflect diverse X-ray imaging set-ups in the hospital, we varied the brightness factor between 0 and 4.5 and form 16 different dataset $D_{brightness}$ (i.e., $D_{brightness}=0.0$, $D_{brightness}=0.2$, \\\\ldots, $D_{brightness}=4.5$), and each group contains X-ray images of 200 males and 200 females.\\n\\nIn this section, we briefly discuss about OOD detection methods and conduct an extensive evaluation of OOD detection methods on our proposed benchmark datasets.\\n\\n4.1 OOD DETECTION METHODS\\n\\nIn this work, we use three widely-used post-hoc and modality-agnostic OOD detection methods. We use maximum of softmax probability (MSP) (Hendrycks & Gimpel, 2017) and ODIN (Liang et al., 2018) as confidence-based OOD detection baselines, and Mahalanobis detector (Lee et al., 2018) as distance-based OOD detection baseline. Note that ODIN and Mahalanobis detector assume the availability of OOD validation dataset to tune their hyperparameters. However, for all our experiments, we use variants of the above methods that do not access the OOD validation dataset as Hsu et al. (2020). The exact equations and details of how each OOD detection method assigns an OOD score to a given sample is provided in Section A of the Appendix.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nFigure 2: Comparison of well-known distance-based and confidence-based OOD detection methods for the CE model on our benchmark datasets. Age '26', year '2005', and brightness '1.0' are in-distribution data in UTKFace, Amazon Review, and RSNA Bone Age dataset, respectively. The NAtS detection performance of these methods is inconsistent across different datasets.\\n\\nNAT dataset in text domain and train a 4-layer Transformer with the cross-entropy loss for the sentiment classification task. Lastly, in medical domain, we use our RSNA Bone Age NAtS dataset and train a ResNet18 with cross-entropy loss to predict the gender given the hand X-ray image of the patient. We then evaluate the trained model on the corresponding test set in image, text, medical domains, respectively. Further, we evaluate the NAtS detection performance of representative OOD detection methods in image, text, and medical domains on their corresponding NAtS datasets, which gradually shift with age, year, and brightness, respectively.\\n\\nResults. We present the classification accuracy of the trained models on the ID test set in Table 1. We observe that the models trained using the cross-entropy loss obtain high accuracy and perform well on their corresponding tasks. We further demonstrate the effectiveness of the existing representative OOD detection methods on our benchmark datasets in Figure 2. The higher AUROC indicates that more NAtS samples are distinguished from the ID samples using OOD detection methods. To provide the reference point, we calculate the AUROC values where there is no shift in the attribute. We observe that in the UTKFace NAtS dataset, samples are detected by ODIN and MSP, which are confidence-based methods, but not by Mahalanobis detector (Figure 2a). In the Amazon Review dataset, NAtS samples are detected only by the Mahalanobis detector, while MSP and ODIN fail (Figure 2b). Moreover, the scores of confidence-based methods are lower than 50 in AUROC. Lastly, Figure 2c shows that inputs from NAtS in the RSNA Bone Age dataset are detected well by all three methods.\\n\\n5 ANALYZING INCONSISTENCY OF OOD DETECTION METHODS\\n\\nIn this section, we first study the behavior of NAtS samples in three datasets using PCA visualization. Then we analyze the inconsistent performance of the OOD detection methods by considering them in two categories, namely confidence-based and distance-based methods. Lastly, based on the analysis, we conclude this section by defining three NAtS categories.\\n\\n5.1 ANALYSIS OF THE LOCATION OF NAtS SAMPLES\\n\\nAs illustrated in Figure 3, we apply principal component analysis (PCA) on the feature representations obtained from the penultimate layer of the models to visualize the movement of NAtS samples as we monotonically increase the degree of attribute shift (i.e., age, year, and brightness). Further, Figure 4 presents the model's prediction confidence across varying degrees of the attribute shift.\\n\\nImage. By gradually changing the age, NAtS samples move toward the space between the two clusters of ID samples (i.e., the decision boundary) as can be seen in Figure 3 [Top]. Further, Figure 4a demonstrates that confidence decrease as we increase the degree of attribute shift, which indicates that NAtS samples move close to the decision boundary. Note that the majority of the NAtS samples still overlap with ID sample clusters as we change the age.\\n\\nText. As shown in Figure 3 [Middle], NAtS samples gradually move away from the ID samples (and away from the decision boundary) as the year changes. In contrast to the UTKFace dataset, the confidence gradually increases, as shown in Figure 4b, since the NAtS samples are getting far away from the decision boundary.\\n\\n| Dataset          | CE     | Ours  |\\n|------------------|--------|-------|\\n| UTKFace          | 94.6 \u00b1 0.6 | 94.1 \u00b1 0.7 |\\n| Amazon Review    | 85.3 \u00b1 0.9 | 84.0 \u00b1 0.9 |\\n| RSNA Bone Age    | 93.0 \u00b1 0.6 | 92.3 \u00b1 0.3 |\\n\\nTable 1: In-distribution classification accuracy on three datasets with cross-entropy loss and our proposed loss.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: PCA visualization to demonstrate the movement of NAtS samples as we vary the age, year and brightness in UTKFace, Amazon Review and RSNA Bone Age dataset respectively.\\n\\n(a) UTKFace\\n(b) Amazon Book Review\\n(c) RSNA Bone Age\\n\\nFigure 4: Impact on prediction confidence on varying age, year, and brightness in UTKFace, Amazon Review, and RSNA Bone Age dataset, respectively.\\n\\nMedical. Figure 3 [Bottom] demonstrates that when we increase the brightness, NAtS samples move to the middle of the two classes and also move towards the outer edge of the ID sample clusters. Furthermore, as shown in Figure 4c, the relatively decreased confidence indicates that NAtS samples are placed near the decision boundary as we increase the brightness of the images.\\n\\n5.2 COMPARISON BETWEEN CONFIDENCE-BASED AND DISTANCE-BASED OOD DETECTION\\n\\nConfidence-based methods. Figure 2 and Figure 3 illustrate that confidence-based methods achieve high AUROC when the NAtS samples are near the decision boundary due to their low confidence. In the image and medical domain, we observed the NAtS samples moving towards the decision boundary with increasing attribute shift, thus they are detected by the confidence-based methods. In contrast, in the text domain, the high prediction confidence of the shifted NAtS samples causes the degradation in the AUROC of the confidence-based methods. Therefore, we conclude that to effectively utilize the confidence-based methods in all three NAtS datasets, it is necessary to reduce the confidence of samples outside the ID, namely, enforce NAtS samples to move near the decision boundary, which is not always possible (e.g., Amazon Review dataset).\\n\\nDistance-based methods. From Figure 2 and Figure 3, we observe that the distance-based OOD detection method (i.e., Mahalanobis Detector) achieves high AUROC when NAtS samples are sufficiently away from the ID samples. In the text and medical domains, the Mahalanobis detector worked well since NAtS samples moved sufficiently away from the ID samples as the shift increased. However, in the image domain, the method fails to detect NAtS samples because instead of deviating from the ID, they move intermediately between the classes. Prior works (Luo et al., 2020; Liu et al., 2017) report that the cross-entropy loss cannot guarantee a sufficient inter-class distance. In other words, representations do not need to be far from the decision boundary to lower the cross-entropy loss. In this regard, we assume that the performance degradation of the Mahalanobis detector is caused by the cross-entropy loss learning latent features that are not separable enough to detect the NAtS samples located between the classes (e.g., Figure 3 [Top]). Specifically, if some classes are located nearby in the feature space, samples moving between classes (i.e., the case of the UTKFace dataset) will not be far from the ID. Even though NAtS samples move away from one of the ID class cluster, they will gradually get closer to another ID class cluster.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.3 NA T S CATEGORIZATION\\n\\nFigure 5: ID score landscape (brighter region means higher ID score) of the existing OOD detection methods (left: MSP, middle: ODIN, right: Mahalanobis). We use a synthetic 2D dataset to train a 4-layer ResNet. The red points represent the ID samples; purple stars, gray diamonds, and orange triangles indicate samples from different NATS categories. A sample is regarded as NATS when it has a low ID score.\\n\\nConsidering the performance of confidence-based and distance-based detection methods, we now divided NATS into three categories based on two criteria: 1) whether the dataset is near the decision boundary or not; 2) whether they are close or far from the in-distribution dataset. Since a dataset (i.e., samples) far from the decision boundary and overlapping with the in-distribution dataset is not a distributionally shifted dataset, our work focuses on the remaining three cases that cover all possible scenarios of NATS. Without loss of generality, we use a classification task with three classes as a motivating example depicted by Figure 5.\\n\\n- **NATS category 1:** This category comprises of NATS samples that are located near the decision boundary, and between ID samples of different classes. Purple stars in Figure 5 represents samples from this category. Such samples are easily detected by confidence-based methods, MSP and ODIN, but harder to be detected by Mahalanobis Detector, which is a distance-based method.\\n\\n- **NATS category 2:** This category consists of NATS samples that are placed away from the decision boundary and the ID data. For example, gray diamonds in Figure 5. Mahalanobis detector regards such samples as NATS, whereas confidence-based methods fail to detect them since NATS samples have a higher prediction confidence (i.e., higher ID score) than ID samples.\\n\\n- **NATS category 3:** This category mainly comprises of NATS samples located anywhere near the decision boundary but far away from ID data. For example, orange triangles in Figure 5. Such samples are easily detected by both distance-based and confidence-based OOD detection methods.\\n\\n| Category   | Near Decision Boundary | Far from ID Data | Performance of Confidence-Based Methods | Performance of Distance-Based Methods |\\n|------------|------------------------|-----------------|----------------------------------------|--------------------------------------|\\n| Category 1 | \u2713                      | \u2717               | \u2713                                      | \u2717                                    |\\n| Category 2 | \u2717                      | \u2713               | \u2717                                      | \u2713                                    |\\n| Category 3 | \u2717                      | \u2713               | \u2717                                      | \u2713                                    |\\n\\nTable 2: Comparison between different categories of NATS based on the location of samples in the features space and performance of confidence-based and distance-based methods.\\n\\n6.1 METHOD FOR CONSISTENT NATS DETECTION PERFORMANCE\\n\\nIn this section, we suggest a modification in the training objective for deep classifiers to encourage consistent NATS detection performance on all NATS categories. Then we provide experiment results where the proposed method was compared against diverse OOD detection methods on the three NATS datasets (UTKFace, Amazon Review, RSNA Bone Age).\\n\\n6.1 METHOD\\n\\nFor a generally applicable OOD detection method to all NATS categories, we suggest a new training objective for deep classifiers comprised of classification loss ($L_{CE}$), distance loss ($L_{dist}$) and entropy loss ($L_{entropy}$). The proposed objective improves the performance of the Mahalanobis detector on NATS samples from category 1 without sacrificing performance on NATS samples from other categories. We focus on improving the distance-based OOD detection method rather than the confidence-based method since it is not always possible to enforce NATS samples to be embedded near the decision boundary, as discussed in Section 5.2.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We conduct additional experiments on a multi-class classification dataset to show that the three NAtS categories can exist in different tasks and datasets. In order to determine the NAtS category to which a shifted sample belongs, we need to train the model for a task on a dataset, evaluate the performance of existing OOD detection methods, and analyze the location of samples in feature space. However, we would have to conduct a series of experiments on numerous tasks and datasets until we encounter samples from each of the three NAtS categories. (There is no way to know the NAtS category of a given dataset until we run experiments on it.) To this end, we synthesized 2D multi-class dataset with 8 classes and conducted experiments on it.\\n\\nWe synthesized the 2-dimensional vectors for training dataset with 8 classes, and those samples are visualized as red points in the 2D plane in Figure 12. In particular, we create samples in each class by randomly sampling from a gaussian distribution with a random mean and variance. Deep neural networks are primarily useful in complex datasets with high dimensional inputs. Therefore, for training, we increase the input dimensions by adding 8-dimensional vector which are filled with the Gaussian noise which have mean of zero and variation of 0.1. The network would focus on the 2 important input features which corresponds x and y coordinates to classify the classes and ignore others.\\n\\nWe train the feed-forward classifier using the standard cross-entropy loss and our modified loss. The classifier comprises 9 linear layers, and each hidden layer consists of 10 units, and the output layer consists of 8 units (one for each class). We then evaluate the performance of MSP, ODIN, Mahalanobis under NAtS for model trained using CE loss in Figure 12 (a), (b), and (c) respectively. Purple Stars, Gray Diamonds and Orange Triangles indicate samples from NAtS categories 1, 2 and 3 respectively. We observe that MSP and ODIN detects samples from NAtS category 1 and 3 but fails to detect from NAtS category 3. Mahalanobis detects samples from NAtS category 2 (Gray Diamonds) and 3 (Orange Triangles), but fails to detect samples from NAtS category 1 (Purple Stars). We then evaluate the performance of Mahalanobis under NAtS for model trained using our proposed loss in Figure 12 (d). We observe that Mahalanobis (on model trained with our loss) effectively detects samples from all NAtS categories. This demonstrates that our proposed training objective makes the Mahalanobis detector a robust NAtS detection method for all NAtS categories.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 12: ID score landscape (brighter region means higher ID score) of the existing OOD detection methods in multi-class classification task. We use a synthetic 2D dataset with 8 distinct classes. The Red points represent the ID samples; Purple Stars, Gray Diamonds and Orange Triangles indicate samples from different NAtS categories. A sample is regarded as NAtS when it has a low ID score.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"propose a unified framework to leverage self-supervised learning for OOD detection. We set the\\n\\nWe also consider the other recent baselines for the comparison. Recently, Sehwag et al. (2021)\\n\\nutilizes the gram matrix in OOD detection (Chandramouli & Sageev, 2020).\\n\\nentropy loss with ensembled version of Mahalanobis detector (Lee et al., 2018) and the method that\\n\\nNAtS datasets. Firstly, as we mentioned in the main paper, we evaluate the model trained with cross-\\n\\nIn this section, we investigate NAtS detection performance using additional baselines, including\\n\\n| Distribution | Brightness | Baselines | Ours |\\n|--------------|------------|-----------|------|\\n| Year         | Baselines  | Ours      |\\n| 2010         | 53.6       | 53.6      | 53.6 |\\n| 2011         | 55.0       | 55.0      | 55.0 |\\n| 2012         | 56.6       | 56.6      | 56.6 |\\n| 2013         | 75.0       | 75.0      | 75.0 |\\n| 2014         | 76.5       | 76.5      | 76.5 |\\n| 2015         | 85.3       | 85.3      | 85.3 |\\n| 2016         | 93.0       | 93.0      | 93.0 |\\n| 2017         | 94.7       | 94.7      | 94.7 |\\n\\nTable 5: Classification accuracy of the in-distribution in three NAtS datasets.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"method as our baseline and denote this baseline as SSD+.\\n\\nThe experiment on SSD+ requires a data augmentation strategy. In the Image domain, we augment the training dataset as Chen et al. (2020). We construct an augmented training dataset using a back-translation model for text experiments as (Fang et al., 2020; Gunel et al., 2020). We train SSD+ for 100 epochs for UTKFace and RSNA Bone age dataset and 50 epochs for Amazon Review dataset.\\n\\nSome recent methods (Hsu et al., 2020; Techapanurak et al., 2020) exploit the cosine similarity to detect OOD samples. Hsu et al. (2020) suggest to decompose confidence scores and to modify input preprocessing method in ODIN (Liang et al., 2018). For OOD detection, they calculate the cosine similarity between the features from the penultimate layer and the class specific weights and use the maximum value as an OOD score. Techapanurak et al. (2020) also propose to compute cosine similarity between the class weight vector and the penultimate layer feature of each input to obtain OOD scores. Specifically, they train the model with cross entropy loss as standard classification model, but logit value is set by the scaled cosine similarity value. After training, the cosine similarity values between the penultimate layer feature of the input and the weight vector for each class are calculated. The maximum cosine similarity value is used as the OOD score for the samples. We name the methods as GODIN and Scaled Cosine in Table 6, respectively.\\n\\nNATS detection performance of our method and other baselines are provided in Table 6. In UTKFace dataset, GODIN, Scaled Cosine, and ensembled version of Mahalanobis detector have relatively low detection performance than the other baselines. Although Gram matrix shows reasonable detection performance in UTKFace and RSNA Bone Age NATS groups, this method is hard to applied to the Amazon Book Review dataset because the notion of gram matrix is vague in the text domain. In three NATS categories, SSD+ has the most robust detection performance among the five baselines, however, our simple remedy in training loss help Mahalanobis detector to demonstrate improved performance particularly under the UTKFace NATS dataset. In conclusion, when compared with the five more recent baselines, our method shows robust and high detection performance in all the three NATS datasets, regardless of the domains. Scaled Cosine, which compute logit based on cosine similarity, have similar tendency with confidence-based methods.\\n\\nWe report the NATS detection performance of baselines and our method based on four other metrics: Detection Accuracy, AUPR-In, AUPR-Out, and TNR@95%TPR, which are often used in the OOD detection community. We present the detection performance on NATS datasets in three domain measured by Detection Accuracy, AUPR-In, AUPR-Out, and TNR@95%TPR in Tables 7,8,9, and 10 respectively. The results indicate that our simple modification in the training objective improves the performance of the Mahalanobis detector, thus making it a robust NATS detection method for the three NATS categories presented in the paper. Specifically, in UTKFace, we observe that using our suggested training loss results in a significant increase in NATS detection performance using the Mahalanobis detector. At the same time, the suggested method effectively detects the NATS samples in the other two datasets. In summary, based on the results obtained using five different metrics (AUROC, Detection Accuracy, AUPR-In, AUPR-Out, and TNR@95%TPR), our suggested modification makes the Mahalanobis detector a general NATS detection method for all three NATS categories.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Distribution | Age | CE | Ours |\\n|---------------|-----|----|------|\\n| MYK:          | MSP | ODIN | Mahalanobis Gram matrix Energy score Mahalanobis |\\n| UTKFace       | ID  | 26- | \u2013   | \u2013   | \u2013   | \u2013   |\\n|               |     |     |     |     |     |     |\\n| Amazon Review | NAtS| 2005| \u2013   | \u2013   | \u2013   | \u2013   |\\n|               |     | 2006| 5.1 | \u00b1   | 0.6 | 5.1 |\\n|               |     | 2007| 4.8 | \u00b1   | 0.4 | 5.1 |\\n|               |     | 2008| 4.4 | \u00b1   | 0.4 | 4.9 |\\n|               |     | 2009| 5.0 | \u00b1   | 1.0 | 4.7 |\\n|               |     | 2010| 4.7 | \u00b1   | 0.5 | 4.9 |\\n|               |     | 2011| 4.4 | \u00b1   | 0.5 | 4.4 |\\n|               |     | 2012| 3.7 | \u00b1   | 0.4 | 4.7 |\\n|               |     | 2013| 3.7 | \u00b1   | 0.2 | 4.5 |\\n|               |     | 2014| 3.8 | \u00b1   | 0.5 | 5.4 |\\n| RSNA Bone Age | NAtS| 0.0 | 40.0| \u00b1   | 54.8| 100.0|\\n|               |     | 0.2 | 12.1| \u00b1   | 4.0 | 59.3 |\\n|               |     | 0.4 | 7.2 | \u00b1   | 2.7 | 13.2 |\\n|               |     | 0.6 | 5.3 | \u00b1   | 2.0 | 4.8 |\\n|               |     | 0.8 | 5.1 | \u00b1   | 1.4 | 4.2 |\\n|               | ID  | 1.0 | \u2013   | \u2013   | \u2013   | \u2013   |\\n|               |     | 1.2 | 5.6 | \u00b1   | 1.9 | 8.4 |\\n|               |     | 1.4 | 7.3 | \u00b1   | 1.5 | 18.1 |\\n|               |     | 1.6 | 10.2| \u00b1   | 3.5 | 29.3 |\\n|               |     | 1.8 | 11.5| \u00b1   | 3.5 | 41.8 |\\n|               |     | 2.0 | 11.5| \u00b1   | 3.7 | 55.2 |\\n|               |     | 2.5 | 11.5| \u00b1   | 2.1 | 77.7 |\\n|               |     | 3.0 | 13.3| \u00b1   | 3.1 | 92.1 |\\n|               |     | 3.5 | 15.5| \u00b1   | 5.0 | 97.5 |\\n|               |     | 4.0 | 16.8| \u00b1   | 6.1 | 99.3 |\\n|               |     | 4.5 | 18.9| \u00b1   | 7.8 | 99.6 |\\n\\nTable 7: NAtS detection performance on three NAtS datasets measured by TNR at TPR 95%.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Distribution | Year | CE | Ours |\\n|--------------|------|----|------|\\n| Amazon Review |      |    |      |\\n| ID | 2005 | \u2013  | \u2013  | \u2013  | \u2013  | \u2013  | \u2013  | \u2013  | \u2013  | \u2013  | \u2013  | \u2013  | \u2013  | \u2013  |\\n| NAtS | 2006 | 50.6 \u00b1 0.1 | 50.6 \u00b1 0.1 | 51.7 \u00b1 0.1 | 50.5 \u00b1 0.2 | 51.6 \u00b1 0.1 |\\n| 2007 | 50.2 \u00b1 0.1 | 50.1 \u00b1 0.2 | 54.9 \u00b1 0.1 | 50.2 \u00b1 0.1 | 55.0 \u00b1 0.1 |\\n| 2008 | 50.1 \u00b1 0.1 | 50.3 \u00b1 0.2 | 54.4 \u00b1 0.2 | 50.2 \u00b1 0.2 | 54.5 \u00b1 0.2 |\\n| 2009 | 50.4 \u00b1 0.4 | 50.4 \u00b1 0.2 | 53.6 \u00b1 0.1 | 50.3 \u00b1 0.3 | 53.6 \u00b1 0.1 |\\n| 2010 | 50.2 \u00b1 0.1 | 50.3 \u00b1 0.3 | 53.3 \u00b1 0.1 | 50.2 \u00b1 0.1 | 53.4 \u00b1 0.1 |\\n| 2011 | 50.2 \u00b1 0.1 | 50.1 \u00b1 0.1 | 54.5 \u00b1 0.1 | 50.3 \u00b1 0.2 | 54.6 \u00b1 0.1 |\\n| 2012 | 50.0 \u00b1 0.0 | 50.2 \u00b1 0.2 | 60.3 \u00b1 0.1 | 50.3 \u00b1 0.3 | 60.4 \u00b1 0.1 |\\n| 2013 | 50.0 \u00b1 0.0 | 50.2 \u00b1 0.1 | 70.3 \u00b1 0.2 | 50.4 \u00b1 0.4 | 70.4 \u00b1 0.2 |\\n| 2014 | 50.1 \u00b1 0.1 | 50.5 \u00b1 0.3 | 70.2 \u00b1 0.1 | 50.4 \u00b1 0.3 | 70.2 \u00b1 0.1 |\\n| RSNA Bone Age |      |    |      |\\n| ID | 1.0 | \u2013  | \u2013  | \u2013  | \u2013  | \u2013  | \u2013  | \u2013  |\\n| NAtS | 1.2 | 53.2 \u00b1 0.7 | 54.4 \u00b1 0.9 | 57.3 \u00b1 1.1 | 52.1 \u00b1 0.9 | 53.2 \u00b1 0.8 |\\n| 1.4 | 55.8 \u00b1 0.8 | 60.4 \u00b1 2.1 | 65.1 \u00b1 2.2 | 55.2 \u00b1 1.1 | 55.4 \u00b1 0.8 |\\n| 1.6 | 58.6 \u00b1 0.9 | 66.3 \u00b1 3.0 | 71.1 \u00b1 3.0 | 59.7 \u00b1 1.8 | 58.7 \u00b1 0.9 |\\n| 1.8 | 60.8 \u00b1 1.1 | 73.1 \u00b1 4.3 | 75.7 \u00b1 4.4 | 62.4 \u00b1 2.0 | 60.7 \u00b1 1.1 |\\n| 2.0 | 61.2 \u00b1 1.0 | 79.4 \u00b1 4.7 | 79.2 \u00b1 5.0 | 63.9 \u00b1 3.5 | 61.2 \u00b1 1.1 |\\n| 2.5 | 63.4 \u00b1 3.3 | 88.5 \u00b1 3.7 | 86.6 \u00b1 5.2 | 68.4 \u00b1 4.5 | 63.1 \u00b1 2.9 |\\n| 3.0 | 70.4 \u00b1 2.9 | 94.3 \u00b1 2.5 | 91.4 \u00b1 4.0 | 74.8 \u00b1 3.8 | 70.3 \u00b1 2.7 |\\n| 3.5 | 75.8 \u00b1 1.7 | 96.9 \u00b1 1.4 | 94.0 \u00b1 3.4 | 78.4 \u00b1 4.9 | 75.5 \u00b1 1.7 |\\n| 4.0 | 78.9 \u00b1 1.8 | 98.3 \u00b1 0.9 | 95.4 \u00b1 2.3 | 82.1 \u00b1 5.4 | 78.6 \u00b1 1.7 |\\n| 4.5 | 80.9 \u00b1 2.5 | 98.8 \u00b1 0.8 | 96.0 \u00b1 2.0 | 85.0 \u00b1 4.4 | 80.7 \u00b1 2.4 |\\n\\nTable 8: NAtS detection performance on three NAtS datasets measured by detection accuracy.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Distribution | Year | CE | Ours | MSP | ODIN | Mahalanobis Gram matrix | Energy score | Mahalanobis |\\n|--------------|------|----|------|-----|------|-------------------------|-------------|------------|\\n| Amazon Review | 2005 | \u2013  | \u2013  | \u2013  | \u2013  | \u2013                       | \u2013           | \u2013          |\\n|              | 2006 | 49.9\u00b10.2 | 49.9\u00b10.4 | 50.7\u00b10.1 | \u2013  | 49.8\u00b10.3 | 50.9\u00b10.1 |\\n|              | 2007 | 47.9\u00b10.3 | 48.4\u00b10.6 | 54.3\u00b10.1 | \u2013  | 47.9\u00b10.4 | 54.6\u00b10.1 |\\n|              | 2008 | 47.8\u00b10.3 | 48.1\u00b10.7 | 52.7\u00b10.1 | \u2013  | 47.8\u00b10.3 | 52.9\u00b10.1 |\\n|              | 2009 | 48.0\u00b10.2 | 48.5\u00b10.5 | 51.7\u00b10.1 | \u2013  | 48.1\u00b10.3 | 51.9\u00b10.1 |\\n|              | 2010 | 48.5\u00b10.2 | 49.0\u00b10.6 | 51.9\u00b10.1 | \u2013  | 48.5\u00b10.3 | 52.2\u00b10.1 |\\n|              | 2011 | 46.9\u00b10.2 | 47.7\u00b10.8 | 52.9\u00b10.1 | \u2013  | 47.0\u00b10.2 | 53.2\u00b10.1 |\\n|              | 2012 | 43.6\u00b10.4 | 45.1\u00b11.0 | 57.9\u00b10.1 | \u2013  | 43.8\u00b10.4 | 58.2\u00b10.1 |\\n|              | 2013 | 40.6\u00b10.3 | 42.4\u00b11.1 | 68.8\u00b10.1 | \u2013  | 40.8\u00b10.4 | 70.3\u00b10.1 |\\n|              | 2014 | 41.2\u00b10.3 | 42.7\u00b11.1 | 72.3\u00b10.3 | \u2013  | 41.3\u00b10.4 | 74.3\u00b10.2 |\\n\\n| Distribution | Brightness | CE | Ours | MSP | ODIN | Mahalanobis Gram matrix | Energy score | Mahalanobis |\\n|--------------|------------|----|------|-----|------|-------------------------|-------------|------------|\\n| RSNA Bone Age | 0.0 | 96.8\u00b12.1 | 99.9\u00b10.0 | 99.8\u00b10.1 | 99.7\u00b10.3 | 95.9\u00b12.1 | 99.9\u00b10.0 |\\n|              | 0.2 | 73.1\u00b14.4 | 88.8\u00b16.4 | 95.4\u00b11.3 | 43.5\u00b12.0 | 72.2\u00b15.1 | 98.5\u00b10.6 |\\n|              | 0.4 | 55.7\u00b12.7 | 64.5\u00b15.4 | 81.8\u00b14.6 | 34.4\u00b12.0 | 54.8\u00b13.5 | 92.2\u00b11.9 |\\n|              | 0.6 | 52.3\u00b11.7 | 53.3\u00b12.9 | 65\u00b16.4 | 32.8\u00b11.1 | 52.2\u00b11.6 | 77.4\u00b12.9 |\\n|              | 0.8 | 49.9\u00b11.0 | 49.1\u00b11.2 | 53.4\u00b12.9 | 32.2\u00b10.6 | 50.0\u00b10.8 | 56.6\u00b11.6 |\\n\\n| ID | 1.0 | 51.0\u00b11.8 | 54.2\u00b11.7 | 58.5\u00b12.6 | 32.8\u00b11.0 | 50.6\u00b11.6 | 54.8\u00b11.4 |\\n|    | 1.2 | 53.0\u00b13.5 | 61.5\u00b13.3 | 69.4\u00b14.1 | 34.4\u00b10.9 | 52.2\u00b13.1 | 65.8\u00b12.3 |\\n|    | 1.4 | 55.5\u00b15.5 | 69.3\u00b14.9 | 77.5\u00b14.7 | 37.0\u00b11.4 | 54.4\u00b14.7 | 77.5\u00b13.3 |\\n|    | 1.6 | 58.1\u00b16.6 | 78.3\u00b15.8 | 83.0\u00b15.7 | 38.7\u00b11.6 | 56.9\u00b15.5 | 84.8\u00b13.3 |\\n|    | 1.8 | 60.6\u00b17.0 | 86.2\u00b15.0 | 87.0\u00b15.8 | 39.8\u00b12.5 | 59.4\u00b16.4 | 89.2\u00b13.0 |\\n|    | 2.0 | 66.4\u00b17.8 | 94.9\u00b12.4 | 92.8\u00b15.1 | 43.8\u00b13.9 | 65.4\u00b18.2 | 94.8\u00b12.4 |\\n|    | 2.5 | 77.3\u00b15.4 | 98.2\u00b10.9 | 96.1\u00b12.8 | 50.5\u00b14.3 | 76.6\u00b15.9 | 97.2\u00b11.5 |\\n|    | 3.0 | 83.5\u00b12.8 | 99.2\u00b10.4 | 97.5\u00b12.0 | 55.7\u00b17.4 | 83.0\u00b12.9 | 98.3\u00b11.0 |\\n|    | 3.5 | 87.3\u00b11.8 | 99.5\u00b10.2 | 98.2\u00b11.4 | 63.5\u00b110.9 | 86.9\u00b11.5 | 98.8\u00b10.7 |\\n|    | 4.0 | 89.0\u00b12.4 | 99.6\u00b10.2 | 98.6\u00b11.1 | 69.4\u00b111.0 | 88.6\u00b12.1 | 99.0\u00b10.5 |\\n\\nTable 9: NAtS detection performance on three NAtS datasets measured by AUPR-In.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Distribution | Brightness | CE | Ours | MSP | ODIN | Mahalanobis | Gram matrix | Energy score | Mahalanobis |\\n|--------------|------------|----|------|-----|------|------------|-------------|--------------|------------|\\n| Amazon Review | 2005 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 |\\n| | 2006 | 49.8 \u00b1 0.5 | 49.9 \u00b1 0.3 | 51.4 \u00b1 0.0 | \u2013 | 49.7 \u00b1 0.3 | 51.4 \u00b1 0.1 |\\n| | 2007 | 48.9 \u00b1 0.4 | 49.4 \u00b1 0.6 | 56.0 \u00b1 0.0 | \u2013 | 48.8 \u00b1 0.7 | 56.4 \u00b1 0.1 |\\n| | 2008 | 47.9 \u00b1 0.5 | 48.4 \u00b1 0.7 | 54.4 \u00b1 0.0 | \u2013 | 48.0 \u00b1 0.5 | 54.9 \u00b1 0.1 |\\n| | 2009 | 48.7 \u00b1 0.7 | 48.9 \u00b1 0.2 | 53.7 \u00b1 0.0 | \u2013 | 48.6 \u00b1 0.6 | 54.1 \u00b1 0.0 |\\n| | 2010 | 48.2 \u00b1 0.4 | 48.9 \u00b1 0.8 | 53.6 \u00b1 0.0 | \u2013 | 48.4 \u00b1 0.7 | 54.0 \u00b1 0.1 |\\n| | 2011 | 46.9 \u00b1 0.4 | 47.3 \u00b1 0.8 | 53.9 \u00b1 0.0 | \u2013 | 47.1 \u00b1 0.6 | 54.5 \u00b1 0.0 |\\n| | 2012 | 44.8 \u00b1 0.6 | 46.8 \u00b1 1.2 | 62.2 \u00b1 0.0 | \u2013 | 45.4 \u00b1 1.2 | 63.4 \u00b1 0.1 |\\n| | 2013 | 43.6 \u00b1 0.5 | 46.0 \u00b1 1.0 | 73.9 \u00b1 0.1 | \u2013 | 44.4 \u00b1 1.3 | 75.9 \u00b1 0.1 |\\n| | 2014 | 44.0 \u00b1 0.7 | 46.6 \u00b1 1.1 | 74.4 \u00b1 0.1 | \u2013 | 44.5 \u00b1 1.2 | 76.2 \u00b1 0.1 |\\n| RSNA Bone Age | 0.0 | 83.8 \u00b1 8.7 | 99.9 \u00b1 0.0 | 99.1 \u00b1 1.1 | 99.0 \u00b1 2.0 | 80.2 \u00b1 7.2 | 99.9 \u00b1 0.0 |\\n| | 0.2 | 67.2 \u00b1 3.4 | 88.1 \u00b1 5.7 | 89.2 \u00b1 4.0 | 64.0 \u00b1 2.5 | 65.7 \u00b1 5.2 | 96.1 \u00b1 2.4 |\\n| | 0.4 | 54.7 \u00b1 1.7 | 62.2 \u00b1 3.9 | 73.1 \u00b1 5.2 | 46.4 \u00b1 4.8 | 53.9 \u00b1 1.7 | 84.9 \u00b1 2.7 |\\n| | 0.6 | 52.0 \u00b1 1.3 | 52.1 \u00b1 2.6 | 60.3 \u00b1 4.8 | 42.3 \u00b1 2.9 | 51.5 \u00b1 1.6 | 68.9 \u00b1 1.5 |\\n| | 0.8 | 50.3 \u00b1 0.9 | 48.9 \u00b1 1.5 | 52.3 \u00b1 1.4 | 40.8 \u00b1 1.6 | 50.0 \u00b1 1.1 | 54.3 \u00b1 1.3 |\\n| 1.0 | 51.6 \u00b1 1.0 | 54.7 \u00b1 1.5 | 56.5 \u00b1 1.7 | 42.5 \u00b1 2.1 | 51.6 \u00b1 1.1 | 53.9 \u00b1 2.4 |\\n| 1.2 | 55.0 \u00b1 1.5 | 64.4 \u00b1 2.9 | 65.8 \u00b1 3.5 | 47.3 \u00b1 2.4 | 55.1 \u00b1 1.4 | 62.9 \u00b1 3.8 |\\n| 1.4 | 58.9 \u00b1 2.8 | 73.2 \u00b1 3.6 | 73.8 \u00b1 4.3 | 54.0 \u00b1 2.9 | 58.9 \u00b1 2.2 | 72.4 \u00b1 4.9 |\\n| 1.6 | 61.3 \u00b1 3.2 | 81.0 \u00b1 3.9 | 79.4 \u00b1 5.5 | 58.1 \u00b1 3.2 | 60.9 \u00b1 2.2 | 78.8 \u00b1 5.4 |\\n| 1.8 | 61.9 \u00b1 3.5 | 87.2 \u00b1 3.6 | 83.2 \u00b1 6.0 | 60.1 \u00b1 4.7 | 61.3 \u00b1 2.6 | 83.6 \u00b1 5.0 |\\n| 2.0 | 63.9 \u00b1 3.5 | 94.7 \u00b1 2.2 | 89.9 \u00b1 5.6 | 66.7 \u00b1 6.1 | 63.1 \u00b1 3.0 | 91.6 \u00b1 3.6 |\\n| 2.5 | 69.9 \u00b1 3.0 | 97.9 \u00b1 1.0 | 93.8 \u00b1 4.3 | 74.6 \u00b1 5.5 | 69.1 \u00b1 2.4 | 95.0 \u00b1 2.5 |\\n| 3.0 | 73.8 \u00b1 3.2 | 98.9 \u00b1 0.5 | 95.4 \u00b1 3.8 | 79.0 \u00b1 5.9 | 72.9 \u00b1 2.0 | 96.5 \u00b1 2.1 |\\n| 3.5 | 76.6 \u00b1 4.0 | 99.3 \u00b1 0.4 | 96.5 \u00b1 3.1 | 83.7 \u00b1 6.5 | 75.4 \u00b1 2.1 | 97.2 \u00b1 1.8 |\\n| 4.0 | 78.2 \u00b1 4.7 | 99.4 \u00b1 0.4 | 97.3 \u00b1 2.4 | 86.9 \u00b1 5.9 | 76.5 \u00b1 3.5 | 97.6 \u00b1 1.6 |\\n| 4.5 | 79.9 \u00b1 4.7 | 99.4 \u00b1 0.4 | 97.3 \u00b1 2.4 | 86.9 \u00b1 5.9 | 76.5 \u00b1 3.5 | 97.6 \u00b1 1.6 |\\n\\nTable 10: NAtS detection performance on three NAtS datasets measured by AUPR-Out.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we provide the additional PCA visualization results. Figure 8 presents PCA plots applied on the penultimate features obtained from the model trained with our loss. We observe that the shifted samples in the image domain move in between the two ID classes, and the majority of the NAtS samples do not overlap with the ID clusters. This helps in effectively detecting the shifted samples as NAtS.\\n\\nFurthermore, we present the PCA visualization on a model trained with a cross-entropy loss in Figure 9. We categorize NAtS samples based on their ground-truth labels as opposed to Figure 3 in which we do not segregate NAtS samples based on their ground truth labels. In the main paper, we intentionally do not categorize (and colorize) the NAtS samples because the ground truth class labels are not available at the inference time. In an ideal situation, the class labels should be predicted with the trained model only if the samples are not classified as NAtS, otherwise, they should be rejected for human assessment. Note that the PCA visualization cannot be used to infer the real increase in the distance between two classes since PCA compresses the original feature space into 2 dimensions.\\n\\nFigure 8: PCA visualization to demonstrate the movement of NAtS samples after training models with our proposed loss. We vary the age, year and brightness in UTKFace, Amazon Review and RSNA Bone Age dataset respectively.\\n\\nFigure 9: PCA visualization to demonstrate the movement of NAtS samples as we vary the age, year and brightness in UTKFace, Amazon Review and RSNA Bone Age dataset respectively.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: NAtS detection performance on distributional shifts in three datasets measured by AU-ROC.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the length of the reviews gradually reduces. Further, Figure 7 presents the distribution of the average ratio of important words in a sequence by year. First, we train a model to classify sentiment polarity using Catboost (Prokhorenkova et al., 2017) using document term frequency vector. We then extract top-100 important words using feature importance. Lastly, we manually select important words over 100 words. We find that the distribution of ratio of important words in a sequence gradually increase over time. Based on this analysis, we figure out that the feature related with downstream task is shifted by time.\\n\\nFigure 6: The density plot of sequence length per year. The x-axis represents the length with log scaling.\\n\\nFigure 7: The density plot of the ratio of important words per sequence.\\n\\nIn this section, we describe the training details. Followed by it, we describe the algorithm we used to select the hyperparameters in our suggested modification to the loss function. Then, we provide the links to download the datasets.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We used ResNet18 (He et al., 2016) that is pretrained with ImageNet (Deng et al., 2009) to train a gender classifier using a UTKFace NATS dataset. After the average pooling layer, we trained a fully-connected network composed of 2 hidden layers which have 128 and 2 units respectively with a relu activation. The network is trained for 100 epochs with a batch size of 64. We used stochastic gradient descent with Adam optimizer (Kingma & Ba, 2014) and set learning rate as $3 \\\\times 10^{-5}$. For data augmentation, the technique of SimCLR (Chen et al., 2020) is used for all the experiments.\\n\\nWe perform experiment with Transformer network with 4 layers. The network is trained for 10 epochs with batch size of 128. We used Adam as an optimizer and set the learning rate as $3 \\\\times 10^{-6}$.\\n\\nWe use a ResNet18 (He et al., 2016) model, pretrained on ImageNet (Deng et al., 2009), and add two fully-connected layers containing 128 and 2 hidden units with a relu activation. We train the network to classify gender given the x-ray image. Each model is trained for 30 epochs using SGD optimizer with a learning rate of 0.01 and momentum of 0.9, using a batch size of 64.\\n\\nAll the experiments are conducted on a single GeForce RTX 3090 GPU with 24 GB memory. The results are measured by computing mean and standard deviation across 5 trials upon randomly chosen seeds.\\n\\nOur suggested modification to the training loss mainly comprises of distance loss ($L_{\\\\text{dist}}$) and an entropy loss ($L_{\\\\text{entropy}}$). More formally, the training loss is given by:\\n\\n$$L_{\\\\text{total}} = L_{\\\\text{CE}} + L_{\\\\text{dist}} + L_{\\\\text{entropy}}.$$  \\n\\nThe entropy loss comprises of two terms and is defined as:\\n\\n$$L_{\\\\text{entropy}} = \\\\lambda_2 \\\\frac{1}{D} \\\\sum_i \\\\text{Var}(z_i) + \\\\lambda_3 \\\\frac{1}{D^2} X_i X_j \\\\neq i C_{ij},$$\\n\\nwhere $\\\\lambda_2 > 0$ and $\\\\lambda_3 > 0$ are hyperparameters, $z \\\\in \\\\mathbb{R}^D$ is the feature representation in the latent feature space, and $C_{ij}$ is the correlation coefficient between $i$th and $j$th dimension of the feature space. For more details, please refer to Section 6.1.\\n\\nFor simplicity, in this section, we will refer the first term of entropy loss as variance loss, and the second term as correlation loss. To obtain the hyperparameters of different terms in our loss function, we explore the value of $\\\\lambda_2$ in $[0.01, 0.1, 1.0, 10.0]$ and $\\\\lambda_3$ in $[0.0001, 0.001, 0.01, 0.1, 1.0]$. The hyperparameter corresponding the distance loss, $\\\\lambda_1$, is set as 0.1, and then the hyperparameters of the variance loss and the correlation loss are chosen by a simple algorithm.\\n\\nAlgorithm: We now describe the algorithm we used to find the hyperparameters of variance and correlation loss. First, we calculate the harmonic mean of the variance loss and correlation loss using our training dataset. Next, we select the hyperparameters with the lowest value of harmonic mean. Then, to ensure that entropy loss prevents the feature space collapsing problem, we apply the singular value decomposition (SVD) in the penultimate features and test if the sum of singular values except the two largest values is improved by the entropy loss or not. Concretely, to prove the enhancement, we compare the values with those of the model trained with $L_{\\\\text{CE}} + \\\\lambda_1 L_{\\\\text{dist}}$. If the selected hyperparameters do not improve the values, we reject them and investigate the hyperparameters that have the next lowest harmonic mean. The hyperparameters satisfying the above steps are selected as the hyperparameters of our loss function.\\n\\nNote that we reject the hyperparameters if they significantly degrade the classification accuracy. In image domain, we applied the algorithm and obtained 0.1, 0.1, 0.0001 as hyperparameters of $\\\\lambda_1$, $\\\\lambda_2$, and $\\\\lambda_3$, respectively. In the medical domain, we obtain the hyperparameter corresponding to $\\\\lambda_3$ as 1.0, and the other hyperparameters are the same as those of the image domain. In the text domain, 10 and 1.0 are used for the $\\\\lambda_2$ and $\\\\lambda_3$. The distance loss is set by 0.1.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In our work, we use the openly available datasets, namely, UTKFace dataset, Amazon Review dataset, and RSNA Bone Age dataset.\\n\\nIn this section, we conduct an ablation study to investigate the effect of each proposed term in the loss function in equation 5. Table 4 presents an ablation study to find the effect of the entropy loss and distance loss in the proposed modification to training loss. In UTKFace, when training with solely attaching $L_{dist}$ or $L_{entropy}$ to $L_{CE}$, the NAtS detection performance using mahalanobis OOD detection method improves compared to training with only $L_{CE}$, but the performance does not monotonically increase in both cases along with the variation of Age. However, when both $L_{dist}$ and $L_{entropy}$ are used, there is a large improvement in performance with the results being monotonically increasing as we move from ID towards NAtS, which implies that these two losses are mutually beneficial. In the text and medical domains, although there is a little degradation of AUROC when only $L_{dist}$ is added, the performance is improved by using $L_{entropy}$ in the training objective.\\n\\nFurther, to analyze the impact of each loss term to the feature-level representations, we perform a singular value decomposition (SVD) in the penultimate layer of ResNet18 trained on the UTKFace dataset, similar to Verma et al. (2019). When trained with $L_{CE}$ only, the sum of the two largest singular values was 617.47, while the sum of the remaining singular values was 785.54. When trained with $L_{CE} + L_{dist}$, the sum of the two largest singular values increased (4481.88), while the sum of the remaining singular values decreased (371.86).\\n\\nAs discussed in Section 6.1 in the main paper, this indicates that the addition of the distance loss is likely to collapse the latent feature space (i.e. rank-deficient), thus possibly decreasing the model's sensitivity to NAtS samples. For example, if a model is trained to classify apples and bananas based only on the color, it will not be able to tell that a firetruck is a NAtS sample. When we train the classifier with $L_{entropy}$ added, the problem is effectively alleviated. The sum of the two largest singular value is reduced from 4481.88 to 2960.63, and the sum of the remaining singular values increased from 371.86 to 513.13.\\n\\nhttps://susanqq.github.io/UTKFace/\\n\\nhttps://jmcauley.ucsd.edu/data/amazon/\\n\\nhttps://www.rsna.org/education/ai-resources-and-training/\"}"}
{"id": "tsg-Lf1MYp", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Distribution Age\\n\\n| ID | 26 | 25 | 50.0 \u00b1 1.2 | 50.3 \u00b1 1.0 | 56.8 \u00b1 2.4 | 50.1 \u00b1 1.7 |\\n|----|----|----|------------|------------|------------|------------|\\n| 24 | 50.6 \u00b1 2.8 | 50.8 \u00b1 0.8 | 54.6 \u00b1 1.7 | 53.3 \u00b1 1.3 |\\n| 23 | 47.9 \u00b1 2.4 | 48.8 \u00b1 1.2 | 58.1 \u00b1 3.3 | 52.7 \u00b1 1.2 |\\n| 22 | 51.2 \u00b1 1.7 | 53.4 \u00b1 1.0 | 61.5 \u00b1 2.8 | 56.5 \u00b1 1.3 |\\n| 21 | 49.5 \u00b1 2.4 | 51.7 \u00b1 2.3 | 56.0 \u00b1 3.6 | 55.3 \u00b1 1.0 |\\n| 19-20 | 52.2 \u00b1 1.5 | 54.8 \u00b1 2.0 | 56.6 \u00b1 3.1 | 57.9 \u00b1 1.2 |\\n| 17-18 | 52.9 \u00b1 1.6 | 58.5 \u00b1 2.4 | 62.0 \u00b1 4.5 | 61.0 \u00b1 1.1 |\\n| 15-16 | 59.9 \u00b1 2.9 | 70.0 \u00b1 3.4 | 66.6 \u00b1 4.2 | 70.7 \u00b1 0.7 |\\n| 12-14 | 57.7 \u00b1 2.6 | 67.4 \u00b1 5.5 | 65.2 \u00b1 5.6 | 75.3 \u00b1 0.9 |\\n| 9-11 | 56.0 \u00b1 3.8 | 66.5 \u00b1 7.6 | 67.7 \u00b1 5.8 | 80.5 \u00b1 0.9 |\\n| 7-8 | 53.3 \u00b1 4.4 | 64.1 \u00b1 7.9 | 69.3 \u00b1 7.3 | 82.1 \u00b1 1.9 |\\n| 5-6 | 53.2 \u00b1 2.4 | 63.0 \u00b1 8.0 | 64.3 \u00b1 5.1 | 83.5 \u00b1 1.4 |\\n| 3-4 | 56.4 \u00b1 3.2 | 64.3 \u00b1 10.1 | 65.8 \u00b1 5.1 | 88.6 \u00b1 1.0 |\\n| 2 | 53.3 \u00b1 2.8 | 58.6 \u00b1 10.1 | 57.7 \u00b1 4.2 | 88.3 \u00b1 1.6 |\\n| 1 | 51.2 \u00b1 4.4 | 58.1 \u00b1 10.2 | 55.6 \u00b1 5.7 | 90.3 \u00b1 2.0 |\\n\\n### Distribution Year\\n\\n| ID | 2006 | 2007 | 2008 | 2009 | 2010 | 2011 | 2012 | 2013 | 2014 |\\n|----|------|------|------|------|------|------|------|------|------|\\n| 2005 | 51.5 \u00b1 0.1 | 56.8 \u00b1 0.2 | 55.2 \u00b1 0.3 | 53.7 \u00b1 0.2 | 54.0 \u00b1 0.5 | 55.6 \u00b1 0.7 | 63.3 \u00b1 0.8 | 75.5 \u00b1 0.5 | 76.8 \u00b1 0.2 |\\n| 2006 | 51.5 \u00b1 0.1 | 51.5 \u00b1 0.1 | 51.4 \u00b1 0.1 | 51.4 \u00b1 0.1 | 51.4 \u00b1 0.1 | 51.4 \u00b1 0.1 | 51.4 \u00b1 0.1 | 51.4 \u00b1 0.1 | 51.4 \u00b1 0.1 |\\n| 2007 | 56.3 \u00b1 0.1 | 56.6 \u00b1 0.1 | 56.5 \u00b1 0.1 | 56.5 \u00b1 0.1 | 56.5 \u00b1 0.1 | 56.5 \u00b1 0.0 | 62.6 \u00b1 0.1 | 75.1 \u00b1 0.1 | 76.7 \u00b1 0.1 |\\n| 2008 | 54.8 \u00b1 0.1 | 54.9 \u00b1 0.1 | 54.9 \u00b1 0.1 | 54.9 \u00b1 0.1 | 54.9 \u00b1 0.1 | 54.9 \u00b1 0.0 | 62.6 \u00b1 0.1 | 75.1 \u00b1 0.1 | 76.7 \u00b1 0.1 |\\n| 2009 | 53.6 \u00b1 0.1 | 53.6 \u00b1 0.1 | 53.6 \u00b1 0.1 | 53.6 \u00b1 0.1 | 53.6 \u00b1 0.1 | 53.6 \u00b1 0.0 | 62.6 \u00b1 0.1 | 75.1 \u00b1 0.1 | 76.7 \u00b1 0.1 |\\n| 2010 | 53.6 \u00b1 0.1 | 53.6 \u00b1 0.1 | 53.6 \u00b1 0.1 | 53.6 \u00b1 0.1 | 53.6 \u00b1 0.1 | 53.6 \u00b1 0.0 | 62.6 \u00b1 0.1 | 75.1 \u00b1 0.1 | 76.7 \u00b1 0.1 |\\n| 2011 | 54.8 \u00b1 0.1 | 54.9 \u00b1 0.0 | 54.9 \u00b1 0.0 | 54.9 \u00b1 0.0 | 54.9 \u00b1 0.0 | 54.9 \u00b1 0.0 | 62.6 \u00b1 0.1 | 75.1 \u00b1 0.1 | 76.7 \u00b1 0.1 |\\n| 2012 | 62.5 \u00b1 0.1 | 62.6 \u00b1 0.1 | 62.6 \u00b1 0.1 | 62.6 \u00b1 0.1 | 62.6 \u00b1 0.1 | 62.6 \u00b1 0.1 | 62.6 \u00b1 0.1 | 75.1 \u00b1 0.1 | 76.7 \u00b1 0.1 |\\n| 2013 | 74.7 \u00b1 0.1 | 75.1 \u00b1 0.1 | 75.1 \u00b1 0.1 | 75.1 \u00b1 0.1 | 75.1 \u00b1 0.0 | 75.1 \u00b1 0.0 | 75.1 \u00b1 0.1 | 75.1 \u00b1 0.1 | 75.1 \u00b1 0.1 |\\n| 2014 | 76.3 \u00b1 0.1 | 76.8 \u00b1 0.1 | 76.8 \u00b1 0.1 | 76.8 \u00b1 0.1 | 76.8 \u00b1 0.1 | 76.7 \u00b1 0.1 | 76.7 \u00b1 0.1 | 76.7 \u00b1 0.1 | 76.7 \u00b1 0.1 |\\n\\n### Distribution Brightness\\n\\n| ID | 0.0 | 0.2 | 0.4 | 0.6 | 0.8 | 1.0 | 1.2 | 1.4 | 1.6 | 1.8 | 2.0 | 2.5 | 3.0 | 3.5 | 4.0 | 4.5 | 5.0 | 5.5 | 6.0 | 6.5 | 7.0 | 7.5 | 8.0 | 8.5 | 9.0 | 9.5 | 10.0 |\\n|----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|\\n| 0.0 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 100.0 \u00b1 0.0 | 100.0 \u00b1 0.0 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99.9 \u00b1 0.2 | 99"}
{"id": "tsg-Lf1MYp", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The proposed training loss is defined as:\\n\\n$$L_{total} = L_{CE} + L_{dist} + L_{entropy}.$$  \\\\(1\\\\)\\n\\nNote that for classification loss, we use the standard cross-entropy loss, but other losses such as focal loss can also be used. The distance loss is used to increase the distance between distinct class distributions of ID samples so that NAtS samples have larger space to move around without overlapping with ID clusters, especially in NAtS category 1:\\n\\n$$L_{dist} = \\\\lambda_1 \\\\frac{1}{K} \\\\sum_{l \\\\neq k} \\\\|\\\\mu_l - \\\\mu_k\\\\|_2 \\\\sqrt{D},$$  \\\\(2\\\\)\\n\\nwhere $\\\\lambda_1 < 0$ is a hyperparameter, $K$ is the number of target classes, $D$ is the dimension of feature representation in the latent space (typically the penultimate layer), and $\\\\mu_l$ is the mean vector of the features of samples with label $l$. Since the value of the vector norm increases as the dimension of the feature space increases, we normalize the distance by the square root of the feature dimension.\\n\\nAs will be discussed in Section D of Appendix, we discovered after some initial experiments that the distance loss often made the model use a very limited number of latent dimensions to increase the distance between class mean vectors, which degraded the NAtS detection performance. In other words, adding only $L_{dist}$ to $L_{CE}$ caused the latent feature space to collapse into a very small number of dimensions (i.e. rank-deficient), which caused all NAtS samples to be embedded near the ID samples. Therefore, we add entropy loss to increase the number of features used to represent samples.\\n\\nThe entropy loss is defined as:\\n\\n$$L_{entropy} = \\\\lambda_2 D \\\\sum_{i} \\\\text{Var}(z_i) + \\\\lambda_3 \\\\frac{1}{D^2} \\\\sum_{i \\\\neq j} C_{ij},$$  \\\\(3\\\\)\\n\\nwhere $\\\\lambda_2 > 0$ and $\\\\lambda_3 > 0$ are hyperparameters, $z \\\\in \\\\mathbb{R}^D$ is the feature representation in the latent feature space, $\\\\text{Var}(\\\\cdot)$ is variance, and $C_{ij}$ is the correlation coefficient between $i$th and $j$th dimension of the feature space. Specifically, $C_{ij}$ is described as:\\n\\n$$C_{ij} = \\\\frac{\\\\text{Cov}(z_i, z_j)}{\\\\sigma(z_i) \\\\sigma(z_j)},$$  \\\\(4\\\\)\\n\\nwhere $\\\\text{Cov}(\\\\cdot)$ and $\\\\sigma(\\\\cdot)$ are covariance and the standard deviation, respectively.\\n\\nIntuitively, the first term in equation 3 encourages each latent dimension to have diverse values, therefore preventing the latent feature space from collapsing into a confined space. With the first term alone, however, all latent dimensions might learn correlated information, thus making the latent space rank-deficient. Therefore, we use the second term in equation 3 to minimize the correlation between different latent dimensions. Note that minimizing the feature correlation was also used in previous works under different contexts such as self-supervised learning (Zbontar et al., 2021).\\n\\n### 6.2 Results and Discussion\\n\\nTo demonstrate the effectiveness of the suggested method, we train all classifiers using the standard cross-entropy loss and our modified loss and compare post-hoc OOD detection methods across three NAtS datasets. Specifically, we present the results of the confidence-based (i.e., MSP and ODIN) and the distance-based methods (i.e., Mahalanobis distance). We also include a recently proposed OOD detection method (Chandramouli & Sageev, 2020) which computes channel-wise correlations in CNN with the gram matrix and estimates the deviation of the test samples from the training samples to detect the OOD samples.\\n\\nAlthough this method uses the distance in the channel correlation space, we expect it to behave more similarly to the Mahalanobis detector than confidence-based methods, namely MSP and ODIN. We also compare with another recent baseline which exploits the energy score to detect OOD samples (Liu et al., 2020). As the method leverage the logit layer to calculate the energy score, and the softmax score is based on the logit values, we conjecture that the energy score demonstrates detection ability similar to the confidence-based methods.\\n\\nHowever, in the text domain, the notion of gram matrix is vague, and hence we do not compare against this baseline.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"values for layers in Section E of the Appendix. We describe the details of experiment setups and selecting the penultimate layer feature space. We also provide results of the ensemble version that utilize all and Gram matrix since our method is developed based on the analysis of the NAtS samples in the\\n\\nFor a fair comparison, we use only the penultimate layer for evaluating the Mahalanobis detector\\n\\nTable 3: NAtS detection performance on distributional shifts in three datasets measured by AUROC.\\n\\n| Dataset       | In-distribution | Energy score | Mahalanobis | Gram matrix | Mahalanobis |\\n|---------------|-----------------|--------------|-------------|-------------|-------------|\\n| UTKFace       | 99.7 \u00b1 3.5      | 92.2 \u00b1 4.9   | 90.3 \u00b1 1.3  | 88.1 \u00b1 1.8  | 85.5 \u00b1 1.2  |\\n| Amazon Review | 97.7 \u00b1 0.3      | 89.1 \u00b1 2.6   | 88.3 \u00b1 1.8  | 88.6 \u00b1 1.5  | 87.0 \u00b1 0.9  |\\n| Distribution Brightness | 95.0 \u00b1 1.6 | 87.1 \u00b1 4.3   | 86.0 \u00b1 2.1  | 85.1 \u00b1 1.7  | 82.1 \u00b1 3.4  |\\n\\nID 1.0 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 | ID 26 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2014 \u2013 \u2013 \u2013 \u2013 | ID 2013 \u2013 \u2013 \u2013 \u2013 | ID 2011 \u2013 \u2013 \u2013 \u2013 | ID 2010 \u2013 \u2013 \u2013 \u2013 | ID 2008 \u2013 \u2013 \u2013 \u2013 | ID 2006 \u2013 \u2013 \u2013 \u2013 | ID 2005 \u2013 \u2013 \u2013 \u2013 | ID "}
{"id": "tsg-Lf1MYp", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the logit values, which are more tempered than the softmax score, it shows NAtS detection performance similar to that of MSP, which is a confidence-based method. These results show that similar to MSP, ODIN, and Mahalanobis, Gram matrix and Energy-based methods also have performance inconsistency on NAtS datasets. We report the additional NAtS detection performance on four other metrics, which are often used in the OOD detection community in Section F of Appendix.\\n\\nAlso, it is readily visible that our proposed training objective makes the Mahalanobis detector a robust NAtS detection method for all NAtS categories. In UTKFace, we can see the dramatic NAtS detection performance increase for Mahalanobis detector. And for the other two datasets, the proposed loss does not decrease the NAtS detection performance of Mahalanobis detector. Note that ODIN is more sensitive to OOD samples than Mahalanobis detector for some brightness levels in the RSNA Bone Age dataset, but it shows inconsistent performance across all three datasets.\\n\\nExtensive evaluation on distribution shifts. Recent works have leveraged the distribution shifts for OOD detection and uncertainty estimation. Rabanser et al. (2019) primarily focus on detecting the entire shifted distribution, not on detecting a single shifted sample. Yaniv et al. (2019) quantify the predictive uncertainty and investigate the quality of the calibration under the dataset shift. Engkarat & Takayuki (2019) aim to predict the classification accuracy of a shifted distribution by utilizing the average OOD score of the distribution. Hsu et al. (2020) also consider the distribution shift with preserved label space. However, Hsu et al. (2020) focus on the shifted distribution from different domain (e.g., real images vs. sketches). To the best of our knowledge, none of these works conduct an in-depth analysis on the performance of existing OOD detection methods on samples shifted based on natural attributes.\\n\\nODD detection methods. Post-hoc OOD detection methods (Hendrycks & Gimpel, 2017; Liang et al., 2018; Lee et al., 2018; Chandramouli & Sageev, 2020) that utilize the classification models to obtain the OOD scores have achieved remarkable performance. All of these works aim to detect shifted sample which might affect decision-making system in terms of confidence and hidden representation. Other OOD detection approach is to train the generative model (Ren et al., 2019; Choi et al., 2018; Mahmood et al., 2020) on the training distribution and estimate the density of OOD samples in test time. While these approaches are viable, it is not directly related with the downstream task but aims to detect features which is different from training distribution. In this paper, we mainly focus on methods that utilize the classification models to detect OOD samples since we are mainly interested in samples that affect decision-making systems.\\n\\nModel uncertainty. A number of previous works measure model uncertainty using various methods such as Bayesian neural networks (Blundell et al., 2015), Monte Carlo dropout (Gal & Ghahramani, 2016), and deep ensembles (Lakshminarayanan et al., 2017). Note that technically, model uncertainty can be used to detect NAtS samples, especially for NAtS categories 1 and 3, since sampling model weights from the function space can be seen as redrawing the decision boundary, and NAtS samples in categories 1 and 3 will be affected heavily by this process. Model uncertainty, however, aims to capture the uncertainty in the model weights rather than detecting OOD samples, making the two rather independent research directions.\\n\\nTo enhance the reliability of decision-making systems, we define a new task, Natural Attribute-based Shift (NAtS) detection, that aims to detect the samples shifted by a natural attribute. We introduce NAtS detection benchmark datasets by adjusting the natural attributes present in the existing datasets. Through extensive evaluation of existing OOD detection methods on NAtS datasets, we observe inconsistent performance depending on the nature of NAtS samples. Then, we analyze the inconsistency by probing the relationship between the location of NAtS samples and the performance of existing OOD detection methods. Based on this observation, we suggest a simple remedy to help Mahalanobis OOD detection method to have consistent performance across all NAtS categories. We hope our dataset and task inspire fellow researchers to investigate practical methods for identifying NAtS, which is crucial for deploying the prediction models in real-world systems.\"}"}
{"id": "tsg-Lf1MYp", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We believe this is not applicable to us.\\n\\nWe describe the implementation and training details along with the details on computation resources in Section C of the Appendix. We submit our source codes along with the instructions to reproduce our results in the Appendix.\\n\\nREFERENCES\\n\\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in AI safety. CoRR, abs/1606.06565, 2016.\\n\\nCharles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International Conference on Machine Learning, pp. 1613\u20131622. PMLR, 2015.\\n\\nMariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, and Karol Zieba. End to end learning for self-driving cars, 2016.\\n\\nFabio M Carlucci, Antonio D\u2019Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2229\u20132238, 2019.\\n\\nSastry Shama Chandramouli and Oore Sageev. Detecting out-of-distribution examples with gram matrices. In Proc. the International Conference on Machine Learning (ICML), 2020.\\n\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597\u20131607. PMLR, 2020.\\n\\nHyunsun Choi, Eric Jang, and Alexander A Alemi. Waic, but why? generative ensembles for robust anomaly detection. arXiv preprint arXiv:1810.01392, 2018.\\n\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848.\\n\\nTechapanurak Engkarat and Okatani Takayuki. Practical evaluation of out-of-distribution detection methods for image classification. 2019. URL https://openreview.net/forum?id=kXwdjtmMbUr.\\n\\nAndre Esteva, Brett Kuprel, Roberto Novoa, Justin Ko, Swetter Susan, Helen Balu, M, and Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks. 2016.\\n\\nHongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan Ding, and Pengtao Xie. Cert: Contrastive self-supervised learning for language understanding. arXiv preprint arXiv:2005.12766, 2020.\\n\\nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Proc. the International Conference on Machine Learning (ICML), 2016.\\n\\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International Conference on Learning Representations, 2020.\\n\\nBeliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoyanov. Supervised contrastive learning for pre-trained language model fine-tuning. arXiv preprint arXiv:2011.01403, 2020.\\n\\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. ArXiv, abs/1706.04599, 2017.\"}"}
