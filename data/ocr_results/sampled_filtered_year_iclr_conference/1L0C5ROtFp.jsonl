{"id": "1L0C5ROtFp", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: Reconstructions produced by the de-rendering module. Our model correctly marks each object in the scene and achieves satisfactory reconstruction. The improvement is less visible for larger numbers of keypoints, since 3D visual details could be encoded via keypoints position, hence coefficients become less relevant. Visualizations are shown in Fig. 11.\\n\\nTable 8: Impact of the number of keypoints and the presence of additional appearance coefficient in the de-rendering module for pure image reconstruction (no dynamic model). We report PSNR (dB) and MSE on the image gradient. $N$ is the maximum number of the objects in the scene. The coefficients significantly improve the reconstruction on low number of keypoints. This table is related to table 3 in the main paper, which measures this impact on the full pipeline.\\n\\nB.2 Navigating the Latent Coefficient Manifold\\n\\nWe evaluate the influence of the additional appearance coefficients on our de-rendering model by navigating its manifold. To do so, we sample a random pair $(X_{\\\\text{source}}, X_{\\\\text{target}})$ from an experiment in BlocktowerCF and compute the corresponding source features and target keypoints and co-efficients. Then, we vary each component of the target keypoints and coefficients and observe the reconstructed image (fig. 12). We observed that the keypoints accurately control the position of the cube along both spatial axes. The rendering module does infer some hints on 3D shape information from the vertical position of the cube, exploiting a shortcut in learning. On the other hand, while not being supervised, the coefficients naturally learn to encode different orientations in space and distance from the camera. Interestingly, a form of disentanglement emerges. For example, coefficients $n^{\\\\circ}_{1}$ and $2$ control rotation around the z-axis, and coefficient $n^{\\\\circ}_{4}$ models rotation around the y-axis. The last coefficient represents both the size of the cube and its presence in the image.\"}"}
{"id": "1L0C5ROtFp", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12: Navigating the manifold of the latent coefficient representation. Each line corresponds to variations of one keypoint coordinate or coefficient and shows the effect on a single cube.\\n\\n**Comparison with the Transporter Baseline**\\n\\nAs described in section 4.1, the Transporter (Kulkarni et al., 2019) is a keypoint detection model somewhat close to our de-rendering module. It leverages the transport equation to compute a reconstruction vector:\\n\\n$$\\\\hat{\\\\Psi}_{\\\\text{target}} = F_{\\\\text{source}} \\\\times (1 - K_{\\\\text{source}}) \\\\times (1 - K_{\\\\text{target}}) + F_{\\\\text{target}} \\\\times K_{\\\\text{target}}.$$  (9)\\n\\nThis equation allows to transmit information from the input by two means: the 2D position of the keypoints ($K_{\\\\text{target}}$) and the dense visual features of the target ($F_{\\\\text{target}}$). In comparison, our de-rendering solely relies on the keypoints from the target image and does not require a dense vector to be computed on the target to reconstruct the target. This makes the Transporter incomparable with our de-rendering module. We nevertheless compare the performances of the two models in Table 13, and provide visual examples in Fig. 13. Even though the two models are not comparable, as the Transporter uses additional information, our model still outperforms the Transporter for small numbers of keypoints. Interestingly, for higher numbers of keypoints, Transporter tends to discover keypoints far from the object. We investigate this behavior in the following section, and show that this is actually a critical problem for learning causal reasoning on the discovered keypoints.\\n\\n**Analysis of Transporter's Behavior**\\n\\nThe original version of the V-CDN model (Li et al., 2020) is based on Transporter (Kulkarni et al., 2019). We have already highlighted the fact that this model is not comparable with our task, as it requires not only the target keypoints ($K_{\\\\text{target}}$) but also a dense feature map ($F_{\\\\text{target}}$), whose dynamics can hardly be learned due to its high dimensionality. More precisely, the transport equation (Eq. 9) allows to pass information from the target by two means: the 2D position of the keypoints ($K_{\\\\text{target}}$) and the dense feature map of the target ($F_{\\\\text{target}}$). The number of keypoints therefore becomes a highly sensible parameter, as the transporter can decide to preferably transfer information through the target features rather than through the keypoint locations. When the number of keypoints is low, they act as a bottleneck, and the model has to carefully discover them to reconstruct the image.\"}"}
{"id": "1L0C5ROtFp", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: PSNR (dB) on the task of reconstructing target from the source (both randomly sampled from Filtered-CoPhy), using 5 coefficients per keypoint. We vary the number of keypoints in both our model and the Transporter. Note that Transporter uses target features to reconstruct the image, hence it is not comparable with our model.\\n\\n| Ground Truth | Ours (4k) | Ours (8k) | Ours (16k) | Learned (4k) | Transporter (4k) | Transporter (8k) | Transporter (16k) |\\n|-------------|----------|-----------|------------|--------------|------------------|------------------|------------------|\\n| Ground Truth | Ours (4k) | Ours (8k) | Ours (16k) | Learned (4k) | Transporter (4k) | Transporter (8k) | Transporter (16k) |\\n\\nOn the other hand, when we increase the number of keypoints, Transporter stops tracking objects in the scene and transfers visual information through the dense feature map, making the predicted keypoints unnecessary for image reconstruction, and therefore not representative of the dynamics.\\n\\nTo illustrate our hypothesis, we set up the following experiment. Starting from a trained Transporter model, we fixed the source image to be $X_0$ (the first frame from the trajectory) during the evaluation step. Then, we compute features and keypoints on the target frame $X_t$ regularly sampled in time. We reconstruct the target image using the transport equation, but without updating the target keypoints.\\n\\nPractically, this consists in computing $\\\\hat{\\\\Psi}_{\\\\text{target}}$ with Eq. (9) substituting $K_{\\\\text{target}}$ for $K_{\\\\text{source}}$. Results are shown in Fig. 14. There is no dynamic forecasting involved in this figure, and the Transporter we used was trained in a regular way, we only change the transport equation on evaluation time. Even though the keypoint positions have been fixed, the Transporter manages to reconstruct a significant part of the images, which indicates that a part of the dynamics has been encoded in the dense feature map.\\n\\nIn contrast, this issue does not arise from our de-rendering module, since our decoder solely relies on the target keypoints to reconstruct the image. Note that this is absolutely not contradictory with the claim in Li et al. (2020), since they do not evaluate V-CDN in pixel space. A rational choice of the number of keypoints leads to satisfactory performance, allowing V-CDN to accurately forecast the trajectory in keypoints space, and retrieve the hidden confounders on their dataset.\\n\\nC.3 TEMPORAL INCONSISTENCY ISSUES\\n\\nIncreasing the number of keypoints of the Transporter may lead to temporal inconsistency during the long-range reconstruction. For example, a keypoint that tracks the edge of a cube in the first...\"}"}
{"id": "1L0C5ROtFp", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 14: We evaluate the Transporter with a varying number of keypoints to reconstruct images regularly sampled in a trajectory while having the target keypoints fixed. Even if the keypoints are not moving, the Transporter still manages to reconstruct a significant part of the image, which indicates that the keypoints are not fully responsible for encoding the dynamics of the scene.\\n\\nOur de-rendering directly addresses this through the usage of additional appearance coefficients, which allows us to limit the number of keypoints to the number of objects in the scene, effectively alleviating the consistency issue. Fig. 15 illustrates this phenomenon by plotting the discovered keypoint locations forward in time, as well as the 2D location of the center of mass of each object. Note that the Transporter suffers from the temporal inconsistency issue with numbers of keypoints as low as 4 (green cube). In contrast, our model manages to solve the problem and accurately tracks the centers of mass, even though they were never supervised.\\n\\n### DETAILS OF MODEL ARCHITECTURES\\n\\n#### D.1 RENDERING MODULE\\n\\nWe call a \\\"block\\\" a 2D convolutional layer followed by a 2D batch norm layer and ReLU activation. The exact architecture of each part of the encoder is described in Table 10a. The decoder hyperparameters are described in Table 10b.\\n\\n**Dense feature map estimator**\\n\\n- We compute the feature vector from $X_{source}$ by applying a convolutional network $F$ on the output of the common CNN of the encoder. This produces the source feature vector $F_{source}$ of shape $(\\\\text{batch}, 16, 28, 28)$.\\n\\n**Keypoin detector**\\n\\n- The convolutional network $K$ outputs a set of 2D heatmaps of shape $(\\\\text{batch}, K, 28, 28)$, where $K$ is the desired number of keypoints. We apply a spatial softmax function on the two last dimensions, then we extract a pair of coordinates on each heatmap by looking for the location of the maximum, which gives us $K_{target}$ of shape $(\\\\text{batch}, K, 2)$.  \\n\"}"}
{"id": "1L0C5ROtFp", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 17: We evaluate our method on a real-world dataset Blocktower IRL. After fine-tuning, CoDy manages to accurately forecast future frames from real videos.\\n\\nWe refer to this dataset as Blocktower IRL. It is composed of 516 videos of wooden blocks stacked in a stable or unstable manner. The amount of cubes in a tower varies from 2 to 4. We aim to predict the dynamics of the tower in pixel space. This is highly related with our task BlocktowerCF (which inspired by the seminal work from (Lerer et al., 2016)) with three main differences: (1) the dataset shows real cube towers, (2) the problem is not counterfactual, i.e. every cube has the same mass and (3) the dataset contains only few videos.\\n\\nTo cope with the lack of data, we exploit our pre-trained models on BlocktowerCF and fine-tune on Blocktower IRL. The adaptation of the de-rendering module is straightforward: we choose the 4 keypoints-5 coefficients configuration and train the module for image reconstruction after loading the weights from previous training on our simulated task. CoDy, on the other hand, requires careful tuning to preserve the learned regularities from BlocktowerCF and prevent over-fitting. Since Blocktower IRL is not counterfactual, we de-activate the confounder estimator and set $u_k$ to vectors of ones. We also freeze the weights of the last layers of the MLPs in the dynamic model.\\n\\nTo the best of our knowledge, we are the first to use this dataset for video prediction. (Lerer et al., 2016) and Wu et al. (2017) leverage the video for stability prediction but actual trajectory forecasting was not the main objective. To quantitatively evaluate our method, we predict 20 frames in the future from a single image sampled in the trajectory. We measured an average PSNR of 26.27 dB, which is of the same order of magnitude compared to the results obtained in simulation. Figure 17 provides visual example of the output.\"}"}
{"id": "1L0C5ROtFp", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 18: Qualitative performance on the BlocktowerCF (BT-CF) benchmark.\"}"}
{"id": "1L0C5ROtFp", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 19: Qualitative performance on the BallsCF (B-CF) benchmark.\"}"}
{"id": "1L0C5ROtFp", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 20: Qualitative performance on the CollisionCF (C-CF) benchmark.\"}"}
{"id": "1L0C5ROtFp", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Visualization of the counterfactual video prediction quality, comparing our proposed model (Filtered CoPhy) with the two baselines, PhyDNet and UV-CDN, over different timestamps.\\n\\nTable 4: Impact of the dynamic CoDy encoder against the baseline operating in the keypoint + coefficient space. We report MSE \u00d7 10\u207b\u00b3 on prediction of keypoints + coefficients (4 pts).\\n\\nTable 5: Learning the filter bank $H$ from scratch has a mild negative effect on the reconstruction task. We report the PSNR on static reconstruction performance without the dynamic model.\\n\\n6 Conclusion\\n\\nWe introduced a new benchmark for counterfactual reasoning in physical processes requiring to perform video prediction, i.e., predicting raw pixel observations over a long horizon. The benchmark has been carefully designed and generated imposing constraints on identifiability and counterfactuality. We also propose a new method for counterfactual reasoning, which is based on a hybrid latent representation combining 2D keypoints and additional latent vectors encoding appearance and shape. We introduce an unsupervised learning algorithm for this representation, which does not require any supervision on confounders or other object properties and processes raw video. Counterfactual prediction of video frames remains a challenging task, and Filtered CoPhy still exhibits failures in maintaining rigid structures of objects over long prediction time-scales. We hope that our benchmark will inspire further breakthroughs in this domain.\"}"}
{"id": "1L0C5ROtFp", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alexander Balke and Judea Pearl. Counterfactual probabilities: Computational methods, bounds and applications. In UAI, 1994.\\n\\nFabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, and Christian Wolf. Cophy: Counterfactual learning of physical dynamics. In International Conference on Learning Representations, 2020.\\n\\nPeter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and Koray kavukcuoglu. Interaction networks for learning about objects, relations and physics. In Proceedings of the 30th International Conference on Neural Information Processing Systems, 2016.\\n\\nKeni Bernardin and Rainer Stiefelhagen. Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics. EURASIP Journal on Image and Video Processing, 2008.\\n\\nKyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing, 2014.\\n\\nEmmanuel de Bezenac, Arthur Pajot, and Patrick Gallinari. Deep learning for physical processes: Incorporating prior scientific knowledge. In International Conference on Learning Representations, 2018.\\n\\nEmily L. Denton and Vighnesh Birodkar. Unsupervised learning of disentangled representations from video. ArXiv, abs/1705.10915, 2017.\\n\\nChelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. In Proceedings of the 30th International Conference on Neural Information Processing Systems, 2016.\\n\\nCatalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014.\\n\\nSteeven Janny, Madiha Nadri Vincent Andrieu, and Christian Wolf. Deep kkl: Data-driven output prediction for non-linear systems. In Conference on Decision and Control (CDC21), 2021.\\n\\nTejas D. Kulkarni, Ankush Gupta, Catalin Ionescu, Sebastian Borgeaud, Malcolm Reynolds, Andrew Zisserman, and Volodymyr Mnih. Unsupervised learning of object keypoints for perception and control. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, 2019.\\n\\nY. Kwon and M. Park. Predicting future frames using retrospective cycle gan. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1811\u20131820, 2019.\\n\\nVincent Le Guen and Nicolas Thome. Disentangling physical dynamics from unknown factors for unsupervised video prediction. In Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\nAdam Lerer, Sam Gross, and Rob Fergus. Learning physical intuition of block towers by example. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48. JMLR.org, 2016.\\n\\nYunzhu Li, Antonio Torralba, Anima Anandkumar, Dieter Fox, and Animesh Garg. Causal discovery in physical systems from videos. Advances in Neural Information Processing Systems, 33, 2020.\\n\\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, G. Heigold, Jakob Uszkoreit, A. Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. ArXiv, abs/2006.15055, 2020.\\n\\nDavid G. Lowe. Object recognition from local scale-invariant features. In ICCV, 1999.\"}"}
{"id": "1L0C5ROtFp", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "1L0C5ROtFp", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "1L0C5ROtFp", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ABSTRACT\\n\\nLearning causal relationships in high-dimensional data (images, videos) is a hard task, as they are often defined on low-dimensional manifolds and must be extracted from complex signals dominated by appearance, lighting, textures and also spurious correlations in the data. We present a method for learning counterfactual reasoning of physical processes in pixel space, which requires the prediction of the impact of interventions on initial conditions. Going beyond the identification of structural relationships, we deal with the challenging problem of forecasting raw video over long horizons. Our method does not require the knowledge or supervision of any ground truth positions or other object or scene properties. Our model learns and acts on a suitable hybrid latent representation based on a combination of dense features, sets of 2D keypoints and an additional latent vector per keypoint. We show that this better captures the dynamics of physical processes than purely dense or sparse representations. We introduce a new challenging and carefully designed counterfactual benchmark for predictions in pixel space and outperform strong baselines in physics-inspired ML and video prediction.\\n\\nINTRODUCTION\\n\\nReasoning on complex, multi-modal and high-dimensional data is a natural ability of humans and other intelligent agents (Martin-Ordas et al., 2008), and one of the most important and difficult challenges of AI. While machine learning is well suited for capturing regularities in high-dimensional signals, in particular by using high-capacity deep networks, some applications also require an accurate modeling of causal relationships. This is particularly relevant in physics, where causation is considered as a fundamental axiom. In the context of machine learning, correctly capturing or modeling causal relationships can also lead to more robust predictions, in particular better generalization to out-of-distribution samples, indicating that a model has overcome the exploitation of biases and shortcuts in the training data. In recent literature on physics-inspired machine learning, causality has often been forced through the addition of prior knowledge about the physical laws that govern the studied phenomena, e.g. (Yin et al., 2021). A similar idea lies behind structured causal models, widely used in the causal inference community, where domain experts model these relationships directly in a graphical notation. This particular line of work allows to perform predictions beyond statistical forecasting, for instance by predicting unobserved counterfactuals, the impact of unobserved interventions (Balke & Pearl, 1994) \u2014 \\\"What alternative outcome would have happened, if the observed event X had been replaced with an event Y (after an intervention)\\\". Counterfactuals are interesting, as causality intervenes through the effective modification of an outcome. As an example, taken from (Sch\u00f6lkopf et al., 2021), an agent can identify the direction of a causal relationship between an umbrella and rain from the fact that removing an umbrella will not affect the weather.\"}"}
{"id": "1L0C5ROtFp", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"was restricted to discrete outcomes, as in CLEVRER (Yi et al., 2020), or to the prediction of 3D trajectories, as in CoPhy (Baradel et al., 2020), which also requires supervision of object positions.\\n\\nIn this work, we address the hard problem of predicting the alternative (counterfactual) outcomes of physical processes in pixel space, i.e. we forecast sequences of 2D projective views of the 3D scene, requiring the prediction over long horizons (150 frames corresponding to $\\\\sim 6$ seconds). We conjecture that causal relationships can be modeled on a low dimensional manifold of the data, and propose a suitable latent representation for the causal model, in particular for the estimation of the confounders and the dynamic model itself. Similar to V-CDN (Kulkarni et al., 2019; Li et al., 2020), our latent representation is based on the unsupervised discovery of keypoints, complemented by additional information in our case. Indeed, while keypoint-based representations can easily be encoded from visual input, as stable mappings from images to points arise naturally, we claim that they are not the most suitable representation for dynamic models. We identified and addressed two principal problems: (i) the individual points of a given set are discriminated through their 2D positions only, therefore shape, geometry and relationships between multiple moving objects need to be encoded through the relative positions of points to each other, and (ii) the optimal representation for a physical dynamic model is not necessarily a 2D keypoint space, where the underlying object dynamics has also been subject to the imaging process (projective geometry).\\n\\nWe propose a new counterfactual model, which learns a sparse representation of visual input in the form of 2D keypoints coupled with a (small) set of coefficients per point modeling complementary shape and appearance information. Confounders (object masses and initial velocities) in the studied problem are extracted from this representation, and a learned dynamic model forecasts the entire trajectory of these keypoints from a single (counterfactual) observation. Building on recent work in data-driven analysis of dynamic systems (Janny et al., 2021; Peralez & Nadri, 2021), the dynamic model is presented in a higher-dimensional state space, where dynamics are less complex. We show, that these design choices are key to the performance of our model, and that they significantly improve the capability to perform long-term predictions.\\n\\nOur proposed model outperforms strong baselines for physics-informed learning of video prediction.\\n\\nWe introduce a new challenging dataset for this problem, which builds on CoPhy, a recent counterfactual physics benchmark (Baradel et al., 2020). We go beyond the prediction of sequences of 3D positions and propose a counterfactual task for predictions in pixel space after interventions on initial conditions (displacing, re-orienting or removing objects). In contrast to the literature, our benchmark also better controls for the identifiability of causal relationships and counterfactual variables and provides more accurate physics simulation.\\n\\nCounterfactual (CF) reasoning \u2014 and learning of causal relationships in ML was made popular by works of J. Pearl, e.g. (Pearl, 2000), which motivate and introduce mathematical tools detailing the principles of do-calculus, i.e. study of unobserved interventions on data. A more recent survey links these concepts to the literature in ML (Scholkopf et al., 2021). The last years have seen the emergence of several benchmarks for CF reasoning in physics. CLEVRER (Yi et al., 2020) is a visual question answering dataset, where an agent is required to answer a CF question after observing a video showing 3D objects moving and colliding. Li et al. (2020) introduce a CF benchmark with two tasks: a scenario where balls interact with each other according to unknown interaction laws (such as gravity or elasticity), and a scenario where clothes are folded by the wind. The agent needs to identify CF variables and causal relationships between objects, and to predict future frames. CoPhy (Baradel et al., 2020) clearly dissociates the observed experiment from the CF one, and contains three complex 3D scenarios involving rigid body dynamics. However, the proposed method relies on the supervision of 3D object positions, while our work does not require any meta data.\\n\\nPhysics-inspired ML \u2014 and learning visual dynamics has been dealt early on with recurrent models (Srivastava et al., 2015; Finn et al., 2016; Lu et al., 2017), or GANs (Vondrick et al., 2016; Mathieu et al., 2016). Kwon & Park (2019) adopt a Cycle-GAN with two discriminator heads, in charge of identifying false images and false sequences in order to improve the temporal consistency of the model in long term prediction. Nonetheless, the integration of causal reasoning and prior knowledge in these models is not straightforward. Typical work in physics-informed models relies on disentanglement between physics-informed features and residual features (Villegas et al., 2017a;\"}"}
{"id": "1L0C5ROtFp", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Denton & Birodkar, 2017) and may incorporate additional information based on the available priors on the scene (Villegas et al., 2017b; Walker et al., 2017). PhyDNet Le Guen & Thome (2020) explicitly disentangles visual features from dynamical features, which are supposed to follow a PDE. It achieves SOTA performance on Human3.6M (Ionescu et al., 2014) and Sea Surface Temperature (de Bezenac et al., 2018), but we show that it fails on our challenging benchmark.\\n\\nKeypoint detection \u2014 is a well researched problem in vision with widely used handcrafted baselines (Lowe, 1999). New unsupervised variants emerged recently and have been shown to provide a suitable object-centric representation, close to attention models, which simplify the use of physical and/or geometric priors (Locatello et al., 2020; Veerapaneni et al., 2020). They are of interest in robotics and reinforcement learning, where a physical agent has to interact with objects (Kulkarni et al., 2019; Manuelli et al., 2020; 2019). KeypointsNet (Suwajanakorn et al., 2018) is a geometric reasoning framework, which discovers meaningful keypoints in 3D through spatial coherence between viewpoints. Close to our work, (Minderer et al., 2019) proposes to learn a keypoints-based stochastic dynamic model. However, the model is not suited for CF reasoning in physics and may suffer from inconsistency in the prediction of dynamics over long horizons.\\n\\nWe build on CoPhy (Baradel et al., 2020), retaining its strengths, but explicitly focusing on a counterfactual scenario in pixel space and eliminating the ill-posedness of tasks we identified in the existing work. Each data sample is called an experiment, represented as a pair of trajectories: an observed one with initial condition $X_0 = A$ and outcome $X_{t=1..T} = B$ (a sequence), and a counterfactual one $\\\\bar{X}_0 = C$ and $\\\\bar{X}_{t=1..T} = D$ (a sequence). Throughout this paper we will use the letters $A$, $B$, $C$ and $D$ to distinguish the different parts of each experiment. The initial conditions $A$ and $C$ are linked through a do-operator $\\\\text{do}(X_0 = C)$, which modifies the initial condition (Pearl, 2018).\\n\\nExperiments are parameterized by a set of intrinsic physical parameters $z$ which are not observable from a single initial image $A$. We refer to these as confounders. As in CoPhy, in our benchmark the do-operator is observed during training, but confounders are not \u2014 they have been used to generate the data, but are not used during training or testing. Following (Pearl, 2018), the counterfactual task consists in inferring the counterfactual outcome $D$ given the observed trajectory $AB$ and the counterfactual initial state $C$, following a three-step process:\\n\\n1. **Abduction**: use the observed data $AB$ to compute the counterfactual variables, i.e. physical parameters, which are not affected by the do-operation.\\n2. **Action**: update the causal model; keep the same identified confounders and apply the do-operator, i.e. replace the initial state $A$ by $C$.\\n3. **Prediction**: Compute the counterfactual outcome $D$ using the causal graph.\\n\\nThe benchmark contains three scenarios involving rigid body dynamics. **BlocktowerCF** studies stable and unstable 3D cube towers, the confounders are masses. **BallsCF** focuses on 2D collisions between moving spheres (confounders are masses and initial velocities). **CollisionCF** is about collisions between a sphere and a cylinder (confounders are masses and initial velocities) (Fig. 1). Unlike CoPhy, our benchmark involves predictions in RGB pixel space only. The do-operation consists in visually observable interventions on $A$, such as moving or removing an object. The confounders cannot be identified from the single-frame observation $A$, identification requires the analysis of the entire $AB$ trajectory.\\n\\n### Identifiability of confounders\\n\\nFor an experiment $\\\\psi(AB, CD, z)$ to be well-posed, the confounders $z$ must be retrievable from $AB$. For example, since the masses of a stable cube tower cannot be identified generally in all situations, it can be impossible to predict the counterfactual outcome of an unstable tower, as collisions are not resolvable without known masses. In contrast to CoPhy, we ensure that each experiment $\\\\psi(AB, CD, z)$ is well posed and satisfies the following constraints:\\n\\n**Definition 1 (Identifiability, (Pearl, 2018))** The experiment $(AB, CD, z)$ is identifiable if, for any set of confounders $z'$:\\n\\n$$\\n\\\\psi(A, z) = \\\\psi(A, z') \\\\Rightarrow \\\\psi(C, z) = \\\\psi(C, z').\\n$$\\n\\n(1)\"}"}
{"id": "1L0C5ROtFp", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: The Filtered-CoPhy benchmark suite contains three challenging scenarios involving 2D or 3D rigid body dynamics with complex interactions, including collision and resting contact. Initial conditions are modified by an intervention. Initial motion is indicated through arrows.\\n\\nIn an identifiable experiment there is no pair \\\\((z, z')\\\\) that gives the same trajectory \\\\(AB\\\\) but different counterfactual outcomes \\\\(CD\\\\). Details on implementation and impact are in appendix A.1.\\n\\nFigure 2: Impact of temporal frequency on dynamics, 3D trajectories of each cube are shown. Black dots are sampled at 5 FPS, colored dots at 25 FPS. Collisions between the red cube and the ground are not well described by the black dots, making it hard to infer physical laws from regularities in data.\\n\\nCounterfactuality \u2014 We enforce sufficient difficulty of the problem through the meaningfulness of confounders. We remove initial situations where the choice of confounder values has no significant impact on the final outcome:\\n\\n**Definition 2 (Counterfactuality).** Let \\\\(z_k\\\\) be the set of confounders \\\\(z\\\\), where the \\\\(k\\\\)th value has been modified. The experiment \\\\((AB, CD, z)\\\\) is counterfactual if and only if:\\n\\n\\\\[\\n\\\\exists k: \\\\psi(C, z_k) \\\\neq \\\\psi(C, z).\\n\\\\]\\n\\nIn other words, we impose the existence of an object of the scene for which the (unobserved) physical properties have a determining effect on the trajectory. Details on how this constraint was enforced are given in appendix A.2.\\n\\nTemporal resolution \u2014 the physical laws we target involve highly non-linear phenomena, in particular collision and resting contacts. Collisions are difficult to learn because their actions are both intense, brief, and highly non-linear, depending on the geometry of the objects in 3D space. The temporal resolution of physical simulations is of prime importance. A parallel can be made with Nyquist-Shannon frequency, as a trajectory sampled with too low frequency cannot be reconstructed with precision. We simulate and record trajectories at 25 FPS, compared to 5 FPS chosen in CoPhy, justified with two experiments. Firstly, Fig. 2 shows the trajectories of the center of masses of cubes in BlocktowerCF, colored dots are shown at 25 FPS and black dots at 5 FPS. We can see that collisions with the ground fall below the sampling rate of 5 FPS, making it hard to infer physical laws from regularities in data at this frequency. A second experiment involves learning a prediction model at different frequencies, confirming the choice 25 FPS \u2014 details are given in appendix A.3.\\n\\n4 U\\n\\n**SUPERVISED LEARNING OF COUNTERFACTUAL PHYSICS**\\n\\nWe introduce a new model for counterfactual learning of physical processes capable of predicting visual sequences \\\\(D\\\\) in the image space over long horizons. The method does not require any supervision other than videos of observed and counterfactual experiences. The code is publicly available online at [https://filteredcophy.github.io](https://filteredcophy.github.io). The model consists of three parts, learning the latent representation and its (counterfactual) dynamics:\\n\\n- **The encoder (De-Rendering module)** learns a hybrid representation of an image in the form of a (i) dense feature map and (ii) 2D keypoints combined with (iii) a low-dimensional vector of coefficients.\"}"}
{"id": "1L0C5ROtFp", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Coefficient estimator $C$\\n\\nWe obtain the coefficient by applying a third convolutional network $C$ to the output of the common encoder CNN, which again results in a set of 2D vectors of shape $(batch, K, 28, 28)$. These vectors are flattened channel-wise and provide a tensor of shape $(batch, K, 28 \\\\times 28)$ fed to an MLP (see Table 10a for the exact architecture) that estimates the coefficients $C_{\\\\text{target}}$ of shape $(batch, K, C + 1)$.\\n\\nGaussian mapping $G$\\n\\nThe keypoint vector $K_{\\\\text{target}}$ is mapped to a 2D vector through a Gaussian mapping process:\\n\\n$$G(k)(x,y) = \\\\exp\\\\left(-\\\\frac{(x-k_x)^2 + (y-k_y)^2}{\\\\sigma^2}\\\\right),$$\\n\\nwhere $G(k) \\\\in \\\\mathbb{R}^{28 \\\\times 28}$ is the Gaussian mapping of the keypoint $k = [k_x, k_y]$. We deform these Gaussian mappings by applying convolutions with filters $H_i$ controlled by the coefficients $c_i^k$.\\n\\nThe filters from $H$ are $5 \\\\times 5$ kernels that elongate the Gaussian in a specific direction. Practically, we obtain the filter $H_i$ by drawing a line crossing the center of the kernel and with a slope angle of $i\\\\pi / C$ where $C$ is the number of coefficients. We then apply a 2D convolution:\\n\\n$$G_i^k = c_{C+1}^k c_i^k (G(k) * H_i).$$\\n\\nNote that we also compute a supplementary coefficient $\\\\alpha_{C+1}^k$ used as a gate on the keypoints. By setting this coefficient to zero, the de-rendering module can deactivate a keypoint (which is redundant with deactivating the full set of coefficients for this keypoint).\\n\\nRefiner $R$\\n\\nTo reconstruct the target image, we channel-wise stack feature vectors from the source with the constructed filters and feed them to the decoder CNN $R$ (Table 10b). We trained the de-rendering module on pairs of images $(X_{\\\\text{source}}, X_{\\\\text{target}})$ randomly sampled from sequences $D$. For a given sequence $D$, we take $T-25$ first frames of the trajectory as a source (where $T$ is the number of frames in the video). The last 25 frames are used as a target. For evaluation, we take the 25th frame as the source, and the 50th frame as the target. We use Adam optimizer with a learning rate of $10^{-3}$, $\\\\gamma_1 = 10^4$ and $\\\\gamma_2 = 10^{-1}$ to minimize equation 4.\"}"}
{"id": "1L0C5ROtFp", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We describe the architectural choices made in CoDy. Let \\\\( s(t) = [k_0 k_1 c_1 k_2 c_2 \\\\ldots k_K c_{C+1}] \\\\) be the state representation of an image \\\\( X_t \\\\), composed of the \\\\( K \\\\) keypoints 2D coordinates with their \\\\( C+1 \\\\) coefficients. The time derivative of each component of the state is computed via an implicit Euler derivation scheme \\\\( \\\\dot{k}(t) = k(t) - k(t-1) \\\\). We use a subscript notation to distinguish the keypoints from \\\\( AB \\\\) and \\\\( CD \\\\).\\n\\n**CF estimator** \u2013 The latent representation of the confounders is discovered from \\\\( s_{AB} \\\\). The graph neural network from this module implements the message passing function \\\\( f() \\\\) and the aggregation function \\\\( g() \\\\) (see equation 5) by an MLP with 3 hidden layers of 64 neurons and ReLU activation unit. The resulting nodes embeddings \\\\( h_{AB}(t) = GN(s_{AB}(t)) \\\\) belong to \\\\( \\\\mathbb{R}^{128} \\\\). We then apply a gated recurrent unit with 2 layers and a hidden vector of size 32 to each node in \\\\( h_{AB}(t) \\\\) (sharing parameters between nodes). The last hidden vector is used as the latent representation of the confounders \\\\( u_k \\\\).\\n\\n**State encoder-decoder** \u2013 The state encoder is modeled as a \\\\( GN \\\\) where the message passing function and the aggregation function are MLPs with one hidden layer of 32 units. The encoded state \\\\( \\\\sigma_{CD} = E(s_{CD}) \\\\) lies in \\\\( \\\\mathbb{R}^{256} \\\\). We perform dynamical prediction in this \\\\( \\\\sigma \\\\) space, and then project back the forecasting in the keypoint space using a decoder. The decoder \\\\( \\\\Delta(\\\\sigma(t)) \\\\) first applies a shared GRU with one layer and a hidden vector size of 256 to each keypoint \\\\( \\\\sigma_k(t) \\\\), followed by a graph neural network with the same structure as the state encoder.\\n\\n**Dynamic system** \u2013 Our dynamic system forecasts the future state \\\\( \\\\hat{\\\\sigma}(t+1) \\\\) from the current estimation \\\\( \\\\hat{\\\\sigma}(t) \\\\) and the confounders \\\\( u = [u_1 \\\\ldots u_K] \\\\). It first applies a graph neural network to the concatenated vector \\\\([\\\\hat{\\\\sigma}(t) u] \\\\). The message passing function \\\\( f \\\\) and the aggregation function \\\\( g \\\\) are MLPs with 3 hidden layers of 64 neurons and ReLU activation function. The resulting nodes embeddings \\\\( GN(\\\\sigma(t)) \\\\) belong to \\\\( \\\\mathbb{R}^{64} \\\\) and are fed to a GRU sharing weights among each nodes with 22.\"}"}
{"id": "1L0C5ROtFp", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This GRU updates the hidden vector \\n\\\\( v_{CD}(t) = [v_1 \\\\ldots v_K] \\\\), \\nthat is then used to compute a displacement vector with a linear transformation:\\n\\\\[\\n\\\\hat{\\\\sigma}_k(t+1) = \\\\sigma_k(t) + Wv_k(t) + b.\\n\\\\] (13)\\n\\nCoDy is trained using Adam to minimize equation 6 (learning rate \\\\(10^{-4} \\\\), \\\\( \\\\gamma^3 = 1 \\\\)). We train each CoDy instance by providing it with fixed keypoints states \\\\( s_{AB}(t) \\\\) and the initial condition \\\\( s_{CD}(t=0) \\\\) computed by our trained de-rendering module. CoDy first computes the latent con\\nfounder representation \\\\( u_k \\\\), and then projects the initial condition into the latent dynamic space \\\\( \\\\sigma(t=0) = E(s_{CD}(t=0)) \\\\). We apply the dynamic model multiple times in order to recursively forecast \\\\( T \\\\) time steps from \\\\( CD \\\\). We then apply the decoder \\\\( \\\\Delta(\\\\hat{\\\\sigma}(t)) \\\\) to compute the trajectory in the keypoint space.\\n\\n\\\\[ MOTA = \\\\frac{1}{\\\\sum_t g_t} \\\\sum_t (m_t + f_t + s_t) \\\\] (14)\\n\\nwhere \\\\( m_t \\\\) is the number of missed objects at time \\\\( t \\\\), \\\\( f_t \\\\) is the number of false positives at time \\\\( t \\\\), \\\\( s_t \\\\) is the number of swaps at time \\\\( t \\\\), and \\\\( g_t \\\\) is the number of objects at time \\\\( t \\\\).\\n\\n\\\\[ MOTP = \\\\frac{\\\\sum_i \\\\sum_t d_{it}}{\\\\sum_t c_t} \\\\] (15)\\n\\nwhere \\\\( c_t \\\\) is the number of accurately tracked objects at time \\\\( t \\\\) and \\\\( d_{it} \\\\) is the distance between the keypoint and the center of mass of the \\\\( i \\\\)th association \\\\{keypoints+center of mass\\\\}.\\n\\nNote that these metrics are related: low MOTP indicates that the tracked objects are tracked precisely, and low MOTA indicates that many objects are missed. Thus, to be efficient, a model needs to achieve both low MOTP and high MOTA.\\n\\nWe also reported the performances of CoPhyNet (Baradel et al., 2020) that predicts counterfactual outcomes in euclidian space using the ground-truth 3D space. As it uses GT object positions during training, it is not comparable and should be considered as a soft upper bound of our method. We present our results in Table 11. This confirms the superiority of our method over UV-CDN in key\\npoint space. The upper bound CoPhyNet takes advantage of the non-ambiguous 3D representation modeled by the ground-truth state of the object of the scene.\\n\\nOur method also outperforms CoPhyNet on the ballsCF task, probably due to two phenomena. First, ballsCF is the only 2D task of FilteredCoPhy. Thus, CoPhyNet does not have an advantage of using ground-truth 3D positions. Second, the state-encoder in CoDy projects the 2D position of each sphere in a space where the dynamics is easier to learn, probably by breaking the non-linearity of collisions.\"}"}
{"id": "1L0C5ROtFp", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: MOT metrics for different methods. While not comparable, we report the CoPhyNet performance as a soft upper bound. Our method and UV-CDN use one keypoint per object.\\n\\n- **MOTA**: higher is better;\\n- **MOTP**: lower is better;\\n\\n---\\n\\n**Figure 16**: Effect of the do-operation on the quality of the forecasted video. (left) our method generalizes well to a wide range of \\\"Move\\\" operation amplitudes. (right) We observe a difference of 3dB in favor of the Move do-operation, which is unsurprising, as it is the least disturbing intervention.\\n\\n---\\n\\n**E.2 Impact of the Do-operations**\\n\\nWe also measure the impact of the do-operation types on the video forecasting. Fig. 16 (left) is obtained by computing PSNR for each example of the training set and reporting the result on a 2D graph, depending on the amplitude of the displacement that characterizes the do-operation. We applied the same method to obtain Fig.16 (right) that focuses on the type of do-operation, that is moving, removing or rotating an object. These figures are computed using the 2N keypoints models.\\n\\nOur method generalizes well across different do-operations, including both the type of the operation, and the amplitude. A key to this success is the careful design of the dataset (balanced with respect to the types of do-operations), and a reasonable representation (our set of keypoints and coefficients) able to detect and model each do-operation from images.\\n\\n---\\n\\n**F. EXPERIMENTS ON REAL-WORLD DATA**\\n\\nOur contributions are focused on the discovery of causality in physics through counterfactual reasoning. We designed our model in order to solve the new benchmark and provided empirical evidence that our method is well suited for modeling rigid-body physics and counterfactual reasoning. The following section aims to demonstrate that our approach can also be extended to a real-world dataset.\\n\\nWe provide qualitative results obtained on a derivative of BlocktowerCF using real cubes tower (Lerer et al., 2016).\"}"}
{"id": "1L0C5ROtFp", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix\\n\\nA. Further Details on Dataset Generation\\n\\nConfounding factors in our setup are masses, which we discretize in \\\\(\\\\{1, 10\\\\}\\\\). For BallsCF and CollisionCF, we can also consider the continuous initial velocities of each object as confounders, since they have to be identified in \\\\(AB\\\\) to forecast \\\\(CD\\\\).\\n\\nWe simulate all trajectories associated with the various possible combinations of masses from the same initial condition. Do-interventions, however, depend on the task. For BlocktowerCF and BallsCF, do-interventions consist in (a) removing the top cube or a ball, or (b) shifting a cube/ball on the horizontal plane. In this case, for BlocktowerCF, we make sure that the cube does not move too far from the tower, in order to maintain contact. For CollisionCF, the do-interventions are restricted to shifting operations, since there are only two objects (a ball and a cylinder). It can consist of either a switch of the cylinder\u2019s orientation between vertical or horizontal, or a shift of the position of the moving object relative to the resting one in one of the three canonical directions \\\\(x, y, z\\\\).\\n\\nA.1 Enforcing the Identifiability Constraint\\n\\nThe identifiability and counterfactual constraints described in section 3 are imposed numerically, i.e. we first sample and simulate trajectories with random parameters and then reject those that violate these constraints.\\n\\nAs stated in section 3, an identifiable experiment guarantees that there is no pair \\\\((z, z')\\\\) that gives the same trajectory \\\\(AB\\\\) but a different counterfactual outcome \\\\(CD\\\\). Otherwise, there will be no way to choose between \\\\(z\\\\) and \\\\(z'\\\\) only from looking at \\\\(AB\\\\), thus no way to correctly forecast the counterfactual experiment. By enforcing this constraint, we make sure that there exists at least a set \\\\(\\\\{z, z', \\\\ldots\\\\}\\\\) of confounders that give at the same time similar observed outcomes \\\\(AB\\\\) and similar counterfactual outcomes \\\\(CD\\\\).\\n\\nIn practice, there exists a finite set of possible variables \\\\(z_i\\\\), corresponding to every combination of masses for each object in the scene (masses take their value in \\\\(\\\\{1, 10\\\\}\\\\)). During generation, we submit each candidate experiment \\\\((AB, CD, z)\\\\) to a test ensuring that the candidate is identifiable.\\n\\nLet \\\\(\\\\psi(X_0, z)\\\\) be the function that gives the trajectory of a system with initial condition \\\\(X_0\\\\) and confounders \\\\(z\\\\). We simulate all possible trajectories \\\\(\\\\psi(A, z_i)\\\\) and \\\\(\\\\psi(C, z_i)\\\\) for every possible \\\\(z_i\\\\).\\n\\nIf there exists \\\\(z' \\\\neq z\\\\) such that the experiment is not identifiable, the candidate is rejected. This constraint requires to simulate the trajectory of each experiment several times by modifying physical properties of the objects.\\n\\nEqualities in Definition 1 are relaxed by thresholding distances between trajectories. We reject a candidate experiment if there exists a \\\\(z'\\\\) such that\\n\\n\\\\[\\n\\\\sum_{t=0}^{T} \\\\| \\\\psi(A, z) - \\\\psi(A, z') \\\\|_2 < \\\\epsilon \\\\quad \\\\text{and} \\\\quad \\\\sum_{t=0}^{T} \\\\| \\\\psi(C, z) - \\\\psi(C, z') \\\\|_2 > \\\\epsilon.\\n\\\\]\\n\\nThe choice of the threshold value \\\\(\\\\epsilon\\\\) is critical, in particular for the identifiability constraint:\\n\\n- If the threshold is too high, all \\\\(AB\\\\) trajectories will be considered as equal, which results in acceptance of unidentifiable experiments.\\n- If the threshold is too low, all trajectories \\\\(CD\\\\) are considered equal. Again, this leads to mistakenly accepting unidentifiable experiments.\\n\\nThere exists an optimal value for \\\\(\\\\epsilon\\\\), which allows to correctly reject unidentifiable experiences. To measure this optimal threshold, we generated a small instance of the BlocktowerCF dataset without constraining the experiments, i.e. trajectories can be unidentifiable and non-counterfactual. We then plot the percentage of rejected experiments in this unfiltered dataset against the threshold.\"}"}
{"id": "1L0C5ROtFp", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Experimental tuning of the threshold parameter. We generate an unconstrained subset of BlocktowerCF and plot the percentage of identifiable experiments as function of the threshold $\\\\varepsilon$.\\n\\n| Threshold $\\\\varepsilon$ | Percentage of identifiable experiment |\\n|-------------------------|--------------------------------------|\\n| 50%                     | 162.38%                              |\\n| 60%                     | 56%                                  |\\n| 70%                     | 100%                                 |\\n| 80%                     | 162.38%                              |\\n| 90%                     | 51%                                  |\\n| 100%                    | 162.38%                              |\\n\\nTable 6: (a) Sanity check of the identifiability constraint in BlocktowerCF, which results in better estimation of cube masses. The corrected accuracy only considers those cubes for which changes in masses are consequential for the trajectory $D$. (b) MSE between ground truth 3D positions and predicted positions after 1 second, depending on the sampling rate of the trajectory.\\n\\nAccuracy\\n\\nCorrected Acc.\\n\\nFPS\\n\\nMSE ($\\\\times 10^{-2}$)\\n\\n4.58 3.97 3.74 3.82 3.93\\n\\nTo demonstrate importance of this, we train a recurrent graph network on BlocktowerCF to predict the cube masses from ground-truth state trajectories $AB$, including pose and velocities, see Fig. 8. It predicts each cube's mass by solving a binary classification task. We train this model on both BlocktowerCF and an alternative version of the scenario generated without the identifiability constraint. The results are shown in Table 6a. We are not aiming 100% accuracy, and this problem remains difficult in a sense that the identifiability constraint ensures the identifiability of a set of confounder variables, while our sanity check tries to predict a unique $z$.\\n\\nHowever, the addition of the identifiability constraint to the benchmark significantly improves the model's accuracy, which indicates that the property acts positively on the feasibility of Filtered-CoPhy. The corrected accuracy metric focuses solely on the critical cubes, i.e. those cubes whose masses directly define the trajectory $CD$.\\n\\nA.2 ENFORCING THE COUNTERFACTUALITY CONSTRAINT\\n\\nLet $(AB, CD, z)$ be a candidate experiment, and $z_k$ be a combination of masses identical to $z$ except for the $k$th value. The counterfactuality constraint consists in checking that there exists at least one $k$ such that $\\\\psi(C, z) \\\\neq \\\\psi(C, z_k)$. To do so, we simulate $\\\\psi(C, z_k)$ for all $k$ and measure the difference with the candidate trajectory $\\\\psi(C, z)$. Formally, we verify the existence of $k$ such that:\\n\\n$$\\\\sum_{t=0}^{T} \\\\| \\\\psi(C, z_k) - \\\\psi(C, z) \\\\|^2 < \\\\varepsilon.$$ \\n\\n(8)\\n\\nA.3 ANALYZING TEMPORAL RESOLUTION\\n\\nWe analyzed the choice of temporal frequency for the benchmark with another sanity check. We simulate a non-counterfactual dataset from BlocktowerCF where all cubes have equal masses.\"}"}
{"id": "1L0C5ROtFp", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### IMULATION DETAILS\\n\\n| Position | Quaternion | Lin. vel. | Ang. vel. |\\n|----------|------------|-----------|-----------|\\n| t = 0 .. T |            |           |           |\\n\\n1. We sample a combination of masses and other physical characteristics of the given experiment, respecting the balance between good coverage of confounder combinations and counterfactuality. We generate the trajectories iteratively:\\n\\n2. Then we search for an initial configuration from the observed trajectory. This check shows clearly that 25 FPS corresponds to the best trade-off between an accurate representation of the collision and the amount of training data. Fig. 9 shows a practical example of the effect of time resolution on dynamical information in a trajectory.\\n\\nFor each task in **Filtered-CoPhy**, we make sure that this augmentation of the sampling rate by training an agent to forecast a 1 second-length trajectory from the states of the previous second.\\n\\nA.4 Simulation Details\\n\\n- **BlocktowerCF**\\n  - For collision and the amount of training data. Fig. 9 shows a practical example of the effect of time resolution on dynamical information in a trajectory.\\n  - We used Pybullet as physics engine to simulate the effect of time resolution on dynamical information in a trajectory.\\n  - We vary the sampling rate by training an agent to forecast a 1 second-length trajectory from the states of the previous second.\\n\\n- **1 second**\\n  - We check the identifiability constraint described above. We generate the trajectories iteratively:\\n  - We used Pybullet as physics engine to simulate the effect of time resolution on dynamical information in a trajectory.\\n  - We vary the sampling rate by training an agent to forecast a 1 second-length trajectory from the states of the previous second.\\n\\n- **6b**\\n  - This check shows clearly that 25 FPS corresponds to the best trade-off between an accurate representation of the collision and the amount of training data. Fig. 9 shows a practical example of the effect of time resolution on dynamical information in a trajectory.\\n  - We used Pybullet as physics engine to simulate the effect of time resolution on dynamical information in a trajectory.\\n  - We vary the sampling rate by training an agent to forecast a 1 second-length trajectory from the states of the previous second.\\n\\n- **2**\\n  - Then we search for an initial configuration from the observed trajectory. This check shows clearly that 25 FPS corresponds to the best trade-off between an accurate representation of the collision and the amount of training data. Fig. 9 shows a practical example of the effect of time resolution on dynamical information in a trajectory.\\n  - We used Pybullet as physics engine to simulate the effect of time resolution on dynamical information in a trajectory.\\n  - We vary the sampling rate by training an agent to forecast a 1 second-length trajectory from the states of the previous second.\\n\\n- **1s**\\n  - This check shows clearly that 25 FPS corresponds to the best trade-off between an accurate representation of the collision and the amount of training data. Fig. 9 shows a practical example of the effect of time resolution on dynamical information in a trajectory.\\n  - We used Pybullet as physics engine to simulate the effect of time resolution on dynamical information in a trajectory.\\n  - We vary the sampling rate by training an agent to forecast a 1 second-length trajectory from the states of the previous second.\\n\\n### Figure 9: Visual examples of the impact of temporal resolution on dynamical information for each simulation.\\n\\n- **Left:** We check the identifiability constraint described above. We generate the trajectories iteratively:\\n- **Right:** We check the identifiability constraint described above. We generate the trajectories iteratively:\\n\\n### Figure 8: Impact of the choice of temporal resolution. Left: We check the identifiability constraint described above. We generate the trajectories iteratively:\\n\\n- **Black dots** are sampled at 6 FPS while **red dots** are sampled at 25 FPS.\\n\\nEach experiment is designed to make sure that this augmentation of the sampling rate by training an agent to forecast a 1 second-length trajectory from the states of the previous second.\\n\\n- **Black dots** are sampled at 6 FPS while **red dots** are sampled at 25 FPS.\"}"}
{"id": "1L0C5ROtFp", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: During the dataset generation process, we carefully balance combinations of masses, i.e. the confounders. For BlocktowerCF, we also guarantee that the proportion of stable CD towers is close to 50% for each confounder configuration.\\n\\n3. We look for a valid do-operation such that identifiability and counterfactuality constraints are satisfied. If no valid do-operation is found after a fixed number of trials, we reject this experiment.\\n\\n4. If a valid pair (AB, CD) is found, we add the sample to the dataset.\\n\\nThe trajectories were simulated with a sample time of 0.04 seconds. The video resolution is 448 \u00d7 448 and represents 6 seconds for BlocktowerCF and BallsCF, and 3 seconds for CollisionCF. We invite interested readers to look at our code for more details, such as do-operation sampling, or intrinsic camera parameters. Fig. 10 shows the confounder distribution in the three tasks.\\n\\n**Performance Evaluation of the De-rendering Module**\\n\\n**B.1 Image Reconstruction**\\n\\n| Keypoints | N | 2N | 4N |\\n|-----------|---|----|----|\\n| BT-CF PSNR | 34.40 | 35.41 | 34.92 |\\n| MSE Grad | 27.24 | 21.39 | 23.99 |\\n| B-CF PSNR | 37.76 | 37.06 | 36.98 |\\n| MSE Grad | 3.47 | 3.77 | 3.95 |\\n| C-CF PSNR | 32.00 | 35.41 | 34.42 |\\n| MSE Grad | 32.00 | 12.57 | 17.09 |\\n\\nTable 7: PSNR (dB) on the task of reconstructing the target from the source (both randomly sampled from Filtered-CoPhy), using 5 coefficients per keypoint. We vary the number of keypoints in our model. Here N is the maximum number of the objects in the scene.\\n\\nWe evaluate the reconstruction performance of the de-rendering module in the reconstruction task. Note that there is trade-off between the reconstruction performance and the dynamic forecasting accuracy: a higher number of keypoints may lead to better reconstruction, but can hurt prediction performance, as the dynamic model is more difficult to learn.\\n\\nReconstruction error \u2013 We first investigate the impact of the number of keypoints in Table 7 by measuring the Peak Signal to Noise Ratio (PSNR) between the target image and its reconstruction. We vary the number of keypoints among multiples of N, the maximum number of objects in the scene. Increasing the number of keypoints increases reconstruction quality (PSNR) up to a certain point, but results in degradation in forecasting performance. Furthermore, doubling the number of keypoints only slightly improves reconstruction accuracy. This tends to indicate that our additional coefficients are already sufficient to model finer-grained visual details. Table 3 in the main paper measures the impact of the number of keypoints and the presence of the additional appearance coefficients on the full pipeline including the dynamic model. Table 8 illustrates the impact of the number of keypoints and the additional appearance coefficient on the reconstruction performance alone. As we can see, the addition of the coefficient consistently improves PSNR for low numbers of keypoints (over 2 dB for N keypoints).\"}"}
{"id": "1L0C5ROtFp", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Counterfactual Dynamic (CoDy) model\\n\\nThe decoder strategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nFigure 3: We de-render visual input into a latent space composed of a dense feature map based on recurrent graph networks, in the lines of which encodes positions in keypoints and appearance and orientation in the coefficients.\\n\\nIf this assumption is not satisfied, global camera motion could be compensated after estimation.\\n\\nThe unsupervised objective is formulated on pairs of images on the decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n\\nX\\n\\nGiven features from\\n\\nThe decoder\\n\\nstrategy emerges naturally through the unsupervised objective: we optimize reconstruction of (static information, a set of keypoints) from same image is predicted with a decoder and coefficients. This formulation requires the decoder to aggregate dense information from the anisotropic shape using a fixed filter bank.\\n\\nA set of corresponding coefficients from same features\\n\\nand keypoints+coefficients from\\n\\n1\\n"}
{"id": "1L0C5ROtFp", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The model is trained to minimize the mean squared error (MSE) reconstruction loss, regularized with a loss on spatial gradients $\\\\nabla X$ weighted by hyper-parameters $\\\\gamma_1, \\\\gamma_2 \\\\in \\\\mathbb{R}$:\\n\\n$$L_{\\text{deren}} = \\\\gamma_1 \\\\|X_{\\\\text{target}} - \\\\hat{X}_{\\\\text{target}}\\\\|^2_2 + \\\\gamma_2 \\\\|\\\\nabla X_{\\\\text{target}} - \\\\nabla \\\\hat{X}_{\\\\text{target}}\\\\|^2_2,$$\\n\\n(4)\\n\\nwhere $\\\\hat{X}_{\\\\text{target}} = D(F(X_{\\\\text{source}}), K(X_{\\\\text{target}}), C(X_{\\\\text{target}}))$ is the reconstructed image, $\\\\gamma_1, \\\\gamma_2$ are weights.\\n\\nRelated work \u2013 our unsupervised objective is somewhat related to Transporter (Kulkarni et al., 2019), which, as our model, computes visual feature vectors $F_{\\\\text{source}}$ and $F_{\\\\text{target}}$ as well as 2D keypoints $K_{\\\\text{source}}$ and $K_{\\\\text{target}}$, modeled as a 2D vector via Gaussian mapping. It leverages a handcrafted transport equation:\\n\\n$$\\\\hat{\\\\Psi}_{\\\\text{target}} = F_{\\\\text{source}} \\\\times (1 - K_{\\\\text{source}}) \\\\times (1 - K_{\\\\text{target}}) + F_{\\\\text{target}} \\\\times K_{\\\\text{target}}.$$\\n\\nAs in our case, the target image is reconstructed through a refiner network $\\\\hat{X}_{\\\\text{target}} = R(\\\\hat{\\\\Psi}_{\\\\text{target}})$. The transporter suffers from a major drawback when used for video prediction, as it requires parts of the target image to reconstruct the target image \u2014 the model was originally proposed in the context of RL and control, where reconstruction is not an objective. It also does not use shape coefficients, requiring shapes to be encoded by several keypoints, or abusively be carried through the dense features $F_{\\\\text{target}}$. This typically leads to complex dynamics non representative of the dynamical objects. We conducted an in-depth comparison between the Transporter and our representation in appendix C.2.\\n\\n**4.2 Dynamic Model and Confounder Estimation**\\n\\nOur counterfactual dynamic model (CoDy) leverages multiple graph network (GN) based modules (Battaglia et al., 2016) that join forces to solve the counterfactual forecasting tasks of Filtered-CoPhy. Each one of these networks is a classical GN, abbreviated as $\\\\text{GN}(x_k)$, which contextualizes input node embeddings $x_k$ through incoming edge interactions $e_{ik}$, providing output node embeddings $\\\\hat{x}_k$ (parameters are not shared over the instances):\\n\\n$$\\\\text{GN}(x_k) = \\\\hat{x}_k,$$\\n\\nsuch that $\\\\hat{x}_k = g(x_k, \\\\sum_i e_{ik})$ with $e_{ij} = f(x_i, x_j)$,\\n\\n(5)\\n\\nwhere $f$ is a message-passing function and $g$ is an aggregation function.\\n\\nWe define the state of frame $X_t$ at time $t$ as a stacked vector composed of keypoints and coefficients computed by the encoder (the de-rendering module), i.e. $s(t) = [s_1(t) \\\\ldots s_k(t)]$ where $s_k(t) = [k_k c_{1k} \\\\ldots c_{C+1k}(t)]$. In the lines of (Baradel et al., 2020), given the original initial condition and outcome $AB$, CoDy estimates an unsupervised representation $u_k$ of the latent confounder variables per keypoint $k$ through the counterfactual estimator (CF estimator in Fig. 4). It first contextualizes the sequence $s_{AB}(t)$ through a graph network $\\\\text{GN}(s_{AB}(t)) = h_{AB}(t) = [h_1(t) \\\\ldots h_K(t)]$. We then model the temporal evolution of this representation with a gated recurrent unit (GRU) per keypoint, sharing parameters over keypoints, taking as input the sequence $h_k$. Its last hidden vector is taken as the confounder estimate $u_k$.\\n\\nRecent works on the Koopman Operator (Lusch et al., 2018) and Kazantzis-Kravaris-Luenberger Observer (Janny et al., 2021; Peralez & Nadri, 2021) have theoretically shown that, under mild assumptions, there exists a latent space of higher dimension, where a dynamical system given as an EDP can have a simpler dynamics. Inspired by this idea, we used an encoder-decoder structure within CoDy, which projects our dynamic system into a higher-dimensional state space, performs forecasting of the dynamics in this latent space, and then projects predictions back to the original keypoint space. Note that this dynamics encoder/decoder is different from the encoder/decoder of the de-rendering/rendering modules discussed in section 4.1. The state encoder $E(s(t)) = \\\\sigma(t)$ is modeled as a graph network $\\\\text{GN}$, whose aggregation function projects into an output embedding space $\\\\sigma(t)$ of dimension 256. The decoder $\\\\Delta(\\\\sigma(t)) = s(t)$ temporally processes the individual contextualized states $\\\\sigma(t)$ with a GRU, followed by new contextualization with a graph network $\\\\text{GN}$. Details on the full architecture are provided in appendix D.2.\\n\\nThe dynamic model CoDy performs forecasting in the higher-dimensional space $\\\\sigma(t)$, computing a displacement vector $\\\\delta(t+1)$ such that $\\\\sigma(t+1) = \\\\sigma(t) + \\\\delta(t+1)$. It takes the projected state embeddings $\\\\sigma_{CD}^k(t)$ per keypoint $k$ concatenated with the confounder representation $u_k$ and contextualizes them with a graph network, resulting in embeddings $h_{CD}^k(t) = \\\\text{GN}([\\\\sigma_{CD}^k(t), u_k])$, which are processed temporally by a GRU. We compute the displacement vector at time $t+1$.\"}"}
{"id": "1L0C5ROtFp", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We compare the proposed model to three strong baselines for physics-inspired video prediction. To do so, we train the encoder+decoder pair without dynamic information on reconstruction loss only, c.f. Equation (4). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Figure 4: During training, we disconnect the dynamic prediction module (CoDy) from the rendering module.\\n\\nThe dynamic model is trained with a loss in keypoint space, \\\\( \\\\hat{\\\\mathbf{s}} \\\\), which is defined as the difference between the ground truth and the predicted keypoints and coefficients. The loss is given by:\\n\\n\\\\[\\n\\\\sum_{t} \\\\left( \\\\mathbf{y}(t) - \\\\mathbf{\\\\hat{y}}(t) \\\\right)^2 + \\\\sum_{t} \\\\left( \\\\mathbf{s}(t) - \\\\mathbf{\\\\hat{s}}(t) \\\\right)^2\\n\\\\]\\n\\nwhere \\\\( \\\\mathbf{y}(t) \\\\) and \\\\( \\\\mathbf{s}(t) \\\\) are the ground truth and predicted features at time \\\\( t \\\\), respectively.\\n\\nEnd-to-end training of all three modules jointly is challenging, as the same pipeline controls both the encoder and decoder. This requires a careful design of the loss function to ensure that the model learns to predict the outcomes accurately while also learning the dynamics.\\n\\n\\\\[\\n\\\\mathcal{L}(\\\\mathbf{y}, \\\\mathbf{\\\\hat{y}}, \\\\mathbf{s}, \\\\mathbf{\\\\hat{s}}) = \\\\frac{1}{N} \\\\sum_{t} \\\\left( \\\\mathbf{y}(t) - \\\\mathbf{\\\\hat{y}}(t) \\\\right)^2 + \\\\frac{1}{N} \\\\sum_{t} \\\\left( \\\\mathbf{s}(t) - \\\\mathbf{\\\\hat{s}}(t) \\\\right)^2\\n\\\\]\\n\\nwhere \\\\( N \\\\) is the number of frames in the video.\\n\\n\\\\( \\\\mathbf{\\\\hat{s}} \\\\) is the reconstructed keypoint representation.\\n\\nThe first term enforces the model to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation (6). Then we freeze the parameters of the keypoint detector and train CoDy to learn to predict the outcomes and the second term favors correct learning of dynamics. Faced with these two contradictory tasks, we minimize the loss in Equation"}
{"id": "1L0C5ROtFp", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Figure 5: Network \\\\( R \\\\) learns a distortion from multiple oriented ellipses to target shapes.\\n\\n# Coefficients:\\n\\n| Model   | PSNR        | L-PSNR       |\\n|---------|-------------|--------------|\\n| BT-CF   | 23.48       | 22.71        |\\n| L-PSNR  | 21.75       | 21.18        |\\n| C-CF    | 24.09       | 23.84        |\\n\\n| Model   | PSNR        | L-PSNR       |\\n|---------|-------------|--------------|\\n| B-CF    | 21.19       | 20.49        |\\n| L-PSNR  | 27.88       | 26.33        |\\n| $C$-CF  | 24.09       | 23.66        |\\n\\nTable 3: Impact of having additional orientation/shape coefficients compared to the keypoint-only solution, for different numbers of keypoints: equal to number of objects ($= N$), $2N$ and $4N$.\\n\\n\u2022 \\\\( V\\\\text{-CDN} \\\\) (Li et al., 2020) is a counterfactual model based on keypoints, close to our work. It identifies confounders from the beginning of a sequence and learns a keypoint predictor through auto-encoding using the Transporter equation (see discussion in Sect. 4.1). As it is, it cannot be used for video prediction and is incomparable with our work, see details in appendix C. We therefore replace the Transporter by our own de-rendering/rendering modules, from which we remove the additional coefficients. We refer to this model as UV-CDN (for Unsupervised \\\\( V\\\\text{-CDN} \\\\)).\\n\\n\u2022 \\\\( \\\\text{PredRNN} \\\\) (Wang et al., 2017) is a ConvLSTM-based video prediction model that leverages spatial and temporal memories through a spatiotemporal LSTM cell.\\n\\nAll models have been implemented in PyTorch, architectures are described in appendix D. For the baselines PhyDNet, UV-CDN and \\\\( \\\\text{PredRNN} \\\\), we used the official source code provided by the authors. We evaluate on each scenario of \\\\( \\\\text{Filtered-CoPhy} \\\\) on the counterfactual video prediction task.\\n\\nFor the two counterfactual models (Ours and UV-CDN), we evaluate on the tasks as intended: we provide the observed sequence \\\\( AB \\\\) and the CF initial condition \\\\( C \\\\), and forecast the sequence \\\\( D \\\\).\\n\\nThe non-CF baselines are required to predict the entire video from a single frame, in order to prevent them from leveraging shortcuts in a part of the video and bypassing the need for physical reasoning.\\n\\nWe measure performance with time-averaged peak signal-to-noise ratio (PSNR) that directly measures reconstruction quality. However, this metric is mainly dominated by error on the static background, which is not our main interest. We also introduce Localized PSNR (L-PSNR), which measures area error on the important regions near moving objects, computed on masked images. We compute the masks using classical background subtraction techniques.\\n\\nComparison to the SOTA \u2014 We compare our model against UV-CDN, PhyDNet and \\\\( \\\\text{PredRNN} \\\\) in Table 1, consistently and significantly outperforming the baselines. The gap with UV-CDN is particularly interesting, as it confirms the choice of additional coefficients to model the dynamics of moving objects. \\\\( \\\\text{PredRNN} \\\\) shows competitive performances, especially on collision$CF$. However, our localized PSNR tends to indicate that the baseline does not reconstruct accurately the foreground, favoring the reconstruction of the background to the detriment of the dynamics of the scene. Fig. 6 visualizes the prediction on a single example, more can be found in appendix G. We also compare to trivial copying baselines in Table 2, namely Copy B, which assumes no intervention and outputs the $B$ sequence, and Copy C, which assumes a stable tower. We evaluate these models in keypoints space measuring MSE on keypoints and coefficients averaged over time, as copying baselines are unbeatable in the regions of static background, making the PSNR metrics unusable.\\n\\nWe provide additional empirical results by comparing the models using Multi-object Tracking metrics and studies on the impact of the do-operations on PSNR in appendix E. We also compute an upper bound of our model using CoPhyNet baseline as described in Baradel et al. (2020).\\n\\nPerformance on real-world data \u2014 is reported in appendix F, showing experiments on 516 videos of real wooden blocks introduced in (Lerer et al., 2016).\\n\\nImpact of appearance coefficients \u2014 are reported in Table 3, comparing to the baseline using a keypoint-only representation. The coefficients have a significant impact: even increasing the number of keypoints to compensate for the loss of information cannot overcome the advantage of disentangling positions and shapes, as done in our model. We provide a deeper analysis of the\"}"}
