{"id": "yfe1VMYAXa4", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Due to the tree-like hierarchical structure of Gene Ontology, we define the depth of GO terms using the shortest path from current GO terms to the root node of GO terms. The distribution of Protein-GO triplets with respect to the depth of GO term is shown at the bottom of Figure 3. Usually, the deeper a GO term is located, the more concrete definition the GO term has, e.g., the small molecule biosynthetic process is the child node of the biosynthetic process. We notice that the number of gene annotations involved with leaf GO terms is a relatively small percentage in all three types of ontologies. We think there are two possibilities: (1) The complexity of annotations of some concrete GO terms, e.g., the identification of whether a protein is involved in the hexose biosynthetic process. (2) The relatively small number of proteins in nature involved with some specific GO terms is intrinsic to these GO terms.\\n\\nWe further pre-process the ProteinKG25 dataset as follows. We observe that the relations of Protein-GO have long-tailed distribution, and mostly focused on involved in, part of and enables. Such data distribution will seriously affect the protein feature embedding during pre-training, so we pre-process our dataset to add more precise and fine-grained relations. Figure 5 illustrates the relation distribution of the original model and the distribution after being pre-processed. We search for GO terms whose frequency of occurrence in ProteinKG25 is the top 10 in MF and CC, top 20 in BP, then we form a new type of Protein2GO relation by their corresponding relationship such as parts of cytoplasm.\\n\\nA.2 DOWNSTREAM TASK DEFINITION\\n\\nWe list the detailed definition of downstream tasks and its corresponding or similar tasks in nature language processing.\\n\\n\u2022 Secondary Structure Prediction is a token-level task and similar to NER (Name Entity Recognition). Each token (amino acid) $x_i$ is mapped to a label $y_i \\\\in \\\\{\\\\text{Helix, Strand, Other}\\\\}$.\\n\\n\u2022 Contact Prediction is a token-level matching task. Each token (amino acid) pair $x_i, x_j$ of sequence (protein) $x$ is mapped to a label $y_{ij} \\\\in \\\\{0, 1\\\\}$.\\n\\n\u2022 Remote Homology Detection is a sequence-level classification task. Each input sequence (protein) $x$ is mapped to a label $y \\\\in \\\\{1, \\\\ldots, 1195\\\\}$ which represents different possible protein folds.\\n\\n\u2022 Fluorescence Landscape Prediction and Stability Landscape Prediction are regression tasks where each sequence (protein) $x$ is mapped to a label $y \\\\in \\\\mathbb{R}$.\\n\\n\u2022 Protein Protein Interface is a sequence-level matching task. Each sequence (protein) pair $x_i, x_j$ is mapped to a label $y_{ij} \\\\in \\\\{0, 1\\\\}$.\\n\\n\u2022 Protein Function Prediction is a sequence-level classification task or a knowledge graph completion task to prediction link of a protein to the Gene Ontology.\\n\\nA.3 EXPERIMENTAL SETTINGS\\n\\nThis section details the training procedures and hyperparameters for each of the datasets. We utilize Pytorch (Paszke et al. (2019)) to conduct experiments with Nvidia V100 GPUs. In pre-training of OntoProtein, similar to Elnaggar et al. (2020), we use the same training protocol such as optimizer, learning rate schedule on BERT model. We set $\\\\gamma$ to 12.0 and the number of negative sampling to 128 in Equation 1.\"}"}
{"id": "yfe1VMYAXa4", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2022\\n\\nFigure 5: Top: Initial relation distribution. Bottom: Pre-processed relation distribution.\\n\\nFigure 6: The timeline of the three Gene Annotation datasets. To generate pre-training set and evaluation set of protein function prediction, we choose three Gene Annotation datasets in different periods.\"}"}
{"id": "yfe1VMYAXa4", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Entity categories of GO terms and specific relation types of these entity categories.\\n\\nTable 6: Hyper-parameters for the downstream task.\\n\\nTable 7: Terms descriptions in Gene Ontology. GO annotations are created by associating a gene or gene product with a GO term. Four pieces of information uniquely identify a GO annotation: Gene Product, Go term, Reference, Evidence. And each Go term contains a description text, and this is Go statement.\"}"}
{"id": "yfe1VMYAXa4", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nSelf-supervised protein language models have proved their effectiveness in learning protein representations. With the increasing computational power, current protein language models pre-trained with millions of diverse sequences can advance the parameter scale from million-level to billion-level and achieve remarkable improvement. However, those prevailing approaches rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better protein representations. We argue that informative biology knowledge in KGs can enhance protein representation with external knowledge. In this work, we propose OntoProtein, the first general framework that makes use of structure in GO (Gene Ontology) into protein pre-training models. We construct a novel large-scale knowledge graph that consists of GO and its related proteins, and gene annotation texts or protein sequences describe all nodes in the graph. We propose novel contrastive learning with knowledge-aware negative sampling to jointly optimize the knowledge graph and protein embedding during pre-training. Experimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction.\\n\\nIntroduction\\n\\nProtein science, the fundamental macromolecules governing biology and life itself, has led to remarkable advances in understanding the disease therapies and human health (Vig et al. (2021)). As a sequence of amino acids, protein can be viewed precisely as a language, indicating that they may be modeled using neural networks that have been developed for natural language processing (NLP). Recent self-supervised pre-trained protein language models (PLMs) such as ESM (Rao et al. (2021b)), ProteinBERT (Brandes et al. (2021)), ProtTrans (Elnaggar et al. (2020)) which can learn powerful protein representations, have achieved promising results in understanding the structure and functionality of the protein. Yet existing PLMs for protein representation learning generally cannot sufficiently capture the biology factual knowledge, which is crucial for many protein tasks but is usually sparse and has diverse and complex forms in sequence.\\n\\nBy contrast, knowledge graphs (KGs) from gene ontology contain extensive biology structural facts, and knowledge embedding (KE) approaches (Bordes et al. (2013), Zheng et al. (2021)) can efficiently embed them into continuous vectors of entities and relations. For example, as shown in Figure 1, without knowing PEX5 has specific biological processes and cellular components, it...\"}"}
{"id": "yfe1VMYAXa4", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Left: A protein example with biology knowledge (molecular function, biological process and cellular component): $K^+$ (potassium ion) Cyclic nucleotide-gated cation channel protein. Right: The corresponding sub-graph regarding $K^+$ carrier proteins in ProteinKG25. Yellow nodes are protein sequences and blue nodes are GO (Gene Ontology) entities with biological descriptions.\\n\\nis challenging to recognize its interaction with other proteins. Furthermore, since protein's shape determines its function, it is more convenient for models to identify protein's functions with the prior knowledge of protein functions having similar shapes. Hence, considering rich knowledge can lead to better protein representation and benefits various biology applications, e.g., protein contact prediction, protein function prediction, and protein-protein interaction prediction. However, different from knowledge-enhanced approaches in NLP (Zhang et al. (2019b), Wang et al. (2021b), Wang et al. (2021a)), protein sequence and gene ontology are two different types of data. Note that protein sequence is composed of amino acids while gene ontology is a knowledge graph with text description; thus, severe issues of structured knowledge encoding and heterogeneous information fusion remain.\\n\\nIn this paper, we take the first to propose protein pre-training with gene ontology embedding (OntoProtein), which is the first general framework to integrate external knowledge graphs into protein pre-training. We propose a hybrid encoder to represent language text and protein sequence and introduce contrastive learning with knowledge-aware negative sampling to jointly optimize the knowledge graph and the protein sequence embedding during pre-training. For the KE objective, we encode the node descriptions (go annotations) as their corresponding entity embeddings and then optimize them following vanilla KE approaches (Bordes et al. (2013)). We further leverage gene ontology of molecular function, cellular component, and biological process and introduce a knowledge-aware negative sampling method for the KE objective. For the MLM (Mask Language Modeling) objective, we follow the approach of existing protein pre-training approaches (Rao et al. (2021b)). OntoProtein has the following strengths:\\n\\n(1) OntoProtein inherits the strong ability of protein understanding from PLMs with the MLM objective.\\n(2) OntoProtein can integrate biology knowledge into protein representation with the supervision from KG by the KE objective.\\n(3) OntoProtein constitutes a model-agnostic method and is readily pluggable into a wide range of protein tasks without additional inference overhead since we do not modify model architecture but add new training objectives.\\n\\nFor pre-training and evaluating OntoProtein, we need a knowledge graph with large-scale biology knowledge facts aligned with protein sequences. Therefore, we construct ProteinKG25, which contains about 612,483 entities, 4,990,097 triples, and aligned node descriptions from GO annotations. To the best of our knowledge, it is the first large-scale KG dataset to facilitate protein pre-training. We deliver data splits for both the inductive and the transductive settings to promote future research.\\n\\nTo summarize, our contribution is three-fold:\\n\\n(1) We propose OntoProtein, the first knowledge-enhanced protein pre-training approach that brings promising improvements to a wide range of protein tasks.\\n(2) By contrastive learning with knowledge-aware sampling to jointly optimize knowledge and protein embedding, OntoProtein shows its effectiveness in widespread downstream tasks, including protein function prediction, protein-protein interaction prediction, contact prediction, and so on.\\n(3) We construct and release the ProteinKG25, a novel large-scale KG dataset, promoting the...\"}"}
{"id": "yfe1VMYAXa4", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Overview of our proposed OntoProtein, which jointly optimize knowledge graph embedding and masked protein model (Best viewed in color.).\\n\\nWe conduct extensive experiments in widespread protein tasks, including TAPE benchmark, protein-protein interaction prediction, and protein function prediction, which demonstrate the effectiveness of our proposed approach.\\n\\n2 METHODS\\n\\nWe begin to introduce our approach of protein pre-training with ontology embedding (OntoProtein), as shown in Figure 2. OntoProtein incorporates external knowledge from Gene Ontology (Go) into language representations by jointly optimizing two objectives. We will first introduce the hybrid encoder, masked protein modeling, and knowledge encoder, and then we will present the details of contrastive learning with knowledge-aware negative sampling. Finally, we will illustrate the overall pre-training objects.\\n\\n2.1 HYBRID ENCODER\\n\\nWe first introduce the hybrid encoder to represent protein and GO knowledge. For the protein encoder, we use the pre-trained ProtBert from Elnaggar et al. (2020). ProtBert is pre-trained using the BERT architecture with UniRef100 datasets. Compared to BERT Devlin et al. (2019), ProtBert encodes amino acid sequences into token level or sentence level representations, which can be used for downstream protein tasks such as contacts prediction tasks. The encoder takes a protein sequence of \\\\( N \\\\) tokens \\\\((x_1, \\\\ldots, x_N)\\\\) as inputs, and computes contextualized amino acid representation \\\\( H_{\\\\text{Protein}} \\\\) and sequence representation \\\\( H_{\\\\text{Protein}} \\\\) via mean pooling. To bridge the gap between text and protein, we utilize affine transformation (an extra linear layer) to project those representation to the same space. We will discuss details of learning protein representation in Section Mask Protein Modeling.\\n\\nFor the Go encoder, we leverage BERT (Devlin et al. (2019)), a Transformer (Vaswani et al. (2017)) based text encoder for biological descriptions in Gene Ontology entities. Specifically, we utilize the pre-trained language model from (Gu et al. (2020)). The encoder takes a sequence of \\\\( N \\\\) tokens \\\\((x_1, \\\\ldots, x_N)\\\\) as inputs, and computes Go representations \\\\( H_{\\\\text{GO}} \\\\in \\\\mathbb{R}^{N \\\\times d} \\\\) by averaging all the token embeddings.\\n\\n3 https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"}"}
{"id": "yfe1VMYAXa4", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Since the relations in Gene Ontology are important for representing the knowledge of biology features, thus, we utilize a relation encoder with the random initialization, and those embeddings of relations will be optimized and updated during pre-training.\\n\\n### 2.2 Knowledge Embedding\\n\\nWe leverage the knowledge embedding (KE) objective to obtain representations in the pre-training process since Gene Ontology is actually a factual knowledge graph. Similar to Bordes et al. (2013), we use distributed representations to encode entities and relations. The knowledge graph here consists of lots of triples to describe relational facts. We define a triplet as $$(h, r, t)$$, where $$h$$ and $$t$$ are head and tail entities, $$r$$ is the relation whose type usually is pre-defined in the schema. Note that there are two different types of nodes $$e_{GO}$$ and $$e_{protein}$$ in our knowledge graph. $$e_{GO}$$ is denoted as nodes that exist in the gene ontology, such as molecular function or cellular component nodes, and $$e_{GO}$$ can be described by annotation texts. $$e_{protein}$$ is the protein node that links to the gene ontology, and we also represent $$e_{protein}$$ with amino acids sequences. Concretely, the triplets in this knowledge graph can be divided into two groups, triple $$GO2GO$$ and triple $$Protein2GO$$. To integrate multi-modal descriptions into the same semantic space and address the heterogeneous information fusion issue, we utilize hybrid encoders introduced in the previous Section. Note that protein encoder and GO encoder represent protein sequence and GO annotations separately.\\n\\n### 2.3 Masked Protein Modeling\\n\\nWe use masked protein modeling to optimize protein representations. The masked protein modeling is similar to masked language modeling (MLM). During model pre-training, we use a 15% probability to mask each token (amino acid) and leverage a cross-entropy loss $$\\\\ell_{MLM}$$ to estimate these masked tokens. We initialize our model with the pre-trained model of ProtBert and regard $$\\\\ell_{MLM}$$ as one of the overall objectives of OntoProtein by jointly training KE (knowledge embedding) and MLM. Our approach is model-agnostic, and other pre-trained models can also be leveraged.\\n\\n### 2.4 Contrastive Learning with Knowledge-Aware Negative Sampling\\n\\nKnowledge embedding (KE) is to learn low-dimensional representations for entities and relations, and contrastive estimation represents a scalable and effective method for inferring connectivity patterns. Note that a crucial aspect of contrastive learning approaches is the choice of corruption distribution that generates hard negative samples, which force the embedding model to learn discriminative representations and find critical characteristics of observed data. However, previous approaches either employ too simple corruption distributions, i.e., uniform, yielding easy uninformative negatives, or sophisticated adversarial distributions with challenging optimization schemes. Thus, in this paper, we propose contrastive learning with knowledge-aware negative sampling, an inexpensive negative sampling strategy that utilizes the rich GO knowledge to sample negative samples. Formally, the KE objective can be defined as:\\n\\n$$\\\\ell_{KE} = -\\\\log \\\\sigma(\\\\gamma - d(h, t)) - \\\\sum_{i=1}^{n} \\\\frac{1}{n} \\\\log \\\\sigma(d(h'_{i}, t'_{i}) - \\\\gamma)$$\\n\\n$$(h'_{i}, t'_{i})$$ is the negative sample, in which head or tail entities are random sampled to construct the corrupt triples. $$n$$ is the number of negative samples, $$\\\\sigma$$ is the sigmoid function, and $$\\\\gamma$$ means the margin. $$d$$ is the scoring function, and we use TransE (Bordes et al. (2013)) for simplicity, where $$d_{r}(h, t) = \\\\|h + r - t\\\\|$$.\"}"}
{"id": "yfe1VMYAXa4", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To avoid plain negative samples, for those \\\\( T_{\\\\text{GO}} \\\\) triples, we sample triples by replacing entities with the same aspect (MFO, CCO, BPO). Finally, we define the negative triple sets \\\\( T'_{\\\\text{GO}} \\\\) and positive triple as \\\\((h,r,t)\\\\), the negative sampling process can be described as follows:\\n\\n\\\\[\\nT'_{\\\\text{GO}} = \\\\{ (h',r,t) \\\\mid h' \\\\in E', h \\\\in E \\\\} \\\\cup \\\\{ (h,r,t') \\\\mid t' \\\\in E', t \\\\in E \\\\}\\n\\\\]\\n\\n(3)\\n\\nwhere \\\\( E' \\\\in \\\\{ E_{\\\\text{MFO}}, E_{\\\\text{CCO}}, E_{\\\\text{BPO}} \\\\} \\\\), and we only replace the tail entities for \\\\( T_{\\\\text{Protein}} \\\\) triples.\\n\\n2.5 PRE-TRAINING OBJECTIVE\\n\\nWe adopt the mask protein modeling object and knowledge embedding objective to construct the overall object of the OntoProtein. We jointly optimize the overall object as follows:\\n\\n\\\\[\\n\\\\ell = \\\\alpha \\\\ell_{KE} + \\\\ell_{MLM}\\n\\\\]\\n\\n(4)\\n\\nwhere \\\\( \\\\alpha \\\\) is the hyper-parameter. Our approach can be embedded into existing fine-tuning scenarios.\\n\\n3 EXPERIMENT\\n\\nExtensive experiments have been conducted to prove the effectiveness of our approach. In the pre-training stage, we construct a new knowledge graph dataset that consists of Gene Ontology and public annotated proteins. Our proposed model is pre-trained with this dataset and evaluated in several downstream tasks. We evaluate OntoProtein in protein function prediction, protein-protein interaction and TAPE benchmark (Rao et al. (2019)).\\n\\n3.1 DATASETS\\n\\nPre-training Dataset\\n\\nTo incorporate Gene Ontology knowledge into language models, we build a new pre-training dataset called ProteinKG25, which is a large-scale KG dataset with aligned descriptions and protein sequences respectively to GO terms and proteins entities. Gene Ontology\\n\\n5 For \\\\( T_{\\\\text{protein}} \\\\) triples, it is also intuitive to replace the proteins with their homologous proteins to generate hard negative triples, and we leave this for future works.\\n\\n6 https://zjunlp.github.io/project/ProteinKG25/\\n\\n7 The structure of GO can be described in terms of a graph, where each GO term is a node, and the relationships between the terms are edges between the nodes.\"}"}
{"id": "yfe1VMYAXa4", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Results on TAPE Benchmark. SS is a secondary structure task that evaluates in CB513. In contact prediction, we test medium- and long-range using P@L/2 metrics. In protein engineering tasks, we test fluorescence and stability prediction using spearman\u2019s $\\\\rho$ metric.\\n\\n| Method          | SS   | SS-Q8 | Contact | Fluorescence | Stability |\\n|-----------------|------|-------|---------|--------------|-----------|\\n| LSTM            | 0.75 | 0.59  | 0.26    | 0.26         | 0.67      |\\n| TAPE Transformer| 0.73 | 0.59  | 0.25    | 0.21         | 0.68      |\\n| ResNet          | 0.75 | 0.58  | 0.25    | 0.17         | 0.21      |\\n| MSA Transformer | -    | 0.73  | -       | -            | -         |\\n| ProtBert        | 0.81 | 0.67  | 0.35    | 0.29         | 0.61      |\\n| OntoProtein     | 0.82 | 0.68  | 0.40    | 0.24         | 0.66      |\\n\\nDownstream Task Dataset\\n\\nWe use TAPE as the benchmark (Rao et al. (2019)) to evaluate protein representation learning. There are three types of tasks in TAPE, including structure, evolutionary, and engineering for proteins. Following Rao et al. (2021a), we select 6 representative datasets including secondary structure (SS), contact prediction to evaluate OntoProtein.\\n\\nProtein-protein interactions (PPI) are physical contacts of high specificity established between two or more protein molecules; we regard PPI as a sequence classification task and use three datasets with different sizes for evaluation. STRING is built by Lv et al. (2021), which contains 15,335 proteins and 593,397 PPIs. We also use SHS27k and SHS148k, which are generated by Chen et al. (2019).\\n\\nProtein function prediction aims to assign biological or biochemical roles to proteins, and we also regard this task as a sequence classification task. We build a new evaluation dataset based on our ProteinKG25 following the standard CAFA protocol (Zhou et al. (2019)). Specifically, we design two evaluation settings, the transductive setting and the inductive setting, which simulate two scenarios of gene annotation in reality. In the transductive setting, the model can generate embeddings of unseen protein entities with entity descriptions. On the contrary, for the inductive setting, those entities have occurred in the pre-training stage. The detailed construction of the dataset can be found in Appendix A.1. As shown in Figure 3, proteins are, on average, annotated by 2 terms in CCO, 4 in MFO, and 3 in BPO, indicating that protein function prediction can be viewed as a multi-label problem. Notably, we notice that leaf GO terms tend to have more specific concepts than non-leaf GO terms. Meanwhile, there exists a challenging long-tail issue for the function prediction task.\\n\\n3.2 RESULTS\\n\\nTAPE BENCHMARK\\n\\nBaselines\\n\\nIn TAPE, we evaluate our OntoProtein compared with five baselines. The first is the model with LSTM encoding of the input amino acid sequence, which provides a simple baseline. The second is TAPE Transformer that provides a basic transformer baseline. We further select ResNet from He et al. (2016) as a baseline. The forth is the MSA Transformer (Rao et al. (2021a)).\"}"}
{"id": "yfe1VMYAXa4", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Protein-Protein Interaction Prediction Results. Breath-First Search (BFS) and Depth-First Search (DFS) are strategies that split the training and testing PPI datasets.\\n\\n| Method          | SHS27k | SHS148k | STRING |\\n|-----------------|--------|---------|--------|\\n| BFS             | 41.43  | 46.12   | 52.12  |\\n| DFS             | 52.03  | 56.68   | 66.82  |\\n| DPPI            | 48.09  | 54.34   | 57.40  |\\n| DNN-PPI         | 53.05  | 64.94   | 58.42  |\\n| PIPR            | 44.48  | 57.80   | 61.83  |\\n| GNN-PPI         | 63.81  | 74.72   | 71.37  |\\n| GNN-PPI (ProtBert) | 70.94  | 73.36   | 70.32  |\\n| GNN-PPI (OntoProtein) | 72.26  | 78.89   | 75.23  |\\n\\nTable 3: Protein Function Prediction Results on three sub-sets with two settings. BPO refers to Biological Process, MFO refers to Molecular Function, and CCO refers to Cellular Component.\\n\\n| Method      | BPO | MFO | CCO |\\n|-------------|-----|-----|-----|\\n| ProtBert    | 0.58| 0.13| 8.47|\\n| OntoProtein | 0.62| 0.13| 8.46|\\n\\nResults\\n\\nWe detail the experimental result on TAPE in Table 1. Concretely, we notice that OntoProtein yields better performance in all token level tests. For the second structure (SS-Q3 and SS-Q8) and contact prediction, OntoProtein outperforms TAPE Transformer and ProtBert, showing that it can benefit from those informative biology knowledge graphs in pre-training. Moreover, OntoProtein can achieve comparable performance with MSA transformer. Note that our proposed OntoProtein does not leverage the information from MSAs. However, with external gene ontology knowledge injection, OntoProtein can obtain promising performance. In sequence level tasks, OntoProtein can achieve better performance than ProtBert in fluorescence prediction. However, we observe that OntoProtein does not perform well in protein engineering, homology, and stability prediction, which are all regression tasks. We think this is due to the lack of sequence-level objectives in our pre-training object, and we leave this for future work.\\n\\nBaselines\\n\\nWe choose four representative methods as baselines for protein-protein interaction. PIPR (Chen et al. (2019)), DNN-PPI (Li et al. (2018)) and DPPI (Hashemifar et al. (2018)) are deep learning based methods. GNN-PPI (Lv et al. (2021)) is a graph neural network based method for better inter-novel-protein interaction prediction. To evaluate our OntoProtein, we replace the initial protein embedding part of GNN-PPI with ProtBERT and OntoProtein as baselines.\\n\\nResults\\n\\nFrom Table 2, we observe that the performance of OntoProtein is better than PIPR, which demonstrates that external structure knowledge can be beneficial for protein-protein interaction prediction. We also notice that our method can achieve promising improvement in smaller dataset SHS2K, even outperforming GNN-PPI and GNN-PPI (ProtBert). With a larger size of datasets, OntoProtein can still obtain comparable performance to GNN-PPI and GNN-PPI (ProtBert).\"}"}
{"id": "yfe1VMYAXa4", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Ablation study of contact prediction. \\n\\n| Sequence Length | TAPE Transformer | LSTM | ResNet | ProtBert | OntoProtein |\\n|-----------------|-----------------|------|--------|----------|-------------|\\n| 6 \u2264 seq < 12    | 0.28            | 0.26 | 0.25   | 0.30     | 0.37        |\\n| 12 \u2264 seq < 24   | 0.35            | 0.36 | 0.34   | 0.40     | 0.46        |\\n| 24 \u2264 seq        | 0.46            | 0.49 | 0.46   | 0.52     | 0.57        |\\n\\nTable 4 illustrates a detailed experimental analysis on the contact prediction. To further analyze the model's performance, we conduct experiments to probe the performance of different sequences. Specifically, protein sequence lengths from short-range (6 \u2264 seq < 12) to long-range (24 \u2264 seq) are tested with three metrics (P@L, P@L/2, P@L/5). We choose several basic algorithms such as LSTM and TAPE transformer as baselines. For fairness, ProtBert is also leveraged for comparison. It can be seen that the performance of OntoProtein exceeds all other methods in all test settings, which is reasonable because the knowledge injected from Gene Ontology is beneficial. Further, we random sample a protein instance from the test dataset and analyze its attention weight of OntoProtein. We conduct visualization analysis as shown in Figure 4 to compare the contacts among amino acids with the contact label matrix.\\n\\n3.3 ANALYSIS\\n\\nApplying techniques from NLP to proteins opens new opportunities to extract information from proteins in a self-supervised, data-driven way. Here we show for the first time that injecting external knowledge from gene ontology can help to learn protein representation better, thus, boosting the downstream protein tasks. However, the gains in our proposed OntoProtein compared to previous pre-trained models using large-scale corpus is still relatively small. Note that the knowledge graph ProteinKG25 can only cover a small subset of all proteins, thus, limiting the advancement. We will continue to maintain the knowledge graph by adding new facts from Gene Ontology. Additionally, previous studies (Liu et al. (2020); Zhang et al. (2021a)) indicate that not all external knowledge are equally beneficial.\"}"}
{"id": "yfe1VMYAXa4", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"beneficial for downstream tasks, and it is necessary to investigate when and how to inject external knowledge into pre-trained models effectively. Finally, our proposed approach can be viewed as jointly pre-training human language and protein (the language of life). Our motivation is to crack the language of life\u2019s code with gene knowledge injected protein pre-training. Our work is but a small step in this direction.\\n\\n4 RELATED WORK\\n\\n4.1 PRE-TRAINED LANGUAGE MODELS\\n\\nUp to now, various efforts have been devoted to exploring large-scale PTMs, either for NLP (Peters et al. (2018); Devlin et al. (2019)), or for CV (Tan & Bansal (2019)). Fine-tuning large-scale PTMs such as ELMo (Peters et al. (2018)), GPT3 (Brown et al. (2020)), BERT (Devlin et al. (2019)), XLNet (Yang et al. (2019)) UniLM (Dong et al. (2019)) for specific AI tasks instead of learning models from scratch has also become a consensus (Han et al. (2021)). Apart from the of large scale language models for natural language processing, there has been considerable interest in developing similar models for proteins (Xiao et al. (2021); Rives et al. (2021)). Rao et al. (2021a) is the first to study protein Transformer language models, demonstrating that information about residue-residue contacts can be recovered from the learned representations by linear projections supervised with protein structures. Vig et al. (2021) performs an extensive analysis of Transformer attention, identifying correspondences to biologically relevant features, and also finds that different layers of the model are responsible for learning different features. Elnaggar et al. (2020) proposes ProtTrans, which explores the limits of up-scaling language models trained on proteins as well as protein sequence databases and compares the effects of auto-regressive and auto-encoding pre-training upon the success of the subsequent supervised training. Human-curated or domain-specific knowledge is essential for downstream tasks, which is extensively studied such as Himmelstein & Baranzini (2015), Smaili et al. (2018), Smaili et al. (2019), Hao et al. (2020), Ioannidis et al. (2020). However these pre-training methods do not explicitly consider external knowledge like our proposed OntoProtein.\\n\\n4.2 KNOWLEDGE-ENHANCED LANGUAGE MODELS\\n\\nBackground knowledge has been considered as an indispensable part of language understanding ((Zhang et al., 2021a; Deng et al., 2021; Li et al., 2021; Zhang et al., 2019a; Yu et al., 2020; Zhu et al., 2021; Zhang et al., 2021b; Chen et al., 2021; Zhang et al., 2022b; Silvestri et al., 2021; Zhang et al., 2021c; Yao et al., 2022; Zhang et al., 2022a)), which has inspired knowledge-enhanced models including ERNIE (Tsinghua) (Zhang et al. (2019b)), ERNIE (Baidu) (Sun et al. (2019)), KnowBERT (Peters et al. (2019)), WKLM (Xiong et al. (2020)), LUKE (Yamada et al. (2020)), KEPLER (Wang et al. (2021b)), K-BERT (Liu et al. (2020)), K-Adaptor (Wang et al. (2021a)), and CoLAKE (Sun et al. (2020)). ERNIE (Zhang et al. (2019b)) injects relational knowledge into the pre-trained model BERT, which aligns entities from Wikipedia to facts in WikiData. KEPLER (Wang et al. (2021b)) jointly optimizes knowledge embedding and pre-trained language representation (KEPLER), which can not only better integrate factual knowledge into PLMs but also effectively learn KE through the abundant information in the text.\\n\\nInspired by these works, we propose OntoProtein that integrates external knowledge graphs into protein pre-training. To the best of our knowledge, we are the first to inject gene ontology knowledge into protein language models.\\n\\n5 CONCLUSION AND FUTURE WORK\\n\\nIn this paper, we take the first step to integrating external factual knowledge from gene ontology into protein language models. We present protein pre-training with gene ontology embedding (OntoProtein), which is the first general framework to integrate external knowledge graphs into protein pre-training. Experimental results on widespread protein tasks demonstrate that efficient knowledge injection helps understand and uncover the grammar of life. Besides, OntoProtein is compatible with the model parameters of lots of pre-trained protein language models, which means that users can directly adopt the available pre-trained parameters on OntoProtein without modifying the architecture.\"}"}
{"id": "yfe1VMYAXa4", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2022.\\n\\nThese positive results point to future work in (1) improving OntoProtein by injecting more informative knowledge with gene ontology selection; (2) extending this approach to sequence generating tasks for protein design.\\n\\nACKNOWLEDGMENTS\\n\\nWe want to express gratitude to the anonymous reviewers for their hard work and kind comments. This work is funded by NSFCU19B2027/NSFC91846204, National Key R&D Program of China (Funding No.SQ2018YFC000004), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Ningbo Natural Science Foundation (2021J190), and Yongjiang Talent Introduction Programme (2021A-156-G).\\n\\nREPRODUCIBILITY STATEMENT\\n\\nOur code and datasets are all available in the https://github.com/zjunlp/OntoProtein for reproducibility. Hyper-parameters are provided in the Appendix A.3.\\n\\nREFERENCES\\n\\nAntoine Bordes, Nicolas Usunier, Alberto Garc\u00eda-Dur\u00e1n, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In Christopher J. C. Burges, L\u00e9on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pp. 2787\u20132795, 2013. URL https://proceedings.neurips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html.\\n\\nNadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, and Michal Linial. Proteinbert: A universal deep-learning model of protein sequence and function. bioRxiv, 2021.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McDermish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\\n\\nMuhao Chen, Chelsea J.-T. Ju, Guangyu Zhou, Xuelu Chen, Tianran Zhang, Kai-Wei Chang, Carlo Zaniolo, and Wei Wang. Multifaceted protein-protein interaction prediction based on siamese residual RCNN. Bioinform., 35(14):i305\u2013i314, 2019. doi: 10.1093/bioinformatics/btz328. URL https://doi.org/10.1093/bioinformatics/btz328.\\n\\nXiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction. CoRR, abs/2104.07650, 2021. URL https://arxiv.org/abs/2104.07650.\\n\\nShumin Deng, Ningyu Zhang, Luoqiu Li, Chen Hui, Huaixiao Tou, Mosha Chen, Fei Huang, and Huajun Chen. Ontoed: Low-resource event detection with ontology embedding. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 2828\u20132839. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.220. URL https://doi.org/10.18653/v1/2021.acl-long.220.\"}"}
{"id": "yfe1VMYAXa4", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "yfe1VMYAXa4", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "yfe1VMYAXa4", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "yfe1VMYAXa4", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2022\\n\\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. KEPLER: A unified model for knowledge embedding and pre-trained language representation. Trans. Assoc. Comput. Linguistics, 9:176\u2013194, 2021b. URL https://transacl.org/ojs/index.php/tacl/article/view/2447.\\n\\nYijia Xiao, Jiezhong Qiu, Ziang Li, Chang-Yu Hsieh, and Jie Tang. Modeling protein using large-scale pretrain language model. CoRR, abs/2108.07435, 2021. URL https://arxiv.org/abs/2108.07435.\\n\\nWenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. Pretrained encyclopaedia: Weakly supervised knowledge-pretrained language model. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=BJlzm64tDH.\\n\\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto. LUKE: deep contextualized entity representations with entity-aware self-attention. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.523. URL https://doi.org/10.18653/v1/2020.emnlp-main.523.\\n\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 5754\u20135764, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html.\\n\\nYunzhi Yao, Shaohan Huang, Ningyu Zhang, Li Dong, Furu Wei, and Huajun Chen. Kformer: Knowledge injection in transformer feed-forward layers. CoRR, abs/2201.05742, 2022. URL https://arxiv.org/abs/2201.05742.\\n\\nHaiyang Yu, Ningyu Zhang, Shumin Deng, Hongbin Ye, Wei Zhang, and Huajun Chen. Bridging text and knowledge with multi-prototype embedding for few-shot relational triple extraction. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pp. 6399\u20136410. International Committee on Computational Linguistics, 2020. doi: 10.18653/v1/2020.coling-main.563. URL https://doi.org/10.18653/v1/2020.coling-main.563.\\n\\nNingyu Zhang, Shumin Deng, Zhanlin Sun, Guanying Wang, Xi Chen, Wei Zhang, and Huajun Chen. Long-tail relation extraction via knowledge graph embeddings and graph convolution networks. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 3016\u20133025. Association for Computational Linguistics, 2019a. doi: 10.18653/v1/n19-1306. URL https://doi.org/10.18653/v1/n19-1306.\\n\\nNingyu Zhang, Shumin Deng, Xu Cheng, Xi Chen, Yichi Zhang, Wei Zhang, and Huajun Chen. Drop redundant, shrink irrelevant: Selective knowledge injection for language pretraining. In Zhi-Hua Zhou (ed.), Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pp. 4007\u20134014. ijcai.org, 2021a. doi: 10.24963/ijcai.2021/552. URL https://doi.org/10.24963/ijcai.2021/552.\\n\\nNingyu Zhang, Qianghuai Jia, Shumin Deng, Xiang Chen, Hongbin Ye, Hui Chen, Huaixiao Tou, Gang Huang, Zhao Wang, Nengwei Hua, and Huajun Chen. Alicg: Fine-grained and evolvable conceptual graph construction for semantic search at alibaba. In Feida Zhu, Beng Chin Ooi, and Chunyan Miao (eds.), KDD '21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pp. 3895\u20133905. ACM, 2021.\"}"}
{"id": "yfe1VMYAXa4", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix A.1 Construction of Protein KG25\\n\\nTo incorporate Gene Ontology knowledge into language models and train OntoProtein, we construct ProteinKG25, a large-scale KG dataset with aligned descriptions and protein sequences respectively to GO terms and proteins entities. We design two evaluation schemes, the transductive and the inductive settings, which simulate two scenarios of gene annotation in reality.\\n\\nWe use the latest Gene Ontology and Gene Annotations released in April 2020. Gene Ontology depicts the relation between GO terms using GO-GO triplet, and Gene Annotation depicts the relations between protein and GO term using Protein-GO triplet. Due to this connectivity, we combine the two structures into a unified knowledge graph. For each GO term in Gene Ontology, we align it to its corresponding name and description and concatenate them by a colon as an entire description. For each protein in Gene annotation, we align it to the Swiss-Prot and extract its corresponding sequence as its description. The final KG contains 4,990,097 triplets (4,879,951 Protein-GO triplets and 110,146 GO-GO triplets), 612,483 entities (565,254 proteins and 47,229 GO terms) and 31 relations.\"}"}
