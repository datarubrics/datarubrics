{"id": "71kocBuhNO", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12: Illustrative examples of NL logical context and questions created using sentences that are generated in \u00a73.2.1 for NM logic.\"}"}
{"id": "71kocBuhNO", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 13: Performance of LLMs in few-shot setting in terms of label-wise accuracy on LogBench(Eval), where A(Yes) and A(No) denote the accuracy for the Yes and No labels, respectively.\"}"}
{"id": "71kocBuhNO", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 14: Evaluation of LLMs in terms of label-wise accuracy on respectively.\\n\\n| LLM    | Rule | Avg | MT | DD | CD | DS | MI | Avg |\\n|--------|------|-----|----|----|----|----|----|-----|\\n| Tk-instruct | 100  | 100 | 75 | 72 | 95 | 25 | 71 | 70  |\\n| FLAN-T5  | 81   | 75  | 64 | 75 | 75 | 88 | 98 | 93  |\\n| LogicBench (Eval) | No | No | No | No | No | No | No | No |\\n| LogicBench (Eval) | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes |\\n\\nLogicBench(Eval) without chain-\"}"}
{"id": "71kocBuhNO", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We know that at least one of the following is true: (1) she is not making a cake for the party and (2) he is writing a report for his boss. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nIf she is making a cake for the party, does this mean that he is writing a report for his boss?\\n\\nYes  \\n\\nNo.\\n\\nLogical reasoning steps:\\n1. We know that at least one of the following is true: (1) she is not making a cake for the party and (2) he is writing a report for his boss.\\n2. The question asks if she is making a cake for the party, does this mean that he is writing a report for his boss?\\n3. Since we do not know which one of the statements (1) and (2) is true, we cannot determine if she making a cake for the party means that he is writing a report for his boss.\\n4. Therefore, the answer is No.\\n\\nWe know that at least one of the following is true: (1) John is not at school and (2) Mary is taking a break from her job. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nIf John is at school, does this entail that Mary is taking a break from her job?\\n\\nYes  \\n\\nNo.\\n\\nLogical reasoning steps:\\n1. We know that at least one of the following is true: (1) John is not at school and (2) Mary is taking a break from her job.\\n2. If John is at school, it means that (1) is not true.\\n3. Since (1) is not true, it means that Mary is not necessarily taking a break from her job.\\n4. Therefore, if John is at school, it does not entail that Mary is taking a break from her job.\\n\\nWe know that at least one of the following is true: (1) Scarlett is not attending the lecture and (2) Dani is studying for her upcoming exam. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nIf Scarlett is attending the lecture, does this mean that Dani is studying for her upcoming exam?\\n\\nYes  \\n\\nNo.\\n\\nLogical reasoning steps:\\n1. We know that at least one of the following is true: (1) Scarlett is not attending the lecture and (2) Dani is studying for her upcoming exam.\\n2. We do not know which one of (1) and (2) is true, or if both are true.\\n3. If Scarlett is attending the lecture, it means that (1) is not true.\\n4. Since we do not know if (2) is true or not, we cannot conclude that Dani is studying for her upcoming exam.\\n5. Therefore, we cannot say for certain if Dani is studying for her upcoming exam if Scarlett is attending the lecture.\\n\\nWe know that at least one of the following is true: (1) Chloe is studying for her exams and (2) Mila is going on vacation. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nIf Chloe isn't studying for her exams, does this entail that Mila is going on vacation?\\n\\nYes  \\n\\nNo.\\n\\nLogical reasoning steps:\\n1. We know that at least one of the following is true: (1) Chloe is studying for her exams and (2) Mila is going on vacation.\\n2. If Chloe isn't studying for her exams, it means that (1) is not true.\\n3. However, we don't have enough information to determine whether (2) is true or not.\\n4. Therefore, we cannot conclude that Mila is going on vacation just because Chloe isn't studying for her exams.\\n\\nWe know that at least one of the following is true: (1) Ava is not attending the music festival and (2) she is studying for her exams. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nIf Ava is attending the music festival, does this mean that she is studying for her exams?\\n\\nYes  \\n\\nNo.\\n\\nLogical reasoning steps:\\n1. We know that at least one of the following is true: (1) Ava is not attending the music festival and (2) she is studying for her exams.\\n2. The question asks if Ava is attending the music festival, does this mean that she is studying for her exams?\\n3. Since we do not know which one of the statements (1) and (2) is true, we cannot conclude that if Ava is attending the music festival, she is studying for her exams.\\n4. Therefore, the answer is No.\\n\\nTable 17: Case study on generating a logical explanation for Disjunctive Syllogism of propositional logic having the worst performance by ChatGPT in this category.\"}"}
{"id": "71kocBuhNO", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LOGIC BENCH: TOWARDS SYSTEMATIC EVALUATION OF LOGICAL REASONING ABILITY OF LARGE LANGUAGE MODELS\\n\\nAnonymous authors\\nPaper under double-blind review\\n\\nABSTRACT\\n\\nRecently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really \\\"reason\\\" over the natural language? This question has been receiving significant research attention and a number of reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. To enable systematic evaluation of logical reasoning, we introduce LogicBench, a natural language question-answering dataset encompassing 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. Key steps of our dataset construction consist of (1) controlled generation of sentences and their negations containing different ontologies, (2) (context, question, answer) triplets creation using heuristically designed templates, and (3) semantic variations of triplets adding more diversity. We present a comprehensive evaluation with a range of LLMs such as GPT-4, GPT-3, ChatGPT, and FLAN-T5 using chain-of-thought prompting in both zero-shot and few-shot settings. Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle on instances involving complex reasoning and negations. Furthermore, they sometimes tend to prioritize parametric knowledge over contextual information and overlook the correct logical reasoning chain. In addition, we also show that LLMs trained using our data exhibit a better understanding of logical reasoning leading to performance improvements on several existing logical reasoning datasets such as LogicNLI, FOLIO, LogiQA, and ReClor.\\n\\nINTRODUCTION\\n\\nLarge language models such as GPT-3 (Brown et al., 2020b), ChatGPT, and FLAN (Wei et al., 2021) have made remarkable progress in NLP research enabling machines to perform a variety of language tasks that were previously thought to be exclusive to humans (OpenAI, 2023; Brown et al., 2020a; Zhao et al., 2023). However, the ability of these LLMs to reason \\\"logically\\\" over natural language text remains under-explored, even though logical reasoning is a fundamental aspect of intelligence and a crucial requirement for many practical applications, such as question-answering systems (Khashabi, 2019) and conversational agents (Beygi et al., 2022). Although several datasets have been proposed (Clark et al., 2021; Tian et al., 2021; Joshi et al., 2020; Saeed et al., 2021) to evaluate the logical reasoning capabilities of LLMs, these datasets are limited in their scope by (1) not evaluating logical reasoning independently of other forms of reasoning such as LogiQA (Liu et al., 2021a) and ReClor (Yu et al., 2020); and (2) evaluating only a single type of logic and covering only few logical inference rules as done in FOLIO (Han et al., 2022) and ProntoQA (Saparov & He, 2023). Thus, our aim in this work is to address the lacuna of having more comprehensive evaluation dataset for LLMs.\\n\\nTo this end, we propose LogicBench, a systematically created question-answering dataset for the evaluation of logical reasoning ability. As illustrated in Figure 1, LogicBench includes a total of\\n\\nData is available at https://anonymous.4open.science/r/LogicBench-EEBB\"}"}
{"id": "71kocBuhNO", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Propositional Logic\\n\\nModus Tollens\\n\\nCommutation\\n\\nHypothetical\\n\\nSyllogism\\n\\nDisjunctive\\n\\nSyllogism\\n\\nDestructive\\n\\nDilemma\\n\\nConstructive\\n\\nDilemma\\n\\nBidirectional\\n\\nDilemma\\n\\nUniversal\\n\\nInstantiation\\n\\nExistential\\n\\nGeneralization\\n\\nNon-monotonic Reasoning\\n\\nDefault Reasoning\\n\\nwith Irrelevant Information\\n\\nDefault Reasoning with Several Defaults\\n\\nDefault Reasoning with Open Domain\\n\\nReasoning with Unknown Exp. 2\\n\\nReasoning with Unknown Exp. 1\\n\\nReasoning with Unknown Exp. 3\\n\\nReasoning with Priorities\\n\\nFigure 1: Comprehensive representation of different inference rules and reasoning patterns covered by propositional, first-order, and non-monotonic logics. To evaluate LLMs, we formulate a binary classification task in LogicBench in which the context represents logical statements and the models have to determine whether a conclusion given in the question is logically entailed by the context. Examples instances of various reasoning patterns are presented in Table 4 and App. C. To construct LogicBench, we use a three-stage procedure (refer to \u00a73). In the first stage, we prompt GPT-3 to generate a variety of coherent natural language sentences consisting of different 'ontologies' (i.e., a collection of concepts such as car, person, and animals) and their corresponding negations (refer to \u00a73.2.1). Then, in the second stage, we generate (context, question, answer) triplets using heuristically designed templates based on the inference rules and patterns. Finally, in the third stage, we generate semantics preserving and inverting variations of these logical rules by incorporating negations. We evaluate a range of LLMs on LogicBench including GPT-4, GPT-3 (Brown et al., 2020b), ChatGPT, FLAN-T5 (Wei et al., 2021), and Tk-instruct (Wang et al., 2022b) using chain-of-thought prompting (Wei et al., 2022). In particular, we measure the accuracy of LLMs predictions on the binary classification task. Our experiments result in several interesting findings such as LLMs often struggle to reason over complex logical contexts and encounter difficulties with inference rules involving negations. Experimental results reveal that these models struggle with respect to many of the inference rules and patterns, suggesting significant room for improvement in their logical reasoning abilities. In addition, we synthetically augment LogicBench and train T5-large. Our initial experimental results show that this improves the logical reasoning ability of existing models leading to performance improvement on other logic datasets, LogicNLI, and FOLIO (\u223c2% on an average), and shows competitive performance on LogiQA and ReClor. In summary, our contributions are:\\n\\n1. Introducing LogicBench, a systematically created dataset to assess the logical reasoning capabilities of LLMs across propositional, first-order, and non-monotonic logics. This benchmark will be publicly available for evaluation and training purposes.\\n\\n2. We propose a three-stage method to construct LogicBench consisting of GPT-3 to generate coherent natural language sentences using prompts and a template-based module to convert them into logical rules. By assessing the performance of existing LLMs, we gain insights into their logical reasoning abilities which further leads to several interesting findings.\\n\\n3. To the best of the authors' knowledge, this is the first benchmark to study non-monotonic reasoning, as well as various inference rules in propositional and first-order logics including hypothetical and disjunctive syllogism; and bidirectional, constructive, and destructive dilemmas in NLP domain.\\n\\nAs LLMs continue to evolve rapidly, it becomes increasingly crucial to evaluate their diverse reasoning capabilities, as well as those of forthcoming LLMs. Past attempts have been made to evaluate the\"}"}
{"id": "71kocBuhNO", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nLogical reasoning abilities of these models. LogiQA (Liu et al., 2021a) and ReClor (Yu et al., 2020) have made notable contributions by compiling multiple-choice questions from standardized examinations that demand diverse forms of logical reasoning. However, in contrast to our logicbench, these datasets involve mixed forms of reasoning and do not focus on assessing logical reasoning in isolation.\\n\\n| Dataset       | Logic Covered | Inference Rules/Axioms Provided with Data | Generation Code Available | Dataset Type          |\\n|---------------|---------------|------------------------------------------|---------------------------|-----------------------|\\n| PL            | \u2713             | \u2717                                        | \u2717                         | Human-annotated       |\\n| FL            | \u2717             | \u2713                                        | \u2717                         | Semi-automated        |\\n| NM            | \u2717             | \u2717                                        | \u2717                         | Human-annotated       |\\n| Ruletaker     | \u2717             | \u2713                                        | \u2717                         | \u2717                     |\\n| LogicNLI      | \u2717             | \u2713                                        | \u2717                         | \u2717                     |\\n| ProofWriter   | \u2713             | \u2713                                        | \u2717                         | \u2717                     |\\n| FOLIO         | \u2717             | \u2713                                        | \u2717                         | \u2717                     |\\n| SimpleLogic   | \u2713             | \u2717                                        | \u2717                         | \u2717                     |\\n| ProntoQA      | \u2717             | \u2713                                        | \u2717                         | \u2717                     |\\n\\nTable 1: Comparison of logicbench with existing datasets\\n\\nA few past attempts have been made to create datasets to evaluate only logical reasoning while excluding other forms of reasoning. For example, CLUTTER (Sinha et al., 2019) covers inductive reasoning, Hahn et al. (2021) covers temporal logic, and Ruletaker (Clark et al., 2021) evaluates whether a transformer-based model emulates deductive reasoning over synthetically generated statements in a limited setting. LogicNLI (Tian et al., 2021) introduced a diagnostic benchmark for FOL reasoning, with the dataset constructed by automatically generating logic expressions and replacing the entity and attribute placeholders.\\n\\nOur proposed dataset is similar (in terms of task formulation) to ProofWriter (Tafjord et al., 2021), FOLIO (Han et al., 2022), and ProntoQA (Saparov & He, 2023) which are QA datasets designed to test reasoning ability. ProofWriter provides multi-hop proofs for each example, while FOLIO gives diverse and complex logical expressions, however, it is only limited to FOL. ProntoQA (Saparov & He, 2023) provides explanation and reasoning steps but is limited to modus ponens in FOL. Nevertheless, several crucial attributes motivated us to create logicbench (see Table 1 for comparison). Additional datasets for evaluating logical reasoning also exist such as SimpleLogic (Zhang et al., 2022) provides a class of logical reasoning problems, TaxiNLI (Joshi et al., 2020) introduces logical taxonomy in NLI task and RuleBert (Saeed et al., 2021) covers only soft logical rules. In summary, logicbench evaluates logical reasoning in isolation and provides diverse inference rules and logic types compared to existing datasets. Extended related work is discussed in App. B.\"}"}
{"id": "71kocBuhNO", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Inference rules and (two) axioms that establish the relationship between premises and conclusions. MP: Modus Ponens, MT: Modus Tollens, HS: Hypothetical Syllogism, DS: Disjunctive Syllogism, CD: Constructive Dilemma, DD: Destructive Dilemma, BD: Bidirectional Dilemma, CT: Commutation, MI: Material Implication, EG: Existential Generalization, UI: Universal Instantiation\\n\\nNon-monotonic (NM) Reasoning\\n\\nIn this work, we analyze a range of logical reasoning templates in NM logics involving \u201cDefault Reasoning,\u201d \u201cReasoning about Unknown Expectations,\u201d and \u201cReasoning about Priorities.\u201d These templates are inspired by the compilation (Lifschitz, 1989) made in 1989 to evaluate the abilities of various non-monotonic logics that were being developed at that time. Below Table 3 shows examples of NM reasoning. Additional examples are given in App. C.4.\\n\\nBasic Default Reasoning\\n\\nContext: Blocks A and B are heavy.\\nHeavy blocks are typically located on the table.\\nA is not on the table.\\nConclusion: B is on the table.\\n\\nContext: Blocks A and B are heavy.\\nHeavy blocks are typically located on the table.\\nA is not on the table.\\nB is red.\\nConclusion: B is on the table.\\n\\nReasoning about Unknown Expectations\\n\\nContext: Blocks A, B, and C are heavy.\\nHeavy blocks are normally located on the table.\\nAt least one of A, B is not on the table.\\nConclusion: C is on the table.\\n\\nExactly one of A, B is not on the table.\\n\\nReasoning about Priorities\\n\\nContext: Jack asserts that block A is on the table.\\nMary asserts that block A is not on the table.\\nWhen people assert something, they are normally right.\\nConclusion: If Mary\u2019s evidence is more reliable than Jack\u2019s, then block A is not on the table.\\n\\nTable 3: Illustrative examples of non-monotonic reasoning adapted from Lifschitz (1989)\\n\\nA key aspect of NM logics is to formalize notions such as \u201cnormally,\u201d \u201ctypically,\u201d and \u201cusually\u201d that are not directly formalizable using classical quantifiers in the first-order setting. The general rule \u201cHeavy blocks are normally located on the table\u201d does not imply that \u201cAll heavy blocks are always located on the table\u201d. Rather, this rule allows for exceptions. Our work explores various NM reasoning patterns, as depicted in Figure 1, to delve deeper into the nuances of this type of reasoning.\\n\\n3.2 Data Creation\\n\\nOur data creation procedure, illustrated in Figure 2, consists of three stages:\\n\\n1. Sentence Generation: Starting with a given prompt, we generate coherent sentences and their negations that incorporate different ontologies.\\n2. NL Conversion: Using predefined templates of reasoning patterns based on their formal expressions, we convert the generated sentences into (context, question, answer) triplets.\\n3. Variation Generation: We generate semantically preserving and inverting variations of these triplets to add more diversity.\\n\\nBy following this method, we construct LogicBench, and examples of generated data corresponding to each logic type and reasoning patterns are presented in App. C.\"}"}
{"id": "71kocBuhNO", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Schematic representation of three-stage procedure for data creation. NL: Natural Language\\n\\n3.2.1 Sentence Generation\\n\\nHere, the first step is to generate sentences with diverse ontologies. An ontology represents a collection of concepts (e.g., car, person, animals, etc.) along with their corresponding associated properties. To generate these sentences, we prompt the GPT-3 model with instructions tailored for each inference rule. The prompt schema, as depicted in Figure 3, comprises three crucial components:\\n\\n- **Definition** provides a detailed explanation of the task and offers a natural language representation of the reasoning pattern for which we are generating sentences.\\n- **Examples** provide sample sentences that need to be generated. We also illustrate how these sentences will be utilized in later stages, emphasizing the importance of coherence and the inclusion of relevant ontological concepts.\\n- **Instruction for Formatting** We provide specific formatting instructions to guide the generation of sentences. An example of a prompt corresponding to the 'Modus Tollens' from PL is presented in App. A for better illustration. Note that our objective at this stage is not to generate logical sentences but rather to generate a diverse and coherent set of sentences that encompass various concepts. We also create a negation sentence corresponding to each generated sentence. In this work, the scope of generating negations is simple (refer to Appendix C for examples), however, negations can be more complicated in the case of logic. These generated sentences will be combined with logical connectives in a later stage to form context and questions.\\n\\n3.2.2 NL Conversion\\n\\nWe leverage the formal expressions of reasoning patterns to create templates that establish the desired NL formulation for each logical connective. For instance, implication: \\\\( p \\\\rightarrow q \\\\) is expressed as \\\"If \\\\( p \\\\), then \\\\( q \\\\),\\\" conjunction: \\\\( p \\\\land q \\\\) as \\\"\\\\( p \\\\) and \\\\( q \\\\),\\\" and disjunction: \\\\( p \\\\lor q \\\\) as \\\"At least one of the following is true: (1) \\\\( p \\\\) and (2) \\\\( q \\\\). Note that we do not know which of (1) and (2) is true. It is possible that only (1) is true, or only (2) is true, or both are true.\\\" since understanding the logical implication of 'or' when integrated into logical formulations posed challenges to both humans and models.\"}"}
{"id": "71kocBuhNO", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"With these established formulations, we proceed to utilize the sentences generated in \u00a73.2.1 to create the context and questions corresponding to reasoning patterns. For instance, let\u2019s consider the \u201cModus Tollens\u201d from PL (\\\\((p \\\\rightarrow q) \\\\land \\\\neg q \\\\vdash \\\\neg p\\\\)), and the \u201cBidirectional Dilemma\u201d from FOL (\\\\(\\\\forall x ((p(x) \\\\rightarrow q(x)) \\\\land (r(x) \\\\rightarrow s(x)) \\\\land (p(a) \\\\lor \\\\neg s(a))) \\\\vdash (q(a) \\\\lor \\\\neg r(a))\\\\)). Table 4 presents examples of logical context and questions for these inference rules and App. D showcases further examples corresponding to each inference rule and patterns from LogicBench.\\n\\n| Inference Rule     | Generated Sentences in Stage 1 | Context and Question                                                                 |\\n|--------------------|--------------------------------|-------------------------------------------------------------------------------------|\\n| Modus Tollens      | \\\\(p: \\\\text{Liam finished his work early.}\\\\)  \\n\\\\(\\\\neg p: \\\\text{Liam did not finish his work early.}\\\\)  \\n\\\\(q: \\\\text{he will order pizza for dinner.}\\\\)  \\n\\\\(\\\\neg q: \\\\text{he will not order pizza for dinner.}\\\\) | Context: If Liam finishes his work early, then he will order pizza for dinner.  \\nQuestion: If he won\u2019t order pizza for dinner, does this imply that Liam didn\u2019t finish his work early? |\\n| Bidirectional Dilemma  | \\\\(p(x): \\\\text{someone drinks lots of water}\\\\)  \\n\\\\(q(x): \\\\text{they will feel hydrated}\\\\)  \\n\\\\(r(x): \\\\text{they eat too much sugar}\\\\)  \\n\\\\(s(x): \\\\text{they will experience a sugar crash}\\\\)  \\n\\\\(p(a): \\\\text{Jane drinks lots of water}\\\\)  \\n\\\\(\\\\neg p(a): \\\\text{Jane does not drink lots of water}\\\\)  \\n\\\\(q(a): \\\\text{she will feel hydrated}\\\\)  \\n\\\\(\\\\neg q(a): \\\\text{she will not feel hydrated}\\\\)  \\n\\\\(r(a): \\\\text{she eats too much sugar}\\\\)  \\n\\\\(\\\\neg r(a): \\\\text{she does not eat too much sugar}\\\\)  \\n\\\\(s(a): \\\\text{she will experience a sugar crash}\\\\)  \\n\\\\(\\\\neg s(a): \\\\text{she will not experience a sugar crash}\\\\) | Context: If someone drinks lots of water, then they will feel hydrated. If they eat too much sugar, then they will experience a sugar crash. We know that at least one of the following is true (1) Jane drinks lots of water and (2) she won\u2019t experience a sugar crash. Note that we do not know which ones of (1) and (2) are true. It might be the case that only (1) is true, or only (2) is true or both are true.  \\nQuestion: If at least one of (1) and (2) is true, can we say, at least one of the following must always be true? (a) she will feel hydrated and (b) she doesn\u2019t eat too much sugar. |\\n\\nTable 4: Illustrative examples of logical context and questions created using sentences that are generated in the first stage \u00a73.2.1.\\n\\n3.2.3 Variation Generation\\n\\nAfter generating the context and questions in \u00a73.2.2, we generate semantically preserving and inverting variations of questions. Let\u2019s consider the example of \u201cModus Tollens\u201d from Table 4, where the question is: \u201cIf he won\u2019t order pizza for dinner, does this imply that Liam didn\u2019t finish his work early?\u201d In this question, we observe two propositions: \\\\(s_1\\\\), representing the statement \u201cLiam didn\u2019t finish his work early,\u201d and \\\\(s_2\\\\), representing the statement \u201cHe won\u2019t order pizza for dinner.\u201d By perturbing these propositions, we can create four possible tuples: \\\\(<s_1, s_2>, <\\\\neg s_1, s_2>, <s_1, \\\\neg s_2>, <\\\\neg s_1, \\\\neg s_2>\\\\). Each tuple represents a combination of true or negation values for the propositions. Although it is possible to create more combinations from \\\\(<s_1, \\\\neg s_1>\\\\), and \\\\(<s_2, \\\\neg s_2>\\\\), we refine and restrict the set of triplets to exclude those that undermine the validity of the inference rule. Moreover, we do not generate variations for the context since it offers no substantial diversity in the dataset. To generate question variations, we replace the propositions in the original question with the corresponding tuples from the generated variations, hence, adding more diversity to LogicBench. This process allows us to create different variations of the question, as illustrated in Figure 2 (Step 3). More examples of question variations are in App. C.\\n\\n3.3 Statistics and Qualitative Analysis\\n\\n| Dataset          | # of Instances | per Axiom | Total # of Instances | Total # of Instances (Including Variations) |\\n|------------------|----------------|-----------|----------------------|--------------------------------------------|\\n| LogicBench(Eval) | 20             | 500       | 1720                 |                                             |\\n| LogicBench(Aug)  | 150            | 3750      | 12908                |                                             |\\n\\nTable 5: Statistics of the LogicBench(Eval) and LogicBench(Aug) Statistics\\n\\nWe introduce two versions of our proposed dataset: LogicBench(Eval) and LogicBench(Aug). Statistics of both versions are presented in Table 5. Here, LogicBench(Eval) is created using the above method along with human-in-loop to ensure the quality of generated data, whereas LogicBench(Aug) is only a synthetically augmented version for training purposes. Here, we use \u201chuman-in-loop\u201d for the authors who conducted qualitative analysis of data. For LogicBench(Eval), out of 1720, 531 samples are for \u2018yes\u2019 and 1189 samples are for \u2018no\u2019. LogicBench(Aug) follows the same ratio.\"}"}
{"id": "71kocBuhNO", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Quality of Data\\n\\nThroughout the data generation phase of LogicBench(Eval), the authors conduct a review of the logical formations to ensure they follow the intended logical structure. We examine each reasoning pattern for any potential discrepancies, ensuring that they were logically sound and correctly represented the intended relationships between propositions. To further support the integrity and reliability of the benchmark, we performed a small-scale human study presented in Appendix L.\\n\\nIn addition to the logical formation, we also dedicated considerable effort to eliminating typos and validating the grammar. Furthermore, we also analyze the diversity in terms of different ontology and the logical nature of the LogicBench(Eval) (presented in App. C.1). We believe that these two versions aim to accommodate different evaluation and training needs to explore logical reasoning.\\n\\n4 RESULTS AND ANALYSIS\\n\\n4.1 EXPERIMENTAL SETUP\\n\\nTask Formulation\\n\\nWe formulate binary classification task using LogicBench to evaluate the logical reasoning ability of LLMs. Let us consider a set of data instances $I_{a,L}$ corresponding to axiom $a$ and logic type $L$. In this set, $i^{th}$ instance is represented as $I_i = \\\\{c_i, Q_i\\\\}$ where $c_i$ represents context and $Q_i = \\\\{q_1, q_2, \\\\ldots, q_n\\\\}$ represents set of question and its variations corresponding to $i^{th}$ instance. As discussed in \u00a73, each context ($c$) represents logical rules (e.g., All cats have fur. Tom is a cat.) and question ($q$) represents the conclusion (e.g., Does Tom have fur?). To each context and question pair, i.e., $<c, q>$, we assign a label from the set $Y = \\\\{Yes, No\\\\}$. We assign a label Yes if the conclusion logically entails the context, otherwise, assign a label No. To evaluate any LLMs on this setup, we provide $<p, c, q>$ as input to predict a label from $Y$ where $p$ is a natural language prompt. In this work, we use chain-of-thought prompts to evaluate LLMs.\\n\\nExperiments\\n\\nWe evaluate a range of prompting models (i.e., GPT-4, GPT-3 (davinci-003) and ChatGPT), and instruction-tuned models (FLAN-T5 (3B) and Tk-instruct (3B)) on LogicBench(Eval). Each model is evaluated in a zero-shot setting where the chain-of-thought prompt is provided to the model without any in-context examples. This approach allows us to determine LLM's inherent ability to do logical reasoning (based on pre-training), as we can not expect that various logical inference rules/patterns will always be made part of prompts. However, we do evaluate these models in a few-shot setting, and present the results in App. E. We also evaluate these models with and without chain-of-thought prompting, presented in App. F. In addition, we present exploratory \u2013 only exploratory because of the limited availability of inference APIs \u2013 analysis over Bard in App. G.\\n\\nIn addition, we trained the T5-large model on the LogicBench(Aug) resulting in a model named LogicT5. Furthermore, we performed fine-tuning on four other logical reasoning datasets: LogiQA, Reclor, LogicNLI, and FOLIO. Our experiments were carried out in two settings: single-task (fine-tuning and evaluation on one dataset) and multi-task (fine-tuning on all four datasets combined, with separate evaluations for each dataset). App. H describes a detailed experimental setup.\\n\\nMetrics\\n\\nHere, we evaluate performance in terms of accuracy corresponding to each label, i.e., $A(Yes)$ and $A(No)$. We evaluate each model on three different chain-of-thought prompts and report average results across these prompts. All prompts used for experiments are described in App. H.\\n\\n4.2 BENCHMARK RESULTS\\n\\nTable 6 represents inference rule-wise performance, and label-wise accuracy ($A(Yes)$ and $A(No)$) corresponding to each LLMs. Here, we focus on analyzing the $A(Yes)$ since the aim is to understand the model's logical reasoning capabilities in answering the question where the conclusion entails the logical context. Table 6 provides valuable insights into the performance of different models on various logic types. For PL, instruction-tuned models FLAN-T5, and Tk-instruct achieve 41.71%, and 30.11% $A(Yes)$, respectively. GPT-3 demonstrates a performance of 39.65%, ChatGPT achieves 46.16%, and GPT-4 shows performance of 44.25%. This indicates the challenge of classical logical reasoning even for larger LLMs like ChatGPT and GPT-4. Moving on to FOL, these models showcase performance accuracy of 64.34%, 60.25%, 52.27%, 66.78%, and 59.53% (on average) for FLAN-T5, Tk-instruct, GPT-3, ChatGPT, and GPT-4, respectively. On the NM reasoning, these models show 7...\"}"}
{"id": "71kocBuhNO", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing instructional prompts to gptk\u2019s language. arXiv preprint arXiv:2109.07830, 2021a.\\n\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773, 2021b.\\n\\nSwaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3505\u20133523, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.246. URL https://aclanthology.org/2022.acl-long.246.\\n\\nOpenAI. Gpt-4 technical report, 2023.\\n\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.\\n\\nMihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, Murad Mohammad, and Chitta Baral. In-BoXBART: Get instructions into biomedical multi-task learning. In Findings of the Association for Computational Linguistics: NAACL 2022, pp. 112\u2013128, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.10. URL https://aclanthology.org/2022.findings-naacl.10.\\n\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080\u20132094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168.\\n\\nPruthvi Patel, Swaroop Mishra, Mihir Parmar, and Chitta Baral. Is a question decomposition unit all we need? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 4553\u20134569, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.302.\\n\\nMohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. RuleBERT: Teaching soft rules to pre-trained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 1460\u20131476, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.110. URL https://aclanthology.org/2021.emnlp-main.110.\\n\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.\\n\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. ICLR, 2021.\\n\\nAbulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=qFVVBzXxR2V.\\n\\nTimo Schick and Hinrich Sch\u00fctze. Exploiting cloze questions for few shot text classification and natural language inference. arXiv preprint arXiv:2001.07676, 2020.\\n\\nKoustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4506\u20134515, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1458. URL https://aclanthology.org/D19-1458.\"}"}
{"id": "71kocBuhNO", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "71kocBuhNO", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui, Satoshi Sekine, Lasha Abzianidze, and Johan Bos. Can neural networks understand monotonicity reasoning?, 2019.\\n\\nQinyuan Ye and Xiang Ren. Zero-shot learning by generating task-specific adapters. arXiv e-prints, pp. arXiv\u20132101, 2021.\\n\\nWenpeng Yin, Jamaal Hay, and Dan Roth. Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. arXiv preprint arXiv:1909.00161, 2019.\\n\\nWeihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. Reclor: A reading comprehension dataset requiring logical reasoning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HJgJtT4tvB.\\n\\nHonghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, and Guy Van den Broeck. On the paradox of learning to reason from data. arXiv preprint arXiv:2205.11502, 2022.\\n\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.\\n\\nRuiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. arXiv preprint arXiv:2104.04670, 2021.\\n\\nBen Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. \\\"going on a vacation\\\" takes longer than \\\"going for a walk\\\": A study of temporal commonsense understanding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3363\u20133369, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1332. URL https://aclanthology.org/D19-1332.\"}"}
{"id": "71kocBuhNO", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Example prompt for Modus Tollens inference rule from PL. Modus tollens is formally represented as \\n\\n\\\\[(p \\\\rightarrow q) \\\\land \\\\neg q \\\\vdash \\\\neg p,\\\\] \\n\\nwhich can be understood in natural language as \u201cIf \\\\(p\\\\) implies \\\\(q\\\\), and we know \\\\(\\\\neg q\\\\), then we can conclude \\\\(\\\\neg p\\\\).\u201d In this prompt, the definition provides a comprehensive description of the inference rule in natural language. To encourage the generation of more relevant and coherent sentences, the prompt includes an examples section that demonstrates how the generated sentences will be utilized in a later stage. This serves as an illustration to guide GPT-3 in producing suitable outputs. In Figure 4, we present three examples involving sentences \\\\(p\\\\) and \\\\(q\\\\), along with their respective contexts and questions. The prompt also includes instructions on how the generated sentences should be formatted.\\n\\n**EXTENDED RELATED WORK**\\n\\nAs LLMs such as GPT-4, and Bard continue to evolve rapidly, it becomes increasingly crucial to evaluate their diverse language capabilities, as well as those of forthcoming LLMs. Recently, many datasets have been created that evaluate different language understanding skills such as pronoun resolution (Sakaguchi et al., 2021; Levesque et al., 2012), commonsense reasoning (Talmor et al., 2019), numerical reasoning (Dua et al., 2019; Patel et al., 2021; Mishra et al., 2022), qualitative reasoning (Tafjord et al., 2019b;a), temporal reasoning (Zhou et al., 2019), and feasibility reasoning (Gupta et al., 2022). Now, we present the advancements in prompt and instruction tuning using LLMs.\\n\\n**Prompt Learning**\\n\\nThe introduction of LLMs has significantly shifted the research trend in NLP to prompt-based learning methodologies (Liu et al., 2021b). Many studies have been conducted to investigate the efficacy of prompt-based learning in various applications including Text classification (Yin et al., 2019), Natural Language Inference (NLI) (Schick & Sch\u00fctze, 2020), and Question Answering (QA) (Jiang et al., 2020), Information Extraction (IE) (Chen et al., 2021; Cui et al., 2021), to name a few. In a recent development, the T0 model employs prompts to achieve zero-shot generalization across various NLP tasks (Sanh et al., 2021). Scao et al. 2021 suggested that the use of prompts could be as valuable as hundreds of data points on average (Le Scao & Rush, 2021).\\n\\n**Instruction Learning**\\n\\nEfrat et al., 2020 (Efrat & Levy, 2020) was focused on whether existing LLMs understand instructions. The same work in the field of instruction by (Hase & Bansal, 2021; Ye & Ren, 2021; Gupta et al., 2021; Zhong et al., 2021) has been proposed to show that models follow natural language instructions. In addition, Weller et al., 2020 (Weller et al., 2020) developed a framework focusing on NLP systems that solve challenging new tasks based on their description. Mishra et al., 2021 (Mishra et al., 2021b) have proposed natural language instructions for cross-task generalization of LLMs. Similarly, PromptSource (Sanh et al., 2021) and FLAN (Wei et al., 2021) have been developed.\"}"}
{"id": "71kocBuhNO", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 20 presents three distinct examples of generated stories that adhere to logical rules given the original context. Below is the prompt used to instruct GPT-3 in generating these stories:\\n\\nRule: [provide rule formulation]\\n\\nContext: [original context from LogicBench]\\n\\nImprove the context in human-like language and make a small story around it. To generate a story, DO NOT change the main character from the context. Make sure that the story adheres to the logical rule presented above. Generate only one paragraph story.\\n\\nHere, we evaluate the GPT-3(davinci-003) model given Prompt 1 from \u00a7H.2 in original and story-like context. It is important to note that only the context has been modified, while the questions remain consistent with those in LogicBench(Eval). Table 21 provides the performance of GPT-3 using original and story-like context. The findings indicate a noticeable drop in GPT-3's performance when the context becomes more human-like. This drop suggests that enhancing the human-like nature of LogicBench(Eval) provides room to further extend our dataset. However, we believe that this more natural context may also become more susceptible to not adhering to logical correctness, requiring manual verification. In contrast, the template-based creation approach employed in LogicBench(Eval) ensures logical correctness in the context.\\n\\nIn this section, we delve into assessing the language model's capability in achieving cross-logic-type generalization, employing T5-Large. Our primary focus is to scrutinize how effectively the model, when fine-tuned extensively on PL, can adapt its knowledge to other logic types (i.e., FOL and NM).\\n\\n| Model   | Training | Model Evaluation |\\n|---------|----------|------------------|\\n|         | PL       | FOL | NM |\\n| PL      | 93.13    | 86.25 | 49.09 |\\n| FOL     | 86.72    | 97.34 | 54.73 |\\n| NM      | 43.28    | 57.66 | 92.73 |\\n\\nTable 22: Results on cross-type logic using T5-Large.\\n\\nTo conduct our experiments, we initially fine-tuned T5-Large separately on PL, FOL, and NM using LogicBench(Aug). Subsequently, we evaluated the model's performance after fine-tuning PL by testing it on FOL and NL, and vice versa using LogicBench(Eval). Our fine-tuning process involved training the T5-Large model for 3 epochs with a batch size of 16, a maximum input length of 1024 tokens, an adaptive learning rate set to $5 \\\\times 10^{-0.5}$, and utilizing the AdamW optimizer. These experiments were executed on NVIDIA RTX A6000 GPUs.\\n\\nTable 22 shows the results of cross-task generalization. Experimental results reveal that the model fine-tuned on PL performs fairly well on FOL ($\\\\sim 86\\\\%$), though it remains lower than the supervised model fine-tuned on FOL ($\\\\sim 97\\\\%$). A similar observation is made for the model fine-tuned on FOL which fairly does well on PL. However, both these models struggle to generalize on non-classical NM reasoning showing lower accuracy ($\\\\sim 52\\\\%$). Additionally, the model fine-tuned on NM reasoning struggles in generalizing to classical logic, PL, and FOL.\\n\\nTo further support the integrity and reliability of the benchmark, we hired three graduate student volunteers to manually check the quality of generated data instances. We randomly selected 5 instances from each inference rule, resulting in a total of 125 instances across 25 reasoning patterns for human evaluation. In particular, annotators are asked to provide binary answers (yes/no) for \u201cValidity of generated context\u201d, and \u201cValidity of generated question\u201d to make sure they adhere to the intended logical structure. Each instance is annotated by three different annotators. The inter-annotator agreement, measured with raw/observed agreement is 0.956. When there was a disagreement between annotators, a majority class was preferred.\"}"}
{"id": "71kocBuhNO", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Questions provided to annotators\\n\u2022 Validity of context: Is the inference rule followed in the context properly? (Answer Choice: Yes or No)\\n\u2022 Validity of question: Does the question include the entailment component of the inference rule and answer align correctly with it? (Answer Choice: Yes or No)\\n\\nTask Instruction (Example of Modus Ponens - FOL):\\nHere is the task instruction example provided to annotators for Modus Ponens. Similar task instructions are provided for other inference rules as well.\\n\\nRule: Modus Ponens\\nMathematical rule/generalized formula: \\n\\\\((p \\\\rightarrow q) \\\\land p \\\\vdash q\\\\)\\n\\nSet of propositions: \\np and q\\n\\nGood Example:\\nContext: If someone is exhausted, then they will take a rest.\\nQuestion: If Jack is exhausted, does this entail that he will take a rest?\\nAnswer: yes\\n\\nExplanation:\\nTo analyze the above example, we will use the asked question to assess the quality of the sample.\\n1. The context has the assumptions as \\\\(p \\\\rightarrow q\\\\) - someone is exhausted implies they will take rest and hence the context follows the assumptions correctly. Thus, the answer to the first question is 'yes'.\\n2. We know that Jack is exhausted (p) and the question states \u201cDoes this entail that he will take rest?\u201d Here, he will take rest indicates entailment part of the inference rule. Furthermore, the answer provided based on a question is correct since it follows the Modus Ponens rule. Thus, the answer to the second question is \u2018yes\u2019.\\n\\nAlong with this instruction, we provide randomly selected 5 instances of Modus Ponens for annotation (similar to other inference rules). Authors closely monitored the annotation process by addressing annotators\u2019 queries regarding their understanding of task instructions.\\n\\nFurther discussion on results\\nLower performance of GPT-4 on PL as compared to NM\\nIn the development of AI, non-monotonic logics were partly developed to formalize natural language constructs, such as \u201cnormally birds fly\u201d, that were not formalizable in a straightforward manner using classical mathematical logics. Thus, while it was difficult for researchers to come up with non-monotonic logics and formalize non-monotonic reasoning, the fact that they were usually motivated by natural language examples, suggests that many of the non-monotonic reasoning aspects are present in the NL text in the wild that is used in the pre-training of the ultra-large LLMs such as GPT4. On the other hand, some of the PL features are counterintuitive to humans such as if we have contradiction (a and a) then everything (even unrelated) is entailed. Also, some PL features are perhaps less prevalent in human writing (on which LLMs are trained) - such as Modes Tollens. Table 6 shows that GPT-4 achieves \u223c97% accuracy (A(yes)) for simple/straightforward inference rules such as MP(FOL) and HS(PL). However, GPT-4 performance dropped to \u223c85% A(Yes) for MT. As the complexity of inference rules increases such as BD, CD, and DD (formal expression presented in Table 2), GPT-4 performance further deteriorates (Table 6). Thus, the evaluations support the hypothesis about the frequency of such reasoning patterns in the \u201ctraining\u201d data.\\n\\nFurther details are presented at https://anonymous.4open.science/r/LogicBench-EEBB/human_eval/readme.md\"}"}
{"id": "71kocBuhNO", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Performance of GPT-4 on PL as compared to FOL\\n\\nResults in Table 6 show an average improvement in FOL results because of LLMs' high accuracy on two axioms, EI and UI. However, when we compare the performance across the six common inference rules between PL and FOL (MT, HS, DS, BD, CD, DD), GPT-4 achieves an average of 49.16% A(Yes) for PL and 39.54% A(Yes) for FOL which shows that our results show the expected behavior. The high accuracy of GPT-4 in handling EI and UI can be attributed to their simplicity. While from human experience and complexity theory, FOL is harder than PL in general; in the LLM context, the crucial factor becomes what kind of logical sentences LLMs are pre-trained on. It seems that LLMs are pre-trained more on simple FOL sentences than on simple PL sentences.\"}"}
{"id": "71kocBuhNO", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: Illustrative examples of NL logical context and questions created using sentences that are generated in \u00a73.2.1 for axioms covered in PL and FOL.\"}"}
{"id": "71kocBuhNO", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\n**Rule**\\n\\n**Context**\\n\\nDRI\\n\\nCats and dogs are mammals. mammals typically have fur. cats don't have fur. dogs are loyal animals.\\n\\n**Question**\\n\\nQ1: Does this imply that dogs have fur? (Yes)\\n\\nQ2: Does this entail that dogs don't have fur? (No)\\n\\n**DRS**\\n\\nJohn and Mary are parents. parents are usually loving and supportive. parents are normally responsible. Mary isn't loving and supportive. John is not responsible.\\n\\n**Question**\\n\\nQ1: Does this imply that Mary is responsible and John is loving and supportive? (Yes)\\n\\nQ2: Does this entail that Mary isn't responsible and John is loving and supportive? (No)\\n\\nQ3: Does this imply that Mary is responsible and John isn't loving and supportive? (No)\\n\\nQ4: Does this entail that Mary isn't responsible and John isn't loving and supportive? (No)\\n\\n**DRD**\\n\\nJenny and Anna are tall. tall people usually play basketball. Anna is possibly an exception to this rule.\\n\\n**Question**\\n\\nQ1: Does this entail that Jenny plays basketball? (Yes)\\n\\nQ2: Does this mean that Jenny doesn't play basketball? (No)\\n\\n**DRO**\\n\\nHummingbirds are birds. Birds migrate south for the winter. Hummingbirds do not migrate south for the winter.\\n\\n**Question**\\n\\nQ1: Does this mean that all other birds than hummingbirds migrate south for the winter? (Yes)\\n\\nQ2: Does this mean that all other birds than hummingbirds don't migrate south for the winter? (No)\\n\\n**RE1**\\n\\nCats, dogs, and horses are animals. animals are usually considered to be intelligent creatures. at least one of the cats or dogs is not considered intelligent.\\n\\n**Question**\\n\\nQ1: Does this entail that horses are considered to be intelligent creatures and exactly one of the cats or dogs is not considered intelligent? (Yes)\\n\\nQ2: Does this mean that horses aren't considered to be intelligent creatures and exactly one of cats or dogs is not considered intelligent? (No)\\n\\nQ3: Does this mean that horses are considered to be intelligent creatures and exactly one of cats or dogs is considered intelligent? (No)\\n\\nQ4: Does this implies that horses aren't considered to be intelligent creatures and exactly one of cats or dogs is considered intelligent? (No)\\n\\n**RE2**\\n\\nCats normally meow. at least one species of cat doesn't meow.\\n\\n**Question**\\n\\nQ1: Does this entail that exactly one species of cat doesn't meow? (Yes)\\n\\nQ2: Does this imply that exactly one species of cat meows? (No)\\n\\n**RE3**\\n\\nCars have four wheels. wheels normally have spokes. at least one wheel does not have spokes.\\n\\n**Question**\\n\\nQ1: Does this imply that cars have four wheels with spokes? (Yes)\\n\\nQ2: Does this mean that cars don't have four wheels with spokes? (No)\\n\\n**RAP**\\n\\nJohn asserts that Sally was in the store. Jane asserts that Sally was not in the store.\\n\\n**Question**\\n\\nQ1: If John's evidence is more reliable than Jane's, does this mean that Sally was in the store? (Yes)\\n\\nQ2: If John's evidence is more reliable than Jane's, does this mean that Sally wasn't in the store? (No)\\n\\nQ3: If John's evidence is less reliable than Jane's, does this entail that Sally was in the store? (No)\\n\\nQ4: If John's evidence is less reliable than Jane's, does this imply that Sally wasn't in the store? (Yes)\\n\\n**Table 10:** Examples of context and question-answer pairs for each axiom of Non-monotonic logic from the LogicBench.\\n\\n20\"}"}
{"id": "71kocBuhNO", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This section discusses the performance of the different LLMs in a few-shot setting on the \\\\textit{LogicBench(Eval)}. Here, we provide a prompt along with four distinct examples (two examples with \\\\textit{Yes} and two examples with \\\\textit{No}). For the fair comparison with Table 6, we analyze an average performance across \\\\(A(\\\\text{Yes})\\\\). Table 13 shows the performance for each inference rule and reasoning patterns achieved by FLAN-T5, Tk-instruct, GPT-3, ChatGPT, and GPT-4.\\n\\nFrom Table 13, we can observe that in-context examples are helpful for GPT-3 and GPT-4 since both models consistently outperform zero-shot baselines by large margins in terms of \\\\(A(\\\\text{Yes})\\\\). In particular, GPT-3 and GPT-4 improve performance by \\\\(\\\\sim 26\\\\%\\\\) and \\\\(\\\\sim 30\\\\%\\\\) (average across all logic) compared to zero-shot baseline, respectively. GPT-4 has been shown to be remarkably good at following the in-context exemplars and mimicking the process to reach the correct conclusions. Thus, leveraging the in-context exemplars, GPT-4 achieves high accuracy in a few-shot setting. As suggested in (Lu et al., 2022), prompt and instruction-tuned models are sensitive to in-context examples. Hence, we see performance variations in Table 13 across all models. Specifically, FLAN-T5 improves an average performance by \\\\(\\\\sim 4\\\\%\\\\) for FOL, however, it shows competitive performance on PL and NM reasoning. Furthermore, Tk-instruct improves an average performance by \\\\(\\\\sim 9\\\\%\\\\) for PL and \\\\(\\\\sim 4\\\\%\\\\) for NM, however, it shows a performance drop on FOL by \\\\(\\\\sim 6\\\\%\\\\). Interestingly, in-context examples in a few-shot setting hamper the performance of ChatGPT by \\\\(\\\\sim 6\\\\%\\\\) for PL, compared to zero-shot. However, ChatGPT improves performance by \\\\(\\\\sim 4\\\\%\\\\), and \\\\(\\\\sim 23\\\\%\\\\) on FOL and NM reasoning, respectively. Improved performance in NM reasoning demonstrates that the inclusion of in-context examples enhances the ability of these models to comprehend the nuanced meanings of logical terms such as \\\"usually\\\" and \\\"typically\\\".\\n\\n\\\\textbf{Evaluation of LogicBench without Chain-of-Thought}\\n\\nIn this section, we discuss experiments carried out in a zero-shot setting without a chain of thought (CoT). All the experiments conducted in the zero-shot setting were performed using three distinct prompts. The reported results in Table 14 represent the average performance across these prompts. The following are the three different prompts utilized in the experiments:\\n\\n- **Prompt 1:** Given the context and question, respond in 'yes' or 'no'.\\n- **Prompt 2:** Answer the given question ONLY in 'yes' or 'no' using logical reasoning ability. DO NOT generate anything as an answer apart from 'yes' and 'no'.\\n- **Prompt 3:** Given context contains rules of logical reasoning in natural language. Answer the given question based on context ONLY in 'yes' or 'no' using logical reasoning ability. DO NOT generate anything as an answer apart from 'yes' and 'no'.\\n\\nFrom Table 14, we can observe that zero-shot CoT prompting (Table 6) consistently improves the performance of FLAN-T5 and Tk-instruct models. Interestingly, GPT-4, GPT-3, and ChatGPT exhibit competitive performance or performance drop when employing CoT-based prompting. It is well-established that CoT generally enhances reasoning task performance (Wei et al., 2022); however, we observed that these models encounter difficulties when dealing with intricate logical rules. Consequently, enabling them to generate rationales can sometimes result in erroneous predictions. However, in the absence of CoT, these models may resort to employing simple heuristics to answer questions rather than following a logical reasoning chain.\"}"}
{"id": "71kocBuhNO", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This section discusses a case study carried out on a Bard (Google) with the subset of the LogicBench dataset for each inference rule and reasoning pattern from PL, FOL, and NM reasoning.\\n\\nTo evaluate this model, we use below prompt:\\n\\nGiven context contains rules of logical reasoning in natural language. Answer the given question based on context ONLY in 'yes' or 'no' using logical reasoning ability. DO NOT generate anything as an answer apart from 'yes' and 'no'.\\n\\n| Reasoning | Bard (PL) | FOL | NM |\\n|-----------|-----------|-----|-----|\\n| Performance | 65.0% | 71.1% | 15.0% |\\n| Average | 51.2% |\\n\\nTable 15: Case study of performance of Bard on a subset of LogicBench.\\n\\nDue to the unavailability of Bard developer API during the evaluation timeline of this paper (August 2023), we manually evaluate a carefully selected subset of LogicBench(Eval). We randomly selected five data samples containing (context, question, 'Yes') triplets since the goal of this evaluation is to see if the model can identify the relationship between logical rules (context) and conclusion (question). The experiment was conducted on a total of 125 samples by combining samples from all 25 inference rules and axioms. Results are presented in Table 15. Bard performs well on the FOL with the highest A(Y es) of 71.1% while it achieves A(Y es) of 65%, and 15% on PL and NM reasoning, respectively. Bard performs poorly on NM reasoning showing that it struggles on understanding the nuance of logical words such as 'normally', 'usually', and 'typically'.\\n\\n**LLaMa-2**\\n\\nWe evaluate LLaMa-2-7B model on LogicBench(Eval) using Zero-shot-CoT. All the experiments conducted in the zero-shot setting were performed using three distinct prompts presented in Appendix H.2. The reported results in Table 16 represent the average performance across these prompts. From the results, we can observe that LLaMa-2 also exhibits similar behavior as other LLMs (Table 6) in terms of performance on LogicBench(Eval).\"}"}
{"id": "71kocBuhNO", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This section represents a case study carried out on an inference rule of each type of logic where ChatGPT is not able to perform well. We use examples generated by Prompt 1 to carry out this manual analysis. Table 17, 18, and 19 represents a case study for Disjunctive Syllogism (DS) from PL, Destructive Dilemma (DD) from FOL, and Default Reasoning with Several Default (DRS) from NM, respectively. From Table 6 (main paper), we can observe that ChatGPT shows poor performance on these inference rules and reasoning patterns, hence, we believe that analysis of logical explanations corresponding to these can give us more insights into the performance of ChatGPT. Here, we prompt the ChatGPT model using Prompt 1 to generate reasoning steps along with a predicted answer. Table 17, 18, and 19 represents five examples of (Context, Question, Correct answer, Logical reasoning steps) pairs generated by ChatGPT.\"}"}
{"id": "71kocBuhNO", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"If someone buys a car, then they will save money on transportation. If they sell their car, then they will have more money to spend on other things. We need to know which ones of (1) and (2) are true. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nIf at least one of (1) and (2) is true, can we say, at least one of the following must always be true? (a) Sarah didn't buy a car and (b) she didn't sell her car.\\n\\nYes\\n\\nNo\\n\\nLogical reasoning steps:\\n\u2022 If someone buys a car, they will save money on transportation.\\n\u2022 If someone sells their car, they will have more money to spend on other things.\\n\\nWe cannot say that (b) must always be true.\\n\\nIf someone takes a walk, then they will get fresh air. If they watch TV, then they will feel relaxed. We know that at least one of (1) and (2) is true. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nIf at least one of (1) and (2) is true, can we say, at least one of the following must always be true? (a) Jill doesn't take a walk and (b) she doesn't watch TV.\\n\\nYes\\n\\nNo\\n\\nBased on the given information, we cannot say that at least one of the following must always be true: (a) Jill doesn't take a walk, and (b) she doesn't watch TV.\\n\\nIf someone takes care of her health, then they will be fit and healthy. If they indulge in unhealthy habits, then they will be prone to diseases. We need to know which ones of (1) and (2) are true. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nIf at least one of (1) and (2) is true, can we say, at least one of the following must always be true? (a) Jenny doesn't take care of her health and (b) she doesn't indulge in unhealthy habits.\\n\\nYes\\n\\nNo\\n\\nLogical reasoning steps:\\n\u2022 From the given context, we have two conditional statements:\\n  \u2013 If someone takes care of her health, then they will be fit and healthy.\\n  \u2013 If they indulge in unhealthy habits, then they will be prone to diseases.\\n\\nWe also know that at least one of the following is true:\\n  \u2013 Jenny won't be fit and healthy.\\n  \u2013 She won't be prone to diseases.\\n\\nWe are asked if we can conclude whether at least one of the following must always be true:\\n  \u2013 Jenny doesn't take care of her health.\\n  \u2013 She doesn't indulge in unhealthy habits.\\n\\nLet's analyze the given options:\\n(a) Jenny doesn't take care of her health: If Jenny is not fit and healthy, then (1) is false. Since (1) is false, (2) must be true. Therefore, we can say that Jenny doesn't take care of her health.\\n(b) She doesn't indulge in unhealthy habits: If Jenny is prone to diseases, then (2) is false. Since (2) is false, (1) must be true. Therefore, we can say that Jenny doesn't indulge in unhealthy habits.\\n\\nIf someone reads a lot, then they will get good grades. If they watch television, then they will have a sound sleep. We need to know which ones of (1) and (2) are true. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nIf at least one of (1) and (2) is true, can we say, at least one of the following must always be true? (a) Jill doesn't read a lot and (b) she doesn't watch television.\\n\\nYes\\n\\nNo\\n\\nLogical reasoning: 1. We know that if someone reads a lot, then they will get good grades. 2. We also know that if someone watches television, then they will have a sound sleep.\\n\\nIf someone takes a test, then they will get a good grade. If they do not revise enough, then they will not score well. We need to know which ones of (1) and (2) are true. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nIf at least one of (1) and (2) is true, can we say, at least one of the following must always be true? (a) John doesn't take a test and (b) he revises enough.\\n\\nYes\\n\\nNo\\n\\nLogical reasoning steps:\\n1. We know that if someone takes a test, they will get a good grade, which can be represented as: Test \u2192 Good Grade.\\n2. We also know that if someone doesn't revise enough, they will not score well, which can be represented as: Not Enough Revision \u2192 Not Score Well.\\n3. We are given that at least one of the following is true: (1) John won't get a good grade and (2) he will score well.\\n4. This means that either Test \u2192 Good Grade is true (2 is true) or Not Enough Revision \u2192 Not Score Well is true (1 is true), or both are true.\\n5. Now, let's analyze the implications of (a) and (b):\\n(a) John doesn't take a test: If John doesn't score well, then (2) is false. Since (2) is false, (1) must be true. Therefore, we cannot say that at least one of these statements must always be true.\"}"}
{"id": "71kocBuhNO", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"John and Mary are parents. Parents are usually loving and supportive. Parents are normally responsible. Mary isn\u2019t loving and supportive. John is not responsible.\\n\\nDoes this imply that Mary is responsible and John is loving and supportive?\\n\\nNo.\\n\\n**Logical Reasoning Steps:**\\n1. The context states that parents are usually loving and supportive.\\n2. The context also states that parents are normally responsible.\\n3. The context further states that Mary isn\u2019t loving and supportive.\\n4. The context states that John is not responsible.\\n5. Based on the context, it can be inferred that Mary is not responsible because she is not loving and supportive.\\n6. It can also be inferred that John is not loving and supportive because he is not responsible.\\n7. Therefore, the context does not imply that Mary is responsible and John is loving and supportive.\\n\\nApples and bananas are fruits. Fruits are usually sweet. Fruits are normally juicy. Apples are not sweet. Bananas are not juicy.\\n\\nDoes this mean that bananas are sweet and apples are juicy?\\n\\nNo.\\n\\n**Logical Reasoning Steps:**\\n1. According to the context, apples and bananas are fruits.\\n2. The context also states that fruits are usually sweet and normally juicy.\\n3. However, it is mentioned that apples are not sweet and bananas are not juicy.\\n4. Therefore, based on the given information, we can conclude that bananas are not sweet and apples are not juicy.\\n5. The question asks if bananas are sweet and apples are juicy.\\n6. Based on the reasoning steps, the answer is No, bananas are not sweet and apples are not juicy.\\n\\nCats and dogs are animals. Animals have four legs. Animals have fur. Cats have no fur. Dogs are not four-legged.\\n\\nDoes this mean that dogs have fur and cats have four legs?\\n\\nNo.\\n\\n**Logical Reasoning Steps:**\\n1. Cats and dogs are animals.\\n2. Animals have four legs.\\n3. Animals have fur.\\n4. Cats have no fur.\\n5. Dogs are not four-legged.\\n\\nBirds and fish are animals. Animals have four legs. Animals are normally omnivores. Birds don\u2019t have four legs. Fish is not an omnivore.\\n\\nDoes this imply that fish have four legs and birds are omnivores?\\n\\nNo.\\n\\n**Logical Reasoning Steps:**\\n1. The context states that animals have four legs.\\n2. Birds are animals, but they don\u2019t have four legs.\\n3. Therefore, it can be concluded that not all animals have four legs.\\n4. The context also states that animals are normally omnivores.\\n5. Fish is an animal, but it is not an omnivore.\\n6. Therefore, it can be concluded that not all animals are omnivores.\\n7. Based on the reasoning steps, it cannot be implied that fish have four legs and birds are omnivores.\\n\\nBlocks A and B are heavy. Heavy blocks are normally located on the table. Heavy blocks are normally red. A is not on the table. B is not red.\\n\\nDoes the context imply that B is on the table and A is red in color?\\n\\nNo, the context does not imply that B is on the table and A is red in color.\\n\\n**Logical reasoning steps:**\\n1. The context states that heavy blocks are normally located on the table.\\n2. It also states that A is not on the table.\\n3. Therefore, we can conclude that A is not located on the table.\\n4. The context also states that heavy blocks are normally red.\\n5. It further states that B is not red.\\n6. Therefore, we can conclude that B is not a heavy block.\\n7. Since B is not a heavy block, it does not necessarily need to be located on the table.\\n8. Additionally, the context does not provide any information about the color of A.\\n9. Therefore, we cannot conclude that A is red in color.\\n10. In conclusion, the context does not imply that B is on the table and A is red in color.\"}"}
{"id": "71kocBuhNO", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Story\\n\\nHS\\nIf Jerry goes for jogging, then he will work up a sweat. If he will work up a sweat, then he will drink lots of water.\\n\\nJerry was determined to stay healthy and fit so he made sure to go for jogging every day. Sure enough, after his jogs he would work up a sweat and, in order to stay hydrated, would drink lots of water.\\n\\nDS\\nWe know that at least one of the following is true (1) Ava is not attending the music festival and (2) she is studying for her exams. Note that we do not know which ones of (1) and (2) are true. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nAva had an important decision to make - attend the music festival or study for her exams. She couldn\u2019t do both, so she had to choose. In the end, she decided that either she would attend the music festival or she would study for her exams \u2013 or both, but only one at a time. She wasn\u2019t sure which one she wanted to do, but she was determined to do one of them.\\n\\nCD\\nIf Lisa eats breakfast in the morning, then she will have energy to start my day. If she sleeps in late, then she will feel groggy in the morning. We know that at least one of the following is true (1) Lisa eats breakfast in the morning and (2) she sleeps in late. Note that we do not know which ones of (1) and (2) are true. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nLisa always faces a tough challenge every morning: eat breakfast or sleep in late. On one hand, if she eats breakfast in the morning, she will have enough energy to get her day started. On the other hand, if she sleeps in late, she will feel groggy in the morning. Despite Lisa\u2019s dilemma, one thing is for certain \u2013 either she eats breakfast or she sleeps in late.\\n\\nTable 20: Examples of stories generated from original context for three distinct propositional rules.\\n\\n| Rule  | Original Story-like | Score |\\n|-------|--------------------|-------|\\n| HS    |                    | 52.63 |\\n| DS    |                    | 50.00 |\\n| CD    |                    | 39.58 |\\n| DD    |                    | 33.33 |\\n| BD    |                    | 39.58 |\\n| MT    |                    | 45.45 |\\n| MI    |                    | 25.68 |\\n| CT    |                    | 45.45 |\\n\\nTable 21: Performance of GPT-3 given Prompt 1 (\u00a7H.2) in terms of label-wise accuracy on PL with original and story-like context from LogicBench (Eval).\"}"}
{"id": "71kocBuhNO", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"were built for leveraging instructions and achieving zero-shot generalization on unseen tasks. Moreover, Parmar et al., 2022 (Parmar et al., 2022) shows the effectiveness of instructions in multi-task settings for the biomedical domain. Furthermore, Mishra et al., 2021 (Mishra et al., 2021a) discuss the impact of task instruction reframing. Min et al., 2021 (Min et al., 2021) introduce a framework to better understand in-context learning. Ouyang et al., 2022 (Ouyang et al., 2022) propose the InstructGPT model that is fine-tuned with human feedback to follow instructions. Wang et al., 2022 (Wang et al., 2022a) has developed an instruction-based multi-task framework for few-shot Named Entity Recognition (NER) tasks. In addition, many approaches have been proposed to improve model performance using instructions (Wu et al., 2022; Lin et al., 2021; Wang et al., 2022b; Luo et al., 2022; Kuznia et al., 2022; Patel et al., 2022; Mishra & Nouri, 2022).\\n\\nLogic and NLI Datasets\\n\\nFraCas (Bernardy & Chatzikyriakidis, 2020) offers a unique approach to temporal semantics by converting syntax trees into logical formulas tailored for inference, emphasizing temporal elements such as references, adverbs, aspectual classes, and progressives. The Monotonicity Entailment Dataset (MED) (Yanaka et al., 2019) dives deep into monotonicity reasoning within NLI, probing the synergy between lexical and syntactic structures and spotlighting inherent challenges in both upward and downward monotonic reasoning trajectories. The SICK (Marelli et al., 2014) dataset, with its foundation in 10,000 English sentence pairs, is designed to rigorously evaluate semantic relatedness and entailment, leveraging crowdsourced annotations for precision. HANS, or Heuristic Analysis for NLI Systems (McCoy et al., 2019), stands out by rigorously scrutinizing the dependability of NLI models, putting the spotlight on potential pitfalls tied to syntactic heuristics such as lexical overlap. Lastly, CAD (Vidgen et al., 2021) introduces a meticulously crafted dataset from Reddit entries, targeting the detection of online abuse. This dataset boasts six distinct primary categories, context-aware annotations, provided rationales, and a rigorous group-adjudication methodology ensuring high-quality annotations.\\n\\nEXAMPLES OF DATA INSTANCES\\n\\nThis section provides examples of (context, question, answer) tuples corresponding to each inference rule and reasoning pattern. Additionally, it highlights the diverse range of question variations within the dataset associated with each inference rule and reasoning pattern.\\n\\nC.1 WORD CLOUD\\n\\nFigure 5 provides a word cloud derived from the LogicBench (Eval). This word cloud highlights the logical nature and diversity of our evaluation dataset. Words such as 'if', 'normally', 'usually', and 'then' are prominently featured, suggesting their frequent use in the dataset, and suggesting the logical nature of the dataset. Moreover, we can also observe several words consisting of different ontologies such as 'cat', 'car', 'garden', and many more, suggesting diversity in the dataset.\\n\\nC.2 PROPOSITIONAL LOGIC (PL)\\n\\nHere, we discuss examples of each inference rule present in the PL of the LogicBench as shown in Table 8. Table 8 has context related to the inference rule and different variations of the question according to the rule. For instance, the first row of Table 8 shows the example for inference rule, Hypothetical Syllogism (HS), formally expressed as \\\\(((p \\\\rightarrow q) \\\\land (q \\\\rightarrow r)) \\\\vdash (p \\\\rightarrow r)\\\\). The context represents the premise, i.e., \\\\(((p \\\\rightarrow q) \\\\land (q \\\\rightarrow r))\\\\), and the first question (Q1) represents the conclusion, i.e., \\\\(p \\\\rightarrow r\\\\). Hence, Q1 is labeled as \\\"Yes\\\" since it supports the conclusion given the logical context. Furthermore, Q2 to Q4 represent different variations of the question by utilizing the variables \\\\((p, \\\\neg p, r, \\\\neg r)\\\\). For the HS, given the provided context, Q2 to Q4 contain the variations \\\\(\\\\neg p \\\\rightarrow r\\\\), \\\\(p \\\\rightarrow \\\\neg r\\\\), and \\\\(\\\\neg p \\\\rightarrow \\\\neg r\\\\), respectively, and are labeled as \\\"No\\\" since they do not support the conclusion.\\n\\nC.3 FIRST-ORDER LOGIC (FOL)\\n\\nHere, we discuss examples of each inference rule and two axioms (i.e., Existential Instantiation and Universal Instantiation) present in the FOL from the LogicBench as shown in Table 9. Existential Generalization (EG), formally expressed as \\\\(P(a) \\\\Rightarrow \\\\exists x P(x)\\\\) indicates that there is an element \\\\(a\\\\)\"}"}
{"id": "71kocBuhNO", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Word cloud of context present in the LB(eval)\\nin the domain for which \\\\( P(a) \\\\) is true, then we know that \\\\( \\\\exists x P(x) \\\\) is true.\\n\\nUniversal Instantiation formally expressed as \\\\( \\\\forall x A \\\\Rightarrow A \\\\{ x \\\\rightarrow a \\\\} \\\\) indicates that a statement holds true for all instances \\\\( x \\\\) within a specific category \\\\( A \\\\), hence it is also true for specific instance \\\\( a \\\\).\\n\\nTable 9 represents context related to the inference rule and variations of the question. The process of generating data instances for FOL follows a similar approach to that of PL. For example, the first row of Table 9 shows the example for axiom, Existential Instantiation (EI) formally expressed as \\\\( \\\\exists x P(x) \\\\Rightarrow P(a) \\\\). The context represents the initial premise \\\\( \\\\exists x P(x) \\\\) and the first question (Q1) represents the conclusion, i.e., \\\\( P(a) \\\\). Hence, Q1 is labeled as \\\"Yes\\\" since it supports the conclusion given the logical context. Furthermore, we generate the only variant of the question based on \\\\( \\\\neg P(a) \\\\) and labeled it as \\\"No\\\" since it does not support the conclusion.\\n\\nC.4 NON-MONOTONIC (NM) REASONING\\n\\nHere, we discuss examples of each reasoning pattern present in the NM reasoning from the LogiBench as shown in Table 10. Table 10 has context related to the reasoning pattern and different variants of the question. For example, the first row of Table 10 shows the example for Default Reasoning with Irrelevant Information (DRI). For this reasoning, based on the given context, there are also two possible variations of the question where one with a correct conclusion labeled as \\\"Yes\\\" and another with an incorrect conclusion labeled as \\\"No\\\".\\n\\nDEEXAMPLES OF NL CONVERSION\\n\\nThis section illustrates the way natural language logical context and questions are created using the generated sentence in Stage 1. Table 11 shows examples of how context and question are generated from sentences corresponding to each inference rule for PL and FOL. Similarly, Table 12 shows examples of NM reasoning. From Table 11, we can see an example of sentence pairs \\\\( (p, q) \\\\) and their corresponding negation pairs \\\\( (\\\\neg p, \\\\neg q) \\\\) for the 'modus tollens' inference rule from PL. These pairs are utilized to generate logical context and questions. Similarly, in the second row, we have four generic rules with variable \\\\( x \\\\) (\\\\( p(x), q(x), r(x), s(x) \\\\)) and their specific cases (i.e., \\\\( x = a \\\\)), along with their respective negative sentence pairs \\\\([ (p(a), \\\\neg p(a)), (q(a), \\\\neg q(a)), (r(a), \\\\neg r(a)), (s(a), \\\\neg s(a)) ]\\\\).\\n\\nThese examples demonstrate the generation of logical context and questions for the FOL inference rule called 'Bidirectional Dilemma (BD)', as shown in Table 11. From Table 12, the first row presents an example of context and questions generated from a sentence pair for the 'Default Reasoning with Irrelevant Information (DRI)' from NM reasoning. In this specific instance, the generated sentences are \\\\( (p, q, r, s, t) \\\\), and the negation is only required for the sentence \\\\( t \\\\). Therefore, there is a single negation pair \\\\( (t, \\\\neg t) \\\\), which is used to generate questions specific to the 'DRI'.\\n\\n17\"}"}
{"id": "71kocBuhNO", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Examples of context and question-answer pairs for each axiom of Proportional logic from the LogicBench; HS: Hypothetical Syllogism, DS: Disjunctive Syllogism, CD: Constructive Dilemma, DD: Destructive Dilemma, BD: Bidirectional Dilemma, MT: Modus Tollens, MI: Material Implication, CT: Commutation\"}"}
{"id": "71kocBuhNO", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rule\\n\\nContext\\n\\nQuestion\\n\\nUI\\n\\nAll students need to take an exam to complete their degree. Reema is a student.\\n\\nQ1: Does Reema need to take an exam to complete her degree? (Yes)\\n\\nQ2: Does Reema need not to take an exam to complete her degree? (No)\\n\\nEG\\n\\nJames won the marathon race\\n\\nQ1: Does this imply that someone won the marathon race? (Yes)\\n\\nQ2: Does this mean that no one won the marathon race? (No)\\n\\nMP\\n\\nIf someone is exhausted, then they will take a rest.\\n\\nQ1: If Jack is exhausted, does this entail that he will take a rest? (Yes)\\n\\nQ2: If Jack isn't exhausted, does this imply that he won't take a rest? (No)\\n\\nQ3: If Jack is exhausted, does this entail that he won't take a rest? (No)\\n\\nQ4: If Jack isn't exhausted, does this entail that he will take a rest? (No)\\n\\nHS\\n\\nIf someone buys all the necessary supplies, then they can start the project. If they can start the project, then they will finish it on time.\\n\\nQ1: If Lily bought all the necessary supplies, does this mean that she will finish it on time? (Yes)\\n\\nQ2: If Lily didn't buy all the necessary supplies, does this imply that she won't finish it on time? (No)\\n\\nQ3: If Lily bought all the necessary supplies, does this entail that she won't finish it on time? (No)\\n\\nQ4: If Lily didn't buy all the necessary supplies, does this imply that she will finish it on time? (No)\\n\\nDS\\n\\nWe know that at least one of the following is true (1) they can go to a museum and (2) they can visit a park. Note that we do not know which ones of (1) and (2) are true. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nQ1: If Jill can't go to a museum, does this imply that she can visit a park? (Yes)\\n\\nQ2: If Jill can't go to a museum, does this entail that she can't visit a park? (No)\\n\\nQ3: If Jill can go to a museum, does this entail that she can't visit a park? (No)\\n\\nQ4: If Jill can go to a museum, does this imply that she can visit a park? (No)\\n\\nCD\\n\\nIf someone is painting a picture, then they will frame it. If they are writing a story, then they will publish it. We know ... know which ones of (1) and (2) are true. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nIf at least one of (1) and (2) is true, can we say, at least one of the following must always be true?\\n\\nQ1: (a) He will frame it. and (b) She will publish it. (Yes)\\n\\nQ2: (a) he won't frame it. and (b) She will publish it. (No)\\n\\nQ3: (a) He will frame it. and (b) she won't publish it. (No)\\n\\nQ4: (a) he won't frame it. and (b) she won't publish it. (No)\\n\\nDD\\n\\nIf someone takes care of her health, then they will be fit and healthy. If they indulge in unhealthy habits, then they will ... know which ones of (1) and (2) are true. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nIf at least one of (1) and (2) is true, can we say, at least one of the following must always be true?\\n\\nQ1: (a) Jenny doesn't take care of her health and (b) she doesn't indulge in unhealthy habits (Yes)\\n\\nQ2: (a) Jenny takes care of her health and (b) she indulges in unhealthy habits (No)\\n\\nQ3: (a) Jenny doesn't take care of her health and (b) she indulges in unhealthy habits (No)\\n\\nQ4: (a) Jenny takes care of her health and (b) she doesn't indulge in unhealthy habits (No)\\n\\nBD\\n\\nIf someone drinks lots of water, then they will feel hydrated. If they eat too much sugar, then they will experience a ... know which ones of (1) and (2) are true. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nIf at least one of (1) and (2) is true, can we say, at least one of the following must always be true?\\n\\nQ1: (a) she will feel hydrated and (b) she doesn't eat too much sugar (Yes)\\n\\nQ2: (a) she won't feel hydrated and (b) she eats too much sugar (No)\\n\\nQ3: (a) she will feel hydrated and (b) she eats too much sugar (No)\\n\\nQ4: (a) she won't feel hydrated and (b) she doesn't eat too much sugar (No)\\n\\nMT\\n\\nIf someone visits the park, then they have to wear a mask.\\n\\nQ1: If he doesn't have to wear a mask, does this imply that John doesn't visit the park? (Yes)\\n\\nQ2: If he doesn't have to wear a mask, does this entail that John visits the park? (No)\\n\\nQ3: If he has to wear a mask, does this imply that John doesn't visit the park? (No)\\n\\nQ4: If he has to wear a mask, does this imply that John visits the park? (No)\\n\\nTable 9: Examples of context and question-answer pairs for each axiom of First order logic from the LogicBench.\"}"}
{"id": "71kocBuhNO", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We prompt ChatGPT to generate reasoning steps along with predictions. For PL, we observe that within NM reasoning. Notably, when it comes to the rule of default reasoning, ChatGPT fails to grasp while the model can effectively reason the initial section of the\\n\\nA\\n\\nanalysis of generated explanations for each logic type are presented in App. I.\\n\\nFOL, ChatGPT encounters challenges in comprehending longer logical contexts, resulting in a lack of confidence in establishing the relationship between given propositions. Furthermore, to derive\\n\\nresponse (i.e., Yes)\\n\\nbetter on questions with a negative response (i.e., No)\\n\\nFrom Table 6, we can observe that models struggle more with inference rules of PL compared\\n\\nUnder review as a conference paper at ICLR 2024\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl\\n\\npl"}
{"id": "71kocBuhNO", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Negations are hard to understand when embedded with logical rules. Regarding PL and FOL, it is apparent that the models struggle more with the DS, DD, and MT inference rules. A closer look at Table 2 reveals that all of these axioms include examples where the models need to draw conclusions based on negated premises. This indicates that the models encounter difficulties when negated premises are introduced. Additionally, the performance of the models tends to decrease when inference rules involve negations.\\n\\nLonger inference rules are still challenging. Table 6 indicates that the models face challenges when handling longer rules, such as BD, CD, and DD, both in PL and FOL. Hence, it can be concluded that these models struggle with longer logical dependencies in the premise, particularly when a higher number of propositions are present. In the case of NM reasoning, the models exhibit lower performance in DRS of NM reasoning, indicating that a higher number of rules in the context often leads to more frequent mistakes.\\n\\nLarge models are better logical reasoners. Based on the observed performance from Table 6, it becomes evident that larger model sizes and extensive pre-training data contribute to a better understanding of logical aspects. Consequently, models with larger sizes tend to exhibit higher performance across different types of logic. Nonetheless, the average performance remains at around 52.83%, indicating room for improvement in these models' logical comprehension capabilities.\\n\\nEffect on other logic datasets\\n\\nTable 7 represents the accuracy comparison between LogicT5 and baseline T5-large in both single-task and multi-task settings.\\n\\n| Methods      | Models | LogiQA | FOLIO | LogicNLI | ReClor |\\n|--------------|--------|--------|-------|----------|--------|\\n| Single-Task  | T5-large | 16.8   | 69.6  | 82.3     | 35.4   |\\n|              | LogicT5 | 16.9   | 71.2  | 84.4     | 36.8   |\\n| Multi-Task   | T5-large | 21.8   | 83.8  | 68.2     | 42.8   |\\n|              | LogicT5 | 19.7   | 85.6  | 69.8     | 40.0   |\\n\\nTable 7: Performance comparison between LogicT5 and baseline T5-large in terms of accuracy. The results indicate that training LLMs on LogicBench(Aug) has a greater impact on logic datasets that primarily focus on logical reasoning, such as FOLIO and LogicNLI. Hence, we can observe that LogicT5 consistently outperforms the baseline for LogicT5 and FOLIO. However, LogiQA and ReClor encompass other forms of reasoning in addition to logical reasoning, hence, LogicT5 demonstrates competitive performance on them. More analysis is presented in Appendix M.\\n\\nCONCLUSIONS\\n\\nTo study the logical reasoning ability of LLMs, we introduced a novel benchmark called LogicBench which consists of 25 distinct inference rules and reasoning patterns covering propositional, first-order, and non-monotonic logics. We released two versions of the dataset: LogicBench(Eval) and LogicBench(Aug). LogicBench(Eval) serves as a high-quality, cost-effective, and reliable dataset for evaluating LLMs, while LogicBench(Aug) can be utilized for training purposes. Through comprehensive experiments, we showed that models such as GPT-3 and ChatGPT do not perform well on LogicBench, even though they require the application of only a single inference rule in positive (i.e., label 'Yes') data instance. Furthermore, we demonstrated that LLMs trained using LogicBench(Aug) showcase an improved understanding of logical reasoning, resulting in a better performance on existing logic datasets. Though LogicBench facilitates the evaluation and improvement of the logical reasoning ability of LLMs, the linguistic diversity of context presented in LogicBench can be improved by leveraging LLMs to generate a more story-like context. To this extent, we conducted a preliminary study over PL, and our findings are presented in App. J. This indicates the room for making LogicBench even more challenging. Furthermore, LogicBench can be further extended by incorporating other inference rules and logic types; and having data instances that require applications of multiple inference rules.\"}"}
{"id": "71kocBuhNO", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jean-Philippe Bernardy and Stergios Chatzikyriakidis. Fracas: Temporal analysis, 2020.\\n\\nSajjad Beygi, Maryam Fazel-Zarandi, Alessandra Cervone, Prakash Krishnan, and Siddhartha Jonnalagadda. Logical reasoning for task oriented dialogue systems. In Proceedings of the Fifth Workshop on e-Commerce and NLP (ECNLP 5), pp. 68\u201379, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.ecnlp-1.10. URL https://aclanthology.org/2022.ecnlp-1.10.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020b.\\n\\nXiang Chen, Xin Xie, Ningyu Zhang, Jiahuan Yan, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. Adaprompt: Adaptive prompt-based finetuning for relation extraction. arXiv e-prints, pp. arXiv\u20132104, 2021.\\n\\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 3882\u20133890, 2021.\\n\\nLeyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue Zhang. Template-based named entity recognition using bart. arXiv preprint arXiv:2106.01760, 2021.\\n\\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2368\u20132378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1246. URL https://aclanthology.org/N19-1246.\\n\\nAvia Efrat and Omer Levy. The turking test: Can language models understand instructions? arXiv preprint arXiv:2010.11982, 2020.\\n\\nHimanshu Gupta, Neeraj Varshney, Swaroop Mishra, Kuntal Kumar Pal, Saurabh Arjun Sawant, Kevin Scaria, Siddharth Goyal, and Chitta Baral. \u201cjohn is 50 years old, can his son be 65?\u201d evaluating nlp models\u2019 understanding of feasibility. arXiv preprint arXiv:2210.07471, 2022.\\n\\nTanmay Gupta, Amita Kamath, Aniruddha Kembhavi, and Derek Hoiem. Towards general purpose vision systems. arXiv preprint arXiv:2104.00743, 2021.\\n\\nChristopher Hahn, Frederik Schmitt, Jens U. Kreber, Markus Norman Rabe, and Bernd Finkbeiner. Teaching temporal logics to neural networks. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=dOcQK-f4byz.\\n\\nSimeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, et al. Folio: Natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840, 2022.\\n\\nPeter Hase and Mohit Bansal. When can models learn from explanations? a formal framework for understanding the roles of explanation data. arXiv preprint arXiv:2102.02201, 2021.\"}"}
{"id": "71kocBuhNO", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
