{"id": "NqaGPQXblk", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this paper, we propose a new visual reasoning task, called Visual Transformation Telling (VTT). Given a series of states (i.e. images), a machine is required to describe what happened (i.e. transformation) between every two adjacent states. Different from most existing visual reasoning tasks, which focus on state reasoning, VTT concentrates on transformation reasoning. We collect 13,547 samples from two instructional video datasets, i.e. CrossTask and COIN, and extract desired states and transformation descriptions to form a suitable VTT benchmark dataset. After that, we introduce an end-to-end learning model for VTT, named TTNet. TTNet consists of three components to mimic human's cognition process of reasoning transformation. First, an image encoder, e.g. CLIP, reads content from each image, then a context encoder links the image content together, and last, a transformation decoder autoregressively generates transformation descriptions between every two adjacent images. This basic version of TTNet is difficult to meet the cognitive challenge of VTT, that is to identify abstract transformations from images with small visual differences, and the descriptive challenge, which asks to describe the transformation consistently. In response to these difficulties, we propose three strategies to improve TTNet. Specifically, TTNet leverages difference features to emphasize small visual gaps, masked transformation model to stress context by forcing attention to neighbor transformations, and auxiliary category and topic classification tasks to make transformations consistent by sharing underlying semantics among representations. We adapt some typical methods from visual storytelling and dense video captioning tasks, considering their similarity with VTT. Our experimental results show that TTNet achieves better performance on transformation reasoning. In addition, our empirical analysis demonstrates the soundness of each module in TTNet, and provides some insight into transformation reasoning.\\n\\nINTRODUCTION\\n\\nWhat will come to your mind when you are given a series of images, e.g. Figure 1? Probably we first notice the content of each image, then we link these images in our mind, and finally conclude a series of events from images, i.e. the whole intermediate process of cooking noodles. In fact, this is a typical reasoning process from states (i.e. single images) to transformation (i.e. changes between images), as described in Piaget's theory of cognitive development (Bovet, 1976; Piaget, 1977). More specifically, children at the preoperational stage (2-7 years old) usually pay their attention mainly to states and ignore the transformations between states, whereas the reverse is true for children at the concrete operational stage (7-12 years old). Interestingly, computer vision is developed through a similar evolution pattern. In the last few decades, image understanding, including image classification, detection, captioning, and question answering, mainly focusing on visual states, has been comprehensively studied and achieved satisfying results.\\n\\nNow it is time to pay more attention to the visual transformation reasoning tasks. Recently, there have been some preliminary studies (Park et al., 2019; Hong et al., 2021) on transformation. For example, Hong et al. (2021) defines a transformation driven visual reasoning (TVR) task, where both initial and final states are given, and the changes of object properties including color, shape, and position are required to be obtained based on a synthetic dataset. However, the current studies of transformation reasoning remain limited in two aspects. Firstly, the task is defined in an artificial environment that is far from reality. Secondly, the definition of transformation is limited to pre-defined properties, which cannot be well generalized to unseen or new environments. As a result,\"}"}
{"id": "NqaGPQXblk", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DenseCap:\\n1. Draw black patterns.\\n2. Cut the raw materials.\\n3. Cut the packing paper.\\n4. Cut the raw materials.\\n5. Nail down or stick together.\\n\\nGLACNet:\\n1. Fold or bent the paper.\\n2. Fold or bent the paper.\\n3. Fold or bent the paper.\\n4. Nail down or stick together.\\n5. Nail down or stick together.\\n\\nTTNet:\\n1. Draw pictures on the materials.\\n2. Fold or bent the paper.\\n3. Cut the raw materials.\\n4. Nail down or stick together.\\n5. Nail down or stick together.\\n\\nGroundtruth:\\n1. Draw pictures on the materials.\\n2. Fold or bent the paper.\\n3. Cut the raw materials.\\n4. Nail down or stick together.\\n5. Nail down or stick together.\\n\\nDenseCap:\\n1. Remove the air nozzle.\\n2. Remove the air nozzle.\\n3. Pump up to the tire.\\n4. Pump up to the tire.\\n\\nGLACNet:\\n1. Screw off the valve cap and open the valve.\\n2. Pump up to the tire.\\n3. Pump up to the tire.\\n4. Tighten the valve and screw on the valve cap.\\n\\nTTNet:\\n1. Screw off the valve cap and open the valve.\\n2. Install the air nozzle.\\n3. Pump up to the tire.\\n4. Remove the air nozzle.\\n\\nGroundtruth:\\n1. Screw off the valve cap and open the valve.\\n2. Install the air nozzle.\\n3. Pump up to the tire.\\n4. Remove the air nozzle.\\n\\nDenseCap:\\n1. Do the first two jumps.\\n2. Do the first two jumps.\\n3. Do the third jump.\\n\\nGLACNet:\\n1. Begin to run up.\\n2. Do the third jump.\\n3. Do the third jump.\\n\\nTTNet:\\n1. Begin to run up.\\n2. Do the first two jumps.\\n3. Do the third jump.\\n\\nGroundtruth:\\n1. Begin to run up.\\n2. Do the first two jumps.\\n3. Do the third jump.\\n\\nDenseCap:\\n1. Dig a pit with proper size.\\n2. Fill the tree into the pit.\\n3. Fill the tree into the pit.\\n4. Fill the tree into the pit.\\n5. Fill the tree into the pit.\\n6. Fill the pit with some soil.\\n\\nGLACNet:\\n1. Dig a pit with proper size.\\n2. Fill the tree into the pit.\\n3. Fill the tree into the pit.\\n4. Fill the pit with some soil.\\n5. Fill the pit with some soil.\\n6. Pour water to the tree.\\n\\nTTNet:\\n1. Dig a pit with proper size.\\n2. Fill the tree into the pit.\\n3. Fill the pit with some soil.\\n4. Pour water to the tree.\\n5. Fill the pit with some soil.\\n6. Pour water to the tree.\\n\\nGroundtruth:\\n1. Dig a pit with proper size.\\n2. Fill the tree into the pit.\\n3. Fill the pit with some soil.\\n4. Pour water to the tree.\\n5. Fill the tree into the pit.\\n6. Pour water to the tree.\"}"}
{"id": "NqaGPQXblk", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DenseCap:\\n1. Set up the brackets.\\n2. Set up the platform.\\n3. Fix the ground nail.\\n\\nGLACNet:\\n1. Lay the cushion evenly.\\n2. Set up the platform.\\n3. Set up the platform.\\n\\nTTNet:\\n1. Lay the cushion evenly.\\n2. Set up the platform.\\n3. Fix the ground nail.\\n\\nGroundtruth:\\n1. Lay the cushion evenly.\\n2. Set up the brackets.\\n3. Fix the ground nail.\\n\\nDenseCap:\\n1. Rise to the sky.\\n2. Rise to the sky.\\n3. Rise to the sky.\\n4. Rise to the sky.\\n5. Rise to the sky.\\n6. Rise to the sky.\\n7. Rise to the sky.\\n8. Rise to the sky.\\n9. Rise to the sky.\\n\\nGLACNet:\\n1. Rise to the sky.\\n2. Rise to the sky.\\n3. Rise to the sky.\\n4. Rise to the sky.\\n5. Rise to the sky.\\n6. Rise to the sky.\\n7. Rise to the sky.\\n8. Rise to the sky.\\n\\nTTNet:\\n1. Ski down from the hill.\\n2. Ski down from the hill.\\n3. Rise to the sky.\\n4. Ski down from the hill.\\n5. Ski down from the hill.\\n6. Rise to the sky.\\n7. Ski down from the hill.\\n8. Ski down from the hill.\\n\\nGroundtruth:\\n1. Ski down from the hill.\\n2. Ski up from the hill.\\n3. Rise to the sky.\\n4. Ski down from the hill.\\n5. Ski up from the hill.\\n6. Rise to the sky.\\n7. Fall to the ground.\\n8. Ski up from the hill.\\n9. Rise to the sky.\\n\\nDenseCap:\\n1. Wipe the glue to a layer.\\n2. Wipe the glue to a layer.\\n3. Wipe nose.\\n\\nGLACNet:\\n1. Wipe up the face.\\n2. Wipe the glue to a layer.\\n3. Wipe up the face.\\n\\nTTNet:\\n1. Disinfect the injecting place.\\n2. Disinfect the injecting place.\\n3. Pull out the needle and press.\\n\\nGroundtruth:\\n1. Add some water to the vessel.\\n2. Grind roundly and evenly.\\n3. Grind roundly and evenly.\\n\\nFigure 12: Bad cases on the VTT dataset.\"}"}
{"id": "NqaGPQXblk", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. SPICE: Semantic Propositional Image Caption Evaluation. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision \u2013 ECCV 2016, Lecture Notes in Computer Science, pp. 382\u2013398, 2016.\\n\\nAnton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick. PHYRE: A New Benchmark for Physical Reasoning. In Advances in Neural Information Processing Systems, volume 32, 2019.\\n\\nSatanjeev Banerjee and Alon Lavie. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pp. 65\u201372, 2005.\\n\\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT Pre-Training of Image Transformers. In International Conference on Learning Representations, 2022.\\n\\nFabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, and Christian Wolf. CoPhy: Counterfactual Learning of Physical Dynamics. In International Conference on Learning Representations, 2020.\\n\\nMagali Bovet. Piaget's Theory of Cognitive Development and Individual Differences. In B\u00e4rbel Inhelder, Harold H. Chipman, and Charles Zwingmann (eds.), Piaget and His School: A Reader in Developmental Psychology, Springer Study Edition, pp. 269\u2013279. 1976.\\n\\nChien-Yi Chang, De-An Huang, Danfei Xu, Ehsan Adeli, Li Fei-Fei, and Juan Carlos Niebles. Procedure Planning in Instructional Videos. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision \u2013 ECCV 2020, volume 12356, pp. 334\u2013350. 2020.\\n\\nBoxing Chen and Colin Cherry. A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pp. 362\u2013367, 2014.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, 2019.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations, 2022.\\n\\nAli Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. Every Picture Tells a Story: Generating Sentences from Images. In Kostas Daniilidis, Petros Maragos, and Nikos Paragios (eds.), Computer Vision \u2013 ECCV 2010, Lecture Notes in Computer Science, pp. 15\u201329, 2010.\\n\\nRohit Girdhar and Deva Ramanan. CATER: A diagnostic dataset for Compositional Actions & Temporal Reasoning. In International Conference on Learning Representations, 2020.\\n\\nDiana Gonzalez-Rico and Gibran Fuentes-Pineda. Contextualize, Show and Tell: A Neural Visual Storyteller. 2018.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013778, 2016.\\n\\nJack Hessel, Jena D. Hwang, Jae Sung Park, Rowan Zellers, Chandra Bhagavatula, Anna Rohrbach, Kate Saenko, and Yejin Choi. The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning. 2022.\"}"}
{"id": "NqaGPQXblk", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "NqaGPQXblk", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "NqaGPQXblk", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Xiang Wang, Shiwei Zhang, Zhiwu Qing, Yuanjie Shao, Changxin Gao, and Nong Sang. Self-Supervised Learning for Semi-Supervised Temporal Action Proposal. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1905\u20131914, 2021.\\n\\nKexin Yi, Chuang Gan*, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B. Tenenbaum. CLEVRER: Collision Events for Video Representation and Reasoning. In International Conference on Learning Representations, 2020.\\n\\nHaonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and Wei Xu. Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4584\u20134593, 2016.\\n\\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From Recognition to Cognition: Visual Commonsense Reasoning. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6713\u20136724, 2019.\\n\\nChen-Lin Zhang, Jianxin Wu, and Yin Li. ActionFormer: Localizing Moments of Actions with Transformers. In Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, and Tal Hassner (eds.), Computer Vision \u2013 ECCV 2022, volume 13664, pp. 492\u2013510. 2022.\\n\\nChi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Songchun Zhu. RA VEN: A Dataset for Relational and Analogical Visual REasoNing. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5312\u20135322, 2019.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. BERTScore: Evaluating Text Generation with BERT. In ICLR, 2020.\\n\\nLuowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason J. Corso, and Marcus Rohrbach. Grounded Video Description. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6571\u20136580, 2019.\\n\\nDimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. Cross-Task Weakly Supervised Learning From Instructional Videos. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3532\u20133540, 2019.\"}"}
{"id": "NqaGPQXblk", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Results of different image encoders.\\n\\n| Image Encoder          | Params | Acc @1 | Acc @5 | Acc @10 | Acc @40 |\\n|------------------------|--------|--------|--------|---------|---------|\\n| InceptionV3 (Szegedy et al., 2016) | 23M | 77.44  | 44.88  | 404.85  | 61.75   |\\n| ResNet152 (He et al., 2016)       | 59M   | 82.82  | 50.71  | 464.01  | 67.40   |\\n| ViT-L (Dosovitskiy et al., 2022)  | 304M  | 85.84  | 58.26  | 540.46  | 73.59   |\\n| Swin-L (Liu et al., 2021)         | 196M  | 86.32  | 57.36  | 531.51  | 73.03   |\\n| BEiT-L (Bao et al., 2022)         | 306M  | 87.48  | 41.57  | 370.00  | 58.80   |\\n\\n| Image-text Pretrained          |       |        |        |         |         |\\n|--------------------------------|-------|--------|--------|---------|---------|\\n| RN50                           | 39M   | 73.30  | 53.35  | 491.80  | 69.79   |\\n| RN101                          | 57M   | 75.70  | 53.78  | 495.30  | 70.08   |\\n| ViT-B/32 (88M)                 |       | 76.10  | 55.21  | 510.08  | 71.27   |\\n| ViT-B/16 (86M)                 |       | 80.20  | 57.73  | 534.92  | 73.37   |\\n| ViT-L/14 (304M)                |       | 83.90  | 61.22  | 570.63  | 76.25   |\\n\\nFor the image encoder part, the default CLIP image encoder is ViT-L/14. Image encoders in all models are pretrained and fixed during training. Further details about the image encoder and the comparison can be found in Section D.1.\\n\\nTransformer-based context encoder consists of two transformer encoder layers. The implementation of the transformer is based on x-transformer. Simplified relative positional encoding (Raffel et al., 2020) is applied in all transformer layers. In the transformation decoder part, we directly borrow CLIP's tokenizer and their vocabulary list. Each transformation description is generated separately with a shared two-layer transformer decoder. The idea of adding transformation representations into word embeddings is inspired by GLACNet (Kim et al., 2019) and we empirically found this way improves a lot on language influence compared with using the representation as the start token. Like the context encoder, simplified relative positional encoding is also used in the transformation decoder. We sample text with top-\\nk\\nand top-\\np\\nfiltering with\\nk=100\\nand\\np=0\\n. The dimension of intermediate vectors is uniformed to be 512, including state representations, transformation representations, and word embeddings. In the training loss part, the adjustment factor\\n\\\\( \\\\alpha \\\\)\\nfor \\\\( L_{category} \\\\) is set to be 0.025 and \\\\( \\\\beta \\\\) for \\\\( L_{topic} \\\\) is 0.1. The optimizer we used is AdamW (Loshchilov & Hutter, 2022), with the learning rate first warming up to 1e-4 in the first 2000 steps and then gradually decreasing to 0. All models are implemented with PyTorch (Paszke et al., 2019) and trained on one single Tesla A100 80G GPU card with 50 epochs. The code for training and inference will be released publicly.\\n\\nIn this section, we show more detailed information about the selection of the image encoders and the hyperparameters of masked transformation model when optimizing our model.\\n\\n### D.1 Selection of Image Encoders\\n\\nImage encoding quality is the basis for subsequent reasoning and description of the model, and thus greatly affects the overall performance of the model. From Table 1, we can see that the original version of CST and GLACNet, with Inception V3 and ResNet as image encoders accordingly perform worse than CST* and GLACNet*, indicating the choice of image encoder matters. We conduct a more detailed analysis of the image encoder by testing 10 state-of-the-art image encoders, 5 were pretrained on ImageNet and 5 are CLIP models pretrained on large-scale image-text data. In the table, we show their parameter size, ImageNet top-1 accuracy, and performance on the VTT dataset. We can see that when the parameter sizes are similar, models pre-trained on image and text data perform better than that pre-trained only on image data, e.g. ViT-L/14 vs. ViT-L. This is consistent with the existing understanding that CLIP encodes more semantic information. In addition to training data, factors that affect model performance include model size, patch size used.\\n\\n---\\n\\n6 [https://github.com/lucidrains/x-transformers](https://github.com/lucidrains/x-transformers)\\n\\n7 Model weights and top-1 accuracy on ImageNet of ImageNet pretrained models are from: [https://github.com/rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models)\\n\\n8 Pretrained weights of CLIP models are from [https://github.com/openai/CLIP](https://github.com/openai/CLIP) and top-1 accuracy on ImageNet is from Table 10 of the original paper.\"}"}
{"id": "NqaGPQXblk", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023 in vision transformers, and training strategies. For example, CLIP models have more parameters and perform better. While the parameter size between ViT-B/16 and ViT-B/32 are similar, ViT-B/16 encodes finer image has smaller patch size resulting in a better image representation. BEiT-L has the highest accuracy on ImageNet but performs the worst among all models. Our explanation is that BEiT has learned enough image pattern information, but there is a defect in the capture of semantic information.\\n\\nThere are two hyperparameters in masked transformation model, i.e. the mask ratio and the sample ratio. The mask ratio is similar with BERT's mask ratio (Devlin et al., 2019), that is, the percentage of state representations and difference features that are replaced with zero. We compare the mask ratio from 0%-30% and find 15% works the best (Table 10), which is in line with BERT's finding. Another hyperparameter is the sample ratio. The motivation is to tackle the inconsistent issue between training and inference, that is, no features are masked during inference. To fill this gap, samples have opportunities to completely bypass the mask strategy during training. The sample ratio is the probability that the sample will accept the mask strategy. From Table 11, 50% probability is the best, better than the strategy of masking all samples used in BERT.\\n\\n### Table 10: Ablation on the mask ratio.\\n\\n| Mask Ratio | B@4 | C | BS |\\n|------------|-----|---|----|\\n| 0%         | 60.38 | 562.83 | 75.72 |\\n| 5%         | 60.93 | 567.92 | 76.11 |\\n| 10%        | 61.02 | 568.71 | 76.13 |\\n| 15%        | 61.22 | 570.63 | 76.25 |\\n| 20%        | 61.07 | 568.99 | 76.21 |\\n| 25%        | 61.16 | 570.18 | 76.35 |\\n| 30%        | 60.72 | 565.43 | 75.94 |\\n\\n### Table 11: Ablation on sample ratio.\\n\\n| Sample Ratio | B@4 | C | BS |\\n|--------------|-----|---|----|\\n| 0%           | 60.38 | 562.83 | 75.72 |\\n| 25%          | 60.39 | 562.15 | 75.63 |\\n| 50%          | 61.22 | 570.63 | 76.25 |\\n| 75%          | 60.96 | 567.99 | 76.00 |\\n| 100%         | 60.95 | 568.18 | 76.10 |\\n\\nThere are two levels of generalization that should be considered in visual transformation telling. At the level of one single transformation, the question is whether machines are able to generalize to different language compositions, i.e. new action-target combinations that do not exist in the training set. At the higher level of multiple transformations, the question becomes to be whether machines can generalize to different transformation combinations, such as different numbers and orders of transformations on the same topic. In this section, we discuss these two kinds of generalization problems and see how well our models perform.\\n\\n### Figure 9: Top 50 words (exclude the and and) appear multiple times in all 853 unique descriptions.\"}"}
{"id": "NqaGPQXblk", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DenseCap:\\n1. Pour espresso.\\n2. Pour espresso.\\n3. Add whipped cream.\\n\\nGLACNet:\\n1. Pour espresso.\\n2. Pour espresso.\\n3. Add whipped cream.\\n\\nTTNet:\\n1. Pour alcohol.\\n2. Pour espresso.\\n3. Add whipped cream.\\n\\nGroundtruth:\\n1. Pour coffee into glass.\\n2. Pour chocolate in glass.\\n3. Pour cream.\\n\\nFigure 10: Models failed to compose unseen transformations with seen words.\\n\\nTable 12: Statistics of unique transformation combinations.\\n\\n| Topics          | Train  | Val Only | Test Only | All   |\\n|-----------------|--------|----------|-----------|-------|\\n| Dish            | 922    | 154      | 95        | 158   |\\n| Drink and Snack| 804    | 141      | 83        | 146   |\\n| Electrical Appliance| 472   | 99       | 34        | 100   |\\n| Furniture and Decoration| 481  | 83       | 51        | 93    |\\n| Gadgets         | 414    | 82       | 33        | 84    |\\n| Housework       | 406    | 75       | 48        | 69    |\\n| Leisure and Performance | 408  | 85       | 39        | 85    |\\n| Nursing and Care| 316    | 66       | 29        | 66    |\\n| Pets and Fruit  | 208    | 42       | 20        | 45    |\\n| Science and Craft| 395   | 82       | 37        | 87    |\\n| Sport           | 167    | 35       | 18        | 38    |\\n| Vehicle         | 541    | 110      | 56        | 120   |\\n| Total           | 5534   | 1054     | 543       | 1091  |\\n\\nIn the VTT dataset, there are 853 unique transformation descriptions, and the top 50 words are counted. These words are from all unique transformation descriptions and are shown in Figure 9.\\n\\nWith such a large proportion of shared vocabulary, the natural language generation of the transformation is more valuable than the classification of the transformation, since models would have more chances to learn common patterns from transformations with shared words. To investigate whether models learned on the VTT have the generalization ability of language composition, we test models on one manually annotated by us, which comes from a related task in CrossTask, i.e. Make Bicerin.\\n\\nThe topic contains transformation descriptions that are not included in our training set but are composed without new words. Figure 10 shows the results of three models. Unfortunately, all models failed to generate these new descriptions from the existing words but with descriptions that match the states as closely as possible but exist in the list of transformations from the training set. We believe there are two main reasons. Firstly, the small size of unique transformations limits models to gain language composition ability from data. Secondly, current models lack effective design for this generalization ability. Therefore, enlarging the dataset to cover more diverse transformations and designing models with stronger generalization ability will be important future directions for visual transformation telling.\\n\\nE.2 TRANSFORMATION COMBINATION\\n\\nThe states and transformations under the same topic can be very different, one important reason is the different combinations of key transformations. For example, add seasoning can be the step after the water is boiling, or the noodles are poured, or both, depending entirely on the preference of the person cooking the noodles. This freedom leads to rich transformation combinations even with few key single transformations under each topic. We count the unique combinations in the VTT dataset.\"}"}
{"id": "NqaGPQXblk", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 13: Performance on all test samples and subsets of test samples whose transformation combinations are shared and non-shared with the training set.\\n\\n| Model     | All Share | Non-share |\\n|-----------|-----------|-----------|\\n| CST*      | 0.85      | 2.04      | 3.16      | 2.96      | 0.99      | 1.95      | 3.22      | 3.00      | 0.73      | 2.17      | 3.08      | 2.91      |\\n| GLACNet*  | 5.08      | 4.75      | 3.82      | 3.78      | 6.21      | 4.80      | 3.90      | 3.91      | 4.11      | 4.69      | 3.70      | 3.59      |\\n| DenseCap* | 4.40      | 4.74      | 3.67      | 3.59      | 5.16      | 4.72      | 3.66      | 3.61      | 3.75      | 4.76      | 3.68      | 3.57      |\\n| TTNet Base| 5.15      | 4.79      | 4.04      | 3.95      | 6.02      | 4.80      | 4.08      | 4.00      | 4.40      | 4.77      | 3.99      | 3.88      |\\n| TTNet     | 5.71      | 4.78      | 4.10      | 4.11      | 7.01      | 4.81      | 4.23      | 4.29      | 4.59      | 4.74      | 3.93      | 3.86      |\\n\\nAs shown in Table 12. From the table, we can see that more than half combinations in the test set do not exist in the training set (559 test only v.s. 1091 total), which means models need a strong generalization ability to predict these unseen transformation combinations well.\\n\\nTo investigate how models perform on these non-share combinations, we separately computed the performance of each model on three subsets of test samples, including all test samples, test samples that have shared combinations with the training set, and test samples that have combinations not included in the training set. From Table 13, we can see that models perform much worse on non-share combinations than shared combinations, e.g. the logical soundness of TTNet is decreased from 4.29 to 3.86. This significant performance degradation suggests that the combination generalization is challenging for existing models. Furthermore, TTNet performs indeed much better on shared combinations than TTNet Base (logical soundness is 4.29 v.s. 4.00), but the situation is not true for non-shared combinations (logical soundness is 3.86 v.s. 3.88), which means the proposed three strategies contribute on learning existing combinations on the training set but has little or even negative effects on model\u2019s combination generalization ability.\\n\\nF M O R E Q U A L I T A V E R E S U L T S.\\n\\nWe show more results in Figure 11 and Figure 12. Figure 12 shows several hard cases that TTNet fails to reason and describe. We point out three potential directions to improve the TTNet. The first one relates the image recognition ability. From the first case, TTNet recognizes the tent as platfond which is wrong. This error might respond to the image encoder that fails to distinguish these objects. Therefore, it may lead to a better result by using a more powerful image encoder. The major point here is that the model needs to identify subtle differences between states and determine specific transformations based on context. The last case is out-of-domain cases, that is, the test samples are quite different from the training data.\"}"}
{"id": "NqaGPQXblk", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of representations are concatenated and fed to the context encoder. Furthermore, a type embedding is added to distinguish these two kinds of features.\\n\\nSince transformations are not independent, we may meet the logical consistency problem in the transformation description process, named the descriptive challenge of VTT. For example, the logic of descriptions does not make sense as shown in Figure 4. TTNet base recognizes oranges as eggs, which is logically unreasonable with the two transformations before and after. In TTNet, we introduce masked transformation model in the context encoder and auxiliary learning in the loss function to alleviate this problem.\\n\\n4.2 MASKED TRANSFORMATION MODEL\\n\\n1. Cut both ends and remove fruit seeds.\\n2. Pour the egg into the bowl.\\n3. Pour the orange juice into the cup.\\n\\nFigure 4: TTNet base fails to effectively use contextual information and mistakenly identifies the orange as an egg.\\n\\nMasked transformation model (MTM) is inspired by masked language model (Devlin et al., 2019). The intuition behind this is that one transformation can be reasoned from nearby transformations. For example, if you are told the previous transformation is washing the watermelon and the next is putting the watermelon into a planet, it is obvious that the intermediate transformation should be related to the watermelon. Following this intuition, 15% of the input features, including state representations and difference features, are randomly masked during training. Furthermore, we empirically found that, for each sample, using this strategy with half the probability works better.\\n\\n4.3 AUXILIARY LEARNING\\n\\nHuman usually tries to guess the category or topic before describing transformations, e.g. cooking noodles. Therefore, this category or topic information may help guide description generation. Inspired by this, we propose to leverage auxiliary tasks, i.e. category and topic classification, to supervise the training process. Specifically, we introduce two additional cross entropy losses $L_{\\\\text{category}}$ and $L_{\\\\text{topic}}$ on the global context vector. We expect to make the learned transformation representations share the underlying category and topic information to enhance the learning of consistent representations. So the final training loss becomes a combination of $L_{\\\\text{text}}$, $L_{\\\\text{category}}$, and $L_{\\\\text{topic}}$:\\n\\n$$L = L_{\\\\text{text}} + \\\\alpha L_{\\\\text{category}} + \\\\beta L_{\\\\text{topic}}$$\\n\\nwhere $\\\\alpha$ and $\\\\beta$ are two adjustment factors.\\n\\n5 EXPERIMENTS\\n\\nIn this section, we first introduce our empirical setups including baseline methods and evaluation metrics. Then we demonstrate the main empirical results on the collected VTT dataset, including both quantitative and qualitative results. After that, we show extensive ablation studies on different strategies used in TTNet.\"}"}
{"id": "NqaGPQXblk", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Performance on the test set of VTT dataset. B@4/M/R/C/S/BS/Fl/Re/LS are short for BLEU@4 / METEOR / ROUGE-L / CIDEr / SPICE / BERT-Score / Fluency / Relevance / Logical Soundness. * indicates to use CLIP image encoder for a fair comparison. \u2020 indicates TTNet significantly ($p < 0.05$) outperforms the corresponding model on this human evaluation metric.\\n\\n| Model     | B@4 | M   | R   | C   | S   | BS  | Fl   | Re   | LS   |\\n|-----------|-----|-----|-----|-----|-----|-----|------|------|------|\\n| CST       | 10.09 | 11.39 | 25.98 | 43.22 | 9.28 | 16.30 |      |      |      |\\n| CST*      | 13.96 | 19.21 | 38.11 | 84.60 | 21.85 | 25.66 | 2.04\u2020 | 3.16\u2020 | 2.96\u2020 |\\n| GLACNet   | 42.77 | 45.26 | 52.98 | 381.48 | 45.33 | 60.12 |      |      |      |\\n| GLACNet*  | 55.24 | 59.48 | 66.25 | 508.18 | 60.21 | 71.13 | 4.75\u2020 | 3.82\u2020 | 3.78\u2020 |\\n| DenseCap* | 48.25 | 52.00 | 59.79 | 439.68 | 53.73 | 66.30 | 4.74\u2020 | 3.67\u2020 | 3.59\u2020 |\\n| TTNet Base| 55.68 | 60.47 | 67.05 | 515.12 | 61.45 | 72.22 | 4.79  | 4.04  | 3.95  |\\n| TTNet     | 61.22 | 66.31 | 71.84 | 570.63 | 66.20 | 76.25 | 4.78  | 4.10  | 4.11  |\\n\\npast and future information into image features to capture the context information. There are many advanced methods for dense video captioning but highly rely on fine-grained video features, which are not suitable for our task. All three methods are implemented as closely as possible according to the original paper and provide a fair comparison by using the same image encoder with TTNet. We describe the implementation details of TTNet as well as baseline models in Section C.\\n\\nEvaluation Metrics. Following previous works on visual descriptions (Ting-Hao et al., 2016; Krishna et al., 2017; Liang et al., 2022), automated metrics including BLEU@4 (Papineni et al., 2002), CIDEr (Vedantam et al., 2015), METEOR (Banerjee & Lavie, 2005), ROUGE-L (Lin & Hovy, 2002), SPICE (Anderson et al., 2016), and BERT-Score (Zhang et al., 2020) are selected as automatic metrics. Furthermore, we asked 25 human annotators to assess the quality of transformation descriptions using a Likert scale ranging from 1 to 5, for following criteria: fluency, measuring how well-written the transformation is; relevance, how relevant the transformations toward the image states; logical soundness, how well the overall logic conforms to common sense.\\n\\n5.2 Results on VTT Dataset\\n\\nQuantitative Results. Table 1 summarizes the results of 7 models on the VTT dataset, including TTNet, TTNet Base, CST and its CLIP version, GLACNet and its CLIP version, and CLIP version DenseCap. From the results, TTNet surpasses other models on most metrics with a large margin, e.g. CIDEr is 11% higher than the second best model, i.e. TTNet Base. This large improvement comes from the three strategies we proposed, which are the only differences between TTNet and TTNet Base.\\n\\nFurther comparing human metrics between them, the main strength of TTNet is the much stronger overall logic of the generated descriptions, while the relevance is only slightly better and the fluency is about the same. Secrefsec:ablation further shows the advantages of TTNet with detailed ablation studies. It is not difficult to find that the performance gap between CST*, GLACNet*, and Densecap* is also very large. While they all use CLIP, the difference lies in the way of context decoding and text generation. GLACNet* outperforms DenseCap* mainly because LSTM captures more information than past and future attention features according to the higher scores of relevance and logical soundness. The gap between GLACNet* and CST* is caused by the way of text generation. GLACNet uses word embeddings and context features as inputs in each LSTM step, while CST only uses the context as the initial state of LSTM. In our empirical studies, this little difference improves the fluency a lot, and it is the reason that TTNet chooses to add context embedding to word embedding as the inputs of the transformation decoder rather than using the context feature as the start token. The underlying design philosophy between TTNet Base and GLACNet* is similar, therefore, the performance is close. However, TTNet Base converges faster than GLACNet* during training because the transformer captures the context information more efficiently than LSTM.\\n\\nQualitative Results. We show two examples from the VTT test data in Figure 5 about sowing and pasting a car sticker. From these two examples, we can first realize that the gap between the states is really small. For example, in the sticker case, only a small area of the sticker is changed, making it difficult to reason a certain transformation without considering the overall pasting process. We can see that when the states are confusing, e.g. DenseCap and GLACNet identify the wrong entity in the sow case, TTNet is able to reason the correct transformations from the differences and the...\"}"}
{"id": "NqaGPQXblk", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DenseCap:\\n1. Put the candle wick into a vessel.\\n2. Clean up the interior of the pumpkin.\\n\\nGLACNet:\\n1. Dig some holes on the soil.\\n2. Dig some holes on the soil.\\n\\nTTNet:\\n1. Sow on the soil.\\n2. Apply a cover on the soil.\\n\\nGroundtruth:\\n1. Sow on the soil.\\n2. Apply a cover on the soil.\\n\\nDenseCap:\\n1. Tear off the front of the sticker.\\n2. Tear off the front of the sticker.\\n3. Tear off the front of the sticker.\\n4. Tear off the front of the sticker.\\n5. Tear off the front of the sticker.\\n6. Tear off the front of the sticker.\\n7. Tear off the other side of the sticker.\\n\\nGLACNet:\\n1. Clean the window surface.\\n2. Tear off the front of the sticker.\\n3. Tear off the front of the sticker.\\n4. Tear off the front of the sticker.\\n5. Tear off the front of the sticker.\\n6. Press the sticker.\\n7. Tear off the other side of the sticker.\\n\\nTTNet:\\n1. Clean the window surface.\\n2. Put the sticker on the window.\\n3. Tear off the front of the sticker.\\n4. Press the sticker.\\n5. Tear off the front of the sticker.\\n6. Press the sticker.\\n7. Tear off the other side of the sticker.\\n\\nGroundtruth:\\n1. Clean the window surface.\\n2. Put the sticker on the window.\\n3. Tear off the front of the sticker.\\n4. Press the sticker.\\n5. Tear off the front of the sticker.\\n6. Press the sticker.\\n7. Tear off the other side of the sticker.\\n\\nFigure 5: Qualitative comparison on the VTT test data. Above: sow. Below: paste car sticker.\\n\\nTable 2: CIDEr of independent transformation prediction.\\n\\n| Model                  | Original | Indep. |\\n|------------------------|----------|--------|\\n| CST*                   | 84.90    | 49.80  |\\n| DenseCap*              | 439.53   | 295.75 |\\n| GLACNet*               | 508.19   | 268.49 |\\n| TTNet w/o diff         | 527.62   | 422.04 |\\n| TTNet                  | 570.63   | 349.96 |\\n| TTNet (retrain)        | -        | 459.84 |\\n\\nTable 3: Ablation studies on the effect of key components.\\n\\n| Diff. MTM Aux. C BS | CIDEr  |\\n|---------------------|--------|\\n| \u221a                   | 515.28 |\\n| \u221a                   | 556.85 |\\n| \u221a                   | 520.04 |\\n| \u221a                   | 521.93 |\\n| \u221a \u221a                 | 562.25 |\\n| \u221a \u221a                 | 562.83 |\\n| \u221a \u221a                 | 527.62 |\\n| \u221a \u221a \u221a               | 570.63 |\\n\\nTable 4: Analysis of difference features and auxiliary tasks.\\n\\n| State Difference | CIDEr  |\\n|------------------|--------|\\n| \u221a early          | 559.78 |\\n| \u221a late           | 570.63 |\\n\\n5.3 ABLATION STUDIES\\n\\nIn Section 4, we introduce three strategies to improve TTNet, including difference sensitive encoding, masked transformation model, and auxiliary learning. In this section, we discuss the effectiveness of these three strategies. But before that, we first need to answer the question that whether the context information is crucial for VTT, since all three strategies act on the context encoder to enhance the ability to capture context information. If the answer is yes, then it comes to answer how these strategies work and whether there exist alternative choices, e.g. other types of difference features. Experimental analyses are organized into five following topics according to this logic.\\n\\nImportance Analysis of Context.\\nTo answer the question of whether the context is really important for reasoning transformations. We design to let models predict each transformation independently, i.e. only from two states before and after. If transformations can be reasoned without considering the context, model performance should remain roughly the same. However, from Table 2, the CIDEr score of all five models drops sharply from the original setting to the independent setting, showing that the context is clearly very important. Without context, reasoning transformations become rather difficult, and retraining the model with independent data does not help either.\\n\\nEffectiveness of Three Strategies.\\nNext, we move on to analyze the effectiveness of the three strategies and their combinations. The first row in Table 3 shows the result of TTNet Base and the next\"}"}
{"id": "NqaGPQXblk", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nThree rows show the results of using each strategy independently on the base model. Among them, the improvement of using difference feature is the most significant, indicating the difference is also crucial for resolving transformation reasoning. The next four rows show the results of combining these strategies and the conclusion is combining all three strategies leads to the best result. The next three topics will go through all these strategies one by one in detail, to see how these strategies work.\\n\\nAnalysis of Difference Sensitive Encoding.\\n\\nWe just show difference feature is the most significant strategy for TTNet. However, it is not clear how difference features help the TTNet model and if there are alternative choices for difference features, e.g. differences of raw images. To answer the first question, we need to go back to Table 2, which contains an interesting result that TTNet without using difference features overtakes the full model in the independent setting. This phenomenon suggests that difference features help to capture contextual information. Contextual information is more important for the original setting, and the model tries to capture it by attention more to the difference features. However, this does not prevail in the independent setting since contextual information is less effective and the model should attention more to the image features. This is why retraining the full model with the independent data works, because the focus of attention is adjusted during retraining. The second question is about the alternative type of difference features. We compare early and late differences. The early difference is pixel-level difference computed on raw images before input to the image encoder, while the late difference is used by TTNet and computed on encoded image vectors to become the semantic difference. In TVR (Hong et al., 2021), early difference is more effective while Table 4 shows the opposite result. This is because TVR requires to predict property changes on synthetic data, which relies more on pixel differences. In contrast, VTT requires event-level descriptions, with more emphasis on semantic distinctions.\\n\\nAnalysis of MTM.\\n\\nWe expect MTM to guide the model to reason transformation from nearby transformations. In order to validate this ability, we design to let models predict transformations with incomplete states, e.g. mask one state of three. Specifically, we test models under two special settings. In the first setting, we randomly mask one state for all test samples. In the second setting, we give even fewer states on average by only providing start and end states for each sample. The results are shown in Figure 6. We can see that when there is less and less information, the performance of all models decreases. However, TTNet has the slowest decline in performance, showing its robustness to missing states. By further comparing the results between TTNet and TTNet w/o MTM, we can conclude this robustness is contributed by the MTM strategy.\\n\\nAnalysis of Auxiliary Tasks.\\n\\nFinally, we analyze the effects of different auxiliary tasks and report the results in Table 4. From the table, topic classification is more effective than category classification, since topics are more granular than categories. Supervision with two classification tasks simultaneously improves the overall performance, e.g. 562.25 \\\\rightarrow 570.63 in terms of CIDEr.\\n\\nConclusion\\n\\nThis paper introduces a new visual reasoning task to focus on transformation reasoning, i.e. changes between every two states, named visual transformation telling. Given a series of images as states, the description of each transformation is required to represent what happened between every two adjacent images. In this way, the task could be used to test the machines' ability of transformation reasoning, which is an important cognitive skill for humans, as described in Piaget's theory. To the best of our knowledge, this is the first real-world application for transformation reasoning by defining transformation descriptions as output. To facilitate the study on VTT, we build benchmark data based on 13,547 samples from two instructional video datasets, i.e. CrossTask and COIN. After that, we design a model named TTNet, by applying three well-designed strategies into a basic human-inspired transformation telling model to make it difference-sensitive and context-aware. From the experiments, we find that the proposed strategies help VTT generate consistent transformation descriptions, and thus obtain better results in terms of natural language generation metrics. The empirical studies provide valuable insights for understanding VTT and the proposed model and may help to design more complicated transformation reasoning tasks or models in the future.\"}"}
{"id": "NqaGPQXblk", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Visual Transformation Telling (VTT): given states represented by images (constructed from videos), the goal is to reason and describe transformations between every two adjacent states.\\n\\nThe existing transformation reasoning task cannot meet the requirement of real-world applications. Furthermore, the lack of strong transformation reasoning ability will hinder some more advanced event-level reasoning tasks, such as visual storytelling (Ting-Hao et al., 2016) and procedure planning (Chang et al., 2020), since transformation plays an important role in these tasks.\\n\\nTo tackle these limitations, we propose a new visual transformation telling (VTT) task in this paper. The main motivation is to provide descriptions for real-world transformations. For example, given two images with dry and wet ground respectively, it should be described it rained, which precisely describes a cause-and-effect transformation. Therefore, the formal definition of VTT is to output language sentences to describe the transformation for a given series of states, i.e., images. VTT is different from video description tasks, e.g., dense video captioning (Krishna et al., 2017), since the complete process of transformations is shown by videos, which reduces the challenge of reasoning.\\n\\nTo facilitate the study of VTT, we collect 13,547 samples from two instructional video datasets, including CrossTask (Zhukov et al., 2019) and COIN (Tang et al., 2019; 2021). They are originally used for evaluating step localization, action segmentation, and other video analysis tasks. But we found them suitable to be modified to fit VTT, because the transformations are mainly about daily activities, and more importantly, some main steps to accomplish a certain job have been annotated in their data, including temporal boundaries and text descriptions. Therefore, we extract key images from a video as input, and directly use their text labels of the main steps as transformation descriptions. More details can be found in Section 3.2.\\n\\nWhen designing an effective VTT model, we face two kinds of challenges. The first one is related to the cognitive challenge, which is to derive abstract transformation from images with small differences, e.g., from the difference between wet and dry ground to rained. The second one is the descriptive challenge, that is, the description of transformations should consider the consistency in a series of images to output a reasonable event. If we only consider the description for a single transformation, i.e., between two images, it is easy to output logical errors in the results.\\n\\nIn order to address these challenges, we propose a difference-sensitive and context-aware model, named TTNet (Transformation Telling Net). TTNet consists of three major components, to mimic the human cognition process of transformation reasoning. To be specific, CLIP (Radford et al., 2021) is utilized as the image encoder to read semantic information from images into image vectors. Then a transformer-based context encoder interacts image vectors together to capture context information. At last, a transformer decoder autoregressively generates descriptions according to context features. However, this basic model is not enough to meet the cognitive and descriptive challenges, so we use three well-designed strategies to improve TTNet. Specifically, the first strategy is to compute difference features on image vectors and fed them into the context encoder as well, to emphasize small visual gaps. Then, masked transformation model is applied to capture the context-aware information, by randomly masking out the inputs of the context encoder like masked language model (Devlin et al., 2019). Finally, in addition to the general text generation loss, the whole network is also supervised under the auxiliary task of category and topic classification, which is to constrain the transformation representations to share underlying semantics, by mimicking human's behavior that forms a global event in mind.\\n\\nSince the task of VTT is new, there is no ready-made baseline model. Considering the similarity of visual storytelling and dense video captioning to VTT, we modify typical methods including...\"}"}
{"id": "NqaGPQXblk", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nCST (Gonzalez-Rico & Fuentes-Pineda, 2018), GLACNet (Kim et al., 2019), and Densecap Johnson et al. (2016) in these two applications as our baseline methods. Our experimental results show that TTNet significantly outperforms these methods. Additionally, we conduct comprehensive studies to show the importance of contextual information for VTT and the effectiveness of three strategies, including difference features, masked transformation model, and auxiliary learning.\\n\\nIn conclusion, our major contributions include: 1) the proposal of a new task called visual transformation telling to emphasize the reasoning of transformation in real world applications; 2) the introduction of TTNet which is a difference-sensitive and context-aware model for transformation reasoning. 3) extensive experiments on our collected data from instructional videos, demonstrating the effectiveness of TTNet and providing many insights for understanding the VTT task.\\n\\n2 RELATED WORKS\\n\\nVTT belongs to the direction of visual reasoning, so we first list some typical visual reasoning tasks and then discuss the relation between VTT and these tasks. CLEVR (Johnson et al., 2017) and GQA (Hudson & Manning, 2019) concentrate on relation and logical reasoning on objects. RA VEN (Zhang et al., 2019) and V-PROM (Teney et al., 2020) care about the induction and reasoning of graphic patterns. VCR (Zellers et al., 2019) and Sherlock (Hessel et al., 2022) test whether machines are able to learn commonsense knowledge to answer daily questions. These tasks mainly concentrate on state-level reasoning. Apart from these tasks, there is a series of works related to dynamic reasoning. Physical reasoning (Bakhtin et al., 2019; Yi et al., 2020; Girdhar & Ramanan, 2020; Baradel et al., 2020; Riochet et al., 2022) evaluates the ability to learn physical rules from data to answer questions or solve puzzles. VisualCOMET (Park et al., 2020) asks to reason beyond the given state to answer what happened before and what will happen next. Visual storytelling (Park et al., 2020) requires completing the missing information between states to describe a story logically.\\n\\nVisual reasoning has a tendency to shift from static scenes to dynamic ones. While state and transformation are both important for reasoning in dynamic scenes, we concentrate on transformation reasoning, between state-only scenarios and more complex composite scenarios.\\n\\nTo the best of our knowledge, there are rare studies on designing specific tasks for visual transformation reasoning. The only work is TVR (Hong et al., 2021). Given the initial and final states, TVR requires to predict a sequence of changes in properties, including size, shape, material, color, and position. However, the synthetic scenario is far from reality and property change is not a common fashion to describe transformations in life. A more natural way is the event-level description. For example, it is more natural to tell it rained when describing what happened between dry and wet ground outside. Visual storytelling (Ting-Hao et al., 2016; Ravi et al., 2021) requires event-level description but transformations are mixed in the description, making it difficult to evaluate transformation only. Visual abductive reasoning (Liang et al., 2022) has a similar core idea to us, which aims to find the most likely explanation for an incomplete set of observations. The difference is they only require machines to reason one single missing transformation from multiple transformations, while our task aims to reason multiple logically related transformations from states. The motivation of procedure planning Chang et al. (2020) is to complete a job given states, while VTT is to explain transformations between states, which has wider scenarios, e.g. explaining the wet ground with rain. Furthermore, the requirement for natural language generation makes VTT have different evaluations and unique challenges such as generalization on language compositions. Walkthrough planning Chang et al. (2020) has a different target which is to predict intermediate states.\\n\\nTalking about transformation description, there is another topic related, i.e. visual description. Here we review some typical visual description tasks and discuss their differences. Tasks that describe a single image include image captioning (Farhadi et al., 2010; Kulkarni et al., 2011), dense image captioning (Johnson et al., 2016), and image paragraphing (Krause et al., 2017). The difference lies in the level of detail. Similarly, tasks for videos include video description (Venugopalan et al., 2015), video paragraph description (Yu et al., 2016), grounded video description (Zhou et al., 2019), and dense video captioning (Krishna et al., 2017). Different from image captioning tasks that focus only on a single state, video description tasks start to describe events. For example, dense video captioning asks to predict temporal boundaries and descriptions of key events in a video. However, they provide the full process of transformation throughout videos, reducing the need for reasoning.\"}"}
{"id": "NqaGPQXblk", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nTo construct a meaningful dataset for VTT, we require the data to cover a large scope of real-world activities, i.e. cross-modal application, vehicle, gadgets, leisure and performance, etc. Furthermore, the distribution of keywords, transformation length, and sentence length. From the category distribution and the word annotation into VTT dataset. We can see that the video is segmented into multiple main steps, and this way, we collected 13,547 samples as well as 55,482 transformation descriptions from CrossTask (Zhukov et al., 2019) and COIN (Tang et al., 2019; 2021), and construct our data.\\n\\nTo illustrate an instruction video from COIN for cooking noodles and how we transform their annotations from a sequence of visual states, i.e. images. Formally, visual transformation telling aims to test the ability of machines to reason and describe transformations that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect transformations that are logically related and semantically different. Logically related means these images given, which are logically related and semantically different. Logically related means these images are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is given, which are logically related and semantically different. Logically related means these images are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaningful to people, i.e. transformation. The target is then to identify the transformations that are associated with a certain event, e.g. completing a job, while semantically different is to expect some substantial changes that are meaning\"}"}
{"id": "NqaGPQXblk", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The architecture of TTNet. Images are first encoded into state representations in the image encoder, then transformed into transformation representations in the context encoder, and finally decoded into text by the transformation decoder. To be more difference-sensitive and context-aware, three strategies are considered to enhance TTNet (marked with *). Difference features computed according to state representations are used as extra features, state representations and difference features are randomly masked as zero during training, and auxiliary tasks are used for supervision.\\n\\nOverview of TTNet.\\n\\nOur TTNet is designed to mimic human's cognitive process of transformation. The first step is independent recognition, which means that people may understand each image independently. Therefore, we introduce an image encoder \\\\( f_{image} \\\\) to represent each image to a vector and obtain a set of image vectors \\\\( V = \\\\{ v_i \\\\}_{i=1}^{N+1} = \\\\{ f_{image}(s_i) \\\\}_{i=1}^{N+1} \\\\). After that, humans will associate these images together, and form an understanding of all images guided by a global event. To reflect this process, we introduce a context encoder, e.g. a bi-directional RNN or a transformer encoder, denoted as \\\\( f_{context} \\\\), to obtain context-aware image representations \\\\( C = \\\\{ c_i \\\\}_{i=1}^{N+1} = \\\\{ f_{context}(i, V) \\\\}_{i=1}^{N+1} \\\\) by considering contextual information. The final step is to describe these transformations based on previous understanding. In TTNet, we feed the last \\\\( N \\\\) context-aware image representations to a transformation decoder \\\\( f_{caption} \\\\), implemented with an RNN or a transformer decoder, to generate each transformation description \\\\( T = \\\\{ t_i \\\\}_{i=1}^{N} = \\\\{ f_{caption}(c_{i+1}) \\\\}_{i=1}^{N} \\\\) separately and auto-regressively. We empirically found adding the transformation representation to the word embedding in each step is better than using it as the start token.\\n\\nThe model is then trained with ground truth transformations \\\\( T^* = \\\\{ t^*_i \\\\}_{i=1}^{N} \\\\) by minimizing the following negative log-likelihood loss, where \\\\( t^*_i = \\\\{ x^*_{i,l} \\\\}_{l=1}^{L} \\\\) is the ground truth description of the \\\\( i \\\\)th transformation.\\n\\n\\\\[\\nL_{text} = -\\\\sum_{i=1}^{N} \\\\sum_{l=1}^{L} \\\\log p(x^*_{i,l} | x^*_{i,<l})\\n\\\\] (2)\\n\\nIn order to tackle the two unique challenges of VTT, i.e. cognitive challenge and descriptive challenge, we propose three specific strategies to enhance the above TTNet, including difference sensitive encoding, masked transformation model, and auxiliary learning. To distinguish more clearly, we called the model that does not use these three strategies TTNet base.\\n\\n4.1 Difference Sensitive Encoding\\n\\nIn visual transformation telling, the differences between two adjacent states are usually very small. Imagine the scene of cooking noodles, the whole picture does not change much before and after the noodles are added to the pot. This characteristic requires the model not only to understand the content of each image, but also to focus on differences between images to facilitate the understanding of transformations. For this purpose, we first utilize CLIP (Radford et al., 2021) as our image encoder, due to its strong semantic representation ability trained on large scale unsupervised data. We also introduce difference features, by subtracting the current state and the previous state representations \\\\( \\\\Delta V = \\\\{ v_i - v_{i-1} \\\\}_{i=1}^{N+1}, \\\\) where \\\\( v_0 = v_{N+1} \\\\), to emphasize the subtle difference. The above two kinds of representations are used as extra features.\"}"}
{"id": "NqaGPQXblk", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In order to provide a deeper understanding of the VTT dataset, we describe how we construct VTT in detail. The whole process consists of four major parts, i.e. combine and complete annotations, extract state images, and split the dataset. Additionally, we mention some annotation information from CrossTask and COIN, which is important for understanding our decisions when building VTT, such as how to choose the start and end states.\\n\\nCombine and complete annotations.\\n\\nBoth CrossTask and COIN provide the annotations of step labels and corresponding segments. Step labels are predefined before they started annotating videos. The difference is that COIN employed experts to define steps while CrossTask derived them from WikiHow, a website teaching how to do many things. With predefined steps, they find annotators to label the step categories and the corresponding segments for each video. We collect and organize these annotations in a uniform format to become the basis of the VTT dataset. Apart from that, category and topic labels are used for auxiliary learning. Both CrossTask and COIN provide topic information, which is the task to solve. However, only COIN provides categories as the domain information and they are missing in the CrossTask. We manually classify all topics from CrossTask into existing categories. The full list of 12 categories and 198 topics is shown in Table 5.\\n\\nTable 5: The Categories and topics in VTT dataset. Topics marked with * are from CrossTask and others belong to COIN.\\n\\n| Category           | Topics                                                                 |\\n|--------------------|-------------------------------------------------------------------------|\\n| Nursing and Care   | Wash Dog, Use Earplugs, Use Neti Pot, Put On Hair Extensions, Use Epinephrine Auto-injector, Perform CPR, Wear Contact Lenses, Remove Blackheads With Glue, Give An Intramuscular Injection, Shave Beard, Wash Hair, Bandage Dog Paw, Draw Blood, Bandage Head |\\n| Pets and Fruit     | Plant Tree, Transplant, Graft, Cut Grape Fruit, Cut Mango, Cut Cantaloupe, Sow |\\n| Furniture and Deco- | Install Shower Head, Install Ceramic Tile, Install Air Conditioner, Install Curtain, Lubricate A Lock, Replace Door Knob, Install Wood Flooring, Install Closestool, Assemble Cabinet, Assemble Sofa, Replace Faucet, Replace Toilet Seat, Assemble Bed, Build Simple Floating Shelves*, Assemble Office Chair |\\n| Decoration         | Install Shower Head, Install Ceramic Tile, Install Air Conditioner, Install Curtain, Lubricate A Lock, Replace Door Knob, Install Wood Flooring, Install Closestool, Assemble Cabinet, Assemble Sofa, Replace Faucet, Replace Toilet Seat, Assemble Bed, Build Simple Floating Shelves*, Assemble Office Chair |\\n| Leisure and Perform- | Make Paper Wind Mill, Perform Vanishing Glass Trick, Raise Flag, Play Frisbee With A Dog, Make Chinese Lantern, Carve Pumpkin, Change Guitar Strings, Perform Paper To Money Trick, Pitch A Tent, Open Champagne Bottle, Blow Sugar, Make Paper Easter Baskets, Cut And Restore Rope Trick, Do Lino Printing, Replace Drum-head, Prepare Sumi Ink, Prepare Canvas |\\n| Electrical Appliance | Replace Graphics Card, Replace Light Socket, Replace Electrical Outlet, Replace Memory Chip, Use Soy Milk Maker, Change Toner Cartridge, Replace Laptop Screen, Replace Refrigerator Water Filter, Use Vending Machine, Replace Filter For Air Purifier, Replace Hard Disk, Replace Blade Of A Saw, Refill Cartridge, Clean Laptop Keyboard, Arc Weld, Install Ceiling Fan, Replace A Bulb, Paste Screen Protector On Pad, Assemble Desktop PC, Use Sewing Machine |\\n| Science and Craft  | Prepare Standard Solution, Make Flower Press, Use V olumetric Pipette, Hang Wallpaper, Make Candle, Make Soap, Use Triple Beam Balance, Make Flower Crown, Use V olumetric Flask, Paste Car Sticker, Make Slime With Glue, Make Paper Dice, Wrap Gift Box, Set Up A Hamster Cage, Use Analytical Balance |\\n| Drink and Snack    | Make Meringue*, Make Salad, Make Lemonade*, Make Taco Salad*, Make Tea, Make Chocolate, Make a Latte*, Make Homemade Ice Cream, Make Jello Shots*, Make Coffee, Make Cocktail, Make Cookie, Make Irish Coffee*, Roast Chestnut, Make Banana Ice Cream*, Make Orange Juice, Make Matcha Tea, Make Sugar Coated Haws, Make Strawberry Smoothie, Make Hummus |\\n| Vehicle            | Change Bike Chain, Replace Car Fuse, Replace Rearview Mirror Glass, Tie Boat To Dock, Pump Up Bicycle Tire, Change Car Tire, Use Jack, Remove Scratches From Windshield, Jack Up a Car*, Change Bike Tires, Install License Plate Frame, Fuel Car, Replace A Wiper Head, Install Bicycle Rack, Replace Tyre Valve Stem, Change a Tire*, Patch Bike Inner Tube, Polish Car, Replace Car Window, Add Oil to Your Car*, Park Parallel |\\n| Housework          | Put On Quilt Cover, Clean Bathtub, Wash Dish, Clean Leather Seat, Pack Sleeping Bag, Clean Wooden Floor, Clean Toilet, Iron Clothes, Drill Hole, Remove Crayon From Walls, Clean Hamster Cage, Make Bed, Unclog Sink With Baking Soda, Clean Rusty Pot, Clean Cement Floor |\\n| Sport              | Practise Karate, Wear Shin Guards, Practise Triple Jump, Throw Hammer, Play Curling, Practise Skiing Aerials, Practise Pole Vault, Attend N B A Skills Challenge, Glue Ping Pong Rubber, Practise Weight Lift |\\n| Gadgets            | Open A Lock With Paperclips, Replace Mobile Screen Protector, Load Grease Gun, Change Mobile Phone Battery, Replace Sewing Machine Needle, Change Battery Of Watch, Replace SIM Card, Resize Watch Band, Replace CD Drive With SSD, Refill Mechanical Pencils, Make Wireless Earbuds, Refill Fountain Pen, Refill A Lighter, Rewrap Battery, Replace Battery On Key To Car, Fix Laptop Screen Scratches, Operate Fire Extinguisher, Replace Battery On TV Control, Use Tapping Gun, Refill A Stapler, Make RJ45 Cable |\"}"}
{"id": "NqaGPQXblk", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nGrill Steak\\nMake Irish Coffee\\nMake French Toast\\nMake Banana Ice Cream\\nMake a Latte\\nMake Taco Salad\\nBuild Simple Floating Shelves\\nSow\\nPut On Hair Extensions\\nReplace Mobile Screen Protector\\nMake Paper Wind Mill\\nAssemble Office Chair\\nMake Flower Crown\\nOpen Champagne Bottle\\nMake Matcha Tea\\nReplace CD Drive With SSD\\nPractise Karate\\nReplace Blade Of A Saw\\nUse Rice Cooker To Cook Rice\\nReplace A Bulb\\nMake Jello Shots\\nReplace Filter For Air Purifier\\nMake Salmon\\nInstall Bicycle Rack\\nOpen A Lock With Paperclips\\nClean Rusty Pot\\nReplace Refrigerator Water Filter\\nRemove Blackheads With Glue\\nReplace Door Knob\\nReplace Laptop Screen\\nUse Vending Machine\\nMake Strawberry Smoothie\\nBoil Noodles\\nAdd Oil to Your Car\\nPaste Car Sticker\\nMake Slime With Glue\\nReplace Memory Chip\\nCook Omelet\\nMake French Fries\\nOperate Fire Extinguisher\\nChange Toner Cartridge\\nTransplant\\nReplace Hard Disk\\nDo Lino Printing\\nGive An Intramuscular Injection\\nMake Salad\\nReplace A Wiper Head\\nMake RJ45 Cable\\nMake Lemonade\\nMake Chinese Lantern\\nMake Pizza\\nInstall Shower Head\\nMake Burger\\nReplace Rearview Mirror Glass\\nRefill A Lighter\\nCut Grape Fruit\\nUse Analytical Balance\\nChange Battery Of Watch\\nMake Pancakes\\nReplace SIM Card\\nMake Pickles\\nMake Bread and Butter Pickles\\nJack Up a Car\\nMake Cocktail\\nPerform CPR\\nReplace Door Knob\\nMake Paper Wind Mill\\nAssemble Office Chair\\nMake Flower Crown\\nOpen Champagne Bottle\\nMake Matcha Tea\\nReplace CD Drive With SSD\\nPractise Karate\\nReplace Blade Of A Saw\\nUse Rice Cooker To Cook Rice\\nReplace A Bulb\\nMake Jello Shots\\nReplace Filter For Air Purifier\\nMake Salmon\\nInstall Bicycle Rack\\nOpen A Lock With Paperclips\\nClean Rusty Pot\\nReplace Refrigerator Water Filter\\nRemove Blackheads With Glue\\nReplace Door Knob\\nReplace Laptop Screen\\nUse Vending Machine\\nMake Strawberry Smoothie\\nBoil Noodles\\nAdd Oil to Your Car\\nPaste Car Sticker\\nMake Slime With Glue\\nReplace Memory Chip\\nCook Omelet\\nMake French Fries\\nOperate Fire Extinguisher\\nChange Toner Cartridge\\nTransplant\\nReplace Hard Disk\\nDo Lino Printing\\nGive An Intramuscular Injection\\nMake Salad\\nReplace A Wiper Head\\nMake RJ45 Cable\\nMake Lemonade\\nMake Chinese Lantern\\nMake Pizza\\nInstall Shower Head\\nMake Burger\\nReplace Rearview Mirror Glass\\nRefill A Lighter\\nCut Grape Fruit\\nUse Analytical Balance\\nChange Battery Of Watch\\nMake Pancakes\\nReplace SIM Card\\nMake Pickles\\nMake Bread and Butter Pickles\\nJack Up a Car\\nMake Cocktail\\nPerform CPR\\nReplace Door Knob\\nMake Paper Wind Mill\\nAssemble Office Chair\\nMake Flower Crown\\nOpen Champagne Bottle\\nMake Matcha Tea\\nReplace CD Drive With SSD\\nPractise Karate\\nReplace Blade Of A Saw\\nUse Rice Cooker To Cook Rice\\nReplace A Bulb\\nMake Jello Shots\\nReplace Filter For Air Purifier\\nMake Salmon\\nInstall Bicycle Rack\\nOpen A Lock With Paperclips\\nClean Rusty Pot\\nReplace Refrigerator Water Filter\\nRemove Blackheads With Glue\\nReplace Door Knob\\nReplace Laptop Screen\\nUse Vending Machine\\nMake Strawberry Smoothie\\nBoil Noodles\\nAdd Oil to Your Car\\nPaste Car Sticker\\nMake Slime With Glue\\nReplace Memory Chip\\nCook Omelet\\nMake French Fries\\nOperate Fire Extinguisher\\nChange Toner Cartridge\\nTransplant\\nReplace Hard Disk\\nDo Lino Printing\\nGive An Intramuscular Injection\\nMake Salad\\nReplace A Wiper Head\\nMake RJ45 Cable\\nMake Lemonade\\nMake Chinese Lantern\\nMake Pizza\\nInstall Shower Head\\nMake Burger\\nReplace Rearview Mirror Glass\\nRefill A Lighter\\nCut Grape Fruit\\nUse Analytical Balance\\nChange Battery Of Watch\\nMake Pancakes\\nReplace SIM Card\\nMake Pickles\\nMake Bread and Butter Pickles\\nJack Up a Car\\nMake Cocktail\\nPerform CPR\\nReplace Door Knob\\nMake Paper Wind Mill\\nAssemble Office Chair\\nMake Flower Crown\\nOpen Champagne Bottle\\nMake Matcha Tea\\nReplace CD Drive With SSD\\nPractise Karate\\nReplace Blade Of A Saw\\nUse Rice Cooker To Cook Rice\\nReplace A Bulb\\nMake Jello Shots\\nReplace Filter For Air Purifier\\nMake Salmon\\nInstall Bicycle Rack\\nOpen A Lock With Paperclips\\nClean Rusty Pot\\nReplace Refrigerator Water Filter\\nRemove Blackheads With Glue\\nReplace Door Knob\\nReplace Laptop Screen\\nUse Vending Machine\\nMake Strawberry Smoothie\\nBoil Noodles\\nAdd Oil to Your Car\\nPaste Car Sticker\\nMake Slime With Glue\\nReplace Memory Chip\\nCook Omelet\\nMake French Fries\\nOperate Fire Extinguisher\\nChange Toner Cartridge\\nTransplant\\nReplace Hard Disk\\nDo Lino Printing\\nGive An Intramuscular Injection\\nMake Salad\\nReplace A Wiper Head\\nMake RJ45 Cable\\nMake Lemonade\\nMake Chinese Lantern\\nMake Pizza\\nInstall Shower Head\\nMake Burger\\nReplace Rearview Mirror Glass\\nRefill A Lighter\\nCut Grape Fruit\\nUse Analytical Balance\\nChange Battery Of Watch\\nMake Pancakes\\nReplace SIM Card\\nMake Pickles\\nMake Bread and Butter Pickles\\nJack Up a Car\\nMake Cocktail\\nPerform CPR\\nReplace Door Knob\\nMake Paper Wind Mill\\nAssemble Office Chair\\nMake Flower Crown\\nOpen Champagne Bottle\\nMake Matcha Tea\\nReplace CD Drive With SSD\\nPractise Karate\\nReplace Blade Of A Saw\\nUse Rice Cooker To Cook Rice\\nReplace A Bulb\\nMake Jello Shots\\nReplace Filter For Air Purifier\\nMake Salmon\\nInstall Bicycle Rack\\nOpen A Lock With Paperclips\\nClean Rusty Pot\\nReplace Refrigerator Water Filter\\nRemove Blackheads With Glue\\nReplace Door Knob\\nReplace Laptop Screen\\nUse Vending Machine\\nMake Strawberry Smoothie\\nBoil Noodles\\nAdd Oil to Your Car\\nPaste Car Sticker\\nMake Slime With Glue\\nReplace Memory Chip\\nCook Omelet\\nMake French Fries\\nOperate Fire Extinguisher\\nChange Toner Cartridge\\nTransplant\\nReplace Hard Disk\\nDo Lino Printing\\nGive An Intramuscular Injection\\nMake Salad\\nReplace A Wiper Head\\nMake RJ45 Cable\\nMake Lemonade\\nMake Chinese Lantern\\nMake Pizza\\nInstall Shower Head\\nMake Burger\\nReplace Rearview Mirror Glass\\nRefill A Lighter\\nCut Grape Fruit\\nUse Analytical Balance\\nChange Battery Of Watch\\nMake Pancakes\\nReplace SIM Card\\nMake Pickles\\nMake Bread and Butter Pickles\\nJack Up a Car\\nMake Cocktail\\nPerform CPR\\nReplace Door Knob\\nMake Paper Wind Mill\\nAssemble Office Chair\\nMake Flower Crown\\nOpen Champagne Bottle\\nMake Matcha Tea\\nReplace CD Drive With SSD\\nPractise Karate\\nReplace Blade Of A Saw\\nUse Rice Cooker To Cook Rice\\nReplace A Bulb\\nMake Jello Shots\\nReplace Filter For Air Purifier\\nMake Salmon\\nInstall Bicycle Rack\\nOpen A Lock With Paperclips\\nClean Rusty Pot\\nReplace Refrigerator Water Filter\\nRemove Blackheads With Glue\\nReplace Door Knob\\nReplace Laptop Screen\\nUse Vending Machine\\nMake Strawberry Smoothie\\nBoil Noodles\\nAdd Oil to Your Car\\nPaste Car Sticker\\nMake Slime With Glue\\nReplace Memory Chip\\nCook Omelet\\nMake French Fries\\nOperate Fire Extinguisher\\nChange Toner Cartridge\\nTransplant\\nReplace Hard Disk\\nDo Lino Printing\\nGive An Intramuscular Injection\\nMake Salad\\nReplace A Wiper Head\\nMake RJ45 Cable\\nMake Lemonade\\nMake Chinese Lantern\\nMake Pizza\\nInstall Shower Head\\nMake Burger\\nReplace Rearview Mirror Glass\\nRefill A Lighter\\nCut Grape Fruit\\nUse Analytical Balance\\nChange Battery Of Watch\\nMake Pancakes\\nReplace SIM Card\\nMake Pickles\\nMake Bread and Butter Pickles\\nJack Up a Car\\nMake Cocktail\\nPerform CPR\\nReplace Door Knob\\nMake Paper Wind Mill\\nAssemble Office Chair\\nMake Flower Crown\\nOpen Champagne Bottle\\nMake Matcha Tea\\nReplace CD Drive With SSD\\nPractise Karate\\nReplace Blade Of A Saw\\nUse Rice Cooker To Cook Rice\\nReplace A Bulb\\nMake Jello Shots\\nReplace Filter For Air Purifier\\nMake Salmon\\nInstall Bicycle Rack\\nOpen A Lock With Paperclips\\nClean Rusty Pot\\nReplace Refrigerator Water Filter\\nRemove Blackheads With Glue\\nReplace Door Knob\\nReplace Laptop Screen\\nUse Vending Machine\\nMake Strawberry Smoothie\\nBoil Noodles\\nAdd Oil to Your Car\\nPaste Car Sticker\\nMake Slime With Glue\\nReplace Memory Chip\\nCook Omelet\\nMake French Fries\\nOperate Fire Extinguisher\\nChange Toner Cartridge\\nTransplant\\nReplace Hard Disk\\nDo Lino Printing\\nGive An Intramuscular Injection\\nMake Salad\\nReplace A Wiper Head\\nMake RJ45 Cable\\nMake Lemonade\\nMake Chinese Lantern\\nMake Pizza\\nInstall Shower Head\\nMake Burger\\nReplace Rearview Mirror Glass\\nRefill A Lighter\\nCut Grape Fruit\\nUse Analytical Balance\\nChange Battery Of Watch\\nMake Pancakes\\nReplace SIM Card\\nMake Pickles\\nMake Bread and Butter Pickles\\nJack Up a Car\\nMake Cocktail\\nPerform CPR\\nReplace Door Knob\\nMake Paper Wind Mill\\nAssemble Office Chair\\nMake Flower Crown\\nOpen Champagne Bottle\\nMake Matcha Tea\\nReplace CD Drive With SSD\\nPractise Karate\\nReplace Blade Of A Saw\\nUse Rice Cooker To Cook Rice\\nReplace A Bulb\\nMake Jello Shots\\nReplace Filter For Air Purifier\\nMake Salmon\\nInstall Bicycle Rack\\nOpen A Lock With Paperclips\\nClean Rusty Pot\\nReplace Refrigerator Water Filter\\nRemove Blackheads With Glue\\nReplace Door Knob\\nReplace Laptop Screen\\nUse Vending Machine\\nMake Strawberry Smoothie\\nBoil Noodles\\nAdd Oil to Your Car\\nPaste Car Sticker\\nMake Slime With Glue\\nReplace Memory Chip\\nCook Omelet\\nMake French Fries\\nOperate Fire Extinguisher\\nChange Toner Cartridge\\nTransplant\\nReplace Hard Disk\\nDo Lino Printing\\nGive An Intramuscular Injection\\nMake Salad\\nReplace A Wiper Head\\nMake RJ45 Cable\\nMake Lemonade\\nMake Chinese Lantern\\nMake Pizza\\nInstall Shower Head\\nMake Burger\\nReplace Rearview Mirror Glass\\nRefill A Lighter\\nCut Grape Fruit\\nUse Analytical Balance\\nChange Battery Of Watch\\nMake Pancakes\\nReplace SIM Card\\nMake Pickles\\nMake Bread and Butter Pickles\\nJack Up a Car\\nMake Cocktail\\nPerform CPR\\nReplace Door Knob\\nMake Paper Wind Mill\\nAssemble Office Chair\\nMake Flower Crown\\nOpen Champagne Bottle\\nMake Matcha Tea\\nReplace CD Drive With SSD\\nPractise Karate\\nReplace Blade Of A Saw\\nUse Rice Cooker To Cook Rice\\nReplace A Bulb\\nMake Jello Shots\\nReplace Filter For Air Purifier\\nMake Salmon\\nInstall Bicycle Rack\\nOpen A Lock With Paperclips\\nClean Rusty Pot\\nReplace Refrigerator Water Filter\\nRemove Blackheads With Glue\\nReplace Door Knob\\nReplace Laptop Screen\\nUse Vending Machine\\nMake Strawberry Smoothie\\nBoil Noodles\\nAdd Oil to Your Car\\nPaste Car Sticker\\nMake Slime With Glue\\nReplace Memory Chip\\nCook Omelet\\nMake French Fries\\nOperate Fire Extinguisher\\nChange Toner Cartridge\\nTransplant\\nReplace Hard Disk\\nDo Lino Printing\\nGive An Intramuscular Injection\\nMake Salad\\nReplace A Wiper Head\\nMake RJ45 Cable\\nMake Lemonade\\nMake Chinese Lantern\\nMake Pizza\\nInstall Shower Head\\nMake Burger\\nReplace Rearview Mirror Glass\\nRefill A Lighter\\nCut Grape Fruit\\nUse Analytical Balance\\nChange Battery Of Watch\\nMake Pancakes\\nReplace SIM Card\\nMake Pickles\\nMake Bread and Butter Pickles\\nJack Up a Car\\nMake Cocktail\\nPerform CPR\\nReplace Door Knob\\nMake Paper Wind Mill\\nAssemble Office Chair\\nMake Flower Crown\\nOpen Champagne Bottle\\nMake Matcha Tea\\nReplace CD Drive With SSD\\nPractise Karate\\nReplace Blade Of A Saw\\nUse Rice Cooker To Cook Rice\\nReplace A Bulb\\nMake Jello Shots\\nReplace Filter For Air Purifier\\nMake Salmon\\nInstall Bicycle Rack\\nOpen A Lock With Paperclips\\nClean Rusty Pot\\nReplace Refrigerator Water Filter\\nRemove Blackheads With Glue\\nReplace Door Knob\\nReplace Laptop Screen\\nUse Vending Machine\\nMake Strawberry Smoothie\\nBoil Noodles\\nAdd Oil to Your Car\\nPaste Car Sticker\\nMake Slime With Glue\\nReplace Memory Chip\\nCook Omelet\\nMake French Fries\\nOperate Fire Extinguisher\\nChange Toner Cartridge\\nTransplant\\nReplace Hard Disk\\nDo Lino Printing\\nGive An Intramuscular Injection\\nMake Salad\\nReplace A Wiper Head\\nMake RJ45 Cable\\nMake Lemonade\\nMake Chinese Lantern\\nMake Pizza\\nInstall Shower Head\\nMake Burger\\nReplace Rearview Mirror Glass\\nRefill A Lighter\\nCut Grape Fruit\\nUse Analytical Balance\\nChange Battery Of Watch\\nMake Pancakes\\nReplace SIM Card\\nMake Pickles\\nMake Bread and Butter Pickles\\nJack Up a Car\\nMake Cocktail\\nPerform CPR\\nReplace Door Knob\\nMake Paper Wind Mill\\nAssemble Office Chair\\nMake Flower Crown\\nOpen Champagne Bottle\\nMake Matcha Tea\\nReplace CD Drive With SSD\\nPractise Karate\\nReplace Blade Of A Saw\\nUse Rice Cooker To Cook Rice\\nReplace A Bulb\\nMake Jello Shots\\nReplace Filter For Air Purifier\\nMake Salmon\\nInstall Bicycle Rack\\nOpen A Lock With Paperclips\\nClean Rusty Pot\\nReplace Refrigerator Water Filter\\nRemove Blackheads With Glue\\nReplace Door Knob\\nReplace Laptop Screen\\nUse Vending Machine\\nMake Strawberry Smoothie\\nBoil Noodles\\nAdd Oil to Your Car\\nPaste Car Sticker\\nMake Slime With Glue\\nReplace Memory Chip\\nCook Omelet\\nMake French Fries\\nOperate Fire Extinguisher\\nChange Toner Cartridge\\nTransplant\\nReplace Hard Disk\\nDo Lino Printing\\nGive An Intramuscular Injection\\nMake Salad\\nReplace A Wiper Head\\nMake RJ45 Cable\\nMake Lemonade\\nMake Chinese Lantern\\nMake Pizza\\nInstall Shower Head\\nMake Burger\\nReplace Rearview Mirror Glass\\nRefill A Lighter\\nCut Grape Fruit\\nUse Analytical Balance\\nChange Battery Of Watch\\nMake Pancakes\\nReplace SIM Card\\nMake Pickles\\nMake Bread and Butter Pickles\\nJack Up a Car\\nMake Cocktail\\nPerform CPR\\nReplace Door Knob\\nMake Paper Wind Mill\\nAssemble Office Chair\\nMake Flower Crown\\nOpen Champagne Bottle\\nMake Matcha Tea\\nReplace CD Drive With SSD\\nPractise Karate\\nReplace Blade Of A Saw\\nUse Rice Cooker To Cook Rice\\nReplace A Bulb\\nMake Jello Shots\\nReplace Filter For Air Purifier\\nMake Salmon\\nInstall Bicycle Rack\\nOpen A Lock With Paperclips\\nClean Rusty Pot\\nReplace Refrigerator Water Filter\\nRemove Blackheads With Glue\\nReplace Door Knob\\nReplace Laptop Screen\\nUse Vending Machine\\nMake Strawberry Smoothie\\nBoil Noodles\\nAdd Oil to Your Car\\nPaste Car Sticker\\nMake Slime With Glue\\nReplace Memory Chip\\nCook Omelet\\nMake French Fries\\nOperate Fire Extinguisher\\nChange Toner Cartridge\\nTransplant\\nReplace Hard Disk\\nDo Lino Printing\\nGive An Intramuscular Injection\\nMake Salad\\nReplace A Wiper Head\\nMake RJ45 Cable\\nMake Lemonade\\nMake Chinese Lantern\\nMake Pizza\\nInstall Shower Head\\nMake Burger\\nReplace Rearview Mirror Glass\\nRefill A Lighter\\nCut Grape Fruit\\nUse Analytical Balance\\nChange Battery Of Watch\\nMake Pancakes\\nReplace SIM Card\\nMake Pickles\\nMake Bread and Butter Pickles\\nJack Up a Car\\nMake Cocktail\\nPerform CPR\\nReplace Door Knob\\nMake Paper Wind Mill\\nAssemble Office Chair\\nMake Flower Crown\\nOpen Champagne Bottle\\nMake Matcha Tea\\nReplace CD Drive With SSD\\nPractise Karate\\nReplace Blade Of A Saw\\nUse Rice Cooker To Cook Rice\\nReplace A Bulb\\nMake Jello Shots\\nReplace Filter For Air Purifier\\nMake Salmon\\nInstall Bicycle Rack\\nOpen A Lock With Paperclips\\nClean Rusty Pot\\nReplace Refrigerator Water Filter\\nRemove Blackheads With Glue\\nReplace Door Knob\\nReplace Laptop Screen\\nUse Vending Machine\\nMake Strawberry Smoothie\\nBoil Noodles\\nAdd Oil to Your Car\\nPaste Car Sticker\\nMake Slime With Glue\\nReplace Memory Chip\\nCook Omelet\\nMake French Fries\\nOperate Fire Extinguisher\\nChange Toner Cartridge\\nTransplant\\nReplace Hard Disk\\nDo Lino Printing\\nGive An Intramuscular Injection\\nMake Salad\\nReplace A Wiper Head\\nMake RJ45 Cable\\nMake Lemonade\\nMake Chinese Lantern\\nMake Pizza\\nInstall Shower Head\\nMake Burger\\nReplace Rearview Mirror Glass\\nRefill A Lighter\\nCut Grape Fruit\\nUse Analytical Balance\\nChange Battery Of Watch\\nMake Pancakes\\nReplace SIM Card\\nMake Pickles\\nMake Bread and Butter Pickles\\nJack Up a Car\\nMake Cocktail\\nPerform CPR\\nReplace Door Knob\\nMake Paper Wind Mill\\nAssemble Office Chair\\nMake Flower Crown\\nOpen Champagne Bottle\\nMake Matcha Tea\\nReplace CD Drive With SSD\\nPractise Karate\\nReplace Blade Of A Saw\\nUse Rice Cooker To Cook Rice\\nReplace A Bulb\\nMake Jello Shots\\nReplace Filter For Air Purifier\\nMake Salmon\\nInstall Bicycle Rack\\nOpen A Lock With Paperclips\\nClean Rusty Pot\\nReplace Refrigerator Water Filter\\nRemove Blackheads With Glue\\nReplace Door Knob\\nReplace Laptop Screen\\nUse Vending Machine\\nMake Strawberry Smoothie\\nBoil Noodles\\nAdd Oil to Your Car\\nPaste Car Sticker\\nMake Slime With Glue\\nReplace Memory Chip\\nCook Omelet\\nMake French Fries\\nOperate Fire Extinguisher\\nChange Toner Cartridge\\nTransplant\\nReplace Hard Disk\\nDo Lino Printing\\nGive An Intramuscular Injection\\nMake Salad\\nReplace A Wiper Head\\nMake RJ45 Cable\\nMake Lemonade\\nMake Chinese Lantern\\nMake Pizza\\nInstall Shower Head\\nMake Burger\\nReplace Rearview Mirror Glass\\nRefill A Lighter\\nCut Grape Fruit\\nUse Analytical Balance\\nChange Battery Of Watch\\nMake Pancakes\\nReplace SIM Card\\nMake Pickles\\nMake Bread and Butter Pickles\\nJack Up a Car\\nMake Cocktail\\nPerform CPR\\nReplace Door Knob\\nMake Paper Wind Mill\\nAssemble Office Chair\\nMake Flower Crown\\nOpen Champagne Bottle\\nMake Matcha Tea\\nReplace CD Drive With SSD\\nPractise Karate\\nReplace Blade Of A Saw\\nUse Rice Cooker To Cook Rice\\nReplace A Bulb\\nMake J"}
{"id": "NqaGPQXblk", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: The VTT human evaluation guidelines.\\n\\n| Metric       | Score | Criteria                                                                 |\\n|--------------|-------|---------------------------------------------------------------------------|\\n| Fluency      | 5     | All sentences are fluent.                                                 |\\n|              | 4     | Most sentences are fluent with only a few flaws.                         |\\n|              | 3     | About half of the sentences are fluent.                                  |\\n|              | 2     | Most of the sentences are difficult to read, only a few are okay.         |\\n|              | 1     | All sentences are hard to read.                                           |\\n| Relevance    | 5     | The descriptions are all related to the corresponding before and after images. |\\n|              | 4     | A few descriptions are slightly irrelevant, e.g. the description is related to the underlying topic but cannot be clearly inferred from the images. |\\n|              | 3     | Many descriptions are slightly irrelevant or a few descriptions are irrelevant, e.g. the action or target object mentioned in the transformation does not match the images. |\\n|              | 2     | Many descriptions are irrelevant.                                         |\\n|              | 1     | Most descriptions are irrelevant, or some descriptions are completely irrelevant, e.g. transformation is unrelated to the underlying topic of the images. |\\n| Logical Soundness | 5 | The underlying logic of the descriptions is consistent with common sense. |\\n|              | 4     | The overall logic is consistent with common sense, with minor flaws.     |\\n|              | 3     | There are a few obvious logical problems between the descriptions, e.g. unreasonable repeating transformations. |\\n|              | 2     | There are some obvious logical problems, e.g. the order of transformations is obviously not in line with common sense. |\\n|              | 1     | Logic cannot be judged because of the extremely poor fluency or poor relevance leading to overall logic inconsistent with the underlying topic. |\\n\\nAnother issue is the precision of the boundary of existing step segments in CrossTask and COIN. For future construction of larger datasets, we suggest a strategy for possible refinement by applying object state-recognition models (Soucek et al., 2022).\\n\\nB.1 Automatic Evaluation\\n\\nWe introduce some details that are not included in the main content of the paper when computing automatic metrics. Firstly, we follow the smooth strategy introduced by Chen & Cherry (2014) when computing BLEU@4 to provide more accurate results. This is because descriptions in VTT are usually short, the original BLEU@4 gives a zero score for short texts. In addition, BERT-Score is rescaled with the pre-computed baseline (Zhang et al., 2020) to have a more meaningful score range. BLEU@4 is computed using the NLTK package. CIDEr, METEOR, ROUGE, and SPICE are computed with the code from coco-caption. BERT-Score is computed by using the official code provided by the authors.\\n\\nB.2 Human Evaluation\\n\\nAutomatic evaluation metrics have limitations on reflecting the quality of generated text mainly because they are uninterpretable and do not correlate with human evaluations (van der Lee et al., 2019). In the VTT task, we consider three levels of text quality, evaluated by people. The first...\"}"}
{"id": "NqaGPQXblk", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Implementations details of baseline models and TTNet.\\n\\n| Model     | Image Encoder | Context Encoder | Transformation | Decoder | Params |\\n|-----------|---------------|-----------------|----------------|---------|--------|\\n| CST       | InceptionV3   | LSTM            | LSTM           |         | 379M   |\\n| CST*      | CLIP (ViT-L/14) | LSTM            | LSTM           |         | 661M   |\\n| GLACNet   | ResNet152     | bi-LSTM         | LSTM           |         | 128M   |\\n| GLACNet*  | CLIP (ViT-L/14) | bi-LSTM         | LSTM           |         | 373M   |\\n| DenseCap* | CLIP (ViT-L/14) | Attention | LSTM           |         | 361M   |\\n| TTNet Base | CLIP (ViT-L/14) | Transformer    | Transformer    |         | 368M   |\\n| TTNet     | CLIP (ViT-L/14) | Transformer    | Transformer    |         | 368M   |\\n\\nLevel considers only the fluency of the text itself. The second level considers the relevance of each individual transformation description to the topic and to the images before and after. The third level considers the logical consistency between transformation descriptions. The assessment uses the 5-point Likert scale and follows the guidelines below in Table 7.\\n\\nWe asked 25 people to evaluate the major baseline models' results shown in Table 1. A subset of samples is selected for human evaluation by randomly sampling testing samples, 1 sample from each topic and 2 extra samples, resulting in 200 samples in total. All annotators are asked to read and follow the guidelines to give their scores. During human evaluation, the annotators can see the images in addition to the category and the topic as a reference. The web interface for human evaluation is shown in Figure 8. Each sample result from each model is evaluated by at least 2 people. Our code of the evaluation webpage will also be released along with the VTT source code.\\n\\nDuring training, we apply commonly used image augmentation tricks, including randomly cropping images into $224 \\\\times 224$ patches, and random flipping. The overall architectures of all baseline models are shown in Table 8. We re-implemented CST and GLACNet following the original paper and their released source code. We didn't find the code of DenseCap and followed their paper to implement the final model. The image encoder of DenseCap is replaced with CLIP since the original model targets video descriptions and uses video encoders. When implementing TTNet, in the\\n\\n4 https://github.com/dianaglzrico/neural-visual-storyteller\\n5 https://github.com/tkim-snu/GLACNet\"}"}
