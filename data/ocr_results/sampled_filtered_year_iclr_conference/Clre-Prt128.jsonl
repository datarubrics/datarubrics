{"id": "Clre-Prt128", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.3 DIFFERENTIALLY PRIVATE SPEECH COMMAND CLASSIFICATION FOR VOICE ASSISTANT APPLICATIONS\\n\\nIn recent years, voice assistants have gained popularity in consumer applications such as home speakers, and rely heavily on ML. Recordings collected from users for training speech processing algorithms can be used in impersonation attacks, resulting in successful identity theft (Sweet, 2016) or in acoustic attacks, which trigger unintended behaviour in voice assistants (Yuan et al., 2018; Carlini et al., 2016). Protecting privacy in this setting is therefore paramount to increase trust and applicability, as well as safeguard both users and systems from adversarial interference. Convolutional neural networks (CNNs) have been demonstrated to yield state-of-the-art performance on spectrogram-transformed audio data (Palanisamy et al., 2020). However, this and other works (Zhou et al., 2021) typically discard the imaginary components. We here experimentally demonstrate the DP training of a 2-dimensional CNN directly on the complex spectrogram data. We utilised a subset of the SpeechCommands dataset (Warden, 2018), specifically samples from the categories \\\"Yes\\\", \\\"No\\\", \\\"Up\\\", \\\"Down\\\", \\\"Left\\\", \\\"Right\\\", \\\"On\\\", \\\"Off\\\", \\\"Stop\\\", and \\\"Go\\\", summing up to 8000 examples. We transformed each waveform signal to a complex-valued 2-D spectrogram and used \\\\( \\\\zeta \\\\)-DP-SGD to train a complex-valued CNN. These results are summarised in Table 1 and Figure 2.\\n\\nFigure 2: Exemplary waveform (A), real (B) and imaginary (C) spectrogram components from the utterance \\\"Stop\\\". Spectrograms are log-magnitude transformed for clarity.\\n\\n5.4 MRI RECONSTRUCTION\\n\\nMRI is an important medical imaging modality and has been studied extensively in the context of deep learning (Akcakaya et al., 2019; Hammernik et al., 2018; Kustner et al., 2020; Muckley et al., 2020). MRI data is acquired in the so-called \\\\( k \\\\)-space. Sampling only a subset of \\\\( k \\\\)-space data allows for a considerable speed-up in acquisition time, benefiting patient comfort and costs, however, typically leads to image artifacts, which reduce the diagnostic quality of the resulting MR images. Although neural networks have the ability to produce high-quality reconstructions, their usage for this task has been shown to sometimes lead to the appearance of spurious image content from the fully-sampled reference images the models have been originally trained on (Hammernik et al., 2021; Muckley et al., 2020; Shimron et al., 2021). DP could counteract such hallucination as it is designed to limit the effect of individual training examples on model training. However, this positive effect of DP may be counterbalanced by an unacceptable decrease in the diagnostic suitability of the reconstructed images. In this section, we investigate the ramifications of DP on the quality of MRI reconstructions. For this purpose, we trained a complex-valued U-Net model architecture on the task of reconstructing single-coil knee MRI images from the fastMRI dataset (Zbontar et al., 2018) using pseudo-random \\\\( k \\\\)-space sampling at \\\\( 4\\\\times \\\\) acceleration. We observed a nearly equivalent performance in the non-DP and the \\\\( \\\\zeta \\\\)-DP-SGD settings, whereby the non-DP model enjoyed a \\\\( \\\\approx 2\\\\% \\\\) performance advantage in all metrics. Moreover, to assess the diagnostic suitability of the reconstructed images, we asked a diagnostic radiologist who was blinded to whether or not \\\\( \\\\zeta \\\\)-DP-SGD was used, ...\"}"}
{"id": "Clre-Prt128", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022 to compare the resulting scans. No differences in diagnostic suitability were observed by the expert in any of the reconstructed images.\\n\\nWe thus conclude that\u2014at least with respect to image quality\u2014DP can indeed match the non-private training of MRI reconstruction models, even at $\\\\varepsilon!^1$; we intend to investigate its effect on preventing training data hallucination into reconstructed images in future work. Results from these experiments are summarised in Table 2 and Figure 3.\\n\\nTable 2: Results on the MRI reconstruction task. NMSE: normalised mean squared error, PSNR: peak signal-to-noise ratio in dB, SSIM: structural similarity index metric. GDP: Gaussian DP.\\n\\n| Non-DP               | $\\\\varepsilon$ \u03b4 | $\\\\zeta$-\\n|----------------------|-----------------|--------------|\\n|                      | 0.042 30.74 0.70 | 10.16 5      |\\n| $\\\\zeta$-DP-SGD       | 0.043 30.57 0.69 | 0.36 1       |\\n\\nFigure 3: Exemplary reconstruction from a coronal proton-density weighted image of the knee. A: reference image, B: reconstruction model trained non-privately, C: reconstruction model trained with $\\\\zeta$-DP-SGD.\\n\\nCONCLUSION\\n\\nOur work presents $\\\\zeta$-DP, an extension of DP to the complex domain and introduces key building blocks of DP model training, namely the complex Gaussian mechanism and $\\\\zeta$-DP-SGD. Our experiments demonstrate that the training of DP complex-valued neural networks is possible with high utility under tight privacy guarantees. Our theoretical analysis allows us to leverage the increased learning capacity of complex-weighted neural networks. We thus show improved learning performance compared to scalar real-valued networks and to networks using two real-valued channels to approximate complex numbers, hence, ignoring the specific relationship between the real and imaginary complex components. Lastly, Wirtinger calculus and Gaussian DP allow us to derive the tight sensitivity estimates and privacy bounds for the complex Gaussian mechanism.\\n\\nWe acknowledge some limitations of our work: Both complex-valued deep learning and DP incur a considerable computational performance penalty. Despite steadily improving complex number support, current deep learning frameworks have not yet implemented a full palette of complex-valued layers and activation functions. Moreover, the software framework utilised to computationally realise $\\\\zeta$-DP-SGD in our work relies on multithreading, which suffers from considerable overhead compared to implementations utilising vector instructions and/or bespoke hardware. We discuss the topic of software implementation and provide computational performance benchmarks in Appendices A.7 and A.8. Our investigation highlights a requirement for mature software frameworks able to offer feature and performance parity with their real-valued counterparts, which we intend to develop and publish as free-and-open-source software in the near future.\\n\\nIn conclusion, we contend that $\\\\zeta$-DP represents a promising future research direction for complex-valued and even for purely real-valued tasks, and the improved privacy-utility trade-offs resulting from our work represent a worthwhile contribution to the implementation of DP to a broad variety of relevant tasks, which can help to increase the amount of data available for scientific studies.\"}"}
{"id": "Clre-Prt128", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our work follows all applicable ethical research standards and laws. All experiments were conducted on publicly available datasets. No new data concerning human or animal subjects was generated during our investigation.\\n\\nWe adhere to ICLRs reproducibility standards and include all necessary information to reproduce our experimental and theoretical results either in the main manuscript or in the Appendix. Theoretical results and proofs can be found in the main manuscript, Section 4 and additional information can be found in Appendix A.5. Details of dataset preparation and analysis can be found in Appendix A.6. Specifically, it contains details about the used datasets, their number of samples, all training, validation and test splits, as well as preprocessing steps. Furthermore, we describe model architectures, employed optimisers, learning rates, and the number of epochs for which models were trained. Lastly, for all DP trainings we provide the noise multipliers, $L_2$ clipping norms and sampling rates, as well as the $\\\\delta$-values at which the $\\\\varepsilon$-values were calculated. Software implementation details and computational resources used can be found in Appendices A.7 and A.8.\\n\\nReferences\\n\\nMartin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pp. 308\u2013318, 2016.\\n\\nJohn M Abowd. The US Census Bureau adopts differential privacy. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2867\u20132867, 2018.\\n\\nMehmet Akc\u00b8akaya, Steen Moeller, Sebastian Weing\u00a8artner, and K\u02c6amil U\u02d8gurbil. Scan-specific robust artificial-neural-networks for k-space interpolation (RAKI) reconstruction: Database-free deep learning for fast imaging. Magnetic Resonance in Medicine, 81(1):439\u2013453, 2019.\\n\\nMartin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International Conference on Machine Learning, pp. 1120\u20131128. PMLR, 2016.\\n\\nBorja Balle and Yu-Xiang Wang. Improving the Gaussian mechanism for differential privacy: Analytical calibration and optimal denoising. In International Conference on Machine Learning, pp. 394\u2013403. PMLR, 2018.\\n\\nBorja Balle, Gilles Barthe, Marco Gaboardi, Justin Hsu, and Tetsuya Sato. Hypothesis testing interpretations and R\u00b4enyi differential privacy. In International Conference on Artificial Intelligence and Statistics, pp. 2496\u20132506. PMLR, 2020.\\n\\nJoshua Bassey, Lijun Qian, and Xianfang Li. A survey of complex-valued neural networks. arXiv preprint arXiv:2101.12249, 2021.\\n\\nChristoph Boeddeker, Patrick Hanebrink, Lukas Drude, Jahn Heymann, and Reinhold Haeb-Umbach. On the computation of complex-valued gradients with application to statistically optimum beamforming. arXiv preprint arXiv:1701.00392, 2017.\\n\\nDH Brandwood. A complex gradient operator and its application in adaptive array theory. In IEE Proceedings H-Microwaves, Optics and Antennas, volume 130, pp. 11\u201316. IET, 1983.\\n\\nNicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, and Wenchao Zhou. Hidden voice commands. In 25th USENIX Security Symposium, pp. 513\u2013530, 2016.\\n\\nSoumick Chatterjee, Chompunuch Sarasaen, Alessandro Sciarra, Mario Breitkopf, Steffen Oeltze-Jafra, Andreas N\u00a8urnberger, and Oliver Speck. Going beyond the image space: undersampled MRI reconstruction directly in the k-space using a complex valued residual neural network. In 2021 ISMRM & SMRT Annual Meeting & Exhibition, pp. 1757, 2021.\"}"}
{"id": "Clre-Prt128", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Elizabeth K Cole, Joseph Y Cheng, John M Pauly, and Shreyas S Vasanawala. Analysis of deep complex-valued convolutional neural networks for MRI reconstruction. arXiv preprint arXiv:2004.01738, 2020.\\n\\nGraham Cormode, Somesh Jha, Tejas Kulkarni, Ninghui Li, Divesh Srivastava, and Tianhao Wang. Privacy at scale: Local differential privacy in practice. In Proceedings of the 2018 International Conference on Management of Data, pp. 1655\u20131658, 2018.\\n\\nJoseph A Cruz and David S Wishart. Applications of machine learning in cancer prediction and prognosis. Cancer informatics, 2:59\u201377, 2006.\\n\\nJinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy. arXiv preprint arXiv:1905.02383, 2019.\\n\\nCynthia Dwork and Guy N Rothblum. Concentrated differential privacy. arXiv preprint arXiv:1603.01887, 2016.\\n\\nCynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found. Trends Theor. Comput. Sci., 9(3-4):211\u2013407, 2014.\\n\\nL Fan and L Xiong. Adaptively sharing real-time aggregate with differential privacy. IEEE Transactions on Knowledge and Data Engineering (TKDE), 26(9):2094\u20132106, 2013.\\n\\nOlga Fink, Enrico Zio, and Ulrich Weidmann. Predicting component reliability and level of degradation with complex-valued neural networks. Reliability Engineering & System Safety, 121:198\u2013206, 2014.\\n\\nFerdinando Fioretto, Terrence WK Mak, and Pascal Van Hentenryck. Differential privacy for power grid obfuscation. IEEE Transactions on Smart Grid, 11(2):1356\u20131366, 2019.\\n\\nJonas Geiping, Hartmut Bauermeister, Hannah Dr\u00f6ge, and Michael Moeller. Inverting Gradients\u2013How easy is it to break privacy in federated learning? arXiv preprint arXiv:2003.14053, 2020.\\n\\nGeorge M Georgiou and Cris Koutsougeras. Complex domain backpropagation. IEEE transactions on Circuits and systems II: analog and digital signal processing, 39(5):330\u2013334, 1992.\\n\\nNitzan Guberman. On complex valued convolutional neural networks. arXiv preprint arXiv:1602.09046, 2016.\\n\\nKerstin Hammernik, Teresa Klatzer, Erich Kobler, Michael P Recht, Daniel K Sodickson, Thomas Pock, and Florian Knoll. Learning a variational network for reconstruction of accelerated MRI data. Magnetic resonance in medicine, 79(6):3055\u20133071, 2018.\\n\\nKerstin Hammernik, Jo Schlemper, Chen Qin, Jinming Duan, Ronald M. Summers, and Daniel Rueckert. Systematic evaluation of iterative deep neural networks for fast parallel MRI reconstruction with sensitivity-weighted coil combination. Magnetic Resonance in Medicine, 86(4):1859\u20131872, 2021.\\n\\nNicholas J Higham. Stability of a method for multiplying complex matrices with three real matrix multiplications. SIAM journal on matrix analysis and applications, 13(3):681\u2013687, 1992.\\n\\nAkira Hirose. Complex-valued neural networks, volume 400. Springer Science & Business Media, 2012.\\n\\nMary C Kociuba and Daniel B Rowe. Complex-valued time-series correlation increases sensitivity in FMRI analysis. Magnetic resonance imaging, 34(6):765\u2013770, 2016.\\n\\nJakub Konecn\u00fd, H Brendan McMahan, Felix X Yu, Peter Richt\u00e1r\u00edk, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.\\n\\nAntti Koskela, Joonas J\u00e4lk\u00f6, and Antti Honkela. Computing tight differential privacy guarantees using fft. In International Conference on Artificial Intelligence and Statistics, pp. 2560\u20132569. PMLR, 2020.\"}"}
{"id": "Clre-Prt128", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"example of the original MNIST dataset, from which PhaseMNIST is constructed, we performed the following procedure: Let $L_R^{P_0}, \\\\ldots, L_R^{P_9}$ be the label corresponding to the real-valued image. We then constructed the imaginary component by (deterministically) sampling uniformly with replacement from the set of images whose label $L_I$ satisfies $L_R \\\\leq L_I \\\\leq 9$. We used the label of the real-valued image as the label of the overall training example.\\n\\nModel training\\n\\nWe used a complex-valued model consisting of three fully connected layers with $p = 784$, $256$, $128$ units and an output layer of $10$ units. The Cardioid activation function was used between layers and the Softmax activation function after the output layer. The model was trained with the Stochastic Gradient Descent optimiser at a learning rate of $0.1$ both for $\\\\zeta$-DP-SGD and for non-private training. The non-private model converged after 3 epochs, whereas the $\\\\zeta$-DP-SGD model required 10 epochs to achieve the same accuracy. The noise multiplier was set to $1.1$ and the $L_2$ clipping norm to $1.0$. The $\\\\epsilon$ value was calculated at a $\\\\delta$ of $10^{-5}$. A sampling rate of $0.001$ was used for $\\\\zeta$-DP-SGD, and a batch size of $64$ for non-DP training.\\n\\n### A.7 SOFTWARE LIBRARIES AND COMPUTATIONAL RESOURCES USED\\n\\nImplementations of the DP-SGD algorithm, and \u2013by extension\u2013 $\\\\zeta$-DP-SGD require access to per-example gradients. We utilised the deepee software library (Ziller et al., 2021) to implement $\\\\zeta$-DP-SGD, as it is compatible with arbitrary neural network architectures, including such containing complex-valued weights. We report results using uniform without replacement sampling and using the RDP option provided by deepee. For complex-valued neural network components, the PyTorch Complex library (Chatterjee et al., 2021) with PyTorch 1.9 were used. Standard PyTorch was also used to create the $R_2^n$ model architectures for the experiments in Section 5.1. TensorFlow 2.4 was used for loading data and the Short Time Fourier Transforms discussed above, but no neural network components were used from this library. Experiments were carried out in Python 3.8.5 on a single workstation computer running Ubuntu Linux 20.04 and equipped with a single NVidia Quadro RTX 8000 GPU, 12 CPU cores and 64 GB of RAM.\\n\\n### A.8 COMPUTATIONAL CONSIDERATIONS\\n\\nWe conclude by presenting a systematic evaluation of the computational considerations incurred by the utilisation of complex-valued neural networks and by the implementation of $\\\\zeta$-DP-SGD using the above-mentioned libraries. Two main sources of computational overhead arise between real-valued and complex-valued neural networks. Complex numbers are internally represented as a pair of $32$-bit floating point numbers. This affects inputs and neural network weights. Moreover, even though a complex-valued architecture may contain the same number of parameters as its real-valued counterpart, an increased number of computational operations is required in $C$. For instance, scalar multiplication requires a single multiplication operation in $R$. However, in $C$ it can require up to $4$ multiplications (although this can be reduced to $3$ multiplications (Higham, 1992)). Performance moreover depends on whether vector hardware is used and whether complex floating point instructions are implemented in the respective framework (e.g., cuDNN). Table A.8 shows results for individual matrix multiplication operations and convolutions with real/complex-valued inputs and weight matrices.\\n\\n|                  | Linear | Conv. |\\n|------------------|--------|-------|\\n| **R** (Real-valued) | $68.6 \\\\mu s$ | $1.21 \\\\text{ ms}$ |\\n| **C** (Complex-valued) | $1.21 \\\\text{ ms}$ | $31.8 \\\\text{ ms}$ |\\n| **R** (Real-valued) | $49.6 \\\\mu s$ | $238 \\\\mu s$ |\\n| **C** (Complex-valued) | $1.38 \\\\text{ ms}$ | $12.5 \\\\text{ ms}$ |\\n\\nR: Real-valued input and weight matrix and C: complex-valued input and weight matrix. Times are given on CPU and GPU.\"}"}
{"id": "Clre-Prt128", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u03b6-DP-SGD carries additional overhead as it requires per-sample gradients. In the utilised deepee framework, this is realised through dispatching one computation thread per example in the minibatch (more precisely, lot) to perform a forward and backward pass, which incurs substantial overhead compared to pure vectorisation. These results are shown in Table A.8. Of note, for the non-private models, the computation time includes the forward pass, backward pass, loss gradient calculation (Mean Squared Error against a vector of dimensions $p_64$, $q_1$) and weight update (Stochastic Gradient Descent). For the \u03b6-DP-SGD model, the following additional steps occur between the loss gradient calculation and the weight update: gradient clipping, averaging of per-sample gradients, noise application. Moreover, the deepee framework requires an additional step between the weight update and the subsequent batch.\\n\\nTable 7: Average computation times for a model consisting of a 2D convolutional layer with 3 input channels, 32 output channels and a kernel shape of $3^3$ followed by a linear layer with matrix dimensions of $p_{28800}$, $q_1$ executed on an input of dimensionality $p_{64}$, $q_3$, $32$, $32$ (batch, channel, height, width) for 100 repetitions.\\n\\n| Method         | CPU    | GPU    | Total Time |\\n|----------------|--------|--------|------------|\\n| Non-DP (\u03b6)     | 156 ms | 187 ms | 27.6 s     |\\n| C              | 45.4 s | 588 ms | 18.1 s     |\\n| R              | 27.6 s | 8.52 s | 2.08 min   |\\n| C              | 45.4 s | 588 ms | 18.1 s     |\"}"}
{"id": "Clre-Prt128", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section we introduce $\\\\zeta$-DP, an extension of DP to complex-valued query functions and mechanisms. $\\\\zeta$-DP generalises real-valued DP and allows the re-use of prior theoretical results and software implementations.\\n\\n### 4.1 The Complex Gaussian Mechanism\\n\\nWe begin by introducing a variant of the GM suitable to query functions with codomain $C^d$.\\n\\n**Definition 7 (Complex Gaussian mechanism)**\\n\\nThe complex Gaussian mechanism $M_C$ on $f: C^n \\\\rightarrow C^d$ outputs $f(p) + \\\\xi$, where $\\\\xi \\\\sim N_{C^d}(p, \\\\sigma^2 I_d)$ and $N_{C^d}(p, \\\\sigma^2 I_d)$ denotes circularly symmetric complex-valued Gaussian noise with variance $\\\\sigma^2$.\\n\\nOf note, a random variable $X \\\\sim N_{C^d}(p, \\\\sigma^2 I_d)$ can be constructed by independently drawing two random variables $A, B \\\\sim N(p, \\\\sigma^2)$ and outputting $X = Ai + Bj$, where $i$ is the imaginary unit. This property is unique to the complex GM and leverages the fact that the inner product in $C^n$ is non-bilinear to add noise scaled by $-2\\\\sigma^2\\\\xi$ to each component of the complex number. Naively simulating $C^n$ as $R^{2n}$, the latter being equipped with a bi-linear inner product, would instead require the addition of independent Gaussian noise to each component of a vector with resulting scale $2\\\\sigma^2$.\\n\\nWe now state our main theoretical results and proof sketches. Detailed proofs can be found in Appendix A.1.\\n\\n**Theorem 1.** Let $f: C^n \\\\rightarrow C^d$ be a query function with sensitivity $\\\\Delta$. Then, $M_C$ preserves $\\\\epsilon, \\\\delta$-DP if and only if the following holds\\n\\n$$\\\\epsilon + \\\\sqrt{\\\\epsilon} \\\\Phi \\\\left( \\\\frac{\\\\Delta \\\\sqrt{2 \\\\delta \\\\sigma^2}}{\\\\epsilon \\\\sigma^2} \\\\right) \\\\leq \\\\frac{1}{2}.$$  \\n\\n**Proof (Sketch).** It suffices to show that the magnitude of the privacy-loss random variable $\\\\Omega$ is bounded by $\\\\epsilon$ with probability $1 - \\\\delta$. This holds, as $\\\\Omega$ is distributed as a real-valued normal distribution even when $O_P C$, where $O$ is any output of $f$, seeing as the mean of $\\\\Omega$ is distributed as $p f p D q$.\\n\\n**Theorem 2.** Let $f$ be defined as above. Then, $M_C$ preserves $\\\\alpha, \\\\rho$-RDP if:\\n\\n$$\\\\sigma^2 \\\\leq \\\\frac{\\\\Delta^2 \\\\rho}{\\\\alpha^2}.$$  \\n\\n**Proof (Sketch).** The R\u00b4enyi divergence of order $\\\\alpha > 1$ between two circularly symmetric, complex-valued normal distributions with means $\\\\mu_0, \\\\mu_1$ and common variance $\\\\sigma^2 I_n$ is:\\n\\n$$D^{(\\\\alpha)} = \\\\| N(p_{\\\\mu_0}, \\\\sigma^2 I_n) \\\\| N(p_{\\\\mu_1}, \\\\sigma^2 I_n) \\\\| = \\\\alpha \\\\mu_0 \\\\mu_1 \\\\sigma^2.$$  \\n\\n**Theorem 3.** Let $f$ be defined as above, $\\\\Delta$ be the $L_2$-sensitivity of $f$ and $\\\\sigma$ be the standard deviation of $M_C$'s noise. Let $G_{\\\\mu}$ be the trade-off function of a $\\\\mu$-GDP real-valued GM. Then, if $\\\\lambda = \\\\Delta{\\\\mu}{\\\\sigma}^2$, $M_C$ preserves $\\\\mu$-GDP if and only if $\\\\lambda \\\\leq \\\\mu$. Thus, to preserve $\\\\mu$-GDP, one must choose $\\\\sigma^2$ such that:\\n\\n$$\\\\sigma^2 \\\\leq \\\\frac{\\\\Delta^2 \\\\mu}{\\\\lambda^2}.$$  \\n\\n\"}"}
{"id": "Clre-Prt128", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proof (Sketch).\\nWe will show that \\\\( T_{p} N_{C} p_{f} D_{q}, \\\\sigma_{q} N_{C} p_{f} D_{1} q \\\\), \\\\( \\\\sigma_{q} \\\\), \\\\( G_{\\\\mu} \\\\).\\n\\nwhich follows from the fact that \\\\( f_{p} D_{q} f_{p} D_{1} q \\\\), \\\\( \\\\sigma_{q} \\\\), \\\\( G_{\\\\mu} \\\\), \\\\( \\\\Delta \\\\), \\\\( \\\\sigma_{q} \\\\), \\\\( \\\\lambda \\\\) and that \\\\( G_{\\\\mu} \\\\) is strictly monotonically decreasing in \\\\( \\\\mu \\\\).\\n\\nThese findings allow for a seamless transfer of results which apply to real-valued functions to the complex domain. In particular, they yield the following insights:\\n\\n1. The complex GM inherits all properties of the real-valued GM, such as composition and sub-sampling amplification, as well as the tight analysis afforded by GDP.\\n2. The complex GM, like the real-valued GM, is fully characterised by the sensitivity \\\\( \\\\Delta \\\\) and the magnitude of the noise \\\\( \\\\sigma \\\\).\\n3. The GM \\\"naturally fits\\\" \\\\( \\\\zeta \\\\)-DP due to the aforementioned convenient properties of the circularly symmetric complex-valued Gaussian distribution. As an additional counterexample, a complex-valued Laplace random variable is naturally non-circular in the complex (and multivariate) case, even when constructed from independent distributions (Kotz et al., 2001). Moreover, the utilisation of the \\\\( L_{1} \\\\)-metric on the output space of \\\\( f \\\\) is disadvantageous, as even for scalar (complex) outputs, the \\\\( L_{1} \\\\) sensitivity can be higher than the \\\\( L_{2} \\\\) sensitivity. Lastly, the utilisation of elliptical Laplace noise is inherently unable to satisfy \\\\( \\\\rho, \\\\epsilon, q \\\\)-DP in any dimension \\\\( q \\\\geq 1 \\\\) (Reimherr & Awan, 2019). We thus leave the introduction of alternative strategies for obtaining \\\\( \\\\rho, \\\\epsilon, q \\\\)-DP in the complex-valued setting to future investigation.\\n\\nWe conclude this section by introducing a modification of the DP stochastic gradient descent (DP-SGD) algorithm, which will be employed in our experimental evaluation.\\n\\n4.2 \\\\( \\\\zeta \\\\)-DP-SGD\\nThe DP-SGD algorithm (Abadi et al., 2016) represents an application of the GM to the training of deep neural networks. Using the terminology above, each training step of the neural network (which, in this setting, represents the query) leads to the release of a privatised gradient. Evidently, the noise magnitude of the GM must be calibrated to the sensitivity of the loss function. However, most neural network loss functions have a Lipschitz constant which is too high to preserve DP while maintaining acceptable utility (and \u2013generally \u2013the Lipschitz constant of neural networks is NP-hard to compute (Scaman & Virmaux, 2018)). Thus, DP-SGD (Abadi et al., 2016) artificially induces a bounded sensitivity condition by clipping the \\\\( L_{2} \\\\)-norm of the gradient to a pre-defined value. A loss function with real-valued outputs is required for minimisation (even when the function's arguments are complex-valued) as the complex plane \u2013contrary to the real number line\u2013 does not admit a natural ordering.\\n\\nOur implementation of the algorithm makes use of Wirtinger (or \\\\( CR \\\\)-) calculus (Kreutz-Delgado, 2009) for gradient computations similar to previous works on complex-valued deep learning (Virtue et al., 2017; Boeddeker et al., 2017). This technique, discussed in detail Appendix A.5, provides several benefits: It relaxes the requirement for component functions to be holomorphic (that is, differentiable in the complex sense), only requiring them to be individually differentiable with respect to their real and imaginary components (differentiable in the real sense). For holomorphic functions \\\\( C \\\\rightarrow C \\\\), \\\\( CR \\\\)-derivatives nevertheless recover the correct derivative definition. Thus, \\\\( CR \\\\)-derivatives can also be used to compute the global sensitivity in the \\\\( C \\\\rightarrow C \\\\)-case via the Lipschitz constant. More importantly, for functions \\\\( C \\\\rightarrow R \\\\), they lead to a correct gradient magnitude calculation, whereas expressing complex-valued functions as vector-valued functions in \\\\( R^{2n} \\\\), a technique often employed in complex-valued neural network training (Trabelsi et al., 2017), can incur an undesirable multiplicative sensitivity increase which would diminish the utility of \\\\( \\\\zeta \\\\)-DP-SGD. We exemplify this phenomenon and the noise savings \\\\( CR \\\\)-calculus can enable in Appendix A.5 and Section 5.1. \\\\( \\\\zeta \\\\)-DP-SGD is presented in Algorithm 1 and relies on a modification of the gradient clipping step: we clip the conjugate gradient, which represents the direction of steepest ascent for a loss function.\"}"}
{"id": "Clre-Prt128", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\n$\\\\nabla L$: \\\\( \\\\hat{B}_L B_{\\\\theta_1}, \\\\ldots, B_{\\\\theta_n} \\\\)\\n\\nwhere \\\\( \\\\theta_1, \\\\ldots, \\\\theta_n \\\\) is the conjugate weight vector. We remark that, due to the aforementioned properties of the complex GM, the algorithm is compatible with other first-order optimisers (such as Adam), as well as other clipping techniques, such as per-layer clipping (McMahan et al., 2018). Moreover, newer methods for analysing mechanism composition, such as the Fourier Accountant (Koskela et al., 2020) can be used.\\n\\n**Algorithm 1** \\\\( \\\\zeta \\\\)-DP-SGD\\n\\n**Require:**\\n- Database with samples \\\\( x_1, \\\\ldots, x_N \\\\)\\n- PReLU neural network with loss function \\\\( L \\\\) and weight vector \\\\( w \\\\).\\n- Hyperparameters: learning rate \\\\( \\\\eta \\\\), noise variance \\\\( \\\\sigma^2 \\\\), sampling probability \\\\( R \\\\), gradient norm bound \\\\( B \\\\), total steps \\\\( T \\\\).\\n\\n1. Initialize \\\\( \\\\theta_0 \\\\) randomly\\n2. For \\\\( t \\\\) from 1 to \\\\( T \\\\):\\n   1. Draw a lot \\\\( L_t \\\\) with sampling probability \\\\( R \\\\) using Poisson or uniform sampling.\\n   2. Compute per-sample conjugate gradient.\\n   3. For each \\\\( i \\\\) from 1 to \\\\( N \\\\), compute \\\\( g_{p x_i q} \\\\) as \\\\( \\\\nabla L_{p \\\\theta_t} \\\\).\\n   4. Clip conjugate gradient.\\n   5. Apply the Complex Gaussian Mechanism and average.\\n   6. Descend \\\\( \\\\theta_t \\\\)\\n\\n**Output**\\n- Updated neural network weight vector \\\\( \\\\theta_T \\\\) and compute the privacy cost.\\n\\n5 E XPERIMENTAL EVALUATION\\n\\nThroughout this section, we present results from the experimental evaluation of \\\\( \\\\zeta \\\\)-DP-SGD. Details on dataset preparation and training can be found in Appendix A.6. All \\\\( \\\\epsilon \\\\)-values are converted (losslessly) from GDP. RDP guarantees are slightly looser and provided in Appendix A.2. We also provide an experimental evaluation of commonly used complex-valued activation functions for neural network training with \\\\( \\\\zeta \\\\)-DP-SGD in Appendix A.3. Lastly, we provide additional benchmark experiments demonstrating a complex-valued variant of the MNIST dataset in Appendix A.4.\\n\\n5.1 BENCHMARKING \\\\( \\\\zeta \\\\)-DP-SGD ON CIFAR-10\\n\\nWe begin by demonstrating that the utilisation of the complex Gaussian mechanism and \\\\( \\\\zeta \\\\)-DP-SGD provides increased model utility for the same privacy budget, even on tasks with real-valued inputs in which no additional information from the input's imaginary component is available. Moreover, we empirically confirm that, for \\\\( \\\\zeta \\\\)-DP-SGD, leveraging the Wirtinger/CR-calculus for gradient computations with respect to complex-valued weights results in a lower sensitivity penalty, and thus improved performance compared to \\\"simulating\\\" complex-valued neural networks with real-valued weights in \\\\( \\\\mathbb{R}^2 \\\\) similar to Trabelsi et al. (2017). Further theoretical and experimental evaluation of this phenomenon can be found in Appendix A.5.\\n\\nWe selected the CIFAR-10 dataset (Krizhevsky et al., 2009), a challenging dataset in the (real-valued) DP setting. We selected the model architecture recently reported by Papernot et al. (2020), which we equipped with either complex-valued weights (for training in \\\\( \\\\mathbb{C} \\\\)) or with an additional weight matrix (for training in \\\\( \\\\mathbb{R}^{2 \\\\times n} \\\\)). We trained to the same \\\\( \\\\epsilon \\\\) values as Papernot et al. (2020) (in RDP), but modified each image such that the real component contained the image and the imaginary component was zero-filled (for training in \\\\( \\\\mathbb{C} \\\\)), or added an additional trailing zero-filled channel (for training in \\\\( \\\\mathbb{R}^{2 \\\\times n} \\\\)). The results were identical when the imaginary/second channel components were filled with the same real-valued image. Table 1 summarises these results. Without DP, the increased\"}"}
{"id": "Clre-Prt128", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"learning capacity of the complex-valued weights led to improved test set performance compared to real-valued weights, with identical performance for training in $C$ and in $R^2_n$. When DP was employed, the model trained with $\\\\zeta$-DP-SGD achieved the best performance, outperforming both the real-valued DP model and the $R^2_n$ \\\"simulated\\\" complex-valued DP model. This result underlines the aforementioned theoretical finding that the complex Gaussian mechanism (contrary to the naive application of a real-valued multivariate Gaussian mechanism in $R^2_n$) provides improved performance for the same privacy budget even in tasks with real-valued input data.\\n\\nTable 1: Classification results for Sections 5.1, 5.2 and 5.3.\\n\\n| Dataset     | C: Complex-valued weights. DP: ($\\\\zeta$-)DP-SGD. For the CIFAR-10 dataset, $R^2_n$ indicates a network architecture utilising two channels to simulate complex inputs/model weights. GDP: Gaussian DP accounting. | \u2713 | | \u2713 | | \u2713 |\\n|-------------|--------------------------------------------------------------------------------------------------|---|---|---|---|\\n| CIFAR-10    | Accuracy | ROC-AUC | $F_1$-Score | Recall | GDP | $\\\\varepsilon$ | $\\\\delta$ |\\n|             | 80%      | 97%     | 80%          | 80%    | \u2713   | 80%          | 80%      |\\n| ECN         | 58%      | 91%     | 58%          | 57%    | \u2713   | 57%          | 70%      |\\n| SpeechCommands | 83%      | 98%     | 83%          | 83%    | \u2713   | 83%          | 83%      |\\n\\nThe advent of wearable devices incorporating electrocardiography (ECG) sensors has provided consumers the ability to detect signs of an abnormal heart rhythm. In this section, we demonstrate the utilisation of a small neural network architecture suitable for deployment, e.g. to a mobile device connected to such a biosensor, to be trained on ECG data from the the China Physiological Signal Challenge (CPSC) 2018 challenge dataset (Liu et al., 2018). We selected the task of automated Left Bundle Branch Block (LBBB) detection, formulated as a binary classification task against a normal (sinus) rhythm. This task is clinically relevant, as the sudden appearance of LBBB can herald acute coronary syndrome which requires urgent attention to avert myocardial infarction. As ECG data constitutes personal health information, its protection is mandated both legally and ethically. We utilised $\\\\zeta$-DP-SGD for training a complex-valued neural network on Fourier-transformed ECG acquisitions. We adopt this strategy as it can benefit from two key properties of the Fourier transform: ECG data can contain high-frequency noise which is irrelevant for diagnosis and can be reduced using Fourier filtering. Concurrently, this technique compresses the signal, which can drastically reduce the amount of data transferred. Table 1 shows classification results and Figure 1 shows exemplary source data.\\n\\nFigure 1: Exemplary ECG data used for classification. A shows an example of a sinus rhythm (normal ECG) and B shows an example of an ECG exhibiting signs of LBBB. Observe also the high frequency noise around the baseline which can be filtered using the Fourier transform.\"}"}
{"id": "Clre-Prt128", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Clre-Prt128", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\nMatthew Reimherr and Jordan Awan. Elliptical Perturbations for Differential Privacy. arXiv preprint arXiv:1905.09420, 2019.\\nKevin Scaman and Aladin Virmaux. Lipschitz regularity of deep neural networks: analysis and efficient estimation. arXiv preprint arXiv:1805.10965, 2018.\\nSimone Scardapane, Steven Van Vaerenbergh, Amir Hussain, and Aurelio Uncini. Complex-valued neural networks with nonparametric activation functions. IEEE Transactions on Emerging Topics in Computational Intelligence, 4(2):140\u2013150, 2018.\\nEfrat Shimron, Jonathan I. Tamir, Ke Wang, and Michael Lustig. Subtle Inverse Crimes: Naively training machine learning algorithms could lead to overly-optimistic results. arXiv preprint arXiv:2109.08237, 2021.\\nCaroline E Sweet. The Hidden Scam: Why Consumers Should No Longer Be Forced to Shoulder the Burden of Liability for Mobile Cramming. J. Bus. & Tech. L., 11:69, 2016.\\nChiheb Trabelsi, Olexa Bilaniuk, Ying Zhang, Dmitriy Serdyuk, Sandeep Subramanian, Joao Feilipe Santos, Soroush Mehri, Negar Rostamzadeh, Yoshua Bengio, and Christopher J Pal. Deep complex networks. arXiv preprint arXiv:1705.09792, 2017.\\nTim Van Erven and Peter Harremos. R\u00e9nyi divergence and Kullback-Leibler divergence. IEEE Transactions on Information Theory, 60(7):3797\u20133820, 2014.\\nPatrick Virtue, Stella Yu, and Michael Lustig. Better than real: Complex-valued neural nets for MRI fingerprinting. In 2017 IEEE international conference on image processing (ICIP), pp. 3953\u20133957. IEEE, 2017.\\nPete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint arXiv:1804.03209, 2018.\\nW. Wirtinger. Zur formalen Theorie der Funktionen von mehr komplexen Ver\u00e4nderlichen. Mathematische Annalen, 97(1):357\u2013375, 1927.\\nHongxu Yin, Arun Mallya, Arash Vahdat, Jose M Alvarez, Jan Kautz, and Pavlo Molchanov. See through Gradients: Image Batch Recovery via GradInversion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16337\u201316346, 2021.\\nXuejing Yuan, Yuxuan Chen, Yue Zhao, Yunhui Long, Xiaokang Liu, Kai Chen, Shengzhi Zhang, Heqing Huang, Xiaofeng Wang, and Carl A Gunter. Commandersong: A systematic approach for practical adversarial voice recognition. In 27th USENIX Security Symposium, pp. 49\u201364, 2018.\\nJure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew J Muckley, Aaron Defazio, Ruben Stern, Patricia Johnson, Mary Bruno, et al. fastMRI: An open dataset and benchmarks for accelerated MRI. arXiv preprint arXiv:1811.08839, 2018.\\nQuan Zhou, Jianhua Shan, Wenlong Ding, Chengyin Wang, Shi Yuan, Fuchun Sun, Haiyuan Li, and Bin Fang. Cough Recognition Based on Mel-Spectrogram and Convolutional Neural Network. Frontiers in Robotics and AI, 8, 2021.\\nAlexander Ziller, Dmitrii Usynin, Rickmer Braren, Marcus Makowski, Daniel Rueckert, and Georgios Kaissis. Medical imaging deep learning with differential privacy. Scientific Reports, 11(1):1\u20138, 2021.\"}"}
{"id": "Clre-Prt128", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proof of Theorem 1.\\nThe claim represents a generalisation of the Analytic Gaussian Mechanism (Balle & Wang, 2018) to MC. It suffices to show that the magnitude of the privacy-loss random variable $\\\\Omega$ is bounded by $\\\\epsilon$ with probability $1 - \\\\delta$. As shown in Dwork & Rothblum (2016) and Balle & Wang (2018), given some fixed output $O_P$, $\\\\Omega$ is given by:\\n\\n$$\\n\\\\Omega = \\\\log \\\\hat{P}_{pM} f_p D q - \\\\hat{P}_{pM} f_p D q\\n$$\\n\\n(14)\\n\\nwhere $\\\\log$ is the natural logarithm, and is distributed as:\\n\\n$$\\nN_{\\\\hat{P}_{pM} f_p D q - \\\\hat{P}_{pM} f_p D q, 2/2}\\\\sigma^2,\\n$$\\n\\n(15)\\n\\nAs $\\\\hat{P}_{pM} f_p D q - \\\\hat{P}_{pM} f_p D q\\\"p f_p D q\\\\hat{P}_{pM} f_p D q - \\\\hat{P}_{pM} f_p D q\\\"H P R$, where $H$ denotes the Hermitian transpose, $\\\\Omega$ has a real-valued mean and hence follows a real-valued normal distribution, even when $O_P$. From here, the proof proceeds identically to the proof to Theorem 8 of (Balle & Wang, 2018).\\n\\nFor proving Theorem 2, we will rely on the following fact about the R\u00e9nyi divergence of order $\\\\alpha$ between arbitrary distributions:\\n\\nCorollary 1 (Definition 2 in (Van Erven & Harremos, 2014)).\\n\\nLet $P$ and $Q$ be two arbitrary distributions defined on a measurable space $X, F$ with densities $p x, q x$. Then, for $\\\\alpha > 1$:\\n\\n$$\\nD_\\\\alpha P \\\\parallel Q = \\\\frac{1}{\\\\alpha - 1} \\\\log \\\\frac{1}{\\\\alpha} \\\\int p x^{\\\\alpha} q x^{1 - \\\\alpha} d x.\\n$$\\n\\n(16)\\n\\nIn particular, for two normal distributions with means $\\\\mu_0$ and $\\\\mu_1$ and common variance $\\\\sigma^2 I$:\\n\\n$$\\nD_\\\\alpha \\\\overline{N_{\\\\mu_0, \\\\sigma^2 I}} \\\\parallel \\\\overline{N_{\\\\mu_1, \\\\sigma^2 I}} = \\\\frac{1}{\\\\alpha - 1} \\\\log \\\\frac{1}{\\\\alpha} \\\\int (x - \\\\mu_0)^{\\\\alpha} (x - \\\\mu_1)^{1 - \\\\alpha} d x.\\n$$\\n\\n(17)\\n\\nwhere $x\\\"y$ denotes the inner product.\\n\\nWe can now prove Theorem 2:\\n\\nProof of Theorem 2.\\n\\nBy Definition 7 and the additive property of the Gaussian distribution, the density functions of $MC$ on $f_p D q$ and $f_p D 1 q$ follow a circularly symmetric complex-valued Gaussian distribution with means $f_p D q$ and $f_p D 1 q$ and common covariance matrix $\\\\sigma^2 I$. By substituting in equation (17):\\n\\n$$\\nD_\\\\alpha \\\\overline{N_{f_p D q, \\\\sigma^2 I}} \\\\parallel \\\\overline{N_{f_p D 1 q, \\\\sigma^2 I}} = \\\\frac{1}{\\\\alpha - 1} \\\\log \\\\frac{1}{\\\\alpha} \\\\int (x - f_p D q)^{\\\\alpha} (x - f_p D 1 q)^{1 - \\\\alpha} d x.\\n$$\\n\\n(18)\\n\\nHence, to preserve $\\\\alpha, \\\\rho - RDP$, it suffices to choose $\\\\sigma^2$ such that $\\\\sigma^2 \\\\geq \\\\alpha \\\\Delta^2 / 2 \\\\rho$.\\n\\nProof of Theorem 3.\\n\\nWe recall that the trade-off function between $MC$ and $MC$ is given by:\\n\\n$$\\nG(\\\\mu) = T_{\\\\overline{N_{\\\\mu, \\\\sigma^2 I}} \\\\parallel \\\\overline{N_{\\\\mu, \\\\sigma^2 I}}},\\n$$\\n\\n(19)\\n\\nand is strictly monotonically decreasing in $\\\\mu$ such that if $\\\\mu_1 \\\\geq \\\\mu_2$, then $G(\\\\mu_1) \\\\leq G(\\\\mu_2)$. To preserve $\\\\mu - GDP$, we require that:\\n\\n$$\\nT_{\\\\overline{N_{f_p D q, \\\\sigma^2 I}} \\\\parallel \\\\overline{N_{f_p D 1 q, \\\\sigma^2 I}}} \\\\geq G(\\\\mu),\\n$$\\n\\n(20)\\n\\n14\"}"}
{"id": "Clre-Prt128", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"intuitively, that the outputs of $M$ are at least as hard to distinguish as the distributions $N_{p_0, q_1}$ and $N_{p_\\\\mu, q_1}$ given a single draw. We have:\\n\\n$$T \\\\overset{\\\\sim}{=} N_{C_p \\\\theta \\\\mathcal{D}_{q_1}, C_p \\\\theta \\\\mathcal{D}_{q_1}, \\\\Phi \\\\hat{\\\\Phi}_{\\\\alpha}} \\\\overset{\\\\sim}{=} G\\\\overset{\\\\sim}{=} f_{p \\\\mathcal{D}_{q_1}, f_{p \\\\mathcal{D}_{q_1}, \\\\sigma}} \\\\overset{\\\\sim}{=}(\\\\Phi \\\\hat{\\\\Phi}_{\\\\alpha})$$  \\\\hspace{1cm} (21)\\n\\nWe require $G\\\\hat{\\\\Phi}_{\\\\alpha}$ such that $\\\\sigma_{\\\\alpha}$ and the strict monotonicity of $G_{\\\\mu}$. Thus, to preserve $\\\\mu$-GDP, one must choose $\\\\sigma_{\\\\alpha} \\\\geq p_\\\\mathcal{D}_{q_1}\\\\hat{\\\\Phi}_{\\\\alpha}$.\\n\\nA.2 R\u00b4ENYI ANALYSIS OF THE MAIN MANUSCRIPT EXPERIMENTS\\n\\nTable 3 provides an overview of the differences between the privacy bounds of RDP and GDP for the experiments in the main manuscript. We recall that GDP provides a tight analysis of the Gaussian mechanism (Dong et al., 2019) and a lossless conversion between GDP and $\\\\varepsilon, \\\\delta$-DP is possible, whereas RDP cannot be losslessly converted. Of note, we are already using the improved conversion formula from RDP to $\\\\varepsilon, \\\\delta$-DP shown in Balle et al. (2020) instead of the original formula proposed in Mironov (2017).\\n\\nTable 3: Comparison of privacy analyses using R\u00b4enyi DP (RDP) and Gaussian DP (GDP).\\n\\n| Dataset          | RDP $\\\\delta$ | GDP $\\\\delta$ |\\n|------------------|--------------|--------------|\\n| CIFAR-10         | 7.63         | 7.54         |\\n| ECG              | 1.76         | 1.62         |\\n| SpeechCommands   | 1.47         | 1.39         |\\n| FastMRI          | 0.67         | 0.16         |\\n| PhaseMNIST       | 0.53         | 0.51         |\\n\\nA.3 BENCHMARKING COMPLEX-VALUED ACTIVATION FUNCTIONS FOR $\\\\zeta$-DP-SGD\\n\\nA number of specialised activation functions designed for utilisation with complex-valued neural networks have been proposed in literature. To guide practitioner choice in our newly proposed setting of $\\\\zeta$-DP-SGD training, we here provide activation function benchmarks on the SpeechCommands dataset used in Section 5.3 of the main manuscript. Table 4 summarises these results. We consistently found the inverted Gaussian (iGaussian) activation function to perform best in the $\\\\zeta$-DP-SGD setting. This may be in part due to its bounded magnitude, thereby recapitulating the effect Papernot et al. (2020) discuss for real-valued networks, i.e. that bounded activation functions lead to improved performance in DP-SGD. We leave the further investigation of this finding to future work.\\n\\nA.4 BENCHMARKING $\\\\zeta$-DP-SGD ON PHASEMnist\\n\\nThe MNIST dataset (LeCun et al., 2010) is widely used as a benchmark dataset in real-valued DP-SGD literature. We here therefore show the experimental evaluation of an adapted, complex-valued version of MNIST, which we term PhaseMNIST. The details of how this dataset can be constructed as well as details on the used model architectures are provided in Appendix A.6. In brief, for each example of the original MNIST dataset with label $L_R p_0, L_I p_0, \\\\ldots, L_I p_9$, we obtain the imaginary component by selecting an image with label $L_I$ such that $L_R \\\\overset{\\\\sim}{=} L_I$ resulting in an input image arrangement $p_0, p_9, p_1, p_8, \\\\ldots, p_9, p_0$. Only the label of the real-valued image is used. The results are summarised in Table 5, where we also provide baselines for real-valued MNIST training on the same architecture (with real-valued weights). For simplicity, we have included the RDP and GDP guarantees in the same table.\"}"}
{"id": "Clre-Prt128", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We present $\\\\zeta$-DP, an extension of differential privacy (DP) to complex-valued functions. After introducing the complex Gaussian mechanism, whose properties we characterise in terms of $p\\\\varepsilon,\\\\delta q$-$DP$, R\u00e9nyi DP and Gaussian DP, we present $\\\\zeta$-DP stochastic gradient descent ($\\\\zeta$-DP-SGD), a variant of DP-SGD for training complex-valued neural networks. We experimentally evaluate $\\\\zeta$-DP-SGD on three complex-valued tasks, i.e. electrocardiogram classification, speech classification and magnetic resonance imaging (MRI) reconstruction. Moreover, we benchmark the performance of our methods on real- and complex-valued variants of the CIFAR-10 and MNIST datasets as well as a large range of complex-valued activation functions. Our experiments demonstrate that DP training of complex-valued neural networks is possible with rigorous privacy guarantees and excellent utility and represents a promising technique to mitigate privacy-utility trade-offs.\\n\\n1 INTRODUCTION\\n\\nThe ability to harness diverse, feature-rich datasets for algorithm training can allow the scientific community to create machine learning (ML) models capable of solving challenging data-driven tasks. These include the creation of robust autonomous vehicles (Rao & Frtunikj, 2018), early-stage cancer discovery (Cruz & Wishart, 2006) or disease survival prediction (Rau et al., 2018). A subclass of these ML problems is able to profit particularly from the ability to execute deep learning workflows over complex-valued datasets, such as magnetic resonance imaging (MRI) (Virtue et al., 2017) or time-series data (Fan & Xiong, 2013; Kociuba & Rowe, 2016). Complex-valued deep learning has seen increased traction in the past years, owing in part to the improved support by ML frameworks and the broader availability of graphics processing unit (GPU) hardware able to tackle the increased computational requirements (Bassey et al., 2021). However, since complex numbers are often used to represent signals derived from sensitive biological or medical records (Cole et al., 2020; K\u00fcstner et al., 2020; Peker, 2016), privacy constraints can render such datasets hard to obtain. The resulting data scarcity impairs effective model training, prompting the adoption of regulation-compliant and privacy-preserving methods for data access.\\n\\nDistributed computation methods such as federated learning (FL) (Kone\u010dn\u00fd et al., 2016) can partially address this requirement by only requiring participants to share results of the local computation rather than exchange data over the network. However, FL on its own has repeatedly been shown to be insufficient in the task of privacy protection (Geiping et al., 2020; Yin et al., 2021). Thus, bridging the gap between data protection and utilisation for algorithmic training requires methods able to offer objective privacy guarantees. Differential privacy (DP) (Dwork et al., 2014) has established itself as the cornerstone of such techniques and has been deployed in contexts like the US Census (Abowd, 2018) and distributed learning on mobile devices (Cormode et al., 2018). DP's purview has been expanded to encompass deep learning through the introduction of DP stochastic gradient descent (DP-SGD) (Abadi et al., 2016), allowing for the training of deep neural networks on private data. So far, however, the application of DP to complex-valued ML tasks remains drastically under-explored. Our work attempts to address this challenge through the following contributions:\"}"}
{"id": "Clre-Prt128", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. We extend DP to the complex domain through a collection of techniques we refer to as $\\\\zeta$-DP. We use this term instead of \\\"complex-valued DP\\\" for brevity and to avoid confusion with the abbreviation \\\"cDP\\\", which is already used for concentrated DP (Dwork & Rothblum, 2016). The letter $\\\\zeta$ alludes to the complex-valued Riemann $\\\\zeta$ function and is intended to convey the notion of \\\"continuation\\\" to the complex domain.\\n\\n2. We define and discuss the complex Gaussian Mechanism (GM) in Section 4.1 and show that its properties generalise corresponding results on real-valued functions. This allows us to interpret the complex GM through the lens of previous work on $p_{\\\\varepsilon,\\\\delta}$-DP, R\u00b4enyi-DP (RDP) and Gaussian DP (GDP).\\n\\n3. To enable the design and privacy-preserving training of complex-valued deep learning models, we introduce $\\\\zeta$-DP-SGD in Section 4.2.\\n\\n4. In Section 5 we experimentally evaluate the aforementioned techniques on several real-life neural network training tasks, i.e. speech classification, abnormality detection in electrocardiograms and magnetic resonance imaging (MRI) reconstruction. Moreover, we establish baselines for future work by providing benchmark results on real- and complex-valued variants of the MNIST and CIFAR-10 datasets and on complex neural network activation functions both with and without $\\\\zeta$-DP-SGD.\\n\\n5. We conclude that our DP analysis and the utilisation of tight sensitivity accounting through the Wirtinger/CR-calculus allows practitioners to avail themselves of the higher learning capacity of neural networks with complex-valued weights to achieve improved utility with the same privacy guarantees as corresponding real-valued architectures.\\n\\n**2 RELATED WORK**\\n\\nPrior work has addressed several challenges in non-private complex-valued deep learning, including the introduction of appropriate activation functions, and has presented applications in domains such as MRI reconstruction (K\u00a8ustner et al., 2020) or time series analysis (Fink et al., 2014). For a detailed overview of methodology and applications, we refer to Hirose (2012); Bassey et al. (2021). Until recently, deep learning frameworks did not fully support complex arithmetic and automatic differentiation. Hence, previous works (Trabelsi et al., 2017; Nazarov & Burnaev, 2020) express $\\\\mathbb{C}^n$ as $\\\\mathbb{R}^{2n}$ and use two real-valued channels rather than complex floating-point numbers. This approach can lead to a spurious increase in function sensitivity by incorrectly computing gradient magnitudes and, by extension, to the addition of excessive noise in the private setting, adversely impacting utility. Our work specifically addresses this shortcoming through the use of complex-valued weights and the Wirtinger/CR-calculus (Wirtinger, 1927; Kreutz-Delgado, 2009). Only a limited number of studies have utilised DP techniques in conjunction with complex-valued data (Fan & Xiong, 2013; Fioretto et al., 2019), however, to our knowledge none has formalised a notion of complex-valued DP or investigated neural network applications.\\n\\nThe $p_{\\\\varepsilon,\\\\delta}$-definition of DP and the Gaussian mechanism are essential to our formalism, and details on their real-valued definitions can be found in Dwork et al. (2014). As stated above, DP-SGD was introduced by Abadi et al. (2016). R\u00b4enyi-DP (RDP) was introduced by Mironov (2017) as a relaxation of $p_{\\\\varepsilon,\\\\delta}$-DP with favourable properties under composition, rendering it particularly useful for DP-SGD privacy accounting. Gaussian DP (GDP) was introduced by Dong et al. (2019) and tailored to the specific properties of the Gaussian mechanism, for which it provides a tight privacy analysis.\\n\\n**3 BACKGROUND**\\n\\nWe begin by introducing key terminology required in the rest of our work. We assume that a trusted analyst in possession of sensitive data wishes to publish the results of some analysis performed on this data while offering the individuals to whom the data belongs a DP guarantee. We will refer to the set of all sensitive records as the **sensitive database** $D$, whereby we assume that one individual\u2019s data is only present in the database once. Let $X$, the metric space of all sensitive databases, be equipped with the Hamming metric $d_X$ and let $D_{\\\\Delta X}$. $D_{\\\\Delta X}$'s adjacent database $D_1$ can be constructed from $D$ by adding or removing exactly one database row (that is, one individual\u2019s data), such that...\"}"}
{"id": "Clre-Prt128", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The analyst executes a query (function) $f$, for example a mean calculation, over the database. We first define the sensitivity of $f$:\\n\\n**Definition 1 (Sensitivity $\\\\Delta$ of $f$).**\\n\\nLet $X$ and $d_X$ be defined as above. $f$ maps the elements of $X$ to elements of a metric space $Y$ equipped with a metric $d_Y$. The (global) sensitivity $\\\\Delta$ of $f$ is then defined as:\\n\\n$$\\\\Delta = \\\\max_{D, D'} \\\\| f(D) - f(D') \\\\|_{d_Y}$$\\n\\n(1)\\n\\nThe maximum is taken over all adjacent database pairs in $X$. When $Y$ is the Euclidean space and $d_Y$ is the $L_2$ metric, $\\\\Delta$ is referred to as the $L_2$ sensitivity. We will only use the $L_2$ sensitivity in this work.\\n\\nIn private data analysis and ML, we are often concerned with differentiable functions; for Lipschitz-continuous query functions, the equivalence of the Lipschitz constant and the $L_2$-sensitivity (Raskhodnikova & Smith, 2016) can be exploited:\\n\\n**Definition 2 (Lipschitz constant $K$ of $f$).**\\n\\nLet $X, Y, d_X$ and $d_Y$ be defined as above. Then $f$ is said to be $K$-Lipschitz continuous if and only if a non-negative real number $K$ exists for which the following holds:\\n\\n$$\\\\| f(D) - f(D') \\\\|_{d_Y} \\\\leq K \\\\| D - D' \\\\|_{d_X}$$\\n\\n(2)\\n\\nEvidently, $K = \\\\Delta$ by Equation (1) and the definition of adjacency. Moreover, let $D$ be the differential operator; then $K = \\\\sup \\\\{ \\\\nabla f \\\\}$, where $\\\\nabla f$ is the operator norm (O'Searcoid, 2006).\\n\\nTherefore, for a scalar-valued query function, $\\\\Delta = K = \\\\sup \\\\{ \\\\nabla f \\\\}$.\\n\\nA DP mechanism adds noise to the results of $f$ calibrated to its sensitivity. Here, we provide the definition of the (real-valued) Gaussian mechanism (GM):\\n\\n**Definition 3 (Gaussian mechanism).**\\n\\nThe Gaussian mechanism $M$ operates on the results of a query function $f: \\\\mathbb{R}^n \\\\rightarrow \\\\mathbb{R}^d$ with sensitivity $\\\\Delta$ over a sensitive database $D$ by outputting $f(D) + \\\\xi$, where $\\\\xi \\\\sim N(0, \\\\sigma^2 I_d)$. Here, $I_d$ denotes the identity matrix with $d$ diagonal elements and $\\\\sigma^2$ is the variance of Gaussian noise calibrated to $\\\\Delta$. The application of the GM with properly calibrated noise satisfies $\\\\epsilon, \\\\delta$-DP:\\n\\n**Definition 4 ($\\\\epsilon, \\\\delta$-DP).**\\n\\nThe randomised mechanism $M$ preserves $\\\\epsilon, \\\\delta$-DP if, for all pairs of inputs $D$ and $D'$ and all subsets $S$ of $M$'s range:\\n\\n$$P(M(f(D)) \\\\in S) \\\\leq e^\\\\epsilon P(M(f(D')) \\\\in S) + \\\\delta$$\\n\\n(3)\\n\\nA number of relaxations have been proposed to characterise the properties of the GM, of which R\u00b4enyi DP is arguably the most widely employed in DP deep learning frameworks owing to its favourable properties under composition.\\n\\n**Definition 5 (R\u00b4enyi DP).**\\n\\n$M$ preserves $\\\\alpha, \\\\rho$-R\u00b4enyi-DP (RDP) if, for all pairs of inputs $D$ and $D'$:\\n\\n$$D_\\\\alpha \\\\parallel M(f(D)) \\\\parallel M(f(D')) \\\\leq \\\\rho$$\\n\\n(4)\\n\\nwhere $D_\\\\alpha$ denotes the R\u00b4enyi divergence of order $\\\\alpha \\\\geq 1$ between $M(f(D))$ and $M(f(D'))$. At $\\\\alpha = 1$, the divergence is defined by continuity as the Kullback-Leibler divergence. We note that the term \u201cbetween\u201d is an abuse of terminology, as the R\u00b4enyi divergence is asymmetric. In general, we will use $D_\\\\alpha(p, q)$ to denote $\\\\sup_t D_\\\\alpha(p_t, q_t)$.\\n\\nGaussian DP (GDP) was introduced as a variant of f-DP by Dong et al. (2019) specifically tailored to the properties of the GM, and provides the tightest possible characterisation of its properties.\\n\\nRelying on statistical hypothesis testing, f-DP interprets DP through a trade-off function between the Type I and Type II statistical errors faced by an adversary trying to determine whether one of the adjacent databases contains the individual or not. GDP is a specialisation of f-DP when the trade-off function has the form\\n\\n$$G(\\\\mu) = \\\\Phi(1 - \\\\alpha)$$\\n\\nwhere $\\\\alpha$ is the Type I statistical error and $\\\\Phi$ is the cumulative distribution of the standard, real-valued normal distribution.\\n\\n**Definition 6 (Gaussian DP).**\\n\\n$M$ preserves $\\\\mu$-Gaussian DP (GDP) if, for all pairs of adjacent databases $D$ and $D'$:\\n\\n$$T(M(f(D))), M(f(D')) \\\\leq G(\\\\mu)$$\\n\\n(5)\\n\\nwhere $T$ denotes a trade-off function. In this case, $G(\\\\mu) = \\\\Phi(1 - \\\\alpha)$.\"}"}
{"id": "Clre-Prt128", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: ROC-AUC (mean \\\\(\\\\bar{\\\\cdot}\\\\) STD) of complex-valued activation functions on the SpeechCommand dataset trained with identical settings and the same network architecture over five repetitions with \\\\(\\\\zeta\\\\)-DP-SGD.\\n\\n| Activation function Reference | ROC-AUC \\\\(\\\\bar{\\\\cdot}\\\\) STD |\\n|-------------------------------|-------------------------------|\\n| Separable Sigmoid Nitta (1997) | 52.9 \\\\(\\\\pm\\\\) 0.02%           |\\n| \\\\(z\\\\)ReLU Guberman (2016)     | 54.7 \\\\(\\\\pm\\\\) 0.03%           |\\n| Trainable Cardioid (per-feature bias) Virtue et al. (2017) | 80.2 \\\\(\\\\pm\\\\) 0.02%           |\\n| SigLog Georgiou & Koutsougeras (1992) | 87.3 \\\\(\\\\pm\\\\) 0.01%           |\\n| Trainable ModReLU (per-feature bias) Arjovsky et al. (2016) | 89.0 \\\\(\\\\pm\\\\) 0.01%           |\\n| Cardioid Virtue et al. (2017) | 89.2 \\\\(\\\\pm\\\\) 0.01%           |\\n| Trainable Cardioid (single bias) Virtue et al. (2017) | 89.4 \\\\(\\\\pm\\\\) 0.02%           |\\n| ModReLU (single bias) Arjovsky et al. (2016) | 89.5 \\\\(\\\\pm\\\\) 0.01%           |\\n| cReLU Trabelsi et al. (2017) | 91.9 \\\\(\\\\pm\\\\) 0.01%           |\\n| iGaussian Virtue et al. (2017) | 93.4 \\\\(\\\\pm\\\\) 0.01%           |\\n\\nTable 5: Results for PhaseMNIST training in a private (\\\\(\\\\zeta\\\\)-DP-SGD) and non-private (non-DP) fashion. Results for real-valued MNIST are provided for approximate comparison using the same model architecture (but with real-valued weights) trained with identical settings.\\n\\n|                          | Accuracy | ROC-AUC | \\\\(F_1\\\\)-score | Recall | \\\\(RDP\\\\)-\\\\(\\\\varepsilon\\\\) | \\\\(GDP\\\\)-\\\\(\\\\varepsilon\\\\) | \\\\(\\\\delta\\\\) |\\n|--------------------------|----------|---------|---------------|--------|--------------------------|--------------------------|-----------|\\n| PhaseMNIST \\\\(\\\\zeta\\\\)-DP-SGD | 99.0%    | 100%    | 99.0%         | 99.0%  | 0.53                     | 0.51                      | 5.53      |\\n| MNIST DP-SGD              | 95.7%    | 99.9%   | 95.7%         | 95.7%  | 0.53                     | 0.51                      | 5.53      |\\n| PhaseMNIST non-DP         | 99.3%    | 100%    | 99.2%         | 99.2%  | 0.88                     | 0.88                      | 8.80      |\\n| MNIST non-DP              | 97.4%    | 100%    | 97.4%         | 97.4%  | 0.88                     | 0.88                      | 8.80      |\\n\\nAs with the CIFAR-10 benchmarks in the main manuscript, we found that \\\\(\\\\zeta\\\\)-DP-SGD outperformed the real-valued network. In this case, additional information is available from the imaginary component of the image in addition to the higher entropic capacity of the network due to the complex-valued weights. A similar phenomenon was observed by Scardapane et al. (2018).\\n\\nA.5 WIRTINGER CR-CALCULUS\\n\\nIn this section, we present key results from Wirtinger (or \\\\(CR\\\\)-) calculus which are used in our work. For a detailed treatment, we refer to Kreutz-Delgado (2009).\\n\\nConsider a function \\\\(f: \\\\mathbb{C} \\\\rightarrow \\\\mathbb{C}\\\\). As for real-valued functions, the derivative of \\\\(f\\\\) at a point \\\\(z \\\\in \\\\mathbb{C}\\\\) can be defined as:\\n\\n\\\\[\\n\\\\frac{df}{dz} = \\\\lim_{h \\\\rightarrow 0} \\\\frac{f(z+h) - f(z)}{h}, \\\\quad h \\\\in \\\\mathbb{C}.\\n\\\\]\\n\\nIf this limit is defined for the (infinitely many) series approaching \\\\(z\\\\), \\\\(f\\\\) is called complex differentiable (equivalently, differentiable in the complex sense). If, in addition, \\\\(\\\\frac{df}{dz}\\\\) exists everywhere in the neighbourhood \\\\(U\\\\) of \\\\(z\\\\), \\\\(f\\\\) is called holomorphic. It is also possible to write \\\\(z = x + yi\\\\) and to then express \\\\(f\\\\) as two real-valued functions \\\\(u\\\\) and \\\\(v\\\\) of the variables \\\\(x\\\\) and \\\\(y\\\\):\\n\\n\\\\[\\nf(x + yi) = u(x,y) + vi(x,y), \\\\quad x, y \\\\in \\\\mathbb{R}.\\n\\\\]\\n\\n\\\\(f\\\\) can then be written as:\\n\\n\\\\[\\n\\\\frac{df}{dz} = \\\\frac{du}{dx} + \\\\frac{dv}{dy} - i\\\\left(\\\\frac{dv}{dy} + \\\\frac{du}{dx}\\\\right).\\n\\\\]\\n\\nIf this derivative exists at \\\\(z\\\\), \\\\(f\\\\) is called differentiable in the real sense. This interpretation represents \\\\(\\\\mathbb{C}\\\\) as \\\\(\\\\mathbb{R}^2\\\\) or, more generally, for vector-valued functions, \\\\(\\\\mathbb{C}^n\\\\) as \\\\(\\\\mathbb{R}^{2n}\\\\). The Cauchy-Riemann equations state that, for \\\\(f\\\\) to be holomorphic, it must satisfy:\\n\\n\\\\[\\n\\\\frac{du}{dx} = \\\\frac{dv}{dy} \\\\quad \\\\text{and} \\\\quad \\\\frac{du}{dx} = -\\\\frac{dv}{dy}.\\n\\\\]\"}"}
{"id": "Clre-Prt128", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nAs discussed above, the complex plane does not admit a natural ordering. Hence, the minimisation of a complex-valued function is not defined. Therefore, for complex-valued deep learning, we only consider real-valued (loss-) functions \\\\( f: \\\\mathbb{C} \\\\rightarrow \\\\mathbb{R} \\\\). By equation (25), \\\\( \\\\|v_{p,x,y,q_i}\\\\| = 0 \\\\). Thus, by the Cauchy-Riemann equations, such a real-valued function is only holomorphic if:\\n\\n\\\\[\\n\\\\frac{\\\\partial u}{\\\\partial x} = \\\\frac{\\\\partial u}{\\\\partial y} = 0.\\n\\\\]\\n\\n(28)\\n\\nThis means that any holomorphic real-valued function must be constant, which invalidates its usefulness for optimisation. The Wirtinger/CR-derivatives provide an alternative interpretation of the Cauchy-Riemann equations which allows us to consider holomorphicity and differentiability in the real sense separately. Thus, they recover the usefulness of interpreting \\\\( \\\\mathbb{C}^n \\\\) as \\\\( \\\\mathbb{R}^{2n} \\\\) while preventing multiplicative penalties on the gradient norm as a consequence of following this interpretation \u201ctoo closely\u201d. We will motivate this somewhat informal notion with an example below. The Wirtinger/CR-derivatives of \\\\( f \\\\) are defined as:\\n\\n\\\\[\\n\\\\frac{\\\\partial f}{\\\\partial z} = \\\\frac{1}{2} \\\\left( \\\\frac{\\\\partial f}{\\\\partial x} - i \\\\frac{\\\\partial f}{\\\\partial y} \\\\right),\\n\\\\]\\n\\n\\\\[\\n\\\\frac{\\\\partial f}{\\\\partial \\\\bar{z}} = \\\\frac{1}{2} \\\\left( \\\\frac{\\\\partial f}{\\\\partial x} + i \\\\frac{\\\\partial f}{\\\\partial y} \\\\right).\\n\\\\]\\n\\n(29)\\n\\nAn immediate consequence of this definition is that the Cauchy-Riemann equations can be expressed as:\\n\\n\\\\[\\n\\\\frac{\\\\partial f}{\\\\partial \\\\bar{z}} = 0.\\n\\\\]\\n\\n(30)\\n\\nTherefore, if a function \\\\( f \\\\) is holomorphic, \\\\( \\\\frac{\\\\partial f}{\\\\partial \\\\bar{z}} \\\\) corresponds to the derivative in the complex sense (that is, \\\\( \\\\frac{df}{dz} \\\\)) while, if \\\\( f \\\\) is differentiable in the real sense, both \\\\( \\\\frac{\\\\partial f}{\\\\partial z} \\\\) and \\\\( \\\\frac{\\\\partial f}{\\\\partial \\\\bar{z}} \\\\) are valid (and are conjugates of each other). As stated above, it can be shown that the steepest ascent of \\\\( f \\\\) is aligned with \\\\( \\\\frac{\\\\partial f}{\\\\partial \\\\bar{z}} \\\\). In this sense, \\\\( \\\\frac{\\\\partial f}{\\\\partial \\\\bar{z}} \\\\) fulfils the role of the \\\\( \\\\nabla \\\\) operator for real, scalar-valued loss functions. Evidently, compared to the actual gradient of \\\\( f \\\\) in the real sense, the following relationship holds:\\n\\n\\\\[\\n\\\\frac{\\\\partial f}{\\\\partial \\\\bar{z}} = \\\\frac{1}{2} \\\\nabla f = \\\\frac{1}{2} \\\\left( \\\\frac{\\\\partial f}{\\\\partial x} - i \\\\frac{\\\\partial f}{\\\\partial y} \\\\right).\\n\\\\]\\n\\n(31)\\n\\nHowever, re-defining \\\\( \\\\nabla f := \\\\frac{\\\\partial f}{\\\\partial \\\\bar{z}} \\\\) is desirable (and correct, as shown by Brandwood (1983)). We will motivate this requirement with an example: Let \\\\( f: \\\\mathbb{C} \\\\rightarrow \\\\mathbb{R} \\\\) be a function such that:\\n\\n\\\\[\\nf_p(z_q) = z \\\\overline{z} = p\\\\overline{p}x^2 + q\\\\overline{q}y^2.\\n\\\\]\\n\\n(32)\\n\\nThe Wirtinger/CR-derivative of \\\\( f \\\\) is \\\\( \\\\frac{\\\\partial f}{\\\\partial \\\\bar{z}} = z \\\\), whose \\\\( L^2 \\\\)-norm is \\\\( |z| = \\\\sqrt{a^2 + b^2} \\\\). The same output can be realised by interpreting \\\\( f \\\\) as a function of a real-valued vector \\\\( a = p\\\\overline{p}x, q\\\\overline{q}y \\\\). The gradient of \\\\( f \\\\) is \\\\( \\\\nabla f = p\\\\overline{p}2x, q\\\\overline{q}2y \\\\), whose \\\\( L^2 \\\\)-norm is:\\n\\n\\\\[\\n\\\\|\\\\nabla f\\\\| = \\\\sqrt{(a^2 + b^2)^2} = |z|^2 = \\\\sqrt{a^2 + b^2}.\\n\\\\]\\n\\nThe undesirable multiplicative penalty, which would translate to a superfluous multiplicative increase in the noise scale of the GM to preserve DP, is a consequence of \u201cignoring\u201d the connection between real and imaginary part inherent to complex numbers, but not to components of vectors. In fact, \\\\( z \\\\overline{z} \\\\) is neither equivalent to \\\\( z^2 \\\\) (as would be the case if \\\\( z_p \\\\overline{R}_a \\\\), \\\\( a \\\\) ), nor is it equivalent to \\\\( x\\\\overline{a}, y\\\\overline{a} \\\\), \\\\( \\\\overline{2} \\\\), as the complex inner product lacks the bilinearity inherent to its real-valued counterpart. Both complications are avoided by the re-definition of the Wirtinger/CR-derivative as the gradient used for optimisation, which prompts its utilisation in our work.\\n\\nTo exemplify this finding in the training of neural networks, we conducted the following experiment: We generated one thousand points using the function \\\\( f_p(z_q) = \\\\sin(p\\\\overline{z}_q) + \\\\mathcal{N}(0,1) \\\\). We then trained two separate neural networks to fit the sine function and measured the norms of the gradient vectors after 100 training steps (when gradient norms were empirically highest). One network included complex-valued weights (\\\\( \\\\mathbb{C} \\\\)), the other simulated complex-valued weights with an additional weight matrix per layer (\\\\( \\\\mathbb{R}^{2n} \\\\)). Both networks utilised the Cardioid activation function and were 1.\\n\\nThe term derivative represents an abuse of terminology, as they are formal operators and not derivatives with respect to actual variables. However, the interpretation as derivatives is intuitive, and we will thus retain it.\"}"}
{"id": "Clre-Prt128", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\ntrained with the Adam optimiser with a learning rate of 0.01 using a set of identical random seeds over 100 repetitions for 1000 steps. We utilised the Mean Squared Error (MSE) loss function given by\\n\\n$$f_p y, \\\\hat{y} \\\\approx \\\\frac{1}{2} \\\\{ y - \\\\hat{y} \\\\}^2, p y, \\\\hat{y} \\\\in \\\\mathbb{C}.$$  \\n\\nFor the $C$ network, we calculated $CR$-derivatives, while for the $R^2_n$ network, standard gradients were obtained using the PyTorch automatic differentiation system.\\n\\nFigure 4 shows the gradient norms of the two networks. The $R^2_n$-network had significantly higher average gradient norms (Student $t$-test $p = 0.0043$) over the 100 repetitions.\\n\\nFigure 4: Gradient norms of the $C$ network calculated using the $CR$-calculus and of the $R^2_n$-network. $N = 100$ repetitions. Significantly higher gradient norms are observed when the $CR$-calculus is not used (Student $t$-test $p = 0.0043$).\\n\\nAs a note to practitioners, certain deep learning frameworks silently re-scale the Wirtinger/$CR$-gradient by $2\\\\hat{y}$ to avoid user confusion by a lower effective learning rate. To ascertain a correct implementation, we therefore recommend examining this behaviour by testing the gradient norm of known functions.\\n\\nA.6 DATASET PREPARATION AND MODEL TRAINING\\n\\nA.6.1 CIFAR-10\\n\\nDataset preparation\\n\\nWe utilised the CIFAR-10 dataset as described in Krizhevsky et al. (2009). For complex-valued training in $C$, we either zeroed or copied each image into the imaginary component, while for training in $R^2_n$, two separate training channels were used for each tensor. Results in Table 1 are for the zero imaginary/second channel component, but were identical (within $\\\\pm 1\\\\%$ for all metrics) for the case where the imaginary component was filled with the same image.\\n\\nModel training\\n\\nFor real-valued non-DP training, we used the model described in Papernot et al. (2020) (see Table 2) with the SGD optimiser at a learning rate of 0.1 with a momentum term of 0.9 and a cosine learning rate scheduler. The hyperbolic tangent (TanH) activation function was used.\\n\\nFor complex-valued non-DP training, we utilised the same architecture and activation functions, albeit with complex weights, or with an additional weight matrix per layer. For DP training, we used an $L^2$ clipping norm of 1.0, a noise multiplier of 0.61, a sampling rate of 0.005 and trained for 20 epochs. The SGD optimiser at a learning rate of 0.1 with a momentum term of 0.9 and a cosine learning rate scheduler was used. Interestingly, this arrangement led to a higher test set performance than reported by Papernot et al. (2020) in the non-DP setting, however we were unable to reproduce the test set accuracy reported in the real-valued DP setting. For training in $R^2_n$, we note that multivariate Gaussian noise was used instead of circularly symmetric complex-valued Gaussian noise, and \u201cstandard\u201d gradient computations were used instead of Wirtinger derivatives.\\n\\nA.6.2 ECG DATASET\\n\\nDataset preparation\\n\\nWe utilised the China Physiological Signal Challenge 2018 (Liu et al., 2018) dataset for this task. We used the normal and left bundle branch block classes and channel 3. The ECGs were loaded from the provided Matlab format using the SciPy library and trimmed or padded to a length of 5000. The numpy Fast Fourier Transform implementation was used whereby the\"}"}
{"id": "Clre-Prt128", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"signal was pre-trimmed to length 512. The final dataset consisted of 1012 training examples and 113 testing examples.\\n\\nModel training\\n\\nWe implemented a complex-valued fully-connected neural network architecture consisting of input/hidden layers with 512, 256, 128 units and a single output unit. The cReLU activation function was used both in the non-DP and the $\\\\zeta$-DP-SGD setting. The output layer implemented the magnitude operation followed by a logistic sigmoid activation function. Models were trained using the SGD optimiser at a learning rate of 0.08 with an $L_2$ regularisation of $5 \\\\hat{10}^{-3}$ for non-DP training and a learning rate of 0.05 for $\\\\zeta$-DP-SGD training, respectively. A batch size of 64 was used for non-private training and a sampling rate of 0.063 at a noise multiplier of 5 and an $L_2$ clipping norm of 0.5 for $\\\\zeta$-DP-SGD. $\\\\epsilon$ was calculated at a $\\\\delta$-value of $10^{-1}$ where $10^{12}$ is the number of training samples. Both models were trained for 100 epochs.\\n\\nA.6.3 SPEECH COMMAND CLASSIFICATION DATASET\\n\\nDataset preparation\\n\\nWe used a subset of the SpeechCommands dataset (Warden, 2018) as described above, consisting of 2000 samples each from the categories \\\"Yes\\\", \\\"No\\\", \\\"Up\\\", \\\"Down\\\", \\\"Left\\\", \\\"Right\\\", \\\"On\\\", \\\"Off\\\", \\\"Stop\\\", and \\\"Go\\\". Of these, 7200 examples were used as the training test and 800 as the testing set. The waveform data was decoded using the TensorFlow library and, where necessary, padded to a length of 16000 samples. The TensorFlow implementation of the Short time Fourier Transform function was used with a frame length of 255 and a frame step of 128.\\n\\nModel training\\n\\nFor this task, we employed a complex-valued 2D CNN consisting using filters of size $3 \\\\hat{3}$ without zero-padding and a stride of 1. The convolutional layers had 8, 16, 32, 64 output filters, whereby a MaxPooling layer was used between the second layer and the third layer and an adaptive MaxPooling layer after the final convolutional layer. The convolutional block was followed by a fully connected layer with 64 units and an output layer of 8 units. Both employed the iGaussian activation function. The non-DP model was trained at a batch size of 64 for 10 epochs at a learning rate of 0.1 using the Stochastic Gradient Descent optimiser, whereas the $\\\\zeta$-DP-SGD network was trained using a sampling rate of 0.009 for 5 epochs with the same learning rate and optimiser, a noise multiplier of 1 and an $L_2$ clipping norm of 2. We calculated $\\\\epsilon$ at a $\\\\delta$-value of $10^{-5}$.\\n\\nA.6.4 FAST MRI KNEE DATASET\\n\\nDataset preparation\\n\\nWe utilised the single coil knee MRI dataset of the fastMRI challenge proposed by Zbontar et al. (2018). We used the reference implementation 2, and employed the default settings using an acceleration rate of 4 and 8% of densely sampled k-space center lines in the mask. Masks are sampled pseudo-randomly during training time. The dataset offers 34742 train and 7135 validation images.\\n\\nModel training\\n\\nWe changed the U-Net network to use complex-valued weights and accept complex-valued inputs instead of the magnitude image employed in the original example. We replaced the original ReLU activation functions with CReLU. In the DP setting, we used a noise multiplier of 1.0, an $L_2$ clipping norm of 1.0 and a sampling rate of 3 for 10 epochs and calculated the $\\\\epsilon$ at a $\\\\delta$-value of $10^{-5}$. The learning rate was set to 0.001 using the RMSProp optimiser and a stepwise learning rate scheduler. We trained both in the non-private and the $\\\\zeta$-DP-SGD setting for 30 epochs and disabled the collection of running statistics in the Batch Normalisation layers to render them compatible with DP (in the utilised library, sample sizes of 1 are used, so Batch Normalisation corresponds to Instance Normalisation in this setting).\\n\\nA.6.5 PHASE MNIST\\n\\nDataset construction\\n\\nAs described in Appendix A.4 above, PhaseMNIST is intended as a benchmark dataset for complex-valued computer vision tasks and contains images of handwritten digits from 0 to 9. The training set consists of 60000 images and the testing set of 10000 images. For each\\n\\n3 The version of the dataset used in this study will be made publicly available upon acceptance.\"}"}
