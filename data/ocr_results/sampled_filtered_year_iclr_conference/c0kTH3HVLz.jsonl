{"id": "c0kTH3HVLz", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the fact that under normal-light condition, the input spike stream contains sufficient information and can be well extracted by the network Zhao et al. (2021); chen et al. (2022). Under low-light condition, proposed GISI can supplement the missing information in the input spike stream by maintaining the release time of both forward and backward spikes. If we do not maintain the release time of forward and backward spikes, we will obtain local inter-spike intervals (LISI), which will result in the loss of low-light scene information (see Fig. 6). Further, the attention module can adaptively select feature information from both the input spike stream and GISI based on light condition and LR-Rep is light-robust.\\n\\nTable 7: Comparison between Spk2ImgNet (S2I), WGSE and our method. The input spike stream size is \\\\(21 \\\\times 41 \\\\times 250 \\\\times 400\\\\). We test the average of 50 rounds for Inference time.\\n\\n| Method  | Para. | Train time | Inference time |\\n|---------|-------|------------|----------------|\\n| S2I     | 3.91m | 2h         | 1458.45ms      |\\n| WGSE    | 3.63m | 1h         | 1344.06ms      |\\n| Our     | 5.32m | 17h        | 818.03ms       |\\n\\nFigure 18: The reconstructed results on the real dataset Zhao et al. (2021).\\n\\nA.4 EXPERIMENT\\nA.4.1 MODEL EFFICIENCY\\n\\nTable. 7 demonstrates the training time and inference time of the supervised methods, i.e., Spk2ImgNet, WGSE and our method. Although our method requires more training time compared to Spk2ImgNet and WGSE (Recurrent-based networks typically consume more time during training due to Backpropagation Through Time (BPTT)), our method outperforms Spk2ImgNet and WGSE in terms of inference speed. Besides, due to the need to fuse both forward and backward\"}"}
{"id": "c0kTH3HVLz", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 19: The reconstructed results on the real dataset Dong et al. (2022).\\n\\nTemporal features, our method is offline, i.e., after spike camera collects spike stream for a long period of time, the data can be reconstructed. In future work, we would extend our method so to online reconstruct.\\n\\nA.4.2 REAL DATA\\nHere, we show more results on two real datasets. Fig. 18 and Fig. 19 show more reconstructed images. We find that for traditional methods, TFI performs better on low-light data than TFP, SNM and TFSTP. For deep learning-based methods, SSML introduces a large amount of motion blur while Spk2ImgNet and WGSE may introduces some loss in dark backgrounds. Our method restores texture details in low-light scenes clearly more than other methods. Besides, we also provide the adjusted results from STP based on our reconstruction results as shown in Fig. 20.\\n\\nA.4.3 SYNTHETIC DATA\\nHere, we show more results on synthetic dataset LLR. Fig. 21 shows more reconstruction results on proposed dataset LLR. We find that for traditional methods, TFI performs better on low-light data than TFP, SNM and TFSTP. For deep learning-based methods, SSML introduces a large amount of motion blur while Spk2ImgNet and WGSE may introduces some loss in dark backgrounds. Our method restores texture details in low-light scenes clearly more than other methods.\"}"}
{"id": "c0kTH3HVLz", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 21: The reconstructed results on LLR. N (L) means normal (low) light.\"}"}
{"id": "c0kTH3HVLz", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 22: A water polo bursting at high speed in a low-light indoor. We selected the reconstruction results under 6 sampling moments, and the interval between two adjacent sampling moments is 41/40000 s. The above is our method and the bottom is the state-of-the-art reconstruction method Zhang et al. (2023). We apply the traditional HDR method Ying et al. (2017) to reconstruction results because the scene is too dark.\\n\\nA.4.4 Stability of Continuously reconstructing\\n\\nOur reconstruction method is stable to spike stream at different moments. Fig. 22 shows the continuous motion of an object in a real high-speed low-light scene. We find that our method can clearly recover motion details at different moments, while the loss introduced by the state-of-the-art WGSE is varied at different time.\\n\\nA.4.5 Comparison of Quanta Image Sensors\\n\\nWe would like to discuss Quanta Image Sensors (QIS). Spike camera Zhu et al. (2019) and QIS Ma et al. (2022) (including CIS-QIS and SPAD-QIS) share some similar characteristics, such as high temporal resolution and 1-bit (0 or 1) data. Besides, they also have differences in principles and circuits. For one sampling (one frame), QIS records whether a photon has arrived during the sampling, with a corresponding pixel output of 1 if photons arrive, and 0 otherwise Ma et al. (2020). Different from QIS, spike camera continuously accumulates photons Zhu et al. (2019), and if the accumulated value reaches a fixed threshold, the pixel outputs 1 and the accumulation is reset. Otherwise, it outputs 0, and the accumulation value. The different principles result in distinct meanings of two data (QIS data and spike streams). In QIS, 1 reflects the information of a specific sampling. In contrast, in spike camera, 1 contains the information from previous multiple sampling, and adjacent spikes are interdependent. This also leads to differences in the data patterns. This characteristic brings both advantages and disadvantages. In terms of advantages, in spike cameras, the influence of photon shot noise on each spike is reduced as multiple samples of photons are dynamically accumulated together, while QIS is sensitive to poisson shot noise Ma et al. (2022). In terms of disadvantages, spike cameras face more challenges in low-light conditions due to difficulties in reaching the accumulation threshold (see limitation in Zhao et al. (2022b)). Furthermore, the pixel circuits of two cameras are also different. A spike camera continuously accumulates photons in the form of voltage and the voltage can be kept for next sampling. QIS cameras (using SPAD-QIS as an example) amplify the signal through the...\"}"}
{"id": "c0kTH3HVLz", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"In low-light spike streams, the valid information can greatly decrease. It includes two reasons: (a) The information carried by each spike from the input signal is greatly reduced due to the interference of noise. (b) The total number of spikes in spike streams decrease greatly. Based on (1) in our main paper, we can get the valid accumulation in a spike. First, for the pixel \\\\(x\\\\), the time to fire a spike, \\\\(t_x\\\\), can be written as,\\n\\n\\\\[ t_x = A - 1_x(\\\\cdot) \\\\]  \\n\\nNote that \\\\(A - 1_x(\\\\cdot)\\\\) exists because \\\\(A_x(\\\\cdot)\\\\) is monotonically increasing. Especially, when the light intensity is fixed, i.e., \\\\(I_{in}(x, \\\\tau) = I\\\\), (11) can be written as,\\n\\n\\\\[ t_x = \\\\phi(I + I_{dark}(x)) - 1 \\\\]  \\n\\nFigure 12: Influence of input current on spikes. In low-light environment, i.e., input current is low, the time to fire a spike is long and the valid accumulation in each spike is small. Further, we can get the valid accumulation from input current \\\\(I_{in}\\\\) in a spike, \\\\(Q_x(I)\\\\), as,\\n\\n\\\\[ Q_x(I) = I_{It} = \\\\phi(I + I_{dark}(x)) - 1 \\\\]  \\n\\nThe orange curve in Fig. 12 shows that the valid accumulation \\\\(Q_x(I)\\\\) increases with increasing input current \\\\(I\\\\) which means each spike in low-light environments is more difficult to record information. The blue curve in Fig. 12 shows that time to fire spikes \\\\(t_x\\\\) decreases with increasing input current \\\\(I\\\\) which means the total information, i.e., the amount of spikes, in low-light spike stream is sparse. The above two characteristics explains the sparsity of information in low-light spike streams.\\n\\n![Diagram](image.png)\\n\\n**Table 1:** Car N (left) and Cook L (right) in LLR. N (L) means normal (low) light.\\n\\n| Light Type | Motion Type |\\n|------------|-------------|\\n| Sunlight   | Fixed Lens  |\\n| Area Light | (Power: 30W) |\\n\\n**A.2 Datasets Details**\\n\\nA.2.1 SCENE\\n\\nLLR serves as the test set and is designed to be as consistent as possible with the real world in order to effectively evaluate different methods. To achieve this, as shown in Fig. 13, we have carefully...\"}"}
{"id": "c0kTH3HVLz", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"designed the light source type and the illumination power for each scene to match the real world. Besides, motion of objects is close to the real world. The motion in Ball, Cook, Fan and Rotate is from Hu et al. (2022) while the motion in Car is created based on vehicle speed in real world.\\n\\nLight source set\\nWe set the lighting parameters in the advanced 3D graphics software, blender, to make the lighting conditions as consistent as possible with the real world. The following are the configuration details in Blender. In Blender, various types of lighting simulation functions, including sunlight, point lights, and area lights, have been integrated into the graphical interface. We can adjust lighting parameters to control brightness and darkness. For sunlight in Blender, the watts per square meter can be modified. Typically, 100 watts per square meter corresponds to a cloudy lighting environment. For the Car scene, we have set sunlight to 10 watts per square meter, which is deemed sufficiently low. For point lights and area lights, Blender allows modification of radiant Power, measured in watts. This is not the electrical power of consumer light bulbs. A light tube with the electrical power of 30W approximately corresponds to a radiant power of 1W. In the Cook scene, we have set an area light with the radiant Power to 1W (the electrical power of 30W). It already represents a very dim indoor light source.\\n\\n![Grayscale histogram](image1)\\n\\nFigure 14: (a) Grayscale histograms of images in low-light scenes, i.e., Ball, Car, Cook, Fan and Rotate with low-light light source. Each bar represents 5 grayscale levels. (b) Reference images.\\n\\nGrayscale The brightness is not only determined by the light source, but also by factors such as camera distance, object occlusion, and so on. These factors are ultimately reflected in the grayscale of the rendered images. Therefore, we calculate the grayscale histograms of images in low-light scenes. As shown in Fig. 14, we can see that the grayscale is diverse and in a lower range. To further demonstrate the performance advantages of our method under different lighting conditions, based on the scene Car, we generate spike streams by modifying the light source parameters. All results are shown in Table 4.\\n\\n![Motion direction histogram](image2)\\n\\nFigure 15: Motion direction histogram of optical flow in LLR.\\n\\nMotion The motion in LLR is diverse. We generate a optical flow every 40 frames for LLR. The degree distribution of the optical flow is in Fig. 15. We can find that the motion in LLR covers all kinds of directions.\"}"}
{"id": "c0kTH3HVLz", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: PSNR and SSIM of reconstruction results under different light sources. We set sunlight in the scene Car to 30, 50, 70, 90, 110 and 130 watts per square meter to render images respectively.\\n\\n| Light Level | WGSE (PSNR) | Ours (PSNR) | WGSE (SSIM) | Ours (SSIM) |\\n|------------|-------------|-------------|-------------|-------------|\\n| 30W        | 37.12       | 41.52       | 0.9514      | 0.9772      |\\n| 50W        | 35.68       | 40.69       | 0.9418      | 0.9748      |\\n| 70W        | 34.45       | 39.99       | 0.9324      | 0.9730      |\\n| 90W        | 33.88       | 39.51       | 0.9306      | 0.9711      |\\n| 110W       | 33.61       | 39.15       | 0.9305      | 0.9701      |\\n| 130W       | 33.36       | 38.91       | 0.9307      | 0.9693      |\\n\\nTable 5: Reconstruction results on synthetic dataset, LLR. Retrain idea: our method is retrained on the noise-free version of RLLR.\\n\\n| Metric | Our | Retrain idea |\\n|--------|-----|--------------|\\n| PSNR   | 45.075 | 37.679 |\\n| SSIM   | 0.98681 | 0.85374 |\\n\\nA.2.2 Impact of Spike Camera Noise on Performance\\n\\nIn proposed datasets, we have considered noise of spike camera refer to Zhao et al. (2022a). We further discuss the impact of noisy and noise-free spike streams on the performance of our method as shown in Table 5. We use an ideal spike camera model in SPCS (Hu et al., 2022) to synthesize a noise-free version of RLLR and retrain our method using the dataset (written as Retrain idea). We can find that our method has better performance than Retrain idea. Besides, Fig. 16 shows our method can handle noise in real spike streams better than Retrain idea.\\n\\nA.2.3 Retrain Dataset Size\\n\\nThe size of train datasets has an impact on the performance of our network. A larger train dataset typically provides more samples and a wider range of variations. In fact, proposed RLLR is enough for the reconstruction task of low-light spike streams. As shown in Table 6, we find that as the dataset size increase, the performance of the model also improve. However, it is observed that the improvement in performance becomes less significant after the dataset size reaches 60% of RLLR. It shows that the proposed RLLR is sufficient for training our network.\\n\\nA.3 LR-REDETAILS\\n\\nA.3.1 GISI Transform\\n\\nAs shown in Fig. 4 in our main paper, we first use the GISI transform to get the global inter-spike interval, GISI \\\\( t_i \\\\), from the input spike stream and the release time of forward and backward spikes. The GISI transform can be summarized as three steps (see Fig. 5): (a). Calculate the local inter-spike interval from input spike stream as chen et al. (2022); Zhao et al. (2022b) and we call it LISI transform for simplicity. (b). Update the local inter-spike interval as global inter-spike interval based on the release time of forward and backward spikes. (c). Maintain the release time of forward (backward) spikes of backward (forward) spike streams. Related details are shown in Algorithm 1. As shown in Fig. 17, GISI (our final method) not only outperform LISI (Baseline (E) in Table 2) in both PSNR and SSIM.\"}"}
{"id": "c0kTH3HVLz", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Evaluation results on LLR. We retrain our network where 20%, 40%, 60%, and 80% of RLLR data are used as training set respectively.\\n\\n| Metric    | 20%     | 40%     | 60%     | 80%     | 100%    |\\n|-----------|---------|---------|---------|---------|---------|\\n| PSNR      | 35.001  | 38.618  | 44.415  | 44.753  | 45.075  |\\n| SSIM      | 0.93411 | 0.97113 | 0.98459 | 0.98581 | 0.98681 |\\n\\nand SSIM on LLR but also have better generalization on real data. More importantly, the cost of using GISI instead of LISI is negligible (we only need to use two $400 \\\\times 250$ matrices to store the time of the forward spike and the backward spike, respectively), which does not affect the parameter and efficiency of the network.\\n\\nFigure 17: Reconstruction results on a real spike stream. The scene is a high-speed train that exceeds 200 km/h. The glass in the former result (left) shows obvious artifacts, and our result (right) is very smooth and natural.\\n\\nAlgorithm 1\\n\\nGISI Transform.\\n\\nRequire:\\n\\nThe spike streams at different time $\\\\{S_{t_i}|i = 1, 2, \\\\ldots, K\\\\}$, $K$ is the number of Continuous spike streams.\\n\\n1: Initialize forward state $\\\\text{Spike}_{\\\\text{forward}} \\\\{t_1\\\\} = 0$.\\n2: Initialize backward state $\\\\text{Spike}_{\\\\text{backward}} \\\\{t_K\\\\} = 2K \\\\Delta t$.\\n3: for $i$ from 1 to $K$ do\\n4: Calculate LISI $t_i$ based on $S_{t_i}$.\\n5: end for\\n6: for $i$ from 2 to $K$ do\\n7: Forward search the recent release time of spike to $t_i+1$, $\\\\text{Spike}_{\\\\text{forward}} \\\\{t_i\\\\}$ based on $S_{t_i}$.\\n8: if $\\\\text{Spike}_{\\\\text{forward}} \\\\{t_i\\\\}$ is None then\\n9: Set $\\\\text{Spike}_{\\\\text{forward}} \\\\{t_i\\\\} = \\\\text{Spike}_{\\\\text{forward}} \\\\{t_i\\\\} - 1$.\\n10: end if\\n11: Update $\\\\text{GISI} \\\\{t_i\\\\}$ based on $S_{t_i}$ and $\\\\text{Spike}_{\\\\text{forward}} \\\\{t_i\\\\}-1$.\\n12: end for\\n13: for $i$ from $K-1$ to 1 do\\n14: Backward search the recent release time of spike to $t_i-1$, $\\\\text{Spike}_{\\\\text{backward}} \\\\{t_i\\\\}$ based on $S_{t_i}$.\\n15: if $\\\\text{Spike}_{\\\\text{backward}} \\\\{t_i\\\\}$ is None then\\n16: Set $\\\\text{Spike}_{\\\\text{backward}} \\\\{t_i\\\\} = \\\\text{Spike}_{\\\\text{backward}} \\\\{t_i\\\\} + 1$.\\n17: end if\\n18: Update $\\\\text{GISI} \\\\{t_i\\\\}$ based on $S_{t_i}$ and $\\\\text{Spike}_{\\\\text{backward}} \\\\{t_i\\\\}+1$.\\n19: end for\\n20: Return $\\\\{\\\\text{GISI} \\\\{t_i\\\\}|i = 1, 2, \\\\ldots, K\\\\}$\\n\\nA.3.2 Robustness to Light Condition\\n\\nIn LR-Rep, we utilize an attention mechanism to fuse the input spike stream and proposed global inter-spike interval (GISI) to extract shallow features of areas with different brightness. We first state...\"}"}
{"id": "c0kTH3HVLz", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 23: The reconstructed results of Rotate N. Our method has clearer images.\\n\\nTable 8: Reconstruction results on synthetic dataset, LLR. We compare the open source Single Photon Avalanche Diode method, 3DCNN Chandramouli et al. (2019) (ICCP 2019) which is retrained on RLLR.\\n\\n| Method       | PSNR  | SSIM  |\\n|--------------|-------|-------|\\n| Our          | 45.075| 0.98681|\\n| 3DCNN        | 34.507| 0.93506|\"}"}
{"id": "c0kTH3HVLz", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Illustration of the proposed light-robust representation. We use convolution blocks to extract shallow features from input spike stream and GISI, respectively. Then they are fused by an attention block.\\n\\nlight intensity received in spike camera at time \\\\( t \\\\). Reconstruction is to use continuous spike streams, \\\\( \\\\{S_{t_i}, t_i=i\\\\cdot(2\\\\Delta t+1) | i=1,2,3,...K\\\\} \\\\) to restore the light intensity information at different time, \\\\( \\\\{Y_{t_i}, t_i=i\\\\cdot(2\\\\Delta t+1) | i=1,2,3,...K\\\\} \\\\). Generally, the temporal window \\\\( 2\\\\Delta t+1 \\\\) is set as 41 which is the same with Zhao et al. (2021); chen et al. (2022); Zhang et al. (2023).\\n\\n4.2 OVERVIEW\\n\\nTo overcome the challenge of low-light spike streams, i.e., the recorded information is sparse (see Fig.1), we propose a light-robust reconstruction method which can fully utilize temporal information of spike streams. It is beneficial from two modules: 1. A light-robust representation, LR-Rep. 2. A fusion module. As shown in Fig. 3, to recover the light intensity information at time \\\\( t_i, Y_{t_i} \\\\), we first calculate the light-robust representation at time \\\\( t_i \\\\), written as \\\\( \\\\text{Rep}_{t_i} \\\\). Then, we use a ResNet module to extract deep features, \\\\( F_{t_i} \\\\), from \\\\( \\\\text{Rep}_{t_i} \\\\). \\\\( F_{t_i} \\\\) is fused with forward (backward) temporal features as \\\\( F_{f_{t_i}}(F_{b_{t_i}}) \\\\). Finally, we reconstruct the image at time \\\\( t_i \\\\), \\\\( \\\\hat{Y}_{t_i} \\\\) with \\\\( F_{f_{t_i}} \\\\) and \\\\( F_{b_{t_i}} \\\\).\\n\\nFigure 5: Illustration of GISI transform for backward in a pixel. (a). Calculate the local inter-spike interval, \\\\( LISI_{t_i} \\\\) from the input spike stream (chen et al., 2022; Zhao et al., 2022b). (b). Update global inter-spike interval, \\\\( GISI_{t_i} \\\\) based on the release time of backward spike, \\\\( \\\\text{Spike}_{b_{t_i+1}} \\\\) and \\\\( LISI_{t_i} \\\\). (c). Maintain and transmit the release time of backward spike, \\\\( \\\\text{Spike}_{b_{t_i}} \\\\). Black (white) circle is a (no) spike and the red line is backward data flow. More details are shown in algorithm. 1.\\n\\n4.3 LIGHT-ROBUST REPRESENTATION\\n\\nAs shown in Fig. 4, a light-robust representation, LR-Rep, is proposed to aggregate the information in low-light spike streams. LR-Rep mainly consists of two parts, GISI transform and feature extraction.\\n\\nGISI transform\\n\\nCalculating the local inter-spike interval from the input spike stream is a common operation (chen et al., 2022; Zhao et al., 2022b) and we call it as LISI transform. Different from LISI transform, we propose a GISI transform which can utilize the release time of forward and backward\"}"}
{"id": "c0kTH3HVLz", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nFigure 6: (a) and (b) show the visualizations of GISI$_t$ and LISI$_t$ in a real spike stream. (c) shows the distribution of pixel-wise values in GISI$_t$ and LISI$_t$.\\n\\nSpikes to obtain the global inter-spike interval GISI$_t$. It needs to be performed twice, i.e., once forward and once backward respectively. Taking GISI transform for backward as an example, it can be summarized as three steps as shown in Fig. 5. Our GISI transform can extract more temporal information from spike streams than LISI transform as shown in Fig. 6.\\n\\nFeature extraction\\nAfter GISI transform, we separately extract shallow features of GISI$_t$ and input spike stream, $F_G$ and $F_S$ through convolution block. Finally, $Rep_t$ is obtained by an attention module where $F_G$ and $F_S$ are integrated, i.e.,\\n\\n$$[\\\\beta_t, \\\\alpha_t] = \\\\text{Att}(\\\\{F_G, F_S\\\\}),$$ (3)\\n\\n$$Rep_t = \\\\beta_t F_G + \\\\alpha_t F_S,$$ (4)\\n\\nwhere $\\\\text{Att}(\\\\cdot)$ denotes an attention block including 3-layer convolution with 3-layer activation function and $Rep_t$ is our LR-Rep at time $t_i$.\\n\\n4.4 Fusion and Reconstruction\\n\\nWe first extract the deep feature $F_{t_i}$ of $Rep_t$ through a ResNet with 16 layers. Then, as shown in Fig. 7(a), for forward, temporal features $F_{f_{t_i}}$ and $F_{t_i}$ are fused as temporal features of the input spike stream $F_{f_{t_i}}$. For backward, temporal features $F_{b_{t_i+1}}$ and $F_{t_i}$ are fused as temporal features of the input spike stream $F_{b_{t_i}}$. To avoid the misalignment of motion from different timestamps, we use a Pyramid Cascading and Deformable convolution (PCD) (Wang et al., 2019) to add alignment information to $F_{t_i}$. The above process can be written as,\\n\\n$$F_{t_i} = f(Rep_t),$$ (5)\\n\\n$$F_{f_{t_i}} = f([F_{t_i} + a(F_{f_{t_i}} - 1), F_{t_i}]),$$ (6)\\n\\n$$F_{b_{t_i}} = f([F_{t_i} + a(F_{b_{t_i+1}}, F_{t_i})]),$$ (7)\\n\\nwhere $f(\\\\cdot)$ denotes the feature extraction and $a(\\\\cdot, \\\\cdot)$ denotes the PCD module. Finally, as shown in Fig. 7(b), we use forward and backward temporal features ($F_{b_{t_i}}$ and $F_{f_{t_i}}$) to reconstruct the current scene at time $t_i$, i.e.,\\n\\n$$\\\\hat{Y}_{t_i} = c([F_{b_{t_i}}, F_{f_{t_i}}]),$$ (8)\\n\\n$$L = \\\\sum_{i=1}^{K} \\\\|\\\\hat{Y}_{t_i} - Y_{t_i}\\\\|_1,$$ (9)\\n\\nwhere $c(\\\\cdot)$ denotes 3-layer convolution with 2-layer ReLU, $L$ is loss function, $\\\\| \\\\cdot \\\\|_1$ denotes 1-norm and $K$ is the number of continuous spike streams.\"}"}
{"id": "c0kTH3HVLz", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We train our method in the proposed dataset, RLLR. Consistent with previous work Zhao et al. (2021); chen et al. (2022); Zhang et al. (2023), the temporal window of each input spike stream is 41. The spatial resolution of input spike streams is randomly cropped the spike stream to $64 \\\\times 64$ during the training procedure and the batch size is set as 8. Besides, forward (backward) temporal features and the release time of spikes in our method are maintained from 21 continuous spike streams. We use Adam optimizer with $\\\\beta_1 = 0.9$ and $\\\\beta_2 = 0.99$. The learning rate is initially set as $1e^{-4}$ and scaled by 0.1 after 70 epochs. The model is trained for 100 epochs on 1 NVIDIA A100-SXM4-80GB GPU.\\n\\n5.2 Results\\n\\nWe compare our method with traditional reconstruction methods, i.e., TFI (Zhu et al., 2019), STP (Zhu et al., 2021), SNM (Zhu et al., 2022) and deep learning-based reconstruction methods, i.e., SSML (chen et al., 2022), Spk2ImgNet (S2I) (Zhao et al., 2021), WGSE (Zhang et al., 2023). The supervised learning methods, S2I and WGSE, are retrained on RLLR and the parameter configuration is the same as their respective papers. We evaluate methods on two kinds of data:\\n\\n1. The carefully designed synthetic dataset, LLR.\\n2. The real spike streams dataset, PKU-Spike-High-Speed (Zhao et al., 2021) and low-light high-speed spike streams dataset (Dong et al., 2022).\\n\\nTable 1: PSNR, PSNR (scale) and SSIM of reconstruction results on synthetic dataset, LLR. PSNR (scale): PSNR is calculated after scaling both the ground truth and reconstructed images to 0-255.\\n\\n| Metric | TFI | SSML | S2I | STP | SNM | WGSE | Ours |\\n|--------|-----|------|-----|-----|-----|------|------|\\n| PSNR   | 31.409 | 38.432 | 40.883 | 24.882 | 25.741 | 42.959 | 45.075 |\\n| PSNR (scale) | 21.665 | 30.176 | 31.202 | 14.894 | 18.527 | 32.439 | 38.131 |\\n| SSIM   | 0.72312 | 0.89942 | 0.95915 | 0.55537 | 0.80281 | 0.97066 | 0.98681 |\\n\\nFig. 8: Results from different reconstruction methods on our LLR. More results are in appendix.\\n\\nResults on our synthetic dataset\\n\\nAs shown in Table 1, we use the two reference image quality assessment (IQA) metrics, i.e., PSNR and SSIM to evaluate the performance of different methods on LLR. We can find that our method achieves the best reconstruction performance and has a PSNR gain over 4dB than the reconstruction method, S2I, which demonstrates its effectiveness. Fig. 8 shows the visualization results from different reconstruction methods. We can find that our method can better restore motion details in dark regions than other methods.\\n\\nResults on real datasets\\n\\nFor real data, we test different methods on two spike stream datasets, PKU-Spike-High-Speed (Zhao et al., 2021) and low-light spike streams (Dong et al., 2022). PKU-Spike-High-Speed includes 4 high-speed scenes under normal-light conditions and Dong et al. (2022) includes 5 high-speed scenes under low-light conditions. Fig. 9 shows the reconstruction results.\\n\\nNote that we apply the traditional HDR method (Ying et al., 2017) to reconstruction results on Dong et al. (2022) because scenes are too dark. Our method can more effectively restore the information in scenes i.e., clear texture and less noise.\\n\\nWe perform a user study written as US (Wilson, 1981; Jiang et al., 2021) to quantify the visual quality of different methods. For each scene in datasets, we randomly select reconstructed images at the same\"}"}
{"id": "c0kTH3HVLz", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nFigure 9: Results from different reconstruction methods on the real datasets, PKU-Spike-High-Speed (Top) and low-light high-speed spike streams dataset (Bottom). More results are in our appendix.\\n\\nFigure 10: User study scores (\u2191) of reconstructed images from different methods. The max (min) score is 7 (1).\\n\\n3.7 A\\nBLATION\\nProposed modules\\nTo investigate the effect of the proposed light-robust representation LR-Rep, the adjacent (forward and backward) deep temporal features (ADF), i.e., F_b and F_f in our fusion module, the alignment information in our fusion module (AIF) and GISI transform in LR-Rep, we compare 5 baseline methods with our final method. (A) is the basic baseline without LR-Rep, ADF and AIF. Table. 2 shows ablation results on proposed dataset, LLR. The comparison between (A) and (C) ((B) and (D)) proves the effectiveness of LR-Rep. The comparison between (A) and (B) ((C) and (D)) proves the effectiveness of ADF. Further, by adding the alignment information in the fusion module i.e., AIF, our final method (F) appropriately reduces the misalignment of motion from different timestamps and can reconstruct high-speed scenes more accurately than (D). Besides, the comparison between (E) and (F) shows GISI has better performance than LISI. It is because GISI can extract more temporal information than LISI (see Fig. 6).\\n\\nComparison with other representation\\nWe compare the performance of different representation in our framework, i.e., (1) General representation of spike stream: TFI and TFP (Zhu et al., 2019) (2) Tailored representation for reconstruction networks: AMIM (chen et al., 2022) in SSML, SALI (Zhao et al., 2021) in S2I and WGSE-1d (Zhang et al., 2023) in WGSE. We replace LR-Rep in our....\"}"}
{"id": "c0kTH3HVLz", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"A LIGHT-ROBUST RECONSTRUCTION METHOD FOR SPIKE CAMERA\\n\\nABSTRACT\\n\\nSpike camera with high temporal resolution can fire continuous binary spike streams to record per-pixel light intensity. By using reconstruction methods, the scene details in high-speed scenes can be restored from spike streams. However, existing methods struggle to perform well in low-light environments due to insufficient information in spike streams. To this end, we propose a recurrent-based reconstruction framework to better handle such extreme conditions. In more detail, a light-robust representation (LR-Rep) is designed to aggregate temporal information in spike streams. Moreover, a fusion module is used to extract temporal features. Besides, we synthesize a reconstruction benchmark for high-speed low-light scenes where light sources are carefully designed to be consistent with reality. The experiment shows the superiority of our method. Importantly, our method also generalizes well to real spike streams. All codes and constructed datasets will be released after publication.\\n\\n1 INTRODUCTION\\n\\nLight intensity\\n\\nSample\\n\\nSample\\n\\nSample\\n\\nSpike streams\\n\\nNetwork\\n\\nNetwork\\n\\nNetwork\\n\\nWGSE\\n\\nWGSE\\n\\nWGSE\\n\\n(b)\\n\\n(a)\\n\\nWGSE\\n\\nOur\\n\\nResults at time 1\\n\\n2\\n\\n1\\n\\n2\\n\\nFigure 1: Overview of reconstruction for high-speed spike streams. Left: with decreasing light intensity, more sparse spike streams are difficult to extract features. A black circle is a spike. Middle: (a) The state-of-the-art method, WGSE (Zhang et al., 2023). The arrow with a gradient color is the timeline. (b) Our reconstruction method. Green (red) lines denote the forward (backward) data flow. \u2460(\u2461) is the release time of spikes (temporal features). \u2460(\u2461) in forward and backward data flow is independent. Right: reconstructed results from WGSE and our method.\\n\\nAs a neuromorphic sensor with high temporal resolution (40000 Hz), spike camera (Zhu et al., 2019; Huang et al., 2022) has shown enormous potential for high-speed visual tasks, such as reconstruction (Zhao et al., 2020; Zhu et al., 2022; Zheng et al., 2021; Zhao et al., 2021; Zhu et al., 2021; chen et al., 2022), optical flow estimation (Hu et al., 2022; Zhao et al., 2022b; Zhai et al., 2023), and depth estimation (Zhang et al., 2022; Liu et al., 2022; Li et al., 2022). Different from event cameras (Lichtsteiner et al., 2008; Delbr\u00fcck et al., 2010; Brandli et al., 2014), it can record per-pixel light intensity.\"}"}
{"id": "c0kTH3HVLz", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"intensity by accumulating photons and firing continuous binary spike streams. Correspondingly, high-speed dynamic scenes can be reconstructed from spike streams. Recently, many deep learning methods have advanced this field and shown great success in reconstructing more detailed scenes. However, existing methods struggle to perform well in low-light environments due to insufficient information in spike streams.\\n\\nA dilemma arises for visual sensors, that is, the quality of sampled data can greatly decrease in a low-light environment (Guo et al., 2020; Li et al., 2021; Zhao et al., 2022; Graca et al., 2023). Low-quality data creates many difficulties for all kinds of vision tasks. Similarly, the reconstruction for the spike camera also suffers from this problem. To improve the performance of reconstruction in low-light high-speed scenes, two non-trivial matters should be carefully considered. First, constructing a low-light high-speed scene benchmark for spike camera is crucial to evaluating different methods. However, due to the frame rate limitations of traditional cameras, it is difficult to capture images clearly in real high-speed scenes as supervised signals. Instead of it, a reasonable way is to synthesize datasets for spike camera (Zhao et al., 2021; Zhu et al., 2021; Hu et al., 2022; Zhang et al., 2022). To ensure the reliability of the reconstruction benchmark, synthetic low-light high-speed scenes should be as consistent as possible with the real world, e.g., light source. Second, as shown in Fig. 1, with the decrease of illuminance in the environment, the total number of spikes in spike streams decreases greatly which means the valid information in spike streams can greatly decrease. Fig. 1(a) shows that previous methods often fail under low-light conditions since they have no choice but to rely on inadequate information.\\n\\nIn this work, we aim to address all two issues above-mentioned. In more detail, a reconstruction benchmark for high-speed low-light scenes is proposed. We carefully design the scene by controlling the type and power of the light source and generating noisy spike streams based on Zhao et al. (2022). Besides, we propose a light-robust reconstruction method as shown in Fig. 1(b). Specifically, to compensate for information deficiencies in low-light spike streams, we propose a light-robust representation (LR-Rep). In LR-Rep, the release time of forward and backward spikes is used to update a global interspike interval (GISI). Then, to further excavate temporal information in spike streams, LR-Rep is fused with forward (backward) temporal features. During the feature fusion process, we add alignment information to avoid the misalignment of motion from different timestamps. Finally, the scene is clearly reconstructed from fused features.\\n\\nEmpirically, we show the superiority of our reconstruction method. Importantly, our method also generalizes well to real spike streams. In addition, extensive ablation studies demonstrate the effectiveness of each component. The main contributions of this paper can be summarized as follows:\\n\\n\u2022 A reconstruction benchmark for high-speed low-light scenes is proposed. We carefully construct varied low-light scenes that are close to reality.\\n\u2022 We propose a recurrent-based reconstruction framework where a light-robust representation, LR-Rep, and fusion module can effectively compensate for information deficiencies in low-light spike streams.\\n\u2022 Experimental results on real and synthetic datasets have shown our method can more effectively handle spike streams under different light conditions than previous methods.\\n\\n2 RELATED WORK\\n\\n2.1 LOW-LIGHT VISION\\n\\nLow-light environment has always been a challenge not only for human perception but also for computer vision methods. For traditional cameras, some works concern the enhancement of low-light images. Wei et al. (2018) propose the LOL dataset containing low/normal-light image pairs and propose a deep Retinex-Net including a Decom-Net for decomposition and an Enhance-Net for illumination adjustment. Jiang et al. (2021) proposes the EnlightenGAN which is first trained on unpaired data to low-light image enhancement. Guo et al. (2020) proposes Zero-DCE which formulates light enhancement as a task of image-specific curve estimation with a deep network. Besides, some work focuses on the robustness of vision task to low-light, e.g., object detection. Sasagawa & Nagahara (2020) proposes a method of domain adaptation for merging multiple models to handle objects in a low-light situation. Li et al. (2021a) integrates a new non-local feature...\"}"}
{"id": "c0kTH3HVLz", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\naggregation method and a knowledge distillation technique to with the detector networks. Wang et al. (2023) combines with the image enhancement algorithm to improves the accuracy of object detection. For spike camera, it are also affected by low-light environments. Dong et al. (2022) propose a real low-light high-speed dataset for reconstruction. However, it lacks corresponding image sequences as ground truth. To solve the challenge in the reconstruction of low-light spike streams, We have fully developed the task including a reconstruction benchmark for high-speed low-light scenes and a light-robust reconstruction method.\\n\\n2.2 Reconstruction for Spike Camera\\n\\nThe reconstruction of high-speed dynamic scenes has been a popular topic for spike camera. Based on the statistical characteristics of spike stream, Zhu et al. (2019) first reconstruct high-speed scenes. Zhao et al. (2020) improved the smoothness of reconstructed scenes by introducing motion aligned filter. Zhu et al. (2022) construct a dynamic neuron extraction model to distinguish the dynamic and static scenes. For enhancing reconstruction results, Zheng et al. (2021) uses short-term plasticity mechanism to exact motion area. Zhao et al. (2021) first proposes a deep learning-based reconstruction framework, Spk2ImgNet (S2I), to handle the challenges brought by both noise and high-speed motion. Chen et al. (2022) build a self-supervised reconstruction framework by introducing blind-spot networks. It achieves desirable results compared with S2I. The reconstruction method (Zhang et al., 2023) presents a novel Wavelet Guided Spike Enhancing (WGSE) paradigm. By using multi level wavelet transform, the noise in the reconstructed results can be effectively suppressed.\\n\\n2.3 Spike Camera Simulation\\n\\nSpike camera simulation is a popular way to generate spike streams and accurate labels. Zhao et al. (2021) first convert interpolated image sequences with high frame rate into spike stream. Based on Zhao et al. (2021), Zhu et al. (2021); Kang et al. (2021); Zhao et al. (2022a) add some random noise to generate spike streams more accurately. To avoid motion artifacts caused by interpolation, Hu et al. (2022) presents the spike camera simulator (SPCS) combining simulation function and rendering engine tightly. Then, based on SPCS, optical flow datasets for spike camera are first proposed. Zhang et al. (2022) generate the first spike-based depth dataset by the spike camera simulation.\\n\\n3 Reconstruction Datasets\\n\\nIn order to train and evaluate the performance of reconstruction methods in low-light high-speed scenes, we propose two low-light spike stream datasets, Low-Light Reconstruction (LRR) and Low-Light Reconstruction (LLR) based on spike camera model. RLLR is used as our train dataset and LLR is carefully designed to evaluate the performance of different reconstruction methods as test dataset. We first introduce the spike camera model, and then introduce our datasets where noisy spike streams are generated by the spike camera model.\\n\\nSpike camera model\\n\\nEach pixel on the spike camera model converts light signal into current signal and accumulate the input current. For pixel \\\\(x = (x, y)\\\\), if the accumulation of input current \\\\(I_{\\\\text{in}}(x, \\\\tau)\\\\) reaches a fixed threshold \\\\(\\\\phi\\\\), a spike is fired and then the accumulation can be reset as,\\n\\n\\\\[\\nA(x, t) = A_x(t) \\\\mod \\\\phi = \\\\int_0^t I_{\\\\text{tot}}(x, \\\\tau) d\\\\tau \\\\mod \\\\phi,\\n\\\\]\\n\\n(1)\\n\\n\\\\[\\nI_{\\\\text{tot}}(x, \\\\tau) = I_{\\\\text{in}}(x, \\\\tau) + I_{\\\\text{dark}}(x, \\\\tau),\\n\\\\]\\n\\n(2)\\n\\nwhere \\\\(A(x, t)\\\\) is the accumulation at time \\\\(t\\\\), \\\\(A_x(t)\\\\) is the accumulation without reset before time \\\\(t\\\\), \\\\(I_{\\\\text{in}}(x, \\\\tau)\\\\) is the input current at time \\\\(\\\\tau\\\\) (proportional to light intensity) and \\\\(I_{\\\\text{dark}}(x, \\\\tau)\\\\) is the main fixed pattern noise in spike camera, i.e., dark current (Zhu et al., 2021; Zhao et al., 2022a). Further, due to limitations of circuits, each spike is read out at discrete time \\\\(nT, n \\\\in \\\\mathbb{N}\\\\) (\\\\(T\\\\) is a micro-second level). Thus, the output of the spike camera is a spatial-temporal binary stream \\\\(S\\\\) with \\\\(H \\\\times W \\\\times N\\\\) size. The \\\\(H\\\\) and \\\\(W\\\\) are the height and width of the sensor, respectively, and \\\\(N\\\\) is the temporal window size of the spike stream. According to the spike camera model, it is natural that the spikes (or information) in low-light spike streams are sparse because reaching the threshold is lengthy. More details about low-light spike streams are in appendix.\"}"}
{"id": "c0kTH3HVLz", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Proposed datasets, RLLR and LLR. RLLR includes the random scenes and LLR includes carefully designed scenes. More details are in our appendix.\\n\\nRLLR\\n\\nAs shown in Fig. 2, RLLR includes 100 random low-light high-speed scenes where high-speed scenes are first generated by SPCS (Hu et al., 2022) and then the light intensity of all pixels in each scene is darkened by multiplying a random constant ($0 - 1$). Each scene in RLLR continuously records a spike stream with $400 \\\\times 250 \\\\times 1000$ size and corresponding image sequence. Then, for each image, we clip a spike stream with $400 \\\\times 250 \\\\times 41$ size from the spike stream as input.\\n\\nLLR\\n\\nAs shown in Fig. 2, LLR includes 5 $\\\\times$ 2 carefully designed high-speed scenes where we use the scenes with five kinds of motion (named Ball, Car, Cook, Fan and Rotate) and each scene corresponds to two light sources (normal and low). To ensure the reliability of our scenes, different light sources are used, and the power of light source is consistent with the real world. Each scene in LLR continuously records 21 spike streams with $400 \\\\times 250 \\\\times 41$ size and 21 corresponding images.\\n\\nIn proposed datasets, we consider noise of spike camera based on Zhao et al. (2022a). We further discuss the impact of noisy (noise-free) spike streams on the performance of methods in the appendix.\\n\\nFigure 3: Illustration of the proposed recurrent-based reconstruction framework. It includes a light-robust representation, feature extractor (ResNet), fusion and reconstruction. The green and red lines represent the forward and backward data flow. The two kinds of data flow are independent.\"}"}
{"id": "c0kTH3HVLz", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our LR-Rep achieves the best performance, meaning it can better adapt to our framework.\\n\\nTable 2: Ablation results on synthetic dataset, LLR.\\n\\n| Index | Effect of different network structures | PSNR | SSIM  |\\n|-------|--------------------------------------|------|-------|\\n| (A)   | Removing LR-Rep & AIF & ADF           | 42.743 | 0.97403 |\\n| (B)   | Removing LR-Rep & AIF                 | 44.151 | 0.98514 |\\n| (C)   | Removing AIF & ADF                    | 44.739 | 0.98636 |\\n| (D)   | Removing AIF                          | 44.956 | 0.98678 |\\n| (E)   | Replace GISI with LISI                | 44.997 | 0.98676 |\\n| (F)   | Our final method                      | 45.075 | 0.98681 |\\n\\nTable 3: Performance of different representation methods in our framework. All methods are trained on RLLR and are tested on LLR.\\n\\n| Rep.                  | PSNR | SSIM  |\\n|----------------------|------|-------|\\n| TFP                  | 38.615 | 0.96641 |\\n| TFI                  | 37.617 | 0.93632 |\\n| AMIM                 | 41.95  | 0.97493 |\\n| SALI                 | 43.314 | 0.98304 |\\n| WGSE-1d              | 42.302 | 0.97438 |\\n| LR-Rep               | 45.075 | 0.98681 |\\n\\nThe number of continuous spike streams has an impact on our method performance. Fig. 11 shows its effect on the performance. We can find that, as the number increases, the performance of our method can greatly increase until convergence. This is because, as the number increases, our method can utilize more temporal information until sufficient.\\n\\nFigure 11: Effect of the number of continuous spike streams on the performance. We test on the dataset, LLR.\\n\\n**CONCLUSION**\\n\\nWe propose a recurrent-based reconstruction framework for spike camera to better handle different light conditions. In our framework, a light-robust representation (LR-Rep) is designed to aggregate temporal information in spike streams. Moreover, a fusion module is used to extract temporal features.\\n\\nTo evaluate the performance of different methods in low-light high-speed scenes, we synthesize a reconstruction benchmark where light sources are carefully designed to be consistent with reality. The experiment on both synthetic data and real data shows the superiority of our method.\\n\\n**Limitation**\\n\\nDue to the need to fuse both forward and backward temporal features, our method is offline, i.e., after spike camera collects spike streams for a long period of time, the data can be reconstructed. However, real-time applications may require spike stream reconstruction to be performed while the spike camera is capturing the scene. In future work, we plan to extend our method so to online reconstruct.\"}"}
{"id": "c0kTH3HVLz", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Christian Brandli, Raphael Berner, Minhao Yang, Shih-Chii Liu, and Tobi Delbruck. A 240\u00d7180 130 dB 3 \u03bcs latency global shutter spatiotemporal vision sensor. *IEEE Journal of Solid-State Circuits (JSSC)*, 49(10):2333\u20132341, 2014.\\n\\nParamanand Chandramouli, Samuel Burri, Claudio Bruschini, Edoardo Charbon, and Andreas Kolb. A bit too much? high speed imaging from sparse photon counts. In *2019 IEEE International Conference on Computational Photography (ICCP)*, pp. 1\u20139. IEEE, 2019.\\n\\nShiyan chen, Chaoteng Duan, Zhaofei Yu, Ruiqin Xiong, and Tiejun Huang. Self-supervised mutual learning for dynamic scene reconstruction of spiking camera. In *Proceedings of the International Joint Conference on Artificial Intelligence, IJCAI*, pp. 2859\u20132866, 2022.\\n\\nTobi Delbr\u00fcck, Bernabe Linares-Barranco, Eugenio Culurciello, and Christoph Posch. Activity-driven, event-based vision sensors. *IEEE International Symposium on Circuits and Systems (ISCAS)*, pp. 2426\u20132429, 2010.\\n\\nYanchen Dong, Jing Zhao, Ruiqin Xiong, and Tiejun Huang. High-speed scene reconstruction from low-light spike streams. In *2022 IEEE International Conference on Visual Communications and Image Processing (VCIP)*, pp. 1\u20135. IEEE, 2022.\\n\\nRui Graca, Brian McReynolds, and Tobi Delbruck. Optimal biasing and physical limits of dvs event noise. *arXiv preprint arXiv:2304.04019*, 2023.\\n\\nChunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and Runmin Cong. Zero-reference deep curve estimation for low-light image enhancement. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 1780\u20131789, 2020.\\n\\nLiwen Hu, Rui Zhao, Ziluo Ding, Lei Ma, Boxin Shi, Ruiqin Xiong, and Tiejun Huang. Optical flow estimation for spiking camera. In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pp. 17844\u201317853, 2022.\\n\\nTiejun Huang, Yajing Zheng, Zhaofei Yu, Rui Chen, Yuan Li, Ruiqin Xiong, Lei Ma, Junwei Zhao, Siwei Dong, Lin Zhu, et al. 1000\u00d7 faster camera and machine vision with ordinary devices. *Engineering*, 2022.\\n\\nYifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, and Zhangyang Wang. Enlightengan: Deep light enhancement without paired supervision. *IEEE transactions on image processing*, 30:2340\u20132349, 2021.\\n\\nZhaodong Kang, Jianing Li, Lin Zhu, and Yonghong Tian. Retinomorphic sensing: A novel paradigm for future multimedia computing. In *Proceedings of the ACM International Conference on Multimedia (ACMMM)*, pp. 144\u2013152, 2021.\\n\\nChengxi Li, Xiangyu Qu, Abhiram Gnanasambandam, Omar A Elgendy, Jiaju Ma, and Stanley H Chan. Photon-limited object detection using non-local feature matching and knowledge distillation. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 3976\u20133987, 2021a.\\n\\nChongyi Li, Chunle Guo, Linghao Han, Jun Jiang, Ming-Ming Cheng, Jinwei Gu, and Chen Change Loy. Low-light image and video enhancement using deep learning: A survey. *IEEE transactions on pattern analysis and machine intelligence*, 44(12):9396\u20139416, 2021b.\\n\\nChongyi Li, Chunle Guo, and Chen Change Loy. Learning to enhance low-light image via zero-reference deep curve estimation. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 44(8):4225\u20134238, 2021c.\\n\\nJianing Li, Jiaming Liu, Xiaobao Wei, Jiyuan Zhang, Ming Lu, Lei Ma, Li Du, Tiejun Huang, and Shanghang Zhang. Uncertainty guided depth fusion for spike camera. *arXiv preprint arXiv:2208.12653*, 2022.\\n\\nPatrick Lichtsteiner, Christoph Posch, and Tobi Delbruck. A 128 \u00d7 128 120 dB 15 \u03bcs latency asynchronous temporal contrast vision sensor. *IEEE Journal of Solid-State Circuits (JSSC)*, 43(2):566\u2013576, 2008.\"}"}
{"id": "c0kTH3HVLz", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jiaming Liu, Qizhe Zhang, Jianing Li, Ming Lu, Tiejun Huang, and Shanghang Zhang. Unsupervised spike depth estimation via cross-modality cross-domain knowledge transfer. arXiv preprint arXiv:2208.12527, 2022.\\n\\nJiaju Ma, Stanley Chan, and Eric R Fossum. Review of quanta image sensors for ultralow-light imaging. IEEE Transactions on Electron Devices, 69(6):2824\u20132839, 2022.\\n\\nSizhuo Ma, Shantanu Gupta, Arin C Ulku, Claudio Bruschini, Edoardo Charbon, and Mohit Gupta. Quanta burst photography. ACM Transactions on Graphics (TOG), 39(4):79\u20131, 2020.\\n\\nXuanyu Qian, Wei Jiang, Ahmed Elsharabasy, and M Jamal Deen. Modeling for single-photon avalanche diodes: State-of-the-art and research challenges. Sensors, 23(7):3412, 2023.\\n\\nYukihiro Sasagawa and Hajime Nagahara. Yolo in the dark-domain adaptation method for merging multiple models. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXI 16, pp. 345\u2013359. Springer, 2020.\\n\\nJing Wang, Peng Yang, Yuansheng Liu, Duo Shang, Xin Hui, Jinhong Song, and Xuehui Chen. Research on improved yolov5 for low-light environment object detection. Electronics, 12(14):3089, 2023.\\n\\nXintao Wang, Kelvin C.K. Chan, Ke Yu, Chao Dong, and Chen Change Loy. Edvr: Video restoration with enhanced deformable convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2019.\\n\\nChen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. arXiv preprint arXiv:1808.04560, 2018.\\n\\nTom D Wilson. On user studies and information needs. Journal of documentation, 37(1):3\u201315, 1981.\\n\\nZhenqiang Ying, Ge Li, and Wen Gao. A bio-inspired multi-exposure fusion framework for low-light image enhancement. arXiv preprint arXiv:1711.00591, 2017.\\n\\nMingliang Zhai, Kang Ni, Jiucheng Xie, and Hao Gao. Spike-based optical flow estimation via contrastive learning. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1\u20135. IEEE, 2023.\\n\\nJiyuan Zhang, Lulu Tang, Zhaofei Yu, Jiwen Lu, and Tiejun Huang. Spike transformer: Monocular depth estimation for spiking camera. In European Conference on Computer Vision (ECCV), 2022.\\n\\nJiyuan Zhang, Shanshan Jia, Zhaofei Yu, and Tiejun Huang. Learning temporal-ordered representation for spike streams based on discrete wavelet transforms. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 137\u2013147, 2023.\\n\\nJing Zhao, Ruiqin Xiong, and Tiejun Huang. High-speed motion scene reconstruction for spike camera via motion aligned filtering. In International Symposium on Circuits and Systems (ISCAS), pp. 1\u20135, 2020.\\n\\nJing Zhao, Ruiqin Xiong, Hangfan Liu, Jian Zhang, and Tiejun Huang. Spk2imgnet: Learning to reconstruct dynamic scene from continuous spike stream. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11996\u201312005, 2021.\\n\\nJunwei Zhao, Shiliang Zhang, Lei Ma, Zhaofei Yu, and Tiejun Huang. Spikingsim: A bio-inspired spiking simulator. In 2022 IEEE International Symposium on Circuits and Systems (ISCAS), pp. 3003\u20133007. IEEE, 2022a.\\n\\nRui Zhao, Ruiqin Xiong, Jing Zhao, Zhaofei Yu, Xiaopeng Fan, and Tiejun Huang. Learning optical flow from continuous spike streams. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2022b.\\n\\nYajing Zheng, Lingxiao Zheng, Zhaofei Yu, Boxin Shi, Yonghong Tian, and Tiejun Huang. High-speed image reconstruction through short-term plasticity for spiking cameras. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6358\u20136367, 2021.\"}"}
{"id": "c0kTH3HVLz", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
