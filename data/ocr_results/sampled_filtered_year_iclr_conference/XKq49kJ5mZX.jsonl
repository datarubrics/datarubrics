{"id": "XKq49kJ5mZX", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Notice, that in our case: \\\\[ \\\\Sigma_1 = \\\\Sigma_2 = \\\\sigma_I. \\\\] It implies: \\\\[ \\\\text{tr}(\\\\Sigma^{-1}\\\\Sigma) = \\\\text{tr}(I) = d \\\\tag{12} \\\\]\\n\\nand \\\\[ x^T\\\\Sigma x = \\\\sigma ||x||_2^2 \\\\quad \\\\forall x \\\\in \\\\mathbb{R}^d \\\\tag{13} \\\\]\\n\\nBy substituting Eq. 12 and Eq. 13 to Eq. 11 we get:\\n\\n\\\\[\\n\\\\text{JS}(||P||, |Q|) = \\\\frac{1}{2} \\\\sigma \\\\left( ||\\\\mu_2|| - ||\\\\mu_1|| \\\\right)^2 + \\\\frac{1}{2} \\\\sigma \\\\left( ||\\\\mu_1|| - ||\\\\mu_2|| \\\\right)^2 \\\\tag{14}\\n\\\\]\\n\\nthat concludes our statement.\\n\\nThe results for policy uncertainty for continuous action space corresponded to Eq. 7 are depicted on Fig. 11.\\n\\nThe results for policy uncertainty for discrete action space corresponded to Eq. 8 action space are depicted on Fig. 12.\\n\\nWe find that the alternative uncertainty measure produces very similar estimates to the one provided in Eq. 3 and Eq. 4 (compare to Fig. 23 and Fig. 28). Both policy uncertainty measures gave similar results within a margin of error in all considered environments. We, therefore, decided to present the more conceptually simple measure in the main part.\\n\\n(a) Masksembles\\n(b) MC Dropout\\n(c) Ensemble\\n\\nFigure 11: ROC curve for alternative JS-based policy uncertainty for HalfCheetah-v3 environment\\n\\n(a) Masksembles\\n(b) MC Dropout\\n(c) Ensemble\\n\\nFigure 12: ROC curve for alternative JS-based policy uncertainty for Seaquest-v4 environment\\n\\nF I N V E S T I G A T I O N O F S C A L I N G A B I L I T I E S O F M A S K S E M B L E S , M C D R O P O U T , A N D E N S E M B L E S\\n\\nScaling Behavior\\nWhen scaling the hidden layer size, we observe that only Masksembles show significantly increased performance. We hypothesize that the scaling can partially compensate for the\"}"}
{"id": "XKq49kJ5mZX", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"smaller layer size due to the introduction of masks. We like to note that we do not observe the same scaling properties for Dropout and Ensemble. For the baseline model, we expect the performance to be either lower or equal compared to the smaller network size, as the model size has been tuned as part of previous hyperparameter tuning. We evaluated the scaling properties in the context of MuJoCo. Results are presented in Fig. 13 and Fig. 14.\\n\\nFigure 13: Scaling properties of different methods in Ant-v3: We find that only Masksembles show improved performance when increasing the network size. The other methods may even drop in performance when scaling the layers. Results averaged across 3 runs for 64 hidden units and 2 runs for 128 hidden units.\\n\\nFigure 14: Scaling properties of different methods in Walker2d-v3: We evaluate the scaling across 64 (default), 96, and 128 hidden units. We find a clear improvement for Masksembles, and a slight improvement for MC Dropout, while the baseline and Ensemble policies are mainly unaffected. Although the baseline policy achieves a higher reward earlier, the final reward of the baseline policy is very close to the original one.\\n\\nG VALUE UNCERTAINTY AND POLICY UNCERTAINTY\\n\\nIn addition to ROC curve for value uncertainty for HalfCheetah-v3 in Fig. 15 we also provide ROC curve for policy uncertainty (please see Fig. 23). Apart from that, we provide value uncertainty (corresponded to Eq. 6) and policy uncertainty (corresponded to Eq. 3 for continuous case and Eq. 4 for discrete action space) ROC curves for the following environments: Ant-v3 (see Fig. 16, 24), Walker-v3 (see Fig. 17, 25), Swimmer-v3 (18, 26), Pacman-v4 (see Fig. 19, 27), Seaquest-v4 (see Fig. 20, 28), Berzerk-v4 (see Fig. 21, 29), and SpaceInvaders.\\n\\nIt is important to note that ROC curves for ensembles could be invalid (see Fig. 16d, 24d) because the qualities of the Ensembles policies are quite poor in general. As a result, it cannot validly differentiate between ID and OOD states.\\n\\nAnother important notice regarding the \\\"convex shape\\\" of ROC curves for MC Dropout (see Fig. 16b, 23b). In the case of simple binary classification, labels are nothing but marking the class belongings, and changing the label \\\"0\\\" to \\\"1\\\" and vice versa will not affect the classification results. This symmetry leads to symmetry of the ROC curve with respect to the diagonal ROC curve (that is, the ROC curve corresponded to a random binary classifier), and it means that labels should be inverted. After the inversion of labels, the ROC curve will have a normal \\\"concave shape.\\\"\"}"}
{"id": "XKq49kJ5mZX", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nA convex curve, in the case of binary classification, for ID and OOD states, actually reveals the inability of the model to correctly identify OOD states, which is particularly dangerous because the model is more confident in its behavior in these states than in known distribution.\\n\\n(a) Masksembles\\n(b) MC Dropout\\n(c) MC Dropconnect\\n(d) Ensemble\\n\\nFigure 15: Value Uncertainty: ROC Curves for the HalfCheetah-v3 environment\\n\\n(a) Masksembles\\n(b) MC Dropout\\n(c) MC Dropconnect\\n(d) Ensemble\\n\\nFigure 16: Value Uncertainty: ROC Curves for the Ant-v3 environment\\n\\n(a) Masksembles\\n(b) MC Dropout\\n(c) MC Dropconnect\\n(d) Ensemble\\n\\nFigure 17: Value Uncertainty: ROC Curves for the Walker2d-v3 environment\\n\\n(a) Masksembles\\n(b) MC Dropout\\n(c) MC Dropconnect\\n(d) Ensemble\\n\\nFigure 18: Value Uncertainty: ROC Curves for the Swimmer-v3 environment\"}"}
{"id": "XKq49kJ5mZX", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\n(a) Masksembles\\n(b) MC Dropout\\n(c) MC Dropconnect\\n(d) Ensemble\\n\\nFigure 19: Value Uncertainty: ROC Curves for the MsPacman-v4 environment\\n\\nFigure 20: Value Uncertainty: ROC Curves for the Seaquest-v4 environment\\n\\nFigure 21: Value Uncertainty: ROC Curves for the Berzerk-v4 environment\\n\\nFigure 22: Value Uncertainty: ROC Curves for the SpaceInvaders-v4 environment\"}"}
{"id": "XKq49kJ5mZX", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 35: ID and OOD samples for Atari: MsPacman-v4. In later stages, the level layout is fundamentally different. Trained agents only perform very poorly in these new layouts, i.e., we find that they do not generalize to these states and therefore treat them as out-of-distribution. Other details like the enemy ship above the water line or changing the color of fish also occur at later stages of the game.\\n\\nFigure 36: ID and OOD samples for Atari: Seaquest-v4. While core parts are shared (i.e., the core gameplay), important on-screen elements are not visible in ID states, e.g., none of the trained RL agents manages to collect a large number of divers or extra lives (yellow submarines at the top of the screen).\"}"}
{"id": "XKq49kJ5mZX", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we provide results regarding additional pixel perturbations. An average uncertainty score for ID states is sampled via rolling out the policy in the environment. This averaged uncertainty score is taken as a baseline for each method (score 1.0). We then compute the uncertainty score for: (1) Predicting based on a white frame, (2) based on a black frame, based on the original with increasing levels of added Gaussian noise: (3) $\\\\pm 50$, (4) $\\\\pm 100$, and (5) $\\\\pm 150$. Lastly, we experimented with applying a custom mask to the input image (e.g., masking out areas of the game frame). As a basis, we chose the SeaquestNoFrameskip-v4 environment. We report both the effect on value uncertainty (see Fig. 38) and policy uncertainty (see Fig. 39). Compared to the average ID uncertainty score (reported as score 1.0 on a per-method basis), the uncertainty scores are noticeably higher than the baseline score (averaged across non-corrupted ID states). The results on the extreme cases (black/white frame), as well as predicting on states with very high noise levels) show that the predicted uncertainty is not necessarily reliable. We based our decision to move to an advanced game-state attack on these initial results, as we found, e.g., that adding Gaussian noise is not a good representation for OOD states.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 39: The effect of different pixel perturbations on policy uncertainty. From left-to-right: no perturbation, replacement to black frame, replacement to white frame, adding Uniform Noise $[-50; 50]$, adding Uniform Noise $[-100; 100]$, adding Uniform Noise $[-150; 150]$, hiding parts of the images with the static mask.\\n\\nIn the Fig. 42 we provide policy uncertainty and value uncertainty plots for MsPacmanNoFrameskip-v4. In the Fig. 43 we provide policy uncertainty and value uncertainty plots for SeaquestNoFrameskip-v4. We observe that Masksembles are better for detecting OOD samples compared to MC Dropout and Ensemble. We see that the uncertainty level for OOD samples is much higher than for ID in the case of Masksembles. Apart from that, we observe that value uncertainty and policy uncertainty are much better synchronized compared to Dropout, Dropconnect and Ensemble.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 40: Value and policy uncertainty plots over time for different methods in HalfCheetah-v3. The first 5000 steps correspond to ID samples, the following 5000 samples \u2013 to OOD samples, separated by the vertical red line.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\n(a) Masksembles\\n(b) MC Dropout\\n(c) MC Dropconnect\\n(d) Ensemble\\n\\nFigure 23: Policy Uncertainty: ROC Curves for the HalfCheetah-v3 environment\\n\\n(a) Masksembles\\n(b) MC Dropout\\n(c) MC Dropconnect\\n(d) Ensemble\\n\\nFigure 24: Policy Uncertainty: ROC Curves for the Ant-v3 environment\\n\\n(a) Masksembles\\n(b) MC Dropout\\n(c) MC Dropconnect\\n(d) Ensemble\\n\\nFigure 25: Policy Uncertainty: ROC Curves for the Walker2d-v3 environment\\n\\n(a) Masksembles\\n(b) MC Dropout\\n(c) MC Dropconnect\\n(d) Ensemble\\n\\nFigure 26: Policy Uncertainty: ROC Curves for the Swimmer-v3 environment\"}"}
{"id": "XKq49kJ5mZX", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nFigure 27: Policy Uncertainty: ROC Curves for the MsPacman-v4 environment\\n\\nFigure 28: Policy Uncertainty: ROC Curves for the Seaquest-v4 environment\\n\\nFigure 29: Policy Uncertainty: ROC Curves for the Berzerk-v4 environment\\n\\nFigure 30: Policy Uncertainty: ROC Curves for the SpaceInvaders-v4 environment\"}"}
{"id": "XKq49kJ5mZX", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To compute the ROC curves, we sampled 2000 steps from the trained policy in the default environment as in-distribution-states. We then sample ca. 2000 OOD states either from the expert dataset or generated in a physically modified environment.\\n\\n**MuJoCo**\\n\\nTo create OOD samples for Mujoco environments, we use interventions on physics (See Table 10). Examples of ID and OOD samples are depicted in Appendix H. We independently sampled 5 configurations, trained and executed the policies in these modified environments for a maximum of 500 steps, after which an episode was restarted with a modified configuration. The first 10 states of each episode were removed from the samples. Because the modified physics partially came into effect only after the agent started performing actions, the perturbed early states might be very similar to the original early states.\\n\\n**Atari**\\n\\nFor Atari, we sample OOD states from human gameplay, recorded for the Atari HEAD (Zhang et al. (2019)) dataset. Multiple factors contribute to the states being OOD: (1) The human gameplay does generally not fully overlap with agent gameplay, which means the agent will often, e.g., be in screen positions that are not part of the trajectories produced by trained agents, (2) we sample states only from advanced levels that have significant differences to the training distribution, e.g., additional elements or changed layouts. We verify, via qualitative inspection, that the trained agents are not able to achieve high performance in presented OOD states: as an example, the level in MsPacman-v4 can be selected at initialization, and therefore trained agents can be tested in this case.\\n\\nTable 10: We sample random configurations from the presented parameter ranges. A configuration, therefore, encompasses a random number of different modifications to the environment. The wind parameter influences resistance/friction inside the medium.\\n\\n![Figure 31: ID and OOD states for MuJoCo: HalfCheetah-v3. Elements of the body are randomly resized, which leads to vastly different effects. The two OOD samples are recorded at the same timestep within an episode, highlighting the different effects the modifications can have on the execution of the policy.](image)\"}"}
{"id": "XKq49kJ5mZX", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 32: ID and OOD states for MuJoCo: Ant-v3. Elements of the body are randomly resized, which leads to vastly different effects.\\n\\nFigure 33: ID and OOD states for MuJoCo: Walker2d-v3. Elements of the body are randomly resized, which leads to vastly different effects.\\n\\nFigure 34: ID and OOD states for MuJoCo: Swimmer-v3. Elements of the body are randomly resized, which leads to different observation inputs.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this appendix, we want to provide a more in-depth analysis of the hyperparameter sweep results. In particular, we investigate the impact of key design dimensions on reward and OOD detection performance. Secondly, we give results for two additional algorithms: TRPO and A2C.\\n\\nD.1 ALGORITHMS OF INTER-MODEL CORRELATION/INDEPENDENCE\\n\\nWe want to give some additional key insights into the most important hyperparameter settings. In the context of our experiments, one of the key aspects we investigated is the effect of the number of sub-models for Masksembles and Ensembles, as well as the dropout probability for Dropout and Dropconnect. Fig. 9 shows the reward for submodel-based methods. Depending on the environment:\\n\\n(a) Avg. cumulative episode reward across all runs and environments\\n(b) Masksembles: Avg. cumulative episode reward split by environment\\n(c) Ensembles: Avg. cumulative episode reward split by environment\\n\\nFigure 6: Reward Performance for submodel-based methods with varying number of models. We see an stagnating or slightly negative trend for adding additional sub-models. We might see an advantage for methods with a lower number of models. As hypothesized above, one reason might be that a lower number of models might be more on-policy, i.e., a larger share of steps is contributed by each individual submodel. For Masksembles, a secondary effect is that the submodel size shrinks with additional masks, as the total number of units in a layer stays constant, which could affect performance as well. For ROC-AUC, we see a clear improvement when adding additional models, i.e., increasing the number of models improves the quality of OOD detection.\\n\\n(a) Avg. ROC-AUC scores across all runs and environments\\n(b) Masksembles: Avg. ROC-AUC split by environment\\n(c) Ensembles: Avg. ROC-AUC split by environment\\n\\nFigure 7: ROC-AUC for submodel-based methods with varying number of models. Here we observe a clear positive effect of adding additional models on the quality of uncertainty estimation. A partially equivalent parameter for Dropout and Dropconnect is the dropout probability $p$, i.e., the probability of deactivating a neuron/weight during a single inference run. A higher $p$ corresponds to less correlated/individual models. Other than for the submodel-based methods, we do not see a noticeable improvement in OOD detection performance when increasing the dropout probability.\\n\\nD.2 OTHER ALGORITHMS\\n\\nWe chose PPO as a representative on-policy actor-critic RL algorithm due to its widespread use across research projects. However, we think that the presented results are representative for other on-policy algorithms.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\n(a) Drop.-Based: Avg. cumulative episode reward across all runs and environments\\n(b) Dropout: Avg. cumulative episode reward split by environment\\n(c) Dropconnect: Avg. cumulative episode reward split by environment\\n\\nFigure 8: Reward Performance for submodel-based methods with varying Dropout Prob. \\n\\n\\\\( p = 0.2 \\\\)\\ncan perform well, but a lower \\\\( p \\\\) is generally preferable. Still, the reward is noticeably below baseline, which corresponds to \\\\( p = 0.0 \\\\).\\n\\n(a) Avg. ROC-AUC scores across all runs and environments\\n(b) Dropout: Avg. ROC-AUC split by environment\\n(c) Dropconnect: Avg. ROC-AUC split by environment\\n\\nFigure 9: ROC-AUC for dropout-based methods with varying Dropout Prob.\\n\\nRL methods. In the following, we present results of the presented hyperparameter-optimization procedure for TRPO (Schulman et al. (2015)) and A2C (Mnih et al. (2016); Sutton & Barto (2018)).\\n\\nAs we can see if Fig. 10, Masksembles still outperforms the other approaches but has a significant variance. Furthermore, we don't see a clear advantage in terms of OOD detection performance, although some runs still perform very well. Additionally, all sample-based methods seem to lack behind the baseline. Although the results for TRPO and A2C are weaker compared to the PPO, they still confirm the general trend. However, we assume that both algorithms are even more impacted by the \u201coff-policy-ness\u201d of the sub-model/dropout-based approaches. Especially A2C produces very weak results with high learning rates and gradient clipping ranges, which can cause a complete failure of the training. However, while Masksemble produces some of the weakest results, it also achieves the overall best training results, showing again the potential of Masksembles to be optimized to a strong configuration. As A2C is impacted by this in particular, we hypothesize that the (approximated) trust region optimization of TRPO and PPO can mitigate some of the worst effects by stabilizing the individual policies of the sub-models. Investigating the particularities of other algorithms and their interplay with the presented sample-based methods is a potential future line of work.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\n(a) TRPO\\n(b) A2C\\n\\nFigure 10: Results of the Hyperparameter Optimization Process for the Half-Cheetah-v3 environment\"}"}
{"id": "XKq49kJ5mZX", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Sec. 3.2, we use the standard deviation of predicted means of actions of the policy distributions in the continuous case; as well as the standard deviation of action probabilities of the policy distributions in the discrete case, as policy uncertainty measure. Here we investigate Jensen-Shannon divergence between output distributions as an alternative measure for policy uncertainty, defined as follows for categorical (Eq. 7) and continuous action distributions (Eq. 8):\\n\\n\\\\[\\nU_{\\\\pi_{\\\\text{cat}}, JS}(s) = \\\\max_{i,j \\\\in 1, \\\\ldots, k} JS(\\\\pi_i(a|s)||\\\\pi_j(a|s)) = \\\\max_{i,j \\\\in 1, \\\\ldots, k} \\\\frac{1}{2}KL(\\\\pi_i(a|s)||\\\\pi_j(a|s)) + KL(\\\\pi_j(a|s)||\\\\pi_i(a|s)) \\n\\\\]\\n\\nand\\n\\n\\\\[\\nU_{\\\\pi_{\\\\text{con}}, JS}(s) = \\\\max_{i,j \\\\in 1, \\\\ldots, k} JS(\\\\pi_i(a|s)||\\\\pi_j(a|s)) = \\\\max_{i,j \\\\in 1, \\\\ldots, k} \\\\frac{1}{2} KL(\\\\tilde{\\\\pi}_i(a|s)||\\\\tilde{\\\\pi}_j(a|s)) + KL(\\\\tilde{\\\\pi}_j(a|s)||\\\\tilde{\\\\pi}_i(a|s)) \\n\\\\]\\n\\nWe compute the pairwise JS-divergence between the \\\\(k\\\\) categorical probability distributions with \\\\(N\\\\) actions for the discrete case. For continuous actions, we only compare the distance of mean vectors because it is proportional to the JS divergence of the action distributions.\\n\\nTo show that we utilize the fact that the covariance matrices are predefined, scalar and equal.\\n\\nLet \\\\(P = N_d(\\\\mu_1, \\\\Sigma_1)\\\\) and \\\\(Q = N_d(\\\\mu_2, \\\\Sigma_2)\\\\), and calculate Kullback-Leibler divergence between two multivariate normal distributions in a straightforward way:\\n\\n\\\\[\\nKL(P||Q) = \\\\mathbb{E}_{P(x)} \\\\log P(x) - \\\\mathbb{E}_{Q(x)} \\\\log Q(x) = \\\\frac{1}{2} \\\\log |\\\\Sigma_2| - |\\\\Sigma_1| - d + tr(\\\\Sigma_1^{-1} \\\\Sigma_2) + (\\\\mu_2 - \\\\mu_1)^T \\\\Sigma_1^{-1} (\\\\mu_2 - \\\\mu_1) \\n\\\\]\\n\\nin the last transition, we used the fact:\\n\\n\\\\[\\n\\\\mathbb{E}_{P(x)} h((x - \\\\mu_2)^T \\\\Sigma_1^{-1} (x - \\\\mu_2)) = tr(\\\\Sigma_1^{-1} \\\\Sigma_2) + (\\\\mu_2 - \\\\mu_1)^T \\\\Sigma_1^{-1} (\\\\mu_2 - \\\\mu_1) \\n\\\\]\\n\\nWe can use result from Eq. 9 to calculate Jensen-Shannon divergence:\\n\\n\\\\[\\nJS(P||Q) = \\\\frac{1}{2} KL(P||Q) + KL(Q||P) = \\\\frac{1}{2} \\\\left\\\\{ \\\\frac{1}{2} \\\\log |\\\\Sigma_1| - |\\\\Sigma_2| - d + tr(\\\\Sigma_1^{-1} \\\\Sigma_2) + (\\\\mu_2 - \\\\mu_1)^T \\\\Sigma_1^{-1} (\\\\mu_2 - \\\\mu_1) \\\\right\\\\} + \\\\left\\\\{ \\\\frac{1}{2} \\\\log |\\\\Sigma_2| - |\\\\Sigma_1| - d + tr(\\\\Sigma_2^{-1} \\\\Sigma_1) + (\\\\mu_1 - \\\\mu_2)^T \\\\Sigma_2^{-1} (\\\\mu_1 - \\\\mu_2) \\\\right\\\\} \\n\\\\]\"}"}
{"id": "XKq49kJ5mZX", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HOW TO ENABLE UNCERTAINTY ESTIMATION IN PROXIMAL POLICY OPTIMIZATION\\n\\nAnonymous authors\\n\\nPaper under double-blind review\\n\\nABSTRACT\\n\\nWhile deep reinforcement learning (RL) agents have showcased strong results across many domains, a major concern is their inherent opaqueness and the safety of such systems in real-world use cases. To resolve these issues, we need agents that can quantify their uncertainty and detect out-of-distribution (OOD) states. Existing uncertainty estimation techniques, like Monte-Carlo Dropout or Deep Ensembles, have not been extended to on-policy deep RL. We posit that this is due to two reasons: concepts like uncertainty and OOD states are not well defined compared to supervised learning, especially for on-policy RL methods. Secondly, available implementations and comparative studies for uncertainty estimation methods in RL have been limited. To fill the first gap, we propose definitions of uncertainty and OOD for Actor-Critic RL algorithms, namely, proximal policy optimization (PPO), and present potentially applicable measures. In particular, we discuss the concepts of value and policy uncertainty. The second problem is addressed by implementing different uncertainty estimation methods and comparing them across a number of environments. The OOD detection performance is evaluated via a custom evaluation benchmark of in-distribution (ID) and OOD states for various RL environments. We identify a trade-off between reward and OOD detection performance. To overcome this, we formulate a bi-objective meta optimization problem in which we simultaneously optimize hyperparameters for reward and OOD detection performance. Our experiments show that the recently proposed method of Masksembles (Durasov et al. (2021)) strikes a favourable balance among the methods, enabling high-quality uncertainty estimation and OOD detection that matches the performance of original RL agents.\\n\\nINTRODUCTION\\n\\nAgents trained via deep RL have achieved several high-profile successes over the last years, e.g., in domains like game playing (Badia et al. (2020)), robotics (Gu et al. (2017)), or navigation (Kahn et al. (2017)). However, generalization and robustness to different conditions and tasks remain a considerable challenge (Kirk et al. (2021); L\u00fctjens et al. (2020)). Without specialized training procedures, e.g., large-scale multitask learning, RL agents only learn a very specific task in given environment conditions (OEL Team (2021). Moreover, even slight variations in the conditions compared to the training environment can lead to severe failure of the agent (L\u00fctjens et al. (2019); Dulac-Arnold et al. (2019)). This is especially relevant for potential safety critical applications. Methods to uncover an agent's uncertainty can help to combat some of the most severe consequences of incorrect decision-making. Agents that could indicate high uncertainty and, therefore, e.g., query for human interventions or use a robust baseline policy (e.g., a hard-coded one) could vastly improve the reliability of systems deploying RL-trained decision-making.\\n\\nIn response, uncertainty estimation has been recognized as an important open issue in deep learning (Abdar et al. (2021); Malinin & Gales (2018)) and has been investigated in domains such as computer vision (Lakshminarayanan et al. (2017); Durasov et al. (2021)) and natural language processing (Xiao & Wang (2018); He et al. (2020)). The topic of uncertainty estimation for deep RL has also gained traction (Clements et al. (2019)). While some work has explored OOD detection for Q-learning-based algorithms (Chen et al. (2017; 2021); Lee et al. (2021)), there is almost a complete lack of studies on uncertainty estimation for actor-critic algorithms, despite the fact that actor-critic algorithms are widely used in practice, e.g., in continuous control applications. Additionally, work specifically...\"}"}
{"id": "XKq49kJ5mZX", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"exploring OOD detection has been mostly focused on very simple environments, and neural network architectures (Mohammed & Valdenegro-Toro (2021); Sedlmeier et al. (2019)). This, therefore, limits the transferability of results to more commonly used research environments. In this paper, we bridge this research gap through a systematic study of in-distribution (ID) and out-of-distribution (OOD) detection for on-policy actor-critic RL across a set of more complex environments.\\n\\nWe implement and evaluate a set of uncertainty estimation methods for Proximal Policy Estimation (PPO) (Schulman et al. (2017)), a versatile on-policy actor-critic algorithm that is popular in research due to its simplicity and high performance. We compare a series of established uncertainty estimation methods in this setting. Specifically, we introduce the recently proposed method of Masksembles (Durasov et al. (2021)) to the domain of RL. For multi-sample uncertainty estimation methods like Ensembles or Dropout, we identify a trade-off in on-policy RL: During training, a \u201ccloseness\u201d to the on-policy setting is required, i.e., models should train on their own recent trajectory data. However, for OOD detection, a sufficiently diverse set of sub-models is required, which leads to the training data becoming off-policy. Existing methods like Monte Carlo Dropout (MC Dropout), Monte Carlo Dropconnect (MC Dropconnect), or Ensembles, therefore, struggle either to achieve good reward or OOD detection performance. The recently proposed method of Masksembles has the ability to smoothly interpolate between different degrees of inter-model correlation via its hyperparameters. We propose a bi-objective meta optimization to find the ideal configuration. We show that Masksembles can produce competitive results for OOD detection while maintaining good performance.\\n\\nThe contributions of this paper are three-fold: (1) We examine the concept of uncertainty for on-policy actor-critic RL, discussing ID and OOD settings; to tackle this, we define value- and policy-uncertainty, as well as a multiplicative measure (Sec. 3). (2) We show different methods to enable uncertainty estimation in PPO; as part of it, we establish Masksembles as a robust method in the domain of RL, particularly for on-policy actor-critic RL (Sec. 4). (3) We present an OOD detection benchmark to evaluate the quality of uncertainty estimation for a series of MuJoCo and Atari environments (Sec. 5), we evaluate the presented methods and highlight key results of the comparison (Sec. 6).\"}"}
{"id": "XKq49kJ5mZX", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we provide the background on actor-critic RL. We first discuss the concept of uncertainty as well as ID and OOD data for on-policy algorithms. We then introduce value uncertainty and policy uncertainty to address the architecture of actor-critic algorithms.\\n\\nActor-Critic Reinforcement Learning\\n\\nA Markov decision process is defined as $\\\\mathcal{M} = (S, A, P, R, P_0, \\\\gamma)$, where $S$ is the state space, $A$ is the action space, $P$ is the family of transition distributions on $S$ indexed by $S \\\\times A$ with $p(s' | s, a)$ describing the probability of transitioning to state $s'$ when taking action $a$ in state $s$, $R: S \\\\times A \\\\to \\\\mathbb{R}$ is the reward function, $P_0$ is the initial state distribution, and $\\\\gamma$ is the discount factor.\\n\\nA policy $\\\\pi_{\\\\theta}$ is a conditional probability distribution of actions $a \\\\in A$ given states $s \\\\in S$ parameterized by $\\\\theta$. RL is the problem of finding the optimal policy of the agent with respect to the specified reward function. In this work, we investigate uncertainty estimation for an actor-critic algorithm. Formally, the actor/policy $\\\\pi_{\\\\theta}(a | s)$ outputs a probability to take an action $a \\\\in A$ in a state $s$.\\n\\nIn actor-critic algorithms, we can define the objective of the policy and the definition of the critic/value function:\\n\\n$$\\nL_{\\\\text{act.}}(\\\\theta) = \\\\hat{\\\\mathbb{E}}[\\\\log \\\\pi_{\\\\theta}(a | s)] \\\\hat{A}_t$$\\n$$\\n\\\\hat{V}_{\\\\pi}\\\\psi(s_t) = \\\\hat{\\\\mathbb{E}}[\\\\sum_{l=0}^{\\\\infty} \\\\gamma^l R(s_{t+l})].\\n$$\\n\\nIn line with common PPO implementations, throughout the paper, we assume the advantage function $\\\\hat{A}_t$ to be computed via generalized advantage estimation (GAE) (Schulman et al. (2016)), which internally uses the value function estimator $\\\\hat{V}_{\\\\pi}\\\\psi(s)$ associated with policy $\\\\pi$ and parameterized by $\\\\psi$.\\n\\nFurther in the text, we will use the notation $\\\\hat{V}$ for simplicity. Both the policy $\\\\pi$ and value estimator $\\\\hat{V}$ are implemented as a neural network. In the on-policy case, the trajectories to update both the policy and value function are rollouts from the most recent policy $\\\\pi_{\\\\theta}$. For off-policy algorithms, accumulated data from current and past policy rollouts are also utilized for optimization. While this re-sampling increases sample efficiency, it is traded with training instability. In on-policy RL, the target policy is directly optimized to maximize cumulative reward.\\n\\n3.1 Defining In-Distribution and Out-of-Distribution for On-Policy RL\\n\\nCommon implementations of actor-critic variants lack a way to output uncertainty with respect to states. We are interested in quantifying uncertainty to find OOD states for an RL agent. In supervised learning, there is a comparatively clear distinction between ID and OOD cases: data that is similar to the training distribution can be generally considered as in-distribution. Importantly, the training distribution is known and fixed at training time. OOD data generally describes data that is not part of the learned manifold describing the training distribution, which means, e.g., a classifier is unable to interpolate to an OOD sample.\\n\\nThe definition of OOD for Markovian data is more complicated than the ones for the independent identically distributed (i.i.d.) case because the training data is constantly changing over the training process. Fig. 1 shows a schematic explanation of ID for different types of RL algorithms. Depending on the type of RL, offline-, off-policy, or on-policy setting, we assume a different set of states to be in-distribution or out-of-distribution.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Aaqib Parvez Mohammed and Matias Valdenegro-Toro. Benchmark for out-of-distribution detection in deep reinforcement learning. arXiv preprint arXiv:2112.02694, 2021.\\n\\nOEL Team. Open-ended learning leads to generally capable agents. CoRR, abs/2107.12808, 2021.\\n\\nIan Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. Advances in neural information processing systems, 29, 2016.\\n\\nIan Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement learning. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18, pp. 8626\u20138638, Red Hook, NY , USA, 2018. Curran Associates Inc.\\n\\nAntonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1\u20138, 2021. URL http://jmlr.org/papers/v22/20-1364.html.\\n\\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pp. 1889\u20131897. PMLR, 2015.\\n\\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1506.02438.\\n\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\n\\nAndreas Sedlmeier, Thomas Gabor, Thomy Phan, Lenz Belzner, and Claudia Linnhoff-Popien. Uncertainty-based out-of-distribution detection in deep reinforcement learning. arXiv preprint arXiv:1901.02219, 2019.\\n\\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\\n\\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026\u20135033. IEEE, 2012.\\n\\nHao Wang and Dit-Yan Yeung. A survey on bayesian deep learning. ACM Computing Surveys (CSUR), 53(5):1\u201337, 2020.\\n\\nYue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning, 2021. URL https://arxiv.org/abs/2105.08140.\\n\\nYijun Xiao and William Yang Wang. Quantifying uncertainties in natural language processing tasks. CoRR, abs/1811.07253, 2018. URL http://arxiv.org/abs/1811.07253.\\n\\nRuohan Zhang, Zhuode Liu, Lin Guan, Luxin Zhang, Mary M. Hayhoe, and Dana H. Ballard. Atari-head: Atari human eye-tracking and demonstration dataset. CoRR, abs/1903.06754, 2019. URL http://arxiv.org/abs/1903.06754.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Architecture Details\\n\\nWe choose three-layer MLPs as a baseline architecture for continuous control, with fully independent networks for the policy and value function. We include Masksembles-layers as a drop-in after the first and second hidden layer of the MLP (see Fig 2 for an overview). For Atari, we apply common pre-processing techniques such as no-op actions at the beginning of each episode to introduce stochasticity, frame skipping, and frame stacking of the last 4 frames, as is common practice for Atari (Mnih et al. (2015)). For image observations, we use the shared three-layer CNN followed by a fully-connected layer and one-layer fully-connected heads for the policy and value, as described in (Mnih et al. (2015)). For the CNN, we use a Masksembles layer after the first convolutional layer and for the final fully-connected layer before the output action distribution. Dropout/Dropconnect layers are added in equivalent positions inside the networks. We use 4 sub-models for both Masksembles and Ensembles across experiments. Equivalently, for we use 4 individual samples for uncertainty estimation with MC Dropout/MC Dropconnect. As reported in Appendix-D, we have seen a decrease in reward with increasing dropout probability, with only marginal gains in ROC-AUC Score. We have therefore decided to keep the dropout probability at 0.2 for all environments.\\n\\nTraining Details\\n\\nDuring training, we use parallel environments and roll out each sub-model policy at a distinct set of environments, e.g., using sub-model 1 policy for action selection in environments 1 and 2, sub-model 2 as the policy for environments 3 and 4, etc. In the case of Masksembles, the sub-models share a significant part of the network, e.g., the intermediate convolutional layers. Furthermore, the policy and value losses are computed on a shared replay buffer, with transitions sampled equally from rollouts of all sub-models. This common loss function is then applied to optimize all four sub-policies. This is equivalent to the cases of the baseline and Dropout policies and, therefore, requires minor modifications to the existing architectures. For Ensemble networks, k completely independent sub-networks are used. We find that training the ensemble models completely independently can lead to subpar results. Without any synchronization, both inference and uncertainty estimation become very difficult (for details, see Appendix C). We, therefore, decided to also use a shared replay buffer for ensembles to ensure interoperability of sub-models.\\n\\nWe do not utilize uncertainty estimates during training, although related work has shown that uncertainty can be used to improve training, e.g., by encouraging exploration (Wu et al. (2021); Chen et al. (2017); Osband et al. (2016); Imagawa et al. (2019)). In this paper, we specifically focus on uncertainty estimation during inference.\\n\\nA.2 Hyperparameters\\n\\nWe followed the hyperparameter settings of the StableBaselines 3-Zoo (Raffin et al. (2021)) library. These hyperparameter settings were tuned for the Baseline architectures, i.e., they favor the baselines over the adapted architectures. For the MuJoCo environments HalfCheetah-v3 and Walker2d-v3, we used larger layer sizes. We utilized 4 masks for Masksembles, 4 individual models for ensembles, or 4 samples in MC Dropout for each environment. As mentioned above (in Sec. 4), we followed these settings to validate the simple applicability of the different methods. In the case of Atari, layer sharing means that, e.g., for Masksembles, we apply the masks to the shared feature extractor (CNN) and, therefore, still get 4 predictions for the policy and value outputs. We utilize 4 separate feature extractors with individual policy and value heads for the ensemble.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Training hyperparameters for HalfCheetah-v3. We chose the larger network size for Masksembles but not for the other methods as we did not see increased performance for the baseline, Dropout, and Ensembles policies.\\n\\nTable 4: Training hyperparameters for Ant-v3.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\n### Hyperparameters: Walker2d-v3\\n\\n| Parameter        | Value                  |\\n|------------------|------------------------|\\n| Algorithm        | PPO with GAE           |\\n| Network Arch.    | 3-layer MLP, hidden size 128 |\\n| Layer Sharing    | No layer sharing, Separate policy and value Networks |\\n| Training Timesteps | 1e6                  |\\n| Num. Parallel Environments | 8                    |\\n| Batch Size       | 32                     |\\n| Learning Rate    | 5.05e-5                |\\n| Num. Epochs      | 10                     |\\n| Sampled Steps per Env. | 512                |\\n| GAE lambda       | 0.95                   |\\n| Gamma            | 0.98                   |\\n| Ent. Coeff       | 0.00059                |\\n| VF Coeff.        | 0.872                  |\\n| Max. Grad Norm   | 1                      |\\n| Clip Range       | 0.1                    |\\n| Obs. Normalization  | true               |\\n\\nTable 5: Training hyperparameters for Walker2d-v3. We decided on hidden size 128 as it slightly boosted performance Masksembles and MC Dropout while not substantially impacting baseline performance.\\n\\n### Hyperparameters: Swimmer-v3\\n\\n| Parameter        | Value                  |\\n|------------------|------------------------|\\n| Algorithm        | PPO with GAE           |\\n| Network Arch.    | 2-layer MLP, hidden size 256 |\\n| Layer Sharing    | No layer sharing, Separate policy and value Networks |\\n| Training Timesteps | 1e6                  |\\n| Num. Parallel Environments | 8                    |\\n| Batch Size       | 32                     |\\n| Learning Rate    | 5.49717e-5             |\\n| Num. Epochs      | 10                     |\\n| Sampled Steps per Env. | 512                |\\n| GAE lambda       | 0.95                   |\\n| Gamma            | 0.9999                 |\\n| Ent. Coeff       | 0.0554757              |\\n| VF Coeff.        | 0.38782                |\\n| Max. Grad Norm   | 0.6                    |\\n| Clip Range       | 0.3                    |\\n| Obs. Normalization  | false               |\\n\\nTable 6: Training hyperparameters for Swimmer-v3.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Parameter                      | Value                                      |\\n|-------------------------------|--------------------------------------------|\\n| **Algorithm**                 | PPO with GAE                               |\\n| **Network Arch.**             | 3-layer CNN, final hidden size 512         |\\n| **Layer Sharing**             | Shared CNN, separate policy and value head layers |\\n| **Training Timesteps**        | $1 \\\\times 10^6$                           |\\n| **Num. Parallel Environments**| 8                                         |\\n| **Batch Size**                | 256                                        |\\n| **Learning Rate**             | 0.00025 (linear decay for SpaceInvaders-v4) |\\n| **Num. Epochs**               | 4                                         |\\n| **Sampled Steps per Env.**    | 128                                        |\\n| **GAE lambda**                | 0.95                                       |\\n| **Gamma**                     | 0.98                                       |\\n| **Ent. Coeff.**               | 0.01 (0.2 for Seaquest)                   |\\n| **VF Coeff.**                 | 0.5                                        |\\n| **Max. Grad Norm**            | 1                                         |\\n| **Clip Range**                | 0.2                                        |\\n\\n**Env. Details**\\n- Basis: NoFrameskip-v4,\\n- AtariWrapper with frameskip 4,\\n- Stacking last 4 frames,\\n- Image normalization\\n\\nTable 7: Training hyperparameters for Atari environments.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fig. 4 shows the results of the hyperparameter optimization process. The discrepancy between the ROC-AUC scores reported in Fig. 4 and Tab. 2 can be explained by the different OOD generation processes during hyperparameter optimization and benchmarking of fully trained models. The observation space for the noise-based approach appears to be simpler, i.e., it produces more obvious OOD states compared to the physics-/level-based generation. However, in all cases we observe an advantage of Masksembles over the other approaches.\\n\\nC R E S U L T S  F O R  E N S E M B L E S\\n\\nAs the results for Ensembles might be surprising, especially given previous experimental evidence, we have decided to dedicate a detailed discussion in order to explain these findings.\\n\\nC.1 S Y N C H R O N I Z A T I O N  O F  E N S E M B L E S\\n\\nWe experimented with an independent ensemble model similar to supervised settings for the method comparison. We found that a fully unsynchronized ensemble, i.e., without a shared replay buffer or action output layer, produces vastly different policy and value outputs (see the following Appendix C.2). This can prevent correct (1) averaging across action outputs during inference and (2) comparison of value predictions for uncertainty estimation. Policies can diverge significantly due to randomness during training in the on-policy case, which explains the mismatch between predictions for non-synchronized ensembles.\\n\\nTo resolve this issue, we opted to use a shared rollout buffer. Each individual submodel performs predictions on a distinct set of parallel environments, which is equivalent to the other ensemble-like methods (especially Masksembles). Individual experiences of the sub-models are then collected and used for common optimization. The loss is computed on the shared rollout buffer. This leads to a synchronization effect between models while also ensuring that each sub-model is trained with the complete set of environment interactions. An explanation of the shared buffer is depicted in Fig. 5.\\n\\nC.2 F U L L Y  I N D E P E N D E N T  E N S E M B L E S  T R A I N I N G\\n\\nIn the main part, Appendix F and Appendix C.1, we considered an ensemble implementation with a synchronized shared buffer. In this section, we consider an alternative ensemble implementation: here, all sub-models are trained independently with different seeds and then combined into a common ensemble at the inference phase. This is comparable to supervised ensembles. The main difference is that in the supervised case, all sub-models are trained on the same dataset, whereas, in on-policy RL, the distribution of states an agent is trained with depends on the policy itself. It is, therefore, individual to each sub-model. While significant overlap can occur, it is not guaranteed. In such a case, the sub-models might have learned significantly different policies and value functions. Combining these sub-models into a single ensemble can lead to unreliable predictions: both in terms of action distribution and uncertainty estimation.\\n\\nTraining Details\\n\\nFor this independent ensemble training, we utilize the baseline architecture presented in Sec. 4.2 and Fig. 2a. Each of the 4 sub-models is trained with 1/4 of the baseline environment interactions compared to the other methods. For the MuJoCo environments HalfCheetah-v3 and Swimmer-v3, each sub-model is trained for 250000 steps, while for MsPacman-v4 and SpaceInvaders-4 we trained for 4 million/3 million steps respectively. This ensures that each method has the same amount of environment interactions. To compensate for this, we increased the number of optimization epochs on the training buffer by a factor of 4, meaning that the number of gradient updates for each ensemble sub-model is equivalent to the number of updates for the full training of the other methods. We report average episode reward during shared inference (Tab. 8) and OOD detection performance (Tab. 9). In order to create the Ensemble models, we trained five sub-models and then used each combination of four models as an ensemble model, i.e. \\\\[ \\\\Pi_1 = \\\\{ \\\\pi_1, \\\\pi_2, \\\\pi_3, \\\\pi_4 \\\\} \\\\], \\\\[ \\\\Pi_2 = \\\\{ \\\\pi_1, \\\\pi_2, \\\\pi_3, \\\\pi_5 \\\\} \\\\] etc. Instead of three random seeds, we report the reward performance averaged over the 5 possible combinations of sub-models without repetitions.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\n(a) HalfCheetah-v3\\n(b) Ant-v3\\n(c) Walker2d-v3\\n(d) Swimmer-v3\\n\\nFigure 4: Results of the Hyperparameter Optimization Process. Each dot represents a single run with a corresponding hyper-parameter configuration (e.g. parameters like number of masks/dropout probability, learning rate, model size etc.). The configurations are sampled from a wide space of possible configurations.\\n\\nResults\\nAs can be expected, due to the lower environment interaction budget for sub-models, the overall performance is lower than for comparable methods, although higher than the reported (synchronized) ensemble baseline for the Atari games. For Swimmer-v3, the phenomenon of missing synchronization is especially visible: Although the sub-models achieve decent performance on average, the averaged probability distribution produces incoherent behavior, which in turn leads to a significantly lower achieved reward. In terms of OOD detection, we see a drop in performance,\"}"}
{"id": "XKq49kJ5mZX", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: The concept of experience synchronization via a shared rollout buffer. The vectorized environment is divided into groups so that the predicted actions of one Masksembles / ensemble sub-model are executed within each group. The blue arrows mean that all of the agents write their experience to the buffer irrespective of the group they belong to. The red dotted arrows indicate that losses (policy loss/value loss) are computed across all samples in the shared buffer. The shared losses are used to optimize all sub-models.\\n\\nTable 8: Average Reward Performance of the independent Ensemble. The average performance can lie significantly below the best single model performance.\\n\\nTable 9: OOD Detection ROC scores for the independent ensemble. We notice a significant drop in OOD detection performance, even compared to the ensemble implementation achieving a lower reward.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\n(a) Masksembles: Value Uncertainty\\n(b) Masksembles: Policy Uncertainty\\n(c) MC Dropout: Value Uncertainty\\n(d) MC Dropout: Policy Uncertainty\\n(e) MC Dropconnect: Value Uncertainty\\n(f) MC Dropconnect: Policy Uncertainty\\n(g) Ensembles: Value Uncertainty\\n(h) Ensembles: Policy Uncertainty\\n\\nFigure 41: Value and policy uncertainty plots over time for different methods in Swimmer-v3. The first 5000 steps correspond to ID samples, the following 5000 samples \u2013 to OOD samples, separated by the vertical red line.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 42: Value and policy uncertainty over time for different methods in MsPacmanNoFrameskip-v4. The first 5000 steps correspond to ID samples, the following 5000 samples \u2013 to OOD samples, separated by the vertical red line.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nFigure 43: Value and policy uncertainty over time for different methods in SeaquestNoFrameskip-v4. The first 5000 steps correspond to ID samples, the following 5000 samples \u2013 to OOD samples, separated by the vertical red line.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: The RL setting influences what can be considered ID and OOD. Depending on the setting, a different area of the state space can be considered as ID for value estimation. We use the background cones to depict the part of state space that is considered as ID; $t_1, t_2, t_3, t_{\\\\text{final}}$ to notate schematic trajectories which have evolved with training time. The green dot shows the initial state of the agent, and the red one is the terminal state.\\n\\nBecause an offline-RL agent is repeatedly trained on a full batch of optimal and suboptimal trajectories, it can produce consistent estimates even for these suboptimal states. In off-policy settings, where a large replay buffer is used, compare, e.g., to DQN (Mnih et al. (2015)) or SAC (Haarnoja et al. (2018)), we have a combination of current and outdated states, i.e., the agent is still trained to predict values for states which are not part of the trajectories produced by the most recent policies. In the on-policy case, we assume that only a narrow set of states along the trajectories produced by active policy can be considered as ID. This is because both the value estimator and the policy are only updated on data from the active policy. As part of the policy optimization procedure, due to catastrophic forgetting (French (1999)), states encountered early during training but not visited frequently due to the policy shifting to other trajectories cannot be considered ID anymore. Importantly, this also means that the set of ID states is specific to the trained policy. Our goal is to enable the trained policy to detect OOD states at inference time. Because of the aforementioned \u201ccloseness\u201d of both policy and value function, we will investigate the suitability of both, the uncertainty of the value function, and the uncertainty of the policy function to quantify the agent's uncertainty.\\n\\n3.2 Quantifying Uncertainty: Policy Uncertainty and Value Uncertainty\\n\\nWe now discuss possible uncertainty measures for actor-critic RL. As mentioned, our goal is to quantify the agent's uncertainty, i.e., the uncertainty of decision-making. A direct application of this is the detection of OOD states in which the agent is not able to predict an action to maximize return with high confidence. In actor-critic RL, we can apply sample-based uncertainty estimation methods to both the policy and value network, which means we both get an output distribution for the action and the value estimate. This enables us to investigate both policy and value uncertainty independently to probe their adequateness as uncertainty measures.\\n\\nDuring the remainder of the paper, we use the term policy uncertainty to refer to any uncertainty measure for the output distribution of the policy. In the following evaluation, we encounter both categorical and continuous output distributions. Formally they are defined as:\\n\\n$$\\\\pi_{\\\\text{cat}}(a_i | s; \\\\theta) = \\\\exp(\\\\tilde{\\\\pi}(a_i | s; \\\\theta)) \\\\prod_{j=0}^{N} \\\\exp(\\\\tilde{\\\\pi}(a_j | s; \\\\theta))$$\\n\\n$$\\\\pi_{\\\\text{con}}(a | s; \\\\theta) \\\\sim N \\\\prod_{j=0}^{N}(\\\\tilde{\\\\pi}(a_j | s), \\\\sigma_{\\\\text{fix}})$$\\n\\nwhere $a_i$ is the $i$-th action of a discrete set of actions, and $a_j$ is the $j$-th component of a multi-variate continuous action. In the case of discrete actions, $N$, therefore, refers to the number of actions, and in the case of continuous, $N$ refers to the dimension of the multi-variate action. For discrete actions, a categorical distribution is defined by a softmax of the logits $\\\\tilde{\\\\pi}$, i.e., the output of the policy network. For continuous actions, a multi-variate diagonal Gaussian distribution is applied, with the mean of each dimension parameterized by the logits of the policy network and a state-independent $\\\\sigma$ (reported in Appendix A).\\n\\nIn the following, we use multiple predictions of different sub-models during inference, we write $\\\\Pi = \\\\{\\\\pi_1, \\\\pi_2, ..., \\\\pi_k\\\\}$ for $k$ different sub-models/samples. From the perspective of supervised classification, a possible uncertainty measure is\\n\\n$$U_{\\\\Pi, \\\\text{max.prob.}}(s) = 1 - p_{\\\\text{max}}(s) = 1 - \\\\max_{j \\\\in 1,..,N} \\\\pi_{\\\\text{max}}(a_j | s)$$\"}"}
{"id": "XKq49kJ5mZX", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of the averaged distribution $\\\\pi \\\\mu (a|s) = \\\\frac{1}{k} \\\\sum_{h=0}^{k} \\\\pi_h (a|s)$. This corresponds to the confidence with which the classifier assigns the top label. However, in RL, there is no clear singular correct action, and thus a low probability for a certain action does not necessarily correspond to high uncertainty. It could also mean that two actions are equivalent with respect to the optimal policy. Thus, instead of capturing epistemic uncertainty, which is required for OOD detection, the measure instead captures the irreducible uncertainty of the environment dynamics, i.e. aleatoric uncertainty. For the same reason, the entropy of the averaged distribution $U_{\\\\text{cat,entropy}}(s) = H(\\\\pi \\\\mu (a|s))$ also does not serve as a reliable uncertainty measure. Lastly, both of these measures are influenced by the magnitude of an entropy bonus to the policy optimization objective (see (Schulman et al. (2017)) for details), which further limits general applicability. In Sec. 6, we report the performance of both uncertainty measures but found them to be unreliable for quantifying uncertainty w.r.t. ID or OOD states.\\n\\nInstead, we focused on uncertainty measures that are based purely on the disagreement between the respective sub-models. We use the following formulations for policy uncertainty for continuous and categorical actions, respectively:\\n\\n$$U_{\\\\text{\\\\Pi,con,std}}(s) = \\\\frac{1}{N} \\\\sum_{j=0}^{N} \\\\text{std} \\\\left( \\\\{ \\\\pi_1(a_j|s), \\\\pi_2(a_j|s), \\\\ldots, \\\\pi_k(a_j|s) \\\\} \\\\right)$$\\n\\n(3)\\n\\n$$U_{\\\\text{\\\\Pi,cat,std}}(s) = \\\\frac{1}{N} \\\\sum_{i=0}^{N} \\\\text{std} \\\\left( \\\\{ \\\\pi_1(a_i|s), \\\\pi_2(a_i|s), \\\\ldots, \\\\pi_k(a_i|s) \\\\} \\\\right).$$\\n\\n(4)\\n\\nWe chose this formulation, because both $\\\\pi(a_i|s)$ and $\\\\pi(a_j|s)$ are directly parameterized by the logits of the respective policy networks. We, therefore, hypothesize that we can interpret these average standard deviations as a measure of the agreement/disagreement between the sub-policies $\\\\pi_1, \\\\ldots, \\\\pi_k$.\\n\\nAn alternative approaches to formulate uncertainty can be found in Appendix E. Equivalently, we refer to uncertainty measures for the value function as value uncertainty and use the following notation $U_{V}$ with $V = \\\\{ \\\\hat{V}_1, \\\\hat{V}_2, \\\\ldots, \\\\hat{V}_k \\\\}$. We investigate the standard deviation of the value function as a potential proxy for state-visitation frequency:\\n\\n$$U_{\\\\text{V,std}}(s) = \\\\text{std} \\\\left( \\\\{ \\\\hat{V}_1(s), \\\\hat{V}_2(s), \\\\ldots, \\\\hat{V}_k(s) \\\\} \\\\right).$$\\n\\n(5)\\n\\nBased on these measures, we define multiplicative uncertainty, which we define as the product between policy and value uncertainty for each state. We may interpret this measure as a state-action uncertainty measure:\\n\\n$$U_{\\\\text{Mult, std}}(s) = U_{\\\\text{\\\\Pi, std}}(s) \\\\times U_{\\\\text{V,std}}(s).$$\\n\\n(6)\\n\\n4 EQUIPPING PPO WITH UNCERTAINTY ESTIMATION\\n\\nBased on the defined uncertainty metrics, we now investigate the integration of uncertainty estimation methods into RL actor-critic architectures. We discuss how to implement uncertainty estimation for an actor-critic algorithm, specifically PPO (Schulman et al. (2017)).\\n\\n4.1 INTEGRATING UNCERTAINTY ESTIMATION\\n\\nAs a pre-condition, we want the uncertainty methods to be easily integrated into existing RL setups as drop-in replacements for existing network architectures. While this limits the set of methods, it ensures high practical applicability of the presented results.\\n\\nRecently, Masksembles (Durasov et al. (2021)) were proposed as an attempt to combine the advantages of Ensembles and MC Dropout: MC Dropout randomly discards some parts of the network by applying a random binary mask at every iteration, i.e., using an infinite number of overlapping models. Ensembles use a set of fixed and separate sub-models. Masksembles create only a limited and fixed set of masks but also share part of the network. Importantly, Masksembles provides a parameter to control the overlap between masks, which in turn controls the correlation between the sub-models. This allows us to flexibly determine the best configuration based on the problem. In terms of training speed, Masksembles performs comparably to MC Dropout and outperforms the latter in terms of...\"}"}
{"id": "XKq49kJ5mZX", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nInference speed because Masksembles can generate output distributions in a single forward-pass. While the technique was proposed in the domain of image classification, to our knowledge, this is the first use for RL.\\n\\nWe compare our Masksembles-based model (Fig. 2e) against three architectures enabling uncertainty estimation and baseline. Both MC Dropout and Ensembles are established uncertainty estimation methods. Additionally, we compare against the more recent MC Dropconnect. Fig. 2 is a schematic illustration of the integration of different methods in the baseline MLP architecture used for continuous control tasks. Architecture details are discussed in Appendix A.\\n\\nBaseline:\\nThe default fully-connected networks and CNNs serve as a comparison for reward performance. It does not provide uncertainty estimation functionality (Fig. 2a).\\n\\nMasksembles (Durasov et al. (2021)): During training and inference, masks are applied across the input batch during the forward pass, which leads to different predictions used for uncertainty estimation. We base our design choices on the respective paper (Fig. 2e).\\n\\nMonte-Carlo Dropout (Gal & Ghahramani (2016)): Here, we train the networks with Dropout enabled. We apply Dropout to the same two layers as Masksembles to enable direct comparability. For MC Dropout, we keep Dropout enabled during inference and sample multiple times with individual Dropout masks to get a distribution of predictions. Compared to Masksembles, Dropout masks are unique for each prediction. To match the number of masks in Masksembles, we base the uncertainty estimation on $k = 4$ independent samples (Fig. 2b).\\n\\nMonte-Carlo Dropconnect (Mobiny et al. (2021)): Dropconnect is closely related to Dropout. Instead of disabling neurons, i.e., the output of a layer, random weights are assigned the value 0, which achieves a similar effect as Dropout (Fig. 2c).\\n\\nEnsembles:\\nLastly, as a comparative baseline, we employ an ensemble of 4 networks, in the case of MuJoCo 4 policy and critic networks each, in the case of Atari 4 independent CNNs with individual policy and value heads each. As mentioned, to ensure basic synchronization between the networks, all networks are trained based on the loss function computed from a shared replay buffer (Fig. 2d).\\n\\nFigure 2: Different MLP architectures: The baseline network (a) is a three-layer MLP. For MC Dropout (b), a random mask is applied to each prediction, which can be interpreted as an infinite number of sub-models. (c) Dropconnect is similar to Dropout. For Ensembles (d) we assume a small number of fully independent networks. Masksembles (e) can be interpreted as an interpolation that uses a fixed set of masks to create partially overlapping sub-models. For Masksembles and Ensembles, each sub-model/mask is assigned to a fixed subset of environments (dashed lines).\\n\\n4.2 Inference with Multi-Sample Methods\\nWe apply multiple inference schemes to translate the distribution of policy sub-models into a final action prediction. For categorical actions, in the games of SpaceInvaders and MsPacman, we use a voting scheme, which is commonly applied in classification: Each of the sub-model assigns probabilities to the available actions. We then choose the action with the highest probability from each submodel and then sample the mode of these $k$ predicted actions. In case two or more actions...\"}"}
{"id": "XKq49kJ5mZX", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023.\\n\\nFor each of the games, we report the best performing configuration. For continuous actions, we use the submodel predictions to parameterize a diagonal Gaussian distribution. For deterministic action selection, we choose the mean predictions; for stochastic selection, we sample from the distributions.\\n\\n**5 BENCHMARKING**\\n\\n**OUT-OF-DISTRIBUTION DETECTION**\\n\\n**Environments**\\n\\nWe evaluate the presented methods on four MuJoCo (Todorov et al. (2012)) control environments, namely, Ant-v3, HalfCheetah-v3, Swimmer-v3, and Walker2d-v3. All of these environments feature vector-valued continuous actions. Furthermore, we apply the surveyed methods to four Atari environments (Bellemare et al. (2013)): Seaquest-v4, MsPacman-v4, Berzerk-v4 and SpaceInvaders-v4. We chose these four games to represent different game-play mechanics and because recorded game-play is available as part of the Atari-Head dataset ((Zhang et al. (2019)), which we use for OOD state generation as described below.\\n\\n**Creating the Benchmark for Out-of-distribution Data**\\n\\nFor MuJoCo-based environments, we apply a number of random perturbations to the parameters of the physics simulation, e.g., increasing/decreasing gravity and/or friction or varying the dimensions of the body elements. These modifications create OOD states when the agent acts in the environment. In Appendix H, we give a detailed description of the applied interventions. To create OOD states, we randomly sample from the set of available parameters and let the respective agent act for 100 steps in the perturbed environment; 50 configurations are sampled to create a dataset of possible OOD states.\\n\\nFor Atari games, previous work has proposed image-based perturbations (like Gaussian noise, blur, etc.) to create OOD states (Mohammed & Valdenegro-Toro (2021)). While this is a simple and generalizable method to create these states, we feel that this approach does not necessarily capture the nature of OOD states in Markovian settings. Therefore, we focus on valid game states that the agent has not yet encountered. Specifically, we use sampled trajectories of advanced levels from the Atari-HEAD dataset (Zhang et al. (2019)). We ensure that the sampled states are from states of the games that the trained agents have not yet reached due to insufficient performance (e.g., with a changed level layout). We give examples of OOD states in the Atari setting in Appendix H. We report some additional results for additional image-based perturbation techniques, like adding noise or masking parts of the input image, in Appendix I.\\n\\n**Methodology for Evaluation**\\n\\nWe sample ID data directly by executing the trained policies in unperturbed environments. This is connected to the discussion of Sec. 3, that ID states are densely distributed around the final model trajectories. OOD states are created as described above. An effective uncertainty measure should be substantially higher for OOD states compared to ID states. We frame OOD detection as a binary classification problem: We assign ID and OOD respective labels. Then, the computed uncertainty scores, as described in Sec. 3, are used as a threshold to classify ID and OOD states. We then compute ROC-AUC to evaluate the OOD detection quality.\\n\\n**Hyperparameter Optimization**\\n\\nMost of the results that will be reported in Sec. 6 are achieved with hyper-parameters that were optimized for the baseline algorithm, i.e., without specifically fine-tuning the specific methods. This means that, in general, the reported results are representative of using the presented methods as full drop-in replacements without any modifications to the training process. However, we are interested in uncertainty estimation methods that are robust to the choice of hyper-parameters (like a number of models, learning rate, batch size, etc.). To enable a systematic investigation of the sensitivity to hyper-parameter choices, we run a bi-objective meta optimization procedure: We are interested in both achieving a high cumulative episode reward and a strong OOD detection performance. We sample 190 configurations per method and environment using Optuna (Akiba et al. (2019)). During the optimization, we choose a simplified OOD state generation scheme: We add uniform noise with a magnitude of $2 \\\\times$ the element-wise standard deviation to the input observations in order to create OOD states. We chose this scheme due to its efficiency in large-scale meta optimization. We see this generation schema as supplemental to the OOD generation method described above. We showcase the results of this procedure in Fig. 4. As we have mentioned previously, this generation schema is not suitable to reliably evaluate epistemic uncertainty, it gives an approximation of the methods capabilities.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Reward summary across environments and methods Masksembles (MS), Monte Carlo Dropout (MCDO), Monte Carlo Dropconnect (MCDC) and Ensembles. Rewards are measured over 10 episodes and averaged over 3 seeds. The upper results are for action sampling based on model averaging/voting. The \\\\textit{Single} scores are for action prediction with a single random sub-model. For Berzerk-v4, we truncated episodes after 10,000 steps if an agent gets stuck. We mark the best results for every environment in bold.\\n\\n| Environment   | Masksembles | Baseline | Baseline | Baseline | Baseline | Baseline | Baseline |\\n|---------------|-------------|----------|----------|----------|----------|----------|----------|\\n|               | MS          | MCDO     | MCDC     | Ens.     | MS       | MCDO     | MCDC     |\\n|               | 2673.6 \u00b1 565 | 4884.8 \u00b1 926 | 2328.2 \u00b1 480 | 151.4 \u00b1 63 | 3524.0 \u00b1 1230 | 1855.6 \u00b1 876 | 761.3 \u00b1 233 |\\n|               | 2014.6 \u00b1 529 | 2471.3 \u00b1 512 | 656.7 \u00b1 121 | 78.2 \u00b1 7 | 3445.3 \u00b1 1154 | 2169.3 \u00b1 27 | 587.3 \u00b1 154 |\\n|               | 651.8 \u00b1 560 | 1610.6 \u00b1 542 | 859.4 \u00b1 68 | 49.3 \u00b1 13 | 3110.3 \u00b1 972 | 1696.6 \u00b1 703 | 759.5 \u00b1 154 |\\n|               | 713.4 \u00b1 283 | 989.2 \u00b1 37 | 128.9 \u00b1 42 | 26.4 \u00b1 6 | 122.6 \u00b1 42 | 210.0 \u00b1 14 | 460.2 \u00b1 277 |\\n|               | 2654.9 \u00b1 490 | 5746.3 \u00b1 830 | 2007.1 \u00b1 760 | 172.6 \u00b1 52 | 3242.6 \u00b1 1107 | 1813.6 \u00b1 851 | 708.0 \u00b1 133 |\\n|               | 2008.8 \u00b1 541 | 2272.4 \u00b1 958 | 669.9 \u00b1 192 | 111.5 \u00b1 21 | 3540.6 \u00b1 1229 | 2204.3 \u00b1 38 | 632.6 \u00b1 108 |\\n|               | 1006.6 \u00b1 689 | 2384.5 \u00b1 906 | 671.0 \u00b1 275 | 58 | 3186.0 \u00b1 1034 | 1681.6 \u00b1 713 | 780.3 \u00b1 60 |\\n|               | 464.0 \u00b1 355 | 989.3 \u00b1 4 | 128.5 \u00b1 10 | 14.6 \u00b1 3 | 183.3 \u00b1 127 | 247.6 \u00b1 53 | 283.6 \u00b1 60 |\\n\\nIn this section, we focus on comparing Masksembles approach following the protocol described in Sec. 5, namely, we measure training results, the performance of the agents based on the ground-truth reward by rolling out trained policies, and the ability to detect OOD states.\\n\\nReward\\nFollowing the procedure defined in Sec. A.1 we observe the training dynamic for control-problems depicted in Fig. 3b, 3a, 3c, 3d and games depicted in Fig. 3e,3f,3c, 3h. During training, actions are only selected via single samples, i.e., one prediction per environment. During inference, multiple samples can be combined into a single prediction as described in Sec. 4.2. The results are presented in Tab. 1. We also report the rewards achieved via inference with a single model. From the results, we cannot conclude that combining multiple predictions via voting or averaging leads to a higher reward compared to a single model.\\n\\nOut-of-distribution Detection\\nThe results for the previously described OOD detection benchmark (see Sec. 5) are summarized in Tab. 2. We found that in most cases, the Masksembles approach outperformed MC Dropout, MC Dropconnect, and Ensembles in terms of OOD detection. Interestingly, in the control environment, e.g., Ant-v3, value uncertainty also acts as a measure of physical instability, particularly detachment from the floor, e.g., it explains substantial uncertainty at the beginning of an episode when Ant is spawned. Such \u201clevitation\u201d states mean that any action can be equivalently reasonable or impaired concerning expected discounted reward.\\n\\nBi-Objective Meta Optimization\\nThe results of the hyperparameter optimization are visualized in Appendix B. The results show that Masksembles has a favorable distribution of results compared to the other methods. We find that both in terms of reward and specifically in terms of ROC-AUC score for OOD detection, Masksembles has a larger set of well performing configurations. We also find that the best results for Masksembles are often close to the baseline. Some agents trained with MC Dropout and MC Dropconnect also show a good performance, but the number of good configurations\"}"}
{"id": "XKq49kJ5mZX", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this work, we have focused on PPO as the algorithm of choice. We found that the value uncertainty\\nmeasure as capturing state-action-uncertainty, similar to using Q-value uncertainty in previous\\napproaches. However, we see a multiplicative measure as being more useful for capturing\\naction-uncertainty. Therefore, we might interpret the multiplicative measure can produce notable performance by itself, the multiplicative measure outperforms value uncertainty\\nin most cases. Value uncertainty captures the state uncertainty, while policy uncertainty could also\\ndiverge. This makes averaging of predictions challenging, and because the sub-models do not share a\\ncommon representation of the state, the uncertainty estimates are not compatible.\\n\\nIn Appendix C.2 we present results for fully separate Ensembles. Because the data distribution can\\nchange significantly during training, an ensemble of independent, non-synchronized policies may\\nnot be able to learn a good policy, even though they might perform well on average. In\\nAppendix D, we show how to train a Meta-Ensemble with a fixed mask, which is known to\\nbe able to learn good policies but can reach good OOD detection performance.\\n\\nFor continuous actions, the uncertainty scores\\n\\n\\\\[ u(t) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} u_i(t) \\\\]\\n\\ncan be estimated by averaging the uncertainty of each sub-model. This is smaller, and there is a noticeable gap in OOD detection performance. Ensemble-based models\\nshow a very weak ability to learn good policies but can reach good OOD detection performance. This can offset the smaller layer size introduced by the fixed masks. However, we see\\nthat for certain tasks, we do not achieve the same performance as with independent\\nEnsembles. In Appendix E, we show how to use a shared representation of the state\\nfor multiple agents, which improves the performance of the ensemble.\\n\\nIn this paper, we investigated uncertainty estimation for RL. We first discussed the background of\\nOOD detection to query for human input in high-uncertainty situations. This can offset the smaller layer size introduced by the fixed masks. However, we see\\nthat for certain tasks, we do not achieve the same performance as with independent\\nEnsembles. In Appendix E, we show how to use a shared representation of the state\\nfor multiple agents, which improves the performance of the ensemble.\\n\\nFuture work will include the following conceptual avenues: (1) utilize uncertainty measures to\\nimprove the performance and exploration abilities of current on-policy RL algorithms, (2) use OOD\\ndetection to query for human input in high-uncertainty situations, and (3) discover a more elaborate\\nway to define uncertainty and disentangle policy and value uncertainty. (4) compare our findings\\nwith those of previous work. The meta-optimization shows that particular runs can already achieve very competitive\\nresults. Specifically, we find specific benefits for Masksembles in increasing the MLP layer sizes; see\\nAppendix F. This can offset the smaller layer size introduced by the fixed masks. However, we see\\nthat for certain tasks, we do not achieve the same performance as with independent\\nEnsembles. In Appendix E, we show how to use a shared representation of the state\\nfor multiple agents, which improves the performance of the ensemble.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U. Rajendra Acharya, Vladimir Makarenkov, and Saeid Nahavandi. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information Fusion, 76:243\u2013297, 2021. ISSN 1566-2535. doi: https://doi.org/10.1016/j.inffus.2021.05.008. URL https://www.sciencedirect.com/science/article/pii/S1566253521001081.\\n\\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '19, pp. 2623\u20132631, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450362016. doi: 10.1145/3292500.3330701. URL https://doi.org/10.1145/3292500.3330701.\\n\\nPeter Auer. Using confidence bounds for exploitation-exploration trade-offs. J. Mach. Learn. Res., 3(null):397\u2013422, mar 2003. ISSN 1532-4435.\\n\\nAdri\u00e0 Puigdom\u00e8nech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark. In International Conference on Machine Learning, pp. 507\u2013517. PMLR, 2020.\\n\\nM. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, jun 2013.\\n\\nBertrand Charpentier, Ransalu Senanayake, Mykel Kochenderfer, and Stephan G\u00fcnnemann. Disentangling epistemic and aleatoric uncertainty in reinforcement learning, 2022. URL https://arxiv.org/abs/2206.01558.\\n\\nRichard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. Ucb exploration via q-ensembles. arXiv preprint arXiv:1706.01502, 2017.\\n\\nXinyue Chen, Che Wang, Zijian Zhou, and Keith Ross. Randomized ensembled double q-learning: Learning fast without a model. arXiv preprint arXiv:2101.05982, 2021.\\n\\nWilliam R. Clements, Bastien Van Delft, Beno\u00eet-Marie Robaglia, Reda Bahi Slaoui, and S\u00e9bastien Toth. Estimating risk and uncertainty in deep reinforcement learning, 2019. URL https://arxiv.org/abs/1905.09638.\\n\\nGabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement learning. arXiv preprint arXiv:1904.12901, 2019.\\n\\nNikita Durasov, Timur Bagautdinov, Pierre Baque, and Pascal Fua. Masksembles for uncertainty estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13539\u201313548, 2021.\\n\\nStanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape perspective. arXiv preprint arXiv:1912.02757, 2019.\\n\\nRobert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128\u2013135, 1999.\\n\\nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 1050\u20131059, New York, New York, USA, 20\u201322 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/gal16.html.\\n\\nShixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In 2017 IEEE international conference on robotics and automation (ICRA), pp. 3389\u20133396. IEEE, 2017.\"}"}
{"id": "XKq49kJ5mZX", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer G. Dy and Andreas Krause (eds.), ICML, volume 80 of Proceedings of Machine Learning Research, pp. 1856\u20131865. PMLR, 2018. URL http://dblp.uni-trier.de/db/conf/icml/icml2018.html#HaarnojaZAL18.\\n\\nJianfeng He, Xuchao Zhang, Shuo Lei, Zhiqian Chen, Fanglan Chen, Abdulaziz Alhamadani, Bei Xiao, and Chang-Tien Lu. Towards more accurate uncertainty estimation in text classification. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.\\n\\nTakuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, and Yoshimasa Tsuruoka. Dropout q-functions for doubly efficient reinforcement learning. arXiv preprint arXiv:2110.02034, 2021.\\n\\nTakahisa Imagawa, Takuya Hiraoka, and Yoshimasa Tsuruoka. Optimistic proximal policy optimization. arXiv preprint arXiv:1906.11075, 2019.\\n\\nGregory Kahn, Adam Villaflor, Vitchyr Pong, Pieter Abbeel, and Sergey Levine. Uncertainty-aware reinforcement learning for collision avoidance. arXiv preprint arXiv:1702.01182, 2017.\\n\\nAlex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? Advances in neural information processing systems, 30, 2017.\\n\\nRobert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rockt\u00e4schel. A survey of generalisation in deep reinforcement learning, 2021. URL https://arxiv.org/abs/2111.09794.\\n\\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017.\\n\\nKimin Lee, Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning. In International Conference on Machine Learning, pp. 6131\u20136141. PMLR, 2021.\\n\\nXiuyuan Lu and Benjamin Van Roy. Ensemble sampling. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, pp. 3260\u20133268, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.\\n\\nBj\u00f6rn L\u00fctjens, Michael Everett, and Jonathan P How. Safe reinforcement learning with model uncertainty estimates. In 2019 International Conference on Robotics and Automation (ICRA), pp. 8662\u20138668. IEEE, 2019.\\n\\nBj\u00f6rn L\u00fctjens, Michael Everett, and Jonathan P How. Certified adversarial robustness for deep reinforcement learning. In Conference on Robot Learning, pp. 1328\u20131337. PMLR, 2020.\\n\\nAndrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18, pp. 7047\u20137058, Red Hook, NY, USA, 2018. Curran Associates Inc.\\n\\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, February 2015. doi: 10.1038/nature14236.\\n\\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928\u20131937. PMLR, 2016.\\n\\nAryan Mobiny, Pengyu Yuan, Supratik K Moulik, Naveen Garg, Carol C Wu, and Hien Van Nguyen. Dropconnect is effective in modeling uncertainty of bayesian deep networks. Scientific reports, 11(1):1\u201314, 2021.\"}"}
