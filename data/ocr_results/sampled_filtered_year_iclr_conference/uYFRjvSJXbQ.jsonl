{"id": "uYFRjvSJXbQ", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Discovering high-entropy alloys (HEAs) with high yield strength is an important yet challenging task in material science. However, the yield strength can only be accurately measured by very expensive and time-consuming real-world experiments, hence cannot be acquired at scale. Learning-based methods could facilitate the discovery process, but the lack of a comprehensive dataset on HEA yield strength has created barriers. We present X-Yield, a large-scale material science benchmark with 240 experimentally measured (\u201chigh-quality\u201d) and over 100K simulated (imperfect or \u201clow-quality\u201d) HEA yield strength annotations. Due to the scarcity of experimental annotations and the quality gap in imperfectly simulated data, existing transfer learning methods cannot generalize well on our dataset. We address this cross-quality few-shot transfer problem by leveraging model sparsification \u201ctwice\u201d \u2014 as a noise-robust feature learning regularizer at the pre-training stage, and as a data-efficient learning regularizer at the few-shot transfer stage. While the workflow already performs decently with ad-hoc sparsity patterns tuned independently for either stage, we take a step further by proposing a bi-level optimization framework termed Bi-RPT, that jointly learns optimal masks and automatically allocates sparsity levels for both stages. The optimization problem is solved efficiently using gradient unrolling, which is seamlessly integrated with the training process. The effectiveness of Bi-RPT is validated through extensive experiments on our new challenging X-Yield dataset, alongside other synthesized testbeds. Specifically, we achieve an 8.9 \u223c 19.8% reduction in terms of the test mean squared error and 0.98 \u223c 1.53% in terms of test accuracy, merely using 5-10% of the experimental data. Codes and sample data are in the supplement.\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"stage), yielding another sparse mask $m_t$. The final model uses the joint sparse mask $m_s \\\\odot m_t$ where $\\\\odot$ represents the point-wise product.\\n\\nHereby, $N_s$ and $N_t$ are hyperparameters that control the sparsity allocation between two stages. Intuitively, while certain sparsity may contribute to noise resilience, an overly large $N_s$ will cause the pre-trained model to be over-sparsified, limiting its capacity to learn sufficiently informative and transferable features. Fine-tuning has a similar trade-off. Therefore, $N_s$ and $N_t$ have to be manually tuned for the two-stage workflow to achieve good performance (see Appendix B.2).\\n\\nPrincipled Bi-Stage Sparsity Integration with Bi-RPT.\\n\\nHand-Tune has some apparent flaws: (1) it removes weight elements by merely using weight magnitude information, which is not explicitly task-driven; (2) the two sparse masks $m_s$ and $m_t$ are decided in a sequential manner rather than jointly optimized, e.g., learning $m_t$ will passively suffer from any artifact in learning $m_s$; (3) the sparsity ratios assigned in both stages, as controlled by $N_s$ and $N_t$, need to be manually tuned, without any obvious insight beyond exhaustive hyperparameter search.\\n\\nWe, therefore, devise a more principled framework that can jointly learn the optimal sparse masks as well as sparsity allocations for both stages, termed Bi-Level Regularized Pre-training and Transfer (Bi-RPT). The optimization problem is expressed as follows ($\\\\gamma$ is a coefficient):\\n\\n$$\\\\min_{\\\\theta, m_s, m_t} E(x_t, y_t) \\\\sim D_t [L_t((m_s \\\\odot m_t) \\\\odot \\\\theta, x_t, y_t | \\\\theta^*, m^*_s)] + \\\\gamma R(m^*_s \\\\odot m^*_t)$$\\n\\n$$\\\\text{s.t.} \\\\{\\\\theta^*, m^*_s\\\\} = \\\\text{arg min}_{\\\\theta, m_s} E(x_s, y_s) \\\\sim D_s [L_s(m_s \\\\odot \\\\theta, x_s, y_s)]$$\\n\\nwhere $L_s/L_t$ represents the objective functions for the two stages, respectively, $\\\\theta$ represents the models' parameters, and $R$ represents the sparsity regularizer. Seemingly complicated at the first glance, the bi-level optimization formulation of Bi-RPT actually admits a clear physics \\\"workflow\\\" interpretation. Let us start from the lower-level problem (2) which instantiates the sparsity regularized pre-training stage over $D_s$: its outputs include the pre-trained weight $\\\\theta^*$ and the corresponding sparse mask $m^*_s$. Then, the upper-level problem (1) depicts the sparsity regularized fine-tuning over $D_t$, which inherits both $\\\\theta^*$ and $m^*_s$ as its starting point. It continues to modify the weight as well as to evolve another sparse mask $m_t$. Eventually, a sparsity-promoting function $R$ enforces the total sparsity over the joint mask $m_s \\\\odot m_t$, and the final model weights could be represented as $(m_s \\\\odot m_t) \\\\odot \\\\theta$.\\n\\nImportantly, the lower- and upper-level problems in Bi-RPT are solved in an end-to-end manner, meaning that even the fine-tuning depends on $\\\\theta^*$ and $m^*_s$, it can, in turn, provide feedbacks for adjusting the latter: hence a synergistic optimization is achieved between two stages. The sparse mask selection now directly hinges on the end task (target domain loss $L_t$) rather than heuristics such as weight magnitudes. Lastly, the sparsity levels of $m_s$ and $m_t$ do not need to be separately designated nor manually controlled: we automatically learn the sparsity ratio allocation, under only the total sparsity regularizer $R$.\\n\\nTo practically solve the bi-level optimization of Bi-RPT, we derive algorithms whose details can be found in Appendix A. For the sparsity regularizer $R$, we adopt the smoothed $\\\\ell_0$ term (Guo et al., 2021) to facilitate differentiable training: a gate function $g_{\\\\epsilon}(x) = x^2 / (x^2 + \\\\epsilon)$, whose outputs are almost binary when the $\\\\epsilon$ is small, is used. In general, for the lower-level optimization problem, we update the models' parameters $\\\\theta$ by gradients to minimize $L_s$; for the upper-level optimization, we utilize the gradient unrolling to develop update rules for $\\\\theta$.\\n\\n4.1 PROOF-OF-CONCEPT EXPERIMENTS ON IMAGE DATA\\n\\nFor proof-of-concept, we conduct experiments on a synthesized testbed of image classification, to compare Hand-Tune and Bi-RPT. We adopt two source-domain dataset options: ImageNet (Deng et al., 2009) and ImageNet-C (Hendrycks & Dietterich, 2019), the latter more noisy and corrupted. Two target-domain options are also accompanied: CUB-200 (Wah et al., 2011) and CUB-200 (10-shot), the latter designed to be rigorously \\\"few-shot\\\" where each class has only 10 training samples. Different combinations of $D_s/D_t$ allow us to conduct controlled experiments for stretch-testing various algorithm options' noise robustness as well as data efficiency.\\n\\nSeveral baselines are compared to Hand-Tune and Bi-RPT: (1) Pretrain-and-transfer: the basic workflow of pre-training on $D_s$ followed by finetuning on $D_t$, with no sparsity involved; (2) Pretrain:\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Experiments on image data: testing accuracy of fine-tuned ResNet-18 on CUB-200 / CUB-200 (10-shot) as \\\\( D_t \\\\) after pretraining on ImageNet and ImageNet-C as \\\\( D_s \\\\), respectively.\\n\\n| Methods                  | Two-stage | \\\\( m_s \\\\) | \\\\( m_t \\\\) | \\\\( D_s \\\\) ImageNet | \\\\( D_s \\\\) ImageNet-C | \\\\( D_t \\\\) CUB-200 (10-shot) |\\n|--------------------------|-----------|-----------|-----------|--------------------|--------------------|--------------------------|\\n| No Pretraining           | \u274c \u2717 \u2717    | 44.27%    | 7.98%     | 44.27%             | 7.98%             |\\n| Mix Training             | \u274c \u2717 \u2717    | 30.88%    | 6.72%     | 27.32%             | 6.99%             |\\n| Pretrain-and-transfer    | \u2713 \u2717 \u2717    | 74.16%    | 38.66%    | 73.01%             | 38.73%            |\\n| Pretrain sparsity only   | \u2713 \u2713 \u2717    | 76.01%    | 40.73%    | 73.70%             | 38.90%            |\\n| Transfer sparsity only   | \u2713 \u2717 \u2713    | 74.16%    | 38.90%    | 71.83%             | 32.14%            |\\n| Hand-Tune                | \u2713 \u2713 \u2713     | 76.01%    | 40.78%    | 74.01%             | 39.94%            |\\n| Bi-RPT                   | \u2713 \u2713 \u2713     | 78.60%    | 51.55%    | 76.29%             | 47.01%            |\\n\\nTask Definition. The most naturally defined task on X-Yield is the regression, i.e., predicting the yield strength of alloys, and calculating the error between the model prediction and \u201cground-truth\u201d (experimental results). Besides the regression task, we formulate another surrogate classification task by constructing five categorical labels based on the bin intervals where the ground-truth yield strength fall in. These intervals are: \\\\([0, 0.5)\\), \\\\([0.5, 1)\\), \\\\([1, 1.5)\\), \\\\([1.5, 2)\\), and \\\\([2, \\\\infty)\\).\\n\\nData Representations. We featurize each HEA by mapping its composition and temperature into a \u201cpseudoimage\u201d (please refer to Appendix B.5 and Figure A5). The pseudoimages have two channels: the first channel is constructed from the alloys\u2019 composition using the randomized periodic table structure (Feng et al., 2021). As the temperatures are originally recorded in Kelvin, we convert and normalize them by \\\\( T_{\\\\text{normalized}} = \\\\frac{(K - 273.15)}{2000} \\\\) where \\\\( K \\\\) is the temperature in Kelvin, and then embed the converted temperature as the second channel in pseudoimages.\\n\\nArchitectures and Baselines. The structure of the ML predictor we use is a convolutional neural network. It consists of 3 convolutional layers, each of which has a kernel size of 3, followed by Batch Normalization (Ioffe & Szegedy, 2015) and ReLU (Glorot et al., 2011) activation. A multi-layer perceptron is appended after the convolutional neural network to generate the final prediction for both the regression and classification tasks. We focus on comparing our main proposal, Bi-RPT, with two baselines of No Pretraining and Pretrain-and-transfer, same as defined in Section 4.1.\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"labels. Eventually, we have only 23(11) training samples and 217(229) testing samples. All the low-quality (simulated) data is used for pretraining where applicable. For the regression task, we report the best mean squared error (MSE) on the test splits; and for the classification task, we report models\u2019 test split accuracy to measure their performance.\\n\\n**Training Hyperparameters**\\n\\nWe pretrain the ML predictor on the simulation data for 10 epochs. During the pretraining, we use the Adam optimizer (Kingma & Ba, 2014) with an initial learning rate of $1 \\\\times 10^{-4}$ and a cosine annealing schedule (Loshchilov & Hutter, 2016). For the transfer stage, we fine-tune the pretrained model on the experimental data for 90 epochs. The optimizer we use is the SGD optimizer with an initial learning rate of $1 \\\\times 10^{-3}$. We also decay the learning rate by 10 for every 30 epoch. The batch sizes for pretraining and fine-tuning are 16 and 4, respectively.\\n\\n### 5.2 RESULTS\\n\\n**Classification and regression with extreme few-shot settings.** We first apply Bi-RPT to solve the regression and classification tasks under the two extreme few-shot settings where only 5% and 10% experimental data are available, respectively. Table 2 shows that: (1) pretraining on simulation data can benefit the ML predictor consistently on both the regression (over 10% reduction in MSE) and classification (over 11% improvement in accuracy) tasks, especially when the data is more scarce; (2) the integration of sparsity into the pretraining and transfer workflow can further strengthen the predictor\u2019s generalization, improving accuracy by 0.98% and reducing MSE by 8.91% using merely 10% training experiment data, and the improvement also becomes even more significant with 5% training data (1.53% increase in terms of the accuracy and 19.75% reduction in terms of the MSE).\\n\\n| Method               | Test MSE | Test Accuracy |\\n|----------------------|----------|---------------|\\n| No Pretraining       | 0.212 \u00b1 0.041 | 47.25 \u00b1 0.80% |\\n| Pretrain-and-transfer| 0.162 \u00b1 0.002 | 62.84 \u00b1 1.96% |\\n| Bi-RPT               | 0.130 \u00b1 0.006 | 64.37 \u00b1 1.10% |\\n\\n**Table 2: Test accuracy on the testing set of different splits of high-fidelity alloy data. The experiments are repeated 10 times, and we report both the mean and the 95% confidence interval.**\\n\\n**Classification and regression with 10-fold cross-validation.** On the slightly \u201cdata-rich\u201d 10-fold cross-validation setting, we have observed a similar trend: the bi-stage regime of pretraining and transfer out-performs the single-stage training pipeline, and incorporating sparsity can consistently provide remarkable improvement to the ML predictor, particularly in the regression performance. Table 3: Classification and regression performance under the ten-folded cross-validation settings.\\n\\n| Method       | Classification | Regression |\\n|--------------|----------------|------------|\\n| No Pretraining| 67.50 \u00b1 5.16%  | 0.226 \u00b1 0.027 |\\n| Pretrain-and-transfer| 82.09 \u00b1 3.86%  | 0.206 \u00b1 0.026 |\\n| Bi-RPT       | 82.50 \u00b1 2.93%  | 0.068 \u00b1 0.009 |\\n\\n**Table 3: Classification and regression performance under the ten-folded cross-validation settings.**\\n\\n**Performance comparison on alloys at various temperatures.** Based on the trained model with 10% experimental data, we predict the yield strength of three alloys, MoNbTaTi, MoNbTaTiW and HfMoNbTaTiZr, at different temperatures. Table 4 shows the predicted yield stress on these three alloys using Bi-RPT and baselines. On the quinary and senary alloy systems, Bi-RPT shows exceptional precision in predicting the experimental yield stress. More scrutiny of those predictions reveals several findings that neatly align with our material science expertise. For example, it is known that screw dislocations are more likely to be dominant than the edge in MoNbTi and NbTaTi ternaries (shown from the ternary comparison in the Citrine database (Borg et al., 2020)). Thus it makes sense that the model under-predicts the MoNbTaTi and MoNbTaTiW cases: our model seems to correctly pick up these differences and predicts a higher yield strength. Another example is that our model over-predicts HfMoNbTaTiZr at lower temperatures (300 K \u223c 900 K). Since all our collected experimental samples are 100% body-centered cubic (which shows, admittedly, a limitation...\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Prediction MSE under different temperatures. We compare the results of three methods: No Pretraining (NP), Pretrain-and-transfer (PT), and Bi-RPT.\\n\\nTable 4: Predicted yield stress of different alloys under different temperatures. Only 10% of the experimental data are available during fine-tuning. We compare the predicted yield stress generated by Bi-RPT with our \\\"No Pretraining\\\" (NP) and \\\"Pretrain-and-transfer\\\" (PT) baselines and the simulation. The numbers with the smallest error are marked in bold.\\n\\n| Alloys         | Temperature (K) | Predicted Yield Stress (GPa) | Experimental (GPa) |\\n|----------------|-----------------|------------------------------|--------------------|\\n|                | 293.15          | 1.078                        | 1.170              |\\n|                | 473.15          | 0.965                        | 0.902              |\\n|                | 673.15          | 0.746                        | 0.731              |\\n|                | 873.15          | 0.508                        | 0.584              |\\n|                | 1273.15         | 0.425                        | 0.488              |\\n| MoNbTaTiW      | 298.15          | 1.268                        | 1.068              |\\n|                | 873.15          | 0.677                        | 0.607              |\\n|                | 1073.15         | 0.618                        | 0.523              |\\n|                | 1273.15         | 0.536                        | 0.486              |\\n| HfMoNbTaTiZr   | 296.15          | 1.527                        | 1.132              |\\n|                | 873.15          | 0.861                        | 0.685              |\\n|                | 1073.15         | 0.762                        | 0.612              |\\n|                | 1273.15         | 0.662                        | 0.573              |\\n\\nPerformance at high temperatures. One of the important tasks in the alloy design community is to find alloys that are capable of withstanding stress at high temperatures. To verify if Bi-RPT can provide reliable recommendations to help the community achieve this goal, we look deeper into the predictive performance in high-temperature regimes. We train our model with 10% data, predict the yield stress for the rest 90%, and compare the predictive quality of models at high temperatures in Figure 3. We can see that Bi-RPT significantly outperforms other baselines, especially at temperatures greater than 1400 K. These results suggest Bi-RPT could serve as a strong tool for designing HEAs with superior yield stress at elevated temperatures.\\n\\nCONCLUSIONS\\n\\nTo address the important yet challenging problem of HEA yield stress prediction, we curated and released X-Yield, the first large-scale, multi-fidelity benchmark. To effectively leverage this benchmark, we also designed a two-stage cross-quality few-shot transfer workflow and proposed to utilize sparsity to tackle both challenges of low data quality at pretraining and scarcity at transfer. Besides ad-hoc methods, we formulated a principled bi-level optimization framework to automatically learn the optimal sparse masks and sparsity allocation between two stages. Extensive experiments on both image data testbeds and X-Yield demonstrate the Bi-RPT showed a substantial improvement over existing baselines. Moving forward, we are now closely working with material scientists to validate our ML prediction results based on their domain expertise, and the team has already identified some alloy candidates that appear promising to be experimentally validated.\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table A7: Test accuracy of fine-tuned ResNet-18 on CUB-200 (10-shot) after pretrained on ImageNet, under different levels of sparsity at pretraining and sparsity at transfer.\\n\\n| Sparsity At Transfer | Sparsity At Pretraining |\\n|----------------------|-------------------------|\\n| 0.00%                | 20.00%                  |\\n| 36.00%               | 48.80%                  |\\n| 59.04%               | 67.23%                  |\\n\\nTable A8: Test accuracy of fine-tuned ResNet-18 on CUB-200 (10-shot) after pretrained on ImageNet-C, under different levels of sparsity at pretraining and sparsity at transfer.\\n\\n| Sparsity At Transfer | Sparsity At Pretraining |\\n|----------------------|-------------------------|\\n| 0.00%                | 20.00%                  |\\n| 36.00%               | 48.80%                  |\\n| 59.04%               | 67.23%                  |\\n\\nThe performance comparison is shown in Table A9, where we can see that learning masks at both stage yields the highest performance.\\n\\nEffects of $\\\\gamma$.\\n\\nWe conduct a set of ablation experiments to study the effects of different $\\\\gamma$ again on ResNet-18 (pretrained by ImageNet-C, fine-tuned on CUB-200). We vary $\\\\gamma$ with in $\\\\gamma \\\\in \\\\{0.5, 1, 2, 3\\\\} \\\\times 10^{-4}$, and we present the results in Table A10. We show that $1 \\\\times 10^{-4}$ yields the highest performance among all the choices.\\n\\nEffects of learning rates.\\n\\nWe conduct a set of ablation experiments on ResNet-18 (pretrained by ImageNet-C, fine-tuned on CUB-200) to study the effects of different learning rate on $m_s$ and $m_t$. The learning rates we study in this ablation experiments are $\\\\lambda \\\\in \\\\{2.5, 3.0, 3.5, 4.0, 4.5, 5.0\\\\}$. We present the test accuracies in Table A11, and we observe that Bi-RPT can stably outperform baselines (74.01%) within a wide range of $\\\\lambda$.\\n\\nTable A9: Ablation study on different sparse masks on image data.\\n\\n| Mask Type | Test Accuracy |\\n|-----------|---------------|\\n| Fixed $m_s$ and $m_t$ | 71.58% |\\n| Fixed $m_s$ | 72.09% |\\n| Fixed $m_t$ | 75.53% |\\n| Ours (Bi-RPT) | 76.29% |\\n\\nTable A10: Ablation study on the effects of different $\\\\gamma$ on the image data. Test accuracy of fine-tuned ResNet-18 on CUB-200 after pretrained on ImageNet-C is reported.\\n\\n| $\\\\gamma \\\\times 10^{-4}$ | Test Accuracy |\\n|-------------------------|---------------|\\n| $0.5 \\\\times 10^{-4}$    | 72.32% |\\n| $1 \\\\times 10^{-4}$      | 76.29% |\\n| $2 \\\\times 10^{-4}$      | 65.42% |\\n| $3 \\\\times 10^{-4}$      | 52.59% |\\n\\nTable A11: Ablation study on the effects of different learning rate on $m_s$ and $m_t$ on image data. Test accuracy of fine-tuned ResNet-18 on CUB-200 after pretrained on ImageNet-C is reported.\\n\\n| $\\\\lambda$ | Test Accuracy |\\n|-----------|---------------|\\n| 2.5       | 72.88% |\\n| 3.0       | 75.73% |\\n| 3.5       | 76.29% |\\n| 4.0       | 75.94% |\\n| 4.5       | 73.69% |\\n| 5.0       | 73.34% |\\n\\nB.4 VISUALIZATION\\n\\nWe visualize the sparsity pattern learned by Bi-RPT at two stages in Figure A4.\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure A4: Layerwise sparsity learned by Bi-RPT on CUB-200 and Birds-S with ImageNet and ImageNet-C pretraining. We report the sparsity level of the two masks, as well as their combined sparsity (note that Bi-RPT allows for the two masks to partially overlap).\\n\\n**B.5 HEADATA**\\n\\n**REPRESENTATIONS**\\n\\nThe raw inputs for our ML predictor are the alloy's composition and the temperature where the experiment is conducted; therefore, they are $11$-dimensional vectors. We map these vectors into 2D images following the pipeline shown in Figure A5. Given a formulation of an alloy, the periodic table representation (PTR) sets the percentage of each element into a specific position according to its position in the periodic table; and the randomized periodic table representation (RPTR) sets the percentage of each element with a pre-defined shuffled periodic table. In our experiments, we use the RPTR to map values in a more balanced way.\\n\\n**ADDITIONAL EXPERIMENTS**\\n\\n**C.1 UNCERTAINTY QUANTIFICATION**\\n\\nWe provide additional analysis of uncertain quantification. We ensemble ten models trained with Bi-RPT and pretrain-and-transfer (PT) methods by averaging their predictions (Lakshminarayanan et al., 2017), and calculate the standard deviation of the predictions as the uncertainty. The results after ensemble are shown in Table A12. We show that an ensemble of sparse models provides more reliable results compared to the pretrain-and-transfer baseline. Compared with the ensemble of dense models (PT), the ensemble of sparse models also exhibits strong correlation between the uncertainty and the prediction error.\\n\\n**DATASET COMPARISON**\\n\\nWe have provided a comparison between different relevant datasets in Table A13. We elaborate more on the differences:\\n\\n1. Maresca & Curtin (2020) have only sparse data from the Mo-Nb-Ta-V-W element family.\\n2. Lee et al. (2021a) has released a database of the predicted yield strength of 10 million alloys from the Al-Cr-Mo-Nb-Ta-V-W-Hf-Ti-Zr family at 1300 K. Our dataset contains alloys\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure A5: The pipeline for converting a raw input into a pseudoimage. The temperature is embedded as the value of the second channel.\\n\\nfrom Al-Cr-Fe-Mo-Nb-Ta-V-W-Hf-Ti-Zr family at temperatures from 300 K to 2500 K. Our simulation data are significantly larger (over 3 billion samples). The whole simulation data will be available, while only 100K are included for training the ML models in this study.\\n\\n3. Borg et al. (2020) compiles experimental data from published material science articles since 2004. The dataset contains 630 samples with different crystal structures. Our experimental dataset also compiles experimental data from published material science articles too, but we have also sub-selected the data points using material science domain knowledge. Specifically, we only focus on alloys with BCC structures in contrast to Borg et al. (2020).\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table A12: Uncertainty estimation calculated by ensembling independently trained models. We study two methods: pretrain-and-transfer (PT) and Bi-RPT. The results after ensemble are reported as PT-Ensemble and Bi-RPT-Ensemble, respectively. The estimated uncertainty is reported in brackets.\\n\\n| Alloy Family      | Temperature (K) | Predicted Yield Stress (GPa) | Experimental (GPa) |\\n|-------------------|-----------------|------------------------------|--------------------|\\n| Bi-RPT            | Bi-RPT-Ensemble | PT                           | PT-Ensemble        |\\n| MoNbTaTi          | 293.15          | 1.078                        | 1.158 (0.083)      |\\n|                   |                 |                              | 1.062              |\\n|                   |                 |                              | 1.054 (0.011)      |\\n|                   | 473.15          | 0.965                        | 1.046 (0.087)      |\\n|                   |                 |                              | 0.902              |\\n|                   |                 |                              | 0.908 (0.015)      |\\n|                   | 673.15          | 0.746                        | 0.850 (0.085)      |\\n|                   |                 |                              | 0.731              |\\n|                   |                 |                              | 0.740 (0.026)      |\\n|                   | 873.15          | 0.508                        | 0.674 (0.103)      |\\n|                   |                 |                              | 0.584              |\\n|                   |                 |                              | 0.604 (0.021)      |\\n|                   | 1273.15         | 0.425                        | 0.482 (0.088)      |\\n|                   |                 |                              | 0.488              |\\n|                   |                 |                              | 0.501 (0.018)      |\\n\\n| Alloys            | Temperature (K) | Predicted Yield Stress (GPa) | Experimental (GPa) |\\n|-------------------|-----------------|------------------------------|--------------------|\\n| MoNbTaTiW         | 298.15          | 1.268                        | 1.268 (0.098)      |\\n|                   |                 |                              | 1.068              |\\n|                   |                 |                              | 1.062 (0.011)      |\\n|                   | 873.15          | 0.677                        | 0.798 (0.102)      |\\n|                   |                 |                              | 0.607              |\\n|                   |                 |                              | 0.624 (0.022)      |\\n|                   | 1073.15         | 0.618                        | 0.681 (0.111)      |\\n|                   |                 |                              | 0.523              |\\n|                   |                 |                              | 0.528 (0.013)      |\\n|                   | 1273.15         | 0.536                        | 0.567 (0.124)      |\\n|                   |                 |                              | 0.486              |\\n|                   |                 |                              | 0.496 (0.017)      |\\n\\n| Alloy Family      | Temperature (K) | Predicted Yield Stress (Gpa) | Experimental (Gpa) |\\n|-------------------|-----------------|------------------------------|--------------------|\\n| HfMoNbTaTiZr      | 296.15          | 1.527                        | 1.392 (0.122)      |\\n|                   |                 |                              | 1.132              |\\n|                   |                 |                              | 1.142 (0.021)      |\\n|                   | 873.15          | 0.861                        | 0.864 (0.098)      |\\n|                   |                 |                              | 0.685              |\\n|                   |                 |                              | 0.698 (0.017)      |\\n|                   | 1073.15         | 0.762                        | 0.747 (0.105)      |\\n|                   |                 |                              | 0.612              |\\n|                   |                 |                              | 0.624 (0.022)      |\\n|                   | 1273.15         | 0.662                        | 0.646 (0.134)      |\\n|                   |                 |                              | 0.573              |\\n|                   |                 |                              | 0.587 (0.022)      |\\n\\nTable A13: Comparison between different datasets.\\n\\n| Dataset                     | Alloy Family                  | Number of data points | Temperature |\\n|-----------------------------|-------------------------------|-----------------------|-------------|\\n| Maresca & Curtin (2020)     | Mo-Nb-Ta-V-W                  | Sparse                | N/A         |\\n| Lee et al. (2021a)          | Al-Cr-Mo-Nb-Ta-V-W-Hf-Ti-Zr   | 10 million            | 1300 K      |\\n| Borg et al. (2020)          | N/A                           | 630                   | N/A         |\\n| X-Yield (Ours)              | Al-Cr-Fe-Mo-Nb-Ta-V-W-Hf-Ti-Zr| 3 billion             | 300 K - 2500 K |\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gleeble. URL https://www.gleeble.com/. Accessed: 2022-09-26.\\n\\nUttam Bhandari, Md Rumman Rafi, Congyan Zhang, and Shizhong Yang. Yield strength prediction of high-entropy alloys using machine learning. Materials Today Communications, 26:101871, 2021a.\\n\\nUttam Bhandari, Congyan Zhang, Congyuan Zeng, Shengmin Guo, Aashish Adhikari, and Shizhong Yang. Deep learning-based hardness prediction of novel refractory high-entropy alloys with experimental validation. Crystals, 11:46, 2021b.\\n\\nChristopher KH Borg, Carolina Frey, Jasper Moh, Tresa M Pollock, St\u00b4ephane Gorsse, Daniel B Miracle, Oleg N Senkov, Bryce Meredig, and James E Saal. Expanded dataset of mechanical properties and observed phases of multi-principal element alloys. Scientific Data, 7(1):1\u20136, 2020.\\n\\nSaikiran Bulusu, Bhavya Kailkhura, Bo Li, Pramod K Varshney, and Dawn Song. Anomalous example detection in deep learning: A survey. IEEE Access, 8:132330\u2013132347, 2020.\\n\\nTianlong Chen, Yu Cheng, Zhe Gan, Jingjing Liu, and Zhangyang Wang. Data-efficient gan training beyond (just) augmentations: A lottery ticket perspective. Advances in Neural Information Processing Systems, 34:20941\u201320955, 2021.\\n\\nTianlong Chen, Zhenyu Zhang, Pengjun Wang, Santosh Balachandra, Haoyu Ma, Zehao Wang, and Zhangyang Wang. Sparsity winning twice: Better robust generaliztion from more efficient training. arXiv preprint arXiv:2202.09844, 2022.\\n\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248\u2013255. Ieee, 2009.\\n\\nJames Diffenderfer, Brian Bartoldson, Shreya Chaganti, Jize Zhang, and Bhavya Kailkhura. A winning hand: Compressing deep networks can improve out-of-distribution robustness. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nShaojin Ding, Tianlong Chen, and Zhangyang Wang. Audio lottery: Speech recognition made ultra-lightweight, noise-robust, and transferable. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=9Nk6AJkVYB.\\n\\nShuo Feng, Huadong Fu, Huiyu Zhou, Yuan Wu, Zhaoping Lu, and Hongbiao Dong. A general and transferable deep learning framework for predicting phase formation in materials. npj Computational Materials, 7(1):1\u201310, 2021.\\n\\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pp. 1126\u20131135. PMLR, 2017.\\n\\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2019.\\n\\nXavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 315\u2013323. JMLR Workshop and Conference Proceedings, 2011.\\n\\nCarla P Gomes, Daniel Fink, R Bruce van Dover, and John M Gregoire. Computational sustainability meets materials science. Nature Reviews Materials, 2021:645\u2013647, 2021.\\n\\nShupeng Gui, Haotao Wang, Haichuan Yang, Chen Yu, Zhangyang Wang, and Ji Liu. Model compression with adversarial robustness: A unified optimization framework. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nYi Guo, Huan Yuan, Jianchao Tan, Zhangyang Wang, Sen Yang, and Ji Liu. Gdp: Stabilized neural network pruning via gates with differentiable polarization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5239\u20135250, 2021.\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "uYFRjvSJXbQ", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "uYFRjvSJXbQ", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nYihua Zhang, Guanhuan Zhang, Prashant Khanduri, Mingyi Hong, Shiyu Chang, and Sijia Liu.\\n\\nRevisiting and advancing fast adversarial training through the lens of bi-level optimization.\\narXiv preprint arXiv:2112.12376, 2021.\\n\\nTao Zheng, Xiaobing Hu, Feng He, Qingfeng Wu, Bin Han, Chen Da, Junjie Li, Zhijun Wang, Jincheng Wang, Ji-jung Kai, Zhenhai Xia, and Liu C T. Tailoring nanoprecipitates for ultra-strong high-entropy alloys via machine learning and prestrain aging.\\nJournal of Materials Science & Technology, 69:156\u2013167, 2021.\\n\\nYin-Dong Zheng, Yun-Tao Ma, Ruo-Ze Liu, and Tong Lu. A novel group-aware pruning method for few-shot learning. In 2019 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20137, 2019. doi: 10.1109/IJCNN.2019.8852221.\\n\\nWenhan Zhu, Wenyi Huo, Shiqi Wang, Xu Wang, Kai Ren, Shuyong Tan, Feng Fang, Zonghan Xie, and Jianqing Jiang. Phase formation prediction of high-entropy alloys: a deep learning study.\\nJournal of Materials Research and Technology, 18:800\u2013809, 2022.\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Similar to the trends in computer vision fields (Tremblay et al., 2018), recent efforts attempt to mitigate the scarcity of real-world measurements using ML-based predictors: to directly predict their yield strengths from the alloy inputs (Bhandari et al., 2021a); and such ML-based predictors could be trained using simulated data. Indeed, material sciences applications are often blessed by developed simulation models, e.g., Maresca & Curtin (2020). However, such a blessing is often compromised by the domain gap between the simulated data and the \u201cground-truth\u201d experimental data, often due to many inevitable simplifications in simulation modeling. For example, the yield strength of a material can vary greatly based on processing and testing conditions as well as grain size and texture (Toda-Carballo et al., 2014; Lin et al., 2014); yet simulation models commonly rely on properties intrinsic to the alloy and do not incorporate variations in experimental conditions. The lack of public datasets in this field also renders it difficult to benchmark ML models\u2019 progress.\\n\\nIn this paper, we start by curating a large-scale benchmark, called X-Yield, that for the first time combines experimental data with simulation data to address the problem of predicting yield strength in HEAs. While using experimental data is always preferred since they are \u201chigh-quality\u201d ground truths, it is impractical to generate high quantities of data, especially for capturing yield strength at elevated temperatures. Thus, simulation data can be acquired by massive quantities to fill the gap, despite their relatively \u201clow quality\u201d due to inherent model misspecification or simplification. The low-quality simulation data was selected to represent ternary-septenary systems from an eleven-element palette consisting of mostly refractory elements (Al-Cr-Fe-Hf-Mo-Nb-Ta-Ti-V-W-Zr). While there are existing experimental databases (Borg et al., 2020) and models to predict high-temperature yield strength in HEAs (Maresca & Curtin, 2020), to our best knowledge, this is the first multi-fidelity dataset in the public domain that combines real experimental measurements and large quantities (over 100K) of simulation data for mechanical property prediction in HEAs. This specialized data set should be able to predict high-temperature yield strength across a broad range of HEAs. The predictions of this model could be used to pinpoint which alloys are the strongest at elevated temperatures, allowing experiments to focus on pre-sorted candidates for future study eliminating the need to spend several weeks testing a candidate without promise.\\n\\nSparsity for noise robustness (learning from massive \u201clow-quality\u201d simulation data) Sparsity for data-efficiency (transferring to few-shot \u201chigh-quality\u201d experimental data)\\n\\nFigure 1: Proposed two-stage workflow. The HEA yield strength prediction model is first pre-trained on massive \u201clow-quality\u201d simulation data, and is then fine-tuned/transferred on few-shot \u201chigh-quality\u201d experimental data to optimize its prediction in this target domain. Note that the tool of sparsity will be leveraged in both pre-training and fine-tuning stages, for the purposes of gaining noise robustness/transferablity and enhancing data efficiency, respectively.\\n\\nThe new X-Yield benchmark is set to facilitate ML for HEA yield strength prediction, but learning from such a multi-fidelity dataset is highly non-trivial. To this end, we next conceptualize a cross-quality few-shot transfer workflow: first pre-training the prediction model on the data-rich yet \u201clow-quality\u201d source domain (simulated data), and then fine-tuning the model towards the data-scarce yet \u201chigh-quality\u201d target domain (experimental data). However, this vanilla workflow is challenged by two issues: a significant quality gap between source and target domains, and an extreme data scarcity of target data. Inspired by the recent success of sparsity regularizers, we propose to\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"incorporate sparsity to regularize both stages: sparsifying pre-training to improve the robustness and cross-domain transferability of learned features (Guo et al., 2018; Sehwag et al., 2019; Chen et al., 2022; Sehwag et al., 2020; Ding et al., 2022; Diffenderfer et al., 2021), and sparsifying fine-tuning to overcome data shortfalls (Liu et al., 2020; Chen et al., 2021; Tao et al., 2022). We demonstrate proof-of-concept experiments that even the simplest magnitude-based weight pruning could play effective regularization roles in our workflow. Furthermore, to avoid the ad-hoc two-step pruning as well as trial-and-error sparsity ratio selection at either stage, we propose a novel integrated optimization framework termed \\\\textit{Bi-Level Regularized Pre-training and Transfer} (\\\\textit{Bi-RPT}), that jointly learns optimal sparse masks and automatically allocates sparsity levels for both stages.\\n\\nOur main contributions are summarized as follows:\\n\\n\u2022 \\\\textbf{Dataset:} We present \\\\textit{X-Yield}, the first public large-scale, multi-quality material science benchmark for HEA yield strength prediction, containing alloys' compositions, processing temperatures, and yield strengths. Specifically, the yield strengths of 240 HEAs are experimentally measured, while that of the remaining samples (over 100K) is calculated by simulations.\\n\\n\u2022 \\\\textbf{Methodology:} we formulate a cross-quality few-shot transfer workflow that can jointly exploit the simulated and experimental data for accurate predictions, and we innovate to leverage sparsity for addressing both the simulated/experimental domain gap and the scarcity of experimental data. While ad-hoc magnitude-based weight pruning is already found to be helpful, we further formulate an integrated bi-level optimization framework called \\\\textit{Bi-RPT} to automate the optimal sparse mask generation and sparsity ratio allocation at both pre-training and fine-tuning stages.\\n\\n\u2022 \\\\textbf{Results:} Extensive experiments show that \\\\textit{Bi-RPT} can boost performance on the \\\\textit{X-Yield} benchmark alongside other synthesized testbeds. In particular, for the yield strength regression task, we achieve a reduction of $19\\\\sim38\\\\%$ on the test mean squared error by merely using 5$\\\\sim$10\\\\% of the available experimental data. For the yield strength classification task, we achieve $0.98\\\\sim1.53\\\\%$ of improvement in terms of the test accuracy.\\n\\n### RELATED WORK\\n\\n#### 2.1 MACHINE LEARNING IN MATERIALS RESEARCH\\n\\nML has been applied to solve a wide range of problems in materials science ranging from the fields of inorganic chemistry (Kailkhura et al., 2019) to sustainability (Gomes et al., 2021), and metallurgy (Stan et al., 2020), with the typical purposes to predict materials properties and accelerate simulations (Pilania, 2021). In both cases, ML techniques are hailed as reducing computational time in contrast to traditional materials science methods and are typically fast to develop (Wei et al., 2019). Later on, deep learning has been successfully applied to problems in the field of HEAs, in particular to predict phase formation (Lee et al., 2021b; Zhu et al., 2022). These approaches provide significant increases in speed compared to phase predictions with CALculation of PHAse Diagrams (CALPHAD) (Saunders & Miodownik, 1998), density functional theory (Parr, 1983), and molecular dynamics methods (Shuichi, 1991) commonly used in materials science. Other properties predicted with deep learning are crystal structures, elastic constants (Liu et al., 2023) and hardness (Bhandari et al., 2021b). When it comes specifically to the yield strength of HEAs, its prediction has also been previously explored with deep learning (Liu et al., 2023; Bhandari et al., 2021a). However, a majority of these efforts are restricted to the development of specific alloys (Zheng et al., 2021; Bhandari et al., 2021b) or consist solely of transition metals (Wen et al., 2019), and many studies also only use a small experimental dataset for prediction (Wen et al., 2021). A generalized multi-fidelity ML model to predict yield HEA strength at scale remains to be absent yet highly demanded.\\n\\n#### 2.2 SPARSITY REGULARIZATION IN DEEP LEARNING\\n\\nSparsity or pruning was traditionally treated as a mainstream model compression approach in deep learning (Han et al., 2015). Recently, sparse regularizers have been increasingly used to enhance deep model robustness to various noise, malicious attacks, and distribution shifts. Guo et al. (2018); Sehwag et al. (2019); Gui et al. (2019) studied the intrinsic relationship between pruning and adversarial robustness. Recently, Diffenderfer et al. (2021) comprehensively demonstrated the benefit of model sparsification to improve robustness to distributional shifts (Hendrycks & Dietterich, 2019;\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sparse regularizers also exhibit promise in improving data efficiency. For example, Zheng et al. (2019); Liu et al. (2020) proposed to learn model pruning strategies for few-shot learning; Tian et al. (2020) combined model sparsification with meta-learning to improve few-shot performance. Sparse regularizers have even been proven effective beyond few-shot image classification, such as enhancing the data efficiency in image generation (Chen et al., 2021).\\n\\n## Bi-level Optimization\\n\\nBi-level optimization is a hierarchical framework where the variables in the upper-level optimization problem are dependent on the lower-level problem. Finn et al. (2017); Rajeswaran et al. (2019) formulated the meta-learning problem in the form of bi-level optimization, and solve it by using first-order approximations. Other applications of bi-level optimization include data and label poisoning (Mehra et al., 2021; Huang et al., 2020), and adversarial training (Zhang et al., 2021). In this work, we utilize bi-level optimization to formulate our two-stage workflow with sparsification and find each stage\u2019s optimal weights and sparse masks while considering their sequential dependency.\\n\\n## X-Yield: A New Benchmark for HEA Yield Strength Prediction\\n\\n**Overview**\\n\\nConventional alloys typically have one principal element with small amounts of other elements added to improve material properties (Ye et al., 2016) while HEAs can have multiple principal elements. The discovery of HEAs opened the door to a significantly wider range of design space to explore, most of which has yet to be examined (Miracle & Senkov, 2017). To address the task of using ML to predict HEA yield strength, we focus on the sub-field of refractory HEAs (RHEAs). These materials have been demonstrated to maintain excellent mechanical properties at high temperatures (Li et al., 2020), making them ideal candidates for hypersonics and aerospace industry applications. Prior work adopting ML to predict RHEA properties either uses solely experimental data (Wen et al., 2021), or restricts predictions to only transition metals (Wen et al., 2019) or specific alloys such as MoNbTaTiW (Bhandari et al., 2021a). Hence, a generalizable ML prediction model for a broad range of RHEAs is still absent. As mentioned earlier, it is impractical to generate high quantities of experimental data, especially for capturing yield strength at elevated temperatures. There are also challenges specific to high-temperature measurements such as controlling oxidation, confirming the heating profile and gradient within the samples, and use of more challenging experimental techniques (crosshead displacement) than those at lower temperatures (extensometers).\\n\\nThis work develops X-Yield, the first publicly available, multi-fidelity dataset consisting of over 100K low-quality simulated points and 240 experimental data points to explore the RHEA design space. In this study alone, the entire composition space of all alloys containing between ternary-septenary systems from the Al-Cr-Fe-Hf-Mo-Nb-Ta-Ti-V-W-Zr family is examined. Since obtaining real high-temperature yield strength data is challenging, a majority of the experimental yield strength data in the literature was taken close to room temperature (Borg et al., 2020) even though there is more interest in RHEA properties at the high-temperature end (Miracle & Senkov, 2017). From X-Yield, a multi-fidelity ML model is expected to be trained to predict high-temperature yield strengths for a broad palette of RHEAs. The combination of high-temperature yield strengths from the simulated dataset and experimental input can generate an ML model to accurately and efficiently predict high-temperature yield strengths of alloys not included in the training set.\\n\\n**Dataset Construction**\\n\\nThe yield strength of the simulation data was predicted using the analytic and parameter-free mechanistic yield strength model developed by Maresca & Curtin (2020). This model describes body-centered cubic (BCC) multi-principal element alloy (MPEA) solid solution strengthening associated with edge dislocations, in terms of elemental atomic volumes and elastic moduli. The yield strength was predicted for all ternary (1% increments), quaternary (1% increments), quinary (5% increments), senary (5% increments), and septenary (5% increments) alloys from the Al-Cr-Fe-Hf-Mo-Nb-Ta-Ti-V-W-Zr element family at temperatures between 300 K - 2500 K in increments of 100 K. This resulted in over three billion data points of which approximately 100,000 were randomly selected for inclusion in this study. Note that even this advanced simulation model suffers from notable oversimplification and data quality issues. For example, the phase stability and dislocation character were not used to filter alloys in the study and the model may overpredict the yield strength of alloys with non-BCC phases and underpredict the yield strength of alloys with different dislocation character, e.g. screw.\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nFigure 2: Left: the distribution of the yield stress; Middle: the distribution of the temperature; Right: pairwise visualization of the yield stress.\\n\\nThe high-quality experimental dataset was carefully filtered and curated from the database generated by Borg et al. (2020) consisting of mechanical property information for MPEAs. All data points were extracted that consisted solely of elements from the above element family, consisted only of BCC phases, were at temperatures higher than 20\u00b0C, and contained a yield strength value.\\n\\nDataset Characteristics and \u201cQuality Gap\u201d\\n\\nAs depicted in Figure 2, the simulation and experimental yield stress have different distributions. In the low-quality simulation data, a considerable portion of yield stress annotations is greater than 2, while the experimental data hardly contains yield stress points beyond 2 (with one datapoint exception) due to the experimental condition constraints. The distribution of the simulated yield stress is also significantly more skewed than the experimental ones. Pairwise visualization of the yield stress on the high-quality experimental samples suggests a substantial deviation between the simulation and experimental results. The distributions of the processing temperatures are also heterogeneous, i.e., the simulation data presents a uniform pattern while the temperatures in the conducted experiments are in a bimodal shape. These observations showcase the domain shifts or \u201cquality gap\u201d between simulations and experiments.\\n\\nROSS-QUALITY-NEW-SHOT TRANSFER: A TWO-STAGE WORKFLOW AIDED BY SPARSITY (TWICE)\\n\\nIn this section, we first introduce the basic two-stage workflow, upon which we propose sparsification methods (a vanilla approach \u201cHand-Tune\u201d and an improved principled framework \u201cBi-RPT\u201d).\\n\\nBasic Two-Stage Workflow: Pre-training then Fine-tuning\\n\\nLet us denote the high-quality target domain (experimental data) by $D_t$, and the low-quality source domain (simulated data) by $D_s$. Our goal is to learn a generalizable predictor over $D_t$ while leveraging the aid of $D_s$. One naive idea is to simply combine the two data domains and jointly train a supervised model. However, the large domain gap between $D_s$ and $D_t$, as well as the sample scarcity in $D_t$, will result in the jointly trained predictor to fit $D_t$ poorly. Instead, we propose to formulate our workflow as a two-stage pipeline: first pre-training a model on $D_s$, and then fine-tuning to optimize the prediction over $D_t$.\\n\\nIncorporating Bi-Stage Sparsity: A Vanilla Approach.\\n\\nHowever, the features learned from $D_s$ will inevitably suffer from domain gap and noise when applied towards $D_t$, and the extreme data scarcity of $D_t$ remains as another challenge. Inspired by the recent success of sparse regularizers in improving both robustness/transferability and data efficiency, we attempt to incorporate sparsity into both stages to address the two-fold challenges.\\n\\nWe first prove our concepts by proposing a vanilla ad-hoc approach, which we refer to as Hand-Tune. Starting from pre-training over $D_s$, we perform the standard iterative magnitude pruning (IMP) (Frankle & Carbin, 2019) during pre-training. In particular, we alternate between (re-)training and pruning; each time, we prune the 20% smallest-magnitude weights from the existing non-zero weights by default and continue (re-)training the remaining non-zero weights. Such a \u201cprune-and-retrain\u201d routine is repeated for $N_s$ rounds to obtain the final sparse mask $m_s$ (1 denotes the element to be non-zero and 0 to be pruned) associated with the pretrained model weight. Then, we move on to fine-tuning over $D_t$, and start another round of IMP on top of the pre-trained model: note that this second-stage IMP continues only on the subset of current non-zero weights, i.e., the 1-valued regions in $m_s$. IMP in fine-tuning repeats another $N_t$ round (with the identical protocol as the first...\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we present the technical details of our proposed method and framework (\\\"Hand-Tune\\\" and \\\"Bi-RPT\\\").\\n\\nA.1 Hand-Tune\\nHand-Tune decides sparse masks for the two stages in an iterative way as explained in Algorithm 1.\\n\\n**Algorithm 1 Hand-Tune**\\n\\n*Input:* Initialization weights $\\\\theta_0$, low-quality pretraining dataset $D_s$, high-quality fine-tuning dataset $D_t$, number of IMP rounds $N_s$ for the pretraining stage and $N_t$ for the fine-tuning stage.\\n\\n*Output:* the trained weights $\\\\theta^*$, the sparse mask $m_s$ for the pretraining stage, and the sparse mask $m_t$ for the fine-tuning stage.\\n\\n1. Initialize the sparse masks $m_s$ for the pretraining stage to be all \\\"1\\\" mask.\\n2. Initialize the model's weight as $\\\\theta_0$ and train the weights on $D_s$ to obtain $\\\\theta_s$.\\n3. For $i = 1, 2, \\\\ldots, N_s$ do\\n   - IMP at the pre-training stage\\n     - Prune 20% of the smallest-magnitude weights from the non-zero regions of $m_s \\\\odot \\\\theta_s$, by setting the values at corresponding positions to those weights in $m_s$ to \\\"0\\\".\\n     - (Re-)train the sparse weights $m_s \\\\odot \\\\theta_s$ on $D_s$. Only $\\\\theta_s$ is updated.\\n   end for\\n4. Initialize the sparse masks at the fine-tuning stage $m_t$ to be all \\\"1\\\" masks and freeze $m_s$.\\n5. Initialize model's weight as $m_s \\\\odot \\\\theta_s$, and train on $D_t$ to obtain $m_s \\\\odot \\\\theta_t$.\\n6. For $i = 1, 2, \\\\ldots, N_t$ do\\n   - IMP at the fine-tuning stage\\n     - Prune 20% of the smallest-magnitude weights from the non-zero regions of weights $(m_s \\\\odot m_t) \\\\odot \\\\theta_s$, by setting the values at corresponding positions to those weights in $m_t$ to \\\"0\\\".\\n     - (Re-)train the sparse weights $(m_s \\\\odot m_t) \\\\odot \\\\theta_t$ on $D_t$. Only $\\\\theta_t$ is updated.\\n   end for\\n7. Obtain the final sparse weights $(m_s \\\\odot m_t) \\\\odot \\\\theta^*$ and return $\\\\theta^*$, $m_s$ and $m_t$.\\n\\nA.2 Bi-RPT\\nWe now build the techniques to solve the bi-level optimization problem formulated in Bi-RPT.\\n\\n**Formulation**\\n\\n$$\\\\min_{\\\\theta, m_s, m_t} E(x_t, y_t) \\\\sim D_t [L_t((m_s \\\\odot m_t) \\\\odot \\\\theta^*, x_t, y_t)|_{\\\\theta^*, m^*_s}] + \\\\gamma R(m^*_s \\\\odot m_t)$$\\n\\ns.t. $\\\\{\\\\theta^*, m^*_s\\\\} = \\\\arg \\\\min_{\\\\theta, m_s} E(x_s, y_s) \\\\sim D_s L_s(m_s \\\\odot \\\\theta, x_s, y_s)$.\\n\\n**Lower-level problem**\\nWe solve the lower-level problem through a $p$-step SGD unrolling. Let $\\\\theta^{(k)}$ be the model weights, and $m^{(k)}_s$ be the mask for the pretraining stage. The superscript $(k)$ indicates they have been updated on the upper-level for $k$ steps. $\\\\theta^{(k)}$ and $m^{(k)}_s$ will be the starting points for the lower-level optimization problem. $\\\\theta^{(t)}_l$ and $m^{(t)}_{s,l}$ are the weights and mask, respectively, after being updated for $t$ steps on the lower-level optimization problem (implying $\\\\theta^{(0)}_l = \\\\theta^{(k)}$ and $m^{(0)}_{s,l} = m^{(k)}_s$). The update rules can be written as\\n\\n$$\\\\theta^{(0)}_l = \\\\theta^{(k)}, \\\\quad \\\\theta^{(p)}_l = \\\\theta^{(p-1)}_l - \\\\lambda_l \\\\nabla_\\\\theta L_s |_{\\\\theta = \\\\theta^{(p-1)}_l},$$\\n\\n$$m^{(0)}_{s,l} = m^{(k)}_s, \\\\quad m^{(p)}_{s,l} = m^{(p-1)}_{s,l} - \\\\lambda_{m,l} \\\\nabla_m L_s |_{m = m^{(p-1)}_{s,l}}.$$\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Upper-level problem and Sparse Regularization Loss\\n\\nThe upper-level problem is the sum of two losses: a normal training loss $L_t$ and a sparse regularization loss $R_\\\\gamma$ ($\\\\gamma$ is a coefficient).\\n\\nWe first develop update rules for the training loss $L_t$. The weights $\\\\theta^\\\\ast$ ($\\\\theta^\\\\ast_p l$) and masks $m^\\\\ast s$ ($m^\\\\ast_s l$) from the lower-level problem after $p$ unroll steps will serve as the initialization of the upper-level problem. We update the model weight $\\\\theta$ and masks at the upper level by applying gradient-based methods (take SGD as an example):\\n\\n$$\\\\theta(k+1) = \\\\theta^\\\\ast - \\\\lambda u d L_t$$\\n\\n$$\\\\theta^\\\\ast = \\\\theta^\\\\ast - \\\\lambda u (\\\\frac{\\\\partial L_t}{\\\\partial \\\\theta^\\\\ast} + \\\\frac{\\\\partial L_t}{\\\\partial m^\\\\ast_s} \\\\frac{\\\\partial m^\\\\ast_s}{\\\\partial \\\\theta^\\\\ast})$$\\n\\nwhere $\\\\lambda u$ is the learning rate for the weights for the upper-level optimization problem. The gradient on $m_t$ is easy enough: $\\\\frac{\\\\partial L_t}{\\\\partial m_t}$, while the gradient on $m_s$ is slightly complicated:\\n\\n$$d L_t d m^\\\\ast_s = \\\\frac{\\\\partial L_t}{\\\\partial m^\\\\ast_s} + \\\\frac{\\\\partial L_t}{\\\\partial \\\\theta^\\\\ast} \\\\frac{\\\\partial \\\\theta^\\\\ast}{\\\\partial m^\\\\ast_s} \\\\frac{\\\\partial m^\\\\ast_s}{\\\\partial \\\\theta^\\\\ast}$$.\\n\\nWe expand the latter terms in Eqn. 5 and Eqn. 6 based on the first-order approximation ($p = 1$) on the lower-level problem:\\n\\n$$\\\\frac{\\\\partial \\\\theta^\\\\ast}{\\\\partial m^\\\\ast_s} = \\\\frac{\\\\partial (\\\\theta^0_l - \\\\lambda l \\\\nabla_{\\\\theta^\\\\ast} L_s)}{\\\\partial (m^0_s,l - \\\\lambda m,l \\\\nabla_{m^\\\\ast_s} L_s)}$$\\n\\n$$\\\\frac{\\\\partial m^\\\\ast_s}{\\\\partial \\\\theta^\\\\ast} = \\\\frac{\\\\partial (m^0_s,l - \\\\lambda m,l \\\\nabla_{m^\\\\ast_s} L_s)}{\\\\partial (\\\\theta^0_l - \\\\lambda l \\\\nabla_{\\\\theta^\\\\ast} L_s)}$$\\n\\nFurther approximations can be made to avoid the matrix inverse and save computation:\\n\\n$$\\\\frac{\\\\partial \\\\theta^\\\\ast}{\\\\partial m^\\\\ast_s} \\\\approx -\\\\lambda l \\\\nabla_{m^\\\\ast_s} \\\\theta L_s$$,\\n\\n$$\\\\frac{\\\\partial m^\\\\ast_s}{\\\\partial \\\\theta^\\\\ast} \\\\approx -\\\\lambda m,l \\\\nabla_{m^\\\\ast_s} \\\\theta L_s$$.\\n\\nBased on the rules, $m_s$ and $m_t$ can be optimized by:\\n\\n$$\\\\hat{m}_t(k+1) = m_t(k) - \\\\lambda m \\\\frac{\\\\partial L_t}{\\\\partial m_t} |_{m_t = m_t(k)}$$\\n\\n$$\\\\hat{m}_s(k+1) = m_s(k) - \\\\lambda m \\\\frac{\\\\partial L_t}{\\\\partial m_s} + \\\\lambda m \\\\lambda l \\\\frac{\\\\partial L_t}{\\\\partial \\\\theta^\\\\ast} \\\\nabla_{m^\\\\ast_s} \\\\theta L_s |_{m^\\\\ast_s = m_s(k)}$$\\n\\nwhere the superscript $k$ means the steps updated.\\n\\nWe then focus on the latter term. We choose $\\\\ell_0$ loss (i.e. the number of non-zero elements) as the sparse regularizer $R$, which is not differentiable and difficult to optimize. Therefore, we follow Guo et al. (2021) to use the smoothed $\\\\ell_0$ formulation to facilitate differentiable training. Specifically, a gate function $g_\\\\epsilon(x) := x^2 / x^2 + \\\\epsilon$, where $\\\\epsilon$ is a small positive number, is used to replace the binary masks, which are instead parameterized by $g_\\\\epsilon(m_s)$ and $g_\\\\epsilon(m_t)$.\\n\\nWe decay the value of $\\\\epsilon$ every 15\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"epoch, and the gate function will gradually output only polarized numbers (i.e., $0$ and $1$). We further apply the proximal-SGD (Nitanda, 2014) to minimize the $\\\\ell_0$ loss: after we update the $m_s$ and $m_t$ with respect to $L_t$ by gradient descent-based methods (Eqn. 9), we use the proximal operator to alternatively update each mask. For $m_s$, the formulation can be written as:\\n\\n$$\\\\text{prox}_{\\\\lambda m_s}(m_s(k+1)) = \\\\arg \\\\min_{m_s} \\\\frac{1}{2} \\\\| m_s(k+1) \\\\odot \\\\hat{m}_s(k+1) - \\\\hat{m}_s(k+1) \\\\odot \\\\hat{m}_t(k+1) \\\\|^2 + \\\\lambda m_s \\\\| m_s(k+1) \\\\odot \\\\hat{m}_t(k+1) \\\\|^0.$$  \\n\\nWe follow (Guo et al., 2021) to solve it by relaxing it to the $\\\\ell_1$ norm problem, which has a closed form solution:\\n\\n$$m_{s,i} = \\\\begin{cases} \\\\hat{m}_{s,i} - \\\\gamma \\\\lambda m_{t,i} & \\\\hat{m}_{s,i} \\\\geq \\\\gamma \\\\lambda m_{t,i} \\\\\\\\ \\\\hat{m}_{s,i} + \\\\gamma \\\\lambda m_{t,i} & \\\\hat{m}_{s,i} \\\\leq -\\\\gamma \\\\lambda m_{t,i} \\\\\\\\ 0 & -\\\\gamma \\\\lambda m_{t,i} < \\\\hat{m}_{s,i} < \\\\gamma \\\\lambda m_{t,i} \\\\end{cases}$$\\n\\n(10)\\n\\nSimilarly, we derive the update for $m_t$:\\n\\n$$m_{t,i} = \\\\begin{cases} \\\\hat{m}_{t,i} - \\\\gamma \\\\lambda m_{s,i} & \\\\hat{m}_{t,i} \\\\geq \\\\gamma \\\\lambda m_{s,i} \\\\\\\\ \\\\hat{m}_{t,i} + \\\\gamma \\\\lambda m_{s,i} & \\\\hat{m}_{t,i} \\\\leq -\\\\gamma \\\\lambda m_{s,i} \\\\\\\\ 0 & -\\\\gamma \\\\lambda m_{s,i} < \\\\hat{m}_{t,i} < \\\\gamma \\\\lambda m_{s,i} \\\\end{cases}$$\\n\\n(11)\\n\\nFinally, we combine all these components into Algorithm 2.\\n\\n**Algorithm 2**\\n\\n**Solving Bi-RPT**\\n\\n**Input:**\\n- Initialization weights $\\\\theta_0$\\n- Training loss functions for two stages $L_s$ and $L_t$\\n- Low-quality pretraining dataset $D_s$\\n- High-quality fine-tuning dataset $D_t$\\n- Number of steps for gradient unroll $p$.\\n\\n**Output:**\\n- Trained model weights $\\\\theta$\\n- Sparse masks $m_s$ and $m_t$.\\n\\n**Train** $\\\\theta_0$ on $D_s$ to get weights $\\\\theta$.\\n\\n**while** not converged **do**\\n\\n1. Given the fixed $m_s$, update the weights $\\\\theta$ on $D_s$ by gradient unrolling (Eqn. 3).\\n2. Update the weights $\\\\theta$ by Eqn. 5.\\n3. Update the masks $m_s$ and $m_t$ by Eqn. 9.\\n4. Update the masks $m_s$ and $m_t$ by Eqn. 10 and Eqn. 11.\\n\\n**end while**\\n\\n**B.1 BASELINES AND HYPERPARAMETERS**\\n\\nWe list the hyper-parameters we used for all the baselines in this section.\\n\\n**General Settings.** When pre-training the models on $D_s$ (ImageNet and ImageNet-C), we use the SGD optimizer and a learning rate is $4 \\\\times 10^{-1}$. We linearly warm-up the learning rate within 5 epochs, and then decay it by 10 for every 30 epochs. Models are pretrained for 95 epochs on $D_s$, with a batch size of 1024. On $D_t$, i.e., CUB-200 and CUB-200 (10-shot), we set the initial learning rate as $1 \\\\times 10^{-3}$. The learning rate is decayed by 10 every 30 epochs, and the model is trained for 90 epochs with a batch size of 64.\\n\\nFor Hand-Tune, we train the models with 95 epochs from scratch on $D_s$ to get a densely pretrained models. The number of training epochs is reduced to 45 after the pretrained model is derived. After the pretraining stage ends, we continue to transfer the model on $D_t$ following the above hyper-parameters. The number of epochs is also reduced to 45 after we prune the weights.\\n\\nFor No-Pretraining, we train the model using an initial learning rate of $1 \\\\times 10^{-2}$ and a batch size of 64. For Mix-Training, as the number of classes is different for ImageNet and CUB-200, we use A16.\"}"}
{"id": "uYFRjvSJXbQ", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"two fully-connected layers on top the normal ResNet-18 backbone, and train them simultaneously. We sample batches from the two domains ($D_s$ and $D_t$) using the same batch size of 64. The initial learning rate for these methods are $1 \\\\times 10^{-2}$, and it is decayed by $10$ every 30 epochs.\\n\\nFor Bi-RPT, we follow the same learning rate settings despite some additional hyper-parameters are newly introduced. The learning rate for the lower-level problem ($\\\\lambda_l$) is $1 \\\\times 10^{-3}$, the same as the learning rate for upper-level problem ($\\\\lambda_u$). The value of $\\\\gamma$ is set to $1 \\\\times 10^{-4}$, which are determined through ablation studies in Table A10. The value of $\\\\lambda_m$ are set to 0.35, which are also determined through ablation studies in Table A11.\\n\\nB.2 PERFORMANCE OF HAND-TUNE UNDER DIFFERENT LEVELS OF SPARSITY\\n\\nWe report the performance of the Hand-Tune method under different levels of sparsity. We conduct experiments with $N_s =$ {0, 1, 2, 3, 4, 5} and $N_t =$ {0, 1, 2, 3, 4}, resulting sparsity levels at pre-training stage of {0.00%, 20.00%, 36.00%, 48.80%, 59.04%, 67.23%} and sparsity levels at transfer stage of {0.00%, 20.00%, 36.00%, 48.80%, 59.04%}. We conduct experiments over all the combinations of pretraining and transfer pruning rounds. More specifically, we first perform IMP on $D_s$ for $N_s$ rounds, and continue to perform IMP on $D_t$ for another $N_t$ rounds. The experiment results over various source and target combinations are shown in Table A5 to Table A8. Note that all the models are evaluated on the testing samples in $D_t$.\\n\\nFrom this series of tables we observe that: (1) sparsity at pretraining helps improve the model's performance on $D_t$ after fine-tuning, and the performance gain is larger when $D_s$ contains more noise and has larger domain shifts; (2) sparsity at transfer is also beneficial to the performance after fine-tuning, and the improvement is more significant when the $D_t$ is more \u201cdata-scarce\u201d; (3) the optimal sparse levels for the two stages vary for different combinations of pretrain and transfer domains, highlighting the importance of choosing the correct pruning rounds for both stages.\\n\\nTable A5: Test accuracy of fine-tuned ResNet-18 on CUB-200 after pretrained on ImageNet, under different levels of sparsity at pretraining and sparsity at transfer.\\n\\n| Sparsity At Transfer | Sparsity At Pretraining |\\n|----------------------|-------------------------|\\n| 0.00%                | 74.16%                  |\\n| 20.00%               | 74.15%                  |\\n| 36.00%               | 74.13%                  |\\n| 48.80%               | 73.84%                  |\\n| 59.04%               | 73.61%                  |\\n\\nTable A6: Test accuracy of fine-tuned ResNet-18 on CUB-200 after pretrained on ImageNet-C, under different levels of sparsity at pretraining and sparsity at transfer.\\n\\n| Sparsity At Transfer | Sparsity At Pretraining |\\n|----------------------|-------------------------|\\n| 0.00%                | 71.59%                  |\\n| 20.00%               | 71.83%                  |\\n| 36.00%               | 71.68%                  |\\n| 48.80%               | 71.13%                  |\\n| 59.04%               | 69.26%                  |\"}"}
