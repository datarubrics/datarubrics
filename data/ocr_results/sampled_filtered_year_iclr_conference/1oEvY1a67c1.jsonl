{"id": "1oEvY1a67c1", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"IF YOUR DATA DISTRIBUTION SHIFTS, USE SELF-LEARNING.\\n\\nAbstract\\n\\nWe demonstrate that self-learning techniques like entropy minimization and pseudo-labeling are simple and effective at improving performance of a deployed computer vision model under systematic domain shifts. We show consistent improvements irrespective of the model architecture, the pre-training technique or the type of distribution shift. At the same time, self-learning is simple to use in practice because it does not require knowledge or access to the original training data or scheme, is robust to hyperparameter choices, is straightforward to implement and requires only a few adaptation epochs. This makes self-learning techniques highly attractive for any practitioner who applies machine learning algorithms in the real world. We present state-of-the-art adaptation results on CIFAR10-C (8.5% error), ImageNet-C (22.0% mCE), ImageNet-R (17.4% error) and ImageNet-A (14.8% error), theoretically study the dynamics of self-supervised adaptation methods and propose a new classification dataset (ImageNet-D) which is challenging even with adaptation.\\n\\nIntroduction\\n\\nDeep Neural Networks (DNNs) can reach human-level performance in complex cognitive tasks (Brown et al., 2020; He et al., 2016a; Berner et al., 2019) if the distribution of the test data is sufficiently similar to the training data. However, DNNs are known to struggle if the distribution of the test data is shifted relatively to the training data (Geirhos et al., 2018; Dodge & Karam, 2017). Two largely distinct communities aim to increase the performance of models under test-time distribution shifts: The robustness community generally considers ImageNet-scale datasets and evaluates models in an ad-hoc scenario. Models are trained on a clean source dataset like ImageNet, using heavy data augmentation (Hendrycks et al., 2020a; Rusak et al., 2020; Geirhos et al., 2019) and/or large-scale pre-training (Xie et al., 2020a; Mahajan et al., 2018). The trained models are not adapted in any way to test-time distribution shifts. This evaluation scenario is relevant for applications in which very different distribution shifts are encountered in an unpredictable order, and hence misses out on the gains of adaptation to unlabeled samples of the target distribution.\\n\\nFigure 1: Robustness and adaptation to new datasets has traditionally been achieved by robust pre-training (with hand-selected/data-driven augmentation strategies, or additional data), unsupervised domain adaptation (with access to unlabeled samples from the test set), or, more recently, self-supervised learning methods. We show that on top of these different pre-training tasks, it is always possible (irrespective of architecture, model size or pre-training algorithm) to further adapt models to the target domain with simple self-learning techniques.\"}"}
{"id": "1oEvY1a67c1", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The unsupervised domain adaptation (UDA) community often considers smaller-scale datasets and assumes that both the source and the (unlabeled) target dataset are known. Models are trained on both datasets (e.g., with an adversarial domain objective, Ganin et al., 2016) before evaluation on the target domain data. This evaluation scenario provides optimal conditions for adaptation, but the reliance on the source dataset makes UDA more computationally expensive, more impractical and prevents the use of pre-trained models for which the source dataset is unknown or simply too large.\\n\\nIn this work, we consider the source-free domain adaptation setting, a middle ground between the classical ad-hoc robustness setting and UDA in which models can adapt to the target distribution but without using the source dataset (Kundu et al., 2020; Kim et al., 2021; Li et al., 2020; Liang et al., 2020). This evaluation scenario is interesting for many practitioners and applications as an extension of the ad-hoc robustness scenario. It evaluates the possible performance of a deployed model on a systematic, unseen distribution shift at inference time: an embedded computer vision system in an autonomous car should adapt to changes without being trained on all available training data; an image-based quality control software may not necessarily open-source the images it has been trained on, but still has to be adapted to the lighting conditions at the operation location; a computer vision system in a hospital should perform robustly when tested on a scanner different from the training images\u2014importantly, it might not be known at development time which scanner it will be tested on, and it might be prohibited to share images from many hospitals to run UDA.\\n\\nCan self-learning methods like pseudo-labeling and entropy-minimization also be used in this source-free domain adaptation setting? To answer this question, we perform an extensive study of several self-learning variants, and find consistent and substantial gains in test-time performance across several robustness and out-of-domain benchmarks and a wide range of models and pre-training methods, including models trained with UDA methods that do not use self-learning. We also find that self-learning outperforms state-of-the-art source-free domain adaptation methods, namely Test-Time Training which is based on a self-supervised auxiliary objective and continual training (Sun et al., 2019b), test-time entropy minimization (Wang et al., 2020) and (gradient-free) BatchNorm adaptation (Schneider et al., 2020; Nado et al., 2020). We perform a large number of ablations to study important design choices for self-learning methods in source-free domain adaptation. Furthermore, we show that a variant of pseudo-labeling with a robust loss function consistently outperforms entropy minimization on ImageNet-scale datasets. We theoretically analyze and empirically verify the influence of the temperature parameter in self-learning and provide guidelines how this single parameter should be chosen. Our approach is visualized in Figure 1. We do not consider test-time adaptation in an online setting like is studied e.g., by Zhang et al. (2021), where the model is adapted to one example at a time, and reset after each example.\\n\\nRelated Work. Variants of self-learning have been used for UDA (Berthelot et al., 2021), for example using auxiliary information (Xie et al., 2020b), consistency (Wei et al., 2020; Cai et al., 2021; Prabhu et al., 2021) or confidence (Zou et al., 2019) regularization. The main difference from these works to ours is that they 1) utilize both source and target data for self-learning whereas we only require access to unlabeled target data, 2) train their models from scratch whereas we merely fine-tune pretrained checkpoints on the unlabeled target data, and 3) are generally more complicated than our approach due to using more than one term in the objective function.\\n\\nOur work is conceptually most similar to virtual adversarial domain adaptation in the fine-tuning phase of DIRT-T (Shu et al., 2018)) and Test-time entropy minimization (TENT; Wang et al., 2020). In contrast to DIRT-T, our objective is simpler and we scale the approach to considerably larger datasets on ImageNet scale. TENT, on the other hand, only evaluated a single method (entropy minimization) on a single vanilla model (ResNet-50) on IN-C. We substantially expand this analysis to show that self-learning almost universally increases test-time performance under distribution shifts, regardless of the type of distribution shift, the model architecture or the pre-training method. Self-learning has also been applied to UDA for semantic segmentation (Zou et al., 2018), for gradual domain adaptation (Kumar et al., 2020), for semi-supervised learning (Rizve et al., 2021; Mukherjee & Awadallah, 2020), for learning in biased datasets (Chen et al., 2020b) and for automated data annotation (De Sousa Ribeiro et al., 2020). Zoph et al. (2020) show that self-learning outperforms pretraining when stronger data augmentation is used and more labeled data is present. A more detailed discussion of related work alongside with the main differences to our work can be found in Appendix F. Our main contribution beyond these works is to show the effectiveness of self-learning on top of both robust, large scale, and domain adapted models, at scale.\"}"}
{"id": "1oEvY1a67c1", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Different variants of self-learning have been used in both unsupervised domain adaptation (French et al., 2018; Shu et al., 2018), self-supervised representation learning (Caron et al., 2021), and in semi-supervised learning (Xie et al., 2020a). In a typical self-learning setting a teacher network $f_{t}$ trained on the source domain predicts labels on the target domain. Then, a student model $f_{s}$ is fine-tuned on the predicted labels.\\n\\nIn the following, let $f_{t}(x)$ denote the logits for sample $x$ and let $p_{t}(j|x) \\\\equiv \\\\sigma_{j}(f_{t}(x))$ denote the probability for class $j$ obtained from a softmax function $\\\\sigma_{j}(\\\\cdot)$. Similarly, $f_{s}(x)$ and $p_{s}(j|x)$ denote the logits and probabilities for the student model $f_{s}$. For all techniques, one can optionally only admit samples where the probability $\\\\max_{j} p_{t}(j|x)$ exceeds some threshold. We consider three popular variants of self-learning: Pseudo-labeling with hard or soft labels, as well as entropy minimization.\\n\\n**Hard Pseudo-Labeling (Lee, 2013; Galstyan & Cohen, 2007).** We generate labels using the teacher and train the student on pseudo-labels $i$ using the standard cross-entropy loss,\\n\\n$$\\\\ell_{H}(x) := -\\\\log p_{s}(i|x),$$\\n\\nwhere $i = \\\\arg\\\\max_{j} p_{t}(j|x)$ (1)\\n\\nUsually, only samples with a confidence above a certain threshold are considered for training the student. We test several thresholds but note that thresholding means discarding a potentially large portion of the data which leads to a performance decrease in itself. The teacher is updated after each epoch.\\n\\n**Soft Pseudo-Labeling (Lee, 2013; Galstyan & Cohen, 2007).** In contrast to the hard pseudo-labeling variant, we here train the student on class probabilities predicted by the teacher,\\n\\n$$\\\\ell_{S}(x) := -\\\\sum_{j} p_{t}(j|x) \\\\log p_{s}(j|x).$$\\n\\n(2)\\n\\nSoft pseudo-labeling is typically not used in conjunction with thresholding, since it already incorporates the certainty of the model. The teacher is updated after each epoch.\\n\\n**Entropy Minimization (ENT; Grandvalet & Bengio, 2004).** This variant is similar to soft pseudo-labeling, but we no longer differentiate between a teacher and student network. It corresponds to an \\\"instantaneous\\\" update of the teacher. The training objective becomes\\n\\n$$\\\\ell_{E}(x) := -\\\\sum_{j} p_{s}(j|x) \\\\log p_{s}(j|x).$$\\n\\n(3)\\n\\nIntuitively, self-training with entropy minimization leads to a sharpening of the output distribution for each sample, making the model more confident in its predictions.\\n\\n**Robust Pseudo-Labeling (RPL).** Virtually all introduced self-training variants use the standard cross-entropy classification objective. However, the standard cross-entropy loss has been shown to be sensitive to label noise (Zhang & Sabuncu, 2018; Zhang et al., 2017). In the setting of domain adaptation, inaccuracies in the teacher predictions and, thus, the labels for the student, are inescapable, with severe repercussions for training stability and hyperparameter sensitivity as we show in the results.\\n\\nAs a straightforward solution to this problem, we propose to replace the cross-entropy loss by a robust classification loss designed to withstand certain amounts of label noise (Ghosh et al., 2017; Song et al., 2020; Shu et al., 2020; Zhang & Sabuncu, 2018). A popular candidate is the Generalized Cross Entropy (GCE) loss which combines the noise-tolerant Mean Absolute Error (MAE) loss (Ghosh et al., 2017) with the CE loss. We only consider the hard labels and use the robust GCE loss as the training loss for the student,\\n\\n$$i = \\\\arg\\\\max_{j} p_{t}(j|x), \\\\quad \\\\ell_{GCE}(x,i) := \\\\frac{q}{q-1}(1-p_{s}(i|x)^{q})$$\\n\\n(4)\\n\\nwith $q \\\\in (0,1]$. For the limit case $q \\\\to 0$, the GCE loss approaches the CE loss and for $q=1$, the GCE loss is the MAE loss (Zhang & Sabuncu, 2018). We test updating the teacher both after every update step of the student (RPL) and once per epoch (RPLep).\"}"}
{"id": "1oEvY1a67c1", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"EXPERIMENT DESIGN\\n\\nDatasets.\\nIN-C (Hendrycks & Dietterich, 2019) contains corrupted versions of the 50,000 images in the IN validation set. There are fifteen test and four hold-out corruptions, and there are five severity levels for each corruption. The established metric to report model performance on IN-C is the mean Corruption Error (mCE) where the error is normalized by the AlexNet error, and averaged over all corruptions and severity levels, see Eq. 20, Appendix C.1. IN-R (Hendrycks et al., 2020a) contains 30,000 images with artistic renditions of 200 classes of the IN dataset. IN-A (an, 2019) is composed of 7,500 unmodified real-world images on which standard IN-trained ResNet50 (He et al., 2016b) models yield chance level performance. CIFAR10 (Krizhevsky et al., 2009) and STL10 (Coates et al., 2011) are small-scale image recognition datasets with 10 classes each, and training sets of 50,000/5,000 images and test sets of 10,000/8,000 images, respectively. The digit datasets MNIST (Deng, 2012) and MNIST-M (Ganin et al., 2016) both have 60,000 training and 10,000 test images.\\n\\nHyperparameters.\\nThe different self-learning variants have a range of hyperparameters such as the learning rate or the stopping criterion. Our goal is to give a realistic estimation on the performance to be expected in practice. To this end, we optimize hyperparameters for each variant of pseudo-labeling on a hold-out set of IN-C that contains four types of image corruptions (\\\"speckle noise\\\", \\\"Gaussian blur\\\", \\\"saturate\\\" and \\\"spatter\\\") with five different strengths each, following the procedure suggested in Hendrycks & Dietterich (2019). We refer to the hold-out set of IN-C as our dev set.\\n\\nModels for ImageNet-scale datasets.\\nWe consider four popular model architectures: ResNet50 (He et al., 2016b), DenseNet161 (Huang et al., 2017), ResNeXt101 (Xie et al., 2017) and EfficientNet-L2 (Tan & Le, 2019) (see Appendix B.1 for details on the used models). For ResNet50, DenseNet and ResNeXt101, we include a simple vanilla version trained on IN only. For ResNet50 and ResNeXt101, we additionally include a state-of-the-art robust version trained with DeepAugment and Augmix (DAug+AM, Hendrycks et al., 2020a)1. For the ResNeXt model, we also include a version that was trained on 3.5 billion weakly labeled images (IG-3.5B, Mahajan et al., 2018). Finally, for EfficientNet-L2 we select the current state of the art on IN-C which was trained on 300 million images from JFT-300M (Chollet, 2017; Hinton et al., 2014) using a noisy student-teacher protocol (Xie et al., 2020a). We validate the IN and IN-C performance of all considered models and match the originally reported scores (Schneider et al., 2020). For EfficientNet-L2, we match IN top-1 accuracy up to 0.1% points, and IN-C up to 0.6% mCE.\\n\\nModels for CIFAR10/MNIST-scale datasets.\\nFor CIFAR10-C experiments, we use two WideResNets (WRN, Zagoruyko & Komodakis, 2016): the first one is trained on CIFAR10 and has a depth of 28 and a width of 10 and the second one is trained with AugMix (Hendrycks et al., 2020b) and has a depth of 40 and a width of 2. The remaining small-scale models are trained with unsupervised domain adaptation (UDA) methods. We propose to regard any UDA method which requires joint training with source and target data as a pre-training step, similar to regular pre-training on IN, and use self-learning on top of the final checkpoint. We consider two popular UDA methods: self-supervised domain adaptation (UDA-SS; Sun et al., 2019a) and Domain-Adversarial Training of Neural Networks (DANN; Ganin et al., 2016). In UDA-SS, the authors seek to align the representations of both domains by performing an auxiliary self-supervised task on both domains simultaneously. In all UDA-SS experiments, we use a WideResNet with a depth of 26 and a width of 16. In DANN, the authors learn a domain-invariant embedding by optimizing a minimax objective. For all DANN experiments except for MNIST \u2192 MNIST-M, we use the same WRN architecture as above. For the MNIST \u2192 MNIST-M experiment, the training with the larger model diverged and we used a smaller WideResNet version with a width of 2. We note that DANN training involves optimizing a minimax objective and is generally harder to tune.\\n\\nRESULTS: SELF-LEARNING UNIVERSALLY IMPROVES MODELS\\n\\nSelf-learning is a powerful learning scheme, and in the following section we show that it allows to perform test-time adaptation on robustified models, models obtained with large-scale pre-training, as well as domain adapted models across a wide range of datasets and distribution shifts. Our main results on large-scale and small-scale datasets are shown in Tables 1 and 2, respectively. These\"}"}
{"id": "1oEvY1a67c1", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022 and empirically found an upper bound for $\\\\tau$ above which the training was no longer stable.\\n\\nTo better understand such behavior, we study the learning dynamics of the loss function in equation 5 theoretically in a simple two-datapoints, two-classes model with linear student and teacher networks $f_s(x) = x^\\\\top w_s$ and $f_t(x) = x^\\\\top w_t$ defined in Appendix A.1. Gradient descent with stop gradient corresponds to hard pseudo-labeling in the limit $\\\\tau_t \\\\to 0$ and to soft pseudo-labeling when $\\\\tau_s = \\\\tau_t = 1$. Gradient descent without stop gradient, i.e., setting $w_s = w_t = w$ corresponds to entropy minimization. We obtain the following result:\\n\\n$$-\\\\frac{2}{\\\\log_{10} \\\\tau_s}$$\\n\\nFigure 2: For the two point model, we show error and for the CIFAR10-C simulation, we show improvement (yellow) vs. degradation (purple) over the non-adapted baseline (BAS). An important convergence criterion for pseudo-labeling (top row) and entropy minimization (bottom row) is the ratio of student and teacher temperatures; it lies at $\\\\tau_s = \\\\tau_t$ for PL, and $\\\\tau_t = 2\\\\tau_s$ for ENT. Despite the simplicity of the two-point model, the general convergence regions transfer to CIFAR10-C.\\n\\nProposition 1 ( Collapse in the two-point model).\\n\\nThe student and teacher networks $w_s$ and $w_t$ trained with stop gradient does not collapse to the trivial representation $\\\\forall x: x^\\\\top w_s = 0, x^\\\\top w_t = 0$ if $\\\\tau_s > \\\\tau_t$. The network $w$ trained without stop gradient does not collapse if $\\\\tau_s > \\\\tau_t / 2$. Proof. see \u00a7 A.2.\\n\\nWe validate the proposition on a simulated two datapoint toy dataset, as well as on the CIFAR-C dataset and outline the results in Figure 2. In general, the size and location of the region where collapse is observed in the simulated model also depends on the initial conditions, the learning rate and the optimization procedure. An in depth discussion, as well as additional simulations are given in the Appendix. In practice, the result suggests that student temperatures should exceed the teacher temperatures for pseudo-labeling, and student temperatures should exceed half the teacher temperature for entropy minimization. Entropy minimization with standard temperatures ($\\\\tau_s = \\\\tau_t = 1$) and hard pseudo-labeling ($\\\\tau_t \\\\to 0$) are hence stable. The two-point learning dynamics vanish for soft pseudo-labeling with $\\\\tau_s = \\\\tau_t$, suggesting that one would have to analyze a more complex model with more data points. While this does not directly imply that the learning is unstable at this point, we empirically observe that both entropy minimization and hard labeling outperform soft-labeling in practice.\\n\\n8 CONCLUSION\\n\\nWe evaluated and analysed how self-learning, an essential component in many unsupervised domain adaptation and self-supervised pre-training techniques, can be applied for adaptation to both small and large-scale image recognition problems common in robustness research. We demonstrated new state-of-the-art adaptation results with the EfficientNet-L2 model on the benchmarks ImageNet-C, -R, and -A, and introduced a new benchmark dataset (ImageNet-D) which remains challenging even after adaptation. Our theoretical analysis shows the influence of the temperature parameter in the self-learning loss function on the training stability and provides guidelines how to choose a suitable value. Self-learning universally improves test-time performance under diverse, but systematic distribution shifts irrespective of the architecture or pre-training method. We hope that our work encourages both researchers and practitioners to use self-learning if their data distribution shifts.\\n\\nReproducibility Statement\\n\\nWe attempted to make our work as reproducible as possible: We mostly used pre-trained models which are publicly available and we denoted the URL addresses of all used checkpoints; for the checkpoints that were necessary to retrain, we report the Github directories with the source code and used an official or verified reference implementation when available. We report all used hyperparameters in the Appendix and will release our code upon acceptance of the paper.\"}"}
{"id": "1oEvY1a67c1", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Implementation (OSDI 16), pp. 265\u2013283, 2016.\\n\\nDan Hendrycks an. Natural adversarial examples. ArXiv preprint, abs/1907.07174, 2019. URL https://arxiv.org/abs/1907.07174.\\n\\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. ArXiv preprint, abs/1912.06680, 2019. URL https://arxiv.org/abs/1912.06680.\\n\\nDavid Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain adaptation, 2021.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\\n\\nTianle Cai, Ruiqi Gao, Jason D Lee, and Qi Lei. A theory of label propagation for subpopulation shift. ArXiv preprint arXiv:2102.11203, 2021.\\n\\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. ArXiv preprint, abs/2104.14294, 2021. URL https://arxiv.org/abs/2104.14294.\\n\\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E. Hinton. Big self-supervised models are strong semi-supervised learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020a. URL https://proceedings.neurips.cc/paper/2020/hash/fcbc95ccdd551da181207c0c1400c655-Abstract.html.\\n\\nYining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. In NeurIPS, 2020b.\\n\\nFran\u00e7ois Chollet. Xception: Deep learning with depthwise separable convolutions. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 1800\u20131807. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.195. URL https://doi.org/10.1109/CVPR.2017.195.\\n\\nAdam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, 2011.\\n\\nFrancesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. ArXiv preprint, abs/2010.09670, 2020. URL https://arxiv.org/abs/2010.09670.\\n\\n10\"}"}
{"id": "1oEvY1a67c1", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nFabio De Sousa Ribeiro, Francesco Caliva, Mark Swainson, Kjartan Gudmundsson, Georgios Leontidis, and Stefanos Kollias. Deep bayesian self-training. Neural Computing and Applications, 32(9):4275\u20134291, 2020.\\n\\nLi Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141\u2013142, 2012.\\n\\nSamuel F. Dodge and Lina J. Karam. A study and comparison of human and deep learning recognition performance under visual distortions. In International Conference on Computer Communications and Networks, ICCCN 2017, 2017.\\n\\nGeoffrey French, Michal Mackiewicz, and Mark H. Fisher. Self-ensembling for visual domain adaptation. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=rkpoTaxA-\\n\\nAram Galstyan and Paul R. Cohen. Empirical comparison of hard and soft label propagation for relational classification. In 17th international conference on Inductive logic programming, 2007.\\n\\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096\u20132030, 2016.\\n\\nRobert Geirhos, Carlos R. Medina Temme, Jonas Rauber, Heiko H. Sch\u00fctz, Matthias Bethge, and Felix A. Wichmann. Generalisation in humans and deep neural networks. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pp. 7549\u20137561, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/0937fb5864ed06ffb59ae5f9b5ed67a9-Abstract.html.\\n\\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bygh9j09KX.\\n\\nAritra Ghosh, Himanshu Kumar, and P. S. Sastry. Robust loss functions under label noise for deep neural networks. In Satinder P. Singh and Shaul Markovitch (eds.), Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, pp. 1919\u20131925. AAAI Press, 2017. URL http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14759.\\n\\nYves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, NIPS 2004, December 13-18, 2004, Vancouver, British Columbia, Canada], pp. 529\u2013536, 2004. URL https://proceedings.neurips.cc/paper/2004/hash/96f2b50b5d3613adf9c27049b2a888c7-Abstract.html.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770\u2013778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770\u2013778. IEEE Computer Society, 2016b. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.\"}"}
{"id": "1oEvY1a67c1", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "1oEvY1a67c1", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use different open source software packages for our experiments, most notably Docker (Merkel, 2014), scipy and numpy (Virtanen et al., 2020), GNU parallel (Tange, 2011), Tensorflow (Abadi et al., 2016), PyTorch (Paszke et al., 2017), timm (Wightman, 2019), Self-ensembling for visual domain adaptation (French et al., 2018), the WILDS benchmark (Koh et al., 2021), and torchvision (Marcel & Rodriguez, 2010).\"}"}
{"id": "1oEvY1a67c1", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022 using learning rates $\\\\{5.0 \\\\times 10^{-6}, 5.0 \\\\times 10^{-5}, \\\\ldots, 5.0 \\\\times 10^{-2}\\\\}$. The best hyperparameter is selected according to OOD Validation accuracy.\\n\\nWhile we see improvements on Camelyon17, neither BN adaptation nor self-learning can improve performance on RxRx1 or FMoW. Initial experiments on PovertyMap and iWildsCam also do not show improvements with self-learning. We hypothesize that the reason lies in the mixing of the domains: Both BN adaptation and our self-learning methods work best on systematic domain shifts. These results support our claim that self-learning is effective, while showing the important limitation when applied to more diverse shifts.\\n\\nE.2 SMALL IMPROVEMENTS ON BIG TRANSFER MODELS WITH GROUP NORMALIZATION LAYERS\\n\\nWe evaluated BigTransfer models (Kolesnikov et al., 2020) provided by the timm library (Wightman, 2019). A difference to the ResNet50, ResNeXt101 and EfficientNet models is the use of group normalization layers, which might influence the optimal method for adaptation\u2014for this evaluation, we followed our typical protocol as performed on ResNet50 models, and used affine adaptation. For affine adaptation, a distilled BigTransfer ResNet50 model improves from 49.6 % to 48.4 % mCE on the ImageNet-C development set, and from 55.0 % to 54.4 % mCE on the ImageNet-C test set when using RPL ($q = 0.8$) for adaptation, at learning rate $7.5 \\\\times 10^{-4}$ at batch size 96 after a single adaptation epoch. Entropy minimization did not further improve results on the ImageNet-C test set. An ablation over learning rates and epochs on the dev set is shown in Table 35, the final results are summarized in Table 36.\\n\\nTable 35: mCE in % on the IN-C dev set for ENT and RPL for different numbers of training epochs when adapting the affine batch norm parameters of a ResNet50 model.\\n\\n| criterion | ENT | RPL |\\n|-----------|-----|-----|\\n| lr         |     |     |\\n| $7.5 \\\\times 10^{-5}$ | 49.63 | 49.63 |\\n| $10^{-4}$   | 49.26 | 48.89 |\\n| $10^{-3}$   | 49.08 | 51.45 |\\n| $10^{-2}$   | 48.91 | 51.60 |\\n| $10^{-1}$   | 48.80 | 52.03 |\\n\\nTable 36: mCE in % on the IN-C dev set for ENT and RPL for different numbers of training epochs when adapting the affine batch norm parameters of a ResNet50 model.\\n\\n| criterion | ENT | RPL |\\n|-----------|-----|-----|\\n| dev        |     |     |\\n| mCE        | 49.63 | 55.03 |\\n| test       | 48.80 | 56.36 |\\n| mCE        | 48.35 | 54.41 |\\n\\nE.3 CAN SELF-LEARNING IMPROVE OVER SELF-LEARNING BASED UDA?\\n\\nAn interesting question is whether test-time adaptation with self-learning can improve upon self-learning based UDA methods. To investigate this question, we build upon French et al. (2018) and their released code base at github.com/Britefury/self-ensemble-visual-domain-adapt. We trained the Baseline models from scratch using the provided shell scripts with the default hyperparameters and verified the reported performance. For adaptation, we tested BN adaptation, ENT, RPL, as well as continuing to train in exactly the setup of French et al. (2018), but without the supervised loss. For the different losses, we adapt the models for a maximum of 10 epochs using learning rates $\\\\{1 \\\\times 10^{-5}, 1 \\\\times 10^{-4}, \\\\ldots, 1 \\\\times 10^{-1}\\\\}$.\\n\\nNote that for this experiment, in contrast to any other result in this paper, we purposefully do not perform proper hyperparameter selection based on a validation dataset\u2014instead we report the best accuracy across all tested epochs and learning rates to give an upper bound on the achievable performance for test-time adaptation. As highlighted in Table 37, none of the four tested variants is able to meaningfully improve over the baseline, corroborating our initial hypothesis that self-learning within a full UDA setting is the optimal strategy, if dataset size and compute permits. On the other hand, results like the teacher...\"}"}
{"id": "1oEvY1a67c1", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Refinement step in DIRT-T (Shu et al., 2018) show that with additional modifications in the loss function, it might be possible to improve over standard UDA with additional adaptation at test time.\\n\\nTable 37: Test-time adaptation marginally improves over self-ensembling.\\n\\n| Baseline BN adapt | ENT | RPL |\\n|-------------------|-----|-----|\\n| Self-ensembling loss | | |\\n\\n| | MNIST | SVHN | USPS | MNIST | SVHN |\\n|-------------------|-------|-------|-------|-------|-------|\\n| MT+TF | 33.88 | 34.44 | 34.87 | 35.09 | 33.27 |\\n| MT+CT* | 32.62 | 34.11 | 34.25 | 34.21 | 33.36 |\\n| MT+CT+TF | 41.59 | 41.93 | 41.95 | 41.95 | 42.70 |\\n| MT+CT+TFA | 30.55 | 32.53 | 32.54 | 32.55 | 30.84 |\\n| SVHN-specific aug. | 97.05 | 96.82 | 96.91 | 96.87 | 97.12 |\\n\\n| | MNIST | SVHN | USPS | MNIST | SVHN |\\n|-------------------|-------|-------|-------|-------|-------|\\n| MT+TF | 98.01 | 97.91 | 97.96 | 97.91 | 98.16 |\\n| MT+CT* | 88.34 | 88.39 | 88.54 | 88.39 | 88.44 |\\n| MT+CT+TF | 98.36 | 98.41 | 98.41 | 98.41 | 98.50 |\\n| MT+CT+TFA | 98.45 | 98.45 | 98.45 | 98.45 | 98.61 |\\n| SVHN-specific aug. | 99.52 | 99.49 | 99.5 | 99.49 | 99.65 |\\n\\n| | MNIST | SVHN | USPS | MNIST | SVHN |\\n|-------------------|-------|-------|-------|-------|-------|\\n| MT+TF | 92.79 | 92.62 | 92.62 | 92.66 | 93.08 |\\n| MT+CT* | 99.11 | 99.13 | 99.14 | 99.13 | 99.21 |\\n| MT+CT+TF | 99.41 | 99.42 | 99.45 | 99.42 | 99.52 |\\n| MT+CT+TFA | 99.48 | 99.54 | 99.57 | 99.54 | 99.54 |\"}"}
{"id": "1oEvY1a67c1", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Self-learning for domain adaptation\\n\\nXie et al. (2020b) introduce \u201cIn-N-Out\u201d which uses auxiliary information to boost both in- and out-of-distribution performance. AdaMatch (Berthelot et al., 2021) builds upon FixMatch (Sohn et al., 2020) and can be used for the tasks of unsupervised domain adaptation, semi-supervised learning and semi-supervised domain adaptation as a general-purpose algorithm. Prabhu et al. (2021) propose SENTRY, an algorithm based on judging the predictive consistency of samples from the target domain under different image transformations. Zou et al. (2019) show that different types of confidence regularization can improve the performance of self-learning. A theoretically motivated framework for self-learning in domain adaptation based on consistency regularization has been proposed by Wei et al. (2020) and then extended by Cai et al. (2021). Self-learning has also been used for semantic segmentation (Zou et al., 2018).\\n\\nThe main difference from these works to ours is that they 1) utilize both source and target data during training (i.e., the classical UDA setup) whereas we only require access to unlabeled target data (source-free setup), and 2) train their models from scratch whereas we adapt pretrained checkpoints to the unlabeled target data, 3) are oftentimes more complicated (also in terms of the number of hyperparameters) than our approach due to using more than one term in the objective function.\\n\\nWe would like to highlight that utilizing source data should always result in better performance compared to not using source data. Our contribution is to show that self-learning can still be very beneficial with a small compute budget and no access to source data. Our setup targets \u201cdeployed systems\u201d, e.g., a self-driving car or a detection algorithm in a production line which adapts to the distribution shift \u201con-the-fly\u201d and cannot (or should not) be retrained from scratch for every new domain shift.\\n\\nKumar et al. (2020) study the setting of self-learning for gradual domain adaptation. They find that self-learning works better if the data distribution changes slowly. The gradual domain adaptation setting differs from ours; instead of a gradual shift over time, we focus on a fixed, systematic shift at test time dataset. Kumar et al. (2020) tested their method on a synthetic Gaussian dataset, MNIST and the Portraits datasets; building and evaluating ImageNet-scale datasets for a gradual domain adaptation perspective is a very interesting extension of our work, but left for future work, and would not only require changes/adaptations to the self-learning method, but also to the evaluation datasets.\\n\\nChen et al. (2020b) prove that under certain conditions, self-learning can improve performance in biased datasets where spurious features correlate with the label in the source domain but are independent of the label in the target domain. While Chen et al. (2020b) also consider the setting of source-free domain adaptation (like we do), they limit their experiments to small scale models on MNIST and Celeb-A, while we conduct a large scale empirical study on all common robustness datasets with large scale models \u2013 some of the observed and studied effects in our paper (effectiveness of different loss functions at different problem scales, adaptation mechanisms, etc.) can be attributed to this large scale evaluation setting, and extending our insights over small scale experiments.\\n\\nSimilar to us, Chen et al. (2020b) find that a strong source classifier is necessary for self-learning to work; however, in their case, a teacher accuracy of 72% (on CMNIST10) is already too low and leads to worse student accuracy. In contrast, in our experiments, self-learning still works for an mCE as high as 80% (cf. appendix Figure 3, severity 5) and teacher accuracies as low as 10.4% (on ImageNet-D \u201cInfograph\u201d), and breaks down at accuracies around 1-2% (on ImageNet-D \u201cQuickdraw\u201d). This discrepancy might be due to the spurious correlations that Chen et al. (2020b) introduced in their dataset leading to systematic biases, which are not present in the datasets we studied.\\n\\nSelf-learning in semi-supervised learning (SSL)\\n\\nIn a different line of work which is not related to domain adaptation directly, self-learning has been used in a semi-supervised setting. Zoph et al. (2020) show that self-learning outperforms pretraining when stronger data augmentation is used and more labeled data is present. They use human labels on the target task (e.g., object detection on COCO) and pseudo-labels on an unlabeled dataset (e.g. ImageNet), and optimize the loss on both datasets, with the aim to improve performance on the task where ground truth labels are known. The work of Zoph et al. (2020) is orthogonal to ours, in the sense that we could adapt their final...\"}"}
{"id": "1oEvY1a67c1", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"checkpoint to a new domain with our method, similar to how we adapted the Noisy Student model which was also trained using self-learning.\\n\\nRizve et al. (2021) propose an uncertainty-aware pseudo-label selection (UPS) framework which outperforms other SSL methods in a few-label regime. UPS is helpful to reduce the impact of noisy pseudo-labels; in our case, we use the generalized cross-entropy loss for this purpose. Testing the UPS framework (and other means for improving the quality of pseudo-labels, or robustness against label noise) on robustness datasets would be an interesting direction for future work.\\n\\nDe Sousa Ribeiro et al. (2020) propose Deep Bayesian Self-Training (DBST) for automatic data annotation. Mukherjee & Awadallah (2020) suggest using self-learning in a semi-supervised setting for text classification with few labels.\"}"}
{"id": "1oEvY1a67c1", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.2 FULL LIST OF USED MODELS\\n\\nImageNet scale models\\nImageNet trained models (ResNet50, DenseNet161, ResNeXt) are taken directly from torchvision (Marcel & Rodriguez, 2010). The model variants trained with DeepAugment and AugMix augmentations (Hendrycks et al., 2020b;a) are taken from https://github.com/hendrycks/imagenet-r. The weakly-supervised ResNeXt101 model is taken from the PyTorch Hub. For EfficientNet (Tan & Le, 2019), we use the PyTorch re-implementation available at https://github.com/rwightman/gen-efficientnet-pytorch. This is a verified re-implementation of the original work by Xie et al. (2020a). We verify the performance on ImageNet, yielding a 88.23% top-1 accuracy and 98.546% top-5 accuracy which is within 0.2% points of the originally reported result (Xie et al., 2020a). On ImageNet-C, our reproduced baseline achieves 28.9% mCE vs. 28.3% mCE originally reported by Xie et al. (2020a). As noted in the re-implementation, this offset is possible due to minor differences in the pre-processing. It is possible that our adaptation results would improve further when applied on the original codebase by Xie et al.\\n\\nSmall scale models\\nWe train the UDA-SS models using the original code base at github.com/yueatsprograms/uda release, with the hyperparameters given in the provided bash scripts. For our DANN experiments, we use the PyTorch implementation at github.com/fungtion/DANN py3. We use the hyperparameters in the provided bash scripts.\\n\\nThe following Table 11 contains all models we evaluated on various datasets with references and links to the corresponding source code.\\n\\n| Model Source | Source Link |\\n|--------------|-------------|\\n| WideResNet(28,10) (Croce et al., 2020) | https://github.com/RobustBench/robustbench/tree/master/robustbench |\\n| WideResNet(40,2)+AugMix (Croce et al., 2020) | https://github.com/RobustBench/robustbench/tree/master/robustbench |\\n| ResNet50 (He et al., 2016b) | https://github.com/pytorch/vision/tree/master/torchvision/models |\\n| ResNeXt101, 32\u00d78d (He et al., 2016b) | https://github.com/pytorch/vision/tree/master/torchvision/models |\\n| DenseNet (Huang et al., 2017) | https://github.com/pytorch/vision/tree/master/torchvision/models |\\n| ResNeXt101, 32\u00d78d (Xie et al., 2017) | https://pytorch.org/hub/facebookresearchWSL-Imagesresnext/ |\\n| ResNet50+DeepAugment+AugMix (Hendrycks et al., 2020a) | https://github.com/hendrycks/imagenet-r |\\n| ResNext101 (Hendrycks et al., 2020a) | https://github.com/hendrycks/imagenet-r |\\n| ResNext101 32\u00d78d IG-3.5B (Mahajan et al., 2018) | https://github.com/facebookresearch/WSL-Images/blob/master/hubconf.py |\\n| Noisy Student EfficientNet-L2 (Xie et al., 2020a) | https://github.com/rwightman/gen-efficientnet-pytorch |\\n| ViT-S/16 (Caron et al., 2021) | https://github.com/facebookresearch/dino |\\n\\n21\"}"}
{"id": "1oEvY1a67c1", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The established performance metric on IN-C is the mean Corruption Error (mCE), which is obtained by normalizing the model\u2019s top-1 errors with the top-1 errors of AlexNet across the $C=15$ test corruptions and $S=5$ severities:\\n\\n$$mCE(model) = \\\\frac{1}{C} \\\\sum_{c=1}^{C} \\\\sum_{s=1}^{S} \\\\frac{err_{model}(c,s)}{err_{AlexNet}(c,s)}.$$ (20)\\n\\nThe AlexNet errors used for normalization are shown in Table 12.\\n\\n| Category | Corruption | top1 error |\\n|----------|------------|------------|\\n| Noise    | Gaussian Noise | 0.886428 |\\n|          | Shot Noise   | 0.894468   |\\n|          | Impulse Noise| 0.922640   |\\n| Blur     | Defocus Blur | 0.819880   |\\n|          | Glass Blur  | 0.826268   |\\n|          | Motion Blur | 0.785948   |\\n|          | Zoom Blur   | 0.798360   |\\n| Weather  | Snow        | 0.866816   |\\n|          | Frost       | 0.826572   |\\n|          | Fog         | 0.819324   |\\n|          | Brightness  | 0.564592   |\\n|          | Contrast    | 0.853204   |\\n| Digital  | Elastic Transform | 0.646056 |\\n|          | Pixelate    | 0.717840   |\\n|          | JPEG Compression | 0.606500 |\\n| Hold-out Noise | Speckle Noise | 0.845388 |\\n| Hold-out Digital | Saturate | 0.658248 |\\n|          | Gaussian Blur| 0.787108   |\\n|          | Spatter     | 0.717512   |\\n\\nTable 12: AlexNet top1 errors on ImageNet-C\\n\\nC.2 Detailed results for tuning epochs and learning rates\\n\\nWe tune the learning rate for all models and the number of training epochs for all models except the EfficientNet-L2. In this section, we present detailed results for tuning these hyperparameters for all considered models. The best hyperparameters that we found in this analysis, are summarized in Table 17.\\n\\n| criterion | ENT | RPL |\\n|-----------|-----|-----|\\n| lr        |     |     |\\n| $10^{-4}$ | 60.2| 60.2|\\n| $10^{-3}$ | 54.3| 57.4|\\n| $10^{-2}$ | 52.4| 55.8|\\n|            | 51.5| 64.2|\\n|            | 51.0| 71.0|\\n|            | 50.7| 76.3|\\n\\nTable 13: mCE in % on the IN-C dev set for ENT and RPL for different numbers of training epochs when adapting the affine batch norm parameters of a ResNet50 model.\\n\\n| epoch | ENT | RPL |\\n|-------|-----|-----|\\n| 60.2  | 60.2|\\n| 54.3  | 57.4|\\n| 52.4  | 55.8|\\n| 51.5  | 64.2|\\n| 51.0  | 71.0|\\n| 50.7  | 76.3|\\n\\nTable 14: mCE (\u2198) in % on the IN-C dev set for different learning rates for EfficientNet-L2. We favor $q=0.8$ over $q=0.7$ due to slightly improved robustness to changes in the learning rate in the worst case error setting.\\n\\n| lr (4.6 \u00d7 $10^{-5}$) base | ENT | RPL |\\n|----------------------------|-----|-----|\\n| $10^{-4}$                  | 25.5| 22.2|\\n| $10^{-3}$                  | 25.3| 23.3|\\n| $10^{-2}$                  | 24.1| n/a|\\n| $10^{-1}$                  | 23.3| n/a|\\n| $10^{0}$                   | 22.2| n/a|\"}"}
{"id": "1oEvY1a67c1", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 17: The best hyperparameters for all models that we found on IN-C. For all models, we fine-tune only the affine batch normalization parameters and use $q = 0.8$ for RPL. The small batchsize for the EfficientNet model is due to hardware limitations.\\n\\n| Model               | Method | Learning rate | batch size | epochs |\\n|---------------------|--------|---------------|------------|--------|\\n| vanilla ResNet50    | ENT    | $1 \\\\times 10^{-3}$ | 128 | 1 |\\n| vanilla ResNet50    | RPL    | $1 \\\\times 10^{-3}$ | 128 | 5 |\\n| vanilla ResNeXt101  | ENT    | $2.5 \\\\times 10^{-4}$ | 128 | 1 |\\n| vanilla ResNeXt101  | RPL    | $2.5 \\\\times 10^{-4}$ | 128 | 4 |\\n| IG-3.5B ResNeXt101  | ENT    | $2.5 \\\\times 10^{-4}$ | 128 | 4 |\\n| IG-3.5B ResNeXt101  | RPL    | $2.5 \\\\times 10^{-3}$ | 128 | 2 |\\n| DAug+AM ResNeXt101  | ENT    | $2.5 \\\\times 10^{-4}$ | 128 | 1 |\\n| DAug+AM ResNeXt101  | RPL    | $2.5 \\\\times 10^{-4}$ | 128 | 4 |\\n| EfficientNet-L2     | ENT    | $4.6 \\\\times 10^{-5}$ | 8 | 1 |\\n| EfficientNet-L2     | RPL    | $4.6 \\\\times 10^{-4}$ | 8 | 1 |\\n\\nTable 15: mCE in % on IN-C dev for entropy minimization for different learning rates and training epochs for ResNeXt101. (div.=diverged)\\n\\n| Method | lr | epoch | BASE | 1 | 2 | 3 | 4 | 5 |\\n|--------|----|-------|------|---|---|---|---|---|\\n| ENT    |    |       | 53.6 | 43.0 | 44.8 | 45.4 | 46.7 | 39.1 |\\n| RPL    |    |       | 53.6 | 43.4 | 42.3 | 42.0 | 42.0 | 44.2 |\\n| Base   |    |       | 53.6 | 43.4 | 42.3 | 42.0 | 42.0 | 44.2 |\\n| IG-3.5B |    |       | 47.4 | 45.0 | 43.4 | 42.4 | 42.4 | 39.4 |\\n| DAug+AM |    |       | 58.6 | 39.9 | 39.3 | 39.4 | 39.4 | 35.6 |\\n\\nTable 16: mCE in % on IN-C dev for robust pseudo-labeling for different learning rates and training epochs for ResNeXt101. (div.=diverged)\\n\\n| Method | lr | epoch | BASE | 1 | 2 | 3 | 4 | 5 |\\n|--------|----|-------|------|---|---|---|---|---|\\n| ENT    |    |       | 53.6 | 43.4 | 42.3 | 42.0 | 42.0 | 44.2 |\\n| RPL    |    |       | 53.6 | 43.4 | 42.3 | 42.0 | 42.0 | 44.2 |\\n| Base   |    |       | 53.6 | 43.4 | 42.3 | 42.0 | 42.0 | 44.2 |\\n| IG-3.5B |    |       | 47.4 | 45.0 | 43.4 | 42.4 | 42.4 | 39.4 |\\n| DAug+AM |    |       | 58.6 | 39.9 | 39.3 | 39.4 | 39.4 | 35.6 |\\n\\nC.3 DETAILED RESULTS FOR ALL IN-C CORRUPTIONS\\n\\nWe outline detailed results for all corruptions and models in Table 18. Performance across the severities in the dataset is depicted in Figure 6. All detailed results presented here are obtained by following the model selection protocol outlined in the main text.\\n\\nFigure 6: Severity-wise mean corruption error (normalized using the average AlexNet baseline error for each corruption) for ResNet50 (RN50), ResNext101 (RNx101) variants and the Noisy Student L2 model. Especially for more robust models (DeepAugment+Augmix and Noisy Student L2), most gains are obtained across higher severities 4 and 5. For weaker models, the baseline variant (Base) is additionally substantially improved for smaller corruptions.\"}"}
{"id": "1oEvY1a67c1", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 18: Detailed results for each corruption along with mean corruption error (mCE) as reported in Table 2 in the main paper. We show (unnormalized) top-1 error rate averaged across 15 test corruptions along with the mean corruption error (mCE: which is normalized). Hyperparameter selection for both ENT and RPL was carried out on the dev corruptions as outlined in the main text. Mismatch in baseline mCE for EfficientNet-L2 can be most likely attributed to pre-processing differences between the original tensorflow implementation Xie et al. (2020a) and the PyTorch reimplementation we employ. We start with slightly weaker baselines for ResNet50 and ResNext101 than Schneider et al. (2020): ResNet50 and ResNext101 results are slightly worse than previously reported results (typically 0.1% points) due to the smaller batch size of 128 and 32. Smaller batch sizes impact the quality of re-estimated batch norm statistics when computation is performed on the fly Schneider et al. (2020), which is of no concern here due to the large gains obtained by pseudo-labeling.\\n\\n| Corruption   | Baseline (Schneider et al., 2020) | Baseline (ours) | ENT | RPL |\\n|--------------|----------------------------------|----------------|-----|-----|\\n| ResNet50     | 62.2                             | 57.2           | 45.5| 44.2 |\\n| ResNext101   | 56.7                             | 52.8           | 40.5| 39.4 |\\n| ResNeXt101 IG-3.5B | 51.6                        | 50.7           | 38.6| 39.1 |\\n| ResNeXt101 DeepAug+Augmix | 38.0                     | 30.0           | 28.7| 28.1 |\\n| Noisy Student L2 | 28.3                          | 21.6           | 18.5| 17.8 |\\n\\nTable 19: Detailed results for each corruption along with mean error on CIFAR10-C as reported in Table 2 in the main paper.\\n\\n| Corruption   | Baseline (Schneider et al., 2020) | Baseline (ours) | ENT | RPL |\\n|--------------|----------------------------------|----------------|-----|-----|\\n| WRN-28-10 vanilla | 53.0                            | 20.8           | 18.5| 19.6 |\\n| WRN-40-2 AM | 19.1                             | 14.1           | 10.8| 12.4 |\\n| WRN-26-16 UDA-SS | 26.0                          | 20.5           | 16.9| 18.1 |\"}"}
{"id": "1oEvY1a67c1", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? Hypothesis transfer for unsupervised domain adaptation. In *International Conference on Machine Learning*, 2020.\\n\\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In *Proceedings of the European Conference on Computer Vision (ECCV)*, 2018.\\n\\nS\u00e9bastien Marcel and Yann Rodriguez. Torchvision: The machine-vision package of torch. In *ACM International Conference on Multimedia*, 2010.\\n\\nDirk Merkel. Docker: Lightweight Linux containers for consistent development and deployment. *Linux J.*, 2014(239), 2014. ISSN 1075-3583.\\n\\nSubhabrata Mukherjee and Ahmed Hassan Awadallah. Uncertainty-aware self-training for text classification with few labels. In *NeurIPS*, 2020.\\n\\nZachary Nado, Shreyas Padhy, D Sculley, Alexander D'Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. *ArXiv preprint*, abs/2006.10963, 2020. URL https://arxiv.org/abs/2006.10963.\\n\\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In *NIPS Autodiff Workshop*, 2017.\\n\\nViraj Prabhu, Shivam Khare, Deeksha Kartik, and Judy Hoffman. Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 8558\u20138567, 2021.\\n\\nMamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning. In *ICLR*, 2021.\\n\\nEvgenia Rusak, Lukas Schott, Roland Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge, and Wieland Brendel. Increasing the robustness of DNNs against image corruptions by playing the game of noise. *ArXiv preprint*, abs/2001.06057, 2020. URL https://arxiv.org/abs/2001.06057.\\n\\nKate Saenko, Xingchao Peng, Ben Usman, Kuniaki Saito, and Ping Hu. Visual Domain Adaptation Challenge (VisDA-2019), 2019. URL http://ai.bu.edu/visda-2019/.\\n\\nSteffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In *Advances in neural information processing systems*, 2020.\\n\\nJun Shu, Qian Zhao, Keyu Chen, Zongben Xu, and Deyu Meng. Learning adaptive loss for robust learning with noisy labels. *ArXiv preprint*, abs/2002.06482, 2020. URL https://arxiv.org/abs/2002.06482.\\n\\nRui Shu, Hung H. Bui, Hirokazu Narui, and Stefano Ermon. A DIRT-T approach to unsupervised domain adaptation. In *6th International Conference on Learning Representations, ICLR 2018*, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=H1q-TM-AW.\\n\\nKihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In *NeurIPS*, 2020.\\n\\nHwanjun Song, Minseok Kim, Dongmin Park, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. *ArXiv preprint*, abs/2007.08199, 2020. URL https://arxiv.org/abs/2007.08199.\\n\\nRui Shu, Hung H. Bui, Hirokazu Narui, and Stefano Ermon. A DIRT-T approach to unsupervised domain adaptation. In *6th International Conference on Learning Representations, ICLR 2018*, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=H1q-TM-AW.\\n\\nKihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In *NeurIPS*, 2020.\"}"}
{"id": "1oEvY1a67c1", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "1oEvY1a67c1", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "1oEvY1a67c1", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Definition of the Two-Point Model\\n\\nTo understand the learning dynamics and properties of different loss functions and their hyperparameters, we propose a simple model of self-learning, both for entropy minimization and pseudo-labeling.\\n\\nA student network $w \\\\in \\\\mathbb{R}^d$ and a teacher network $w_t \\\\in \\\\mathbb{R}^d$ are trained on $N$ data points $\\\\{x_i\\\\}_{i=1}^N$ with the cross-entropy loss function $L$ defined as\\n\\n$$L = -\\\\sum_{i=1}^N \\\\ell (x_i) = -\\\\sum_{i=1}^N (\\\\sigma_t (x_i^\\\\top w_t) \\\\log \\\\sigma_s (x_i^\\\\top w_s) + \\\\sigma_t (-x_i^\\\\top w_t) \\\\log \\\\sigma_s (-x_i^\\\\top w_s)),$$\\n\\nwhere $\\\\sigma_t (z) = \\\\frac{1}{1 + e^{-z/\\\\tau_t}}$ and $\\\\sigma_s (z) = \\\\frac{1}{1 + e^{-z/\\\\tau_s}}$.\\n\\nHere $\\\\tau_s$ and $\\\\tau_t$ denote the student and teacher temperature parameters. With stop gradient, student and teacher evolve in time according to\\n\\n$$\\\\dot{w}_s = -\\\\nabla_{w_s} L (w_s, w_t),$$\\n$$\\\\dot{w}_t = \\\\alpha (w_s - w_t),$$\\n\\nwhere $\\\\alpha$ is the learning rate of the teacher. Without stop gradient, student and teacher are set equal to each other, and they evolve as\\n\\n$$\\\\dot{w} = -\\\\nabla_{w} L (w),$$\\n\\nwhere $w_s = w_t = w$.\\n\\nWe restrict the theoretical analysis to the time evolution of the components of $w_s, t$ in direction of two data points $x_k$ and $x_l$, $y_{s,t}^k \\\\equiv x_i^\\\\top w_{s,t}$ and $y_{s,t}^l \\\\equiv x_i^\\\\top w_{s,t}$. All other components $y_{s,t}^i$ with $i \\\\neq k, l$ are neglected to reduce the dimensionality of the equation system. It turns out that the resulting model captures the neural network dynamics quite well despite the drastic simplification of taking only two data points into account (see Figure 2).\\n\\nWith stop gradient:\\n\\n$$\\\\dot{y}_{s}^k = -x_i^\\\\top \\\\nabla_{w_s} (\\\\ell (x_k) + \\\\ell (x_l)),$$\\n$$\\\\dot{y}_{s}^l = -x_i^\\\\top \\\\nabla_{w_s} (\\\\ell (x_k) + \\\\ell (x_l)),$$\\n$$\\\\dot{y}_{t}^k = \\\\alpha (y_{s}^k - y_{t}^k),$$\\n$$\\\\dot{y}_{t}^l = \\\\alpha (y_{s}^l - y_{t}^l).$$\\n\\nWithout stop gradient:\\n\\n$$\\\\dot{y}_{s}^k = -x_i^\\\\top \\\\nabla_{w_s} (\\\\ell (x_k) + \\\\ell (x_l)),$$\\n$$\\\\dot{y}_{s}^l = -x_i^\\\\top \\\\nabla_{w_s} (\\\\ell (x_k) + \\\\ell (x_l)),$$\\n$$\\\\dot{y}_{t}^k = \\\\alpha (y_{t}^k - y_{t}^k),$$\\n$$\\\\dot{y}_{t}^l = \\\\alpha (y_{t}^l - y_{t}^l).$$\\n\\nA.2 Proof of Proposition 1\\n\\nLearning dynamics with stop gradient. Computing the stop gradient evolution defined in equation 7 explicitly yields\\n\\n$$\\\\dot{w}_s = -\\\\nabla_{w_s} L = \\\\frac{1}{\\\\tau_s N} \\\\sum_{i=1}^N (\\\\sigma_t (x_i^\\\\top w_t) \\\\sigma_s (-x_i^\\\\top w_s) - \\\\sigma_t (-x_i^\\\\top w_t) \\\\sigma_s (x_i^\\\\top w_s)) x_i,$$\\n$$\\\\dot{w}_t = \\\\alpha (w_s - w_t).$$\\n\\nThe second equality uses the well-known derivative of the sigmoid function, $\\\\frac{\\\\partial}{\\\\partial z} \\\\sigma (z) = \\\\sigma (z) \\\\sigma (-z)$.\\n\\nThe equation system of $2d$ nonlinear, coupled ODEs for $w_s \\\\in \\\\mathbb{R}^d$ and $w_t \\\\in \\\\mathbb{R}^d$ in equation 10 is analytically difficult to analyze. Instead of studying the ODEs directly, we act on them with the data points $x_i^\\\\top k$, $k = 1, \\\\ldots, N$, and investigate the dynamics of the components $x_i^\\\\top k w_{s,t} \\\\equiv y_{s,t}^k$:\"}"}
{"id": "1oEvY1a67c1", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 20: Detailed results for the UDA methods reported in Table 2 of the main paper.\\n\\n| Method          | UDA CIFAR10 \u2192 STL10, top1 error on target [%] |\\n|-----------------|---------------------------------------------|\\n| WRN-26-16 UDA-SS | 28.7 24.6 22.9 21.8                        |\\n| WRN-26-16 DANN   | 25.0 25.0 24.0 23.9                        |\\n\\nC.5 ABLATION OVER THE HYPERPARAMETER \\\\( q \\\\) FOR RPL\\n\\nFor RPL, we must choose the hyperparameter \\\\( q \\\\). We performed an ablation study over \\\\( q \\\\) and show results in Table 21, demonstrating that RPL is robust to the choice of \\\\( q \\\\), with slight preference to higher values. Note: In the initial parameter sweep for this paper, we only compared \\\\( q = 0.7 \\\\) and \\\\( q = 0.8 \\\\). Given the result in Table 21, it could be interesting to re-run the models in Table 1 of the main paper with \\\\( q = 0.9 \\\\), which could yield another (small) improvement in mCE.\\n\\nTable 21: ImageNet-C dev set mCE in %, vanilla ResNet50, batch size 96. We report the best score across a maximum of six adaptation epochs.\\n\\n| \\\\( q \\\\) | 0.5  | 0.6  | 0.7  | 0.8  | 0.9  |\\n|--------|------|------|------|------|------|\\n| mCE (dev) | 49.5 | 49.3 | 49.2 | 49.2 | 49.1 |\\n\\nC.6 SELF-TRAINING OUTPERFORMS CONTRASTIVE TEST-TIME TRAINING (Sun et al., 2019b)\\n\\nSun et al. (2019b) use a ResNet18 for their experiments on ImageNet and only evaluate their method on severity 5 of IN-C. To enable a fair comparison, we trained a ResNet18 with both hard labeling and RPL and compare the efficacy of both methods to Test-Time Training in Table 22. For both hard labeling and RPL, we use the hyperparameters we found for the vanilla ResNet50 model and thus, we expect even better results for hyperparameters tuned on the vanilla ResNet18 model and following our general hyperparameter search protocol.\\n\\nWhile all methods (self-learning and TTT) improve the performance over a simple vanilla ResNet18, we note that even the very simple baseline using hard labeling already outperforms Test-Time Training; further gains are possible with RPL. The result highlights the importance of simple baselines (like self-learning) when proposing new domain adaptation schemes. It is likely that many established DA techniques more complex than the basic self-learning techniques considered in this work will even further improve over TTT and other adaptation approaches developed exclusively in robustness settings.\\n\\nTable 22: Comparison of hard-pseudo labeling and robust pseudo-labeling to Test-Time Training Sun et al. (2019b): Top-1 error for a ResNet18 and severity 5 for all corruptions. Simple hard pseudo-labeling already outperforms TTT, robust pseudo-labeling over multiple epochs yields additional gains.\\n\\n| Corruption | vanilla ResNet18 | Test-Time Training | hard PL, (1 epoch) | RPL (4 epochs) |\\n|------------|------------------|-------------------|--------------------|---------------|\\n| avg        | 98.8             | 73.7              | 73.2               | 71.3          |\\n| shot       | 98.2             | 71.4              | 70.8               | 68.3          |\\n| impulse    | 99.0             | 73.1              | 73.6               | 71.7          |\\n| defocus    | 88.6             | 76.3              | 76.5               | 76.2          |\\n| glass      | 91.3             | 93.4              | 75.6               | 75.6          |\\n| motion     | 88.8             | 71.3              | 63.9               | 61.5          |\\n| zoom       | 82.4             | 66.6              | 56.1               | 54.4          |\\n| snow       | 89.1             | 64.4              | 59.0               | 56.9          |\\n| frost      | 83.5             | 81.3              | 65.9               | 67.1          |\\n| fog        | 85.7             | 52.4              | 48.4               | 47.3          |\\n| bright     | 48.7             | 39.7              | 39.7               | 39.3          |\\n| contrast   | 96.6             | 41.7              | 85.2               | 93.2          |\\n| elastic    | 83.2             | 55.7              | 50.4               | 48.9          |\\n| pixelate   | 76.9             | 52.2              | 47.0               | 45.7          |\\n| jpeg       | 70.4             | 55.7              | 51.5               | 50.4          |\\n| Avg        | 85.4             | 64.7              | 66.3               | 67.1          |\"}"}
{"id": "1oEvY1a67c1", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How is self-learning performance affected by batch size constraints? We compare the effect of different batch sizes and linear learning rate scaling. In general, we found that affine adaptation experiments on ResNet50 scale can be run with batch size 128 on a Nvidia V100 GPU (16GB), while only batch size 96 experiments are possible on RTX 2080 GPUs.\\n\\nThe results in Table 23 show that for a ResNet50 model, higher batch size yields a generally better performance.\\n\\n### Table 23: ImageNet-C dev set mCE for various batch sizes with linear learning rate scaling. All results are computed for a vanilla ResNet50 model using RPL with \\\\( q = 0.8 \\\\), reporting the best score across a maximum of six adaptation epochs.\\n\\n| Batch Size | Learning Rate \\\\( \\\\times 10^{-3} \\\\) | Dev mCE |\\n|------------|----------------------------------|---------|\\n| 16         | 0.125                            | 53.8    |\\n| 32         | 0.250                            | 51.0    |\\n| 64         | 0.500                            | 49.7    |\\n| 80         | 0.625                            | 49.3    |\\n| 96         | 0.750                            | 49.2    |\\n| 128        | 1                                | 48.9    |\\n\\nTo limit the amount of compute, we ran RPL and ENT for our vanilla ResNet50 model three times with the optimal hyperparameters. The averaged results, displayed as \u201cmean (unbiased std)\u201d, are:\\n\\n### Table 24: ImageNet-C performance for three seeds on a ResNet50 for ENT and RPL.\\n\\n| Model       | mCE on IN-C dev [%] | mCE on IN-C test [%] |\\n|-------------|---------------------|-----------------------|\\n| ENT         | 50.0 (0.04)         | 51.6 (0.04)           |\\n| RPL         | 48.9 (0.02)         | 50.5 (0.03)           |\\n\\nWe test our method on continuous test-time adaptation where the model adapts to a continuous stream of data from the same domain. In Fig. 7, we display the error of the Noisy Student L2 model while it is being adapted to ImageNet-C and ImageNet-R. The model performance improves as the model sees more data from the new domain. We differentiate continuous test-time adaptation from the online test-time adaptation setting (Zhang et al., 2021) where the model is adapted to each test sample individually, and reset after each test sample.\"}"}
{"id": "1oEvY1a67c1", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The domains in IN-D differ in terms of their difficulty for the studied models. Therefore, to calculate an aggregate score, we propose normalizing the error rates by the error achieved by AlexNet on the respective domains to calculate the mean error, following the approach in Hendrycks & Dietterich (2019) for IN-C. This way, we obtain the aggregate score mean Domain Error (mDE) by calculating the mean over different domains,\\n\\n\\\\[\\nmDE = \\\\frac{\\\\sum_{d=1}^{D} E_{f_d} - E_{\\\\text{AlexNet}_d}}{D},\\n\\\\]\\n\\nwhere \\\\( E_{f_d} \\\\) is the top-1 error of a classifier \\\\( f \\\\) on domain \\\\( d \\\\).\\n\\nLeave-one-out-cross-validation\\nFor all IN-D results we report in this paper, we chose the hyperparameters on the IN-C dev set. We tried a different model selection scheme on IN-D as a control experiment with \\\"Leave one out cross-validation\\\" (L1outCV): with a round-robin procedure, we choose the hyperparameters for the test domain on all other domains. We select the same hyperparameters as when tuning on the \\\"dev\\\" set: For the ResNet50 model, we select over the number of training epochs (with a maximum of 7 training epochs) and search for the optimal learning rate in the set \\\\([0.01, 0.001, 0.0001]\\\\). For the EfficientNet-L2 model, we train only for one epoch as before and select the optimal learning rate in the set \\\\([4.6 \\\\times 10^{-3}, 4.6 \\\\times 10^{-4}, 4.6 \\\\times 10^{-5}, 4.6 \\\\times 10^{-6}]\\\\). This model selection leads to worse results both for the ResNet50 and the EfficientNet-L2 models, highlighting the robustness of our model selection process, see Table 25.\\n\\nTable 25: mDE in % on IN-D for different model selection strategies.\\n\\n| Model         | Model Selection | Clipart | Infograph | Painting | Quickdraw | Real Sketch | mDE IN-C | mDE IN-D | mDE IN-R |\\n|---------------|-----------------|---------|-----------|----------|-----------|-------------|----------|----------|----------|\\n| vanilla       |                 | 76.0    | 89.6      | 65.1     | 99.2      | 40.1        | 82.0     | 88.2     | 76.7     |\\n| SIN           |                 | 71.3    | 88.6      | 62.6     | 97.5      | 40.6        | 77.0     | 85.6     | 69.3     |\\n| ANT           |                 | 73.4    | 88.9      | 63.3     | 99.2      | 39.9        | 80.8     | 86.9     | 62.4     |\\n| ANT+SIN      |                 | 68.4    | 88.6      | 60.6     | 95.5      | 40.8        | 70.3     | 83.1     | 60.7     |\\n| AugMix        |                 | 70.8    | 88.6      | 62.1     | 99.1      | 39.0        | 78.5     | 85.4     | 65.3     |\\n| DeepAugment   |                 | 72.0    | 88.8      | 61.4     | 98.9      | 39.4        | 78.5     | 85.6     | 60.4     |\\n| DeepAug+Augmix|                 | 68.4    | 88.1      | 58.7     | 98.2      | 39.2        | 75.2     | 83.4     | 53.6     |\\n\\nD.2 Detailed Results for Robust ResNet50 Models on IN-D\\nWe show detailed results for all models on IN-D for vanilla evaluation (Table 26) BN adaptation (Table 27), RPL \\\\( q = 0.8 \\\\) (Table 28) and ENT (Table 29). For RPL \\\\( q = 0.8 \\\\) and ENT, we use the same hyperparameters that we chose on our IN-C 'dev' set. This means we train the models for 5 epochs with RPL \\\\( q = 0.8 \\\\) and for one epoch with ENT.\\n\\nWe evaluate the pre-trained and public checkpoints of SIN (Geirhos et al., 2019), ANT (Rusak et al., 2020), ANT+SIN (Rusak et al., 2020), AugMix (Hendrycks et al., 2020b), DeepAugment (Hendrycks et al., 2020a) and DeepAug+Augmix (Hendrycks et al., 2020a) in the following tables.\"}"}
{"id": "1oEvY1a67c1", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 29: Top-1 error on IN-D in % as obtained by state-of-the-art robust ResNet50 models and ENT. See main text for references to the used models.\\n\\n| Model        | Clipart | Infograph | Painting | Quickdraw | Real | Sketch | mDE |\\n|--------------|---------|-----------|----------|-----------|------|--------|-----|\\n| vanilla      | 65.1    | 85.8      | 59.2     | 98.5      | 38.4 | 75.8   | 77.3|\\n| SIN          | 62.1    | 87.0      | 57.3     | 99.1      | 39.0 | 68.6   | 75.5|\\n| ANT          | 64.2    | 86.9      | 58.7     | 97.1      | 38.8 | 72.8   | 76.5|\\n| ANT+SIN      | 62.2    | 86.8      | 57.7     | 95.8      | 40.1 | 68.7   | 75.2|\\n| AugMix       | 60.2    | 84.6      | 55.8     | 97.6      | 36.8 | 72.0   | 74.4|\\n| DeepAugment  | 59.5    | 85.7      | 54.4     | 98.0      | 37.1 | 66.4   | 73.3|\\n| DeepAug+Augmix | 58.4   | 84.3      | 54.7     | 98.5      | 38.1 | 63.6   | 72.7|\\n\\nTable 30: mDE on IN-D in % as obtained by robust ResNet50 models with a baseline evaluation, batch norm adaptation, RPL $q = 0$, and ENT. See main text for model references.\\n\\n| Model        | Baseline | BN adapt | RPL $q = 0$. | ENT |\\n|--------------|----------|----------|---------------|-----|\\n| vanilla      | 88.2     | 80.2     | 76.1          | 77.3|\\n| SIN          | 85.6     | 79.6     | 76.8          | 75.5|\\n| ANT          | 86.9     | 80.7     | 78.1          | 76.5|\\n| ANT+SIN      | 83.1     | 77.8     | 76.1          | 75.2|\\n| AugMix       | 85.4     | 78.4     | 74.6          | 74.4|\\n| DeepAugment  | 85.6     | 78.8     | 74.8          | 73.3|\\n| DeepAug+Augmix | 83.4   | 74.9     | 72.6          | 72.7|\\n\\nTable 27: Top1 error on IN-D in % as obtained by state-of-the-art robust ResNet50 models and batch norm adaptation, with a batch size of 128. See main text for model references.\\n\\n| Model        | Clipart | Infograph | Painting | Quickdraw | Real | Sketch | mDE |\\n|--------------|---------|-----------|----------|-----------|------|--------|-----|\\n| vanilla      | 70.2    | 88.2      | 63.5     | 97.8      | 41.1 | 78.3   | 80.2|\\n| SIN          | 67.3    | 89.7      | 62.2     | 97.2      | 44.0 | 75.2   | 79.6|\\n| ANT          | 69.2    | 89.4      | 63.0     | 97.5      | 42.9 | 79.5   | 80.7|\\n| ANT+SIN      | 64.9    | 88.2      | 60.0     | 96.8      | 42.6 | 73.0   | 77.8|\\n| AugMix       | 66.9    | 88.1      | 61.2     | 97.1      | 40.4 | 75.0   | 78.4|\\n| DeepAugment  | 66.6    | 89.7      | 60.0     | 97.2      | 42.5 | 75.1   | 78.8|\\n| DeepAug+Augmix | 61.9   | 85.7      | 57.5     | 95.3      | 40.2 | 69.2   | 74.9|\\n\\nTable 28: Top-1 error on IN-D in % as obtained by state-of-the-art robust ResNet50 models and RPL $q = 0$. See main text for model references.\\n\\n| Model        | Clipart | Infograph | Painting | Quickdraw | Real | Sketch | mDE |\\n|--------------|---------|-----------|----------|-----------|------|--------|-----|\\n| vanilla      | 63.6    | 85.1      | 57.8     | 99.8      | 37.3 | 73.0   | 76.1|\\n| SIN          | 60.8    | 86.4      | 56.0     | 99.0      | 37.8 | 67.0   | 76.8|\\n| ANT          | 63.4    | 86.3      | 57.7     | 99.2      | 37.7 | 71.0   | 78.1|\\n| ANT+SIN      | 61.5    | 86.4      | 56.8     | 97.0      | 39.0 | 67.1   | 76.1|\\n| AugMix       | 59.7    | 83.4      | 54.1     | 98.2      | 35.6 | 70.1   | 74.6|\\n| DeepAugment  | 58.1    | 84.6      | 53.3     | 99.0      | 36.2 | 64.2   | 74.8|\\n| DeepAug+Augmix | 57.0   | 83.2      | 53.4     | 99.1      | 36.5 | 61.3   | 72.6|\\n\\nThe summary results for all models are shown in Table 30.\"}"}
{"id": "1oEvY1a67c1", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Top-1 error for the different IN-D domains for a ResNet50 and training with RPL \\\\( q = 0.8 \\\\) and ENT. We indicate the epochs at which we extract the test errors by the dashed black lines (epoch 1 for ENT and epoch 5 for RPL \\\\( q = 0.8 \\\\)).\\n\\n### D.3 Detailed Results for the EfficientNet-L2 Noisy Student Model on IN-D\\n\\nWe show the detailed results for the EfficientNet-L2 Noisy Student model on IN-D in Table 31.\\n\\n| Domain   | Baseline | ENT | RPL \\\\( q = 0.8 \\\\) |\\n|----------|----------|-----|-------------------|\\n| Clipart  | 45.0     | 39.8| 37.9              |\\n| Infograph| 77.9     | 91.3| 94.3              |\\n| Painting | 42.7     | 41.7| 40.9              |\\n| Quickdraw| 98.4     | 99.4| 99.4              |\\n| Real     | 29.2     | 28.7| 27.9              |\\n| Sketch   | 56.4     | 48.0| 51.5              |\\n\\n### D.4 Detailed Results on the Error Analysis on IN-D\\n\\n#### Frequently predicted classes\\n\\nWe analyze the most frequently predicted classes on IN-D by a vanilla ResNet50 and show the results in Fig. 9. We make several interesting observations:\\n\\n1. First, we find most errors interpretable: it makes sense that a ResNet50 assigns the label \u201ccomic book\u201d to images from the \u201cclipart\u201d or \u201cpainting\u201d domains, or \u201cwebsite\u201d to images from the \u201cinfograph\u201d domain, or \u201cenvelope\u201d to images from the \u201csketch\u201d domain.\\n2. Second, on the hard domain \u201cquickdraw\u201d, the ResNet50 mostly predicts non-sensical classes that are not in IN-D, mirroring its almost chance performance on this domain.\\n3. Third, we find no systematic errors on the \u201creal\u201d domain which is expected since this domain should be similar to IN.\\n\\n#### Filtering predictions on IN-D that cannot be mapped to ImageNet-D\\n\\nWe perform a second analysis: We filter the predicted labels according to whether they can be mapped to IN-D and report the filtered top-1 errors as well as the percentage of filtered out inputs in Table 32. We note that for the domains \u201cinfograph\u201d and \u201cquickdraw\u201d, the ResNet50 predicts labels that cannot be mapped to IN-D in over 70% of all cases, highlighting the hardness of these two domains.\"}"}
{"id": "1oEvY1a67c1", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 32: top-1 error on IN and different IN-D domains for different settings: left column: default evaluation, middle column: predicted labels that cannot be mapped to IN-D are filtered out, right column: percentage of filtered out labels.\\n\\n| Dataset  | top-1 error in % | top-1 error on filtered labels in % | percentage of rejected inputs |\\n|----------|------------------|-----------------------------------|-------------------------------|\\n| IN val   | 12.1             | 13.4                              | 52.7                          |\\n| IN-D real| 40.2             | 17.2                              | 27.6                          |\\n| IN-D clipart | 76.1          | 59.0                              | 59.0                          |\\n| IN-D infograph | 89.7          | 59.3                              | 74.6                          |\\n| IN-D painting | 65.2          | 39.5                              | 42.4                          |\\n| IN-D quickdraw | 99.3          | 96.7                              | 76.1                          |\\n| IN-D sketch | 82.1            | 65.6                              | 47.9                          |\\n\\nFiltering labels and predictions on IN that cannot be mapped to ImageNet-D\\n\\nTo test for possible class-bias effects, we test the performance of a ResNet50 model on IN classes that can be mapped to IN-D and report the results in Table 32. First, we map IN labels to IN-D to make the setting as similar as possible to our experiments on IN-D and report the top-1 error (12.1%). This error is significantly lower compared to the top-1 error a ResNet50 obtains following the standard evaluation protocol (23.9%). This can be explained by the simplification of the task: While in IN there are 39 bird classes, these are all mapped to the same hierarchical class in IN-D. Therefore, the classes in IN-D are more dissimilar from each other than in IN. Additionally, there are only 164 IN-D classes compared to the 1000 IN classes, raising the chance level prediction.\\n\\nIf we further only accept predictions that can be mapped to IN-D, the top-1 error is slightly increased to 13.4%. In total, about 52.7% of all images in the IN validation set cannot be mapped to IN-D.\"}"}
{"id": "1oEvY1a67c1", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"We report the top-1 error numbers on different IN-D as achieved by AlexNet in Table 33. We used these numbers for normalization when calculating mDE.\\n\\n| Dataset         | Top-1 Error in % |\\n|-----------------|------------------|\\n| IN-D real       | 54.887           |\\n| IN-D clipart    | 84.010           |\\n| IN-D infograph  | 95.072           |\\n| IN-D painting   | 79.080           |\\n| IN-D quickdraw  | 99.745           |\\n| IN-D sketch     | 91.189           |\"}"}
{"id": "1oEvY1a67c1", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The WILDS benchmark (Koh et al., 2021) is comprised of ten tasks to test domain generalization, subpopulation shift, and combinations thereof. In contrast to the setting considered here, many of the datasets in WILDS mix several 10s or 100s domains during test time.\\n\\nThe Camelyon17 dataset in WILDS contains histopathological images, with the labels being binary indicators of whether the central $32 \\\\times 32$ region contains any tumor tissue; the domain identifies the hospital that the patch was taken from. Camelyon17 contains three different test splits with different domains and varying difficulty levels. For evaluation, we took the pretrained checkpoint from worksheets.codalab.org/worksheets/0x00d14c55993548a1823a710642f6d608 (camelyon17 erdensenet121 seed0) for a DenseNet121 model (Huang et al., 2017) and verified the reported baseline performance numbers. We adapt the models using ENT or RPL for a maximum of 10 epochs using learning rates $\\\\{3 \\\\times 10^{-5}, 3 \\\\times 10^{-4}, ..., 3 \\\\times 10^{-1}\\\\}$. The best hyperparameter is selected according to OOD Validation accuracy.\\n\\nThe RxRx1 dataset in WILDS contains RGB images of cells obtained by fluorescent microscopy, with the labels indicating which of the 1,139 genetic treatments (including no treatment) the cells received; the domain identifies the batch in which the imaging experiment was run. The RxRx1 dataset contains three test splits, however, unlike Camelyon17, in all of the splits the domains are mixed. For evaluation, we took the pretrained checkpoint from worksheets.codalab.org/bundles/0x7d33860545b64acca5047396d42c0ea0 for a ResNet50 model and verified the reported baseline performance numbers. We adapt the models using ENT or RPL for a maximum of 10 epochs using base learning rates $\\\\{6.25 \\\\times 10^{-6}, 6.25 \\\\times 10^{-5}, ..., 6.25 \\\\times 10^{-2}\\\\}$, which are scaled to the admissible batch size for single GPU adaptation using linear scaling. The best hyperparameter is selected according to OOD Validation accuracy.\\n\\nTable 34: Self-learning can improve performance on WILDS if a systematic shift is present \u2014 on Camelyon17, the ood validation and test sets are different hospitals, for example. On datasets like RxRx1 and FMoW, we do not see an improvement, most likely because the ood domains are shuffled, and a limited amount of images exist for each test domain.\\n\\n| Dataset     | Top-1 accuracy [%]               |\\n|-------------|----------------------------------|\\n|             | Validation (ID) | Validation (OOD) | Test (OOD) |\\n| Camelyon17  | Baseline 81.4  | 88.7             | 63.1       |\\n|             | BN adapt 97.8 (+16.4) | 90.9 (+2.2) | 88.0 (+24.9) |\\n|             | ENT 97.6 (+16.2) | 92.7 (+4.0) | 91.6 (+28.5) |\\n|             | RPL 97.6 (+16.2) | 93.0 (+4.3) | 91.0 (+27.9) |\\n| RxRx1       | Baseline 35.9  | 19.1             | 29.7       |\\n|             | BN adapt 35.0 (-0.9) | 19.1 (0.0) | 29.4 (-0.3) |\\n|             | ENT 34.8 (-1.1) | 19.2 (+0.1) | 29.4 (-0.3) |\\n|             | RPL 34.8 (-1.1) | 19.2 (+0.1) | 29.4 (-0.3) |\\n| FMoW        | Baseline 60.5  | 59.2             | 52.9       |\\n|             | BN adapt 59.9 (-0.6) | 57.6 (-1.6) | 51.8 (-1.1) |\\n|             | ENT 59.9 (-0.6) | 58.5 (-0.7) | 52.2 (-0.7) |\\n|             | RPL 59.8 (-0.7) | 58.6 (-0.6) | 52.1 (-0.8) |\\n\\nThe FMoW dataset in WILDS contains RGB satellite images, with the labels being one of 62 building or land use categories; the domain specifies the year in which the image was taken and its geographical region (Africa, the Americas, Oceania, Asia, or Europe). The FMoW dataset contains four test splits for different time periods, for which all regions are mixed together. For evaluation, we took the pretrained checkpoint from worksheets.codalab.org/bundles/0x20182ee424504e4a916fe88c91afd5a2 for a DenseNet121 model and verified the reported baseline performance numbers. We adapt the models using ENT or RPL for a maximum of 10 epochs.\"}"}
{"id": "1oEvY1a67c1", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Self-learning successfully adapts ImageNet-scale models across different model architectures on IN-C, IN-A and IN-R. We adapt the vanilla ResNet50, ResNeXt101 and DenseNet161 models to IN-C and decrease the mCE by over 19 percent points in all models. Further, self-learning works for models irrespective of their size: Self-learning substantially improves the performance of the ResNet50 and the ResNext101 trained with DAug+AM, on IN-C by 11.9 and 9.7 percent points, respectively. Finally, we further improve the current state-of-the-art model on IN-C\u2014the EfficientNet-L2 Noisy Student model\u2014and report a new state-of-the-art result of 22% mCE (which corresponds to a top1 error of 17.1%) on this benchmark with test-time adaptation (compared to 28% mCE without adaptation).\\n\\n| Model                          | Parameters | mCE [%] (w/o adapt) | mCE [%] (w/ adapt) | \u2206 mCE [%] |\\n|-------------------------------|------------|----------------------|---------------------|-----------|\\n| ResNet50 vanilla (He et al., 2016b) | 2.65 \u00d7 10^7 | 76.7 | 50.5 | -26.2 |\\n| ResNet50 DAug+AM (Hendrycks et al., 2020a) | 2.65 \u00d7 10^7 | 53.6 | 41.7 | -11.9 |\\n| DenseNet161 vanilla (Huang et al., 2017) | 2.86 \u00d7 10^7 | 66.4 | 47.0 | -19.4 |\\n| ResNeXt101 32\u00d78d vanilla (Xie et al., 2017) | 8.86 \u00d7 10^7 | 66.6 | 43.2 | -23.4 |\\n| ResNeXt101 32\u00d78d DAug+AM (Hendrycks et al., 2020a) | 8.86 \u00d7 10^7 | 44.5 | 34.8 | -9.7 |\\n| ResNeXt101 32\u00d78d IG-3.5B (Mahajan et al., 2018) | 8.86 \u00d7 10^7 | 51.7 | 40.9 | -10.8 |\\n| EfficientNet-L2 Noisy Student (Xie et al., 2020a) | 4.88 \u00d7 10^8 | 28.3 | 22.0 | -6.3 |\\n\\nOn IN-R, a dataset with renditions, self-learning improves both the vanilla ResNet50 and the EfficientNet-L2 model, the latter of which improves from 23.5% to a new state-of-the art of 17.4% top-1 error. For a vanilla ResNet50, we improve the top-1 error from 63.8% (Hendrycks et al., 2020a) to 54.1%.\\n\\nTable 2: Self-learning improves robustified and domain adapted models on small-scale datasets. We test common domain adaptation techniques like DANN (Ganin et al., 2016) and UDA-SS (Sun et al., 2019a), and show that self-learning is effective at further tuning such models to the target domain. We suggest to view unsupervised source/target domain adaptation as a step comparable to pre-training under corruptions, rather than an adaptation technique specifically tuned to the target set\u2014indeed, we can achieve error rates using, e.g., DANN + target adaptation previously only possible with source/target based pseudo-labeling, across different common domain adaptation benchmarks. Self-learning also decreases the error on CIFAR10-C of the Wide ResNet model trained with AugMix (AM, Hendrycks et al., 2020b) and reaches a new state of the art on CIFAR10-C of 8.5% top1 error with test-time adaptation.\\n\\n\u2020 denotes preliminary results on CIFAR-C dev only, due to instabilities in training the adversarial network in DANN.\\n\\n| Model                          | Parameters | top1 error [%] (w/o adapt) | top1 error [%] (w/ adapt) | \u2206 top1 error [%] |\\n|-------------------------------|------------|-----------------------------|-----------------------------|------------------|\\n| WRN-28-10 vanilla (Zagoruyko & Komodakis, 2016) | 3.66 \u00d7 10^7 | 26.5 | 13.3 | -13.2 |\\n| WRN-40-2 AM (Hendrycks et al., 2020b) | 2.22 \u00d7 10^6 | 11.2 | 8.5 | -2.7 |\\n| WRN-26-16 UDA-SS (Sun et al., 2019a) | 9.33 \u00d7 10^7 | 27.7 | 16.7 | -11.0 |\\n| WRN-26-16 DANN (Ganin et al., 2016) | 9.33 \u00d7 10^7 | 29.7\u2020 | 28.5\u2020 | -1.2 |\\n\\nUDA CIFAR10 \u2192 STL10, top1 error on target [%] (\u2198)\\n\\n| Model                          | Parameters | top1 error [%] (w/o adapt) | top1 error [%] (w/ adapt) | \u2206 top1 error [%] |\\n|-------------------------------|------------|-----------------------------|-----------------------------|------------------|\\n| WRN-26-16 UDA-SS (Sun et al., 2019a) | 9.33 \u00d7 10^7 | 28.7 | 21.8 | -6.9 |\\n| WRN-26-16 DANN (Ganin et al., 2016) | 9.33 \u00d7 10^7 | 25.0 | 23.9 | -1.1 |\\n\\nUDA MNIST \u2192 MNIST-M, top1 error on target [%] (\u2198)\\n\\n| Model                          | Parameters | top1 error [%] (w/o adapt) | top1 error [%] (w/ adapt) | \u2206 top1 error [%] |\\n|-------------------------------|------------|-----------------------------|-----------------------------|------------------|\\n| WRN-26-16 UDA-SS (Sun et al., 2019a) | 9.33 \u00d7 10^7 | 4.8 | 2.0 | -2.8 |\\n| WRN-26-2 DANN (Ganin et al., 2016) | 1.55 \u00d7 10^6 | 11.4 | 5.1 | -6.3 |\"}"}
{"id": "1oEvY1a67c1", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Self-learning also improves large pre-trained models. Unlike BatchNorm adaptation (Schneider et al., 2020), we show that self-learning transfers well to models pre-trained on a large amount of unlabeled data: self-learning decreases the mCE on IN-C of the ResNeXt101 trained on 3.5 billion weakly labeled samples (IG-3.5B, Mahajan et al., 2018) from 51.7% to 40.9%.\\n\\n| Model   | ResNeXt101 | vanilla | 66.6 | 56.8 | 43.2 |\\n|---------|------------|---------|------|------|------|\\n|         |            |         |      |      |      |\\n|         |            | w/ adapt|      |      |      |\\n\\nTable 4: Self-learning outperforms previously published test-time adaptation approaches on IN-C. The robustness benchmark IN-C has so far mostly been regarded in the ad-hoc evaluation setting as discussed in our introduction. Thus, there are only few published methods that report numbers for test-time adaptation: BatchNorm adaptation (Schneider et al., 2020), Test-Time Training (TTT, Sun et al., 2019b), and TENT (Wang et al., 2020). In particular, note that TTT requires a special loss function at training time, while our approach is agnostic to the pre-training phase. Our self-training results outperform all three baselines (also after tuning TENT with our full experimental protocol):\\n\\n| Model   | ResNet50 | vanilla | 76.7 | 62.2 | 53.5 (51.6) |\\n|---------|----------|---------|------|------|-------------|\\n|         |          | w/ adapt|      |      | 50.5        |\\n\\nTable 5: Self-supervised methods based on self-learning allow out-of-the-box test-time adaptation. The recently published DINO method (Caron et al., 2021) is another variant of self-supervised learning that has proven to be effective for unsupervised representation learning. At the core, the method uses soft pseudo-labeling. Here, we test whether a model trained with DINO on the source dataset can be test-time adapted on IN-C using DINO to further improve out-of-distribution performance. Since the used model is a vision transformer model, we test different choices of adaptation parameters and find considerable performance improvements in all cases, yielding an mCE of 43.5% at a parameter count comparable to a ResNet50 model. For adapting the affine layers, we follow Houlsby et al. (2019):\\n\\n| Model   | ViT-S/16 | w/ adapt|      |      | 43.5 | 43.5 |\\n|---------|----------|---------|------|------|------|------|\\n|         |          | w/ adapt|      |      |      |      |\\n\\nTable 6: Robust pseudo-labeling outperforms entropy minimization on large-scale datasets while the reverse is true on small-scale datasets. We find that robust pseudo-labeling consistently improves over entropy minimization on IN-C, while entropy minimization performs better on smaller scale data (CIFAR10, STL10, MNIST). The finding highlights the importance of testing both algorithms on new datasets. The improvement is typically on the order of one percent point:\\n\\n| Model   | ResNet50 | ENT | 50.0 \u00b1 0.04 | 43.0 | 22.2 |\\n|---------|----------|-----|--------------|------|------|\\n|         | ResNeXt-101 | ENT | 48.9 \u00b1 0.02  | 42.0 | 21.3 |\\n|         | EfficientNet-L2 | RPL | 48.9 \u00b1 0.02  | 42.0 | 21.3 |\\n\\nTable 7: Robust pseudo-labeling allows usage of the full dataset without a threshold. Classical hard labeling needs a confidence threshold (T) for best performance, thereby reducing the dataset size, while best performance for RPL is reached for full dataset training with a threshold T of 0.0:\\n\\n| Method  | diff. self-learning methods | no adapt | soft PL | hard PL (T): 0.0 | 0.5 | 0.9 |\\n|---------|-------------------------------|----------|---------|------------------|-----|-----|\\n| mCE on IN-C dev [%]  | 69.5 | 60.1 | 53.8 | 51.9 | 52.4 | 49.7 | 49.9 | 51.8 |\"}"}
{"id": "1oEvY1a67c1", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Short update intervals are crucial for fast adaptation. Having established that RPL generally performs better than soft- and hard-labeling, we vary the update interval for the teacher. We find that instant updates are most effective. In entropy minimization, the update interval is instant per default.\\n\\n| Update interval for RPL w/o adapt | no update | epoch instant | mCE on IN-C dev [%] |\\n|-----------------------------------|-----------|---------------|---------------------|\\n| mCE on IN-C dev [%]              | 69.5      | 54.0          | 49.7                |\\n\\nTable 9: Adaptation of only affine layers is important in CNNs. On IN-C, adapting only the affine parameters after the normalization layers (i.e., the rescaling and shift parameters $\\\\beta$ and $\\\\gamma$) works better on a ResNet50 architecture than adapting all parameters or only the last layer. We indicate the number of adapted parameters in brackets.\\n\\n| Adaptation mechanism w/o adapt | last layer | full model | affine | mCE on IN-C dev [%] |\\n|-------------------------------|------------|------------|--------|---------------------|\\n| mCE on IN-C dev [%]           | 69.5       | 60.2       | 51.5   | 51.5                |\\n| mCE on IN-C dev [%]           | 48.9       | 5.3k       | 5.3k   | 5.3k                |\\n\\nNote that for Vision Transformers, full model adaptation works better than affine adaptation (see Table 5). We also noticed that on convolutional models with a smaller parameter count like ResNet18, full model adaptation is possible.\\n\\nHyperparameters obtained on corruption datasets transfer well to real world datasets. When evaluating models, we select the hyperparameters discussed above (the learning rate and the epoch used for early stopping are the most critical ones) on the holdout set of IN-C. We note that this technique transfers well to IN-R, -A and -D, highlighting the practical value of corruption robustness datasets for adapting models on real distribution shifts.\\n\\nOn IN-D, we performed a control experiment where we selected hyperparameters with leave-one-out cross validation\u2014this selection scheme actually performed worse than IN-C parameter selection (see Appendix D.1).\\n\\n6 A\\n\\nDAPTING MODELS ON A WIDER RANGE OF DISTRIBUTION SHIFTS\\n\\nREVEALS LIMITATIONS OF ROBUSTIFICATION AND ADAPTATION METHODS\\n\\nRobustness datasets on ImageNet-scale have so far been limited to a few selected domains (image corruptions in IN-C, image renditions in IN-R, difficult images for ResNet50 classifiers in IN-A). In order to test our approach on a wider range of complex distribution shifts, we re-purpose the dataset from the Visual Domain Adaptation Challenge 2019 (DomainNet, Saenko et al., 2019) as an additional robustness benchmark. This dataset comes with six image styles: Clipart, Real, Infograph, Painting, Quickdraw and Sketch. It has 345 classes in total, of which 164 overlap with IN. To benchmark robustness of IN trained models out of the box, we filter out the classes that cannot be mapped to IN and refer to the smaller version of DomainNet as ImageNet-D (IN-D). We map 463 classes in IN to these 164 IN-D classes, e.g., for an image from the \\\"bird\\\" class in IN-D, we accept all 39 bird classes in IN as valid predictions. We show example images from IN-D in Table 10. The detailed evaluation protocol along with justifications for our design choices and additional analysis are outlined in Appendix D.\\n\\nThe benefit of IN-D over DomainNet is the re-mapping to ImageNet classes which allows robustness researchers to easily benchmark on this dataset, without the need of re-training a model (as common in UDA). To test whether self-learning is helpful for more complex distribution shifts, we adapt a vanilla ResNet50, several robust IN-C models and the EfficientNet-L2 Noisy Student model on IN-D. We use the same hyperparameters we obtained on IN-C dev for all our IN-D experiments. We show our main results in Table 10.\\n\\nMore robust models perform better on IN-D. Comparing the performance of the vanilla ResNet50 model to its robust DAug+AM variant, we find that the DAug+AM model performs better on all domains, with the most significant gains on the \\\"Clipart\\\", \\\"Painting\\\" and \\\"Sketch\\\" domains. We show detailed results for all domains and all tested models in Appendix D.2, along with results on IN-C and IN-R for comparison. We find that the best performing models on IN-D are also the\"}"}
{"id": "1oEvY1a67c1", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 10: Self-learning decreases the top1 error on some IN-D domains but increases it on others.\\n\\n| Domain     | Real Painting | Clipart | Sketch | Infograph | Quickdraw | adapt w/o | w/ | w/o | w/ | w/o | w/ |\\n|------------|--------------|---------|--------|-----------|-----------|------------|----|-----|----|-----|----|\\n| model      | EffNet-L2    | Noisy Student | 29.2   | 27.9      | 42.7      | 40.9       | 45.0| 37.9| 56.4| 51.5| 77.9|\\n|            | ResNet50     | DAug+AM  | 39.2   | 36.5      | 58.7      | 53.4       | 68.4| 57.0| 75.2| 61.3| 88.1|\\n|            | ResNet50 vanilla |       | 40.1   | 37.3      | 65.1      | 57.8       | 76.0| 63.6| 82.0| 73.0| 89.6|\\n|            |              |          |        |           |           |            |    |     |     |     |     |\\n\\nStrongest ones on IN-C and IN-R which indicates good generalization capabilities of the techniques combined for these models, given the large differences between the three considered datasets. However, even the best models perform 20 to 30 percentage points worse on IN-D compared to their performance on IN-C or IN-R, indicating that IN-D might be a more challenging benchmark. All models struggle with some domains of IN-D. The EfficientNet-L2 Noisy Student model obtains the best results on most domains. However, we note that the overall error rates are surprisingly high compared to the model's strong performance on the other considered datasets (IN-A: 14.8% top-1 error, IN-R: 17.4% top-1 error, IN-C: 22.0% mCE). Even on the \u201cReal\u201d domain closest to clean IN where the EfficientNet-L2 model has a top-1 error of 11.6%, the model only reaches a top-1 error of 29.2%. Self-learning decreases the top1 error on all domains except for \u201cInfograph\u201d and \u201cQuickdraw\u201d. We note that both domains have very high error rates from the beginning and thus hypothesize that the produced pseudo-labels are of low quality.\\n\\nError analysis on IN-D. We investigate the errors a ResNet50 model makes on IN-D by analyzing the most frequently predicted classes for different domains to reveal systematic errors indicative of the encountered distribution shifts. We find most errors interpretable: the classifier assigns the label \u201ccomic book\u201d to images from the \u201cClipart\u201d or \u201cPainting\u201d domains, \u201cwebsite\u201d to images from the \u201cInfograph\u201d domain, and \u201cenvelope\u201d to images from the \u201cSketch\u201d domain. Thus, the classifier predicts the domain rather than the class. We find no systematic errors on the \u201cReal\u201d domain which is expected since this domain should be similar to IN. Detailed results on the top-3 most frequently predicted classes for different domains can be found in Fig. 9, Appendix D.4.\\n\\nIN-D should be used as an additional robustness benchmark. While the error rates on IN-C, -R and -A are at a well-acceptable level for our largest EfficientNet-L2 model after adaptation, IN-D performance is consistently worse for all models. We propose to move from isolated benchmark settings like IN-R (single domain) to benchmarks more common in domain adaptation (like DomainNet) and make IN-D publicly available as an easy to use dataset for this purpose.\\n\\nAdditional experiments and limitations. We discuss additional proof-of-concept implementations on the WILDS benchmark (Koh et al., 2021), BigTransfer (BiT; Chen et al., 2020a) models and on self-learning based UDA models in Appendix E. On WILDS, self-learning is effective for the Camelyon17 task with a systematic shift between train, validation and test sets (each set is comprised of different hospitals), while self-learning fails to improve on tasks with mixed domains.\\n\\nSIMPLE MODEL OF STABILITY IN SELF-LEARNING\\n\\nWe observed that different self-learning schemes are optimal for small-scale vs. large-scale datasets and varying amount of classes. We reconsider the used loss functions, and unify them into\\n\\n$$\\\\ell(x) = -\\\\sum_j \\\\sigma_j(f_t(x)_{\\\\tau_t}) \\\\log(\\\\sigma_j(f_s(x)_{\\\\tau_s}))$$,\\n\\nwhere $f_t(x)$ = \\\\begin{cases} f(x), & \\\\text{entropy minimization} \\\\\\\\ \\\\sigma(f(x)), & \\\\text{pseudo-labeling} \\\\end{cases}$.\\n\\nWe introduced student and teacher temperature $\\\\tau_s$ and $\\\\tau_t$ as parameters in the softmax function and the stop gradient operation $\\\\sg$. Caron et al. (2021) fixed $\\\\tau_s$ and varied $\\\\tau_t$ during training, 8\"}"}
{"id": "1oEvY1a67c1", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"value) largest terms $i = k, l$ for a given $k$ in the sum in equation 11. Any changes that $y_s, t$ and $y_s, l$ might induce in other modes $y_s, i$ are neglected, and so we are left with only four ODEs:\\n\\n\\\\[\\n\\\\dot{y}_s k = \\\\frac{1}{\\\\tau_s} \\\\parallel x_k \\\\parallel_2 \\\\left( \\\\sigma_t (y_t k) \\\\sigma_s (-y_s k) - \\\\sigma_t (-y_t k) \\\\sigma_s (y_s k) \\\\right) + \\\\frac{1}{\\\\tau_s} (x_k^\\\\top x_l) \\\\left( \\\\sigma_t (y_t l) \\\\sigma_s (-y_s l) - \\\\sigma_t (-y_t l) \\\\sigma_s (y_s l) \\\\right),\\n\\\\]\\n\\n\\\\[\\n\\\\dot{y}_s l = \\\\frac{1}{\\\\tau_s} \\\\parallel x_l \\\\parallel_2 \\\\left( \\\\sigma_t (y_t l) \\\\sigma_s (-y_s l) - \\\\sigma_t (-y_t l) \\\\sigma_s (y_s l) \\\\right) + \\\\frac{1}{\\\\tau_s} (x_k^\\\\top x_l) \\\\left( \\\\sigma_t (y_t k) \\\\sigma_s (-y_s k) - \\\\sigma_t (-y_t k) \\\\sigma_s (y_s k) \\\\right),\\n\\\\]\\n\\n\\\\[\\n\\\\dot{y}_t k = \\\\alpha (y_s k - y_t k), \\\\quad \\\\dot{y}_t l = \\\\alpha (y_s l - y_t l).\\n\\\\]\\n\\nThe fixed points of equation 12 satisfy $\\\\dot{y}_s k = \\\\dot{y}_s l = \\\\dot{y}_t k = \\\\dot{y}_t l = 0$.\\n\\nFor $\\\\alpha > 0$, requiring $\\\\dot{y}_t k = \\\\dot{y}_t l = 0$ implies that $y_s k = y_t k$ and $y_s l = y_t l$. For $\\\\tau_s = \\\\tau_t$, the two remaining equations $\\\\dot{y}_s k = \\\\dot{y}_s l = 0$ vanish automatically so that there are no non-trivial two-point learning dynamics. For $\\\\tau_s \\\\neq \\\\tau_t$, there is a fixed point at $y_{s,t} k = y_{s,t} l = 0$ since at this point, each bracket in equation 12 vanishes individually:\\n\\n\\\\[\\n\\\\left| \\\\sigma_t (y_{k,l}) \\\\sigma_s (-y_{k,l}) - \\\\sigma_t (-y_{k,l}) \\\\sigma_s (y_{k,l}) \\\\right| \\\\bigg|_{y_{k,l} = 0} = 1/4 - 1/4 = 0.\\n\\\\]\\n\\nAt the fixed point $y_{s,t} k = y_{s,t} l = 0$, $w_s$ and $w_t$ are orthogonal to both $x_k$ and $x_l$ and hence classification fails. If this fixed point is stable, $w_s$ and $w_t$ will stay at the fixed point once they have reached it, i.e. the model collapses. The fixed point is stable when all eigenvalues of the Jacobian $J$ of the ODE system equation 12 evaluated at $y_{s,t} k = y_{s,t} l = 0$ are negative. This is the case whenever $\\\\tau_s < \\\\tau_t$:\\n\\n\\\\[\\nJ \\\\bigg|_{y_{s,t} k = y_{s,t} l = 0} = \\\\begin{pmatrix}\\n\\\\frac{1}{2} - \\\\frac{1}{\\\\tau_s} & 0 & 0 & \\\\frac{1}{\\\\tau_t} - \\\\frac{1}{\\\\tau_s} \\\\\\\\\\n0 & \\\\frac{1}{2} - \\\\frac{1}{\\\\tau_s} & 0 & \\\\frac{1}{\\\\tau_t} - \\\\frac{1}{\\\\tau_s} \\\\\\\\\\n0 & 0 & \\\\frac{1}{\\\\tau_t} - \\\\frac{1}{\\\\tau_s} & \\\\frac{1}{2} - \\\\frac{1}{\\\\tau_s} \\\\\\\\\\n0 & 0 & 0 & \\\\frac{1}{\\\\tau_t} - \\\\frac{1}{\\\\tau_s}\\n\\\\end{pmatrix},\\n\\\\]\\n\\neigenvalues: $\\\\lambda_1 = \\\\lambda_2 = -\\\\alpha < 0$, $\\\\lambda_3, 4 = 1/8 (1/\\\\tau_t - 1/\\\\tau_s) \\\\pm \\\\sqrt{1/\\\\tau_t + 1/\\\\tau_s - 2/\\\\tau_s}$.\\n\\nTo sum up, training with stop gradient and $\\\\tau_s > \\\\tau_t$ avoids a collapse of the two-point model to the trivial representation $y_{s,t} k = y_{s,t} l = 0$ since the fixed point is not stable in this parameter regime.\\n\\nLearning dynamics without stop gradient\\n\\nWithout stop gradient, we set $w_t = w_s \\\\equiv w$ which leads to an additional term in the gradient:\\n\\n\\\\[\\n\\\\dot{w} = -\\\\nabla_w L = \\\\frac{1}{\\\\tau_s} \\\\sum_{i=1}^N \\\\left( \\\\sigma_t (x_i^\\\\top w) \\\\sigma_s (-x_i^\\\\top w) - \\\\sigma_t (-x_i^\\\\top w) \\\\sigma_s (x_i^\\\\top w) \\\\right) x_i + \\\\frac{1}{\\\\tau_t} \\\\sum_{i=1}^N \\\\sigma_t (x_i^\\\\top w) \\\\sigma_t (x_i^\\\\top w) \\\\left( \\\\log \\\\sigma_s (x_i^\\\\top w) - \\\\log \\\\sigma_s (-x_i^\\\\top w) \\\\right),\\n\\\\]\\n\\n\\\\[\\n= \\\\log \\\\left( \\\\frac{1 + e^{y_i/\\\\tau_s}}{1 + e^{-y_i/\\\\tau_s}} \\\\right) = y_i/\\\\tau_s x_i.\\n\\\\]\"}"}
{"id": "1oEvY1a67c1", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As before, we focus on the evolution of the two components $y_k = w_k^\\\\top x_k$ and $y_l = w_l^\\\\top x_l$.\\n\\n\\\\[\\n\\\\dot{y}_k = \\\\|x_k\\\\|^2_2 \\\\left(1 - \\\\frac{\\\\sigma_t(y_k) \\\\sigma_s(-y_k) - \\\\sigma_t(-y_k) \\\\sigma_s(y_k)}{\\\\tau_s \\\\tau_t \\\\sigma_t(y_k) \\\\sigma_t(-y_k) y_k} \\\\right) + \\\\left(x_k^\\\\top x_l \\\\right) \\\\left(1 - \\\\frac{\\\\sigma_t(y_l) \\\\sigma_s(-y_l) - \\\\sigma_t(-y_l) \\\\sigma_s(y_l)}{\\\\tau_s \\\\tau_t \\\\sigma_t(y_l) \\\\sigma_t(-y_l) y_l} \\\\right)\\n\\\\]\\n\\n\\\\[\\n\\\\dot{y}_l = \\\\|x_l\\\\|^2_2 \\\\left(1 - \\\\frac{\\\\sigma_t(y_k) \\\\sigma_s(-y_k) - \\\\sigma_t(-y_k) \\\\sigma_s(y_k)}{\\\\tau_s \\\\tau_t \\\\sigma_t(y_k) \\\\sigma_t(-y_k) y_k} \\\\right) + \\\\left(x_k^\\\\top x_l \\\\right) \\\\left(1 - \\\\frac{\\\\sigma_t(y_l) \\\\sigma_s(-y_l) - \\\\sigma_t(-y_l) \\\\sigma_s(y_l)}{\\\\tau_s \\\\tau_t \\\\sigma_t(y_l) \\\\sigma_t(-y_l) y_l} \\\\right)\\n\\\\]\\n\\nThere is a fixed point at $y_k = y_l = 0$ where each bracket in equation (17) vanishes individually,\\n\\n\\\\[\\n\\\\lambda_1,2 = \\\\frac{1}{8 \\\\tau_s} \\\\left(2 \\\\tau_t - 1 \\\\right) \\\\sqrt{\\\\|x_k\\\\|^4_2 + \\\\|x_l\\\\|^4_2 - 2 \\\\|x_k\\\\|^2_2 \\\\|x_l\\\\|^2_2 + 4 \\\\left(x_k^\\\\top x_l \\\\right)^2} \\\\leq \\\\|x_k\\\\|^2_2 + \\\\|x_l\\\\|^2_2 + \\\\|x_k\\\\|^2_2 + \\\\|x_l\\\\|^2_2 \\\\geq 0\\n\\\\]\\n\\nwith equality if $x_k = \\\\pm x_l$.\\n\\nHence the fixed point is unstable when $\\\\tau_s > \\\\tau_t / 2$ and thus the model without stop gradient does not collapse onto $y_k = y_l = 0$ in this regime.\\n\\n### A.3 Simulation of the Two-Point Model\\n\\nFor visualization purposes in the main paper, we set $w_s = w_t = [0.5, 0.5]^\\\\top$ and train the model using instant gradient updates on the dataset with points $x_1 = [1, 0]$ and $x_2 = [0, -1]$ using SGD with learning rate $0.1$ and momentum $0.9$. We varied student and teacher temperatures on a log-scale with 250 points from $10^{-3}$ to $10^3$. Qualitatively similar results can be obtained without momentum training, at higher learning rates (most likely due to the implicit learning rate scaling introduced by the momentum term).\\n\\nNote that the temperature scales for observing the collapse effect depend on the learning rate, and the exact training strategy\u2014lower learning rates can empirically prevent the model from collapsing and shift the convergence region. The result in Figure 2 will hence depend on the exact choice of learning rate (which is currently not considered in our continuous time evolution theory), while the predicted region without collapse is robust to details of the optimization.\\n\\nTo visualize the impact of different hyperparameters, we show variants of the two point model with different learning rates using gradient descent with (Figure 3) and without momentum (Figure 4), and with different start conditions (Figure 5), which all influence the regions where the model degrades, but not the stable regions predicted by our theory.\"}"}
{"id": "1oEvY1a67c1", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3: Entropy minimization (top) Training two point model with momentum 0.9 and different learning rates\\n\\nFigure 4: Training a two point model without momentum and different learning rates with initialization $w_s = w_t = [0.5, 0.5]$. Note that especially for lower learning rates, longer training would increase the size of the collapsed region.\\n\\nFigure 5: Training a two point model with momentum 0.9 and different learning rates with initialization $w_s = w_t = [0.6, 0.3]$. \\n\\n$\\\\log_{10} \\\\tau_s$ $\\\\log_{10} \\\\tau_t$ $\\\\log_{10} \\\\tau_t = 2 \\\\log_{10} \\\\tau_s$ $\\\\log_{10} \\\\tau_t = \\\\log_{10} \\\\tau_s$ $\\\\log_{10} \\\\tau_t$\"}"}
{"id": "1oEvY1a67c1", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Additional Information on Used Models\\n\\nB.1 Details on All Hyperparameters We Tested for Different Models\\n\\nFor all models except EfficientNet-L2, we adapt the batch norm statistics to the test domains following (Schneider et al., 2020). We do not expect significant gains for combining EfficientNet-L2 with batch norm adaptation: as demonstrated in (Schneider et al., 2020), models trained with large amounts of weakly labeled data do not seem to benefit from batch norm adaptation.\\n\\n**ResNet50 models**\\n\\nWe use a vanilla ResNet50 model and compare soft- and hard-labeling against entropy minimization and robust pseudo-labeling. To find optimal hyperparameters for all methods, we perform an extensive evaluation and test (i) three different adaptation mechanisms (ii) several learning rates $1.0 \\\\times 10^{-4}$, $1.0 \\\\times 10^{-3}$, $1.0 \\\\times 10^{-2}$ and $5.0 \\\\times 10^{-2}$, (iii) the number of training epochs and (iv) updating the teacher after each epoch or each iteration. For all experiments, we use a batch size of 128. The hyperparameter search is performed on IN-C dev. We then use the optimal hyperparameters to evaluate the methods on the IN-C test set.\\n\\n**ResNeXt101 models**\\n\\nThe ResNeXt101 model is considerably larger than the ResNet50 model and we therefore limit the number of ablation studies we perform for this architecture. Besides a baseline, we include a state-of-the-art robust version trained with DeepAugment+Augmix (DAug+AM, Hendrycks et al., 2020a) and a version that was trained on 3.5 billion weakly labeled images (IG-3.5B, Mahajan et al., 2018). We only test the two leading methods on the ResNeXt101 models (ENT and RPL). We vary the learning rate in the same interval as for the ResNet50 model but scale it down linearly to account for the smaller batch size of 32. We only train the affine batch normalization parameters because adapting only these parameters leads to the best results on ResNet50 and is much more resource efficient than adapting all model parameters. Again, the hyperparameter search is performed only on the development corruptions of IN-C. We then use the optimal hyperparameters to evaluate the methods on the IN-C test set.\\n\\n**EfficientNet-L2 models**\\n\\nThe current state of the art on IN, IN-C, IN-R and IN-A is an EfficientNet-L2 trained on 300 million images from JFT-300M (Chollet, 2017; Hinton et al., 2014) using a noisy student-teacher protocol (Xie et al., 2020a). We adapt this model for only one epoch due to resource constraints. During the hyperparameter search, we only evaluate three corruptions on the IN-C development set and test the learning rates $4.6 \\\\times 10^{-2}$, $4.6 \\\\times 10^{-3}$, $4.6 \\\\times 10^{-4}$ and $4.6 \\\\times 10^{-5}$. We use the optimal hyperparameters to evaluate ENT and RPL on the full IN-C test set (with all severity levels).\\n\\n**UDA-SS models**\\n\\nWe trained the models using the scripts from the official code base at github.com/yueatsprograms/uda-release. We used the provided scripts for the cases: (a) source: CIFAR10, target: STL10 and (b) source: MNIST, target: MNIST-M. For the case (c) source: CIFAR10, target: CIFAR10-C, we used the hyperparameters from case (a) since this case seemed to be the closest match to the new setting. We think that the baseline performance of the UDA-SS models can be further improved with hyperparameter tuning.\\n\\n**DANN models**\\n\\nTo train models with the DANN-method, we used the PyTorch implementation of this paper at https://github.com/fungtion/DANNpy3. The code base only provides scripts and hyperparameters for the case (b) source: MNIST, target: MNIST-M. For the cases (a) and (c), we used the same optimizer and trained the model for 100 epochs. We think that the baseline performance of the DANN models can be further improved with hyperparameter tuning.\\n\\n**Preprocessing**\\n\\nFor IN, IN-R, IN-A and IN-D, we resize all images to $256 \\\\times 256$ px and take the center $224 \\\\times 224$ px crop. The IN-C images are already rescaled and cropped. We center and re-scale the color values with $\\\\mu_{RGB} = [0.485, 0.456, 0.406]$ and $\\\\sigma_{RGB} = [0.229, 0.224, 0.225]$. For the EfficientNet-L2, we follow the procedure in Xie et al. (2020a) and rescale all inputs to a resolution of $507 \\\\times 507$ px and then center-crop them to $475 \\\\times 475$ px.\\n\\nWe compare the results of computing the dev set on the 1, 3 and 5 severities versus the 1, 2, 3, 4 and 5 severities on our ResNeXt101 model in the Supplementary material.\"}"}
