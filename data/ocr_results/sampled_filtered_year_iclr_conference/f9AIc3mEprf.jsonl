{"id": "f9AIc3mEprf", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Being uncertain when facing the unknown is key to intelligent decision making. However, machine learning algorithms lack reliable estimates about their predictive uncertainty. This leads to wrong and overly-confident decisions when encountering classes unseen during training. Despite the importance of equipping classifiers with uncertainty estimates ready for the real world, most prior work focuses on small datasets with little or no class discrepancy between training and testing data. To close this gap, we introduce UIMNET: a realistic, ImageNet-scale test-bed to evaluate predictive uncertainty estimates for deep image classifiers. Our benchmark provides implementations of ten state-of-the-art algorithms, six uncertainty measures, four in-domain metrics, three out-domain metrics, and a fully automated pipeline to train, calibrate, ensemble, select, and evaluate models. Our test-bed is open-source and all of our results are reproducible from a fixed commit in our repository. Adding new datasets, algorithms, measures, or metrics is a matter of a few lines of code\u2014in so hoping that UIMNET becomes a stepping stone towards realistic, rigorous, and reproducible research in uncertainty estimation.\\n\\nOur experimental results reveal that, in order to obtain the best possible uncertainty estimates in large-scale image classification, the practitioner should favor large, calibrated models. We recommend the use of single ERM models, single MIMO models, or ensembles of ERM models, in order of increasing performance and required computational budget.\\n\\nINTRODUCTION\\n\\nI don't think I've ever seen anything quite like this before\u2014HAL 9000 in 2001: A Space Odyssey\\n\\nDeep image classifiers exceed at discriminating the set of in-domain classes observed during training. However, when confronting test examples from unseen out-domain classes, these classifiers can only predict in terms of known categories, leading to wrong and overly-confident decisions (Hein et al., 2019; Ulmer & Cina, 2020). In short, machine learning systems are unaware of their own limits, since \u201cthey do not know what they do not know\u201d. Since out-domain data cannot be safely identified and treated accordingly, it is reasonable to fear that, when deployed in-the-wild, the safety and performance of these classifiers crumbles by leaps and bounds (Ovadia et al., 2019). The inability of machine learning systems to estimate their uncertainty and abstaining to classify out-domain classes roadblocks their use in critical applications. These include self-driving (Michelmore et al., 2018), medicine (Begoli et al., 2019), and the analysis of satellite imagery (Wadoux, 2019). Good uncertainty estimates are also a key ingredient in anomaly detection (Chalapathy & Chawla, 2019), active learning (Settles, 2009), safe reinforcement learning (Henaff et al., 2019), defending against adversarial examples (Goodfellow et al., 2014), and model interpretability (Alvarez-Melis & Jaakkola, 2017). For an extensive literature review on uncertainty estimation and its applications, we refer the curious reader to the surveys of Abdar et al. (2020) and Ruff et al. (2021). Despite a research effort spanning multiple decades, machine learning systems still lack trustworthy estimates of their predictive uncertainty. In our view, one hindrance to this research program is the absence of realistic benchmarking and evaluation protocols. More specifically, prior attempts are limited in two fundamental ways. First, these experiment on small datasets such as SVHN and CIFAR-10 (van Amersfoort et al., 2021). Second, these do not provide a challenging set of out-domain data. Instead, they construct out-domain classes by using a second dataset (e.g., using MNIST in-domain versus FashionMNIST out-domain, cf. Van Amersfoort et al. (2020)) or by perturbing the in-domain classes.\"}"}
{"id": "f9AIc3mEprf", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This work does not involve the release of any new data, or the development of any new algorithm. However, we believe that the development of better uncertainty estimates for our predictive models is at the core of responsible, fair machine learning. As such, we hope that the UIMNET baseline is a stepping stone towards advancing the ability of our systems to say \\\"I don't know\\\". Also, we hope that our manuscript and software aids future research to study uncertainty estimates as components also vulnerable to attacks and with their own set of limitations.\\n\\nA main focus of this manuscript is reproducibility. To replicate all of our experimental results, obtain our code from the Supplementary Material and follow the instructions at README.md. Upon publication, our code will be open-sourced in a GitHub repository, and all of our experimental results will be replicable from a fixed commit hash.\\n\\nReferences\\n\\nMoloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Abbas Khosravi, U Rajendra Acharya, Vladimir Makarenkov, et al. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. arXiv, 2020.\\n\\nDavid Alvarez-Melis and Tommi S Jaakkola. A causal framework for explaining the predictions of black-box sequence-to-sequence models. arXiv, 2017.\\n\\nArsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls of in-domain uncertainty estimation and ensembling in deep learning. arXiv, 2020.\\n\\nEdmon Begoli, Tanmoy Bhattacharya, and Dimitri Kusnezov. The need for uncertainty quantification in machine-assisted medical decision making. Nature Machine Intelligence, 2019.\\n\\nJames Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of machine learning research, 13(2), 2012.\\n\\nLeon Bottou. Stochastic gradient descent tricks. In Neural networks: Tricks of the trade, pp. 421\u2013436. Springer, 2012.\\n\\nDavid S Broomhead and David Lowe. Radial basis functions, multi-variable functional interpolation and adaptive networks. Technical report, Royal Signals and Radar Establishment Malvern (United Kingdom), 1988.\\n\\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv, 2018.\\n\\nRaghavendra Chalapathy and Sanjay Chawla. Deep learning for anomaly detection: A survey. arXiv, 2019.\\n\\nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In ICML, 2016.\\n\\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv, 2014.\\n\\nHenry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree. Regularisation of neural networks by enforcing lipschitz continuity. Machine Learning, 110(2):393\u2013416, 2021.\\n\\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv, 2020.\\n\\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In ICML, 2017.\"}"}
{"id": "f9AIc3mEprf", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nMarton Havasi, Rodolphe Jenatton, Stanislav Fort, Jeremiah Zhe Liu, Jasper Snoek, Balaji Lakshminarayanan, Andrew Mingbo Dai, and Dustin Tran. Training independent subnetworks for robust prediction. In ICLR, 2021.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\nMatthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem. In CVPR, 2019.\\n\\nMikael Henaff, Alfredo Canziani, and Yann LeCun. Model-predictive policy learning with uncertainty regularization for driving in dense traffic. arXiv, 2019.\\n\\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv, 2019.\\n\\nDan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv, 2016.\\n\\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15262\u201315271, 2021.\\n\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv, 2015.\\n\\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. NeurIPS, 2016.\\n\\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 2015.\\n\\nJeremiah Zhe Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss, and Balaji Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. arXiv, 2020.\\n\\nRhiannon Michelmore, Marta Kwiatkowska, and Yarin Gal. Evaluating uncertainty quantification in end-to-end autonomous driving control. arXiv, 2018.\\n\\nTakeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv, 2018.\\n\\nJishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip HS Torr, and Yarin Gal. Deterministic neural networks with appropriate inductive biases capture epistemic and aleatoric uncertainty. arXiv, 2021.\\n\\nRoman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv, 2018.\\n\\nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua V Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model\u2019s uncertainty? evaluating predictive uncertainty under dataset shift. arXiv, 2019.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. arXiv, 2019.\\n\\nJohn Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers, 1999.\\n\\nJoaquin Quinonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate gaussian process regression. The Journal of Machine Learning Research, 2005.\"}"}
{"id": "f9AIc3mEprf", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "f9AIc3mEprf", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Datasets Algorithms Uncertainty In-Domain Out-Domain Ablations\\n\\nTable 1: The UIMNET test-bed suite for uncertainty estimation.\\n\\nFormal setup\\nWe learn classifiers $f$ using in-domain data from some distribution $P$ in $(X, Y)$. After training, we endow the classifier with a real-valued uncertainty measure $u(f, x^\\\\dagger)$. Given a test example $(x^\\\\dagger, y^\\\\dagger) \\\\sim P$ with unobserved label $y^\\\\dagger$, we declare $x^\\\\dagger$ in-domain (hypothesizing $P = P_{\\\\text{in}}$) if $u(f, x^\\\\dagger)$ is small, whereas we declare $x^\\\\dagger$ out-domain (hypothesizing $P \\\\neq P_{\\\\text{in}}$) if $u(f, x^\\\\dagger)$ is large.\\n\\nUsing these tools, our goal is to abstain from classifying out-domain test examples, and to classify with calibrated probabilities in-domain test examples. The sequel assumes that the difference between in- and out-domain resides in that the two groups of data concern disjoint classes.\\n\\nContributions\\nWe introduce UIMNET, a test-bed for large-scale, realistic evaluation of uncertainty estimates in deep image classifiers. We build UIMNET as follows (see also Table 1).\\n\\n(Sec. 2) We construct ImageNot, a perceptual partition of ImageNet into in-domain and out-domain classes. Unlike most prior work focusing on small datasets like SVHN and CIFAR-10, ImageNot provides a benchmark for uncertainty estimators at a much larger scale. Moreover, both in-domain and out-domain categories in ImageNot originate from the original ImageNet dataset. This provides realistic out-domain data, as opposed to prior work relying on a second dataset (e.g., MNIST as in-domain versus SVHN as out-domain), or handcrafted perturbations of in-domain classes (Gaussian noise or blur as out-domain).\\n\\n(Sec. 3) We re-implement eight state-of-the-art algorithms from scratch, listed in Table 1. This allows a fair comparison under the exact same experimental conditions (training/validation splits, hyper-parameter search, neural network architectures and random initializations). Furthermore, we also study ensembles of multiple training instances for each algorithm.\\n\\n(Sec. 4) Each algorithm can be endowed with one out of six possible uncertainty measures, allowing an exhaustive study of what algorithms play well with what measures. Listed in Table 1, these are the largest softmax score, the gap between the two largest softmax scores, the softmax entropy, the norm of the Jacobian, a per-class Gaussian density model, and (for those available) an algorithm-specific measure.\\n\\n(Sec. 5) For each classifier-measure pair, we study four in-domain metrics (top-1 and top-5 classification accuracy, log-likelihood, expected calibration error) and three out-domain metrics (the AUC at classifying in-domain versus out-domain samples using the selected...\"}"}
{"id": "f9AIc3mEprf", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We explore three popular ablations to understand the impact of model calibration by temperature scaling, model size, and the use of spectral normalization.\\n\\nUIMNET is entirely hands-off, since the pipeline from zero to \\\\( L^A T^E \\\\) tables is fully automated: this includes hyper-parameter search, model calibration, model ensembling, and the production of all the tables included in our experimental results.\\n\\nOur experimental results reveal that, in order to obtain the best possible uncertainty estimates in large-scale image classification, the practitioner should favor large, calibrated models. We recommend the use of single ERM models, single MIMO models, or ensembles of ERM models, in order of increasing performance and required computational budget.\\n\\nUIMNET is open sourced at \\\\( \\\\text{https://github.com/ANONYMOUS} \\\\). All of the tables presented in this paper are reproducible by running the main script in the repository at commit \\\\( 0xANON \\\\).\\n\\n**CONSTRUCTING THE IMAGE NOT BENCHMARK**\\n\\nThe ImageNet dataset (Russakovsky et al., 2015) is a gold standard to conduct research in computer vision pertaining image data of 1000 different classes. Here we use the ImageNet dataset to derive ImageNot, a large-scale and realistic benchmark for uncertainty estimation. ImageNot partitions the 1000 classes of the original ImageNet dataset into in-domain classes (used to train and evaluate algorithms in-distribution) and out-domain classes (used to evaluate algorithms out-of-distribution).\\n\\nTo partition ImageNet into in-domain and out-domain, we featurize the entire dataset to understand the perceptual similarity between classes. To this end, we use a pre-trained ResNet-18 (He et al., 2016) to compute the average last-layer representation for each of the classes. Next, we use agglomerative hierarchical clustering Ward Jr (1963) to construct a tree describing the perceptual similarities between the 1000 average feature vectors. Such perceptual tree has 1000 leafs, each of them being a cluster containing one of the classes. During each step of the iterative agglomerative clustering algorithm the two closest clusters are merged, where the distance between two clusters is computed using the criterion of Ward Jr (1963). The algorithm halts when there are only two clusters left to merge, forming the root node of the tree.\\n\\nAt this point, we declare the 266 classes to the left of the root as in-domain, and the first 266 classes to the right of the root as out-domain. In the sequel, we call \u201ctraining set\u201d and \u201cvalidation set\u201d to a 90/10 random split from the original ImageNet \u201ctrain\u201d set. We call \u201ctesting set\u201d to the original ImageNet \u201cval\u201d split. The exact in-/out-domain class partition as well as the considered train/validation splits are specified in Appendix D. Our agglomerative clustering procedure ended-up congregating different types of objects as in-domain classes, while grouping animals as out-domain classes.\\n\\nWhile inspired by the BREEDS ImageNet splits (Santurkar et al., 2020), our benchmark ImageNot is conceived to tackle a different problem. The aim of the BREEDS dataset is to classify ImageNet into a small number of super-classes, each of them containing a number of perceptually-similar sub-classes. The BREEDS training and testing distributions differ on the sub-class proportions contributing to their super-classes. Since the BREEDS task is to classify super-classes, the set of labels remains constant from training to testing conditions. This is in contrast to ImageNot, where the algorithm observes only in-domain classes during training, but both in-domain and out-domain classes during evaluation. While BREEDS studies the important problem of domain generalization (Gulrajani & Lopez-Paz, 2020), where there is always a right prediction to make within the in-domain classes during evaluation, here we focus on measuring uncertainty and abstaining from predicting about those out-domain classes unavailable during training.\\n\\nImageNot is also similar to the ImageNet-O dataset of (Hendrycks et al., 2021). However, their out-domain images are collected adversarially, that is, to maximize the prediction confidence of ResNet50 classifiers. We believe that this drastic change in selection bias from ImageNet to ImageNet-O may result in an unnecessarily difficult uncertainty estimation benchmark. Thus, here we favor using only the original ImageNet data, as described above. In contrast, the starting point for both in-domain and...\"}"}
{"id": "f9AIc3mEprf", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"out-domain classes of our ImageNet is the ImageNet dataset, and thus should maximally overlap in terms of image statistics, leading to a challenging and realistic benchmark.\\n\\n**3A** LGORITHMS\\n\\nWe benchmark ten supervised learning algorithms commonly applied to tasks involving uncertainty estimation. Each algorithm consumes one in-domain training set of image-label pairs \\\\( \\\\{ (x_i, y_i) \\\\}_{i=1}^n \\\\) and returns a predictor \\\\( f(x) = w(h(x)) \\\\), composed by a featurizer \\\\( h : \\\\mathbb{R}^{3\\times224\\times224} \\\\rightarrow \\\\mathbb{R}^k \\\\) and a classifier \\\\( w : \\\\mathbb{R}^k \\\\rightarrow \\\\mathbb{R}^C \\\\). We consider predictors implemented using deep convolutional neural networks (LeCun et al., 2015).\\n\\nGiven an input image \\\\( x^* \\\\), all predictors return a softmax vector \\\\( f(x^*) = (f_c)_{c=1}^C \\\\) over \\\\( C \\\\) classes. The considered algorithms are:\\n\\n- **Empirical Risk Minimization**, or ERM (Vapnik, 1992), or vanilla training.\\n- **Mixup** (Zhang et al., 2017) chooses a predictor minimizing the empirical risk on mixed examples \\\\( (\\\\alpha x_i + (1-\\\\alpha) x_j, \\\\alpha y_i + (1-\\\\alpha) y_j) \\\\), where \\\\( \\\\alpha \\\\sim \\\\text{Beta}(\\\\alpha, \\\\alpha) \\\\), \\\\( \\\\alpha \\\\) is a mixing parameter, and \\\\((x_i, y_i), (x_j, y_j)\\\\) is a random pair of training examples. Mixup improves generalization performance (Zhang et al., 2017) and calibration (Thulasidasan et al., 2019).\\n- **Random Network Distillation**, or RND (Burda et al., 2018), finds an ERM predictor \\\\( f(x) = w(h(x)) \\\\), but simultaneously trains an auxiliary classifier \\\\( w_{\\\\text{student}} \\\\) to minimize \\\\( k_{w_{\\\\text{student}}(h(x)) - w_{\\\\text{teacher}}(h(x))}^2 \\\\), where \\\\( w_{\\\\text{teacher}} \\\\) is a fixed classifier with random weights. RND has shown good performance as a tool for exploration in reinforcement learning.\\n- **Orthogonal Certificates**, or OC (Tagasovska & Lopez-Paz, 2018), is analogous to RND for \\\\( w_{\\\\text{teacher}}(h(x)) = \\\\sim 0_k \\\\) for all \\\\( x \\\\). That is, the goal of \\\\( w_{\\\\text{student}} \\\\) is to map all the in-domain training examples to zero in \\\\( k \\\\) different ways (or certificates). To ensure diverse and non-trivial certificates, we regularize each weight matrix \\\\( W \\\\) of \\\\( w_{\\\\text{student}} \\\\) to be orthogonal by adding a regularization term \\\\( \\\\| W \\\\|_2^2 \\\\). OCs have shown good performance at the task of estimating uncertainty across a variety of classification tasks.\\n- **Autoencoder**, or DeepAE (Vincent et al., 2010), is analogous to RND for \\\\( w_{\\\\text{teacher}}(h(x)) = x \\\\) and a \\\\( w_{\\\\text{student}} \\\\) with a bottleneck.\\n- **MC-Dropout** (Gal & Ghahramani, 2016) trains ERMs with one or more dropout layers (Srivastava et al., 2014). These stochastic dropout layers remain active at test time, allowing the predictor to produce multiple softmax vectors \\\\( \\\\{ f(x^*, \\\\text{dropout}_t) \\\\}_{t=1}^T \\\\) for each test example \\\\( x^* \\\\). Here, \\\\( \\\\text{dropout}_t \\\\) is a random dropout mask sampled anew. MCDropout is one of the most popular baselines to estimate uncertainty.\\n- **MIMO** (Havasi et al., 2021) is a variant of ERM over predictors accepting \\\\( T \\\\) images and producing \\\\( T \\\\) softmax vectors. For example, MIMO with \\\\( T = 3 \\\\) is trained to predict jointly the label vector \\\\( (y_i, y_j, y_k) \\\\) using a predictor \\\\( h(x_i, x_j, x_k) \\\\), where \\\\((x_t, y_t)_{t=1}^3\\\\) is a random triplet of training examples. Given a test point \\\\( x^* \\\\), we form predictions by replicating and averaging, that is \\\\( f(x^*) = \\\\frac{1}{3} \\\\sum_{t=1}^T h(x^*, x^*, x^*) \\\\).\\n- **Radial Basis Function**, or RBF (Broomhead & Lowe, 1988), is a variant of ERM where we transform the logit vector \\\\( z \\\\rightarrow e^z \\\\) before passing them to the final softmax layer. In such a way, as the logit norm \\\\( \\\\| z \\\\|_1 \\\\), the predicted softmax vector tends to the maximum entropy solution \\\\( (\\\\frac{1}{C})_{c=1}^C \\\\), signaling high uncertainty far away from the training data. RBFs have been proposed as defense to adversarial examples (Goodfellow et al., 2014), but they remain under-explored given the difficulties involved in their training.\\n- **Soft labeler** (Hinton et al., 2015; Szegedy et al., 2016) is a variant of ERM where the one-hot vector labels \\\\( y_i \\\\) are smoothed such that every zero becomes \\\\( \\\\min > 0 \\\\) and the solitary one becomes \\\\( \\\\max < 1 \\\\). Using soft labels, we can identify softmax vectors with entries exceeding \\\\( \\\\max \\\\) as \u201cover-shoots\u201d, and regard them as uncertain predictions.\\n- **DUE** (van Amersfoort et al., 2021) enforces the smoothness of the featurizer using spectral normalization (Miyato et al., 2018), implements the classifier \\\\( w \\\\) as a sparse Gaussian Process (Quinonero-Candela & Rasmussen, 2005), and trains the resulting predictor using variational inference (Titsias, 2009). Gaussian processes are considered one of the main tools to estimate predictive uncertainty in machine learning systems.\\n\\n**Ensembles of predictors**\\n\\nWe also consider ensembles of predictors trained by each of the algorithms above. Ensembles are commonly regarded as the state-of-the-art in uncertainty estimation (Lakshminarayanan et al., 2016). In particular, and for each algorithm, we construct bagging ensembles.\"}"}
{"id": "f9AIc3mEprf", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We equip a trained predictor $f$ with six different uncertainty measures. An uncertainty measure is a real-valued function $u(f, x^\\\\dagger)$ designed to return small values for in-domain instances $x^\\\\dagger$, and large values for out-domain instances $x^\\\\dagger$. To describe the different measures, let $\\\\{s(1), \\\\ldots, s(C)\\\\}$ be the softmax scores returned by $f(x^\\\\dagger)$ sorted in decreasing order.\\n\\n- **Largest** (Hendrycks & Gimpel, 2016) returns minus the largest softmax score, $-s(1)$.\\n- **Softmax gap** (Tagasovska & Lopez-Paz, 2018) returns $s(2) - s(1)$.\\n- **Entropy** (Shannon, 1948) returns $\\\\sum_{c=1}^{C} s(c) \\\\log_{\\\\text{base} 2} (1)$.\\n- **Norm of the Jacobian** (Novak et al., 2018) returns $\\\\|x^\\\\dagger f\\\\|_2$.\\n- **GMM** (Mukhoti et al., 2021) estimates one Gaussian density $N(\\\\mathbf{x}; \\\\mu_c, \\\\Sigma_c)$ per-class, on top of the feature vectors $(x)$ collected from a in-domain validation set. Given a test example $x^\\\\dagger$, return $\\\\sum_{c=1}^{C} c \\\\cdot N(\\\\mathbf{x}^\\\\dagger; \\\\mu_c, \\\\Sigma_c)$, where $c$ is the proportion of in-domain validation examples from class $c$.\\n- **Test-time augmentation** (Ashukha et al., 2020) returns $\\\\max_c (\\\\frac{1}{A} \\\\sum_{a=1}^{A} f(x^\\\\dagger_a))$. This is the measure \\\"Largest\\\" about the average prediction over $A$ random data augmentations $\\\\{x^\\\\dagger_a\\\\}_{a=1}^{A}$ of the test instance $x^\\\\dagger$.\\n\\nAdditionally, some algorithms provide their native uncertainty measures, outlined below.\\n\\n- **For Mixup**, we return $\\\\frac{1}{K} \\\\sum_{k=1}^{K} k \\\\cdot f(x^\\\\dagger) + (1 - \\\\frac{1}{K}) \\\\cdot \\\\bar{y}_k f(x^\\\\dagger) + (1 - \\\\frac{1}{K}) \\\\cdot \\\\bar{x}_k$, where $\\\\sim \\\\text{Beta}(\\\\alpha, \\\\beta)$, and $(\\\\bar{x}_k, \\\\bar{y}_k)$ is an example saved from the training set. This measures if the test example $x^\\\\dagger$ violates the Mixup criterion wrt the training dataset average.\\n- **For RND, OC, and DeepAE**, we return $\\\\frac{1}{K} \\\\sum_{k=1}^{K} \\\\text{softmax}(x^\\\\dagger) - \\\\text{softmax}(x^\\\\dagger)_k$, that is, we consider a prediction uncertain if the outputs of the student and teacher disagree. We expect this disagreement to be related predictive uncertainty, as the student did not observe the behaviour of the teacher at out-domain instances $x^\\\\dagger$.\\n- **For Soft labeler**, we return $(s(1) - \\\\max_2)$. This measures the discrepancy between the largest softmax and the positive soft label target, able to signal overly-confident predictions.\\n- **For MC-Dropout and Ensembles**, and following (Lakshminarayanan et al., 2016), we return the Jensen-Shannon divergence between the $K$ members (or stochastic forward passes) $f_1, \\\\ldots, f_K$ of the ensemble, $H(1) \\\\sum_{k=1}^{K} f_k(x^\\\\dagger) \\\\cdot H(f_k(x^\\\\dagger))$.\\n- **For DUE**, we return the predictive variance of the Gaussian process classifier.\\n\\nFor each algorithm-measure pair, we evaluate several metrics both in-domain and out-domain. Following (Havasi et al., 2021), we implement four in-domain metrics to assess the performance and calibration of predictors when facing in-domain test examples.\\n\\n- **Top-1** and **Top-5** classification accuracy (Russakovsky et al., 2015).\\n- **Expected Calibration Error** or ECE (Guo et al., 2017): $1 \\\\sum_{b=1}^{B} \\\\frac{|B_b|}{n} |\\\\text{acc}(f, B_b) - \\\\text{conf}(f, B_b)|$, where $B_b$ contains the examples where the algorithm predicts a softmax score of $b$. The functions acc and conf compute the average classification accuracy and largest softmax score of $f$ over $B_b$. ECE is minimized when $f$ is calibrated, that is, $f$ is wrong $p\\\\%$ of the times it predicts a largest softmax score $p$.\\n- **Negative Log Likelihood (NLL)** Also known as the cross-entropy loss, this is the objective minimized during the training process of the algorithms.\"}"}
{"id": "f9AIc3mEprf", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Area Under the Curve, or AUC (Tagasovska & Lopez-Paz, 2018), describes how well the predictor-measure pair distinguishes between in-domain and out-domain examples over all thresholds of the uncertainty measure.\\n\\nConfusion matrix at fixed threshold. To reject out-domain examples in real scenarios, one must fix a threshold $\u2713$ for the selected uncertainty measure. We do so by computing the 95% quantile of the uncertainty measure, computed over an in-domain validation set. Then, at testing time, we declare one example out-domain if the uncertainty measure exceeds $\u2713$.\\n\\nTo understand where does the uncertainty measure hit or miss, we monitor the metrics $\\\\text{InAsIn}$ (percentage of in-domain examples classified as in-domain) and $\\\\text{OutAsOut}$ (percentage of in-domain examples classified as out-domain).\\n\\nWe execute our entire test-bed under three popular ablations.\\n\\n- We study the effect of calibration by temperature scaling (Platt et al., 1999). To this end, we introduce a temperature scaling $\\\\tau > 0$ before the softmax layer, resulting in predictions $\\\\text{Softmax}(\\\\tau z)$ about the logit vector $z$. We estimate the optimal temperature $\\\\hat{\\\\tau}$ by minimizing the $\\\\text{NLL}$ of the predictor across an in-domain validation set. We evaluate all metrics for both the un-calibrated ($\\\\tau = 1$) and calibrated ($\\\\tau = \\\\hat{\\\\tau}$) predictors. According to previous literature (Guo et al., 2017), calibrated models provide better in-domain uncertainty estimates.\\n\\n- We analyze the impact of spectral normalization to control the behavior of the featurizer. More specifically, recent works (Liu et al., 2020; Van Amersfoort et al., 2020; van Amersfoort et al., 2021; Mukhoti et al., 2021) have highlighted the importance of controlling both the smoothness and sensitivity of the feature extraction process to avoid feature collapse and achieve high-quality uncertainty estimates. On the one hand, enforcing smoothness upper bounds the Lipschitz constant of $\\\\|\\\\cdot\\\\|_\\\\infty$, limiting its reaction to changes in the input. Smoothness is often enforced by normalizing each weight matrix in by its spectral norm (Miyato et al., 2018). On the other hand, enforcing sensitivity lower bounds the Lipschitz constant of $\\\\|\\\\cdot\\\\|_\\\\infty$, ensuring that the feature space reacts in some amount when the input changes. Sensitivity is often enforced by the residual connections of the hereby used ResNet models (He et al., 2016). We apply one-sided spectral normalization, with a target spectral norm of 5, to the weights and the batch normalization layers following Miyato et al. (2018) and Gouk et al. (2021) respectively.\\n\\n- We analyze the impact of the model size (ResNet-18 versus ResNet-50).\\n\\nWe now conduct experiments on the ImageNot benchmark (Section 2) for all algorithms (Section 3) and measures (Section 4), wrt all metrics (Section 5) and ablations (Section 6).\\n\\nEmpirical oracle upper bounds. To have a measure of the maximally achievable separation of the in-domain and out-of-domain partitions, we train a ResNet-18 classifier to discriminate between the In-domain classes and the out-of-domain partition. The oracle performance is $0.985 \\\\pm 0.002$, $0.944 \\\\pm 0.001$ and $0.947 \\\\pm 0.002$ for the AUC, $\\\\text{InAsIn}$ and $\\\\text{OutAsOut}$ metrics respectively.\\n\\nHyper-parameter search. We train each algorithm using (i) ResNet-18 or ResNet-50 architectures, (ii) spectral normalization or not, (iii) ten hyper-parameter trials, and (iv) three random train/validation splits of the in-domain data (data seeds). We opt for a random hyper-parameter search (Bergstra & Bengio, 2012), where the search grid for each algorithm is detailed in Appendix C. More specifically, while the first trial uses the default hyper-parameter configuration suggested by the authors of each algorithm, the additional four trials explore random hyper-parameters.\\n\\nModel selection. After training all instances of a given algorithm, we report the test average and standard deviation (over data seeds) of all metrics. We report these metrics for (a) the best model.\"}"}
{"id": "f9AIc3mEprf", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\n(a) The ensemble containing the best single model ($k=1$), and (b) the ensemble containing the best five models ($k=5$) in terms of the validation average (over data seeds) negative log-likelihood.\\n\\nOptimization. We use PyTorch (Paszke et al., 2019) and SGD (Bottou, 2012) for 100 epochs using mini-batches of 256 examples distributed over 8 NVIDIA Tesla V100 GPUs, and a learning rate decaying by a factor of 10 every 30 epochs.\\n\\nRESULTS\\n\\nThe full experimental results are available in Appendix A (in-domain) and Appendix B (out-domain).\\n\\nBelow, we summarize our findings.\\n\\nFrom the in-domain results summarized in Table 2, we identify the following key takeaways:\\n\\n\u2022 No single method out-performs ERM significantly on any in-domain metric, except MIMO on ECE (-30% ECE).\\n\u2022 No ensemble method outperforms ensembles of ERMs in any in-domain metric.\\n\u2022 Increasing model size is the most effective strategy to improve in-domain metrics (+5% ACC@1, +2% ACC@5, -17% NLL, -13% ECE).\\n\u2022 Ensembling is the second most effective strategy to improve in-domain metrics (+2.5% ACC@1, +1% ACC@5, -8.5% NLL, -8% ECE).\\n\u2022 Calibration is an effective strategy to improve test negative log-likelihood and expected calibration error (-2% NLL, -45% ECE).\\n\u2022 Spectral normalization has little effect to improve in-domain metrics (<1%).\\n\u2022 DUE has a brittle behavior, performing well only for default hyper-parameters and ResNet18 architectures.\\n\\nFrom the out-domain results summarized in Table 3, corresponding to the best performing uncertainty measure \u201cEntropy\u201d, we identify the following key takeaways:\\n\\n\u2022 No single method outperforms ERM significantly on any out-domain metric, except MIMO on OutAsOut (+5% OutAsOut).\\n\u2022 No ensemble method outperforms ensembles of ERMs in any out-domain metric.\\n\u2022 Increasing model size is the most effective strategy to improve out-domain metrics (+4% AUC, +35% OutAsOut).\\n\u2022 Ensembling is the second most effective strategy to improve out-domain metrics (+1% AUC, +4% OutAsOut).\\n\u2022 Calibration has a small negative effect on out-domain metrics (-1%).\\n\u2022 Spectral normalization has a small positive effect on out-domain metrics (<1%).\\n\u2022 All methods are able to upper-bound their type-I errors to the requested threshold of 5%.\\n\u2022 The best performing uncertainty measure is Entropy; then Largest, and Gap follow. The worst performing measures are Augmentations, Jacobian, and Native (Appendix B).\\n\\nOther results\\n\\nWe were unable to obtain competitive performances for the algorithm RBF (Broomhead & Lowe, 1988) and the measure GMM (Mukhoti et al., 2021). We believe that training RBFs at this large scale are challenging optimization problems that deserve further study in our community.\\n\\nFurthermore, the large number of classes in our study (266 ImageNet classes instead of the 10 CIFAR-10 classes often considered) poses a difficult problem for the density-based measures GMM.\\n\\nCONCLUSION\\n\\nTo obtain the best possible uncertainty estimates in large-scale image classification, we recommend the use of large and calibrated models. We favor single ERM models (low computational budget), single MIMO models (medium budget), or ensembles of ERM models (high budget).\\n\\nGraphs showing the relationship between validation set negative likelihood and out-of-domain measures on the test set are show in the appendix.\"}"}
{"id": "f9AIc3mEprf", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model | ImageNet1k | ImageNet2k | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet-2K | ImageNet-1K | ImageNet"}
{"id": "f9AIc3mEprf", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Out-domain results for ResNet50 using measure \\\"Entropy\\\"\\n\\n|            | ERM | Mixup |\\n|------------|-----|-------|\\n| True       | 0.01| 0.00  |\\n| False      | 0.00| 0.00  |\"}"}
