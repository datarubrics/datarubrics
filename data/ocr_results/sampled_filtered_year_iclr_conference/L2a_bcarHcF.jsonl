{"id": "L2a_bcarHcF", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.1 LEARNING CURVES FOR DIFFERENT ENCODINGS AND ARCHITECTURES\\n\\nFigure 2 presents learning curves for loss and accuracy (with 5% and 1% tolerance) on different models, for four problems. These curves indicate the number of training examples needed for each problem. On average, our best models learn basic operations on matrices in less than 50 epochs (15 million examples). Training size requirement increases with operation complexity: from 30 million for eigenvalues, to 120 million for eigenvectors, and over 150 million for matrix inversion.\\n\\nOn the inversion problem, we experiment with the number of attention heads in the encoder. Increasing the number of head from 8 to 10 and 12 improves learning speed and accuracy. Over 12 heads, this benefit disappears: with 16 heads, our models need 800 epochs to train to 55% accuracy (with 5% tolerance). We believe that this reflects the trade-off being the number of heads (more heads catch more dependencies between elements in the input sequence) and the downsampling of attention patterns (when internal model dimension remains fixed).\\n\\nFinally, we notice that the learning curves for the harder problems (eigenvalues, vectors and inversion) are noisy. This is caused by the learning rates: our models usually need small learning rates (10^{-4} before scheduling is typical) and there is a trade-off between low rates that will stabilize the learning curve, and larger rates that accelerate training.\\n\\nC.2 MODEL SIZE\\n\\nThe two main factors influencing model size are depth and the number of dimensions (see Appendix F). In this section we discuss how model size impacts accuracy for addition of 10\u00d710 matrices, multiplication of a 5\u00d75 matrix by a vector, and computation of the eigenvalues of a 5\u00d75 matrix. All models in this section are symmetric (same dimension and number of layers in the encoder and decoder) and have 8 attention heads.\\n\\nFor the addition task, tables 12 and 13 present the accuracy reached after 60 epochs (18 million examples) and the number of epochs (of 300,000 examples) needed to reach 95% accuracy, for models using the P1000 and B1999 encoding. Both encodings allow shallow architectures (1/1 and 2/2 layers) to learn addition with high accuracy, but the more compact B1999 support smaller models (256 dimensions). In terms of speed, with B1999, shallow models are learned very fast, but it takes a lot of examples to train deeper models. The opposite is true for P1000 models.\\n\\n| Dimension | 64   | 128  | 256  | 512  |\\n|-----------|------|------|------|------|\\n| 1/1 layers| 31   | 7    | 82   | 100  |\\n| 2/2 layers| 0    | 0    | 100  | 100  |\\n| 4/4 layers| 0    | 0    | 100  | 100  |\\n| 6/6 layers| 0    | 0    | 0    | 99   |\\n\\nTable 12: Accuracy of matrix addition for different model sizes. 10\u00d710 matrices, 60 epochs (18 millions examples). 5% tolerance.\\n\\n| Dimension | 64   | 128  | 256  | 512  |\\n|-----------|------|------|------|------|\\n| 1/1 layers| -    | -    | 76   | 15   |\\n| 2/2 layers| -    | -    | 26   | 6    |\\n| 4/4 layers| -    | -    | 70   | 63   |\\n| 6/6 layers| -    | -    | -    | 23   |\\n\\nTable 13: Learning speed of matrix addition for different model sizes. Number of epochs needed to reach 95% accuracy (with 5% tolerance). 1 epoch = 300,000 examples.\\n\\nTable 14 presents the learning speed of models of different sizes for the matrix/vector product and eigenvalue computation tasks (5\u00d75 matrices, and P1000 encoding). For each problem, there exist a\"}"}
{"id": "L2a_bcarHcF", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"minimal dimension and depth under which models struggle to learn: one layer and 128 dimensions.\\n\\nOver that limit, increasing the dimension accelerates learning. Increasing the depth, on the other hand,bring no clear improvement in speed or accuracy.\\n\\n| Layers | 1/1 | 2/2 | 4/4 | 6/6 | 8/8 |\\n|--------|-----|-----|-----|-----|-----|\\n|        | 128 | 256 | 512 | 1024|\\n|        |     |     |     |     |\\n| 1 epoch = 300,000 examples. |\\n\\nFinally, we experiment with larger models on larger problems. We trained models with 8 to 12 layers and 512 to 2048 dimensions on sets of 10x10 matrices, without success. As discussed in section 4.4, those problems are out of reach of the models we use in this paper (unless we use curriculum learning and train on mixed-size datasets). Increasing model size does not seem to help scaling to larger matrices.\\n\\nC.3 Model Performance on Different Datasets\\n\\n| Dataset | FP15 | P1000 |\\n|---------|------|-------|\\n| 4/1 layers | 99.6 100 | 99.8 100 |\\n| 6/1 layers | 99.8 100 | 99.8 100 |\\n| Wigner matrices (iid coefficients) | | |\\n| Uniform iid A=10 | 99.0 99.2 | 99.8 100 |\\n| Gaussian iid A=10 | 99.2 99.5 | 99.7 99.8 |\\n| Uniform iid A=1,100 | 99.0 99.2 | 99.8 100 |\\n| Gaussian iid A=1,1000 | 99.2 99.5 | 99.7 99.8 |\\n| Non Wigner | | |\\n| Positive A=10 | 12.7 100 | 100 100 |\\n| Uniform A=10 | 8.2 10.8 | 99.9 100 |\\n| Gaussian A=10 | 99.6 100 | 100 100 |\\n| Laplace A=10 | 99.4 99.9 | 99.9 99.9 |\\n| Gaussian+uniform+Laplace A=10 | 3.8 99.8 | 99.6 99.9 |\\n| Wigner and non Wigner mixtures | | |\\n| iid+gaussian A=10 | 99.5 99.9 | 98.0 99.7 |\\n| iid+positive A=10 | 99.8 99.9 | 99.8 99.8 |\\n| iid+Laplace A=10 | 99.6 99.8 | 99.6 99.5 |\\n| iid+positive+gaussian A=10 | 99.8 99.9 | 99.7 99.9 |\\n| iid+positive+Laplace A=10 | 99.0 99.8 | 99.6 99.8 |\\n\\nTable 15: In-distribution eigenvalue accuracy (tolerance 2%) for different training distributions. All models have 512 dimensions, and 8 attention heads, and are trained on 5x5 matrices.\"}"}
{"id": "L2a_bcarHcF", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"We experimented with two popular recurrent architectures: long short-term memories (Hochreiter & Schmidhuber, 1997), and gated recurrent units (Cho et al., 2014), on three tasks: addition of $5 \\\\times 5$ and $10 \\\\times 10$ matrices, eigenvalues and matrix inversion of $5 \\\\times 5$ matrices.\\n\\nWe experiment with sequence to sequence models, featuring an encoder and a decoder (LSTM or GRU), with 2 to 8 layers, and 1024 or 2048 hidden dimensions. The input and output sequences, encoded as in the rest of the paper, are pre-processed (and decoded) via an embedding layer with 256 or 512 dimensions.\\n\\nAddition, a very easy task for transformers (see section 4.2) proves difficult for LSTM and GRU. None of our models can learn addition of $10 \\\\times 10$ matrices. Some models can learn addition of $5 \\\\times 5$ matrices, but whereas transformers achieve 100% accuracy for all tolerances, our best LSTM and GRU only exceed 90% at 1% tolerance. GRU seem to perform better than LSTM on this task, and 2-layer models perform better than 4-layer models, but transformers have a distinct advantage over LSTM and GRU on addition.\\n\\n| Hidden dimension | 1024 | 2048 |\\n|------------------|------|------|\\n| Embedding dimension | 256 | 512 |\\n\\n| Task                | LSTM 5% | LSTM 2% | LSTM 1% | LSTM 0.5% | GRU 5% | GRU 2% | GRU 1% | GRU 0.5% |\\n|---------------------|---------|---------|---------|-----------|--------|--------|--------|----------|\\n| Addition            | 100     | 100     | 95      | 95        | 100    | 28     | 44     | 0        |\\n| Matrix inversion    | 100     | 100     | 100     | 100       | 100    | 100    | 100    | 100      |\\n\\nBoth LSTM and GRU can be trained to predict eigenvalues of $5 \\\\times 5$ matrices with the same accuracy as transformers, for the P1000 and FP15 encoding (table 17). Matrix inversion, on the other hand, cannot be learned. Overall, these experiments show that other sequence to sequence architectures, LSTM and GRU, can learn tasks like eigenvalues and addition of small matrices. However, they are less efficient on addition (in terms of precision and scaling to larger matrices) and fail on more complex tasks, like matrix inversion.\\n\\n| FP15 | P1000 |\\n|------|-------|\\n| Hidden dimension | 1024 | 2048 |\\n| Layers | 4 6 8 | 4 6 8 | 4 6 8 | 4 6 8 |\\n\\n| Task                | LSTM 5% | LSTM 2% | LSTM 1% | LSTM 0.5% | GRU 5% | GRU 2% | GRU 1% | GRU 0.5% |\\n|---------------------|---------|---------|---------|-----------|--------|--------|--------|----------|\\n| Eigenvalue          | 100     | 100     | 100     | 100       | 100    | 100    | 100    | 100      |\\n| Matrix inversion    | 100     | 100     | 100     | 100       | 100    | 100    | 100    | 100      |\\n\\nTable 16: Matrix addition with LSTM and GRU.\\n\\nTable 17: Eigenvalue computation with LSTM and GRU.\"}"}
{"id": "L2a_bcarHcF", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the Universal Transformer (Dehghani et al., 2018), the stacked layers of usual transformer implementations are replaced by one layer that is looped through a fixed number of times (feeding the output of one iteration into the input of the next). This amounts to sharing the weights of the different layers, therefore greatly reducing the number of parameters in the model. This technique can be applied to the encoder, the decoder or both. The number of iterations is a fixed hyperparameter, but the original paper also proposed a halting mechanism inspired by Adaptive Computation Time (Graves, 2016), to adaptively control loop length at the token level. In this version, a stopping probability is learned for every position in the input sequence, and once it reaches a certain threshold, the layer merely copies the input onto the output. The iteration stops when all positions have halted, or a specific value is reached. A recent paper (Anonymous, 2022) proposed to use a similar copy-gating mechanism to skip iterations in a fixed-length loop. We experiment with these three variants (fixed length, adaptive halting, copy gating) on the addition (of $10 \\\\times 10$ matrices), eigenvalue and matrix inversion tasks ($5 \\\\times 5$ matrices).\\n\\nFor the addition task, we train universal transformers with one layer and in the encoder and decoder, 256 or 512 dimensions and 8 attention heads. We use the B1999 encoding for the data. We experiment with looped encoder, looped decoder, and loop in both, a loop length of 4, copy-gating and ACT (the 4 loops in then a maximum number of iterations) and copy-gating. Table 18 summarizes our findings. Only models with encoder loops learn to add, and models with 512 dimensions learn with over 95% accuracy for all tolerances. Universal Transformers with one layer (looped-encoder only) perform as well as 2/2 transformers.\\n\\n|                      | Looped encoder | Looped decoder | Looped encoder and decoder | 2/2 transformer (baseline) |\\n|----------------------|----------------|----------------|----------------------------|----------------------------|\\n| 256 dimensions, 4 loops | 15 1 0 0       | 0 0            | 0 0                        | 100 100 100 100            |\\n| 512 dimensions, 4 loops | 100 100 100 100 | 0 0            | 0 0                        | 100 100 100 100            |\\n| 256 dimensions, 4 loops, gated | 97 66 41 29  | 0 0            | 0 0                        | 100 92 76 66              |\\n| 512 dimensions, 4 loops, gated | 100 100 100 100 | 0 0            | 0 0                        | 100 100 98 96             |\\n\\nTable 18: Accuracy of Universal transformers on matrix addition for different tolerances.\\n\\nOn the eigenvalue task, we experiment on the P1000 and FP15 encoding, with encoder-loop only 1/1 Universal Transformers with 4 or 8 loops. Universal transformers using the P1000 encoding achieve the same performances (with only one layer) than the transformers in our main research. 4 loop transformers seem best, with gates not improving performance and ACT slightly degrading it.\\n\\nWith the FP15 encoding, universal transformers become very difficult to train: only the 4 loop gated version achieves significant accuracy (still lower than the 6/1 transformers).\\n\\nFinally, we experimented with matrix inversion, with FP15/P1000 and P1000/P1000 encodings, and 4 or 8 loops in the encoder. A gated universal transformer using FP15 in the input and P1000 in the output achieved 73% accuracy, a significant result albeit lower than the best result achieved with 6/1 transformers using the same encodings (90%). With the P1000 encoding, the best universal transformers reach 55% accuracy, compared to 80% for their 6/1 transformer counterparts. Overall, Universal Transformers seem to achieve comparable performances with deep transformers (except on the inversion tasks), using less parameters. This makes shared layer transformers an interesting direction for future work.\"}"}
{"id": "L2a_bcarHcF", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 19: Accuracy of Universal transformers on eigenvalue computation for different tolerances.\\n\\nTable 20: Accuracy of noisy matrix addition for different error levels and tolerances.\"}"}
{"id": "L2a_bcarHcF", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A similar pattern appears in eigenvalue calculations (table 21), but trained models prove more resistant to noise in the data than for addition. For instance, the eigenvectors of matrices with error standard deviation up to 0.05\u03c3 can be learnt to high accuracy within 5% tolerance (vs 0.02\u03c3 for addition). As before, model size has no impact on robustness. However, FP15 models seem more difficult to train over noisy data than P1000.\\n\\n| FP15 | P1000 |\\n|------|-------|\\n| 4/4 layers | 6/6 layers |\\n| 512 | 1024 |\\n| 5% tolerance | \\n| 0.01\u03c3 error | 6.1 100 |\\n| 0.02\u03c3 | 5.1 6.0 100 100 |\\n| 0.05\u03c3 | 99.1 99.3 99.3 6.4 99.3 99.0 99.0 98.8 |\\n\\n| 2% tolerance | \\n| 0.01\u03c3 error | 0.7 99.8 |\\n| 0.02\u03c3 | 0.5 0.8 99.3 99.6 99.9 99.8 |\\n| 0.05\u03c3 | 37.9 38.4 38.4 40.6 0.5 40.1 37.3 37.5 35.3 |\\n\\n| 1% tolerance | \\n| 0.01\u03c3 error | 0.1 82.1 |\\n| 0.02\u03c3 | 0.1 0.2 79.7 83.8 87.9 83.8 |\\n| 0.05\u03c3 | 3.8 4.2 4.1 0.1 4.1 3.8 3.9 3.4 |\\n\\nTable 21: Accuracy of noisy eigenvalue computations, for different error levels and tolerances.\\n\\nE.2 CO-TRAINING\\n\\nWe have shown that transformers can be trained to performed all the tasks mentioned above, training one specific model for each task. In this section, we experiment with co-training: learning several tasks at once. We add a token at the beginning of the input and output sequence indicating the task to be solved (e.g. Transpose or Add), and generate data by randomly selecting a task (with equal probability for all tasks) and producing the corresponding pairs.\\n\\nWe train transformers with 4 or 6 layers, 512 dimensions and 8 attention heads on eight datasets corresponding to different co-training tasks:\\n\\n- Transpose and add (TA)\\n- Transpose, add and dot product (vector matrix multiplication) (TAD)\\n- Transpose, add, dot product and matrix multiplication (TADM)\\n- Transpose, add, dot product, matrix multiplication and eigenvalues (TADME)\\n- Transpose, add, dot product, matrix multiplication, eigenvalues and eigenvectors (TADMEF)\\n- Transpose, add, dot product, matrix multiplication, eigenvalues, eigenvectors and matrix inversion (TADMEFI)\\n- Eigenvalues, eigenvectors and matrix inversion (EFI)\\n\\nTable 22 summarizes our findings. Lines correspond to a co-training tasks, columns to the performance achieved on this specific task (with 5% tolerance). Co-training over a mixture of basic operations (transposition, addition, dot products and multiplication: the TA, TAD and TADM tasks) learn to predict the results of all operations with almost perfect accuracy. Co-training on the basic operations and eigenvalue computations (the TADME task) allows the model to predict eigenvalues with 80% accuracy, in exchange for a loss of performances on the dot product task. In other experiments with this task, the model learned all basic operation to 100% accuracy (as in the TADM setting), and the eigenvalue to a few percents. Adding more tasks, eigenvectors and inversion, results in the same performance. Co-training on the advanced tasks only (eigenvalues, vectors and inversion) results in 100% accuracy on eigenvalue computation, 22% on eigenvectors, and 0 on inversion.\\n\\nThese results demonstrate the feasibility of co-training on basic matrix operations, but also suggest that further research is needed if one wants to extend it to all the tasks considered in this paper.\"}"}
{"id": "L2a_bcarHcF", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 22: Accuracy of co-training.\\n\\n5 \u00d7 5 matrices, 5% tolerance.\\n\\nThe number of parameters in the sequence to sequence transformer we use in this paper can be calculated as follows.\\n\\n\u2022 A self-attention mechanism with dimension $d$ has $4d(d+1)$ parameters: it is composed of four linear layers (K, Q, V and the output layer), with $d$ input, $d$ output and a bias.\\n\\n\u2022 A cross-attention mechanism with $d_e$ dimensions in the encoder, and $d$ in the decoder has $2d(d+d_e+2)$ parameters (K and V are $d_e \\\\times d$ layers).\\n\\n\u2022 A FFN with one hidden layer, $d$ input and output, and $h$ hidden units has $d(h+1)+h(d+1)$ parameters.\\n\\n\u2022 A layer normalization with $d$ dimensions has $2d$ parameters.\\n\\n\u2022 An encoder layer with dimension $d$ has a self-attention mechanism, a FFN with $4d$ hidden units (in our implementation) and two layer normalizations, for a total number of parameters of $12d^2+13d$.\\n\\n\u2022 A decoder layer has a cross-attention layer (encoding dimension $d_e$) and a layer normalization on top of an encoder, for a total of $14d^2+19d+2d_e$ parameters.\\n\\n\u2022 An embedding of dimension $d$ for a vocabulary of $w$ words will use $dw$ parameters, and $2d$ more if it is coupled to a layer normalization.\\n\\n\u2022 The final prediction layer with an output dimension of $d$ and a decoded vocabulary of $w_o$ words will use $(d+1)w$ parameters (but in our case, $dw$ will be shared with the decoder embedding).\\n\\nOverall, the number of parameters for a transformer with $n_e$ encoding layers with dimension $d_e$, $n_d$ decoding layers with dimension $d_d$, an input vocabulary of $w_i$ words, an output vocabulary of $w_o$ words and a positional embedding of $w_p$ words (corresponding to the maximum sequence length) can be computed by the formula:\\n\\n$$P = d_e(w_i+w_p+2) + ((w_o+w_p+2)d_d+w_o) + n_ed_e(12d_e+13) + n_dd_d(14d_d+2d_e+19)$$\\n\\nthe four terms in the sum corresponding to the input embedding, the output embedding, the encoder and the decoder.\\n\\nTable 23 provides the number of parameters for some of the models used in this paper. For the positional embedding, we set the number of words as the longest input and output sequence studied with that model.\"}"}
{"id": "L2a_bcarHcF", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 23: Number of parameters of transformers used in the paper.\\n\\n| Layer Type       | Parameters |\\n|------------------|------------|\\n| 1/1 layers 256 dims P10 | 2,276,171  |\\n| 1/1 layers 256 dims P1000 | 2,737,871  |\\n| 1/1 layers 256 dims B1999 | 3,297,554  |\\n| 1/1 layers 256 dims FP15  | 17,045,441 |\\n| 2/2 layers, 512 dims, B1999 | 17,619,218 |\\n| 2/2 layers 512 dimensions P10 | 15,578,443 |\\n| 2/2 layers 512 dimensions P1000 | 16,500,943 |\\n| 4/4 layers 512 dimensions P1000 | 31,213,775 |\\n| 1/4 layers 512 dimensions P1000 | 21,756,623 |\\n| 1/6 layers 512 dimensions P1000 | 30,164,687 |\\n| Eigen decomposition 1/6 layers 512 dimensions FP15 | 58,751,937 |\\n| 6/1 layers 512 dimensions FP15 | 53,493,697 |\\n| 6/1 layers 512 dimensions P1000 | 24,906,447 |\\n| 661 layers 512 dimensions P1000 | 45,926,607 |\\n| Matrix inversion 6/1 layers 512 dimensions FP15/P1000 | 39,186,127 |\\n\\nFigure 3 provides an empirical confirmation of the property of Wigner matrices mentioned in sections 2.2 and 5: the standard deviation of their eigenvalues is a function of their dimension and standard deviation of their coefficients only, and does not depend on the actual distribution of the coefficient. In particular, for coefficients with standard deviation $\\\\sigma = \\\\frac{10}{\\\\sqrt{3}} = 5.77$, we expect the standard deviation of their eigenvalue distribution to be $\\\\sigma = 12.91, 18.26, 22.36, 25.81$ for square matrices of dimension $5, 10, 15, 20$. For three distributions, uniform, Laplace and gaussian, and four dimensions ($5, 10, 15, 20$), we generated 100,000 random matrices with the same standard deviation of coefficients, and computed their eigenvalues. Standard deviations are within $0.01$ of theoretical values for all distributions and dimensions. It is interesting to note how the distributions (which correspond to the original coefficient distribution for $n = 1$) resemble the semi-circle as dimension increases.\"}"}
{"id": "L2a_bcarHcF", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"EXPERIMENTS AND RESULTS\\n\\n4.1 TRANSPOSITION\\n\\nLearning to transpose a matrix amounts to learning a permutation of its elements. For a square matrix, the permutation is composed of cycles of two elements. Permutations for rectangular matrices involve longer cycles. This task involves no arithmetic operations: tokens from the input sequence are merely copied to the output, in different positions. We investigate two formulations of this problem: a fixed-size case, where all matrices in the dataset have the same dimension and only one permutation is to be learned, and a variable-size case, where the dataset includes matrices of different dimensions, with as many permutations to learn. We train transformers with one layer, 256 dimensions and 8 attention heads in the encoder and decoder, over datasets using our four encoding schemes. All models learn to predict the exact solution (with 0% tolerance) in more than 99% of test cases, for fixed-size matrices, square or rectangular, with dimensions up to 30 \u00d7 30. This holds for all encodings, and for input or output sequences up to 2000 tokens long. Similar accuracies are achieved for variable-size datasets: (over 99% for 5\u221215 and 96% for 5\u221220), with the rectangular cases proving slightly more difficult to train. Table 2 summarizes our results.\\n\\n| Fixed dimensions | Variable dimensions |\\n|------------------|---------------------|\\n| Square | Rectangular |\\n| Size | 5x5 | 10x10 | 20x20 | 30x30 | 5x6 | 7x8 | 9x11 |\\n| Layers | 2/2 | 2/2 | 2/2 | 2/2 | 2/2 | 1/6 | 1/6 |\\n| 5% | 100 | 99.9 | 99.9 | 100 | 100 | 98.8 |\\n| 2% | 100 | 99.5 | 99.8 | 100 | 100 | 98.4 |\\n| 1% | 100 | 99.3 | 99.7 | 100 | 99.9 | 87.9 |\\n| 0.5% | 100 | 98.1 | 98.9 | 100 | 99.5 | 48.8 |\\n\\nTable 2: Exact prediction of matrix transposition for different matrix dimensions. Transformers with 1 layer, 256 dimensions and 8 attention heads.\\n\\n4.2 ADDITION\\n\\nLearning to add two m \u00d7 n matrices amounts to learning the correspondence between the positions of input and output (as in the transposition task) and the algorithm for adding two numbers in floating point representation, which will be performed on mn pairs of elements. We train transformers with 1 or 2 layers, 8 attention heads and 512 dimensions. Sum of fixed-size matrices with dimensions up to 10, both square and rectangular, are predicted with over 99% accuracy within 1% tolerance (and over 98% within 0.5%), with all four encodings. As dimensions increase, models using the P10 and P1000 encodings become more difficult to train as input sequences grow longer: adding two 15 \u00d7 15 matrices involves 450 input coefficients: a sequence of 1352 tokens in P1000 and 2252 in P10. Nevertheless, FP15 models achieve 99.5% accuracy within 0.5% tolerance for 15 \u00d7 15 matrices and B1999 models 89.7% accuracy with 1% tolerance on 20 \u00d7 20 matrices. Variable-size matrices with dimensions up to 10 are predicted by 2-layer transformers using the B1999 encoding with over 99.5% accuracy within 1% tolerance. Over matrices with larger dimensions (5\u221215), shallow models with 1 or 2 layers struggle, and their accuracy drops to 48 and 37% in the square and rectangular case. This can be mitigated by using deeper decoders: models with one layer in the encoder and 6 in the decoder achieve 77 and 87% accuracy on the same datasets. Table 3 summarizes our results.\\n\\n| Fixed dimensions | Variable dimensions |\\n|------------------|---------------------|\\n| Square | Rectangular |\\n| Size | 5x5 | 6x4 | 3x8 | 10x10 | 15x15 | 20x20 |\\n| Layers | 2/2 | 2/2 | 2/2 | 2/2 | 2/2 | 1/1 |\\n| 5% | 100 | 99.9 | 99.9 | 100 | 100 | 98.8 |\\n| 2% | 100 | 99.5 | 99.8 | 100 | 100 | 98.4 |\\n| 1% | 100 | 99.3 | 99.7 | 100 | 99.9 | 87.9 |\\n| 0.5% | 100 | 98.1 | 98.9 | 100 | 99.5 | 48.8 |\\n\\nTable 3: Accuracies of matrix sums, for different tolerances. B1999 encoding, 512 dimension and 8 attention heads.\"}"}
{"id": "L2a_bcarHcF", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3 MULTIPLICATION\\n\\nMultiplication of a matrix $M$ of dimension $m \\\\times n$ by a vector $V \\\\in \\\\mathbb{R}^n$ amounts to computing the $m$ dot products between $V$ and the lines of $M$. Each calculation features $n$ multiplications and $n - 1$ additions, and involve one row in the matrix and all coefficients in the vector. The model must now learn the position of the $2n$ elements in the computation, and two operations (add and multiply). Experimenting with models with 1 or 2 layers, over $5 \\\\times 5$ matrices, we observe that only models using the $P_{10}$ and $P_{1000}$ encoding can be trained to high accuracy. The $P_{1000}$ encoding performs best, with little difference between two and one layer models. Accuracies over 99.9%, at 1% tolerance, are achieved by 2-layer transformers using the $P_{1000}$ encoding for $5 \\\\times 5$ and $10 \\\\times 10$ square matrices. Comparable accuracies are achieved when multiplying rectangular matrices by vectors with the same overall number of coefficients ($30$).\\n\\nExperiments with datasets of matrices with variable size (from $5$ to $10$) achieve non-trivial performance (from $48\\\\%$ with $1\\\\%$ tolerance, to $72\\\\%$ with $5\\\\%$ tolerance, for square matrices). Results are summarized in Table 4.\\n\\n| $P_{10}$ | $P_{1000}$ | $P_{1000}$ | Variable 5-10 (P1000) |\\n|---------|------------|------------|-----------------------|\\n| $Tolerance$ | $5x5$ | $5x5$ | $10x10$ |\\n| $1\\\\%$ | $98.5$ | $99.9$ | $99.9$ |\\n| $2\\\\%$ | $99.0$ | $99.7$ | $99.8$ |\\n| $5\\\\%$ | $99.3$ | $99.9$ | $99.9$ |\\n\\nTable 4: Accuracies of matrix-vector products, for different tolerances. Fixed-size models have 1 or 2 layers, variable-size have 2 or 4. All models have 512 dimensions and 8 attention heads.\\n\\nMultiplication of matrices $M$ and $P$ is a scaled-up version of the matrix-vector multiplication, which is now performed for every column in matrix $P$. As previously, only models using the $P_{10}$ and $P_{1000}$ encoding can be trained to predict to high accuracy. Over $5 \\\\times 5$ matrices and rectangular matrices of similar size, trained model accuracy is the same as vector-multiplication (over 99% at 1% tolerance, see Table 5), but deeper decoders (with 4 to 6 layers) are needed.\\n\\n| $Square$ | $Rectangular$ |\\n|----------|---------------|\\n| $5x5$ | $2x13$ |\\n| $2x12$ | $3x8$ |\\n| $4x6$ | $6x4$ |\\n| $8x3$ | $12x2$ |\\n| $13x2$ | |\\n\\n| $Tolerance$ | $P_{10}$ | $2/2$ layers | $1/4$ layers | $4/4$ layers | $4/4$ layers | $2/6$ layers | $1/4$ layers | $1/6$ layers | $1/6$ layers | $1/6$ layers | $1/4$ layers | $1/4$ layers | $1/6$ layers | $1/6$ layers | $1/6$ layers | $1/4$ layers | $1/4$ layers | $1/6$ layers | $1/6$ layers | $1/6$ layers |\\n|------------|----------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|\\n| $5\\\\%$ | $100$ | $100$ | $100$ | $100$ | $100$ | $100$ | $100$ | $100$ | $99.9$ |\\n| $2\\\\%$ | $100$ | $100$ | $100$ | $100$ | $100$ | $100$ | $100$ | $100$ | $99.7$ | $99.8$ |\\n| $1\\\\%$ | $99.8$ | $100$ | $99.9$ | $100$ | $100$ | $99.9$ | $100$ | $99.9$ | $99.3$ | $99.8$ |\\n| $0.5\\\\%$ | $64.5$ | $99.9$ | $97.1$ | $98.5$ | $99.6$ | $99.7$ | $99.5$ | $99.5$ | $99.0$ | $99.8$ |\\n\\nTable 5: Accuracy of matrix multiplication, for different tolerances. Fixed-size matrices with 24-26 coefficients. All encodings are $P_{1000}$ unless specified. Models have 512 dimensions and 8 attention heads.\\n\\n4.4 EIGENVALUES\\n\\nWe now turn to non-linear problems that are usually solved by iterative algorithms. We train models with 4 or 6 layers in the encoder or the decoder to predict the eigenvalues of symmetric matrices. Over samples of $5 \\\\times 5$ random matrices, we reach $100\\\\%$ accuracy at $5\\\\%$ tolerance, and $98.5\\\\%$ at $1\\\\%$ for all four encodings. For $8 \\\\times 8$ matrices, we achieve accuracies of $100$ and $85\\\\%$ at $5$ and $1\\\\%$ tolerance. Larger problems, however, prove difficult to learn: on $10 \\\\times 10$ matrices, $25\\\\%$ accuracy at $5\\\\%$ tolerance is reached after 360 million examples. As a comparison, $5 \\\\times 5$ models train to maximum accuracy in about $40$ million examples, and $8 \\\\times 8$ models in about $60$ million.\\n\\nWe can overcome this limitation by training models on variable-size datasets. On samples of matrices with 5-10, 5-15 and 5-20 dimensions, we achieve $100\\\\%$ accuracy at $5\\\\%$ tolerance, and $88\\\\%$, $94\\\\%$ and $45\\\\%$ at $1\\\\%$. Using the 5-15 model, the eigenvalues of $10 \\\\times 10$ matrices can be predicted with $100\\\\%$ accuracy at $2\\\\%$ tolerance, and $73\\\\%$ at $1\\\\%$. Table 6 summarizes our results.\"}"}
{"id": "L2a_bcarHcF", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Accuracy of eigenvalues for different tolerances and dimensions. All models have 512 dimensions and 8 attention heads, except the 10x10 model, which has 12.\\n\\n| Tolerance | 5% | 2% | 1% | 0.5% |\\n|-----------|----|----|----|------|\\n| P10       | 97.0 | 83.4 | 31.2 | 0.6 |\\n| P1000     | 94.0 | 77.9 | 41.5 | 2.9 |\\n| FP15      | 51.6 | 12.6 | 0.6  | 0.1  |\\n\\nTable 7: Accuracies of eigenvectors, for different tolerances and depths. All models have 512 dimensions and 8 attention heads.\\n\\n| Tolerance | 5% | 2% | 1% | 0.5% |\\n|-----------|----|----|----|------|\\n| P10       | 73.6 | 46.9 | 15.0 | 0.2  |\\n| P1000     | 80.4 | 61.0 | 30.1 | 3.1  |\\n| FP15/P1000| 78.8 | 61.7 | 34.2 | 5.9  |\\n\\nTable 8: 5x5 matrix inversion. All models have 6/1 layers, except P1000 10 heads, which has 6/6 (and 512 dimensions).\\n\\n| Tolerance | 5% | 2% | 1% | 0.5% |\\n|-----------|----|----|----|------|\\n| P10       | 8/8 | 8/8 | 8/8 | 8/8  |\\n| P1000     | 10/8| 12/8| 10/4| 12/8 |\"}"}
{"id": "L2a_bcarHcF", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Accuracies remain high, 100% and 86.7% for singular values (5% and 1% tolerance), and 98.9% and 75.3% for the full decomposition.\\n\\n| Singular values | Singular vectors |\\n|-----------------|-----------------|\\n| P10 2/2 layers  | P1000 4/4 layers |\\n| 5%              | 100%            |\\n| 2%              | 98.5            |\\n| 1%              | 84.5            |\\n| 0%              | 41.1            |\\n\\nTable 9: Accuracies of SVD for 4x4 matrices. All models have 512 dimensions and 8 attention heads.\\n\\nIn this section, we focus on the prediction of eigenvalues of symmetric matrices. To train our models, we generate random \\\\( n \\\\times n \\\\) matrices with independent and identically distributed (iid) coefficients, sampled from a uniform distribution over \\\\([-A, A]\\\\). They belong to a common class of random matrices, known as Wigner matrices. Their eigenvalues have a centered distribution with standard deviation \\\\( \\\\sigma = \\\\sqrt{ns} \\\\), where \\\\( s \\\\) is the standard deviation of the coefficients (\\\\( s = A/\\\\sqrt{3} \\\\) when uniform).\\n\\nAs \\\\( n \\\\) increases, this distribution converges to the semi-circle law (Mehta (2004)). Whereas Wigner matrices are very common in science, random matrices with different eigenvalue distributions (and non iid coefficients) appear in many practical cases. For instance, statistical covariance matrices have all their eigenvalues positive, and the adjacency matrices of scale-free and other non-Erdos-Renyi graphs have centered but non semi-circle distributions of eigenvalues (Preciado & Rahimian, 2017). It is, therefore, important to understand how models trained on Wigner matrices perform on matrices with different distributions of eigenvalues.\\n\\nTo this effect, we create test sets of 10,000 matrices with different distributions than the training set. First, we generate matrices with uniform iid coefficients (as in the training set), but different standard deviation: \\\\( \\\\sigma_{\\\\text{test}} \\\\in [0.1 \\\\sigma_{\\\\text{train}}, 1.5 \\\\sigma_{\\\\text{train}}] \\\\). Over these test sets, our trained models achieve over 96% accuracy (with 2% tolerance) for \\\\( \\\\sigma_{\\\\text{test}} \\\\in [0.6 \\\\sigma_{\\\\text{train}}, \\\\sigma_{\\\\text{train}}] \\\\). However, model accuracy drops when \\\\( \\\\sigma_{\\\\text{test}} \\\\) is out of this range: 54% for \\\\( 0.4 \\\\sigma_{\\\\text{train}} \\\\), 0% for \\\\( 0.2 \\\\sigma_{\\\\text{train}} \\\\), 26% for \\\\( 1.1 \\\\sigma_{\\\\text{train}} \\\\) and 2% for \\\\( 1.3 \\\\sigma_{\\\\text{train}} \\\\). Out-of-distribution generalization only takes place when test set variance is lower, and not too far from (over 25% of) training set variance.\\n\\nThen, we generate test sets of matrices with different eigenvalue distributions: positive eigenvalues (Wigner matrices with eigenvalues replaced by their absolute values), and eigenvalues distributions according to the uniform, gaussian or Laplace law (see section 2.2), with standard deviation \\\\( \\\\sigma_{\\\\text{train}} \\\\) and \\\\( 0.6 \\\\sigma_{\\\\text{train}} \\\\). Over test sets with \\\\( \\\\sigma_{\\\\text{test}} = \\\\sigma_{\\\\text{train}} \\\\), accuracies are 26% for Laplace, 25% for gaussian, 19% for uniform, and 0% for positive. Results are slightly better for test sets with lower standard deviation (\\\\( 0.6 \\\\sigma_{\\\\text{train}} \\\\)): 28%, 44%, 60% and 0% for Laplace, gaussian, uniform and positive, but out-of-distribution accuracies are low, and matrices with positive eigenvalues cannot be predicted at all.\\n\\nTo improve out-of-distribution accuracy, we train new models on datasets with different distributions of eigenvalues, and evaluate them on the test sets previously created. First, we generate matrices with uniform coefficients but variable standard deviation (by randomly selecting \\\\( A \\\\in [1, 100] \\\\) for each matrix). Unsurprisingly, models trained on this dataset achieve high accuracies on test sets of Wigner matrices with high or low variance. Performances also increase over the gaussian, uniform and Laplace-distributed test sets (from 25%\u221260% to 53%\u221268%). Yet, matrices with positive eigenvalues cannot be predicted. Training models over a mixture of (Wigner) matrices with uniform iid coefficients and matrices with positive eigenvalues results in better prediction of positive eigenvalues, but degrades performances over all other tests sets.\\n\\nHowever, models trained on a mixture of matrices with uniform coefficients and matrices with gaussian eigenvalues, or uniform iid and Laplace eigenvalues, achieve high accuracies over all test sets, as do models trained on matrices with Laplace eigenvalues only, or a mixture of uniform, gaussian and Laplace eigenvalues (all non-Wigner matrices). These experiments are presented in table 10.\\n\\nThis is an important result: it suggests that Wigner matrices, often considered as the default model for random matrices, might not be the best choice for training transformers. Models trained on\"}"}
{"id": "L2a_bcarHcF", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nnon-Wigner matrices (non-iid coefficients, limit distribution of eigenvalues not a semi-circle) generalize to matrices with iid coefficients, whereas the reverse is not true. This confirms that out-of-distribution generalization requires that particular attention is paid to training data generation.\\n\\n| Test set eigenvalue distribution | iid coeff. A=10 (baseline) | iid coeff. $A \\\\in [1,100]$ |\\n|---------------------------------|----------------------------|-----------------------------|\\n| Semicircle-Positive             | 99 98 97                   | 99 98 97                   |\\n| Semicircle-Gaussian             | 99 99 99                   | 100 100 100                |\\n| Semicircle-Laplace              | 100 100 100                 | 100 100 100                |\\n| Laplace                         | 100 100 100                 | 100 100 100                |\\n| Gaussian-Uniform-Laplace        | 100 100 100                 | 100 100 100                |\\n\\nTable 10: Out-of-distribution eigenvalue accuracy (tolerance 2%) for different training distributions. All models have 512 dimensions and 8 attention heads, and use the P1000 encoding.\\n\\nModels trained on matrices of a given size do not generalize to different dimensions, but they can be retrained over samples of matrices of different size. This takes comparatively few examples: a $5 \\\\times 5$ model, that takes 40 million examples to be trained, can learn to predict with high accuracy eigenvalues of matrices of dimension 6 and 7 with about 25 million additional examples. Table 11 presents those results. The capacity of pre-trained large transformers (such as GPT-3) to perform few-shot learning is well documented, but it is interesting to observe the same phenomenon in smaller models.\\n\\n| Encoding | Retrain dimensions | Accuracy (5%) | Accuracy (2%) | Retrain examples |\\n|----------|--------------------|---------------|---------------|------------------|\\n| P10      | 5-6                | 100           | 99.9          | 10M              |\\n| P10      | 5-7                | 100           | 93.6          | 25M              |\\n| P1000    | 5-6                | 100           | 97.7          | 25M              |\\n\\nTable 11: Model accuracy after retraining. Models trained over 5x5 matrices, retrained over 5-6 and 5-7.\\n\\nOverall performance after retraining (tolerance 5 and 2%), and number of examples needed for retraining. All models have 512 dimensions and 8 attention heads.\\n\\n**DISCUSSION AND FUTURE DIRECTIONS**\\n\\nOur experiments demonstrate that transformers can be trained to solve problems of linear algebra, using randomly generated datasets. However, their accuracy depends on the encodings used to represent matrix coefficients. We introduce four encoding schemes, and our experiments suggest that P10 is generally dominated by P1000, which is also more economical, and that B1999 never really finds its use, as FP15 is more compact and P1000 more efficient. P1000 seems to be a good choice for problems of moderate size, and FP15 for longer sequences. For advanced problems like eigenvectors and inversion, asymmetric architectures, with a deep FP15 encoder and a shallow P1000 decoder, achieve the best performances. Our interpretation is that P1000 in the decoder facilitates training because the meaningful representation of output as (sign, mantissa, exponent) triplets allows for better error feedback during training. On the other hand, a FP15 deep encoder can provide more complex representations of the input matrix, while being easier to train thanks to the shorter sequences. Such asymmetric architectures also benefit from more attention heads (10 to 12) in the encoder, while less heads (4) in the decoder improve training stability at no cost in accuracy. Those asymmetric architectures deserve further study.\\n\\nMost of our experiments focus on matrices with 5 to 10 lines and columns. Our results on the eigenvalue problem suggest that larger problems can be solved by training over matrices of variable size, or retraining over larger matrices. In this work, matrices of different dimensions are sampled in equal proportion and presented for training in random order. Varying their proportions and scheduling (i.e. curriculum learning) should result in better performance. Yet, as dimension increases, sequence lengths will reach the practical limits of quadratic attention mechanisms. Experimenting with transformers with linear or log-linear attention (Zaheer et al., 2021; Wang et al., 2020a; Vyas et al., 2021)\"}"}
{"id": "L2a_bcarHcF", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022. In terms of asymptotic complexity, matrix inversion (and the other non linear tasks) is usually handled by $O(n^3)$ algorithms (although $O(n^{2.37})$ methods are known). Since our sequence length is $O(n^2)$, transformers with quadratic attention mechanisms are $O(n^4)$. Linear attention would reduce this to $O(n^2)$.\\n\\nThe out-of-distribution experiments are our most significant results. They prove that models trained on random data can generalize to a wide range of test distributions. They also confirm the importance of wisely selecting training data distributions, a process that can be counter-intuitive. In our specific case, the \u201cobvious\u201d random model (Wigner matrices) is not the best for out-of-domain generalization. In fact, we show that sets of \u201cspecial\u201d matrices (non-iid coefficients with Laplace eigenvalues) can produce models with better capability for generalization, notably on Wigner matrices. This matches the intuitive idea that we learn more from edge cases than averages.\\n\\n**RELATED WORK**\\n\\nAlgorithms using neural networks to compute eigenvalues and eigenvectors have been proposed since the early 1990s (Samardzija & Waterland, 1991; Cichocki & Unbehauen, 1992; Yi et al., 2004; Tang & Li, 2010; Oja, 1992), and improvements to the original techniques have been suggested until recently (Finol et al., 2019). Similar approaches have been proposed for other problems in linear algebra (Wang, 1993a;b; Zhang et al., 2008). All these methods leverage the Universal Approximation Theorem (Cybenko, 1989; Hornik, 1991), which states that, under weak conditions on their activation functions, neural networks can approximate any continuous mapping (in our case, the mapping between the coefficients of a matrix and their associated eigenvalues and vectors). These approaches rely on the fact that eigenvalues and vectors appear in the solutions of particular differential equations involving the matrix coefficients (e.g. Brockett (1991)). By designing a neural network that represents this differential equation, with the matrix to decompose as the input, and the coefficients in the output layer as the solution, and by defining a loss function that measures how well the output layer approximates the correct solution, the network can be trained to find better and better approximations to the solution. These techniques have two main limitations: they rely on a problem-specific network architecture that has to be hand-coded, and computation is done at train time, which makes them slow and implies retraining the network every time a new matrix is to be processed. In comparison, the techniques proposed in this paper are trained once, and can compute at inference for any matrix input.\\n\\nTechniques have been proposed to train neural networks to compute basic mathematical operations, and use them as building blocks for larger components. Kaiser & Sutskever (2015) introduced the Neural GPU, that could learn addition and multiplication over binary representations of integers. Trask et al. (2018) proposed Neural Arithmetic Logic Units (NALU), that can learn to perform exact additions, substractions, multiplications and divisions by constraining the weights of a linear network to remain close to 0, 1 or -1. Both Neural GPU and NALU have been shown to be able to extrapolate to numbers far larger than those they were trained on. For matrix multiplication, Blalock & Guttag (2021) use learning techniques to improve on known approximate techniques.\\n\\nUse of transformers in mathematics has mostly focused on symbolic computations. Lample & Chariton (2019) showed that transformers could be trained to integrate functions and solve ordinary differential equations and, in a follow-up work (Charton et al., 2020), predict properties of differential systems. Transformers have also been applied to formal systems, in theorem proving (Polu & Sutskever, 2020) and temporal logic (Hahn et al., 2021). The use of sequence to sequence models for arithmetic and the exact solution of mathematical problem has been studied by Saxton et al. (2019). In a recent paper, Nogueira et al. (2021) point to the limitations of experiments on arithmetic.\\n\\n**CONCLUSION**\\n\\nWe have shown that transformers can be trained over generated data to solve problems of linear algebra with high accuracy, and that careful selection of the generative model for their training data can allow them to generalize out of their training distribution. This demonstrates that applications of transformers to mathematics are not limited to symbolic calculation, and can cover a wide range of scientific problems, featuring numerical computations. We believe our results pave the way for wider applicability of transformers in science.\"}"}
{"id": "L2a_bcarHcF", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Reproducibility statement\\nThe transformer implementation and the framework for running the experiments were used in several prior works, and rely on standard libraries (Pytorch for the models, Numpy for mathematical calculations). The model source code, data generation code, and parameters for experiments will be open-sourced and made publicly available. All experiments were run several times (on average 10 times), with multiple random seeds and light modifications of the hyperparameters (e.g. small changes in model size, weight initialization, activation functions), to guarantee their robustness.\\n\\nEthics statement\\nGiven the subject of the paper, and the fact that all data used are randomly generated, we believe that no potential ethical concerns are raised by this research.\\n\\nREFERENCES\\nAnonymous. Adaptive control flow in transformers improves systematic generalization. In Submitted to The Tenth International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=KBQP4A_J1K. under review.\\n\\nLuca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Giambattista Parascondolo. Neural symbolic regression that scales. arXiv preprint arXiv:2106.06427, 2021.\\n\\nDavis Blalock and John Guttag. Multiplying matrices without multiplying. arXiv preprint arXiv:2106.10860, 2021.\\n\\nRoger W Brockett. Dynamical systems that sort lists, diagonalize matrices, and solve linear programming problems. Linear Algebra and its applications, 146:79\u201391, 1991.\\n\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. arXiv preprint arXiv:2005.12872, 2020.\\n\\nFran\u00e7ois Charton, Amaury Hayat, and Guillaume Lample. Learning advanced mathematical computations from examples. arXiv preprint arXiv:2006.06462, 2020.\\n\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.\\n\\nKyunghyun Cho, Bart Van Merri\u00ebnoor, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\\n\\nAndrzej Cichocki and Rolf Unbehauen. Neural networks for computing eigenvalues and eigenvectors. Biological Cybernetics, 68(2):155\u2013164, 1992.\\n\\nGeorge Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303\u2013314, 1989.\\n\\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \u0141ukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.\\n\\nLinhao Dong, Shuang Xu, and Bo Xu. Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5884\u20135888, 2018.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2021.\\n\\nDavid Finol, Yan Lu, Vijay Mahadevan, and Ankit Srivastava. Deep convolutional neural networks for eigenvalue problems in mechanics. International Journal for Numerical Methods in Engineering, 118(5):258\u2013275, 2019.\"}"}
{"id": "L2a_bcarHcF", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nAlex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016.\\n\\nChristopher Hahn, Frederik Schmitt, Jens U. Kreber, Markus N. Rabe, and Bernd Finkbeiner. Teaching temporal logics to neural networks. arXiv preprint arXiv:2003.04218, 2021.\\n\\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation 9(8):1735\u20131780, 1997.\\n\\nKurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks 4:251\u2013257, 1991.\\n\\n\u0141ukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.\\n\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nDonald E. Knuth. The Art of Computer Programming, Volume 2: Seminumerical Algorithms. Addison-Wesley, third edition, 1997.\\n\\nGuillaume Lample and Fran\u00e7ois Charton. Deep learning for symbolic mathematics. arXiv preprint arXiv:1912.01412, 2019.\\n\\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.\\n\\nMadan Lal Mehta. Random Matrices. Academic Press, 3rd edition, 2004.\\n\\nRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021.\\n\\nErkki Oja. Principal components, minor components, and linear neural networks. Neural networks 5(6):927\u2013935, 1992.\\n\\nStanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020.\\n\\nVictor M. Preciado and M. Amin Rahimian. Moment-based spectral analysis of random graphs with given expected degrees. arXiv preprint arXiv:1512.03489, 2017.\\n\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n\\nNikola Samardzija and RL Waterland. A neural network for computing eigenvectors and eigenvalues. Biological Cybernetics 65(4):211\u2013214, 1991.\\n\\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019.\\n\\nFeng Shi, Chonghan Lee, Mohammad Khairul Bashar, Nikhil Shukla, Song-Chun Zhu, and Vijaykrishnan Narayanan. Transformer-based machine learning for fast sat solvers and logic synthesis. arXiv preprint arXiv:2107.07116, 2021.\\n\\nYing Tang and Jianping Li. Another neural network based approach for computing eigenvalues and eigenvectors of real skew-symmetric matrices. Computers & Mathematics with Applications 60(5):1385\u20131392, 2010.\\n\\nAndrew Trask, Felix Hill, Scott Reed, Jack Rae, Chris Dyer, and Phil Blunsom. Neural arithmetic logic units. arXiv preprint arXiv:1808.00508, 2018.\"}"}
{"id": "L2a_bcarHcF", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nMost applications of transformers to mathematics, from integration to theorem proving, focus on symbolic computation. In this paper, we show that transformers can be trained to perform numerical calculations with high accuracy. We consider problems of linear algebra: matrix transposition, addition, multiplication, eigenvalues and vectors, singular value decomposition, and inversion. Training small transformers (up to six layers) over datasets of random matrices, we achieve high accuracies (over 90%) on all problems. We also show that trained models can generalize out of their training distribution, and that out-of-domain accuracy can be greatly improved by working from more diverse datasets (in particular, by training from matrices with non-independent and identically distributed coefficients). Finally, we show that few-shot learning can be leveraged to re-train models to solve larger problems.\\n\\n1 Introduction\\nSince their introduction by Vaswani et al. (2017), transformers, originally designed for machine translation, were applied to various problems, from text generation (Radford et al., 2018; 2019) to image processing (Carion et al., 2020) and speech recognition (Dong et al., 2018) where they soon achieved state-of-the-art performance (Dosovitskiy et al., 2021; Wang et al., 2020b). In mathematics, transformers were used for symbolic integration (Lample & Charton, 2019), theorem proving (Polu & Sutskever, 2020), formal logic (Hahn et al., 2021), SAT solving (Shi et al., 2021), symbolic regression (Biggio et al., 2021) and dynamical systems (Charton et al., 2020). All these problems pertain to symbolic mathematics, or involve a large amount of symbolic computation. When working on these tasks, transformers manipulate mathematical symbols, just like words in natural language. But mathematics are not limited to symbol manipulation: many practical applications involve numerical calculations, either exact (e.g. arithmetic) or approximate (e.g. function evaluation, numerical solutions of equations). The use of transformers for numerical computation has been less studied, and many early experiments with arithmetic have proved disappointing (Nogueira et al., 2021). This is, nevertheless, an important question: most problems in mathematics and science involve both symbolic and numerical computations. If we want transformers to solve these problems end-to-end, they need to be able to perform numerical calculations with high accuracy.\\n\\nIn this paper, we train transformers to compute solutions of problems of linear algebra, which serve as fundamental building blocks in many scientific problems: basic operations on matrices, matrix inversion, eigenvalue and singular value decompositions. We introduce and discuss four encodings to represent problems and solutions as sequences that transformers can process, and train small transformers (up to 6 layers, 10 to 50 million trainable parameters) over generated datasets of random matrices. Trained models can compute approximate solutions to these problems (to a few percents of their $L_1$ norm) with over 90% accuracy (99% in most cases). We also show that they can generalize out of their training distribution, and be retrained to extrapolate to larger problems than the ones they were trained on. We believe these results pave the way for using transformers as end to end solvers for problems of mathematics and science.\\n\\nAfter introducing the problems of linear algebra we are studying and presenting the encodings we use to represent them as sequences that can be used by our models, we discuss data generation, architecture and experimental settings. Then, we present our experiments on nine different problems, and discuss out-of-distribution generalization and few shot learning for eigenvalue computation. Finally, we discuss our results and future directions for research, and present related works.\"}"}
{"id": "L2a_bcarHcF", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"PROBLEMS AND DATASETS\\n\\nLet $M$ and $N$ be $m \\\\times n$ matrices and $V \\\\in \\\\mathbb{R}^m$. We study nine problems of linear algebra:\\n\\n- **Matrix transposition**: find $M^T$, an $n \\\\times m$ matrix,\\n- **Matrix addition**: find $M + N$, a $m \\\\times n$ matrix,\\n- **Matrix-vector multiplication**: find $M^T V$, a vector in $\\\\mathbb{R}^n$,\\n- **Matrix multiplication**: find $M^T N$, an $n \\\\times n$ matrix,\\n- **Eigenvalues**: $M$ symmetric, find its $n$ (real) eigenvalues, sorted in descending order,\\n- **Eigenvectors**: $M$ symmetric, find $D$ diagonal and $Q$ orthogonal such that $M = Q^T D Q$, set as a $(n+1) \\\\times n$ matrix, with (sorted) eigenvalues in its first line,\\n- **Singular values**: find the $n$ eigenvalues of $M^T M$, sorted in descending order,\\n- **Singular value decomposition**: find orthogonal $U, V$ and diagonal $S$ such that $M = USV$, set as a $(m+n+1) \\\\times \\\\min(m,n)$ matrix,\\n- **Inversion**: $M$ square and invertible, find its inverse $P$, such that $MP = PM = \\\\text{Id}$.\\n\\nThese problems range from basic operations on individual coefficients of the input matrices (transposition and addition), to computations involving several arithmetic operations over many coefficients (multiplication), and complex nonlinear transformations involving the whole matrix, with cubic complexity (decompositions and inversion). For each problem, we generate datasets of pairs of matrices $(I, O)$, by sampling random input matrices (see section 2.2), and computing the output $O$ with a linear algebra package (NumPy linalg). When a problem has several input or output matrices, they are concatenated into one (for instance, the two $m \\\\times n$ operands of the addition task are concatenated into one $m \\\\times 2n$ matrix $I$). All coefficients in $I$ and $O$ are set in base ten floating-point representation, and rounded to three significant digits in the mantissa.\\n\\n2.1 ENCODING MATRICES AS SEQUENCES\\n\\nThe input and output to our problems are matrices. To be processed by transformers, they need to be converted into sequences of tokens. We encode a $m \\\\times n$ matrix by first coding its dimensions as two symbolic tokens ($V_m$ and $V_n$), followed by its $mn$ coefficients, encoded as sequences. Through this paper, we will use four encoding schemes for matrix coefficients: $P_{10}$, $P_{1000}$, $B_{1999}$, and $FP_{15}$.\\n\\nIn base $10$ positional encoding ($P_{10}$), a number is represented as a sequence of five tokens: one sign token (+ or -), 3 digits (from 0 to 9) for the mantissa, and a symbolic token (from $E-100$ to $E+100$) for the exponent. For instance $3.14$ will be represented as $314 \\\\cdot 10^{-2}$, and encoded as $[+, 3, 1, 4, E-2]$. $P_{1000}$ (positional base 1000) provides a more compact representation by encoding the mantissa as a single token (from 0 to 999), and representing a number as the triplet (sign, mantissa, exponent). $B_{1999}$ (balanced base 1999) pushes this one step further by encoding together the sign and mantissa (from $-999$ to $999$). Finally, $FP_{15}$ encodes each floating point number $x = m 10^b$ as a unique token $FP_m/b$.\\n\\n| Encoding | Tokens / coefficient | Vocabulary |\\n|----------|----------------------|------------|\\n| $P_{10}$ | $[+, 3, 1, 4, E-2]$  | 5          |\\n| $P_{1000}$ | $[+, 314, E-2]$    | 3          |\\n| $B_{1999}$ | $[314, E-2]$      | 2          |\\n| $FP_{15}$ | $[FP_{314/-2}]$   | 1          |\\n\\nTable 1: Four encodings for matrix coefficients.\\n\\nSelecting an encoding is a trade-off. Long encodings ($P_{10}, P_{1000}$) embed knowledge about numbers that the model can leverage (e.g. numbers can be compared using their signs and exponents, addition and multiplication can be learned by memorizing small tables). Compact encodings use a larger vocabulary (harder to learn), but generate shorter sequences that facilitate training with transformers. In $P_{10}$, a $20 \\\\times 20$ matrix is a sequence of 2002 tokens, close to the practical limit of transformer implementations that use a quadratic attention mechanism. In $FP_{15}$, it is only 402 tokens long.\"}"}
{"id": "L2a_bcarHcF", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2 RANDOM MATRIX GENERATION\\n\\nIn most of our experiments, we train models over datasets of random matrices with uniformly distributed coefficients in $[-A, A]$ (with $A = 10$). Occasionally, we sample gaussian coefficients with the same standard deviation ($\\\\sigma = A/\\\\sqrt{3}$). In the symmetric case, these matrices are known as Wigner matrices. Their eigenvalues have a centered distribution with standard deviation $\\\\sigma = \\\\sqrt{n s}$, where $s$ is the standard deviation of the coefficients (Mehta, 2004). As $n$ increases, this distribution converges to the semi-circle law ($p(\\\\lambda) = \\\\sqrt{4\\\\sigma^2 - \\\\lambda^2}/2\\\\pi\\\\sigma^2$) for all coefficient distributions with bounded variance. If the coefficients are gaussian, the associated eigenvectors are uniformly distributed over the unit sphere.\\n\\nWhen investigating out-of-distribution generalization for the eigenvalue problem, we will need to generate random symmetric matrices with different distributions of their eigenvalues (corresponding to random matrices with non iid coefficients). To this effect, we randomly sample symmetric matrices $M$, with gaussian coefficients, and compute their eigenvalue decomposition $M = PDP^T$, with $P$ the orthogonal matrix of eigenvectors (uniformly distributed over the unit sphere since the coefficients are gaussian). We then replace $D$, the diagonal matrix of eigenvalues of $M$, with a diagonal $D'$ sampled from another distribution. Finally, we recompute $M' = PD'P^T$, a symmetric matrix (because $P$ is orthogonal) with eigenvalues distributed as we choose, and eigenvectors uniformly distributed over the unit sphere.\\n\\n3 MODELS AND EXPERIMENTAL SETTINGS\\n\\nWe use the standard transformer architecture introduced in Vaswani et al. (2017), with an encoder and a decoder connected by a cross-attention mechanism. Most of our models have 512 dimensions, 8 attention heads and up to 6 layers. We experiment with different number of layers and attention heads in the encoder and decoder. All training is supervised, and minimizes the cross-entropy between model prediction and the correct solution. We use the Adam optimizer (Kingma & Ba, 2014) with a learning rate of $10^{-4}$, an initial warm-up phase of 10,000 steps and cosine scheduling (Loshchilov & Hutter, 2016). All training data is generated on the fly, in batches of 64 examples. Every 300,000 examples, 10,000 random problems are generated and used to evaluate the model. When evaluating, we consider that a predicted sequence $\\\\text{seq}$ is a correct solution to the problem $(I, O)$ (where $I$ and $O$ are the input and output matrices) if it can be decoded as a valid matrix (several matrices for singular and eigen decomposition) that approximates the correct solution to a given tolerance $\\\\tau$ ($\\\\tau \\\\in \\\\{5, 2, 1, 0.5\\\\%\\\\}$).\\n\\nFor addition, transposition, multiplication, eigen and singular values we check that $P$ verifies $\\\\|P - O\\\\| < \\\\tau \\\\|O\\\\|$. For eigenvalue decomposition, we check that the solution $(Q, D)$ predicted by the model can reconstruct the input matrix, i.e. $\\\\|Q^T D Q - I\\\\| < \\\\tau \\\\|I\\\\|$. For singular value decomposition, we check that $\\\\|USV - I\\\\| < \\\\tau \\\\|I\\\\|$. For matrix inversion, we check that $\\\\|PI - Id\\\\| < \\\\tau \\\\|Id\\\\| = \\\\tau$. The choice of the $L_1$ norm is important: norms like $L_2$ and $L_\\\\infty$ will favor models that correctly predict the largest coefficients in the solution. For eigen and singular value problems, this amounts to predicting the largest values, an easier problem than the one we want to solve.\\n\\nWe consider different tolerances for different problems. Since we round numbers to three significant digits, 0.5% is the best we can hope. In fact, a number $x$ with mantissa $1.00$ is subjected to a maximal rounding error of 0.5% ($x \\\\in [1.005, 0.995]$), which may accumulate when several (rounded) numbers are summed, and increase again when nonlinear operations are considered. When discussing results, we consider tolerances of 0% for transposition, which involves no arithmetic, 1% for basic matrix operations (addition, multiplication), and 2 or 5% for non linear operations (decomposition, inversion), but we usually provide results for all tolerance levels.\\n\\nMost of our experiments focus on $5 \\\\times 5$ square matrices, or rectangular matrices with as many coefficients (e.g. $6 \\\\times 4$, $2 \\\\times 13$). This helps when comparing encodings: for larger dimensions, varying sequence lengths make comparisons difficult. We also study scaled-up versions of the problems (from $8 \\\\times 8$ to $15 \\\\times 15$), and datasets with matrices of variable dimensions ($5$-$10$ or $5$-$15$). In this paper, we limit ourselves to problem that can be solved using small models (with up to 6 layers). Scaling to larger problems, and leveraging deeper architectures is left for future research.\"}"}
{"id": "L2a_bcarHcF", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 6000\u20136010, 2017.\\n\\nApoorv Vyas, Angelos Katharopoulos, and Fran\u00e7ois Fleuret. Fast transformers with clustered attention. arXiv preprint arXiv:2007.04825, 2020.\\n\\nJun Wang. A recurrent neural network for real-time matrix inversion. Applied Mathematics and Computation, 55(1):89\u2013100, 1993a.\\n\\nJun Wang. Recurrent neural networks for solving linear matrix equations. Computers & Mathematics with Applications, 26(9):23\u201334, 1993b.\\n\\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020a.\\n\\nYongqiang Wang, Abdelrahman Mohamed, Due Le, Chunxi Liu, Alex Xiao, Jay Mahadeokar, Hongzhao Huang, Andros Tjandra, Xiaohui Zhang, Frank Zhang, and et al. Transformer-based acoustic modeling for hybrid speech recognition. 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2020b.\\n\\nZhang Yi, Yan Fu, and Hua Jin Tang. Neural networks based approach for computing eigenvectors and eigenvalues of symmetric matrix. Computers & Mathematics with Applications, 47(8-9):1155\u20131164, 2004.\\n\\nManzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. arXiv preprint arXiv:2007.14062, 2021.\\n\\nYunong Zhang, Weimu Ma, and Binghuang Cai. From Zhang neural network to Newton iteration for matrix inversion. IEEE Transactions on Circuits and Systems I: Regular Papers, 56(7):1405\u20131415, 2008.\"}"}
{"id": "L2a_bcarHcF", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NUMBER ENCODINGS\\n\\nLet \\\\(x\\\\) be a non-zero real number, it can be represented uniquely as\\n\\\\[x = s \\\\cdot m \\\\cdot 10^e,\\\\]\\nwith \\\\(s \\\\in \\\\{-1, 1\\\\}, m \\\\in [100, 1000[, e \\\\in \\\\mathbb{Z}\\\\). Rounding \\\\(m\\\\) to the nearest integer \\\\(n\\\\), we get the base ten, floating-point representation of \\\\(x\\\\), with three significant digits:\\n\\\\[x \\\\approx s \\\\cdot n \\\\cdot 10^e,\\\\]\\n\\\\((s, n, e) \\\\in \\\\mathbb{Z}^3\\\\)\\n\\nBy convention \\\\(0\\\\) is encoded as \\\\(+0\\\\cdot 10^0\\\\). All our encodings are possible representations of the triplets \\\\((s, n, e)\\\\).\\n\\nIn base \\\\(N\\\\) positional encoding, we encode \\\\(s\\\\) (the sign) and \\\\(e\\\\) (the exponent) as unique tokens: \\\\(+\\\\) or \\\\(-\\\\) for \\\\(s\\\\), and a token from \\\\(E-100\\\\) to \\\\(E100\\\\) for \\\\(e\\\\). The mantissa, \\\\(n\\\\), is encoded as the representation of \\\\(n\\\\) in base \\\\(N\\\\) (e.g. binary representation if \\\\(N = 2\\\\), decimal representation if \\\\(N = 10\\\\)), a sequence of \\\\(\\\\lceil \\\\log_N(1000) \\\\rceil\\\\) tokens from \\\\(0\\\\) to \\\\(N-1\\\\). Overall, a number will be encoded as a sequence of \\\\(\\\\lceil \\\\log_N(1000) \\\\rceil + 2\\\\) tokens, from a vocabulary of \\\\(202 + N\\\\) tokens.\\n\\nFor instance, \\\\(x = e\\\\pi \\\\approx 23.14069\\\\), will be represented by \\\\(+231\\\\cdot 10^{-1}\\\\), and encoded in \\\\(P_{10}\\\\) (base 10 positional) as the sequence \\\\([+,2,3,1,E-1]\\\\], and in \\\\(P_{1000}\\\\) (base 1000 positional) as \\\\([+,231,E-1]\\\\).\\n\\n\\\\(x = -0.5\\\\) will be represented as \\\\(-500\\\\cdot 10^{-3}\\\\), and encoded in \\\\(P_{10}\\\\) as \\\\([-5,0,0,E-3]\\\\), and in \\\\(P_{1000}\\\\) as \\\\([-500,E-3]\\\\).\\n\\nOther bases \\\\(N\\\\) could be considered, as well as different bases for the exponent, and different sizes of the mantissa. In this paper, we use \\\\(P_{10}\\\\) to encode numbers with absolute value in \\\\([10^{-100}, 10^{101}]\\\\) as sequences of 5 tokens, using a vocabulary of \\\\(213\\\\) tokens (10 digits, 2 signs, and 201 values of the exponent), and \\\\(P_{1000}\\\\) as sequences of 3 tokens, with a vocabulary of \\\\(1104\\\\).\\n\\nBalanced base \\\\(2^{a+1}\\\\) uses digits between \\\\(-a\\\\) and \\\\(a\\\\) (Knuth, 1997). For instance, in balanced base 11, digits range from \\\\(-5\\\\) to \\\\(5\\\\) (an every day example of a balanced base can be found in the way we state the hour as \\\"twenty to two\\\", or \\\"twenty past two\\\"). Setting \\\\(a\\\\) to \\\\(999\\\\), we define \\\\(B_{999}\\\\), and encode the sign an mantissa as a single token between \\\\(-999\\\\) and \\\\(999\\\\). Numbers are then encoded on two tokens with a vocabulary of \\\\(2004\\\\).\\n\\nFinally, we encode floating point numbers as unique tokens by rewriting any number \\\\(x = m \\\\cdot 10^b\\\\), with \\\\(m \\\\in [-999, 999[, b \\\\in [-\\\\frac{p+2}{2}, \\\\frac{p+2}{2}]\\\\), \\\\(p+2 = 0\\\\), \\\\(\\\\mathbb{Z}^2\\\\), and encoding it as the unique token \\\\(FP_m,b\\\\). This allows to represent numbers with 3 significant digits and a dynamic range of \\\\(10^p+2\\\\), using a vocabulary of \\\\((18p+2)10^3\\\\) tokens. In this paper, we use \\\\(p = 16\\\\): encoding numbers as unique tokens, with a vocabulary of \\\\(30,000\\\\) (\\\\(FP_{15}\\\\)).\\n\\n\\\\(B_L1, L_2\\\\) AND \\\\(L_\\\\infty\\\\) NORMS FOR EVALUATION\\n\\nWe evaluate the accuracy of our trained models by decoding model predictions and verifying that they approximate the correct solution up to a fixed tolerance \\\\(\\\\tau\\\\). In the general case, if the model predict a sequence \\\\(\\\\text{seq} P\\\\), and the solution of the problem is \\\\(O\\\\), we consider that the prediction is correct if \\\\(\\\\text{seq} P\\\\) can be decoded into a matrix \\\\(P\\\\) and \\\\(\\\\|P - O\\\\| < \\\\tau\\\\|O\\\\|\\\\)\\n\\nFor eigenvalue decomposition, we check that the solution \\\\((Q,D)\\\\) predicted by the model can reconstruct the input matrix, i.e. \\\\(\\\\|Q^T D Q - I\\\\| < \\\\tau\\\\|I\\\\|\\\\). For singular value decomposition, we check that \\\\(\\\\|USV - I\\\\| < \\\\tau\\\\|I\\\\|\\\\). For matrix inversion, we check that \\\\(\\\\|PI - Id\\\\| < \\\\tau\\\\|Id\\\\| = \\\\tau\\\\). All our published results use the norm \\\\(L_1\\\\):\\n\\\\[\\\\|A\\\\| = \\\\sum_{i,j} |a_{i,j}|,\\\\]\\nfor \\\\(A = (a_{i,j})\\\\). In this section, we discuss the impact of using different norms, namely \\\\(L_2\\\\) (\\\\(\\\\|A\\\\| = \\\\sum_{i,j} a_{i,j}^2\\\\)), or \\\\(L_\\\\infty\\\\) (\\\\(\\\\|A\\\\| = \\\\max_{i,j} |a_{i,j}|\\\\)).\\n\\nUsing \\\\(L_1\\\\) norm in equation 1 amounts to comparing the average absolute errors on the predicted coefficients \\\\((P - O)\\\\) to the average absolute value of coefficients of \\\\(O\\\\). Using \\\\(L_2\\\\) compares the squared errors, and will bias the estimation towards large absolute errors, and coefficients of \\\\(O\\\\) with large absolute values. \\\\(L_\\\\infty\\\\) will compare the largest absolute error to the largest coefficient in \\\\(|O|\\\\). The choice of the norm has different impact for different problems. Figure 1 presents learning curves using the three norms for our best models on different problems.\\n\\nFor basic arithmetic operations (transposition, addition, multiplication), there is little difference between \\\\(L_1\\\\) and \\\\(L_2\\\\) accuracies, and therefore no reason to prefer one over the other for model.\"}"}
{"id": "L2a_bcarHcF", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Figure 1: Learning accuracies for different problems measured with norms $L_1$, $L_2$ and $L_\\\\infty$.\\n\\nEvaluation. For eigen and singular value problems, $L_2$ accuracies reach a high value early during training, long before the model begins to learn according to the other norms. This is due to the fact that the eigenvalues of Wigner matrices tend to be regularly spaced over the interval $[-2\\\\sigma, 2\\\\sigma]$ ($\\\\sigma = \\\\sqrt{(n)_{s}}$ with $s$ the standard deviation of coefficients and $n$ the dimension of the matrix). This means that, in many cases, the model can predict the largest absolute eigenvalues from the bounds of the interval (which can be estimated from the dataset). For this reason, $L_2$ accuracy is not a good evaluation metric for the eigenvalue or singular value problem. This is particularly clear in the $10 \\\\times 10$ case: transformers struggle with such matrices, and $L_1$ and $L_\\\\infty$ accuracies remain very low even after a thousand epochs (300 million examples), but $L_2$ accuracy is close to 100% since the beginning of training. A similar phenomenon takes place for eigenvector calculations: $L_2$ and $L_\\\\infty$ accuracy rise steeply, long before the model begins to learn (according to the $L_1$ norm). This justifies the choice of $L_1$ as our evaluation norm.\"}"}
{"id": "L2a_bcarHcF", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2: Learning curves for different problems. All problems except addition use $5 \\\\times 5$ matrices. All models have 512 dimensions and $8/8$ heads (except when mentioned in the legend). Inversion models have $6/1$ layers. Epochs correspond to 300,000 training examples. Test loss is cross-entropy.\"}"}
{"id": "L2a_bcarHcF", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nFigure 3: Empirical distributions of eigenvalues for Wigner matrices of dimension 5x5 (left) to 20x20 (right), with uniform (top), gaussian (middle) and Laplace (bottom) coefficients. All distributions computed from 100 000 random matrices.\"}"}
