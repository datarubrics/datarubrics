{"id": "hpBTIv2uy_E", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "hpBTIv2uy_E", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "hpBTIv2uy_E", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Johannes Klicpera, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=H1gL-2A9Ym.\\n\\nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning, pp. 3744\u20133753. PMLR, 2019.\\n\\nGuoyin Li, Liqun Qi, and Gaohang Yu. The z-eigenvalues of a symmetric tensor and its application to spectral hypergraph theory. Numerical Linear Algebra with Applications, 20(6):1001\u20131029, 2013.\\n\\nPan Li and Olgica Milenkovic. Inhomogeneous hypergraph clustering with applications. Advances in Neural Information Processing Systems, 2017:2309\u20132319, 2017.\\n\\nPan Li and Olgica Milenkovic. Submodular hypergraphs: p-laplacians, cheeger inequalities and spectral clustering. In International Conference on Machine Learning, pp. 3014\u20133023. PMLR, 2018.\\n\\nPan Li, Hoang Dau, Gregory Puleo, and Olgica Milenkovic. Motif clustering and overlapping clustering for social network analysis. In IEEE INFOCOM 2017-IEEE Conference on Computer Communications, pp. 1\u20139. IEEE, 2017.\\n\\nPan Li, Gregory J Puleo, and Olgica Milenkovic. Motif and hypergraph correlation clustering. IEEE Transactions on Information Theory, 66(5):3065\u20133078, 2019.\\n\\nPan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design provably more powerful neural networks for graph representation learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 4465\u20134478. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/2f73168bf3656f697507752ec592c437-Paper.pdf.\\n\\nYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.\\n\\nDerek Lim, Xiuyu Li, Felix Hohne, and Ser-Nam Lim. New benchmarks for learning on non-homophilous graphs. arXiv preprint arXiv:2104.01404, 2021.\\n\\nLek-Heng Lim. Singular values and eigenvalues of tensors: a variational approach. In 1st IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing, 2005., pp. 129\u2013132. IEEE, 2005.\\n\\nDavid Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Scholkopf, and L\u00e9on Bottou. Discovering causal signals in images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6979\u20136987, 2017.\\n\\nLinyuan L\u00fc and Tao Zhou. Link prediction in complex networks: A survey. Physica A: statistical mechanics and its applications, 390(6):1150\u20131170, 2011.\\n\\nJiaqi Ma, Bo Chang, Xuefei Zhang, and Qiaozhu Mei. CopulaGNN: Towards integrating representational and correlational roles of graphs in graph neural networks. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=XI-OJ5yyse.\\n\\nRyan L. Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=BJluy2RcFm.\\n\\nLawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford InfoLab, 1999.\"}"}
{"id": "hpBTIv2uy_E", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "hpBTIv2uy_E", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A BSTRACT\\n\\nHypergraphs are used to model higher-order interactions amongst agents and there exist many practically relevant instances of hypergraph datasets. To enable the efficient processing of hypergraph data, several hypergraph neural network platforms have been proposed for learning hypergraph properties and structure, with a special focus on node classification tasks. However, almost all existing methods use heuristic propagation rules and offer suboptimal performance on benchmarking datasets. We propose AllSet, a new hypergraph neural network paradigm that represents a highly general framework for (hyper)graph neural networks and for the first time implements hypergraph neural network layers as compositions of two multiset functions that can be efficiently learned for each task and each dataset. The proposed AllSet framework also for the first time integrates Deep Sets and Set Transformers with hypergraph neural networks for the purpose of learning multiset functions and therefore allows for significant modeling flexibility and high expressive power. To evaluate the performance of AllSet, we conduct the most extensive experiments to date involving ten known benchmarking datasets and three newly curated datasets that represent significant challenges for hypergraph node classification. The results demonstrate that our method has the unique ability to either match or outperform all other hypergraph neural networks across the tested datasets: As an example, the performance improvements over existing methods and a new method based on heterogeneous graph neural networks are close to 4% on the Yelp and Zoo datasets, and 3% on the Walmart dataset. Our AllSet network implementation is available online.\\n\\nINTRODUCTION\\n\\nGraph-centered machine learning, and especially graph neural networks (GNNs), have attracted great interest in the machine learning community due to the ubiquity of graph-structured data and the importance of solving numerous real-world problems such as semi-supervised node classification and graph classification (Zhu, 2005; Shervashidze et al., 2011; L\u00fc & Zhou, 2011). Graphs model pairwise interactions between entities, but fail to capture more complex relationships. Hypergraphs, on the other hand, involve hyperedges that can connect more than two nodes, and are therefore capable of representing higher-order structures in datasets. There exist many machine learning and data mining applications for which modeling high-order relations via hypergraphs leads to better learning performance when compared to graph-based models (Benson et al., 2016). For example, in subspace clustering, in order to fit a \\\\(d\\\\)-dimensional subspace, we need at least \\\\(d+1\\\\) data points (Agarwal et al., 2005); in hierarchical species classification of a FoodWeb, a carbon-flow unit based on four species is significantly more predictive than that involving two or three entities (Li & Milenkovic, 2017). Hence, it is desirable to generalize GNN concepts to hypergraphs.\\n\\nOne straightforward way to generalize graph algorithms for hypergraphs is to convert hypergraphs to graphs via clique-expansion (CE) (Agarwal et al., 2005; Zhou et al., 2006). CE replaces hyperedges by (possibly weighted) cliques. Many recent attempts to generalize GNNs to hypergraphs can be viewed as redefining hypergraph propagation schemes based on CE or its variants (Yadati et al., 2017).\"}"}
{"id": "hpBTIv2uy_E", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Despite the simplicity of CE, it is well-known that CE causes distortion and leads to undesired losses in learning performance (Hein et al., 2013; Li & Milenkovic, 2018; Chien et al., 2019b). In parallel, more sophisticated propagation rules directly applicable on hypergraphs, and related to tensor eigenproblems, have been studied as well. One such example, termed Multilinear PageR-rank (Gleich et al., 2015), generalizes PageRank techniques (Page et al., 1999; Jeh & Widom, 2003) directly to hypergraphs without resorting to the use of CE. Its propagation scheme is closely related to the Z eigenproblem which has been extensively investigated in tensor analysis and spectral hypergraph theory (Li et al., 2013; He & Huang, 2014; Qi & Luo, 2017; Pearson & Zhang, 2014; Gautier et al., 2019). An important result of Benson et al. (2017) shows that tensor-based propagation outperforms a CE-based scheme on several tasks. The pros and cons of these two types of propagation rule in statistical learning frameworks were examined in Chien et al. (2021a). More recently, it was shown in Tudisco et al. (2020) that label propagation based on CE of hypergraphs does not always lead to acceptable performance. Similarly to Chien et al. (2021a), Benson (2019) identified positive traits of CE eigenvectors but argued in favor of using Z eigenvectors due to their more versatile nonlinear formulation compared to that of the eigenvectors of CE graphs.\\n\\nWe address two natural questions pertaining to learning on hypergraphs: \u201cIs there a general framework that includes CE-based, Z-based and other propagations on hypergraphs?\u201d and, \u201cCan we learn propagation schemes for hypergraph neural networks suitable for different datasets and different learning tasks?\u201d We give affirmative answers to both questions. We propose a general framework, AllSet, which includes both CE-based and tensor-based propagation rules as special cases. We also propose two powerful hypergraph neural network layer architectures that can learn adequate propagation rules for hypergraphs using multiset functions. Our specific contributions are as follows.\\n\\n1. We show that using AllSet, one can not only model CE-based and tensor-based propagation rules, but also cover propagation methods of most existing hypergraph neural networks, including HyperGCN (Yadati et al., 2019), HGNN (Feng et al., 2019), HCHA (Bai et al., 2021), HNHN (Dong et al., 2020) and HyperSAGE (Arya et al., 2020). Most importantly, we show that all these propagation rules can be described as a composition of two multiset functions (leading to the proposed method name AllSet). Furthermore, we also show that AllSet is a hypergraph generalization of Message Passing Neural Networks (MPNN) (Gilmer et al., 2017), a powerful graph learning framework encompassing many GNNs such as GCN (Kipf & Welling, 2017) and GAT (Veli\u010dkovi\u0107 et al., 2018).\\n\\n2. Inspired by Deep Sets (Zaheer et al., 2017) and Set Transformer (Lee et al., 2019), we propose AllDeepSets and AllSetTransformer layers which are end-to-end trainable. They can be plugged into most types of graph neural networks to enable effortless generalizations to hypergraphs. Notably, our work represents the first attempt to connect the problem of learning multiset function with hypergraph neural networks, and to leverage the powerful Set Transformer model in the design of these specialized networks.\\n\\n3. We report, to the best of our knowledge, the most extensive experiments in the hypergraph neural network literature pertaining to semi-supervised node classification. Experimental results against ten baseline methods on ten benchmark datasets and three newly curated and challenging datasets demonstrate the superiority and consistency of our AllSet approach. As an example, AllSetTransformer outperforms the best baseline method by close to 4% in accuracy on Yelp and Zoo datasets and 3% on the Walmart dataset; furthermore, AllSetTransformer matches or outperforms the best baseline models on nine out of ten datasets. Such improvements are not possible with modifications of HAN (Wang et al., 2019b), a heterogeneous GNN, adapted to hypergraphs or other specialized approaches that do not use Set Transformers.\\n\\n4. As another practical contribution, we also provide a succinct pipeline for standardization of the hypergraph neural networks evaluation process based on Pytorch Geometric (Fey & Lenssen, 2019). The pipeline is built in a fashion similar to that proposed in recent benchmarking GNNs papers (Hu et al., 2020a; Lim et al., 2021). The newly introduced datasets, along with our reported testbed, may be viewed as an initial step toward benchmarking hypergraph neural networks.\\n\\nAll proofs and concluding remarks are relegated to the Appendix.\"}"}
{"id": "hpBTIv2uy_E", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Combining the two equations leads to the HyperGCN layer from (25).\\n\\nNext, we show that HNHN layer introduced in Dong et al. (2020) is a special case of AllSet (5). The definition of HNHN layer is as follows\\n\\n\\\\[\\nZ(t+1)_e, : = \\\\sigma \\\\left[ \\\\sum_{u \\\\in e} d_{u,r,\\\\beta} X(t)_u, : \\\\right] \\\\Theta(t) + b(t),\\n\\\\]\\n\\n(30)\\n\\n\\\\[\\nX(t+1)_v, : = \\\\sigma \\\\left[ \\\\sum_{e : v \\\\in e} d_{v,l,\\\\alpha} Z(t+1)_e, : \\\\right] \\\\Theta(t) + b(t),\\n\\\\]\\n\\n(31)\\n\\nwhere \\\\(d_{e,l,\\\\beta} = \\\\sum_{u \\\\in e} |d_u|^{\\\\beta} \\\\) and \\\\(d_{u,r,\\\\beta} = d_{u}^{\\\\beta} \\\\).\\n\\nNote that \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are two hyperparameters that can be tuned in HNHN. As before, \\\\(\\\\sigma(\\\\cdot)\\\\) is a non-linear activation function (e.g., ReLU). It is obvious that we can choose \\\\(f_{V \\\\rightarrow E}\\\\) and \\\\(f_{E \\\\rightarrow V}\\\\) according to (30) and (31), respectively. This is due to the fact that these expressions only involve degree normalizations and linear transformations.\\n\\nFinally, we show that the HyperSAGE layer from Arya et al. (2020) is a special case of AllSet (5). The HyperSAGE layer update rules are as follows:\\n\\n\\\\[\\nZ(t+1)_e, : = \\\\left( \\\\sum_{u \\\\in e} \\\\frac{X(t)_u, :}{|e|} \\\\right)^{1/p},\\n\\\\]\\n\\n(33)\\n\\n\\\\[\\nX(t+1), \\\\star_v, : = \\\\left( \\\\sum_{e : v \\\\in e} \\\\frac{Z(t)_e, :}{|\\\\{e \\\\in e : v \\\\in e\\\\}|} \\\\right)^{1/p} + X(t)_v, : ,\\n\\\\]\\n\\n(34)\\n\\n\\\\[\\nX(t+1)_v, : = \\\\sigma \\\\left( \\\\left| X(t+1), \\\\star_v, : \\\\right| \\\\right) \\\\Theta(t+1).\\n\\\\]\\n\\n(35)\\n\\nwhere \\\\(\\\\sigma(\\\\cdot)\\\\) is a nonlinear activation function. The update (33) can be recovered by simply choosing \\\\(f_{V \\\\rightarrow E}\\\\) to be the \\\\(l_p\\\\) (power) mean. For the \\\\(f_{E \\\\rightarrow V}\\\\) component, we first model \\\\(f_{E \\\\rightarrow V}\\\\) as a composition of another two multiset functions. The first is the \\\\(l_p\\\\) (power) mean with respect to its first input, while the second is addition with respect to the second input. This recovers (34). The second of the two defining functions can be chosen as (35), which is also a multiset function. This procedure leads to \\\\(f_{E \\\\rightarrow V}\\\\). This completes the proof of the first claim pertaining to the universality of AllSet.\\n\\nTo address the claim that the above described hypergraph neural network layers have strictly smaller expressive power then AllSet, one only has to observe that these hypergraph neural network layers cannot approximate arbitrary multiset functions in either the \\\\(f_{V \\\\rightarrow E}\\\\) or \\\\(f_{E \\\\rightarrow V}\\\\) component. We discuss both these functions separately.\\n\\nWith regards to the HGNN layer, it is clear that two linear transformations cannot model arbitrary multiset functions such as the product in Zprop (4). For the HCHA layer, we note that if all node features are identical and the hyperedge features are all-zero vectors, all the attention weights \\\\(\\\\alpha_{ue}\\\\) are the same. This setting is similar to that described for the HGNN layer and thus this rule cannot model arbitrary multiset functions. For the HyperGCN layer (25), consider a 3-uniform hypergraph as its input. In this case, by definition, the weights \\\\(w_{uv,e}\\\\) are all equal. Again, using only linear transformations one cannot model the product operation in Zprop (4); the same argument holds even when degree normalizations are included. For the HNHN layer, note that the \\\\(E \\\\rightarrow V\\\\) component of (31) is just one MLP layer that follows the sum \\\\(\\\\sum_{e : v \\\\in e}\\\\), which cannot model arbitrary multiset functions based on the results of Deep Sets (Zaheer et al., 2017). As a final note, we point out that the HyperSAGE layer involves only one learnable matrix \\\\(\\\\Theta\\\\) and is hence also a linear transformation, which cannot model arbitrary multiset functions.\\n\\nThis completes the proof.\"}"}
{"id": "hpBTIv2uy_E", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The propagation rule of MPNN reads as follows:\\n\\n\\\\[\\nm(t+1)v, : = \\\\sum_{u \\\\in N(v)} M_t(X(t)u, : , X(t)v, : , Z(0), v, e, : ) \\\\]\\n\\\\[\\nX(t+1)v, : = U_t(X(t)v, : , m(t+1)v, : ). \\\\tag{36}\\n\\\\]\\n\\nHere, \\\\(m\\\\) denotes the message obtained by aggregating messages from the neighborhood of the node \\\\(v\\\\), while \\\\(M_t\\\\) and \\\\(U_t\\\\) are certain functions selected at the \\\\(t\\\\)-th step of propagation. To recover MPNN from AllSet (6), we simply choose \\\\(f_{V \\\\rightarrow E}\\\\) and \\\\(f_{E \\\\rightarrow V}\\\\) as\\n\\n\\\\[\\nZ(t+1), v, e, : \\\\equiv f_{V \\\\rightarrow E}(V_e \\\\setminus v, X(t)); Z(t), v, e, : ) = f_{V \\\\rightarrow E}(\\\\{X(t)u, : \\\\}; Z(t), v, e, : ) = X(t)u, : \\\\| Z(0), v, e, : \\\\]\\n\\\\[\\ns.t. u \\\\in e \\\\setminus v, X(t+1)v, : \\\\equiv f_{E \\\\rightarrow V}(E_v, Z(t+1), v, ; X(t)v, : ) = f_{E \\\\rightarrow V}(\\\\{Z(t+1), v, e, : \\\\} e \\\\setminus v; X(t)v, : ) = U_t(X(t)v, : , \\\\sum_{e \\\\in e \\\\setminus v} M'_t(Z(t+1), v, e, : , X(t)v, : )). \\\\tag{37}\\n\\\\]\\n\\nHence, MPNN is a special case of AllSet for graph inputs.\\n\\nThe proof is largely based on the proof of Proposition 2 of the Set Transformer paper (Lee et al., 2019) but is nevertheless included for completeness. We ignore all layer normalizations as in the proof of (Lee et al., 2019) and start with the following theorem.\\n\\nTheorem F.1 (Theorem 1 in Lee et al. (2019))\\n\\nFunctions of the form MLP \\\\(\\\\sum (\\\\text{MLP})\\\\) are universal approximators in the space of permutation invariant functions.\\n\\nThis result is based on Zaheer et al. (2017). According to Wagstaff et al. (2019), we have the constraint that the input multiset has to be finite.\\n\\nNext, we show that the simple mean operator is a special case of a multihead attention module. For simplicity, we consider the case of a single head, which corresponds to \\\\(h = 1\\\\), as we can always set the other heads to zero by choosing MLP \\\\(V,i(S) = 0\\\\) for all \\\\(i \\\\geq 2\\\\). Following the proof of Lemma 3 in Lee et al. (2019), we set the learnable weight \\\\(\\\\theta\\\\) to be the all-zero vector and use \\\\(\\\\omega(\\\\cdot) = 1 + g(\\\\cdot)\\\\) (element-wise) as an activation function such that \\\\(g(0) = 0\\\\). The MH \\\\(1, \\\\omega\\\\) module in (8) then becomes\\n\\n\\\\[\\n\\\\text{MH}(\\\\theta, S, S) = O(1) \\\\equiv \\\\omega(\\\\theta(\\\\text{MLP}(K, 1(S)))) T \\\\text{MLP}(V, 1(S)) = |S| \\\\sum_{i=1}^{\\\\text{MLP}(V, 1(S))} i. \\\\tag{38}\\n\\\\]\\n\\nThe AllSetTranformer layer (8) takes the form\\n\\n\\\\[\\nf_{V \\\\rightarrow E}(S) = Y + \\\\text{MLP}(Y),\\n\\\\]\\n\\nwhere \\\\(Y = |S| \\\\sum_{i=1}^{\\\\text{MLP}(V, 1(S))} i\\\\). \\\\(\\\\tag{39}\\\\)\\n\\nNote that \\\\(Y\\\\) is a vector. Clearly, we can choose an MLP such that \\\\(\\\\text{MLP}(Y)\\\\) equals to another \\\\(\\\\text{MLP}(Y)\\\\) that results in subtracting \\\\(Y\\\\). Thus, we have:\\n\\n\\\\[\\nf_{V \\\\rightarrow E}(S) = \\\\text{MLP}(Y) = \\\\text{MLP} \\\\left( \\\\left| S \\\\right| \\\\sum_{i=1}^{\\\\text{MLP}(V, 1(S))} i \\\\right). \\\\tag{40}\\\\]\\n\\nBy Theorem F.1, it is clear that \\\\(f_{V \\\\rightarrow E}(S)\\\\) is a universal approximator for permutation invariant functions. The same analysis applies to \\\\(f_{E \\\\rightarrow V}(S)\\\\).\\n\\nLine expansion (LE) is a procedure that transform a hypergraph or a graph into a homogeneous graph; a node in the LE graph represents a pair of node-hyperedge from the original hypergraph.\"}"}
{"id": "hpBTIv2uy_E", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Nodes in the LE graph are linked if and only if there is a nonempty intersection of their associated node-hyperedge pairs. As stated by the authors of LEGCN (Yang et al., 2020), for a fully connected $d$-uniform hypergraph their resulting LE graph has $\\\\Theta(d |E|)$ nodes and $\\\\Theta(d^2 |E|^2)$ edges. This expansion hence leads to very large graphs which require large memory units and have high computational complexity when coupled with GNNs such as GCN. To alleviate this drawback, the authors use random sampling techniques which unfortunately lead to an undesired information loss. In contrast, we define our AllSet layers directly on hypergraph which leads to a significantly more efficient approach.\\n\\nAs outlined in the main text, in this case we associate a $d$-uniform hypergraph $G$ with an adjacency tensor $A$. The H eigenproblem for $A$ states that\\n\\n$$Ax^{d-1} = \\\\lambda x^{d-1},$$\\n\\nwhere $x^{d-1}_i = (x_i)^{d-1}$.\\n\\n(41)\\n\\nSimilar to Zprop, we can define $Hprop$ (41) in a node-wise fashion as\\n\\n$$Hprop: X(t+1)_{v} = \\\\sum_{e:v \\\\in e} (d-1) \\\\prod_{u:u \\\\in e \\\\setminus v} X(t)_{u},$$\\n\\n$$^1\\\\frac{d}{d-1}.$$ \\n\\n(42)\\n\\nAlthough taking the $^{1/(d-1)}$-th root resolves the unit issue that exists in Zprop (4), it may lead to imaginary features during updates. It remains an open question to formulate $Hprop$ in a manner that ensures that features remain real-valued during propagation.\\n\\n**Additional Details Concerning the Tested Datasets**\\n\\nTable 3: Full dataset statistics: $|e|$ refers to the size of the hyperedges while $d_v$ refers to the node degree.\\n\\n| Dataset          | $|V|$ | $|E|$ | #feature | #class | max $|e|$ | min $|e|$ | ave $|e|$ | med $|e|$ | max $d_v$ | min $d_v$ | ave $d_v$ | med $d_v$ |\\n|------------------|------|------|---------|--------|---------|---------|----------|----------|-----------|---------|-----------|----------|\\n| Cora             | 2708 | 1579 | 1433    | 7      | 5       | 2       | 3.03     | 3        | 145       | 0       | 1.77      | 1        |\\n| Citeseer         | 3312 | 1079 | 3703    | 6      | 26      | 2       | 3.2      | 2        | 88        | 0       | 1.04      | 0        |\\n| Pubmed           | 19717| 7963 | 500     | 3      | 171     | 2       | 4.35     | 3        | 99        | 0       | 1.76      | 0        |\\n| Cora-CA          | 2708 | 1072 | 1433    | 7      | 43      | 2       | 4.28     | 3        | 23        | 0       | 1.69      | 2        |\\n| DBLP-CA          | 41302| 22363| 1425    | 6      | 202     | 1       | 4.45     | 3        | 18        | 1       | 2.41      | 1        |\\n| Zoo              | 101  | 43   | 16      | 7      | 93      | 1       | 39.93    | 40       | 7855      | 2       | 44        | 4        |\\n| 20News           | 16242| 100  | 100     | 4      | 4563    | 29      | 654.51   | 136      | 5         | 5       | 89.12     | 5        |\\n| Mushroom         | 8124 | 298  | 22      | 2      | 2241    | 1       | 224.11   | 1908     | 5         | 5       | 34.72     | 4        |\\n| NTU2012          | 2012 | 2012 | 100     | 67     | 5       | 5       | 5        | 5        | 12311     | 1       | 5         | 1        |\\n| ModelNet40       | 1290 | 679302| 1862    | 40     | 2838    | 2       | 869302   | 341      | 50758     | 1       | 69906     | 2        |\\n| Yelp             | 88860| 69906| 100     | 11     | 81      | 2       | 69906    | 69906    | 5733      | 1       | 69906     | 1        |\\n\\nWe use 10 available benchmark datasets from the existing hypergraph neural networks literature and introduce three newly created datasets as described in the exposition to follow. The benchmark datasets include cocitation networks Cora, Citeseer and Pubmed, obtained from Yadati et al. (2019). The coauthorship networks Cora-CA and DBLP-CA are also adapted from Yadati et al. (2019). In the cocitation and coauthorship networks datasets, the node features are the bag-of-words representations of the corresponding documents. Datasets from the UCI Categorical Machine Learning Repository (Dua & Graff, 2017) include 20Newsgroups, Mushroom and Zoo. In 20Newsgroups, the node features are the TF-IDF representations of news messages. In the Mushroom dataset, the node features represent categorical descriptions of 23 species of mushrooms. In Zoo, the node features are a mix of categorical and numerical measurements describing different animals. The computer vision and graphics datasets include the Princeton CAD ModelNet40 (Wu et al., 2015) and the NTU2012 3D dataset (Chen et al., 2003). The visual objects contain features extracted using Group-View Convolutional Neural Network (GVCNN) (Feng et al., 2018) and Multi-View Convolutional Neural Network.\"}"}
{"id": "hpBTIv2uy_E", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Network (MVCNN) (Su et al., 2015). The hypergraph construction follows the setting described in Feng et al. (2019); Yang et al. (2020).\\n\\nThe three newly introduce datasets are adapted from Yelp (Yelp), House (Chodrow et al., 2021) and Walmart (Amburg et al., 2020). For Yelp, we selected all businesses in the \u201crestaurant\u201d catalog as our nodes, and formed hyperedges by selecting restaurants visited by the same user. We use the number of stars in the average review of a restaurant as the corresponding node label, starting from 1 and going up to 5 stars, with an interval of 0.5 stars. We then form the node features from the latitude, longitude, one-hot encoding of city and state, and bag-of-word encoding of the top-1000 words in the name of the corresponding restaurants. In the House dataset, each node is a member of the US House of Representatives and hyperedges are formed by grouping together members of the same committee. Node labels indicate the political party of the representatives. In Walmart, nodes represent products being purchased at Walmart, and hyperedges equal sets of products purchased together; the node labels are the product categories. Since there are no node features in the original House and Walmart dataset, we impute the same using Gaussian random vectors, in a manner similar to what was done for the contextual stochastic block model (Deshpande et al., 2018). In both datasets, we fix the feature vector dimension to 100 and use one-hot encodings of the labels with added Gaussian noise \\\\( N(0, \\\\sigma^2 I) \\\\) as the actual features. The noise standard deviation \\\\( \\\\sigma \\\\) is chosen to be 1 and 0.6.\\n\\nJ C O M P U T A T I O N A L E F F I C I E N C Y A N D E X P E R I M E N T A L S E T T I N G S\\n\\nAll our test were executed on a Linux machine with 48 cores, 376 GB of system memory, and two NVIDIA Tesla P100 GPUs with 12 GB of GPU memory each. In AllSetTransformer, we also used a single layer MLP at the end for node classification. The average training times with their corresponding standard deviations in second per run, for all baseline methods at all tested datasets, are reported in Table 4. Note that the reported times do not include any preprocessing times for the hypergraph datasets as these are only performed once before training. Table 4 lists the average training time per run with the optimal set of hyperparameters obtained after tunning; the average training times per run for different sets of hyperparameters used in the tuning process are reported in Table 5.\\n\\nChoices of hyperparameters.\\n\\nWe tune the hidden dimension of all hypergraph neural networks over \\\\( \\\\{64, 128, 256, 512\\\\} \\\\), except for the case of HyperGCN and HAN, where the original implementations do not allow for changing the hidden dimension. For HNHN, AllSetTransformer and AllDeepSets we use one layer (a full \\\\( V \\\\rightarrow E \\\\rightarrow V \\\\) propagation rule layer), while for all the other methods we use two layers as recommended in the previous literature. We tune the learning rate over \\\\( \\\\{0.1, 0.01, 0.001\\\\} \\\\), and the weight decays over \\\\( \\\\{0, 0.00001\\\\} \\\\), for all models under consideration. For models with multihead attentions, we also tune the number of heads over the set \\\\( \\\\{1, 4, 8\\\\} \\\\).\\n\\nThe best hyperparameters for each model and dataset are listed in Table 6. Note that we only use default setting for HAN due to facts that its DGL implementation has all parameters set as constants and its much higher time complexity per run.\"}"}
{"id": "hpBTIv2uy_E", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hypergraph learning.\\n\\nLearning on hypergraphs has attracted significant attention due to its ability to capture higher-order structural information (Li et al., 2017; 2019). In the statistical learning and information theory literature, researchers have studied community detection problem for hypergraph stochastic block models (Ghoshdastidar & Dukkipati, 2015; Chien et al., 2018; 2019a). In the machine learning community, methods that mitigate drawbacks of CE have been reported in (Li & Milenkovic, 2017; Chien et al., 2019b; Hein et al., 2013; Chan et al., 2018). Despite these advances, hypergraph neural network developments are still limited beyond the works covered by the AllSet paradigm. Exceptions include Hyper-SAGNN (Zhang et al., 2019), LEGCN (Yang et al., 2020) and MPNN-R (Yadati, 2020), the only three hypergraph neural networks that do not represent obvious special instances of AllSet. But, Hyper-SAGNN focuses on hyperedge prediction while we focus on semi-supervised node classification. LEGCN first transforms hypergraphs into graphs via line expansion and then applies a standard GCN. Although the transformation used in LEGCN does not cause a large distortion as CE, it significantly increases the number of nodes and edges. Hence, it is not memory and computationally efficient (more details about this approach can be found in the Appendix G). MPNN-R focuses on recursive hypergraphs (Joslyn & Nowak, 2017), which is not the topic of this work. It is an interesting future work to investigate the relationship between AllSet and Hyper-SAGNN or MPNN-R when extending AllSet to hyperedge prediction tasks or recursive hypergraph implementations.\\n\\nFurthermore, Tudisco & Higham (2021); Tudisco et al. (2021) also proposed to develop and analyze hypergraph propagations that operate \u201cbetween\u201d CE-prop and Z-prop rules. These works focus on the theoretical analysis of special types of propagation rules, which can also be modeled by our AllSet frameworks. Importantly, our AllSet layers do not fix the rules but instead learn them in an adaptive manner. Also, one can view Tudisco & Higham (2021); Tudisco et al. (2021) as the hypergraph version of SGC, which can help with interpreting the functionalities of AllSet layers.\\n\\nStar expansion, UniGNN and Heterogeneous GNNs.\\n\\nA very recent work concurrent to ours, UniGNN (Huang & Yang, 2021), also proposes to unify hypergraph and GNN models. Both Huang & Yang (2021) and our work can be related to hypergraph star expansion (Agarwal et al., 2006; Yang et al., 2020), which results in a bipartite graph (see Figure 1). One particular variant, UniGIN, is related to our AllDeepSets model, as both represent a hypergraph generalization of GIN (Xu et al., 2019). However, UniGNN does not make use of deep learning methods for learning multiset functions, which is crucial for identifying the most appropriate propagation and aggregation rules for individual datasets and learning tasks as done by AllSetTransformer (see Section 6 for supporting information). Also, the most advanced variant of UniGNN, UniGCNII, is not comparable to AllSetTransformers. Furthermore, one could also naively try to apply heterogeneous GNNs, such as HAN (Wang et al., 2019b), to hypergraph datasets converted into bipartite graphs. This approach was not previously examined in the literature, but we experimentally tested it nevertheless to show that the performance improvements in this case are significantly smaller than ours and that the method does not scale well even for moderately sized hypergraphs. Another very recent work (Xue et al., 2021) appears at first glance similar to ours, but it solves a very different problem by transforming a heterogeneous bipartite graph into a hypergraph and then apply standard GCN layers to implement $f_{V \\\\rightarrow E}$ and $f_{E \\\\rightarrow V}$, which is more related to HNHN (Dong et al., 2020) and UniGCN (Huang & Yang, 2021) and very different from AllSetTransformer.\\n\\nLearning (multi)set functions.\\n\\n(Multi)set functions, which are also known as pooling architectures for (multi)sets, have been used in numerous problems including causality discovery (Lopez-Paz et al., 2017), few-shot image classification (Snell et al., 2017) and conditional regression and classification (Garnelo et al., 2018). The authors of Zaheer et al. (2017) provide a universal way to parameterize the (multi)set functions under some mild assumptions. Similar results have been proposed independently by the authors of Qi et al. (2017) for computer vision applications. The authors of Lee et al. (2019) propose to learn multiset functions via attention mechanisms. This further inspired the work in Baek et al. (2021), which adopted the idea for graph representation learning. The authors of Wagstaff et al. (2019) discuss the limitation of Zaheer et al. (2017), and specifically focus on continuous functions. A more general treatment of how to capture complex dependencies among multiset elements is available in Murphy et al. (2019). To the best of our knowledge, this work is the first to build the connection between learning multiset functions with propagations on hypergraph.\"}"}
{"id": "hpBTIv2uy_E", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Dataset statistics:\\n\\n| Dataset     | V   | E   | #feature | #class | max e |\\n|-------------|-----|-----|----------|--------|-------|\\n| Cora        | 2708|1579 | 1433     | 7      | 5     |\\n| Citeseer    | 3312|1079 | 3703     | 6      | 26    |\\n| Pubmed      | 19717|7963 | 500      | 3      | 171   |\\n| Cora-CA     | 2708|1072 | 1433     | 7      | 43    |\\n| DBLP-CA     | 41302|22363| 1425     | 6      | 202   |\\n| Zoo         | 101 | 43  | 16       | 7      | 93    |\\n| 20News      | 16242|100  | 100      | 4      | 22    |\\n| Mushroom    | 8124 |298  | 100      | 2      | 100   |\\n| NTU2012     | 2012|2012 | 100      | 67     | 12311 |\\n| ModelNet40  | 1290|679302| 1862     | 40     | 50758 |\\n| Yelp        | 88860|341  | 100      | 9      | 25    |\\n| Walmart     | 69906|341  | 100      | 2      | 2838  |\\n\\nEXPERIMENTS\\n\\nWe focus on semi-supervised node classification in the transductive setting. We randomly split the data into training/validation/test samples using (50% / 25% / 25%) splitting percentages. We aggregate the results of 20 experiments using multiple random splits and initializations.\\n\\nMethods used for comparative studies.\\n\\nWe compare our AllSetTransformer and AllDeepSets with ten baseline models, MLP, CE+GCN, CE+GAT, HGNN (Feng et al., 2019), HCHA (Bai et al., 2021), HyperGCN (Yadati et al., 2019), HNHN (Dong et al., 2020), UniGCNII (Huang & Yang, 2021) and HAN (Wang et al., 2019b), with both full batch and mini-batch settings. All architectures are implemented using the Pytorch Geometric library (PyG) (Fey & Lenssen, 2019) except for HAN, in which case the implementation was retrieved from Deep Graph Library (DGL) (Wang et al., 2019a). The implementation of HyperGCN is used in the same form as reported in the official repository. We adapted the implementation of HCHA from PyG. We also reimplemented HNHN and HGNN using the PyG framework. Note that in the original implementation of HGNN, propagation is performed via matrix multiplication which is far less memory and computationally efficient when compared to our implementation. MLP, GCN and GAT are executed directly from PyG. The UniGCNII code is taken from the official site. For HAN, we tested both the full batch training setting and a mini-batch setting provided by DGL. We treated the vertices (V) and hyperedges (E) as two heterogeneous types of nodes, and used two metapaths: {V \u2192E \u2192V, E \u2192V \u2192E} in its semantic attention. In the mini-batch setting, we used the DGL's default neighborhood sampler with a number of neighbors set to 32 during training and 64 for validation and testing purposes. In both settings, due to the specific conversion of hyperedges into nodes, our evaluations only involve the original labeled vertices in the hypergraph and ignore the hyperedge node-proxies. More details regarding the experimental settings and the choices of hyperparameters are given in the Appendix J.\\n\\nBenchmarking and new datasets.\\n\\nWe use ten available datasets from the existing hypergraph neural networks literature and three newly curated datasets from different application domains. The benchmark datasets include cocitation networks Cora, Citeseer and Pubmed, downloaded from Yadati et al. (2019). The coauthorship networks Cora-CA and DBLP-CA were adapted from Yadati et al. (2019). We also tested datasets from the UCI Categorical Machine Learning Repository (Dua & Graff, 2017), including 20Newsgroups, Mushroom and Zoo. Datasets from the area of computer vision and computer graphics include ModelNet40 (Wu et al., 2015) and NTU2012 (Chen et al., 2003). The hypergraph construction follows the recommendations from Feng et al. (2019); Yang et al. (2020). The three newly introduced datasets are adaptations of Yelp (Yelp), House (Chodrow et al., 2021) and Walmart (Amburg et al., 2020). Since there are no node features in the original datasets of House and Walmart, we use Gaussian random vectors instead, in a fashion similar to what was proposed for Contextual stochastic block models (Deshpande et al., 2018). We use postfix (x) to indicate the standard deviation of the added Gaussian features. The partial statistics of all datasets are provided in Table 1, and more details are available in the Appendix I.\\n\\nResults.\\n\\nWe use accuracy (micro-F1 score) as the evaluation metric, along with its standard deviation. The relevant findings are summarized in Table 2. The results show that AllSetTransformer is the most robust hypergraph neural network model and that it has the best overall performance when compared to state-of-the-art and new models such as hypergraph HAN. In contrast, all baseline methods perform poorly on at least two datasets. For example, UniGCNII and HAN are two of the best performing baseline models according to our experiments. However, UniGCNII performs poorly on Zoo, Yelp and Walmart when compared to our AllSetTransformer. More precisely, AllSetTransformer outperforms UniGCNII by 11.01% in accuracy on the Walmart(1) dataset. HAN has poor performance when compared AllSetTransformer on Mushroom, NTU2012 and ModelNet40. Further details are available in the Appendix J.\"}"}
{"id": "hpBTIv2uy_E", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Results for the tested datasets: Mean accuracy (% \u00b1 standard deviation. Boldfaced letters shaded grey are used to indicate the best result, while blue shaded boxes indicate results within one standard deviation of the best result. NA indicates that the method has numerical precision issue.\\n\\nFor HAN\u2217, additional preprocessing of each dataset is required (see the Section 6 for more details).\\n\\n| Dataset       | AllSetTransformer | AllDeepSets  | MLP           | CECGN        | CEGAT        | HNHN         | HGNN         | HCHA         | HyperGCN     | UniGCNII     | HAN (full batch) | HAN (mini batch) |\\n|---------------|-------------------|--------------|---------------|--------------|--------------|--------------|--------------|--------------|---------------|---------------|------------------|------------------|\\n|               | 78.59 \u00b1 1.47      | 76.88 \u00b1 1.80 | 75.17 \u00b1 1.21  | 76.17 \u00b1 1.39 | 76.41 \u00b1 1.53 | 76.36 \u00b1 1.92 | 79.39 \u00b1 1.36 | 79.14 \u00b1 1.02 | 78.45 \u00b1 1.26  | 78.81 \u00b1 1.05  | 80.18 \u00b1 1.15     | 79.70 \u00b1 1.77     |\\n|               | 73.08 \u00b1 1.20      | 70.83 \u00b1 1.63 | 72.67 \u00b1 1.56  | 70.16 \u00b1 1.31 | 70.63 \u00b1 1.30 | 72.64 \u00b1 1.57 | 72.45 \u00b1 1.16 | 72.42 \u00b1 1.42 | 71.28 \u00b1 0.82  | 73.05 \u00b1 2.21  | 74.05 \u00b1 1.43     | 74.12 \u00b1 1.52     |\\n|               | 88.72 \u00b1 0.37      | 88.75 \u00b1 0.33 | 87.47 \u00b1 0.51  | 86.45 \u00b1 0.43 | 86.81 \u00b1 0.42 | 86.90 \u00b1 0.30 | 86.44 \u00b1 0.44 | 86.59 \u00b1 0.29 | 82.84 \u00b1 8.67  | 88.25 \u00b1 0.40  | 84.04 \u00b1 1.02     | 85.32 \u00b1 2.25     |\\n|               | 83.63 \u00b1 1.47      | 81.97 \u00b1 1.50 | 74.31 \u00b1 1.89  | 77.05 \u00b1 1.26 | 76.16 \u00b1 1.19 | 77.19 \u00b1 1.49 | 82.64 \u00b1 1.65 | 80.92 \u00b1 0.97 | 79.48 \u00b1 2.08  | 83.60 \u00b1 1.14  | 90.89 \u00b1 0.23     | 90.17 \u00b1 1.73     |\\n|               | 91.53 \u00b1 0.23      | 91.27 \u00b1 0.27 | 84.83 \u00b1 0.22  | 88.00 \u00b1 0.26 | 88.59 \u00b1 0.29 | 86.78 \u00b1 0.29 | 91.03 \u00b1 0.20 | 90.92 \u00b1 0.22 | 89.38 \u00b1 0.25  | 91.69 \u00b1 0.19  | 90.89 \u00b1 0.23     | 90.17 \u00b1 0.65     |\\n|               | 97.50 \u00b1 3.59      | 95.39 \u00b1 4.77 | 87.18 \u00b1 4.44  | 51.54 \u00b1 11.19| 47.88 \u00b1 14.03| 93.59 \u00b1 5.88 | 92.50 \u00b1 4.58 | 93.65 \u00b1 6.15 | 47.90 \u00b1 1.04  | 93.65 \u00b1 4.37  | OOM              | OOM              |\\n|               | 81.38 \u00b1 0.58      | 81.06 \u00b1 0.54 | 100.00 \u00b1 0.00 | 95.27 \u00b1 0.47 | 96.60 \u00b1 1.67 | 81.35 \u00b1 0.61 | 99.99 \u00b1 0.02 | 98.70 \u00b1 0.39 | 81.05 \u00b1 0.59  | 99.96 \u00b1 0.05  | OOM              | 75.77 \u00b1 7.10     |\\n|               | 100.00 \u00b1 0.00     | 100.00 \u00b1 0.00| 100.00 \u00b1 0.01 | OOM          | OOM          | OOM          | OOM          | OOM          | 79.72 \u00b1 0.62  | 93.45 \u00b1 1.31  |                  |                  |\\n\\nMoreover, it experiences memory overload issues on Yelp and Walmart with a full batch setting and performs poorly with a mini-batch setting on the same datasets. Numerically, AllSetTransformer outperforms HAN with full batch setting by 9.14% on the Mushroom and HAN with mini-batch setting and by 16.89% on Walmart(1). These findings emphasize the importance of including diverse datasets from different application domains when trying to test hypergraph neural networks fairly. Testing standard benchmark datasets such as Cora, Citeseer and Pubmed alone is insufficient and can lead to biased conclusions. Indeed, our experiments show that all hypergraph neural networks work reasonably well on these three datasets. Our results also demonstrate the power of Set Transformer when applied to hypergraph learning via the AllSet framework.\\n\\nThe execution times of all methods are available in the Appendix J: They show that the complexity of AllSet is comparable to that of the baseline hypergraph neural networks. This further strengthens the case for AllSet, as its performance gains do not come at the cost of high computational complexity.\\n\\nNote that in HAN's mini-batch setting, the neighborhood sampler is performed on CPU instead of GPU, hence its training time is significantly higher than that needed by other methods, due to the frequent I/O operation between CPU and GPU. On some larger datasets such as Yelp and Walmart, 20 runs take more than 24 hours so we only recorded the results for first 10 runs.\\n\\nThe performance of AllDeepSets is not on par with that of AllSetTransformer, despite the fact that both represent universal approximators of the general AllSet formalism. This result confirms the assessment of Lee et al. (2019) that attention mechanisms are crucial for learning multiset functions in practice. Furthermore, we note that combining CE with existing GNNs such as GCN and GAT leads to suboptimal performance. CE-based approaches are also problematic in terms of memory efficiency when large hyperedges are present. This is due to the fact the CE of a hyperedge $e$ leads to $\\\\Theta(|e|^2)$ edges in the resulting graph. Note that as indicated in Table 2, CE-based methods went out-of-memory (OOM) for 20Newsgroups and Yelp. These two datasets have the largest maximum hyperedge size, as can be see from Table 1. HAN encounters the OOM issue on even more datasets when used in the full batch setting, and its mini-batch setting mode perform poorly on Yelp and Walmart. This shows that a naive application of standard heterogeneous GNNs on large hypergraphs often fails and is thus not as robust as our AllSetTransformer. The mediator HyperGCN approach exhibits numerical instabilities on some datasets (i.e., Zoo and House) and may hence not be suitable for learning on general hypergraph datasets.\"}"}
{"id": "hpBTIv2uy_E", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGMENTS\\nThe authors would like to thank Prof. Pan Li at Purdue University for helpful discussions. The authors would also like to thank Chaoqi Yang for answering questions regarding the LEGCN method. This work was funded by the NSF grant 1956384.\\n\\nETHICS STATEMENT\\nWe do not aware of any potential ethical issues regarding our work. For the newly curated three datasets, there is no private personally identifiable information. Note that although nodes in House datasets represent congresspersons and hyperedges represent members serving on the same committee, this information is public and not subject to privacy constraints. For the Walmart dataset, the nodes are products and hyperedges are formed by sets of products purchased together. We do not include any personal information regarding the buyers (customers) in the text. In the Yelp dataset the nodes represent restaurants and hyperedges are collections of restaurants visited by the same user. Similar to the Walmart case, there is no user information included in the dataset. More details of how the features in each datasets are generated can be found in Appendix I.\\n\\nREPRODUCIBILITY STATEMENT\\nWe have tried our best to ensure reproducibility of our results. We built a succinct pipeline for standardization of the hypergraph neural networks evaluation process based on Pytorch Geometric. This implementation can be checked by referring to our supplementary material. We include all dataset in our supplementary material and integrate all tested methods in our code. Hence, one can simply reproduce all experiment results effortlessly (by just running a one-line command). All details regarding how the datasets were prepared are stated in Appendix I. All choices of hyperparameters and the process of selecting them are available in Appendix J. Our experimental settings including the specifications of our machine and environment, the training/validation/test split and the evaluation metric. Once again, refer to the Appendix J and Section 6. We also clearly specify all the package dependencies used in our code in the Supplementary material.\\n\\nREFERENCES\\nSameer Agarwal, Jongwoo Lim, Lihi Zelnik-Manor, Pietro Perona, David Kriegman, and Serge Belongie. Beyond pairwise clustering. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), volume 2, pp. 838\u2013845. IEEE, 2005.\\nSameer Agarwal, Kristin Branson, and Serge Belongie. Higher order learning with graphs. In Proceedings of the 23rd international conference on Machine learning, pp. 17\u201324, 2006.\\nIlya Amburg, Nate Veldt, and Austin Benson. Clustering in graphs and hypergraphs with categorical edge labels. In Proceedings of The Web Conference 2020, WWW '20, pp. 706\u2013717, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450370233. doi: 10.1145/3366423.3380152. URL https://doi.org/10.1145/3366423.3380152.\\nDevanshu Arya, Deepak K Gupta, Stevan Rudinac, and Marcel Worring. Hypersage: Generalizing inductive representation learning on hypergraphs. arXiv preprint arXiv:2010.04558, 2020.\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\nJinheon Baek, Minki Kang, and Sung Ju Hwang. Accurate learning of graph representations with graph multiset pooling. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=JHcqXGaqiGn.\\nSong Bai, Feihu Zhang, and Philip HS Torr. Hypergraph convolution and hypergraph attention. Pattern Recognition, 110:107637, 2021.\\nAustin R Benson. Three hypergraph eigenvector centralities. SIAM Journal on Mathematics of Data Science, 1(2):293\u2013312, 2019.\"}"}
{"id": "hpBTIv2uy_E", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "hpBTIv2uy_E", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We further discuss related works for completeness. Graph neural networks on graphs. GNNs have received significant attention in the past, often under the umbrella of Geometric Deep Learning (Bronstein et al., 2017). Many successful architectures have been proposed for various tasks on graphs, including GCN (Kipf & Welling, 2017), GAT (Vaswani et al., 2017), GraphSAGE (Hamilton et al., 2017), GIN (Xu et al., 2019) and many others (Klicpera et al., 2019; Wu et al., 2019; Velickovic et al., 2019; Li et al., 2020; Xu et al., 2018; Chien et al., 2021b). There have also been some recent attempts to use Transformers in GNNs (Baek et al., 2021; Yun et al., 2019; Hu et al., 2020b). Nevertheless, all these methods only apply to graphs and not to hypergraphs. Although CE can be used in all these cases, this is clearly not an optimal strategy due to the previously mentioned distortion issues. There are also many techniques that can improve GNNs in various directions. For example, PairNorm (Zhao & Akoglu, 2019) and DropEdge (Rong et al., 2019) allow one to build deeper GNNs. ClusterGCN (Chiang et al., 2019) and GraphSAINT (Zeng et al., 2019), on the other hand, may be used to significantly scale up GNNs. Residual correlation (Jia & Benson, 2020) and CopulaGNN (Ma et al., 2021) have been shown to improve GNNs on graph regression problems. These works highlight new directions and limitations not only of our method, but also of hypergraph neural networks in general. Nevertheless, we believe that AllSet layers can be adapted to resolve these issues using similar ideas as those proposed for graph layers.\\n\\nAdditional contributions to learning on hypergraphs. After the submission of our manuscript, we were made aware of some loosely related lines of works. Ding et al. (2020) and Wang et al. (2021) proposed hypergraph attention networks based on the $V \\\\rightarrow E$ and $E \\\\rightarrow V$ formulation, specialized for certain downstream tasks. However, these approaches fall under our AllSet framework and have smaller expressiveness compared to AllSet. The work Jo et al. (2021) proposes to learn edge representations in a graph via message passing on its dual hypergraph. Although the authors used GMT (Baek et al., 2021) as their node pooling module, they do not explore the idea of treating $f_{V \\\\rightarrow E}$ and $f_{E \\\\rightarrow V}$ as multiset functions for hypergraph neural networks as AllSet does. It is also possible to define propagation schemes based on other tensor eigenproblems, such as H eigenproblems (Qi & Luo, 2017). Defining propagation rules based on H eigenproblems can lead to imaginary features which is problematic: A more detailed discussion regarding this direction is available in the Appendix H.\"}"}
{"id": "hpBTIv2uy_E", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"First, we show that one can recover CEpropH (1) from AllSet (5). By choosing \\\\( f_{V \\\\rightarrow E} \\\\) and \\\\( f_{E \\\\rightarrow V} \\\\) to be the sums over appropriate input multisets and by ignoring the second input, we obtain\\n\\n\\\\[\\nZ(t+1)_{e, v} = f_{V \\\\rightarrow E}(V_{e, X(t)}) = \\\\sum_{x \\\\in V_{e, X(t)}} x = \\\\sum_{u \\\\in e_{v}} X(t)_{u, :},\\n\\\\]\\n\\n(9)\\n\\n\\\\[\\nX(t+1)_{v, :} = f_{E \\\\rightarrow V}(E_{v, Z(t+1)}) = \\\\sum_{x \\\\in E_{v, Z(t+1)}} x = \\\\sum_{e : v \\\\in e} Z(t+1)_{e, :}.\\n\\\\]\\n\\n(10)\\n\\nCombining these two equations gives\\n\\n\\\\[\\nX(t+1)_{v, :} = \\\\sum_{e : v \\\\in e} Z(t+1)_{e, :} = \\\\sum_{e : v \\\\in e} \\\\sum_{u \\\\in e} X(t)_{u, :},\\n\\\\]\\n\\n(11)\\n\\nwhich matches the update equations of CEpropH (1).\\n\\nNext, we prove that one can recover CEpropA (1) from AllSet (6). We once again choose \\\\( f_{V \\\\rightarrow E} \\\\) and \\\\( f_{E \\\\rightarrow V} \\\\) to be the sums over appropriate input multisets and ignore the second input term. This results in the following update equations:\\n\\n\\\\[\\nZ(t+1)_{v, e} = f_{V \\\\rightarrow E}(V_{e \\\\setminus v, X(t)}) = \\\\sum_{x \\\\in V_{e \\\\setminus v, X(t)}} x = \\\\sum_{u \\\\in e \\\\setminus v} X(t)_{u, :},\\n\\\\]\\n\\n(12)\\n\\n\\\\[\\nX(t+1)_{v, :} = f_{E \\\\rightarrow V}(E_{v, Z(t+1)}) = \\\\sum_{x \\\\in E_{v, Z(t+1)}} x = \\\\sum_{e : v \\\\in e} Z(t+1)_{e, :}.\\n\\\\]\\n\\n(13)\\n\\nCombining the two equations we obtain\\n\\n\\\\[\\nX(t+1)_{v, :} = \\\\sum_{e : v \\\\in e} Z(t+1)_{e, :} = \\\\sum_{e : v \\\\in e} \\\\sum_{u \\\\in e \\\\setminus v} X(t)_{u, :},\\n\\\\]\\n\\n(14)\\n\\nwhich represents the update rule of CEpropA from (1).\\n\\nIn the last step, we prove that one can recover Zprop (4) from AllSet (6). By choosing \\\\( f_{V \\\\rightarrow E} \\\\) and \\\\( f_{E \\\\rightarrow V} \\\\) to be the product and sum over appropriate input multisets, respectively, and by ignoring the second input, we have:\\n\\n\\\\[\\nZ(t+1)_{v, e} = f_{V \\\\rightarrow E}(V_{e \\\\setminus v, X(t)}) = \\\\prod_{x \\\\in V_{e \\\\setminus v, X(t)}} x = \\\\prod_{u \\\\in e \\\\setminus v} X(t)_{u, :},\\n\\\\]\\n\\n(15)\\n\\n\\\\[\\nX(t+1)_{v, :} = f_{E \\\\rightarrow V}(E_{v, Z(t+1)}) = \\\\sum_{x \\\\in E_{v, Z(t+1)}} x = \\\\sum_{e : v \\\\in e} Z(t+1)_{e, :}.\\n\\\\]\\n\\n(16)\\n\\nCombining these two equations we arrive at\\n\\n\\\\[\\nX(t+1)_{v, :} = \\\\sum_{e : v \\\\in e} Z(t+1)_{e, :} = \\\\sum_{e : v \\\\in e} \\\\prod_{u \\\\in e \\\\setminus v} X(t)_{u, :},\\n\\\\]\\n\\n(17)\\n\\nwhich matches Zprop from (4). This completes the proof.\\n\\nFirst we prove that the hypergraph neural network layer of HGNN (2) is a special instance of AllSet (5). By choosing \\\\( f_{V \\\\rightarrow E} \\\\) and \\\\( f_{E \\\\rightarrow V} \\\\) to be the weighted sum over appropriate input multisets, where the weights are chosen according to the node degree, edge degree, and edge weights, and by ignoring the second input, we have:\\n\\n\\\\[\\nZ(t+1)_{e, :} = f_{V \\\\rightarrow E}(V_{e, X(t)}) = \\\\sum_{u \\\\in e} X(t)_{u, :} \\\\sqrt{d_u},\\n\\\\]\\n\\n(18)\\n\\n\\\\[\\nX(t+1)_{v, :} = f_{E \\\\rightarrow V}(E_{v, Z(t+1)}) = \\\\sigma(\\\\frac{1}{\\\\sqrt{d_v}} \\\\sum_{e : v \\\\in e} w_e Z(t+1)_{e, :} \\\\Theta(t)_{|e|} + b(t)),\\n\\\\]\\n\\n(19)\"}"}
{"id": "hpBTIv2uy_E", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Since $f_{V \\\\rightarrow E}$ and $f_{E \\\\rightarrow V}$ both use the hypergraph $G$ as their input, the functions can use the node degrees, edge degrees and edge weights as their arguments. Also, it is not hard to see that our choices for $f_{V \\\\rightarrow E}$ (18) and $f_{E \\\\rightarrow V}$ (19) are permutation invariant in $V_e$, $X(t)$ and $E_v$, $Z(t+1)$, respectively. Plugging (18) into (19) leads to:\\n\\n$$X(t+1)_v,: = \\\\sigma\\\\left(\\\\frac{1}{\\\\sqrt{d_v}} \\\\sum_{e:v \\\\in e} w_e |e| \\\\sum_{u \\\\in e} \\\\alpha(t)_u \\\\sigma(X(t)_u,: \\\\sqrt{d_u} \\\\Theta(t) + b(t))\\\\right),$$\\n\\n(20)\\n\\nwhich represents the HGNN layer (2).\\n\\nFor the HCHA layer, we have a node-wise formulation that reads as follows:\\n\\n$$X(t+1)_v,: = \\\\sigma\\\\left(\\\\frac{1}{d_v} \\\\sum_{e:v \\\\in e} \\\\alpha(t)_v \\\\sum_{u \\\\in e} \\\\alpha(t)_u \\\\sigma(X(t)_u,:) \\\\sqrt{d_u} \\\\Theta(t) + b(t))\\\\right),$$\\n\\n(21)\\n\\nwhere the attention weight $\\\\alpha(t)_u$ depends on the node features $X(t)_u,$ and the feature of hyperedge $e$. Here, $\\\\sigma(\\\\cdot)$ is a nonlinear activation function such as LeakyReLU and eLU. An analysis similar to that described for HGNN can be used in this case as well. The only difference is in the choice of the attention weights $\\\\alpha(t)_u$ and $\\\\alpha(t)_v$. The definition of attention weights for HCHA is presented below.\\n\\n$$\\\\alpha(t)_u = \\\\exp(\\\\sigma(a^T[X(t)_u,: \\\\parallel Z(t)_e,:])) \\\\sum_{e:v \\\\in e} \\\\exp(\\\\sigma(a^T[X(t)_u,: \\\\parallel Z(t)_e,:])),$$\\n\\n(22)\\n\\nClearly, the attention function in (22) has $V_e$, $X(t)$ and $Z(t)_e,$ as its arguments. Furthermore, we can choose $f_{V \\\\rightarrow E}$ and $f_{E \\\\rightarrow V}$ as:\\n\\n$$Z(t+1)_e,: = f_{V \\\\rightarrow E}(V_e, X(t)); Z(t)_e,: = \\\\sum_{u \\\\in e} \\\\alpha(t)_u X(t)_u,: \\\\sigma(Z(t)_e,: \\\\Theta(t) + b(t)).$$\\n\\n(23)\\n\\n$$X(t+1)_v,: = f_{E \\\\rightarrow V}(E_v, Z(t+1)); X(t+1)_v,: = \\\\sigma\\\\left(\\\\frac{1}{\\\\sqrt{d_v}} \\\\sum_{e:v \\\\in e} w_e |e| \\\\sum_{u \\\\in e} \\\\alpha(t)_u \\\\sigma(X(t)_u,:) \\\\sqrt{d_u} \\\\Theta(t) + b(t))\\\\right).$$\\n\\n(24)\\n\\nNote that both (23) and (24) are permutation invariant, which means that they are valid choices for $f_{V \\\\rightarrow E}$ and $f_{E \\\\rightarrow V}$. Combining these two equations reproduces the HCHA layer of (21).\\n\\nFor the HyperGCN layer, ignoring the degree normalization, one can use the following formulation:\\n\\n$$X(t+1)_v,: = \\\\sigma\\\\left(\\\\sum_{e:v \\\\in e} \\\\sum_{u \\\\in e} w(t)_{uv,e} X(t)_u,:,: \\\\Theta(t) + b(t))\\\\right).$$\\n\\n(25)\\n\\nHere, the weight $w(t)_{uv,e}$ depends on all node features within the hyperedge $\\\\{X(t)_u,: u \\\\in e\\\\}$, and $\\\\sigma(\\\\cdot)$ is a nonlinear activation function (for example, ReLU). The same analysis as presented for the previous cases may be used in this case as well: The only difference lies in the choice of the weights $w(t)_{uv,e}$. According to the original HyperGCN paper (Yadati et al., 2019), these weights are defined as:\\n\\n$$w(t)_{uv,e} = \\\\begin{cases} \\\\frac{1}{2} |e| - 3 & \\\\text{if } u \\\\in \\\\{i_e, j_e\\\\} \\\\text{ or } v \\\\in \\\\{i_e, j_e\\\\}, \\\\\\\\ 0 & \\\\text{otherwise.} \\\\end{cases}$$\\n\\n(26)\\n\\n(27)\"}"}
{"id": "hpBTIv2uy_E", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multiset functions\\n\\n124356ab124356CE\\n\\nFigure 1: Left: The difference between a hypergraph and clique-expanded graph. Right: Illustration of our AllSet framework for the hypergraph depicted on the left. Included is an example on the aggregation rule for hyperedge $b$ and node $4$. The key idea is that $f: V \\\\rightarrow E$ and $f: E \\\\rightarrow V$ are two multiset functions, which by definition are permutation invariant with respect to their input multisets.\\n\\nBackground\\n\\nNotation. A hypergraph is an ordered pair of sets $G(V, E)$, where $V = \\\\{1, 2, ..., n\\\\}$ is the set of nodes while $E$ is the set of hyperedges. Each hyperedge $e \\\\in E$ is a subset of $V$, i.e., $e \\\\subseteq V$. Unlike a graph edge, a hyperedge $e$ may contain more than two nodes. If $\\\\forall e \\\\in E$ one has $|e| = d \\\\in \\\\mathbb{N}$, the hypergraph $G$ is termed $d$-uniform. A $d$-uniform hypergraph can be represented by a $d$-dimensional supersymmetric tensor such that for all distinct collections $i_1, ..., i_d \\\\in V$, $A_{i_1, ..., i_d} = 1/(d - 2)!$ if $e = \\\\{i_1, ..., i_d\\\\} \\\\in E$, and $A_{i_1, ..., i_d} = 0$ otherwise. Henceforth, $A_{i_1, ..., i_d}$ is used to denote the slice of $A$ along the first coordinate. A hypergraph can alternatively be represented by its incidence matrix $H$, where $H_{ve} = 1$ if $v \\\\in e$ and $H_{ve} = 0$ otherwise. We use the superscript $(t)$ to represent the functions or variables at the $t$-th step of propagation and $\\\\|\\\\|$ to denote concatenation. Furthermore, $\\\\Theta$ and $b$ are reserved for a learnable weight matrix and bias of a neural network, respectively. Finally, we use $\\\\sigma(\\\\cdot)$ to denote a nonlinear activation function (such as ReLU, eLU or LeakyReLU), which depends on the model used.\\n\\nCE-based propagation on hypergraphs. The CE of a hypergraph $G(V, E)$ is a weighted graph with the same set of nodes $V$. It can be described in terms of the associated adjacency or incidence matrices which we write with a slight abuse of notation as $A\\\\text{\\\\tiny \\\\text{CE}}_{ij} = \\\\sum_{i_3, ..., i_d \\\\in V} A_{i,j,i_3, ..., i_d}$ and $H\\\\text{\\\\tiny \\\\text{CE}} = HH^T$, respectively. It is obvious that these two matrices only differ in their diagonal entries ($0$s versus node degrees, respectively). One step of propagation of an $F$-dimensional node feature matrix $X \\\\in \\\\mathbb{R}^{n \\\\times F}$ is captured by $A\\\\text{\\\\tiny \\\\text{CE}}X$ or $H\\\\text{\\\\tiny \\\\text{CE}}X$; alternatively, in terms of node feature updates, we have $\\\\mathcal{C}\\\\mathcal{E}\\\\mathcal{P}\\\\mathcal{A}: X_{(t+1)v, :} = \\\\sum_{e : v \\\\in e} \\\\sum_{u : u \\\\in e \\\\setminus v} X_{(t)u, :}$; $\\\\mathcal{C}\\\\mathcal{E}\\\\mathcal{P}\\\\mathcal{H}: X_{(t+1)v, :} = \\\\sum_{e : v \\\\in e} \\\\sum_{u : u \\\\in e} X_{(t)u, :}$.\\n\\nMany existing hypergraph convolutional layers actually perform CE-based propagation, potentially with further degree normalization and nonlinear hyperedge weights. For example, the propagation rule of HGNN (Feng et al., 2019) takes the following node-wise form:\\n\\n$$X_{(t+1)v, :} = \\\\sigma\\\\left(1/\\\\sqrt{d_v} \\\\sum_{e : v \\\\in e} w_e |e| \\\\sum_{u : u \\\\in e} X_{(t)u, :} \\\\sqrt{d_u}\\\\right) \\\\Theta_{(t)} + b_{(t)}.$$\"}"}
{"id": "hpBTIv2uy_E", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HyperGCN replaces each hyperedge by an incomplete clique via so-called media-\\ntors (Yadati et al., 2019). When the hypergraph is 3-uniform, the aforementioned approach becomes\\na standard weighted CE. Hence, all the described hypergraph neural networks adapt propagation\\nrules based on CE or its variants, potentially with the addition of nonlinear hyperedge weights.\\nThe described methods achieve reasonable good performance on standard cocitation and coauthor\\nbenchmarking datasets.\\n\\nTensor-based propagations. As mentioned in the introduction, there exist more elaborate tensor-\\nbased propagation schemes which in some cases outperform CE-based methods. The propagation\\nrules related to Z eigenproblems such as multilinear PageRank (Gleich et al., 2015) and spacey\\nrandom walks (Benson et al., 2017) are two such examples. The Z eigenproblem for an adjacency\\ntensor $A$ of a $d$-uniform hypergraph is defined as:\\n\\n$$\\\\lambda x = A x^{d-1} \\\\equiv \\\\sum_{i_2, \\\\ldots, i_d} A_{i_2, \\\\ldots, i_d} x_{i_2} \\\\cdots x_{i_d}, x \\\\in \\\\mathbb{R}^n$$\\n\\n(3)\\n\\nHere, $A x^{d-1}$ equals $\\\\sum_{i_2, \\\\ldots, i_d} A_{i_2, \\\\ldots, i_d} x_{i_2} \\\\cdots x_{i_d}$, an entity frequently used in the tensor analysis\\nliterature. The Z eigenproblem has been extensively studied both in the context of tensor analysis\\nand network sciences; the problem is also known as the $l_2$ eigenproblem (Lim, 2005; Gautier et al.,\\n2019). We refer the interested readers to Qi & Luo (2017) for a more detailed theoretical analysis of\\nthe Z eigenproblems and Benson (2019) for its application in the study of hypergraph centralities.\\n\\nBy ignoring the norm constraint, one can define the following tensor-based propagation rule based\\non (3) according to:\\n\\n$$Z_{\\\\text{prop}}: X(t+1)_{v, e} = \\\\sum_{e \\\\neq v} X(t)_{u, e} \\\\prod_{u \\\\in e \\\\setminus v} X(t)_{u, e},$$\\n\\n(4)\\n\\nDespite its interesting theoretical properties, $Z_{\\\\text{prop}}$ is known to have what is termed the \\\"unit prob-\\nlem\\\" (Benson, 2019). In practice, the product can cause numerical instabilities for large hyperedges.\\nFurthermore, $Z_{\\\\text{prop}}$ has only been studied for the case of $d$-uniform hypergraphs, which makes it\\nless relevant for general hypergraph learning tasks. Clearly, CEprop and $Z_{\\\\text{prop}}$ have different advan-\\ntages and disadvantages for different dataset structures. This motivates finding a general framework\\nthat encompasses these two and other propagation rules. In this case, we aim to learn the suitable\\npropagation scheme under such a framework for hypergraph neural networks.\\n\\nWe show that all the above described propagation methods can be unified within one setting, termed\\nAllSet. The key observation is that all propagation rules equal a composition of two multiset func-\\ntions, defined below.\\n\\nDefinition 3.1. A function $f$ is permutation invariant if and only if $\\\\forall \\\\pi \\\\in S_n$, where $S_n$ denotes the\\nsymmetric group of order $n!$,\\n\\n$$f(x_{\\\\pi(1)}, \\\\ldots, x_{\\\\pi(n)}) = f(x_1, \\\\ldots, x_n).$$\\n\\nDefinition 3.2. We say that a function $f$ is a multiset function if it is permutation invariant.\\n\\nNext, let $V_{e, X} = \\\\{X_u: u \\\\in e\\\\}$ denote the multiset of hidden node representations contained in\\nthe hyperedge $e$. Also, let $Z \\\\in \\\\mathbb{R}^{|E| \\\\times F'}$ denote the hidden hyperedge representations. Similarly, let\\n$E_{v, Z} = \\\\{Z_e: v \\\\in e\\\\}$ be the multiset of hidden representations of hyperedges that contain the node\\n$v$. The AllSet framework uses the update rules\\n\\n$$Z(t+1)_{e, v} = f_{V \\\\rightarrow E}(V_e, X(t); Z(t)_{e, v}), X(t+1)_{v, e} = f_{E \\\\rightarrow V}(E_v, Z(t+1)_{e, v}; X(t)_{v, e}),$$\\n\\n(5)\\n\\nwhere $f_{V \\\\rightarrow E}$ and $f_{E \\\\rightarrow V}$ are two multiset functions with respect to their first input. For the initial\\ncondition, we choose $Z(0)_{e, v}$ and $X(0)_{v, e}$ to be hyperedge features and node features, respectively (if\\navailable). If these are not available, we set both entities to be all-zero matrices. Note that we\\nmake the tacit assumption that both functions $f_{V \\\\rightarrow E}$ and $f_{E \\\\rightarrow V}$ also include the hypergraph\\n$G$ as an\\ninput. This allows degree normalization to be part of our framework. As one can also distinguish\\nthe aggregating node $v$ from the multiset $V_{e, X}(t)$, we also have the following AllSet variant:\\n\\n$$Z(t+1)_{e, v} = f_{V \\\\rightarrow E}(V_e \\\\setminus v, X(t); Z(t)_{e, v}), X(t+1)_{v, e} = f_{E \\\\rightarrow V}(E_v, Z(t+1)_{e, v}; X(t)_{v, e}),$$\\n\\n(6)\\n\\n4\"}"}
{"id": "hpBTIv2uy_E", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For simplicity, we omit the last input argument $X(t)$, for $f: V \\\\rightarrow E$ in (6), unless explicitly needed (as in the proof pertaining to HyperGCN). The formulation (5) lends itself to a significantly more computationally- and memory-efficient pipeline. This is clearly the case since for each hyperedge $e$, the expression in (5) only uses one hyperedge hidden representation while the expression (6) uses $|e|$ distinct hidden representations. This difference can be substantial when the hyperedge sizes are large. Hence, we only use (6) for theoretical analysis purposes and (5) for all experimental verifications. The next theorems establish the universality of the AllSet framework.\\n\\nTheorem 3.3. CEpropH of (1) is a special case of AllSet (5). Furthermore, CEpropA of (1) and $Zprop$ of (4) are also special cases of AllSet (6).\\n\\nSketch of proof. If we ignore the second input and choose both $f: V \\\\rightarrow E$ and $f: E \\\\rightarrow V$ to be sums over their input multisets, we recover the CE-based propagation rule CEpropH of (1) via (5). For CEpropA, the same observation is true with respect to (6). If one instead chooses $f: V \\\\rightarrow E$ to be the product over its input multiset, multiplied by the scalar $|V_e, X(t)|^{-1}$, one recovers the tensor-based propagation $Zprop$ of (4) via (6).\\n\\nNext, we show that many state-of-the-art hypergraph neural network layers also represent special instances of AllSet and are strictly less expressive than AllSet.\\n\\nTheorem 3.4. The hypergraph neural network layers of HGNN (2), HCHA (Bai et al., 2021), HNHN (Dong et al., 2020) and HyperSAGE (Arya et al., 2020) are all special cases of AllSet (5).\\n\\nThe HyperGCN layer (Yadati et al., 2019) is a special instance of AllSet (6). Furthermore, all the above methods are strictly less expressive than AllSet (5) and (6). More precisely, there exists a combination of multiset functions $f: V \\\\rightarrow E$ and $f: E \\\\rightarrow V$ for AllSet (5) and (6) that cannot be modeled by any of the aforementioned hypergraph neural network layers.\\n\\nThe first half of the proof is by direct construction. The second half of the proof consists of counter-examples. Intuitively, none of the listed hypergraph neural network layers can model $Zprop$ (3) while we established in the previous result that $Zprop$ is a special case of AllSet.\\n\\nThe last result shows that the MPNN (Gilmer et al., 2017) framework is also a special instance of our AllSet for graphs, which are (clearly) special cases of hypergraphs. Hence, AllSet may also be viewed as a hypergraph generalization of MPNN. Note that MPNN itself generalizes many well-known GNNs, such as GCN (Kipf & Welling, 2017), Gated Graph Neural Networks (Li et al., 2015) and GAT (Veli\u010dkovi\u0107 et al., 2018).\\n\\nTheorem 3.5. MPNN is a special case of AllSet (6) when applied to graphs.\\n\\n4 HOW TO LEARN ALLSET LAYERS\\n\\nThe key idea behind AllSet is to learn the multiset functions $f: V \\\\rightarrow E$ and $f: E \\\\rightarrow V$ on the fly for each dataset and task. To facilitate this learning process, we first have to properly parametrize the multiset functions. Ideally, the parametrization should represent a universal approximator for a multiset function that allows one to retain the higher expressive power of our architectures when compared to that of the hypergraph neural networks described in Theorem 3.4. For simplicity, we focus on the multiset inputs of $f: V \\\\rightarrow E$ and $f: E \\\\rightarrow V$ and postpone the discussion pertaining to the second arguments of the functions to the end of this section.\\n\\nUnder the assumption that the multiset size is finite, the authors of Zaheer et al. (2017) and Wagstaff et al. (2019) proved that any multiset functions $f$ can be parametrized as $f(S) = \\\\rho(\\\\sum_{s \\\\in S} \\\\phi(s))$, where $\\\\rho$ and $\\\\phi$ are some bijective mappings (Theorem 4.4 in Wagstaff et al. (2019)). In practice, these mappings can be replaced by any universal approximator such as a multilayer perceptron (MLP) (Zaheer et al., 2017). This leads to the purely MLP AllSet layer for hypergraph neural networks, termed AllDeepSets.\\n\\nAllDeepSets: $f: V \\\\rightarrow E(S) = f: E \\\\rightarrow V(S) = \\\\text{MLP}(\\\\sum_{s \\\\in S} \\\\text{MLP}(s))$. (7)\"}"}
{"id": "hpBTIv2uy_E", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Paradigm which was shown to offer better performance than Deep Sets as a learnable multiset function architecture. Based on this result, we also propose an attention-based AllSet layer for hypergraph neural networks, termed AllSetTransformer.\\n\\nGiven the matrix $\\\\mathbf{S} \\\\in \\\\mathbb{R}^{\\\\left|\\\\mathbf{S}\\\\right| \\\\times F}$ which represents the multiset $\\\\mathbf{S}$ of $F$-dimensional real vectors, the definition of AllSetTransformer is:\\n\\n$$\\\\text{AllSetTransformer}: f_{\\\\mathbf{V} \\\\rightarrow \\\\mathbf{E}}(\\\\mathbf{S}) = f_{\\\\mathbf{E} \\\\rightarrow \\\\mathbf{V}}(\\\\mathbf{S}) = \\\\text{LN}(\\\\mathbf{Y} + \\\\text{MLP}(\\\\mathbf{Y})),$$\\n\\nwhere $\\\\mathbf{Y} = \\\\text{LN}(\\\\theta + \\\\text{MH}_{h,\\\\omega}(\\\\theta, \\\\mathbf{S}, \\\\mathbf{S}))$,\\n\\n$$\\\\text{MH}_{h,\\\\omega}(\\\\theta, \\\\mathbf{S}, \\\\mathbf{S}) = \\\\|_{i=1}^{h} O(i) = \\\\omega(\\\\theta(i)(K(i))^T \\\\mathbf{V}(i), \\\\theta) \\\\equiv \\\\|_{i=1}^{h} \\\\theta(i),$$\\n\\n$$K(i) = \\\\text{MLP}_{K,i}(\\\\mathbf{S}), \\\\quad V(i) = \\\\text{MLP}_{V,i}(\\\\mathbf{S}).$$\\n\\nHere, LN represents the layer normalization (Ba et al., 2016), $\\\\| \\\\cdot$ denotes concatenation and $\\\\theta \\\\in \\\\mathbb{R}^{1 \\\\times hF}$ is a learnable weight; in addition, $\\\\text{MH}_{h,\\\\omega}$ is a multihead attention mechanism with $h$ heads and activation function $\\\\omega$ (Vaswani et al., 2017). Note that the dimension of the output of $\\\\text{MH}_{h,\\\\omega}$ is $1 \\\\times hF$ where $F_h$ is the hidden dimension of $\\\\mathbf{V}(i) \\\\in \\\\mathbb{R}^{\\\\left|\\\\mathbf{S}\\\\right| \\\\times F_h}$. In our experimental setting, we choose $\\\\omega$ to be the softmax function which is robust in practice. Our formulation of projections in $\\\\text{MH}_{h,\\\\omega}$ is slightly more general than standard linear projections. If one restricts $\\\\text{MLP}_{K,i}$ and $\\\\text{MLP}_{V,i}$ to one-layer perceptrons without a bias term (i.e., including only a learnable weight matrix), we obtain standard linear projections in the multihead attention mechanism. The output dimensions of $\\\\text{MLP}_{K,i}$ and $\\\\text{MLP}_{V,i}$ are both equal to $\\\\mathbb{R}^{\\\\left|\\\\mathbf{S}\\\\right| \\\\times F_h}$. It is worth pointing out that all the MLP modules operate row-wise, which means they are applied to each multiset element (a real vector) independently and in an identical fashion. This directly implies that MLP modules are permutation equivariant.\\n\\nBased on the above discussion and Proposition 2 of Lee et al. (2019) we have the following result.\\n\\n**Proposition 4.1.** The functions $f_{\\\\mathbf{V} \\\\rightarrow \\\\mathbf{E}}$ and $f_{\\\\mathbf{E} \\\\rightarrow \\\\mathbf{V}}$ defined in AllSetTransformer (8) are permutation invariant. Furthermore, the functions $f_{\\\\mathbf{V} \\\\rightarrow \\\\mathbf{E}}$ and $f_{\\\\mathbf{E} \\\\rightarrow \\\\mathbf{V}}$ defined in (8) are universal approximators of multiset functions when the size of the input multiset is finite.\\n\\nNote that in practice, both the size of hyperedges and the node degrees are finite. Thus, the finite size multiset assumption is satisfied. Therefore, the above results directly imply that the AllDeepSets (7) and AllSetTransformer (8) layers have the same expressive power as the general AllSet framework. Together with the result of Theorem 3.4, we arrive at the conclusion that AllDeepSets and AllSetTransformer are both more expressive than any other aforementioned hypergraph neural network.\\n\\nConceptually, one can also incorporate the Set Attention Block and the two steps pooling strategy of Lee et al. (2019) into AllSet. However, it appears hard to make such an implementation efficient and we hence leave this investigation as future work. In practice, we find that even our simplified design (8) already outperforms all baseline hypergraph neural networks, as demonstrated by our extensive experiments. Also, it is possible to utilize the information of the second argument of $f_{\\\\mathbf{V} \\\\rightarrow \\\\mathbf{E}}$ and $f_{\\\\mathbf{E} \\\\rightarrow \\\\mathbf{V}}$ via concatenation, another topic relegated to future work.\\n\\nAs a concluding remark, we observe that the multiset functions in both AllDeepSets (7) and AllSetTransformer (8) are universal approximators for general multiset functions. In contrast, the other described hypergraph neural network layers fail to retain the universal approximation property for general multiset functions, as shown in Theorem 3.4. This implies that all these hypergraph neural network layers have strictly weaker expressive power compared to AllDeepSets (7) and AllSetTransformer (8). We also note that any other universal approximators for multiset functions can easily be combined with our AllSet framework (as we already demonstrated by our Deep Sets and Set Transformer examples). One possible candidate is Janossy pooling (Murphy et al., 2019) which will be examined as part of our future work. Note that more expressive models do not necessarily have better performance than less expressive models, as many other factors influence system performance (as an example, AllDeepSet performs worse than AllSetTransformer albeit both have the same theoretical expressive power; this is in agreement with the observation from Lee et al. (2019) that Set Transformer can learn multiset functions better than Deep Set.). Nevertheless, we demonstrate in the experimental verification section that our AllSetTransformer indeed has consistently better performance compared to other baseline methods.\\n\\n### 5 RELATED WORK\\n\\nDue to space limitations, a more comprehensive discussion of related work is relegated to the Appendix A.\"}"}
{"id": "hpBTIv2uy_E", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4: Running times for the best hyperparameter choice: Mean (sec or hour)\\n\\n| Model                  | Mean (sec) | Std. Dev. (sec) |\\n|------------------------|------------|-----------------|\\n| AllSetTransformer      | 7.37s      | \u00b1 1.10s         |\\n| HAN (mini batch)       | 99.87s     | \u00b1 4.74s         |\\n| HAN (full batch)       | 3.47s      | \u00b1 1.73s         |\\n| AllDeepSets            | 11.70s     | \u00b1 6.50s         |\\n| HyperGCN               | 2.13s      | \u00b1 0.61s         |\\n| UniGCNII               | 58.01s     | \u00b1 29.10s        |\\n| CEGCN OOM              | 67.66s     | \u00b1 31.30s        |\\n| CEGAT OOM              | 132.53s    | \u00b1 66.26s        |\\n| HGNN                   | 9.69s      | \u00b1 3.32s         |\\n| HCHA                   | 9.61s      | \u00b1 3.57s         |\\n| MLP                    | 1.68s      | \u00b1 1.00s         |\\n| 20Newsgroups           | 17.96s     | \u00b1 4.47s         |\\n| Mushroom               | 3.59s      | \u00b1 0.46s         |\\n| NTU2012                | 46.61s     | \u00b1 11.70s        |\\n| ModelNet40             | 11.21s     | \u00b1 5.61s         |\\n| Yelp                   | 21.73s     | \u00b1 6.83s         |\\n| House(1)               | 30.05s     | \u00b1 10.84s        |\\n| Walmart(1)             | 93.42s     | \u00b1 19.68s        |\\n| Cora                   | 14.35s     | \u00b1 3.57s         |\\n| Citeseer               | 17.05s     | \u00b1 4.47s         |\\n| Pubmed                 | 3.11s      | \u00b1 0.45s         |\\n| Cora-CA                | 21.73s     | \u00b1 6.83s         |\\n| DBLP-CA                | 93.42s     | \u00b1 19.68s        |\\n| Zoo                    | 23.94s     | \u00b1 5.62s         |\\n\\nTable 5: Average running times over all different choices of hyperparameters: Mean\\n\\n| Model                  | Mean (sec) |\\n|------------------------|------------|\\n| AllSetTransformer      | 21.97s     |\\n| HAN (mini batch)       | 25.47s     |\\n| HAN (full batch)       | 313.28s    |\\n| AllDeepSets            | 46.61s     |\\n| HyperGCN               | 4.34s      |\\n| UniGCNII               | 58.01s     |\\n| CEGCN OOM              | 69.67s     |\\n| CEGAT OOM              | 121.39s    |\\n| HGNN                   | 9.69s      |\\n| HCHA                   | 9.61s      |\\n| MLP                    | 1.68s      |\\n| 20Newsgroups           | 17.96s     |\\n| Mushroom               | 3.59s      |\\n| NTU2012                | 46.61s     |\\n| ModelNet40             | 11.21s     |\\n| Yelp                   | 21.73s     |\\n| House(1)               | 30.05s     |\\n| Walmart(1)             | 93.42s     |\\n| Cora                   | 14.35s     |\\n| Citeseer               | 17.05s     |\\n| Pubmed                 | 3.11s      |\\n| Cora-CA                | 21.73s     |\\n| DBLP-CA                | 93.42s     |\\n| Zoo                    | 23.94s     |\\n\\nPublished as a conference paper at ICLR 2022\"}"}
{"id": "hpBTIv2uy_E", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: Choice of hyperparameters for each method: lr refers to the learning rate, wd refers to the weight decaying factor, h1 refers to the dimension of MLP hidden layer and heads refers to the number of attention heads. Remarkably, not all hyperparameters are used for some models. For example, MLP, CEGCN, HNHN, HGNN, HCHA and HyperGCN does not take heads as input.\\n\\n|                | AllSetTransformer | AllDeepSets | MLP          | CEGCN          | HNHN          | HGNN          | HCHA          | HyperGCN       |\\n|----------------|-------------------|-------------|--------------|----------------|----------------|----------------|----------------|----------------|\\n|                | lr                | wd          | h1           | heads          | lr             | wd             | h1             | heads          |\\n| Cora           | 0.001             | 0           | 256          | 4              | 0.001          | 0              | 512            | 1              |\\n| Citeseer       | 0.001             | 0           | 512          | 8              | 0.001          | 0              | 512            | 1              |\\n| Pubmed         | 0.01              | 1.00E-05    | 64           | 8              | 0.01           | 0              | 64             | 1              |\\n| Cora-CA        | 0.001             | 0           | 128          | 8              | 0.001          | 0              | 512            | 1              |\\n| DBLP-CA        | 0.001             | 0           | 256          | 8              | 0.001          | 0              | 512            | 1              |\\n| Zoo            | 0.001             | 1.00E-05    | 64           | 8              | 0.1            | 0              | 64             | 1              |\\n| 20News         | OOM               | OOM         | OOM          | OOM            | OOM            | OOM            | OOM            | OOM            |\\n| Mushroom       | 0.001             | 0           | 128          | 1              | 0.001          | 0              | 256            | 1              |\\n| NTU2012        | 0.001             | 0           | 256          | 1              | 0.001          | 0              | 256            | 1              |\\n| ModelNet40     | 0.001             | 0           | 512          | 8              | 0.001          | 0              | 512            | 1              |\\n| Yelp           | OOM               | OOM         | OOM          | OOM            | OOM            | OOM            | OOM            | OOM            |\\n| House(1)       | 0.001             | 0           | 128          | 8              | 0.01           | 1.00E-05       | 64             | 1              |\\n| Walmart(1)     | 0.001             | 0           | 256          | 1              | 0.001          | 0              | 512            | 1              |\\n| House(0.6)     | 0.01              | 1.00E-05    | 64           | 8              | 0.001          | 0              | 256            | 1              |\\n| Walmart(0.6)   | 0.001             | 0           | 256          | 1              | 0.001          | 0              | 512            | 1              |\\n\\n|                | CEGAT             | HNHN        | HGNN         | HCHA          | HyperGCN       |\\n|----------------|-------------------|-------------|--------------|----------------|----------------|\\n|                | lr                | wd          | h1           | heads          | lr             | wd             | h1             | heads          |\\n| Cora           | 0.001             | 0           | 256          | 8              | 0.001          | 0              | 512            | 1              |\\n| Citeseer       | 0.001             | 0           | 64           | 4              | 0.001          | 0              | 256            | 1              |\\n| Pubmed         | 0.01              | 1.00E-05    | 64           | 8              | 0.001          | 0              | 512            | 1              |\\n| Cora-CA        | 0.01              | 0           | 64           | 4              | 0.001          | 0              | 128            | 1              |\\n| DBLP-CA        | 0.01              | 1.00E-05    | 64           | 8              | 0.001          | 0              | 256            | 1              |\\n| Zoo            | 0.001             | 1.00E-05    | 64           | 8              | 0.1            | 0              | 64             | 1              |\\n| 20News         | OOM               | OOM         | OOM          | OOM            | OOM            | OOM            | OOM            | OOM            |\\n| Mushroom       | 0.01              | 1.00E-05    | 64           | 1              | 0.001          | 0              | 128            | 1              |\\n| NTU2012        | 0.001             | 0           | 256          | 4              | 0.001          | 0              | 256            | 1              |\\n| ModelNet40     | 0.001             | 0           | 512          | 8              | 0.001          | 0              | 512            | 1              |\\n| Yelp           | OOM               | OOM         | OOM          | OOM            | OOM            | OOM            | OOM            | OOM            |\\n| House(1)       | 0.001             | 0           | 512          | 8              | 0.001          | 0              | 256            | 1              |\\n| Walmart(1)     | 0.001             | 0           | 256          | 1              | 0.001          | 0              | 512            | 1              |\\n| House(0.6)     | 0.01              | 1.00E-05    | 64           | 8              | 0.001          | 0              | 256            | 1              |\\n| Walmart(0.6)   | 0.001             | 0           | 256          | 1              | 0.001          | 0              | 512            | 1              |\\n\\n|                | HyperGCN          | UniGCNII    |\\n|----------------|-------------------|-------------|\\n|                | lr                | wd          |\\n| Cora           | 0.001             | 0           |\\n| Citeseer       | 0.01              | 1.00E-05    |\\n| Pubmed         | 0.01              | 1.00E-05    |\\n| Cora-CA        | 0.01              | 1.00E-05    |\\n| DBLP-CA        | 0.01              | 1.00E-05    |\\n| Zoo            | 0.001             | 1.00E-05    |\\n| 20News         | OOM               | OOM         |\\n| Mushroom       | 0.001             | 0           |\\n| NTU2012        | 0.01              | 1.00E-05    |\\n| ModelNet40     | 0.001             | 0           |\\n| Yelp           | OOM               | OOM         |\\n| House(1)       | 0.01              | 1.00E-05    |\\n| Walmart(1)     | 0.001             | 0           |\\n| House(0.6)     | 0.01              | 1.00E-05    |\\n| Walmart(0.6)   | 0.001             | 0           |\"}"}
