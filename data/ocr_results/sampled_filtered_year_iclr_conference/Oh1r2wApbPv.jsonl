{"id": "Oh1r2wApbPv", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2022\\n\\nCONTEXTUALIZED SCENE IMAGINATION FOR GENERATIVE COMMONSENSE REASONING\\n\\nPeifeng Wang,3, Jonathan Zamora,2\u2217, Junfeng Liu,1\u2217, Filip Ilievski,3, Muhao Chen,1,3, Xiang Ren,1,3\\n\\n1Department of Computer Science, University of Southern California\\n2Department of Computer Science, University of California, San Diego\\n3Information Sciences Institute, University of Southern California\\n\\n{peifengw,liujunfe,muhaoche,xiangren}@usc.edu, jzamoraa@ucsd.edu, ilievski@isi.edu\\n\\nABSTRACT\\n\\nHumans use natural language to compose common concepts from their environment into plausible, day-to-day scene descriptions. However, such generative commonsense reasoning (GCSR) skills are lacking in state-of-the-art text generation methods. Descriptive sentences about arbitrary concepts generated by neural text generation models (e.g., pre-trained text-to-text Transformers) are often grammatically fluent but may not correspond to human common sense, largely due to their lack of mechanisms to capture concept relations, to identify implicit concepts, and to perform generalizable reasoning about unseen concept compositions.\\n\\nIn this paper, we propose an Imagine-and-Verbalize (I&V) method, which learns to imagine a relational scene knowledge graph (SKG) with relations between the input concepts, and leverage the SKG as a constraint when generating a plausible scene description. We collect and harmonize a set of knowledge resources from different domains and modalities, providing a rich auxiliary supervision signal for I&V. The experiments demonstrate the effectiveness of I&V in improving language models on both concept-to-sentence and concept-to-story generation tasks, while enabling the model to learn well from fewer task examples and generate SKGs that make common sense to human annotators.\\n\\nINTRODUCTION\\n\\nHumans describe everyday scenes in natural language based on their understanding of common concepts encountered in their environment (Tincoff & Jusczyk, 1999). Analogously, the task of generative commonsense reasoning (GCSR) asks machines to generate a description of everyday situations based on a set of concepts and an initial context (Liu et al., 2020; Li et al., 2021). For example, given concept words \\\\{dog, frisbee, catch, throw\\\\}, a machine is expected to generate a plausible description, e.g., \\\"A man throws a frisbee and his dog catches it in the air.\\\" Machines with GCSR skills would communicate fluidly with humans, e.g., when summarizing a document by preserving its key details (Sha, 2020), composing a creative story according to a set of clues (Yao et al., 2019), and generating a conversation reply that includes specified keywords (Mou et al., 2016).\\n\\nGCSR poses three unique challenges for automatic text generation methods. To depict plausible scenes when composing sentences, machines require commonsense knowledge to reason about the relations between concepts and the affordances of objects (e.g., \\\"dog\\\" performs the action \\\"catch\\\" but not the action \\\"throw\\\"). Moreover, machines require a compositional generalization ability (Keysers et al., 2019), i.e., the ability to judge the plausibility of a new concept composition that has not been observed during training, and to identify concepts related to the scene that are not explicitly provided (e.g., \\\"person\\\" to perform \\\"throw\\\" in the above example).\\n\\nGCSR can be directly attempted by fine-tuning pre-trained text-to-text language models (LMs) (Rafael et al., 2019; Radford et al., 2019). While pre-trained LMs capture certain encyclopedic knowledge...\"}"}
{"id": "Oh1r2wApbPv", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Overview of the proposed I&V method: (1) We leverage SKGs for unifying scene knowledge from different resources. (2) We pre-train a contextualized imagination module to construct an SKG for a set of concepts, based on the collected SKG instances. (3) At inference time, our verbalization module realizes the generated SKG into natural language.\\n\\nedge mentioned in text corpora (e.g., Wikipedia) (Petroni et al., 2019) and can combine concepts in novel ways, they may generate grammatically fluent but implausible sentences that conflict with human common sense (Lin et al., 2020). This is because LMs have no intrinsic mechanism to reason over high-level relations between concepts Zhou et al. (2020). To close the knowledge gap, recent work augment LM input with knowledge graph triples (e.g., (dog, CapableOf, catch)) retrieved from ConceptNet (Liu et al., 2020; Li et al., 2020), or prototype sentences that cover input concepts retrieved from external text corpora (Fan et al., 2020; Wang et al., 2021). However, despite the input augmentation, GCSR skills are implicitly learned based on the concept-text pairs in the training data, without explicit supervision. While some recent work propose content planning in story generation in the form of plots or scripts (Yao et al., 2019; Fan et al., 2019), only the narrative order of concepts are planned in those methods instead of their plausible roles and relations. Given the complexity of the GCSR task, machines need a direct mechanism to create a high-level relational representation of the provided concepts, which would allow them to judge the plausibility of their combination.\\n\\nIn this paper, we propose to model an explicit scene imagination step which constructs a structured representation of a plausible scene based on input concepts and initial context. The scene imagination module formalizes the background knowledge required for the reasoning through a contextualized relational graph, called scene knowledge graph (SKG). An SKG allows us to collect and harmonize diverse commonsense knowledge across resources and modalities into a comprehensive SKG distribution (see Figure 1 for an illustration). We develop an imagine-and-verbalize framework: an imagination module learns to construct a contextualized SKG from input concepts and context by pretraining over a large amount of external SKGs; a verbalization module learns to faithfully realize the imagined SKG into natural language by training over downstream datasets. By learning from a large number of diverse SKGs, our method is able to capture plausible relations between concepts. By integrating these SKGs with LMs, the imagination module is able to compose new objects in novel ways, and identify implicit concepts for a scene. Imagine-and-verbalize decomposes the challenging scene description task into two realistic tasks for which a wealth of training data can be collected, simultaneously enabling for effective and explainable GCSR.\\n\\nWe experiment with two GCSR tasks and three scene graph resources, observing consistently better or competitive performance to SotA baselines. We find that (1) SKGs extracted from visual captions and story datasets are more helpful than other resources; (2) our model can learn faster (with less training data) with the help of scene imagination; and (3) the imagination module with a larger backbone LM demonstrates larger capacity in encoding commonsense knowledge. Our human evaluation study on the generated imagination indicates that these SKGs capture common sense and that the verbalization module generates the text by following the guidance of the imagination.\"}"}
{"id": "Oh1r2wApbPv", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"word \\\\( x \\\\) (or concept for brevity) is a commonly seen object (nouns such as \\\"dog\\\" or \\\"frisbee\\\") or commonly performed action (verbs such as \\\"throw\\\" or \\\"catch\\\"). The goal of GCSR is to generate \\\\( K \\\\) sentences \\\\( \\\\{y_1, y_2, \\\\ldots, y_K\\\\} \\\\), each describing a plausible situation following human common sense for a concept set \\\\( x_i \\\\). The \\\\( i \\\\)-th sentence \\\\( y_i \\\\subset Y \\\\) should be generated using all concepts in \\\\( x_i \\\\).\\n\\nWe consider two variants of GCSR: 1) concepts-to-sentence generation (Lin et al., 2020), where no context is given (i.e., \\\\( c \\\\) is empty) and only one concept set is provided (\\\\( K = 1 \\\\)); and 2) concepts-to-story generation task, where \\\\( c \\\\) is the leading sentence of a multi-sentence story and more than one concept sets are provided, each corresponding to one sentence to be generated (\\\\( K > 1 \\\\)). Both tasks are evaluated by comparing the machine-generated text with human-generated (gold) references.\\n\\n2.1 THE IMAGINE-AND-VERBALIZE APPROACH\\n\\nPre-trained LMs struggle with learning a generalizable mapping from concepts to plausible sentences solely based on the training data. Augmenting concepts with external knowledge to form the input \\\\( X' \\\\) and fine-tuning a pretrained LM to model \\\\( P(Y|C, X') \\\\) (Liu et al., 2020; Fan et al., 2020; Li et al., 2021) alleviates this issue partially, while still learning a direct mapping of \\\\( \\\\{C, X'\\\\} \\\\rightarrow Y \\\\).\\n\\nIn this work (Figure 1), we decompose the GCSR task into two sub-tasks, namely contextualized scene generation (imagination) and scene-aware text generation (verbalization):\\n\\n\\\\[\\nP(Y|C, X) = XZP(Y|C, X, Z)P(Z|C, X),\\n\\\\]\\n\\nwhere \\\\( Z \\\\) denotes the scene representation for the given concepts and context.\\n\\nThe contextualized scene imagination module \\\\( P(Z|C, X) \\\\) aims to construct a multi-relational graph representation \\\\( Z \\\\) (scene knowledge graph, or SKG) that describes a plausible scene that involves all input concepts and corresponds to the provided context. To learn this module, we collect a diverse set of SKG instances from different resources and modalities to form a comprehensive distribution of scenes (\u00a72.2). The imagination module is pre-trained over the collected scene instances and learns to generate SKGs depicting plausible day-to-day situation. The imagination module is based on a neural architecture, which enables it to generate concept compositions that might not have been observed during training (\u00a72.3).\\n\\n2 We leverage the contextualized SKG for text generation with a verbalization module \\\\( P(Y|C, X, Z) \\\\) which takes the context, concepts, and the generated SKG as input, and composes a grammatical and plausible scene description in natural language (\u00a72.4).\\n\\nTo perform GCSR, where one or multiple concept sets are given, we apply the imagination module to sample \\\\( z_i \\\\). Since the marginalization over \\\\( Z \\\\) is generally intractable due to the complex structure of the SKGs, we only sample the most probable scene representation \\\\( z^*_i \\\\) that maximizes \\\\( P(z_i|c', x_i) \\\\), where \\\\( c' \\\\) includes the given context \\\\( c \\\\) and the previously generated \\\\( y_j \\\\), \\\\( (j < i) \\\\). We then apply the verbalization module to generate one sentence at a time by sampling from \\\\( P(y_i|c', x_i, z_i^*) \\\\). Multiple sentences are generated by iteratively applying the imagination and verbalization modules.\\n\\n2.2 IMAGINATION VIA GENERATING SKG\\n\\nImagination through SKGs\\n\\nWe adopt the term \\\"scene graph\\\" from the computer vision community, and we generalize it to a novel relational schema that represents knowledge from multiple modalities. Our SKG is defined as a relational graph \\\\( G = (E, R) \\\\) that organizes a set of concepts in a coherent scene. The node set \\\\( E \\\\) of the graph includes both given and implicit concepts, while each relation (edge type) \\\\( r \\\\in R \\\\) denotes how two concepts should be related. We follow the Abstract Meaning Representation (AMR) (Banarescu et al., 2013) schema to consider the core relations between two concepts, which corresponds to the commonsense knowledge required by GCSR. Table 7 in the appendix illustrates a few representative relations and their examples.\\n\\nCollecting Diverse SKGs\\n\\nWe consider two complementary modalities, text and vision, as some concepts and relationships are more likely to occur in one modality versus another. (1) Textual Modality: According to pragmatic principles of human language, people generally leave out expected details about common scenes (Grice, 1975). For this reason, we extract SKGs from visual captions and narrative stories, in which human annotators are asked to explicitly describe scenes that...\\n\\nThe imagination module can be further fine-tuned over the downstream datasets.\"}"}
{"id": "Oh1r2wApbPv", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gold SKGs from external resources for continual pretraining (Optional)\\n\\nSilver SKGs from task dataset for fine-tuning\\n\\nTransformer\\n\\nFigure 2: Continual pretraining and fine-tuning of the imagination module to output a linearized SKG based on a sequential input (context and concepts).\\n\\nmay happen using descriptive language as shown in Figure 1(a,b). To extract an SKG out of these textual signals, we adopt the AMR parsing tool to transform each sentence into an AMR graph. This process yields a single SKG per sentence. For the story SKGs, we also keep the sentences (up to 256 tokens) that precede the sentence that corresponds to the SKG, as context.\\n\\nVisual Modality:\\n\\nImage captions focus on salient information and may not capture all useful visual signals. Thus, we also capture the scene structures directly from images, by using VisualGenome (Krishna et al., 2016), a large-scale scene graph dataset annotated by humans. To adopt a unified SKG schema, we manually map the relations in scene graphs from VisualGenome to the ones used in textual SKGs. A full set of mapping rules can be found in the Appendix (A.1). The statistics of the SKGs collected from each resource/modality are summarized in Table 1. We note that visual scene graphs may be biased towards knowledge about spatial relationships and object affordance, which further motivates our decision to extract SKGs from multiple modalities.\\n\\nTable 1: Statistics of the SKG instances collected from different resources.\\n\\n| Knowledge source | # SKGs     | # Concepts |\\n|------------------|------------|-----------|\\n| Caption-AMR      | 584,252    | 22,961    |\\n| Story-AMR       | 927,163    | 41,272    |\\n| VG-SceneGraph    | 292,596    | 41,629    |\\n| All              | 1,792,941  | 84,835    |\\n\\nWe describe how we pre-train the scene imagination model using multimodal SKG examples collected from diverse sources, and how we fine-tune the imagination module to downstream datasets.\\n\\nA straightforward way to construct a SKG is to retrieve instances that contain all the given concepts from the collected SKGs. However, performance of such method is limited by the coverage of the SKG collection and will fail when encountering novel concept composition. We propose to model \\\\( P(Z|C, X) \\\\) with a neural graph generator. Inspired by previous work on (conditional) graph generation (You et al., 2018), we formulate SKG construction as an auto-regressive sequence generation task, where a linearized SKG is generated sequentially conditioned on the context, input concepts, and the graph sequence generated so far. Since the sequence generation problem can be efficiently tackled by pre-trained auto-regressive LMs (e.g., GPT-2 Radford et al. (2019)), we adopt LMs as the backbone of our imagination module (Bosselut et al., 2019; Wang et al., 2020).\\n\\nLinearized SKG Generation\\n\\nTo form training instances for the imagination module, we treat the nodes in an SKG instance as input concepts and the linearized SKG as the target output (Figure 2). The input concepts are concatenated into a sequence \\\\( x = [x_1, x_2, ..., x_n] \\\\), preceded by the context \\\\( c' \\\\in C \\\\). When \\\\( c' \\\\) is not given, we prepend the word \\\"none\\\" to the concept sequence. To linearize an AMR-based SKG into a sequence \\\\( z = [z_1, z_2, ..., z_m] \\\\), we adopt the PENMAN serialization format (Goodman, 2020) which converts AMR into a spanning tree over the graph. This format is shown to be more suitable than other linearization strategies like depth-first-search (DFS) in enabling LMs to learn the graph structure (Mager et al., 2020).\\n\\nDuring training, we randomize the order of the concepts at every training step such that the graph generator learns to be invariant to concept order (Zhang et al., 2019). For each training instance, we randomly discard a small subset of the SKG nodes (concepts) in each training epoch. This simulates the scenario in which a subset of the concepts that constitute a scene will be given, thus teaching the model to infer implicit concepts for completing a plausible scene.\"}"}
{"id": "Oh1r2wApbPv", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: The most common relation types in SKG instances and their example triplets.\\n\\n| Relation types | Examples              |\\n|----------------|-----------------------|\\n| ARG1           | (play, ARG1, guitar)  |\\n| ARG0           | (play, ARG0, man)     |\\n| ARG2           | (ask, ARG2, girl)     |\\n| Location       | (play, Location, stage)|\\n| Time           | (play, Time, sing)    |\\n| Op1            | (down, Op1, stair)    |\\n| Part           | (dog, Part, ear)      |\\n\\nA.2 IMPLEMENTATION DETAILS\\n\\nFor the main experiments, we develop the imagination module by continually pre-train a T5-large model over the caption, story, and vision SKGs. We then further adapt the imagination module over each task dataset annotated with the silver-standard SKGs by further fine-tuning. To train the verbalization module, we fine-tune T5-base and BART-large as two backend LMs. During training, we use both silver-standard SKGs and generated SKGs, while averaging the training loss associated with each of them. We use the Adam optimizer with weight decay $10^{-2}$. We search the optimal hyper-parameters based on the perplexity over the development set, where the learning rate is chosen from \\\\{2e^{-6}, 1e^{-5}, 3e^{-5}, 1e^{-4}\\\\}, the batch size is chosen from \\\\{8, 16, 32, 64, 128\\\\}. \\n\\nA.3 STATISTICAL ANALYSIS\\n\\nTable 8: Statistical analysis (p-values) for the ablation study on the backend LM used by the verbalization module and the low-resource experiment.\\n\\n| BART-base | BART-large | T5-base | T5-base |\\n|-----------|------------|---------|---------|\\n|           |            |         |         |\\n| <0.01     | <0.01      | <0.01   | <0.01   |\\n\\nTable 8: Statistical analysis (p-values) for the ablation study on the backend LM used by the verbalization module and the low-resource experiment.\\n\\nTable 8: Statistical analysis (p-values) for the ablation study on the backend LM used by the verbalization module and the low-resource experiment.\\n\\nTable 8: Statistical analysis (p-values) for the ablation study on the backend LM used by the verbalization module and the low-resource experiment.\\n\\nTable 8: Statistical analysis (p-values) for the ablation study on the backend LM used by the verbalization module and the low-resource experiment.\\n\\nTable 8: Statistical analysis (p-values) for the ablation study on the backend LM used by the verbalization module and the low-resource experiment.\\n\\n| # Training examples | CommonGen (in-house) | VIST ROC | 50 | <0.01 | <0.01 | <0.01 | <0.01 |\\n|---------------------|----------------------|----------|----|-------|-------|-------|-------|\\n| 50                  | <0.01                | <0.01    |    |       |       |       |       |\\n| 500                 | <0.01                | <0.01    |    |       |       |       |       |\\n| 5000                | <0.01                | NA       |    |       |       |       |       |\\n| All                 | NA                   | <0.05    |    |       |       |       |       |\\n\\nWe conduct statistical significance analysis on the experiments involving baselines, which include the ablation study on the backend LM used by the verbalization module (Figure 4) and the low-resource experiment (Figure 5). The p-values are shown in Table 8, where <0.01 indicates a significant improvement and <0.05 indicates a fairly significant improvement, and NA indicates that our method does not outperform the best baseline.\\n\\nA.4 QUALITATIVE ANALYSIS ON HOW IMAGINATION HELPS\\n\\nWe show how imagination can help generating sentences that follow common sense via qualitative analysis in Table 9-10. As comparison, we also show the results from the Node2Text baseline which does not imagines. We organize the results based on 5 (not necessarily exclusive) types of errors made by Node2Text, which include incorrect role attribution to 1) agents, 2) actions or 3) objects, 4) failing to infer the implicit concepts and 5) misunderstanding the relations between events.\\n\\nA.5 QUALITY EVALUATION ON THE GENERATED SKGS\\n\\nSince there are no groundtruth SKGs annotated in downstream datasets, we use the silver-standard SKGs as reference to give a rough estimation of the quality of the generated SKGs. We focus on re-\"}"}
{"id": "Oh1r2wApbPv", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 9: Qualitative analysis on errors made without imagination and how imagination can help fix the errors (Part 1). The left arrow \u2190\u2212 indicates the key relations that fix the errors.\\n\\n### Error 1 (Incorrect Agent)\\n\\n| Example 1 | Example 2 |\\n|-----------|-----------|\\n| Input concepts | owner, chase, dog, ball, throw |\\n| Text w/o imagination | The dog is chasing the ball and throwing it at the owner. |\\n| Text w/ imagination | A dog chases a ball being thrown by its owner. |\\n| Generated SKG | (chase, ARG0, dog), (chase, ARG1, ball), (throw, ARG1, ball), (throw, ARG0, owner) \u2190\u2212 (pull, ARG0, boat), \u2190\u2212 (pull, ARG1, person), (ski, ARG0, person), (hold, ARG0, person), (hold, ARG1, rope) |\\n\\n### Error 2 (Incorrect Action)\\n\\n| Example 1 | Example 2 |\\n|-----------|-----------|\\n| Input concepts | butter, pot, crack, egg, add |\\n| Text w/o imagination | She adds eggs, crackers, and butter to a pot. |\\n| Text w/ imagination | You crack an egg and add butter to a pot. |\\n| Generated SKG | (crack, ARG0, you), (crack, ARG1, egg), \u2190\u2212 (add, ARG0, you), (add, ARG1, butter), (add, ARG2, pot) (stand, ARG1, man), (man, part, tongue), (stick, ARG0, man), (stick, ARG1, tongue), \u2190\u2212 (stick, ARG2, out) |\\n\\n### Error 3 (Incorrect Object)\\n\\n| Example 1 | Example 2 |\\n|-----------|-----------|\\n| Input concepts | hit, bottle, shoe, open, wall |\\n| Text w/o imagination | Someone opens his shoe and hits a bottle on the wall. |\\n| Text w/ imagination | A man opens a bottle and hits his shoe against a wall. |\\n| Generated SKG | (open, ARG0, man), (open, ARG1, bottle), \u2190\u2212 (hit, ARG0, man), (hit, ARG1, shoe), \u2190\u2212 (shoe, poss, man), (hit, ARG2, against), (against, op1, wall) (talk, ARG0, man), (wear, ARG0, man), (wear, ARG1, glasses), \u2190\u2212 (talk, medium, phone) |\\n\\nCall since the silver-standard SKGs may not cover all the plausible scenes. Our evaluation considers the following three metrics. 1) Average recall of the given concepts (Explicit Concepts) to examine whether an SKG contains all the given concepts. 2) Average recall of the implicit concepts (Implicit Concepts) to examine whether an SKG also contains implicit concepts. The implicit concepts for reference are the nodes from the silver-standard SKGs excluding the given concepts. 3) Average recall of the relations (Relation) to examine the proportion of the referenced relations that are covered by the generated SKGs. Here, a relation is considered as correct only if the head concept, relation and the tail concept all match the reference.\\n\\nThe results shown in Table 11 indicate a fairly good quality of the generated SKGs, which connect all the given concepts for over 99% of the cases and have a large overlap (over 68%) with the silver-standard SKGs. Note that the particular low recall on implicit concepts is due to the fact that there can be many different implicit concepts to constitute a complete SKG.\"}"}
{"id": "Oh1r2wApbPv", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Qualitative analysis on errors made without imagination and how imagination can help fix the errors (Part 2). The left arrow \\\\( \\\\leftarrow \\\\) indicates the key relations that fix the errors.\\n\\nError 4 (Implicit Concepts)\\n\\nExample 1\\nExample 2\\n\\nInput concepts\\n\\n\\\\{fill, liquid, machine, bottle\\\\} \\\\{lasso, catch, horse, animal, ride\\\\}\\n\\nText w/o imagination\\n\\nA machine holding a bottle filled with liquid.\\n\\nAnimals ride a horse that caught a lasso.\\n\\nText w/ imagination\\n\\nA man holds a bottle filled with liquid in a machine.\\n\\nA man riding a horse to catch an animal with a lasso.\\n\\nGenerated SKG\\n\\n\\\\( (hold, ARG0, man) \\\\leftarrow (hold, ARG1, bottle) \\\\)\\n\\\\( (fill, ARG1, bottle) \\\\leftarrow (fill, ARG2, liquid) \\\\)\\n\\\\( (hold, location, machine) \\\\)\\n\\nError 5 (Event Relations)\\n\\nExample 1\\nExample 2\\n\\nInput concepts\\n\\n\\\\{trick, perform, begin, stunt, dance\\\\} \\\\{stir, pour, pot, ingredient, begin\\\\}\\n\\nText w/o imagination\\n\\nA group of people begin performing a stunt while performing a trick.\\n\\nShe begins stirring the ingredients in the pot and begins pouring them into the water.\\n\\nText w/ imagination\\n\\nA man performs stunts and tricks as he begins to dance.\\n\\nHe pours the ingredients into the pot and begins to stir them.\\n\\nGenerated SKG\\n\\n\\\\( (perform, ARG0, man) \\\\)\\n\\\\( (perform, ARG1, stunt) \\\\)\\n\\\\( (perform, ARG1, trick) \\\\)\\n\\\\( (perform, time, begin) \\\\leftarrow (begin, ARG0, man) \\\\)\\n\\\\( (begin, ARG1, dance) \\\\)\\n\\\\( (dance, ARG0, man) \\\\)\\n\\n\\\\( (pour, ARG0, he) \\\\)\\n\\\\( (pour, ARG1, ingredient) \\\\)\\n\\\\( (pour, ARG2, pot) \\\\)\\n\\\\( (begin, ARG0, he) \\\\leftarrow (begin, ARG1, stir) \\\\)\\n\\\\( (stir, ARG0, he) \\\\)\\n\\\\( (stir, ARG1, ingredient) \\\\)\"}"}
{"id": "Oh1r2wApbPv", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12: Ablation study on what input is fed to the verbalization module.\\n\\n| Input               | CommonGen (in-house) | VIST ROC | SKG-only | Concept + SKG | Context + SKG | Context + Concept + SKG |\\n|---------------------|----------------------|----------|----------|---------------|--------------|-------------------------|\\n|                     | 33.39                | 17.13    | 18.90    | 33.49         | NA           | 57.99                   |\\n|                     | 27.01                | 36.42    |          |               | 59.21        | 60.63                   |\\n\\nTable 13: Ablation study on using 1) silver-standard SKGs, 2) generated SKGs or 3) both to train the verbalization module.\\n\\n| Input SKGs                  | CommonGen (in-house) | VIST ROC | SKG-only |\\n|-----------------------------|----------------------|----------|----------|\\n| Silver-standard             | 33.19                | 53.26    | 60.55    |\\n| Generated                   | 32.56                | 58.34    | 59.82    |\\n| Silver. + Generated         | 33.49                | 59.21    | 60.63    |\\n\\nTable 14: Ablation study on the design of the generation process.\\n\\n| Generation Process          | VIST ROC  |\\n|-----------------------------|-----------|\\n| All-at-once                 | 57.16     |\\n| Independent                 | 27.01     |\\n| Iterative (I&V)             | 59.21     |\\n\\nA.7 Ablation study on what SKGs to use when learning verbalization\\n\\nWe conduct an ablation study where we use 1) silver-standard SKGs only, 2) generated SKGs only and 3) both types of SKGs during training the verbalization module. The results in Table 13 validates that using both types of SKGs yield to the best performance of our method.\\n\\nA.8 Ablation study on concept-dropout\\n\\nWe conduct an ablation study where we do not drop any concepts when we train the imagination module. We then apply the imagination module on CommonGen and conduct the experiments. The final performance is 28.28 in SPICE while the system with the imagination module trained with concept-dropout achieves 33.49. This validates that dropping concepts is necessary since in the downstream tasks not all the concepts are provided and the model needs to infer the implicit ones.\\n\\nA.9 Ablation study on the generation process\\n\\nWe conduct an ablation study to verify that the iterative generation process is more effective than 1) generating all the sentences at once, and 2) generating each sentence independently. For baseline 1), we learn an uncontextualized imagination module which only takes concepts as input and does not need the previous generated context. We apply the uncontextualized imagination module to generate all the SKGs at once and then the verbalization module generates all the target sentences at once by taking the provided context, all the concept sets and all the SKGs as input. For baseline 2), we still use the contextualized imagination module to generate an SKG at a time. But we learn a verbalization module which does not take the previously generated sentences as input and thus generate each target sentence independently. We conduct the ablation study on the two datasets of the concept2story task (we do not consider the CommonGen benchmark here since there is only one target sentence to be generated in CommonGen). The results of the average SPICE scores from 3 runs are shown in Table 14. The two baselines are both outperformed by our iterative approach, which verifies that the previously generated sentences are important for both imagination and verbalization and thus the iterative generation process is necessary.\"}"}
{"id": "Oh1r2wApbPv", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"sentences resulting from the corresponding SKGs and the ground-truth sentences for reference. For each dataset, 100 instances are randomly chosen for evaluation. Annotators are students majoring in computer science and not all of them know about AMR language prior to the human evaluation. To facilitate annotators' understanding of the evaluation task and AMR, we provide the detailed instruction and the examples of AMR relations. The annotators are asked to judge for: 1) **Completeness**, whether the SKG includes all the concepts (both given and implicit) to constitute a coherent scene; 2) **CommonSense**, whether the SKG organizes the concepts in a way that follows common sense; 3) **Alignment**, whether the generated sentence aligns with the SKG and 4) **Similarity**, whether the predicted sentence is similar to any referenced sentence in semantic. Annotation is based on a 3-point scale: a) 0 \u2013 \u201cI do not agree\u201d, b) 0.5 \u2013 \u201cI partially agree\u201d and c) 1.0 \u2013 \u201cI fully agree\u201d.\\n\\nTable 6: Human evaluation on the generated SKGs regarding Completeness (COM), CommonSense (CS) and Alignment (AL) and Similarity (SIM).\\n\\n|        | COM  | CS   | AL   | SIM  |\\n|--------|------|------|------|------|\\n| CommonGen | 97.30 | 90.15 | 89.90 | 88.30 |\\n| VIST    | 93.80 | 89.70 | 91.40 | 76.20 |\\n| ROC     | 95.70 | 86.60 | 87.80 | 75.68 |\\n\\nTable 6 shows the evaluation results where we get a fair level of agreement measured by Fleiss Kappa (\u03ba = 0.21). We observe that the generated SKGs are complete and follow common sense in a high degree across three datasets, which demonstrates the effectiveness of learning useful common-sense knowledge with vast indirect supervision from different resources. Moreover, the SKGs are well-aligned with the generated text, which indicates that the verbalization module consistently follows the guidance of the imagination module when generating sentences. The moderate similarity scores validate that the generated text is generally similar to the natural language sentences annotated by humans.\\n\\n**5 RELATED WORK**\\n\\n**Knowledge-Enhanced GCSR**\\n\\nRecent works (Liu et al., 2020; Li et al., 2021) on GCSR propose to retrieve external knowledge to enhance the text generation. Prototype-based models, including EKI-BART (Fan et al., 2020), Re-T5 (Wang et al., 2021), and KFCNet (Li et al., 2021) retrieve massive prototype sentences from external corpora like visual captions and Wikipedia as auxiliary input to the LM. Though the retrieved prototype sentences provide high coverage on the concepts, their model is supervised to compose sentences that are very similar to those existing prototypes. It is thus unclear whether their models are conducting commonsense reasoning or only mimicking the prototypes. KG-BART (Liu et al., 2020) incorporates the embedding of relational facts about the concepts from ConceptNet into both the encoders and decoders of the BART architecture (Lewis et al., 2020). As there could be multiple relations between two concepts, it is unclear how to select the relation that fits a given context (Fadnis et al., 2019). Our imagination module infers the relations between concepts by taking all the concepts into consideration and organizes them in a coherent way.\\n\\n**Content Planning**\\n\\nOur method is also related to prior works (Goldfarb-Tarrant et al., 2020) that propose intermediate representations as a way to \u201cplan ahead\u201d before generating long narratives. Plan-and-write (Yao et al., 2019) generates chains of keywords as a storyline, but do not consider relations between keywords (concepts) as we do. Action-plan (Fan et al., 2019) takes a step further by using predicate-argument with semantic role labeling, but still does not involve all the concepts in a sentence. Moreover, these methods are limited to obtaining supervision from task-specific datasets, while we gather effective indirect supervision signals from rich multi-source, multi-modal SKG representations without the need for additional annotations.\\n\\n**6 CONCLUSIONS**\\n\\nThis paper proposed to enhance neural architectures for GCSR with an intermediate imagination layer. We divided the GCSR process into two steps: imagination, which generated a plausible scene knowledge graph for a given set of concepts, and verbalization, which transformed this scene graph into a fluent sentence that corresponds to human common sense. The method was trained with diverse scene knowledge graphs derived from both text and vision modalities. Our experiments demonstrated the ability of the proposed method to perform GCSR effectively, by describing plausible scenes, and efficiently, by requiring less training data. The image caption graphs proved most beneficial to learn from. Future work should investigate the impact of imagination on interactive commonsense tasks, like dialogue generation, and include scene graphs from the audio modality.\"}"}
{"id": "Oh1r2wApbPv", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGMENTS\\n\\nWe thank the anonymous reviewers and all the collaborators in USC INK research lab for their valuable feedback. This material is based upon work sponsored by the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research.\\n\\nREFERENCES\\n\\nPeter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In European conference on computer vision, pp. 382\u2013398. Springer, 2016.\\n\\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. Abstract meaning representation for sembanking. In Proceedings of the 7th linguistic annotation workshop and interoperability with discourse, pp. 178\u2013186, 2013.\\n\\nSatanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 65\u201372, 2005.\\n\\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4762\u20134779, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1470. URL https://www.aclweb.org/anthology/P19-1470.\\n\\nKshitij Fadnis, Kartik Talamadupula, Pavan Kapanipathi, Haque Ishfaq, Salim Roukos, and Achille Fokoue. Heuristics for interpretable knowledge graph contextualization. arXiv preprint arXiv:1911.02085, 2019.\\n\\nAngela Fan, Mike Lewis, and Yann Dauphin. Strategies for structuring story generation. arXiv preprint arXiv:1902.01109, 2019.\\n\\nZhihao Fan, Yeyun Gong, Zhongyu Wei, Siyuan Wang, Yameng Huang, Jian Jiao, Xuanjing Huang, Nan Duan, and Ruofei Zhang. An enhanced knowledge injection model for commonsense generation. arXiv preprint arXiv:2012.00366, 2020.\\n\\nSteven Feng, Jessica Huynh, Chaitanya Prasad Narisetty, Eduard Hovy, and Varun Gangal. SAPPHIRE: Approaches for enhanced concept-to-text generation. In Proceedings of the 14th International Conference on Natural Language Generation, pp. 212\u2013225, Aberdeen, Scotland, UK, August 2021a. Association for Computational Linguistics. URL https://aclanthology.org/2021.inlg-1.21.\\n\\nSteven Y Feng, Kevin Lu, Zhuofu Tao, Malihe Alikhani, Teruko Mitamura, Eduard Hovy, and Varun Gangal. Retrieve, caption, generate: Visual grounding for enhancing commonsense in text generation models. arXiv preprint arXiv:2109.03892, 2021b.\\n\\nSeraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph Weischedel, and Nanyun Peng. Content planning for neural story generation with aristotelian rescoring. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4319\u20134338, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.351. URL https://aclanthology.org/2020.emnlp-main.351.\\n\\nMichael Wayne Goodman. Penman: An open-source library and tool for AMR graphs. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 312\u2013319, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-demos.35. URL https://aclanthology.org/2020.acl-demos.35.\\n\\nH Paul Grice. Logic and conversation, syntax and semantics. Speech Acts, 3:41\u201358, 1975.\"}"}
{"id": "Oh1r2wApbPv", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2022\\n\\nTing-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1233\u20131239, 2016.\\n\\nDaniel Keysers, Nathanael Sch\u00e4rli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, et al. Measuring compositional generalization: A comprehensive method on realistic data. In International Conference on Learning Representations, 2019.\\n\\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. 2016. URL https://arxiv.org/abs/1602.07332.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871\u20137880, 2020.\\n\\nHaonan Li, Yeyun Gong, Jian Jiao, Ruofei Zhang, Timothy Baldwin, and Nan Duan. Kfcnet: Knowledge filtering and contrastive learning network for generative commonsense reasoning. arXiv preprint arXiv:2109.06704, 2021.\\n\\nYikang Li, Pulkit Goel, Varsha Kuppur Rajendra, Har Simrat Singh, Jonathan Francis, Kaixin Ma, Eric Nyberg, and Alessandro Oltramari. Lexically-constrained text generation through commonsense knowledge extraction and injection. arXiv preprint arXiv:2012.10813, 2020.\\n\\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1823\u20131840, Online, November 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.findings-emnlp.165.\\n\\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74\u201381, 2004.\\n\\nYe Liu, Yao Wan, Lifang He, Hao Peng, and Philip S Yu. Kg-bart: Knowledge graph-augmented bart for generative commonsense reasoning. arXiv preprint arXiv:2009.12677, 2020.\\n\\nManuel Mager, Ram\u00f3n Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu Florian, and Salim Roukos. Gpt-too: A language-model-first approach for amr-to-text generation. arXiv preprint arXiv:2005.09123, 2020.\\n\\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 839\u2013849, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1098. URL https://aclanthology.org/N16-1098.\\n\\nLili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, and Zhi Jin. Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation. arXiv preprint arXiv:1607.00970, 2016.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311\u2013318, 2002.\\n\\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463\u20132473, Hong Kong, 11\"}"}
{"id": "Oh1r2wApbPv", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"APPENDIX A.1 RULES FOR MAPPING VISUAL SCENE GRAPHS TO SKG\\n\\nThere are 3858 relation types in our processed VisualGenome dataset due to the noisy annotation. We map these relations into 8 relations. For relations that are annotated as verbs by VisualGenome, we break the relationship (subject, relation, object) into (relation, :ARG0, subject) and (relation, :ARG1, object). For other popular relations, we conduct the following mapping:\\n\\n(subject, be, object) \u2192 (subject, domain, object),\\n(subject, displace, object) \u2192 (subject, possible, object),\\n(subject, have/of, object) \u2192 (subject, part, object),\\n(subject, with, object) \u2192 (subject, poss, object),\\n(subject, on/behind/at/under/along/in/..., object) \u2192 (subject, location, object).\\n\\nThe remaining relations that do not follow the above mapping criteria are mapped to an \u201cother\u201d relation. Note that the 7 non-\u201cother\u201d relations make up 97.73% of the triplets in VisualGenome.\"}"}
{"id": "Oh1r2wApbPv", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"We report the \u201coracle\u201d performance of our system using silver-standard SKGs during inference to estimate the upper-bound of our method. The result is shown in Table 15.\\n\\n### Table 15: Evaluation results (SPICE) with I&V using silver-standard SKGs during inference.\\n\\n| SKGs (inference) | CommonGen (in-house) | VIST | ROC |\\n|------------------|----------------------|------|-----|\\n| Silver-standard  | 41.85                | 69.34| 67.70|\\n| Generated        | 33.49                | 59.21| 60.63|\"}"}
{"id": "Oh1r2wApbPv", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Continual-Pretraining and Fine-tuning\\n\\nWith both the input concepts (plus context) and the output graph linearized as sequences based on the collected SKG instances, we continually pretrain an auto-regressive LM to generate $z = \\\\text{Transformer}(c', x)$. The training objective is to maximize $P(Z|C, X)$ by minimizing the negative log-likelihood:\\n\\n$$L = -\\\\sum_{t=1}^{m} \\\\log P(z_t|z_{<t}, c', x).$$ (2)\\n\\nOur pre-trained imagination module generates an SKG on the fly, and it can be further fine-tuned on downstream datasets, when their distributions of context and concepts are different from the pretraining data (see Figure 2 for illustration). Since downstream datasets cannot be expected to have ground-truth SKGs paired with each training example, we apply the AMR parsing tool described in \u00a72.2 on the training sentences to obtain silver-standard SKGs. We then follow the same training procedure to continually pretrain the module into a customized imagination module for a specific downstream dataset.\\n\\n2.4 SCENE-AWARE VERBALIZATION\\n\\nIterative Imagine-and-Verbalize\\n\\nAt the inference time, we apply the trained imagination module iteratively to generate the most plausible SKG for each given concept set $x_i$, i.e., $z_i^* = \\\\arg\\\\max_z P(z_i|c', x_i)$, where the context $c'$ includes both the given context $c$ and the previously generated sentences $\\\\{y_j\\\\}$ ($j < i$). The generated SKG is used by the scene-aware verbalization module to model $P(Y|C, X, Z)$. The verbalization module generates the $i$-th sentence by sampling from $P(y_i|c', x_i, z_i^*)$. Multiple sentences are generated iteratively by alternating between the scene imagination (to construct SKG) and verbalization (to produce the next sentence). See Figure 3 for an illustration of this iterative inference process.\\n\\nModel Training\\n\\nSince both the linearized SKG (generated by the imagination module) and the target sentences are sequences by nature, we design $P(Y|C, X, Z)$ as a sequence-to-sequence generative model and learn this verbalization module by fine-tuning another pre-trained auto-regressive LM, i.e., $y_i = \\\\text{Transformer}(c', x_i, z_i^*)$. To form the input for generating the sentence $y_i$, we concatenate the context $c'$, the concept set sequence $x_i$ and $z_i^*$ into one sequence as illustrated in Figure 3. We then train the model to maximize $P(Y|C, X, Z)$ by minimizing the negative log-likelihood:\\n\\n$$L_{verbalize} = -\\\\sum_{t=1}^{l} \\\\log P(y_i|y_{<i}, c', x_i, z_i^*).$$ (3)\\n\\nFor each training instance $(y_i, c', x_i)$, we construct two types of SKG instances as the input $z_i$:\\n\\n1. We perform AMR parsing on $y_i$ to obtain a silver-standard SKG;\\n2. We apply the trained imagination module to generate a SKG $z_i^* = \\\\arg\\\\max_z P(z_i|c', x_i)$, where $c'$ includes the given context $c$ and the ground-truth prefix sentences $\\\\{y_j\\\\}$ ($j < i$). We find it beneficial to train the verbalization module over these two types of SKGs as evidenced by our ablation study (\u00a7A.7). During inference, the SKG $z_i$ is generated by the imagination module, while $c'$ includes the given context $c$ and the previous sentences $\\\\{y_j\\\\}$ ($j < i$) generated by the verbalization module.\\n\\n3 EXPERIMENTAL SETUP\\n\\nTasks & Datasets\\n\\nWe consider two GCSR tasks: Concept2Sentence and Concept2Story.\\n\\n(1) Concept2Sentence is a task of generating a single sentence for a given set of concepts and no context.\\n\\nOur ablation study in Appendix A.6 shows that including all these elements as input is helpful.\"}"}
{"id": "Oh1r2wApbPv", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We evaluate Concept2Sentence on the CommonGen (Lin et al., 2020) benchmark. Since the labels of the official test set are not publicly available, we submit our method to the leaderboard to obtain its performance. Notably, the concept sets in CommonGen's test set are novel and do not appear in the training set. We also create an in-house split of CommonGen to facilitate comparison between different variants of our method and the baselines.\\n\\n(2) Concept2Story is a generalization of the concept2sentence task, where the goal is to generate a coherent story with $K = 4$ sentences given a set of concepts and an initial verbal context. We construct two benchmarks based on the Visual Story Telling (VIST) (Huang et al., 2016) and ROCStories (Mostafazadeh et al., 2016) datasets. Following CommonGen, we conduct part-of-speech tagging over the sentences and further lemmatize the recognized verbs and nouns to obtain the concept sets.\\n\\n**Baselines**\\n\\n(1) **Concept2Sentence:** We consider several recent submissions to the leaderboard of CommonGen that leverage auxiliary information for GCSR. KFCNet (Li et al., 2021), Re-T5 (Wang et al., 2021), and EKI-BART (Fan et al., 2020) are prototype-based models, which retrieve sentences containing as many input concepts as possible from external captions and NLI datasets, and then use these sentences as auxiliary inputs. VisCTG (Feng et al., 2021b) is an image-augmented model which retrieves images from Google by using concepts as a query, followed by an image captioning model that generates captions as auxiliary inputs. KG-BART (Liu et al., 2020) is a knowledge graph-augmented model which retrieves relations between concepts from ConceptNet as auxiliary inputs. SAPPHIRE (Feng et al., 2021a) is a keyword-based model which extracts keywords from sentences as auxiliary inputs only during training. We also compare to Node2Text, which fine-tunes a pre-trained auto-regressive LM to take the concatenation of concepts as input and output the target sentences.\\n\\n(2) **Concept2Story:** We augment Node2Text with the iterative generation pipeline as in our method, which generates the next sentence given the provided context, previously generated sentences and the current concept set. In addition, we experiment with two representative methods from the controlled text generation literature. Plan-and-write (Yao et al., 2019) first generates storyline keywords, then uses the keywords to generate a story. We use the concept set and context to generate storyline keywords. Action-Plan (Fan et al., 2019) uses predicate-argument pairs as storyline. We adapt the KFCNet model to retrieve prototype sentences. All Concept2Story baselines are used in an iterative generation pipeline, to enable fair comparison to our method.\\n\\n**Table 2:** Performance comparison with the top-ranked, published models on the official CommonGen test set.\\n\\n| Model                     | BLEU-4 | CIDEr | SPICE |\\n|---------------------------|--------|-------|-------|\\n| KFCNet (Li et al., 2021)  | *43.62 | 18.85 | 33.91 |\\n| RE-T5 (Wang et al., 2021) | 40.86  | 17.66 | 31.08 |\\n| VisCTG (Feng et al., 2021b)| 36.94  | 17.20 | 29.97 |\\n| SAPPHIRE (Feng et al., 2021a) | 37.12  | 16.90 | 29.75 |\\n| KG-BART (Liu et al., 2020) | 33.87  | 16.93 | 29.63 |\\n| EKI-BART (Fan et al., 2020) | 35.95  | 17.00 | 29.58 |\\n| T5-base (our implementation) | 33.81  | 15.79 | 28.34 |\\n| T5-large (our implementation) | 32.85  | 15.76 | 28.38 |\\n| T5-large (reported)       | 31.96  | 15.13 | 28.86 |\\n| I&V (T5-base)             | 40.16  | 17.44 | 30.57 |\\n| I&V (T5-large)            | 40.57  | 17.71 | 31.29 |\\n\\n**Evaluation Metric**\\n\\nWe evaluate systems against the $K$ reference sentences provided by a dataset, by measuring the similarities between the machine-generated text and the gold references. Following CommonGen (Lin et al., 2020), we adopt widely-used automatic metrics for evaluating text generation, which focus on (1) n-gram overlap: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee & Lavie, 2005), and (2) concept association: CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). Lin et al. (2020) reports that SPICE yields the best correlation with human judgments and thus we used it as the main evaluation metric.\\n\\n**4 RESULTS AND ANALYSIS**\\n\\nWe design experiments to answer the following questions: (1) Does contextualized scene imagination improve the performance of GCSR models? (2) Does imagination allow GCSR models to learn with less data? (3) How does each source of scene knowledge for pretraining affect the GCSR performance? (4) Do generated SKGs make common sense and correspond to the generated text?\\n\\n**4.1 MAIN RESULTS**\\n\\nWe compare our proposed approach with state-of-the-art text generation methods on two GCSR tasks to understand whether scene imagination helps GCSR. Table 2 shows the performance of different models.\"}"}
{"id": "Oh1r2wApbPv", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Performance of the compared methods on the Concept2Story tasks. Best results are bold-faced. We mark them with an asterisk if they exceed the second best with statistical significance (p-value < 0.05).\\n\\n| Model            | BLEU-4 | CIDEr  | SPICE | BLEU-4 | CIDEr  | SPICE |\\n|------------------|--------|--------|-------|--------|--------|-------|\\n| Node2Text        | 20.64  | 25.41  | 58.55 | 18.52  | 22.91  | 55.48 |\\n| Keyword          | 16.75  | 21.87  | 56.23 | 15.62  | 20.86  | 53.80 |\\n| Action-Plan      | 17.84  | 22.77  | 57.11 | 16.20  | 21.10  | 54.77 |\\n| Prototype        | 20.28  | 25.05  | 58.17 | 22.81  | 26.78  | 58.84 |\\n| I&V              | 21.05  | 25.78  | 59.21 | 22.45  | 26.80  | 59.11 |\\n\\nI&V drastically improves the vanilla T5-large model (Node2Text), demonstrating the effectiveness of the imagination module in GCSR. We also provide concrete examples in \u00a7A.4 which showcase how imagination fixes errors made by Node2Text. All these errors can be attributed to the fact that Node2Text does not properly capture the commonsense relations between concepts while our imagination module learns how concepts are related from indirect supervision.\\n\\nOur model outperforms other models using different auxiliary inputs, including prototypes (Re-T5 and EKI-BART), knowledge facts (KG-BART) and images (VisCTG), showing the benefit of SKGs over these knowledge sources. Although our model under-performs KFCNet, our analysis in their work reveals that 97.4% of the test cases have perfectly matched prototypes, i.e., sentences containing all the queried concepts. It is thus unclear whether KFCNet is conducting commonsense reasoning or merely rephrasing the prototypes. Note that we filter out any collected SKGs that cover the concept sets from the downstream datasets. This ensures that the imagination module is examined with its compositional generalization.\\n\\nTable 3 shows the experimental results by I&V on the two Concept2Story datasets using T5-base and BART-large as the backend respectively. Among most evaluation metrics, our method outperforms Node2Text and baselines with other intermediate representations incorporated in the same backends. This demonstrates that our imagination module can provide contextualized scene imagination that are more helpful in guiding long narrative generation.\\n\\n4.2 Performance Analysis\\n\\nHow does the knowledge source affect GCSR? We perform an ablation study in order to understand how effectively each source of SKGs contributes to the imagination. Specifically, we use each of the following SKG sources to pre-train an imagination module using T5-large as the backend: the silver-standard SKGs extracted from the training set from the downstream task (Task-AMR), and the external SKGs: Caption-AMR, Story-AMR, and VG-SceneGraph (\u00a72.2). For CommonGen, we do not further fine-tune the imagination module in order to distinguish the contributions from each knowledge source more clearly. For Concept2Story (ROCstories), we conduct further fine-tuning using the task-AMR. Since this task provides the context as input, we find it helpful to adapt the imagination module with the task dataset.\\n\\nThe results are shown in Table 4 and we have the following observations. For CommonGen, the contribution comes mostly from the SKGs based on Caption-AMR while being less from VG-SceneGraph. This may due to the fact that VG-SceneGraph is biased towards spatial relations and attributes of objects. For Concept2Story, we find both Story-AMR and Caption-AMR to be helpful for continual pretraining. The former teaches the model to generate contextualized imagination which is necessary for story generation in particular while the latter teaches the model about general commonsense knowledge. For both datasets, the imagination modules that are pre-trained over all the SKG instances yield significantly better results than the ones trained on the task-AMR datasets. This validates our intuition that different sources of SKGs contain complementary commonsense knowledge, and they should be used together for machine imagination.\\n\\nHow does the backbone LM size affect the module's performance? We also ablate the LM architecture of the imagination module and the verbalization module respectively to see how our method work with different pre-trained LMs. For the imagination module, we use T5-base and T5-large. This is to investigate how the capacity of LMs affects the learning of scene knowledge.\"}"}
{"id": "Oh1r2wApbPv", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Performance of our method using different SKG sources to train the imagination module, with T5-large as the backbone LM.\\n\\n| Knowledge Source       | BLEU-4 | CIDEr | SPICE | BLEU-4 | CIDEr | SPICE |\\n|------------------------|--------|-------|-------|--------|-------|-------|\\n| Task-AMR               | 28.87  | 15.74 | 31.22 | 23.14  | 29.25 | 57.91 |\\n| Caption-AMR            | 32.21  | 16.14 | 32.16 | 23.77  | 29.76 | 58.46 |\\n| Story-AMR              | 23.73  | 13.51 | 27.53 | 24.17  | 30.10 | 58.59 |\\n| VG-SceneGraph          | 21.00  | 13.36 | 29.07 | 22.84  | 25.33 | 53.96 |\\n| All-SKG                | 33.27  | 16.95 | 33.49 | 26.77  | 32.33 | 60.63 |\\n\\nTable 5: SPICE performance of our method using different sizes of T5 as backbone for the imagination module.\\n\\n| Dataset / Backbone LM | T5-base | T5-large |\\n|-----------------------|---------|----------|\\n| CommonGen (in-house)  | 32.00   | 33.49    |\\n| Concept2Story-ROC     | 59.56   | 60.63    |\\n\\nFigure 4: Ablation study on backbone LM sizes of our verbalization module and Node2Text using the Concept2Story-ROC dataset.\\n\\nFigure 5: Results (SPICE) of the low-resource experiment on the three benchmark datasets with different number of training examples. The results are shown in Table 5. Compared to T5-large, we observe a slight performance drop for T5-base, which indicates that larger LMs are able to encode our rich set of SKG instances in a more expressive manner. For the verbalization module, we use BART-base/large and T5-base/large. The results are shown in Figure 4. We observe that compare to baseline, our method consistently yields a better performance regardless of what LM architecture is used.\\n\\nDoes imagination allow models to learn (faster) with less data? Next, we study how the indirect supervision provided to the imagination module help the system effectively learn with limited task-specific training data. Accordingly, we conduct a low-resource experiment where we randomly sample \\\\{50, 500, 5000\\\\} training and development examples from each dataset. For each data size, we use 5 random seeds to obtain 5 different training and development splits. On each split, we train and test with random initialization of 3 seeds, and we report the average on the total 15 ways of results. In this study, the imagination module is fixed untrainable after continual pretraining and is not fine-tuned over the sampled task datasets.\\n\\nFigure 5 shows that our model consistently outperforms the baselines, and the performance gain is larger when less training data are used. This indicates that rich sources of SKGs provide practical forms of indirect supervision to complement limited task-specific training data. The robustness of our model in low-resource settings also justifies the need for including contextualized SKGs as an intermediate representation, which further enhances the verbalization module to generate plausible sentences even with little training data.\\n\\nIs context helpful for imagination? To validate that the textual context, including the provided context as well as the previously generated sentences, is helpful for imagination in the Concept2Story task, we conduct an ablation study where we learn an uncontextualized imagination module which only takes concepts as input. The final results on VIST and ROC datasets are 47.32 and 45.18 (SPICE) respectively, which are much lower than the results from contextualized I&V (59.21 and 60.63). This demonstrates that the context is critical in generating SKGs which are more relevant to the storyline and thus lead to better text generation.\\n\\n4.3 Human Evaluation on Generated SKGs\\nWe conduct human evaluation on the SKGs generated by our imagination module to examine their quality. Annotators are presented with the input concepts, the generated SKGs, the predicted sentences, and the ground truth sentences. The annotators are asked to rate the coherence, relevance, and grammatical correctness of the generated sentences. The results show that our imagination module generates high-quality SKGs that are coherent and relevant to the input concepts. The_SKGs_are_also_grammatically_correct_and_provide_practical_forms_of_indirect_supervision_to_complement_limited_task-specific_training_data.\"}"}
