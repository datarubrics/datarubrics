{"id": "Yen1lGns2o", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DoRA\u2019s performance across diverse tasks and provide valuable guidance in optimizing its training strategy to suit specific applications.\\n\\nUsing a larger model.\\n\\nViT-B/16 pretrained and then fine-tuned for semantic segmentation on ADE20K and object detection on MS-COCO. We report mIoU on ADE20K and mAP on MS-COCO.\\n\\nWe observe that with ViT-B/16, DoRA achieves 4.9% gain in terms of mIoU on ADE20K and 2.2% in terms of mAP on MS-COCO as compared to using ViT-S/16. This shows that DoRA also scales well with the model size.\\n\\nTable 9:\\n\\n| METHOD            | ARCH   | RETRAIN | ADE20K | MS-COCO |\\n|-------------------|--------|---------|--------|---------|\\n| DoRA (ours)       | ViT-S/16 |         | 35.4   | 39.5    |\\n| DoRA (ours)       | ViT-B/16 |         | 40.3   | 41.7    |\\n\\nTable 10: Pretraining on ImageNet-1k. ViT-S/16 pretrained, then frozen (classification and object discovery, same settings as Table 5) or fine-tuned (semantic segmentation and object detection, same settings as Table 3). DoRA\u22c6: DoRA without tracking; when pretrained for 60 epochs, it has the same training time as DINO and iBOT.\"}"}
{"id": "Yen1lGns2o", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"METHOD\\nPRETRAINED\\nC-10 C-100\\n\\nINAT18 FLWRS\\nCARS\\n\\nDINO IN-1k\\n98.7 89.8 71.5 98.3 92.2 81.3\\n\\nDoRA (ours)\\nVenice\\n98.5 89.4 69.8 94.0 92.5 80.8\\n\\nDoRA (ours)\\nall\\n98.8 89.9 72.2 98.7 93.1 81.4\\n\\nTable 11: Fine-tuning for classification. ViT-S/16 pretrained, then fine-tuned on different image-based datasets. WT Venice: Walking Tours (ours), single video of Venice; WT all: all videos. IN-1k: ImageNet-1k. We report the top-1 accuracy (%).\\n\\nWe also observe such performance gains when fine-tuning on semantic segmentation on ADE20K and object detection on MS-COCO (Table 3).\\n\\nEMORE VISUALIZATIONS\\n\\nFigures 5 and 6 show example attention maps obtained using SK on different clips. These figures show that SK (6) leads to attention maps that exhibit spatial locality and are well aligned with objects in the scene. Remarkably, the masks seem to be even robust to occlusions, as shown in the sequence with a bicycle moving behind traffic lights.\"}"}
{"id": "Yen1lGns2o", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"For each input frame $X_t$ of a video clip, refined cross-attention map $T'_t \\\\in \\\\mathbb{R}^{k \\\\times n}$ (6), using Sinkhorn-Knopp. For each object, one row of $T'_t$ is reshaped as $h/p \\\\times w/p$ and upsampled to an attention map overlaid on the input frame for $k = 3$ objects encoded in blue, red and green channel. $T'_t$ yields three well separated objects shown in blue, red and green.\"}"}
{"id": "Yen1lGns2o", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 6: For each input frame $X_t$ of a video clip, refined cross-attention map $T'_t \\\\in \\\\mathbb{R}^{k \\\\times n}$ (6), using Sinkhorn-Knopp. For each object, one row of $T'_t$ is reshaped as $h/p \\\\times w/p$ and upsampled to an $h \\\\times w$ attention map overlaid on the input frame for $k = 3$ objects encoded in blue, red and green channel. $T'_t$ yields three well separated objects shown in blue, red and green.\"}"}
{"id": "Yen1lGns2o", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary. But are we making the best use of data? How more economical can we be? In this work, we attempt to answer this question by making two contributions. First, we investigate first-person videos and introduce a \u201cWalking Tours\u201d dataset. These videos are high-resolution, hours-long, captured in a single uninterrupted take, depicting a large number of objects and actions with natural scene transitions. They are unlabeled and uncurated, thus realistic for self-supervision and comparable with human learning. Second, we introduce a novel self-supervised image pretraining method tailored for learning from continuous videos. Existing methods typically adapt image-based pretraining approaches to incorporate more frames. Instead, we advocate a \u201ctracking to learn to recognize\u201d approach. Our method called DORA, leads to attention maps that Distinguish and Track objects over time in an end-to-end manner, using transformer cross-attention. We derive multiple views from the tracks and use them in a classical self-supervised distillation loss. Using our novel approach, a single Walking Tours video remarkably becomes a strong competitor to ImageNet for several image and video downstream tasks. Dataset and code can be found at https://shashankvkt.github.io/dora.\\n\\n1 INTRODUCTION\\n\\n(To the question \u201cHave you read all the books in here?\u201d)\\n\\nNo, only four of them. But I read those very, very carefully.\\n\\nJacques Derrida\\n\\nLearning from large scale datasets has been at the core of great progress. In particular, the field of self-supervised learning has allowed pretraining of neural networks to scale beyond the size of labelled datasets. By avoiding costly annotation, strong performance has been demonstrated by increasing the training dataset sizes into billions of images.\\n\\nBut how well are those images really used? At a rate of one image per second, a dataset of 1B images would take 317 years to watch. Yet, humans develop functioning visual systems much faster.\\n\\n* Besides potential genetic visual priors in humans, one stark difference is the type of data. Humans observe their visual surroundings in one continuous stream, only interrupted by sleep. Indeed, learning visual representations of images from videos is not new. However, previous works have found significant gaps in performance to image-pretrained models. They have mostly used object-centric videos scraped from the internet, and adapted image-based pretraining methods to use different frames as an extra form of data augmentation (Gordon et al., 2020; Parthasarathy et al., 2022).\\n\\n* Equal last authors. Order determined randomly\\n\\n* Humans develop face recognition (de Haan et al., 2001) and color sensitivity (Adams, 1987) in three months, depth perception in five months (Campos et al., 1978) and visual acuity in six months (Sokol, 1978).\"}"}
{"id": "Yen1lGns2o", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Examples of frames from the Walking Tours dataset, containing hours-long, continuous egocentric 4K videos from urban scenes in different cities, under CC-BY license. There are a large number of objects and actions in a variety and natural transition of places, e.g., residential area, park, market, waterfront, etc., with natural transition of lighting conditions and object augmentations.\\n\\nIn this work, we investigate two directions. First, in the direction of data, we introduce a new dataset of open-source first-person videos, recorded for the purpose of virtual \u201cwalking tours\u201d, inspired by (Wiles et al., 2022). These videos have several advantages. Not only are the individual frames dense in semantic categories \u2013 much more so than movies, as we analyze \u2013 but these videos also directly represent the viewpoint of a human, contain few or no shot cuts nor special effects and are long (1-3h). Another benefit is their transparency: indeed, one can watch the whole dataset in one setting. The dataset we create contains 10 Walking Tours (WT) videos with CC-BY license.\\n\\nSecond, in the direction of the method, we develop a new self-supervised image-pretraining method that is uniquely suited for learning from natural, non-object-centric videos. Our approach is inspired by observing toddlers first learn to track objects and animals, then to recognize and differentiate them (Bomba & Siqueland, 1983; Quinn et al., 1993; Spelke & Kinzler, 2007). Our method, called DORA, is an end-to-end training approach that \u201ctracks to learn to recognize\u201d: given a video clip, objects in an initial frame are implicitly discovered and tracked across time. The tracked objects are incentivized to be diverse by introducing a Sinkhorn-Knopp clustering of patch embeddings; the tracked instances are used as a learning signal for a classical multi-view SSL loss.\\n\\nSurprisingly, contrary to previous works, we find that our novel method obtains ImageNet-level performances by training on a single WT video, as evidenced by performances on segmentation and object detection downstream tasks. While humorously intentioned, Derrida\u2019s quote rings true to this finding and our results give some hope for alternative directions in SSL that depart from blind dataset scaling towards more efficient and smarter use of existing video data.\\n\\nTo summarize, our key contributions in this work are as follows:\\n\\n1. We introduce a new dataset of 10 WT videos, with single-video and mixed-video splits. The latter is conveniently equal in size to ImageNet. We analyze their usefulness compared to existing video and image datasets.\\n\\n2. We propose a new end-to-end self-supervised visual pretraining method called DORA. It builds upon DINO but is tailored to promote tracking of multiple objects across frames. We use it to learn strong image encoders and trace the source of its improvements through extensive ablations.\\n\\n3. We obtain strong performance on ADE20k segmentation and MS COCO detection, outperforming ImageNet-pretrained DINO, while instead pretraining on a single long video.\\n\\nRelated Work\\n\\nSelf-supervised learning of image encoders from video data is a very active area of research. Video, and more generally temporal streams, have long been theorized to be ideal signals for unsupervised learning (Wiskott & Sejnowski, 2002). In computer vision, early methods have been very diverse and included pretext tasks such as egomotion prediction (Agrawal et al., 2015; Jayaraman & Grauman, 2015), active recognition (Jayaraman & Grauman, 2016), pose estimation (Chakraborty & Namboodiri, 2017), unsupervised object discovery (Croitoru et al., 2017), dense prediction (Pathak et al., 2017; Li et al., 2019), optical flow (Mahendran et al., 2018; Xiong et al., 2021), frame order prediction (Misra et al., 2016), viewpoint matching (Sermanet et al., 2018; Pirk et al., 2020) or learning visual correspondences (Wang et al., 2019).\\n\\nMore recently, there have been considerable advances in self-supervised learning using ImageNet, with the main theme being extracting multiple augmentations of an image (Chen et al., 2020; Caron et al., 2021).\"}"}
{"id": "Yen1lGns2o", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Now, given the key embeddings $K_t \\\\in \\\\mathbb{R}^{(n+1) \\\\times d}$ at another frame $t$, we track the objects by the refined cross-attention (6), similarly with (3):\\n\\n$$T_t' = \\\\text{softmax} \\\\left( \\\\tilde{K}_t^\\\\top / \\\\sqrt{d} \\\\right) \\\\in \\\\mathbb{R}^{k \\\\times n},$$\\n\\nwhere $\\\\tilde{K}_t \\\\in \\\\mathbb{R}^{n \\\\times d}$.\\n\\nIndeed, Figure 4 confirms that each of the $k$ resulting attention maps is associated with a spatially distinct object, thanks to the established correspondences.\\n\\nIn contrast to previous works that use SK in the context of self-supervised learning to force an equi-partitioning of images to cluster labels (Asano et al., 2020; Caron et al., 2020; Oquab et al., 2023), we rather use optimal transport to re-balance spatial correspondences to different objects.\\n\\n**Multi-object masking**\\n\\nWe use the cross-attention (6) to mask the input video clip for the student network, such that each masked clip can be considered as a multi-object crop. This crop plays a similar role with local crops in DINO (Caron et al., 2021), but it has arbitrary shape and tracks an object over video frames. In particular, given an input frame $X \\\\in \\\\mathbb{R}^{h \\\\times w \\\\times c}$ with cross-attention matrix $T_t' \\\\in \\\\mathbb{R}^{k \\\\times n}$ (6) and an object $i \\\\in \\\\{1, \\\\ldots, k\\\\}$, we reshape the $i$-th row of $T_t'$ as $\\\\frac{h}{p} \\\\times \\\\frac{w}{p}$ and upsample to a $h \\\\times w$ attention map to match the spatial resolution of $X$, as shown in Figure 4.\\n\\nWe repeat along the channel dimension to form tensor $T_i \\\\in \\\\mathbb{R}^{h \\\\times w \\\\times c}$ and we mask $X$ as $X_o_i := X \\\\odot T_i$, (7) where $\\\\odot$ is the Hadamard product. Following DINO (Caron et al., 2021), given an input frame $X_t$, we generate two standard resolution augmented global views $X_a_t, X_b_t$. We introduce a multi-object loss $L_O$ for frame $t$, applied to the $[CLS]$ token between the teacher $f_{\\\\theta'}$ output for one global view $X_u_t$ and the student $f_{\\\\theta}$ output for the masked version $X_v,o_i t$ of the other view $X_v t$ for $i \\\\in \\\\{1, \\\\ldots, k\\\\}$, where $u, v \\\\in V = \\\\{a, b\\\\}$ and $u \\\\neq v$:\\n\\n$$L_O : = \\\\sum_{i=1}^{k} f_{\\\\theta'}(X_u t)[CLS] \\\\log f_{\\\\theta}(X_v,o_i t)[CLS].$$\\n\\n(8)\\n\\nIn addition, as detailed in subsection B.1, we apply a local loss, following multi-crop (Caron et al., 2020). The overall loss $L$ is the sum of the two losses, averaged over all $T$ frames.\\n\\n### 5 EXPERIMENTS\\n\\n#### 5.1 SETUP\\n\\n**Tasks and methods**\\n\\nWe perform self-supervised pretraining on a single WT tour video in Venice (referred to as WT Venice) or all 10 WT videos (referred to as WT all) and compare with other image and video datasets. To evaluate the quality of the learned representations, we use frozen features for classification, unsupervised object discovery and video object segmentation. We fine-tune for semantic segmentation, object detection and object tracking. We compare DO RA with SoTA SSL methods (da Costa et al., 2022) using our settings. We provide more details in individual sections per task. Implementation details and hyperparameters are given in Appendix B.\\n\\n#### 5.2 ABALATIONS\\n\\nWe examine the effect of using different pretraining video dataset and different options and parameters for DO RA, measuring performance of classification on ImageNet-1k (Deng et al., 2009) by linear probing (LP) accuracy and unsupervised object discovery on Pascal-VOC 2012 (Everingham et al.) by correct localization (CorLoc) (Sim\u00e9oni et al., 2021).\\n\\n**Pretraining video dataset**\\n\\nWe study the impact of pretraining on diverse video datasets, encompassing object-centric videos such as Kinetics-400 (K-400) (Kay et al., 2017), egocentric videos like Epic-Kitchens (EK) (Damen et al., 2022) and a single movie, Movie roman (Central). To maintain uniformity in terms of the number of frames, we curate a subset of videos from K-400 and EK, such that their total duration is the same as a single WT video. In Table 2a, we observe that although K-400 is object-centric, pretraining on WTours videos yields superior performance on ImageNet and Pascal-VOC 2012. Pretraining on a single movie yields is inferior to both WTours and K-400 by a large margin. This is possibly due to the presence of cuts, as studied in Appendix C.\"}"}
{"id": "Yen1lGns2o", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Effect of parameters. ViT-S/16 pretrained, then frozen. (a) Different pretraining video dataset, (b) Number of tracked objects. (c) Random or multi-object mask, without SK (3) and with SK (6).\\n\\n* Subset of videos with same total duration as a single WTours video. K-400: Kinetics-400, EK: Epic-Kitchens. LP: top-1 accuracy (%) of linear probing on the validation set of ImageNet-1k. CorLoc: correct localization on validation set of Pascal-VOC 2012.\\n\\nTable 3: Semantic segmentation, object detection and instance segmentation. ViT-S/16 pretrained, then fine-tuned. WT Venice: Walking Tours (ours), single video of Venice; WT all: all videos. IN-1k: ImageNet-1k. (a) Semantic segmentation: fine-tuning on ADE20k using UperNet. mIoU: mean IoU; Acc m: mean-class accuracy. (b) Object detection and (c) Instance segmentation: fine-tuning on MS-COCO using Cascade RCNN. mAP: mean average precision; mIoU: mean IoU.\\n\\nNumber of tracked objects\\nWe study the impact of the number $k$ of objects. Objects are discovered using attention heads, where the total number of heads is in ViT-S/16 is $h = 6$. For $k > h$, we modify the MSA block as described in subsection B.2. In Table 2b, we observe that $k = 3$ works best. We hypothesize that this is a compromise between the number of objects that can be tracked and the multi-object loss (8) attempting to match small objects with the global crop.\\n\\nChoice of masking and Sinkhorn-Knopp\\nWe explore the effect of using a multi-object mask (7) vs random block-wise (Zhou et al., 2022a) and the effect of improving object-patch correspondence through SK in refined cross-attention (6) vs (3). In Table 2c, we observe that a multi-object mask leads to a remarkable performance improvement even in the absence of SK. In fact, random block-wise mask undermines object-patch correspondence, making the effect of SK negative. By contrast, SK improves performance in the presence of multi-object mask.\\n\\n5.3 Comparison with State-of-the-Art\\nDense scene understanding\\nTable 3(a) shows semantic segmentation by fine-tuning on ADE20k (Zhou et al., 2017) using UperNet (Xiao et al., 2018). DORA outperforms DINO by 3% mIoU, and 1.8% Acc m. It is interesting to note that DORA pretrained on 200k frames of a single WTours video outperforms DINO pretrained on 1.3M images of ImageNet-1k by 1.5% mIoU. A more comparable setting is DORA pretrained on 1.5M frames of WT all, which outperforms DINO pretrained on ImageNet by 3% mIoU. Table 3(b) shows object detection and instance segmentation by fine-tuning on MS-COCO (Lin et al., 2014) using Cascade RCNN (Cai & Vasconcelos, 2019). DORA outperforms DINO by 2.4% mAP and 2.6% mIoU. DORA pretrained on WT all outperforms DINO pretrained on ImageNet by 0.8% mIoU and 1.2% mAP. This shows that pretraining on WTours videos significantly improves the generality of DORA to dense prediction tasks, requiring only one tenth of the total images.\"}"}
{"id": "Yen1lGns2o", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Video object segmentation and object tracking. ViT-S/16 pretrained, then frozen or fine-tuned. WT Venice: Walking Tours (ours), single video from Venice; WT all: all videos. IN-1k: ImageNet-1k. (a) Video object segmentation: frozen features on DA VIS-2017. Jm: mean region similarity; Fm: mean contour-based accuracy. (b) Multi-object tracking: fine-tuning on GOT-10k. mAO: mean average overlap; SR: success rate, threshold 50% and 75%.\\n\\nTable 5: Image classification and object discovery. ViT-S/16 pretrained, then frozen. WT Venice: Walking Tours (ours), single video from Venice; WT all: all videos. (a) Classification top-1 accuracy (%) on validation set of ImageNet-1k. LP: linear probing. (b) Unsupervised object discovery on validation set of Pascal-VOC 2012. Jacc.: Jaccard similarity; CorLoc: Correct Localization.\\n\\nVideo understanding\\nTable 4(a) shows video object segmentation by using frozen features on DA VIS-2017 (Pont-Tuset et al., 2017), which assesses the ability to segment an object over its dynamic temporal changes. DO RA captures detailed temporal deformations and outperforms baseline DINO by 3.4% Jm and 4.2% Fm. Using only a single video for pretraining, DO RA achieves almost the same performance of DINO pretrained on ImageNet (56.4% vs 57.4% Jm). Table 4(b) shows multi-object tracking by fine-tuning on GOT-10k (Huang et al., 2021) using SeqTrack (Chen et al., 2023). GOT-10k assesses the ability to track extremely fast moving objects, objects with illumination variation and low resolution. DO RA achieves significant gains between 4-6% over DINO.\\n\\nImage classification and unsupervised object discovery\\nWe pretrain DO RA on WTours and then we keep it frozen on the downstream task, indicating the quality of the pretrained features. Table 5(a) shows image classification on ImageNet-1k, measuring accuracy for linear probing and k-nearest neighbor. Table 5(b) shows unsupervised object discovery on Pascal-VOC 2012, using attention maps as segmentation masks to measure Jaccard similarity and CorLoc. On both tasks, non-contrastive methods (DINO, iBOT, VICReg) outperform contrastive methods (SimCLR, SwA V), when pretrained on a single WT video. Importantly, non-contrastive methods are also more efficient to train, since no negative pairs are used. Also on both tasks, DO RA outperforms DINO by a large margin, e.g. 11.6% LP and 3.9% k-NN on classification, when trained on a single WT video. Comparing DO RA on WT Venice with the WT all dataset, the improvement brought by the full dataset is small when using DO RA, although it is 10 times larger.\\n\\nIn Appendix D, we show that DO RA outperforms SoTA methods on all tasks on ImageNet-1k.\\n\\nConclusions\\nWe have introduced a dataset of 10 walking tour videos \u2013 first-person videos taken by people touring a city, with no cuts, high resolution and that are hours long. We show that learning from clips taken from these videos is surprisingly powerful: with an appropriately tailored self-supervised learning method for videos, we obtain representations that rival those obtained on ImageNet when\"}"}
{"id": "Yen1lGns2o", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"transferring to popular downstream image and video tasks. This differs from previous state-of-the-art approaches to learning image encoders from video, which also obtain such results but require large video datasets, following closely the ImageNet blueprint.\\n\\nOur proposed learning method DOGRA is inspired by DINO, generalizing it to video by incorporating implicit multi-object tracking across video clips. We observe that the method leads to interesting emergent attention masks within the transformer model, that seem to latch on to particular objects, even through occlusions. This makes it uniquely suited to our newly introduced dataset.\\n\\nACKNOWLEDGEMENTS\\n\\nThis work was in part supported by the ANR-19-CE23-0028 MEERQAT project and was performed using the HPC resources from GENCI-IDRIS Grant 2021 AD011012528. We also thank Dilara Gokay and Andrew Zisserman for their valuable feedback on this paper.\\n\\nREFERENCES\\n\\nRussell J Adams. An evaluation of color preference in early infancy. *Infant Behavior and Development*, 1987.\\n\\nPulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning to see by moving. In *ICCV*, 2015.\\n\\nYuki M. Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In *ICLR*, 2020.\\n\\nMax Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In *ICCV*, 2021.\\n\\nAdrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. *arXiv preprint arXiv:2105.04906*, 2021.\\n\\nPaul C Bomba and Einar R Siqueland. The nature and structure of infant form categories. *Journal of Experimental Child Psychology*, 1983.\\n\\nZhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: High quality object detection and instance segmentation. *IEEE TPAMI*, 2019.\\n\\nJoseph J Campos, Susan Hiatt, Douglas Ramsay, Charlotte Henderson, and Marilyn Svejda. The emergence of fear on the visual cliff. *The development of affect*, 1978.\\n\\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. *NeurIPS*, 2020.\\n\\nMathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In *ICCV*, 2021.\\n\\nBrandon Castellano. PySceneDetect. https://github.com/Breakthrough/PySceneDetect.\\n\\nWorld Movie Central. The night we met. https://www.youtube.com/watch?v=joIzqAueexA.\\n\\nPrabuddha Chakraborty and Vinay P. Namboodiri. Learning to estimate pose by watching videos. *arXiv preprint arXiv:1704.04081*, 2017.\\n\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In *ICML*, 2020.\\n\\nXin Chen, Houwen Peng, Dong Wang, Huchuan Lu, and Han Hu. Seqtrack: Sequence to sequence learning for visual object tracking. In *CVPR*, 2023.\\n\\nIoana Croitoru, Simion-Vlad Bogolin, and Marius Leordeanu. Unsupervised learning from video to detect foreground objects in single images. In *ICCV*, 2017.\\n\\nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. *NeurIPS*, 2013.\\n\\nVictor Guilherme Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. solo-learn: A library of self-supervised methods for visual representation learning. *JMLR*, 2022.\"}"}
{"id": "Yen1lGns2o", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Very long video datasets. A large dataset of 10k WTours videos was created recently by (Wiles et al., 2022) but was not publicly released and not studied for self-supervised learning. Another dataset having hour-long videos is introduced in (Khan et al., 2020), in the context of sports analytics; it has not been explored for self-supervised learning either.\\n\\nA.2 DATASET ANALYSIS\\n\\nHere we present a more detailed discussion on the dataset analysis of subsection 3.3. We refer to Figure 2, where we analyse the properties of a single WTours video compared with videos of the same length from Epic-Kitchens (Damen et al., 2022) and A V A (Gu et al., 2018) datasets, as well as two movie videos, an action movie and a romantic movie.\\n\\nVariation in lightness\\n\\nWe measure the change in perceived brightness using the lightness value (L) across consecutive frames. From Figure 2(a), we observe a gradual shift at roughly 150 min into the WTours video, transitioning from bright to dim to dark. By contrast, Epic-Kitchens and A V A videos exhibit random brightness fluctuations, alternating between dim and bright conditions. Typically, self-supervised pretraining happens on datasets with uniform brightness levels. Datasets featuring such brightness variations are less explored.\\n\\nVariation in number of objects\\n\\nUsing Detic (Zhou et al., 2022b), a DETR-style object detector trained on ImageNet-21k, we detect objects in each frame. Figure 2(b) shows the number of unique objects per frame and Figure 2(c) shows their frequency in the entire video. We observe that WTours contains 703 unique objects, while Epic-Kitchens has 373, A V A has 663 and Movie-2 has 259. The unique objects appear more frequently and there are more unique objects per frame in WTours than in the other datasets. This makes WTours semantically richer, despite coming from one continuous stream of video. Using videos with a large number of objects can encourage the model to capture complex relations and variations in the data.\\n\\nVariation in shots\\n\\nEgocentric videos are typically captured in a single uninterrupted take, with exceptions being post-processed special effects or cuts. In Figure 2(d), we find that, on average, WTours and Epic-Kitchens videos contain only one or two shots per entire video, while A V A contains 406, an action movie (Movie act) (Skiptrace) contains 2000 and a romantic movie (Movie rom) (Central) contains 667. The substantial number of shots in movies and A V A poses challenges for representation learning methods that rely on object tracking or optical flow. In subsection 5.2, we show that WTours significantly outperforms movies in downstream tasks, which may be attributed to the absence of cuts.\\n\\nB.1 MORE ON EXPERIMENTAL SETUP\\n\\nFollowing DINO (Caron et al., 2021) and iBOT (Zhou et al., 2022a), we apply the multi-crop strategy (Caron et al., 2020). In particular, we generate m local crops $X_{\\\\ell i}$ of smaller resolution for $i \\\\in \\\\{1, \\\\ldots, m\\\\}$. The local loss $L_{LC t}$ for frame $t$ is applied to the [CLS] token between the teacher $f_{\\\\theta'}$ output for a global view $X_u t$ and the student $f_{\\\\theta}$ output for the local crop $X_{\\\\ell i}$ for $i \\\\in \\\\{1, \\\\ldots, m\\\\}$:\\n\\n$$L_{LC t} := \\\\sum_{v \\\\in V} \\\\sum_{i=1}^{m} \\\\log \\\\frac{f_{\\\\theta'}(X_{v t})[CLS]}{f_{\\\\theta}(X_{\\\\ell i t})[CLS]}$$\\n\\n(9)\\n\\nThe overall loss $L$ is the sum of the multi-object loss $L_{Ot}$ (8) and the local loss $L_{LC t}$ (9), averaged over all $T$ frames:\\n\\n$$L := \\\\frac{1}{T} \\\\sum_{t=1}^{T} (L_{Ot} + L_{LC t}).$$\\n\\n(10)\"}"}
{"id": "Yen1lGns2o", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.2 IMPLEMENTATION DETAILS\\n\\nWe use ViT-S/16 (Dosovitskiy et al., 2020) as the backbone in all our experiments. For each mini-batch, we randomly sample clips from the video, consisting of $T = 8$ frames temporally separated by 1 second i.e., we sample one frame every 30. Objects discovered in the first frame are tracked over the following 7 frames. Since each frame contains several different objects, applying the standard multi-crop augmentation (Caron et al., 2020) to the entire frame would result in crops with very different visual content or noisy positive pairs. Instead, we apply multi-crop to a $300 \\\\times 300$ crop that we first take from the frame. Following DINO (Caron et al., 2021), we obtain two global crops and six local crops. Masking (7) is applied to the global crops seen by the student for the multi-object loss (8), while local crops are seen directly by the student for the local loss (9). We train for 100 epochs by default.\\n\\nObjects are discovered using attention heads, where the total number of heads is in ViT-S/16 is limited to $h = 6$. For the purpose of the ablation of the number $k$ of objects for $k > h$ in Table 2b, we modify the MSA block in the final layer, resulting in configurations of 16 and 32 heads. Consequently, we can identify and track up to 16 and 32 objects within the video clip. To accomplish this, we decompose the query and key embeddings of dimension $d = 768$ into 16 and 32 subvectors, resulting in new feature dimensions of 24 and 12 respectively, as opposed to 64 for 6 heads. In Table 2b, we observe that tracking 16 or 32 objects results in overall poor performance possibly due to the small feature dimension, which encodes poor representations.\\n\\nB.3 HYPERPARAMETERS\\n\\nImageNet-1k: Linear probing and $k$-NN\\n\\nWe pretrain DORA in a self-supervised setting with ViT-S/16 using DINO for 100 and 300 epochs. We use two global and six local crops for each clip and train on 8 A100 GPUs with a global batch size of $16 \\\\times 8 = 128$. We use LARS with a learning rate of $5 \\\\times 10^{-4}$, minimum learning rate of $1 \\\\times 10^{-6}$, global crop scale of $[0.4, 1.0]$ and local crop scale $[0.05, 0.4]$.\\n\\nFor linear probing, we follow (Caron et al., 2021) and use the frozen features of the transformer backbone to train a linear classifier in a supervised setting. We use global batch size of 1024 on the training set and evaluate on the validation set of ImageNet-1k. We use top-1 accuracy (%) as our evaluation metric. For $k$-NN, we freeze the backbone and extract features of training images, then use a $k$-nearest neighbour classifier with $k = 20$.\\n\\nPascal-VOC 2012: Object discovery\\n\\nWe use the validation set of Pascal VOC 2012 (Everingham et al.), which comprises a total of 1449 images. Following LOST (Sim\u00e9oni et al., 2021), we use the averaged self-attention map, extracted from the final layer of our pretrained ViT-S/16, to retain 80% of the mass. We use the Jaccard similarity $J$ measured as overlap between predicted mask $P$ and the ground truth mask $G$ as $J(P, G) = \\\\frac{P \\\\cap G}{P \\\\cup G}$. We also use CorLoc, which measures the number of correct predicted boxes, where a predicted box is said to be correct if its IoU $\\\\geq 0.5$.\\n\\nADE20k: Semantic segmentation\\n\\nWe evaluate DORA on ADE20k (Zhou et al., 2017) for semantic segmentation. The dataset includes 20,000 images in the training set and 2,000 images in the validation set. We use UperNet (Xiao et al., 2018) as the segmentation model and use DORA pretrained on WT to initialize the backbone. Following the experimental settings in iBOT (Zhou et al., 2022a), we use AdamW (Loshchilov & Hutter, 2019) with an initial learning rate of $6 \\\\times 10^{-5}$, weight decay of $1 \\\\times 10^{-2}$, and linear warmup of 1,500 iterations. We fine-tune for 160,000 iterations with a batch size of 4.\\n\\nMS-COCO: Object detection\\n\\nWe evaluate DORA for object detection and instance segmentation on MS-COCO. We use Cascade Mask R-CNN (Cai & Vasconcelos, 2019), which produces bounding boxes and instance masks simultaneously on the COCO dataset. We use a multi-scale training strategy, where we resize images to have a shorter side ranging between 480 and 800, ensuring that the longer side does not exceed 1,333 pixels. The learning rate is $1 \\\\times 10^{-4}$ and the weight decay is 0.05. During training, we fine-tune the entire network using a $1 \\\\times$ schedule, which involves 12 epochs with learning rate reductions by a factor of 10 at epochs 9 and 11. We explore different layer decay rates, specifically 0.65, 0.75, 0.8, 0.9, with a rate of 1.0 indicating no decay.\"}"}
{"id": "Yen1lGns2o", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| City       | Accuracy (LP) | CorLoc (LP) |\\n|------------|---------------|-------------|\\n| Amsterdam  | 45.4          | 54.5        |\\n| Bangkok    | 42.1          | 54.3        |\\n| Chiang Mai | 44.9          | 55.5        |\\n| Istanbul   | 44.5          | 54.6        |\\n| Kuala Lampur | 43.9        | 54.1        |\\n| Singapore  | 42.7          | 54.7        |\\n| Stockholm  | 44.1          | 54.7        |\\n| Venice     | 44.5          | 56.2        |\\n| Zurich     | 44.9          | 54.4        |\\n\\n**Mean**: 44.1 (LP), 54.8 (CorLoc)  \\n\\n(a) WT videos  \\n\\n**METHOD**  \\n\\n|          | Accuracy (LP) | CorLoc (LP) |\\n|----------|---------------|-------------|\\n| DINO WT  | 33.8          | 51.2        |\\n| DO RA Movie | 35.3        | 51.6        |\\n| **DO RA Movie\u2020** | **39.8** | **54.8** |\\n| **DO RA** | **44.5**      | **56.2**    |\\n\\n**(b) Cuts**  \\n\\nTable 6: Effect of pretraining video and cuts. ViT-S/16 pretrained, then frozen. (a) Different WTours video, using DO RA. (b) Effect of cuts.  \\n\\n*: subset of videos with same total duration as a single WTours video.  \\n\u2020: sampling without cuts. LP: top-1 accuracy (%) of linear probing on the validation set of ImageNet-1k. CorLoc: correct localization on validation set of Pascal-VOC 2012.  \\n\\nTo generate hierarchical feature maps, we utilize the features produced by layers 4, 6, 8, and 12 of our network and apply two deconvolutions for layer 4, one deconvolution for layer 6, identity mapping for layer 8, and max-pooling for layer 12. These post-processing steps enable the creation of hierarchical feature representations. It is important to note that we do not employ multi-scale testing in our experiments.  \\n\\n**DA VIS-2017: Video object segmentation**  \\n\\nWe assess the performance of DO RA for video object segmentation on DA VIS 2017 dataset (Pont-Tuset et al., 2017), which involves segmenting between 2 to 4 objects within the video frames. We follow DINO (Caron et al., 2021) and evaluate on video frames with a resolution of 480p. We apply label propagation on the attention map from our pretrained model and use mean region-based similarity $J_m$ and mean contour-based accuracy $F_m$ as our evaluation metrics.  \\n\\n**GOT-10k: Object tracking**  \\n\\nWe evaluate the object-tracking performance of DO RA on the GOT-10k dataset (Huang et al., 2019). This is a large-scale benchmark for object tracking that contains 563 categories of common moving objects. The training set contains around 10,000 videos and the test set contains 180 videos. Another challenging aspect of this dataset is that the object classes in the training and test set are non-overlapping. We use the SeqTrack (Chen et al., 2023) codebase to evaluate the performance of different methods on this dataset. In particular, we initialize the encoder weights of SeqTrack with the self-supervised weights and keep them frozen during training. While training, we only update the parameters of the lightweight decoder which consists of 2 transformer blocks. We use all the default hyperparameters. We report mean average overlap (mAO) and success rate (SR) at different thresholds. The mAO measures the class-balanced average overlap between the ground truth and predicted bounding boxes whereas SR indicates the percentage of accurately tracked ground truth bounding boxes where the overlap crosses a certain threshold.  \\n\\n**CORE ABLATIONS**  \\n\\n**Blurring faces in videos**  \\n\\nWe address the privacy concerns that may arise from potential lack of consent during recording by the creator. Given the significant presence of individuals in WTours videos, we employ Deface\u2021 to automatically detect and blur faces in WT videos. Specifically, we blur all faces in WT-Amsterdam and observe that DoRA achieves a top-1 accuracy of 45.5% on linear probing, compared to 45.4% without any blur. This demonstrates that face blurring does not impact performance and can effectively mitigate potential privacy and safety issues.  \\n\\n**Pretraining WT video**  \\n\\nWe study the effect of pretraining on different videos of WTours. In Table 6a, we observe that the effect is minimal on both image classification and unsupervised object discovery. Notably, the fluctuation in illumination conditions within the Bangkok video influences the performance on image classification. It is also interesting to note that, while pretraining on Amsterdam is best on image classification, pretraining on Venice is best on object discovery. This\\n\\n\u2021 [https://github.com/ORB-HD/deface](https://github.com/ORB-HD/deface)\"}"}
{"id": "Yen1lGns2o", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Effect of longer pretraining ViT-S/16 pretrained, then frozen (object discovery and VOS, same settings as Table 5, Table 5) or fine-tuned (semantic segmentation and object detection, same settings as Table 3). IN-1K: ImageNet-1K.\\n\\n| Method     | Frames | Classification | Object Discovery | Semantic Segmentation | Object Detection | VOS |\\n|------------|--------|----------------|------------------|-----------------------|-----------------|-----|\\n| DINO (Caron et al., 2021) IN-1K | 100    | 44.5           | 59.6             | 33.9                  | 44.3            | 39.9 | 57.4 | 61.4 |\\n|            | 300    | 45.9           | 64.0             | 38.2                  | 49.4            | 42.4 | 60.2 | 63.4 |\\n\\nTable 8: Effect of # frames in a video clip. ViT-S/16 pretrained on WT Venice, then frozen features used for classification on ImageNet-1K and object discovery on PASCAL-VOC; features fine-tuned on ADE20K for semantic segmentation.\\n\\nPresence of cuts\\nWe now analyse the effect of cuts in representation learning. Cuts are defined as instant transitions from one shot to the next, which is frequent in movies. In action movies, a single shot lasts around 4 seconds, while in romance movies, around 12 seconds on average. To understand the effect of cuts, we compare pretraining on WTours videos and a romance movie. We use PySceneDetect (Castellano) to extract the cut timestamps in the movie and we pretrain DoRA by sampling clips that do not intersect cuts; cuts naturally do not exist in WT videos. In Table 6b, we observe that the performance improves significantly in the absence of cuts, as tracking in DoRA will fail across a cut.\\n\\nLonger pretraining\\nWe investigate the impact of pretraining DoRA on a single WT video and all WT videos over 300 epochs. The results presented in Table 7 indicate a notable enhancement in performance when DoRA is pretrained for 300 epochs in general. Furthermore, we observe that DoRA pretrained on WTall surpasses WTVenice in VOS pretraining performance, in contrast to the situation where DoRA was pretrained for 100 epochs. It is worth highlighting that the extended pretraining duration also enables DoRA to achieve comparable results to DINO pretrained on ImageNet-1K, while demonstrating superior performance in dense scene understanding tasks.\\n\\nNumber of frames in a video clip\\nWe study the effectiveness of DoRA in classification, object discovery and semantic segmentation tasks using a lower number of frames in a video clip, specifically when $T \\\\in \\\\{2, 4, 6\\\\}$. Using the frame-wise loss (8), our goal is to improve the training efficiency of DoRA by achieving comparable performance through pretraining with fewer frames than the default $T = 8$ frames.\\n\\nWe observe from Table 11 that the performance of DoRA in classification and object discovery improves as the number of frames in a video clip increases. Employing $T = 2$ enhances training throughput by 60%, albeit with a 2.3% decrease in linear probing accuracy. However, fine-tuning DoRA for semantic segmentation on ADE20K shows no additional performance gain when varying the number of frames in the video clip. These insights contribute to a detailed understanding of...\"}"}
{"id": "Yen1lGns2o", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Yen1lGns2o", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuffle and learn: unsupervised learning using temporal\\norder verification. In ECCV, 2016.\\n\\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes.\\nIn ICVGIP, 2008.\\n\\nMaxime Oquab, Timoth\u00e9e Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fer-\\nnandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu\\nSharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve,\\nIshan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2:\\nLearning robust visual features without supervision. arXiv:2304.07193, 2023.\\n\\nEmin Orhan, Vaibhav Gupta, and Brenden M Lake. Self-supervised learning through the eyes of a child.\\nNeurIPS, 2020.\\n\\nNikhil Parthasarathy, SM Eslami, Joao Carreira, and Olivier J Henaff. Self-supervised video pretraining yields\\nstrong image representations. arXiv preprint arXiv:2210.06433, 2022.\\n\\nDeepak Pathak, Ross Girshick, Piotr Doll\u00e1r, Trevor Darrell, and Bharath Hariharan. Learning features by\\nwatching objects move. In CVPR, 2017.\\n\\nS\u00f6ren Pirk, Mohi Khansari, Yunfei Bai, Corey Lynch, and Pierre Sermanet. Online learning of object represen-\\ntations by appearance space feature alignment. In ICRA, 2020.\\n\\nJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\u00e1ez, Alex Sorkine-Hornung, and Luc Van Gool.\\nThe 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017.\\n\\nPaul C Quinn, Peter D Eimas, and Stacey L Rosenkrantz. Evidence for representations of perceptually similar\\nnatural categories by 3-month-old and 4-month-old infants. Perception, 1993.\\n\\nFrancesco Ragusa, Antonino Furnari, and Giovanni Maria Farinella. Meccano: A multimodal egocentric dataset\\nfor humans behavior understanding in the industrial-like domain. CVIU, 2023.\\n\\nMohammadreza Salehi, Efstratios Gavves, Cees G. M. Snoek, and Yuki M. Asano. Time does tell: Self-\\nsupervised time-tuning of dense image representations. ICCV, 2023.\\n\\nF. Sener, D. Chatterjee, D. Shelepov, K. He, D. Singhania, R. Wang, and A. Yao. Assembly101: A large-scale\\nmulti-view video dataset for understanding procedural activities. CVPR, 2022.\\n\\nPierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and\\nGoogle Brain. Time-contrastive networks: Self-supervised learning from video. In ICRA, 2018.\\n\\nOriane Simeoni, Gilles Puy, Huy V Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Perez, Renaud\\nMarlet, and Jean Ponce. Localizing objects with self-supervised transformers and no labels. In BMVC.\\n\\n7, 3\\n\\nSkiptrace. https://www.youtube.com/watch?v=LbRNBQaO5a0.\\n\\nSamuel Sokol. Measurement of infant visual acuity from pattern reversal evoked potentials. Vision research,\\n18(1):33\u201339, 1978.\\n\\nElizabeth S Spelke and Katherine D Kinzler. Core knowledge. Developmental science, 2007.\\n\\nMichael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain Gelly, and\\nMario Lucic. Self-supervised learning of video-induced visual invariances. In CVPR, pp. 13806\u201313815,\\n2020.\\n\\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\\nPerona, and Serge Belongie. The inaturalist species classification and detection dataset. In CVPR, 2018.\\n\\nXiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. In ICCV,\\n2015.\\n\\nXiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-consistency of time.\\nIn CVPR, 2019.\\n\\nOlivia Wiles, Joao Carreira, Iain Barr, Andrew Zisserman, and Mateusz Malinowski. Compressed vision for\\nefficient video understanding. In ACCV, 2022.\"}"}
{"id": "Yen1lGns2o", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Yen1lGns2o", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Table 1, we compare WTours with different types of existing video datasets. Self-supervised pretraining on videos has been mostly limited to video datasets that rely on weak annotation in the form of video-text pairs (Bain et al., 2021; Miech et al., 2019) or even are curated, e.g., their class balance is controlled, even if their annotation is unused (Kay et al., 2017). Their average clip duration is small, e.g., less than 20 sec, and their resolution is also small, limiting the capacity to detect objects at a greater distance. By contrast, WTours videos are continuous, hours-long at high resolution and provide natural transitions of scenes and viewing conditions. They are not curated and thus better suited for the self-supervised setting.\\n\\nImageNet-aligned datasets such as R2V2 (Gordon et al., 2020) and VideoNet (Parthasarathy et al., 2022) contain videos that are curated and annotated with the same distribution and classes as ImageNet, meant for pretraining image encoders. These videos are short, i.e., 10 seconds on average. By contrast, WTours consists of a continuous stream of egocentric video, where the average number of classes is close to that of ImageNet, as shown in subsection 3.3. The rich information contained in 4K resolution, together with a high number of objects in a frame, makes it appropriate for representation learning. Importantly, the continuity and absence of curation make it more realistic and more comparable with human learning. Our dataset does not rely on a set of objects, human activities or other search terms but instead is data-first and more open-ended.\\n\\nDespite the large number of high-quality videos, egocentric video datasets (Damen et al., 2022; Grauman et al., 2022; Sener et al., 2022) have been used only for downstream tasks and thus come with extensive annotation. In comparison, WTours has 4-10 times longer average duration and twice the frame resolution. While WTours is smaller in terms of total duration and number of videos, it is scalable under the self-supervised setting since it requires no human labeling effort and more videos.\"}"}
{"id": "Yen1lGns2o", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Walking Tours vs. existing video datasets.\\n\\n| Dataset                  | EGO: egocentric | PRE: used for pretraining | BAL: class balance control | ANNOT: annotation type |\\n|--------------------------|-----------------|---------------------------|---------------------------|------------------------|\\n| Kinetics-400 (Kay et al., 2017) Actions |\u2717 | \u2713 | \u2713 | Class |\\n| WebVid-2M (Bain et al., 2021) |\u2717 | \u2713 | \u2717 | Weak |\\n| HowTo100M (Miech et al., 2019) Instructions |\u2717 | \u2713 | \u2717 | Weak |\\n| Epic-Kitchens (Damen et al., 2022) Cooking |\u2713 | \u2717 | \u2717 | Loc. |\\n| Ego-4D (Grauman et al., 2022) Daily |\u2713 | \u2717 | \u2717 | Loc. |\\n| Meccano (Ragusa et al., 2023) Industry |\u2713 | \u2717 | \u2717 | Loc. |\\n| Assembly-101 (Sener et al., 2022) Assembly |\u2713 | \u2717 | \u2717 | Loc. |\\n| ImageNet-aligned R2V2 (Gordon et al., 2020) |\u2717 | \u2713 | \u2713 | Class |\\n| VideoNet (Parthasarathy et al., 2022) |\u2717 | \u2713 | \u2713 | Class |\\n| Walking Tours (ours) |\u2713 | \u2713 | \u2717 | None |\\n\\nImageNet-aligned VideoNet (Gordon et al., 2020) and training models to pull them together/apart. These methods have since percolated to learning from video frames (Gordon et al., 2020; Parthasarathy et al., 2022; Tschannen et al., 2020; Wang & Gupta, 2015; Orhan et al., 2020). Similar to this work, TimeTuning (Salehi et al., 2023) leverages the passage of time in videos by not treating it as simple augmentations. However, in contrast to our work, it requires an already image-pretrained backbone. VITO (Parthasarathy et al., 2022) improves performance relative to ImageNet, by using VideoNet, a large YouTube dataset of 10s videos from a similar class distribution and the same number of examples as ImageNet. In this paper, we show that it is possible to obtain strong results from a single long video, with a very different visual distribution compared to ImageNet / VideoNet.\\n\\n3.1 DATASET COLLECTION AND PROPERTIES\\n\\nWe collect from YouTube a new dataset of urban scenes called \u201cWalking Tours\u201d (WTours, or WT) comprising 10 egocentric videos of a person walking in different cities in Europe and Asia. The cities include Amsterdam, Bangkok, Chiang Mai, Istanbul, Kuala Lampur, Singapore, Stockholm, Venice, and Zurich. We also include a video from a Wildlife safari. Examples are shown in Figure 1.\\n\\nThese videos are captured in 4K resolution (3840 \u00d7 2160 pixels) at 60 frames-per-second and are under Creative Commons License (CC-BY). The minimum video duration is 59 minutes (Wildlife safari), the maximum is 2 hours 55 minutes (Bangkok) and the average is 1 hour 38 minutes. Such videos are particularly interesting for visual learning because of the following properties:\\n\\n1. Large number of objects and actions. Each frame or clip taken from a video depicts several objects and actions, e.g., walking, riding a bike, sitting, drinking etc.\\n2. Natural transition in lighting conditions. In some videos, the lighting gradually transitions from bright (late afternoon) to dim (dusk) then to dark (post sunset).\\n3. Natural transition in scenes. The videos depict transitions between places, e.g., from city center to market place to residential areas to parks to water fronts etc.\\n4. Natural object augmentations. Continuous variation e.g., of pose, deformation, viewpoint, perspective distortion, relative object position, occlusion, background clutter.\\n\\nThe abundance of information within these videos, encompassing a multitude of objects and complex scenes, presents a formidable challenge for manual annotation or curation, making it appropriate for unsupervised pretraining. To the best of our knowledge, we are the first to propose an egocentric video dataset for pretraining and evaluate it on a wealth of downstream tasks.\"}"}
{"id": "Yen1lGns2o", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2024\\n\\nWTours\\n\\nEPIC-Kitchen\\n\\nA V A\\n\\nMovie\\n\\n(a) Lightness vs. time. (b) Number of objects per frame vs. time. (c) Frequency of classes in entire video. (d) Number of shots. Objects detected by Detic (Zhou et al., 2022b), trained on ImageNet-21k.\\n\\n3.2 COMPARISON WITH OTHER VIDEO DATASETS\\n\\nIn Table 1, we compare WTours with existing video datasets. In summary, self-supervised pre-training on videos has been mostly limited to short, low-resolution datasets that rely on weak annotation in the form of video-text pairs (Bain et al., 2021; Miech et al., 2019) or are curated, e.g., their class balance is controlled, even if their annotation is unused (Kay et al., 2017). ImageNet-aligned datasets (Gordon et al., 2020; Parthasarathy et al., 2022) contain short videos that are (semi-)automatically curated and annotated with the same distribution and classes as ImageNet. Egocentric video datasets (Damen et al., 2022; Grauman et al., 2022; Sener et al., 2022) have long, high-quality videos, but are the result of significant manual work. In this paper we aim to learn from videos publicly available online.\\n\\nWTours videos are continuous, longer and higher-resolution than even other egocentric datasets. Using object detectors, we find that the average number of object classes is close to that of ImageNet and there is a high number of objects per frame, making WTours appropriate for representation learning. WTours is not curated and does not rely on search terms. It is data-first and more open-ended, thus well suited for the self-supervised setting. It is scalable since it requires no human labeling effort and more videos can be easily downloaded or even made. We are inspired by a 10k walking tours videos created by Wiles et al. (2022), which however is not publicly released and not studied for self-supervised learning. A more detailed discussion is given in subsection A.1.\\n\\n3.3 DATASET ANALYSIS\\n\\nIn Figure 2, we analyse the properties of a single WTours video compared with videos of the same length from two other datasets, as well as two movie videos. In summary, our findings are as follows. From Figure 2(a), WT may exhibit gradual shifts in lightness, transitioning from bright to dim to dark, while Epic-Kitchens and A V A videos exhibit random brightness fluctuations. Lightness variations are not well explored in self-supervised pretraining. From Figure 2(b,c), unique classes appear more frequently and there are more unique objects per frame in WTours than in the other datasets. This makes WTours semantically richer. From Figure 2(d), WTours and Epic-Kitchens videos contain only one or two shots per entire video on average, while the other datasets contain hundreds. In subsection 5.2 and in Appendix C, we show that WTours significantly outperforms movies in downstream tasks, which is partially attributed to the absence of cuts. More detailed discussion of dataset analysis is given in subsection A.2.\\n\\n4 ATTENTION-BASED MULTI-OBJECT TRACKING\\n\\nOur goal is to build robust representations by leveraging the rich information in video frames. Standard SSL frameworks (Chen et al., 2020; Caron et al., 2020) often assume correspondences between different views. This is true whether using dense (Zhou et al., 2022a) or global representations by pooling (Caron et al., 2021). While it is relatively straightforward to establish correspondences in images, it becomes more challenging when dealing with temporal deformations, requiring some form of object tracking (Salehi et al., 2023). In videos with a large field of view or ego-motion, obtaining correspondences becomes even more difficult.\"}"}
{"id": "Yen1lGns2o", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Following DINO (Caron et al., 2021), there is a student network with parameters \\\\( \\\\theta \\\\) of exponential moving average \\\\( \\\\theta' \\\\). We denote by \\\\( f \\\\) an MLP and a scaled softmax, such that the output token embeddings can be interpreted as probabilities.\\n\\nWe introduce \\\\( D \\\\) as a multi-head attention transformer encoder to a \\\\( d \\\\) embeddings of dimension \\\\( d \\\\). Given an embedding \\\\( X_t \\\\), we draw at random a subset \\\\( I_t \\\\), where mapping \\\\( g \\\\) to \\\\( \\\\mathrm{CLS} \\\\) token embedding and patch embeddings, where \\\\( \\\\theta \\\\) includes the tokenizer and encoder, while \\\\( \\\\theta' \\\\) contains the tokenizer and encoder and head.\\n\\nFor simplicity, we drop the \\\\( t \\\\) from the notation. According to multi-head self-attention, the \\\\( Q, K, V \\\\) embeddings are partitioned as \\\\( \\\\theta \\\\) and \\\\( \\\\theta' \\\\) at the second-last layer of the teacher model is used by a multi-object tracker, we obtain the query \\\\( \\\\tilde{Q} \\\\). We refine them into \\\\( \\\\tilde{Q} \\\\). We refine them into different heads attend to different objects.\\n\\nHigh-level idea: We are given a video clip consisting of \\\\( n \\\\) frames. Starting at a first frame, we discover distinct objects \\\\( I_t \\\\) for \\\\( t = 1, \\\\ldots, n \\\\) in the frame. \\\\( \\\\tilde{Q} \\\\) and key embeddings \\\\( K \\\\) of \\\\( \\\\mathrm{CLS} \\\\) and patch embeddings, where \\\\( h \\\\) is the spatial resolution and \\\\( h + 1 \\\\) non-overlapping patches of resolution \\\\( w \\\\) into \\\\( n \\\\times h \\\\times w \\\\) vector between the \\\\( n \\\\) heads and form object prototypes \\\\( P \\\\). We introduce \\\\( D \\\\) is the \\\\( [\\\\mathrm{CLS}] \\\\) token embedding and \\\\( Z \\\\) parameters. Given an embedding \\\\( R \\\\), we draw at random a subset \\\\( I \\\\), where mapping \\\\( g \\\\) to \\\\( \\\\mathrm{CLS} \\\\) token embedding is prepended. This representation is input frame \\\\( X_t \\\\) with frame \\\\( X_t \\\\) that includes an \\\\( \\\\alpha \\\\theta \\\\) of \\\\( \\\\alpha = 0 \\\\) for \\\\( \\\\alpha \\\\) multi-head attention, these embeddings are partitioned as \\\\( \\\\theta \\\\) and \\\\( \\\\theta' \\\\) at the second-last layer of the teacher model is used by a multi-object tracker. As shown in Figure 3, it leverages the attention from the \\\\( \\\\mathrm{CLS} \\\\) token of distinct heads in a vision transformer to generate cross-attention maps \\\\( Z \\\\). We use those to mask \\\\( \\\\tilde{A} \\\\) to establish correspondences \\\\( M \\\\) between \\\\( \\\\tilde{A} \\\\) and \\\\( \\\\tilde{Q} \\\\). We introduce \\\\( D \\\\) is the \\\\( [\\\\mathrm{CLS}] \\\\) token of distinct heads in a vision transformer to a \\\\( d \\\\) embeddings of dimension \\\\( d \\\\). Given an attention matrix \\\\( A \\\\), we draw at random a subset \\\\( I \\\\), where mapping \\\\( g \\\\) to \\\\( \\\\mathrm{CLS} \\\\) token embedding is prepended. This representation is input frame \\\\( X_t \\\\) with frame \\\\( X_t \\\\) that includes an \\\\( \\\\alpha \\\\theta \\\\) of \\\\( \\\\alpha = 0 \\\\) for \\\\( \\\\alpha \\\\) multi-head attention, these embeddings are partitioned as \\\\( \\\\theta \\\\) and \\\\( \\\\theta' \\\\) at the second-last layer of the teacher model is used by a multi-object tracker. As shown in Figure 3, it leverages the attention from the \\\\( \\\\mathrm{CLS} \\\\) token of distinct heads in a vision transformer to generate cross-attention maps \\\\( Z \\\\). We use those to mask \\\\( \\\\tilde{A} \\\\) to establish correspondences \\\\( M \\\\) between \\\\( \\\\tilde{A} \\\\) and \\\\( \\\\tilde{Q} \\\\). We introduce \\\\( D \\\\) is the \\\\( [\\\\mathrm{CLS}] \\\\) token of distinct heads in a vision transformer to a \\\\( d \\\\) embeddings of dimension \\\\( d \\\\). Given an attention matrix \\\\( A \\\\), we draw at random a subset \\\\( I \\\\), where mapping \\\\( g \\\\) to \\\\( \\\\mathrm{CLS} \\\\) token embedding is prepended. This representation is input frame \\\\( X_t \\\\) with frame \\\\( X_t \\\\) that includes an \\\\( \\\\alpha \\\\theta \\\\) of \\\\( \\\\alpha = 0 \\\\) for \\\\( \\\\alpha \\\\) multi-head attention, these embeddings are partitioned as \\\\( \\\\theta \\\\) and \\\\( \\\\theta' \\\\) at the second-last layer of the teacher model is used by a multi-object tracker. As shown in Figure 3, it leverages the attention from the \\\\( \\\\mathrm{CLS} \\\\) token of distinct heads in a vision transformer to generate cross-attention maps \\\\( Z \\\\). We use those to mask \\\\( \\\\tilde{A} \\\\) to establish correspondences \\\\( M \\\\) between \\\\( \\\\tilde{A} \\\\) and \\\\( \\\\tilde{Q} \\\\). We introduce \\\\( D \\\\) is the \\\\( [\\\\mathrm{CLS}] \\\\) token of distinct heads in a vision transformer to a \\\\( d \\\\) embeddings of dimension \\\\( d \\\\). Given an attention matrix \\\\( A \\\\), we draw at random a subset \\\\( I \\\\), where mapping \\\\( g \\\\) to \\\\( \\\\mathrm{CLS} \\\\) token embedding is prepended. This representation is input frame \\\\( X_t \\\\) with frame \\\\( X_t \\\\) that includes an \\\\( \\\\alpha \\\\theta \\\\) of \\\\( \\\\alpha = 0 \\\\) for \\\\( \\\\alpha \\\\) multi-head attention, these embeddings are partitioned as \\\\( \\\\theta \\\\) and \\\\( \\\\theta' \\\\) at the second-last layer of the teacher model is used by a multi-object tracker. As shown in Figure 3, it leverages the attention from the \\\\( \\\\mathrm{CLS} \\\\) token of distinct heads in a vision transformer to generate cross-attention maps \\\\( Z \\\\). We use those to mask \\\\( \\\\tilde{A} \\\\) to establish correspondences \\\\( M \\\\) between \\\\( \\\\tilde{A} \\\\) and \\\\( \\\\tilde{Q} \\\\). We introduce \\\\( D \\\\) is the \\\\( [\\\\mathrm{CLS}] \\\\) token of distinct heads in a vision transformer to a \\\\( d \\\\) embeddings of dimension \\\\( d \\\\). Given an attention matrix \\\\( A \\\\), we draw at random a subset \\\\( I \\\\), where mapping \\\\( g \\\\) to \\\\( \\\\mathrm{CLS} \\\\) token embedding is prepended. This representation is input frame \\\\( X_t \\\\) with frame \\\\( X_t \\\\) that includes an \\\\( \\\\alpha \\\\theta \\\\) of \\\\( \\\\alpha = 0 \\\\) for \\\\( \\\\alpha \\\\) multi-head attention, these embeddings are partitioned as \\\\( \\\\theta \\\\) and \\\\( \\\\theta' \\\\) at the second-last layer of the teacher model is used by a multi-object tracker. As shown in Figure 3, it leverages the attention from the \\\\( \\\\mathrm{CLS} \\\\) token of distinct heads in a vision transformer to generate cross-attention maps \\\\( Z \\\\). We use those to mask \\\\( \\\\tilde{A} \\\\) to establish correspondences \\\\( M \\\\) between \\\\( \\\\tilde{A} \\\\) and \\\\( \\\\tilde{Q} \\\\). We introduce \\\\( D \\\\) is the \\\\( [\\\\mathrm{CLS}] \\\\) token of distinct heads in a vision transformer to a \\\\( d \\\\) embeddings of dimension \\\\( d \\\\). Given an attention matrix \\\\( A \\\\), we draw at random a subset \\\\( I \\\\), where mapping \\\\( g \\\\) to \\\\( \\\\mathrm{CLS} \\\\) token embedding is prepended. This representation is input frame \\\\( X_t \\\\) with frame \\\\( X_t \\\\) that includes an \\\\( \\\\alpha \\\\theta \\\\) of \\\\( \\\\alpha = 0 \\\\) for \\\\( \\\\alpha \\\\) multi-head attention, these embeddings are partitioned as \\\\( \\\\theta \\\\) and \\\\( \\\\theta' \\\\) at the second-last layer of the teacher model is used by a multi-object tracker. As shown in Figure 3, it leverages the attention from the \\\\( \\\\mathrm{CLS} \\\\) token of distinct heads in a vision transformer to generate cross-attention maps \\\\( Z \\\\). We use those to mask \\\\( \\\\tilde{A} \\\\) to establish correspondences \\\\( M \\\\) between \\\\( \\\\tilde{A} \\\\) and \\\\( \\\\tilde{Q} \\\\). We introduce \\\\( D \\\\) is the \\\\( [\\\\mathrm{CLS}] \\\\) token of distinct heads in a vision transformer to a \\\\( d \\\\) embeddings of dimension \\\\( d \\\\). Given an attention matrix \\\\( A \\\\), we draw at random a subset \\\\( I \\\\), where mapping \\\\( g \\\\) to \\\\( \\\\mathrm{CLS} \\\\) token embedding is prepended. This representation is input frame \\\\( X_t \\\\) with frame \\\\( X_t \\\\) that includes an \\\\( \\\\alpha \\\\theta \\\\) of \\\\( \\\\alpha = 0 \\\\) for \\\\( \\\\alpha \\\\) multi-head attention, these embeddings are partitioned as \\\\( \\\\theta \\\\) and \\\\( \\\\theta' \\\\) at the second-last layer of the teacher model is used by a multi-object tracker. As shown in Figure 3, it leverages the attention from the \\\\( \\\\mathrm{CLS} \\\\) token of distinct heads in a vision transformer to generate cross-attention maps \\\\( Z \\\\). We use those to mask \\\\( \\\\tilde{A} \\\\) to establish correspondences \\\\( M \\\\) between \\\\( \\\\tilde{A} \\\\) and \\\\( \\\\tilde{Q} \\\\). We introduce \\\\( D \\\\) is the \\\\( [\\\\mathrm{CLS}] \\\\) token of distinct heads in a vision transformer to a \\\\( d \\\\) embeddings of dimension \\\\( d \\\\). Given an attention matrix \\\\( A \\\\), we draw at random a subset \\\\( I \\\\), where mapping \\\\( g \\\\) to \\\\( \\\\mathrm{CLS} \\\\) token embedding is prepended. This representation is input frame \\\\( X_t \\\\) with frame \\\\( X_t \\\\) that includes an \\\\( \\\\alpha \\\\theta \\\\) of \\\\( \\\\alpha = 0 \\\\) for \\\\( \\\\alpha \\\\) multi-head attention, these embeddings are partitioned as \\\\( \\\\theta \\\\) and \\\\( \\\\theta' \\\\) at the second-last layer of the teacher model is used by a multi-object tracker. As shown in Figure 3, it leverages the attention from the \\\\( \\\\mathrm{CLS} \\\\) token of distinct heads in a vision transformer to generate cross-attention maps \\\\( Z \\\\). We use those to mask \\\\( \\\\tilde{A} \\\\) to establish correspondences \\\\( M \\\\) between \\\\( \\\\tilde{A} \\\\) and \\\\( \\\\tilde{Q} \\\\). We introduce \\\\( D \\\\) is the \\\\( [\\\\mathrm{CLS}] \\\\) token of distinct heads in a vision transformer to a \\\\( d \\\\) embeddings of dimension \\\\( d \\\\). Given an attention matrix \\\\( A \\\\), we draw at random a subset \\\\( I \\\\), where mapping \\\\( g \\\\) to \\\\( \\\\mathrm{CLS} \\\\) token embedding is prepended. This representation is input frame \\\\( X_t \\\\) with frame \\\\( X_t \\\\) that includes an \\\\( \\\\alpha \\\\theta \\\\) of \\\\( \\\\alpha = 0 \\\\) for \\\\( \\\\alpha \\\\) multi-head attention, these embeddings are partitioned as \\\\( \\\\theta \\\\) and \\\\( \\\\theta' \\\\) at the second-last layer of the teacher model is used by a multi-object tracker. As shown in Figure 3, it leverages the attention from the \\\\( \\\\mathrm{CLS} \\\\) token of distinct heads in a vision transformer to generate cross-attention maps \\\\( Z \\\\). We use those to mask \\\\( \\\\tilde{A} \\\\) to establish correspondences \\\\( M \\\\) between \\\\( \\\\tilde{A} \\\\) and \\\\( \\\\tilde{Q} \\\\). We introduce \\\\( D \\\\) is the \\\\( [\\\\mathrm{CLS}] \\\\) token of distinct heads in a vision transformer to a \\\\( d \\\\) embeddings of dimension \\\\( d \\\\). Given an attention matrix \\\\( A \\\\), we draw at random a subset \\\\( I \\\\), where mapping \\\\( g \\\\) to \\\\( \\\\mathrm{CLS} \\\\) token embedding is prepended. This representation is input frame \\\\( X_t \\\\) with frame \\\\( X_t \\\\) that includes an \\\\( \\\\alpha \\\\theta \\\\) of \\\\( \\\\alpha = 0 \\\\) for \\\\( \\\\alpha \\\\) multi-head attention, these embeddings are partitioned as \\\\( \\\\theta \\\\) and \\\\( \\\\theta' \\\\) at the second-last layer of the teacher model is used by a multi-object tracker. As shown in Figure 3, it leverages the attention from the \\\\( \\\\mathrm{CLS} \\\\) token of distinct heads in a vision transformer to generate cross-attention maps \\\\( Z \\\\). We use those to mask \\\\( \\\\tilde{A} \\\\) to establish correspondences \\\\( M \\\\) between \\\\( \\\\tilde{A} \\\\) and \\\\( \\\\tilde{Q} \\\\). We introduce \\\\( D \\\\) is the \\\\( [\\\\mathrm{CLS}] \\\\) token of distinct heads in a vision transformer to a \\\\( d \\\\) embeddings of dimension \\\\( d \\\\). Given an attention matrix \\\\( A \\\\), we draw at random a subset \\\\( I \\\\), where mapping \\\\( g \\\\) to \\\\( \\\\mathrm{CLS} \\\\) token embedding is prepended. This representation is input frame \\\\( X_t \\\\) with frame \\\\( X_t \\\\) that includes an \\\\( \\\\alpha \\\\theta \\\\) of \\\\( \\\\alpha = 0 \\\\) for \\\\( \\\\alpha \\\\) multi-head attention, these embeddings are partitioned as \\\\( \\\\theta \\\\) and \\\\( \\\\theta' \\\\) at the second-last layer of the teacher model is used by a multi-object tracker. As shown in Figure 3, it leverages the attention from the \\\\( \\\\mathrm{CLS} \\\\) token of distinct heads in a vision transformer to generate cross-attention maps \\\\( Z \\\\). We use those to mask \\\\( \\\\tilde{A} \\\\) to establish correspondences \\\\( M \\\\) between \\\\( \\\\tilde{A} \\\\) and \\\\( \\\\tilde{Q} \\\\). We introduce \\\\( D \\\\) is the \\\\( [\\\\mathrm{CLS}] \\\\) token of distinct heads in a vision transformer to a \\\\( d \\\\) embeddings of dimension \\\\( d \\\\). Given an attention matrix \\\\( A \\\\), we draw at random a subset \\\\( I \\\\), where mapping \\\\( g \\\\) to \\\\( \\\\mathrm{CLS} \\\\) token embedding is prepended. This representation is input frame \\\\( X_t \\\\) with frame \\\\( X_t \\\\) that includes an \\\\( \\\\alpha \\\\theta \\\\) of \\\\( \\\\alpha = 0 \\\\) for \\\\( \\\\alpha \\\\) multi-head attention, these embeddings are partitioned as \\\\( \\\\theta \\\\) and \\\\( \\\\theta' \\\\) at the second-last layer of the teacher model is used by a multi-object tracker. As shown in Figure 3, it leverages the attention from the \\\\( \\\\mathrm{CLS} \\\\) token of distinct heads in a vision transformer to generate cross-attention maps \\\\( Z \\\\). We use those to mask \\\\( \\\\tilde{A} \\\\) to establish correspondences \\\\( M \\\\) between \\\\( \\\\tilde{A} \\\\) and \\\\( \\\\tilde{Q} \\\\). We introduce \\\\( D \\\\) is the \\\\( [\\\\mathrm{CLS}] \\\\) token of distinct heads in a vision transformer to a \\\\( d \\\\) embeddings of dimension \\\\( d \\\\). Given an attention matrix \\\\( A \\\\), we draw at random a subset \\\\( I \\\\), where mapping \\\\( g \\\\) to \\\\( \\\\mathrm{CLS} \\\\) token embedding is prepended. This representation is input frame \\\\( X_t \\\\) with frame \\\\( X_t \\\\) that includes an \\\\( \\\\alpha \\\\theta \\\\) of \\\\( \\\\alpha = 0 \\\\) for \\\\( \\\\alpha \\\\) multi-head attention, these embeddings are partitioned as \\\\( \\\\theta \\\\) and \\\\( \\\\theta' \\\\) at the second-last layer of the teacher model is used by a multi-object tracker. As shown in Figure 3, it leverages the attention from the \\\\( \\\\mathrm{CLS} \\\\) token of distinct heads in a vision transformer to generate cross-attention maps \\\\( Z \\\\). We use those to mask \\\\( \\\\tilde{A} \\\\) to establish correspondences \\\\( M \\\\) between \\\\( \\\\tilde{A} \\\\) and \\\\( \\\\tilde{Q} \\\\). We introduce \\\\( D \\\\) is the \\\\( [\\\\mathrm{CLS}] \\\\) token of distinct heads in a vision transformer to a \\\\( d \\\\) embeddings of dimension \\\\( d \\\\). Given an attention matrix \\\\( A \\\\), we draw at random a subset \\\\( I \\\\), where mapping \\\\( g \\\\) to \\\\( \\\\mathrm{CLS} \\\\) token embedding is prepended. This representation is input frame \\\\( X_t \\\\) with frame \\\\( X_t \\\\) that includes an \\\\( \\\\alpha \\\\theta \\\\) of \\\\( \\\\alpha = 0 \\\\) for \\\\( \\\\alpha \\\\) multi-head attention, these embeddings are partitioned as \\\\( \\\\theta \\\\) and \\\\( \\\\theta' \\\\) at the second-last layer of the teacher model is used by a multi-object tracker. As shown in Figure 3, it leverages the attention from the \\\\( \\\\mathrm{CLS} \\\\) token of distinct heads in a vision transformer to generate cross-attention maps \\\\( Z \\\\). We use those to mask \\\\( \\\\tilde{A} \\\\) to establish correspondences \\\\( M \\\\) between \\\\( \\\\tilde{A} \\\\) and \\\\( \\\\tilde{Q"}
{"id": "Yen1lGns2o", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: For each input frame $t$ of a video clip (top), cross-attention map $T_t \\\\in \\\\mathbb{R}^{k \\\\times n}$ (middle) and refined cross-attention map $T'_t \\\\in \\\\mathbb{R}^{k \\\\times n}$ (bottom), using Sinkhorn-Knopp algorithm. For each object, one row of $T_t$ or $T'_t$ is reshaped as $h/p \\\\times w/p$ and upsampled to an attention map overlaid on the input frame for $k = 3$ objects encoded in blue, red and green channel. Mixed colors yellow and cyan for $T_t$ (middle, in red circle) indicate spatial overlap of two objects, while $T'_t$ yields three well separated objects shown in primary colors blue, red and green.\\n\\nTo represent the $k$ objects in the embedding space, we use matrix $A_I \\\\in \\\\mathbb{R}^{k \\\\times n}$ to form linear combinations of patch embeddings $\\\\tilde{Q} \\\\in \\\\mathbb{R}^{n \\\\times d}$, obtaining object prototypes $P := A_I \\\\tilde{Q} \\\\in \\\\mathbb{R}^{k \\\\times d}$. This can be seen as the representation of $k$ different $[CLS]$ tokens in the full embedding space, capturing $k$ objects at frame $t_0$. Then, given the key embeddings $K_t \\\\in \\\\mathbb{R}^{(n+1) \\\\times d}$ at another frame $t$, we could track the objects by cross-attention $T_t := \\\\text{softmax} \\\\left( \\\\frac{P \\\\tilde{Q}^\\\\top_t}{\\\\sqrt{d}} \\\\right) \\\\in \\\\mathbb{R}^{k \\\\times n}$, where $\\\\tilde{K}_t \\\\in \\\\mathbb{R}^{n \\\\times d}$.\\n\\nUnfortunately, we observe in Figure 4 that the $k$ attention maps obtained this way are spatially overlapping, meaning that each attention map is not delineating a single object.\\n\\nEstablishing object-patch correspondences\\n\\nTo discover spatially distinct objects, we propose to establish correspondences between prototypes and patch tokens. Let $Z_g^{\\\\theta'}(X_{t_0}) \\\\in \\\\mathbb{R}^{(n+1) \\\\times d}$ be the output embeddings of the teacher network, still at frame $t_0$. We seek a correspondence between the rows of $P \\\\in \\\\mathbb{R}^{k \\\\times d}$ and $\\\\tilde{Z} \\\\in \\\\mathbb{R}^{n \\\\times d}$, where $\\\\tilde{Z}$ are the patch token embeddings. The goal is to find a transport plan $M \\\\in \\\\mathbb{R}^{k \\\\times n}$ that minimizes the expected pairwise cost $C := -P \\\\tilde{Z}^\\\\top \\\\in \\\\mathbb{R}^{k \\\\times n}$ between prototypes and patches, while incorporating an entropic regularizer with coefficient $\\\\epsilon$. Matrix $M$ is non-negative with row-wise sum $1/k$ and column-wise sum $1/n$, representing a joint probability over $P$ and $\\\\tilde{Z}$ with uniform marginals. The minimal solution $M^*$ is unique and can be found by forming the matrix $e^{-C/\\\\epsilon}$ and then applying the Sinkhorn-Knopp (SK) algorithm (Cuturi, 2013), i.e., iteratively normalizing its rows and columns: $M^* = \\\\text{SK}_{\\\\epsilon} \\\\left( \\\\frac{P \\\\tilde{Z}^\\\\top}{\\\\sqrt{d}} \\\\right) \\\\in \\\\mathbb{R}^{k \\\\times n}$.\\n\\nObserve the similarity with (1) and (3), where scaling is by $\\\\sqrt{d}$ rather than $\\\\epsilon$, $\\\\exp$ is included in $\\\\text{softmax}$ and normalization is on rows only rather than iterative. Then, similarly with (2), we use the optimal transport plan $M^* \\\\in \\\\mathbb{R}^{k \\\\times n}$ to form linear combinations of patch embeddings $\\\\tilde{Z} \\\\in \\\\mathbb{R}^{n \\\\times d}$, obtaining the refined object prototypes $P' := M^* \\\\tilde{Z} \\\\in \\\\mathbb{R}^{k \\\\times d}$. (5)\"}"}
