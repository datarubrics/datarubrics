{"id": "ro4CgvfUKy", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here, we plot all model outputs for our testing set for a single seed for both model architectures (AE and VAE) to qualitatively confirm that our models capture the meaningful and relevant desired properties in our datasets, and that our qualitative model performance holds in general, and not only in a few hand-picked examples.\\n\\nFigure 16: All Variational Autoencoder outputs for a single seed of the GG Kanizsa Squares dataset. Inputs and outputs are displayed as horizontal pairs, with the input being the left image, and the model output being the right image.\\n\\nFigure 17: All Variational Autoencoder outputs for a single seed of the GG Closure dataset. Inputs and outputs are displayed as horizontal pairs, with the input being the left image, and the model output being the right image.\"}"}
{"id": "ro4CgvfUKy", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 18: All Variational Autoencoder outputs for a single seed of the GG Continuity dataset. Inputs and outputs are displayed as horizontal pairs, with the input being the left image, and the model output being the right image.\\n\\nFigure 19: All Variational Autoencoder outputs for a single seed of the GG Proximity dataset. Inputs and outputs are displayed as horizontal pairs, with the input being the left image, and the model output being the right image.\"}"}
{"id": "ro4CgvfUKy", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 20: All Variational Autoencoder outputs for a single seed of the GG Gradient Occlusion dataset. Inputs and outputs are displayed as horizontal pairs, with the input being the left image, and the model output being the right image.\\n\\nFigure 21: All Variational Autoencoder outputs for a single seed of the GG Illusory Occlusion dataset. Inputs and outputs are displayed as horizontal pairs, with the input being the left image, and the model output being the right image.\"}"}
{"id": "ro4CgvfUKy", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nFigure 22: All Autoencoder outputs for a single seed of the GG Kanizsa Squares dataset. Inputs and outputs are displayed as horizontal pairs, with the input being the left image, and the model output being the right image.\\n\\nFigure 23: All Autoencoder outputs for a single seed of the GG Closure dataset. Inputs and outputs are displayed as horizontal pairs, with the input being the left image, and the model output being the right image.\"}"}
{"id": "ro4CgvfUKy", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In our Segmentation algorithm, the expected number of clusters is provided on an image-by-image basis as a priori information for the model. In most cases, the expected number of clusters coincides with the actual clusters spontaneously formed by $\\\\Delta X_i$, which can be qualitatively verified with a UMAP visualization (McInnes et al., 2018). However, there are exceptions where $\\\\Delta X_i$ is not readily grouped into the expected numbers of clusters. We observe such exceptions in segmentation using AE on the GG Proximity dataset (when there only exists 1 group of 6 squares) and the GG Illusory Occlusion dataset. Here, we present the UMAP visualization for them using one example per dataset. Note that while not shown here, we observe that the qualitative pattern of the AE being unable to capture the expected number of clusters generally holds for these two cases.\\n\\nIn Figures 12b and 12c, we show the distribution of the vectors corresponding to each pixel in the $C \\\\times (N-1)$-dimensional space where clustering is performed, when the models are confronted with an image from the GG Proximity dataset with 1 group of 6 squares. The image and the expected target mask are shown in Figure 12a. In this case, the AE tends to separate the pixels belonging to the squares into 3 clusters, perhaps as a result of being affected by training samples where there exist 3 groups of 2 squares; while the VAE tends to perceive the right number of clusters. However, when given the expected number of clusters, which is 2 in this case, the AE managed to nevertheless yield the correct segmentation mask (indicated by the inset in Figure 12b).\\n\\nA similar situation happens also for the GG Illusory Occlusion dataset. As shown in Figure 13, the AE tends to separate the background from the background stripes. We would expect these two parts to be grouped together, since they always co-vary with each other. Nonetheless, given the expected number of clusters, the AE manages to ultimately segment correctly.\\n\\nWe further show the UMAP visualization of $\\\\Delta X_i$ to demonstrate that LNS forms a meaningful clusterable representation of the pixels in general. $\\\\Delta X_i$ is first reduced to $\\\\text{imag} \\\\times \\\\text{imag} \\\\times 2$ with UMAP performed on its last two dimensions, which contains $C \\\\times (N-1)$-dimensional representations for each pixels. The resulting UMAP representation, accordingly, contains 2-dimensional representations for all the pixels, which we visualize in Figure 15. The position of each dot shows the 2-dimensional representation for each pixel, and we differentiate between different parts of the image.\"}"}
{"id": "ro4CgvfUKy", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13: The AE perceives the wrong number of clusters for the GG Illusory Occlusion Dataset. Please refer to the caption of Figure 12.\\n\\nFigure 14: AE and V AE Model Reconstruction Examples. The first row shows inputs, the second row shows V AE reconstructions, and the third row shows AE reconstructions. Randomly selected examples from the Kanizsa Squares, Closure, Continuity, Proximity, Gradient Occlusion, and Illusory Occlusion test datasets.\\n\\nTo support our control experiment, we also visualize the model output reconstructions. In accordance with the control experiments, we find that the outputs in some cases contain some information about the identity of the objects, even when it does not veridically exist in the input image. However, as reported in Table 2, the amount of information in the reconstructions is not as high as it is when using Latent Noise Segmentation. This is because of the tension between reconstructing an image well (which would lead to poor segmentation without LNS in test samples), and segmenting the image components based on their color values. In other words, the fact that the model is still able to segment in some cases can be understood as an artefact caused by the small size of our model and imperfect reconstruction performance. The training process implicitly optimizes toward the success of LNS segmentation, but against the success of direct reconstruction segmentation.\"}"}
{"id": "ro4CgvfUKy", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\n(a) Model: VAE, dataset: Kanizsa Squares\\n(b) Model: VAE, dataset: Closure\\n(c) Model: VAE, dataset: Continuity\\n(d) Model: VAE, dataset: Proximity\\n(e) Model: VAE, dataset: Gradient Occlusion\\n(f) Model: VAE, dataset: Illusory Occlusion\"}"}
{"id": "ro4CgvfUKy", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\n(g) Model: AE, dataset: Kanizsa Squares\\n\\n(h) Model: AE, dataset: Closure\\n\\n(i) Model: AE, dataset: Continuity\\n\\n(j) Model: AE, dataset: Proximity\\n\\n(k) Model: AE, dataset: Gradient Occlusion\\n\\n(l) Model: AE, dataset: Illusory Occlusion\\n\\nFigure 15: Visualization of $\\\\Delta \\\\tilde{X}_i$ with UMAP\"}"}
{"id": "ro4CgvfUKy", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Neural Networks (DNNs) that achieve human-level performance in general tasks like object segmentation typically require supervised labels. In contrast, humans are able to perform these tasks effortlessly without supervision. To accomplish this, the human visual system makes use of perceptual grouping: for example, the black and white stripes of a zebra are perceptually grouped together despite their vastly different colors. Understanding how perceptual grouping arises in an unsupervised manner is critical for improving both computer vision models and models of the visual system. In this work, we propose a counterintuitive approach to unsupervised perceptual grouping and segmentation: that they arise because of neural noise, rather than in spite of it. We (1) mathematically demonstrate that under realistic assumptions, neural noise can be used to separate objects from each other, and (2) show that adding noise in a DNN enables the network to segment images even though it was never trained on any segmentation labels. Interestingly, we find that (3) segmenting objects using noise results in segmentation performance that aligns with the perceptual grouping phenomena observed in humans. We introduce the Good Gestalt (GG) datasets \u2014 six datasets designed to specifically test perceptual grouping, and show that our DNN models reproduce many important phenomena in human perception, such as illusory contours, closure, continuity, proximity, and occlusion. Finally, we (4) demonstrate the ecological plausibility of the method by analyzing the sensitivity of the DNN to different magnitudes of noise. We find that some model variants consistently succeed with remarkably low levels of neural noise ($\\\\sigma < 0.001$), and surprisingly, that segmenting this way requires as few as a handful of samples. Together, our results suggest a novel unsupervised segmentation method requiring few assumptions, a new explanation for the formation of perceptual grouping, and a novel potential benefit of neural noise.\"}"}
{"id": "ro4CgvfUKy", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"A substantial number of biases into the model architecture (Hamilton et al., 2022; Engelcke et al., 2022, 2020a). The current prevailing approach is the implementation of object slots in the model architecture (Locatello et al., 2020), which maintains information about object instances in a separately maintained vector. Slot-based approaches are conceptually limited by the fact that the number of slots is fixed in the model architecture and cannot be adapted by learning, nor on an image-by-image basis. An approach that attempts to remedy the problems above was proposed by L\\\"owe et al. (2022), who suggest a neuroscience-inspired solution to object learning: the object identities are stored in phase values of complex variables, which mimics the temporal synchrony hypothesis of brain function (Milner, 1974). While the approach can handle a variable number of objects, it is limited in the maximum number of objects it can represent (but see Stani \u00b4c et al. (2023)). To date, there is no unified and simple principle that allows DNNs to perform segmentation and grouping in a number of different, seemingly unrelated contexts with few in-principle limitations. In this work, we seek to remedy this gap.\\n\\n### Figure 1: Latent Noise Segmentation Schematic Illustration\\n\\n- **(a)** Biological neurons are highly noisy. For example, thermal noise and ion channel shot noise (Manwani and Koch, 1998) contribute to independent noise in neurons.\\n- **(b)** Independent noise affects the output of neurons that are highly selective to a stimulus feature (left) than neurons that are less selective (right). Solid lines indicate the mean of the noise-free activity distribution, and dashed lines indicate the actual sample after independent noise is added. The x-axis in the illustration is not meaningful.\\n- **(c)** In the system\u2019s representational space (as indicated by the surface where input images are mapped to points on that surface), the changes caused by independent noise cause meaningful changes to the model\u2019s representation in relevant directions (e.g., local PC directions), but not irrelevant directions (that would substantially change the model\u2019s representation of the input).\\n- **(d)** This yields information about the objects in the input image, and can be used to segment the input images. An input image $x(i)$ is fed into an autoencoder network, and noisy samples are drawn and consecutively subtracted from each other. These outputs contain information about the changes induced by noise in latent space, cast into image space. The outputs are stacked and clustered pixel-wise to generate a segmentation mask.\\n\\nSpecifically, we develop a direct test benchmark of a network\u2019s segmentation and grouping capability by requiring it to segment images in a way that necessitates perceptual grouping \u2014- furthermore, the network must do this without ever being trained on the stimuli in question, and without ever being trained on the task of segmentation, similarly to how the human visual system is not. To simultaneously accomplish all these goals, we posit a seemingly counterintuitive idea that we call Latent Noise Segmentation (LNS):\\n\\nNeural noise enables deep neural networks to perceptually group and segment.\\n\\nSee Figure 1 for a schematic overview of the approach. In brief, we show that by injecting independent noise to a hidden layer of a DNN, we can turn DNNs trained on generic image reconstruction tasks to systems capable of perceptual grouping and segmentation.\\n\\nIn the following sections, we describe the general setting of LNS and its specific implementation in an autoencoder DNN (Section 2). We mathematically demonstrate that under certain (relatively loose) assumptions, the presence of latent noise allows visual entities, which do not perfectly co-vary with each other, to be separated into individual objects (Section 2.1 and Appendix A.1). We develop a comprehensive test benchmark of segmentation tasks inspired by a century of Gestalt psy...\"}"}
{"id": "ro4CgvfUKy", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We empirically show that LNS directly reproduces the result of the elusive Gestalt principles of grouping in segmentation masks (Section 2.3). Finally, we study how practically feasible LNS is (that is, how in-principle usable the method is by any system constrained by limitations like compute time or noise magnitude, such as the primate visual system) by evaluating how segmentation performance varies with different model learning rules, noise levels, and the number of time steps the model takes to segment (Section 3). Our results suggest that a practically feasible number of time steps (as few as a handful) are sufficient to reliably segment and that while encouraging a prior distribution in a model does not improve its segmentation performance, it stabilizes the optimal amount of noise needed for segmentation across all datasets and correctly identifies the appropriate number of objects.\\n\\n2 MODEL AND METHODS\\n\\nHere, we describe the general setting of our methodology, how we obtain segmentation masks with LNS, the design of our datasets, and model evaluation metrics.\\n\\nBasic setup. Our model architecture consists of two primary components. The backbone of the model is a pre-trained (Variational) Autoencoder (VAE; Kramer (1991); Kingma and Welling (2014); Higgins et al. (2017)), where an encoder $\\\\text{Enc}(x) = q(\\\\theta(x))$ learns a compressed latent distribution, and a decoder $\\\\text{Dec}(z) = p(\\\\theta(z))$ models the data that generated the latent representation $z$. In practice, the model is optimized using the Evidence Lower Bound (ELBO):\\n\\n$$\\\\text{ELBO} = \\\\mathbb{E}_{q} \\\\left[ \\\\log p(\\\\theta(x)) \\\\right] - \\\\beta \\\\mathbb{D}_{KL}(q || p), \\\\quad (1)$$\\n\\nwhere $\\\\mathbb{D}_{KL}$ is the Kullback-Leibler divergence, and $p(z)$ is the prior set to $\\\\mathcal{N}(0, I)$. $\\\\beta$ is a configurable coefficient facilitating reconstruction quality, which we set automatically through the GECO loss (Rezende and Viola, 2018). For a discussion about alternate architectures, see Appendix A.1.5.\\n\\nAlgorithm 1 Latent Noise Segmentation.\\n\\nRequire: image $x_i$, time steps $N$, small noise variance $\\\\sigma^2_{\\\\text{small}}$.\\n\\nRequire: trained $\\\\beta$-VAE with $\\\\text{Enc}_\\\\phi$, $\\\\text{Dec}_\\\\theta$.\\n\\n1: procedure $\\\\text{SEGMENT}(x_i, N)$\\n2: $\\\\mu(x_i) \\\\leftarrow \\\\text{Enc}(x_i)$ \\\\Comment{Get latent unit means}\\n3: for $n = 1, \\\\ldots, N$ do\\n4: $\\\\xi_n \\\\leftarrow \\\\mathcal{N}(0, \\\\sigma^2_{\\\\text{small}})$ \\\\Comment{i.i.d. Gaussian noise}\\n5: $\\\\tilde{x}_i^n \\\\leftarrow \\\\text{Dec} \\\\mu(x_i) + \\\\xi_n$ \\\\Comment{Save perturbed decoder output}\\n6: if $n \\\\geq 2$ then\\n7: $\\\\Delta \\\\tilde{x}_i^{n-1} \\\\leftarrow \\\\tilde{x}_i^n - \\\\tilde{x}_i^{n-1}$ \\\\Comment{Subtract outputs}\\n8: end if\\n9: end for\\n10: Let $\\\\Delta \\\\tilde{X}_i \\\\equiv [\\\\Delta \\\\tilde{x}_i^1, \\\\Delta \\\\tilde{x}_i^2, \\\\ldots, \\\\Delta \\\\tilde{x}_i^{N-1}]$ \\\\Comment{$\\\\Delta \\\\tilde{X}_i$ has dimension $\\\\text{img}_x \\\\times \\\\text{img}_y \\\\times c \\\\times (N-1)$}\\n11: Cluster $\\\\Delta \\\\tilde{X}_i$ \\\\Comment{Separate pixel identities}\\n12: Assign segmentation mask values to pixels according to clustering\\n13: end procedure\\n\\n2.1 LATENT NOISE SEGMENTATION\\n\\nAfter training the model backbone, we build a noisy segmentation process which enables the extraction and combination of information from the model\u2019s latent space to generate segmentation masks. To do this, we add i.i.d. noise $\\\\xi_n$ to the latent variables $z$. The process is repeated $N$ times, allowing us to obtain information about which objects exist in the model outputs. For every time step, two consecutive model outputs are subtracted from each other to find what in the output images $x$ changed with respect to changes in latent space $z$. At the end of the process, the stack of subtracted outputs is clustered using Agglomerative Clustering (Pedregosa et al., 2011), resulting in a segmentation mask. Full details of the algorithm are shown in Algorithm 1, a schematic is shown in Figure 1, and additional details can be found in Appendix E.2.\"}"}
{"id": "ro4CgvfUKy", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The Good Gestalt (GG) Datasets. Zoom in for an ideal viewing experience. The first two rows of images show training image examples, while the second two rows show images of testing examples.\\n\\n2.2 The Role of Independent Noise\\n\\nLatent Noise Segmentation works due to several key reasons. Independent noise $\\\\xi$ induces the locally monotone mapping $f: \\\\Delta z \\\\rightarrow \\\\Delta x$ learned by the model. A potential problem is that this mapping happens for all units \u2014 even those that are not meaningful for the task of segmentation \u2014 and as such, they might adversely affect segmentation performance. To solve this, one could identify the meaningful units by perturbing them individually (like in latent traversal (Kingma and Welling, 2014; Higgins et al., 2017), but not only is this computationally expensive and impractical, it is also biologically implausible.\\n\\nThe solution to the problem comes from the intuition that units that meaningfully code for relevant stimulus features have a substantially higher derivative $\\\\Delta x = \\\\Delta z \\\\ast \\\\delta x \\\\delta z$ in the neighborhood of the stimulus $x$. This is because for the decoder to achieve its goal of low reconstruction loss, it should be sensitive to small changes in the activity of units that are coding for relevant features in $x$, but not for irrelevant features. This means that we can perturb all units simultaneously with independent noise: training on a generic reconstruction loss encourages the network to code in a manner that allows independent noise to reveal the relevant derivative direction, making the representation clusterable for segmentation (Figure 1b). Indeed, the goal of independent noise is not to perform segmentation itself, but to reveal the local neighborhood of the input stimulus and thus enable segmentation, from the perspective of the decoder.\\n\\nWe provide a more complete mathematical intuition of exactly why noise perturbation on latent units causes a meaningful object segmentation in the case of the VAE in Appendix A.1. In summary, we observe that noise induces the output pixels belonging to the same part of the image to co-vary with each other, while pixels belonging to different parts do not. Following a principal component argument (Zietlow et al., 2021; Rol\u00ednek et al., 2019), we show that the derivatives of the decoder's output with respect to the latent units are the local principal component directions of the training data. Together, these phenomena enable meaningful object segmentation.\\n\\n2.3 The Good Gestalt (GG) Datasets\\n\\nTo evaluate our models, we developed the six datasets that we together call the Good Gestalt (GG) datasets. The notion of Gestalt comes from the study of perception (Wertheimer, 1923; J\u00e4kel et al., 2016; Wagemans et al., 2012a; b), and is perhaps best summarized as 'the whole is greater than the sum of its parts'. The GG datasets aim to address a large variety of different rules of perceptual organization, otherwise known as Gestalt principles of grouping like continuity, closure and prox-\"}"}
{"id": "ro4CgvfUKy", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Target segmentation mask examples of the GG datasets. The first row of images shows training image examples, while the second row shows images of testing examples. Different colors indicate different objects.\\n\\nKanizsa Squares: The model should segment a square in the center, as well as four background circles, separately from the background.\\n\\nClosure: The model should segment a square.\\n\\nContinuity: The model should segment a circle traced by the relevant line segments; to \u201ccomplete the circle\u201d.\\n\\nProximity: The model should segment a set of six squares together, or three sets of two squares when the proximity cue is given.\\n\\nGradient Occlusion: The model should segment the two rectangles separately.\\n\\nIllusory Occlusion: The model should segment the static background and stripes together, and the foreground object parts together.\\n\\nReasons for studying Gestalt.\\nIn almost all the Gestalt stimuli we study in this work, we assume that in training the luminances (pixel values) of different objects in a visual scene are independent, while the luminances (pixel values) of the same object co-vary. This assumption follows a simplified view on optics (for details, see Appendix E.1), and is important as some principles of perceptual grouping have been shown to be consistent with the statistical structure of the natural environment (Elder and Goldberg, 2002; Geisler et al., 2001; Sigman et al., 2001). In testing, the model should generalize this information in ambiguous contexts, where the pixel values of the images are not fully informative of the object. For example, in the Kanizsa Squares dataset (Figure 2, Kanizsa Squares, Testing examples), the pixel value of the background is the same as the pixel value of the illusory square in the middle of the cornering circles.\\n\\nWe show target segmentation masks for the GG datasets in Figure 3. To determine what these segmentation masks should be, our focus here is not about what specifically humans perceive in a specific psychophysical experiment, but rather about what humans can perceive given the right instruction. This is part of what makes studying Gestalt so difficult: simple phrasing about what a participant should segment in such an image can potentially crucially affect how they segment the image. This is why in our GG datasets, we did not collect human data, but instead focus on the generic question of \u201cwhich elements in the image belong together?\u201d, and defined the target segmentation masks for the test set evaluation on the basis of this question, focusing on evidence from human research in general from the past century (Wertheimer, 1923; Kimchi, 1992).\\n\\nIn total, the GG datasets are made up of six individual datasets, for which we explain the main premise individually below. More information on the datasets can be found in Appendix E.1.\\n\\nKanizsa Squares: to study the perception of illusory figures. Humans are able to perceive illusory contours or illusory objects when cued by, for example, Pacman-like circles (Wang et al., 2012; Lesher, 1995).\\n\\nClosure: to study whether a model can combine individual image elements to form an object. The human ability to group objects together when they form a closed figure is well-documented (Elder and Zucker, 1993; Pomerantz et al., 1977; Ringach and Shapley, 1996; Marini and Marzi, 2016).\\n\\nContinuity: to study the integration of individual elements across the visual field. When elements are oriented such that they would form a continuous object, humans are able to group the elements together to form a whole figure (Kwon et al., 2016; Kov\u00e1cs and Julesz, 1993; Field et al., 1993).\\n\\nProximity: to study the cue of intra-element distance for grouping. When multiple elements in a figure are similar, humans can perceive many similar elements as belonging together with...\"}"}
{"id": "ro4CgvfUKy", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 24: All Autoencoder outputs for a single seed of the GG Continuity dataset. Inputs and outputs are displayed as horizontal pairs, with the input being the left image, and the model output being the right image.\\n\\nFigure 25: All Autoencoder outputs for a single seed of the GG Proximity dataset. Inputs and outputs are displayed as horizontal pairs, with the input being the left image, and the model output being the right image.\"}"}
{"id": "ro4CgvfUKy", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 26: All Autoencoder outputs for a single seed of the GG Gradient Occlusion dataset. Inputs and outputs are displayed as horizontal pairs, with the input being the left image, and the model output being the right image.\\n\\nFigure 27: All Autoencoder outputs for a single seed of the GG Illusory Occlusion dataset. Inputs and outputs are displayed as horizontal pairs, with the input being the left image, and the model output being the right image.\"}"}
{"id": "ro4CgvfUKy", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We tested two high-performing unsupervised object segmentation models from the literature: Genesis (Engelcke et al., 2020a) and Genesis-v2 (Engelcke et al., 2022). We show quantitative results in Table 2. While Genesis performs quantitatively relatively well, a closer inspection of the actual segmentation performance of the model shows that with the exception of the Closure and Continuity datasets, it fails to find the correct Gestalt grouping of the stimuli, often opting to follow the color value cues in the testing dataset. The Genesis-v2 performs generally worse than Genesis, failing on all datasets both quantitatively and qualitatively with perhaps the exception of the Continuity dataset.\\n\\n**Figure 10:** Control Model Segmentation Mask Examples. The first row shows inputs, the second row shows Genesis segmentation masks, and the third row shows Genesis-v2 segmentation masks. Randomly selected examples from the Kanizsa Squares, Closure, Continuity, Proximity, Gradient Occlusion, and Illusory Occlusion datasets. The specific color assignment to different object identities is arbitrary (for example, whether the model assigns the identity represented by yellow as the background, or purple, is not a meaningful distinction).\\n\\n**C.2 IMPLEMENTATION DETAILS AND CONVERGENCE**\\n\\nTo ensure that the baseline failure is not due to an implementation error or a failure in convergence, we take two precautions: 1) We follow the official implementation of Genesis and Genesis-v2 (Engelcke et al., 2022; 2020a,b) (GPL v3 license); 2) We verify convergence by visualizing model reconstructions on a separate validation set with samples that were not shown during training. Furthermore, we follow the hyperparameter choices reported in the original papers (5e5 training iterations with batch size 64, K = 5). Models were trained using an RTX 4090 GPU. Genesis-v2 trained for approximately 30 GPU-hours per dataset, while Genesis trained for approximately 20 GPU-hours per dataset, for a combined total of approximately 300 GPU-hours.\\n\\n**C.3 NOISY AE/VAE CONTROL EXPERIMENT IMPLEMENTATION DETAILS**\\n\\nThe noise standard deviation applied to the reconstructions is 0.3235, the mean of the best noise values for the AE and VAE. For clustering we used Agglomerative Clustering with the Euclidean metric and Ward linkage.\"}"}
{"id": "ro4CgvfUKy", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: Control Model Reconstruction Examples. The first row shows inputs, the second row shows Genesis reconstructions, and the third row shows Genesis-v2 reconstructions. Randomly selected examples from the Kanizsa Squares, Closure, Continuity, Proximity, Gradient Occlusion, and Illusory Occlusion validation datasets. The validation samples come from the same distribution as the training data, but the specific samples are never seen in training.\\n\\n**D C LUSTERING**\\n\\nWe used Agglomerative Clustering (Pedregosa et al., 2011) with the Euclidean metric and Ward linkage. We selected the desired number of clusters on an image-by-image basis. The desired number of clusters is the number of co-varying parts in the visual scene. For example, for an image in the GG Proximity dataset, if it contains one group of 6 squares, then the desired number of clusters is 2, corresponding to the co-varying squares and the background; if it contains 3 groups of 2 squares, then the desired number of clusters is 4, corresponding to the three groups of co-varying squares and the background.\\n\\nPrior to applying the agglomerative clustering algorithm, we normalize $\\\\Delta \\\\tilde{X}$ along its last dimension, so the vectors (with a size of $C \\\\times (N-1)$) have the same norm.\\n\\n**D.1 C OMPARISON OF D IFFERENT C LUSTERING A LGORITHMS**\\n\\nWe conducted a control experiment to test whether the specific clustering we used is a critical part of why LNS works. We tested three clustering algorithms: Agglomerative Clustering ((Pedregosa et al., 2011) with the Euclidean metric and Ward linkage; Agglomerative Clustering with the Euclidean metric and Complete linkage; and K-means clustering. We find that while the choice of clustering algorithm can affect results (Ward linkage outperforms Complete linkage), the specific algorithm is not a critical part of LNS (K-means achieves similar or better performance than Agglomerative Clustering with the Ward linkage). This result supports the conclusion that the key intuition behind LNS is that it allows the model to extract a meaningful clusterable representation, that can then be clustered using various clustering algorithms.\"}"}
{"id": "ro4CgvfUKy", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Model Kanizsa Closure Continuity Proximity Gradient Occ. Illusory Occ.\\n\\nAE+Agg 0.859 \u00b1 0.008\\nV AE+Agg 0.871 \u00b1 0.005\\nAE+KMeans 0.854 \u00b1 0.003\\nV AE+KMeans 0.866 \u00b1 0.003\\nAE+Agg' 0.557 \u00b1 0.007\\nV AE+Agg' 0.457 \u00b1 0.008\\n\\nTable 2: Model results for different clustering algorithms (ARI \u00b1 standard error of the mean).\\n\\nAgg = the agglomerative clustering algorithm from sklearn with metric being \\\"euclidean\\\" and linkage being \\\"ward\\\". Agg' = the agglomerative clustering algorithm from sklearn with metric being \\\"euclidean\\\" and linkage being \\\"complete\\\". KMeans = the K-means algorithm from sklearn. These scores are obtained for the optimal noise level and number of samples found for Agg.\\n\\nE  IMPLEMENTATION\\nE.1 GG DATASETS\\n\\nHere we introduce the design of the GG datasets. Each image of the datasets contains several parts including one or several (groups of) objects and a background. In the training dataset, the pixel values belonging to the same part co-vary with each other in the same channel while the pixel values belonging to different parts take independent values in the same channels. Such a scheme is expected to abstract, at a high level, the statistical structure of realistic visual scenes, which is based on basic optics:\\n\\n\\\\[ L = I \\\\times R, \\\\]\\n\\nwhere \\\\( L \\\\) is luminance, \\\\( I \\\\) is illuminance, and \\\\( R \\\\) is reflectance. The rationale behind the above-mentioned relationship of different pixels is explained below.\\n\\nThe images in the datasets all have 3 channels. For simplicity, let us only consider one of the channels. The pixel values of one channel are the luminances of the reflected light that has the same wavelength. If two pixels belong to the same object, the ratio between their reflectances is fixed. This is because, in real life, the reflectance ratio between the different parts of the same object is fixed, as it is determined by the material. On the other hand, the reflectances of two pixels belonging to two different objects that can appear in a visual scene are independent. Furthermore, we assume that all the pixels of one object are exposed to the same lighting condition, namely the same illuminance, which is a reasonable simplifying assumption for simple visual scenes in real life. With the above, one can draw the conclusion that the luminances (pixel values) of different pixels belonging to the same object should keep a fixed ratio between each other throughout the training dataset, the luminances (pixel values) of different pixels belonging to different objects should be independent. Moreover, the pixel values of the same pixel in different channels are independent. Because the object might be in different lighting conditions in different visual scenes, and the illuminances of different channels under different lighting conditions should be independent.\\n\\nHere, we describe how the different GG datasets are constructed. We encourage the reader to revisit Figures 2 and 3 while reading the following for a better understanding.\\n\\nKanizsa Squares contains three parts: a square, four circles, and a background. The four circles always have the same pixel values. Their relative positions are fixed in the testing dataset but not in the training dataset. In the testing dataset, although the square and the background have the same color, the model is expected to separate these two parts since it is primed by the training samples to do so. Such expectation is reflected in the segmentation masks (Figure 3).\\n\\nClosure contains two parts, one outlined square and a background. In the testing dataset, the interior of the outlined square and the background share the same pixel values, and the outlines of the squares are sometimes occluded partially. The pixel value ratio of the outline and the square interior is kept the same as in the training dataset. The model is expected to perceive the squares together with the outlines.\"}"}
{"id": "ro4CgvfUKy", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Continuity contains two parts in the training dataset, a circle and a background. In the testing dataset, the circle is fragmented, and several randomly generated fragments are placed at arbitrary positions. The model should perceive the existence of a circle in the testing dataset despite these disturbances.\\n\\nProximity contains either $2$ parts or $4$ parts. In the former case, it contains a background and a group of $6$ squares with an interspace of $1$ pixel. In the latter case, it contains a background and $3$ groups of $2$ squares. The interspaces between different squares within the same group are $1$ pixel wide, and the interspaces between different groups are at least $3$ pixel wide so that the two cases cannot be confounded. In the testing dataset, all the squares have the same color. However, the model is expected to group different squares together correctly given the interspaces.\\n\\nGradient Occlusion contains three parts, one tall rectangle with homogeneous color, one wide rectangle with gradient color, and a background. In the testing dataset, the homogeneously-colored rectangle always occludes the gradient-colored rectangle. However, the model is expected to group together the pixels belonging to the gradient-color rectangle, even if they do not have the same pixel values and are sometimes separated by the homogeneously-colored rectangle.\\n\\nIllusory Occlusion contains $2$ parts, one striped background, and one striped square. In the testing dataset, part of the background and the foreground object have the same pixel value. However, the model is expected to identify the foreground-background structure.\\n\\nFor code to generate the GG datasets, please refer to `create_datasets.py` in [our git repo](our git repo) will be here in the camera-ready version of the manuscript]. The training set in our experiments contained $30,000$ images, the validation set contained $300$ images that were not in the training set, and the test set contained $100$ images.\\n\\n### E.2 Deep Neural Networks\\n\\n| Hyperparameter          | Value                          |\\n|-------------------------|--------------------------------|\\n| Optimizer               | Adam                           |\\n| Learning rate           | $5 \\\\times 10^{-5}$             |\\n| Adam betas              | $(0.9, 0.999)$                 |\\n| Batch size              | 64                             |\\n| Model latent dim        | $15$ (GG) / $50$ (CelebA)      |\\n| Training iterations     | $1 \\\\times 10^6$                |\\n| AE loss function        | MSE                            |\\n| V AE loss function      | GECO* ([Rezende and Viola, 2018](Rezende and Viola, 2018)) |\\n| GECO goal               | $0.0006$ (GG) / $0.0125$ (CelebA) |\\n\\nTable 3: Autoencoder and V AE hyperparameters used in the experiments.\\n\\n*Adapted from the implementation by Engelcke et al. (2022).*\\n\\n| Layer         | Output Dimension | Kernel size | Stride | Padding | Activation function |\\n|---------------|------------------|-------------|--------|---------|---------------------|\\n| Enc Conv2d    | 32               | 3           | 1      | 1       | ReLU                |\\n| Conv2d        | 64               | 4           | 2      | 1       | ReLU                |\\n| Conv2d        | 64               | 4           | 2      | 1       | ReLU                |\\n| Conv2d        | 128              | 4           | 2      | 1       | ReLU                |\\n| Conv2d        | 128              | 4           | 2      | 1       | ReLU                |\\n| Conv2d        | 256              | 4           | 1      | 0       | ReLU                |\\n| Linear        | 128              | -           | -      | -       | ReLU                |\\n| Linear        | 15               | -           | -      | -       | -                   |\\n\\nTable 4: Autoencoder and V AE architecture.\"}"}
{"id": "ro4CgvfUKy", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "ro4CgvfUKy", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nRichard B. Stein, E. Roderich Gossen, and Kelvin E. Jones. Neuronal variability: noise or part of the signal? Nature Reviews Neuroscience, 6(5):389\u2013397, 2005. doi: 10.1038/nrn1668.\\n\\nDejan Todorovic. Gestalt principles. Scholarpedia, 3(12):5345, 2008. doi: 10.4249/scholarpedia.5345.\\n\\nJohan Wagemans, James H. Elder, Michael Kubovy, Stephen E. Palmer, Mary A. Peterson, Manish Singh, and R\u00fcdiger von der Heydt. A century of Gestalt psychology in visual perception: I. Perceptual grouping and figure\u2013ground organization. Psychological Bulletin, 138(6):1172\u20131217, 2012a. doi: 10.1037/a0029333.\\n\\nJohan Wagemans, Jacob Feldman, Sergei Gepshtein, Ruth Kimchi, James R. Pomerantz, Peter A. van der Helm, and Cees van Leeuwen. A century of Gestalt psychology in visual perception: II. Conceptual and theoretical foundations. Psychological Bulletin, 138(6):1218\u20131252, 2012b. doi: 10.1037/a0029334.\\n\\nLan Wang, Xuchu Weng, and Sheng He. Perceptual Grouping without Awareness: Superiority of Kanizsa Triangle in Breaking Interocular Suppression. PLOS ONE, 7(6):e40106, 2012. doi: 10.1371/journal.pone.0040106.\\n\\nMax Wertheimer. Laws of Organization in Perceptual Forms. Psycologische Forschung, 4:301\u2013350, 1923.\\n\\nDominik Zietlow, Michal Rolinek, and Georg Martius. Demystifying inductive biases for $\\\\beta$-vae based architectures. ArXiv, abs/2102.06822, 2021.\"}"}
{"id": "ro4CgvfUKy", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OBJECT SEPARABILITY\\n\\nThe Latent Noise Segmentation approach proposed in this work is capable of separating pixels into different objects regardless of their actual values in the input. This fact is a necessary result yielded by a Variational Autoencoder (VAE) pursuing its optimization objective. Here, we formalize the mathematical intuition behind why this is the case.\\n\\nA.1.1 BACKGROUND\\n\\nRol\u00ednek et al. (2019); Zietlow et al. (2021) showed that VAE-based Deep Neural Networks (DNNs) learn to represent local Principal Component (PC) axes. The training process of a VAE aims to minimize the reconstruction loss, part of which can be re-assembled as the stochastic reconstruction loss:\\n\\n$$\\\\hat{L}_{\\\\text{rec}} = \\\\mathbb{E}_{z_i \\\\sim N(\\\\mu_{x_i}, \\\\text{diag}(\\\\sigma^2_{x_i}))} \\\\| \\\\text{Dec}_{\\\\theta} \\\\mu_{x_i} - \\\\text{Dec}_{\\\\theta} z_i \\\\|_2,$$\\n\\nwhere the superscript $i$ denotes the index of the training samples.\\n\\nWe further define a Jacobian matrix to denote the decoded output's derivative with respect to the latent variables:\\n\\n$$J_i = \\\\frac{\\\\partial \\\\text{Dec}_{\\\\theta}(z)}{\\\\partial z} \\\\in \\\\mathbb{R}^{D \\\\times d},$$\\n\\nwhere $D = \\\\text{img}_x \\\\times \\\\text{img}_y \\\\times C$ is the dimension of the images, and $d$ is the number of latent nodes.\\n\\nRol\u00ednek et al. (2019) proved that optimizing equation 2 will promote pairwise orthogonality on the columns of equation 3, which means that the latent variables locally encode orthogonal features in the image space in $\\\\mathbb{R}^D$. Furthermore, Zietlow et al. (2021) analyzed what those pairwise orthogonal directions are. With experiments, they empirically showed that those directions correspond to local PC directions. More concretely, those directions are the PC directions yielded by a Principal Component Analysis (PCA) performed on a subset of all the training samples that lie close to each other in the data space.\\n\\nBuilt on the theoretical framework of Rol\u00ednek et al. (2019) and Zietlow et al. (2021), we are able to link these results to our context, and provide intuition for why the VAE is able to separate the object and the background.\\n\\nOne relevant technical detail is that the above statement regarding the latent nodes pursuing local local PC directions is derived from a VAE structure with a standardized objective as formulated by Higgins et al. (2017). However, it is easy to check the proof from Rol\u00ednek et al. (2019) to see that the same conclusion applies to our case, where the GECO (Rezende and Viola, 2018) loss is used. Furthermore, although the above-mentioned argument from Rol\u00ednek et al. (2019); Zietlow et al. (2021) is for the Jacobians (equation 3) calculated at training samples, we posit that the same conclusion holds for the testing samples. After all, in our setting, the training samples and the testing samples are close to each other. Henceforth, $i$ will be used to denote the index of testing samples.\\n\\nA.1.2 MATHEMATICAL INTUITION UNDERPINNING ESTELO PERCEPTION\\n\\nIn the following, we will use $p_1$ to denote a pixel belonging to one object of the image, and $p_2$ to denote a pixel belonging to a different object of the image.\\n\\nIn our segmentation algorithm, we first repeatedly apply noise to the latent units and observe the resultant changes in output pixels. We denote the added noise, at the time step of $n$, to be $\\\\xi_n = (\\\\xi_{n,j})^\\\\top \\\\in \\\\mathbb{R}^d$, where the second subscript $j$ denotes the indices of the latent units to which noise is added.\\n\\nBased on multivariate Taylor expansion, the reconstruction yielded by the latent affected by the noise can be written as:\"}"}
{"id": "ro4CgvfUKy", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ \\\\tilde{x}_i^n = \\\\text{Dec}(\\\\mu x_i + \\\\xi_n) \\\\]\\n\\n\\\\[ X_j = 1 \\\\delta_{n,j} \\\\frac{\\\\partial \\\\text{Dec}(z)}{\\\\partial z_j} z = \\\\mu(x_i) + o(\\\\xi_n), \\\\tag{4} \\\\]\\n\\nIn which the superscript \\\\( i \\\\) of \\\\( x \\\\) denotes the index of the input image, the subscript \\\\( j \\\\) of \\\\( z \\\\) denotes the latent index. \\\\( \\\\frac{\\\\partial \\\\text{Dec}(z)}{\\\\partial z_j} \\\\) is the \\\\( j \\\\)th column of the Jacobian in equation 3. \\\\( o(\\\\xi_n) \\\\) denotes the residual terms of the Taylor expansion.\\n\\nIn our segmentation algorithm, we subtract the reconstruction affected by one set of noise from the reconstruction affected by the previous set of noise. If we use \\\\( \\\\delta_n = (\\\\delta_{n,j})^\\\\top \\\\in \\\\{1, 2, \\\\ldots, d\\\\} \\\\in \\\\mathbb{R}^d \\\\) to represent \\\\((\\\\xi_{n+1} - \\\\xi_n)\\\\), then we can write the difference between the two consecutive noisy reconstructions as:\\n\\n\\\\[ \\\\Delta \\\\tilde{x}_i^n = \\\\tilde{x}_i^{n+1} - \\\\tilde{x}_i^n = d X_j = 1 \\\\delta_{n,j} \\\\frac{\\\\partial \\\\text{Dec}(z)}{\\\\partial z_j} z = \\\\mu(x_i) \\\\in \\\\mathbb{R}^D, \\\\tag{5} \\\\]\\n\\nwhere we omit the \\\\( o(\\\\cdot) \\\\) term, as they are negligible when the perturbing noise is sufficiently small.\\n\\nFurthermore, we use \\\\( \\\\Delta \\\\tilde{x}_i^n,c,p_1 \\\\in \\\\mathbb{R} \\\\) to denote the value of the component of \\\\( \\\\Delta \\\\tilde{x}_i^n \\\\) corresponding to the value of the pixel \\\\( p_1 \\\\) in the channel \\\\( c \\\\) and \\\\( \\\\Delta \\\\tilde{x}_i^n,c,p_2 \\\\in \\\\mathbb{R} \\\\) to denote the value of the component of \\\\( \\\\Delta \\\\tilde{x}_i^n \\\\) corresponding to the value of the pixel \\\\( p_2 \\\\) in the channel \\\\( c \\\\).\\n\\nOur visual segmentation pipeline separates pixel \\\\( p_1 \\\\) from pixel \\\\( p_2 \\\\) by separating the vector \\\\( \\\\Delta \\\\tilde{X}_i p_1 = \\\\Delta \\\\tilde{x}_i^n,c,p_1^\\\\top (n,c) \\\\in \\\\{1, 2, \\\\ldots, N-1\\\\} \\\\times \\\\{1, \\\\ldots, C\\\\} \\\\in \\\\mathbb{R}^{(N-1) \\\\times C} \\\\) from the vector \\\\( \\\\Delta \\\\tilde{X}_i p_2 = \\\\Delta \\\\tilde{x}_i^n,c,p_2^\\\\top (n,c) \\\\in \\\\{1, 2, \\\\ldots, N-1\\\\} \\\\times \\\\{1, \\\\ldots, C\\\\} \\\\in \\\\mathbb{R}^{(N-1) \\\\times C} \\\\).\\n\\nMore concretely, as indicated in Appendix D, the separation between the two vectors are based on the angles they point toward in the \\\\((N-1) \\\\times C\\\\)-dimensional space. In other words, the case in which the two pixels cannot be separated with our pipeline can only be caused by the case that no matter how we sample our noise, the following condition holds with probability 1.\\n\\n**Condition for Inseparability.** There exist some constant \\\\( r_{p_1,p_2} > 0 \\\\) such that, for the given pixel pair \\\\((p_1, p_2)\\\\) and any channel \\\\( c \\\\) and any time step \\\\( n \\\\):\\n\\n\\\\[ \\\\Delta \\\\tilde{x}_i^n,c,p_1 = r_{p_1,p_2} \\\\Delta \\\\tilde{x}_i^n,c,p_2. \\\\]\\n\\nThe equation in the condition for inseparability can be expanded as:\\n\\n\\\\[ d X_j = 1 \\\\delta_{n,j} \\\\frac{\\\\partial \\\\text{Dec}(z)}{\\\\partial z_j} c,p_1 \\\\frac{\\\\partial \\\\text{Dec}(z)}{\\\\partial z_j} c,p_2 = \\\\mu(x_i) = r_{p_1,p_2} d X_j = 1 \\\\delta_{n,j} \\\\frac{\\\\partial \\\\text{Dec}(z)}{\\\\partial z_j} c,p_2 \\\\frac{\\\\partial \\\\text{Dec}(z)}{\\\\partial z_j} c,p_2 = \\\\mu(x_i) \\\\tag{6} \\\\]\\n\\nThe fact that condition for inseparability holding with probability 1 implies inseparability is intuitive, and we also provide a proof for it in A.1.3.\\n\\nIn the following, we will argue why the condition for inseparability is unlikely to hold with probability 1 given the background in Section A.1.1.\\n\\nFor the condition of inseparability to hold, the Jacobian (in equation 3) and the noise must satisfy one of the following two cases.\\n\\nThe **first case** is when the Jacobian satisfies:\\n\\n\\\\[ \\\\frac{\\\\partial \\\\text{Dec}(z)}{\\\\partial z_j} c,p_1 = \\\\mu(x_i) = r_{p_1,p_2} \\\\frac{\\\\partial \\\\text{Dec}(z)}{\\\\partial z_j} c,p_2, \\\\]\\n\\nfor some fixed \\\\( r_{p_1,p_2} \\\\) for all \\\\( j \\\\in \\\\{1, 2, \\\\ldots, d\\\\} \\\\) and all channels \\\\( c \\\\). Here \\\\( \\\\text{Dec}(z)_{c,p_2} / p_2 \\\\in \\\\mathbb{R} \\\\) is the component of the \\\\( D \\\\)-dimensional vector \\\\( \\\\text{Dec}(z) \\\\) that corresponds to the \\\\( p_1 \\\\) or \\\\( p_2 \\\\) pixel in channel \\\\( c \\\\).\\n\\nIn this case, the condition for inseparability trivially holds with probability 1.\\n\\nThe **second case** is where equation 7 does not hold for any fixed \\\\( r_{p_1,p_2} \\\\) for some \\\\( j \\\\in \\\\{1, 2, \\\\ldots, d\\\\} \\\\) and some channel \\\\( c \\\\). In this case, the condition for inseparability might still hold.\"}"}
{"id": "ro4CgvfUKy", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mathematical details about our results are included in Appendix A.1. Full details on our GG datasets are included in Appendix E.1, and full details on model architecture and training are included in Appendix E.2. Code and data will be made available in the camera-ready version of the manuscript.\"}"}
{"id": "ro4CgvfUKy", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"REFERENCES\\n\\nDaniel M. Bear, Kevin Feigelis, Honglin Chen, Wanhee Lee, Rahul Venkatesh, Klemen Kotar, Alex Durango, and Daniel L. K. Yamins. Unifying (Machine) Vision via Counterfactual World Modeling. ArXiv pre-print, (arXiv:2306.01828), 2023.\\n\\nR. Benzi, A. Sutera, and A. Vulpiani. The mechanism of stochastic resonance. Journal of Physics A: Mathematical and General, 14(11):L453, 1981. doi: 10.1088/0305-4470/14/11/006.\\n\\nValerio Biscione and Jeffrey S. Bowers. Mixed Evidence for Gestalt Grouping in Deep Neural Networks. Computational Brain & Behavior, 6(3):438\u2013456, 2023. doi: 10.1007/s42113-023-00169-2.\\n\\nVictor Boutin, Aimen Zerroug, Minju Jung, and Thomas Serre. Iterative VAE as a predictive brain model for out-of-distribution generalization. ArXiv pre-print, (arXiv:2012.00557), 2020.\\n\\nJeffrey S. Bowers, Gaurav Malhotra, Marin Dujmovi\u0107, Milton Llera Montero, Christian Tsvetkov, Valerio Biscione, Guillermo Puebla, Federico Adolfi, John E. Hummel, Rachel F. Heaton, Benjamin D. Evans, Jeffrey Mitchell, and Ryan Blything. Deep Problems with Neural Network Models of Human Vision. Behavioral and Brain Sciences, pages 1\u201374, 2022. doi: 10.1017/S0140525X22002813.\\n\\nAnatoly Buchin, Sarah Rieubland, Michael H\u00e4usser, Boris S. Gutkin, and Arnd Roth. Inverse Stochastic Resonance in Cerebellar Purkinje Cells. PLoS Computational Biology, 12(8):e1005000, 2016. doi: 10.1371/journal.pcbi.1005000.\\n\\nAntonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A. Bharath. Generative adversarial networks: An overview. IEEE Signal Processing Magazine, 35(1):53\u201365, 2018. doi: 10.1109/MSP.2017.2765202.\\n\\nA. Destexhe and M. Rudolph-Lilith. Neuronal Noise. Springer US, 2012.\\n\\nA. Doerig, A. Bornet, O. H. Choung, and M. H. Herzog. Crowding reveals fundamental differences in local vs. global processing in humans and machines. Vision Research, 167:39\u201345, 2020. doi: 10.1016/j.visres.2019.12.006.\\n\\nJames Elder and Steven Zucker. The effect of contour closure on the rapid discrimination of two-dimensional shapes. Vision Research, 33(7):981\u2013991, 1993. doi: 10.1016/0042-6989(93)90080-G.\\n\\nJames H. Elder and Richard M. Goldberg. Ecological statistics of Gestalt laws for the perceptual organization of contours. Journal of Vision, 2(4):5, 2002. doi: 10.1167/2.4.5.\\n\\nMartin Engelcke, Adam R Kosiorek, Oiwi Parker Jones, and Ingmar Posner. GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations. In International Conference on Learning Representations (ICLR), 2020a.\\n\\nMartin Engelcke, Oiwi Parker Jones, and Ingmar Posner. Reconstruction Bottlenecks in Object-Centric Generative Models. ICML Workshop on Object-Oriented Learning, 2020b.\\n\\nMartin Engelcke, Oiwi Parker Jones, and Ingmar Posner. GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement. ArXiv pre-print, (arXiv:2104.09958), 2022.\\n\\nDavid J. Field, Anthony Hayes, and Robert F. Hess. Contour integration by the human visual system: Evidence for a local \u201cassociation field\u201d. Vision Research, 33(2):173\u2013193, 1993. doi: 10.1016/0042-6989(93)90156-Q.\\n\\nKarl Friston. The free-energy principle: a unified brain theory? Nature Reviews Neuroscience, 11(2):127\u2013138, 2010. doi: 10.1038/nrn2787.\\n\\nRobert Geirhos, Carlos R. M. Temme, Jonas Rauber, Heiko H. Sch\u00fctz, Matthias Bethge, and Felix A. Wichmann. Generalisation in humans and deep neural networks. Advances in Neural Information Processing Systems, 31, 2018.\"}"}
{"id": "ro4CgvfUKy", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.\\n\\nW. S. Geisler, J. S. Perry, B. J. Super, and D. P. Gallogly. Edge co-occurrence in natural images predicts contour grouping performance.\\n\\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.\\n\\nKlaus Greff, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. On the Binding Problem in Artificial Neural Networks.\\n\\nDaqing Guo, Matja\u017e Perc, Tiejun Liu, and Dezhong Yao. Functional importance of noise in neuronal information processing.\\n\\nMark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T. Freeman. Unsupervised Semantic Segmentation by Distilling Feature Correspondences.\\n\\nMichael H. Herzog, Bilge Sayim, Vitaly Chicherov, and Mauro Manassi. Crowding, grouping, and object recognition: A matter of appearance.\\n\\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-V AE: Learning Basic Visual Concepts with a Constrained Variational Framework.\\n\\nIrina Higgins, Le Chang, Victoria Langston, Demis Hassabis, Christopher Summerfield, Doris Tsao, and Matthew Botvinick. Unsupervised deep learning identifies semantic disentanglement in single inferotemporal face patch neurons.\\n\\nGeoffrey Hinton. How to Represent Part-Whole Hierarchies in a Neural Network.\\n\\nAlex O. Holcombe and Patrick Cavanagh. Early binding of feature pairs for visual perception.\\n\\nLawrence Hubert and Phipps Arabie. Comparing partitions.\\n\\nFrank J\u00e4kel, Manish Singh, Felix A. Wichmann, and Michael H. Herzog. An overview of quantitative approaches in Gestalt perception.\\n\\nGaetano Kanizsa. Organization in Vision: Essays on Gestalt Perception.\\n\\nPhilip J. Kellman and Elizabeth S. Spelke. Perception of partly occluded objects in infancy.\\n\\nR. Kimchi. Primacy of wholistic processing and global/local paradigm: a critical review.\\n\\nDiederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes.\\n\\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick. Segment Anything.\"}"}
{"id": "ro4CgvfUKy", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nKeiichi Kitajo, Daichi Nozaki, Lawrence M. Ward, and Yoshiharu Yamamoto. Behavioral Stochastic Resonance within the Human Brain. Physical Review Letters, 90(21):218103, 2003. doi: 10.1103/PhysRevLett.90.218103.\\n\\nKurt Koffka. Perception: An introduction to the Gestalt-theorie. Psychological bulletin, 19(10):531\u2013585, 1922.\\n\\nI Kov\u00e1cs and B Julesz. A closed curve is much more than an incomplete one: effect of closure in figure-ground segmentation. Proceedings of the National Academy of Sciences, 90(16):7495\u20137497, 1993. doi: 10.1073/pnas.90.16.7495.\\n\\nMark A. Kramer. Nonlinear principal component analysis using autoassociative neural networks. AIChE Journal, 37(2):233\u2013243, 1991. doi: 10.1002/aic.690370209.\\n\\nMichael Kubovy and Martin van den Berg. The whole is equal to the sum of its parts: A probabilistic model of grouping by proximity and similarity in regular patterns. Psychological Review, 115(1):131\u2013154, 2008. doi: 10.1037/0033-295X.115.1.131.\\n\\nTaeKyu Kwon, Kunal Agrawal, Yunfeng Li, and Zygmunt Pizlo. Spatially-global integration of closed, fragmented contours by finding the shortest-path in a log-polar representation. Vision Research, 126:143\u2013163, 2016. doi: 10.1016/j.visres.2015.06.007.\\n\\nGregory W. Lesher. Illusory contours: Toward a neurally based perceptual theory. Psychonomic Bulletin & Review, 2(3):279\u2013321, 1995. doi: 10.3758/BF03210970.\\n\\nDrew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre. Learning long-range spatial dependencies with horizontal gated recurrent units. Advances in Neural Information Processing Systems, 31, 2018.\\n\\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild. Proceedings of International Conference on Computer Vision (ICCV), 2015.\\n\\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-Centric Learning with Slot Attention. Advances in Neural Information Processing Systems, 33:11525\u201311538, 2020.\\n\\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully Convolutional Networks for Semantic Segmentation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\\n\\nSindy L\u00f6we, Phillip Lippe, Maja Rudolph, and Max Welling. Complex-Valued Autoencoders for Object Discovery. Transactions on Machine Learning Research, 2022.\\n\\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks. International Conference on Learning Representations, 2018.\\n\\nAmit Manwani and Christof Koch. Signal Detection in Noisy Weakly-Active Dendrites. Advances in Neural Information Processing Systems, 11, 1998.\\n\\nFrancesco Marini and Carlo A. Marzi. Gestalt Perceptual Organization of Visual Stimuli Captures Attention Automatically: Electrophysiological Evidence. Frontiers in Human Neuroscience, 10, 2016.\\n\\nJoseph Marino. Predictive coding, variational autoencoders, and biological connections. Neural Computation, 34(1):1\u201344, 2022.\\n\\nPietro Mazzaglia, Tim Verbelen, Ozan \u00c7atal, and Bart Dhoedt. The free energy principle for perception and action: A deep learning perspective. Entropy, 24(2):301, 2022. doi: 10.3390/e24020301.\\n\\nMark D. McDonnell and Lawrence M. Ward. The benefits of noise in neural systems: bridging the-ory and experiment. Nature Reviews Neuroscience, 12(7):415\u2013425, 2011. doi: 10.1038/nrn3061.\\n\\nLeland McInnes, John Healy, and James Melville. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. ArXiv pre-print, (arXiv:1802.03426), 2018.\"}"}
{"id": "ro4CgvfUKy", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nthose that are closer in proximity (Kubovy and van den Berg, 2008; Quinlan and Wilton, 1998).\\n\\nGradient Occlusion to study the ability to handle full or partial occlusion of an element without a constant color value. The basis of objecthood in humans does not depend crucially on the exact color of the elements, and long-range integration and inference about objects and how they change under occlusion is possible (Kellman and Spelke, 1983; Spelke, 1990).\\n\\nIllusory Occlusion to study grouping across the visual field while being occluded by an illusory object. Regular textures can be grouped together, and foreground objects that break the pattern can be separated from the complex textural background. A classic example is the Dalmatian Dog illusion by R. C. James, of which our Illusory Occlusion dataset is a simplification.\\n\\nTogether, the GG datasets comprise tests of a substantial number of the most major Gestalt laws of perceptual organization (for reviews, see Wagemans et al. (2012a;b); J\u00e4kel et al. (2016); Todorovic (2008); Kanizsa (1979)), including figure-ground segregation, proximity, continuity, closure, and the general notion of Good Gestalt (the tendency of the perceptual system to organize objects according to the aforementioned laws).\\n\\n2.4 EVALUATION METRICS AND BASELINE MODELS\\n\\nMetrics. We evaluate all models using the Adjusted Rand Index (ARI) metric (Hubert and Arabie, 1985), which measures the above-chance similarity between two clusterings: the target segmentation mask, and the predicted segmentation mask. Here we choose to judge our models based on the ARI metric (instead of ARI minus background) because the point of the GG datasets (and of the study of Gestalt in general) is that of object-background separation, and not of fine-grained object-specific details. We verified that in all of the GG datasets, an all-background assignment yields an ARI of \\\\( \\\\approx 0 \\\\), i.e. no better than random assignment. In addition, we plot randomly drawn examples of segmentation masks in Figure 4, and all segmentation masks for our highest-performing models in Appendix F.4, Figures 16 - 27.\\n\\nModels and control experiments. We trained 5 Autoencoder (AE) and Variational Autoencoder (VAE) models with MSE and GECO (Rezende and Viola, 2018) losses, respectively. In addition, we computed two control experiments, one for each model variant (Noisy AE/V AE Reconstruction) with the goal of verifying whether adding noise at the latent layer is important. These control experiments compute a clustering on the AE/V AE reconstruction pixel values, adding noise to the reconstruction instead of the latent layer, keeping the rest of the model architecture and methodology the same. Implementation details and model architectures are shown in Appendix E.2, and reconstructions are shown in Appendix 14. We also evaluate two baseline models, Genesis (Engelcke et al., 2020a) and Genesis-v2 (Engelcke et al., 2022), for which details are shown in Appendix C.\\n\\n3 EXPERIMENTAL RESULTS\\n\\n| Model          | Kanizsa Closure | Continuity | Proximity | Gradient Oc. | Illusory Oc. |\\n|----------------|-----------------|------------|-----------|--------------|--------------|\\n| AE             | 0.859 \u00b1 0.008   | 0.766 \u00b1 0.008 | 0.552 \u00b1 0.008 | 0.996* \u00b1 0.001 | 0.926 \u00b1 0.009 |\\n| VAE            | 0.871 \u00b1 0.005   | 0.795 \u00b1 0.009 | 0.593 \u00b1 0.012 | 0.943 \u00b1 0.010 | 0.918 \u00b1 0.002 |\\n| AE Rec.        | 0.246 \u00b1 0.006   | 0.064 \u00b1 0.001 | 0.570 \u00b1 0.006 | 0.141 \u00b1 0.001 | 0.982 \u00b1 0.000 |\\n| VAE Rec.       | 0.343 \u00b1 0.004   | 0.084 \u00b1 0.000 | 0.611 \u00b1 0.005 | 0.144 \u00b1 0.001 | 0.977 \u00b1 0.001 |\\n| Genesis        | 0.346           | 0.755       | 0.394      | 0.879         | 0.933        |\\n| Genesis-v2     | 0.415           | 0.000       | 0.399      | 0.059         | 0.000        |\\n\\nTable 1: Model results (ARI \u00b1 standard error of the mean). AE (Autoencoder) and VAE (Variational Autoencoder) model scores reported for the best noise level and number of samples. AE Rec. and VAE Rec. stand for noisy reconstruction control experiments, and (see Section 2.4 for details).\\n\\nBold indicates top-performing model. For the AE and the VAE, a star (*) indicates a statistically significant difference in model performance (Bonferroni corrected).\"}"}
{"id": "ro4CgvfUKy", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Both the AE and V AE models achieve good quantitative performance in general perhaps with the exception of the Continuity dataset (Table 2). Adding noise in latent space as opposed to image space unlocks a substantial improvement in segmentation performance, improving ARI by 0.506 on average across all GG datasets. As described in Section 3.2, we do not see the somewhat lower quantitative score as a failure per se, but rather as related to the way in which the models fundamentally segment using LNS. Since the segmentation process seeks to vary the latent representation of the models, the segmentation process introduces artifacts related to the model's representational space, such as small shifts of the stimulus in a given direction, or small changes to its size. The ARI metric is punishing to even single-pixel shifts of the target object \u2014 qualitatively, it is clear that even in cases where the model does not achieve an excellent ARI score, it captures the desired effect. This is in contrast to the Genesis and Genesis-v2 models, which fail at capturing the desired effect even when they do well quantitatively. A similar effect of minor performance deterioration with respect to the baseline of clustering a reconstruction can be observed in the Gradient Occlusion dataset, where good reconstruction quality of the model is congruent with a good segmentation performance under output clustering. This is to be expected - the dataset by design is such that the pixel values are mostly informative of the correct segmentation label identities.\\n\\nTo understand whether a structured latent prior could improve model segmentation capability under LNS, we conducted a two-sample two-tailed t-test to compare the AE and V AE (\u03b2 > 1) performance. The test indicated that the AE and V AE performance statistically significantly differed only for the Proximity (t = 4.54, p < 0.05) and Illusory Occlusion datasets (t = 5.22, p < 0.05) after the Bonferroni correction, while no statistically significant difference was found for the other datasets. Due to the additional loss term that places tension on the quality of the reconstruction, it is not surprising that the V AE would show small decreases in performance \u2014 indeed, we conclude that encouraging a structured latent prior on the model does not in general improve the model's capability to segment stimuli.\\n\\nFigure 4: Model Output Segmentation Mask Examples. The first row shows inputs, the second row shows V AE segmentation masks, and the third row shows AE segmentation masks. Randomly selected examples from the Kanizsa Squares, Closure, Continuity, Proximity, Gradient Occlusion, and Illusory Occlusion datasets are shown using the best model hyperparameters shown in Table 2. Since the model segments by clustering noisy outputs, the specific color assignment to different object identities is arbitrary (for example, whether the model assigns the identity represented by yellow as the background, or purple, is not a meaningful distinction).\\n\\n3.2 Qualitative Evaluation\\nTo understand how the models perform qualitatively, we plot a random selection of examples in Figure 4, and all model outputs of the shown seed for all datasets in Appendix F.4. Important are two things: first, while model outputs are consistently good and generally reflect appropriate grouping of the correct object elements in the correct groups, segmenting by noise causes noisy artifacts in...\"}"}
{"id": "ro4CgvfUKy", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"some cases (e.g. Figure 4: Continuity, Gradient Occlusion). Second, a qualitative inspection reveals how quantitative metrics fail to appropriately judge the goodness of model performance. Since segmentation is done by inference over a noisy set of outputs, and the noise affects the representation of those outputs, the outputs in some datasets (like Continuity and Closure) are often shifted or misshaped by a small number of pixels, resulting in a disproportionately large drop in quantitative performance, while qualitative performance remains relatively unchanged.\\n\\nTo further understand how elements are grouped in our model, we conducted two additional qualitative analyses: (1) Using UMAP visualization, we inspected model clustering and found that while the AE does not find the correct object binding, the V AE does (Appendix F.1). (2) To understand how well the segmentation method generalizes, we qualitatively evaluated the models on the CelebA dataset (Liu et al., 2015) and found that while the AE generally fails to find a semantically meaningful segmentation mask (and instead opts for more basic image-level features), the V AE often finds a semantically meaningful segmentation of face-hair-background (Appendix B).\\n\\n3.3 NOISE SENSITIVITY\\n\\nFigure 5: The optimal noise level for the AE and the V AE across datasets. The AE performs best with a larger range of noise values, while the V AE consistently prefers low noise values.\\n\\nTo understand how practically feasible LNS is for a system constrained by compute time or noise magnitude, we analyzed how the amount of noise added in latent space would affect the model's segmentation performance. While enforcing a prior distribution on the model's latent space (as is the case in the V AE) had no benefit in model performance (and perhaps a slight deterioration), it may have had a minor benefit in the across-task best-performing noise consistency. We tested the same models across different levels of noise used for segmentation and found that enforcing the prior \\\\( N(0, I) \\\\) caused the model to consistently perform best at very small levels of noise, while the AE optimal level of noise varied across datasets (Figure 5). This is intuitively not surprising, since the V AE enforces a specific activity magnitude for uninformative units in its prior \\\\( N(0, I) \\\\), while the AE does not \u2014 as such, the AE has no reason to prefer a specific coding magnitude in any given task. To quantify the result, we performed an F-test for the equality of variances, and found that the level of noise that yielded the best results had higher variance for the AE than the V AE \\\\((F = 2994.56, p = 1.11 \\\\times 10^{-8})\\\\). The result suggests that enforcing a prior distribution on the representation may have an advantage in stabilizing the amount of noise needed to best segment stimuli across a variety of tasks. That being said, the absolute performance of the AE model did not substantially decrease as a function of noise (Figure 6 left), which means that it is possible that even the AE could yield meaningful segmentation results using a consistently low magnitude of noise.\\n\\n3.4 NUMBER OF TIME STEPS NEEDED\\n\\nWe evaluated model performance as a function of the number of time steps (in other words, the number of samples collected from the model) used for segmentation (Figure 6, right). We found that for an appropriate level of noise used in segmentation, both the AE and the V AE models' performance asymptotes quickly, with as few as a handful of time steps. Indeed, reducing the number of time steps used from 80 (Figure 6 left) down to as few as 5 barely reduces segmentation accuracy (Figure 6 middle).\\n\\n4 CONCLUSION AND DISCUSSION\\n\\nWe present Latent Noise Segmentation (LNS) that allows us to turn a deep neural network that is trained on a generic image reconstruction task into a segmentation algorithm without training any additional parameters, using only independent neural noise. The intuition behind why independent neural noise works for segmentation is that independent noise affects neurons that are selective to a presented stimulus more than those that are not. This idea is not entirely new \u2014 noise has...\"}"}
{"id": "ro4CgvfUKy", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2024\\n\\nFigure 6: Sensitivity to Noise Magnitude and Time Steps. Left: AE and VAE segmentation performance as a function of noise standard deviation, evaluated with 80 time steps. Middle: AE and VAE segmentation performance as a function of noise standard deviation, evaluated with 5 time steps. Right: AE and VAE segmentation performance as a function of the number of samples for a fixed noise $\\\\sigma = 0.001$. Note the change of y-axis scale. Shaded areas show 75th and 25th percentiles over all evaluated segmentation masks.\\n\\nbeen shown to theoretically be able to enhance the signal of a stimulus when the signal is small (Benzi et al., 1981; Kitajo et al., 2003; Buchin et al., 2016); however, its application to segmentation and perceptual grouping, to the best of our knowledge, is novel.\\n\\nImportantly, our goal is not to propose a state of the art segmentation algorithm, but rather to understand the role of neural noise and how it can be applied to achieve segmentation and perceptual grouping. We implement LNS and find that it is possible to make pre-trained Autoencoders (AE) and Variational Autoencoders (VAE) segment images. We present the Good Gestalt (GG) datasets\u2014a collection of datasets intended to investigate perceptual organization in humans\u2014and show that LNS is able to explain the formation of 'Good Gestalt' in a neural network model. Our findings give a unified high-level account of the result of many perceptual grouping phenomena in humans: 1) learning encodes an approximation of the training dataset distribution, and 2) neural noise reveals the learned representation in out-of-distribution samples, resulting in certain hallmarks of perception, like effects of illusory contours, closure, continuity, proximity and occlusion. Establishing this link between training data (Appendix E.1), neural noise, and human perception is an important and timely step towards better models of visual representation (Bowers et al., 2022).\\n\\nInterestingly, model learning rule plays a role in segmentation. We show that a very small amount of independent noise is optimal for segmentation in the case of the VAE, but not for the AE. Similarly, the VAE finds the correct number of objects in the image (Appendix F.1) even when the AE does not. This deviation between the VAE and the AE is interesting, as the VAE has connections with predictive coding (Marino, 2022; Boutin et al., 2020) and the free energy principle (Friston, 2010; Mazzaglia et al., 2022), and has been shown empirical support as a coding principle in primate cortex (Higgins et al., 2021). With both learning rules, segmentation performance asymptotes quickly, suggesting that the time scale of the segmentation in our models is practically feasible for constrained systems.\\n\\nThese results suggest a number of interesting questions for future research. First, in this work our focus has been on revealing a potential mechanism for perceptual grouping and segmentation. While we have demonstrated that the approach partially scales to the case of face segmentation (Appendix B), a relevant question is whether it is possible to scale the approach to improve unsupervised segmentation state-of-the-art (Bear et al., 2023; Engelcke et al., 2022). Second, while we have targeted a large collection of most relevant grouping principles, there are many that we have not tested here. As we have shown for the datasets we have tested, we speculate that most or all phenomena in Gestalt perception can be reproduced as a result of a combination of past experience (learning) and noise in the perceptual system. Of interest is also understanding how LNS interacts with perception under a motion stream, which we have not considered here. Finally, while we do not make any claims about the primate visual system, our results suggest a potential benefit of independent noise in the visual system. Understanding how noise interacts with perception in primate cortex is an active area of research (Destexhe and Rudolph-Lilith, 2012; Miller and Troyer, 2002; Stein et al., 2005; Guo et al., 2018; McDonnell and Ward, 2011), and we hope our results stimulate discussion about the potential benefits of noise for perceptual systems in general.\"}"}
{"id": "ro4CgvfUKy", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the following, we will explain why both cases cannot hold with probability 1 when \\\\( p_1 \\\\) and \\\\( p_2 \\\\) are pixels belonging to different objects of the image, which means that they have independent pixel values in the training dataset (see Appendix E.1 for more details).\\n\\n(1) Why is the first case unlikely to hold?\\n\\nThe argument of the first case boils down to a characterization of the Jacobian in equation 3. A complete theoretical understanding of the Jacobian has yet to be established in the past literature to the best of our knowledge. However, from the conclusions derived from Rol\u00ednek et al. (2019) and Zietlow et al. (2021), which we briefly mentioned in Appendix A.1.1, the first case is unlikely to happen as it intuitively goes against the behavior of PCA.\\n\\nIn more detail, as demonstrated in Zietlow et al. (2021), the columns of the Jacobian (equation 3) should point into directions in the \\\\( D \\\\)-dimensional space that are principal components extracted from a compact cluster of training images in the \\\\( D \\\\)-dimensional space. In addition, equation 7 implies that all the principal components characterize the pixel values (in each channel) of \\\\( p_1 \\\\) and \\\\( p_2 \\\\) to have a strict linear relationship with the same ratio \\\\( r_{p_1 p_2} \\\\). However, as introduced in Appendix E.1, the training dataset is generated by assuming the pixel values of \\\\( p_1 \\\\) and \\\\( p_2 \\\\) (when they do not belong to the same object) are independent. Thus, it is unlikely to have all local PC directions satisfying equation 7.\\n\\nFor clarity, we provide a diagram in the left panel of Figure 7. Here we project the entire training dataset (denoted by the gray dots) to a 2-dimensional space. Each dimension stands for the pixel value of \\\\( p_1 \\\\) or \\\\( p_2 \\\\). The grey dots are arranged in a way to demonstrate the independence of the two pixel values. The orange arrow denotes a potentially learned local PC direction, moving along which the variation of the two pixel values follows a strict linear ratio. Since there are data variances in other directions, it is highly unlikely that all the local PC directions learned from local data points only capture directions of such kind.\\n\\nOn the other hand, we can also develop intuition into why any two pixels belonging to the same object can be inseparable with the right panel of Figure 7. If \\\\( p_1 \\\\) and \\\\( p_2 \\\\) are pixels belonging to the same object, the way the dataset is generated (discussed in Appendix E.2) dictates that the training samples projected onto the pixel value space will only stretch in a direction where there exists a strict linear ratio between the values of the two objects. In this case, the learned local PC directions are highly likely to point only in the direction on this \\\\( 2 \\\\)-dimensional plane, as there exists no data variance along other directions. Such learned local PC directions guarantee that the two pixels of the same object will not be separated in the segmentation process, for it implies the Jacobian satisfies the condition described in equation 7 in the first case.\\n\\n(2) Why can the second case not hold with probability 1?\\n\\nTo understand this case, we first rewrite equation 6 as:\"}"}
{"id": "ro4CgvfUKy", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ \\\\frac{d}{dx} \\\\text{Dec}(z) = \\\\mu(x_i - \\\\mathbf{r}_{p_1, p_2}) \\\\]\\n\\n\\\\[ \\\\frac{d}{dx} \\\\text{Dec}(z) = \\\\mu(x_i - \\\\delta_{n,j}) \\\\]\\n\\n\\\\[ C_{c,j}(r_{p_1, p_2}) \\\\delta_{n,j} = 0 \\\\]\\n\\nIn which \\\\( C_{c,j}(r_{p_1, p_2}) \\\\) is used to summarize the term in the square bracket \\\\[ \\\\cdot \\\\]. As we are considering the second case, we know that \\\\( C_{c,j}(r_{p_1, p_2}) \\\\neq 0 \\\\) for at least one \\\\( j \\\\in \\\\{1, 2, \\\\ldots, d\\\\} \\\\) and one channel \\\\( c \\\\).\\n\\nThus, in the second case, the condition for inseparability can be interpreted as:\\n\\n\\\\[ \\\\delta_{n} = (\\\\delta_{n,1}, \\\\delta_{n,2}, \\\\ldots, \\\\delta_{n,d})^\\\\top \\\\in \\\\mathbb{R}^d \\\\]\\n\\nlies in a hyperplane as described by equation 8, and the hyperplane has less than \\\\( d \\\\) dimensions. Such an event cannot happen with probability 1. This is because \\\\( \\\\delta_{n} \\\\) has a Gaussian distribution in the \\\\( d \\\\)-dimensional space, implying its probability mass on a hyperplane with lower dimensions is 0.\\n\\n### A.1.3 Why Probability Implies Inseparability\\n\\nHere, we give a formal derivation of why the condition for inseparability must hold with probability 1 to make \\\\( p_1 \\\\) and \\\\( p_2 \\\\) inseparable.\\n\\nTo simplify notation, let us denote the event that equation 6 holds for some time step \\\\( n \\\\) some channel \\\\( c \\\\) and some positive constant \\\\( r_{p_1, p_2} \\\\) to be \\\\( A_{n,c}(r_{p_1, p_2}) \\\\). It would require \\\\( A_{n,c}(r_{p_1, p_2}) \\\\) to hold for all \\\\( n \\\\in \\\\{1, 2, \\\\ldots, N - 1\\\\} \\\\) and all \\\\( c \\\\in \\\\{1, \\\\ldots, C\\\\} \\\\) to make \\\\( p_1 \\\\) and \\\\( p_2 \\\\) inseparable, otherwise \\\\( \\\\Delta \\\\tilde{X}_{i, p_1} \\\\) and \\\\( \\\\Delta \\\\tilde{X}_{i, p_2} \\\\), both belonging to \\\\( \\\\mathbb{R}^{(N - 1) \\\\times C} \\\\), do not point into the same direction, making the two pixels separable.\\n\\nIntuitively, if the condition for inseparability does not hold with probability 1, then the events \\\\( \\\\{A_{n,c}(r_{p_1, p_2})\\\\}_{n \\\\in \\\\{1, 2, \\\\ldots, N - 1\\\\}} \\\\) are less and less likely to happen simultaneously as \\\\( N \\\\) increases, meaning \\\\( p_1 \\\\) and \\\\( p_2 \\\\) are always separable given \\\\( N \\\\) sufficiently large. Here below, we characterize this intuition rigorously.\\n\\nNote that \\\\( A_{n,c}(r_{p_1, p_2}) \\\\) and \\\\( A_{n+1,c}(r_{p_1, p_2}) \\\\) may be dependent on each other, as we can show that \\\\( \\\\delta_{n} \\\\) and \\\\( \\\\delta_{n+1} \\\\) are dependent of each other. \\\\( \\\\delta_{n} \\\\) involves the noise pair of \\\\( (\\\\xi_n, \\\\xi_{n+1}) \\\\) and \\\\( \\\\delta_{n+1} \\\\) involves the noise pair of \\\\( (\\\\xi_{n+1}, \\\\xi_{n+2}) \\\\). Both are affected by \\\\( \\\\xi_{n+1} \\\\) and are thus dependent (a more formal explanation of which will be given in the last part of this section). Nonetheless, we have that \\\\( A_{n,c}(r_{p_1, p_2}) \\\\) and \\\\( A_{n+2,c}(r_{p_1, p_2}) \\\\) are independent of each other, since the former concerns the noise pair of \\\\( (\\\\xi_n, \\\\xi_{n+1}) \\\\) and the latter concerns the noise pair of \\\\( (\\\\xi_{n+2}, \\\\xi_{n+3}) \\\\), which are unaffected by each other.\\n\\nThus, suppose we arbitrarily choose a channel \\\\( c_0 \\\\):\\n\\n\\\\[ P(\\\\text{inseparability}) \\\\leq P(A_{1,c_0}(r_{p_1, p_2}), A_{3,c_0}(r_{p_1, p_2}), \\\\ldots, A_{N-1,c_0}(r_{p_1, p_2})) \\\\]\\n\\n\\\\[ = \\\\sum_{i=1}^{N-1} P(A_{2i-1,c_0}(r_{p_1, p_2})) \\\\]\\n\\nThe independence between \\\\( A_{n,c_0}(r_{p_1, p_2}) \\\\) and \\\\( A_{n+2,c_0}(r_{p_1, p_2}) \\\\) enables the fully factorized representation in equation 9c. Note that every probability term in equation 9c has the same value, and if they are all smaller than 1 (for all \\\\( r_{p_1, p_2} > 0 \\\\)), the product will decay to zero exponentially (for all \\\\( r_{p_1, p_2} > 0 \\\\)). Thus, we require equation 6 to happen with the probability 1 to achieve inseparability.\\n\\nThe last part of this section demonstrates why \\\\( \\\\delta_{n} \\\\) and \\\\( \\\\delta_{n+1} \\\\) are not independent. If \\\\( \\\\delta_{n} \\\\) and \\\\( \\\\delta_{n+1} \\\\) are independent, then we must have that the variance of their sum should be equal to the sum of their variances. However,\\n\\n\\\\[ \\\\text{Var} (\\\\delta_{n+1} + \\\\delta_{n}) = \\\\text{Var} (\\\\xi_{n+2} - \\\\xi_{n+1} + \\\\xi_{n+1} - \\\\xi_{n}) = \\\\text{Var} (\\\\xi_{n+2} - \\\\xi_{n}) = 2 \\\\sigma^2 \\\\]\\n\\n\\\\[ \\\\neq 4 \\\\sigma^2 = \\\\text{Var} (\\\\xi_{n+2} - \\\\xi_{n+1}) + \\\\text{Var} (\\\\xi_{n+1} - \\\\xi_{n}) = \\\\text{Var} (\\\\delta_{n+1}) + \\\\text{Var} (\\\\delta_{n}) \\\\]\\n\\nresulting in a contradiction. With slight abuse of notation, all the terms in equation 10 are \\\\( d \\\\)-dimensional vectors, and all the operations are component-wise.\\n\\n### A.1.4 Discussion\\n\\nWith the above, we demonstrated how the underlying local PCA enables VAE to segment the images as we desired. In our experiments, we also show that the Autoencoders can also manage to segment...\"}"}
{"id": "ro4CgvfUKy", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the images similarly, but that their preferred number of objects is different to the VAEs'. Indeed, qualitatively, the Autoencoder generalizes worse than the VAE (Appendix B). Previous works have drawn a link between PCA and Linear Autoencoders (Plaut, 2018). However, a full theoretical account for the connection between PCA and the more complicated non-linear autoencoder to the best of our knowledge does not yet exist. With the intuitions provided in the experimental observation in this paper, we are at a better place to formulate the connection, which we consider as a meaningful future direction. Furthermore, we have provided a mathematical link between perception in humans, representation in Deep Neural Networks, and semantic segmentation. We hope our work encourages future work in forming rigorous links between these established fields.\\n\\nA.1.5 Applicability to Other Generative Algorithms\\n\\nIn this work, we have focused primarily on the Variational Autoencoder (VAE) in lieu of other generative models, such as Diffusion models (Sohl-Dickstein et al., 2015; Rombach et al., 2022) or Adversarial Generative (Goodfellow et al., 2014; Creswell et al., 2018) models. There are two primary reasons behind this choice: 1) The VAE formulation allows us to neatly follow the argument in Zietlow et al. (2021) and Rol\u00edn et al. (2019) to arrive at a more firm understanding of why Latent Noise Segmentation (LNS) works; 2) the mathematical and computational links between the VAE formulation and other biologically plausible coding and learning schemes such as predictive coding (Boutin et al., 2020; Marino, 2022) and the Free Energy Principle (Friston, 2010; Mazzaglia et al., 2022) are clearer than for other generative algorithms to the best of our knowledge.\\n\\nThat being said, the fact that the Autoencoder works relatively well for segmentation using LNS is interesting, and raises the question of whether other model architectures could also perform well. We suggest that future work could empirically evaluate LNS on other model classes, such as Latent Diffusion models (Rombach et al., 2022) to understand whether the mathematical intuition behind why LNS works also generalizes beyond the simple Autoencoder model class to any model class that learns meaningful representations about images.\"}"}
{"id": "ro4CgvfUKy", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To test the generalization and potential scalability of our models, we evaluated them qualitatively on the CelebA dataset (Liu et al., 2015). The dataset consists of a large variety of faces in different contexts, lighting conditions, and backgrounds. We trained the model on the CelebA training dataset as provided, and tested on the first 100 samples of the testing dataset.\\n\\nWe found that the VAE model successfully finds a desired face-hair-background segmentation mask relatively often, when prompted on 3 output clusters (Figure 8). In some cases the clustering is coherent but not semantically aligned with the desired output \u2014 we suspect that in the CelebA dataset, lighting condition forms one of the strongest local PC components, and that this leads to a semantically less meaningful cluster on occasion. This effect is seen more strongly in the case of the AE, which appears to primarily follow a lighting-related clustering (Figure 9).\\n\\nFigure 8: All tested Variational Autoencoder outputs for a single seed of the CelebA dataset. Inputs and outputs are displayed as horizontal pairs, with the input being the left image, and the model output being the right image.\\n\\nFigure 9: All tested Autoencoder outputs for a single seed of the CelebA dataset. Inputs and outputs are displayed as horizontal pairs, with the input being the left image, and the model output being the right image.\"}"}
