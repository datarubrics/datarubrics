{"id": "zAxuIJLb38", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 4: Varying the learning rate for unlearning the GPT-N\\\\textsubscript{EO} 1.3B with $s = 32$. We report the average of 3 random samplings and display the standard deviations as the shaded regions. Red dotted lines denote the memorization accuracy forgetting threshold of the 1.3B model reported in Table 1.\\n\\nWe show the FLOPs of pretraining OPT denoted as \\\\textsc{duplication} and the average FLOPs of performing knowledge unlearning until $s = 32$ token sequences reach the Forgetting Threshold denoted as \\\\textsc{unlearning} in Table 8. We calculate FLOPs by $(6 \\\\times \\\\text{Total Training Tokens} \\\\times \\\\text{Parameter Size})$ following Brown et al. (2020).\\n\\nD\\\\textsc{VARYING THE LEARNING RATE}\\n\\nIn Figure 4, we show the results of varying the learning rate for knowledge unlearning where we fix the total epoch to 10 and perform 3 random runs with $s = 32$ on the GPT-N\\\\textsubscript{EO} 1.3B. Overall, we observe that higher learning rates lead to faster forgetting, but with substantial LM performance degradation. While lower learning rates retain the LM performance, they fail to meet the Forgetting Threshold within 10 epochs. Thus, we set the learning rate to $5e-5$ for our experiments to get the best trade-off.\"}"}
{"id": "zAxuIJLb38", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"We show how the EL$_{10}$ of each individual chunks and the average LM performance change as we perform sequential unlearning in Figure 5. Results show that the chunks that are forgotten stay forgotten and that later chunks are forgotten much faster (one or two epochs) compared to the initial chunk. We hypothesize that this might be because of the similarity of the token sequences from the 15,000 examples from the Training Extraction Challenge Benchmark. Also, this result hints at the generalization of unlearning, which we do not further explore because of the scope of this work.\\n\\nFigure 5: Additional results of sequential unlearning for GPT-N 125M, 1.3B, and 2.7B. Red dotted lines denote the memorization accuracy forgetting threshold reported of each model in Table 1.\\n\\nWe show an example token sequence from each of the 8 domains used for the analysis section in Table 9.\\n\\nIn addition to the extraction attack example shown in the analysis section, we provide 3 additional examples to provide readers with more empirical examples of how knowledge unlearning ensures protection against extraction attacks in Table 10.\\n\\nFirst we show the Extraction Liklihood (EL) Forgetting Threshold values for $n=\\\\{5, 10, 20, 40\\\\}$ by measuring the value on the 10,000 validation instances unseen during training in Table 11. Next, we show the average LM performance (on the 9 classification benchmarks) where we perform unlearning on the LM on 32 samples until the target token sequences are forgotten (the EL MA value 15).\"}"}
{"id": "zAxuIJLb38", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023 are both lower than the threshold values) in Table 12. Performance shows the average of 5 random samplings.\\n\\n**Limitations**\\n\\nWhile we provide privacy guarantee through unlearning, our Forgetting Threshold is dependent on which data samples are chosen as $D'$. Furthermore, varying the prefix length can be seen as a na\u00efve way of varying the strength of the extraction attacks. In a real-world scenario, extraction attacks may be more complicated and may require other prevention methods. Also, we could not directly compare our approach with a Differential Privacy (DP) (Anil et al., 2021) approach because there are no open-sourced LMs pretrained with a DP algorithm. We could not replicate the pretraining phase because of the heavy computational resources needed to pretrain an LM with DP which is estimated to require thousands of GPU hours. We leave this comparison for future work. Finally, a recent work (Carlini et al., 2022b) has suggested that machine unlearning (for the vision domain) can bring negative effects harming the privacy of other users. Future work should explore this phenomenon in the setting of performing unlearning on large language models as well.\"}"}
{"id": "zAxuIJLb38", "page_num": 17, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "zAxuIJLb38", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for language models has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM. We propose knowledge unlearning as an alternative method to reduce privacy risks for LMs post hoc. We show that simply applying the unlikelihood training objective to target token sequences is effective at forgetting them with little to no degradation of general language modeling performances for larger LMs; it sometimes even substantially improves the underlying LM with just a few iterations. We also find that sequential unlearning is better than trying to unlearn all the data at once and that unlearning is highly dependent on which kind of data (domain) is forgotten. By showing comparisons with a previous data preprocessing method and decoding method known to mitigate privacy risks for LMs, we show that unlearning can give a strong empirical privacy guarantee in scenarios where the data vulnerable to extraction attacks are known a priori while being orders of magnitude more computationally efficient and robust. We release the code and dataset needed to replicate our results at http://www.omitted.link/.\\n\\n1 INTRODUCTION\\nRecent work has shown that an adversary can extract training data from Pretrained Language Models (LMs) including Personally Identifiable Information (PII) such as names, phone numbers, and email addresses, and other information such as licensed code, private clinical notes, and 128-bit UUIDs (Carlini et al., 2021; Lee et al., 2022; Huang et al., 2022; Lehman et al., 2021). In 2021, an AI chatbot Iruda became the first AI system to be sued for violating the Personal Information Protection Act after generating the exact home addresses and bank account numbers of actual individuals unintentionally (Park, 2021). Heikkil\u00e4 (2022) has also shown that GPT-3 (Brown et al., 2020), one of the most well known LM currently in commercial use, offered detailed private information about the Editor-in-Chief of MIT Technology Review including his family members, work address, and phone number. Considering findings that show extracting training data gets easier as LMs scale to larger sizes (Carlini et al., 2022a) and that it is common practice for practitioners to release billion parameter pretrained LMs for public use (Gao et al., 2020; Black et al., 2021; Zhang et al., 2022), it has become important to provide privacy guarantees for large LMs.\\n\\nPractitioners are required to delete personal information from the LMs by individuals' request because each individual has the \\\"Right To Be Forgotten (RTBF)\\\" (Mantelero, 2013; Graves et al., 2021) and can limit the direct and indirect commercial use of their personal information (Villaronga et al., 2018). Previous methods addressing privacy risks for language models attempt to remove all private information from the training data (data preprocessing) (Aura et al., 2006; Dernoncourt et al., 2017; Lison et al., 2021; Kandpal et al., 2022) or attempt to design algorithms that ensure differential privacy (DP) (Dwork, 2008; Dwork et al., 2006; Abadi et al., 2016; Anil et al., 2021; Li et al., 2022; Yu et al., 2022). Both approaches require retraining the underlying LM every time individuals want to practice their RTBF, which makes them inadequate for large LMs that are extremely costly to retrain. Furthermore, as pointed out by Brown et al. (2022), data preprocessing methods assume private information to be easily identifiable, specified, and removed and DP algorithms can only...\"}"}
{"id": "zAxuIJLb38", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model (Size) | LM Avg. (Acc) |\\n|-------------|--------------|\\n| EL 5        | 49.93        |\\n| EL 10       | 49.93        |\\n| EL 20       | 49.85        |\\n| EL 40       | 49.88        |\"}"}
{"id": "zAxuIJLb38", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pp. 308\u2013318, 2016.\\n\\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2357\u20132367, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1245. URL https://aclanthology.org/N19-1245.\\n\\nRohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. Large-scale differentially private bert. arXiv preprint arXiv:2108.01624, 2021.\\n\\nTuomas Aura, Thomas A Kuhn, and Michael Roe. Scanning electronic documents for personally identifiable information. In Proceedings of the 5th ACM workshop on Privacy in electronic society, pp. 41\u201350, 2006.\\n\\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 7432\u20137439, 2020.\\n\\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow. If you use this software, please cite it using these metadata, 58, 2021.\\n\\nLucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP), pp. 141\u2013159. IEEE, 2021.\\n\\nHannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, and Florian Tram`er. What does it mean for a language model to preserve privacy? arXiv preprint arXiv:2202.05520, 2022.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\nYinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In 2015 IEEE Symposium on Security and Privacy, pp. 463\u2013480. IEEE, 2015.\\n\\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pp. 2633\u20132650, 2021.\\n\\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646, 2022a.\\n\\nNicholas Carlini, Matthew Jagielski, Chiyuan Zhang, Nicolas Papernot, Andreas Terzis, and Florian Tramer. The privacy onion effect: Memorization is relative. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022b. URL https://openreview.net/forum?id=ErUlLrGaVEU.\\n\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018.\\n\\nFranck Dernoncourt, Ji Young Lee, Ozlem Uzuner, and Peter Szolovits. De-identification of patient notes with recurrent neural networks. Journal of the American Medical Informatics Association, 24(3):596\u2013606, 2017.\"}"}
{"id": "zAxuIJLb38", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "zAxuIJLb38", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume, Tomas Kocisky, Sebastian Ruder, et al. Mind the gap: Assessing temporal generalization in neural language models. Advances in Neural Information Processing Systems, 34:29348\u201329363, 2021.\\n\\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8424\u20138445, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.577. URL https://aclanthology.org/2022.acl-long.577.\\n\\nEric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, and Byron C. Wallace. Does bert pre-trained on clinical notes reveal sensitive data? In NAACL-HLT, pp. 946\u2013959, 2021. URL https://doi.org/10.18653/v1/2021.naacl-main.73.\\n\\nXuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong differentially private learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=bVuP3ltATMz.\\n\\nPierre Lison, Ildik\u00f3 Pil\u00e1n, David Sanchez, Montserrat Batet, and Lilja \u00d8vrelid. Anonymisation models for text data: State of the art, challenges and future directions. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4188\u20134203, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.323. URL https://aclanthology.org/2021.acl-long.323.\\n\\nJimit Majmudar, Christophe Dupuy, Charith Peris, Sami Smaili, Rahul Gupta, and Richard Zemel. Differentially private decoding in large language models. arXiv preprint arXiv:2205.13621, 2022.\\n\\nAlessandro Mantelero. The eu proposal for a general data protection regulation and the roots of the 'right to be forgotten'. Computer Law & Security Review, 29(3):229\u2013235, 2013.\\n\\nRonak Mehta, Sourav Pal, Vikas Singh, and Sathya N Ravi. Deep unlearning via randomized conditionally independent Hessians. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10422\u201310431, 2022.\\n\\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1525\u20131534, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https://aclanthology.org/P16-1144.\\n\\nJasmine Park. South korea: The first case where the personal information protection act was applied to an ai system, May 2021. URL shorturl.at/bfOP2.\\n\\nFabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? In EMNLP, 2019.\\n\\nHannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. Towards empathetic open-domain conversation models: A new benchmark and dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 5370\u20135381, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1534. URL https://aclanthology.org/P19-1534.\\n\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.\\n\\nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pp. 3\u201318. IEEE, 2017.\"}"}
{"id": "zAxuIJLb38", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We provide all of the results for the 5 random samplings for our main experimental setting in Table 4 and the full results for the domain analysis setting in Table 5. We also provide the evaluation of the 4 dialogue tasks for $s = 32$ for all model sizes in Table 6.\\n\\nTable 7 shows the results of measuring perplexity on 500 samples from the validation set of Pile and Wikitext corpora on the LMs from the main experimental setting (Table 2). Results show that LMs that underwent knowledge unlearning show higher perplexity while the main experimental table (Table 2) does not show degradation of performance on 9 different LM benchmarks. We believe the discrepancy to be due to the inherent attributes of performing unlearning: since we are doing gradient ascent, we are likely softening the probability to generate each token from the vocabulary, giving it a more uniform distribution that will inevitably result in a higher perplexity. However, since it does not show much degradations in the LM benchmarks, it also means that the argmax of the most likely token to be generated has not changed much. However, further exploration of what exactly knowledge unlearning does to the representations of the LM should be done in future work.\"}"}
{"id": "zAxuIJLb38", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To this end, we propose knowledge unlearning (Figure 1) as an efficient solution that can be applied with just a few parameter updates instead of pretraining the underlying LM again. We perform experiments on GPT-Neo LMs (125M, 1.3B, 2.7B) (Black et al., 2021) and show that simply changing the gradient descent to the opposite direction during language modeling (which can also be seen as maximizing instead of minimizing the loss function) is effective at protecting target sequences from extraction attacks with little to no performance degradation on the initial LM capabilities measured via 9 common NLP classification benchmarks (Hellaswag (Zellers et al., 2019), Lambada (Paperno et al., 2016), Winogrande (Sakaguchi et al., 2021), COPA (Gordon et al., 2012), ARC-Easy (Clark et al., 2018), ARC-Challenge (Clark et al., 2018), Piqa (Bisk et al., 2020), MathQA (Amini et al., 2019), and PubmedQA (Jin et al., 2019)) and 4 dialogue tasks (Wizard of Wikipedia (Dinan et al., 2019), Empathetic Dialogues (Rashkin et al., 2019), Blended Skill Talk (Smith et al., 2020), and Wizard of Internet (Komeili et al., 2022)). For some cases, knowledge unlearning surprisingly shows significant improvements in LM performance for some of the benchmarks.\\n\\nWe compare our approach with data deduplication method (Kandpal et al., 2022) and differential privacy decoding method (Majmudar et al., 2022) which are both known to mitigate privacy risks, and show the effectiveness of knowledge unlearning by providing a strong privacy protection while being much more efficient and robust. We also provide a general guideline that can be used to quantify the memorization and extraction likelihood of target token sequences and suggest when we can empirically consider them to have been \\\"forgotten\\\". Specifically, we introduce a novel metric that measures the extraction likelihood by varying the prefix length of the target token sequence and quantifying how much of the suffix is actually extracted from the LM.\\n\\nSurprisingly, for knowledge unlearning, we find that it is easier to forget a chunk of instances sequentially rather than trying to forget them all at once. We provide further analysis and show that the difficulty of knowledge unlearning depends heavily on the target data being forgotten, especially the domain of the target data. We also provide empirical examples of performing extraction attacks and how exactly knowledge unlearning provides a privacy protection for the LM.\\n\\nTo summarize, our main contributions are fourfold:\\n\\n\u2022 We compare knowledge unlearning with two approaches from literature known to mitigate privacy risks: a data preprocessing approach and a Differential Privacy (DP) Decoding approach. We show that our approach results in little to no performance degradation of general capabilities (sometimes resulting in improvement) while providing a strong privacy protection in situations individuals practice their RTBF whereas the data preprocessing approach provides a weaker privacy protection while being orders of magnitude computationally demanding and the DP Decoding approach results in a severe degradation of modeling performance.\"}"}
{"id": "zAxuIJLb38", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We perform additional experiments to determine which factors contribute to the difficulty of knowledge unlearning and find that (1) trying to forget many samples at once results in substantial LM performance degradation which can be mitigated by sequentially forgetting chunks of data and that (2) the domain of the target data (Code, License, Wikipedia, etc.) plays a critical role in determining how hard they are to forget.\\n\\nWe provide a novel metric and a general guideline for quantifying the privacy risks for LMs and determine when they should be considered to have \u201cforgotten\u201d a given target sequence.\\n\\nKnowledge unlearning surprisingly seems to make LMs stronger where the extreme cases bring +8.0% (37.6% \u2192 45.6%), +10.1% (57.4% \u2192 67.5%), and +7.9% (62.2% \u2192 70.1%) improvements on Lambada for GPT-NEO 125M, 1.3B, and 2.7B, respectively.\\n\\nPrior work that tries to mitigate privacy risks for LMs can be divided mainly into data pre/post-processing methods and differential privacy methods.\\n\\n(Data) Pre/Post-Processing\\n\\nData preprocessing aims to sanitize the training data; it aims to get rid of all data that might violate any kind of privacy from the training data prior to training. These methods mostly utilize measures such as parsers and classification models that try to identify and predict patterns that constitute private information. This is effective at identifying well-formatted private information such as social security numbers or special forms of medical notes (Aura et al., 2006; Dernoncourt et al., 2017; Lison et al., 2021; Kandpal et al., 2022). However, as pointed out by Brown et al. (2022), considering that private information is mostly context-dependent and sometimes in a non-specific format, data preprocessing methods cannot fully claim that they provide privacy guarantees, especially guarantees that match each individual\u2019s standards. Methods that attempt to utilize post-processing methods such as applying censorship to the LM outputs still face the same limitations.\\n\\nIn this work, we compare our proposed method with a data preprocessing approach proposed by Kandpal et al. (2022) which shows that deduplicating the training corpora before pretraining helps pretrain LMs that show stronger robustness against extraction attacks than an LM pretrained under the same circumstances without deduplicating the pretraining corpora. However, we highlight that this approach, which may still be effective at mitigating the overall privacy risks, is not the most suitable approach when considering a realistic scenario of individuals requesting the removal of their information from the implicit parameters of the LMs.\\n\\nDifferential Privacy\\n\\nDifferential Privacy (DP) aims to guarantee that the effect of an individual input on the output of a specific function is bounded (Dwork, 2008; Dwork et al., 2006). In the context of deep neural networks, DP, which needs to be applied during the training phase, aims to construct models that can provide general guarantees that the individual information within the training data cannot be inferred (Abadi et al., 2016). While DP has shown to be surprisingly effective at fine-tuning LMs (Li et al., 2022; Yu et al., 2022), pretraining LMs with DP still suffers from substantial performance gap, expensive computation, and slow convergence (Anil et al., 2021). Furthermore, as pointed out by Brown et al. (2022), DP can only provide limited guarantees for LMs because DP requires a unified definition for privacy boundaries, which is inherently impossible for natural language data. Most importantly, in a realistic scenario where individuals may practice their Right-To-Be-Forgotten (RTBF) dynamically after model deployment, it is nontrivial to apply existing descent-based DP algorithms such as DP-SGD to only protection against targeted extraction attacks.\\n\\nMachine Unlearning\\n\\nMachine unlearning has received attention as an alternative approach to overcome data privacy issues in machine learning (Cao & Yang, 2015; Ginart et al., 2019; Bourtoule et al., 2021; Graves et al., 2021). Several studies attempt to explore machine unlearning for deep neural networks (Golatkar et al., 2020; Mehta et al., 2022). However, they mostly focus on proposing algorithms for image\"}"}
{"id": "zAxuIJLb38", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"classification models where they aim to forget a whole class; that is, achieve random performance for specific image classes such as \\\"cats\\\" or \\\"ships\\\". We are the first, to the best of our knowledge, to explore unlearning a specific sequence of tokens for LMs which is a quite different set-up from traditional image classification models ($\\\\sim$ tens of image classes vs. a sequence of tokens that can each be classified into $V \\\\in \\\\mathbb{R} \\\\sim 50,000$). In this work, we coin this approach as knowledge unlearning since we are more focused on forgetting specific knowledge represented by sequences of tokens.\\n\\nZhou et al. (2022) focus on how forgetting can be leveraged to improve the performance of the underlying model. They propose \\\"forget-and-relearn\\\" that unifies existing iterative training algorithms by selectively removing undesirable information and re-learning good features, helping boost performance for the task of image classification and multi-agent emergence communication. The underlying assumption is that it is often easier to define and stop unwanted behavior than to teach good behavior. We also show this phenomenon in Section 4 where we unintentionally find unlearning just a few sequences of tokens sometimes boosts general LM capabilities.\\n\\n2.3 Memorization in Language Models\\n\\nPrevious work that explores to which extent LMs have memorized their training data approach the phenomenon with two different viewpoints. Some work view memorization of LMs simply as a threat to individual privacy (Carlini et al., 2021; 2022a; Jagielski et al., 2022) and utilize metrics that quantify how much the LMs are susceptible to adversarial attacks. These metrics are mostly dependent on the specific types of attacks such as the membership inference attack (Shokri et al., 2017) and measure the privacy risks of LMs by quantifying the success rate of these attacks. In our work, we instead focus on more targeted extraction attacks.\\n\\nAnother line of work simply quantifies how much knowledge is accumulated and forgotten during pretraining by extracting relational knowledge about the world (Petroni et al., 2019; Lazaridou et al., 2021; Jang et al., 2022b;a). This line of work does not view memorization as a negative trait, but as a positive one that can be leveraged to extract world knowledge from its implicit parameters and perform knowledge-intensive tasks such as question answering or training knowledgeable conversation agents.\\n\\nOur work is highly related to Jagielski et al. (2022)'s work where they also assert that forgetting can be a relaxed version of differential privacy. However, there are two main differences between our work and theirs. First, they only analyze forgetting as a passive form of mitigating privacy, asserting that data seen early in large-scale training obtain privacy benefits, whereas we suggest a more active form of forgetting. Second, they only show analysis results with image classification and audio generation models while we specifically focus on large LMs.\\n\\n3 Knowledge Unlearning for Language Models\\n\\n3.1 Methodology\\n\\nWe propose simply negating the original training objective of minimizing the negative log-likelihood of the token sequences as our main method of knowledge unlearning in LMs. Specifically, given a sequence of tokens $x = (x_1, \\\\ldots, x_T)$, our unlearning training objective is simply maximizing the following loss function:\\n\\n$$L_{UL}(f_\\\\theta, x) = - \\\\sum_{t=1}^{T} \\\\log(p_{\\\\theta}(x_t|x_{<t}))$$\\n\\nwhere $x_{<t}$ denotes the token sequence $x = (x_1, \\\\ldots, x_{t-1})$ and $p_{\\\\theta}(x_t|x_{<t})$ denotes the conditional probability of predicting the next token to be $x_t$ when given $x_{<t}$ to an LM $f_\\\\theta$ with parameters $\\\\theta$.\\n\\nPrior work refer to this training objective as unlikelihood training and combines it together with the original loss of minimizing the negative log-likelihood for the final objective of enhancing language modeling quality (Welleck et al., 2020) and few-shot learning for downstream NLP tasks (Tam et al., 2021). In contrast, we simply optimize the unlikelihood training objective since we are only concerned with forgetting. While this method seems simple, it is highly effective at forgetting specific token sequences without affecting the overall LM capabilities as shown in Section 4.\"}"}
{"id": "zAxuIJLb38", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this subsection, we introduce two metrics we use to quantify the privacy risks given a specific token sequence and how we empirically define the token sequence to be forgotten. In this work, we do not utilize metrics such as membership inference attack recall (Shokri et al., 2017) since we are not interested in quantifying the general privacy risks of LMs, but instead the privacy risks on the specific target token sequences.\\n\\n**Extraction Likelihood (EL)**\\n\\nWe first introduce a new metric, EL. Given a sequence of tokens $x = (x_1, ..., x_T)$ and an LM $f$ with pre-trained parameters $\\\\theta$, we define EL to be as follows:\\n\\n$$EL_n(x) = \\\\frac{\\\\sum_{t=1}^{T-n} \\\\text{OVERLAP}_n(f_{\\\\theta}(x_{<t}), x_{\\\\geq t})}{T-n}$$\\n\\nwhere $n$-grams denotes the list of $n$-grams in the given token sequence and $f_{\\\\theta}(x_{<t})$ denotes the output token sequences from the LM $f_{\\\\theta}$ when given $x_{<t}$ as input that can have max lengths $|x_{\\\\geq t}|$ but may be shorter when the EOS (end-of-sequence) token is generated beforehand.\\n\\nThe process of varying the prefix length $|x_{<t}|$ can be seen as varying the strength of adversarial attacks. This is based on the assumption that the more prior information is provided about the target token sequence, the easier the LM will be able to extract it. Overall, EL can be seen as estimating the general extraction likelihood since we are measuring the average success rate of varying extraction attacks quantified via getting the n-gram overlap of generated and target token sequences. While previous metrics quantifying the privacy risks of LMs are dependent on specific adversarial attacks, this characteristic of EL allows it to quantify the general likelihood of extraction without any dependency on specific extraction attacks.\\n\\nWe regard $n$ to be a hyper-parameter that can be varied depending on the stringency of privacy standards. The higher $n$ is set, the stricter we set the standard for a successful extraction attack.\\n\\n**Memorization Accuracy (MA)**\\n\\nWe define Memorization Accuracy (MA) as follows:\\n\\n$$MA(x) = \\\\frac{\\\\sum_{t=1}^{T-1} 1 \\\\{ \\\\arg\\\\max p_{\\\\theta}(\\\\cdot|x_{<t}) = x_t \\\\}}{T-1}$$\\n\\nMA quantifies how much $f_{\\\\theta}$ has memorized the given token sequences and was proposed by Tirumala et al. (2022) to analyze the training dynamics of large LMs.\\n\\n**Empirical Definition of Forgetting**\\n\\nBy utilizing both EL and MA, we empirically define a specific token sequence $x$ to be forgotten and is no longer susceptible to extraction attacks when the following conditions are met:\\n\\n$$EL_n(x) \\\\leq \\\\frac{1}{|D'|} \\\\sum_{x' \\\\in D'} EL_n(x')$$\\n\\nand\\n\\n$$MA(x) \\\\leq \\\\frac{1}{|D'|} \\\\sum_{x' \\\\in D'} MA(x')$$\\n\\nwhere $D'$ represents a validation corpora not seen during training. In other words, we define $x$ to be forgotten when the EL and MA reach a value that is lower than the average EL and MA on token sequences that were not seen during training.\"}"}
{"id": "zAxuIJLb38", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023 as other corpora from different domains. For the experiments, we perform unlearning the GPT-N LMs and quantify the privacy risks of the target data compared to the OPT LMs to measure how effective our proposed approach is in contrast to deduplicating the training corpora before pretraining the underlying LM. We do not use the exact LMs from Kandpal et al. (2022) because the LMs were not open-sourced, and thus use the OPT LMs instead. We also consider the Differential Privacy (DP) Decoding (Majmudar et al., 2022) as one of the baselines; This approach proposes a decoding strategy that performs linear interpolation of the original logits with the uniform distribution and performs nucleus sampling, which they theoretically show provides DP guarantees. \\\\( \\\\lambda \\\\) is set as the linear interpolation weight where \\\\( \\\\lambda = 0 \\\\) performs nucleus sampling from the uniform distribution and \\\\( \\\\lambda = 1 \\\\) performs regular nucleus sampling, using the logits as weights during random sampling.\\n\\n**Target Data**\\n\\nFor the actual target data used to quantify the privacy risks of the LMs, we sample instances from the Training Data Extraction Challenge where 15,000 examples (each are 200 token sequences long) from 16 different domains of the Pile corpora that are identified to be somewhat easy-to-extract are provided. For our experiments, we randomly sample \\\\( s \\\\) samples from the 15,000 examples and make the underlying LM forget the \\\\( s \\\\) samples at once. As a default, we show the average results of 5 random samplings of \\\\( s \\\\) samples for all of our experimental settings. We only provide the average of the 5 samplings and do not separately report the standard deviation. Instead, we provide the results of each individual run in Appendix A.\\n\\n**Evaluation Datasets**\\n\\nProviding stronger privacy protections for LMs may become meaningless if it requires sacrificing their original capabilities. Thus, while quantifying the privacy risks of LMs, we also quantify the original LM capabilities by evaluating the LMs on 9 different classification tasks quantifying the general capabilities: Hellaswag (Zellers et al., 2019) and Lambada (Paperno et al., 2016) benchmarks to measure linguistic reasoning abilities, Winogrande (Sakaguchi et al., 2021) and COPA (Gordon et al., 2012) to measure commonsense reasoning abilities, and ARC-Easy (Clark et al., 2018), ARC-Challenge (Clark et al., 2018), Piqa (Bisk et al., 2020), MathQA (Amini et al., 2019), PubmedQA (Jin et al., 2019) benchmarks to measure the scientific reasoning abilities. We also evaluate on 4 dialogue tasks (Wizard of Wikipedia (Dinan et al., 2019), Empathetic Dialogues (Rashkin et al., 2019), Blended Skill Talk (Smith et al., 2020), and Wizard of Internet (Komeili et al., 2022)) to evaluate the generation capabilities of the LMs. We use the test set for Lambada and the validation set for the rest of the datasets. We also show the results of measuring the perplexity on the validation corpora of Pile and Wikitext in Appendix B. We do not include measuring perplexity as one of the main evaluations because perplexity might not be the most suitable metric for quantifying general LM performance, especially in the case of unlearning (further explanation given in Appendix B. We evaluate DP Decoding only on the 4 dialogue tasks because the decoding strategy cannot be applied for performing the classification tasks which is evaluated by utilizing a verbalizer.\\n\\n**Configurations**\\n\\nFor the learning rate, we set it to 5e-5. We show the effect of varying learning rates in Appendix D. We use a constant learning rate scheduling throughout the run. We fix the global batch size to be the same as \\\\( s \\\\) (how many samples are forgotten at once) because having global batch sizes smaller than \\\\( s \\\\) proved to degrade general LM capabilities. For EL, we set \\\\( n = 10 \\\\) which means EL measures the extraction likelihood of extracting \\\\( n \\\\) consecutive tokens of varying extraction attack. For calculating EL and MA, we use a na\u00efve greedy decoding strategy. We set both the dropout and weight decay rates to 0. Lastly, while we provide a guideline of empirically deciding a single token sequence to be forgotten in Section 3.2, for considering a chunk of \\\\( s \\\\) token sequences to be forgotten, we use the average EL and MA as an approximation of the individual EL and MA.\\n\\n### 4.2 METHODS\\n\\n**Forgetting Threshold**\\n\\nFirst, we show how we get the Forgetting Threshold for EL and MA, the values where we consider the token sequence to be forgotten and unsusceptible from extraction. We set the \\\\( n \\\\) value to 10 since we empirically consider an extraction to be successful when 10 consecutive token sequences are successfully generated by the LM. We show varying the \\\\( n \\\\) with values from \\\\([5,10,20,40]\\\\) in Appendix H.\"}"}
{"id": "zAxuIJLb38", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Forgetting Threshold for GPT-N EOMs\\n\\n| Model (Size) | Threshold (EL/MA) |\\n|-------------|------------------|\\n| GPT-N EOM (125M) | 4.99/29.94 |\\n| GPT-N EOM (1.3B) | 5.68/33.27 |\\n| GPT-N EOM (2.7B) | 5.53/34.02 |\\n\\nTable 2: Main Results showing the average of 5 random sampling of $s = 32$ (forgetting 32 samples at once). OPT represents the LM with deduplication applied. N EOM denotes the initial GPT-N EOM LM, N EOM + DPD + represents applying the DP Decoding strategy by varying the $\\\\lambda$ to match the forgetting criteria, N EOM + UL represents performing unlearning on the initial N EOM until it provides a stronger security for the target sequences than OPT, N EOM + UL + represents performing unlearning on GPT-N EOM until target sequences match the forgetting criteria, LM Avg. denotes the average accuracy of the 9 classification datasets, and Dialogue Avg. denotes the average F1 score of the 4 dialogue datasets. Best comparable performances are bolded and second best underlined.\\n\\n| Model # | EL \u2193 (%) | MA \u2193 (%) | LM Avg. | Dialogue Avg. |\\n|---------|----------|----------|---------|---------------|\\n| OPT 125M | 8.6 | 52.9 | 42.4 | 10.2 |\\n| N EOM 125M | 30.9 | 77.4 | 43.4 | 9.4 |\\n| N EOM + DPD + 125M | 0.0 | 27.4 | N/A | 7.3 |\\n| N EOM + UL 125M | 3.7 | 50.1 | 42.6 | 8.0 | 11.0 |\\n| N EOM + UL + 125M | 1.0 | 27.4 | 39.9 | 2.6 | 17.2 |\\n| OPT 1.3B | 23.3 | 67.1 | 50.6 | 12.4 |\\n| N EOM 1.3B | 67.6 | 92.2 | 49.8 | 11.5 |\\n| N EOM + DPD + 1.3B | 0.0 | 21.4 | N/A | 7.1 |\\n| N EOM + UL 1.3B | 11.0 | 62.2 | 49.7 | 11.6 | 8.0 |\\n| N EOM + UL + 1.3B | 1.9 | 30.4 | 49.7 | 8.5 | 13.8 |\\n| OPT 2.7B | 25.6 | 69.2 | 52.7 | 12.9 |\\n| N EOM 2.7B | 70.4 | 93.4 | 52.3 | 11.5 |\\n| N EOM + DPD + 2.7B | 0.0 | 24.2 | N/A | 6.9 |\\n| N EOM + UL 2.7B | 13.0 | 66.0 | 52.3 | 12.5 | 5.4 |\\n| N EOM + UL + 2.7B | 1.6 | 31.0 | 51.9 | 11.1 | 10.8 |\\n\\nTable of attacks, for all model sizes of GPT-N EOM LMs in Table 1. For $D'$, we perform weighted sampling (same domain distribution as the Pile training corpora) of 10,000 instances each with token lengths 200 from the Pile validation corpora and measure the average EL and MA (Equation 5), which are empirically set as the Forgetting Threshold values.\\n\\nMain Results\\n\\nTable 2 shows the main results of performing unlearning on LMs of varying sizes and the baselines. While we provide the average performances of the 5 random samplings in Table 2, we provide each individual runs in Appendix A for reference.\\n\\nWe highlight five main observations regarding the results. (1) OPT LMs show a much lower EL and MA than GPT-N EOM LMs, confirming that deduplicating the pretraining corpora is indeed helpful for mitigating privacy risks. (2) N EOM + DPD + enables effective protection against extraction attacks demonstrated via the lowest EL and MA score; however, it brings severe degradation of generation capabilities measured via the Average F1 score of the 4 dialogue generation tasks. (3) N EOM + UL + results in severe degradation of both classification and dialogue tasks for the 125M, only severe degradation of dialogue tasks for 1.3B LM while for the 2.7B LMs, it enables retaining most of its previous capabilities. (4) While the LMs scale to larger sizes, it takes fewer epochs for the target sequences to be forgotten. Together with (3), this implies that larger LMs are strong unlearners. (5) While N EOM + UL + provides a stronger privacy protection than OPT without sacrificing its performance from N EOM for the 2.7B LM, it is much more computationally efficient (3,500,000x) than re-training the underlying LM, which is required for all data preprocessing approaches.\\n\\nComputational efficiency is measured via FLOPs which is calculated by $(6 \\\\times \\\\text{Total Training Tokens} \\\\times \\\\text{Parameter Size})$ as in Brown et al. (2020). FLOPs for OPT LMs were estimated using information from Zhang et al. (2022). We provide the FLOPs for the methods in Appendix C.\"}"}
{"id": "zAxuIJLb38", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Overall, results show unlearning to be an effective approach to providing a strong privacy protection while retaining and sometimes even improving general LM capabilities. Sequential Unlearning is more Stable than Batch Unlearning. We show the effect of varying $s$ (the # of data instances to be forgotten at once) in Figure 2a across model scales. We denote this approach as batch unlearning. As shown by the $s = 128$ results, it is harder to forget more samples at once, resulting in substantial degradation of average LM performance regardless of how large the LM is. Since $s \\\\leq 32$ does not show much degradation, we explore if sequentially unlearning can be a solution. In Figure 2b, we show the result of dividing the 128 samples into 4 chunks of 32 and performing sequential unlearning; we unlearn each chunk at a time until the chunk reaches the forgetting threshold. Surprisingly, as shown by the performance gap at $s = 128$ between the dotted lines (the $s = 128$ performance of Figure 2a) and straight lines, the end result is vastly different even though exactly the same instances were forgotten. Sequential unlearning shows almost no degradation of average LM performance. In Appendix G, we show that chunks once forgotten stay forgotten and that later chunks are forgotten much faster compared to the initial chunk. This result hints at the generalization of unlearning, which we do not further explore in the scope of this work. The result also suggests that knowledge unlearning can be continually applied to LMs when needed.\\n\\n4.3 ANALYSIS OF KNOWLEDGE UNLEARNING\\n\\nProviding Better Intuition of What Exactly Happens During Knowledge Unlearning. To show exactly what happens to the LM during knowledge unlearning, we show how the performance of each of the LM benchmarks changes as we perform 10 runs of unlearning to the GPT-N EO (1.3B) model (each run with $s = 1$) in Figure 3. As shown in the figure, the LM performance for each benchmark varies tremendously on which sample is chosen to be forgotten. Furthermore, the ending time of each run is different, indicating that some samples are forgotten faster than others. We also show empirical examples of performing actual extraction attacks with prefix length of 100 in Appendix F.\\n\\nTowards Understanding Why Some Instances are Harder to Forget\\n\\nTo measure why some instances are harder to forget, we perform 5 random samplings of $s = 8$ from 8 different domains from the Training Data Extraction Challenge and perform unlearning on the GPT-N EO 1.3B LM. We also show the results of each individual run in Appendix A. As shown in Table 3, despite undergoing the same number of token updates (10 epochs of unlearning), different domains result in vastly different outcomes; ENRON EMAILS results in the average LM performance degradation of only -0.4% while USPTO BACKGROUNDS results in -4.5% degradation. Furthermore, the final EL varies depending on the domain, suggesting that some domains (e.g., FIREELAW) are harder to forget than others. Lastly, domains that are more structured, which means the data consists of some kind of patterns such as a list of emails (ENRON EMAILS) or code (GITHUB (CODE)), seem to result in less degradation of LM performance in contrast to domains that are more unstructured, which means the data consist of mostly raw English text such as a review for journal submission (PUBMED).\"}"}
{"id": "zAxuIJLb38", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this paper, we propose knowledge unlearning as a method for mitigating privacy risks in LMs that provides a strong privacy protection with little to no degradation of general LM capabilities measured by evaluating on 9 common LM classification benchmarks and 4 dialogue benchmarks for the larger sized LMs. As large LMs expand their use cases, potentially affecting the daily lives of people, the research community should make sure that the privacy of individuals is not violated intentionally or unintentionally by the knowledge stored in the implicit parameters of these models. Since it is inherently impossible to prevent and predict all future privacy concerns prior to pretraining the LM, we suggest the community consider knowledge unlearning for ensuring privacy upon individuals' requests post hoc pretraining.\"}"}
{"id": "zAxuIJLb38", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5:\\n\\n| Domains         | Initial | Final | Avg. | EL | ACC | EL | ACC | EL | ACC | EL | ACC | EL | ACC |\\n|-----------------|---------|-------|------|----|-----|----|-----|----|-----|----|-----|----|-----|\\n|                 |         |       |      |    |     |    |     |    |     |    |     |    |     |\\n| Hella. Lambda. Wino. COPA ARC-E ARC-C Piqa MathQ PubQ | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | 21.9 | 53.8 | 49.9 | 37.0 | 57.4 | 54.9 | 70.0 | 56.6 | 25.8 | 70.4 | "}
{"id": "zAxuIJLb38", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 7: Measuring perplexity on Pile and Wikitext corpora for the main unlearning experiments (Table 2).\\n\\n| Model         | Pile | Wikitext |\\n|---------------|------|----------|\\n| N E O 125M    | 17.83| 38.27    |\\n| N E O + UL 125M | 34.02| 75.24    |\\n| N E O + UL 125M | 577.56| 1986.07  |\\n| OPT 125M      | 32.26| 38.74    |\\n| N E O 1.3B    | 11.46| 18.63    |\\n| N E O + UL 1.3B | 15.56| 20.26    |\\n| N E O + UL 1.3B | 15.83| 26.82    |\\n| OPT 1.3B      | 19.55| 19.39    |\\n| N E O 2.7B    | 10.44| 16.15    |\\n| N E O + UL 2.7B | 11.32| 16.84    |\\n| N E O + UL 2.7B | 17.93| 21.13    |\\n| OPT 2.7B      | 17.81| 16.81    |\\n\\nTable 8: Training compute comparison of methods mitigating privacy risks in LMs for sizes 125M, 1.3B, and 2.7B measured via FLOPs.\\n\\n| Method         | FLOPs   |\\n|----------------|---------|\\n| D EDUPLICATION (125M) | 2.25E+20 |\\n| U N LEARNING (125M)   | 5.28E+13  |\\n| D EDUPLICATION (1.3B) | 2.34E+21  |\\n| U N LEARNING (1.3B)   | 6.69E+14  |\\n| D EDUPLICATION (2.7B) | 4.86E+21  |\\n| U N LEARNING (2.7B)   | 1.12E+15  |\"}"}
{"id": "zAxuIJLb38", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nTable 9: Examples from each of the 8 domains from the Pile corpora.\\n\\n| Domain  | Text                                                                                                                                 |\\n|---------|-------------------------------------------------------------------------------------------------------------------------------------|\\n| FREELAW | U. S. (2010) 1 Opinion of the Court NOTICE: This opinion is subject to formal revision before publication in the preliminary print of the United States Reports. Readers are requested to notify the Reporter of Decisions, Supreme Court of the United States, Washington, D. C. 20543, of any typographical or other formal errors, in order that corrections may be made before the preliminary print goes to press. SUPREME COURT OF THE UNITED STATES |\\n| GITHUB  | (CODE) setLen(length int) {iov.Len = uint64(length)}\\n|         | func (msghdr *Msghdr) SetControllen(length int) {msghdr.Controllen = uint64(length)}\\n|         | func (cmsg *Cmsghdr) SetLen(length int) {cmsg.Len = uint64(length)}\\n|         | sys poll(fds *PollFd, nfds int, timeout int) func Poll(fds []PollFd, timeout int) (n int, err error) {if len(fds) == 0 {return poll(nil, 0, timeout)}} |\\n\\nGITHUB (LICENSE)\\n\\n## Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \\\"Software\\\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \\\"AS IS\\\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE\\n\\nENRON EMAILS\\n\\nTo: Hedy Govenar hgovenar@govadv.com , Mike Day MDay@GMSSR.com , Bev Hansen bhansen@lhom.com , Jeff Dasovich jdasovic@enron.com , Susan J Mara smara@enron.com , Joseph Alamo JAlamo@enron.com , Paul Kaufman paul.kaufman@enron.com , David Parquet David.Parquet@enron.com , Rick Johnson rick.johnson@enron.com , Marcie Milner mmilner@enron.com , Sandra McCubbin Sandra.McCubbin@enron.com , Tim Belden Tim.Belden@enron.com\\n\\nBOOKS\\n\\nAbout the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia http://www.harpercollinsebooks.com.au Canada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada http://www.harpercollinsebooks.ca New Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand http://www.harpercollinsebooks.co.nz United Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK http://www.harpercollinsebooks.co.uk\\n\\nPILE CC\\n\\nThis website and its associated newspaper adheres to the Independent Press Standards Organisation's Editors' Code of Practice. If you have a complaint about editorial content which relates to inaccuracy or intrusion, then contact the Editor by clicking here. If you remain dissatisfied with the response provided then you can contact the IPSO by clicking here. Bury Free Press provides news, events and sport features from the Bury St Edmunds area. For the best up to date information relating to Bury St Edmunds and the surrounding areas visit us at Bury Free Press regularly or bookmark this page. For you to enjoy all the features of this website Bury Free Press requires permission to use cookies. Find Out More What is a Cookie? What is a Flash Cookie? Can I opt out of receiving Cookies?\\n\\nUSPTO BACKGROUNDS\\n\\nThe pharmaceutical formulations of the present invention, which may conveniently be presented in unit dosage form, may be prepared according to conventional techniques well known in the pharmaceutical industry. Such techniques include the step of bringing into association the active ingredients with the pharmaceutical carrier(s) or excipient(s). In general the formulations are prepared by uniformly and intimately bringing into association the active ingredients with liquid carriers or finely divided solid carriers or both, and then, if necessary, shaping the product. The compositions of the present invention may be formulated into any of many possible dosage forms such as, but not limited to, tablets, capsules, gel capsules, liquid syrups, soft gels, suppositories, and enemas.\\n\\nPUBMED CENTRAL\\n\\nI am pleased to inform you that your manuscript has been formally accepted for publication in PLOS Computational Biology. Your manuscript is now with our production department and you will be notified of the publication date in due course. The corresponding author will soon receiving a typeset proof for review, to ensure errors have not been introduced during production. Please review the PDF proof of your manuscript carefully, as this is the last chance to correct any errors. Please note that major changes, or those which affect the scientific understanding of the work, will likely cause delays to the publication date of your manuscript. Soon after your final files are uploaded, unless you have opted out, the early version of your manuscript will be published online. The date of the early version will be your article\u2019s publication date.\"}"}
{"id": "zAxuIJLb38", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 10: Examples performing extraction attacks on token sequences, showing knowledge unlearning provides protection against extraction attacks. Underlined denotes the model generated text given the prefix of length 100 as input. For the extraction attack, we utilize a na\u00efve greedy decoding strategy.\\n\\n| Domain   | Status Text                                                                 |\\n|----------|-----------------------------------------------------------------------------|\\n| BOOKS    | Original About the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\\nCanada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\\nNew Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\\nUnited Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK |\\n\\n| Domain   | Text                                                                 |\\n|----------|-----------------------------------------------------------------------|\\n| BOOKS    | Before About the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\\nCanada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\\nNew Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\\nUnited Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK |\\n\\n| Domain   | Text                                                                 |\\n|----------|-----------------------------------------------------------------------|\\n| BOOKS    | Unlearning After About the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\\nCanada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\\nNew Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\\nUnited Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK |\\n\\n| Domain   | Text                                                                 |\\n|----------|-----------------------------------------------------------------------|\\n| BOOKS    | Unlearning After About the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\\nCanada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\\nNew Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\\nUnited Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK |\\n\\n| Domain   | Text                                                                 |\\n|----------|-----------------------------------------------------------------------|\\n| BOOKS    | Unlearning After About the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\\nCanada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\\nNew Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\\nUnited Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK |\\n\\n| Domain   | Text                                                                 |\\n|----------|-----------------------------------------------------------------------|\\n| BOOKS    | Unlearning After About the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\\nCanada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\\nNew Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\\nUnited Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK |\\n\\n| Domain   | Text                                                                 |\\n|----------|-----------------------------------------------------------------------|\\n| BOOKS    | Unlearning After About the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\\nCanada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\\nNew Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\\nUnited Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK |\\n\\n| Domain   | Text                                                                 |\\n|----------|-----------------------------------------------------------------------|\\n| BOOKS    | Unlearning After About the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\\nCanada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\\nNew Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\\nUnited Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK |\\n\\n| Domain   | Text                                                                 |\\n|----------|-----------------------------------------------------------------------|\\n| BOOKS    | Unlearning After About the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\\nCanada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\\nNew Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\\nUnited Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK |\\n\\n| Domain   | Text                                                                 |\\n|----------|-----------------------------------------------------------------------|\\n| BOOKS    | Unlearning After About the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\\nCanada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\\nNew Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\\nUnited Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK |\\n\\n| Domain   | Text                                                                 |\\n|----------|-----------------------------------------------------------------------|\\n| BOOKS    | Unlearning After About the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\\nCanada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\\nNew Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\\nUnited Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK |\\n\\n| Domain   | Text                                                                 |\\n|----------|-----------------------------------------------------------------------|\\n| BOOKS    | Unlearning After About the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\\nCanada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\\nNew Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\\nUnited Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK |\\n\\n| Domain   | Text                                                                 |\\n|----------|-----------------------------------------------------------------------|\\n| BOOKS    | Unlearning After About the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\\nCanada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\\nNew Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\\nUnited Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK |\\n\\n| Domain   | Text                                                                 |\\n|----------|-----------------------------------------------------------------------|\\n| BOOKS    | Unlearning After About the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\\nCanada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\\nNew Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\\nUnited Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK |\\n\\n| Domain   | Text                                                                 |\\n|----------|-----------------------------------------------------------------------|\\n| BOOKS    | Unlearning After About the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\\nCanada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\\nNew Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\\nUnited Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK |\\n\\nTable 11: Forgetting Threshold for GPT-N LMs for varying \\\\( n \\\\).\\n\\n| Model (Size) | Threshold Threshold Threshold Threshold Threshold |\\n|--------------|-----------------------------------------------|\\n| GPT-N EO (1.3B) | 7.85 5.68 4.07 2.66 33.27 21 |\"}"}
