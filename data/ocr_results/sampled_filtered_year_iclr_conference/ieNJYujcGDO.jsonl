{"id": "ieNJYujcGDO", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Understanding the Data Dependency of Mixup-Style Training\\n\\nMuthu Chidambaram, Xiang Wang, Yuzheng Hu, Chenwei Wu, and Rong Ge\\n\\n1 Duke University, 2 University of Illinois at Urbana-Champaign\\n\\nAbstract\\n\\nIn the Mixup training paradigm, a model is trained using convex combinations of data points and their associated labels. Despite seeing very few true data points during training, models trained using Mixup seem to still minimize the original empirical risk and exhibit better generalization and robustness on various tasks when compared to standard training. In this paper, we investigate how these benefits of Mixup training rely on properties of the data in the context of classification.\\n\\nFor minimizing the original empirical risk, we compute a closed form for the Mixup-optimal classification, which allows us to construct a simple dataset on which minimizing the Mixup loss can provably lead to learning a classifier that does not minimize the empirical loss on the data. On the other hand, we also give sufficient conditions for Mixup training to also minimize the original empirical risk. For generalization, we characterize the margin of a Mixup classifier, and use this to understand why the decision boundary of a Mixup classifier can adapt better to the full structure of the training data when compared to standard training. In contrast, we also show that, for a large class of linear models and linearly separable datasets, Mixup training leads to learning the same classifier as standard training.\\n\\nIntroduction\\n\\nMixup (Zhang et al., 2018) is a modification to the standard supervised learning setup which involves training on convex combinations of pairs of data points and their labels instead of the original data itself. In the original paper, Zhang et al. (2018) demonstrated that training deep neural networks using Mixup leads to better generalization performance, as well as greater robustness to adversarial attacks and label noise on image classification tasks. The empirical advantages of Mixup training have been affirmed by several follow-up works (He et al., 2019; Thulasidasan et al., 2019; Lamb et al., 2019; Arazo et al., 2019; Guo, 2020). The idea of Mixup has also been extended beyond the supervised learning setting, and been applied to semi-supervised learning (Berthelot et al., 2019; Sohn et al., 2020), contrastive learning (Verma et al., 2021; Lee et al., 2020), privacy-preserving learning (Huang et al., 2021), and learning with fairness constraints (Chuang & Mroueh, 2021).\\n\\nHowever, from a theoretical perspective, Mixup training is still mysterious even in the basic multi-class classification setting \u2013 why should the output of a linear mixture of two training samples be the same linear mixture of their labels, especially when considering highly nonlinear models? Despite several recent theoretical results (Guo et al., 2019; Carratino et al., 2020; Zhang et al., 2020; 2021), there is still not a complete understanding of why Mixup training actually works in practice. In this paper, we try to understand why Mixup works by first understanding when Mixup works: in particular, how the properties of Mixup training rely on the structure of the training data.\\n\\nWe consider two properties for classifiers trained with Mixup. First, even though Mixup training does not observe many original data points during training, it usually can still correctly classify all of the original data points (empirical risk minimization (ERM)). Second, the aforementioned empirical works have shown how classifiers trained with Mixup often have better adversarial robustness and generalization than standard training. In this work, we show that both of these properties can rely heavily on the data used for training, and that they need not hold in general.\\n\\nMain Contributions and Related Work.\\n\\nThe idea that Mixup can potentially fail to minimize the original risk is not new; Guo et al. (2019) provide examples of how Mixup labels can conflict with actual data point labels. However, their theoretical results do not characterize the data and...\"}"}
{"id": "ieNJYujcGDO", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The goal of this section is to understand when Mixup training can also minimize the empirical risk. Our main technique for doing so is to derive a closed-form for the Mixup-optimal classifier over a sufficiently powerful function class, which we do in Section 2.2 after introducing the basic setup in Section 2.1. We use this closed form to motivate a concrete example on which Mixup training does not minimize the empirical risk in Section 2.3, and show under mild nondegeneracy conditions that Mixup will minimize the empirical risk in Section 2.4.\\n\\n### Section 2.1 Setup\\nWe consider the problem of $k$-class classification where the classes $1, ..., k$ correspond to compact disjoint sets $X_1, ..., X_k \\\\subset \\\\mathbb{R}^n$ with an associated probability measure $P_X$ supported on $X = \\\\bigcup_{i=1}^k X_i$. We use $C$ to denote the set of all functions $g: \\\\mathbb{R}^n \\\\to [0, 1]^k$ satisfying the property that $\\\\sum_{i=1}^k g_i(x) = 1$ for all $x$ (where $g_i$ represents the $i$-th coordinate function of $g$). We refer to a function $g \\\\in C$ as a classifier, and say that $g$ classifies $x$ as class $j$ if $j = \\\\text{argmax}_i g_i(x)$. The cross-entropy loss associated with such a classifier $g$ is then:\\n\\n$$ J(g, P_X) = -\\\\sum_{i=1}^k \\\\int_{X_i} \\\\log g_i(x) \\\\, dP_X(x) $$\\n\\nThe goal of standard training is to learn a classifier $h \\\\in \\\\arg\\\\min_{g \\\\in C} J(g, P_X)$. Any such classifier $h$ will necessarily satisfy $h_i(x) = 1$ on $X_i$ since the $X_i$ are disjoint.\\n\\n### Mixup\\nIn the Mixup version of our setup, we are interested in minimizing the cross-entropy of convex combinations of the original data and their classes. These convex combinations are determined according to a probability measure $P_f$ whose support is $[0, 1]$, and we assume this measure has a density $f$. For two points $s, t \\\\in X$, we let $z_{st}(\\\\lambda) = \\\\lambda s + (1 - \\\\lambda)t$ (and use $z_{st}$ when $\\\\lambda$ is understood) and define the Mixup cross-entropy on $s, t$ with respect to a classifier $g$ as:\\n\\n$$ \\\\ell_{\\\\text{mix}}(g, s, t, \\\\lambda) = \\\\begin{cases} -\\\\log g_i(z_{st}(\\\\lambda)) & s, t \\\\in X_i \\\\\\\\ -\\\\lambda \\\\log g_i(z_{st}(\\\\lambda)) + (1 - \\\\lambda) \\\\log g_j(z_{st}(\\\\lambda)) & s \\\\in X_i, t \\\\in X_j \\\\end{cases} $$\\n\\nHaving defined $\\\\ell_{\\\\text{mix}}$ as above, we may write the component of the full Mixup cross-entropy loss corresponding to mixing points from classes $i$ and $j$ as:\\n\\n$$ J_{i,j}\\\\text{mix}(g, P_X, P_f) = \\\\int_{X_i \\\\times X_j \\\\times [0, 1]} \\\\ell_{\\\\text{mix}}(g, s, t, \\\\lambda) \\\\, d(P_X \\\\times P_X \\\\times P_f)(s, t, \\\\lambda) $$\\n\\nFinally, we note the related works that are beyond the scope of our paper; namely the many Mixup-like training procedures such as Manifold Mixup (Verma et al., 2019), Cut Mix (Yun et al., 2019), Puzzle Mix (Kim et al., 2020), and Co-Mixup (Kim et al., 2021).\"}"}
{"id": "ieNJYujcGDO", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "ieNJYujcGDO", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, volume 32, pp. 8026\u20138037. Curran Associates, Inc., 2019.\\n\\nMohammad Pezeshki, S\u00e9kou-Oumar Kaba, Yoshua Bengio, Aaron C. Courville, Doina Precup, and Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. CoRR, abs/2011.09468, 2020. URL https://arxiv.org/abs/2011.09468.\\n\\nKevin Scaman and Aladin Virmaux. Lipschitz regularity of deep neural networks: analysis and efficient estimation, 2019.\\n\\nKihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 596\u2013608. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf.\\n\\nDaniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822\u20132878, 2018.\\n\\nSunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. On mixup training: Improved calibration and predictive uncertainty for deep neural networks. Advances in Neural Information Processing Systems, 32:13888\u201313899, 2019.\\n\\nVikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In International Conference on Machine Learning, pp. 6438\u20136447. PMLR, 2019.\\n\\nVikas Verma, Minh-Thang Luong, Kenji Kawaguchi, Hieu Pham, and Quoc V. Le. Towards domain-agnostic contrastive learning, 2021.\\n\\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6023\u20136032, 2019.\\n\\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018.\\n\\nLinjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup help with robustness and generalization?, 2020.\\n\\nLinjun Zhang, Zhun Deng, Kenji Kawaguchi, and James Zou. When and how mixup improves calibration, 2021.\"}"}
{"id": "ieNJYujcGDO", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For convenience, we first recall the definitions and assumptions stated throughout the paper below.\\n\\n\\\\[ \\\\ell_{mix}(g, s, t, \\\\lambda) = \\\\{ -\\\\log g_i(z_{st}) s, t \\\\in X \\\\} - (\\\\lambda \\\\log g_i(z_{st}) + (1 - \\\\lambda) \\\\log g_j(z_{st})) s \\\\in X, t \\\\in X_j \\\\]\\n\\n\\\\[ J_{mix}(g, P_X, P_f) = \\\\int_{X_i \\\\times X_j \\\\times [0,1]} \\\\ell_{mix}(g, s, t, \\\\lambda) d(P_X \\\\times P_X \\\\times P_f)(s, t, \\\\lambda) \\\\]\\n\\n\\\\[ J_{mix}(g, P_X, P_f) = k \\\\sum_{i=1}^{k} \\\\sum_{j=1}^{k} J_{i,j}_{mix}(g, P_X, P_f) \\\\]\\n\\n\\\\[ A_{i,j}(x, \\\\epsilon) = \\\\{ (s, t, \\\\lambda) \\\\in X_i \\\\times X_j \\\\times [0,1] : \\\\lambda s + (1 - \\\\lambda) t \\\\in B_{\\\\epsilon}(x) \\\\} \\\\]\\n\\n\\\\[ A_{i,j}(x, \\\\epsilon, \\\\delta) = \\\\{ (s, t, \\\\lambda) \\\\in X_i \\\\times X_j \\\\times [0,1] : \\\\lambda s + (1 - \\\\lambda) t \\\\in B_{\\\\epsilon}(x) \\\\} \\\\]\\n\\n\\\\[ X_{mix} = \\\\{ x \\\\in \\\\mathbb{R}^n : \\\\bigcup_{i,j} A_{i,j}(x, \\\\epsilon) \\\\text{ has positive measure for every } \\\\epsilon > 0 \\\\} \\\\]\\n\\n\\\\[ \\\\xi_{i,j}(x, \\\\epsilon) = \\\\int_{A_{i,j}(x, \\\\epsilon)} d(P_X \\\\times P_X \\\\times P_f)(s, t, \\\\lambda) \\\\]\\n\\n\\\\[ \\\\xi_{i,j}(x, \\\\epsilon, \\\\lambda) = \\\\int_{A_{i,j}(x, \\\\epsilon)} \\\\lambda d(P_X \\\\times P_X \\\\times P_f)(s, t, \\\\lambda) \\\\]\\n\\n**Definition 2.1.** Let \\\\( C^* \\\\) to be the subset of \\\\( C \\\\) for which every \\\\( h \\\\in C^* \\\\) satisfies \\\\( h(x) = \\\\lim_{\\\\epsilon \\\\to 0} \\\\arg\\\\min_{\\\\theta \\\\in [0,1]} k J_{mix}(\\\\theta) |_{B_{\\\\epsilon}(x)} \\\\) for all \\\\( x \\\\in X_{mix} \\\\) when the limit exists. Here \\\\( J_{mix}(\\\\theta) |_{B_{\\\\epsilon}(x)} \\\\) represents the Mixup loss for a constant function with value \\\\( \\\\theta \\\\) with the restriction of each term in \\\\( J_{mix} \\\\) to the set \\\\( A_{i,j}(x, \\\\epsilon) \\\\).\\n\\n**Definition 2.6.** [3-Point Alternating Line] We define \\\\( X_2 \\\\) to be the binary classification dataset consisting of the points \\\\( \\\\{0, 1, 2\\\\} \\\\) classified as \\\\( \\\\{1, 2, 1\\\\} \\\\). In our setup, this corresponds to \\\\( X_1 = \\\\{0, 2\\\\} \\\\) and \\\\( X_2 = \\\\{1\\\\} \\\\) with \\\\( P_X = \\\\frac{1}{3} \\\\{0, 1, 2\\\\} \\\\).\\n\\n**Assumption 2.9.** For any point \\\\( x \\\\in X_i \\\\), there do not exist \\\\( u \\\\in X \\\\) and \\\\( v \\\\in X_j \\\\) for \\\\( j \\\\neq i \\\\) such that there is a \\\\( \\\\lambda > 0 \\\\) for which \\\\( x = \\\\lambda u + (1 - \\\\lambda) v \\\\).\\n\\n**Assumption 3.1.** For a class \\\\( i \\\\) and a point \\\\( x \\\\in X_{mix} \\\\), suppose there exists an \\\\( \\\\epsilon > 0 \\\\) and a \\\\( \\\\delta < \\\\frac{1}{2} \\\\) such that \\\\( A_{i,j}(x, \\\\epsilon) \\\\) and \\\\( A_{j,q}(x, \\\\epsilon) \\\\) have measure zero for all \\\\( \\\\epsilon' \\\\leq \\\\epsilon \\\\) and \\\\( j,q \\\\neq i \\\\), and the measure of \\\\( A_{i,j}(x, \\\\epsilon) \\\\) is at least that of \\\\( A_{j,i}(x, \\\\epsilon) \\\\).\\n\\n**Definition 3.3.** We say \\\\( \\\\hat{\\\\theta} \\\\) is an interpolating solution, if there exists a \\\\( k > 0 \\\\) such that \\\\( \\\\hat{\\\\theta}^\\\\top x_i = -\\\\hat{\\\\theta}^\\\\top z_j = k \\\\) for all \\\\( x_i \\\\in X_1 \\\\), \\\\( \\\\forall z_j \\\\in X_{-1} \\\\).\\n\\n**Definition 3.4.** The maximum margin solution \\\\( \\\\tilde{\\\\theta} \\\\) is defined through:\\n\\n\\\\[ \\\\tilde{\\\\theta} := \\\\arg\\\\max_{\\\\|\\\\theta\\\\|_2 = 1} \\\\{ \\\\min_{x \\\\in X_1, z \\\\in X_{-1}} \\\\{ \\\\theta^\\\\top x_i, -\\\\theta^\\\\top z_j \\\\} \\\\} \\\\]\\n\\n**Visualizations of Definitions and Assumptions**\\n\\nDue to the technical nature of the definitions and assumptions above, we provide several visualizations in Figures 4 to 7 to help aid the reader's intuition for our main results.\\n\\n**Full Proofs for Section 2**\\n\\nWe now prove all results found in Section 2 of the main body of the paper in the order that they appear.\"}"}
{"id": "ieNJYujcGDO", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2022\\n\\nFigure 4: A visualization of $X_{mix}$ and $A_{i,j\\\\times,\\\\epsilon}$.\\n\\nFigure 5: A visualization of the $X_{2,3}$ dataset and how the Mixup sandwiching works.\"}"}
{"id": "ieNJYujcGDO", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the above benchmarks degrades with high values of $\\\\alpha$ due to conflicts between original points and Mixup points.\\n\\n2.5 \\\\text{ THE RATE OF E\\\\textsc{MPIRICAL R\\\\textsc{ISK MINIMIZATION USING MIXUP}}}\\n\\nAnother striking aspect of the experiments in Figure 2 is that Mixup training minimizes the original empirical risk at a very similar rate to that of direct empirical risk minimization. A priori, there is no reason to expect that Mixup should be able to do this - a simple calculation shows that Mixup training only sees one true data point per epoch in expectation (each pair of points is sampled with probability $\\\\frac{1}{m^2}$ and there are $m$ true point pairs and $m$ pairs seen per epoch, where $m$ is the dataset size). The experimental results are even more surprising given that we are training using $\\\\alpha = 1024$, which essentially corresponds to training using the midpoints of the original data points. This seems to imply that it is possible to recover the classifications of the original data points from the midpoints alone (not including the midpoint of a point and itself), and similar phenomena has indeed been observed in recent empirical work (Guo, 2021). We make this rigorous with the following result:\\n\\nTheorem 2.11. Suppose $\\\\{x_1, \\\\ldots, x_m\\\\}$ with $m \\\\geq 6$ are sampled from $X$ according to $P_X$, and that $P_X$ has a density. Then with probability 1, we can uniquely determine the points $\\\\{x_1, \\\\ldots, x_m\\\\}$ given only the $(m^2)$ midpoints $\\\\{x_{i,j}\\\\}_{1 \\\\leq i < j \\\\leq m}$.\\n\\nProof Sketch. The idea is to represent the problem as a linear system, and show using rank arguments that the sets of $m$ points that cannot be uniquely determined are a measure zero set.\\n\\nTheorem 2.11 shows, in an information-theoretic sense, that it is possible to obtain the original data points (and therefore also their labels) from only their midpoints. While this gives more theoretical backing as to why it is possible for Mixup training using $\\\\text{Beta}(1024, 1024)$ to recover the original data point classifications with very low error, it does not explain why this actually happens in practice at the rate that it does. A full theoretical analysis of this phenomenon would necessarily require analyzing the training dynamics of neural networks (or another model of choice) when trained only on midpoints of the original data, which is outside the intended scope of this work. That being said, we hope that such analysis will be a fruitful line of investigation for future work.\\n\\n3 \\\\text{ GENERALIZATION PROPERTIES OF MIXUP CLASSIFIERS}\\n\\nHaving discussed how Mixup training differs from standard empirical risk minimization with regards to the original training data, we now consider how a learned Mixup classifier can differ from one learned through empirical risk minimization on unseen test data. To do so, we analyze the per-class margin of Mixup classifiers, i.e. the distance one can move from a class support $X_i$ while still being classified as class $i$.\\n\\n3.1 \\\\text{ THE MARGIN OF MIXUP CLASSIFIERS}\\n\\nIntuitively, if a point $x$ falls only on line segments between $X_i$ and some other classes $X_j, \\\\ldots$, and if $x$ always falls closer to $X_i$ than the other classes, we can expect $x$ to be classified according to class $i$ by the Mixup-optimal classifier due to Lemma 2.3. To make this rigorous, we introduce another assumption that generalizes Assumption 2.9 to points outside of the class supports:\\n\\nAssumption 3.1. For a class $i$ and a point $x \\\\in X_{\\\\text{mix}}$, suppose there exists an $\\\\epsilon > 0$ and a $0 < \\\\delta < \\\\frac{1}{2}$ such that $A_{i,j}^{x,\\\\epsilon}$ and $A_{j,q}^{x,\\\\epsilon}$ have measure zero for all $\\\\epsilon' \\\\leq \\\\epsilon$ and $j,q \\\\neq i$, and the measure of $A_{i,j}^{x,\\\\epsilon}$ is at least that of $A_{j,i}^{x,\\\\epsilon}$.\\n\\nHere the measure zero conditions are codifying the ideas that the point $x$ falls closer to $X_i$ than any other class on every line segment that intersects it, and there are no line segments between non-$i$ classes that intersect $x$. The condition that the measure of $A_{i,j}^{x,\\\\epsilon}$ is at least that of $A_{j,i}^{x,\\\\epsilon}$ handles asymmetric mixing distributions that concentrate on pathological values of $\\\\lambda$. A visualization of Assumption 3.1 is provided in Section B of the Appendix. Now we can prove:\\n\\nTheorem 3.2. Consider $k$-class classification where the supports $X_1, \\\\ldots, X_k$ are finite and $P_X$ corresponds to the discrete uniform distribution. If a point $x$ satisfies Assumption 3.1 with respect to a class $i$, then for every $h \\\\in \\\\arg\\\\min_{g \\\\in C^*} J_{\\\\text{mix}}(g, P_X, P_f)$, we have that $h$ classifies $x$ as class $i$ and that $h$ is continuous at $x$. \\n\\n7\"}"}
{"id": "ieNJYujcGDO", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Decision boundary plots for standard and Mixup training on the two moons dataset of Pezeshki et al. (2020) with a class separation of 0.5. Each boundary represents the average of 10 training runs of 1500 epochs.\\n\\nProof Sketch.\\nThe limit in Lemma 2.3 can be shown to exist using the Lebesgue differentiation theorem, and we can bound the limit below since the $A_{i,j,x,\\\\epsilon,\\\\delta}$ have measure zero.\\n\\nAssumption 2.9 implies Assumption 3.1 with respect to each class, and hence we get Theorem 2.10 as a corollary of Theorem 3.2 as mentioned in Section 2. To use Theorem 3.2 to understand generalization, we make the observation that a point $x$ can satisfy Assumption 3.1 while being a distance of up to $\\\\min_j d(X_i, X_j)^2$ from some class $i$. This distance can be significantly farther than, for example, the optimal linear separator in a linearly separable dataset.\\n\\nExperiments.\\nTo illustrate that Mixup can lead to more separation between points than a linear decision boundary, we consider the two moons dataset (Buitinck et al., 2013), which consists of two classes of points supported on semicircles with added Gaussian noise. Our motivation for doing so comes from the work of Pezeshki et al. (2020), in which it was noted that neural network models trained on a separated version of the two moons dataset essentially learned a linear separator while ignoring the curvature of the class supports. While Pezeshki et al. (2020) introduced an explicit regularizer to encourage a nonlinear decision boundary, we expect due to Theorem 3.2 that Mixup training will achieve a similar result without any additional modifications.\\n\\nTo verify this empirically, we train a two-layer neural network with 500 hidden units with and without Mixup, to have a 1-to-1 comparison with the setting of Pezeshki et al. (2020). We use $\\\\alpha = 1$ and $\\\\alpha = 1024$ for Mixup to capture a wide band of mixing densities. The version of the two moons dataset we use is also identical to that of the one used in the experiments of Pezeshki et al. (2020), and we are grateful to the authors for releasing their code under the MIT license. We do full-batch training with all other training, implementation, and compute details remaining the same as the previous section. Results are shown in Figure 3.\\n\\nOur results affirm the observations of Pezeshki et al. (2020) and previous work (des Combes et al., 2018) that neural network training dynamics may ignore salient features of the dataset; in this case the \\\"Base Model\\\" learns to differentiate the two classes essentially based on the $x$-coordinate alone. On the other hand, the models trained using Mixup have highly nonlinear decision boundaries. Further experiments for different class separations and values of $\\\\alpha$ are included in Section F of the Appendix.\\n\\n3.2 When Mixup Training Earns the Same Classifier\\nThe experiments and theory of the previous sections have shown how a Mixup classifier can differ significantly from one learned through standard training. In this subsection, we now consider the opposing question - when is the Mixup classifier the same as the one learned through standard training? Prior work (Archambault et al., 2019) has considered when Mixup training coincides with certain adversarial training, and our results complement this line of work. The motivation for our results comes from the fact that a practitioner need not spend compute on Mixup training in addition to standard training in settings where the performance will be provably the same.\\n\\nWe consider the case of binary classification using a linear model $\\\\theta^\\\\top x$ on high-dimensional Gaussian data, which is a setting that arises naturally when training using Gaussian kernels. Specifically, we consider the dataset $X$ to consist of $n$ points in $\\\\mathbb{R}^d$ distributed according to $N(0, I_d)$ with $d > n$ (to be made more precise shortly). We also consider the mixing distribution to be any symmetric distribution supported on $[0, 1]$ (thereby including as a special case $\\\\text{Beta}(\\\\alpha, \\\\alpha)$).\"}"}
{"id": "ieNJYujcGDO", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2022\\n\\nPoints in $X$ be $\\\\pm 1$ (so that the sign of $\\\\theta^\\\\top x$ is the classification), and use $X_1$ and $X_{-1}$ to denote the individual class points. We will show that in this setting, the optimal Mixup classifier is the same (up to rescaling of $\\\\theta$) as the ERM classifier learned using gradient descent with high probability. To do so we need some additional definitions.\\n\\nDefinition 3.3. We say $\\\\hat{\\\\theta}$ is an interpolating solution, if there exists $k > 0$ such that $\\\\hat{\\\\theta}^\\\\top x_i = -\\\\hat{\\\\theta}^\\\\top z_j = k \\\\forall x_i \\\\in X_1, \\\\forall z_j \\\\in X_{-1}$.\\n\\nDefinition 3.4. The maximum margin solution $\\\\tilde{\\\\theta}$ is defined through:\\n\\n$$\\\\tilde{\\\\theta} := \\\\arg\\\\max_{\\\\theta} \\\\|\\\\theta\\\\|_2 = 1 \\\\{ \\\\min_{x_i \\\\in X_1, z_j \\\\in X_{-1}} \\\\{ \\\\theta^\\\\top x_i, -\\\\theta^\\\\top z_j \\\\} \\\\}$$\\n\\nWhen the maximum margin solution coincides with an interpolating solution for the dataset $X$ (i.e. all the points are support vectors), we have that Mixup training leads to learning the max margin solution (up to rescaling).\\n\\nTheorem 3.5. If the maximum margin solution for $X$ is also an interpolating solution for $X$, then any $\\\\theta$ that lies in the span of $X$ and minimizes the Mixup loss $J_{mix}$ for a symmetric mixing distribution $P_f$ is a rescaling of the maximum margin solution.\\n\\nProof Sketch. It can be shown that $\\\\theta$ is an interpolating solution using a combination of the strict convexity of $J_{mix}$ as a function of $\\\\theta$ and the symmetry of the mixing distribution.\\n\\nRemark 3.6. For every $\\\\theta$, we can decompose it as $\\\\theta = \\\\theta_X + \\\\theta_{X\\\\perp}$ where $\\\\theta_X$ is the projection of $\\\\theta$ onto the subspace spanned by $X$. By definition we have that $\\\\theta_{X\\\\perp}$ is orthogonal to all possible mixings of points in $X$. Hence, $\\\\theta_{X\\\\perp}$ does not affect the Mixup loss or the interpolating property, so for simplicity we may just assume $\\\\theta$ lies in the span of $X$.\\n\\nTo characterize the conditions on $X$ under which the maximum margin solution interpolates the data, we use a key result of Muthukumar et al. (2020), restated below. Note that Muthukumar et al. (2020) actually provide more settings in their paper, but we constrain ourselves to the one stated below for simplicity.\\n\\nLemma 3.7. [Theorem 1 in Muthukumar et al. (2020), Rephrased] Assuming $d > 10n \\\\ln n + n - 1$, then with probability at least $1 - 2/n$, the maximum margin solution for $X$ is also an interpolating solution.\\n\\nTo tie the optimal Mixup classifier back to the classifier learned through standard training, we appeal to the fact that minimizing the empirical cross-entropy of a linear model using gradient descent leads to learning the maximum margin solution on linearly separable data (Soudry et al., 2018; Ji & Telgarsky, 2018). From this we obtain the desired result of this subsection:\\n\\nCorollary 3.8. Under the same conditions as Lemma 3.7, the optimal Mixup classifier has the same direction as the classifier learned through minimizing the empirical cross-entropy using gradient descent with high probability.\\n\\n4 Conclusion\\n\\nThe main contribution of our work has been to provide a theoretical framework for analyzing how Mixup training can differ from empirical risk minimization. Our results characterize a practical failure case of Mixup, and also identify conditions under which Mixup can provably minimize the original risk. They also show in the sense of margin why the generalization of Mixup classifiers can be superior to those learned through empirical risk minimization, while again identifying model classes and datasets for which the generalization of a Mixup classifier is no different (with high probability). We also emphasize that the generality of our theoretical framework allows most of our results to hold for any continuous mixing distribution. Our hope is that the tools developed in this work will see applications in future works concerned with analyzing the relationship between benefits obtained from Mixup training and properties of the training data.\"}"}
{"id": "ieNJYujcGDO", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We do not anticipate any direct misuses of this work due to its theoretical nature. That being said, the failure case of Mixup discussed in Section 2 could serve as a way for an adversary to potentially exploit a model trained using Mixup to classify data incorrectly. However, as this requires knowledge of the mixing distribution and other hyperparameters of the model, we do not flag this as a significant concern - we would just like to point it out for completeness.\\n\\nFull proofs for all results in the main body of the paper can be found in Sections C and E of the Appendix. All of the code used to generate the plots and experimental results in this paper can be found at: https://github.com/2014mchidamb/Mixup-Data-Dependency. We have tried our best to organize the code to be easy to use and extend. Detailed instructions for how to run each type of experiment are provided in the README file included in the GitHub repository.\\n\\nRong Ge, Muthu Chidambaram, Xiang Wang, and Chenwei Wu are supported in part by NSF Award DMS-2031849, CCF-1704656, CCF-1845171 (CAREER), CCF-1934964 (Tripods), a Sloan Research Fellowship, and a Google Faculty Research Award. Muthu would like to thank Michael Lin for helpful discussions during the early stages of this project.\\n\\nEric Arazo, Diego Ortego, Paul Albert, Noel E O'Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction. arXiv preprint arXiv:1904.11238, 2019.\\n\\nGuillaume P. Archambault, Yongyi Mao, Hongyu Guo, and Richong Zhang. Mixup as directional adversarial training. CoRR, abs/1906.06875, 2019. URL http://arxiv.org/abs/1906.06875.\\n\\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00e9-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf.\\n\\nLars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Ga\u00ebl Varoquaux. API design for machine learning software: experiences from the scikit-learn project. CoRR, abs/1309.0238, 2013. URL http://arxiv.org/abs/1309.0238.\\n\\nLuigi Carratino, Moustapha Ciss\u00e9, Rodolphe Jenatton, and Jean-Philippe Vert. On mixup regularization, 2020.\\n\\nChing-Yao Chuang and Youssef Mroueh. Fair mixup: Fairness via interpolation. CoRR, abs/2103.06503, 2021. URL https://arxiv.org/abs/2103.06503.\\n\\nRemi Tachet des Combes, Mohammad Pezeshki, Samira Shabanian, Aaron C. Courville, and Yoshua Bengio. On the learning dynamics of deep neural networks. CoRR, abs/1809.06848, 2018. URL http://arxiv.org/abs/1809.06848.\\n\\nMahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George J. Pappas. Efficient and accurate estimation of lipschitz constants for deep neural networks, 2019.\"}"}
{"id": "ieNJYujcGDO", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Since the above also holds for a sufficiently small neighborhood about $x$, we have the desired result.\\n\\n**E.2 Proof of Theorem 3.5**\\n\\nLet us first recall the setting of Theorem 3.5, since it is more specialized than that of the previous results.\\n\\n**Setting.**\\n\\nWe consider the case of binary classification using a linear model $\\\\theta^\\\\top x$ on high-dimensional Gaussian data, which is a setting that arises naturally when training using Gaussian kernels. Specifically, we consider the dataset $X$ to consist of $n$ points in $\\\\mathbb{R}^d$ distributed according to $N(0, I^d)$ with $d > n$ (to be made more precise shortly). We let the labels of points in $X$ be $\\\\pm 1$ (so that the sign of $\\\\theta^\\\\top x$ is the classification), and use $X_1$ and $X_{-1}$ to denote the individual class points. Additionally, we let $n_1 = |X_1|$ and $n_2 = |X_{-1}|$.\\n\\nBefore introducing the proof of Theorem 3.5, we first present a lemma that will be necessary in the proof.\\n\\n**Lemma E.1.** [Strict Convexity of $J_{\\\\text{mix}}$ on Data Span] Suppose $n_1 = n_2 = 1$, i.e. there are two data points $x, z$ with opposite labels. If $x$ and $z$ are linearly independent, then $J_{\\\\text{mix}}$ is strictly convex with respect to $\\\\theta$ on the span of $x$ and $z$.\\n\\nThe reason this lemma focuses on the two data point case is that in the proof of Theorem 3.5 we will break up the Mixup loss into the sum over these cases. With this lemma, we may now prove Theorem 3.5. Before doing so, we point out that the version of $J_{\\\\text{mix}}$ considered here is after composition with the logistic loss (since we are considering binary classification with a linear classifier).\\n\\n**Theorem 3.5.**\\n\\nIf the maximum margin solution for $X$ is also an interpolating solution for $X$, then any $\\\\theta$ that lies in the span of $X$ and minimizes the Mixup loss $J_{\\\\text{mix}}$ for a symmetric mixing distribution $P_f$ is a rescaling of the maximum margin solution.\\n\\n**Proof.**\\n\\nSince $d > n$, we have that all of the points $\\\\{x_i\\\\}_{i=1}^{n_1}, \\\\{z_j\\\\}_{j=1}^{n_2}$ are linearly independent with probability one. We will break the proof into two parts. In doing so, we make the following important observation: it suffices to prove the result for mixings of distinct points. This is because an interpolating solution (as given in Definition 3.3) is immediately seen to be optimal for the ERM part of $J_{\\\\text{mix}}$ (the terms corresponding to mixing points with themselves). Thus, in what follows, we disclude these ERM terms from $J_{\\\\text{mix}}$ to simplify the presentation.\\n\\n**Part I:** $n_1 = n_2 = 1$.\\n\\nDenote $\\\\theta^\\\\top x_1 = u$ and $\\\\theta^\\\\top z_1 = -v$, then $J_{\\\\text{mix}} = E_\\\\lambda \\\\left[ \\\\lambda \\\\log(1 + \\\\exp(-\\\\lambda u + (1 - \\\\lambda)v)) + (1 - \\\\lambda) \\\\log(1 + \\\\exp(\\\\lambda u - (1 - \\\\lambda)v)) \\\\right]$. (2)\\n\\nWhere $\\\\lambda$ is distributed according to $P_f$ which is symmetric and has full support on $[0, 1]$. Therefore, we can do the change of variables $\\\\lambda := 1 - \\\\lambda$, and $J_{\\\\text{mix}} = E_\\\\lambda \\\\left[ \\\\lambda \\\\log(1 + \\\\exp(-\\\\lambda v + (1 - \\\\lambda)u)) + (1 - \\\\lambda) \\\\log(1 + \\\\exp(\\\\lambda v - (1 - \\\\lambda)u)) \\\\right]$. (3)\\n\\nCombining Eq.(2) and Eq.(3), we know if $(u, -v)$ is a global minimum of $J_{\\\\text{mix}}$, then so is $(v, -u)$. But the strict convexity in Lemma E.1 implies such a global minimum is unique, so we must have $u = v = k(P_f)$, where $k(P_f)$ is a constant that only depends on the density of $P_f$. Furthermore, $\\\\forall \\\\lambda \\\\in [0, 1]$, define $h_\\\\lambda(k) = \\\\lambda \\\\log(1 + \\\\exp((1 - 2\\\\lambda)k)) + (1 - \\\\lambda) \\\\log(1 + \\\\exp((2\\\\lambda - 1)k))$, (4)\\n\\nthen $\\\\forall k > 0$, $h_\\\\lambda(-k) - h_\\\\lambda(k) = (1 - 2\\\\lambda) \\\\log(1 + \\\\exp((1 - 2\\\\lambda)k)) + (2\\\\lambda - 1) \\\\log(1 + \\\\exp((2\\\\lambda - 1)k)) = (1 - 2\\\\lambda) \\\\log \\\\left( \\\\frac{1 + \\\\exp((1 - 2\\\\lambda)k)}{1 + \\\\exp((2\\\\lambda - 1)k)} \\\\right) \\\\geq 0$, (5)\\n\\nand $h_\\\\lambda'(k) |_{k=0} = -\\\\frac{1}{2}(1 - 2\\\\lambda)^2 \\\\leq 0$. (6)\"}"}
{"id": "ieNJYujcGDO", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hence, we must have $k(P_f) > 0$.\\n\\nPart II: General $n_1$ and $n_2$. For the general case, we extend the observation we made prior to the proof of Part I. Namely, if we can show that an interpolating solution is optimal for mixing across the two classes, it follows immediately that the solution is optimal for all of $J_{mix}$ (it is not hard to see that the calculation for mixing points from the same class is essentially no different from the ERM case in this context, as the $\\\\lambda$ and $1 - \\\\lambda$ terms can be combined). We thus focus only on mixing across classes, and overload the $J_i,j_{mix}$ notation to indicate mixing of points $x_i$ and $z_j$, so that we may write the loss in consideration as:\\n\\n$$J_{mix}(\\\\theta) = \\\\frac{1}{n_1 n_2} \\\\sum_{i=1}^{n_1} \\\\sum_{j=1}^{n_2} J_{i,j_{mix}}(\\\\theta).$$\\n\\n(7)\\n\\nBy the proof in the previous part we know if $J_{i,j_{mix}}$ is minimized, then we must have $\\\\theta^\\\\top x_i = -\\\\theta^\\\\top z_j = k(P_f) > 0$. On the other hand, if $J_{i,j_{mix}}(\\\\theta)$ are minimized simultaneously for all pairs $(i,j)$, then clearly $J_{mix}(\\\\theta)$ is also minimized. This is possible since the data points are linearly independent, so there exists $\\\\theta \\\\in \\\\mathbb{R}^d$, such that $\\\\theta^\\\\top x_i = -\\\\theta^\\\\top z_j = k(P_f) > 0 \\\\ \\\\forall i \\\\in [n_1], \\\\forall j \\\\in [n_2]$.\\n\\n(8)\\n\\nNow we can conclude that any $\\\\theta$ that minimizes the Mixup loss $J_{mix}$ is an interpolating solution. Restricting $\\\\theta$ to the span of $X_{fin}$ finishes the proof.\\n\\nE.2.1 PROOF OF SUPPORTING LEMMA\\n\\nLemma E.1. [Strict Convexity of $J_{mix}$ on Data Span] Suppose $n_1 = n_2 = 1$, i.e. there are two data points $x, z$ with opposite labels. If $x$ and $z$ are linearly independent, then $J_{mix}$ is strictly convex with respect to $\\\\theta$ on the span of $x$ and $z$.\\n\\nProof. We note again that it suffices to prove the strict convexity with respect to only the mixings of different points, as the ERM part is clearly strictly convex and the sum of two strictly convex functions remains strictly convex. Denote $f(\\\\lambda) = \\\\lambda x + (1 - \\\\lambda) z$, then $J_{mix}$ can be expressed as\\n\\n$$J_{mix}(\\\\theta) = \\\\mathbb{E}_\\\\lambda \\\\left[ \\\\lambda \\\\log (1 + \\\\exp(-\\\\theta^\\\\top f(\\\\lambda))) + (1 - \\\\lambda) \\\\log (1 + \\\\exp\\\\{\\\\theta^\\\\top f(\\\\lambda)\\\\}) \\\\right].$$\\n\\n(10)\\n\\nWhere again $\\\\lambda \\\\sim P_f$. Note that the second term in Eq.(10) is linear in $\\\\theta$, hence the Hessian of $J_{mix}$ can be written as\\n\\n$$\\\\nabla^2 J_{mix}(\\\\theta) = \\\\mathbb{E}_\\\\lambda [\\\\exp(-\\\\theta^\\\\top f(\\\\lambda)) (1 + \\\\exp(-\\\\theta^\\\\top f(\\\\lambda)))^2 f(\\\\lambda)^\\\\top f(\\\\lambda)] := \\\\mathbb{E}_\\\\lambda g(\\\\lambda).$$\\n\\nDefine $B := \\\\text{Span}\\\\{x,z\\\\}$. To show $J_{mix}$ is strictly convex on $B$, it suffices to show for every non-zero vector $a \\\\in B$, we always have $a^\\\\top \\\\nabla^2 J_{mix}(\\\\theta) a > 0$.\\n\\n(12)\\n\\nNote that $g(\\\\lambda)$ is continuous w.r.t. $\\\\lambda$ and that $P_f$ has full support on $[0,1]$, it suffices to show either $a^\\\\top f(0) f(0)^\\\\top a > 0$ or $a^\\\\top f(1) f(1)^\\\\top a > 0$, which is equivalent to either $a^\\\\top x \\\\neq 0$ or $a^\\\\top z \\\\neq 0$.\\n\\n(13)\\n\\n(14)\\n\\nThis is obvious since $a$ is a non-zero vector in $B$, and that $x$ and $z$ are linearly independent.\"}"}
{"id": "ieNJYujcGDO", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 10: Decision boundary plots for $\\\\alpha = 32$, $64$ and a class separation of $0.5$.\\n\\nFigure 11: Decision boundary plots for $\\\\alpha = 128$, $512$ and a class separation of $0.5$. \"}"}
{"id": "ieNJYujcGDO", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we consider different class separations and choices of the mixing parameter $\\\\alpha$ when training on the two moons dataset, with all other experimental settings being the same as in Section 3.1. Upon decreasing the class separation to 0.1, we note that even standard training captures more of the nonlinear aspects of the data, as was observed in the prior work of Pezeshki et al. (2020).\\n\\nFigure 12: Decision boundary plots for $\\\\alpha = 1, 1024$ and a class separation of 0.1.\\n\\nFigure 13: Decision boundary plots for $\\\\alpha = 32, 64$ and a class separation of 0.1.\\n\\nFigure 14: Decision boundary plots for $\\\\alpha = 128, 512$ and a class separation of 0.1.\"}"}
{"id": "ieNJYujcGDO", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Suppose $A$ is a Mixup matrix and $P$ is a permutation matrix. Suppose $PA$ is not a permutation of the columns in $A$. We would need $Aw^* = PAw$, which is equivalent to $\\begin{bmatrix} A, PA \\\\ \\end{bmatrix} \\\\begin{bmatrix} (w^*)^\\\\top, -w^\\\\top \\\\end{bmatrix}^\\\\top = 0$ (here $\\\\begin{bmatrix} (w^*)^\\\\top, -w^\\\\top \\\\end{bmatrix}^\\\\top$ indicates the vector resulting from concatenating $-w$ to $w^*$). According to Lemma C.2, we know the rank of $[A, PA]$ is at least $m+1$, which implies that the solution set of $w^*$ is at most $m-1$.\\n\\nSo fixing $A$ and $P$, the set of non-recoverable $w^*$ has measure zero. There are only a finite number of combinations of $A$ and $P$. Thus, considering all of these $A$ and $PA$, the full set of non-recoverable $w^*$ still has measure zero.\\n\\nC.2.1 Proof of Supporting Lemma\\n\\nLemma C.2. Assume $m > 6$, and $P \\\\in \\\\mathbb{R}^{m^2 \\\\times m^2}$ is a permutation matrix. If $PA$ is not a permutation of the columns of $A$, then the rank of $[A, PA]$ is larger than $m$.\\n\\nProof. First, we show that both the ranks of $A$ and $PA$ are $m$. For all $i \\\\in [m-1]$, define $u_i = e_1 + e_{i+1}$, and define $u_m = e_2 + e_3$. Note that these $m$ vectors are all rows of $A$. The first $(m-1)$ vectors $\\\\{u_i\\\\}_{i=1}^{m-1}$ are linearly independent because each $u_i$ has a unique direction $e_{i+1}$ that is not a linear combination of any other vectors in $\\\\{u_i\\\\}_{i=1}^{m-1}$. Besides, we know that the span of $\\\\{u_i\\\\}_{i=1}^{m-1}$ is a subspace of $\\\\{v \\\\in \\\\mathbb{R}^m: \\\\sum_{i=2}^{m} v(i) = v(1)\\\\}$ where $v(i)$ is the $i$-th entry of $v$. Therefore, $u_m$ doesn't lie in the span of $\\\\{u_i\\\\}_{i=1}^{m-1}$, which implies that these $n$ vectors $\\\\{u_i\\\\}_{i=1}^{n}$ are linearly independent. This shows that $A$ is at least rank $m$. Since $A$ only has $m$ columns, we know that the rank of $A$ is $m$. The matrix $PA$ is also rank $m$ because $P$ is full rank.\\n\\nTherefore, to show that the rank of $[A, PA]$ is larger than $m$, we only need to find a vector $v$ that lies in the column span of $A$ but is not in the column span of $PA$. To do this, we need the following claim:\\n\\nClaim 1. There exists a row index subset $I \\\\subseteq \\\\{m^2\\\\}$ with size $(m-1)$ such that the sub-matrix composed by the rows of $A$ with indices in $I$ has a column that is all-one vector, but every column of the sub-matrix composed by the rows of $PA$ with indices in $I$ is not all-one vector.\\n\\nProof of Claim 1. By the definition of $A$, each column of $A$ has only $(m-1)$ ones and the other entries are zero. Therefore, the position of $(m-1)$ ones in a column of $A$ can uniquely determine that column vector. Besides, for an index set $I$ with size $(m-1)$, the sub-matrix composed by the rows of $A$ with indices in $I$ cannot have two columns that are both all-one vector. This is because otherwise $A$ will have duplicate rows, which contradicts the definition of $A$. Therefore, there are $m$ possible choices of $I$, each of which corresponds to the positions of all ones in a column of $A$.\\n\\nAssume by contradiction that for these $m$ choices of $I$, there exist a column of the sub-matrix composed by the rows of $PA$ with indices in $I$ that is all-one vector. Then these $m$ choices of $I$ also correspond to the positions of all ones in a column of $PA$. This means that the columns of $A$ and $PA$ are the same up to permutations, which contradicts the assumption of our theorem. This finishes the proof of this claim.\\n\\nNow define $B_1$ as the sub-matrix composed by the rows of $A$ with indices in $I$, and $C_1$ as the sub-matrix composed by the rows of $PA$ with indices in $I$. Without loss of generality, suppose $I = [m-1]$, and suppose the first column of $B_1$ is all-one vector. Let $u = -e_1 + \\\\sum_{i=2}^{m^2} e_i \\\\in \\\\mathbb{R}^{m^2}$, we know that $Au = 2 \\\\sum_{i=2}^{m^2} e_i \\\\in \\\\mathbb{R}^{m^2}$, i.e., the first $(m-1)$ entries of $Au$ are $0$, and the other entries are $2$. Define $v = Au$, we are going to show that $v$ is not in the column span of $PA$. Let $C_2$ be the sub-matrix in $PA$ consisting of the rows that are not in $C_1$, then the following claim shows that $C_2$ has full column rank.\\n\\nClaim 2. $\\\\text{rank}(C_2) = m$.\\n\\nProof of Claim 2. In this proof, we will consider each column of $C_2$ as a vertex. Since each row of $C_2$ has only two $1$s, we view each row as an edge connecting the two columns which correspond to the two $1$s in that row. From the definition of $A$ we know that the graph we constructed is a simple undirected graph. Then we are going to show that we can select $m$ \\\"edges\\\" from $C_2$ which are linearly independent.\"}"}
{"id": "ieNJYujcGDO", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"There are \\\\( (m^2) - (m - 1) \\\\) edges in \\\\( C_2 \\\\). From \\\\( m > 6 \\\\) we know that \\\\( (m^2) - (m - 1) > m^2 / 4 \\\\), so from Tur\u00e1n's theorem, \\\\( C_2 \\\\) contains at least one triangle. Assume this triangle is \\\\( (i,j,k) \\\\), then we select edges \\\\( (i,j) \\\\), \\\\( (j,k) \\\\) and \\\\( (i,k) \\\\) and define \\\\( E_3 = \\\\{i,j,k\\\\} \\\\). \\\\( \\\\forall r \\\\in [m], r \\\\geq 3 \\\\), we select an edge connecting \\\\( E_r \\\\) with \\\\( [m] \\\\setminus E_r \\\\). Assume that edge is \\\\( (s,t) \\\\) where \\\\( s \\\\in E_r \\\\) and \\\\( t \\\\not\\\\in E_r \\\\), we then add \\\\( t \\\\) to \\\\( E_r \\\\), i.e., \\\\( E_r + 1 = E_r \\\\cup \\\\{t\\\\} \\\\). In the next two paragraphs, we are going to show that there are always edges between \\\\( E_r \\\\) and \\\\( [m] \\\\setminus E_r \\\\) (so we can successfully select \\\\( m \\\\) edges in total), and the \\\\( m \\\\) edges we selected are linearly independent.\\n\\nIn matrix \\\\( PA \\\\), there are \\\\( r (m - r) \\\\) edges between \\\\( E_r \\\\) and \\\\( [m] \\\\setminus E_r \\\\). Since \\\\( C_2 \\\\) is constructed by deleting \\\\( (m - 1) \\\\) edges from \\\\( PA \\\\), the number of edges left between \\\\( E_r \\\\) and \\\\( [m] \\\\setminus E_r \\\\) is at least \\\\( r (m - r) - (m - 1) \\\\). When \\\\( 3 \\\\leq r \\\\leq m - 2 \\\\), we have \\\\( r (m - r) - (m - 1) > 0 \\\\). When \\\\( r = m - 1 \\\\), the only case where there is no edge between \\\\( E_r \\\\) and \\\\( [m] \\\\setminus E_r \\\\) is when all edges from vertex \\\\( [m] \\\\setminus E_r \\\\) is in \\\\( C_1 \\\\), which means that \\\\( C_1 \\\\) has a column that is all-one vector and is a contradiction. Therefore, there are always edges between \\\\( E_r \\\\) and \\\\( [m] \\\\setminus E_r \\\\) and we can successfully select \\\\( m \\\\) edges.\\n\\nThen we only need to show that these \\\\( m \\\\) selected edges are linearly independent. We use \\\\( \\\\{u_i\\\\}_{i=1}^m \\\\) to denote the vectors that correspond to these edges, i.e., \\\\( u_1 = e_i + e_j \\\\), \\\\( u_2 = e_j + e_k \\\\), \\\\( u_3 = e_i + e_k \\\\), \\\\( \\\\cdots \\\\). Assume by contradiction that they are linearly independent, then there exists \\\\( x \\\\in \\\\mathbb{R}^m \\\\), \\\\( x \\\\neq 0 \\\\) such that \\\\( \\\\sum_{i=1}^m x_i u_i = 0 \\\\). By the selection process of the edges, we know that \\\\( \\\\forall r \\\\geq 4 \\\\), \\\\( u_r \\\\) has a unique direction, so \\\\( x_r = 0 \\\\). Therefore, \\\\( x(1) u_1 + x(2) u_2 + x(3) u_3 = 0 \\\\). Since \\\\( \\\\{u_1, u_2, u_3\\\\} \\\\) are linearly independent, we have \\\\( x = 0 \\\\), which is a contradiction. Thus, these \\\\( m \\\\) selected edges are linearly independent, proving the claim.\\n\\nNow assume by contradiction that \\\\( v \\\\) is in the column span of \\\\( PA \\\\), then \\\\( \\\\exists w \\\\in \\\\mathbb{R}^m \\\\) such that \\\\( v = PA w \\\\). Let \\\\( v_2 \\\\in \\\\mathbb{R}^{(m^2) - (m - 1)} \\\\) be the bottom \\\\( (m^2) - (m - 1) \\\\) entries of \\\\( v \\\\), then \\\\( v_2 = C_2 w \\\\). Define \\\\( w_0 \\\\) to be the all-one vector in \\\\( \\\\mathbb{R}^m \\\\), we know that \\\\( w_0 \\\\) is a valid solution to \\\\( v_2 = C_2 w \\\\). Since \\\\( C_2 \\\\) has full column rank, \\\\( w_0 \\\\) must be the unique solution to \\\\( v_2 = C_2 w \\\\). This implies that \\\\( v = PA w_0 \\\\). However, we know that \\\\( PA w_0 = 2 \\\\sum_{i=1}^{m^2} e_i' \\\\neq v \\\\), which is a contradiction. Thus, \\\\( v \\\\) is not in the column span of \\\\( A \\\\), which finishes the proof of the lemma.\\n\\n### D.1 Experiments for Section 2.3\\n\\nAs noted in the main paper, it is not difficult to extend Definition 2.6 to construct datasets on which Mixup training empirically fails for small values of \\\\( \\\\alpha \\\\). The observation to make is that a major part of the proof of Theorem 2.7 is the fact that the same-point mixing probability of point 1 is small relative to the same-class mixing probability of class 0. As the former probability decreases quadratically with the dataset size \\\\( m \\\\), we are motivated to consider extensions of \\\\( X_{23} \\\\) consisting of more points and more classes. Towards that end, we consider the following generalization of \\\\( X_{23} \\\\):\\n\\n**Definition D.1** (\\\\( m \\\\)-Point \\\\( k \\\\)-Class Alternating Line). We define \\\\( X_{km} \\\\) to be the \\\\( k \\\\)-class classification dataset consisting of the points \\\\( \\\\{0, 1, \\\\ldots, m-1\\\\} \\\\) classified according to their value mod \\\\( k \\\\) incremented by 1. As before, \\\\( P_{X} \\\\) is the normalized counting measure on \\\\( X_{km} \\\\).\\n\\nWe now consider training a two-layer feedforward network on \\\\( X_{210} \\\\) and \\\\( X_{1010} \\\\) using the same procedure and hyperparameters as in the main paper. The results are shown in Figure 8 below. Here we see that even at \\\\( \\\\alpha = 1 \\\\), Mixup training fails to minimize the original empirical risk on \\\\( X_{210} \\\\) and \\\\( X_{1010} \\\\), whereas in the main paper we noted that at \\\\( \\\\alpha = 1 \\\\) Mixup training had no issues minimizing the original risk on \\\\( X_{23} \\\\). Interestingly, we find that even standard training does not completely minimize the original empirical risk on \\\\( X_{210} \\\\), once again perhaps a result of the regularity of the two-layer network model (although this is merely a hypothesis, and we do not analyze this phenomenon further here).\\n\\n### D.2 Experiments for Section 2.4\\n\\nHere we include the additional experiments mentioned in Section 2.4, namely the results of training ResNet-18 on MNIST, CIFAR-10, and CIFAR-100 with and without Mixup for \\\\( \\\\alpha = 1, 32, 128 \\\\) (with the other hyperparameters being as described in Section 2.4). The results are shown in Figure 9.\"}"}
{"id": "ieNJYujcGDO", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Training error plots for Mixup and regular training on $X_{210}$ and $X_{1010}$. Each curve corresponds to the mean of 10 training runs, and the area around each curve represents a region of one standard deviation. Note that the ERM curves appear slightly different across $\\\\alpha$ values due to changes in $y$-axis scale.\\n\\nAs mentioned in the main body, these choices of $\\\\alpha$ only lead to Mixup performing more similarly to ERM than $\\\\alpha = 1024$.\\n\\nE U L P R O O F S F O R S E C T I O N 3\\n\\nE.1 P R O O F O F T H E O R E M 3.2\\n\\nTheorem 3.2. Consider $k$-class classification where the supports $X_1, \\\\ldots, X_k$ are finite and $P_X$ corresponds to the discrete uniform distribution. If a point $x$ satisfies Assumption 3.1 with respect to a class $i$, then for every $h \\\\in \\\\arg\\\\min_{g \\\\in C^*} J_{\\\\text{mix}}(g, P_X, P_f)$, we have that $h$ classifies $x$ as class $i$ and that $h$ is continuous at $x$.\\n\\nProof. For two points $p, q$ in the supports $X_1, \\\\ldots, X_k$ with a line segment between them intersecting $x$, let $\\\\lambda(p, q, x)$ denote the value of $\\\\lambda$ for which $\\\\lambda p + (1 - \\\\lambda) q = x$. Since the supports are finite, we have that there exists $\\\\epsilon_1 > 0$ such that for all $\\\\epsilon \\\\leq \\\\epsilon_1$ we have that:\\n\\n$$\\n\\\\xi_{i,j}(x, \\\\epsilon) = \\\\sum_{p} \\\\sum_{q} \\\\int_{B_\\\\epsilon(\\\\lambda(p, q, x))} dP_f\\n$$\\n\\nWhere the summations are over all points $p, q$ with line segments containing $x$. Now we have by the Lebesgue differentiation theorem applied to the integral term in the summations above (this\"}"}
{"id": "ieNJYujcGDO", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Mean and single standard deviation of 5 training runs for Mixup ($\\\\alpha = 1$, 32, 128) and ERM on the original training data.\\n\\nNow by Assumption 3.1, we need only consider the $\\\\xi_{i,i,x,\\\\epsilon}$, $\\\\xi_{i,j,x,\\\\epsilon}$, $\\\\xi_{i,j,x,\\\\epsilon,\\\\lambda}$ terms (as there are no other line segments that contain $x$). Furthermore, we have that there exists $\\\\epsilon_2 > 0$ such that for all $\\\\epsilon \\\\leq \\\\epsilon_2$ we have $\\\\lambda(p,q,x) - \\\\epsilon \\\\geq \\\\frac{1}{2}$ for $p \\\\in X_i$ and $q \\\\in X_j$, and that the measure of $A_{i,j,x,\\\\epsilon}$ is at least that of $A_{j,i,x,\\\\epsilon}$.\\n\\nFrom this we get that for all $\\\\epsilon \\\\leq \\\\min(\\\\epsilon_1, \\\\epsilon_2)$:\\n\\n$$h_{i,\\\\epsilon} = \\\\xi_{i,i,x,\\\\epsilon} + \\\\sum_{j \\\\neq i} (\\\\xi_{i,j,x,\\\\epsilon,\\\\lambda} + (\\\\xi_{j,i,x,\\\\epsilon} - \\\\xi_{j,i,x,\\\\epsilon,\\\\lambda}))$$\\n\\n$$\\\\geq \\\\xi_{i,i,x,\\\\epsilon} + \\\\sum_{j \\\\neq i} (\\\\xi_{i,j,x,\\\\epsilon,\\\\lambda} + \\\\xi_{j,i,x,\\\\epsilon,\\\\lambda} + (\\\\xi_{j,i,x,\\\\epsilon} - \\\\xi_{j,i,x,\\\\epsilon,\\\\lambda}) + (\\\\xi_{i,j,x,\\\\epsilon} - \\\\xi_{i,j,x,\\\\epsilon,\\\\lambda}))$$\\n\\n$$\\\\geq \\\\frac{1}{2}$$\"}"}
{"id": "ieNJYujcGDO", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: A visualization of Assumption 2.9, i.e. the \u201cno collinearity\u201d assumption.\\n\\nFigure 7: A visualization of Assumption 3.1. Once again, the key idea is that $x$ falls at most $\\\\delta$ away from $X_i$ on every line between $X_i$ and $X_j$ that intersects it.\"}"}
{"id": "ieNJYujcGDO", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proposition 2.2. Any function \\\\( h \\\\in \\\\arg\\\\min_{g \\\\in C^*} J_{\\\\text{mix}}(g, P_X, P_f) \\\\) satisfies \\\\( J_{\\\\text{mix}}(h) \\\\leq J_{\\\\text{mix}}(g) \\\\) for any continuous \\\\( g \\\\in C \\\\).\\n\\nProof. Intuitively, the idea is that when \\\\( h \\\\) and \\\\( g \\\\) differ at a point \\\\( x \\\\in X_{\\\\text{mix}} \\\\), there must be a neighborhood of \\\\( x \\\\) for which the constant function that takes value \\\\( h(x) \\\\) has lower loss than \\\\( g \\\\) due to the continuity constraint on \\\\( g \\\\). We formalize this below.\\n\\nLet \\\\( h \\\\) be an arbitrary function in \\\\( \\\\arg\\\\min_{g \\\\in C^*} J_{\\\\text{mix}}(g, P_X, P_f) \\\\) and let \\\\( g \\\\) be a continuous function in \\\\( C \\\\). Consider a point \\\\( x \\\\in X_{\\\\text{mix}} \\\\) such that the limit in Definition 2.1 exists and that \\\\( h(x) \\\\neq g(x) \\\\) (if such an \\\\( x \\\\) did not exist, we would be done). Now let \\\\( \\\\theta_h \\\\) and \\\\( \\\\theta_g \\\\) be the constant functions whose values are \\\\( h(x) \\\\) and \\\\( g(x) \\\\) (respectively) on all of \\\\( \\\\mathbb{R}^n \\\\), and further let \\\\( \\\\theta_h \\\\delta = \\\\arg\\\\min_{\\\\theta \\\\in [0, 1]} k J_{\\\\text{mix}}(\\\\theta) \\\\vert_{B_\\\\delta(x)} \\\\) (this is shown to be a single value in the proof of Lemma 2.3 below). Finally, as a convenient abuse of notation, we will use \\\\( J_{\\\\text{mix}}(\\\\epsilon') \\\\vert_{B_\\\\delta(x)} \\\\) to indicate the result of replacing all \\\\( \\\\log g_i \\\\) terms in the integrands of \\\\( J_{\\\\text{mix}}(g) \\\\vert_{B_\\\\delta(x)} \\\\) with \\\\( \\\\epsilon' \\\\), as shown below (note that in doing so, we can combine the \\\\( \\\\lambda \\\\) and \\\\( 1 - \\\\lambda \\\\) terms from mixing classes \\\\( i \\\\) and \\\\( j \\\\); this simplifies the \\\\( J_{i,j,\\\\text{mix}} \\\\) expression obtained in the proof of Lemma 2.3).\\n\\n\\\\[\\nJ_{\\\\text{mix}}(\\\\epsilon') \\\\vert_{B_\\\\delta(x)} = -\\\\epsilon' \\\\sum_{i=1}^k \\\\sum_{j=1}^k \\\\xi_{i,j}(x,\\\\epsilon,\\\\lambda) \\\\log \\\\theta_i + (\\\\xi_{i,j}(x,\\\\epsilon) - \\\\xi_{i,j}(x,\\\\epsilon,\\\\lambda)) \\\\log \\\\theta_j)\\n\\\\]\\n\\nSince \\\\( \\\\theta_h \\\\neq \\\\theta_g \\\\), we have that there exists a \\\\( \\\\delta' > 0 \\\\) such that for \\\\( \\\\delta \\\\leq \\\\delta' \\\\) we have \\\\( |\\\\theta_h(\\\\delta) - \\\\theta_g(\\\\delta)| = \\\\epsilon > 0 \\\\).\\n\\nFrom this we get that there exists \\\\( \\\\epsilon' > 0 \\\\) depending only on \\\\( \\\\epsilon \\\\) and \\\\( \\\\theta_g \\\\) such that:\\n\\n\\\\[\\nJ_{\\\\text{mix}}(\\\\theta_g) \\\\vert_{B_\\\\delta(x)} - J_{\\\\text{mix}}(\\\\theta_h) \\\\vert_{B_\\\\delta(x)} \\\\geq J_{\\\\text{mix}}(\\\\epsilon') \\\\vert_{B_\\\\delta(x)}\\n\\\\]\\n\\nNow by the continuity of \\\\( g \\\\) (and thus the continuity of \\\\( \\\\log g_i \\\\)), we may choose \\\\( \\\\delta \\\\leq \\\\delta' \\\\) such that \\\\( J_{\\\\text{mix}}(g) \\\\vert_{B_\\\\delta(x)} \\\\in J_{\\\\text{mix}}(\\\\theta_g) \\\\vert_{B_\\\\delta(x)} \\\\pm J_{\\\\text{mix}}(\\\\epsilon') \\\\vert_{B_\\\\delta(x)} \\\\). This implies \\\\( J_{\\\\text{mix}}(g) \\\\vert_{B_\\\\delta(x)} \\\\geq J_{\\\\text{mix}}(\\\\theta_h) \\\\vert_{B_\\\\delta(x)} \\\\), and since \\\\( x \\\\) was arbitrary (within the initially mentioned constraints, as outside of them \\\\( h \\\\) is unconstrained) we have the desired result.\\n\\nLemma 2.3. For any point \\\\( x \\\\in X_{\\\\text{mix}} \\\\) and \\\\( \\\\epsilon > 0 \\\\), there exists a continuous function \\\\( h_\\\\epsilon \\\\) satisfying:\\n\\n\\\\[\\nh_\\\\epsilon(x) = \\\\xi_{i,i}(x,\\\\epsilon) + \\\\sum_{j \\\\neq i} (\\\\xi_{i,j}(x,\\\\epsilon,\\\\lambda) + (\\\\xi_{j,i}(x,\\\\epsilon) - \\\\xi_{j,i}(x,\\\\epsilon,\\\\lambda))) \\\\sum_{q=1}^k \\\\xi_{q,q}(x,\\\\epsilon) + \\\\sum_{j \\\\neq q} (\\\\xi_{q,j}(x,\\\\epsilon,\\\\lambda) + (\\\\xi_{j,q}(x,\\\\epsilon) - \\\\xi_{j,q}(x,\\\\epsilon,\\\\lambda)))\\n\\\\]\\n\\nWith the property that \\\\( \\\\lim_{\\\\epsilon \\\\to 0} h_\\\\epsilon(x) = h(x) \\\\) for every \\\\( h \\\\in \\\\arg\\\\min_{g \\\\in C^*} J_{\\\\text{mix}}(g, P_X, P_f) \\\\) when the limit exists.\\n\\nProof. First, the condition that \\\\( x \\\\in X_{\\\\text{mix}} \\\\) is necessary, since if \\\\( \\\\bigcup_{i,j} A_{i,j} x,\\\\epsilon \\\\) has measure zero the \\\\( \\\\text{LHS of Equation 1} \\\\) is not even defined.\\n\\nNow we simply take \\\\( h_\\\\epsilon(x) = \\\\arg\\\\min_{\\\\theta \\\\in [0, 1]} k J_{\\\\text{mix}}(\\\\theta) \\\\vert_{B_\\\\epsilon(x)} \\\\) as in Definition 2.1 and show that \\\\( h_\\\\epsilon \\\\) is well-defined. Since the \\\\( \\\\arg\\\\min \\\\) is over constant functions, we may unpack the definition of \\\\( J_{\\\\text{mix}} \\\\vert_{B_\\\\epsilon(x)} \\\\) and pull each of the \\\\( \\\\log \\\\theta_i(z) \\\\) terms out of the integrands and rewrite them simply as \\\\( \\\\log \\\\theta_i \\\\). Doing so, we obtain:\\n\\n\\\\[\\nJ_{i,j,\\\\text{mix}} = -\\\\xi_{i,j}(x,\\\\epsilon,\\\\lambda) \\\\log \\\\theta_i + (\\\\xi_{i,j}(x,\\\\epsilon) - \\\\xi_{i,j}(x,\\\\epsilon,\\\\lambda)) \\\\log \\\\theta_j)\\n\\\\]\\n\\nPlugging the above back into \\\\( J_{\\\\text{mix}} \\\\) and collecting the \\\\( \\\\log \\\\theta_i \\\\) terms as \\\\( i = 1, \\\\ldots, k \\\\) we get:\\n\\n\\\\[\\nJ_{\\\\text{mix}}(g, P_X, P_f) \\\\vert_{B_\\\\epsilon(x)} = -k \\\\sum_{i=1}^k (\\\\xi_{i,i}(x,\\\\epsilon) + \\\\sum_{j \\\\neq i} (\\\\xi_{i,j}(x,\\\\epsilon,\\\\lambda) + (\\\\xi_{j,i}(x,\\\\epsilon) - \\\\xi_{j,i}(x,\\\\epsilon,\\\\lambda))) \\\\sum_{q=1}^k \\\\xi_{q,q}(x,\\\\epsilon) + \\\\sum_{j \\\\neq q} (\\\\xi_{q,j}(x,\\\\epsilon,\\\\lambda) + (\\\\xi_{j,q}(x,\\\\epsilon) - \\\\xi_{j,q}(x,\\\\epsilon,\\\\lambda))) \\\\log \\\\theta_i)\\n\\\\]\"}"}
{"id": "ieNJYujcGDO", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We have thus far defined the first part of the summation above from mixing $X$. We must have that $h$ wish to show that $\\\\theta$. As such, we may consider each of the $\\\\theta$.\\n\\nProof. In the symmetric case, we have $\\\\xi$.\\n\\nProof. Considering the point $x$, we note that only $x$ have $h$.\\n\\nOn the other hand, every other point on the line segment connecting the points in $X$ mix-optimal value at $x,\\\\epsilon$. Furthermore, we have that $h$ uniform over $X$. Let $\\\\epsilon$.\\n\\nFinally, if $\\\\lim$ by the Tietze Extension Theorem that $h$ $h$ thus established the continuity of $h$.\\n\\nNow we can check that $h$ $h$ $\\\\epsilon$ positive measure for every $X$ distance from $X$. $X$ $X$ $x$ closed, since any limit point $h$ to the restriction of a continuous function from $R$.\\n\\nComputed via Lagrange multipliers, and the solution is given in Equation 1. $h$ $h$ greater than 0 and less than 1. Thus, as $\\\\log$ mind, $\\\\log$ $\\\\theta$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$ $\\\\log$"}
{"id": "ieNJYujcGDO", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proof. Fix a classifier \\\\( h \\\\in \\\\mathcal{H} \\\\) as defined in Lemma 2.3 on \\\\( X \\\\). Now for \\\\( Y \\\\sim \\\\text{Beta}(\\\\alpha,\\\\alpha) \\\\) we have by the fact that \\\\( \\\\text{Beta}(\\\\alpha,\\\\alpha) \\\\) is strictly subgaussian that\\n\\\\[\\nP(\\\\left|\\\\left| Y - \\\\frac{1}{2} \\\\right|\\\\right| \\\\leq \\\\epsilon) \\\\geq 1 - 2 \\\\exp\\\\left(\\\\frac{-\\\\epsilon^2}{2\\\\sigma^2}\\\\right)\\n\\\\]\\nwhere \\\\( \\\\sigma^2 = \\\\frac{1}{4\\\\alpha + 2} \\\\) is the variance of \\\\( \\\\text{Beta}(\\\\alpha,\\\\alpha) \\\\). As a result, we can choose \\\\( \\\\alpha > \\\\frac{1}{2} \\\\left( \\\\log 4 - \\\\frac{1}{2} \\\\right) \\\\) to guarantee that\\n\\\\[\\nP(\\\\left|\\\\left| Y - \\\\frac{1}{2} \\\\right|\\\\right| \\\\leq \\\\epsilon) > \\\\frac{1}{2}\\n\\\\]\\nand therefore that \\\\( \\\\xi_{1,1}, \\\\xi_{1,2}, \\\\epsilon > \\\\frac{1}{9} = \\\\xi_{2,1}, \\\\xi_{2,2} \\\\).\\n\\nNow we have by Lemma 2.3 (or more precisely, Corollary C.1) that:\\n\\\\[\\nh(\\\\epsilon) = \\\\xi_{1,1}, \\\\xi_{1,2}, \\\\epsilon + 2 \\\\xi_{1,2}, \\\\xi_{2,1}, \\\\epsilon, \\\\lambda \\\\xi_{1,1}, \\\\xi_{1,2}, \\\\epsilon + 4 \\\\xi_{1,2}, \\\\xi_{2,1}, \\\\epsilon + \\\\xi_{2,2}, \\\\xi_{2,2} > \\\\frac{1}{2}\\n\\\\]\\nThus, we have shown that \\\\( h(\\\\epsilon) \\\\) will classify the point 1 as class 1 despite it belonging to class 2.\\n\\nProposition 2.8. Consider \\\\( k \\\\)-class classification where the supports \\\\( X_1, \\\\ldots, X_k \\\\) are finite and \\\\( P_X \\\\) corresponds to the discrete uniform distribution. Then for every \\\\( h \\\\in \\\\arg\\\\min_{g \\\\in \\\\mathcal{C}} J_{\\\\text{mix}}(g, P_X, P_f) \\\\), we have that \\\\( h_i(x) = 1 \\\\) on \\\\( X_i \\\\).\\n\\nProof. The full proof is not much more than the proof sketch in the main body. For \\\\( x \\\\in X_i \\\\), we have that \\\\( \\\\xi_{j,q}(x,\\\\epsilon) \\\\to 0 \\\\) and \\\\( \\\\xi_{j,q}(x,\\\\epsilon,\\\\lambda) \\\\to 0 \\\\) as \\\\( \\\\epsilon \\\\to 0 \\\\) for every \\\\( (j,q) \\\\neq (i,i) \\\\), while \\\\( \\\\xi_{i,i}(x,\\\\epsilon) \\\\to 1 |\\\\bigcup_i X_i|^2 \\\\) and \\\\( \\\\xi_{i,i}(x,\\\\epsilon,\\\\lambda) \\\\to \\\\mathbb{E}_{P_f}[(\\\\lambda)|\\\\bigcup_i X_i|^2] \\\\). As a result, we have\\n\\\\[\\n\\\\lim_{\\\\epsilon \\\\to 0} h_i(\\\\epsilon)(x) = 1 \\\\quad \\\\text{as desired.}\\n\\\\]\\n\\nTheorem 2.10. We consider the same setting as Proposition 2.8 and further suppose that Assumption 2.9 is satisfied. Then for every \\\\( h \\\\in \\\\arg\\\\min_{g \\\\in \\\\mathcal{C}} J_{\\\\text{mix}}(g, P_X, P_f) \\\\), we have that \\\\( h_i(x) = 1 \\\\) on \\\\( X_i \\\\) and that \\\\( h \\\\) is continuous on \\\\( X \\\\).\\n\\nProof. Obtained as a corollary of Theorem 3.2.\\n\\nC.2 Proof for Theorem 2.11\\n\\nPrior to proving Theorem 2.11, we first introduce some additional notation as well as a lemma that will be necessary for the proof.\\n\\nNotation: Throughout the following \\\\( m \\\\) corresponds to the number of data points being considered (as in Theorem 2.11), and as a shorthand we use \\\\( \\\\{1,\\\\ldots,m\\\\} \\\\) to indicate \\\\( \\\\{1,\\\\ldots,m\\\\} \\\\). Additionally, for two matrices \\\\( A \\\\) and \\\\( B \\\\), we use the notation \\\\( [A,B] \\\\) to indicate the matrix formed by the concatenation of the columns of \\\\( B \\\\) to \\\\( A \\\\). We use \\\\( e_i \\\\) to denote the \\\\( i \\\\)-th basis vector in \\\\( \\\\mathbb{R}^m \\\\), and use \\\\( e_i' \\\\) to denote the \\\\( i \\\\)-th basis vector in \\\\( \\\\mathbb{R}^{(m^2)} \\\\). Let \\\\( A \\\\in \\\\mathbb{R}^{(m^2) \\\\times m} \\\\) be the \\\"Mixup matrix\\\" where each row has two 1s and the other entries are 0, representing a mixture of the two data points whose associated indices have a 1. The \\\\( (m^2) \\\\) rows enumerate all the possible mixings of the \\\\( m \\\\) data points. In this way, \\\\( A \\\\) is uniquely defined up to a permutation of rows. We can pick any representative as our \\\\( A \\\\) matrix, and prove the following lemma.\\n\\nLemma C.2. Assume \\\\( m > 6 \\\\), and \\\\( P \\\\in \\\\mathbb{R}^{(m^2) \\\\times (m^2)} \\\\) is a permutation matrix. If \\\\( PA \\\\) is not a permutation of the columns of \\\\( A \\\\), then the rank of \\\\( [A,PA] \\\\) is larger than \\\\( m \\\\).\\n\\nUsing this lemma we can prove Theorem 2.11.\\n\\nTheorem 2.11. Suppose \\\\( \\\\{x_1,\\\\ldots,x_m\\\\} \\\\) with \\\\( m \\\\geq 6 \\\\) are sampled from \\\\( X \\\\) according to \\\\( P_X \\\\), and that \\\\( P_X \\\\) has a density. Then with probability 1, we can uniquely determine the points \\\\( \\\\{x_1,\\\\ldots,x_m\\\\} \\\\) given only the \\\\( (m^2) \\\\) midpoints \\\\( \\\\{x_{i,j}\\\\} \\\\). 1 \\\\leq i < j \\\\leq m.\\n\\nProof. We only need to show that the set of samples that cannot be uniquely determined from their midpoints has Lebesgue measure zero, since \\\\( P_X \\\\) is absolutely continuous with respect to the Lebesgue measure. It suffices to show this for the first entry (dimension) of the \\\\( x_i \\\\)'s, as the result then follows for all dimensions. Let \\\\( \\\\{x'_i\\\\} \\\\) be another sample of \\\\( m \\\\) points. For convenience, we group the first entries of the data points \\\\( \\\\{x_i\\\\} \\\\) \\\\( m i=1 \\\\) into a vector \\\\( w^* \\\\in \\\\mathbb{R}^m \\\\), and similarly obtain \\\\( w \\\\in \\\\mathbb{R}^m \\\\) from \\\\( \\\\{x'_i\\\\} \\\\). Suppose \\\\( w^* \\\\in \\\\mathbb{R}^m \\\\) is not a permutation of \\\\( w \\\\in \\\\mathbb{R}^m \\\\) but that they have the same set of Mixup points. We only need to show that the set of such \\\\( w^* \\\\) has measure zero.\"}"}
{"id": "ieNJYujcGDO", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The final Mixup cross-entropy loss is then the sum of $J_{i,j}^{\\\\text{mix}}$ over all $i,j \\\\in \\\\{1,\\\\ldots,k\\\\}$ (corresponding to all possible mixings between classes, including themselves):\\n\\n$$J_{\\\\text{mix}}(g, P_X, P_f) = \\\\sum_{i=1}^{k} \\\\sum_{j=1}^{k} J_{i,j}^{\\\\text{mix}}(g, P_X, P_f).$$\\n\\n**Relation to Prior Work.**\\n\\nWe have opted for a more general definition of the Mixup loss (at least when constrained to multi-class classification) than prior works. This is not generality for generality's sake, but rather because many of our results apply to any mixing distribution supported on $[0,1]$. One obtains the original Mixup formulation of Zhang et al. (2018) for multi-class classification on a finite dataset by taking the $X_i$ to be finite sets, and choosing $P_X$ to be the normalized counting measure (corresponding to a discrete uniform distribution). Additionally, $P_f$ is chosen to have density $\\\\text{Beta}(\\\\alpha, \\\\alpha)$, where $\\\\alpha$ is a hyperparameter.\\n\\n2.2 Mixup-Optimal Classifier\\n\\nGiven our setup, we now wish to characterize the behavior of a Mixup-optimal classifier at a point $x \\\\in \\\\mathbb{R}^n$. However, if the optimization of $J_{\\\\text{mix}}$ is considered over the class of functions $C$, this is intractable (to the best of our knowledge) due to the lack of regularity conditions imposed on functions in $C$. We thus wish to constrain the optimization of $J_{\\\\text{mix}}$ to a class of functions that is sufficiently powerful (so as to include almost all practical settings) while still allowing for local analysis. To do so, we will need the following definitions, which will also be referenced throughout the results in this section and the next:\\n\\n$$A_{i,j}(x, \\\\epsilon) = \\\\{(s, t, \\\\lambda) \\\\in X_i \\\\times X_j \\\\times [0,1] : \\\\lambda s + (1 - \\\\lambda) t \\\\in B_\\\\epsilon(x)\\\\}$$\\n\\n$$A_{i,j}(x, \\\\epsilon, \\\\delta) = \\\\{(s, t, \\\\lambda) \\\\in X_i \\\\times X_j \\\\times [0,1-\\\\delta] : \\\\lambda s + (1 - \\\\lambda) t \\\\in B_\\\\epsilon(x)\\\\}$$\\n\\n$$X_{\\\\text{mix}} = \\\\{x \\\\in \\\\mathbb{R}^n : \\\\bigcup_{i,j} A_{i,j}(x, \\\\epsilon) \\\\text{ has positive measure for every } \\\\epsilon > 0\\\\}$$\\n\\n$$\\\\xi_{i,j}(x, \\\\epsilon) = \\\\int_{A_{i,j}(x, \\\\epsilon)} d(P_X \\\\times P_X \\\\times P_f)(s, t, \\\\lambda)$$\\n\\n$$\\\\xi_{i,j}(x, \\\\epsilon, \\\\lambda) = \\\\int_{A_{i,j}(x, \\\\epsilon)} \\\\lambda d(P_X \\\\times P_X \\\\times P_f)(s, t, \\\\lambda)$$\\n\\nThe set $A_{i,j}(x, \\\\epsilon)$ represents all points in $X_i \\\\times X_j$ that have lines between them intersecting an $\\\\epsilon$-neighborhood of $x$, while the set $A_{i,j}(x, \\\\epsilon, \\\\delta)$ represents the restriction of $A_{i,j}(x, \\\\epsilon)$ to only those points whose connecting line segments intersect an $\\\\epsilon$-neighborhood of $x$ with $\\\\lambda$ values bounded by $1 - \\\\delta$ (used in Section 3). The set $X_{\\\\text{mix}}$ corresponds to all points for which every neighborhood factors into $J_{\\\\text{mix}}$.\\n\\nThe $\\\\xi_{i,j}(x, \\\\epsilon)$ term represents the measure of the set $A_{i,j}(x, \\\\epsilon)$ while $\\\\xi_{i,j}(x, \\\\epsilon, \\\\lambda)$ represents the expectation of $\\\\lambda$ over the same set. To provide better intuition for these definitions, we provide visualizations in Section B of the appendix. We can now define the subset of $C$ to which we will constrain our optimization of $J_{\\\\text{mix}}$.\\n\\n**Definition 2.1.** Let $C^*$ be the subset of $C$ for which every $h \\\\in C^*$ satisfies $h(x) = \\\\lim_{\\\\epsilon \\\\to 0} \\\\arg\\\\min_{\\\\theta \\\\in [0,1]} k J_{\\\\text{mix}}(\\\\theta) |_{B_\\\\epsilon(x)}$ for all $x \\\\in X_{\\\\text{mix}}$ when the limit exists. Here $J_{\\\\text{mix}}(\\\\theta) |_{B_\\\\epsilon(x)}$ represents the Mixup loss for a constant function with value $\\\\theta$ with the restriction of each term in $J_{\\\\text{mix}}$ to the set $A_{i,j}(x, \\\\epsilon)$.\\n\\nWe immediately justify this definition with the following proposition.\\n\\n**Proposition 2.2.** Any function $h \\\\in \\\\arg\\\\min_{g \\\\in C^*} J_{\\\\text{mix}}(g, P_X, P_f)$ satisfies $J_{\\\\text{mix}}(h) \\\\leq J_{\\\\text{mix}}(g)$ for any continuous $g \\\\in C$.\\n\\n**Proof Sketch.** We can argue directly from definitions by considering points in $X_{\\\\text{mix}}$ for which $h$ and $g$ differ.\"}"}
{"id": "ieNJYujcGDO", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proposition 2.2 demonstrates that optimizing over $C^*$ is at least as good as optimizing over the subset of $C$ consisting of continuous functions, so we cover most cases of practical interest (i.e. optimizing deep neural networks). As such, the term \\\"Mixup-optimal\\\" is intended to mean optimal with respect to $C^*$ throughout the rest of the paper. We may now characterize the classification of a Mixup-optimal classifier on $X_{mix}$.\\n\\nLemma 2.3. For any point $x \\\\in X_{mix}$ and $\\\\epsilon > 0$, there exists a continuous function $h_\\\\epsilon$ satisfying:\\n\\n\\\\[ h_\\\\epsilon(x) = \\\\sum_{i} \\\\left( \\\\sum_{j \\\\neq i} \\\\left( \\\\frac{\\\\xi_{i,j} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{q,j} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{q} \\\\left( \\\\xi_{q,q} x, \\\\epsilon + \\\\sum_{j \\\\neq q} \\\\left( \\\\frac{\\\\xi_{j,q} x, \\\\epsilon, \\\\lambda}{\\\\sum_{k=1}^{"}
{"id": "ieNJYujcGDO", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Training error for Mixup and regular training on $X_2^3$. Each curve corresponds to the mean of 10 training runs, and the area around each curve represents a region of one standard deviation.\\n\\nExperiments. The result of Theorem 2.7 leads us to believe that the Mixup training of a continuous model should fail on $X_2^3$ for appropriately chosen $\\\\alpha$. To verify that the theory predicts the experiments, we train a two-layer feedforward neural network with 512 hidden units and ReLU activations on $X_2^3$ with and without Mixup. The implementation of Mixup training does not differ from the theoretical setup; we uniformly sample pairs of data points and train on their mixtures. Our implementation uses PyTorch (Paszke et al., 2019) and is based heavily on the open source implementation of Manifold Mixup (Verma et al., 2019) by Shivam Saboo. Results for training using (full-batch) Adam (Kingma & Ba, 2015) with the suggested (and common) hyperparameters of $\\\\beta_1 = 0.9$, $\\\\beta_2 = 0.999$ and a learning rate of 0.001 are shown in Figure 1. The class 1 probabilities for each point in the dataset outputted by the learned Mixup classifiers from Figure 1 are shown in Table 1 below:\\n\\n| $\\\\alpha$ | $P_1$ | $P_2$ | $P_3$ |\\n|---------|-------|-------|-------|\\n| 1       | 0.995 | 0.156 | 0.979 |\\n| 32      | 1.000 | 0.603 | 0.997 |\\n| 128     | 1.000 | 0.650 | 0.997 |\\n\\nTable 1: Mixup model evaluations on $X_2^3$ for different choices of $\\\\alpha$.\\n\\nWe see from Figure 1 and Table 1 that Mixup training fails to correctly classify the points in $X_2^3$ for $\\\\alpha = 32$, and this misclassification becomes more exacerbated as we increase $\\\\alpha$. The choice of $\\\\alpha$ for which misclassifications begin to happen is largely superficial; we show in Section D of the Appendix that it is straightforward to construct datasets in the style of $X_2^3$ for which Mixup training will fail even for the very mild choice of $\\\\alpha = 1$. We focus on the case of $X_2^3$ here to simplify the theory. The key takeaway is that, for datasets that exhibit (approximately) collinear structure amongst points, it is possible for inappropriately chosen mixing distributions to cause Mixup training to fail to minimize the original empirical risk.\\n\\n2.4 Sufficient Conditions for Minimizing the Original Risk\\n\\nThe natural follow-up question to the results of the previous subsection is: under what conditions on the data can this failure case be avoided? In other words, when can the Mixup-optimal classifier classify the original data points correctly while being continuous at those points?\\n\\nPrior to answering that question, we first point out that if discontinuous functions are allowed, then Mixup training always minimizes the original risk on finite datasets:\\n\\nProposition 2.8. Consider $k$-class classification where the supports $X_1, \\\\ldots, X_k$ are finite and $P_X$ corresponds to the discrete uniform distribution. Then for every $h \\\\in \\\\arg\\\\min_{g \\\\in C^*} J_{mix}(g, P_X, P_f)$, we have that $h_i(x) = 1$ on $X_i$.\\n\\nProof Sketch. Only the $\\\\xi_{i,i,x,\\\\epsilon}$ term doesn't vanish in $h_i(\\\\epsilon)$ as $\\\\epsilon \\\\to 0$, as the mixing distribution is continuous and cannot assign positive measure to $x$ alone when mixing two points that are not $x$. Note that Proposition 2.8 holds for any continuous mixing distribution $P_f$ supported on $[0,1]$ - we just need a rich enough model class.\"}"}
{"id": "ieNJYujcGDO", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Assumption 2.9. For any point $x \\\\in X_i$, there do not exist $u \\\\in X$ and $v \\\\in X_j$ for $j \\\\neq i$ such that there is a $\\\\lambda > 0$ for which $x = \\\\lambda u + (1 - \\\\lambda)v$.\\n\\nA visualization of Assumption 2.9 is provided in Section B of the appendix. With this assumption in hand, we obtain the following result as a corollary of Theorem 3.2 which is proved in the next section:\\n\\nTheorem 2.10. We consider the same setting as Proposition 2.8 and further suppose that Assumption 2.9 is satisfied. Then for every $h \\\\in \\\\text{argmin}_{g \\\\in C^*} J_{\\\\text{mix}}(g, P_{X}, P_{f})$, we have that $h_i(x) = 1$ on $X_i$ and that $h$ is continuous on $X$.\\n\\nApplication of Sufficient Conditions. The practical take-away of Theorem 2.10 is that if a dataset does not exhibit approximate collinearity between points of different classes, then Mixup training should achieve near-identical training error on the original training data when compared to ERM. We validate this by training ResNet-18 (He et al., 2015) (using the popular implementation of Kuang Liu) on MNIST (LeCun, 1998), CIFAR-10, and CIFAR-100 (Krizhevsky, 2009) with and without Mixup for 50 epochs with a batch size of 128 and otherwise identical settings to the previous subsection. For Mixup, we consider mixing using $\\\\text{Beta}(\\\\alpha, \\\\alpha)$ for $\\\\alpha = 1, 32, 128, 1024$ to cover a wide band of mixing distributions. Our experimental results for the \\\"worst case\\\" of $\\\\alpha = 1024$ (the other choices are strictly closer to ERM in training accuracy) are shown in Figure 2, while the other experiments can be found in Section D of the Appendix.\\n\\nWe now check that the theory can predict the results of our experiments by verifying Assumption 2.9 approximately (exact verification is too expensive). We sample one epoch's worth of Mixup points (to simulate training) from a downsampled version of each train dataset, and then compute the minimum distances between each Mixup point and points from classes other than the two mixed classes. The minimum over these distances corresponds to an estimate of $\\\\epsilon$ in Assumption 2.9. We compute the distances for both training and test data, to see whether good training but poor test performance can be attributed to test data conflicting with mixed training points. Results are shown below in Table 2.\\n\\n| Comparison Type | MNIST | CIFAR-10 | CIFAR-100 |\\n|----------------|-------|----------|-----------|\\n| Mixup/Train    | 11.433| 17.716   | 12.936    |\\n| Mixup/Test     | 11.897| 22.133   | 15.076    |\\n\\nTable 2: Minimum Euclidean distance results using our approximation procedure with Mixup points generated using $\\\\text{Beta}(1024, 1024)$. We downsample all datasets to 20%, to compare to the experiments of Guo et al. (2019).\\n\\nTo interpret the estimated $\\\\epsilon$ values in Table 2, we note that unless $\\\\epsilon \\\\ll 1/L$ (where $L$ is the Lipschitz constant of the model being considered), a Mixup point cannot conflict with an original point (since the function has enough flexibility to fit both). Due to the estimated large Lipschitz constants of deep networks (Scaman & Virmaux, 2019), our $\\\\epsilon$ values certainly do not fall in this regime, explaining how the near-identical performance to ERM is possible on the original datasets. We remark that our results challenge an implication of Guo et al. (2019), which was that training/test performance on\"}"}
