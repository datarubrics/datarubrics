{"id": "DTXZqTNV5nW", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "DTXZqTNV5nW", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 THE GAME RULES\\n\\n| Category     | Name     | Type     | Copy | Count |\\n|--------------|----------|----------|------|-------|\\n| Simples      | Characters | 4        | 36   |\\n| Honors      | Winds    | 4        | 16   |\\n| Honors      | Dragons  | 4        | 12   |\\n| Bonus       | Flowers  | 1        | 4    |\\n| Bonus       | Seasons  | 1        | 4    |\\n\\nFigure 5: A list of all tiles in the 1-on-1 Mahjong game.\\n\\nThere are 24 unique tiles and 72 tiles in total in the 1-on-1 Mahjong game, as shown in Figure 5. At the beginning of the 1-on-1 Mahjong game, each player is dealt with 13 tiles, the content of which is invisible to the other. Afterwards, each player takes actions in turn. Typically, the first player draws a tile from the deck wall and then discards a tile, and the next player takes the same types of actions in sequence. There are exceptional cases where the player does not draw a tile from the deck wall but Chow, Pong, or Kong the tile the opponent just discarded. Afterwards, the player discards a tile, and the game proceeds. Also, after drawing a tile from the deck wall, there are cases where the player could Kong, after which the player draws and discards a tile in sequence. There are 10 types of actions with 10^5 different actions in total, a full description of which is listed in Table 2.\"}"}
{"id": "DTXZqTNV5nW", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The goal of each player is to complete a legal hand prior to the opponent, by drawing a tile or using the tile the opponent just discarded. A legal hand is generally in the form of four melds and a pair, with an exception of 7 pairs. Different categories of legal hands, 64 in total, come with different points, which is fully described in Table 3. Besides, a legal hand can belong to multiple categories, and the score of a legal hand is the sum of points of corresponding categories. Legal hands with higher points are generally more difficult to form in terms of either luck or strategy, and it is critical for a player to trade off the winning probability and the corresponding winning points. If no player completes a legal hand before the tiles are exhausted, the game is tied. A flow chart of the game is illustrated in Figure 6.\"}"}
{"id": "DTXZqTNV5nW", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Number | Description                                                                 | Value |\\n|--------|-----------------------------------------------------------------------------|-------|\\n| 11     | Dragon Pung                                                                 | 2     |\\n| 12     | Concealed Hand                                                              | 2     |\\n| 13     | All Chows                                                                   | 2     |\\n| 14     | Tile Hog                                                                    | 2     |\\n| 15     | Two Concealed Pungs                                                         | 2     |\\n| 16     | Concealed Kong                                                              | 2     |\\n| 17     | All Simples                                                                 | 2     |\\n| 18     | Outside Hand                                                                | 4     |\\n| 19     | Fully Concealed Hand                                                        | 4     |\\n| 20     | Two Melded Kongs                                                            | 4     |\\n| 21     | Last Tile                                                                   | 4     |\\n| 22     | Little Three Winds                                                          | 6     |\\n| 23     | All Pungs                                                                   | 6     |\\n| 24     | Half Flush                                                                  | 6     |\\n| 25     | Two Dragon Pungs                                                            | 6     |\\n| 26     | Two Concealed Kongs                                                         | 6     |\\n| 27     | Melded Hand                                                                 | 6     |\\n| 28     | Out with Replacement Tile                                                   | 8     |\\n| 29     | Rob Kong                                                                    | 8     |\\n| 30     | Last Tile Claim                                                             | 8     |\\n| 31     | Pure Straight                                                               | 16    |\\n| 32     | Pure Shifted Chows                                                          | 16    |\\n| 33     | All Flowers                                                                 | 16    |\\n| 34     | Full Flush                                                                  | 16    |\\n| 35     | Three Concealed Pungs                                                       | 16    |\\n| 36     | Four Honour Pungs                                                           | 24    |\\n| 37     | Big Three Winds                                                             | 24    |\"}"}
{"id": "DTXZqTNV5nW", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A hand with seven pairs.\\n\\nConcealed Hand, Single Wait, Fully Concealed Hand.\\n\\nA hand with three identical sequences.\\n\\nPure Triple Chow.\\n\\nA hand with three number triplets or quads with successive numbers.\\n\\nPure Shifted Pungs.\\n\\nFour shifted number sequences, each shifted either one or two numbers up from the last, but not a combination of both.\\n\\nPure Shifted Chows, Short Straight, Two Terminal Chows, Pure Double Chow.\\n\\nA hand with three quad melds.\\n\\nTwo Melded Kongs, Melded Kong, Two Concealed Kongs, Concealed Kong.\\n\\nA hand consisting of only terminal and honor tiles.\\n\\nAll Pungs, Outside Hand.\\n\\nA hand that is one tile away from winning at the beginning of the game.\\n\\nReady Hand.\\n\\nA hand with four identical sequences.\\n\\nPure Triple Chow, Tile Hog, Pure Double Chow, Two Terminal Chow, Pure Shifted Pungs.\\n\\nA hand with four character triplets or quads with successive numbers.\\n\\nPure Shifted Pungs, Pure Triple Chow, All Pungs.\\n\\nSeven pairs hand with four different wind pairs.\\n\\nSeven Pairs, Concealed Hand, Single Wait, Fully Concealed Hand.\\n\\nSeven pairs hand with three different dragon pairs.\\n\\nSeven Pairs, Concealed Hand, Single Wait, Fully Concealed Hand.\\n\\nA hand with three winds as triplets or quads, and the last wind as pair.\\n\\nBig Three Winds, Little Three Winds.\"}"}
{"id": "DTXZqTNV5nW", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Little Three\\n\\nA hand with two dragons be triplets or quads, and the last dragon be pair.\\n\\nTwo Dragon Pungs,\\n\\nAll Honours\\n\\nA hand consisting of only honor tiles.\\n\\nAll Terminals\\n\\nAll Pungs,\\n\\nFour Honour Pungs,\\n\\nOutside Hand\\n\\nFour Concealed Pungs\\n\\nA hand with four concealed triplets or quads.\\n\\nConcealed Hand,\\n\\nAll Pungs,\\n\\nThree Concealed Pungs,\\n\\nTwo Concealed Pungs,\\n\\nFully Concealed Hand\\n\\nPure Terminal Chows\\n\\nA hand with two Two Terminal Chows and a pair of number 5 in character.\\n\\nAll Chows,\\n\\nSeven Pairs,\\n\\nFull Flush,\\n\\nPure Double Chow,\\n\\nTwo Terminal Chows\\n\\nBig Four Winds\\n\\nA hand with triplets or quads of all four winds and an pair.\\n\\nAll Pungs,\\n\\nLittle Three Winds,\\n\\nBig Three Winds,\\n\\nFour Honor Pungs\\n\\nBig Three Dragons\\n\\nA hand with triplets or quads of all three dragons.\\n\\nDragon Pung,\\n\\nTwo Dragon Pungs\\n\\nNine Gates\\n\\nCollecting number tiles 1112345678999 without melding, and completing with any tile of characters.\\n\\nFull Flush,\\n\\nConcealed Hand,\\n\\nFully Concealed Hand\\n\\nFour Kongs\\n\\nA hand with four quad melds.\\n\\nThree Kongs,\\n\\nTwo Melded Kongs,\\n\\nMelded Kong,\\n\\nSingle Wait,\\n\\nConcealed Kong,\\n\\nTwo Concealed Kongs,\\n\\nAll Pungs\\n\\nSeven Shifted Pairs\\n\\nSeven pairs hand with successive seven numbers in characters.\\n\\nSeven Pairs,\\n\\nSingle Wait,\\n\\nConcealed Hand,\\n\\nFull Flush,\\n\\nFully Concealed Hand\"}"}
{"id": "DTXZqTNV5nW", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A hand consisting of character tiles of 6, 7, 8 or 9\\n\\nSeven Pairs\\n\\nA hand consisting of character tiles of 1, 2, 3 or 4\\n\\nSeven Pairs\\n\\nBig Seven\\n\\nHonours\\n\\nSeven pairs hand with four different wind pairs and three different dragon pairs.\\n\\nSeven Pairs, Four Winds\\n\\nSeven Pairs, Three Dragons\\n\\nSeven Pairs, Outside Hand, Single Wait, Concealed Hand, Fully Concealed Hand, All Honours\\n\\nTable 3: The categories of legal hands in ascending order of corresponding points.\\n\\nThe winning points of a legal hand are obtained by summing the points of the matched categories in reverse order while excluding the conflict categories.\\n\\nHeavenly Hand\\n\\nThe dealer draws a winning hand at the beginning of the game.\\n\\nSelf Draw, Concealed Hand\\n\\nEarthly Hand\\n\\nA player completes a winning hand with the dealer's first discard and in most variants, provided the dealer does not draw a quad.\\n\\nSelf Draw, Concealed Hand\\n\\nHumanly Hand\\n\\nA player completes a winning hand with the opponent player's first discard. And before that any action of Chow, Pong or Kong is not available.\\n\\nA.2 The State and Action Space\\n\\nThe state space size of 1-on-1 Mahjong, shown as the infoset count in Figure 4, is approximately $10^{74}$.\\n\\nYet, the state space of 1-on-1 Mahjong is not as easily abstracted as in poker. The primary reason is that a single tile difference in the state could significantly impact the policy, e.g., making a legal hand illegal and vice versa. In contrast, states that have similar strength in poker could share a common policy. For instance, the optimal preflop policy could be very similar for \\\"Ace-Four\\\" and \\\"Ace-Three\\\" in poker. Another reason is that a state in 1-on-1 Mahjong is divided into different information groups, as demonstrated in Figure 7(a). Different information groups have significantly different meanings.\\n\\nFor instance, one group denotes the player's hand, which is invisible to the opponent, while another one denotes the player's discarded tiles, which are visible to both players.\\n\\nThere are 105 different actions in total, as demonstrated in Table 2. The number of legal actions in a state is relatively small compared to poker. Yet, the game length in 1-on-1 Mahjong is larger than that in poker. Players can decide up to about 40 sequential actions in 1-on-1 Mahjong, whereas most 1-on-1 poker games end within 10 steps. As a result, the reaching probability of states in 1-on-1 Mahjong may vary more significantly than that in poker.\\n\\nA.3 The Effects of A Larger Infoset Size and A Longer Game Length\\n\\nAs shown in Figure 4, 1-on-1 Mahjong has a larger infoset size than poker. The infoset size does not seem to have an influence on the convergence of a tabular CFR (Zinkevich et al., 2008). However, when trajectory sampling and function approximation are used together, the situation may be different.\\n\\nTo be more specific, in a trajectory sampling algorithm, the variance of the sampled instantaneous counterfactual value (regret) of a larger infoset may tend to be higher, which may have a large\"}"}
{"id": "DTXZqTNV5nW", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Published as a conference paper at ICLR 2022\\n\\n**Figure 6:** A flow chart of the 1-on-1 Mahjong game.\\n\\n- **Influence on Performance:** When neural network function approximation is used, it is important to consider the impact on performance. In other words, 1-on-1 Mahjong may be complementary to poker in evaluating algorithms using deep neural networks and only trajectory samples.\\n\\n- **Game Length Impact:** The game length has a direct impact on the sampling methods used. For poker, which has a relatively short game length, methods that sample multiple actions in a state are common. However, consistently sampling multiple actions in game trees with long episodes is prohibitive, as the number of samples goes exponentially with the game length. 1-on-1 Mahjong has a maximal game length of about 40, which may be daunting for methods that try to sample multiple actions in a state. In other words, 1-on-1 Mahjong may be complementary to poker in evaluating algorithms when only trajectory samples are allowed.\\n\\n---\\n\\n**Model Design of Our 1-on-1 Mahjong Agent:**\\n\\nThe model of JueJong is an end-to-end neural network that takes all relevant information as input and outputs both the probabilities of all actions and the state value. This is different from five separated neural networks in Suphx (Li et al., 2020b), representing five different types of actions. Also, we train JueJong from zero by pure self-play using ACH, while the five neural networks in Suphx were trained by supervised learning on human data, with only the \\\"Discard\\\" network further enhanced by RL. Figure 7 gives an overview of the model design of JueJong.\"}"}
{"id": "DTXZqTNV5nW", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 7:  \\n(a) The graphical user interface of 1-on-1 Mahjong with marked regions representing different groups of information, which are encoded in either image-like features or one-hot features.  \\n(b) The image-like feature encoding scheme.  \\n(c) The model architecture design.\\n\\nThe marked regions in Figure 7(a) summarize the information an agent can observe. Regions 1 to 7 are encoded in image-like features, which represent the player's hand, the player's Chow, Pong, and Kong, the player's concealed-Kong, the player's Discard, the opponent's Chow, Pong, and Kong, the opponent's concealed-Kong, and the opponent's Discard respectively. A feature map, as shown in Figure 7(b), of height 4 and width 17 is employed, where a 0 or 1 in the $i$th row and $j$th column means whether there are $i$ tiles of the $j$th type in the input set of tiles. Therefore, a single or two successive convolutional layers with $3 \\\\times 3$ kernels efficiently derive row and column combinations. Note that, in Suphx (Li et al., 2020b), the feature maps are one-dimensional vectors, where $1 \\\\times 3$ kernels are used. The black tile in Figure 7(b) is designed exclusively for Region 6 to indicate how many concealed-Kongs the opponent has. For discarded tiles in Region 4 or 7, 24 such feature maps are used to indicate the latest 24 discarded tiles in order, with one feature map encoding one discard tile. All the feature maps are concatenated in the channel dimension, and therefore the final image-like features are in the shape of $4 \\\\times 17 \\\\times 53$ ($h \\\\times w \\\\times c$).\\n\\nRegions 8 to 11 are encoded in one-hot features, which represent the player's position (0 or 1), the is_ready state of both players, and the bonus tiles of both players. Additionally, the last action (not shown as a region in Figure 7(a)) for each player is encoded in a one-hot feature vector as well.\\n\\nWe apply residual blocks (He et al., 2016) to transform the image-like features. There are totally 3 stages with 3 residual blocks and 1 transition layer in each stage, as shown in Figure 7(c). Each block contains two convolutional layers with kernel size $3 \\\\times 3$. The transition layers are point-wise convolutions that scale the number of output channels to 64, 128, and 32, respectively for each stage. Subsequently, the transformed image-like features are reshaped to a vector, which is concatenated with the one-hot feature vectors. A fully-connected layer (dimension 1024) is then used to transform the concatenated feature vector, and two branches with two fully-connected layers each output the action probabilities and the state value respectively. Besides, we apply batch normalization (Ioffe & Szegedy, 2015) and ReLU non-linearity after all convolutional layers and fully-connected layers.\"}"}
{"id": "DTXZqTNV5nW", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma 1. For a weighted CFR, if $w_t(s) \\\\in [w_l(s), w_h(s)]$, $0 < w_l(s) \\\\leq w_h(s) \\\\leq 1$, then\\n\\\\[\\n\\\\max_{a \\\\in A(s)} \\\\sum_{k=1}^{r} r_{c_k}(s,a) \\\\leq \\\\max_{a \\\\in A(s)} R_{w_t}(s,a) w_h(s) + (w_h(s) - w_l(s)) \\\\Delta_t w_h(s).\\n\\\\] (4)\\n\\nProof. First, for any state $s \\\\in S$ and action $a \\\\in A(s)$, we have\\n\\\\[\\nR_{w_t}(s,a) := \\\\sum_{k=1}^{r} w_t(s) r_{c_k}(s,a) = \\\\sum_{k: r_{c_k}(s,a) \\\\geq 0} w_k(s) r_{c_k}(s,a) - \\\\sum_{k': r_{c_k'}(s,a) < 0} w_{k'}(s) |r_{c_k'}(s,a)| \\\\geq \\\\sum_{k: r_{c_k}(s,a) \\\\geq 0} w_l(s) r_{c_k}(s,a) - \\\\sum_{k': r_{c_k'}(s,a) < 0} w_{k'}(s) |r_{c_k'}(s,a)|.\\n\\\\] (5)\\n\\nSo,\\n\\\\[\\nR_{w_t}(s,a) \\\\geq w_h(s) \\\\sum_{k=1}^{r} r_{c_k}(s,a) - (w_h(s) - w_l(s)) \\\\sum_{k': r_{c_k'}(s,a) < 0} w_{k'}(s) |r_{c_k'}(s,a)|. \\\\] (6)\\n\\nIn other words,\\n\\\\[\\n\\\\sum_{k=1}^{r} r_{c_k}(s,a) \\\\leq R_{w_t}(s,a) w_h(s) + (w_h(s) - w_l(s)) \\\\sum_{k': r_{c_k'}(s,a) < 0} w_{k'}(s) |r_{c_k'}(s,a)|. \\\\] (7)\\n\\nThen we can prove the Theorem:\\n\\nProof. For NW-CFR, the policy at iteration $t$ is generated according to Hedge:\\n\\\\[\\n\\\\pi_t(s,a) = e^{\\\\eta(s) R_{a_{t-1}}(s,a)} \\\\sum_{a'} e^{\\\\eta(s) R_{a_{t-1}}(s,a')}.\\n\\\\] (9)\\n\\nwhere\\n\\\\[\\nR_{a_{t-1}}(s,a) = \\\\sum_{k=1}^{r} f_{\\\\mu_k} p(s) r_{c_k}(s,a).\\n\\\\] (10)\\n\\nMeanwhile, weighted CFR with Hedge generates policy at iteration $t$ according to\\n\\\\[\\n\\\\pi_t(s,a) = e^{\\\\eta(s) R_{w_t-1}(s,a)} \\\\sum_{a'} e^{\\\\eta(s) R_{w_t-1}(s,a')}.\\n\\\\] (11)\\n\\nwhere\\n\\\\[\\nR_{w_t-1}(s,a) = \\\\sum_{k=1}^{r} w_k(s) r_{c_k}(s,a).\\n\\\\] (12)\\n\\nSo, when $w_k(s) = f_{\\\\mu_k} p(s) \\\\geq w_l(s) > 0$ for any $s \\\\in S$ and $k > 0$, NW-CFR and weighted CFR with Hedge generate the same policy at each iteration, and therefore NW-CFR is equivalent to weighted CFR with Hedge.\\n\\nThen, we can prove the convergence property of NW-CFR. Let\\n\\\\[\\nS_{w_t}(s) = \\\\sum_{a \\\\in A(s)} e^{\\\\eta(s) R_{w_t}(s,a)}.\\n\\\\] (13)\"}"}
{"id": "DTXZqTNV5nW", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We have\\n\\\\[ \\\\ln S_w = \\\\ln \\\\sum_{a \\\\in A} (s^e_\\\\eta(R_w(s,a) - \\\\ln |A(s)|) \\\\geq \\\\ln \\\\left( \\\\max_{a \\\\in A} (s^e_\\\\eta R_w(s,a)) - \\\\ln |A(s)| \\\\right). \\\\] \\n\\nMeanwhile, for each \\\\( k = 1,\\\\ldots,t \\\\),\\n\\\\[ \\\\ln S_w^k S_w - 1 = \\\\ln \\\\left( \\\\sum_{a \\\\in A} (s^e_\\\\eta R_w^k - 1(s,a) e_\\\\eta w_k^r c_k(s,a) \\\\sum_{a \\\\in A} (s^e_\\\\eta R_w^k - 1(s,a)) \\\\right) = \\\\ln \\\\left( \\\\sum_{a \\\\in A} (s^e_\\\\eta R_w^k - 1(s,a) e_\\\\eta w_k^r c_k(s,a) \\\\sum_{a \\\\in A} (s^e_\\\\eta R_w^k - 1(s,a)) \\\\right). \\\\]\\n\\nNote that \\\\( r c_k(s,a) = v c_k(s,a) - \\\\sum_{a \\\\in A} (s^e_\\\\eta R_w^k - 1(s,a) e_\\\\eta w_k^r c_k(s,a) \\\\sum_{a \\\\in A} (s^e_\\\\eta R_w^k - 1(s,a)) = 0 \\\\). So\\n\\\\[ \\\\sum_{a \\\\in A} (s^e_\\\\eta R_w^k - 1(s,a) e_\\\\eta w_k^r c_k(s,a) = 0. \\\\]\\n\\nTherefore,\\n\\\\[ \\\\ln S_w^k S_w^0 \\\\leq \\\\eta^2 (s^e_\\\\eta) \\\\Delta^2 (s) \\\\sum_{t=1}^k w_k^2(s) \\\\leq \\\\eta^2 (s^e_\\\\eta) \\\\Delta^2 (s) \\\\sum_{t=1}^k w_k^2(s) \\\\]\\n\\nAccording to Lemma 1,\\n\\\\[ \\\\max_{a \\\\in A} (s^e_\\\\eta R_w^T (s,a)) \\\\leq \\\\ln |A(s)| \\\\eta (s) \\\\leq \\\\ln |A(s)| \\\\eta (s) + \\\\eta (s) \\\\Delta^2 (s) \\\\sum_{t=1}^k w_k^2(s) \\\\leq \\\\Delta \\\\sqrt{T} \\\\Delta^2 (s) \\\\sum_{t=1}^k w_k^2(s) + (w_h(s) - w_l(s)) \\\\Delta T w_h(s). \\\\]\\n\\nAccording to Theorem 2 in Zinkevich et al. (2008), the total regret\\n\\\\[ R_T \\\\leq \\\\sum_{s \\\\in S} \\\\max_{a \\\\in A} (s^e_\\\\eta [R_c^T (s,a)]) \\\\leq \\\\sum_{s \\\\in S} \\\\left( \\\\Delta \\\\sqrt{T} \\\\Delta^2 (s) \\\\sum_{t=1}^k w_k^2(s) + (w_h(s) - w_l(s)) \\\\Delta T w_h(s) \\\\right) \\\\leq |S| \\\\Delta \\\\sqrt{T} \\\\Delta^2 |A| + \\\\Delta T \\\\sum_{s \\\\in S} w_h(s) - w_l(s) w_h(s). \\\\]\"}"}
{"id": "DTXZqTNV5nW", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"As a result, according to the folk theorem in Zinkevich et al. (2008), the average policy has a risk of \\\\( \\\\epsilon \\\\) exploitability, where\\n\\\\[\\n\\\\epsilon = \\\\frac{1}{|P|} \\\\sum_{p \\\\in P} R_{T,T} \\\\leq |S| \\\\Delta \\\\sqrt{\\\\frac{1}{2T} \\\\ln |A| + \\\\Delta \\\\sum_{s \\\\in S} (w_h(s) - w_l(s)) w_h(s)}.\\n\\\\]\\n\\n(C2)\\n\\nC.2 Proof for Corollary 1\\n\\nProof. When the behavioral policy \\\\( \\\\mu_{p,k} \\\\) of each player \\\\( p \\\\in P \\\\) is constant across iterations \\\\( \\\\forall k > 0 \\\\), the reaching probability \\\\( f_{\\\\mu_k}(s) \\\\) of any state \\\\( s \\\\in S \\\\) is also constant. Assume \\\\( f_{\\\\mu_k}(s) = w(s) \\\\), then,\\n\\\\[\\nR_{a} - 1(s,a) = t - 1 \\\\sum_{k=1}^{t} f_{\\\\mu_k}(s)r_{ct}(s,a) = w(s) t - 1 \\\\sum_{k=1}^{t} r_{ct}(s,a) = w(s) R_{ct}(s,a).\\n\\\\]\\n\\n(26)\\n\\nIn other words, \\\\( R_{a} - 1(s,a) \\\\) is equal to the cumulative counterfactual regret scaled by a time-invariant weight \\\\( w(s) \\\\). Hence, the policy at iteration \\\\( t \\\\) is\\n\\\\[\\n\\\\pi_t(a|s) = e^{\\\\eta(s) w(s) R_{ct}(s,a)} \\\\sum_{a'} e^{\\\\eta(s) w(s) R_{ct-1}(s,a')} = e^{\\\\eta'(s)} R_{ct}(s,a) \\\\sum_{a'} e^{\\\\eta'(s) R_{ct-1}(s,a')}.\\n\\\\]\\n\\n(27)\\n\\nAs a result, for constant \\\\( \\\\mu_{p,k} \\\\), NW-CFR is equivalent to CFR with Hedge when \\\\( y(a|s; \\\\theta_t) \\\\) is sufficiently close to \\\\( R_{t}(s,a) \\\\). Furthermore, since \\\\( w_l(s) = f_{\\\\mu_k}(s) = w_h(s) \\\\), the second term in Equation 25 vanishes, i.e.,\\n\\\\[\\n\\\\epsilon \\\\leq |S| \\\\Delta \\\\sqrt{\\\\frac{1}{2} \\\\ln |A(p)| / |S| T}.\\n\\\\]\\n\\n(28)\\n\\nAs a result, the exploitability bound of CFR with Hedge is recovered.\\n\\nDE XPERIMENTAL RESULTS OF THE WEIGHTED CFR\\n\\nAs stated in Section 5, ACH is a practical implementation of NW-CFR, which is a straightforward neural extension to the weighted CFR defined in Definition 1, together with Hedge. In order to investigate the behavior of weighted CFR, in this section, we instantiate multiple weighted CFR algorithms with different settings of the weight \\\\( w_t(s) \\\\) by varying \\\\( \\\\mu_{p,t} \\\\), since \\\\( f_{\\\\mu_t}(s) \\\\) depends only on \\\\( \\\\mu_{p,t} \\\\). Note that the weighted CFR traverses the full game tree at every iteration and that \\\\( \\\\mu_{p,t} \\\\) is only used to calculate the state reaching probability \\\\( f_{\\\\mu_t}(s) \\\\). We test these algorithms on three small IIGs in OpenSpiel: Kuhn poker, Leduc poker, and Liar's Dice.\\n\\nAs shown in Figure 8, the weighted CFR with \\\\( w_t(s) = f_{\\\\mu_t}(s) \\\\) induced by a stationary \\\\( \\\\mu_{p,t} \\\\) (i.e., the uniform policy) converges at the same pace with CFR(Hedge). As a result, Corollary 1 is verified on the three\"}"}
{"id": "DTXZqTNV5nW", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"small benchmarks. Also, as Theorem 1 states, the exploitability of the weighted CFR is influenced by\\nthe range of \\\\( w(s) = f_{\\\\mu(t)p(s)} \\\\). This is experimentally demonstrated in Figure 8 by setting\\n\\\\( \\\\mu_{p,t}(s) = 0.5 \\\\) and \\\\( \\\\pi_{p,t}(s) = 0.5 \\\\).\\n\\nTo address the practical issues mentioned in Section 5, we provide a practical and parallel implemen-\\ntation of NW-CFR, i.e., ACH, which employs a framework of decoupled acting and learning, similar\\nto IMPALA (Espeholt et al., 2018). ACH maintains a policy net \\\\( y(a|s;\\\\theta) \\\\) and a value net \\\\( V(s;\\\\omega) \\\\),\\nwhere \\\\( \\\\theta \\\\) and \\\\( \\\\omega \\\\) share a large portion of parameters (see Figure 7). Both players use the same\\n\\\\( \\\\theta \\\\) and \\\\( \\\\omega \\\\). We do not use an additional time-invariant behavioral policy for sampling actions. Instead, we\\nuse the current policy \\\\( \\\\pi_t \\\\), i.e., \\\\( \\\\mu_{p,t} = \\\\pi_{p,t} \\\\), \\\\( \\\\forall p \\\\in P \\\\). As a result, we can use the same samples to train\\nboth the value net and the policy net. Also, \\\\( \\\\eta(s) \\\\) is incorporated into the learned target value in the\\npolicy net, so the policy \\\\( \\\\pi(a|s) \\\\) is obtained by directly softmaxing on \\\\( y(a|s;\\\\theta) \\\\).\\n\\nThe advantage \\\\( A(s,a) \\\\) is estimated by GAE(\\\\( \\\\lambda \\\\)) (Schulman et al., 2016), using sampled rewards\\nand \\\\( V(s;\\\\omega) \\\\), for only sampled states and actions. The value and policy nets are updated as soon\\nas a mini-batch of samples is available. In other words, we update \\\\( \\\\theta \\\\) and \\\\( \\\\omega \\\\) once using a single\\nmini-batch at each iteration. As a result, the policy loss reduces to\\n\\\\[\\nL_{\\\\pi}(s) = \\\\eta(s)y(a|s;\\\\theta)\\\\pi_{old}(a|s)A(a,s),\\n\\\\]\\nwhere \\\\( \\\\pi_{old}(a|s) \\\\) accounts for the fact that the action \\\\( a \\\\) was sampled using \\\\( \\\\pi_{old}(a|s) \\\\). ACH handles\\nasynchronous training with the importance ratio clipping \\\\([1-\\\\epsilon, 1+\\\\epsilon]\\\\) of PPO (Schulman et al.,\\n2017). To avoid numerical issues, the mean \\\\( \\\\bar{y}(\\\\cdot|s;\\\\theta) \\\\) is subtracted from the policy output, which is\\nthen clipped within a range \\\\([-lth, lth]\\\\). The pseudocode of ACH is given in Algorithm 2.\\n\\nAlgorithm 2: ACH\\n\\nInitialize the policy and critic parameters: \\\\( \\\\theta \\\\) and \\\\( \\\\omega \\\\).\\n\\nStart multiple actor and learner threads in parallel.\\n\\nActors:\\n\\n\\\\[\\n\\\\text{while } \\\\text{true} \\\\text{ do}\\n\\\\]\\n\\\\[\\n\\\\text{Fetch the latest model from the learners.}\\n\\\\]\\n\\\\[\\n\\\\text{Generate samples via self-play in the form:} [a,s,A(s,a),G,\\\\pi_{old}(a|s)]\\n\\\\]\\n\\\\[\\n\\\\text{Send the samples to the replay buffer.}\\n\\\\]\\n\\nLearners:\\n\\n\\\\[\\n\\\\text{for } t \\\\in 1,2,3,\\\\ldots \\\\text{ do}\\n\\\\]\\n\\\\[\\n\\\\text{Fetch a mini-batch of samples from the replay buffer.}\\n\\\\]\\n\\\\[\\nL_{\\\\text{sum}} = 0\\n\\\\]\\n\\\\[\\n\\\\text{for each sample } [a,s,A(s,a),G,\\\\pi_{old}(a|s)] \\\\in \\\\text{the mini-batch do}\\n\\\\]\\n\\\\[\\nc = \\\\begin{cases}\\n1 & \\\\text{if } A(s,a) \\\\geq 0, \\\\\\\\\\n1 & \\\\text{if } A(s,a) < 0,\\n\\\\end{cases}\\n\\\\]\\n\\\\[\\nL_{\\\\text{sum}} += -c\\\\eta(s)y(a|s;\\\\theta)\\\\pi_{old}(a|s)A(s,a) + \\\\alpha^2 \\\\left[ V(s;\\\\omega) - G \\\\right]^2 + \\\\beta \\\\sum_a \\\\pi(a|s;\\\\theta) \\\\log \\\\pi(a|s;\\\\theta).\\n\\\\]\\n\\\\[\\n\\\\text{Update } \\\\theta \\\\text{ and } \\\\omega \\\\text{ once using gradient on } L_{\\\\text{sum}}.\\n\\\\]\\n\\nWe employ an entropy loss to encourage exploration during training and hopefully the convergence\\nof current policy to a NE (Srinivasan et al., 2018). ACH updates \\\\( \\\\theta \\\\) and \\\\( \\\\omega \\\\) simultaneously, and the\\noverall loss is:\\n\\\\[\\nL_{ACH} = -c\\\\eta(s)y(a|s;\\\\theta)\\\\pi_{old}(a|s)A(s,a) + \\\\alpha^2 \\\\left[ V(s;\\\\omega) - G \\\\right]^2 + \\\\beta \\\\sum_a \\\\pi(a|s;\\\\theta) \\\\log \\\\pi(a|s;\\\\theta).\\n\\\\]\"}"}
{"id": "DTXZqTNV5nW", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As we noted in the paper, the behavior policy $\\\\mu_{p,t}$ in ACH could be set to either the current policy $\\\\pi_{p,t}$ or simply a uniform sampling policy. As Corollary 1 states, a tighter bound on the exploitability of the average policy of NW-CFR can be obtained, if $\\\\mu_{p,t}$ is stationary over iterations. However, since we have decided to use the current policy (trained with an entropy regularization) for evaluation in ACH, the effect of the behavior policy is unclear from a theoretical perspective. Hence, we conduct an experiment to investigate this using FHP, which is a non-trivial poker benchmark but still has the property that the exact exploitability of an agent can be efficiently computed.\\n\\nIn Figure 9, we compare the exploitability of the current policy of ACH on FHP, by setting the behavior policy in ACH to either the current policy or a uniform sampling policy. From the comparison, it seems that the performance of the current policy of ACH is not sensitive to the choice of behavior policy. One reason might be that the additional entropy loss forces the current policy to be stable and prone to a uniform random policy. Another reason might be that the current policy instead of the average policy is evaluated. We will investigate this in more depth in the future.\\n\\nWe further evaluate ACH and compare it with A2C, RPG, and NeuRD on three benchmarks from OpenSpiel: Kuhn poker, Leduc poker, and Liar\u2019s Dice. All the experiments were run single-threaded on a 2.24GHz CPU. We use the network architecture provided in OpenSpiel, which has a 128-neurons fully-connected layer followed by ReLU and two separate linear layers for the policy and the state/action value. For A2C, RPG, and NeuRD, we use the default hyper-parameters in OpenSpiel. ACH shares most of the hyper-parameters with A2C. All the hyper-parameters are listed in the Appendix H.3.\\n\\nThe exploitability of an agent is exactly calculated using tools in OpenSpiel. For each method, we compute the exploitability of each agent every $1e5$ training steps, the results of which are plotted in Figure 10. Clearly, ACH converges significantly faster and achieves a lower exploitability than other methods across the three benchmarks. There is still some gap between $0$ and the exploitability ACH converges to. This may due to the neural network approximation error and the fact that we use the current policy instead of the average policy for the evaluation. As expected, A2C has the worst performance, since it is designed for single-agent environments. Moreover, the superiority of ACH is most significant on the Liar\u2019s Dice benchmark, which is the most complex one of the three benchmarks.\\n\\nAs a complement, we also present the head-to-head performance of A2C, RPG, NeuRD, and ACH on the three benchmarks in OpenSpiel. As demonstrated in Table 4, the agent of ACH won all other agents across the three benchmarks.\"}"}
{"id": "DTXZqTNV5nW", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 10: The training curves of each agent on the three benchmarks from OpenSpiel. We report the mean as solid curves and the range of the exploitability across 8 independent runs as shaded regions. We notice that there exists some discrepancy between the NeuRD results on Kuhn poker and Leduc poker reported here and those reported in Hennes et al. (2020) and Lanctot et al. (2019). The reason might be due to NeuRD's sensitivity to running environments including hyper-parameters and random seeds.\\n\\n(a) Kuhn poker\\n\\n(b) Leduc poker\\n\\n(c) Liar's Dice\\n\\nTable 4: Mean (\u00b1 standard deviation) of the average winning scores of the row agents against the column agents. The mean and the standard deviation are estimated by 8 independent runs. In each run, the average winning scores are obtained via 10,000 head-to-head plays. All the agents are selected at the 1e7th training step.\"}"}
{"id": "DTXZqTNV5nW", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We used the Adam optimizer (Kingma & Ba, 2014) for the experiments on FHP. All the experiments on FHP were run multi-threaded and synchronously using ten 2.24 GHz CPUs. We update the neural networks of ACH and other methods (A2C, RPG, and NeuRD) every 1000 episodes, with a batch consisting all the samples collected within the latest 1000 episodes. Since the average game length of FHP is around 2, the batch size is roughly 2000. We performed a mild hyper-parameter search for ACH, A2C, RPG, and NeuRD, as shown in Table 6. Note that we do not need to clip the advantages in a synchronous method, so the ratio clip hyper-parameter \\\\( \\\\epsilon \\\\) is not needed. We multiply the rewards in FHP with a reward normalizer, as the rewards in FHP are in the range \\\\([-700, 700]\\\\). Besides, we found that A2C, RPG, and NeuRD are very sensitive to the entropy coefficient, and we had to use a larger entropy coefficient in these algorithms than ACH. For OS-DCFR and DREAM, we use the same hyper-parameters presented in Steinberger et al. (2020). An overview of the hyper-parameters on FHP is given in Table 6. Note that we report the performance of the current policy for ACH, A2C, RPG, and NeuRD, while the average policy is used for evaluation in OS-DCFR and DREAM.\\n\\n| Parameter     | Range               | Best |\\n|---------------|---------------------|------|\\n| Shared GAE \\\\( \\\\lambda \\\\) | - 0.95             | 0.95 |\\n| Learning rate | \\\\{1e-3, 1e-4\\\\}    | 1e-4 |\\n| Discount factor \\\\( \\\\gamma \\\\) | - 0.995            | 0.995|\\n| Value loss coefficient \\\\( \\\\alpha \\\\) | - 2.0              | 2.0  |\\n| Batch size    | \\\\( \\\\approx 2000 \\\\) | 2000 |\\n| Entropy coefficient \\\\( \\\\beta \\\\) | \\\\{1e-2, 3e-2, 5e-2\\\\} | 5e-2 |\\n| Reward normalizer | - 0.002          | 0.002|\\n| NeuRD Logit threshold \\\\( l_{th} \\\\) | \\\\{2.0, 4.0\\\\}     | 2.0  |\\n| ACH Logit threshold \\\\( l_{th} \\\\) | \\\\{2.0, 4.0\\\\}     | 2.0  |\\n| Entropy coefficient \\\\( \\\\beta \\\\) | \\\\{1e-2, 3e-2, 5e-2\\\\} | 3e-2 |\\n| Hedge coefficient \\\\( \\\\eta(s) \\\\) | \\\\{1.0, 0.1\\\\}     | 1.0  |\\n\\nTable 6: The hyper-parameters used for the FHP experiment.\\n\\nWe used stochastic gradient descent with a constant learning rate for all the experiments on benchmarks from OpenSpiel. For A2C and RPG, we used the default implementations and hyper-parameters in OpenSpiel. NeuRD was originally implemented using the counterfactual regret in OpenSpiel. We re-implemented NeuRD using predicted advantages and set the shared hyper-parameters of NeuRD identical to those in RPG. All methods employ an entropy loss to encourage exploration during training and hopefully the convergence of the current policy to a NE. Also, we report the performance of the current policy, instead of the average policy, for each method.\\n\\n| Parameter     | Range               | Best |\\n|---------------|---------------------|------|\\n| Learning rate | \\\\{1e-3, 5e-3\\\\}    | 1e-3 |\\n| Value loss coefficient \\\\( \\\\alpha \\\\) | \\\\{1.0, 2.0\\\\}     | 2.0  |\\n| Hedge coefficient \\\\( \\\\eta(s) \\\\) | \\\\{1.0, 1e-1, 1e-2\\\\} | 1.0  |\\n\\nTable 7: The hyper-parameter search ranges and best settings of ACH for the OpenSpiel experiment.\"}"}
{"id": "DTXZqTNV5nW", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: (a) The training curves of the best response against each agent. Lower is better. (b) The training curves of each agent. The performance of an agent is evaluated by the average scores the agent wins against a common rule-based agent. Higher is better. We report the mean as solid curves and the range of the average scores across 5 independent runs as shaded regions. Each method is selected at the 1e6th training step. Note that the agent is fixed as a part of the 1-on-1 Mahjong environment when training the best response. We train the best response using PPO with the same hyper-parameters that were used to train the PPO agent with self-play. During the training of the best response, we evaluate the best response every 500 training steps using 10,000 head-to-head plays against each agent. According to the average scores each agent loses to its best response in Figure 1(a), we may conclude that ACH is significantly more difficult to exploit than other methods in the large-scale 1-on-1 Mahjong environment.\\n\\nHead-to-Head Evaluation. We then compare the head-to-head performance of PPO, RPG, NeuRD, and ACH in the 1-on-1 Mahjong environment. First, we compare the training process of each method by evaluating each agent every 500 training steps against a common rule-based agent using 10,000 head-to-head plays, the results of which are shown in Figure 1(b). As we can see, all methods beat the common rule-based agent significantly, while ACH has a clear advantage over other methods in terms of stability and final performance. The relatively slow convergence of RPG may due to the threshold operation on the advantage, which could reduce sample efficiency in large-scale IIGs.\\n\\nSecond, the superior performance of ACH is further validated in head-to-head evaluations with other agents in Table 1, where all the agents are selected at the 1e6th training step. The agent of ACH wins all other agents by a significant margin.\\n\\n|               | PPO   | RPG              | NeuRD | ACH              |\\n|---------------|-------|------------------|-------|------------------|\\n| Average Score | -0.05 | -0.21 \u00b1 0.05     | -0.02 | 0.39 \u00b1 0.02      |\\n|               | 0.66  | 0.45 \u00b1 0.07     | 0.04  | 0.10 \u00b1 0.04      |\\n\\nTable 1: Mean (\u00b1 standard deviation) of the average winning scores of the row agents against the column agents. The statistics are estimated by 5 independent runs (resulting 5 different agents for each method). In each run, the average winning scores are obtained via 10,000 head-to-head plays.\\n\\nHuman Evaluation. We evaluate the agent, selected at the 1e6th training step, of ACH against human players. First, the agent, named JueJong, is roughly evaluated by playing over 7,700 games against 157 practiced Mahjong players, where JueJong won an average of 4.56 scores per game.\\n\\nSecond, we select the top 4 out of 157 players according to their performances against JueJong and play JueJong against the four players for 200 games each. As shown in Figure 2(a), the average winning scores of JueJong oscillate in the first 120 games but all plateau above 0 afterwards. More importantly, we evaluate JueJong against the Mahjong champion Haihua Cheng for 1,000 games, as shown in Figure 2(b). After playing 1,000 games, JueJong won the champion by a score of 0.82 \u00b1 0.96 (mean \u00b1 standard deviation), with a p-value of 0.19 under one-tailed t-test. Hence, we may conclude that Haihua Cheng failed to exploit JueJong effectively within 1,000 games.\\n\\nThe rule-based agent is implemented such that it selects the action Hu, Ting, Kong, Chow, and Pong in descending priority whenever available and discards the tile that has the fewest neighbours.\"}"}
{"id": "DTXZqTNV5nW", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Performance of JueJong against human players.\\n\\nFigure 3: The exploitability on FHP, with the x-axis being the number of episodes generated (left and right) and the number of samples consumed (middle). We report the mean as solid curves and the range as shaded regions across 3 independent runs. OS-DCFR and DREAM were run once as their performances are relatively stable, according to Brown et al. (2019) and Steinberger et al. (2020).\\n\\n7.3 RESULTS ON THE FHP BENCHMARK\\n\\nWe further evaluate ACH and compare it with OS-DCFR, DREAM, A2C, RPG, and NeuRD on FHP. FHP is a simplified Heads-up Limit Texas Hold'em (HULH), which includes only the first two of the four bettings in HULH. It is a medium-sized game with over 10\\\\(^12\\\\) nodes and 10\\\\(^9\\\\) infosets. All the methods share the same neural network architecture proposed in Brown et al. (2019). We perform a mild hyper-parameter search for ACH, A2C, RPG, and NeuRD. For OS-DCFR and DREAM, we follow the hyper-parameters presented in Steinberger et al. (2020). The exploitability is measured in the number of chips per game, where a big blind is 100 chips. All the hyper-parameters and the running environment are described in the Appendix H.2.\\n\\nAs shown in Figure 3, ACH performs competitively with OS-DCFR and slightly worse than DREAM on FHP in terms of exploitability per episodes generated. However, ACH is much more training efficient: ACH achieves an exploitability of 10 almost 100 times faster than DREAM and 1,000 times faster than OS-DCFR. Also, in comparison with methods of similar training complexity (A2C, RPG, and NeuRD), ACH converges significantly faster and achieves a lower exploitability.\\n\\n8 CONCLUSIONS\\n\\nIn this paper, we investigated the problem of adapting policy gradient methods in deep RL to tackle a large-scale IIG, i.e., 1-on-1 Mahjong. To this end, we developed a new model-free actor-critic algorithm, i.e., ACH, for approximating a NE in large-scale IIGs. ACH is memory and computation efficient, as it uses only trajectory samples at the current iteration and requires no computation of best response. ACH is theoretically justified as it is derived from a new neural-based CFR, i.e., NW-CFR, of which we proved the convergence to an approximate NE in 2-player zero-sum IIGs under certain conditions. The superior performance of ACH was validated on both our 1-on-1 Mahjong benchmark and other common benchmarks. Secondly, to facilitate research on large-scale IIGs, we proposed the first 1-on-1 zero-sum Mahjong benchmark, whose infoset size and game length are much larger than poker. Finally, using ACH we obtained the 1-on-1 Mahjong agent JueJong, which has demonstrated stronger performance against the Mahjong champion Haihua Cheng.\"}"}
{"id": "DTXZqTNV5nW", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGEMENT\\n\\nWe thank the Mahjong champion Haihua Cheng for his efforts in this work. We appreciate the support from Tencent Mahjong (https://majiang.qq.com). We are grateful to Tencent AI Arena (https://aiarena.tencent.com) for providing the powerful computing capability to the experiments on 1-on-1 Mahjong.\\n\\nREPRODUCIBILITY STATEMENT\\n\\nThe experiments on 1-on-1 Mahjong were run in a large cluster of thousands of machines, on which we have developed an efficient actor-learner training platform, similar to IMPALA (Espeholt et al., 2018). The code of the platform is not released currently but is planned to be open sourced in the near future. The code of the 1-on-1 Mahjong benchmark is available at https://github.com/yata0/Mahjong. The code of ACH is available at https://github.com/Liuweiming/ACH_poker. All the hyper-parameters for all the experiments are listed in the Appendix H. All the theoretical results are presented in the main text, with all the proofs given in the Appendix C.\\n\\nREFERENCES\\n\\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Christopher Hesse, and et al. Dota 2 with large scale deep reinforcement learning. CoRR, abs/1912.06680, 2019.\\n\\nNoam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. Science, 359(6374):418\u2013424, 2018.\\n\\nNoam Brown and Tuomas Sandholm. Superhuman AI for multiplayer poker. Science, 365(6456):885\u2013890, 2019.\\n\\nNoam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret minimization. In International Conference on Machine Learning (ICML), pp. 793\u2013802, 2019.\\n\\nNicolo Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games. Cambridge university press, 2006.\\n\\nTrevor Davis, Martin Schmid, and Michael Bowling. Low-variance and zero-variance baselines for extensive-form games. In International Conference on Machine Learning, pp. 2392\u20132401. PMLR, 2020.\\n\\nLasse Espeholt, Hubert Soyer, R\u00e9mi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures. In International Conference on Machine Learning (ICML), volume 80, pp. 1406\u20131415, 2018.\\n\\nYoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119\u2013139, 1997.\\n\\nSam Ganzfried and Tuomas Sandholm. Potential-aware imperfect-recall abstraction with earth mover\u2019s distance in imperfect-information games. In Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.\\n\\nNicola Gatti, Fabio Panozzo, and Marcello Restelli. Efficient evolutionary dynamics with extensive-form games. In AAAI Conference on Artificial Intelligence, 2013.\\n\\nAudrunas Gruslys, Marc Lanctot, R\u00e9mi Munos, Finbarr Timbers, Martin Schmid, Julien P\u00e9rolat, Dustin Morrill, Vin\u00edcius Flores Zambaldi, Jean-Baptiste Lespiau, John Schultz, Mohammad Gheshlaghi Azar, Michael Bowling, and Karl Tuyls. The advantage regret-matching actor-critic. CoRR, abs/2008.12234, 2020.\\n\\nSergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68(5):1127\u20131150, 2000.\"}"}
{"id": "DTXZqTNV5nW", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013778, 2016.\\n\\nJohannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect-information games. CoRR, abs/1603.01121, 2016.\\n\\nDaniel Hennes, Dustin Morrill, Shayegan Omidshafiei, R\u00e9mi Munos, Julien Perolat, Marc Lanctot, Audrunas Gruslys, Jean-Baptiste Lespiau, Paavo Parmas, Edgar Du\u00e9\u00f1ez-Guzm\u00e1n, et al. Neural replicator dynamics: Multiagent learning via hedging policy gradients. In International Joint Conference on Autonomous Agents and Multi-agent Systems (AAMAS), pp. 492\u2013501, 2020.\\n\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML), pp. 448\u2013456, 2015.\\n\\nMichael Johanson, Neil Burch, Richard Valenzano, and Michael Bowling. Evaluating state-space abstractions in extensive-form games. In Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems, pp. 271\u2013278, 2013.\\n\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nMarc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael H. Bowling. Monte carlo sampling for regret minimization in extensive games. In Yoshua Bengio, Dale Schuurmans, John D. Lafferty, Christopher K. I. Williams, and Aron Culotta (eds.), Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada, pp. 1078\u20131086. Curran Associates, Inc., 2009.\\n\\nMarc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien P\u00e9rolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), pp. 4190\u20134203, 2017.\\n\\nMarc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vin\u00edcius Flores Zambaldi, Satyaki Upadhyay, Julien P\u00e9rolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, and et al. Openspiel: A framework for reinforcement learning in games. CoRR, abs/1908.09453, 2019.\\n\\nHui Li, Kailiang Hu, Shaohua Zhang, Yuan Qi, and Le Song. Double neural counterfactual regret minimization. In International Conference on Learning Representations (ICLR), 2020a.\\n\\nJunjie Li, Sotetsu Koyamada, Qiwei Ye, Guoqing Liu, Chao Wang, Ruihan Yang, Li Zhao, Tao Qin, Tie-Yan Liu, and Hsiao-Wuen Hon. Suphx: Mastering mahjong with deep reinforcement learning. CoRR, abs/2003.13590, 2020b.\\n\\nEdward Lockhart, Marc Lanctot, Julien P\u00e9rolat, Jean-Baptiste Lespiau, Dustin Morrill, Finbarr Timbers, and Karl Tuyls. Computing approximate equilibria in sequential adversarial games by exploitability descent. In International Joint Conference on Artificial Intelligence (IJCAI), pp. 464\u2013470, 2019.\\n\\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning (ICML), pp. 1928\u20131937, 2016.\\n\\nMatej Morav\u02c7c\u00edk, Martin Schmid, Neil Burch, Viliam Lis\u02c7y, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508\u2013513, 2017.\\n\\nJulien Perolat, Remi Munos, Jean-Baptiste Lespiau, Shayegan Omidshafiei, Mark Rowland, Pedro Ortega, Neil Burch, Thomas Anthony, David Balduzzi, Bart De Vylder, et al. From poincar\u00e9 recurrence to convergence in imperfect information games: Finding equilibrium via regularization. In International Conference on Machine Learning, pp. 8525\u20138535. PMLR, 2021.\"}"}
{"id": "DTXZqTNV5nW", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: The hyper-parameters used for the OpenSpiel experiment.\\n\\n| Parameter          | A2C       | RPG       | NeuRD     | ACH       |\\n|--------------------|-----------|-----------|-----------|-----------|\\n| Batch size         | 4         | 64        | 64        | 64        |\\n| Critic learning rate | $10^{-4}$ | $10^{-2}$ | $10^{-2}$ | -         |\\n| Policy learning rate | $10^{-4}$ | $10^{-2}$ | $10^{-2}$ | -         |\\n| Critic updates per policy update | 32        | 32        | 32        | -         |\\n| Entropy coefficient ($\\\\beta$) | $10^{-2}$ | $10^{-2}$ | $10^{-2}$ | $10^{-2}$ |\\n| Logit threshold ($l_{th}$) | -         | -         | 2.0       | 2.0       |\\n\\nAlso note that, in a single-threaded training environment, the actor and the learner run in sequence. As a result, $\\\\pi(a|s;\\\\theta)$ is always identical to $\\\\pi_{old}(a|s)$ in ACH in Algorithm 2.\\n\\nWe performed a mild hyper-parameter search for ACH, which is illustrated in Table 7. The final hyper-parameters used for each method are listed in Table 8, with 3 additional hyper-parameters of ACH listed in the \u201cBest\u201d column in Table 7.\\n\\nRecently, Suphx (Li et al., 2020b) has achieved stunning performance on Japanese Riichi Mahjong. The development of Suphx is enabled by a novel integration of existing supervised learning and RL methods in addition to some newly developed techniques. Three new techniques were introduced in Suphx: global reward prediction, oracle guiding, and run-time policy adaptation. The global reward prediction technique is to handle the multi-round game situation, which is irrelevant to our 1-on-1 Mahjong setting (only one round per game in our test setting). The oracle guiding technique decays the invisible feature during training, which is of independent interest in dealing with imperfect-information. The run-time policy adaptation technique adapts the trained policy at test time, and this may be combined with ACH, which is a training algorithm. In summary, Suphx is more of a novel system than a new algorithm. For this reason, we did not implement and compare Suphx with ACH in our 1-on-1 Mahjong experiment. Nonetheless, the oracle guiding technique may be complementary to ACH in handling imperfect-information, and we will investigate this in future work.\"}"}
{"id": "DTXZqTNV5nW", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACTOR-CRITIC POLICY OPTIMIZATION IN LARGE-SCALE IMPERFECT-INFO GAME\\n\\nHaobo Fu,1\u2217 Weiming Liu,2\u2020 Shuang Wu,1 Yijia Wang,3\u2020 Tao Yang,1 Kai Li,45 Junliang Xing,45 Bin Li,2 Bo Ma,1 Qiang Fu,1 and Wei Yang1\\n\\n1 Tencent AI Lab, Shenzhen, China\\n2 University of Science and Technology of China, Hefei, China\\n3 Peking University, Beijing, China\\n4 Institute of Automation, Chinese Academy of Sciences, Beijing, China\\n5 School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China\\n\\nABSTRACT\\n\\nThe deep policy gradient method has demonstrated promising results in many large-scale games, where the agent learns purely from its own experience. Yet, policy gradient methods with self-play suffer convergence problems to a Nash Equilibrium (NE) in multi-agent situations. Counterfactual regret minimization (CFR) has a convergence guarantee to a NE in 2-player zero-sum games, but it usually needs domain-specific abstractions to deal with large-scale games. Inheriting merits from both methods, in this paper we extend the actor-critic algorithm framework in deep reinforcement learning to tackle a large-scale 2-player zero-sum imperfect-information game, 1-on-1 Mahjong, whose information set size and game length are much larger than poker. The proposed algorithm, named Actor-Critic Hedge (ACH), modifies the policy optimization objective from originally maximizing the discounted returns to minimizing a type of weighted cumulative counterfactual regret. This modification is achieved by approximating the regret via a deep neural network and minimizing the regret via generating self-play policies using Hedge. ACH is theoretically justified as it is derived from a neural-based weighted CFR, for which we prove the convergence to a NE under certain conditions. Experimental results on the proposed 1-on-1 Mahjong benchmark and benchmarks from the literature demonstrate that ACH outperforms related state-of-the-art methods. Also, the agent obtained by ACH defeats a human champion in 1-on-1 Mahjong.\\n\\nINTRODUCTION\\n\\nPolicy gradient methods using deep neural networks as policy and value approximators have been successfully applied to many large-scale games (Berner et al., 2019; Vinyals et al., 2019; Ye et al., 2020). Usually, a score function representing the discounted returns is maximized by the policy, i.e., the actor. In the meantime, a value function, known as the critic, is learned to guide the directions and magnitudes of the policy gradients. This type of actor-critic methods are efficiently scalable with regard to the game size and the amount of computational resources. However, as pointed out in Srinivasan et al. (2018) and Hennes et al. (2020), policy gradient methods with self-play have no convergence guarantee to optimal solutions in competitive Imperfect-Information Games (IIGs). The main reason is that the policy gradient theorem (Sutton et al., 1999) is established within the single agent situation, where the environment is Markovian. However, learning becomes non-stationary and non-Markovian when multiple agents learn simultaneously in a competitive environment.\\n\\nAn optimal solution to a 2-player zero-sum IIG usually refers to a Nash Equilibrium (NE), where no player could improve by unilaterally deviating to a different policy. Tremendous progress in computing NE solutions has been made by a family of tabular methods: Counterfactual regret minimization (CFR) and its variants (Brownlees et al., 2015; Houdayer et al., 2016; Houdayer et al., 2018; Houdayer et al., 2019; Houdayer et al., 2020; Houdayer et al., 2021). However, these methods usually require domain-specific abstractions such as state aggregation and feature engineering to deal with large-scale games.\\n\\nIn this paper, we propose a novel actor-critic algorithm, named Actor-Critic Hedge (ACH), which combines the strengths of policy gradient methods and CFR. The core idea is to modify the policy optimization objective from maximizing the discounted returns to minimizing a type of weighted cumulative counterfactual regret. This modification is achieved by approximating the regret via a deep neural network and minimizing the regret via generating self-play policies using Hedge. ACH is theoretically justified as it is derived from a neural-based weighted CFR, for which we prove the convergence to a NE under certain conditions. Experimental results on the proposed 1-on-1 Mahjong benchmark and benchmarks from the literature demonstrate that ACH outperforms related state-of-the-art methods. Also, the agent obtained by ACH defeats a human champion in 1-on-1 Mahjong.\"}"}
{"id": "DTXZqTNV5nW", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2022\\n\\nminimization (CFR) (Zinkevich et al., 2008). CFR is a type of iterative self-play algorithm based on regret minimization, and it guarantees to converge to a NE with regard to the average policy in 2-player zero-sum IIGs. A perfect game model is required in CFR to sample many if not all actions from a state. To handle large-scale IIGs with CFR, abstractions (applied to either the action space or the state space) are usually employed to reduce the game to a manageable size (Morav\u02c7c\u00edk et al., 2017; Brown & Sandholm, 2018; 2019).\\n\\nHowever, abstractions are domain specific (Waugh et al., 2009; Johanson et al., 2013; Ganzfried & Sandholm, 2014). More importantly, some large-scale IIGs are inherently difficult to be abstracted, such as the game of Mahjong (Li et al., 2020b).\\n\\nIn this paper, we investigate a large-scale IIG, i.e., 2-player (1-on-1) zero-sum Mahjong, whose information set size and game length are much larger than poker. Li et al. (2020b) has recently developed a strong 4-player Mahjong agent based on supervised learning and traditional Reinforcement Learning (RL). In comparison, we study 1-on-1 Mahjong from a game-theoretic perspective, i.e., aiming for a NE. We are interested in methods using only trajectory samples to learn, as it is infeasible to consistently sample multiple actions for each state in large-scale IIGs with long episodes. We employ deep neural networks to generalize across states, since the state abstraction in 1-on-1 Mahjong is inherently difficult, as explained in the Appendix A.2.\\n\\nWe make the following contributions.\\n\\n\u2022 Inheriting the scalability of deep RL methods and the convergence property of CFR, we develop a new actor-critic algorithm, named Actor-Critic Hedge (ACH), for approaching a NE in large-scale 2-player zero-sum IIGs. ACH employs a deep neural network to approximate a type of weighted cumulative counterfactual regret. In the meantime, ACH minimizes the regret via generating self-play policies using Hedge (Freund & Schapire, 1997).\\n\\n\u2022 We introduce a Neural-based Weighted CFR (NW-CFR), of which ACH is a practical implementation. We prove that the exploitability of the average policy in NW-CFR decreases at the rate of \\\\( O(T^{-1/2}) \\\\) under certain conditions, where \\\\( T \\\\) denotes the number of iterations in NW-CFR.\\n\\n\u2022 To facilitate research on large-scale 2-player zero-sum IIGs, we propose a 1-on-1 Mahjong benchmark. The corresponding game enjoys a large population in online games.\\n\\n\u2022 We build a 1-on-1 Mahjong agent, named JueJong, based on ACH. In an initial evaluation against human players including a Mahjong champion, JueJong demonstrates superior performance.\\n\\n2 NOTATIONS AND BACKGROUND\\n\\n2.1 IMPERFECT- INFORMATION GAMES AND EQUILIBRIUM\\n\\nAn IIG is usually described in an extensive-form game tree. A node (history) \\\\( h \\\\in H \\\\) in the tree represents all information of the current situation. For each history \\\\( h \\\\), there is a player \\\\( p \\\\in P \\\\) or a chance player \\\\( c \\\\) that should act at \\\\( h \\\\). Define \\\\( P: H \\\\to P \\\\cup \\\\{c\\\\} \\\\). When \\\\( P(h) \\\\in P \\\\), the player \\\\( P(h) \\\\) has to take an action \\\\( a \\\\in A(h) \\\\), and \\\\( A(h) \\\\) is the set of legal actions in \\\\( h \\\\). The chance player is responsible for taking actions for random events. The set of terminal nodes is denoted by \\\\( Z \\\\). For each player \\\\( p \\\\in P \\\\), there is a payoff function defined on the set of terminal nodes, \\\\( u_p: Z \\\\to \\\\mathbb{R} \\\\). In this paper, we focus on 2-player zero-sum games, where \\\\( P = \\\\{0, 1\\\\} \\\\) and \\\\( u_0(z) + u_1(z) = 0 \\\\) for each \\\\( z \\\\in Z \\\\).\\n\\nFor either player \\\\( p \\\\), the set of histories \\\\( H \\\\) is partitioned into information sets (infosets). We denote the set of infosets for player \\\\( p \\\\) by \\\\( I_p \\\\) and an infoset in \\\\( I_p \\\\) by \\\\( I_p \\\\). Two histories \\\\( h, h' \\\\in H \\\\) are in the same infoset if and only if \\\\( h \\\\) and \\\\( h' \\\\) are indistinguishable from the perspective of player \\\\( p \\\\). Hence, a player \\\\( p \\\\)'s policy \\\\( \\\\pi_p \\\\) is defined as a function that maps an infoset to a probability distribution over legal actions. We further define \\\\( A(I_p) = A(h) \\\\) and \\\\( P(I_p) = P(h) \\\\) for any \\\\( h \\\\in I_p \\\\). A policy profile \\\\( \\\\pi \\\\) is a tuple of policies \\\\((\\\\pi_0, \\\\pi_{-p})\\\\), where \\\\( \\\\pi_{-p} \\\\) represents the player \\\\( p \\\\)'s opponent policy. The expected payoff for player \\\\( p \\\\) under \\\\( \\\\pi \\\\) is denoted by \\\\( u_p(\\\\pi_p, \\\\pi_{-p}) \\\\). We use \\\\( \\\\Delta(I) \\\\) to denote the range of payoffs reachable from a history \\\\( h \\\\) in infoset \\\\( I \\\\). Let \\\\( \\\\Delta = \\\\max_{I \\\\in I_p, p \\\\in P} \\\\Delta(I) \\\\).\\n\\nf_{\\\\pi}(h) \\\\) denotes the joint probability of reaching \\\\( h \\\\) under \\\\( \\\\pi \\\\). \\\\( f_{\\\\pi_p}(h) \\\\) is the contribution of player \\\\( p \\\\) to \\\\( f_{\\\\pi}(h) \\\\), and \\\\( f_{\\\\pi_{-p}}(h) \\\\) is the contribution of the opponent and chance:\\n\\n\\\\[\\nf_{\\\\pi}(h) = f_{\\\\pi_p}(h)f_{\\\\pi_{-p}}(h)\\n\\\\]\"}"}
{"id": "DTXZqTNV5nW", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"which results in a best response value function as the baseline: $B$\\n\\nFor either player $p$, Matching (RM) (Hart & Mas-Colell, 2000) and Hedge (Cesa-Bianchi & Lugosi, 2006). In RM, a sub-linearly with $T$ clipping the ratio to a small interval around $1$. It is proven in Zinkevich et al. (2008) that $\\\\pi$ local policies minimize the regret, a regret minimizer is utilized in each state at each iteration, generating a series of here to reflect the fact that reaching state $s$ he does not select the action $a$. Moreover, the average policy predicted values of future states. A2C updates the parameters $\\\\theta$ and $G$ is an unbiased estimate of $\\\\theta$ is equal to $A$ \u03c0 at iteration $t$. As a result, the total regret can be minimized by minimiz- in state $s$, $r$ in state $a$ counterfactual regret of action $a$ is defined as $CFR$ is an iterative algorithm that minimizes the total regret of policy by minimizing the cumulative clipping the ratio to a small interval around $1$. \u03c0 $r$ policy. Proximal Policy Optimization (PPO) (Schulman et al., 2017) takes this discrepancy into In an asynchronous training environment, the behavioral policy is usually different from the learning predicted values of future states. A2C updates the parameters $\\\\theta$ $E$ REINFORCE algorithm (Williams, 1992) that updates $\\\\theta$ $G$ is an unbiased estimate of $\\\\theta$ $\\\\pi$ $G$ $\\\\theta$ $\\\\pi$ $s$ $s$ $s$ $s$ from the environment and transitions to a new state $s\u2032$. Moreover, the value $G$ $\\\\pi$ $\\\\pi$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s$ $s"}
{"id": "DTXZqTNV5nW", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We can rewrite the NW-CFR policy loss (in Equation 2) at iteration $t$ as\\n\\\\[\\n\\\\Delta\\\\left(t \\\\cdot \\\\tau_{k} \\\\mid \\\\cdot \\\\right)\\n\\\\]\\nwhere $\\\\tau_{k}$ is the sampled advantage over the sampled instantaneous counterfactual regret. We then define a new family of CFR algorithms: weighted CFR. Finally, we show that NW-CFR is equivalent to a type of weighted CFR with Hedge under certain conditions. The convergence properties of weighted CFR with Hedge and hence NW-CFR are proven accordingly.\\n\\nIn this section, we prove the convergence of NW-CFR to an approximate NE. To this end, we first deduce that the policy target in NW-CFR is $R(t-1)$; the sampling policy profile is the sampling policy $\\\\pi_{k}$, $\\\\tau_{k}$ from the perspective of player $k$. NW-CFR runs concurrently for both players and synchronizes at the beginning of each iteration. The pseudocode of NW-CFR is given in Algorithm 1. Note that we present NW-CFR employing a neural network to generalize across states and relies on only trajectory samples for training.\\n\\nAt the beginning of iteration $t$, the policy net, parameterized as $\\\\theta$, is then optimized according to the loss function $L_{p}$ for more flexibility in the convergence (more details are provided in the next section). The policy $\\\\pi_{k}$ is parameterized as $\\\\omega$ via Hedge, i.e., softmaxing on $\\\\eta$.\\n\\nIn this section, we motivate ACH by introducing a new neural-based CFR algorithm, NW-CFR, which employs a neural network to generalize across states and relies on only trajectory samples for training. If the player plays according to Hedge in state $s$, the policy $\\\\pi_{k}$ selects an action with probability in proportion to its positive cumulative counterfactual regret. In Hedge, the policy $\\\\pi_{k}$ is shift-invariant, which may be more robust to the function approximation error compared to the threshold operation in RM. In the policy profile $\\\\pi_{k}$, softmaxing is shift-invariant, which may be more robust to the function approximation error compared to the threshold operation in RM.\\n\\nIn Hedge, the policy $\\\\pi_{k}$ may have a negative influence on the performance when function approximation is used with only $s$. A key idea in NW-CFR is that a neural network (called the policy net) is used to approximate the counterfactual regret. We then define a new family of CFR algorithms: weighted CFR.\\n\\nThe sampled advantage $\\\\tau$ may have a negative influence on the performance when function approximation is used with only $s$. As a result, the expectation of the sum of sampled advantages is sub-linearly with $T$.\\n\\nAt the beginning of iteration $t$, the policy $\\\\pi_{k}$ is set to the threshold operation in RM.\\n\\nIf the player plays according to Hedge in state $s$, the player selects an action with probability in proportion to its positive cumulative counterfactual regret. We then define a new family of CFR algorithms: weighted CFR. Finally, we show that NW-CFR is equivalent to a type of weighted CFR with Hedge under certain conditions. The convergence properties of weighted CFR with Hedge and hence NW-CFR are proven accordingly.\\n\\nIn this section, we motivate ACH by introducing a new neural-based CFR algorithm, NW-CFR, which employs a neural network to generalize across states and relies on only trajectory samples for training. If the player plays according to Hedge in state $s$, the policy $\\\\pi_{k}$ selects an action with probability in proportion to its positive cumulative counterfactual regret. In Hedge, the policy $\\\\pi_{k}$ is shift-invariant, which may be more robust to the function approximation error compared to the threshold operation in RM. In the policy profile $\\\\pi_{k}$, softmaxing is shift-invariant, which may be more robust to the function approximation error compared to the threshold operation in RM.\\n\\nIn the policy profile $\\\\pi_{k}$, softmaxing is shift-invariant, which may be more robust to the function approximation error compared to the threshold operation in RM. In the policy profile $\\\\pi_{k}$, softmaxing is shift-invariant, which may be more robust to the function approximation error compared to the threshold operation in RM. In the policy profile $\\\\pi_{k}$, softmaxing is shift-invariant, which may be more robust to the function approximation error compared to the threshold operation in RM.\"}"}
{"id": "DTXZqTNV5nW", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As a result, also\\n\\nTheorem 1.\\n\\nNW-CFR with\\n\\nsecond term to an arbitrarily small value via tightening the range of\\n\\nIn Theorem 1, we proved that the exploitability\\n\\niterations, where\\n\\ngiven that enough trajectories are sampled and\\n\\nDefinition 1.\\n\\nWeighted CFR follows the same procedure as the original CFR (Zinkevich et al., 2008), except that the instantaneous counterfactual regret\\ndemonstrated in the Appendix D.\\n\\nFunction\\n\\n\\\\(\\\\pi\\\\) is sufficiently close to\\n\\nGiven\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)ing probability of state\\n\\nFor\\n\\n\\\\(t\\\\) do\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(t\\\\) \u2190 0\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\mu\\\\)\\n\\n\\\\(\\\\theta\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi\\\\)\\n\\n\\\\(\\\\epsilon\\\\)\\n\\n\\\\(\\\\sum\\\\)\\n\\n\\\\(\\\\omega\\\\)\\n\\n\\\\(\\\\pi"}
{"id": "DTXZqTNV5nW", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Corollary 1. If the behavioral policy $\\\\mu_{p,t}$ for each player $p \\\\in P$ is constant across iterations, and $f_{\\\\mu_{t_p}}(s) > 0$, $\\\\forall s \\\\in S, t > 0$, NW-CFR is equivalent to CFR with Hedge when $y(a|s; \\\\theta_t)$ is sufficiently close to $R_a(s,a)$.\\n\\nAs shown in Corollary 1, when the behavioral policy $\\\\mu_{p,t}$ for each player $p \\\\in P$ is time-invariant, i.e., $w_h(s) = f_{\\\\mu_t_p}(s) = w_l(s)$, $\\\\forall s \\\\in S, t > 0$, the second term of $\\\\epsilon$ in Equation 3 vanishes, and CFR with Hedge is recovered. All the proofs are given in the Appendix C.\\n\\n5 ACH: A Practical Implementation of NW-CFR\\n\\nWhen applying NW-CFR to large-scale problems, two practical issues need to be addressed: The average policy. Theorem 1 and Corollary 1 state the convergence property of the average policy in NW-CFR. Yet, as pointed out in Srinivasan et al. (2018), Hennes et al. (2020), and Perolat et al. (2021), obtaining the average policy with deep neural nets in large-scale games is inherently difficult, due to either the computation or the memory demand. Alternatively, we could employ some additional technique to hopefully induce the current policy convergence towards a NE. Srinivasan et al. (2018) and Hennes et al. (2020) handled this by adding an entropy regularization to the current policy training, which is, to some extent, theoretically justified later in Perolat et al. (2021).\\n\\nTraining on states not sampled. Theoretically, in order to optimize Equation 2, we need to collect both sampled and non-sampled states. Optimizing with only sampled states makes $y(a|s; \\\\theta_t)$ a biased estimation of $R_a(s,a)$. Yet, collecting non-sampled states may be intractable in large-scale games (Li et al., 2020a) or in situations where a perfect environment model is not available.\\n\\nTo strike a balance between theoretical soundness and practical efficiency, we provide a practical implementation of NW-CFR, which is ACH. ACH adapts NW-CFR by training the current policy with an entropy regularization on only sampled states, without the calculation of the average policy. In order to utilize distributed clusters, ACH employs a framework of decoupled acting and learning (similar to IMPALA (Espeholt et al., 2018)), trains the network with mini-batches, and handles asynchronous training with the importance ratio clipping of PPO. The behavior policy $\\\\mu_{p,t}$ is set to $\\\\pi_{p,t}$ in ACH. More details of ACH are presented in the Appendix E.\\n\\n6 RELATED WORK\\n\\nTo obviate the need of abstractions, various neural forms of CFR methods have been developed. An early work of this direction is regression CFR (Waugh et al., 2015), which calculates weights for a number of hand-crafted features to approximate the regret. Deep CFR (Brown et al., 2019) is similar to regression CFR but employs a neural network to approximate the regret. Also, deep CFR traverses a part of the game tree using external sampling, in comparison to the full traversal of the game tree in regression CFR. Double neural CFR (Li et al., 2020a) is another method that approximates the regret and the average policy using deep neural networks, where a novel robust sampling technique is developed. Both deep CFR and double neural CFR build on the tabular Monte Carlo CFR (MCCFR) (Lanctot et al., 2009), where either outcome sampling (sampling one action in a state) or external sampling (sampling all actions in a state) could be employed.\\n\\nWhen dealing with games with long episodes, a necessity may be that only trajectory samples are allowed. To improve the learning performance with only trajectory samples, DREAM (Steinberger et al., 2020) adapts deep CFR by using a learned Q-baseline, which is inspired by the variance reduction techniques in tabular MCCFR (Schmid et al., 2019; Davis et al., 2020). Another recent work using only trajectory samples is ARMAC (Gruslys et al., 2020). By replaying through past policies and using a history-based critic, ARMAC predicts conditional advantages, based on which the policy for each iteration is generated. Other popular neural network based methods, which learn from trajectory samples and are inspired by game-theoretic approaches other than CFR, include neural fictitious self-play (Heinrich & Silver, 2016), policy space response oracles (Lanctot et al., 2017), and exploitability descent (Lockhart et al., 2019), all of which require to compute an approximate best response at each iteration. Such computation may be prohibitive in large-scale games.\\n\\nIn a preliminary experiment presented in the Appendix F, we find that the performance of the current policy in ACH (trained with an entropy regularization) is not sensitive to the choice of $\\\\mu_{p,t}$. 6\"}"}
{"id": "DTXZqTNV5nW", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The most related methods to ACH are Regret Policy Gradient (RPG) (Srinivasan et al., 2018) and Neural Replicator Dynamics (NeuRD) (Hennes et al., 2020), both of which employ the actor-critic framework and thus have similar computation and memory complexities as ACH. RPG minimizes a loss that is an upper bound on the regret after threshold, and the corresponding policy gradient is\\n\\\\[ \\\\nabla_{\\\\theta}^{\\\\text{RPG}}(s) = -\\\\sum_{a} \\\\nabla_{\\\\theta} [Q(s,a;w) - \\\\sum_{b} \\\\pi(b|s;\\\\theta) Q(s,b;w)] + \\\\] However, RPG requires an \\\\( l^2 \\\\) projection after every gradient step for the convergence to a NE, while such projection is not required in ACH. NeuRD is inspired by the replicator dynamics, a well studied model in evolutionary game theory (Gatti et al., 2013). The policy gradient in NeuRD is\\n\\\\[ \\\\nabla_{\\\\theta}^{\\\\text{NeuRD}}(s) = \\\\sum_{a} \\\\left[ \\\\nabla_{\\\\theta} y(a|s;\\\\theta) \\\\right] [Q(s,a;w) - \\\\sum_{b} \\\\pi(b|s) Q(s,b;w)] \\\\]\\nwhere \\\\( y(a|s;\\\\theta) \\\\) is the output of the policy net. There are important differences between ACH and NeuRD in how the algorithm is motivated and how the policy net at each iteration is optimized. Also, the convergence analysis is given only for the single-state all-actions tabular NeuRD (Hennes et al., 2020). Yet, we prove the convergence of NW-CFR, of which ACH is a practical implementation, in full extensive-form games.\\n\\n## EXPERIMENTAL STUDIES\\n\\nWe firstly introduce a 1-on-1 Mahjong benchmark, on which we compare ACH with related state-of-the-art methods of similar computation complexity: PPO, RPG, and NeuRD. Since our goal is to approximate a NE, the standard and default performance metric, exploitability, is employed. We approximate a lower bound on the exploitability of an agent by training a best response against it as suggested in Timbers et al. (2020) and Steinberger et al. (2020), because traversing the full game tree to compute the exact exploitability is intractable in such a large-scale game as 1-on-1 Mahjong. As a complement, head-to-head performance of different methods on 1-on-1 Mahjong is also presented. Moreover, the agent obtained by ACH is evaluated against practised Mahjong human players. To further validate the performance of ACH in IIGs other than 1-on-1 Mahjong, experimental results on a non-trivial poker game, i.e., heads-up Flop Hold\u2019em Poker (FHP) (Brown et al., 2019) are presented. Deep CFR with outcome sampling (OS-DCFR) and DREAM are added to enable a more thorough comparison. Additional results on smaller benchmarks from OpenSpiel (Lanctot et al., 2019) are given in the Appendix G. Note that results are reported for the current policy (of ACH, PPO, RPG, and NeuRD) and the average policy (of OS-DCFR and DREAM) respectively.\\n\\n### 7.1 A 2-LAYER ZERO-SUM MAHJONG BENCHMARK\\n\\nMahjong is a tile-based game that is played worldwide with many regional variations, such as Japanese Riichi Mahjong and Competition Mahjong. Like poker, Mahjong is an IIG and is full of strategy, chance, and calculation. To facilitate Mahjong research from a game-theoretic perspective, we propose a 2-player zero-sum Mahjong benchmark, whose game rules are similar to Competition Mahjong. The corresponding game, \u201c2-player Mahjong Master\u201d, is played by humans in Tencent mobile games. A full description of the game rules is in the Appendix A.1. Apart from being the first benchmark for the 1-on-1 Mahjong game, our benchmark has a larger infoset size and a longer game length (the effects are explained in the Appendix A.3), compared with existing poker benchmarks (Lanctot et al., 2019). The infoset size (i.e., the number of distinct histories in an infoset) in 1-on-1 Mahjong is around \\\\( 10^{11} \\\\), compared to \\\\( 10^3 \\\\) in poker. This is due to the fact that only two private cards are invisible in poker, while there are \\\\( 13 \\\\) invisible tiles in 1-on-1 Mahjong. In addition, players can decide up to about \\\\( 40 \\\\) sequential actions in 1-on-1 Mahjong, whereas most 1-on-1 poker games end within 10 steps. More details about the 1-on-1 Mahjong benchmark are given in the Appendix A.\\n\\n### 7.2 RESULTS ON OUR 1-ON-1 MAHJONG BENCHMARK\\n\\nAll methods run in an asynchronous training platform with overall 800 CPUs, 3200 GB memory, and 8 M40 GPUs in the Ubuntu 16.04 operating system. Each method shares the same neural network architecture, a full description of which is given in the Appendix B. We performed a mild hyper-parameter search on PPO and shared the best setting for all methods. The advantage value is estimated by the Generalized Advantage Estimator (GAE(\\\\( \\\\lambda \\\\))) (Schulman et al., 2016) for all methods. An overview of the hyper-parameters is listed in the Appendix H.1. Approximate Lower Bound Exploitability. To approximate a lower bound on the exploitability of the agents obtained by each method, we train a best response against each agent. The agent of...\"}"}
