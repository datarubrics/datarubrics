{"id": "w2mDq-p9EEf", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: Learning latent SCM parameters given a fixed node ordering for linearly projected causal variables for random ER-1 DAGs with $d = 6, 20, 50$ nodes. The model was trained for 5000 iterations over 3500 data samples out of which 500 were observational points for the single and multi node intervention runs.\\n\\nFigure 9: Learning latent SCM parameters given a fixed node ordering for linearly projected causal variables for random ER-2 DAGs with $d = 6, 20, 50$ nodes. The model was trained for 5000 iterations over 3500 data samples out of which 500 were observational points for the single and multi node intervention runs.\\n\\nA.7 ADDITIONAL RELATED WORK\\n\\nLatent variable models with predefined structure: Examples include VAE (Kingma and Welling, 2013; Rezende et al., 2014) which has an independence assumption between latent variables. To overcome this, S\u00f8nderby et al. (2016) and Zhao et al. (2017) define latent variables with a chain structure in VAEs. Kingma et al. (2016) uses inverse autoregressive flows to improve upon the diagonal covariance of latent variables in VAEs.\"}"}
{"id": "w2mDq-p9EEf", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Learning latent SCM parameters given a fixed node ordering for linearly projected causal variables for random ER-4 DAGs with $d = 6$, $20$, $50$ nodes. The model was trained for 5000 iterations over 3500 data samples out of which 500 were observational points for the single and multi node intervention runs.\\n\\nFigure 11: Learning the latent SCM (i) given a node ordering (top) and (ii) over node orderings (bottom) for linear projection of causal variables for $d = 10$ nodes, $D = 100$ dimensions.\\n\\nCausal discovery: Annadani et al. (2021) casts the Bayesian structure learning problem as an autoregressive one by sequentially predicting edges, in hopes of capturing the potentially multi-modal posterior. Deleu et al. (2022) uses Generative Flow Networks, or GFlowNets (Bengio et al., 2021), a new class of probabilistic methods that lies at the intersection of reinforcement learning and variational inference. The work uses the transitive closure property ensuring that the action space is constrained to actions that do not introduce cycles. Chickering (2002) proposes a greedy search algorithm, but does not scale to large number of nodes. Wang et al. (2022) leverages sum product networks to perform exact Bayesian structure learning. H\u00e4gele et al. (2022) extends the framework of Lorch et al. (2021) to perform Bayesian causal discovery in a setting where interventions are unknown. Xie et al. (2022) is in a setting where the edges exist not just between latent causal variables but with high-dimensional variables in the dataset as well. Other efforts include (Shimizu et al., 2011; Lopez-Paz and Oquab, 2016; Yu et al., 2019; Ghoshal and Honorio, 2018; Ng et al., 2020; Li et al., 2022).\\n\\nA.8 COMPLETE EVALUATION\\n\\nFor the experiments already presented in the main text, this section contains additional a more comprehensive evaluation on the following metrics:\\n\\n\u2022 SHD: The expected CPDAG SHD between the GT and predicted DAG's skeletons.\\n\\n18\"}"}
{"id": "w2mDq-p9EEf", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"- **SHD**: The expected SHD between the GT and predicted DAG.\\n- **MCC**: The expected Mean Correlation Coefficient between the predicted and true causal variables.\\n- **AUROC**: Area under receiver operating characteristic curve between predicted and true graph structure.\\n- **FPR**: False positive rate\\n- **FN**: False negative\\n- **TP**: True positive\\n- **AUPRC**: Area under precision-recall curves\\n- **Precision**: Precision of the graph structure prediction\\n- **TN**: True negatives\\n- **FP**: False positives\\n- **TPR**: True positive rate\\n- **Recall**: Recall of the graph structure prediction\\n- **F1 Score**: $\\\\frac{2 \\\\times \\\\text{Precision} \\\\times \\\\text{Recall}}{\\\\text{Precision} + \\\\text{Recall}}$\\n- **MSE**: Mean squared error between predicted and true edge weights.\"}"}
{"id": "w2mDq-p9EEf", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nFigure 14: Learning the latent SCM given a node ordering for nonlinear projection of causal variables for $d = 20$ nodes, $D = 100$ dimensions.\\n\\nFigure 15: Effect of number of intervention types seen versus on the performance of learning the latent SCM given a node ordering for $d = 5$ nodes, $D = 100$ dimensions.\\n\\n\u2022 $X_{MSE}$: Mean squared reconstruction error over the high dimensional (low-level) data\\n\u2022 $true_{KL}$: The KL divergence between the predicted and GT observational joint distributions.\\n\\nFigure 19 shows the complete evaluation on 18 different metrics for $d = 5$ nodes. The dataset consists of 500 observational points and 2000 interventional points. To sample the 2000 interventional points, we randomly choose 20 intervention sets, and for each intervention set we sample 100 data points with random intervention values.\\n\\nA.9 DISCUSSION ON IDENTIFIABILITY\\n\\nIdentifiability is an important topic of discussion when making conclusions about the recovering the structure of a causal model. However in our work, we approximate a full posterior distribution over the latent SCMs, instead of returning just a single graph. In this setting, questions of identifiability become less critical, as we can assign probabilities for many possible candidate graphs (and parameters) to express our level of confidence that a particular SCM yields the correct causal conclusions. This is a softer guarantee than what identifiability would provide.\\n\\nNevertheless, we refer the reader to recent works in causal representation learning (Brehmer et al., 2022; Ahuja et al., 2022a;c), that have given some identifiability guarantees in conditions similar to ours. Particularly, we rely on the identifiability results presented in Brehmer et al. (2022). But identifiability does not imply learnability. Thus, in this work, we are concerned with the problem of how one can devise a principled practical algorithm to learn a distribution over latent SCMs.\"}"}
{"id": "w2mDq-p9EEf", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 16: Effect of number of intervention types seen versus on the performance of learning the latent SCM given a node ordering for $d = 10$ nodes, $D = 100$ dimensions.\\n\\nFigure 17: Ablation: effect of number of intervention types seen versus on the performance of learning the latent SCM given a node ordering for $d = 5$ nodes, $D = 100$ dimensions.\\n\\nFigure 18: Ablation: effect of number of intervention types seen versus on the performance of learning the latent SCM given a node ordering for $d = 10$ nodes, $D = 100$ dimensions.\\n\\nTable 3: Average experiment runtimes\\n\\n| Nodes ($d$) | Dataset size | Experiment | Projection | Steps | Avg. runtime (min) |\\n|------------|--------------|------------|-----------|-------|-------------------|\\n| 5          | 2500         | Vector data | Linear (fixed ordering) | 5000  | 2                 |\\n| 10         | 10500        | Vector data | Linear (fixed ordering) | 3000  | 6                 |\\n| 20         | 10500        | Vector data | Linear (learned ordering) | 3000  | 60                |\\n| 20         | 20500        | Vector data | Linear (fixed ordering) | 3000  | 150               |\\n| 20         | 80500        | Vector data | Linear (learned ordering) | 8000  | 540               |\\n| 5          | 2500         | Vector data | Nonlinear   | 5000  | 12                |\\n| 20         | 10500        | Vector data | Nonlinear   | 10000 | 100               |\\n| 5          | 5500         | Vector data | Nonlinear   | 2000  | 40                |\\n| 5          | 2500         | Image data  | Nonlinear   | 2000  | 75                |\\n| 10         | 2500         | Image data  | Nonlinear   | 2000  | 50                |\\n| 10         | 5500         | Image data  | Nonlinear   | 2000  | 80                |\"}"}
{"id": "w2mDq-p9EEf", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 19: Training curves: Metric versus number of iterations for $d = 5$ nodes linearly projected to $D = 100$ dimensions, with 20 intervention sets, 100 interventional samples per intervention set, and 500 observational samples.\"}"}
{"id": "w2mDq-p9EEf", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 20: Learning the latent SCM (i) given a node ordering (top) and (ii) over node orderings (bottom) for linear projection of causal variables for $d = 5$ nodes, $D = 100$ dimensions with non-equal noise variance, stochastic decoder, and 10000 iterations: $E$-SHD (\u2193), AUROC (\u2191), MCC (\u2191), MSE (\u2193) [updated with non-equal noise variance, stochastic observation generating process, and stochastic decoder].\\n\\nFigure 21: Learning the latent SCM for nonlinear projection of causal variables for $d = 5$ (top) and $d = 10$ (bottom) nodes, $D = 100$ dimensions, given the node ordering. $E$-SHD (\u2193), AUROC (\u2191), MCC (\u2191), MSE (\u2193) [updated with non-equal noise variance, stochastic observation generating process, and stochastic decoder].\"}"}
{"id": "w2mDq-p9EEf", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Causal learning has long concerned itself with the accurate recovery of underlying causal mechanisms. Such causal modelling enables better explanations of out-of-distribution data. Prior works on causal learning assume that the high-level causal variables are given. However, in machine learning tasks, one often operates on low-level data like image pixels or high-dimensional vectors. In such settings, the entire Structural Causal Model (SCM) \u2013 structure, parameters, and high-level causal variables \u2013 is unobserved and needs to be learnt from low-level data. We treat this problem as Bayesian inference of the latent SCM, given low-level data. For linear Gaussian additive noise SCMs, we present a tractable approximate inference method which performs joint inference over the causal variables, structure and parameters of the latent SCM from random, known interventions. Experiments are performed on synthetic datasets and a causally generated image dataset to demonstrate the efficacy of our approach. We also perform image generation from unseen interventions, thereby verifying out of distribution generalization for the proposed causal model.\\n\\n1 I N T R O D U C T I O N\\n\\nLearning variables of interest and uncovering causal dependencies is crucial for intelligent systems to reason and predict in scenarios that differ from the training distribution. In the causality literature, causal variables and mechanisms are often assumed to be known. This knowledge enables reasoning and prediction under unseen interventions. In machine learning, however, one does not have direct access to the underlying variables of interest nor the causal structure and mechanisms corresponding to them. Rather, these have to be learned from observed low-level data like pixels of an image which are usually high-dimensional. Having a learned causal model can then be useful for generalizing to out-of-distribution data (Scherrer et al., 2022; Ke et al., 2021), estimating the effect of interventions (Pearl, 2009; Sch\u00f6lkopf et al., 2021), disentangling underlying factors of variation (Bengio et al., 2012; Wang and Jordan, 2021), and transfer learning (Sch\u00f6lkopf et al., 2012; Bengio et al., 2019).\\n\\nStructure learning (Spirtes et al., 2000; Zheng et al., 2018) learns the structure and parameters of the Structural Causal Model (SCM) (Pearl, 2009) that best explains some observed high-level causal variables. In causal machine learning and representation learning, however, these causal variables may no longer be observable. This serves as the motivation for our work. We address the problem of learning the entire SCM \u2013 consisting its causal variables, structure and parameters \u2013 which is latent, by learning to generate observed low-level data. Since one often operates in low-data regimes or non-identifiable settings, we adopt a Bayesian formulation so as to quantify epistemic uncertainty over the learned latent SCM. Given a dataset, we use variational inference to learn a joint posterior over the causal variables, structure and parameters of the latent SCM. To the best of our knowledge, ours is the first work to address the problem of Bayesian causal discovery in linear Gaussian latent SCMs from low-level data, where causal variables are unobserved. Our contributions are as follows:\\n\\n\u2022 We propose a general algorithm for Bayesian causal discovery in the latent space of a generative model, learning a distribution over causal variables, structure and parameters in linear Gaussian latent SCMs with random, known interventions. Figure 1 illustrates an overview of the proposed method.\\n\\n\u2022 By learning the structure and parameters of a latent SCM, we implicitly induce a joint distribution over the causal variables. Hence, sampling from this distribution is equivalent to ancestral sampling through the latent SCM. As such, we address a challenging, simultane\"}"}
{"id": "w2mDq-p9EEf", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nFigure 1: Model architecture of the proposed generative model for the Bayesian latent causal discovery task to learn latent SCM from low-level data.\\n\\n\u2022 On a synthetically generated dataset and an image dataset used to benchmark causal model performance (Ke et al., 2021), we evaluate our method along three axes \u2013 uncovering causal variables, structure, and parameters \u2013 consistently outperforming baselines. We demonstrate its ability to perform image generation from unseen interventional distributions.\\n\\n2 PRELIMINARIES\\n\\n2.1 STRUCTURAL CAUSAL MODELS\\n\\nA Structural Causal Model (SCM) is defined by a set of equations which represent the mechanisms by which each endogenous variable $Z_i$ depends on its direct causes $Z_{pa}(i)$ and a corresponding exogenous noise variable $\\\\epsilon_i$. The direct causes are subsets of other endogenous variables. If the causal parent assignment is assumed to be acyclic, then an SCM is associated with a Directed Acyclic Graph (DAG) $G = (V, E)$, where $V$ corresponds to the endogenous variables and $E$ encodes direct cause-effect relationships. The exact value $z_i$ taken on by a causal variable $Z_i$, is given by local causal mechanisms $f_i$ conditional on the values of its parents $z_{pa}(i)$, the parameters $\\\\Theta_i$, and the node's noise variable $\\\\epsilon_i$, as given in equation 1.\\n\\nFor linear Gaussian additive noise SCMs with equal noise variance, i.e., the setting that we focus on in this work, all $f_i$'s are linear functions, and $\\\\Theta$ denotes the weighted adjacency matrix $W$, where each $W_{ji}$ is the edge weight from $j \\\\rightarrow i$. The linear Gaussian additive noise SCM thus reduces to equation 2,\\n\\n$z_i = f_i(z_{pa}(i), \\\\Theta, \\\\epsilon_i)$,  \\\\hspace{1cm} (1)\\n\\n$z_i = \\\\sum_{j \\\\in pa_G(i)} W_{ji} \\\\cdot z_j + \\\\epsilon_i$.  \\\\hspace{1cm} (2)\\n\\n2.2 CAUSAL DISCOVERY\\n\\nStructure learning in prior work refers to learning a DAG according to some optimization criterion with or without the notion of causality (e.g., He et al. (2019)). The task of causal discovery on the other hand, is more specific in that it refers to learning the structure (also parameters, in some cases) of SCMs, and subscribes to causality and interventions like that of Pearl (2009). That is, the methods aim to estimate $(G, \\\\Theta)$. These approaches often resort to modular likelihood scores over causal variables \u2013 like the BGe score (Geiger and Heckerman, 1994; Kuipers et al., 2022) and BDe.\"}"}
{"id": "w2mDq-p9EEf", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nscore (Heckerman et al., 1995) \u2013 to learn the right structure. However, these methods all assume a dataset of observed causal variables. These approaches either obtain a maximum likelihood estimate,\\n\\n\\\\[ G^* = \\\\arg \\\\max_G p(Z|G) \\\\]\\n\\nor \\\\( (G^*, \\\\Theta^*) = \\\\arg \\\\max_{G, \\\\Theta} p(Z|G, \\\\Theta) \\\\), (3)\\n\\nor in the case of Bayesian causal discovery (Heckerman et al., 1997), variational inference is typically used to approximate a joint posterior distribution \\\\( q_{\\\\phi}(G, \\\\Theta) \\\\) to the true posterior \\\\( p(G, \\\\Theta|Z) \\\\) by minimizing the KL divergence between the two,\\n\\n\\\\[ D_{KL}(q_{\\\\phi}(G, \\\\Theta) || p(G, \\\\Theta|Z)) = -E_{G, \\\\Theta \\\\sim q_{\\\\phi}} \\\\log p(Z|G, \\\\Theta) - \\\\log q_{\\\\phi}(G, \\\\Theta) p(G, \\\\Theta), \\\\] (4)\\n\\nwhere \\\\( p(G, \\\\Theta) \\\\) is a prior over the structure and parameters of the SCM \u2013 possibly encoding DAGness.\\n\\nFigure 2 shows the Bayesian Network (BN) over which inference is performed for causal discovery tasks.\\n\\n2.3 LATENT CAUSAL DISCOVERY\\n\\nIn more realistic scenarios, the learner does not directly observe causal variables and they must be learned from low-level data. The causal variables, structure, and parameters are part of a latent SCM. The goal of causal representation learning models is to perform inference of, and generation from, the true latent SCM. Yang et al. (2021) proposes a Causal VAE but is in a supervised setup where one has labels on causal variables and the focus is on disentanglement. Kocaoglu et al. (2018) present causal generative models trained in an adversarial manner but assumes observations of causal variables.\\n\\nGiven the right causal structure as a prior, the work focuses on generation from conditional and interventional distributions. In both the causal representation learning and causal generative model scenarios mentioned above, the Ground Truth (GT) causal graph and parameters of the latent SCM are arbitrarily defined on real datasets and the setting is supervised. Contrary to this, our setting is unsupervised and we are interested in recovering the GT underlying SCM and causal variables that generate the low-level observed data \u2013 we define this as the problem of latent causal discovery, and the BN over which we want to perform inference on is given in figure 3. In the upcoming sections, we discuss related work, formulate our problem setup and propose an algorithm for Bayesian latent causal discovery, evaluate with experiments on causally created vector data and image data, and perform sampling from unseen interventional image distributions to showcase generalization of learned latent SCMs.\\n\\n3 RELATED WORK\\n\\nPrior work can be classified into Bayesian (Koivisto and Sood, 2004; Heckerman et al., 2006; Friedman and Koller, 2013) or maximum likelihood (Brouillard et al., 2020; Wei et al., 2020; Ng et al., 2022) methods, that learn the structure and parameters of SCMs using either score-based (Kass and Raftery, 1995; Barron et al., 1998; Heckerman et al., 1995) or constraint-based (Cheng et al., 2002; Lehmann and Romano, 2005) approaches.\\n\\nCausal discovery: Work in this category assume causal variables are observed and do not operate on low-level data (Spirtes et al., 2000; Viinikka et al., 2020; Yu et al., 2021; Zhang et al., 2022). Peters and B\u00fchlmann (2014) prove identifiability of linear Gaussian SCMs with equal noise variances. Bengio et al. (2019) use the speed of adaptation as a signal to learn the causal direction. Ke et al. (2019) explore learning causal models from unknown, while Scherrer et al. (2021); Tigas et al. (2022); Agrawal et al. (2019); Toth et al. (2022) focus on active learning and experimental design setups on how to perform interventions to efficiently learn causal models. Transformer (Vaswani et al., 2017) based approach learns structure from synthetic datasets and generalize to naturalistic graphs (Ke et al., 2022). Zheng et al. (2018) introduce an acyclicity constraint that penalizes cyclic\"}"}
{"id": "w2mDq-p9EEf", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"graphs, thereby restricting search close to the DAG space. Lachapelle et al. (2019) leverages this constraint to learn DAGs in nonlinear SCMs. Pamfil et al. (2020); Lippe et al. (2022) perform structure learning on temporal data.\\n\\nStructure learning with latent variables: Markham and Grosse-Wentrup (2020) introduces the concept of Measurement Dependence Inducing Latent Causal Models (MCM). The proposed algorithm finds a minimal-MCM that induces the dependencies between observed variables. However, similar to V AEs, the method assumes no causal links between latent variables. Kivva et al. (2021) provides the conditions under which the number of latent variables and structure can be uniquely identified for discrete latent variables, given the adjacency matrix between the hidden and measurement variables has linearly independent columns. Elidan et al. (2000) detects the signature of hidden variables using semi-cliques and then performs structure learning using the structural-EM algorithm (Friedman, 1998) for discrete random variables. Anandkumar et al. (2012) and Silva et al. (2006) considers the identifiability of linear Bayesian Networks when some variables are unobserved. In the former work, the identifiability results hold only for particular classes of DAGs which follow certain structural constraints. Assuming non-Gaussian noise and that certain sets of latents have a lower bound on the number of pure measurement child variables, Xie et al. (2020) proposes the GIN condition to identify the structure between latent confounders. The formulation in the above works involves SCMs where some variables are observed and the others are unobserved. In contrast, the entire SCM is latent in our setup. Finally, GraphV AE (He et al., 2019) learns a structure between latent variables but does not incorporate notions of causality.\\n\\nSupervised causal representation learning: Brehmer et al. (2022) present identifiability theory for learning causal representations and propose an algorithm assuming access to pairs of observational and interventional data. Ahuja et al. (2022a) studies identifiability in a similar setup with the use of sparse perturbations. Ahuja et al. (2022c) discusses identifiability for causal representation learning when one has access to interventional data. Kocaoglu et al. (2018); Shen et al. (2021); Moraffah et al. (2020) introduce generative models that use an SCM-based prior in latent space. In Shen et al. (2021), the goal is to learn causally disentangled variables. Yang et al. (2021) learn a DAG but assumes complete access to the causal variables. Lopez-Paz et al. (2016) establishes observable causal footprints in images. We refer the reader to section A.9 for a discussion on identifiability.\\n\\n4 LEARNING LATENT SCM\\n\\n4.1 PROBLEM\\n\\nWe are given a dataset $D = \\\\{x^{(1)}, \\\\ldots, x^{(N)}\\\\}$, where each $x^{(i)}$ is a high-dimensional observed data \u2013 for simplicity, we assume $x^{(i)}$ is a vector in $\\\\mathbb{R}^D$ but the setup extends to other inputs as well. We assume that there exist latent causal variables $Z = \\\\{z^{(i)} \\\\in \\\\mathbb{R}^d \\\\}_{i=1}^N$ where $d \\\\leq D$, that explain the data $D$, and these latent variables belong to a GT SCM $G_{GT}$, $\\\\Theta_{GT}$. We wish to invert the data generation process $g: (Z, G_{GT}, \\\\Theta_{GT}) \\\\rightarrow D$. In the setting, we also have access to the intervention targets $I = \\\\{I^{(i)}\\\\}_{i=1}^N$ where each $I^{(i)} \\\\in \\\\{0, 1\\\\}^d$. The $j$th dimension of $I^{(i)}$ takes a value of 1 if node $j$ was intervened on in data sample $i$, and 0 otherwise. We will take $X, Z, G, \\\\Theta$ to be random variables over low-level data, latent causal variables, the SCM structure, and SCM parameters respectively.\\n\\n4.2 GENERAL METHOD\\n\\nWe aim to obtain a posterior estimate over the entire latent SCM, $p(Z, G, \\\\Theta | D)$. Computing the true posterior analytically requires calculating the marginal likelihood $p(D)$ which gets quickly intractable due to the number of possible DAGs growing super-exponentially with respect to the number of nodes. Thus, we resort to variational inference (Blei et al., 2017) that provides a tractable way to learn an approximate posterior $q_\\\\phi(Z, G, \\\\Theta)$ with variational parameters $\\\\phi$, close to the true posterior $p(Z, G, \\\\Theta | D)$ by maximizing the Evidence Lower Bound (ELBO), $L(\\\\psi, \\\\phi) = \\\\mathbb{E}_{q_\\\\phi(Z, G, \\\\Theta)} \\\\log p_\\\\psi(D | Z, G, \\\\Theta) - \\\\log q_\\\\phi(Z, G, \\\\Theta)$.\\\\(L(\\\\psi, \\\\phi) = \\\\mathbb{E}_{q_\\\\phi(Z, G, \\\\Theta)} \\\\log p_\\\\psi(D | Z, G, \\\\Theta) - \\\\log q_\\\\phi(Z, G, \\\\Theta).\\)}"}
{"id": "w2mDq-p9EEf", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 6: Learning the latent SCM from pixels of the chemistry dataset for $d = 5$ (top) and $d = 10$ nodes (bottom), given the node ordering.\\n\\nResults: We perform experiments to evaluate latent causal discovery from pixels and known interventions. The results are summarized in figure 6. It can be seen that the proposed approach can recover the SCM significantly better than the baseline approaches in all the metrics even in the realistic dataset. In figure 7, we also assess the ability of the model to sample images from unseen interventions in the chemistry dataset by examining the generated images with GT interventional samples. The matching intensity of each block corresponds to matching causal variables, which demonstrates model generalization.\\n\\nFigure 7: Image sampling from 10 random, unseen interventions: Mean over GT (top row) and predicted (bottom row) image samples in the chemistry dataset for $d = 5$ nodes.\\n\\nLIMITATIONS\\nOur approach makes a number of assumptions. First, we assume the latent SCM is linear Gaussian. However, removing this restriction might be crucial to make the approach more general. Second, we assume access to the number of latent causal variables $d$. Extensions must consider how to infer the number of latent variables. We also make the assumption of known intervention targets whereas this might be restrictive for real-world applications. Future work could overcome this limitation by inferring interventions as in H\u00e4gele et al. (2022). Finally, we have assumed feasibility of interventions and known causal orderings for some of our experiments. However, in reality, some interventions could be infeasible and the causal ordering of latent variables might not be known.\\n\\nCONCLUSION\\nWe presented a tractable approximate inference technique to perform Bayesian latent causal discovery that jointly infers the causal variables, structure and parameters of linear Gaussian latent SCMs under random, known interventions from low-level data. The learned causal model is also shown to generalize to unseen interventions. Our Bayesian formulation allows uncertainty quantification and mutual information estimation which is well-suited for extensions to active causal discovery. Extensions of the proposed method to learn nonlinear, non-Gaussian latent SCMs from unknown interventions would also open doors to general algorithms that can learn causal representations.\"}"}
{"id": "w2mDq-p9EEf", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"REPRODUCIBILITY STATEMENT\\n\\nThe details regarding synthetic data generation of the DAGs, causal variables, and the high dimensional data is mentioned in section 5. The code for data generation, the models used, as well as for all the experiments is available at anonymous.4open.science/r/anon-biols-86E7. The average run-time of all experiments is documented in table 3. Appendix A.8 further evaluates experiments along additional metrics.\\n\\nREFERENCES\\n\\nRaj Agrawal, Chandler Squires, Karren Yang, Karthikeyan Shanmugam, and Caroline Uhler. Abcd-strategy: Budgeted experimental design for targeted causal structure discovery. In Kamalika Chaudhuri and Masashi Sugiyama, editors, Proceedings of the Twenty-Second Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pages 3400\u20133409. PMLR, 16\u201318 Apr 2019. URL https://proceedings.mlr.press/v89/agrawal19b.html.\\n\\nKartik Ahuja, Jason Hartford, and Yoshua Bengio. Weakly supervised representation learning with sparse perturbations. arXiv preprint arXiv:2206.01101, 2022a.\\n\\nKartik Ahuja, Divyat Mahajan, Vasilis Syrgkanis, and Ioannis Mitliagkas. Towards efficient representation identification in supervised learning, 2022b. URL https://arxiv.org/abs/2204.04606.\\n\\nKartik Ahuja, Yixin Wang, Divyat Mahajan, and Yoshua Bengio. Interventional causal representation learning. arXiv preprint arXiv:2209.11924, 2022c.\\n\\nAnimashree Anandkumar, Daniel Hsu, Adel Javanmard, and Sham M Kakade. Learning topic models and latent bayesian networks under expansion constraints. arXiv preprint arXiv:1209.5350, 2012.\\n\\nYashas Annadani, Jonas Rothfuss, Alexandre Lacoste, Nino Scherrer, Anirudh Goyal, Yoshua Bengio, and Stefan Bauer. Variational causal networks: Approximate bayesian inference over causal structures. arXiv preprint arXiv:2106.07635, 2021.\\n\\nA. Barron, J. Rissanen, and Bin Yu. The minimum description length principle in coding and modeling. IEEE Transactions on Information Theory, 44(6):2743\u20132760, 1998. doi: 10.1109/18.720554.\\n\\nEmmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Advances in Neural Information Processing Systems, 34:27381\u201327394, 2021.\\n\\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives, 2012. URL https://arxiv.org/abs/1206.5538.\\n\\nYoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, S\u00b4ebastien Lachapelle, Olexa Bila\u0144ski, Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle causal mechanisms, 2019. URL https://arxiv.org/abs/1901.10912.\\n\\nDavid M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518):859\u2013877, 2017.\\n\\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, et al. Jax: composable transformations of python+ numpy programs. Version 0.2, 5:14\u201324, 2018.\\n\\nJohann Brehmer, Pim De Haan, Phillip Lippe, and Taco Cohen. Weakly supervised causal representation learning. arXiv preprint arXiv:2203.16437, 2022.\\n\\nPhilippe Brouillard, S\u00b4ebastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and Alexandre Drouin. Differentiable causal discovery from interventional data. Advances in Neural Information Processing Systems, 33:21865\u201321877, 2020.\"}"}
{"id": "w2mDq-p9EEf", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "w2mDq-p9EEf", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Aapo Hyvarinen and Hiroshi Morioka. Nonlinear ICA of Temporally Dependent Stationary Sources. In Aarti Singh and Jerry Zhu, editors, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 460\u2013469. PMLR, 20\u201322 Apr 2017. URL https://proceedings.mlr.press/v54/hyvarinen17a.html.\\n\\nRobert E. Kass and Adrian E. Raftery. Bayes factors. Journal of the American Statistical Association, 90(430):773\u2013795, 1995. ISSN 0162-1459. URL http://www.jstor.org/stable/2291091.\\n\\nNan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Bernhard Sch\u00f6lkopf, Michael C Mozer, Chris Pal, and Yoshua Bengio. Learning neural causal models from unknown interventions. arXiv preprint arXiv:1910.01075, 2019.\\n\\nNan Rosemary Ke, Aniket Didolkar, Sarthak Mittal, Anirudh Goyal, Guillaume Lajoie, Stefan Bauer, Danilo Rezende, Yoshua Bengio, Michael Mozer, and Christopher Pal. Systematic evaluation of causal discovery in visual model based reinforcement learning. arXiv preprint arXiv:2107.00848, 2021.\\n\\nNan Rosemary Ke, Silvia Chiappa, Jane Wang, Jorg Bornschein, Theophane Weber, Anirudh Goyal, Matthew Botvinic, Michael Mozer, and Danilo Jimenez Rezende. Learning to induce causal structure. arXiv preprint arXiv:2204.04875, 2022.\\n\\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes, 2013. URL https://arxiv.org/abs/1312.6114.\\n\\nDurk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. Advances in neural information processing systems, 29, 2016.\\n\\nBohdan Kivva, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam. Learning latent causal graphs via mixture oracles. Advances in Neural Information Processing Systems, 34:18087\u201318101, 2021.\\n\\nMurat Kocaoglu, Christopher Snyder, Alexandros G Dimakis, and Sriram Vishwanath. Causalgan: Learning causal implicit generative models with adversarial training. In International Conference on Learning Representations, 2018.\\n\\nMikko Koivisto and Kismat Sood. Exact bayesian structure discovery in bayesian networks. The Journal of Machine Learning Research, 5:549\u2013573, 2004.\\n\\nH. W. Kuhn. The hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2(1-2):83\u201397, 1955. doi: https://doi.org/10.1002/nav.3800020109. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109.\\n\\nJack Kuipers, Polina Suter, and Giusi Moffa. Efficient sampling and structure learning of bayesian networks. Journal of Computational and Graphical Statistics, pages 1\u201312, 2022.\\n\\nS\u00e9bastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradient-based neural dag learning. arXiv preprint arXiv:1906.02226, 2019.\\n\\nE. L. Lehmann and Joseph P. Romano. Testing statistical hypotheses. Springer Texts in Statistics. Springer, New York, third edition, 2005. ISBN 0-387-98864-5.\\n\\nYuke Li, Kenneth Li, Pin Wang, Donglai Wei, Hanspeter Pfister, and Ching-Yao Chan. Intervention-based recurrent causal model for non-stationary video causal discovery, 2022. URL https://openreview.net/forum?id=JvGzKO1QLet.\\n\\nPhillip Lippe, Sara Magliacane, Sindy L\u00f6we, Yuki M Asano, Taco Cohen, and Stratis Gavves. Citris: Causal identifiability from temporal intervened sequences. In International Conference on Machine Learning, pages 13557\u201313603. PMLR, 2022.\"}"}
{"id": "w2mDq-p9EEf", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nYuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel, Kun Zhang, and Javen Qinfeng Shi. Identifying weight-variant latent causal models. arXiv preprint arXiv:2208.14153, 2022.\\n\\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE international conference on computer vision, pages 3730\u20133738, 2015.\\n\\nDavid Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests, 2016. URL https://arxiv.org/abs/1610.06545.\\n\\nDavid Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Sch\u00f6lkopf, and L\u00e9on Bottou. Discovering causal signals in images, 2016. URL https://arxiv.org/abs/1605.08179.\\n\\nLars Lorch, Jonas Rothfuss, Bernhard Sch\u00f6lkopf, and Andreas Krause. Dibs: Differentiable bayesian structure learning. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 24111\u201324123. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ca6ab34959489659f8c3776aaf1f8efd-Paper.pdf.\\n\\nAlex Markham and Moritz Grosse-Wentrup. Measurement dependence inducing latent causal models. In Jonas Peters and David Sontag, editors, Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI), volume 124 of Proceedings of Machine Learning Research, pages 590\u2013599. PMLR, 03\u201306 Aug 2020. URL https://proceedings.mlr.press/v124/markham20a.html.\\n\\nGonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. Learning latent permutations with gumbel-sinkhorn networks. arXiv preprint arXiv:1802.08665, 2018.\\n\\nRaha Moraffah, Bahman Moraffah, Mansooreh Karami, Adrienne Raglin, and Huan Liu. Causal adversarial network for learning conditional and interventional distributions, 2020. URL https://arxiv.org/abs/2008.11376.\\n\\nIgnavier Ng, AmirEmad Ghassami, and Kun Zhang. On the role of sparsity and dag constraints for learning linear dags. Advances in Neural Information Processing Systems, 33:17943\u201317954, 2020.\\n\\nIgnavier Ng, Shengyu Zhu, Zhuangyan Fang, Haoyang Li, Zhitang Chen, and Jun Wang. Masked gradient-based causal structure learning. In Proceedings of the 2022 SIAM International Conference on Data Mining (SDM), pages 424\u2013432. SIAM, 2022.\\n\\nRoxana Pamfil, Nisara Sriwattanaworachai, Shaan Desai, Philip Pilgerstorfer, Konstantinos Georgatzis, Paul Beaumont, and Bryon Aragam. Dynotears: Structure learning from time-series data. In International Conference on Artificial Intelligence and Statistics, pages 1595\u20131605. PMLR, 2020.\\n\\nJudea Pearl. Causality. Cambridge University Press, 2 edition, 2009. doi: 10.1017/CBO9780511803161.\\n\\nJonas Peters and Peter B\u00fchlmann. Identifiability of gaussian structural equation models with equal error variances. Biometrika, 101(1):219\u2013228, 2014.\\n\\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models, 2014. URL https://arxiv.org/abs/1401.4082.\\n\\nNino Scherrer, Olexa Bilaniuk, Yashas Annadani, Anirudh Goyal, Patrick Schwab, Bernhard Sch\u00f6lkopf, Michael C Mozer, Yoshua Bengio, Stefan Bauer, and Nan Rosemary Ke. Learning neural causal models with active interventions. arXiv preprint arXiv:2109.02429, 2021.\\n\\nNino Scherrer, Anirudh Goyal, Stefan Bauer, Yoshua Bengio, and Nan Rosemary Ke. On the generalization and adaption performance of causal models, 2022. URL https://arxiv.org/abs/2206.04620.\"}"}
{"id": "w2mDq-p9EEf", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "w2mDq-p9EEf", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"APPENDIX A.1 DERIVATION OF THE ELBO\\n\\nWe want to minimize the KL divergence between the true and approximate posterior:\\n\\n$$D_{KL}(q_{\\\\phi}(Z,G,\\\\Theta)||p(Z,G,\\\\Theta|D)) = -E_{G,\\\\Theta \\\\sim q_{\\\\phi}(G,\\\\Theta)} \\\\left[ E_{Z \\\\sim p(Z|G,\\\\Theta)} \\\\left[ \\\\log p_{\\\\psi}(D|Z) \\\\right] \\\\right] - \\\\log q_{\\\\phi}(Z,G,\\\\Theta)p(G,\\\\Theta) + \\\\log p(D)$$\\n\\nwhere the prior and posterior factorize as (according to the explanation in 4.2):\\n\\n$$p(Z,G,\\\\Theta) = p(Z|G,\\\\Theta) \\\\cdot p(G,\\\\Theta)$$\\n\\n$$q_{\\\\phi}(Z,G,\\\\Theta) = p(Z|G,\\\\Theta) \\\\cdot q_{\\\\phi}(G,\\\\Theta)$$\\n\\nThus, we have that\\n\\n$$D_{KL}(q_{\\\\phi}(Z,G,\\\\Theta)||p(Z,G,\\\\Theta|D))$$\\n\\nreduces to:\\n\\n$$-E_{G,\\\\Theta \\\\sim q_{\\\\phi}(G,\\\\Theta)} \\\\left[ E_{Z \\\\sim p(Z|G,\\\\Theta)} \\\\left[ \\\\log p_{\\\\psi}(D|Z) \\\\right] \\\\right] - \\\\log q_{\\\\phi}(G,\\\\Theta)p(G,\\\\Theta) + \\\\log p(D)$$\\n\\n(from 8)\\n\\n$$= -E_{L,\\\\Sigma \\\\sim q_{\\\\phi}(L,\\\\Sigma)} \\\\left[ E_{P \\\\sim q_{\\\\phi}(P|L,\\\\Sigma)} \\\\left[ E_{Z \\\\sim p(Z|P,L,\\\\Sigma)} \\\\left[ \\\\log p_{\\\\psi}(D|Z) \\\\right] \\\\right] \\\\right] - \\\\log q_{\\\\phi}(P|L,\\\\Sigma)p(P|L,\\\\Sigma)p(\\\\Sigma) + \\\\log p(D)$$\\n\\n(15)\"}"}
{"id": "w2mDq-p9EEf", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Since the log evidence \\\\( \\\\log p(D) \\\\) is a constant, minimizing the KL divergence corresponds to maximizing the following ELBO:\\n\\n\\\\[\\n\\\\max_{\\\\phi, \\\\psi} E_{L, \\\\Sigma} \\\\sim q_{\\\\phi}(L, \\\\Sigma) \\\\\\\\\\nE_{P \\\\sim q_{\\\\phi}(P | L, \\\\Sigma)} \\\\\\\\\\nE_{Z \\\\sim p(Z | P, L, \\\\Sigma)} [ \\\\log p_{\\\\psi}(D | Z) ] - \\\\log q_{\\\\phi}(P | L, \\\\Sigma) p(P) - \\\\log q_{\\\\phi}(L, \\\\Sigma) p(L) \\\\]\\n\\\\]\\n\\n### A.2 IMPLEMENTATION DETAILS\\n\\nFor all our experiments, we use the AdaBelief (Zhuang et al., 2020) optimizer with \\\\( \\\\epsilon = 10^{-8} \\\\) and a learning rate of 0.0008. \\\\( \\\\tau \\\\) is set to 0.2. Our experiments are fairly robust with respect to hyperparameters and we did not perform hyperparameter tuning for any of our experiments. Table 1 and 2 summarize the network details for MLP \\\\( \\\\phi(T) \\\\) and the decoder \\\\( p_{\\\\psi}(X | Z) \\\\).\\n\\n#### Table 1: Network architecture for MLP \\\\( \\\\phi(T) \\\\)\\n\\n| Layer type | Layer output | Activation |\\n|------------|--------------|------------|\\n| Linear     | 128          | GeLU       |\\n| Linear     | 128          | GeLU       |\\n\\n#### Table 2: Network architecture for the decoder \\\\( p_{\\\\psi}(X | Z) \\\\)\\n\\n| Layer type | Layer output | Activation |\\n|------------|--------------|------------|\\n| Linear     | 16           | GeLU       |\\n| Linear     | 64           | GeLU       |\\n| Linear     | 64           | GeLU       |\\n| Linear     | 64           | GeLU       |\\n| Linear     | \\\\( D \\\\)      |            |\\n\\n### A.3 TRAINING CURVES\\n\\nFigures 8, 9, and 10 summarize the training curves for \\\\( d = 6, 20, 50 \\\\) nodes on ER-DAGs of degree 1, 2, and 4. Each plot contains 3 lines that show training with observational, single-interventional, and multi-interventional data.\\n\\n### A.4 LINEAR PROJECTION EXPERIMENTS\\n\\nDetails for figure 11, \\\\( d = 10 \\\\) nodes: The dataset consists of 500 observational points and 20000 interventional points. To sample the 20000 interventional points, we randomly choose 200 intervention sets, and for each intervention set we sample 100 data points. The model was trained for 8000 epochs to reach convergence.\\n\\nDetails for figure 12, \\\\( d = 20 \\\\) nodes: The dataset consists of 500 observational points and 20000 interventional points. To sample the 20000 interventional points, we randomly choose 100 intervention sets, and for each intervention set we sample 200 data points. The model was trained for 3000 epochs to reach convergence.\\n\\n### A.5 NONLINEAR PROJECTION EXPERIMENTS\\n\\nFigure 13 contains the results for the latent causal discovery problem with and without learning a permutation. Figure 14 shows the results for 500 observational samples and 10000 interventional\"}"}
{"id": "w2mDq-p9EEf", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\n... posterior could be to factorize it as\\n\\n\\\\[ q_{\\\\phi}(Z, G, \\\\Theta) = q_{\\\\phi}(Z) \\\\cdot q_{\\\\phi}(G, \\\\Theta | Z) \\\\quad (6) \\\\]\\n\\nGiven a way to obtain \\\\( q_{\\\\phi}(Z) \\\\), the conditional \\\\( q_{\\\\phi}(G, \\\\Theta | Z) \\\\) can be obtained using existing Bayesian structure learning methods. Otherwise, one has to perform a hard simultaneous optimization which would require alternating optimizations on \\\\( Z \\\\) and on \\\\( (G, \\\\Theta) \\\\) given an estimate of \\\\( Z \\\\). Difficulty of such an alternate optimization is discussed in Brehmer et al. (2022).\\n\\nAlternate factorization of the posterior: Rather than factorizing as in equation 6, we propose to only introduce a variational distribution \\\\( q_{\\\\phi}(G, \\\\Theta) \\\\) over structures and parameters, so that the approximation is given by\\n\\n\\\\[ q_{\\\\phi}(Z, G, \\\\Theta) = p(Z | G, \\\\Theta) \\\\cdot q_{\\\\phi}(G, \\\\Theta) \\\\]\\n\\nThe advantage of this factorization is that the true distribution \\\\( p(Z | G, \\\\Theta) \\\\) over \\\\( Z \\\\) is completely determined from the SCM given \\\\( (G, \\\\Theta) \\\\) and exogenous noise variables (assumed to be Gaussian). This conveniently avoids the hard simultaneous optimization problem mentioned above since optimizing for \\\\( q_{\\\\phi}(Z) \\\\) is not necessary.\\n\\nHence, equation 5 simplifies to:\\n\\n\\\\[ L(\\\\psi, \\\\phi) = E_{q_{\\\\phi}(Z, G, \\\\Theta)} \\\\log p_{\\\\psi}(D | Z) - \\\\log q_{\\\\phi}(G, \\\\Theta) p(G, \\\\Theta) - \\\\log p(Z | G, \\\\Theta) p(Z | G, \\\\Theta) \\\\quad (7) \\\\]\\n\\nSuch a posterior can be used to obtain an SCM by sampling \\\\( \\\\hat{G} \\\\) and \\\\( \\\\hat{\\\\Theta} \\\\) from the approximated posterior. As long as the samples \\\\( \\\\hat{G} \\\\) are always acyclic, one can perform ancestral sampling through the SCM to obtain predictions of the causal variables \\\\( \\\\hat{z}(i) \\\\). For additive noise models like in equation 2, these samples are already reparameterized and differentiable with respect to their parameters. The samples of causal variables are then fed to the likelihood model to predict samples \\\\( \\\\hat{x}(i) \\\\) that reconstruct the observed data \\\\( x(i) \\\\).\\n\\n4.3 POSTERIOR PARAMETERIZATIONS AND PRIORS\\n\\nFor linear Gaussian latent SCMs, which is the focus of this work, learning a posterior over \\\\( (G, \\\\Theta) \\\\) is equivalent to learning \\\\( q_{\\\\phi}(W, \\\\Sigma) \\\\) \u2013 a posterior over weighted adjacency matrices \\\\( W \\\\) and noise covariances \\\\( \\\\Sigma \\\\). We follow an approach similar to (Cundy et al., 2021). We express \\\\( W \\\\) via a permutation matrix \\\\( P \\\\) and a lower triangular edge weight matrix \\\\( L \\\\), according to\\n\\n\\\\[ W = P^T L^T P \\\\]\\n\\nHere, \\\\( L \\\\) is defined in the space of all weighted adjacency matrices with a fixed node ordering where node \\\\( j \\\\) can be a parent of node \\\\( i \\\\) only if \\\\( j > i \\\\). Search over permutations corresponds to search over different node orderings and thus, \\\\( W \\\\) and \\\\( \\\\Sigma \\\\) parameterize the space of SCMs. Further, we factorize the approximate posterior \\\\( q_{\\\\phi}(P, L, \\\\Sigma) \\\\) as\\n\\n\\\\[ q_{\\\\phi}(G, \\\\Theta) \\\\equiv q_{\\\\phi}(W, \\\\Sigma) \\\\equiv q_{\\\\phi}(P, L, \\\\Sigma) = q_{\\\\phi}(P | L, \\\\Sigma) \\\\cdot q_{\\\\phi}(L, \\\\Sigma) \\\\quad (8) \\\\]\\n\\nCombining equation 7 and 8 leads to the following ELBO which has to be maximized (derived in A.1), and the overall method is summarized in algorithm 1,\\n\\n\\\\[ L(\\\\psi, \\\\phi) = E_{q_{\\\\phi}(L, \\\\Sigma)} \\\\left[ E_{q_{\\\\phi}(P | L, \\\\Sigma)} \\\\left[ E_{q_{\\\\phi}(Z|P,L,\\\\Sigma)} \\\\left[ \\\\log p_{\\\\psi}(D | Z) \\\\right] \\\\right] \\\\right] - \\\\log q_{\\\\phi}(P | L, \\\\Sigma) p(P) - \\\\log q_{\\\\phi}(L, \\\\Sigma) p(L) p(\\\\Sigma) \\\\quad (9) \\\\]\\n\\nDistribution over \\\\((L, \\\\Sigma)\\\\): The posterior distribution \\\\( q_{\\\\phi}(L, \\\\Sigma) \\\\) has \\\\((d(d-1)/2+1)\\\\) elements to be learnt in the equal noise variance setting. This is parameterized as a diagonal covariance normal distribution. For the prior \\\\( p(L) \\\\) over the edge weights, we promote sparse DAGs by using a horseshoe prior (Carvalho et al., 2009), similar to Cundy et al. (2021). A Gaussian prior is defined over \\\\( \\\\log \\\\Sigma \\\\).\\n\\nDistribution over \\\\( P \\\\): Since the values of \\\\( P \\\\) are discrete, performing a discrete optimization is combinatorial and becomes quickly intractable with increasing \\\\( d \\\\). This can be handled by relaxing the discrete permutation learning problem to a continuous optimization problem. This is commonly done by introducing a Gumbel-Sinkhorn (Mena et al., 2018) distribution and where one has to calculate \\\\( S((T+\\\\gamma)/\\\\tau) \\\\), where \\\\( T \\\\) is the parameter of the Gumbel-Sinkhorn, \\\\( \\\\gamma \\\\) is a matrix of standard Gumbel noise, and \\\\( \\\\tau \\\\) is a fixed temperature parameter. The logits \\\\( T \\\\) are predicted by passing the\\n\\n\\\\[ 1 \\\\rightarrow \\\\]\"}"}
{"id": "w2mDq-p9EEf", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\n\\nBayesian latent causal discovery to learn \\\\( G, \\\\Theta, Z \\\\) from high dimensional data\\n\\nInput: \\\\( D, I \\\\)\\nOutput: Posterior samples over \\\\( G, \\\\Theta, Z \\\\)\\n\\n1: Initialize \\\\( q(\\\\phi(L, \\\\Sigma)) \\\\), MLP \\\\( \\\\phi(T) \\\\), \\\\( p(\\\\psi(X | Z)) \\\\), \\\\( \\\\tau \\\\) and set learning rate \\\\( \\\\alpha \\\\)\\n\\n2: for num epochs do\\n\\n3: \\\\((b_L, b_\\\\Sigma) \\\\sim q(\\\\phi(L, \\\\Sigma))\\\\) \\\\(\\\\triangleright\\\\) Compute logits for sampling from \\\\( q(\\\\phi(P | L, \\\\Sigma)) \\\\)\\n\\n4: \\\\( T \\\\leftarrow \\\\text{MLP}(\\\\phi(T))(b_L, b_\\\\Sigma) \\\\)\\n\\n5: \\\\( \\\\gamma \\\\in \\\\mathbb{R}^{d \\\\times d} \\\\sim \\\\text{standard Gumbel} \\\\)\\n\\n6: \\\\( b_P^{\\\\text{soft}} \\\\leftarrow \\\\text{Sinkhorn}(\\\\frac{T + \\\\gamma}{\\\\tau}) \\\\)\\n\\n7: \\\\( b_P^{\\\\text{hard}} \\\\leftarrow \\\\text{Hungarian}(b_P^{\\\\text{soft}}; \\\\tau \\\\to 0) \\\\)\\n\\n8: \\\\( c_W \\\\leftarrow b_P^T b_L^T b_P \\\\)\\n\\n9: for \\\\( i \\\\leftarrow 1 \\\\) to \\\\( N \\\\) do\\n\\n10: \\\\( C(i) \\\\leftarrow \\\\text{argwhere}(I(i) = 1) \\\\)\\n\\n11: \\\\( f_W = \\\\text{copy}(c_W) \\\\)\\n\\n12: \\\\( f_W[:, C(i)] \\\\leftarrow 0 \\\\) \\\\(\\\\triangleright\\\\) Mutated weighted adjacency matrix according to \\\\( I(i) \\\\)\\n\\n13: \\\\( c_W^I(i) \\\\leftarrow f_W \\\\)\\n\\n14: \\\\( \\\\hat{z}(i) \\\\leftarrow \\\\text{AncestralSample}(c_W^I(i), b_\\\\Sigma) \\\\)\\n\\n15: end for\\n\\n16: \\\\( \\\\hat{Z} \\\\leftarrow \\\\{\\\\hat{z}(i)\\\\}_{i=1}^N \\\\)\\n\\n17: \\\\( \\\\hat{D} \\\\sim p(\\\\psi(X | Z = \\\\hat{Z})) \\\\)\\n\\n18: \\\\( \\\\psi \\\\leftarrow \\\\psi + \\\\alpha \\\\cdot \\\\nabla \\\\psi(L(\\\\psi, \\\\phi)) \\\\) \\\\(\\\\triangleright\\\\) Update network parameters\\n\\n19: \\\\( \\\\phi \\\\leftarrow \\\\phi + \\\\alpha \\\\cdot \\\\nabla \\\\phi(L(\\\\psi, \\\\phi)) \\\\)\\n\\n20: end for\\n\\n21: return binary \\\\((c_W, (c_W, b_\\\\Sigma))\\\\), \\\\( \\\\hat{Z} \\\\) predicted \\\\((L, \\\\Sigma)\\\\) through an MLP. In the limit of infinite iterations and as \\\\( \\\\tau \\\\to 0 \\\\), sampling from the distribution returns a doubly stochastic matrix. During the forward pass, a hard permutation \\\\( P \\\\) is obtained by using the Hungarian algorithm (Kuhn, 1955) which allows \\\\( \\\\tau \\\\to 0 \\\\). During the backward pass, a soft permutation is used to calculate gradients similar to (Cundy et al., 2021; Charpentier et al., 2022). We use a uniform prior \\\\( p(P) \\\\) over permutations.\\n\\n5 EXPERIMENTS AND EVALUATION\\n\\nWe perform experiments to evaluate the learned posterior over \\\\((Z, G, \\\\Theta)\\\\) of the true linear Gaussian latent SCM from high-dimensional data. We aim to highlight the performance of our proposed method on latent causal discovery. As proper evaluation in such a setting would require access to the GT causal graph that generated the high-dimensional observations, we test our method against baselines on synthetically generated vector data and in the realistic case of learning the SCM from pixels in the chemistry environment dataset of (Ke et al., 2021), both of which have a GT causal structure to be compared with. Further, we evaluate the ability of our model to sample images from unseen interventional distributions.\\n\\nBaselines: Since we are, to the best of our knowledge, the first to study this setting of Bayesian learning of latent SCMs from low level observations, we are not aware of baseline methods that solve this task. However, we compare our approach against two baselines: (i) Against V AE that has a marginal independence assumption between latent variables and thus have a predefined structure in the latent space, and (ii) against GraphV AE (He et al., 2019) that learns a structure between latent variables. For all baselines, we treat the learned latent variables as causal variables and compare the recovered structure, parameters, and causal variables recovered. Since GraphV AE does not learn the parameters, we fix the edge weight over all predicted edges to be 1.\\n\\nEvaluation metrics: To evaluate the learned structure, we use two metrics commonly used in the literature \u2013 the expected Structural Hamming Distance (\\\\( E\\\\text{-SHD} \\\\), lower is better) obtains the SHD (number of edge flips, removals, or additions) between the predicted and GT graph and then takes an expectation over SHDs of posterior DAG samples, and the Area Under the Receiver Operating\"}"}
{"id": "w2mDq-p9EEf", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Learning the latent SCM (i) given a node ordering (top) and (ii) over node orderings (bottom) for linear projection of causal variables for $d = 5$ nodes, $D = 100$ dimensions: $E$-SHD ($\\\\downarrow$), AUROC ($\\\\uparrow$), MCC ($\\\\uparrow$), MSE ($\\\\downarrow$).\\n\\nCharacteristic curve (AUROC, higher is better) where a score of 0.5 corresponds to a random DAG baseline. To evaluate the learned parameters of the linear Gaussian latent SCM, we use the Mean Squared Error (MSE, lower is better) between the true and predicted edge weights. To evaluate the learned causal variables, we use the Mean Correlation Coefficient (MCC, higher is better) following Hyvarinen and Morioka (2017); Zimmermann et al. (2021) and Ahuja et al. (2022) which calculates a score between the true and predicted causal variables. See appendix A.3 and A.8 for training curves and more extensive evaluations of the experiments along other metrics. All our implementations are in JAX (Bradbury et al., 2018) and results are presented over 20 random DAGs.\\n\\n5.1 EXPERIMENTS ON SYNTHETIC DATA\\n\\nWe evaluate our proposed method with the baselines on synthetically generated dataset, where we have complete control over the data generation procedure.\\n\\n5.1.1 SYNTHETIC VECTOR DATA GENERATION\\n\\nTo generate high-dimensional vector data with a known causal structure, we first generate a random DAG and linear SCM parameters, and generate true causal variables by ancestral sampling. This is then used to generate corresponding high-dimensional dataset with a random projection function.\\n\\nGenerating the DAG and causal variables: Following many works in the literature, we sample random Erd\u0151s\u2013R\u00e9nyi (ER) DAGs (Erdos et al., 1960) with degrees in $\\\\{1, 2, 4\\\\}$ to generate the DAG. For every edge in this DAG, we sample the magnitude of edge weights uniformly as $|L| \\\\sim U(0.5, 2.0)$ and randomly sample the permutation matrix. We perform ancestral sampling through this random DAG with intervention targets $I$, to obtain $Z$ and then project it to $D$ dimensions to obtain $\\\\{x(i)\\\\}_{N_i=1}^\\\\infty$.\\n\\nGenerating high-dimensional vectors from causal variables: We consider two different cases of generating the high-dimensional data from the causal variables obtained in the previous step: (i) $x(i)$ is a random linear projection of causal variables, $z(i)$, from $\\\\mathbb{R}^d$ to $\\\\mathbb{R}^D$, according to $x = \\\\tilde{P}z$, where $\\\\tilde{P} \\\\in \\\\mathbb{R}^d \\\\times D$ is a random projection matrix. (ii) $x(i)$ is a nonlinear projection of causal variables, $z(i)$, modeled by a 3-layer MLP.\"}"}
{"id": "w2mDq-p9EEf", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\n$d=5$, $ER-1$\\n\\n$d=5$, $ER-2$\\n\\n$d=5$, $ER-4$\\n\\n$SHD$\\n\\n$AUROC$\\n\\n$MCC(z, z)$\\n\\n$MSE(L, L)$\\n\\n$d=10$, $ER-1$\\n\\n$d=10$, $ER-2$\\n\\n$d=10$, $ER-4$\\n\\n$0$\\n\\n$20$\\n\\n$40$\\n\\n$60$\\n\\n$0.4$\\n\\n$0.6$\\n\\n$0.8$\\n\\n$1.0$\\n\\n$0$\\n\\n$25$\\n\\n$50$\\n\\n$75$\\n\\n$100$\\n\\n$0$\\n\\n$1$\\n\\n$2$\\n\\n$3$\\n\\nOurs\\n\\nGraphVAE\\n\\nVAE\\n\\nFigure 5: Learning the latent SCM for nonlinear projection of causal variables for $d=5$ (top) and $d=10$ (bottom) nodes, $D = 100$ dimensions, given the node ordering. $E$-SHD (\u2193), AUROC (\u2191), MCC (\u2191), MSE (\u2193).\\n\\nepochs so as to reach convergence. Figure 4 summarizes the results for $d=5$ nodes, for which we use 500 observational data points and 2000 interventional data points. Of the 2000 interventional data, we generate 100 random interventional data points per set over 20 intervention sets. It can be seen that when permutation is given, the proposed method can recover the causal graph correctly in all the cases, achieving $E$-SHD of 0 and AUROC of 1. When the permutation is learned, the proposed method still recovers the true causal structure very well. However, this is not the case with baseline methods of VAE and GraphVAE, which perform significantly worse on most of the metrics. Figure 11 and 12 (in Appendix) show the results for $d=10$ and $d=20$ nodes.\\n\\nResults on nonlinear projection of causal variables: For $d=5$, 10, 20 nodes projected to $D = 100$ dimensions, we evaluate our algorithm on synthetic ER-1, ER-2, and ER-4 DAGs, given the permutation. Figure 5 summarizes the results for 5 and 10 nodes. As in the linear case, the proposed method recovers the true causal structure and the true causal variables, and is significantly better than the VAE and GraphVAE baselines on all the metrics considered. For experiments in this setting, we noticed empirically that learning the permutation is hard, and performs not so different from a null graph baseline (Figure 13). This observation complements the identifiability result that recovery of latent variables is possible only up to a permutation in latent causal models (Brehmer et al., 2022; Liu et al., 2022) for general nonlinear mappings between causal variables and low-level data. This supports our observation of not being able to learn the permutation in nonlinear projection settings \u2013 but once the permutation is given to the model, it can quickly recover the SCM (figures 5, 13). Refer figure 14 (in Appendix) for results on $d=20$ nodes.\\n\\n5.2 RESULTS ON LEARNING LATENT SCM FROM PIXEL DATA\\n\\nDataset and Setup: A major challenge with evaluating latent causal discovery models on images is that it is hard to obtain images with corresponding GT graph and parameters. Other works (Kocaoglu et al., 2018; Yang et al., 2021; Shen et al., 2021) handle this by assuming the dataset is generated from certain causal variables (assumed to be attributes like gender, baldness, etc.) and a causal structure that is heuristically set by experts, usually in the CelebA dataset (Liu et al., 2015). This makes evaluation particularly noisy. Given these limitations, we verify if our model can perform latent causal discovery by evaluating on images from the chemistry dataset proposed in Ke et al. (2021) \u2013 a scenario where all GT factors are known. We use the environment to generate blocks of different intensities according to a linear Gaussian latent SCM where the parent block colors affect the child block colors then obtain the corresponding images of blocks. The dataset allows generating pixel data from random DAGs and linear SCMs. For this step, we use the same technique to generate causal variables as in the synthetic dataset section. Similar to experiments on nonlinear vector data, we are given the node ordering in this setting.\"}"}
