{"id": "W08IqLMlMer", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline reinforcement learning leverages static datasets to learn optimal policies with no necessity to access the environment. This is desirable for multi-agent systems due to the expensiveness of agents' online interactions and the demand for sample numbers. Yet, in multi-agent reinforcement learning (MARL), the paradigm of offline pre-training with online fine-tuning has never been reported, nor datasets or benchmarks for offline MARL research are available. In this paper, we intend to investigate whether offline training is able to learn policy representations that elevate performance on downstream MARL tasks. We introduce the first offline dataset based on StarCraftII with diverse quality levels and propose a multi-agent decision transformer (MADT) for effective offline learning. MADT integrates the powerful temporal representation learning ability of Transformer into both offline and online multi-agent learning, which promotes generalisation across agents and scenarios. The proposed method demonstrates superior performance than the state-of-the-art algorithms in offline MARL. Furthermore, when applied to online tasks, the pre-trained MADT largely improves sample efficiency, even in zero-shot task transfer. To our best knowledge, this is the first work to demonstrate the effectiveness of pre-trained models in terms of sample efficiency and generalisability enhancement in MARL.\\n\\nIntroduction\\n\\nMulti-Agent reinforcement learning (MARL) plays an essential role for solving complex decision-making tasks by learning from the interaction data between machine or autonomous agents and simulated physical environments. It typically applies to self-driving (Shalev-Shwartz et al., 2016), traffic control (Bazzan, 2009), and recommendation system (Xian et al., 2019), and, to specifically mention, surpasses human beings on some video games (Bellemare et al., 2013; Wang et al., 2020). However, the experience-based policy learning scheme requires the algorithms with high sample efficiency because of the limited computing resources and high cost resulting from the data collection (Haarnoja et al., 2018; Munos et al., 2016; Espeholt et al., 2019; 2018). Furthermore, even in domains where the online environment is feasible, we might still prefer to utilize previously collected data instead, for example, if the domain's complex requires large datasets for effective generalization. In addition, a policy trained on one scenario usually cannot perform well on another even though under the same task. Therefore, a universal policy is critical for saving the training time of general reinforcement learning. Offline RL (Levine et al., 2020) attracts more researchers motivated by improving online reinforcement learning with offline datasets (Fu et al., 2020). Those methods like CQL (Kumar et al., 2020), BEAR (Kumar et al., 2019), and BCQ (Fujimoto et al., 2019) conventionally leverage the current off-policy RL algorithms, e.g., DQN (Mnih et al., 2015), with provable constraints (Rashidinejad et al., 2021). They mainly focus on solving the overestimation of Q values learned in offline datasets and the distribution shift between offline and online. However, those constraints limit the performance with insufficiently offline dataset exploitation (Kumar et al., 2019). Recently, the transformer shows its advantage of representing the sequential data for the classification or prediction tasks. Most related to our work, Chen et al. (2021) replace the constrained Q networks in conventional offline RL algorithms with causal transformer and demonstrate the superiority comparing with baselines such as Behavior Cloning (BC) and state-of-the-art offline algorithms in single-agent offline RL. Nevertheless, the overestimation problems emerge in the offline multi-agent RL when one directly applies the sequential modeling method. We propose leveraging...\"}"}
{"id": "W08IqLMlMer", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This paper proposes Multi-Agent Decision Transformers (MADT) for pre-training the general policy on offline datasets, capable of generalizing onto other seen/unseen environments. Firstly, we collect the offline datasets from the well-known, challenging MARL environment, StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019), interacting with the state-of-the-art algorithm, multi-agent PPO (Yu et al., 2021). Then we construct transformer variants for multiple agents with parameter sharing or acing as a part. We give the paradigm of pre-training a model with the transformer and fine-tuning it with MARL. We conclude the main contribution as follows:\\n\\n\u2022 We propose a series of transformer variants for offline MARL via leveraging the sequential modeling of the attention mechanism.\\n\u2022 We derive a framework with offline pretraining and online fine-tuning to improve sample efficiency and generality across different scenarios.\\n\u2022 We are the first to apply the pre-trained model across different multi-agent scenarios to validate the transferring ability. Experimental results show fast adaptation and superior performance.\\n\u2022 We build a dataset with different skill-level covering all SMAC scenarios for benchmarking progress in MARL pre-training.\"}"}
{"id": "W08IqLMlMer", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Details of hyper-parameters used for MADT experiments are listed from Table 4 to 8.\\n\\n| Hyper-parameter Value | Hyper-parameter Value | Hyper-parameter Value |\\n|-----------------------|-----------------------|-----------------------|\\n| offline_train_critic  | True                  | max_timestep          | 400                    |\\n| eval_epochs           | 32                    | n_layer               | 2                      |\\n| n_head                | 2                     | n_embd                | 32                     |\\n| online_buffer_size    | 64                    | model_type            | state_only             |\\n| mini_batch_size       | 128                   |                        |                        |\\n\\nTable 4: Common hyper-parameters for all MADT experiments\\n\\n| Maps             | offline_episode_num | offline_lr | 3s5z | 1000  | 1e-4 | 5s5z | 1000  | 5e-4 | 3s5z_vs_3s6z | 1000  | 5e-4 | 1000 | 5e-4 | corridor | 1000  | 5e-4 | 1000 | 5e-4 | 3s_vs_5z | 1000  | 1e-4 | 5e-4 | 10 |\\n|------------------|---------------------|-------------|------|-------|------|------|-------|------|--------------|-------|------|-------|------|----------|-------|------|-------|------|---------|-------|------|-------|------|---------|-------|------|-------|------|\\n| offline_episode_num | offline_lr | 3s5z | 1000 | 5e-4 | 5s5z | 1000 | 5e-4 | 3s5z_vs_3s6z | 1000 | 5e-4 | 1000 | 5e-4 | corridor | 1000 | 5e-4 | 1000 | 5e-4 | 3s_vs_5z | 1000 | 1e-4 | 5e-4 | 10 |\\n\\nTable 5: Hyper-parameters for MADT experiments in Figure 3\\n\\n| Maps             | offline_episode_num | offline_lr | online_lr | online_ppo_epochs |\\n|------------------|---------------------|-------------|-----------|-------------------|\\n| 2c_vs_64zg       | 1000                | 5e-4        | 5e-4      | 10                |\\n| 10m_vs_11m       | 1000                | 5e-4        | 5e-4      | 10                |\\n| 8m_vs_9m         | 1000                | 1e-4        | 5e-4      | 10                |\\n| 3s_vs_5z         | 1000                | 1e-4        | 5e-4      | 10                |\\n| 3s5z             | 1000                | 1e-4        | 5e-4      | 10                |\\n| 3m               | 1000                | 1e-4        | 5e-4      | 15                |\\n| 2s_vs_1sc        | 1000                | 1e-4        | 5e-4      | 15                |\\n| MMM              | 1000                | 1e-4        | 1e-4      | 5                 |\\n| so_many_baneling | 1000                | 1e-4        | 1e-4      | 5                 |\\n| 8m               | 1000                | 1e-4        | 1e-4      | 5                 |\\n| 3s_vs_3z         | 1000                | 1e-4        | 1e-4      | 5                 |\\n| 3s_vs_4z         | 1000                | 1e-4        | 1e-4      | 5                 |\\n| bane_vs_bane     | 1000                | 1e-4        | 1e-4      | 5                 |\\n| 2m_vs_1z         | 1000                | 1e-4        | 1e-4      | 5                 |\\n| 2c_vs_64zg       | 1000                | 1e-4        | 1e-4      | 5                 |\\n| 5m_vs_6m         | 1000                | 1e-4        | 1e-4      | 10                |\\n| corridor         | 1000                | 1e-4        | 1e-4      | 10                |\\n| 3s5z_vs_3s6z     | 1000                | 1e-4        | 1e-4      | 10                |\\n\\nTable 6: Hyper-parameters for MADT experiments in Figure 4, 6 and Table 1\"}"}
{"id": "W08IqLMlMer", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nTable 7: Hyper-parameters for MADT experiments in Figure 5a\\n\\n| Hyper-parameter     | Value                          |\\n|--------------------|--------------------------------|\\n| offline_map_lists  | [3s_vs_4z, 2m_vs_1z, 3m, 2s_vs_1sc, 3s_vs_3z] |\\n| offline_episode_num| [200, 200, 200, 200, 200]      |\\n| offline_lr         | 5e-4                           |\\n| online_lr          | 1e-4                           |\\n| online_ppo_epochs  | 5                              |\\n\\nTable 8: Hyper-parameters for MADT experiments in Figure 5b\\n\\n| Hyper-parameter     | Value                          |\\n|--------------------|--------------------------------|\\n| offline_map_lists  | [2m_vs_1z, 3m, 2s_vs_1sc, 3s_vs_3z] |\\n| offline_episode_num| [250, 250, 250, 250]           |\\n| offline_lr         | 5e-4                           |\\n| online_lr          | 1e-4                           |\\n| online_ppo_epochs  | 5                              |\"}"}
{"id": "W08IqLMlMer", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The position-wise feed-forward network is another core module of the transformer. It consists of two linear transformations with a ReLU activation in between. The dimensionality of inputs and outputs is \\\\(d_{\\\\text{model}}\\\\), and that of the feed-forward layer is \\\\(d_{\\\\text{ff}}\\\\). Specially, \\\\[ \\\\text{FFN}(x) = \\\\text{max}(0, x W_1 + b_1) W_2 + b_2 \\\\] (4)\\n\\nwhere \\\\(W_1 \\\\in \\\\mathbb{R}^{d_{\\\\text{model}} \\\\times d_{\\\\text{ff}}}\\\\) and \\\\(W_2 \\\\in \\\\mathbb{R}^{d_{\\\\text{ff}} \\\\times d_{\\\\text{model}}}\\\\) are the weights, and \\\\(b_1 \\\\in \\\\mathbb{R}^{d_{\\\\text{ff}}}\\\\) and \\\\(b_2 \\\\in \\\\mathbb{R}^{d_{\\\\text{model}}}\\\\) are the biases. Across different positions are the same linear transformations. Note that the position encoding for leveraging the order of the sequence as follows:\\n\\n\\\\[ \\\\text{PE}(\\\\text{pos}, 2^i) = \\\\text{sin}(\\\\text{pos}/10000^{\\\\frac{2^i}{d_{\\\\text{model}}}}) \\\\]\\n\\\\[ \\\\text{PE}(\\\\text{pos}, 2^i + 1) = \\\\text{cos}(\\\\text{pos}/10000^{\\\\frac{2^i}{d_{\\\\text{model}}}}) \\\\] (5)\\n\\n3. Methodology\\n\\nIn this section, we demonstrate how the transformer is applied to our offline pre-training MARL framework. Firstly, we introduce an offline MARL method, in which the transformer maps between the local observations and actions of each agent in the offline dataset via parameter sharing sequentially. Then we leverage the hidden representation as the input of the multi-agent decision transformer (MADT) to minimize the cross-entropy loss. Furthermore, we introduce how to integrate the online MARL with MADT in constructing our whole framework to train a universal MARL policy. To accelerate the online learning, we load the pre-trained model as a part of MARL algorithms and learn the policy based on experience in the latest buffer stored from the online environment. To train a universal MARL policy quickly adapting to other tasks, we bridge the gap between different scenarios from observations, actions, and available actions, respectively. Figure 1 overviews our method from the perspective of offline pre-training with supervised learning and online fine-tuning with MARL algorithms.\\n\\n3.1 Multi-Agent Decision Transformer\\n\\nAlgorithm 1 shows the training process of Multi-Agent Decision Transformer (MADT), in which we autoregressively encode the trajectories from the offline datasets in offline pre-trained MARL and train the transformer-based network with supervised learning. We carefully reformulate the trajectories as the inputs of the causal transformer different from those in the Decision Transformer (Chen et al., 2021). Denote that we deprecate the reward-to-go and actions that are encoded with states together in the single-agent DT. We will interpret the reason for this in the next section. Similar to the seq2seq models, MADT is based on the autoregressive architecture with the reformulated sequential inputs across timescale. The left part of Figure 2 shows the architecture. The causal transformer encodes the agent \\\\(i\\\\)'s trajectory sequence \\\\(\\\\tau_{ti}\\\\) at the time step \\\\(t\\\\) to a hidden representation \\\\(h_t = (h_1, h_2, ..., h_l)\\\\) with a dynamic mask. Given \\\\(h_t\\\\), the output at the time step \\\\(t\\\\) only based on the previous data then consumes the previously emitted actions as additional inputs when predicting a new action.\"}"}
{"id": "W08IqLMlMer", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2: The detailed model structure for offline and online MADT.\\n\\nelements such as \\\\(\\\\langle\\\\text{global state, local observation}\\\\rangle\\\\), different from the single agent. It is reasonable for sequential modeling methods to model them in Markov Decision Process (MDP).\\n\\nTherefore, we formulate the trajectory as follows:\\n\\\\[\\n\\\\tau_i = (x_1, \\\\ldots, x_t, \\\\ldots, x_T)\\n\\\\]\\nwhere \\\\(x_t = (s_t, o_{it}, a_{it})\\\\) where \\\\(s_i\\\\) denotes the global shared state, \\\\(o_i\\\\) denotes the individual observation for agent \\\\(i\\\\) at time step \\\\(t\\\\), and \\\\(a_i\\\\) denotes the action. We take \\\\(x_t\\\\) as a token similar to the input of natural language processing.\\n\\nOutput Sequence Constructing.\\nTo bridge the gap between training with the whole context trajectory and testing with only previous data, we mask the context data to autoregressively output in the time step \\\\(t\\\\) with previous data in \\\\(\\\\langle 1 \\\\ldots t-1 \\\\rangle\\\\). Therefore, MADT predicts the sequential actions at each time step using the decoder as follows:\\n\\\\[\\ny_t = a_t = \\\\arg\\\\max_a p_\\\\theta(a | \\\\tau, a_1, \\\\ldots, a_{t-1})\\n\\\\]\\nwhere \\\\(\\\\theta\\\\) denotes the parameter of MADT and \\\\(\\\\tau\\\\) denotes the trajectory including the global state \\\\(s\\\\), local observation \\\\(o\\\\) before the time step \\\\(t\\\\), \\\\(p_\\\\theta\\\\) is the distribution over the legal action space under the available action \\\\(v\\\\).\\n\\nCore Module Description.\\nMADT differs from the transformers in conventional sequence modeling tasks that take inputs with position encoding and decode the encoded hidden representation autoregressively. We use the masking mechanism with a lower triangular matrix to compute the attention:\\n\\\\[\\n\\\\text{Attention}(Q, K, V) = \\\\text{softmax}(QK^T\\\\sqrt{d_k} + M)V\\n\\\\]\\nwhere \\\\(M\\\\) is the mask matrix which ensures the input at the time step \\\\(t\\\\) can only correlate with the input from \\\\(\\\\langle 1 \\\\ldots t-1 \\\\rangle\\\\). We employ the Cross-entropy (CE) as the total sequential prediction loss and utilize the available action \\\\(v\\\\) to ensure agents taking those illegal actions with probability zero. The CE loss can be represented as follows:\\n\\\\[\\nL_{CE}(\\\\theta) = \\\\frac{1}{C} \\\\sum_{t=1}^C P(a_t) \\\\log P(\\\\hat{a}_t | \\\\tau_t, \\\\hat{a}_{<t}; \\\\theta),\\n\\\\]\\nwhere \\\\(C\\\\) is the context length.\\n\\nwhere \\\\(a_t\\\\) is the ground truth action, \\\\(\\\\tau_t\\\\) includes \\\\{\\\\(s_1:t, o_1:t\\\\}\\\\). \\\\(\\\\hat{a}_t\\\\) denotes the output of MADT. The cross-entropy loss shown above aims to minimize the distribution distance between the prediction and the ground truth.\\n\\n3.2 Multi-Agent Decision Transformer with PPO\\nThe method above can fit the data distribution well resulting from the sequential modeling capacity of the transformer. But it fails to work well when pre-training on the offline datasets and improving continually by interacting with the online environment. The reason originates from the mismatch between the objective in MADT and ignore the value of each action. When the pre-trained model\"}"}
{"id": "W08IqLMlMer", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"is loaded to interact with the online environment, the buffer will only collect actions conforming to the distributions of the offline datasets rather than those corresponding to high reward at this state. That means the pre-trained policy is regarded as good once the selected action is identical to that in the dataset, even though it leads to a low reward. Therefore, we need to design another paradigm, MADT-PPO, to integrate RL and supervised learning for fine-tuning continually in Algorithm 2.\\n\\nFigure 2 shows the pre-training and fine-tuning framework. A direct method is to share the pre-trained model across each agent and implement the REINFORCE algorithm (Williams, 1992). However, only actors result in higher variance, and the employment of a critic to assess state values is necessary. Therefore, in online MARL, we leverage an extension of PPO, the state-of-the-art algorithm on tasks of StarCraft, MPE, and even return-based game Hanabi (Mordatch & Abbeel, 2018).\\n\\nAlgorithm 2: MADT-Online: Multi-Agent Decision Transformer with PPO\\n\\nInput: Offline dataloader $D$, Pretrained MADT Policy with parameter $\\\\theta$\\n\\nInitialize $\\\\theta$ and $\\\\phi$ are the parameters of an actor $\\\\pi_{\\\\theta}(a_i|o_i)$ and critic $V_{\\\\phi}(s)$ respectively, which could be inherited directly from pre-trained models.\\n\\nInitialize $n$ as the agent number, $\\\\gamma$ as the discount factor, and $\\\\epsilon$ as clip ratio for $\\\\tau = \\\\{\\\\tau_1, ..., \\\\tau_i, ..., \\\\tau_n\\\\}$ in $D$ do\\n\\nSample $\\\\tau_i = \\\\{s_t, o_t, a_t\\\\} t \\\\in 1:T$ as the ground truth, where $T$ is the context length\\n\\nCompute the advantage function $A(s,a_i) = \\\\sum_t \\\\gamma^t r(s,a_i) - V_{\\\\phi}(s)$\\n\\nComputing the important weight $w = \\\\pi_{\\\\theta}(o_i,a_i)/\\\\pi_{\\\\theta_{old}}(o_i,a_i)$\\n\\nUpdate $\\\\theta_i$ for $i \\\\in 1 ... n$ via:\\n\\n$\\\\theta_i = \\\\arg \\\\max E_{s \\\\sim \\\\rho, a \\\\sim \\\\pi_{\\\\theta_{old}}}[clip(w, 1-\\\\epsilon, 1+\\\\epsilon)A(s,a_i)]$\\n\\nCompute the MSE loss $L_{\\\\phi} = \\\\frac{1}{2} \\\\left[ \\\\sum_t \\\\gamma^t r - V_{\\\\phi}(s) \\\\right]^2$\\n\\nUpdate the critic network via $\\\\phi = \\\\arg \\\\min \\\\phi L_{\\\\phi}$\\n\\nend\\n\\nreturn $\\\\theta$, $\\\\phi$\\n\\n3.3 UNIVERSAL MODEL ACROSS SCENARIOS\\n\\nTo train a universal policy for each of the scenarios in the SMAC which might be vary in agent number, feature space, action space and reward ranges, we consider the modification list below.\\n\\nParameters Sharing Across Agents. When offline examples are collected from multiple tasks or the test phase owns the different agent numbers from the offline datasets, the difference of agent number across tasks is an intractable problem for deciding the number of actors. Thus, we consider sharing the parameters across all actors with one model as well as attaching one-hot agent IDs into observations, for compatibility with a variable number of agents.\\n\\nFeature Encoding. When the policy needs to generalize to new scenarios that arises from different feature shapes, we propose encoding all features into a universal space by padding zero at the end and mapping to a low-dimensional space with fully connected networks.\\n\\nAction Masking. Another issue is the different action space across scenarios. For example, less enemies in a scenario means less potential attack options as well as available actions. Therefore, an extra vector is utilized to mute the unavailable actions so that their probabilities are always zero during both learning and evaluating process.\\n\\nReward Scaling. Different scenarios might be vary in reward ranges and lead to unbalanced models during multi-task offline learning. To balance the influence of examples from different scenarios, we scale their rewards into the same range to ensure the output models have comparable performance between different tasks.\\n\\n4 EXPERIMENTS\\n\\nWe show three experimental settings: offline MARL, online MARL by loading the pre-trained model, and few-shot or zero-shot offline learning. For the offline MARL, we expect to verify the performance of our method in pre-training the policy and directly testing on the corresponding maps. For the fine-tuning in the online environment, we aim to demonstrate the capacity of the pre-trained policy on the original or new scenarios. Experimental results in offline MARL shows our MADT-offline...\"}"}
{"id": "W08IqLMlMer", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Performance of offline MADT comparing with baselines on four easy or (super-)hard SMAC maps. The dotted lines represent the mean values in the training set. Columns (a-d) are average returns from (poor, medium, good) datasets from top to the bottom.\\n\\n4.1 OFFLINE DATASETS\\nThe offline datasets are collected from the running policy, MAPPO (Yu et al., 2021), on the well-known SMAC task (Samvelyan et al., 2019). Each dataset contains a large number of trajectories:\\n\\n\\\\[ \\\\tau := (s_t, o_t, a_t, r_t, done_t, v_t)_{t=1}^T \\\\].\\n\\nDifferent from D4RL (Fu et al., 2020), our datasets consider the property of DecPOMDP, which owns local observations and available actions for each agent. In Appendix, we list the statistical properties of the offline datasets in Table 2 and Table 3.\\n\\n4.2 OFFLINE MULTITASK REINFORCEMENT LEARNING\\nIn this experiment, we aim to validate the effectiveness of MADT offline version in Section 3.1 as a framework for offline MARL on the static offline datasets. We train a policy on the offline datasets with various qualities and then apply it to an online environment, StarCraft (Samvelyan et al., 2019). There are also baselines under this setting, such as Behavior Cloning (BC) as a kind of imitation learning method showing stable performance on single-agent offline RL. Besides, we employ the conventional effective single-agent offline RL algorithms, BCQ (Fujimoto et al., 2019), CQL (Kumar et al., 2020), and ICQ (Yang et al., 2021), then add the mixing network for multi-agent setting, denoting as \u201cxx-MA\u201d. To verify the quality of our collected datasets, we choose data from different level and train the baselines as well as our MADT. Figure 3 shows the overall performance on various quality datasets. The baseline methods enhance their performance stably, indicating the quality of our offline datasets. The results also show that our MADT outperforms the offline MARL baselines and converges faster across easy, hard, and super hard maps (2s3z, 3s5z, 3s5z_vs_3s6z, corridor).\\n\\n4.3 OFFLINE PRE-TRAINING AND ONLINE FINE-TUNING\\nThe experiment designed in this subsection intends to answer the question: Is the pre-training process necessary for online MARL? Firstly we compare the online version of MADT in Section 3.2 with and without loading the pre-trained model. If training MADT only by online experience, we can view it as a transformer-based MAPPO replacing the actor and critic backbone networks with the\"}"}
{"id": "W08IqLMlMer", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Necessity of Pretrained Model.\\n\\nWe train our MADT based on the datasets collected from a map and fine-tune it on the same map online with the MAPPO algorithm. For comparison fairness, we use the transformer as both actor and critic networks with and without the pre-trained model. Primarily, we choose three maps from Easy, Hard, and Super Hard maps to validate the effectiveness of the pre-trained model in Figure 4. Experimental results show that the pre-trained model converges faster than the algorithm trained from scratch, especially in the challenging maps.\\n\\nImproving Sample Efficiency.\\n\\nFor validating the improvement of the sample efficiency via loading our pre-trained MADT and fine-tuning it with MAPPO, we compare the overall framework with the state-of-the-art algorithm, MAPPO (Yu et al., 2021), without the pre-training phase. We measure the number of interactions with the environment in Table 1, and our pre-trained model needs much less than the traditional MAPPO to access the same win rate.\\n\\n| Maps          | # Samples to Access the Win Rate |\\n|---------------|---------------------------------|\\n| 2m vs. 1z (Easy) | 8e4/- 1e5/- 1.3e5/- 1.5e5/- 1.6e5/- |\\n| 2s vs 1sc (Easy) | 1e4/- 2.5e4/- 3e4/- 8e4/4e4 |\\n| 3m (Easy) | 3.2e3/- 8.3e4/- 3.2e5/- 4e5/- 7.2e5/- |\\n| 2c vs. 64 zg (Hard) | 2e5/- 3e5/- 4e5/8e4 5e5/1e5 |\\n| 8m vs. 9m (Hard) | 3e5/- 6e5/- 1.4e6/2e4 2e6/8e4 |\\n| 2s vs. 1sc (Easy) | 1e4/- 2.5e4/- 3e4/- 8e4/4e4 3e5/1.2e5 |\\n| MMM vs. MMM (Super Hard) | 1e6/ 1.8e6/ 2.3e6/ 4e6/\u221e  |\\n| MMM vs. MMM (Easy) | 5.2e4/- 8e4/- 3e5/- 4.5e5/- 1.8e6/6e5 |\\n\\nTable 1: The sample numbers needed to access the win rate 20%, 40%, 60%, 80%, and 100% with (MAPPO/pre-trained MADT). \\\"-\\\" represents that no more samples is needed to reach the target win rate. \\\"\u221e\\\" represents that policies cannot reach the target win rate.\\n\\n4.4 GENERALIZATION WITH MULTITASK PRE-TRAINING\\n\\nExperiments in this section are to explore the transferability of the universal MADT mentioned in Section 3.3, which is pre-trained with mixed data from multiple tasks. According to whether the downstream tasks have been seen or not, the few-shot experiments are designed to validate the adaptability on seen tasks, while the zero-shot experiments are designed for the held-out maps.\\n\\nFew-shot Learning.\\n\\nThe results in Figure 5a shows that our method can utilize multi-task datasets to train a universal policy and generalize to all tasks well. Pre-trained MADT can achieve higher return than the model trained from scratch when we limit the interactions with environment.\\n\\nZero-shot Learning.\\n\\nFigure 5b shows that our universal MADT can surprisingly improve performance on downstream task even if it has not been seen before. (3 stalkers vs. 4 zealots).\"}"}
{"id": "W08IqLMlMer", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"The experiments in this subsection are designed to answer the following research questions:\\n\\nRQ1: Why we choose MAPPO for online phase?\\n\\nRQ2: Which kind of input shall we use to make the pre-trained model beneficial for the online MARL?\\n\\nRQ3: Why cannot the offline version of MADT be improved in the online fine-tune period after pre-training?\\n\\nSuitable Online Algorithm. Although the selection of MARL algorithm for online phase should be flexible according to specific task, we design experiments to answer RQ1 here. As discussed in Section 3, we can train Decision Transformer for each agent and online fine-tune it with a MARL algorithm. An intuitive method is to load the pre-trained transformer and take it as the policy network for fine-tuning with the policy gradient method, e.g. REINFORCE (Williams, 1992). However, for the reason of high variance mentioned in Section 3.2, we choose MAPPO as the online algorithm and compare their performance in improving sample efficiency during the online period in Figure 6a.\\n\\nDropping Reward-to-go in MADT. To answer RQ2, we compare different inputs embedded into the transformer, including the combination of state, reward-to-go, and action. We find the reward-to-go harmful to online fine-tuning performance, as shown in Figure 6b. We suppose the distribution of reward-to-go is mismatch between offline data and online samples. That is, rewards of online samples are usually lower than offline data due to stochastic exploration at the beginning of the online phase. It deteriorates the fine-tuning capability of the pre-trained model, and based on Figure 6b, we only choose states as our inputs for pre-training and fine-tuning.\\n\\nIntegrating Online MARL with MADT. To answer RQ3, we directly apply the offline version of MADT for pre-training and fine-tune it online. However, Figure 6c shows that it cannot be improved during the online phase. We analyze the results from the absence of motivation for chasing higher rewards, and conclude that offline MADT is in fact supervised learning and tends to fit its collected experience even with unsatisfactory reward.\"}"}
{"id": "W08IqLMlMer", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Deep Reinforcement Learning.\\n\\nConventionally, offline RL trains a policy based on the static and previously collected data without any environmental interaction (Levine et al., 2020). This policy is then leveraged to interact with the online environment to obtain promising results. A straightforward method for the current reinforcement learning is to use the off-policy algorithm and regard the offline datasets as a replay buffer to learn a policy with good performance. However, experience existing in offline datasets and interaction with online environment have different distributions which cause the overestimation in the off-policy (value-based) method (Kumar et al., 2019). Substantial works presented in offline RL aim at resolving the distribution shift between the static offline datasets and the online environment interaction (Kumar et al., 2020; Fujimoto et al., 2019; Kumar et al., 2019). Yang et al. (2021); Jiang & Lu (2021) constrain off-policy algorithms in offline MARL. Related to our work for the improvement of sample efficiency, Nair et al. (2020) load the pretrained model to generate a weight for actor-critic online RL algorithms. Recently, the Decision Transformer outperforms many state-of-the-art offline RL algorithms via regarding the training process as a sequential modeling phase and test on the online environment (Chen et al., 2021; Janner et al., 2021). In contrast, we show a transformer-based method in the multi-agent field, attempting to transfer across many scenarios without extra constraints.\\n\\nMulti-Agent Reinforcement Learning.\\n\\nAs a natural extension from single-agent RL, MARL attracts much attention to solve more complex problems under Markov Games (Zhang et al., 2021). Related algorithms conventionally enforce multiple agents to interact with the environment online and collect the experience to train the joint policy from scratch (Rashid et al., 2018; Sunehag et al., 2017; Son et al., 2019; Mahajan et al., 2019; Yu et al., 2021). The framework of traditional multi-agent learning employs the centralized training and decentralized execution framework due to local observation and increasing action space. We build offline datasets by collecting elements from the DecPOMDP and train the general policy with parameter sharing. We utilize transformer to learn a policy with previously collected data from the golden policy in advance and fine-tune it online.\\n\\nTransformer.\\n\\nTransformer (Vaswani et al., 2017) has achieved a great breakthrough to model relations between the input and output sequence with variable length, for the sequence-to-sequence problems (Sutskever et al., 2014), especially on machine translation (Wang et al., 2019) and speech recognition (Dong et al., 2018). Recent works even reorganize the vision problems as the sequential modeling process and construct the SOTA model with pretraining, named ViT (Han et al., 2020; Dosovitskiy et al., 2020; Liu et al., 2021). Due to the Markovian property of trajectories in offline datasets, we can utilize Transformer as that in language modeling. Therefore, Transformer can bridge the gap between supervised learning in the offline setting and reinforcement learning in the online interaction because of the representation capability. We claim that the components under Markov Games are sequential then utilize the transformer for each agent to fit a transferable MARL policy. Furthermore, we fine-tune the learned policy under via trial-and-error.\\n\\nConclusions\\n\\nIn this work, we propose MADT, an offline pre-trained model for MARL, which integrates the transformer to improve sample efficiency and generalizability. We build an offline MARL dataset based on SMAC. Our method outperforms the state-of-the-art methods in offline MARL, including BC, BCQ, CQL, and ICQ. Our whole framework can also drastically improve the sample efficiency of online MARL via loading pre-trained model. Furthermore, we apply our method to train a universal policy over the multi-task datasets and fine-tune it under the few-shot and zero-shot settings. Results demonstrate that this universal policy adapts fast to new tasks and promises performance on different level maps. To our best knowledge, we are the first to pre-train the multi-agent model with offline datasets and fine-tune it under the few-shot and zero-shot settings. Future research will pre-train a more general policy, which can transfer across more diverse tasks.\"}"}
{"id": "W08IqLMlMer", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ana LC Bazzan. Opportunities for multiagent systems and multiagent reinforcement learning in traffic control. *Autonomous Agents and Multi-Agent Systems*, 18(3):342, 2009.\\n\\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. *Journal of Artificial Intelligence Research*, 47:253\u2013279, 2013.\\n\\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. *arXiv preprint arXiv:2106.01345*, 2021.\\n\\nLinhao Dong, Shuang Xu, and Bo Xu. Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition. In *2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, pp. 5884\u20135888. IEEE, 2018.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In *International Conference on Learning Representations*, 2020.\\n\\nLasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In *International Conference on Machine Learning*, pp. 1407\u20131416. PMLR, 2018.\\n\\nLasse Espeholt, Rapha\u00ebl Marinier, Piotr Stanczyk, Ke Wang, and Marcin Michalski. Seed rl: Scalable and efficient deep-rl with accelerated central inference. In *International Conference on Learning Representations*, 2019.\\n\\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. *arXiv preprint arXiv:2004.07219*, 2020.\\n\\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In *International Conference on Machine Learning*, pp. 2052\u20132062. PMLR, 2019.\\n\\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. *arXiv preprint arXiv:1812.05905*, 2018.\\n\\nKai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. A survey on visual transformer. *arXiv preprint arXiv:2012.12556*, 2020.\\n\\nMichael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence modeling problem. *arXiv preprint arXiv:2106.02039*, 2021.\\n\\nJiechuan Jiang and Zongqing Lu. Offline decentralized multi-agent reinforcement learning. *arXiv preprint arXiv:2108.01832*, 2021.\\n\\nAviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. *Advances in Neural Information Processing Systems*, 32:11784\u201311794, 2019.\\n\\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. *arXiv preprint arXiv:2006.04779*, 2020.\\n\\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. *arXiv preprint arXiv:2005.01643*, 2020.\\n\\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. *arXiv preprint arXiv:2103.14030*, 2021.\"}"}
{"id": "W08IqLMlMer", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2022\\n\\nAnuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent variational exploration. Advances in Neural Information Processing Systems, 32:7613\u20137624, 2019.\\n\\nV olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.\\n\\nIgor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent populations. In Thirty-second AAAI conference on artificial intelligence, 2018.\\n\\nRemi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. Advances in Neural Information Processing Systems, 29:1054\u20131062, 2016.\\n\\nAshvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.\\n\\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4295\u20134304. PMLR, 2018.\\n\\nParia Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021, 2021.\\n\\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.\\n\\nShai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.\\n\\nKyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 5887\u20135896. PMLR, 2019.\\n\\nPeter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.\\n\\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104\u20133112, 2014.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\\n\\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao. Learning deep transformer models for machine translation. arXiv preprint arXiv:1906.01787, 2019.\\n\\nTonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang. Rode: Learning roles to decompose multi-agent tasks. arXiv preprint arXiv:2010.01523, 2020.\\n\\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229\u2013256, 1992.\\n\\nYikun Xian, Zuohui Fu, S Muthukrishnan, Gerard De Melo, and Yongfeng Zhang. Reinforcement knowledge graph reasoning for explainable recommendation. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 285\u2013294, 2019.\\n\\nYiqin Yang, Xiaoteng Ma, Chenghao Li, Zewu Zheng, Qiyuan Zhang, Gao Huang, Jun Yang, and Qianchuan Zhao. Believe what you see: Implicit constraint approach for offline multi-agent reinforcement learning. arXiv preprint arXiv:2106.03400, 2021.\"}"}
{"id": "W08IqLMlMer", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.\\n\\nKaiqing Zhang, Zhuoran Yang, and Tamer Ba\u015far. Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of Reinforcement Learning and Control, pp. 321\u2013384, 2021.\"}"}
{"id": "W08IqLMlMer", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"We list the properties of our offline datasets in Tables 2 and 3.\\n\\n| Dataset   | Difficulty | Maps    | Difficulty | Data Quality | # Samples | Reward Distribution (mean \u00b1 std) |\\n|-----------|------------|---------|------------|--------------|-----------|----------------------------------|\\n| 3m        | Easy       | 3m-poor | Poor       | 62528        | 6.29 \u00b1 2.17 |\\n|           |            | 3m-medium|            |              |           |                                  |\\n|           |            | 3m-good  |          | 1775159      | 19.99 \u00b1 0.18 |\\n| 8m        | Easy       | 8m-poor | Poor       | 157133       | 6.43 \u00b1 2.41 |\\n|           |            | 8m-medium|            |              |           |                                  |\\n|           |            | 8m-good  |          | 297439       | 11.95 \u00b1 0.94 |\\n| 2s3z      | Easy       | 2s3z-poor| Poor       | 147314       | 7.69 \u00b1 1.77 |\\n|           |            | 2s3z-medium|           |              |           |                                  |\\n|           |            | 2s3z-good|          | 441474       | 12.85 \u00b1 1.37 |\\n| 2s_vs_1sc | Easy       | 2s_vs_1sc-poor| Poor | 12887       | 6.62 \u00b1 2.74 |\\n|           |            | 2s_vs_1sc-medium|   |              |           |                                  |\\n|           |            | 2s_vs_1sc-good|    | 33232       | 11.70 \u00b1 0.73 |\\n| 3s_vs_4z  | Easy       | 3s_vs_4z-poor| Poor | 216499      | 7.58 \u00b1 1.45 |\\n|           |            | 3s_vs_4z-medium|       |              |           |                                  |\\n|           |            | 3s_vs_4z-good|         | 335580      | 12.13 \u00b1 1.38 |\\n| MMM       | Easy       | MMM-poor | Poor      | 326516       | 7.64 \u00b1 2.05 |\\n|           |            | MMM-medium|           |              |           |                                  |\\n|           |            | MMM-good  |          | 648115      | 12.23 \u00b1 1.37 |\\n| so_many_baneling | Easy | so_many_baneling-poor | Poor | 1542 | 9.08 \u00b1 0.66 |\\n|           |            | so_many_baneling-medium|       | 59659 | 13.31 \u00b1 1.14 |\\n|           |            | so_many_baneling-good|         | 1376861      | 19.46 \u00b1 1.29 |\\n| 3s_vs_3z  | Easy       | 3s_vs_3z-poor | Poor | 52807       | 8.10 \u00b1 1.37 |\\n|           |            | 3s_vs_3z-medium |       | 80948       | 11.87 \u00b1 1.19 |\\n|           |            | 3s_vs_3z-good |          | 149906      | 20.02 \u00b1 0.09 |\\n| 2m_vs_1z  | Easy       | 2m_vs_1z-poor | Poor | 25333       | 5.20 \u00b1 1.66 |\\n|           |            | 2m_vs_1z-medium |          | 300         | 11.00 \u00b1 0.91 |\\n|           |            | 2m_vs_1z-good |          | 120127      | 20.00 \u00b1 0.01 |\\n| bane_vs_bane | Easy | bane_vs_bane-poor | Poor | 63        | 1.59 \u00b1 3.56 |\\n|           |            | bane_vs_bane-medium |       | 3507        | 14.00 \u00b1 0.93 |\\n|           |            | bane_vs_bane-good |          | 458795      | 19.97 \u00b1 0.36 |\\n| 1c3s5z    | Easy       | 1c3s5z-poor | Poor | 52988       | 8.10 \u00b1 1.65 |\\n|           |            | 1c3s5z-medium |         | 180357      | 12.68 \u00b1 1.42 |\\n|           |            | 1c3s5z-good |          | 2400033     | 19.88 \u00b1 0.69 |\\n\\nTable 2: The properties for our offline dataset collected from the experience of multi-agent PPO on the easy maps of SMAC.\"}"}
{"id": "W08IqLMlMer", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Maps         | Difficulty | # Samples      | Reward Distribution (mean \u00b1 std) |\\n|--------------|------------|----------------|----------------------------------|\\n| 5m_vs_6m     | Hard       | 5m_vs_6m-poor  | 1324213                          |\\n|              |            |                | 8.53 (\u00b11.18)                     |\\n|              |            | 5m_vs_6m-medium| 657520                           |\\n|              |            |                | 11.03 (\u00b10.58)                    |\\n|              |            | 5m_vs_6m-good  | 503746                           |\\n|              |            |                | 20 (\u00b10.01)                       |\\n| 10m_vs_11m   | Hard       | 10m_vs_11m-poor| 140522                           |\\n|              |            |                | 7.64 (\u00b12.39)                     |\\n|              |            | 10m_vs_11m-medium| 916845                         |\\n|              |            |                | 12.72 (\u00b11.25)                    |\\n|              |            | 10m_vs_11m-good | 895609                         |\\n|              |            |                | 20 (\u00b10.01)                       |\\n| 2c_vs_64zg   | Hard       | 2c_vs_64zg-poor| 10830                            |\\n|              |            |                | 8.91 (\u00b11.01)                     |\\n|              |            | 2c_vs_64zg-medium| 97702                           |\\n|              |            |                | 13.05 (\u00b11.37)                    |\\n|              |            | 2c_vs_64zg-good | 2631121                        |\\n|              |            |                | 19.95 (\u00b11.24)                    |\\n| 8m_vs_9m     | Hard       | 8m_vs_9m-poor  | 184285                           |\\n|              |            |                | 8.18 (\u00b12.14)                     |\\n|              |            | 8m_vs_9m-medium| 743198                           |\\n|              |            |                | 12.19 (\u00b11.14)                    |\\n|              |            | 8m_vs_9m-good  | 911652                           |\\n|              |            |                | 20 (\u00b10.01)                       |\\n| 3s_vs_5z     | Hard       | 3s_vs_5z-poor  | 423780                           |\\n|              |            |                | 6.85 (\u00b12.00)                     |\\n|              |            | 3s_vs_5z-medium| 686570                           |\\n|              |            |                | 12.12 (\u00b11.39)                    |\\n|              |            | 3s_vs_5z-good  | 2604082                          |\\n|              |            |                | 20.89 (\u00b11.38)                    |\\n| 3s5z         | Hard       | 3s5z-poor      | 365389                           |\\n|              |            |                | 8.32 (\u00b11.44)                     |\\n|              |            | 3s5z-medium     | 2047601                         |\\n|              |            |                | 12.61 (\u00b11.32)                    |\\n|              |            | 3s5z-good       | 1448424                         |\\n|              |            |                | 18.45 (\u00b12.03)                    |\\n| 3s5z_vs_3s6z | Super Hard | 3s5z_vs_3s6z-poor| 594089                         |\\n|              |            |                | 7.92 (\u00b11.77)                     |\\n|              |            | 3s5z_vs_3s6z-medium| 2214201                       |\\n|              |            |                | 12.56 (\u00b11.37)                    |\\n|              |            | 3s5z_vs_3s6z-good | 1542571                        |\\n|              |            |                | 18.35 (\u00b12.04)                    |\\n| 27m_vs_30m   | Super Hard | 27m_vs_30m-poor| 102003                           |\\n|              |            |                | 7.18 (\u00b12.08)                     |\\n|              |            | 27m_vs_30m-medium| 456971                         |\\n|              |            |                | 13.19 (\u00b11.25)                    |\\n|              |            | 27m_vs_30m-good | 412941                          |\\n|              |            |                | 17.33 (\u00b11.97)                    |\\n| MMM2        | Super Hard | MMM2-poor      | 1017332                          |\\n|              |            |                | 7.87 (\u00b11.74)                     |\\n|              |            | MMM2-medium    | 1117508                          |\\n|              |            |                | 11.79 (\u00b11.28)                    |\\n|              |            | MMM2-good      | 541873                           |\\n|              |            |                | 18.64 (\u00b11.47)                    |\\n| corridor    | Super Hard | corridor-poor  | 362553                           |\\n|              |            |                | 4.91 (\u00b11.71)                     |\\n|              |            | corridor-medium| 439505                           |\\n|              |            |                | 13.00 (\u00b11.32)                    |\\n|              |            | corridor-good  | 3163243                          |\\n|              |            |                | 19.88 (\u00b10.99)                    |\\n\\nTable 3: The properties for our offline dataset collected from the experience of multi-agent PPO on the hard and super hard maps of SMAC.\"}"}
