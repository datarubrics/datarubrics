{"id": "KRLUvxh8uaX", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Despite the use of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode the compositional relationships between objects and attributes. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order information. ARO consists of Visual Genome Attribution, to test the understanding of objects\u2019 properties; Visual Genome Relation, to test for relational understanding; and COCO-Order & Flickr30k-Order, to test for order sensitivity in VLMs. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We present the settings in which state-of-the-art VLMs behave like bags-of-words\u2014i.e. when they have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity.\\n\\nVLMs are predominantly trained and evaluated on large scale datasets with rich compositional structure in the images and captions. Yet, training on these datasets has not been enough to address the lack of compositional understanding, and evaluating on these datasets has failed to surface this deficiency. To understand why these limitations emerge and are not represented in the standard tests, we zoom into the training and evaluation procedures. We demonstrate that it is possible to perform well on image-text retrieval over existing datasets without using the composition and order information. This further motivates the value of using ARO to benchmark VLMs. Given that contrastive pretraining optimizes for retrieval on large datasets with similar shortcuts, we hypothesize that this can explain why the models do not need to learn to represent compositional information. This finding suggests a natural solution: composition-aware hard negative mining. We show that a simple-to-implement modification of contrastive learning significantly improves the performance on tasks requiring an understanding of order and compositionality.\\n\\nINTRODUCTION\\n\\nVision and language models (VLMs) have demonstrated high performance on dozens of well-established benchmarks (Radford et al., 2021; Li et al., 2022; Singh et al., 2022; Alayrac et al., 2022; Wang et al., 2022a;b; Zhai et al., 2022). Yet it is unclear whether performance on these benchmarks indicates rich compositional understanding of either text or images. For example, does CLIP distinguish between \u201cthe horse is eating the grass\u201d and \u201cthe grass is eating the horse\u201d? Natural scenes are complex, composed of many objects and attributes, in relationships with one another. While there have been important efforts to test compositional representations of objects, attributes, and relations (Thrush et al., 2022), such efforts are based on small sets of hand-crafted examples, often combined with testing many other types of knowledge. This makes it hard to evaluate the role of relational and attributional knowledge in isolation and lacks the statistical power to quantify how well VLMs perform on granular subtypes of compositions. Here, we provide a large-scale test bed to evaluate VLMs\u2019 attribution, relation, and order understanding. Using the test bed we create, we find significant deficiencies: many models fail to perform beyond chance level at simple tasks requiring compositional understanding.\"}"}
{"id": "KRLUvxh8uaX", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Many VLMs are pretrained and tested on large datasets with complex scenes and detailed captions with rich compositional structure. Yet, training on these datasets has not been enough to address the lack of compositional understanding, and evaluating on these datasets has failed to surface this deficiency. In the recent literature, the dominant VLM training paradigm is image-text contrastive pretraining (Jia et al., 2021; Radford et al., 2021; Zhang et al., 2020) over these large pretraining datasets. Contrastive pretraining optimizes for the task of image-text retrieval, and naturally many VLMs are tested in the retrieval task. In this work, we provide an analysis of retrieval, as an evaluation and objective. We propose experiments to analyze how these models are evaluated and trained, to understand the underlying issues.\\n\\nOur main contributions are three-fold:\\n\\n1. Introducing the Attribution, Relation, and Order benchmark (ARO) for fine-grained evaluation of VLMs' relation, attribution, and order understanding.\\n   - We present four new tasks: Visual Genome Attributions and Visual Genome Relations, to test the understanding of objects' attributes and relations in complex natural scenes; and COCO Order and Flickr30k Order, to test the models' ability to identify the correct ordering of the words in a caption (Section 2).\\n   - Using these evaluations, we show that state-of-the-art VLMs fail to represent simple relations such as \u201cto the right of\u201d and \u201cbehind\u201d, fail to represent the attributive difference between \u201cthe black jacket and the blue sky\u201d versus \u201cthe blue jacket and the black sky\u201d, and fail to represent the difference between correct and permuted captions. We provide fine-grained insights into the types of attributions and relations that models most frequently fail to understand.\\n\\n2. A critique of retrieval and contrastive pretraining.\\n   - Given we find VLMs exhibit poor compositional understanding, why have these issues not surfaced in many previous evaluations? Existing retrieval datasets are equipped with complex scenes and detailed descriptions as captions, typically full of rich compositional structure. Intriguingly, the models can perform well on retrieval without having a good compositional understanding. Our experiments (Section 3) show that models can achieve a high performance on retrieval even when the order and composition cues are removed from captions or images. Hence, it is natural that models with compositional deficiencies can still perform well on the standard evaluations. This suggests that standard retrieval tasks are limited in their ability to assess compositional understanding of the model, further motivating the need for our comprehensive ARO benchmark. Since contrastive pretraining optimizes for retrieval, our findings also show that models can perform well on contrastive pretraining without learning compositional information. Given our results, we argue that not learning the compositional information is a valid shortcut strategy (Geirhos et al., 2020), and VLMs have little incentive to learn to encode compositionality during contrastive pretraining.\\n\\n3. Composition-aware hard negatives can go a long way.\\n   - We propose a simple fix: mining of composition-aware hard negatives (Section 4). First, we introduce hard negatives consisting of the nearest neighboring images into each batch, to force models to represent fine-grained differences between very similar scenes. Second, we introduce hard negative captions into each batch, consisting of the true captions with word order perturbed, to force models to distinguish between correct and incorrect order. Finally, we show that this simple finetuning modification provides significant improvements in model understanding of attributes and relations.\"}"}
{"id": "KRLUvxh8uaX", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: ARO (Attribution, Relation and Order) a benchmark to test composition and order understanding. We present four large-scale tasks to test the model's relational, attributive, and order understanding. These datasets probe the models' ability to pick the correct ordering of the constituents of a caption, e.g. by asking the model to pick between 'the horse is eating the grass' vs 'the grass is eating the horse'. Existing VLMs exhibit intriguing deficiencies at these simple tasks: several models remain at or below chance level. For example, BLIP chooses 'the grass is eating the horse', with 81% probability.\\n\\n2.1 NEW BENCHMARKS FOR ASSESSING RELATIONAL AND ATTRIBUTIVE UNDERSTANDING\\n\\nWe leverage Visual Genome (VG) (Krishna et al., 2017) \u2013 a large-scale dataset with over 100,000 images, annotated with objects, attributes, and relations \u2013 and the high-quality GQA annotations (Hudson & Manning, 2019) established in past work. Building upon these, we generate two novel datasets for probing relation and attribution understanding:\\n\\n\u2022 Visual Genome Relation. Given an image and a constituent relation of the form $X$ relation $Y$, we test whether the model can pick the correct order. Specifically, we probe models to pick between $X$ relation $Y$ and $Y$ relation $X$ with test cases from various relations, such as prepositional relations (e.g. \\\"the dog is behind the tree\\\" vs \\\"the tree is behind the dog\\\") and verbs (e.g. \\\"the horse is eating the grass\\\" vs \\\"the grass is eating the horse\\\").\\n\\n\u2022 Visual Genome Attribution. We test the ability to attribute properties to objects appropriately. For instance, we probe the model to pick between \\\"the crouched cat and the open door\\\" and \\\"the open cat and the crouched door\\\".\\n\\nWe extract images with 48 relations including sitting on, eating, inside, and below, with 23,937 test cases in total; and 117 unique attribute pairs including 'gray vs wood', 'open vs white', and 'small vs brown', with 28,748 test cases in total. The details of the dataset generation procedure are presented in Appendix A, along with the full list of relations with the count statistics. In brief, we mined Visual Genome for cases in which both of the constituent objects of the relations/attributes took a meaningfully large space in the image, and extracted the smallest bounding box containing both of the constituents, presenting this cropped image alongside the correct and swapped relation/attribute annotation. Each test case is thus made of an image (e.g., the image of a horse eating the grass) a correct caption (e.g., \\\"the horse is eating the grass\\\") and a grammatically correct, but swapped,\"}"}
{"id": "KRLUvxh8uaX", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Here, we report the result of fine-tuning CLIP with hard-negatives. Metrics reported: Accuracy for CIFAR10, CIFAR100, ImageNet; Recall@1 for Flickr30k and COCO text-to-image and image-to-text retrieval; Macro Accuracy for VG-Relations and VG-Attribution; Accuracy for Flickr30k and COCO pick-the-right-caption tasks.\\n\\n| Task                | CLIP | CLIP-FT | NegCLIP |\\n|---------------------|------|---------|---------|\\n| VG-Relation         | 0.95 | 0.96    | 0.86    |\\n| VG-Attribution      | 0.62 | 0.65    | 0.71    |\\n| Flickr30k-PRC       | 0.59 | 0.50    | 0.91    |\\n| COCO-PRC           | 0.46 | 0.36    | 0.86    |\\n| CIFAR10             | 0.95 | 0.95    | 0.94    |\\n| CIFAR100            | 0.80 | 0.80    | 0.79    |\\n| ImageNet            | 0.75 | 0.74    | 0.72    |\\n| Flickr30k Image R@1 | 0.59 | 0.67    | 0.67    |\\n| Flickr30k Text R@1  | 0.78 | 0.83    | 0.79    |\\n| COCO Image R@1      | 0.30 | 0.42    | 0.41    |\\n| COCO Text R@1       | 0.50 | 0.59    | 0.56    |\\n\\nC.1 Negative Mining\\n\\nStarting from the MSCOCO image dataset, we use spacy to swap the position of two elements of the caption. These elements can be either nouns, adjectives, adverbs, verb phrases and noun phrases (only if the noun phrase is composed by three tokens or more, to not overlap with noun swapping). For each caption we thus build a set of 5 possible negative captions (note that, a negative caption might have less then 5 negative captions if there are not enough elements to swap). If a caption has zero negative captions, we remove it from the dataset. During training, at each epoch for each caption we sample one of its negative captions as additional element of the batch.\\n\\nC.2 Negative Image Mining\\n\\nStarting from the MSCOCO image dataset, we compute the pairwise similarity between all the images. Then, for each image we collect the 3 most similar images. During training, at each epoch for each image we sample one of its negative images as additional element of the batch.\\n\\nC.3 Fine-tuning Details\\n\\nFor finetuning models, we build our code on https://github.com/mlfoundations/open_clip/. We finetune all models on the training split of the COCO dataset, and validate on the validation split. We finetune both CLIP-FT and NegCLIP for 5 epochs, with sweeping learning rates in \\\\{10^{-5}, 5 \\\\times 10^{-6}, 10^{-6}\\\\} and picking the models based on the retrieval performance in the COCO validation set. We use 50 steps of warmup and AdamW optimizer with a cosine-annealing learning rate schedule with \\\\(N = 1024\\\\) batch size using a single NVIDIA RTX 2080 Ti GPU.\\n\\nLimitations:\\n\\nWe note that we were not able to train an entire model from scratch due to computational resources. We expect future work further to verify the benefits of composition-aware negative mining in contrastive pretraining. Furthermore, note that Radford et al. (2021) use \\\\(N = 32,000\\\\) as the batch size, while we only used a single gpu with \\\\(N = 1024\\\\). Similarly, we expect to gain further improvements from larger batch sizes.\"}"}
{"id": "KRLUvxh8uaX", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Finetuning CLIP with targeted alternatives. We propose a straightforward extension of CLIP. For each image, we sample strong alternatives among the dataset using nearest neighbors, and we create targeted negative captions to enhance order sensitivity. This method improves CLIP in a suite of compositional tasks, while not substantially hurting performance in important downstream tasks. Text in blue: tasks we proposed. Text in green: standard downstream evaluations.\\n\\nMining targeted and cheap negatives can lead to substantial improvements in compositional tasks, without causing a loss of performance in existing downstream tasks. While contrastive learning provided substantial increases in representation learning, it is unclear if simply scaling the size of pretraining datasets will be as efficient as exploring algorithmic improvements, especially in learning compositional structure. We believe our results provide evidence that seeking such modifications can bring various increases in model capability. We do not suggest that the model presented here is the best possible model for encoding relations; rather, when the goal is to represent these bits of compositional information, our addition of targeted negatives should be viewed as a significant avenue for improving contrastive learning.\\n\\n5 RELATED WORK\\n\\nVisio-linguistic compositionality: Understanding what aspects of language and vision VLMs capture is the main objective of several recent papers. Frank et al. (2021) suggest that information sharing between the text and vision modalities is not balanced, as the representations from the text encoder are more influenced by the vision modality than the other way round. Parcalabescu et al. (2021b) show that VLMs have difficulties in counting objects in images. In terms of the evaluation part of our paper, Winoground (Thrush et al., 2022) presents the nearest neighbor to our work. Winoground is a carefully curated dataset of 400 test cases that aims to evaluate compositional and pragmatics language understanding of VLMs where VLMs perform around chance level over a set of image-text matching tasks. Diwan et al. (2022) suggest that Winoground has challenges beyond compositionality that requires complex reasoning with visually/textually difficult or unusual examples. Our main goal is to propose an isolated fine-grained evaluation of relation and attribution understanding with a large scale dataset; hence we believe our dataset complements Winoground. While Winoground proposes a smaller scale dataset with great breadth across types of model knowledge; we attempt to focus on a large scale dataset for in-depth testing for relation and attributive understanding.\\n\\nBogin et al. (2021) utilizes VG to test VQA systems for compositional generalization; in comparison here our purpose is to implement tasks that can probe any VLM. Closer in spirit to our VG - Relations dataset, Conwell & Ullman (2022) demonstrate the lack of competency of a text-to-image model, DALL-E, in generating images faithful to the relationships described in the textual prompt. Zerroug et al. (2022) design a benchmark to evaluate vision models\u2019 understanding of simple compositional attributes of abstract shapes. GQA (Hudson & Manning, 2019) is a visual question-answering dataset derived from Visual Genome (Krishna et al., 2017) to test visual question answering systems for scene understanding with compositional question answering. Other VQA datasets such as NVLR (Suhr et al., 2017; 2018) cover a broad range of linguistic phenomena including compositionality. VALSE (Parcalabescu et al., 2021a) is a dataset that tests whether VLMs can identify the correct\"}"}
{"id": "KRLUvxh8uaX", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"linguistic phenomenon appearing in an image. Recently, Saharia et al. (2022) released a set of prompts, collectively called DrawBench, as a benchmark for text-to-image models' ability to generate images faithful to the given challenging prompts, involving a set of compositional tests. Previous evaluations, while informative, were limited by small data size; Winoground has 400 examples, limiting the statistical strength of the fine-grained conclusions. Our ARO dataset (50,000 test cases) is two orders of magnitude larger, enabling us to characterize the performance for different types of compositions. Moreover, previous works did not quantify how using composition-aware hard negatives in contrastive learning could affect VLMs' performance for compositional tasks.\\n\\nOrder information in language and vision:\\n\\nSeveral works have highlighted the lack of word order sensitivity in large language models (Hessel & Schofield, 2021; O'Connor & Andreas, 2021; Pham et al., 2021; Sinha et al., 2021). Sinha et al. (2021) show that pre-training BERT with sentences with shuffled words, marginally affect the performance on downstream tasks. Pham et al. (2021) show that some of the tasks in GLUE (Wang et al., 2018) can be solved even when disregarding word order. Our analysis of image retrieval leans in a very similar direction, further suggesting the need for more careful benchmarks. O'Connor & Andreas (2021) show that for long-range contexts, models use content words and local co-occurrence statistics to make predictions. Ettinger (2020) uses a set of psycholinguistic tasks to evaluate the linguistic and contextual information used by BERT, showing BERT's insensitivity to elements like negation. On the vision side, Brendel & Bethge (2019) show that a bag-of-local-features model performs almost as well as their state-of-the-art counterparts. Closer to our experiments, the work of Tejankar et al. (2021) shows that training contrastive vision language models using only bag-of-words in place of the caption does not significantly hurt performance on zero-shot classification. Our work generalize these results, showcasing the general limits of vision language models when dealing with relations, attributes and shuffled captions.\\n\\nNegative mining and contrastive learning:\\n\\nUsing hard negatives has been successful in improving representation learning (Harwood et al., 2017; Wu et al., 2017; Ge, 2018). Furthermore, hard negatives have also been shown to improve contrastive learning (Kalantidis et al., 2020; Robinson et al., 2021) or used with a contrastive loss to improve ViTs (Qin et al., 2021). Our work differs in that we propose exploring hard negatives with contrastive learning, particularly in the context of vision-language models and compositional abilities. Note that Li et al. (2021) and Li et al. (2022) use negative mining by selecting pairs of items with high similarity. However, in light of our results, this strategy alone does not seem to be enough to effectively train the model to deal with relationships and word order.\\n\\nCONCLUSION\\n\\nIn this work, we evaluate the ability of VLMs to encode composition and order structure, introducing large-scale test beds to generate fine-grained and statistically strong insights. We show that models struggle with relation, attribution, and order understanding, and our datasets revealed various limitations of models. We show that models can achieve high performance on the task of cross-modal retrieval without needing to learn order and composition information. Given that contrastive pretraining optimizes models for the task of retrieval, we argue that this can explain why VLMs need not learn to encode order and compositional cues. Using these insights, we presented a simple modification to the training procedure, namely composition-aware hard negative mining. Through several evaluations, we demonstrate that by generating composition-aware hard negatives during model training, the compositional and order understanding of VLMs can be improved.\\n\\nOur work demonstrates the importance of the interaction between the pretraining objective and large datasets VLMs are trained on. In this work, we focused on finetuning of VLMs for demonstrating the use of the composition-aware negative mining. For future work, we are interested in further exploring composition-aware contrastive pretraining of VLMs. Given that VLMs are trained with large datasets with rich text corpora, the limited language understanding of these models are intriguing. While here we specifically focused on contrastive learning, in light of our findings, studying the interaction between different pretraining objectives and compositional understanding is an emerging future avenue. Our results further highlight the importance of rich evaluations of VLMs. We hope future VLMs will release results on these fine-grained evaluations in addition to standard tasks, and more fine-grained evaluations will be developed. Focused evaluation of state of the art models illuminates the strengths and deficiencies of these models, and is key to understanding in which contexts and for which goals these models can be used.\"}"}
{"id": "KRLUvxh8uaX", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACKNOWLEDGMENTS\\n\\nWe would like to thank Adarsh Jeewajee, Candace Ross, Duygu Yilmaz, Edward Chen, Kyle Swanson, Rishi Bommasani, Tristan Thrush, Tuomas Oikarinen, and Weixin Liang for their support and comments on the manuscript, and all members of the Zou Lab, Jurafsky Lab, and Guestrin Lab for helpful discussions. We thank the anonymous reviewers for their suggestions to improve the paper. This work was funded in part by the Hoffman\u2013Yee Research Grants Program and the Stanford Institute for Human-Centered Artificial Intelligence. P.K. is supported in part by the Open Philanthropy AI Fellowship. J.Z. is supported by NSF CAREER 1942926 and the Chan-Zuckerberg Biohub.\\n\\nETHICAL STATEMENT\\n\\nThere are substantial critiques of image datasets established in prior literature for lacking careful consideration of privacy and stereotypical representations of people (Peng et al., 2021; Birhane & Prabhu, 2021; Krishna et al., 2017), as well as critiques of the use of significant resources needed to train and evaluate models on such large datasets. We do not introduce any new images, so avoid introducing substantial new data concerns; yet, in order to facilitate comparison with prior work, the datasets in this paper are based on and use these standard, existing datasets, perpetuating their use. More broadly, the goals and downstream consequences of papers like ours have key ethical dimensions. A central goal of this paper is to challenge broad assertions of high performance and illuminate specific strengths and deficiencies of state of the art machine learning models, which can contribute to understanding and advocating for which contexts and which goals these models should and should not be used. Understanding strengths and deficiencies of vision and language models like CLIP in particular is increasingly important as these models have become core components of text-to-image generation models, which are now being used by millions of users to generate images, for personal, creative, or commercial purposes (OpenAI, 2022). Everyday users are frequently confronted with the compositional failings of these models. Moreover, early evidence suggests when these models fail to correctly represent compositional information (like attribution of properties to entities), they may default to stereotypes; e.g., Bianchi et al. (2022) notes a case in which DallE fails to represent the compositional information in the prompt \u201ca disabled woman leading a meeting\u201d, instead generating an image attributing the property \u201cdisabled\u201d to an audience member and the property of able-bodiedness to the meeting leader; this is in contrast to other more conventional attributes such as in \u201ca blonde woman leading a meeting\u201d correctly generating images of blonde leaders. Highlighting technical and social failings and their uneven distribution across people can assist in advocating to reform, avoid, or reject the use of these models, and this connects to a broader body of literature directly exposing many biases and stereotypes perpetuated by these models (Cho et al., 2022; Bansal et al., 2022; Wolfe et al., 2022). Crucially, beyond contributing to the more visible and emerging experience of everyday users, vision language models serve as a modern iteration of image-classification models. Accordingly, improving model capabilities is likely to contribute to institutions\u2019 more obscured, historical and ongoing use of extracted materials and labor for the design of machine-learning-based surveillance. This is emphasized, for example, by the use of models in prior work and this work to uncritically label humans in everyday situations (Raji & Fried, 2021; Broussard, 2018). It is a complex, ongoing responsibility for machine learning researchers like ourselves to work to understand and carefully consider the possible and actual use of our evaluation of these models and the models themselves.\\n\\nREPRODUCIBILITY STATEMENT\\n\\nThe code to reproduce all experiments, along with the code to generate the datasets and tasks we propose are released at https://github.com/mertyg/vision-language-models-are-bows Experiments on caption perturbation have been run with three different seeds to take into account the randomness of the permutation methods. All of the models that are used were obtained from the checkpoints released with the respective\"}"}
{"id": "KRLUvxh8uaX", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"particularly, we obtained the checkpoints released at BLIP repositories. CLIP, X-VLM, and Flava.\\n\\nExperiments with CLIP and NegCLIP have not been run multiple times due to the computational requirements. However, the experiments have been run using fixed seed, so they can be replicated by other researchers. In addition to this, our NegCLIP is implemented as a fork of the open clip project.\\n\\nThe code will be released with the same license.\\n\\nREFERENCES\\n\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.\\n\\nHritik Bansal, Da Yin, Masoud Monajatipoor, and Kai-Wei Chang. How well can Text-to-Image Generative Models understand Ethical Natural Language Interventions? ArXiv preprint, abs/2210.15230, 2022. URL https://arxiv.org/abs/2210.15230.\\n\\nFederico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng, Debora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, and Aylin Caliskan. Easily accessible text-to-image generation amplifies demographic stereotypes at large scale, 2022. URL https://arxiv.org/abs/2211.03759.\\n\\nAbeba Birhane and Vinay Uday Prabhu. Large image datasets: A pyrrhic win for computer vision? In 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1536\u20131546. IEEE, 2021.\\n\\nBen Bogin, Shivanshu Gupta, Matt Gardner, and Jonathan Berant. Covr: A test-bed for visually grounded compositional generalization with real images. arXiv preprint arXiv:2109.10613, 2021.\\n\\nWieland Brendel and Matthias Bethge. Approximating CNNs with bag-of-local-features models works surprisingly well on imagenet. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SkfMWhAqYQ.\\n\\nMeredith Broussard. Artificial unintelligence: How computers misunderstand the world. mit Press, 2018.\\n\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597\u20131607. PMLR, 2020.\\n\\nJaemin Cho, Abhaysinh Zala, and Mohit Bansal. DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers. ArXiv preprint, abs/2202.04053, 2022. URL https://arxiv.org/abs/2202.04053.\\n\\nColin Conwell and Tomer D Ullman. Testing relational understanding in text-guided image generation. arXiv preprint arXiv:2208.00005, 2022.\\n\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248\u2013255. Ieee, 2009.\\n\\nAnuj Diwan, Layne Berry, Eunsol Choi, David Harwath, and Kyle Mahowald. Why is winoground hard? investigating failures in visuolinguistic compositionality. arXiv preprint arXiv:2211.00768, 2022.\"}"}
{"id": "KRLUvxh8uaX", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"caption (e.g., \\\"the grass is eating the horse\\\"). For each of the test cases in these datasets, we quantify the performance of each model in identifying the correct caption from the two choices; chance level performance is 50%. Examples of these tests can be seen in Figure 1.\\n\\n2.2 NEWBENCHMARKS FOR ASSESSING ORDER SENSITIVITY\\n\\nWhereas Visual Genome Relation and Visual Genome Attribution assess the model's ability to understand order and compositionality related to attributes and relations, we also wish to discern whether this is connected to a broader inability to represent word order in general. With this motivation, we specifically want to test the order sensitivity of models. Do models broadly exhibit any preference towards the correct ordering of the words in a scene description, or are they indifferent towards any permutation, even for those that are unreasonable?\\n\\nWe propose an additional stress test, to test the models' ability to pick the correct ordering of the words within a caption. Given an image, we probe the models to pick between the correct ordering of a caption versus alternatives where the words are reordered in systematic ways. Given four systematic permutations of a given caption and the caption itself, can the model 'pick the right caption'? We augment existing retrieval datasets to derive COCO Order and Flickr30k Order (Lin et al., 2014; Young et al., 2014). To generate these datasets, we utilize four different perturbations of a caption, provided in Table 1. These largely follow prior work that evaluates language models (O'Connor & Andreas, 2021). We use Spacy (Honnibal & Montani, 2017) for part-of-speech tagging to perform the perturbations.\\n\\n2.3 EVALUATING VLMS ON ARO\\n\\nWe evaluate four state-of-the-art VLMs: CLIP (Radford et al., 2021), BLIP (Li et al., 2022), Flava (Singh et al., 2022), and X-VLM (Zeng et al., 2022b). More details on these models can be found in Appendix B.\\n\\nModels exhibit deficiencies in compositional understanding: In Figure 1, we present model performance on the Visual Genome Relation and Visual Genome Attribution evaluations. In relation tests, we observe that most models are near or below chance level, indicating severe deficiencies in relational understanding. In Appendix Table 2, we provide the performance on each relation separately. For instance, while BLIP is relatively accurate at understanding positional relations, its performance is generally near chance level for verbs, such as 'eating' or 'watching'; whereas CLIP generally performs at chance level on positional relations. Quantitatively, while BLIP obtains 66% macro accuracy for spatial relations, it obtains 56% accuracy in verbs. In contrast, CLIP achieves 56% in spatial relations and 61% in verbs. In attribution tests, although BLIP (88%) and XLVM (87%) perform remarkably well, CLIP (62%) is again close to chance level. While Flava is reasonably good at attribution (73%), its performance is below-chance for relations (25%). Overall, VLMs exhibit significant deficiencies in compositional understanding, particularly relational understanding. These deficiencies motivate our interest in probing whether these models are failing to represent compositional information in particular \u2014 e.g. failing to represent the order of relation constituents \u2014 or whether models are in fact failing to represent word order more broadly.\\n\\nTable 1: List of perturbations used in order sensitivity experiments.\\n\\n| Perturbation Type                  | Example                                                        |\\n|-----------------------------------|----------------------------------------------------------------|\\n| Original Caption                  | remarkable scene with a blue ball behind a green chair         |\\n| Shuffle nouns and adjectives      | green ball with a remarkable chair behind a blue scene          |\\n| Shuffle everything but nouns and adjectives | remarkable scene behind a blue ball with a green chair       |\\n| Shuffle trigrams                  | a green chair remarkable scene with a blue ball behind         |\\n| Shuffle words within each trigram | scene with remarkable a ball blue a green behind chair         |\\n\\nModels have little to no preference toward correctly formed sentences: In Figure 1, we present the COCO/Flickr30k order task and the performance of the tested VLMs. Given an image, the VLM must pick between the original caption and four alternative captions, augmented with the 4 perturbations listed in Table 1 respectively; thus, chance-level performance is 20%. Given the randomness over the permutations of captions, we repeat the experiment with 5 different seeds and\"}"}
{"id": "KRLUvxh8uaX", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Overall, models exhibit different levels of preference toward the correct ordering. For instance, while BLIP performs relatively well on the earlier tasks compared to CLIP, we observe here that its performance is much closer to chance level. Similarly, while Flava obtains a good performance with the Attribution task, its performance is below chance level for the COCO and Flickr30k Order task.\\n\\nConnection to prior evaluations with text-condition image generation:\\nWe observe that CLIP cannot identify the correct ordering of constituents in a relation. This is in line with prior observations showing that text-conditioned image generators that use CLIP as the text encoder struggle with generating images that are faithful to the relations in the descriptions (Conwell & Ullman, 2022). We hypothesize the problem may lie with CLIP's inability to encode order. Given that Imagen (Saharia et al., 2022) has better results on compositionality tests, we speculate that this can be because they use T5 (Raffel et al., 2020), a language model, as the text encoder. We believe our results suggest there is potential for language model priors in VLMs to contribute increased compositional understanding.\\n\\n3 WHY DO MODELS BEHAVE LIKE BAG-OF-WORDS? A CRITIQUE OF RETRIEVAL AND CONTRASTIVE PRETRAINING\\n\\nGiven that VLMs exhibit poor compositional and order understanding, why have these issues not surfaced in many previous evaluations? Most VLMs are consistently evaluated on image-to-text retrieval, and demonstrate high performance. In this section, we demonstrate why retrieval can be incomplete, both as an evaluation and as an objective. We first show that models can perform well in the text-image retrieval evaluations on existing large-scale datasets, without using order or composition information. It is thus natural that the issues of lack of compositional and order information have been masked by the high performance. Next, we discuss the connection between contrastive pretraining on large datasets and the task of retrieval and argue that models may not have an incentive to learn composition and order.\\n\\n3.1 LIMITATIONS OF RETRIEVAL AS AN EVALUATION\\n\\nEven though existing retrieval datasets are equipped with complex scenes and detailed descriptions, it is unclear how much complexity the models need to understand to perform well on this task. In particular, do the models have to use compositional information to perform well on a large-scale retrieval task? To understand what it does and does not take to perform well on retrieval, we propose evaluations with modified datasets. First, we propose augmentations to the existing datasets, where we remove the order and composition cues. Next, we evaluate models on these augmented versions of these datasets to understand whether it is possible to perform well without these cues.\\n\\nDatasets: We zoom into two of the standard cross-modal text-image retrieval datasets, namely COCO (Lin et al., 2014) and Flickr30k (Young et al., 2014). Following prior work (Li et al., 2022), we use Karpathy splits (Karpathy & Fei-Fei, 2015) for both of the datasets. We test the models on the test splits; where COCO contains 5k images, and Flickr30k contains 1k images. We report Recall@1 and Recall@5 for both datasets.\\n\\nExperimental Protocol: Our goal is to understand whether models need order information / compositional understanding to perform well on existing text-image retrieval datasets. We test our hypothesis on the augmented versions of the existing datasets. We propose two augmented setups (see Figure 2):\\n\\n1. Perturbing the order and composition information in the captions: To understand if models need the compositional information in captions to perform well, we aim to remove these, and test whether models can perform well without them. In this case, we take the permutations of the words in a given caption, using the strategies specified in Table 1. For instance, we take the COCO dataset, shuffle all of the words in all of the captions, and compute the retrieval performance over this caption-modified dataset. Given that all words are shuffled, the compositional structure within the captions is altered, e.g. \u201cthe grass is eating the horse\u201d becomes \u201ceating the grass horse the\u201d.\\n\\n2. Perturbing the order and composition information in the images: Similar to captions, to understand if models need the compositional information in the images to perform well, we aim to test the models in the absence of these. As it is harder to manipulate entities in an image, we resort to a more severe method and take the permutations over the patches of an image. For example, we could permute the patches of each image independently.\"}"}
{"id": "KRLUvxh8uaX", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"instance, we split images into 9 equally sized patches and take a permutation of those patches to form the new images. Qin et al. (2021) used a similar strategy to show the insensitivity of vision transformers to patch-based augmentations, in the context of image classification. Again, this should alter most compositional structures within an image; for instance, an object that is \\\"below\\\" another can move into an arbitrary point in an image. We then compute the retrieval performance with this image-modified dataset. We explore three such strategies: splitting the image into either 4 equally-sized rows, 4 equally-sized columns, or 9 equally-sized patches and then shuffling.\\n\\nFigure 2: Retrieval without access to order information.\\n\\nWe show that models can achieve substantially high performance on standard evaluations even when order information is removed. In particular, in datasets where the captions are augmented with order perturbations, models show marginal performance degradation. Models can achieve high performance even when order information is inaccessible. In Figure 2, we present the retrieval performance of existing models under different augmentation strategies, and a detailed table can be found in Appendix 3. For each perturbation, we augment the dataset and compute the retrieval performance 3 times with different random seeds, and report standard error bars. Notably, across all of the mentioned perturbation strategies, most models lose marginal performance in performing the retrieval task with the perturbed caption, or the perturbed image. These results demonstrate that it is possible to obtain high performance in a retrieval task without utilizing the compositional structure, even in the case of these arguably large-scale datasets.\\n\\n3.2 LIMITATIONS OF RETRIEVAL AND CONTRASTIVE PRETRAINING AS AN OBJECTIVE\\n\\nTo understand why these deficiencies emerge in the first place, we discuss the training procedure of VLMs. Most state-of-the-art VLMs, at their core, are trained with a contrastive loss (Radford et al., 2021; Chen et al., 2020; Zeng et al., 2022a; Li et al., 2021; 2022; Zhang et al., 2020) on a large pretraining dataset scraped from the web. We hypothesize that the lack of compositional understanding can be attributed to the way the models are trained. Here, we discuss the connection between contrastive pretraining to retrieval, to better understand the underlying phenomenon.\\n\\nQuoting from Radford et al. (2021): \\\"CLIP pre-trains for the task of image-text retrieval on our noisy web-scale dataset\\\". The goal in contrastive text-image pretraining is to optimize the model to identify the matching pairs of caption and text; namely retrieval. Simple as it is, training models with a retrieval objective, on large pretraining datasets, has demonstrated remarkable off-the-shelf performance, for instance in tasks requiring single object recognition. Although the existing large retrieval/pretraining datasets can have long and detailed captions, our results demonstrate that it is possible to perform well on these datasets without using compositional structure. Accordingly, these results provide evidence that models can achieve high performance in retrieval objectives, thus also obtaining low contrastive loss, without order information, unless the datasets are carefully designed. These datasets are designed to cover a large conceptual and semantic space, in order to make models broadly useful for downstream tasks; yet these datasets are not designed to contain many images with captions containing similar words that must be differentiated. Without such alternatives in the dataset, the task can be solved without taking order information into account \u2014 and behaving like a bag-of-words becomes a high-reward strategy. There is a large body of evidence showing that neural networks are prone to exploiting shortcut strategies (Geirhos et al., 2018; Hsu et al., 2018; Liu et al., 2020; Odena et al., 2017; van den Oord et al., 2018).\"}"}
{"id": "KRLUvxh8uaX", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We thus argue that it is unclear what should incentivize models trained with a contrastive loss to learn to pay attention to order structure unless the datasets or algorithms are carefully designed with this consideration.\\n\\n**SIMPLE FIX**\\n\\nOur analysis of the retrieval objective and datasets leads to a natural solution: hard negatives for contrastive learning (Robinson et al., 2021; Kalantidis et al., 2020). We propose a natural extension of CLIP\u2019s contrastive objective to alleviate some of the issues discovered in previous sections. To make CLIP sensitive to word order and better at capturing compositions, we use strong alternatives:\\n\\n1. **Generation of negative captions**: For each image-caption pair, we generate a negative caption by swapping different linguistic elements: noun phrases, nouns, adjectives, adverbs, verb phrases. For example, the caption \\\"The horse is eating the grass and the zebra is drinking the water\\\" either becomes \\\"The zebra is eating the grass and the horse is drinking the water\\\" (noun swapping) or \\\"The horse is drinking the grass and the zebra is eating the water\\\" (verb phrase swapping).\\n\\n2. **Sampling strong alternative images**: To generate images that are strong alternatives to the images in a batch, we first use CLIP to compute the pairwise similarity between all images in the dataset. During training, for each image in the batch, we sample one of the $K = 3$ nearest neighbors as the strong alternative image. The sampled alternative images (and the respective captions and negative captions) are added to the batch.\\n\\nExperimental protocol: Due to the computational cost of training CLIP from scratch, we focused on finetuning experiments. Specifically, we finetune the ViT-B/32 variant of CLIP on the COCO dataset with hard negatives (NegCLIP). As an ablation, we also perform finetuning on COCO, without the sampled hard negatives to disentangle the effect of finetuning. The details of finetuning, hyperparameter selection, and ablation are provided in Appendix C.\\n\\nEvaluation: We propose two main sets of evaluations. First, we evaluate models on the four order and composition-sensitive tasks, namely Visual Genome Relation, Visual Genome Attribution, COCO & Flickr30k Order. In addition to these, to ensure that the model is still comparable to the original CLIP, we perform evaluations on five downstream tasks: CIFAR10, 100 (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009) for image classification; and Flickr30k and COCO for retrieval.\\n\\nResults: In Figure 3, we provide a comparison of CLIP to NegCLIP with a radar plot for an overview; numerical values can be found in Appendix Table 6 with the additional ablation on a CLIP model fine-tuned on MSCOCO without negative samples. NegCLIP does not suffer in downstream tasks, and it improves the performance on VG-Relation from 63% to 81%, on VG-Attribution from 62% to 71%, on COCO Order from 46% to 86%, and on Flickr30k Order from 59% to 91%. Further, in VG-Relations, NegCLIP becomes the best model, in comparison to all other models, and in VG-Attribution it becomes comparable to X-VLM and BLIP.\\n\\nOverall, we observe that NegCLIP does not have a substantial loss in performance on the downstream tasks, yet provides substantial gains on the order-sensitive tasks. Our results highlight that...\"}"}
{"id": "KRLUvxh8uaX", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Allyson Ettinger. What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models. Transactions of the Association for Computational Linguistics, 8:34\u201348, 2020.\\n\\nStella Frank, Emanuele Bugliarello, and Desmond Elliott. Vision-and-language or vision-for-language? On cross-modal influence in multimodal transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021). Association for Computational Linguistics, nov 2021. URL https://arxiv.org/abs/2109.04448.\\n\\nWeifeng Ge. Deep metric learning with hierarchical triplet loss. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 269\u2013285, 2018.\\n\\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bygh9j09KX.\\n\\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665\u2013673, 2020.\\n\\nBen Harwood, Vijay Kumar BG, Gustavo Carneiro, Ian Reid, and Tom Drummond. Smart mining for deep metric learning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2821\u20132829, 2017.\\n\\nJack Hessel and Alexandra Schofield. How effective is bert without word ordering? implications for language understanding and data privacy. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 204\u2013211, 2021.\\n\\nMatthew Honnibal and Ines Montani. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear, 2017.\\n\\nDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6700\u20136709, 2019.\\n\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pp. 4904\u20134916. PMLR, 2021.\\n\\nYannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning. Advances in Neural Information Processing Systems, 33:21798\u201321809, 2020.\\n\\nAndrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3128\u20133137, 2015.\\n\\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32\u201373, 2017.\\n\\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\\n\\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 34:9694\u20139705, 2021.\"}"}
{"id": "KRLUvxh8uaX", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "KRLUvxh8uaX", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "KRLUvxh8uaX", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "KRLUvxh8uaX", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A PROPOSED DATASETS\\n\\nA.1 GENERATING THE DATASETS\\n\\nWe follow the below steps while generating the datasets:\\n\\n1. We go through the scene graphs annotated in GQA (Hudson & Manning, 2019).\\n\\n2. First, we identify candidate objects in all scenes. Namely, we require the objects to be large enough such that they are recognizable in an image. We enforce a heuristical criterion, where we discard all objects that have a width lower than $\\\\frac{1}{4}$ of the width of the entire image, or with a height lower than $\\\\frac{1}{4}$ of the height of the entire image.\\n\\n3. For VG-Relation, we identify all pairings of objects, where we make sure that the pair of objects are not from the same category (say, both are not simultaneously \u201cdogs\u201d). Similarly, for VG-Attribution, we identify all pairs of objects that are modified by at least 1 attribute, where both objects and attributes are different from each other.\\n\\n4. After identifying the pairs of objects, we extract the smallest bounding box containing both of the objects from the scene. This is to minimize the distraction in the rich scenes in Visual Genome.\\n\\n5. Finally, for each identified pairs of objects, we fill the preset templates. For relations, we fill the templates of \u201cthe [object 1] is [relation] [object 2]\u201d for the true caption, and \u201cthe [object 2] is [relation] [object 1]\u201d for the false caption. For attributes, we fill the templates of \u201cthe [attribute 1] [object 1] and the [attribute 2] [object 2]\u201d and \u201cthe [attribute 2] [object 1] and the [attribute 1] [object 2]\u201d.\\n\\n6. For Visual Genome Relations, we post-process the relations to remove symmetric relations; such as \u201cnear\u201d or \u201cnext to\u201d.\\n\\nOverall, this process results in a set of 23,937 test cases for relations, and 28,748 test cases for attributes.\"}"}
{"id": "KRLUvxh8uaX", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Fine-grained results in Visual Genome Relation dataset.\\n\\n| Models | CLIP | NegCLIP | CLIP-FT | XVLM | BLIP | Flava |\\n|--------|------|---------|---------|------|------|-------|\\n| # Samples | 269 | 75 | 574 | 209 | 10 | 588 |\\n\\n### Spatial Relationships\\n\\n| Verbs | Accuracy | CLIP | NegCLIP | CLIP-FT | XVLM | BLIP | Flava |\\n|-------|----------|------|---------|---------|------|------|-------|\\n| above | 0.48 | 0.60 | 0.54 | 0.80 | 0.64 | 0.55 |\\n| at | 0.59 | 0.93 | 0.71 | 0.72 | 0.49 | 0.15 |\\n| behind | 0.56 | 0.29 | 0.34 | 0.82 | 0.77 | 0.28 |\\n| below | 0.56 | 0.46 | 0.48 | 0.74 | 0.69 | 0.44 |\\n| beneath | 0.80 | 0.70 | 0.70 | 0.80 | 0.70 | 0.40 |\\n| in | 0.63 | 0.89 | 0.63 | 0.73 | 0.72 | 0.09 |\\n| in front of | 0.54 | 0.75 | 0.70 | 0.66 | 0.55 | 0.78 |\\n| inside | 0.50 | 0.91 | 0.67 | 0.69 | 0.72 | 0.12 |\\n| on | 0.52 | 0.86 | 0.58 | 0.86 | 0.76 | 0.12 |\\n| on top of | 0.43 | 0.75 | 0.58 | 0.85 | 0.79 | 0.19 |\\n| to the left of | 0.49 | 0.50 | 0.50 | 0.52 | 0.51 | 0.50 |\\n| to the right of | 0.49 | 0.50 | 0.50 | 0.52 | 0.49 | 0.51 |\\n| under | 0.64 | 0.43 | 0.54 | 0.86 | 0.73 | 0.27 |\\n\\n### Verbs\\n\\n| Verbs | Accuracy | CLIP | NegCLIP | CLIP-FT | XVLM | BLIP | Flava |\\n|-------|----------|------|---------|---------|------|------|-------|\\n| carrying | 0.33 | 0.83 | 0.75 | 0.75 | 0.67 | 0.08 |\\n| covered by | 0.47 | 0.36 | 0.36 | 0.61 | 0.58 | 0.56 |\\n| covered in | 0.79 | 0.50 | 0.50 | 0.14 | 0.29 | 0.14 |\\n| covered with | 0.56 | 0.56 | 0.50 | 0.56 | 0.50 | 0.19 |\\n| covering | 0.39 | 0.58 | 0.45 | 0.67 | 0.55 | 0.06 |\\n| cutting | 0.75 | 0.83 | 0.83 | 0.67 | 0.25 | 0.00 |\\n| eating | 0.57 | 1.00 | 0.67 | 0.62 | 0.52 | 0.00 |\\n| feeding | 0.90 | 0.80 | 0.80 | 0.60 | 0.30 | 0.20 |\\n| grazing on | 0.10 | 0.90 | 0.30 | 0.60 | 0.40 | 0.50 |\\n| hanging on | 0.79 | 1.00 | 0.93 | 0.93 | 0.79 | 0.00 |\\n| holding | 0.58 | 0.97 | 0.79 | 0.67 | 0.44 | 0.27 |\\n| leaning on | 0.67 | 1.00 | 1.00 | 0.75 | 0.58 | 0.08 |\\n| looking at | 0.84 | 1.00 | 0.68 | 0.68 | 0.55 | 0.26 |\\n| lying in | 0.47 | 1.00 | 0.60 | 0.87 | 0.67 | 0.00 |\\n| lying on | 0.60 | 0.88 | 0.50 | 0.93 | 0.75 | 0.17 |\\n| parked on | 0.67 | 0.86 | 0.38 | 0.76 | 0.86 | 0.00 |\\n| reflected in | 0.64 | 0.71 | 0.57 | 0.50 | 0.43 | 0.43 |\\n| resting on | 0.38 | 0.85 | 0.23 | 0.92 | 0.54 | 0.15 |\\n| riding | 0.71 | 0.98 | 0.78 | 0.82 | 0.41 | 0.02 |\\n| sitting at | 0.62 | 1.00 | 0.88 | 0.88 | 0.46 | 0.00 |\\n| sitting in | 0.57 | 0.96 | 0.78 | 0.87 | 0.83 | 0.30 |\\n| sitting on | 0.58 | 0.97 | 0.78 | 0.94 | 0.73 | 0.14 |\\n| sitting on top of | 0.50 | 0.90 | 0.80 | 1.00 | 0.80 | 0.10 |\\n| standing by | 0.67 | 0.92 | 0.67 | 0.83 | 0.67 | 0.67 |\\n| standing in | 0.73 | 0.98 | 0.69 | 0.69 | 0.49 | 0.05 |\\n| standing on | 0.60 | 1.00 | 0.63 | 0.83 | 0.73 | 0.06 |\\n| surrounded by | 0.64 | 0.71 | 0.64 | 0.71 | 0.64 | 0.79 |\\n| using | 0.84 | 1.00 | 1.00 | 0.68 | 0.58 | 0.00 |\\n| walking in | 0.70 | 1.00 | 0.70 | 0.60 | 0.50 | 0.00 |\\n| walking on | 0.79 | 1.00 | 0.79 | 0.84 | 0.42 | 0.05 |\\n| watching | 0.45 | 0.55 | 0.27 | 0.59 | 0.68 | 0.36 |\\n| wearing | 0.47 | 0.99 | 0.88 | 0.68 | 0.48 | 0.64 |\\n\\n1. CLIP (Radford et al., 2021): We use the 'ViT-B/32' variant of CLIP, released at https://github.com/openai/CLIP/.\"}"}
{"id": "KRLUvxh8uaX", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Performance in the text shuffled retrieval task. Performance is averaged over 3 seeds, and standard deviations are reported next to each mean.\\n\\n| Strategy       | Model       | Flickr30k Text Shuffle | COCO Text Shuffle | Visual Genome Text Shuffle |\\n|----------------|-------------|------------------------|-------------------|---------------------------|\\n| Shuffle all but nouns and adj. | BLIP        | 0.004 \u00b1 0.003          | 0.002 \u00b1 0.001     | 0.001 \u00b1 0.000             |\\n| Shuffle only nouns and adj. | BLIP        | 0.003 \u00b1 0.002          | 0.002 \u00b1 0.001     | 0.001 \u00b1 0.000             |\\n| No Shuffling   | BLIP        | 0.002 \u00b1 0.001          | 0.001 \u00b1 0.000     | 0.001 \u00b1 0.000             |\\n| Shuffle within Trigrams | BLIP        | 0.001 \u00b1 0.000          | 0.001 \u00b1 0.000     | 0.001 \u00b1 0.000             |\\n| Shuffle trigrams | BLIP        | 0.001 \u00b1 0.000          | 0.001 \u00b1 0.000     | 0.001 \u00b1 0.000             |\\n| Shuffle trigrams | VLM        | 0.000 \u00b1 0.000          | 0.000 \u00b1 0.000     | 0.000 \u00b1 0.000             |\\n| Shuffle all words | VLM        | 0.000 \u00b1 0.000          | 0.000 \u00b1 0.000     | 0.000 \u00b1 0.000             |\\n| Shuffle all but nouns and adj. | VLM        | 0.000 \u00b1 0.000          | 0.000 \u00b1 0.000     | 0.000 \u00b1 0.000             |\\n| Shuffle all words | CLIP       | 0.000 \u00b1 0.000          | 0.000 \u00b1 0.000     | 0.000 \u00b1 0.000             |\\n| Shuffle all but nouns and adj. | CLIP       | 0.000 \u00b1 0.000          | 0.000 \u00b1 0.000     | 0.000 \u00b1 0.000             |\\n| Shuffle all words | Flava      | 0.000 \u00b1 0.000          | 0.000 \u00b1 0.000     | 0.000 \u00b1 0.000             |\\n| Shuffle all but nouns and adj. | Flava      | 0.000 \u00b1 0.000          | 0.000 \u00b1 0.000     | 0.000 \u00b1 0.000             |\\n\\nPublished as a conference paper at ICLR 2023\\n\\nhttps://github.com/zengyan-97/X-VLM/\\nhttps://github.com/salesforce/BLIP\"}"}
{"id": "KRLUvxh8uaX", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: Performance in the Pick the Right Caption task. Performance is averaged over 3 seeds, and standard deviations are reported next to each mean.\\n\\n| Strategy      | Model       | Text Recall@1 | Text Recall@5 | Image Recall@1 | Image Recall@5 |\\n|---------------|-------------|---------------|---------------|----------------|---------------|\\n| Shuffle Rows (4) | XVLM        | 921 \u00b1 967     | 594 \u00b1 875     | 892 \u00b1 739      | 687 \u00b1 791     |\\n| Shuffle Rows (4) | Flava       | 550 \u00b1 664     | 404 \u00b1 559     | 699 \u00b1 753      | 602 \u00b1 784     |\\n| Shuffle Rows (4) | CLIP        | 393 \u00b1 493     | 256 \u00b1 311     | 748 \u00b1 629      | 990 \u00b1 362     |\\n| Shuffle Rows (4) | BLIP        | 849 \u00b1 941     | 553 \u00b1 463     | 129 \u00b1 369      | 531 \u00b1 369     |\\n| No Shuffling   | CLIP        | 238 \u00b1 252     | 388 \u00b1 493     | 763 \u00b1 473      | 835 \u00b1 920     |\\n| No Shuffling   | BLIP        | 982 \u00b1 999     | 546 \u00b1 664     | 948 \u00b1 948      | 974 \u00b1 974     |\\n| No Shuffling   | XVLM        | 971 \u00b1 990     | 546 \u00b1 664     | 948 \u00b1 948      | 974 \u00b1 974     |\\n| No Shuffling   | Flava       | 876 \u00b1 991     | 546 \u00b1 664     | 948 \u00b1 948      | 974 \u00b1 974     |\\n| No Shuffling   | CLIP        | 855 \u00b1 908     | 546 \u00b1 664     | 948 \u00b1 948      | 974 \u00b1 974     |\\n| No Shuffling   | BLIP        | 918 \u00b1 991     | 546 \u00b1 664     | 948 \u00b1 948      | 974 \u00b1 974     |\\n\\nPublished as a conference paper at ICLR 2023\"}"}
