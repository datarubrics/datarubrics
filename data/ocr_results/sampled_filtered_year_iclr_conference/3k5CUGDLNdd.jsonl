{"id": "3k5CUGDLNdd", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "3k5CUGDLNdd", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "3k5CUGDLNdd", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "3k5CUGDLNdd", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "3k5CUGDLNdd", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fig. S2 and Fig. S3 show bar plots that summarize the performance of the algorithms on the two tasks in the simulated and real environment. Before averaging results from different datasets, we normalized the algorithm performance by the dataset performance, i.e., reaching the average success rate or return of the dataset corresponds to a value of 1.\\n\\nWhile AWAC (and IQL on the real data) can exceed the behavior policy on average on the Push task, all algorithms fall short of matching the dataset performance on the challenging Lift datasets. While success rates on the Lift-Real datasets are particularly low, the returns indicate that CRR and IQL significantly outperform BC.\\n\\nSince the hyperparameter optimization was done on the Lift-Sim-Weak&Expert dataset, it has the biggest impact on the performance on the Lift-Sim datasets. IQL in particular improves considerably there. The increase in performance on the Lift-Real datasets is considerably smaller, however. This suggests that optimizing the hyperparameters offline RL algorithms in simulation may have limited benefits on real environments.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| BC | CRR | AWAC | CQL | IQL |\\n|----|-----|------|-----|-----|\\n| 0.00 | 0.25 | 0.50 | 0.75 | 1.00 |\\n\\n**Normalized return**\\n\\n| BC | CRR | AWAC | CQL | IQL |\\n|----|-----|------|-----|-----|\\n| 0.00 | 0.25 | 0.50 | 0.75 | 1.00 |\\n\\n**Normalized success**\\n\\n| BC | CRR | AWAC | CQL | IQL |\\n|----|-----|------|-----|-----|\\n| 0.00 | 0.25 | 0.50 | 0.75 | 1.00 |\\n\\nFigure S3: Normalized performance for the Lift task: Success rates at the end of episodes (or returns) were normalized to the dataset success rate (or mean return) and then averaged over all datasets corresponding to a task (treating simulated and real data separately).\"}"}
{"id": "3k5CUGDLNdd", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we provide learning curves for the offline RL algorithms on all datasets. Since the evaluation of all training checkpoints on the real robots is prohibitively expensive and time-consuming, we evaluate the checkpoints learned on the real data in the simulated environment. This gives an over-optimistic estimate of the learning performance on the real robots.\\n\\nFor the challenging Lift-Real-Weak&Expert task we performed a grid search for each algorithm (BC, CRR, AWAC, CQL, IQL) and selected the best hyperparameters. We present the grid search in Table S7, the corresponding optimal hyperparameters in Table S8, and the default hyperparameters in Table S9. We note that due to the poor performance of CQL, we expanded our grid search specifically for this algorithm (on the Push-Sim-Expert data) and selected the corresponding optimal hyperparameters as its default parameters (see Figure S16). Our newly performed gridsearch, as mentioned above, was unfortunately not leading to improvements for CQL. Similar difficulties are reported in Kumar et al. (2021) and are tackling via case distinction and appropriate regularization techniques. It would be interesting to test the preceding two algorithmic techniques on our datasets, once their implementation is integrated in the D3RLPY library (Seno & Imai, 2021).\\n\\nWe proceed by presenting the learning curves for the offline RL algorithms on the datasets: for the Lift task in section A.6.1 and for the Push task in section A.6.2. Shaded areas indicate the interval between the 0.25 and 0.75 quantiles.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"A.6.1 LIFT DATASETS\\n\\nOptimized Hyperparameters\\n\\nReal-Expert  Real-Half-Expert  Real-Weak&Expert  Real-Mixed\\n\\nDefault Hyperparameters\\n\\nReal-Expert  Real-Half-Expert  Real-Weak&Expert  Real-Mixed\\n\\nFigure S4: Training curves for the Lift-Real datasets: Offline RL algorithms are trained on the real datasets and are evaluated in the simulated environment. Success rates and returns for the optimized hyperparameters (in Table S8) and the default hyperparameters (in Table S9). The selected hyperparameters, by the grid-search procedure (in Table S7), optimize the algorithm's final average return on Sim-Weak&Expert. The black dashed line shows the dataset performance of Real-Expert and the red dashed line the performance of the Real-Weak&Expert (first 3 columns) and Real-Mixed (last column).\"}"}
{"id": "3k5CUGDLNdd", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning policies from previously recorded data is a promising direction for real-world robotics tasks, as online learning is often infeasible. Dexterous manipulation in particular remains an open problem in its general form. The combination of offline reinforcement learning with large diverse datasets, however, has the potential to lead to a breakthrough in this challenging domain analogously to the rapid progress made in supervised learning in recent years. To coordinate the efforts of the research community toward tackling this problem, we propose a benchmark including: i) a large collection of data for offline learning from a dexterous manipulation platform on two tasks, obtained with capable RL agents trained in simulation; ii) the option to execute learned policies on a real-world robotic system and a simulation for efficient debugging. We evaluate prominent open-sourced offline reinforcement learning algorithms on the datasets and provide a reproducible experimental setup for offline reinforcement learning on real systems. Visit https://sites.google.com/view/benchmarking-offline-rl-real for more details.\\n\\n1 Introduction\\n\\nReinforcement learning (RL) (Sutton et al., 1998) holds great potential for robotic manipulation and other real-world decision-making problems as it can solve tasks autonomously by learning from interactions with the environment. When data can be collected during learning, RL in combination with high-capacity function approximators can solve challenging high-dimensional problems (Mnih et al., 2015; Lillicrap et al., 2016; Silver et al., 2017; Berner et al., 2019). However, in many cases online learning is not feasible because collecting a large amount of experience with a partially trained policy is either prohibitively expensive or unsafe (Dulac-Arnold et al., 2020). Examples include autonomous driving, where suboptimal policies can lead to accidents, robotic applications where the hardware is likely to get damaged without additional safety mechanisms, and collaborative robotic scenarios where humans are at risk of being harmed.\\n\\nOffline reinforcement learning (offline RL or batch RL) (Lange et al., 2012) tackles this problem by learning a policy from prerecorded data generated by experts or handcrafted controllers respecting the system's constraints. Independently of how the data is collected, it is essential to make the best possible use of it and to design algorithms that improve performance with the increase of available data. This property has led to unexpected generalization in computer vision (Krizhevsky et al., 2012; He et al., 2016; Redmon et al., 2016) and natural language tasks (Floridi & Chiriatti, 2020; Devlin et al., 2018) when massive datasets are employed. With the motivation to learn similarly capable decision-making systems from data, the field of offline RL has gained considerable attention. Progress is currently measured by benchmarking algorithms on simulated domains, both in terms of data collection and evaluation.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: The TriFinger manipulation platform (W\u00fcthrich et al., 2021; Bauer et al., 2022).\\n\\nLeft: The robot has 3 arms with 3 DoF each. The cube is constrained by a bowl-shaped arena, allowing for unattended data collection. Right: A cluster of these robots for parallel data collection and evaluation.\\n\\nYet, real-world data differs from simulated data qualitatively and quantitatively in several aspects (Dulac-Arnold et al., 2020). First, observations are noisy and sometimes faulty. Second, real-world systems introduce delays in the sensor readings and often have different sampling rates for different modalities. Third, the action execution can also be delayed and can get quantized by low-level hardware constraints. Fourth, real-world environments are rarely stationary. For instance, in autonomous robotics, battery voltages might drop, leading to reduced motor torques for the same control command. Similarly, thermal effects change sensor readings and motor responses. Abrasion changes friction behavior and dust particles can change object appearances and sensing in general. Fifth, contacts are crucial for robotic manipulation but are only insufficiently modeled in current physics simulations, in particular for soft materials. Lastly, physical robots have individual variations.\\n\\nSince real-world data is different from simulated data, it is important to put offline RL algorithms to the test on real systems. We propose challenging robotic manipulation datasets recorded on real robots for two tasks: object pushing and object lifting with reorientation on the TriFinger platform (W\u00fcthrich et al., 2021). To study the differences between real and simulated environments, we also provide datasets collected in simulation. Our benchmark of state-of-the-art offline RL algorithms on these datasets reveals that they are able to solve the moderately difficult pushing task while their performance on the more challenging lifting task leaves room for improvement. In particular, there is a much larger gap between the performance of the expert policy and offline-learned policies on the real system compared to the simulated system. This underpins the importance of real-world benchmarks for offline RL. We furthermore study the impact of adding suboptimal trajectories to expert data and find that all algorithms are 'distracted' by them, i.e., their success rate drops significantly. This identifies an important open challenge for the offline RL community: robustness to suboptimal trajectories.\\n\\nImportantly, a cluster of TriFinger robots is set up for evaluation of offline-learned policies for which remote access can be requested for research purposes. With our dataset and evaluation platform, we therefore aim to provide a breeding ground for future offline RL algorithms.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Overview of our approach. Policies are trained with domain randomization in a parallel simulation using Isaac Gym (Makoviychuk et al., 2021) and then deployed in the PyBullet (Coumans & Bai, 2016) simulation and on the real system without fine-tuning to collect the datasets. We train state-of-the-art offline RL algorithms on these datasets and evaluate them on the respective system, i.e., the simulator or the real-robot cluster.\\n\\nTo abstract away the low-level details, we developed a software with a simple Gym (Brockman et al., 2016) interface in Python that can interact with the robots at a maximal rate of 1 kHz in position control or torque control mode (see (W\u00fcthrich et al., 2021) for details). We use a control frequency of 50 Hz and torque control for this work. We have a custom object tracking tool to provide position and orientation of a single colored cube in the environment, allowing algorithms to work without visual input.\\n\\nOn top of this interface, we have developed a submission system that allows users to submit jobs to a cluster of these robots for unattended remote execution. This setup was specifically adapted for ease of use in the offline RL setting and was extensively tested. We will provide researchers with access to the robot cluster, which will allow them to evaluate the policies they trained on the dataset proposed herein. To ease development and study the fundamental differences between simulated and real-world data, we provide a corresponding simulated environment using PyBullet (Coumans & Bai, 2016).\\n\\nTo summarize, the hardware and software have the following three key properties: i) physically capable of dexterous manipulation; ii) robust enough for running and evaluating learning methods, and iii) easy to use (robot hardware and simulator) and integrated in existing code frameworks.\\n\\n3. THE TRIFINGER DATASETS\\n\\nWe consider two tasks that involve a colored cube: pushing the cube to a target location and lifting the cube to a desired location and orientation. To create behavioral datasets for these tasks on the TriFinger platform, we need expert policies, which we obtain using reinforcement learning in a parallel simulation environment with domain randomization. Fig. 2 visualizes the entire procedure \u2013 from training to data collection to offline learning. In this section, we describe the tasks and the data collection while Sec. 4 is dedicated to benchmarking offline RL algorithms on the collected data.\\n\\n3.1 EXTERIOR MANIPULATION TASKS\\n\\nWe consider two tasks on the TriFinger platform that require dexterous manipulation of a cube:\\n\\n- **Push**: The goal is to move the cube to a target location. This task does not require the agent to align the orientation of the cube; the reward is based only on the desired and the achieved position.\\n\\n- **Lift**: The cube has to be picked up and moved to a target pose in the air which includes position and orientation. This requires flipping the cube on the ground, obtaining a stable grasp, lifting it to a target location and rotating it to match the target orientation.\\n\\nFollowing prior work (Hwangbo et al., 2019; Allshire et al., 2022) we define the reward by applying a logistic kernel $k(x) = (b + 2) \\\\left( \\\\exp(a \\\\|x\\\\|) + b \\\\exp(-a \\\\|x\\\\|) \\\\right) - 1$ to the difference between desired and achieved position (for the Push task) or the desired and achieved corner points of the cube (for the Lift task).\"}"}
{"id": "3k5CUGDLNdd", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nThe parameters $a$ and $b$ control the length scale over which the reward decays and how sensitive it is for small distances $x$, respectively. This yields a smooth, dense and bounded reward, see Appendix B.3 for details. We define success in the pushing task as reaching the goal position with a tolerance of 2 cm. For the lifting task we additionally require to not deviate more than 22 degrees from the goal orientation.\\n\\nWe note that pushing is more challenging than it may appear due to inelastic collisions between the soft fingertips and the cube which are not modeled in rigid body physics simulators. Furthermore, the performance of policies can be quite sensitive to the value of sliding friction. Lifting is, however, even more challenging as flipping the cube based on a noisy object-pose with a low refresh rate is error-prone, learning the right sequence of behaviors requires long-term credit assignment and dropping the cube often results in losing all progress in an episode.\\n\\n3.2 TRAINING EXPERT POLICIES\\n\\nWe train expert policies with online RL in simulation which we then use for data collection on the real system (Fig. 2). We build on prior work which achieved sim-to-real transfer for a dexterous manipulation task on the TriFinger platform (Allshire et al., 2022). This approach replicates the real system in a fast, GPU-accelerated rigid body physics simulator (Makoviychuk et al., 2021) and trains with an optimized implementation (Makoviichuk & Makoviychuk, 2022) of Proximal Policy Optimization (Schulman et al., 2017) with a high number of parallel actors. We furthermore adopt their choice of a control frequency of 50 Hz and use torque control to enable direct control of the fingers. The sensor input is the proprioceptive sensor information and the object pose. In order to obtain policies that are robust enough to work on the real system, domain randomization (Tobin et al., 2017; Mandlekar et al., 2017; Peng et al., 2018) is applied: for each episode, physics parameters like friction coefficients are sampled from a distribution that is likely to contain the parameters of the real environment. Additionally, noise is added to the observations and actions to account for sensor noise and the stochasticity of the real robot. Furthermore, random forces are applied to the cube to obtain a policy that is robust against perturbations, as in (Andrychowicz et al., 2020). The object and goal poses are represented by keypoints, i.e., the Cartesian coordinates of the corners of the cube. This choice was empirically shown to accelerate training compared to separately encoding position and orientation in Cartesian coordinates and a quaternion (Allshire et al., 2022).\\n\\nTo improve the robustness of the trained policies across the different robot instances and against other real-world effects like tracking errors due to accumulating dust, we modified the implementation of Allshire et al. (2022) in several ways. While the original code correctly models the 10 Hz refresh rate of the object pose estimate, we found it beneficial to also simulate the delay between the time when the camera images are captured and when they are provided to the agent. This delay typically ranges between 100 ms and 200 ms. We furthermore fully randomize the initial orientation of the cube and use a hand-crafted convex decomposition of the barrier collision mesh to avoid artifacts of the automatic decomposition, to which the policy can overfit. For the pushing task we penalized rapid changes in the cube position and orientation as sliding and flipping the cube transfers less well to the real system than moving it in a slow and controlled manner. We observed that policies trained with RL in simulation tend to output oscillatory actions, which does not transfer well to the real system and causes stronger wear effects (Mysore et al., 2021). To avoid this, we penalized changing the action which led to smoother movements and better performance. For the Lift task we additionally consider a policy which was trained with an Exponential Moving Average on the actions as we observed that vibrations on the real robot can lead to slipping and dropping (see Fig. S14 (b)). These vibrations might be caused by elastic deformations of the robot hardware and the complex contact dynamics between the soft fingertips and the cube that are not modeled in simulation. As also observed in Wang et al. (2022), the performance on the real system varies significantly with training seeds. We evaluated 20 seeds on the real system and used the best one for data collection. More details on training in simulation can be found in the Appendix C.1.\\n\\n3.3 DATA COLLECTION\\n\\nWe collected data for the pushing and lifting tasks both in a PyBullet (Coumans & Bai, 2016) simulation (Joshi et al., 2020) and on the real system, as shown in Fig. 2. To ensure that the data collection procedures are identical, we run the same code with the simulator backend and with the\"}"}
{"id": "3k5CUGDLNdd", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Published as a conference paper at ICLR 2023\\n\\nFigure 3: An example behavior of our expert policy on the lifting task. The robot is grasping and reorienting the object to reach the desired target location and orientation (transparent cube).\\n\\nThe robot is using a real-robot backend on six TriFinger platforms (Fig. 1). We observed that policies learned in simulation struggle with retrieving the cube from the barrier on the real robot. To alleviate this problem, we employ a predefined procedure to push the cube away from the barrier between episodes. The resulting starting positions together with the target positions are visualized in Fig. S12. During data collection, self-tests are performed at regular intervals to ensure that all robots are fully functional.\\n\\nFor the dataset to be useful to the community, it should be possible to evaluate offline-learned policies on the real robots. As machine learning models can be computationally demanding, we wait for a fixed time interval between receiving a new observation and starting to apply the action based on this observation. This time budget is allocated for running the policy without deviating from the control frequency used for data collection. We choose 10 ms for the Push task and 2 ms for the Lift task as we found training with bigger delays difficult. Note that our expert policies run in less than 1 ms.\\n\\nWe provide as much information about the system as possible in the observations. The robot state is captured by joint angles, angular velocities, recorded torques, fingertip forces, fingertip positions, fingertip velocities (both obtained via forward kinematics), and the ID of the robot. The object pose is represented by a position, a quaternion encoding orientation, the keypoints, the delay of the camera images for tracking and the tracking confidence. The observation additionally contains the last action, which is applied during the fixed delay between observation and action. The desired and achieved goal are also included in the observation and contain either a position for the Push task or keypoints for the Lift task. Some observations provide redundant information; however, we believe this simplifies working with the datasets, as we provide user-friendly Python code that implements filtering as well as automatically converting observations to a flat array. Moreover, we additionally publish a version of each dataset with camera images from three viewpoints. We believe that directly learning from these image datasets is an exciting challenge for the community and that they are moreover valuable in their own right as a large collection of robotic manipulation video footage.\\n\\nFor each task we consider pure expert data (Expert), mixed data recorded with a range of training checkpoints (Mixed), a combination of 50% expert trajectories and 50% trajectories recorded with a weaker policy with additive Gaussian noise on the actions (Weak&Expert) and the 50% expert data in Weak&Expert (Half-Expert). We run the same policies in simulation and on the real system. An exemplary expert behavior for the Lift task is shown in Fig. 3. Episodes last for 15 s for the Push task and 30 s for the Lift task as reorienting, grasping and lifting the cube requires more time. For the Push task, we collect 16 h of interaction for each dataset, corresponding to 3840 episodes and 2.8 million transitions. For the more demanding Lift task, we collect 20 h of robot interactions corresponding to 2400 episodes and 3.6 million transitions. The Half-Expert datasets contain the expert data that is included in the corresponding Weak&Expert datasets to isolate the effects of reducing the amount of expert data and adding suboptimal trajectories. The average success rates are provided next to the offline learning results in Table 1 and 2 (see data column). In Appendix B, we give a detailed analysis along with a specification (Table S4) and statistics (Table S5) of the offline RL datasets.\\n\\nWe benchmark offline RL algorithms on pairs of simulated and real datasets and study the impact of data quality on their performance. We limit our evaluation to the best algorithms provided in the benchmark.\\n\\n1 Possibly because the object tracking performance is slightly worse at the barrier or the dynamics of the outstretched fingers aligns less well between simulation and reality.\\n2 At the time of writing we cannot benchmark on these datasets since the robot cluster does not provide GPU-access at the moment. This will likely change in the near future.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"open-source library d3rlpy (Seno & Imai, 2021) to keep the required robot time for the evaluation of the trained policies manageable. Namely, we benchmark the following algorithms: BC (Bain & Sammut, 1995; Pomerleau, 1991; Ross et al., 2011; Torabi et al., 2018), CRR (Wang et al., 2020), AWAC (Nair et al., 2020), CQL (Kumar et al., 2020), and IQL (Kostrikov et al., 2021b). We report results for two sets of hyperparameters: the default values (except for CQL for which we performed a grid search on Push-Sim-Expert as the default parameters did not learn) and the results of a grid search on Lift-Sim-Weak&Expert (marked by \u2020). Details about the hyperparameters and their optimization are in Appendix C.3. We think that the performance at the default hyperparameters is highly relevant for offline RL on real data as optimizing the hyperparameters without a simulator is often infeasible.\\n\\n4.1 RESULTS\\n\\nWe train with five different seeds for each algorithm and evaluate with a fixed set of randomly sampled goals. Details about the policy evaluation on the simulated and real set-up can be found in Appendix D. We report success rates in the main text and returns in Appendix A.\\n\\nThe benchmarking results for the Push task are summarized in Table 1. Most of the offline RL algorithms perform well but the performance on the real data is generally worse. An exception to this pattern is CQL which performs poorly on the simulated Push task and gains some performance on the real data, perhaps due to the broader data distribution of the stochastic real-world environment. As expected BC performs well on the expert datasets but cannot exceed the dataset success rate on the Weak&Expert data, unlike CRR, AWAC, and IQL. On the expert data, CRR and AWAC match the performance of the expert policy in simulation but fall slightly behind on the real data. Interestingly, the performance of CRR is not negatively impacted by weak trajectories (e.g. 84% on Real-Weak&Expert compared to 78% on Real-Half-Expert, where the latter only contains the expert data portion). We provide insights into how the behavior policy compares to an offline-trained policy for the Push task in Fig. 6 (b) and (c).\\n\\nThe more challenging Lift task separates the algorithms more clearly as summarized in Table 2. CQL does not reach a non-zero success rate at all, despite our best efforts to optimize the hyperparameters (see Appendix C.3). This is in line with the results reported in Mandlekar et al. (2021) where CQL failed to learn on the datasets corresponding to more complex manipulation tasks. Kumar et al. (2021) furthermore discusses the sensitivity of CQL to the choice of hyperparameters, especially on robotic data. While the best algorithms come close to matching the performance of the expert on Lift-Sim-Expert, they fall short of reaching the dataset success rate on the real-robot data (Lift-Real-Expert).\"}"}
{"id": "3k5CUGDLNdd", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\n\\nTakuma Seno and Michita Imai. d3rlpy: An offline deep reinforcement learning library. CoRR, abs/2111.03788, 2021.\\n\\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354\u2013359, 2017.\\n\\nRichard S Sutton, Andrew G Barto, and Co-Director Autonomous Learning Laboratory Andrew G Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.\\n\\nJosh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pp. 23\u201330. IEEE, 2017. doi: 10.1109/IROS.2017.8202133.\\n\\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026\u20135033. IEEE, 2012.\\n\\nFaraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In J\u00e9r\u00f4me Lang (ed.), Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, pp. 4950\u20134957. ijcai.org, 2018.\\n\\nQiang Wang, Francisco Roldan Sanchez, Robert McCarthy, David Cordova Bulens, Kevin McGuinness, Noel O'Connor, Manuel W\u00fcthrich, Felix Widmaier, Stefan Bauer, and Stephen J. Redmond. Dexterous robotic manipulation using deep reinforcement learning and knowledge transfer for complex sparse reward-based tasks. Expert Systems, n/a(n/a):e13205, 2022.\\n\\nZiyu Wang, Alexander Novikov, Konrad Zolna, Josh Merel, Jost Tobias Springenberg, Scott E. Reed, Bobak Shahriari, Noah Y. Siegel, \u00c7aglar G\u00fcl\u00e7ehre, Nicolas Heess, and Nando de Freitas. Critic regularized regression. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\\n\\nManuel W\u00fcthrich, Felix Widmaier, Felix Grimminger, Shruti Joshi, Vaibhav Agrawal, Bilal Hammoud, Majid Khadiv, Miroslav Bogdanovic, Vincent Berenz, Julian Viereck, Maximilien Naveau, Ludovic Righetti, Bernhard Sch\u00f6lkopf, and Stefan Bauer. Trifinger: An open-source robot for learning dexterity. In Proceedings of the 2020 Conference on Robot Learning (CoRL), volume 155, pp. 1871\u20131882. PMLR, 2021.\\n\\nHaoran Xu, Xianyuan Zhan, Jianxiong Li, and Honglei Yin. Offline reinforcement learning with soft behavior regularization. CoRR, abs/2110.07395, 2021.\\n\\nBrian Yang, Jesse Zhang, Vitchyr Pong, Sergey Levine, and Dinesh Jayaraman. Replab: A reproducible low-cost arm benchmark platform for robotic learning. arXiv preprint arXiv:1905.07447, 2019.\\n\\nTianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. COMBO: conservative offline model-based policy optimization. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 28954\u201328967, 2021.\\n\\nChi Zhang, Sanmukh R. Kuppannagari, and Viktor K. Prasanna. BRAC+: improved behavior regularized actor critic for offline reinforcement learning. In Vineeth N. Balasubramanian and Ivor W. Tsang (eds.), Asian Conference on Machine Learning, ACML 2021, 17-19 November 2021, Virtual Event, volume 157 of Proceedings of Machine Learning Research, pp. 204\u2013219. PMLR, 2021.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "3k5CUGDLNdd", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Returns for the Push Task\\n\\nWe report the returns achieved on the Push task in Table S1. Note that for the simulated datasets the best performing algorithm in terms of return is not necessarily the same as the best performing one in terms of success rates (see Table 1). For instance, AWAC has the highest average return on Sim-Expert while CRR achieves a higher success rate. This has two reasons. First, the success is measured only at the final step of the episode, while the return is computed as the cumulative reward over time. Second, the return is dense as it is computed from the distance between cube and target, while the success rate is a sparse signal.\\n\\nTable S1: Push: Returns on the TriFinger-Push Datasets. \u2018data\u2019 denotes the mean over the dataset. Average and standard deviation over five training seeds.\\n\\n| Push-Datasets       | data | BC   | CRR | AWAC | CQL | IQL |\\n|---------------------|------|------|-----|------|-----|-----|\\n| Sim-Expert          | 674  | 585  | 636 | 657  | 184 | 631 |\\n| Sim-Half-Expert     | 674  | 535  | 576 | 586  | 226 | 565 |\\n| Sim-Weak&Expert     | 512  | 460  | 613 | 603  | 311 | 543 |\\n| Sim-Mixed           | 583  | 460  | 205 | 636  | 138 | 588 |\\n| Real-Expert         | 660  | 563  | 638 | 624  | 514 | 592 |\\n| Real-Half-Expert    | 660  | 546  | 627 | 587  | 471 | 573 |\\n| Real-Weak&Expert    | 429  | 387  | 622 | 568  | 346 | 555 |\\n| Real-Mixed          | 419  | 335  | 373 | 569  | 206 | 600 |\\n\\nA.2 Returns for the Lift Task\\n\\nIn this section, we report the returns achieved on the Lift task (Table S2). Interestingly, although IQL achieves a significantly higher return on the Real-Weak&Expert dataset than CRR, its final success rate is lower. An analysis of the rollout statistics reveals that IQL often moves the cube close to the goal pose but not close enough to satisfy the success criteria (defined in section 3). CRR, on the other hand, on average deviates further from the goal pose than IQL but has a bigger fraction of rollouts in which it satisfies the success criteria. In summary IQL does better on average on this dataset but lacks precision in matching the goal pose.\\n\\nA.3 Evaluation on the Holdout Robot\\n\\nTo quantify how well the policies obtained with offline RL generalize to unseen hardware, we evaluate them on a holdout robot which was not used for data collection. Table S3 shows the results for TriFinger-Push-Real-Expert and TriFinger-Lift-Real-Expert. The performance of the algorithms on the hold-out robot is within the performance margin of the other robots, suggesting that there is no significant difference between the different robots.\\n\\nA.4 Impact of Noise on Performance\\n\\nAs mentioned in section 4.1, we studied the impact of noise on the performance of the expert and the considered offline RL algorithms by recording a sequence of datasets with increasing noise scale in simulation. A relative noise scale of 1 corresponds to the variance measured on the actions and observations of the real system. Fig. S1 shows the success rate and return as a function of the noise scale up to a value of 2. As the performance of the expert and the offline RL policies degrades only slowly when increasing noise, we conclude that the stochasticity of the real system cannot be the only reason for the performance gap between the simulated and the real system. As delays in\"}"}
{"id": "3k5CUGDLNdd", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table S2: Lift: Returns on the TriFinger-Lift Datasets. \u2018data\u2019 denotes the mean over the dataset. Average and standard deviation over five training seeds.\\n\\n| Dataset            | BC   | CRR  | AW   | AC   | CQL  | IQL  |\\n|--------------------|------|------|------|------|------|------|\\n| Sim-Expert         | 1334 | 1129 | \u00b156  | 1246 | \u00b110  | 1280 |\\n| Sim-Half-Expert    | 1337 | 1112 | \u00b139  | 1231 | \u00b19   | 1211 |\\n| Sim-Weak&Expert    | 1133 | 791  | \u00b143  | 727  | \u00b144  | 1103 |\\n| Sim-Mixed          | 1173 | 409  | \u00b19   | 604  | \u00b179  | 931  |\\n| Real-Smooth-Expert | 1206 | 915  | \u00b136  | 1059 | \u00b154  | 1031 |\\n| Real-Expert        | 1064 | 711  | \u00b192  | 1014 | \u00b162  | 820  |\\n| Real-Half-Expert   | 1064 | 553  | \u00b177  | 837  | \u00b158  | 613  |\\n| Real-Weak&Expert   | 851  | 346  | \u00b121  | 633  | \u00b159  | 397  |\\n| Real-Mixed         | 862  | 272  | \u00b156  | 631  | \u00b162  | 346  |\\n\\nTable S3: Evaluation on hold-out robot. Success rate on the Real-Expert datasets.\\n\\n| Dataset          | BC   | CRR  | AW   | AC   | CQL  | IQL  |\\n|------------------|------|------|------|------|------|------|\\n| Push-Real-Expert | 0.80 | \u00b10.04| 0.91 | \u00b10.08| 0.84 | \u00b10.06|\\n| Lift-Real-Expert | 0.29 | \u00b10.04| 0.64 | \u00b10.05| 0.31 | \u00b10.08|\\n\\nThe observation and action execution are already implemented in the simulated environment, we hypothesize that other factors like more complex contact dynamics and elastic deformations of the fingertips and robot limbs are likely causing the larger performance gap between data and learned policies on the real robots.\\n\\nAWAC and CRR perform consistently over a wide range of noise scales with a slight decrease in performance for high relative noise scales (probably due to a large variance of the estimated object pose). BC and IQL seem to struggle with the narrow data distribution generated by a deterministic environment but improve with increasing stochasticity. While the performance of BC drops significantly again for large noise scales, IQL becomes competitive in this regime.\\n\\nFigure S1: Success rate and return for simulated lifting for varying relative environment noise scales (1.0 corresponds to the noise level of the real system) when the algorithms were trained on data recorded with that noise scale using the expert policy. The dashed lines indicate the performance on the real system.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table S4: Overview of the TriFinger offline RL datasets.\\n\\n| Dataset Type    | Overall Duration [h] | Episodes | Transitions | Episode Length [s] |\\n|-----------------|-----------------------|----------|-------------|-------------------|\\n| Push            | 16                    | 3840     | 2.8         | 15                |\\n| Sim-Expert      | 8                     | 1920     | 1.4         | 15                |\\n| Sim-Half-Expert | 16                    | 3840     | 2.8         | 15                |\\n| Sim-Weak&Expert | 16                    | 3840     | 2.8         | 15                |\\n| Sim-Mixed       | 16                    | 3840     | 2.8         | 15                |\\n| Real-Expert     | 16                    | 3840     | 2.8         | 15                |\\n| Real-Half-Expert| 8                     | 1920     | 1.4         | 15                |\\n| Real-Weak&Expert| 16                    | 3840     | 2.8         | 15                |\\n| Real-Mixed      | 16                    | 3840     | 2.8         | 15                |\\n\\nLift\\n\\n| Dataset Type   | Overall Duration [h] | Episodes | Transitions | Episode Length [s] |\\n|----------------|-----------------------|----------|-------------|-------------------|\\n| Sim-Expert     | 20                    | 2400     | 3.6         | 30                |\\n| Sim-Half-Expert| 10                    | 1200     | 1.8         | 30                |\\n| Sim-Weak&Expert| 20                    | 2400     | 3.6         | 30                |\\n| Sim-Mixed      | 20                    | 2400     | 3.6         | 30                |\\n| Real-Smooth-Expert | 20                | 2400     | 3.6         | 30                |\\n| Real-Expert    | 20                    | 2400     | 3.6         | 30                |\\n| Real-Half-Expert| 10                   | 1200     | 1.8         | 30                |\\n| Real-Weak&Expert| 20                    | 2400     | 3.6         | 30                |\\n| Real-Mixed     | 20                    | 2400     | 3.6         | 30                |\\n\\n\\\\[\\n\\\\text{Last for a finite number of time steps } H, \\\\text{ and the RL objective is to maximize the discounted return (or cumulative reward) } J = H - 1 \\\\sum_{t=0}^{T} \\\\gamma^t r_t, \\\\tag{S2}\\n\\\\]\\n\\nwhere \\\\( \\\\gamma \\\\) denotes the discounting factor and \\\\( r_t \\\\) the reward in step \\\\( t \\\\). A Markov state therefore has to keep track of how much time remains until the episode ends. The observation, on the other hand, does not contain this information. This omission is a deliberate choice as we want to obtain a policy which is independent of the time step. This makes it impossible, however, to accurately estimate the value based on the observation if \\\\( \\\\gamma \\\\) is too large. Intuitively, without access to the time step \\\\( t \\\\), the agent cannot know whether a cube far away from the goal corresponds to a large expected return to go because it is the beginning of an episode or a small expected return to go because there is no time left to move the cube and accumulate reward. See Pardo et al. (2018) for a more detailed discussion of issues arising from training with a finite horizon.\\n\\nA practical solution to this problem is to not set the terminals and choose a gamma which is appropriate for the time scale of the task (for offline learning we chose \\\\( \\\\gamma = 0.99 \\\\)). This choice hides the episodic nature of the task from the agent which results in good performance while avoiding a dependence of the policy on time. It may, however, sacrifice optimality in some corner cases like dropping the cube close to the end of the episode when there is not enough time to flip the cube over before lifting it again. Nevertheless, as we provide a flag to set the terminals at the episode ends and as it is straightforward to augment the observation with the intra-episode time step, the datasets are also suitable for experiments with time-dependent policies.\\n\\nB.4 Dataset Analysis\\n\\nOverview: We provide an overview of the various dataset types and their properties in Table S4.\\n\\nStatistics: Table S5 summarizes the statistics of the TriFinger datasets. While the success rate and the mean return constitute a limit to what can be achieved with pure imitation learning, the high numbers for the transient success rate (fraction of episodes in which the goal was achieved at least in one time step but not necessarily at the end of the episode) indicate that offline RL can potentially outperform the behavior policy significantly on these datasets.\\n\\nImpact of variations between robots: Fig. S10 compares the success rates of the expert policies on the individual robots used for data collection. While the performance differences between the robot instances are significant, the expert policies perform reasonably well on all of them, achieving at least 80% success rate on the Push task and 60% on the Lift task on all robots.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table S5: Statistics of the proposed TriFinger offline RL datasets: TriFinger-Push and TriFinger-Lift. Definitions of the success rates and the reward can be found in section B.2 and section B.3, respectively.\\n\\n| Dataset                  | TriFinger-Push | Sim-Expert | Sim-Half-Expert | Sim-Weak&Expert | Sim-Mixed | Real-Expert | Real-Half-Expert | Real-Weak&Expert | Real-Mixed |\\n|-------------------------|----------------|------------|-----------------|----------------|-----------|-------------|------------------|-----------------|------------|\\n|                         | mean | momentary | success rate | mean | momentary | success rate | mean | momentary | success rate | mean | momentary | success rate | 674   | 667   | 512   | 583   | 660   | 660   | 429   | 419   |\\n| Real-Smooth-Expert      | 0.64 | 0.53 | 0.82 | 1206 |\\n| Real-Expert             | 0.67 | 0.52 | 0.87 | 1064 |\\n| Real-Half-Expert        | 0.68 | 0.52 | 0.86 | 1064 |\\n| Real-Weak&Expert        | 0.40 | 0.30 | 0.66 | 851  |\\n| Real-Mixed              | 0.42 | 0.32 | 0.65 | 862  |\\n\\nFigure S10: Success rates on the individual robots. The transitions were recorded with policies trained with PPO in simulation. While there are significant differences between the success rates, the expert policies achieve at least 80% and 60% on every robot for TriFinger-Push-Real-Expert and TriFinger-Lift-Real-Expert, respectively.\\n\\nDistribution of returns:\\n\\nFig. S11 shows the distribution of the episode returns for the datasets collected on the real system. It reveals that the expert policy on the Push task performed more consistently on the real robot than its counterpart for the Lift task. Qualitatively, this can be attributed to either (i) not being able to flip the cube to the approximately correct orientation, (ii) failing to establish a stable grasp on the cube, or (iii) dropping it after already having lifted the object. Flipping the cube on the ground might fail due to incomplete modeling of interactions between the fingertips and the cube in the rigid body physics simulator or because it is sensitive to the value of sliding friction, the noise on the object pose estimate makes it difficult to maintain a stable grasp after having lifted the object.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure S11: Distribution of the episode returns in the datasets recorded on the real system.\\n\\nFigure S12: Initial cube positions (blue) and goals (red) in the real datasets for the Push task (left) and Lift task (right). Note that we avoid initial positions close to the barrier.\\n\\nInitial cube position and goal distributions: Fig. S12 visualizes the distribution of the initial cube position (blue) and the goal position (red) for the Push and Lift tasks. The goal positions are sampled on the ground for the Push task and in the air for the Lift task. The distribution of the initial cube position results from the reset procedure which removes the cube from the boundaries by moving the fingers along a pre-recorded trajectory.\\n\\nAction statistics: Fig. S13 shows the distribution of actions of the expert policy that recorded TriFinger-Push-Real-Expert and of a policy learned from this data by AWAC.\\n\\nReset procedure: As mentioned in section 3.3 of the main text, we removed the cube from the barrier between episodes because we observed that the expert policies we trained in simulation struggled with retrieving the cube from the barrier. Fig. S14a shows a histogram of the distance between the center of the cube and the arena center. While the reset procedure is sufficiently stochastic...\"}"}
{"id": "3k5CUGDLNdd", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FIGURE S13: Distribution of action components, i.e., desired torques, for the expert policy that recorded the TriFinger-Push-Real-Expert dataset and for a policy learned with AWAC from this dataset. Averaged over five offline RL seeds.\\n\\nto randomize the initial cube position (see Fig. S12), it clearly removes the cube from the barrier in the majority of all resets. More precisely, only in 3% of all resets the cube center was within 6 cm of the boundary (the cube has a width of 6.5 cm).\\n\\nFIGURE S14: (a) Histogram of the distance between the center of the cube and the arena center after the reset for the Lift-Real-Expert dataset. In the shaded gray area the cube can potentially touch the barrier which begins at the vertical black line. (b) Dropping frequency over a real lift episode. \u2018Pure feedforward\u2019 refers to directly applying the output of an MLP policy whereas \u2018smoothed\u2019 denotes a policy with an Exponential Moving Average applied. (c) Comparison of the momentary success rate of a policy learned from Push-Real-Expert (AWAC in green) with the expert policy (PPO-Expert in blue) that recorded the data. The momentary success rate captures how often the policy achieved the goal at a time step averaged over all episodes. The momentary success rate of the weak policy used in Push-Real-Weak&Expert is shown in orange.\\n\\nDropping frequency \u2013 pure feedforward vs. smoothed:\\nOne reason for the lower success rate on the Lift task as compared to the Push task is frequent dropping of the cube. This can be the result of vibrations of the fingers which can occur when applying policies trained in simulation on the 30\"}"}
{"id": "3k5CUGDLNdd", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure S5: Training curves for the Lift-Sim datasets: Offline RL algorithms are trained on a simulated dataset and are evaluated in the simulated environment. Success rates and returns for the optimized hyperparameters (in Table S8) and the default hyperparameters (in Table S9). The selected hyperparameters, by the grid-search procedure (in Table S7), optimize the algorithm's final average return on Sim-Weak&Expert. The black dashed line shows the dataset performance of Sim-Expert and the red dashed line the performance of the Sim-Weak&Expert (first 3 columns) and Sim-Mixed (last column).\"}"}
{"id": "3k5CUGDLNdd", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Here, we use the default hyperparameters in Table S9, where for the CQL algorithm we selected hyperparameters optimized by a grid search (see Figure S16 for histograms). The success rates and returns are evaluated in the simulated environment.\\n\\n**Figure S6:** Training curves for the Push-Real datasets: Offline RL algorithms are trained on the real datasets and are evaluated in the simulated environment. The black dashed line shows the dataset performance of Real-Expert and the red dashed line the performance of the Real-Weak&Expert (first 3 columns) and Real-Mixed (last column).\\n\\n**Figure S7:** Training curves for the Push-Sim datasets: Offline RL algorithms are trained on a simulated dataset and are evaluated in the simulated environment. The black dashed line shows the dataset performance of Sim-Expert and the red the performance of the Sim-Weak&Expert (first 3 columns) and Sim-Mixed (last column).\"}"}
{"id": "3k5CUGDLNdd", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section we provide additional details on the datasets we collected. Section B.1 summaries the data collection process, section B.2 contains a discussion of the different notions of success we report, section B.3 is dedicated to the reward and the terminals, and section B.4 provides a detailed analysis of the statistics of the datasets.\\n\\nB.1 Data Collection\\n\\nData is collected by running jobs on a cluster of TriFinger platforms without human intervention. Before the recording of each dataset the platforms were cleaned from particle dust to ensure consistent object tracking performance. Each job consists of the following steps:\\n\\n1. Move fingers to initial state and sample new goal.\\n2. Run one episode/rollout with the selected policy and store transitions.\\n3. Move the cube away from the barrier with a pre-recorded trajectory of the fingers.\\n4. Repeat from 1. unless the desired number of episodes per job is reached.\\n5. Do a self-check including cameras, pose estimation and joints.\\n6. Approximately reset the cube to the center of the arena.\\n\\nThe number of episodes collected per job is eight for the Push task (15 s per episode) and six for the Lift task (30 s per episode). If a self-check fails, the corresponding robot is deactivated and the maintainers are alerted via mail. Finally, the data from all episodes is combined in a single HDF5 file following the conventions of D4RL (Fu et al., 2020).\\n\\nThe expert policies are obtained after the training in Isaac Gym (see section 3.2 and section C.1). The weak policies are training checkpoints at $210 \\\\cdot 10^6$ training steps for pushing and $288 \\\\cdot 10^6$ training steps for lifting. Gaussian noise is added to the actions with an amplitude of $0.2 \\\\text{ Nm}$ and an update frequency of 8 for pushing and $0.04 \\\\text{ Nm}$ with an update frequency of 4 for lifting.\\n\\nThe Mixed datasets are collected with policies from a range of training checkpoints where the total number of jobs was distributed as uniformly as possible over all training checkpoints. For pushing 22 checkpoints up to $668 \\\\cdot 10^6$ training steps were used while for lifting 59 training checkpoints up to $1721 \\\\cdot 10^6$ training steps were considered. Fig. S8 shows the return and success rate of these checkpoints on the real TriFinger cluster.\\n\\nB.2 Three notions of success\\n\\nWe consider dexterous manipulation tasks which require the agent to match a goal position (for the Push task) or a goal pose (for the Lift task) with a tracked cube. The tolerance for goal achievement is 2 cm for the position and 22 degrees for the orientation. We define momentary success as achieving the goal at a single point in time, i.e., in an individual time step. This notion of success is rather weak, however, as achieving the goal pose for a short amount of time is significantly easier than maintaining it. For the pushing task in particular, it is much more likely to move through the goal by accident than to stabilize the cube after having reached it. When lifting the cube, it is quite challenging to maintain a stable grasp due to the variance of the object pose estimate which introduces a considerable amount of noise to the observations. We therefore define success as achieving the goal at the end of the episode which is only likely to happen when maintaining the goal pose for an extended period of time.\\n\\nFrom the perspective of offline RL, however, it is highly relevant whether parts of the trajectories in the datasets lead to success, even if it is short-lived. As offline RL algorithms can, in principle, combine information from several trajectories, even trajectories that do not end with goal achievement may contain enough information to learn a successful policy. We therefore also report transient success which indicates whether the goal has been achieved at any time during the episode and mean momentary success which corresponds to the fraction of time steps during which the goal has been achieved.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure S8: Return and success rates of the checkpoints used for collecting the Real-Mixed datasets. Note that the checkpoints were obtained by training in Isaac Gym (see section 3.2 and section C.1) and were evaluated on the real system. The shaded areas indicate the interval between the 0.25 and 0.75 quantiles of the return whereas the error bars represent the standard error of the mean of the success rate.\\n\\nB.3 REWARD AND TERMINALS\\n\\nAs already mentioned in section 3 of the main text, we use a logistic kernel Hwangbo et al. (2019); Allshire et al. (2022)\\n\\n\\\\[ k(x) = b + 2\\\\exp(a \\\\|x\\\\|) + b + \\\\exp(-a \\\\|x\\\\|) \\\\] (S1)\\n\\nto define the reward where \\\\(\\\\|\\\\cdot\\\\|\\\\) denotes the Euclidean norm. For the Push task, we apply it to the difference between the achieved and the desired cube position. Since we also want to take the orientation of the cube into account for the Lift task, we apply \\\\(k\\\\) separately to the differences between the desired and achieved corner points (or keypoints) of the cube and average over the results Allshire et al. (2022). We use \\\\(a = 30\\\\) and \\\\(b = 2\\\\). See Fig. S9 for a visualization of the reward function.\\n\\nBy default, the terminals in the dataset (a Boolean indicating whether a terminal state has been reached) are never set. We chose this default setting to avoid problems due to state aliasing: Episodes\\n\\nFigure S9: Reward as a function of the Euclidean distance between the desired and achieved position (for the Push task) or keypoint (for the Lift task). The shaded grey area corresponds to a cube centered at the goal and the green area corresponds to the goal achievement threshold (2 cm).\"}"}
{"id": "3k5CUGDLNdd", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Well. We conclude that the benchmarked offline RL algorithms still exhibit BC-like behavior, i.e., they perform significantly better, ruling out that the performance drop on the Lift-Weak&Expert dataset is exclusively caused by a lack of data. Fig. 5 shows that this is true for the simulated Lift datasets as well. The Lift datasets, in particular those recorded on the real robots, generally contain more useful sub-trajectories than apparent from the success rate, which is defined as achieving the goal pose at some point in an episode. For example, during 87% of all episodes in the Lift-Real-Expert dataset the goal pose is achieved.\\n\\nThis effect is most pronounced on the real robot data and calls the ability of the algorithms to make smooth expert reaching higher returns (see Table S2). On the Real-Smooth-Expert dataset the success rates are, on average, slightly lower, probably due to the non-Markovian behavior policy. The returns are generally higher, however, likely due to the data. We refer to this dataset as Half-Expert. We find that the resulting policy performs significantly better, ruling out that the performance drop on the Lift-Weak&Expert dataset is due to only half of the expert data being available, we also train solely on the expert trajectories contained in the Weak&Expert dataset. We find that the resulting policy is exclusively caused by a lack of data. Shaded areas indicate the interval between the 0.25 and 0.75 quantiles. Learning curves for all experiments are shown in Appendix A.6.\\n\\nOn the Lift-Weak&Expert datasets all algorithms perform significantly worse than on the expert data. On the Real-Smooth-Expert dataset the success rates are, on average, slightly lower, probably due to the non-Markovian behavior policy. The returns are generally higher, however, likely due to the smooth expert reaching higher returns (see Table S2).\\n\\nWe refer to this dataset as Half-Expert. We find that the resulting policy performs significantly better, ruling out that the performance drop on the Lift-Weak&Expert dataset is exclusively caused by a lack of data. Shaded areas indicate the interval between the 0.25 and 0.75 quantiles. Learning curves for all experiments are shown in Appendix A.6.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Expert BC \u2020 CRR \u2020 AW AC \u2020 CQL \u2020 IQL \u2020\\n\\nFigure 6: Analysis of policy behavior.\\n\\n(a) Success rate for simulated lifting after training on data with varying relative environment noise scale (1.0 corresponds to the noise level of the real system). The dashed lines indicate the performance on the real system. (b) Average speed of the cube over a pushing episode on the real system. Some offline RL algorithms learn to move the cube faster than the expert. (c) Momentary success during a pushing episode on the real system: the fraction of episodes during which the goal was achieved at a given point in time (trained on expert data).\\n\\nare furthermore noisy and do not contain estimates of the velocity of the cube, training recurrent policies could potentially lead to an increase in performance.\\n\\nTo investigate the impact of noise on the performance of offline RL policies and the expert, we collected datasets in simulation for up to twice the noise amplitudes measured on the real system. Fig. 6 (a) shows that the performance of the expert and the offline RL policies degrades only slowly when increasing the noise scale, ruling out that noise is the sole explanation for the performance gap between simulated and real system. As delays in the observation and action execution are already implemented in the simulated environment, we conclude that other factors like more complex contact dynamics and elastic deformations of the fingertips and robot limbs are likely causing the larger performance gap between data and learned policies on the real robots.\\n\\nTo test how well the policies learned from the datasets generalize over instances of the robot hardware, we evaluated on a hold-out robot which was not used for data collection. We did not see a significant difference in performance (see Appendix A.3 for details on this experiment) suggesting that the datasets cover enough variations in the robot hardware to enable generalization to unseen robots.\\n\\nIn summary, CRR and AW AC generally perform best on the proposed datasets with IQL also being competitive after hyperparameter optimization. The performance gap between expert and offline RL is in general bigger on the real system, perhaps due to more challenging dynamics.\\n\\n5 RELATED WORK\\n\\nOffline RL: The goal of offline RL (Levine et al., 2020; Prudencio et al., 2022) is to learn effective policies without resorting to an online interaction by leveraging large and diverse datasets covering a sufficient amount of expert transitions. This approach is particularly interesting if interactions with the environment are either prohibitively costly or even dangerous.\\n\\nOffline RL faces a fundamental challenge, known as the Distributional Shift (DS) problem, originating from two sources. There is a distribution mismatch in training, as we use the behavioral data for training, but the learned policy would create a different state-visitation distribution. The second problem is that during policy improvement, the learned policy requires an evaluation of the Q-function on unseen actions (out of distribution). An over-estimation of the Q-value on these out-of-distribution samples leads to learning of a suboptimal policy.\\n\\nSeveral algorithmic schemes were proposed for addressing the DS problem: i) constraining the learned policy to be close to the behavior data (Fujimoto et al., 2019; Kumar et al., 2019b; Zhang et al., 2021; Kostrikov et al., 2021a); ii) enforcing conservative estimates of future rewards (Kumar et al., 2020; Yu et al., 2021; Cheng et al., 2022); and iii) model-based methods that estimate the uncertainty via ensembles (Janner et al., 2019; Kidambi et al., 2020).\\n\\nAdditionally, other approaches include: implicitly tackling the DS problem via (advantage-weighted) variants of behavioral cloning (Nair et al., 2020; Wang et al., 2020; Chen et al., 2020; Fujimoto & Gu, 2021) or even completely bypassing it either by removing the off-policy evaluation and...\"}"}
{"id": "3k5CUGDLNdd", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"performing a constrained policy improvement using an on-policy Q estimate of the behavior policy (Brandfonbrener et al., 2021), or by approximating the policy improvement step implicitly by learning on-data state value function (Kostrikov et al., 2021b). An orthogonal research line considers combining importance sampling and off-policy techniques (Nachum et al., 2019b;a; Zhang et al., 2020; Xu et al., 2021), and recently (Chen et al., 2021; Janner et al., 2021) investigated learning an optimal trajectory distribution via transformer architectures.\\n\\nThe simplest strategy for learning from previously collected data is behavioral cloning (BC), which is fitting a policy to the data directly. Offline RL can outperform BC: i) on long-horizon tasks with mixed expert data, e.g., trajectories collected via an expert and a noisy-expert; and ii) with expert or near expert data, when there is a mismatch between the initial and the deployment state distribution.\\n\\nRL for dexterous manipulation: Reinforcement learning was recently successfully applied to dexterous manipulation on real hardware (OpenAI et al., 2018; 2019; Allshire et al., 2022; Wang et al., 2022). These results rely on training with online RL in simulation, however, and are consequently limited by the fidelity of the simulator. While domain randomization (Tobin et al., 2017; Mandlekar et al., 2017; Peng et al., 2018) can account for a mismatch between simulation and reality in terms of physics parameters, it cannot compensate oversimplified dynamics. The challenges of real-world environments have been recognized and partly modeled in simulated environments (Dulac-Arnold et al., 2020). To overcome the sim-to-real gap entirely, however, data from real-world interactions is still required, in particular for robotics problems involving contacts.\\n\\nOffline RL datasets: While offline RL datasets with data from simulated environments, like D4RL (Fu et al., 2020) and RL Unplugged (Gulcehre et al., 2020), have propelled the field forward, the lack of real-world robotics data has been recognized (Behnke, 2006; Bonsignorio & del Pobil, 2015; Calli et al., 2015; Amigoni et al., 2015; Murali et al., 2019). Complex contact dynamics, soft deformable fingertips and vibrations are particularly relevant for robotic manipulation but are not modeled sufficiently well in simulators used by the RL community (Todorov et al., 2012; Makoviychuk et al., 2021; Freeman et al., 2021). Recently, three small real-world datasets with human demonstrations for a robot arm with a gripper (using operational space control) have been proposed (Mandlekar et al., 2021). Two of them require only basic lifting and dropping while the third, more challenging task could not be solved with the available amount of data. For more challenging low-level physical manipulation, a dataset suitable for offline RL is still missing. We therefore provide the first real-robot dataset for dexterous manipulation which is sufficiently large for offline RL (one order of magnitude more data on real robots than prior work (Mandlekar et al., 2021)) and for which learned policies can easily be evaluated remotely on a real-robot platform.\\n\\nAffordable open-source platforms: Our hardware platform is open source. Other affordable robotic open-source platforms are, for instance, a manipulator (Yang et al., 2019), a simple robotic hand and quadruped (Ahn et al., 2020). Since it is hard to set up and maintain such platforms, we provide access to our real platform upon request, and hope that this will bring the field forward.\\n\\nRemote benchmarks: For mobile robotics, Pickem et al. (2017) propose the Robotarium, a remotely accessible swarm robotics research platform, and Kumar et al. (2019a) offer OffWorld gym consisting of two navigation tasks with a wheeled robot. Similarly, Duckietown (Paull et al., 2017) hosts the AI Driving Olympics (AI-DO-team, 2022).\\n\\nCONCLUSION\\nWe present benchmark datasets for robotic manipulation that are intended to help improving the state-of-the-art in offline reinforcement learning. To record datasets, we trained capable policies using online learning in simulation with domain randomization. Our analysis and evaluation on two tasks, Push and Lift, show that offline RL algorithms still leave room for improvement on data from real robotic platforms. We identified two factors that could translate into increased performance on our datasets: trajectory stitching and robustness to non-expert trajectories. Further, our analysis indicates that noise and delay alone cannot explain the larger gap between dataset and offline RL performance on real systems, underpinning the importance of real-robot benchmarks.\\n\\nWe invite the offline RL community to train their algorithms with the new datasets and test the empirical performance of the latest offline RL algorithms, e.g. (Kostrikov et al., 2021a; Xu et al., 2021; Kumar et al., 2021; Cheng et al., 2022), on real-robot hardware.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"AUTHOR CONTRIBUTIONS\\n\\nN.G., S.Bl., P.K., M.W., S.Ba., B.S., and G.M. conceived the idea, methods and experiments. G.M. initiated the project, M.W., S.Ba. and B.S. conceived the robotic platform. F.W. implemented the low-level robot control and parts of the submission system. N.G. trained the expert policies and created the datasets. N.G., S.Bl., and P.K. conducted the offline RL experiments, collected the results and analyzed them under the supervision of G.M. N.G. ran all experiments on the real systems. N.G. and F.W. wrote the software for downloading and accessing the datasets. N.G., S.Bl., P.K., F.W. and G.M. drafted the manuscript, and all authors revised it.\\n\\nACKNOWLEDGMENTS\\n\\nWe are grateful for the help of Thomas Steinbrenner in repairing and maintaining the robot cluster. Moreover, feedback by Arthur Allshire on training expert policies in simulation and by Huanbo Sun on domain randomization proved valuable. We acknowledge the support from the German Federal Ministry of Education and Research (BMBF) through the T\u00fcbingen AI Center (FKZ: 01IS18039B). Georg Martius is a member of the Machine Learning Cluster of Excellence, EXC number 2064/1 \u2013 Project number 390727645. Pavel Kolev was supported by the Cyber Valley Research Fund and the Volkswagen Stiftung (No 98 571). We thank the anonymous reviewers for comments which helped improve the presentation of the paper.\\n\\nREPRODUCIBILITY STATEMENT\\n\\nWe publish the datasets we propose as benchmarks (sections 3.3 and B) and provide access to the cluster of real TriFinger platforms (section 2) we used for data collection. Submissions to the cluster do not require any robotics experience and can be made in the form of a Python implementation of a RL policy (section B.5). A simulated version of the TriFinger platform (Joshi et al., 2020) and a low-cost hardware variant are furthermore publicly available as open source (W\u00fcthrich et al., 2021). For offline RL training we moreover use open-source software (Seno & Imai, 2021). Finally, we describe our hyperparameter optimization in detail and provide the resulting hyperparameters in section C.3.\\n\\nREFERENCES\\n\\nMichael Ahn, Henry Zhu, Kristian Hartikainen, Hugo Ponte, Abhishek Gupta, Sergey Levine, and Vikash Kumar. Robel: Robotics benchmarks for learning with low-cost robots. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura (eds.), Proceedings of the Conference on Robot Learning, volume 100 of Proceedings of Machine Learning Research, pp. 1300\u20131313. PMLR, 30 Oct\u201301 Nov 2020.\\n\\nAI-DO-team. The ai driving olympics (ai-do). https://www.duckietown.org/research/ai-driving-olympics, 2022.\\n\\nArthur Allshire, Mayank MittaI, Varun Lodaya, Viktor Makoviychuk, Denys Makoviichuk, Felix Widmaier, Manuel W\u00fcthrich, Stefan Bauer, Ankur Handa, and Animesh Garg. Transferring dexterous manipulation from GPU simulation to a remote real-world trifinger. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 11802\u201311809, 2022.\\n\\nFrancesco Amigoni, Emanuele Bastianelli, Jakob Berghofer, Andrea Bonarini, Giulio Fontana, Nico Hochgeschwender, Luca Iocchi, Gerhard Kraetzschmar, Pedro Lima, Matteo Matteucci, Pedro Miraldo, Daniele Nardi, and Viola Schiaffonati. Competitions for benchmarking: Task and functionality scoring complete performance assessment. IEEE Robotics & Automation Magazine, 22(3):53\u201361, 2015.\\n\\nOpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3\u201320, 2020.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We found that applying a low-pass filter, more precisely an Exponential Moving Average, helps with both problems. Fig. S14b shows the frequency at which the cube is dropped over the course of a lifting episode. Dropping the cube is defined as reaching the height of the goal pose up to a tolerance of 2 cm at some point during the episode and then dropping the cube flat on the ground (up to a tolerance of 1 cm). Smoothing the actions before applying them clearly helps with avoiding dropping, in particular later in the episode. Note, however, that the smoothing has to be applied already during training in simulation for the policy to adapt to it. Despite the action smoothing the dropping rate does not reach zero. This can be partly attributed to unstable grasps that lead to slipping on the real robot.\\n\\nWe provide an easy-to-use Python package that is compatible with a popular collection of offline RL datasets Fu et al. (2020). To use the datasets, it is sufficient to clone the repository, instantiate the desired environment and call a method that returns the dataset. The correct dataset is then downloaded automatically. We also provide options for filtering the observations and for converting them to flat arrays.\\n\\nThe following example code demonstrates how (parts of) a dataset can be loaded:\\n\\n```python\\nimport gymnasium as gym\\nimport numpy as np\\nimport trifinger_rl_datasets\\nenv = gym.make(\\\"trifinger-cube-push-real-expert-v0\\\", disable_env_checker=True, visualization=False)\\n# load data at timesteps specified in array\\ndataset_part = env.get_dataset(indices=np.array([42, 1000, 5000]))\\n# load data corresponding to a range of timesteps\\ndataset_part = env.get_dataset(rng=(1000, 2000))\\n# load the whole dataset\\ndataset = env.get_dataset()\\n```\\n\\nFor installation instructions and further details we refer to the repository of the Python package at https://github.com/rr-learning/trifinger_rl_datasets.\\n\\nAccess to the robot cluster can be requested at https://webdav.tuebingen.mpg.de/trifinger/. The policy has to be implemented in a GitHub repository following a fixed interface. We recommend adapting the example package available at https://github.com/rr-learning/trifinger-rl-example.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table S6: Performance of the expert policies in the Isaac Gym environment. The numbers for the success rate and return are not directly comparable to those in the PyBullet simulator and on the real robot for two reasons: (i) In Isaac Gym Lift episodes last for 15 s instead of 30 s, and (ii) the return in the Isaac Gym environment contains auxiliary reward terms as described in Allshire et al. (2022) and section 3.2.\\n\\n| Action | Success Rate | Return | Training Steps |\\n|--------|--------------|--------|----------------|\\n| Push   | 0.99         | 629.83\\\\times10^9 | 7 |\\n| Lift   | 0.87         | 589.72\\\\times10^9 | 7 |\\n\\nIn addition to the modifications mentioned in section 3.2, we also ported training from Isaac Gym 2 to the current version Isaac Gym 3 (Makoviychuk et al., 2021). We furthermore increased the torque range from \\\\([-0.36, 0.36]\\\\) Nm to \\\\([-0.397, 0.397]\\\\) Nm to make sure the fingers are strong enough to lift the cube when supporting it from below. To make the training environment resemble the data collection setting on the real robots, we furthermore implemented an adjustable delay between when an observation is received and when the action based on this observation is applied for the first time. The success rates and returns reached after training are shown in Table S6.\\n\\nNote that the success rates we give for the lifting task on the real robot are not directly comparable to the ones reported in Allshire et al. (2022) as our lifting task is more challenging. While Allshire et al. (2022) evaluate success after 60 s, we evaluate after 30 s. As policies usually need several attempts to flip the cube to roughly the correct orientation and for picking it up, the success rate after 30 s is lower in general. We chose the shorter episode length because it is, in principle, sufficient to solve the task and because we wanted to avoid episodes which consist, to a large part, of the cube being held in place. Moreover, unlike Allshire et al. (2022), we do not push the cube to the center of the arena before each episode but only remove it from the barrier.\\n\\nC.2 Training with Offline RL on the Datasets\\n\\nWe use the implementations of BC, CRR, AWAC, CQL and IQL provided by the open-source library D3RLPY (Seno & Imai, 2021). The code is available at https://github.com/takuseno/d3rlpy and the documentation can be found at https://d3rlpy.readthedocs.io/. For our experiments we used versions 1.1.0 and 1.1.1 of D3RLPY. The used hyperparameters and the performed optimization are discussed in the next section.\\n\\nC.3 Hyperparameters\\n\\nWe performed a grid search over hyperparameters for all algorithms as documented in Table S7. The hyperparameter setting with the highest performance in terms of final average return on Lift-Sim-Weak&Expert was selected, as listed in Table S8. In the paper, the results with optimized parameters are marked with a \u2020. Otherwise, the default parameters were used, as listed in Table S9. The rest of the section contains a detailed analysis of the grid search.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table S7: Hyperparameter grid search (on dataset Lift-Sim-Weak&Expert). Each algorithm is trained with the same two seeds (sampled uniformly at random).\\n\\n| Algorithms | Parameters                                                                 |\\n|------------|-----------------------------------------------------------------------------|\\n| AW AC      | \\\\{(actor_learning_rate=R, critic_learning_rate=R) : R \\\\in \\\\{1.5E-4, 3.0E-4, 6.0E-4\\\\}\\\\}; batch_size \\\\in \\\\{256, 512\\\\}; lam \\\\in \\\\{0.3, 1.0, 3.0\\\\}\\\\) |\\n| BC         | \\\\{learning_rate={1.5E-4; 3.0E-4; 6.0E-4}; batch_size \\\\in \\\\{256, 512\\\\}\\\\} |\\n| CRR        | \\\\{(actor_learning_rate=R, critic_learning_rate=R) : R \\\\in \\\\{1.5E-4, 3.0E-4, 6.0E-4\\\\}\\\\}; batch_size \\\\in \\\\{256, 512\\\\}; beta \\\\in \\\\{0.25, 1.0, 4.0\\\\}\\\\) |\\n| CQL        | \\\\{(actor_learning_rate=R, critic_learning_rate=3.0*R, initial_alpha=T[1], alpha_learning_rate={R if T[0] else 0}, temp_learning_rate=R, alpha_threshold=T[2]) : R \\\\in \\\\{5.0E-5, 1.0E-4\\\\}, T \\\\in \\\\{[true, 1.0, 1.0], [true, 1.0, 5.0], [true, 1.0, 10.0], [false, 0.3, 10.0], [false, 1.0, 10.0], [false, 3.0, 10.0]\\\\}\\\\}; conservative_weight \\\\in \\\\{2.5, 5.0, 10.0, 20.0\\\\}\\\\) |\\n| IQL        | \\\\{(actor_learning_rate=R, critic_learning_rate=R) : R \\\\in \\\\{1.5E-4, 3.0E-4, 6.0E-4\\\\}\\\\}; batch_size \\\\in \\\\{256, 512\\\\}; expectile \\\\in \\\\{0.7, 0.8, 0.9\\\\}; weight_temp \\\\in \\\\{3.0, 10.0\\\\}\\\\) |\\n\\nTable S8: Optimized Hyperparameters using grid search Table S7.\\n\\n| Algorithms | Parameters                                                                 |\\n|------------|-----------------------------------------------------------------------------|\\n| AW AC      | \u2020 actor_learning = critic_learning_rate = 0.00015; batch_size=256; lam=3.0 |\\n| BC         | \u2020 learning_rate=0.00015; batch_size=512                                    |\\n| CRR        | \u2020 actor_learning = critic_learning_rate = 0.00015; batch_size=256; beta=1.0 |\\n| CQL        | \u2020 actor_learning_rate=0.0001; critic_learning_rate=0.0003; initial_alpha=1.0; conservative_weight=20.0; alpha_learning_rate=0.0; action_scaler: minmax |\\n| IQL        | \u2020 actor_learning_rate = critic_learning_rate= 0.00015; batch_size=256; expectile=0.9; weight_temp=3.0 |\\n\\nTable S9: Default hyperparameters. All algorithms except BC (with batch_size=100 and without a critic) have batch_size=256 and n_critics=2. Note that due to bad performance we optimized CQL's parameters on Push-Sim-Expert with an extensive grid search as shown in Fig. S16. For all other algorithms, we used the default values of the implementation.\\n\\n| Algorithms | Parameters                                                                 |\\n|------------|-----------------------------------------------------------------------------|\\n| AW AC      | \u2020 actor_learning = critic_learning_rate = 0.0003; batch_size=1024; lam=1.0 |\\n| BC         | \u2020 learning_rate=0.001; batch_size=100                                       |\\n| CRR        | \u2020 actor_learning = critic_learning_rate = 0.0003; batch_size=256; beta=1.0 |\\n| CQL        | \u2020 actor_learning_rate=0.0001; critic_learning_rate=0.0003; initial_alpha=1.0; conservative_weight=20.0; alpha_learning_rate=0.0; action_scaler: minmax |\\n| IQL        | \u2020 actor_learning_rate = critic_learning_rate= 0.0003; batch_size=256; expectile=0.7; weight_temp=3.0 |\"}"}
{"id": "3k5CUGDLNdd", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Fig. S15 we present the returns and success rates for each of the hyperparameter settings for the Lift task in simulation. We see that the different algorithms have very different sensitivity to their hyperparameters. For CRR a large fraction of parameters leads to good results. For AWAC the sensitivity is a bit higher. IQL seems to degrade more gracefully with changed parameters. For CQL we were unable to find good hyperparameters, despite running 48 configurations. The performance of the individual runs over training time are shown in Fig. S17.\\n\\nFigure S15: Hyperparameter grid search: Histogram of returns and success rates for the Lift-Sim-Weak&Expert task.\\n\\nOn the Push task, we ran an even larger hyperparameter scan with 405 configurations for CQL, as presented in Fig. S16. Even for this much simpler task, we see that the majority of parameters yield low success rates. In addition, we note that the training became quite unstable for alpha_lr > 0.0 and due to this, we conducted our main grid-search with alpha_lr = 0.0. Then, we chose the best hyperparameter configuration as the default setting for CQL.\\n\\nFigure S16: Hyperparameter grid search on the Push-Sim-Expert task for CQL. Shown is the histogram of returns and success rates for the 405 hyperparameter settings, as defined on the right.\"}"}
{"id": "3k5CUGDLNdd", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure S17: Lift-Sim-Weak&Expert. Hyperparameter grid search. Returns (top row) and success rate (bottom row). Every curve corresponds to one parameter configuration (averaged over 2 seeds). The black dashed line shows the dataset performance of Sim-Expert and the red dashed line the performance of the Sim-Weak&Expert.\\n\\nOn the real platforms, we report numbers for 48 trials for the Push task and 36 trials for the Lift task, each for 5 training seeds. Each algorithm and seed is evaluated on 5 to 6 robots with the same set of 6 (for the Lift task) to 8 (for the Push task) goals per robot (computed from the robot ID). The success rate and return of each algorithm and seed is computed from the resulting trials. The mean and standard deviations of the success rates and returns of each algorithm is computed across seeds.\\n\\nIn the simulated environment, we perform 100 testing episodes per final policy for 5 independent training runs.\"}"}
