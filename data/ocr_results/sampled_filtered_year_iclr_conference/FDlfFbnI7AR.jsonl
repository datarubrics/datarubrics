{"id": "FDlfFbnI7AR", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"If \\\\( z \\\\in \\\\text{co} \\\\Pi_{23} \\\\text{SAT} \\\\), then there exists \\\\( x^* \\\\).\\n\\nLet \\\\( x^{**} \\\\) be defined as follows:\\n\\n\\\\[\\n\\\\begin{align*}\\nx^{**}_i &= 1 & \\\\text{if } x_*^i = 0 \\\\\\\\\\nx^{**}_i &= 0 & \\\\text{if } x_*^i = 1 \\\\\\\\\\nx^{**}_i &= 3 & \\\\text{if } x_*^i = 1 \\\\\\\\\\n\\\\end{align*}\\n\\\\]\\n\\nNote that:\\n\\n- \\\\( x^{**} \\\\in B^p(x(s), \\\\varepsilon) \\\\); \\n- \\\\( e(x) = x^* \\\\); \\n- \\\\( f(x^{**}) = 0 \\\\), since \\\\( x^{**} \\\\in \\\\{1/4, 3/4\\\\} \\\\); \\n- Since \\\\( \\\\gamma' < 1/4 \\\\), there is no \\\\( i \\\\) such that \\\\( x''^i \\\\in B^p(x^{**}, \\\\gamma') \\\\). \\n- For all \\\\( x'' \\\\in B^p(x^{**}, \\\\gamma') \\\\):\\n  - If \\\\( x''^i \\\\) is not a valid encoding (i.e. \\\\( x''^i \\\\in \\\\{1/4, 3/4\\\\} \\\\)), then \\\\( h'(x'') = 0 \\\\); \\n  - Otherwise, \\\\( h'(x'') = 1 \\\\) iff \\\\( R(e(x)(x''), e(y)(x'')) \\\\) is true.\\n\\nTherefore, since \\\\( \\\\forall \\\\hat{y}. \\\\neg R(x^*, \\\\hat{y}) \\\\), we know that \\\\( \\\\forall x'' \\\\in B^p(x^{**}, \\\\gamma') \\\\). \\\\( f(x'') = 0 \\\\). In other words, \\\\( x^{**} \\\\) is a solution to Equation (5).\"}"}
{"id": "FDlfFbnI7AR", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In other words, for all $\\\\hat{y}$ there exists a corresponding $x'' \\\\in B_p(x^*, \\\\gamma')$ such that $e(y)(x'') = \\\\hat{y}$.\\n\\nTherefore, since $h'(x'') = 1$ iff $R(e(x')(x^*), e(y)(x''))$ is true and since $\\\\forall x'' \\\\in B_p(x^*, \\\\gamma')$. $h'(x'') = 0$, we can conclude that $\\\\forall \\\\hat{y}$. $\\\\neg R(e(x')(x^*), \\\\hat{y})$. In other words, $z \\\\in \\\\text{co} \\\\Pi_{3\\\\text{SAT}}$.\\n\\nSimilarly to the proof of Corollary 1.3, it follows from the fact that ReLU classifiers are polynomial-time classifiers (w.r.t. the size of the tuple).\\n\\nModels\\nAll models were trained using Adam (Kingma & Ba, 2014) and dataset augmentation. We performed a manual hyperparameter and architecture search to find a suitable compromise between accuracy and MIPVerify convergence. The process required approximately 4 months. When performing adversarial training, following (Madry et al., 2018) we used the final adversarial example found by the Projected Gradient Descent attack, instead of the closest. To maximize uniformity, we used for each configuration the same training and pruning hyperparameters (when applicable), which we report in Table 1. We report the chosen architectures in Tables 2 and 3, while Table 4 outlines their accuracies and parameter counts.\\n\\nUG100\\nThe first 250 samples of the test set of each dataset were used for hyperparameter tuning and were thus not considered in our analysis. For our G100 dataset, we sampled uniformly across each ground truth label and removed the examples for which MIPVerify crashed. Table 5 details the composition of the dataset by ground truth label.\\n\\nAttacks\\nFor the Basic Iterative Method (BIM), the Fast Gradient Sign Method (FGSM) and the Projected Gradient Descent (PGD) attack, we used the implementations provided by the AdverTorch library (Ding et al., 2019). For the Brendel & Bethge (B&B) attack and the Deepfool (DF) attack, we used the implementations provided by the Foolbox Native library (Rauber et al., 2020). The Carlini & Wagner and the uniform noise attacks were instead implemented by the authors. We modified the attacks that did not return the closest adversarial example found (i.e. BIM, Carlini & Wagner, Deepfool, FGSM and PGD) to do so. For the attacks that accept $\\\\varepsilon$ as a parameter (i.e. BIM, FGSM, PGD and uniform noise), for each example we first performed an initial search with a decaying value of $\\\\varepsilon$, followed by a binary search. In order to pick the attack parameters, we first selected the strong set by performing an extensive manual search. The process took approximately 3 months. We then modified the strong set in order to obtain the balanced parameter set. We report the parameters of both sets (as well as the parameters of the binary and $\\\\varepsilon$ decay searches) in Table 6.\\n\\nMIPVerify\\nWe ran MIPVerify using the Julia library MIPVerify.jl and Gurobi (Gurobi Optimization, LLC, 2022). Since MIPVerify can be sped up by providing a distance upper bound, we used the same pool of adversarial examples utilized throughout the paper. For CIFAR10 we used the strong parameter set, while for MNIST we used the strong parameter set with some differences (reported in Table 7). Since numerical issues might cause the distance upper bound computed by the heuristic attacks to be slightly different from the one computed by MIPVerify, we ran a series of exploratory runs, each with a different correction factor (1.05, 1.25, 1.5, 2), and picked the first factor that caused MIPVerify to find a feasible (but not necessarily optimal) solution. If the solution was not optimal, we then performed a main run with a higher computational budget. We provide the parameters of MIPVerify in Table 8. We also report in Table 9 the percentage of tight bounds for each combination.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Training and pruning hyperparameters.\\n\\n| Parameter Name | Value |\\n|----------------|-------|\\n| MNIST CIFAR10  |       |\\n| **Common Hyperparameters** |       |\\n| Epochs         | 425   |\\n| Learning Rate  | 1e-4  |\\n| Batch Size     | 32 128|\\n| Adam \u03b2         | (0.9, 0.999) |\\n| Flip %         | 50%   |\\n| Translation Ratio | 0.1  |\\n| Rotation (deg.) | 15\u00b0   |\\n| **Adversarial Hyperparameters (Adversarial and ReLU only)** |       |\\n| Attack PGD     |       |\\n| Attack #Iterations | 200 |\\n| Attack Learning Rate | 0.1  |\\n| Adversarial Ratio | 1     |\\n| \u03b5              | 0.05 2/255 |\\n| **ReLU Hyperparameters (ReLU only)** |       |\\n| L1 Regularization Coeff. | 2e-5 1e-5 |\\n| RS Loss Coeff.  | 1.2e-4 1e-3 |\\n| Weight Pruning Threshold | 1e-3  |\\n| ReLU Pruning Threshold | 90%   |\\n\\nTable 2: MNIST Architectures.\\n\\n(a) MNIST A\\n\\n| Input | Flatten | Linear (in = 784, out = 100) | ReLU | Linear (in = 100, out = 10) | Output |\\n|-------|---------|-------------------------------|------|-------------------------------|--------|\\n\\n(b) MNIST B\\n\\n| Input | Conv2D (in = 1, out = 4, 5x5 kernel, stride = 3, padding = 0) | ReLU | Flatten | Linear (in = 256, out = 10) | Output |\\n|-------|-----------------------------------------------------------------|------|---------|-------------------------------|--------|\\n\\n(c) MNIST C\\n\\n| Input | Conv2D (in = 1, out = 8, 5x5 kernel, stride = 4, padding = 0) | ReLU | Flatten | Linear (in = 288, out = 10) | Output |\\n|-------|-----------------------------------------------------------------|------|---------|-------------------------------|--------|\"}"}
{"id": "FDlfFbnI7AR", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: CIFAR10 architectures.\\n\\n(a) CIFAR10 A\\n\\nInput\\nConv2D (in = 3, out = 8, 3x3 kernel, stride = 2, padding = 0)\\nReLU\\nFlatten\\nLinear (in = 1800, out = 10)\\nOutput\\n\\n(b) CIFAR10 B\\n\\nInput\\nConv2D (in = 3, out = 20, 5x5 kernel, stride = 4, padding = 0)\\nReLU\\nFlatten\\nLinear (in = 980, out = 10)\\nOutput\\n\\n(c) CIFAR10 C\\n\\nInput\\nConv2D (in = 3, out = 8, 5x5 kernel, stride = 4, padding = 0)\\nReLU\\nConv2D (in = 8, out = 8, 3x3 kernel, stride = 2, padding = 0)\\nReLU\\nFlatten\\nLinear (in = 72, out = 10)\\nOutput\\n\\nTable 4: Parameter counts and accuracies of trained models.\\n\\n| Architecture | #Parameters | Training Accuracy |\\n|--------------|-------------|------------------|\\n| MNIST A      | 79510       |                   |\\n| Standard     |             | 95.87%           |\\n| Adversarial  |             | 94.24%           |\\n| ReLU         |             | 93.57%           |\\n| MNIST B      | 2674        |                   |\\n| Standard     |             | 89.63%           |\\n| Adversarial  |             | 84.54%           |\\n| ReLU         |             | 83.69%           |\\n| MNIST C      | 3098        |                   |\\n| Standard     |             | 90.71%           |\\n| Adversarial  |             | 87.35%           |\\n| ReLU         |             | 85.67%           |\\n| CIFAR10 A    | 18234       |                   |\\n| Standard     |             | 53.98%           |\\n| Adversarial  |             | 50.77%           |\\n| ReLU         |             | 32.85%           |\\n| CIFAR10 B    | 11330       |                   |\\n| Standard     |             | 55.81%           |\\n| Adversarial  |             | 51.35%           |\\n| ReLU         |             | 37.33%           |\\n| CIFAR10 C    | 1922        |                   |\\n| Standard     |             | 47.85%           |\\n| Adversarial  |             | 45.19%           |\\n| ReLU         |             | 32.27%           |\"}"}
{"id": "FDlfFbnI7AR", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nCOUNTERING THE ATTACK-DEFENSE COMPLEXITY\\n\\nSummary:\\n\\nWe consider the decision version of defending and attacking Machine Learning classifiers. We provide a rationale for known difficulties in building robust models by proving that, under broad assumptions, attacking a polynomial-time classifier is NP-complete in the worst case; conversely, training a polynomial-time model that is robust on even a single input is \\\\( \\\\Sigma^P_2 \\\\)-complete, barring collapse of the Polynomial Hierarchy. We also provide more general bounds for non-polynomial classifiers. We point out an alternative take on adversarial defenses that can sidestep such a complexity gap, by introducing Counter-Attack (CA), a system that computes on-the-fly robustness certificates for a given input up to an arbitrary distance bound \\\\( \\\\epsilon \\\\). Finally, we empirically investigate how heuristic attacks can approximate the true decision boundary distance, which has implications for a heuristic version of CA. As part of our work, we introduce UG100, a dataset obtained by applying both heuristic and provably optimal attacks to limited-scale networks for MNIST and for CIFAR10. We hope our contributions can provide guidance for future research.\\n\\nIntroduction:\\n\\nAdversarial attacks, i.e. algorithms designed to fool machine learning models, represent a significant threat to the applicability of such models in real-world contexts (Brendel et al., 2019; Brown et al., 2017; Wu et al., 2020). Despite years of research effort, countermeasures (i.e. \u201cdefenses\u201d) to adversarial attacks are frequently fooled by applying small tweaks to existing techniques (Carlini & Wagner, 2016; 2017a; Croce et al., 2022; He et al., 2017; Hosseini et al., 2019; Tramer et al., 2020). We argue that this pattern is due to differences between the fundamental mathematical problems that defenses and attacks need to tackle. Specifically, we prove that while attacking a polynomial-time classifier is NP-complete in the worst case, training a polynomial-time model that is robust even on a single input is \\\\( \\\\Sigma^P_2 \\\\)-complete. We also provide more general bounds for non-polynomial classifiers, showing that a \\\\( \\\\mathcal{O}(T) \\\\)-time classifier can be attacked in \\\\( \\\\mathcal{O}(T^{\\\\eta}) \\\\) time. We then give an informal intuition for our theoretical results, which also applies to heuristic attacks and defenses. Our result highlights that, unless the Polynomial Hierarchy collapses, there exists a potential, structural, difficulty for defense approaches that focus on building robust classifiers at training time.\\n\\nWe then show that the asymmetry can be sidestepped by an alternative perspective on adversarial defenses. As an exemplification, we introduce a new technique, named Counter-Attack (CA) that, instead of training a robust model, evaluates robustness on the fly for a specific input by running an adversarial attack. This simple approach, while very simple, provides robustness guarantees against perturbations of an arbitrary magnitude \\\\( \\\\epsilon \\\\). Additionally, we prove that while generating a certificate is NP-complete in the worst case, attacking CA using perturbations of magnitude \\\\( \\\\epsilon' > \\\\epsilon \\\\) is \\\\( \\\\Sigma^P_2 \\\\)-complete, which represents a form of computational robustness \u2013 weaker than the one by (Garg et al., 2020), but holding under much more general assumptions. CA can be applied in any setting where at least one untargeted attack is known, while also allowing one to capitalize on future algorithmic improvements: as adversarial attacks become stronger, so does CA.\\n\\nFinally, we investigate the empirical performance of an approximate version of CA where a heuristic attack is used instead of an exact one. This version achieves reduced computational time, at the cost of providing only approximate guarantees. We found heuristic attacks to be high-quality approximators for exact decision boundary distances, in experiments over a subsample of MNIST and CIFAR10 and small-scale Neural Networks. In particular, a pool of seven heuristic attacks...\"}"}
{"id": "FDlfFbnI7AR", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We provided an accurate (average over-estimate between 2.04% and 4.65%) and predictable (average $R^2 > 0.99$) approximation of the true optimum. We compiled our benchmarks and generated adversarial examples (both exact and heuristic) in a new dataset, named UG100, and made it publicly available. Overall, we hope our contributions can support future research by highlighting potential structural challenges, pointing out key sources of complexity, inspiring research on heuristics and tractable classes, and suggesting alternative perspectives on how to build robust classifiers.\\n\\nRobustness bounds for NNs were first provided in (Szegedy et al., 2013), followed by (Hein & Andriushchenko, 2017) and (Weng et al., 2018b). One major breakthrough was the introduction of automatic verification tools, such as the Reluplex solver (Katz et al., 2017). However, the same work also showed that proving properties of a ReLU network is NP-complete. Researchers tried to address this issue by working in three directions. The first is building more efficient solvers based on alternative formulations (Dvijotham et al., 2018; Singh et al., 2018; Tjeng et al., 2019). The second involves training models that can be verified with less computational effort (Leino et al., 2021; Xiao et al., 2019) or provide inherent robustness bounds (Sinha et al., 2018). The third focuses on guaranteeing robustness under specific threat models (Han et al., 2021) or input distribution assumptions (Dan et al., 2020; Sinha et al., 2018). Since all these approaches have limitations that reduce their applicability (Silva & Najafirad, 2020), heuristic defenses tend to be more common in practice. Exact approaches can also be used to compute provably optimal adversarial examples (Carlini et al., 2017; Tjeng et al., 2019), although generating them requires a non-trivial amount of computational resources. Refer to Appendix M for a more in-depth overview of certified defenses.\\n\\nAnother line of research has focused on understanding the nature of robustness and adversarial attacks. Frameworks such as (Dreossi et al., 2019), (Pinot et al., 2019) and (Pydi & Jog, 2021) focused on formalizing the concept of adversarial robustness. Some studies have highlighted trade-offs between robustness (under specific definitions) and properties such as accuracy (Dobriban et al., 2020; Zhang et al., 2019), generalization (Min et al., 2021) and invariance (Tram\u00e8r et al., 2020). However, some of these results have been recently questioned, suggesting that these trade-offs might not be inherent in considered approaches (Yang et al., 2020; Zhang et al., 2020). Adversarial attacks have also been studied from the point of view of Bayesian learning to derive robustness bounds and provide insight into the role of uncertainty (Rawat et al., 2017; Richardson & Weiss, 2021; Vidot et al., 2021). Adversarial attacks have also been studied in the context of game theory (Ren et al., 2021), identifying Nash equilibria between attacker and defender (Pal & Vidal, 2020; Zhou et al., 2019).\\n\\nFinally, some works have also focused on the computational complexity of specific adversarial attacks and defenses. In particular, Mahloujifar & Mahmoody (2019) showed that there exist exact polynomial-time attacks against classifiers trained on product distributions. Similarly, Awasthi et al. (2019) showed that for degree-2 polynomial threshold functions there exists a polynomial-time algorithm that either proves that the model is robust or finds an adversarial example. Other works have also provided hardness results; Degwekar et al. (2019) showed that there exist certain classification tasks such that learning a robust model is as hard as solving the Learning Parity with Noise problem (which is NP-hard); Song et al. (2021) showed that learning a single periodic neuron over noisy isotropic Gaussian distributions in polynomial time would imply that the Shortest Vector Problem (conjectured to be NP-hard) can be solved in polynomial time. Finally, Garg et al. (2020) showed that, by requiring attackers to provide a valid cryptographic signature for inputs, it is possible to prevent attacks with limited computational resources from fooling the model in polynomial time.\\n\\nExtensive literature in the field of adversarial attacks suggests that generating adversarial examples is comparatively easier than building robust classifiers (Carlini & Wagner, 2016; 2017a; Croce et al., 2022; He et al., 2017; Hosseini et al., 2019; Tramer et al., 2020). In this section, we introduce some key definitions that we will employ to provide a theoretically grounded, potential, motivation for such...\"}"}
{"id": "FDlfFbnI7AR", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"discrepancy. We aim at capturing the key traits shared by most of the literature on adversarial attacks, so as to identify properties that are valid under broad assumptions.\\n\\nWe start by defining the concept of adversarial example, which intuitively represents a modification of a legitimate input that is so limited as to be inconsequential from a practical perspective, but classified erroneously by a target model. Formally, let $f : X \\\\rightarrow \\\\{1, \\\\ldots, N\\\\}$ be a discrete classifier.\\n\\nLet $B_p(x, \\\\varepsilon) = \\\\{x' \\\\in X | \\\\|x - x'\\\\|_p \\\\leq \\\\varepsilon\\\\}$ be a $L_p$ ball of radius $\\\\varepsilon$ and center $x$. Then we have:\\n\\n**Definition 1 (Adversarial Example).** Given an input $x$, a threshold $\\\\varepsilon$, and a $L_p$ norm $p$, an adversarial example is an input $x' \\\\in B_p(x, \\\\varepsilon)$ such that $f(x') \\\\in C(x)$, where $C(x) \\\\subseteq \\\\{1, \\\\ldots, N\\\\} \\\\setminus \\\\{f(x)\\\\}$.\\n\\nThis definition is a simplification compared to human perception, but it is adequate for a sufficiently small $\\\\varepsilon$, and it is adopted in most of the relevant literature. An adversarial attack can then be viewed as an optimization procedure that attempts to find an adversarial example. We define an adversarial attack for a classifier $f$ as a function $a_{f,p} : X \\\\rightarrow X$ that solves the following optimization problem:\\n\\n$$\\\\arg\\\\min_{x' \\\\in X} \\\\{\\\\|x' - x\\\\|_p | f(x') \\\\in C(x)\\\\}$$\\n\\n(1)\\n\\nThe attack is considered successful if the returned solution $x' = a_{f,p}(x)$ also satisfies $\\\\|x' - x\\\\|_p \\\\leq \\\\varepsilon$.\\n\\nWe say that an attack is exact if it solves Equation (1) to optimality; otherwise, we say that the attack is heuristic. An attack is said to be targeted if $C(x) = C_{t,y}(x) = \\\\{y'\\\\}$ with $y' \\\\neq f(x)$; it is instead untargeted if $C_u(x) = \\\\{1, \\\\ldots, N\\\\} \\\\setminus \\\\{f(x)\\\\}$.\\n\\nWe define the decision boundary distance $d^*_p(x)$ of a given input $x$ as the minimum $L_p$ distance between $x$ and another input $x'$ such that $f(x) \\\\neq f(x')$.\\n\\nNote that this is also the value of $\\\\|a_{f,p}(x) - x\\\\|_p$ for an exact, untargeted, attack.\\n\\nIntuitively, a classifier is robust w.r.t an example $x$ iff $x$ cannot be successfully attacked. Formally:\\n\\n**Definition 2 ((\\\\varepsilon, p)-Local Robustness).** A discrete classifier $f$ is $(\\\\varepsilon, p)$-locally robust w.r.t. an example $x \\\\in X$ iff $\\\\forall x' \\\\in B_p(x, \\\\varepsilon)$ we have $f(x') = f(x)$.\\n\\nWe then provide some additional definitions that are needed for our results, namely ReLU networks and FSFP spaces. ReLU networks are defined as follows:\\n\\n**Definition 3 (ReLU network).** A ReLU network is a composition of sum, multiplication by a constant, and ReLU activation, where ReLU: $\\\\mathbb{R} \\\\rightarrow \\\\mathbb{R}^+$ is defined as $\\\\text{ReLU}(x) = \\\\max(x, 0)$.\\n\\nNote that any hardness result for ReLU classifiers also applies to the more general class of classifiers.\\n\\nFixed-Size Fixed-Precision (FSFP) spaces, on the other hand, capture two common assumptions about real-world input spaces: all inputs can be represented with the same number of bits and there exists a positive minorant of the distance between inputs.\\n\\n**Definition 4 (Fixed-Size Fixed-Precision space).** Given a real $p > 0$, a space $X \\\\subseteq \\\\mathbb{R}^n$ is FSFP if there exists a $\\\\nu \\\\in \\\\mathbb{R}$ such that $\\\\forall x$. $|r(x')| \\\\leq \\\\nu$ (where $|r(x)|$ is the size of the representation of $x$) and there exists a $\\\\mu \\\\in \\\\mathbb{R}$ such that $\\\\mu > 0$ and $\\\\forall x, x' \\\\in X$. ($\\\\|x' - x\\\\|_p < \\\\mu \\\\Rightarrow x = x'$).\\n\\nExamples of FSFP spaces include most image encodings, as well as 32-bit and 64-bit IEE754 tensors. Examples of non-FSFP spaces include the set of all rational numbers in an interval. Similarly to ReLU networks, hardness results for FSFP spaces also apply to more general spaces.\\n\\n**4 A SYMMETRICAL SETTING**\\n\\nIn this section, we provide a theoretically sound result that is a viable explanation for why attacks seem to outperform defenses. The core of our analysis is proving that attacks are less computationally expensive than defenses in the worst case, unless the Polynomial Hierarchy collapses. Specifically, we prove that the decision version of attacking a ReLU classifier is NP-complete:\\n\\n**Theorem 1 (Untargeted $L_\\\\infty$ attacks against ReLU classifiers are NP-complete).**\\n\\nLet $U_{-ATT_\\\\infty}$ be the set of all tuples $\\\\langle x, \\\\varepsilon, f \\\\rangle$ such that:\\n\\n$$\\\\exists x' \\\\in B_p(x, \\\\varepsilon). f(x') \\\\neq f(x)$$\\n\\n(2)\\n\\nwhere $x \\\\in X$, $X$ is a FSFP space and $f$ is a ReLU classifier. Then $U_{-ATT_\\\\infty}$ is NP-complete.\\n\\n**Note:** The term \\\"norm\\\" for $0 < p < 1$ even if in such cases the $L_p$ function is not subadditive.\\n\\n**3** The proofs of all our theorems and corollaries can be found in the appendices.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nFigure 1: Distances of the nearest adversarial example found by the strong attack pool compared to those found by MIPVerify on MNIST A and CIFAR10 A with standard training. The black line represents the theoretical optimum. Note that no samples are below the black line.\\n\\nstrong parameter set, we find that the average $R^2$ across all settings is 0.992 \u00b10.004 for MNIST and 0.997\u00b10.003 for CIFAR10. The balanced parameter set performs similarly, achieving an $R^2$ of 0.990\u00b10.006 for MNIST and 0.998\u00b10.002 for CIFAR10. From these results, we conjecture that increasing the computational budget of heuristic attacks does not necessarily improve predictability, although further tests would be needed to confirm such a claim. Note that such a linear model can also be used as a buffer function for heuristic CA. Another (possibly more reliable) procedure would consist in using quantile fitting; results for this approach are reported in Appendix H.\\n\\nAttack Pool Ablation Study\\n\\nDue to the nontrivial computational requirements of running several attacks on the same input, we now study whether it is possible to drop some attacks from the pool without compromising its predictability. Specifically, we consider all possible pools of size $n$ (with a success rate of 100%) and pick the one with the highest average $R^2$ value over all architectures and training techniques. As show in Figure 2, adding attacks does increase predictability, although with diminishing returns. For example, the pool composed of the Basic Iterative Method, the Brendel & Bethge Attack and the Carlini & Wagner attack achieves on its own a $R^2$ value of 0.988\u00b10.004 for MNIST+strong, 0.986 \u00b10.005 for MNIST+balanced, 0.935 \u00b10.048 for CIFAR10+strong and 0.993\u00b10.003 for CIFAR10+balanced. Moreover, dropping both the Fast Gradient Sign Method and uniform noise leads to negligible ($\\\\ll 0.01$) absolute variations in the mean $R^2$. These findings suggest that, as far as consistency is concerned, the choice of attacks represents a more important factor than the number of attacks in a pool. Refer to Appendix J for a more in-depth overview of how different attack selections affect consistency and accuracy.\\n\\nEfficient Attacks\\n\\nWe then explore if it possible to increase the efficiency of attacks by optimizing for fast, rather than accurate, results. We pick three new parameter sets (namely Fast-100, Fast-1k and Fast-10k) designed to find the nearest adversarial examples within the respective number of calls to the model. We find that while Deepfool is not the strongest adversarial attack (see Appendix I), it provides adequate results in very few model calls. For details on these results see Appendix K.\\n\\nFooling the Heuristic Attack-Based CA\\n\\nAn open question from Section 5.2 is the empirical difficulty of fooling the version of CA based on heuristic attacks. Specifically, we carried on a limited experimentation by attempting to fool a CA-defended model. We used Deepfool Fast-1k as a heuristic attack for CA, then we built a proof-of-concept CCA implementation based on the PGD method, thus setting a baseline for attacks against CA. This variant uses a custom loss $L_{CCA}(x, y) = L_{PGD}(x, y) + \\\\lambda \\\\|x - af,p(x)\\\\|_p$, which rewards adversarial examples with over-estimated decision boundary distances. We then vary $\\\\lambda$ in order to test various trade-offs between the two terms. In order to estimate the gradient of the second term, we use Natural Evolution Strategies (Wierstra et al., 2014). As a sanity check, we also attack using uniform noise. Due to the high computational requirements of such an experiment (30-60 minutes and $\\\\sim 1.2M$ model calls).\"}"}
{"id": "FDlfFbnI7AR", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023.\\n\\nFigure 2: Best mean $R^2$ value in relation to the number of attacks in the pool.\\n\\nCalls per sample on the GTX 1060 machine), we only attack MNIST A Standard and CIFAR10 A Standard on 100 samples each. For comparison, running DeepFool on ten 250-element batches takes approximately 10 seconds. Overall, the attacks have a success rate of 0% - 3%. However, we find that increasing $\\\\varepsilon'$ (while keeping $\\\\varepsilon$ constant) increases the success rate, up to 100% for $\\\\varepsilon' = 10 \\\\cdot \\\\varepsilon$. This suggests that as the difference between $\\\\varepsilon'$ and $\\\\varepsilon$ grows, so does the feasibility of fooling CA, which is consistent with our analysis. More in-depth results of our experiments can be found in Appendix L.\\n\\nUG100 Dataset\\n\\nWe collect all the adversarial examples found by both MIPVerify and the heuristic attacks into a new dataset, which we name UG100. UG100 can be used to benchmark new adversarial attacks. Specifically, we can determine how strong an attack is by comparing it to both the theoretical optimum and heuristic attack pools. Another potential application involves studying factors that affect whether adversarial attacks perform sub-optimally.\\n\\nCONCLUSION\\n\\nWe proved that attacking is $NP$-complete in the worst case, while training a robust model is $\\\\Sigma_2^P$-complete, barring collapse of the Polynomial Hierarchy. We then showed how such a structural asymmetry can be sidestepped by adopting a different perspective on defense. This is exemplified by Counter-Attack, a technique that can identify non-robust points in $NP$ time. We showed that CA can provide robustness guarantees up to an arbitrary $\\\\varepsilon$. The CA approach naturally benefits from improvements in the field of adversarial attacks, and can be combined with other forms of defense. Due to its independence from the specific characteristics of the defended model, CA can also be applied to non-ML tools (e.g. signature-based malware detectors). We also believe that it should be possible to extend CA beyond classification. Finally, in an empirical evaluation we showed that heuristic attacks can provide an accurate and consistent approximation of the true decision boundary, which has implications for the viability of a heuristic version of CA. While our investigation is limited to small scale networks, we expect improvements in the field of NN verification will enable testing whether the observed results generalize to larger architectures.\\n\\nOverall, we hope that our contributions can provide broad benefits to the field of adversarial robustness by 1) highlighting a potential, structural, challenge; 2) pointing out how that can be sidestepped by a change in perspective; 3) showing a proof-of-concept defense based on this idea; 4) providing an experimentation and dataset to serve as a baseline and starting point.\\n\\nREPRODUCIBILITY\\n\\nWe provide all our code and data in the repository linked in Section 1. Additionally, we report the key reproducibility information in Section 6, while all the other information can be found in Appendices G and L. To ensure maximum reproducibility, we also used consistent seeds across all experiments (one for parameter tuning and one for actual experiments). We also made sure to only rely on tools that are either open-source or for which there are free academic licenses. Concerning theoretical results, we provide full proofs of all theorems and corollaries in the appendices. Finally, we used consistent seeds across all experiments (one for parameter tuning and one for actual experiments).\"}"}
{"id": "FDlfFbnI7AR", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023 for users with a slow internet connection, we also provide UG100 in JSON format (containing only the found adversarial distances).\\n\\nREFERENCES\\n\\nAnish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International Conference on Machine Learning, pp. 274\u2013283. PMLR, 2018.\\n\\nPranjal Awasthi, Abhratanu Dutta, and Aravindan Vijayaraghavan. On robustness to adversarial examples and polynomial optimization. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nWieland Brendel, Jonas Rauber, Matthias K\u00fcmmerer, Ivan Ustyuzhaninov, and Matthias Bethge. Accurate, reliable and fast robustness evaluation. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nTom B Brown, Dandelion Man\u00e9, Aurko Roy, Mart\u00edn Abadi, and Justin Gilmer. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.\\n\\nNicholas Carlini and David Wagner. Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311, 2016.\\n\\nNicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 3\u201314, 2017a.\\n\\nNicholas Carlini and David Wagner. Magnet and \u201cefficient defenses against adversarial attacks\u201d are not robust to adversarial examples. arXiv preprint arXiv:1711.08478, 2017b.\\n\\nNicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 39\u201357. IEEE, 2017c.\\n\\nNicholas Carlini, Guy Katz, Clark Barrett, and David L Dill. Provably minimally-distorted adversarial examples. arXiv preprint arXiv:1709.10207, 2017.\\n\\nNicholas Carlini, Florian Tramer, J Zico Kolter, et al. (certified!!) adversarial robustness for free! arXiv preprint arXiv:2206.10550, 2022.\\n\\nJeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In International Conference on Machine Learning, pp. 1310\u20131320. PMLR, 2019.\\n\\nFrancesco Croce, Sven Gowal, Thomas Brunner, Evan Shelhamer, Matthias Hein, and Taylan Cemgil. Evaluating the adversarial robustness of adaptive test-time defenses. arXiv preprint arXiv:2202.13711, 2022.\\n\\nChen Dan, Yuting Wei, and Pradeep Ravikumar. Sharp statistical guarantees for adversarially robust gaussian classification. In International Conference on Machine Learning, pp. 2345\u20132355. PMLR, 2020.\\n\\nAkshay Degwekar, Preetum Nakkiran, and Vinod Vaikuntanathan. Computational limitations in robust classification and win-win results. In Conference on Learning Theory, pp. 994\u20131028. PMLR, 2019.\\n\\nGavin Weiguang Ding, Luyu Wang, and Xiaomeng Jin. AdverTorch v0.1: An adversarial robustness toolbox based on pytorch. arXiv preprint arXiv:1902.07623, 2019.\\n\\nEdgar Dobriban, Hamed Hassani, David Hong, and Alexander Robey. Provable tradeoffs in adversarially robust classification. arXiv preprint arXiv:2006.05161, 2020.\\n\\nTommaso Dreossi, Shromona Ghosh, Alberto Sangiovanni-Vincentelli, and Sanjit A Seshia. A formalization of robustness for deep neural networks. arXiv preprint arXiv:1903.10033, 2019.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\nKrishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A Mann, and Pushmeet Kohli. A dual approach to scalable verification of deep networks. In UAI, volume 1, pp. 3, 2018.\\n\\nReuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.\\n\\nSanjam Garg, Somesh Jha, Saeed Mahloujifar, and Mahmoody Mohammad. Adversarially robust learning could leverage computational hardness. In Algorithmic Learning Theory, pp. 364\u2013385. PMLR, 2020.\\n\\nIan Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015. URL http://arxiv.org/abs/1412.6572.\\n\\nKathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017.\\n\\nGurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2022. URL https://www.gurobi.com.\\n\\nHusheng Han, Kaidi Xu, Xing Hu, Xiaobing Chen, Ling Liang, Zidong Du, Qi Guo, Yanzhi Wang, and Yunji Chen. Scalecert: Scalable certified defense against adversarial patches with sparse superficial layers. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nWarren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. Adversarial example defenses: ensembles of weak defenses are not strong. In Proceedings of the 11th USENIX Conference on Offensive Technologies, pp. 15\u201315, 2017.\\n\\nMatthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. Advances in Neural Information Processing Systems, 30, 2017.\\n\\nDan Hendrycks and Kevin Gimpel. Early methods for detecting adversarial images. In International Conference on Learning Representations (Workshop Track), 2017.\\n\\nHossein Hosseini, Sreeram Kannan, and Radha Poovendran. Are odds really odd? bypassing statistical detection of adversarial examples. arXiv preprint arXiv:1907.12138, 2019.\\n\\nRobin Jia, Aditi Raghunathan, Kerem G\u00f6ksel, and Percy Liang. Certified robustness to adversarial word substitutions. arXiv preprint arXiv:1909.00986, 2019.\\n\\nGuy Katz, Clark Barrett, David Dill, Kyle Julian, and Mykel Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. arXiv preprint arXiv:1702.01135, 2017.\\n\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nRoger Koenker and Gilbert Bassett Jr. Regression quantiles. Econometrica: journal of the Econometric Society, pp. 33\u201350, 1978.\\n\\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, 2009.\\n\\nAlexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. 2017.\\n\\nYann LeCun, Corinna Cortes, and Christopher J.C. Burges. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.\\n\\nKlas Leino, Zifan Wang, and Matt Fredrikson. Globally-robust neural networks. In International Conference on Machine Learning, pp. 6212\u20136222. PMLR, 2021.\\n\\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Ground truth labels of the UG100 dataset.\\n\\n(a) MNIST\\n\\n| Ground Truth Count % |\\n|----------------------|\\n| 0                    |\\n| 219                  |\\n| 9.77%                |\\n| 1                    |\\n| 228                  |\\n| 10.17%               |\\n| 2                    |\\n| 225                  |\\n| 10.04%               |\\n| 3                    |\\n| 225                  |\\n| 10.04%               |\\n| 4                    |\\n| 225                  |\\n| 10.04%               |\\n| 5                    |\\n| 220                  |\\n| 9.82%                |\\n| 6                    |\\n| 227                  |\\n| 10.13%               |\\n| 7                    |\\n| 221                  |\\n| 9.86%                |\\n| 8                    |\\n| 225                  |\\n| 10.04%               |\\n| 9                    |\\n| 226                  |\\n| 10.08%               |\\n\\n(b) CIFAR10\\n\\n| Ground Truth Count % |\\n|----------------------|\\n| Airplane             |\\n| 228                  |\\n| 10.05%               |\\n| Automobile           |\\n| 227                  |\\n| 10.00%               |\\n| Bird                 |\\n| 228                  |\\n| 10.05%               |\\n| Cat                  |\\n| 228                  |\\n| 10.05%               |\\n| Deer                 |\\n| 226                  |\\n| 9.96%                |\\n| Dog                  |\\n| 227                  |\\n| 10.00%               |\\n| Frog                 |\\n| 227                  |\\n| 10.00%               |\\n| Horse                |\\n| 227                  |\\n| 10.00%               |\\n| Ship                 |\\n| 225                  |\\n| 9.92%                |\\n| Truck                |\\n| 226                  |\\n| 9.96%                |\\n\\nThe buffer function in CA can be empirically calibrated so as to control the chance of false positives (i.e. inputs wrongly reported as not robust) and false negatives (i.e. non-robust inputs reported as being robust).\\n\\nGiven the strong correlation that we observed between the distance of heuristic adversarial examples and the true decision boundary distance, using a linear model for $\\\\alpha$ seems a reasonable choice. Under this assumption, the buffer value depends only on the distance between the original example and the adversarial one, i.e. on $d(x, a_f(\\\\theta(x)))$. This property allows us to rewrite the main check performed by CA as:\\n\\n$$||x - a_f(x)|| - b(x) = \\\\alpha_1 ||x - a_f(\\\\theta(x))|| + \\\\alpha_0 \\\\leq \\\\varepsilon$$\\n\\n(59)\\n\\nThe parameters $\\\\alpha_1, \\\\alpha_0$ can then be obtained via quantile regression (Koenker & Bassett Jr, 1978) by using the true decision boundary distance (i.e. $d^*(x)$) as a target. The approach provides a simple, interpretable mechanism to control how conservative the detection check should be: with a small quantile, CA will tend to underestimate the decision boundary distance, leading to fewer missed detections, but more false alarms; using a high quantile will lead to the opposite behavior.\\n\\nWe test this type of buffer using 5-fold cross-validation on each configuration. Specifically, we calibrate the model using 1%, 50% and 99% as quantiles. Tables 10 to 13 provide a comparison between the expected quantile and the average true quantile of each configuration on the validation folds. Additionally, we plot in Figures 3 to 8 the mean $F_1$ score in relation to the choice of $\\\\varepsilon$.\\n\\nWe outline the best attack pools by size in Tables 18 to 21. Additionally, we report the performance of pools composed of individual attacks in Tables 22 to 25. Finally, we detail the performance of dropping a specific attack in Tables 26 to 29.\\n\\nWe list the chosen parameter sets for Fast-100, Fast-1k and Fast-10k in Table 30. We plot the difference between the distance of the closest adversarial examples and the true decision boundary distance.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Attack                | Parameter Name | MNIST | CIFAR10 |\\n|-----------------------|----------------|-------|---------|\\n|                       | Attack Parameter |       |         |\\n| Brendel & Bethge      | Initial Attack  | Blended Noise |         |\\n|                       | Overshoot       | 1.1   |         |\\n|                       | LR Decay        | 0.75  | 0.75    |\\n|                       | LR Decay Every n Steps | 50   |         |\\n|                       | #Iterations      | 5k    | 200     |\\n|                       | Learning Rate   | 1e-3  | 1e-3    |\\n|                       | Momentum        | 0.8   |         |\\n|                       | Initial Directions | 1000 |         |\\n|                       | Init Steps      | 1000  |         |\\n|                       |                 |       |         |\\n| Carlini & Wagner      | Minimum $\\\\tau$  | 1e-5  |         |\\n|                       | Initial $\\\\tau$  | 1     |         |\\n|                       | $\\\\tau$ Factor   | 0.95  | 0.9    |\\n|                       | Initial Const   | 1e-5  | 2      |\\n|                       | Const Factor    | 2     |         |\\n|                       | Maximum Const   | 20    |         |\\n|                       | Reduce Const    | False |         |\\n|                       | Warm Start      | True  |         |\\n|                       | Abort Early     | True  |         |\\n|                       | #Iterations      | 1k    | 100     |\\n|                       | Learning Rate   | 1e-2  | 1e-2    |\\n|                       | Max Iterations  | 1k    | 100     |\\n|                       | $\\\\tau$ Check Every n Steps | 1  |         |\\n|                       | Const Check Every n Steps | 5 |         |\\n|                       | Iter. Check Every n Steps | Disabled | |\\n| Deepfool              | #Iterations      | 5k    |         |\\n|                       | Candidates      | 10    |         |\\n|                       | Overshoot       | 1e-5  |         |\\n| FGSM                  | Initial Search Factor | 0.75 |         |\\n|                       | Initial Search Steps | 30 |         |\\n|                       | Binary Search Steps | 20 |         |\\n|                       | Starting $\\\\epsilon$ | 1    |         |\\n| PGD                   | Initial Search Factor | 0.75 |         |\\n|                       | Initial Search Steps | 30 |         |\\n|                       | Binary Search Steps | 20 |         |\\n|                       | #Iterations      | 5k    | 200     |\\n|                       | Learning Rate   | 1e-4  | 1e-3    |\\n|                       | Random Initialization | True |         |\\n| Uniform Noise         | Initial Search Factor | 0.75 |         |\\n|                       | Initial Search Steps | 30 |         |\\n|                       | Binary Search Steps | 20 |         |\\n|                       | Runs            | 8k    | 200     |\"}"}
{"id": "FDlfFbnI7AR", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 7: Parameter set used to initialize MIPVerify for MNIST. All other parameters are identical to the strong MNIST attack parameter set.\\n\\n| Attack Name       | Parameter Name | Value |\\n|-------------------|----------------|-------|\\n| BIM               | #Iterations    | 5k    |\\n|                   | Learning Rate  | 1e-5  |\\n| Brendel & Bethge  | Learning Rate  | 1e-3  |\\n|                   | Tau Factor      | 0.99  |\\n| Carlini & Wagner  | #Iterations    | 5k    |\\n|                   | Learning Rate  | 1e-3  |\\n\\nTable 8: Parameters of MIPVerify.\\n\\n| Parameter Name | Value |\\n|----------------|-------|\\n| Exploration    |       |\\n| Absolute Tolerance | 1e-5 |\\n| Relative Tolerance  | 1e-10 |\\n| Threads         | 1     |\\n| Timeout (s)     | 120   |\\n| Tightening      |       |\\n| Absolute Tolerance | 1e-4 |\\n| Relative Tolerance  | 1e-10 |\\n| Timeout (s)     | 20    |\\n| Threads         | 1     |\\n\\nTable 9: MIPVerify bound tightness statistics.\\n\\n| Architecture | Training % Tight |\\n|--------------|------------------|\\n| MNIST A      |                  |\\n| Standard     | 95.40%           |\\n| Adversarial  | 99.60%           |\\n| ReLU         | 82.46%           |\\n| MNIST B      |                  |\\n| Standard     | 74.61%           |\\n| Adversarial  | 85.68%           |\\n| ReLU         | 75.55%           |\\n| MNIST C      |                  |\\n| Standard     | 86.21%           |\\n| Adversarial  | 97.28%           |\\n| ReLU         | 95.63%           |\\n| CIFAR10 A    |                  |\\n| Standard     | 81.18%           |\\n| Adversarial  | 82.50%           |\\n| ReLU         | 92.73%           |\\n| CIFAR10 B    |                  |\\n| Standard     | 56.32%           |\\n| Adversarial  | 58.88%           |\\n| ReLU         | 81.67%           |\\n| CIFAR10 C    |                  |\\n| Standard     | 100.00%          |\\n| Adversarial  | 100.00%          |\\n| ReLU         | 100.00%          |\"}"}
{"id": "FDlfFbnI7AR", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 10: Expected vs true quantile for MNIST strong with 5-fold cross validation.\\n\\n| Architecture | Training | Expected Quantile | True Quantile |\\n|--------------|----------|-------------------|---------------|\\n|              | A        | 1.00%             | 0.99\u00b11.02%    |\\n|              |          | 50.00%            | 49.93\u00b12.35%   |\\n|              |          | 99.00%            | 95.60\u00b13.77%   |\\n|              | Adversarial | 1.00%             | 1.11\u00b10.53%    |\\n|              |          | 50.00%            | 50.25\u00b11.58%   |\\n|              |          | 99.00%            | 89.84\u00b16.42%   |\\n|              | ReLU     | 1.00%             | 1.11\u00b10.45%    |\\n|              |          | 50.00%            | 50.02\u00b11.72%   |\\n|              |          | 99.00%            | 91.95\u00b15.64%   |\\n|              | B        | 1.00%             | 1.07\u00b10.48%    |\\n|              |          | 50.00%            | 49.80\u00b10.76%   |\\n|              |          | 99.00%            | 97.76\u00b10.71%   |\\n|              | Adversarial | 1.00%             | 1.22\u00b11.01%    |\\n|              |          | 50.00%            | 49.88\u00b14.63%   |\\n|              |          | 99.00%            | 98.10\u00b10.36%   |\\n|              | ReLU     | 1.00%             | 1.04\u00b10.77%    |\\n|              |          | 50.00%            | 49.98\u00b13.17%   |\\n|              |          | 99.00%            | 97.69\u00b11.41%   |\\n|              | C        | 1.00%             | 1.07\u00b10.37%    |\\n|              |          | 50.00%            | 50.17\u00b11.64%   |\\n|              |          | 99.00%            | 98.73\u00b10.42%   |\\n|              | Adversarial | 1.00%             | 1.05\u00b10.29%    |\\n|              |          | 50.00%            | 49.87\u00b13.58%   |\\n|              |          | 99.00%            | 99.00\u00b10.47%   |\\n|              | ReLU     | 1.00%             | 1.06\u00b10.67%    |\\n|              |          | 50.00%            | 50.02\u00b11.85%   |\\n|              |          | 99.00%            | 93.99\u00b13.51%   |\"}"}
{"id": "FDlfFbnI7AR", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Saeed Mahloujifar and Mohammad Mahmoody. Can adversarially robust learning leverage computational hardness? In Algorithmic Learning Theory, pp. 581\u2013609. PMLR, 2019.\\n\\nDongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 135\u2013147, 2017.\\n\\nYifei Min, Lin Chen, and Amin Karbasi. The curious case of adversarially robust models: More data can help, double descend, or hurt generalization. In Uncertainty in Artificial Intelligence, pp. 129\u2013139. PMLR, 2021.\\n\\nSeyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2574\u20132582, 2016.\\n\\nAmbar Pal and Ren\u00e9 Vidal. A game theoretic analysis of additive adversarial attacks and defenses. Advances in Neural Information Processing Systems, 33:1345\u20131355, 2020.\\n\\nNicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp. 506\u2013519, 2017.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024\u20138035. Curran Associates, Inc., 2019.\\n\\nRafael Pinot, Florian Yger, C\u00e9dric Gouy-Pailler, and Jamal Atif. A unified view on differential privacy and robustness to adversarial examples. arXiv preprint arXiv:1906.07982, 2019.\\n\\nMuni Sreenivas Pydi and Varun Jog. The many faces of adversarial risk. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nAditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018.\\n\\nJonas Rauber, Roland Zimmermann, Matthias Bethge, and Wieland Brendel. Foolbox native: Fast adversarial attacks to benchmark the robustness of machine learning models in pytorch, tensorflow, and jax. Journal of Open Source Software, 5(53):2607, 2020.\\n\\nAmbrish Rawat, Martin Wistuba, and Maria-Irina Nicolae. Adversarial phenomenon in the eyes of bayesian deep learning. arXiv preprint arXiv:1711.08244, 2017.\\n\\nJie Ren, Die Zhang, Yisen Wang, Lu Chen, Zhanpeng Zhou, Yiting Chen, Xu Cheng, Xin Wang, Meng Zhou, Jie Shi, et al. A unified game-theoretic interpretation of adversarial robustness. arXiv preprint arXiv:2111.03536, 2021.\\n\\nEitan Richardson and Yair Weiss. A bayes-optimal view on adversarial examples. Journal of Machine Learning Research, 22(221):1\u201328, 2021.\\n\\nKevin Roth, Yannic Kilcher, and Thomas Hofmann. The odds are odd: A statistical test for detecting adversarial examples. In International Conference on Machine Learning, pp. 5498\u20135507. PMLR, 2019.\\n\\nHadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J Zico Kolter. Denoised smoothing: A provable defense for pretrained classifiers. Advances in Neural Information Processing Systems, 33:21945\u201321957, 2020.\\n\\nSamuel Henrique Silva and Peyman Najafirad. Opportunities and challenges in deep learning adversarial robustness: A survey. arXiv preprint arXiv:2007.00753, 2020.\\n\\n12\"}"}
{"id": "FDlfFbnI7AR", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Aman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi. Certifying some distributional robustness with principled adversarial training. In International Conference on Learning Representations, 2018.\\n\\nMin Jae Song, Ilias Zadik, and Joan Bruna. On the cryptographic hardness of learning single periodic neurons. Advances in neural information processing systems, 34:29602\u201329615, 2021.\\n\\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\\n\\nVincent Tjeng, Kai Y Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed integer programming. In International Conference on Learning Representations, 2019.\\n\\nFlorian Tram\u00e8r, Jens Behrmann, Nicholas Carlini, Nicolas Papernot, and J\u00f6rn-Henrik Jacobsen. Fundamental tradeoffs between invariance and sensitivity to adversarial perturbations. In International Conference on Machine Learning, pp. 9561\u20139571. PMLR, 2020.\\n\\nFlorian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial example defenses. Advances in Neural Information Processing Systems, 33:1633\u20131645, 2020.\\n\\nGuillaume Vidot, Paul Viallard, Amaury Habrard, and Emilie Morvant. A pac-bayes analysis of adversarial robustness. arXiv preprint arXiv:2102.11069, 2021.\\n\\nLily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning, and Inderjit Dhillon. Towards fast computation of certified robustness for relu networks. In International Conference on Machine Learning, pp. 5276\u20135285. PMLR, 2018a.\\n\\nTsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. In International Conference on Learning Representations, 2018b.\\n\\nDaan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and J\u00fcrgen Schmidhuber. Natural evolution strategies. The Journal of Machine Learning Research, 15(1):949\u2013980, 2014.\\n\\nZuxuan Wu, Ser-Nam Lim, Larry S Davis, and Tom Goldstein. Making an invisibility cloak: Real world adversarial attacks on object detectors. In European Conference on Computer Vision, pp. 1\u201317. Springer, 2020.\\n\\nKai Y Xiao, Vincent Tjeng, Nur Muhammad Shafiullah, and Aleksander Madry. Training for faster adversarial robustness verification via inducing relu stability. In International Conference on Learning Representations, 2019.\\n\\nYao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R Salakhutdinov, and Kamalika Chaudhuri. A closer look at accuracy vs. robustness. In Advances in Neural Information Processing Systems, volume 33, pp. 8588\u20138601, 2020.\\n\\nBohang Zhang, Tianle Cai, Zhou Lu, Di He, and Liwei Wang. Towards certifying l-infinity robustness using neural networks with l-inf-dist neurons. In International Conference on Machine Learning, pp. 12368\u201312379. PMLR, 2021.\\n\\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International Conference on Machine Learning, pp. 7472\u20137482. PMLR, 2019.\\n\\nJingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankanhalli. Attacks which do not kill training make adversarial learning stronger. In International Conference on Machine Learning, pp. 11278\u201311287. PMLR, 2020.\\n\\nYan Zhou, Murat Kantarcioglu, and Bowei Xi. A survey of game theoretic approach for adversarial machine learning. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9(3):e1259, 2019.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 NOTATION\\nWe use $f_i$ to denote the $i$-th output of a network. We define $f_i$ as\\n\\n$$f_i(x) = \\\\text{arg max} \\\\{f_i(x)\\\\} \\\\quad (6)$$\\n\\nfor situations where multiple outputs are equal to the maximum, we use the class with the lowest index.\\n\\nA.2 ARITHMETIC\\nGiven two FSFP spaces $X$ and $X'$ with distance minorants $\\\\mu$ and $\\\\mu'$, we can compute new positive minorants after applying functions to the spaces as follows:\\n\\n- Sum of two vectors: $\\\\mu_X + \\\\mu_{X'} = \\\\min(\\\\mu, \\\\mu')$;\\n- Multiplication by a constant: $\\\\alpha \\\\mu_X = \\\\alpha \\\\mu$;\\n- ReLU: $\\\\mu_{\\\\text{ReLU}}(X) = \\\\mu$.\\n\\nSince it is possible to compute the distance minorant of a space transformed by any of these functions in polynomial time, it is also possible to compute the distance minorant of a space transformed by any composition of such functions in polynomial time.\\n\\nA.3 FUNCTIONS\\nWe now provide an overview of several functions that can be obtained by using linear combinations and ReLUs.\\n\\n\\\\textbf{max} Carlini et al. (2017) showed that we can implement the \\\\textbf{max} function using linear combinations and ReLUs as follows:\\n\\n$$\\\\text{max}(x, y) = \\\\text{ReLU}(x - y) + y \\\\quad (7)$$\\n\\nWe can also obtain an $n$-ary version of \\\\textbf{max} by chaining multiple instances together.\\n\\n\\\\textbf{step} If $X$ is a FSFP space, then the following scalar function:\\n\\n$$\\\\text{step}_0(x) = 1_{\\\\mu}(\\\\text{ReLU}(x) - \\\\text{ReLU}(x - \\\\mu)) \\\\quad (8)$$\\n\\nis such that $\\\\forall i. \\\\forall x \\\\in X, \\\\text{step}_0(x_i)$ is 0 for $x_i \\\\leq 0$ and 1 for $x_i > 0$.\\n\\nSimilarly, let $\\\\text{step}_1$ be defined as follows:\\n\\n$$\\\\text{step}_1(x) = 1_{\\\\mu}(\\\\text{ReLU}(x + \\\\mu) - \\\\text{ReLU}(x)) \\\\quad (9)$$\\n\\nNote that $\\\\forall i. \\\\forall x \\\\in X, \\\\text{step}_1(x_i) = 0$ for $x_i < 0$ and $\\\\text{step}_1(x_i) = 1$ for $x_i \\\\geq 0$.\\n\\n\\\\textbf{Boolean Functions}\\nWe then define the Boolean functions $\\\\text{not}$:\\n\\n$$\\\\{0, 1\\\\} \\\\rightarrow \\\\{0, 1\\\\},$$\\n\\nand $\\\\text{and}$:\\n\\n$$\\\\{0, 1\\\\}^2 \\\\rightarrow \\\\{0, 1\\\\},$$\\n\\n\\\\text{or}:\\n\\n$$\\\\{0, 1\\\\}^3 \\\\rightarrow \\\\{0, 1\\\\}$$\\n\\nas follows:\\n\\n$$\\\\text{not}(x) = 1 - x \\\\quad (10)$$\\n\\nand\\n\\n$$\\\\text{or}(x, y) = \\\\text{step}_1(x + y - 2) \\\\quad (11)$$\\n\\n$$\\\\text{or}(x, y) = \\\\text{step}_1(x + y) \\\\quad (12)$$\\n\\nif $\\\\text{if}(a, b, c) = \\\\text{or}(\\\\text{not}(a), b, c)$, and $\\\\text{if}(a, c)$.\\n\\nNote that we can obtain $n$-ary variants of $\\\\text{and}$ and $\\\\text{or}$ by chaining multiple instances together.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Given a set \\\\( z = \\\\{ \\\\{ z_{1,1}, 1, \\\\ldots, z_{1,3} \\\\}, \\\\ldots, \\\\{ z_{n,1}, z_{n,3} \\\\} \\\\} \\\\) of Boolean atoms (i.e. \\\\( z_{i,j}(x) = x_k \\\\lor \\\\neg x_k \\\\) for a certain \\\\( k \\\\)) defined on an \\\\( n \\\\)-long Boolean vector \\\\( x \\\\), \\\\( \\\\text{cnf}^3(z) \\\\) returns the following Boolean function:\\n\\n\\\\[\\n\\\\text{cnf}'_3(x) = \\\\bigwedge_{i=1}^n \\\\bigvee_{j=1}^3 z_{i,j}(x)\\n\\\\]\\n\\nWe refer to \\\\( z \\\\) as a 3CNF formula.\\n\\nSince \\\\( \\\\text{cnf}'_3 \\\\) only uses negation, conjunction and disjunction, it can be implemented using respectively \\\\( \\\\text{neg} \\\\), \\\\( \\\\text{and} \\\\) and \\\\( \\\\text{or} \\\\). Note that, given \\\\( z \\\\), we can build \\\\( \\\\text{cnf}'_3 \\\\) in polynomial time w.r.t. the size of \\\\( z \\\\).\\n\\nComparison Functions\\n\\nWe can use \\\\( \\\\text{step} \\\\), \\\\( \\\\text{step} \\\\), \\\\( \\\\text{neg} \\\\) to obtain comparison functions as follows:\\n\\n\\\\[\\n\\\\text{geq}(x, k) = \\\\text{step}_1(x - k)\\n\\\\]\\n\\n\\\\[\\n\\\\text{gt}(x, k) = \\\\text{step}_0(x, k)\\n\\\\]\\n\\n\\\\[\\n\\\\text{leq}(x, k) = \\\\neg\\\\text{gt}(x, k)\\n\\\\]\\n\\n\\\\[\\n\\\\text{lt}(x, k) = \\\\neg\\\\text{geq}(x, k)\\n\\\\]\\n\\n\\\\[\\n\\\\text{eq}(x, k) = \\\\text{and}(\\\\text{geq}(x, k), \\\\text{leq}(x, k))\\n\\\\]\\n\\nMoreover, we define \\\\( \\\\text{open}: \\\\mathbb{R}^3 \\\\rightarrow \\\\{0, 1\\\\} \\\\) as follows:\\n\\n\\\\[\\n\\\\text{open}(x, a, b) = \\\\text{and}(\\\\text{gt}(x, a), \\\\text{lt}(x, b))\\n\\\\]\\n\\n**Proof of Theorem 1**\\n\\n**B.1 \\\\( U^T \\\\in NP \\\\)**\\n\\nTo prove that \\\\( U^T \\\\in NP \\\\), we show that there exists a polynomial certificate for \\\\( U^T \\\\) that can be checked in polynomial time. The certificate is the value of \\\\( x' \\\\), which will have a representation of the same size as \\\\( x \\\\) (due to the FSFP space assumption) and can be checked by verifying:\\n\\n- \\\\( \\\\| x - x'\\\\|_\\\\infty \\\\leq \\\\varepsilon \\\\), which can be checked in linear time;\\n- \\\\( f_{\\\\theta}(x') \\\\neq f(x) \\\\), which can be checked in polynomial time.\\n\\n**B.2 \\\\( U^T \\\\) is NP-hard**\\n\\nWe will prove that \\\\( U^T \\\\) is NP-hard by showing that \\\\( \\\\text{3SAT} \\\\leq U^T \\\\).\\n\\nGiven a set of 3CNF clauses \\\\( z = \\\\{ \\\\{ z_{1,1}, z_{1,2}, z_{1,3} \\\\}, \\\\ldots, \\\\{ z_{m,1}, z_{m,2}, z_{m,3} \\\\} \\\\} \\\\) defined on \\\\( n \\\\) Boolean variables \\\\( x_1, \\\\ldots, x_n \\\\), we construct the following query \\\\( q(z) \\\\) for \\\\( U^T \\\\):\\n\\n\\\\[\\nq(z) = \\\\langle x(s), 1, f \\\\rangle\\n\\\\]\\n\\nwhere \\\\( x(s) = 1, \\\\ldots, 1 \\\\) is a vector with \\\\( n \\\\) elements. Verifying \\\\( q(z) \\\\in U^T \\\\) is equivalent to checking:\\n\\n\\\\[\\n\\\\exists x' \\\\in B^\\\\infty x s, 1. f(x') \\\\neq f(x(s))\\n\\\\]\\n\\nNote that \\\\( x \\\\in B^\\\\infty x s, 1 \\\\) is equivalent to \\\\( x \\\\in [0, 1]^n \\\\).\\n\\n**Truth Values**\\n\\nWe will encode the truth values of \\\\( \\\\hat{x} \\\\) as follows:\\n\\n\\\\[\\nx'_i \\\\in 0, 1 \\\\iff \\\\hat{x}_i = 0\\n\\\\]\\n\\n\\\\[\\nx'_i \\\\in 1, 1 \\\\iff \\\\hat{x}_i = 1\\n\\\\]\\n\\nWe can obtain the truth value of a scalar variable by using \\\\( \\\\text{isT}(x_i) = \\\\text{gt}(x_i, 1). \\\\text{bin}(x) = \\\\text{or}(\\\\text{isT}(x_1), \\\\ldots, \\\\text{isT}(x_n)) \\\\).\"}"}
{"id": "FDlfFbnI7AR", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 11: Expected vs true quantile for MNIST balanced with 5-fold cross validation.\\n\\n| Architecture | Training | Expected | True Quantile |\\n|--------------|----------|----------|---------------|\\n|              |          | 1.00%    | 1.30\u00b10.79%    |\\n|              |          | 50.00%   | 49.98\u00b13.10%   |\\n|              |          | 99.00%   | 93.99\u00b12.59%   |\\n| Standard     | Adversarial | 0.97\u00b10.40% | 1.17\u00b10.97%    |\\n|              |          | 50.12\u00b11.14% | 50.17\u00b14.54%   |\\n|              |          | 90.44\u00b11.90% | 98.69\u00b10.59%   |\\n| ReLU         |          | 1.02\u00b10.31% | 1.04\u00b10.49%    |\\n|              |          | 50.02\u00b11.05% | 50.34\u00b12.49%   |\\n|              |          | 95.10\u00b12.82% | 98.73\u00b10.53%   |\\n|              |          | 1.03\u00b10.36% | 1.10\u00b10.37%    |\\n|              |          | 49.98\u00b10.70% | 50.12\u00b14.15%   |\\n|              |          | 98.88\u00b10.45% | 99.00\u00b10.35%   |\\n|              |          | 98.88\u00b10.91% | 98.62\u00b10.50%   |\"}"}
{"id": "FDlfFbnI7AR", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 12: Expected vs true quantile for CIFAR10 strong with 5-fold cross validation.\\n\\n| Architecture | Training | 1.00%       | 50.00%      | 99.00%       |\\n|--------------|----------|-------------|-------------|--------------|\\n| Standard     |          | 1.00% 1.09\u00b10.86% | 50.00% 50.09\u00b11.84% | 99.00% 98.82\u00b10.63% |\\n| Adversarial  |          | 1.00% 1.05\u00b10.23% | 50.00% 49.86\u00b13.59% | 99.00% 98.90\u00b10.62% |\\n| ReLU         |          | 1.00% 0.97\u00b10.41% | 50.00% 49.93\u00b13.42% | 99.00% 97.66\u00b11.35% |\\n\\n| Architecture | Training | 1.00%       | 50.00%      | 99.00%       |\\n|--------------|----------|-------------|-------------|--------------|\\n| Standard     |          | 1.00% 0.98\u00b10.18% | 50.00% 49.91\u00b11.18% | 99.00% 98.84\u00b10.56% |\\n| Adversarial  |          | 1.00% 0.91\u00b10.48% | 50.00% 50.00\u00b13.58% | 99.00% 98.69\u00b10.72% |\\n| ReLU         |          | 1.00% 1.10\u00b10.72% | 50.00% 49.98\u00b12.21% | 99.00% 98.85\u00b10.61% |\\n\\n| Architecture | Training | 1.00%       | 50.00%      | 99.00%       |\\n|--------------|----------|-------------|-------------|--------------|\\n| Standard     |          | 1.00% 0.93\u00b10.60% | 50.00% 50.00\u00b11.86% | 99.00% 98.71\u00b10.71% |\\n| Adversarial  |          | 1.00% 1.09\u00b10.17% | 50.00% 50.14\u00b12.63% | 99.00% 98.27\u00b10.81% |\\n| ReLU         |          | 1.00% 1.01\u00b10.62% | 50.00% 50.02\u00b12.09% | 99.00% 96.17\u00b12.40% |\"}"}
{"id": "FDlfFbnI7AR", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 13: Expected vs true quantile for CIFAR10 balanced with 5-fold cross validation.\\n\\n| Architecture | Training | Expected Quantile | True Quantile |\\n|--------------|----------|-------------------|--------------|\\n|              |          | 1.00%             | 0.95\u00b10.61%   |\\n|              |          | 50.00%            | 50.32\u00b12.38%  |\\n|              |          | 99.00%            | 98.87\u00b10.59%  |\\n| Standard     |          | 1.00%             | 1.05\u00b10.23%   |\\n|              |          | 50.00%            | 50.23\u00b12.65%  |\\n|              |          | 99.00%            | 98.81\u00b10.96%  |\\n| Adversarial  |          | 1.00%             | 4.14\u00b15.32%   |\\n|              |          | 50.00%            | 50.37\u00b11.02%  |\\n|              |          | 99.00%            | 94.62\u00b12.87%  |\\n| ReLU         |          | 1.00%             | 1.07\u00b10.46%   |\\n|              |          | 50.00%            | 49.91\u00b12.78%  |\\n|              |          | 99.00%            | 98.93\u00b10.73%  |\\n| Standard     |          | 1.00%             | 1.13\u00b10.57%   |\\n|              |          | 50.00%            | 50.18\u00b12.05%  |\\n|              |          | 99.00%            | 98.82\u00b10.71%  |\\n| Adversarial  |          | 1.00%             | 1.23\u00b10.38%   |\\n|              |          | 50.00%            | 50.11\u00b10.38%  |\\n|              |          | 99.00%            | 98.77\u00b10.51%  |\\n| Standard     |          | 1.00%             | 0.98\u00b10.50%   |\\n|              |          | 50.00%            | 50.09\u00b12.21%  |\\n|              |          | 99.00%            | 98.85\u00b10.43%  |\\n| Adversarial  |          | 1.00%             | 1.09\u00b10.26%   |\\n|              |          | 50.00%            | 49.96\u00b12.72%  |\\n|              |          | 99.00%            | 98.86\u00b10.32%  |\\n| ReLU         |          | 1.00%             | 1.01\u00b10.36%   |\\n|              |          | 50.00%            | 49.93\u00b11.60%  |\\n|              |          | 99.00%            | 97.93\u00b10.63%  |\\n\\nTable 14: Performance of the strong attack set on MNIST.\\n\\n| Architecture | Training | Success Rate | Difference % Below 1/255 |\\n|--------------|----------|--------------|----------------------------|\\n| MNIST A      | Standard | 100.00%      | 1.51%                      |\\n|              | Adversarial | 100.00%     | 2.48%                      |\\n|              | ReLU      | 100.00%      | 2.14%                      |\\n| MNIST B      | Standard | 100.00%      | 3.38%                      |\\n|              | Adversarial | 100.00%    | 4.34%                      |\\n|              | ReLU      | 100.00%      | 4.80%                      |\\n| MNIST C      | Standard | 100.00%      | 4.52%                      |\\n|              | Adversarial | 100.00%    | 8.76%                      |\\n|              | ReLU      | 100.00%      | 4.84%                      |\"}"}
{"id": "FDlfFbnI7AR", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3: $F_1$ scores in relation to $\\\\epsilon$ for MNIST A for each considered percentile. For ease of visualization, we set the graph cutoff at $F_1 = 0.8$. We also mark $8/255$ (a common choice for $\\\\epsilon$) with a dotted line.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 48, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 19: Best pools of a given size by success rate and $R^2$ for MNIST balanced.\\n\\n| n  | Attacks           | Success Rate | Difference < 1/255 | $R^2$ |\\n|----|------------------|--------------|-------------------|-------|\\n| 1  | BIM              | 100.00\u00b10.00% | 11.72\u00b14.18%       | 0.965\u00b10.010 |\\n| 2  | BIM, B&B         | 100.00\u00b10.00% | 6.11\u00b12.28%        | 0.980\u00b10.007 |\\n| 3  | BIM, B&B, C&W    | 100.00\u00b10.00% | 5.29\u00b12.06%        | 0.986\u00b10.005 |\\n| 4  | BIM, B&B, C&W, DF| 100.00\u00b10.00% | 4.85\u00b12.10%        | 0.989\u00b10.005 |\\n| 5  | No FGSM, Uniform | 100.00\u00b10.00% | 4.65\u00b12.16%        | 0.990\u00b10.006 |\\n| 6  | No Uniform       | 100.00\u00b10.00% | 4.65\u00b12.16%        | 0.990\u00b10.006 |\\n| 7  | All              | 100.00\u00b10.00% | 4.65\u00b12.16%        | 0.990\u00b10.006 |\\n\\nTable 20: Best pools of a given size by success rate and $R^2$ for CIFAR10 strong.\\n\\n| n  | Attacks           | Success Rate | Difference < 1/255 | $R^2$ |\\n|----|------------------|--------------|-------------------|-------|\\n| 1  | DF               | 100.00\u00b10.00% | 6.11\u00b13.49%        | 0.989\u00b10.011 |\\n| 2  | DF, PGD          | 100.00\u00b10.00% | 4.71\u00b12.37%        | 0.995\u00b10.007 |\\n| 3  | C&W, DF, PGD     | 100.00\u00b10.00% | 2.54\u00b11.30%        | 0.996\u00b10.006 |\\n| 4  | B&B, C&W, DF, PGD| 100.00\u00b10.00% | 2.21\u00b11.16%        | 0.997\u00b10.003 |\\n| 5  | No FGSM, Uniform | 100.00\u00b10.00% | 2.21\u00b11.16%        | 0.997\u00b10.003 |\\n| 6  | No Uniform       | 100.00\u00b10.00% | 2.21\u00b11.16%        | 0.997\u00b10.003 |\\n| 7  | All              | 100.00\u00b10.00% | 2.21\u00b11.16%        | 0.997\u00b10.003 |\\n\\nTable 21: Best pools of a given size by success rate and $R^2$ for CIFAR10 balanced.\\n\\n| n  | Attacks           | Success Rate | Difference < 1/255 | $R^2$ |\\n|----|------------------|--------------|-------------------|-------|\\n| 1  | DF               | 100.00\u00b10.00% | 6.11\u00b13.49%        | 0.989\u00b10.011 |\\n| 2  | B&B, DF          | 100.00\u00b10.00% | 2.52\u00b11.51%        | 0.995\u00b10.004 |\\n| 3  | BIM, B&B, DF     | 100.00\u00b10.00% | 2.21\u00b11.25%        | 0.997\u00b10.002 |\\n| 4  | BIM, B&B, C&W, DF| 100.00\u00b10.00% | 2.06\u00b11.16%        | 0.998\u00b10.002 |\\n| 5  | No FGSM, Uniform | 100.00\u00b10.00% | 2.04\u00b11.13%        | 0.998\u00b10.002 |\\n| 6  | No FGSM          | 100.00\u00b10.00% | 2.04\u00b11.13%        | 0.998\u00b10.002 |\\n| 7  | All              | 100.00\u00b10.00% | 2.04\u00b11.13%        | 0.998\u00b10.002 |\\n\\nTable 22: Performance of individual attacks for MNIST strong.\\n\\n| Attack       | Success Rate | Difference < 1/255 | $R^2$ |\\n|--------------|--------------|-------------------|-------|\\n| BIM          | 100.00\u00b10.00% | 10.90\u00b14.42%       | 0.966\u00b10.012 |\\n| B&B          | 99.99\u00b10.01%  | 18.50\u00b17.09%       | 0.812\u00b10.044 |\\n| C&W          | 100.00\u00b10.00% | 17.52\u00b12.74%       | 0.910\u00b10.024 |\\n| Deepfool     | 100.00\u00b10.00% | 21.59\u00b17.73%       | 0.923\u00b10.027 |\\n| FGSM         | 99.72\u00b10.51%  | 44.43\u00b115.76%      | 0.761\u00b10.132 |\\n| PGD          | 100.00\u00b10.00% | 10.98\u00b14.41%       | 0.975\u00b10.010 |\\n| Uniform      | 99.52\u00b10.91%  | 414.47\u00b1140.54%    | 0.623\u00b10.138 |\\n\\nTable 23: Performance of individual attacks for MNIST balanced.\\n\\n| Attack       | Success Rate | Difference < 1/255 | $R^2$ |\\n|--------------|--------------|-------------------|-------|\\n| BIM          | 100.00\u00b10.00% | 11.72\u00b14.18%       | 0.965\u00b10.010 |\\n| B&B          | 99.99\u00b10.03%  | 18.65\u00b17.29%       | 0.812\u00b10.039 |\\n| C&W          | 100.00\u00b10.00% | 22.55\u00b13.83%       | 0.904\u00b10.025 |\\n| Deepfool     | 100.00\u00b10.00% | 21.59\u00b17.73%       | 0.923\u00b10.027 |\\n| FGSM         | 99.72\u00b10.51%  | 44.43\u00b115.76%      | 0.761\u00b10.132 |\\n| PGD          | 100.00\u00b10.00% | 16.23\u00b16.59%       | 0.905\u00b10.070 |\\n| Uniform      | 98.66\u00b11.90%  | 521.61\u00b1181.40%    | 0.484\u00b10.122 |\"}"}
{"id": "FDlfFbnI7AR", "page_num": 49, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 24: Performance of individual attacks for CIFAR10 strong.\\n\\n| Attack     | Success Rate Difference < 1/255 |\\n|------------|----------------------------------|\\n|            | R2                               |\\n| BIM        | 91.96\u00b17.40% 19.97\u00b15.95% 80.32\u00b112.97% 0.934\u00b10.041 |\\n| B&B        | 100.00\u00b10.00% 508.66\u00b1196.37% 42.74\u00b17.85% 0.174\u00b10.074 |\\n| C&W        | 99.98\u00b10.02% 10.67\u00b13.64% 90.09\u00b15.51% 0.926\u00b10.030 |\\n| Deepfool   | 100.00\u00b10.00% 6.11\u00b13.49% 95.06\u00b14.81% 0.989\u00b10.011 |\\n| FGSM       | 100.00\u00b10.00% 31.80\u00b111.12% 69.20\u00b117.72% 0.847\u00b10.123 |\\n| PGD        | 100.00\u00b10.00% 19.36\u00b15.99% 77.23\u00b115.89% 0.952\u00b10.027 |\\n| Uniform    | 99.99\u00b10.02% 1206.79\u00b1277.68% 2.48\u00b10.88% 0.910\u00b10.044 |\\n\\nTable 25: Performance of individual attacks for CIFAR10 balanced.\\n\\n| Attack     | Success Rate Difference < 1/255 |\\n|------------|----------------------------------|\\n|            | R2                               |\\n| BIM        | 100.00\u00b10.00% 19.23\u00b15.92% 77.33\u00b115.89% 0.954\u00b10.025 |\\n| B&B        | 100.00\u00b10.00% 50.64\u00b152.17% 81.20\u00b110.68% 0.615\u00b10.349 |\\n| C&W        | 99.89\u00b10.09% 17.44\u00b14.01% 84.82\u00b18.51% 0.923\u00b10.026 |\\n| Deepfool   | 100.00\u00b10.00% 6.11\u00b13.49% 95.06\u00b14.81% 0.989\u00b10.011 |\\n| FGSM       | 100.00\u00b10.00% 31.80\u00b111.12% 69.20\u00b117.72% 0.847\u00b10.123 |\\n| PGD        | 100.00\u00b10.00% 20.18\u00b16.56% 76.97\u00b116.07% 0.947\u00b10.031 |\\n| Uniform    | 99.85\u00b10.26% 1617.74\u00b1390.50% 1.80\u00b10.67% 0.853\u00b10.068 |\\n\\nTable 26: Performance of pools without a specific attack for MNIST strong.\\n\\n| Attack | Success Rate Difference < 1/255 |\\n|--------|----------------------------------|\\n|        | R2                               |\\n| None   | 100.00\u00b10.00% 4.09\u00b12.02% 79.81\u00b115.70% 0.992\u00b10.005 |\\n| BIM    | 100.00\u00b10.00% 4.35\u00b12.03% 79.02\u00b115.62% 0.991\u00b10.005 |\\n| B&B    | 100.00\u00b10.00% 6.76\u00b13.31% 64.46\u00b125.01% 0.990\u00b10.005 |\\n| C&W    | 100.00\u00b10.00% 4.65\u00b12.20% 77.70\u00b116.02% 0.989\u00b10.006 |\\n| Deepfool | 100.00\u00b10.00% 4.33\u00b11.97% 79.04\u00b115.75% 0.990\u00b10.004 |\\n| FGSM   | 100.00\u00b10.00% 4.09\u00b12.02% 79.81\u00b115.70% 0.992\u00b10.005 |\\n| PGD    | 100.00\u00b10.00% 4.26\u00b11.99% 79.36\u00b115.59% 0.991\u00b10.004 |\\n| Uniform | 100.00\u00b10.00% 4.09\u00b12.02% 79.81\u00b115.70% 0.992\u00b10.005 |\\n\\nTable 27: Performance of pools without a specific attack for MNIST balanced.\\n\\n| Attack | Success Rate Difference < 1/255 |\\n|--------|----------------------------------|\\n|        | R2                               |\\n| None   | 100.00\u00b10.00% 4.65\u00b12.16% 77.78\u00b116.08% 0.990\u00b10.006 |\\n| BIM    | 100.00\u00b10.00% 5.13\u00b12.27% 76.14\u00b115.98% 0.988\u00b10.007 |\\n| B&B    | 100.00\u00b10.00% 7.93\u00b13.69% 60.79\u00b125.99% 0.987\u00b10.006 |\\n| C&W    | 100.00\u00b10.00% 4.93\u00b12.22% 77.05\u00b115.96% 0.988\u00b10.006 |\\n| Deepfool | 100.00\u00b10.00% 5.03\u00b12.14% 76.34\u00b116.36% 0.988\u00b10.005 |\\n| FGSM   | 100.00\u00b10.00% 4.65\u00b12.16% 77.78\u00b116.08% 0.990\u00b10.006 |\\n| PGD    | 100.00\u00b10.00% 4.85\u00b12.10% 77.33\u00b115.85% 0.989\u00b10.005 |\\n| Uniform | 100.00\u00b10.00% 4.65\u00b12.16% 77.78\u00b116.08% 0.990\u00b10.006 |\"}"}
{"id": "FDlfFbnI7AR", "page_num": 50, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 28: Performance of pools without a specific attack for CIFAR10 strong.\\n\\n| Attack      | Success Rate | Success Rate Difference | Dropped Attack | $R^2$  |\\n|-------------|--------------|-------------------------|----------------|-------|\\n| None        | 100.00\u00b10.00% | 2.21\u00b11.16%              | 98.40\u00b11.63%    | 0.997\u00b10.003 |\\n| BIM         | 100.00\u00b10.00% | 2.21\u00b11.16%              | 98.40\u00b11.63%    | 0.997\u00b10.003 |\\n| B&B         | 100.00\u00b10.00% | 2.54\u00b11.30%              | 98.17\u00b12.00%    | 0.996\u00b10.006 |\\n| C&W         | 100.00\u00b10.00% | 3.83\u00b12.06%              | 96.84\u00b13.12%    | 0.996\u00b10.004 |\\n| Deepfool    | 100.00\u00b10.00% | 4.02\u00b11.19%              | 95.65\u00b13.10%    | 0.992\u00b10.005 |\\n| FGSM        | 100.00\u00b10.00% | 2.21\u00b11.16%              | 98.40\u00b11.63%    | 0.997\u00b10.003 |\\n| PGD         | 100.00\u00b10.00% | 2.50\u00b11.48%              | 98.11\u00b11.93%    | 0.995\u00b10.005 |\\n| Uniform     | 100.00\u00b10.00% | 2.21\u00b11.16%              | 98.40\u00b11.63%    | 0.997\u00b10.003 |\\n\\nTable 29: Performance of pools without a specific attack for CIFAR10 balanced.\\n\\n| Attack      | Success Rate | Success Rate Difference | Dropped Attack | $R^2$  |\\n|-------------|--------------|-------------------------|----------------|-------|\\n| None        | 100.00\u00b10.00% | 2.04\u00b11.13%              | 98.74\u00b11.29%    | 0.998\u00b10.002 |\\n| BIM         | 100.00\u00b10.00% | 2.07\u00b11.15%              | 98.72\u00b11.31%    | 0.998\u00b10.002 |\\n| B&B         | 100.00\u00b10.00% | 4.08\u00b11.95%              | 97.26\u00b12.70%    | 0.996\u00b10.006 |\\n| C&W         | 100.00\u00b10.00% | 2.18\u00b11.22%              | 98.54\u00b11.50%    | 0.997\u00b10.002 |\\n| Deepfool    | 100.00\u00b10.00% | 4.00\u00b10.99%              | 95.89\u00b13.13%    | 0.993\u00b10.003 |\\n| FGSM        | 100.00\u00b10.00% | 2.04\u00b11.13%              | 98.74\u00b11.29%    | 0.998\u00b10.002 |\\n| PGD         | 100.00\u00b10.00% | 2.06\u00b11.16%              | 98.73\u00b11.32%    | 0.998\u00b10.002 |\\n| Uniform     | 100.00\u00b10.00% | 2.04\u00b11.13%              | 98.74\u00b11.29%    | 0.998\u00b10.002 |\\n\\nWe report the parameters for the variant of our attack in Table 31, while we report its success rate in Table 32. We set $\\\\varepsilon = \\\\varepsilon'$ equal to \\\\{0.025, 0.05, 0.1\\\\} for MNIST and \\\\{2/255, 4/255, 8/255\\\\} for CIFAR10. Note that, since Deepfool is a deterministic attack, no measures against randomizations were taken.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 51, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 30: Parameters for the Fast-100, Fast-1k and Fast-10k sets.\\n\\n| Attack Parameter | Name | MNIST 100 | CIFAR10 1k | MNIST 10k |\\n|------------------|------|----------|-----------|----------|\\n| Initial Search Factor | BIM | N/A      | N/A       | N/A      |\\n| Initial Search Steps | BIM | N/A      | N/A       | N/A      |\\n| Binary Search Steps | BIM | 10       | 20        | 20       |\\n| Starting $\\\\varepsilon$ | BIM | 0.5      | 0.1       |          |\\n| #Iterations | BIM | 10       | 50        | 500      |\\n| Learning Rate | BIM | 0.1      | 0.01      | 1e-3     |\\n| #Iterations | Deepfool | 100      | 500       | 500      |\\n| Candidates | Deepfool | 10       |           |          |\\n| Overshoot | Deepfool | 0.1      | 1e-5      | 1e-5     |\\n| Loss Logits | FGSM | 0.75     | 0.5       |          |\\n| Initial Search Factor | FGSM | N/A      | 0.5       | 0.5      |\\n| Initial Search Steps | FGSM | 30       | 10        | 10       |\\n| Binary Search Steps | FGSM | 20       |           |          |\\n| Starting $\\\\varepsilon$ | FGSM | 1        | 0.1       | 0.1      |\\n| #Iterations | PGD | 10       | 50        | 500      |\\n| Learning Rate | PGD | 0.1      | 0.01      | 1e-3     |\\n| Random Initialization | PGD | True     | True      | True     |\\n| Uniform Initialization | PGD | True     | True      | True     |\\n| Initial Search Factor | Uniform Initialization | N/A      | 0.5       | 0.75     |\\n| Initial Search Steps | Uniform Initialization | N/A      | 10        | 30       |\\n| Binary Search Steps | Uniform Initialization | 10       | 10        | 10       |\\n| Starting $\\\\varepsilon$ | Uniform Initialization | 0.1      | 0.1       | 1        |\\n| Runs | Uniform Initialization | 200      | 500       | 200      |\\n\\nTable 31: Parameters for the variant of the PGD attack.\\n\\n| Parameter | Value |\\n|-----------|-------|\\n| $\\\\lambda$ | $\\\\{10^{-4}, 10^{-2}, 1, 10^{2}, 10^{4}\\\\}$ |\\n| NES Estimation Samples | 200 |\\n| NES Estimation $\\\\varepsilon$ | $10^{-4}$ |\\n| PGD Iterations | 400 |\\n| PGD $\\\\varepsilon$ | $10^{-3}$ |\\n\\nTable 32: Success rate of the pool composed of the anti-CA variant of PGD and uniform noise for the A architectures.\\n\\n| $\\\\varepsilon$ | $\\\\varepsilon'$ | Multiplier |\\n|---------------|---------------|------------|\\n| 0.025 | 0% | 1x |\\n| 0.05 | 23% | 2x |\\n| 0.1 | 95% | 5x |\\n| 0.25 | 100% | 10x |\\n\\n(a) MNIST A Standard\\n\\n| $\\\\varepsilon$ | $\\\\varepsilon'$ | Multiplier |\\n|---------------|---------------|------------|\\n| 2/255 | 0% | 1x |\\n| 4/255 | 34% | 2x |\\n| 8/255 | 96% | 5x |\\n| 16/255 | 100% | 10x |\\n\\n(b) CIFAR10 A Standard\"}"}
{"id": "FDlfFbnI7AR", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Definition of $f$\\n\\nWe define $f$ as follows:\\n\\n\\\\[ f_1(x) = \\\\text{not}(\\\\exists s \\\\in x(s)) \\\\]\\n\\n\\\\[ f_0(x) = \\\\text{not}(f_1(x)) \\\\]\\n\\nwhere $cnf' = cnf_3(z)$ and $isx(s)$ is defined as follows:\\n\\n\\\\[ isx(s)(x) = \\\\text{and}\\\\ eq(x_1, x_2, \\\\ldots, eq(x_n, x_{n+1})) \\\\]\\n\\nNote that $f$ is designed such that $f(x(s)) = 0$, while for $x' \\\\neq x(s)$, $f(x') = 1$ iff the formula $z$ is true for the variable assignment $bin(x')$.\\n\\nLemma 1.\\n\\n$z \\\\in 3\\\\text{SAT} \\\\Rightarrow q(z) \\\\in U_{\\\\text{ATT}}$\\\\text{\u221e}\\n\\nProof.\\n\\nLet $z \\\\in 3\\\\text{SAT}$. Therefore $\\\\exists x^* \\\\in \\\\{0, 1\\\\}^n$ such that $cnf_3(z)(x^*) = 1$. Since $bin(x^*) = x^*$ and $x^* \\\\neq x(s)$, $f(x^*) = 1$, which means that it is a valid solution for Equation (22). From this we can conclude that $q(z) \\\\in U_{\\\\text{ATT}}$\\\\text{\u221e}.\\n\\nLemma 2.\\n\\n$q(z) \\\\in U_{\\\\text{ATT}} \\\\Rightarrow z \\\\in 3\\\\text{SAT}$\\n\\nProof.\\n\\nSince $q(z) \\\\in U_{\\\\text{ATT}}$, $\\\\exists x^* \\\\in [0, 1]^n \\\\setminus \\\\{x(s)\\\\}$ that is a solution to Equation (22) (i.e. $f(x^*) = 1$). Then $cnf'(bin(x^*)) = 1$, which means that there exists a $\\\\hat{x}$ (i.e. $bin(x^*)$) such that $cnf'(\\\\hat{x}) = 1$. From this we can conclude that $z \\\\in 3\\\\text{SAT}$.\\n\\nSince:\\n\\n\u2022 $q(z)$ can be computed in polynomial time;\\n\u2022 $z \\\\in 3\\\\text{SAT} \\\\Rightarrow q(z) \\\\in U_{\\\\text{ATT}}$\\\\text{\u221e};\\n\u2022 $q(z) \\\\in U_{\\\\text{ATT}} \\\\Rightarrow z \\\\in 3\\\\text{SAT}$.\\n\\nwe can conclude that $3\\\\text{SAT} \\\\leq U_{\\\\text{ATT}}$\\\\text{\u221e}.\\n\\nB.3 Proof of Corollary 1.1\\n\\nB.3.1 $U_{\\\\text{ATT}} p \\\\in \\\\text{NP}$\\n\\nThe proof is identical to the one for $U_{\\\\text{ATT}}$\\\\text{\u221e}.\\n\\nB.3.2 $U_{\\\\text{ATT}} p$ is $\\\\text{NP}$-hard\\n\\nThe proof that $q(z) \\\\in U_{\\\\text{ATT}} \\\\Rightarrow z \\\\in 3\\\\text{SAT}$ is very similar to the one for $U_{\\\\text{ATT}}$\\\\text{\u221e}. Since $q(z) \\\\in U_{\\\\text{ATT}}$, we know that $\\\\exists x^* \\\\in B_p(x(s), \\\\varepsilon) \\\\setminus \\\\{x(s)\\\\}. f(x^*) = 1$, which means that there exists a $\\\\hat{x}$ (i.e. $bin(x^*)$) such that $cnf'(\\\\hat{x}) = 1$. From this we can conclude that $z \\\\in 3\\\\text{SAT}$.\\n\\nThe proof that $z \\\\in 3\\\\text{SAT} \\\\Rightarrow q(z) \\\\in U_{\\\\text{ATT}}$ is slightly different, due to the fact that since $x^* \\\\notin B_p(x(s))$, we need to use a different input to prove that $\\\\exists x' \\\\in B_p(x(s)) \\\\cdot f(x') = 1$.\\n\\nLet $0 < p < \\\\infty$. Given a positive integer $n$ and a real $0 < p < \\\\infty$, let $\\\\rho_{p,n}(r)$ be a positive minorant of the $L_\\\\infty$ norm of a vector on the $L_p$ sphere of radius $r$. For example, for $n = 2$, $p = 2$ and $r = 1$, any positive value less than or equal to $\\\\sqrt{2}$ is suitable. Note that, for $0 < p < \\\\infty$ and $n, r > 0$, $\\\\rho_{p,n}(r) < r$.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The proof is identical to the proof of Theorem 1 (for $p=\\\\infty$) and Corollary 1.1 (for $0 < p < \\\\infty$), with the exception of requiring $f(x') = 1$.\\n\\nThe proof that attacking a polynomial-time classifier is in $\\\\text{NP}$ is the same as that for Theorem 1. Attacking a polynomial-time classifier is $\\\\text{NP}$-hard due to the fact that the ReLU networks defined in the proof of Theorem 1 are polynomial-time classifiers. Since attacking a general polynomial-time classifier is a generalization of attacking a ReLU polynomial-time classifier, the problem is $\\\\text{NP}$-hard.\\n\\nProving that $U_{ATT}(f) \\\\in \\\\text{NP}$ means proving that it can be solved in polynomial time by a non-deterministic Turing machine with an oracle that can solve a problem in $A$. Since $Zf \\\\in A$, we can do so by picking a non-deterministic Turing machine with access to an oracle that solves $Zf$. We then generate non-deterministically the adversarial example and return the output of the oracle. Due to the FSFP assumption, we know that the size of this input is the same as the size of the starting point, which means that it can be generated non-deterministically in polynomial time. Therefore, $U_{ATT}(f) \\\\in \\\\text{NP}$.\\n\\nFollows directly from Theorem 2 and the definition of $\\\\Sigma_{Pn}$.\\n\\n$\\\\Pi_2^P$-SAT is the set of all $z$ such that:\\n\\n$$\\\\forall \\\\hat{x} \\\\exists \\\\hat{y}. R(\\\\hat{x}, \\\\hat{y})(29)$$\\n\\nwhere $R(\\\\hat{x}, \\\\hat{y}) = \\\\text{cnf}^3(z)(\\\\hat{x}_1, \\\\ldots, \\\\hat{x}_n, \\\\hat{y}_1, \\\\ldots, \\\\hat{y}_n)$.\\n\\nStockmeyer (1976) showed that $\\\\Pi_2^P$-SAT is $\\\\Pi_2^P$-complete. Therefore, $\\\\text{co}\\\\Pi_2^P$-SAT, which is defined as the set of all $z$ such that:\\n\\n$$\\\\exists \\\\hat{x} \\\\forall \\\\hat{y}. \\\\neg R(\\\\hat{x}, \\\\hat{y})(30)$$\\n\\nis $\\\\Sigma_2^P$-complete.\\n\\n$\\\\mathbb{P}_{\\\\text{L}}^{\\\\infty}$ belongs to $\\\\Sigma_2^P$ if there exists a problem $A \\\\in \\\\text{P}$ and a polynomial $q$ such that\\n\\n$$\\\\forall \\\\Gamma = \\\\langle x, \\\\epsilon, f_{\\\\theta}, v_f \\\\rangle: \\\\Gamma \\\\in \\\\mathbb{P}_{\\\\text{L}}^{\\\\infty} \\\\iff \\\\exists y. |y| \\\\leq q(|\\\\Gamma|) \\\\land (\\\\forall z. (|z| \\\\leq q(|\\\\Gamma|) = \\\\Rightarrow \\\\langle \\\\Gamma, y, z \\\\rangle \\\\in A)) (31)$$\\n\\nThis can be proven by setting $y = \\\\theta'$, $z = x'$ and $A$ as the set of triplets $\\\\langle \\\\Gamma, \\\\theta', x' \\\\rangle$ such that all of the following are true:\\n\\n\u2022 $v_f(\\\\theta') = 1$;\\n\u2022 $\\\\|x - x'\\\\|_{\\\\infty} \\\\leq \\\\epsilon$;\\n\u2022 $f_{\\\\theta}(x) = f_{\\\\theta}(x')$.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We will prove that $P_L^{\\\\text{ROB}}$ is $\\\\Sigma^P_2$-hard by showing that $\\\\text{co}\\\\Pi^P_3 \\\\leq P_L^{\\\\text{ROB}}$.\\n\\nLet $n^\\\\theta x$ be the length of $n^\\\\theta x$ and let $n^\\\\theta y$ be the length of $n^\\\\theta y$.\\n\\nGiven a set $z$ of 3CNF clauses, we construct the following query $q(z)$ for $P_L^{\\\\text{ROB}}$:\\n\\n$$q(z) = \\\\langle x^s, 1^2, f^\\\\theta, v^f \\\\rangle$$ (32)\\n\\nwhere $x^s(1^2, \\\\ldots, 1^2_1)$ is a vector with $n^\\\\theta y$ elements and $v^f(\\\\theta) = 1 \\\\iff \\\\theta \\\\in \\\\{0, 1\\\\}^{n^\\\\theta x}$.\\n\\nNote that $\\\\theta' \\\\in \\\\{0, 1\\\\}^{n^\\\\theta x}$ can be checked in polynomial time w.r.t. the size of the input.\\n\\nTruth Values\\n\\nWe will encode the truth values of $n^\\\\theta x$ as a set of binary parameters $\\\\theta'$, while we will encode the truth values of $n^\\\\theta y$ using $x'$ through the same technique mentioned in Appendix B.2.\\n\\nDefinition of $f^\\\\theta$\\n\\nWe define $f^\\\\theta$ as follows:\\n\\n- $f^\\\\theta, 1(x) = \\\\text{and}(\\\\text{~isx}(s), \\\\text{~cnf}(\\\\theta, x))$, where $\\\\text{~isx}(s)$ is defined over $\\\\theta$ and $\\\\text{~bin}(x)$ using the same technique mentioned in Appendix B.2 and $\\\\text{isx}(s) = \\\\text{and}(i = 1, \\\\ldots, n^{eq}(x_i, 1_2))$;\\n- $f^\\\\theta, 0(x) = \\\\text{~not}(f^\\\\theta, 1(x))$.\\n\\nNote that $f^\\\\theta(x^s(1^2, \\\\ldots, 1^2_1)) = 0$ for all choices of $\\\\theta$.\\n\\nAdditional, $f^\\\\theta$ is designed such that:\\n\\n- $\\\\forall x' \\\\in B^\\\\infty x^s(1^2, \\\\ldots, 1^2_1), x' \\\\in \\\\text{~isx}(s)$;\\n- $\\\\forall \\\\theta'$. $(v^f(\\\\theta') = 1 \\\\iff R(\\\\theta', \\\\text{~bin}(x')))$ (33).\\n\\nLemma 3.\\n\\n$z \\\\in \\\\text{co}\\\\Pi^P_3 \\\\leq q(z) \\\\in P_L^{\\\\text{ROB}}$.\\n\\nProof.\\n\\nSince $z \\\\in \\\\text{co}\\\\Pi^P_3$, there exists a Boolean vector $x^*$ such that $\\\\forall \\\\hat y$. $\\\\neg R(x^*, \\\\hat y)$.\\n\\nThen both of the following statements are true:\\n\\n- $v^f(x^*) = 1$, since $x^* \\\\in \\\\{0, 1\\\\}^{n^\\\\theta x}$;\\n- $\\\\forall x' \\\\in B^\\\\infty(x^s(1^2, \\\\ldots, 1^2_1), \\\\epsilon). f(x^*, x') = 0$, since $f(x^*, x') = 1 \\\\iff R(x^*, \\\\text{~bin}(x'))$;\\n\\nTherefore, $x^*$ is a valid solution for Equation (3) and thus $q(z) \\\\in P_L^{\\\\text{ROB}}$.\\n\\nLemma 4.\\n\\n$q(z) \\\\in P_L^{\\\\text{ROB}} \\\\rightarrow z \\\\in \\\\text{co}\\\\Pi^P_3$.\\n\\nProof.\\n\\nSince $q(z) \\\\in P_L^{\\\\text{ROB}}$, there exists a $\\\\theta^*$ such that:\\n\\n- $v^f(\\\\theta^*) = 1 \\\\land \\\\forall x' \\\\in B^\\\\infty(x^s(1^2, \\\\ldots, 1^2_1), \\\\epsilon). f(\\\\theta^*, x') = f(\\\\theta^*, x^s(1^2, \\\\ldots, 1^2_1))$ (34)\\n\\nNote that $\\\\theta^* \\\\in \\\\{0, 1\\\\}^{n^\\\\theta x}$, since $v^f(\\\\theta^*) = 1$. Moreover, $\\\\forall \\\\hat y$. $\\\\neg R(\\\\theta^*, \\\\hat y)$, since $\\\\text{~bin}(\\\\hat y) = \\\\hat y$ and $f(\\\\theta^*, \\\\hat y) = 1 \\\\iff R(\\\\theta^*, \\\\hat y)$.\\n\\nTherefore, $\\\\theta^*$ is a valid solution for Equation (30), which implies that $z \\\\in \\\\text{co}\\\\Pi^P_3$. Since:\\n\\n- $q(z)$ can be computed in polynomial time;\\n- $z \\\\in \\\\text{co}\\\\Pi^P_3 \\\\leq q(z) \\\\in P_L^{\\\\text{ROB}}$;\\n- $q(z) \\\\in P_L^{\\\\text{ROB}} \\\\leq z \\\\in \\\\text{co}\\\\Pi^P_3$.\\n\\nwe can conclude that $\\\\text{co}\\\\Pi^P_3 \\\\leq P_L^{\\\\text{ROB}}$.18\"}"}
{"id": "FDlfFbnI7AR", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The proof is identical to the one for PL-ROB\\\\(^\\\\infty\\\\).\\n\\nD.4.2\\n\\n\\\\(PL-ROB_p \\\\in \\\\Sigma_{PL-ROB} - \\\\text{Hard}\\\\)\\n\\nWe follow the same approach used in the proof for Corollary 1.1.\\n\\nProof of \\\\(q(z) \\\\in PL-ROB_p = \\\\Rightarrow z \\\\in co\\\\Pi_3^{SAT}\\\\)\\n\\nIf \\\\(q(z) \\\\in PL-ROB_p\\\\), it means that \\\\(\\\\exists \\\\theta^* v_f(\\\\theta^*) = 1 = \\\\Rightarrow \\\\forall x' \\\\in B_p x(s), f(x') = 0\\\\).\\n\\nThen \\\\(\\\\forall \\\\hat{y}\\\\), there exists a corresponding input \\\\(y^{**} \\\\in B_p x(s),_{2}^{1}\\\\) defined as follows:\\n\\n\\\\[\\ny^{**}_i = \\\\begin{cases} \\n1/2 - \\\\rho_{p,n} & \\\\text{if } \\\\hat{y}_i = 0 \\\\\\\\\\n1/2 + \\\\rho_{p,n} & \\\\text{if } \\\\hat{y}_i = 1\\n\\\\end{cases}\\n\\\\]\\n\\nsuch that \\\\(e(y)(y^{**}) = \\\\hat{y}.\\\\) Since \\\\(y^{**} \\\\in B_p x(s),_{2}^{1}, cnf''_3(\\\\theta^*, \\\\text{bin}(y^{**})) = 0\\\\), which means that \\\\(R(\\\\theta^*, \\\\hat{y})\\\\) is false. In other words, \\\\(\\\\exists \\\\theta^* \\\\forall \\\\hat{y}. \\\\neg R(\\\\theta^*, \\\\hat{y})\\\\), i.e. \\\\(z \\\\in co\\\\Pi_3^{SAT}\\\\).\\n\\nProof of \\\\(z \\\\in co\\\\Pi_3^{SAT} = \\\\Rightarrow q(z) \\\\in PL-ROB_p\\\\)\\n\\nThe proof is very similar to the corresponding one for Theorem 3.\\n\\nIf \\\\(z \\\\in co\\\\Pi_3^{SAT}\\\\), then \\\\(\\\\exists \\\\hat{x} \\\\forall \\\\hat{y}. \\\\neg R(\\\\hat{x}, \\\\hat{y})\\\\). Set \\\\(\\\\theta^* = \\\\hat{x}\\\\). We know that \\\\(f^*_{\\\\theta^*}(x(s)) = 0\\\\). We also know that \\\\(\\\\forall x' \\\\in B_p x(s),_{2}^{1}, \\\\{x(s)\\\\} f_{\\\\theta^*}(x') = 1 \\\\iff cnf'_3(\\\\theta^*, x') = 1\\\\)\\n\\nIn other words, \\\\(\\\\forall x' \\\\in B_p x(s),_{2}^{1}, \\\\{x(s)\\\\} f_{\\\\theta^*}(x') = 1 \\\\iff R(\\\\theta^*, \\\\text{bin}(x'))\\\\).\\n\\nSince \\\\(R(\\\\theta^*, \\\\hat{y})\\\\) is false for all choices of \\\\(\\\\hat{y}\\\\), \\\\(\\\\forall x' \\\\in B_p x(s),_{2}^{1}, f_{\\\\theta^*}(x') = 0\\\\). Given the fact that \\\\(f^*_{\\\\theta^*}(x(s)) = 0\\\\), we can conclude that \\\\(\\\\theta^*\\\\) satisfies Equation (3).\\n\\nD.5\\n\\nProof of Corollary 3.2\\n\\nSimilarly to the proof of Corollary 1.3, it follows from the fact that ReLU classifiers are polynomial-time classifiers (w.r.t. the size of the tuple).\\n\\nE\\n\\nProof of Theorem 4\\n\\nThere are two cases:\\n\\n\u2022 \\\\(\\\\forall x' \\\\in B_p x(\\\\varepsilon) f(x') = f(x):\\\\) then the attack fails because \\\\(f(x) \\\\notin C(x);\\\\)\\n\\n\u2022 \\\\(\\\\exists x' \\\\in B_p x(\\\\varepsilon) f(x') \\\\neq f(x):\\\\) then due to the symmetry of the \\\\(L_p\\\\) norm \\\\(x \\\\in B_p x', \\\\varepsilon\\\\). Since \\\\(f(x) \\\\neq f(x')\\\\), \\\\(x\\\\) is a valid adversarial example for \\\\(x'\\\\), which means that \\\\(f(x') = \\\\star\\\\).\\n\\nSince \\\\(\\\\star \\\\notin C(x),\\\\) the attack fails.\\n\\nE.1\\n\\nProof of Corollary 4.1\\n\\nAssume that \\\\(\\\\forall x. ||x||_r \\\\geq \\\\eta ||x||_p\\\\) and fix \\\\(x(s) \\\\in X\\\\). Let \\\\(x' \\\\in B_r(x(s), \\\\eta\\\\varepsilon)\\\\) be an adversarial example.\\n\\nThen \\\\(||x' - x(s)||_r \\\\leq \\\\eta\\\\varepsilon\\\\), and thus \\\\(\\\\eta ||x' - x(s)||_p \\\\leq \\\\eta\\\\varepsilon\\\\). Dividing by \\\\(\\\\eta\\\\), we get \\\\(||x' - x(s)||_p \\\\leq \\\\varepsilon\\\\), which means that \\\\(x(s)\\\\) is a valid adversarial example for \\\\(x'\\\\) and thus \\\\(x'\\\\) is rejected by \\\\(p\\\\)-CA.\\n\\nWe now proceed to find the values of \\\\(\\\\eta\\\\).\\n\\nE.1.1\\n\\n\\\\(1 \\\\leq r < p\\\\)\\n\\nWe will prove that \\\\(||x||_r \\\\geq ||x||_p\\\\).\"}"}
{"id": "FDlfFbnI7AR", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"| \u03b5  | Mean F1 | 1%  | 50% | 99% |\\n|----|---------|-----|-----|-----|\\n| 0  | 0.8     |     |     |     |\\n| 0.5| 0.85    |     |     |     |\\n| 1  | 0.9     |     |     |     |\\n| 0.5| 0.95    |     |     |     |\\n| 1  | 1       |     |     |     |\\n\\n(a) MNIST B Standard Strong\\n(b) MNIST B Standard Balanced\\n(c) MNIST B Adversarial Strong\\n(d) MNIST B Adversarial Balanced\\n(e) MNIST B ReLU Strong\\n(f) MNIST B ReLU Balanced\\n\\nFigure 4: F1 scores in relation to \u03b5 for MNIST B for each considered percentile. For ease of visualization, we set the graph cutoff at F1 = 0.8. We also mark $\\\\frac{8}{255}$ (a common choice for \u03b5) with a dotted line.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"|                 | F\u2081 scores |                  |                  |\\n|-----------------|-----------|------------------|------------------|\\n| (a) MNIST C     | Standard  | Strong           | Balanced         |\\n| (b) MNIST C     | Standard  | Balanced         | Adversarial      |\\n| (c) MNIST C     | Adversarial| Strong           | Balanced         |\\n| (d) MNIST C     | ReLU      | Strong           | Balanced         |\\n\\nFigure 5: F\u2081 scores in relation to \u03b5 for MNIST C for each considered percentile. For ease of visualization, we set the graph cutoff at F\u2081 = 0.8. We also mark 8/255 (a common choice for \u03b5) with a dotted line.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 6: \\\\(F_1\\\\) scores in relation to \\\\(\\\\varepsilon\\\\) for CIFAR10 A for each considered percentile. For ease of visualization, we set the graph cutoff at \\\\(F_1 = 0.8\\\\). We also mark \\\\(8/255\\\\) (a common choice for \\\\(\\\\varepsilon\\\\)) with a dotted line.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 7: $F_1$ scores in relation to $\\\\varepsilon$ for CIFAR10 B for each considered percentile. For ease of visualization, we set the graph cutoff at $F_1 = 0.8$. We also mark 8/255 (a common choice for $\\\\varepsilon$) with a dotted line.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Corollary 1.1. For every $0 < p \\\\leq \\\\infty$, $U_{\\\\text{AT T}}^p$ is $\\\\mathsf{NP}$-complete.\\n\\nCorollary 1.2. Targeted $L^p$ attacks (for $0 < p \\\\leq \\\\infty$) against ReLU classifiers are $\\\\mathsf{NP}$-complete.\\n\\nCorollary 1.3. Theorem 1 holds even if we consider the more general set of polynomial-time classifiers w.r.t. the size of the tuple.\\n\\nTheorem 1 represents a minor generalization of existing results in the literature (Katz et al., 2017). However, together with the following more general bound for non-polynomial-time classifiers, it lays the groundwork for our main result.\\n\\nTheorem 2. Let $A$ be a complexity class, let $f$ be a classifier, let $Z_f = \\\\{\\\\langle x, y \\\\rangle | y = f(x), x \\\\in X\\\\}$ and let $U_{\\\\text{AT T}}^p(f) = \\\\{\\\\langle x, \\\\varepsilon, g \\\\rangle \\\\in U_{\\\\text{AT T}}^p | g = f \\\\}$, where $U_{\\\\text{AT T}}^p$ is the same as $U_{\\\\text{AT T}}^p$ but without the ReLU classifier restriction. If $Z_f \\\\in A$, then for every $0 < p \\\\leq \\\\infty$, $U_{\\\\text{AT T}}^p(f) \\\\in \\\\mathsf{NP}^A$.\\n\\nCorollary 2.1. For every $0 < p \\\\leq \\\\infty$, if $Z_f \\\\in \\\\Sigma^P_\\\\infty$, then $U_{\\\\text{AT T}}^p(f) \\\\in \\\\Sigma^P_{\\\\infty+1}$.\\n\\nThe latter result implies that, if $Z_f \\\\in \\\\mathsf{P}$, then $U_{\\\\text{AT T}}^p(f) \\\\in \\\\mathsf{NP}$. Informally, Corollary 2.1 establishes that, under broad assumptions, evaluating and attacking a classifier are in complexity classes that are strongly conjectured to be distinct, with the attack problem being the harder one.\\n\\nThe decision version of training a robust model is even more complex (again in the worst case):\\n\\nTheorem 3 (Finding a set of parameters that make a ReLU network $(\\\\varepsilon, p)$-locally robust on an input is $\\\\Sigma^P_2$-complete).\\n\\nLet $P_L - \\\\text{ROB}^p$ be the set of tuples $\\\\langle x, \\\\varepsilon, f_{\\\\theta}, v \\\\rangle$ such that:\\n\\n$\\\\exists \\\\theta'. (v f(\\\\theta') = 1 = \\\\Rightarrow \\\\forall x' \\\\in B^p(x, \\\\varepsilon). f_{\\\\theta'}(x') = f_{\\\\theta'}(x)) (3)$\\n\\nwhere $x \\\\in X$, $X$ is a FSFP space and $v f$ is a polynomial-time function that is 1 iff the input is a valid parameter set for $f$. Then $P_L - \\\\text{ROB}^\\\\infty$ is $\\\\Sigma^P_2$-complete.\\n\\nCorollary 3.1. $P_L - \\\\text{ROB}^p$ is $\\\\Sigma^P_2$-complete for all $0 < p \\\\leq \\\\infty$.\\n\\nCorollary 3.2. Theorem 3 holds even if, instead of ReLU classifiers, we consider the more general set of polynomial-time classifiers w.r.t. the size of the tuple.\\n\\nOur results rely on worst-case constructions and assume that the Polynomial Hierarchy does not collapse; moreover, both $\\\\mathsf{NP}$ and $\\\\Sigma^P_2$ can be solved via super-polynomial algorithms. That said, we believe our theorems to have a strong practical relevance. First, the Polynomial Hierarchy collapse is strongly conjectured to be false; second, even super-polynomial algorithms can have dramatically different run times (e.g. SAT vs Quantified Boolean Formula solvers). Finally, generic classifiers can learn (and are known to learn) complex input-output mappings with many local optima. Intuitively, this is the core of the scenario captured by our worst-case construction, and also what makes Equation (1) difficult to solve. Again intuitively, robustness requires solving a nested optimization problem with universal quantification (since we need to guarantee the same prediction on all neighboring points), thus motivating the higher complexity class. For this reason, we think our results provide a plausible explanation for the hardness gap that is routinely observed in the relevant literature. Of course, there are definitely sub-cases where the problem is simple enough for exact attacks to run in polynomial time (e.g. (Awasthi et al., 2019)); this suggests that, under specific circumstances, guaranteed robustness could be achieved at reasonable effort. By this argument, our proof also provides additional motivation for research on tractable classes of robust classifiers.\\n\\nAdditional Sources of Asymmetry\\n\\nThere are additional, complementary, factors that may provide an advantage to the attacker. We review them informally, since they can support efforts to build more robust defenses. First, the attacker can gather information about the target model, e.g. by using genuine queries (Papernot et al., 2017), while the defender has not such advantage. As a result, the defender often needs to either make assumptions about adversarial examples (Hendrycks & Gimpel, 2017; Roth et al., 2019) or train models to identify common properties (Feinman et al., 2017; Grosse et al., 2017). These assumptions can be exploited, such as in the case of Carlini & Wagner (2017a), who generated adversarial examples that did not have the expected properties. Second, the attacker can focus on one input at the time, while the defender has to guarantee robustness on a large subset of the input space. This weakness can be exploited: for example, MagNet (Meng & Chen, 2017) relies on a model of the entire genuine distribution, which can be sometimes inaccurate. Carlini & Wagner (2017b) broke MagNet by searching for examples that were both classified differently and mistakenly considered genuine. Finally, defenses cannot significantly compromise the accuracy of a model. Adversarial training, for example, often reduces the clean accuracy of the model (Madry et al., 2018), leading to a trade-off between accuracy and robustness.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The limitations imposed by Theorem 3 cannot be addressed directly in the general case (barring collapse of the Polynomial Hierarchy). However, they can be sidestepped by changing perspective: we exemplify this by introducing an alternative approach to provide robust classification, which also allows us to take advantage of existing defenses. Instead of obtaining a robust model from scratch, we propose to evaluate the robustness of the classifier on a case-by-case basis, flagging the input if a robust answer cannot be provided. Specifically, given a norm-order $p$ and threshold $\\\\varepsilon$, we propose to:\\n\\n- Design a model that is as robust as possible using available and practically viable defenses;\\n- For every input received, determine if the model is $(\\\\varepsilon, p)$-locally robust on the input by running an adversarial attack on the input;\\n- If the attack succeeds, flag the input.\\n\\nWe name this technique Counter-Attack (CA). Instead of attempting to build a robust model, CA ensures that answers from a partially robust model are flagged as unreliable when they could be the result of an attack. This approach, while very simple, can take advantage of existing defenses, provides robustness guarantees, and is considerably hard to fool, as we will later prove.\\n\\nThe behavior in case an input is flagged depends on the context. Examples include relying on a slower but more robust model (e.g. a human), or rejecting the input altogether. This kind of approach is viable in all cases where the goal is to support (rather than replace) human decision-making.\\n\\nNote that the flagging rate of CA is heavily dependent on the robustness of the model: a model that is robust on the entire input distribution will have a flagging rate of zero. Therefore, any improvement in the field of adversarial defenses also decreases the flagging rate of CA. Moreover, if there are known robustness bounds, they can be exploited to simplify the attack: for example, if the model is known to be $(\\\\varepsilon_{\\\\text{cert}}, p)$-robust on $x$, with $\\\\varepsilon_{\\\\text{cert}} < \\\\varepsilon$, the attack can focus on searching adversarial examples in $B_p(x, \\\\varepsilon_{\\\\text{cert}}) \\\\setminus B_p(x, \\\\varepsilon)$.\\n\\nAt the same time, developing stronger and faster attacks also benefits CA, since better attacks can find adversarial examples more quickly.\\n\\nThe major drawback of CA is that it requires running an exact adversarial attack on every input. We will investigate a possible mitigation for this phenomenon based on employing heuristic attacks, which still provide a significant degree of robustness (see Section 6). Finally, we stress that CA is just one of potentially several alternative paradigms that could circumvent the computational asymmetry. We hope our contribution will encourage other researchers to investigate this direction.\\n\\n### 5.1 Formal Properties\\n\\nWhen used with an exact attack, CA provides formal robustness guarantees for an arbitrary $p$ and $\\\\varepsilon$:\\n\\n**Theorem 4.** Let $0 < p \\\\leq \\\\infty$ and let $\\\\varepsilon > 0$. Let $f : X \\\\rightarrow \\\\{1, \\\\ldots, N\\\\}$ be a classifier and let $a$ be an exact attack. Let $f_{CA} : X \\\\rightarrow \\\\{1, \\\\ldots, N\\\\} \\\\cup \\\\{\\\\star\\\\}$ be defined as:\\n\\n$$f_{CA}(x) = \\\\begin{cases} f(x) & \\\\|a_{f,p}(x) - x\\\\|_p > \\\\varepsilon \\\\\\\\ \\\\star & \\\\text{otherwise} \\\\end{cases}$$\\n\\nThen $\\\\forall x \\\\in X$ an $L_p$ attack on $x$ with radius greater than or equal to $\\\\varepsilon$ and with $\\\\star \\\\not\\\\in C(x)$ fails.\\n\\nThe notation $f_{CA}(x)$ refers to the classifier $f$ combined with CA, relying on attack $a$. The condition $\\\\star \\\\not\\\\in C(x)$ requires that the input generated by the attack should not be flagged by CA.\\n\\n**Corollary 4.1.** Let $1 \\\\leq p \\\\leq \\\\infty$ and let $\\\\varepsilon > 0$. Let $f$ be a classifier on inputs with $n$ elements that uses CA with norm $p$ and radius $\\\\varepsilon$. Then for all inputs and for all $1 \\\\leq r < p$, $L_r$ attacks of radius greater than or equal to $\\\\varepsilon$ and with $\\\\star \\\\not\\\\in C(x)$ will fail. Similarly, for all inputs and for all $r > p$, $L_r$ attacks of radius greater than or equal to $\\\\frac{1}{r-1} \\\\frac{1}{p} \\\\varepsilon$ and with $\\\\star \\\\not\\\\in C(x)$ will fail (treating $1^\\\\infty$ as 0).\"}"}
{"id": "FDlfFbnI7AR", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023.\\n\\nFormalize the task of attacking CA (referred to as Counter-CA, or CCA). This involves finding, given a starting point \\\\( x \\\\), an input \\\\( x' \\\\in B^p(x, \\\\varepsilon') \\\\) that is adversarial but not flagged by CA, i.e., such that \\\\( f(x') \\\\in C(x) \\\\wedge \\\\forall x'' \\\\in B^p(x', \\\\varepsilon).f(x'') = f(x') \\\\). Note that, for \\\\( \\\\varepsilon' \\\\leq \\\\varepsilon \\\\), no solution exists, since \\\\( x \\\\in B^p(x', \\\\varepsilon) \\\\) and \\\\( f(x) \\\\neq f(x') \\\\).\\n\\nTheorem 5 (Attacking CA with a higher radius is \\\\( \\\\Sigma^P_2 \\\\)-complete).\\n\\nLet \\\\( CCA^p \\\\) be the set of all tuples \\\\( \\\\langle x, \\\\varepsilon, \\\\varepsilon', C, f \\\\rangle \\\\) such that:\\n\\n\\\\[\\n\\\\exists x' \\\\in B^p(x, \\\\varepsilon').\\\\left(f(x') \\\\in C(x) \\\\wedge \\\\forall x'' \\\\in B^p(x', \\\\varepsilon).f(x'') = f(x') \\\\right) \\\\tag{5}\\n\\\\]\\n\\nwhere \\\\( x \\\\in X \\\\) is a FSFP space, \\\\( \\\\varepsilon' > \\\\varepsilon \\\\), \\\\( f(x') \\\\notin C(x) \\\\) is a ReLU classifier and whether an output is in \\\\( C(x^*) \\\\) for some \\\\( x^* \\\\) can be decided in polynomial time. Then \\\\( CCA^\\\\infty \\\\) is \\\\( \\\\Sigma^P_2 \\\\)-complete.\\n\\nCorollary 5.1. \\\\( CCA^p \\\\) is \\\\( \\\\Sigma^P_2 \\\\)-complete for all \\\\( 0 < p \\\\leq \\\\infty \\\\).\\n\\nCorollary 5.2. Theorem 5 holds even if, instead of ReLU classifiers, we consider the more general set of polynomial-time classifiers w.r.t. the size of the tuple.\\n\\nIn other words, under our assumptions, fooling CA is harder than running it. This phenomenon represents a form of computational robustness, a term introduced by Garg et al. (2020) in a very different setting where genuine examples can be cryptographically signed. Corollary 2.1 also implies that, unless the Polynomial Hierarchy collapses, it is impossible to obtain a better gap between running the model and attacking (e.g., a \\\\( P \\\\)-time model that is \\\\( \\\\Sigma^P_2 \\\\)-hard to attack). Note that while Theorem 5 shows that fooling CA is \\\\( \\\\Sigma^P_2 \\\\)-complete in general, attacking can be expected to be easy in practice when \\\\( \\\\varepsilon' \\\\gg \\\\varepsilon \\\\): this is however a very extreme case, where the threshold may have been poorly chosen or the adversarial examples might be visually distinguishable from genuine examples.\\n\\n5.2 USING HEURISTIC ATTACKS WITH CA\\n\\nCA in its exact form has limited scalability due to Theorem 1. This could be addressed by using approaches with guaranteed bounds, as suggested in Section 5, or by simply relying on heuristic attacks. In this second scenario, to compensate for the heuristic nature of the employed attacks, we can flag the input \\\\( x' \\\\) if the attack fails to find an adversarial example in a radius of \\\\( \\\\varepsilon + b(x') \\\\), where \\\\( b: X \\\\to \\\\mathbb{R}_+ \\\\) is a buffer model. The idea behind \\\\( b \\\\) is that if a heuristic attack can identify an adversarial example within a radius of \\\\( \\\\varepsilon + b(x') \\\\), an exact attack would be able to find an adversarial example within a radius of \\\\( \\\\varepsilon \\\\).\\n\\nThe effectiveness of this approach depends on how well heuristic attacks approximate the decision boundary distance, which is an interesting topic for investigation by itself. Note that consistency of the estimation is in fact more important than its accuracy: if a heuristic attack overestimates \\\\( d^*_p(x') \\\\) in a predictable manner, we can train a buffer model to accurately correct the error.\\n\\nWith this approach, if the heuristic attack finds an adversarial example with distance less than \\\\( \\\\varepsilon \\\\), we can confidently flag the input (i.e., false positives are guaranteed to be impossible). However, if the distance is above \\\\( \\\\varepsilon \\\\), it is possible to have a false negative. Note that using approaches with guaranteed bounds would lead to a complementary situation.\\n\\nFooling the Heuristic Attack-Based CA\\n\\nThe fact that the heuristic relaxation of CA can overestimate the decision boundary distance means that it is possible to generate adversarial examples with \\\\( \\\\varepsilon' \\\\leq \\\\varepsilon \\\\). Specifically, if an adversarial example \\\\( x_{\\\\text{adv}} \\\\) for an input \\\\( x \\\\) is such that \\\\( d^*_p(x_{\\\\text{adv}}) \\\\leq \\\\varepsilon \\\\) and \\\\( f(x_{\\\\text{adv}}) \\\\neq f(x) \\\\) but \\\\( \\\\|a_{f,p}(x_{\\\\text{adv}}) - x_{\\\\text{adv}}\\\\|_p > \\\\varepsilon + b(x_{\\\\text{adv}}) \\\\), CA will incorrectly accept \\\\( x_{\\\\text{adv}} \\\\).\\n\\nHowever, there are several informal considerations suggesting that fooling CA might be harder than running it. Such considerations are backed by empirical evidence in Section 6. First, both CA and CCA need to attack the same model, but CCA has at most as much information regarding the target model as CA, thus making the attacker at most as sample efficient as the defender. Second, fooling CA involves solving a nested optimization problem, while CA only needs to solve one; specifically, verifying the feasibility of a CCA solution involves running CA on the solution. Finally, as better attacks are developed the chances of CA being fooled become slimmer, since these attacks will be less likely to find sub-optimal adversarial examples.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Section 5.2 introduced the problem of investigating how accurately heuristic attacks can approximate the true decision boundary distance, which is needed for the heuristic version of CA to work, but also an interesting topic per se. In this section, we test whether \\\\( \\\\|x - x_h\\\\|_p \\\\), where \\\\( x_h \\\\) is an adversarial example found by a heuristic attack, is predictably close to the true decision boundary distance (i.e. \\\\( d^*_p(x) \\\\)). Consistently with Athalye et al. (2018) and Weng et al. (2018a), we focus on the \\\\( L_\\\\infty \\\\) norm. Additionally, we focus on pools of heuristic attacks. The underlying rationale is that different adversarial attacks should be able to cover for their reciprocal blind spots, providing a more reliable estimate. Since this evaluation is empirical, it requires sampling from a chosen distribution, in our case specific classifiers and the MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky et al., 2009) datasets. This means that the results are not guaranteed for other distributions, or for other defended models: studying how adversarial attacks fare in these cases is an important topic for future work.\\n\\nExperimental Setup\\n\\nWe randomly selected ~2.3k samples each from the test set of two datasets, MNIST and CIFAR10. We used three architectures per dataset (named A, B and C), each trained in three settings, namely standard training, PGD adversarial training (Madry et al., 2018) and PGD adversarial training with ReLU loss and pruning (Xiao et al., 2019) (from now on referred to as ReLU training), for a total of nine configurations per dataset. Since our analysis requires computing exact decision boundary distances, and size and depth both have a strong adverse impact on solver times, we used small and relatively shallow networks with parameters between ~2k and ~80k. Note that using (more scalable) NN verification approaches that can provide bounds without tightness guarantees is not an option, as they would prevent us from drawing any firm conclusion. For this reason, the natural accuracies for standard training are significantly below the state of the art (89.63% - 95.87% on MNIST and 47.85% - 55.81% on CIFAR10). Adversarial training also had a negative effect on natural accuracies (84.54% - 94.24% on MNIST and 45.19% - 51.35% on CIFAR10), similarly to ReLU training (83.69% - 93.57% on MNIST and 32.27% - 37.33% on CIFAR10).\\n\\nWe first ran a pool of heuristic attacks on each example, namely (Kurakin et al., 2017; Brendel et al., 2019; Carlini & Wagner, 2017c; Moosavi-Dezfooli et al., 2016; Goodfellow et al., 2015; Madry et al., 2018), as well as simply adding uniform noise to the input. Our main choice of attack parameters (from now on referred to as the \u201cstrong\u201d parameter set) prioritizes finding adversarial examples at the expense of computational time. For each example, we considered the nearest feasible adversarial example found by any attack in the pool. We then ran the exact solver-based attack MIPVerify (Tjeng et al., 2019), which is able to find the nearest adversarial example to a given input. The entire process (including test runs) required ~45k core-hours on an HPC cluster. Each node of the cluster has 384 GB of RAM and features two Intel CascadeLake 8260 CPUs, each with 24 cores and a clock frequency of 2.4GHz. We removed the examples for which MIPVerify crashed in at least one setting, obtaining 2241 examples for MNIST and 2269 for CIFAR10. We also excluded from our analysis all adversarial examples for which MIPVerify did not find optimal bounds (atol = 1e-5, rtol = 1e-10), which represent on average 11.95% of the examples for MNIST and 16.30% for CIFAR10.\\n\\nAdditionally, we ran the same heuristic attacks with a faster parameter set (from now on referred to as the \u201cbalanced\u201d set) on a single machine with an AMD Ryzen 5 1600X six-core 3.6 GHz processor, 16 GBs of RAM and an NVIDIA GTX 1060 6 GB GPU. The process took approximately 8 hours. Refer to Appendix G for a more comprehensive overview of our experimental setup.\\n\\nDistance Approximation\\n\\nAcross all settings, the mean distance found by the strong attack pool is 4.09\u00b12.02% higher for MNIST and 2.21 \u00b11.16% higher for CIFAR10 than the one found by MIPVerify. For 79.81\u00b115.70% of the MNIST instances and 98.40\u00b11.63% of the CIFAR10 ones, the absolute difference is less than 1/255, which is the minimum distance in 8-bit image formats. The balanced attack pool performs similarly, finding distances that are on average 4.65\u00b12.16% higher for MNIST and 2.04\u00b11.13% higher for CIFAR10. The difference is below 1/255 for 77.78\u00b116.08% of MNIST examples and 98.74\u00b11.13% of CIFAR10 examples. We compare the distances found by the strong attack pool for MNIST A and CIFAR10 (using standard training) with the true decision bound distances in Figure 1. Refer to Appendix I for the full data.\\n\\nFor all datasets, architectures and training techniques there appears to be a strong, linear, correlation between the distance of the output of the heuristic attacks and the true decision boundary distance. We chose to measure this by training a linear regression model linking the two distances. For the...\"}"}
{"id": "FDlfFbnI7AR", "page_num": 68, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 31: $R^2$ of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 C Adversarial. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 69, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"| Model | Dataset | Heuristic Adversarial Distances |\\n|-------|---------|---------------------------------|\\n| R2    | (a) MNIST C ReLU Fast-100 | ![Graph](a.png) |\\n|       | (b) CIFAR10 C ReLU Fast-100 | ![Graph](b.png) |\\n|       | (c) MNIST C ReLU Fast-1k   | ![Graph](c.png) |\\n|       | (d) CIFAR10 C ReLU Fast-1k | ![Graph](d.png) |\\n|       | (e) MNIST C ReLU Fast-10k  | ![Graph](e.png) |\\n|       | (f) CIFAR10 C ReLU Fast-10k| ![Graph](f.png) |\\n\\nFigure 32: R2 of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 C ReLU. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 70, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In order to put our defense into context, we provide a slightly more in-depth overview of common approaches to certified robustness, as well as their strengths and weaknesses.\\n\\nInitially, theoretical work focused on providing robustness bounds based on general properties. For example, Szegedy et al. (2013) computed robustness bounds against $L_2$-bounded perturbations by studying the upper Lipschitz constant of each layer, while Hein & Andriushchenko (2017) achieved similar results for $L_p$-bounded perturbations by focusing on local Lipschitzness. While these studies do not require any modifications to the network or distribution hypotheses, in practice the provided bounds are too loose to be used in practice. For this reason, Weng et al. (2018b) derived stronger bounds through a local Lipschitz constant estimation technique; however, finding this bound is computationally expensive, which is why the authors also provide a heuristic to estimate it.\\n\\nSimilarly, solver-based approaches provide tight bounds but require expensive computations. For example, Reluplex was used to verify networks of at most $\\\\sim 300$ ReLU nodes (Katz et al., 2017). Tjeng et al. (2019) was able to use a MIP-based formulation to significantly speed up verification, although large networks are still not feasible to verify. Solver-friendly training techniques can boost the performance of verifiers (such as in (Xiao et al., 2019)); however, this increase in speed often comes at the cost of accuracy (see Section 6).\\n\\nAnother solution to the trade-off between speed and bound tightness is to focus on specific (and more tractable) threat models. For example, Han et al. (2021) provide robustness guarantees against adversarial patches (Brown et al., 2017), while Jia et al. (2019) focus on adversarial word substitutions. In the same vein, Raghunathan et al. (2018) provide robustness bounds for specific architectures (i.e. 1-layer and 2-layer neural networks), while Zhang et al. (2021) introduce custom neurons that, if used in place of regular neurons, provide $L_\\\\infty$ robustness guarantees. These techniques thus trade generality for speed.\\n\\nThe most common approach, however, consists in providing statistical guarantees. For example, Sinha et al. (2018) showed that using a custom loss can bound the adversarial risk. Similarly, Dan et al. (2020) proved adversarial risk bounds for Gaussian mixture models depending on the \\\"adversarial Signal-to-Noise Ratio\\\". Finally, Cohen et al. (2019) introduced a smoothing-based certified defense that, due to its high computational cost, is replaced by a Monte Carlo estimate with a given probability of being robust. This work was later expanded upon in (Salman et al., 2020) and (Carlini et al., 2022). The main drawback of these techniques is the fact that they cannot be used in contexts where statistical guarantees are not sufficient, such as safety-critical applications.\\n\\nAll of these certified defenses prioritize certain aspects (speed, strength, generality) over others. In the context of this (simplified) framework, CA in its exact form can be thus considered a defense that prioritizes strength and generality over speed, similarly to Katz et al. (2017) and Tjeng et al. (2019).\"}"}
{"id": "FDlfFbnI7AR", "page_num": 60, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Calls to the Model\\nHeuristic Distance - Exact Distance\\n\\n(a) MNIST C ReLU Fast-100\\n\\n(b) CIFAR10 C ReLU Fast-100\\n\\n(c) MNIST C ReLU Fast-1k\\n\\n(d) CIFAR10 C ReLU Fast-1k\\n\\n(e) MNIST C ReLU Fast-10k\\n\\n(f) CIFAR10 C ReLU Fast-10k\\n\\nFigure 23: Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 C ReLU. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 61, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Calls to the Model\\n\\nR\u00b2\\n\\n(a) MNIST A Standard Fast-100\\n\\n(b) CIFAR10 A Standard Fast-100\\n\\n(c) MNIST A Standard Fast-1k\\n\\n(d) CIFAR10 A Standard Fast-1k\\n\\n(e) MNIST A Standard Fast-10k\\n\\n(f) CIFAR10 A Standard Fast-10k\\n\\nFigure 24: R\u00b2 of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 A Standard. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 62, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 25: $R^2$ of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 A Adversarial. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 63, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 26: $R^2$ of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 A ReLU. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 52, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 15: Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 A Standard. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 53, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 16: Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 54, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 17: Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 A ReLU. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 55, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 18: Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 B Standard. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 56, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 19: Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 B Adversarial. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 57, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 20: Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 B ReLU. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 58, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 21: Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 C Standard. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 59, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 22: Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 C Adversarial. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 64, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"BIM\\nDeepfool\\nFast Gradient\\nPGD\\nUniform\\n\\nFigure 27: $R^2$ of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 B Standard. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 65, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 28: $R^2$ of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 B Adversarial. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 66, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 29: $R^2$ of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 B ReLU. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 67, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 30: \\\\( R^2 \\\\) of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 C Standard. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under review as a conference paper at ICLR 2023\\n\\n\\\\[ e = x_1 | x_1 | p \\\\]\\n\\nis such that \\\\(| e | p = 1 \\\\) and for all \\\\( i \\\\) we have \\\\(| e_i | \\\\leq 1 \\\\). Since \\\\( r < p \\\\), for all \\\\( 0 \\\\leq t \\\\leq 1 \\\\) we have \\\\(| t | p \\\\leq | t | r \\\\). Therefore:\\n\\n\\\\[ | | e | r = n \\\\sum_{i=1}^{n} | e_i | r \\\\geq 1/p \\\\sum_{i=1}^{n} | e_i | p = | | e | p/r p = 1 \\\\quad (36) \\\\]\\n\\nThen, since \\\\(| | e | r | \\\\geq 1 \\\\):\\n\\n\\\\[ | | x | r = | | x | p e | r \\\\]\\n\\nCase \\\\( p = \\\\infty \\\\)\\n\\nSince \\\\(| | x | r | \\\\geq | | x | p | for all \\\\( r < p \\\\) and since the expressions on both sides of the inequality are compositions of continuous functions, as \\\\( p \\\\to \\\\infty \\\\) we get \\\\(| | x | r | \\\\geq | | x | \\\\infty | .\\n\\nE.1.2 \\\\( r > p \\\\)\\n\\nWe will prove that \\\\(| | x | r | \\\\geq \\\\frac{n}{r-p} | | x | p | .\\n\\nCase \\\\( r < \\\\infty \\\\)\\n\\nH\u00f6lder's inequality states that, given \\\\( \\\\alpha, \\\\beta \\\\geq 1 \\\\) such that \\\\( \\\\frac{1}{\\\\alpha} + \\\\frac{1}{\\\\beta} = 1 \\\\) and given \\\\( f \\\\) and \\\\( g \\\\), we have:\\n\\n\\\\[ | | fg | | 1 \\\\leq | | f | | \\\\alpha | | g | | \\\\beta \\\\quad (38) \\\\]\\n\\nSetting \\\\( \\\\alpha = \\\\frac{r}{r-p}, \\\\beta = \\\\frac{r}{p} \\\\), \\\\( f = (1, \\\\ldots, 1) \\\\) and \\\\( g = (x_1^p, \\\\ldots, x_n^p) \\\\), we know that:\\n\\n\u2022 \\\\(| | fg | | 1 = \\\\sum_{i=1}^{n} (1 \\\\cdot x_i^p) = | | x | p | ;\\n\\n\u2022 \\\\(| | f | | \\\\alpha = \\\\left( \\\\sum_{i=1}^{n} 1 \\\\right)^{1/\\\\alpha} = n^{1/\\\\alpha} ;\\n\\n\u2022 \\\\(| | g | | \\\\beta = \\\\prod_{i=1}^{n} x_i^{p/r} = \\\\left( \\\\prod_{i=1}^{n} x_i^r \\\\right)^{p/r} = | | x | p | .\\n\\nTherefore \\\\(| | x | p | \\\\leq n^{1/\\\\alpha} \\\\sum_{i=1}^{n} | e_i | p \\\\); Raising both sides to the power of \\\\( \\\\frac{1}{p} \\\\), we get \\\\(| | x | p | \\\\leq \\\\frac{n}{r-p} | | x | p | .\\n\\nTherefore:\\n\\n\\\\[ | | x | p | \\\\leq \\\\frac{n}{r-p} \\\\quad (39) \\\\]\\n\\nDividing by \\\\( \\\\frac{n}{r-p} \\\\) we get:\\n\\n\\\\[ \\\\frac{n}{r-p} | | x | p | \\\\leq | | x | r | \\\\quad (40) \\\\]\\n\\nCase \\\\( r = \\\\infty \\\\)\\n\\nSince the expressions on both sides of the inequality are compositions of continuous functions, as \\\\( r \\\\to \\\\infty \\\\) we get \\\\(| | x | \\\\infty | \\\\geq | | x | p | .\\n\\nF.1. CCA\\n\\n\\\\[ CCA_{\\\\infty} \\\\in \\\\Sigma^P_2 \\\\iff \\\\text{there exists a problem } A \\\\in P \\\\text{ and a polynomial } p \\\\text{ such that } \\\\forall \\\\Gamma = \\\\langle x, \\\\varepsilon, \\\\varepsilon', C, f \\\\rangle : \\\\Gamma \\\\in CCA_{\\\\infty} \\\\iff \\\\exists y. | y | \\\\leq p(| \\\\Gamma |) \\\\land (\\\\forall z. (| z | \\\\leq p(| \\\\Gamma |) = \\\\Rightarrow \\\\langle \\\\Gamma, y, z \\\\rangle \\\\in A) \\\\quad (41) \\\\]\\n\\nThis can be proven by setting \\\\( y = x' \\\\), \\\\( z = x'' \\\\) and \\\\( A \\\\) as the set of all triplets \\\\( \\\\langle \\\\Gamma, x', x'' \\\\rangle \\\\) such that all of the following are true:\\n\\n\u2022 \\\\(| | x - x' | | \\\\leq \\\\varepsilon'\\n\\n\u2022 \\\\( f(x') \\\\in C(x)\\n\\n\u2022 \\\\(| | x'' - x' | | \\\\leq \\\\varepsilon\\n\\n\u2022 \\\\( f(x'') = f(x') \\\\)\\n\\nSince all properties can be checked in polynomial time, \\\\( A \\\\in P \\\\).\"}"}
{"id": "FDlfFbnI7AR", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We will show that CCA\\\\(_\\\\infty\\\\) is \\\\(\\\\Sigma^P_2\\\\)-hard by proving that \\\\(\\\\text{co-}\\\\Pi^P_3\\\\text{-SAT} \\\\leq \\\\text{CCA}\\\\_\\\\infty\\\\).\\n\\nFirst, suppose that the length of \\\\(\\\\hat{x}\\\\) and \\\\(\\\\hat{y}\\\\) differ. In that case, we pad the shortest one with additional variables that will not be used.\\n\\nLet \\\\(n\\\\) be the maximum of the lengths of \\\\(\\\\hat{x}\\\\) and \\\\(\\\\hat{y}\\\\).\\n\\nGiven a set \\\\(z\\\\) of 3CNF clauses, we construct the following query \\\\(q(z)\\\\) for CCA\\\\(_\\\\infty\\\\):\\n\\n\\\\[\\nq(z) = \\\\langle x(s), \\\\gamma, 1/2, C, h \\\\rangle\\n\\\\]\\n\\nwhere \\\\(1/4 < \\\\gamma < 1/2\\\\) and \\\\(x(s) = \\\\frac{1}{2}, \\\\ldots, \\\\frac{1}{2}\\\\) is a vector with \\\\(n\\\\) elements. Verifying \\\\(q(z) \\\\in \\\\text{CCA}\\\\_\\\\infty\\\\) is equivalent to checking:\\n\\n\\\\[\\n\\\\exists x' \\\\in B x, s, \\\\frac{1}{2} \\\\cdot h(x') \\\\neq h(x) \\\\land \\\\forall x'' \\\\in B x', \\\\frac{1}{4} \\\\cdot h(x'') = h(x')\\n\\\\]\\n\\nNote that \\\\(x' \\\\in [0, 1]^n\\\\).\\n\\nTruth Values\\n\\nWe will encode the truth values of \\\\(\\\\hat{x}\\\\) and \\\\(\\\\hat{y}\\\\) as follows:\\n\\n\\\\[\\nx''_i \\\\in 0, \\\\frac{1}{4} \\\\iff \\\\hat{x}_i = 0 \\\\land \\\\hat{y}_i = 0\\n\\\\]\\n\\n\\\\[\\nx''_i \\\\in \\\\frac{1}{4}, \\\\frac{1}{2} \\\\iff \\\\hat{x}_i = 0 \\\\land \\\\hat{y}_i = 1\\n\\\\]\\n\\n\\\\[\\nx''_i \\\\in \\\\frac{1}{2}, \\\\frac{3}{4} \\\\iff \\\\hat{x}_i = 1 \\\\land \\\\hat{y}_i = 0\\n\\\\]\\n\\n\\\\[\\nx''_i \\\\in \\\\frac{3}{4}, 1 \\\\iff \\\\hat{x}_i = 1 \\\\land \\\\hat{y}_i = 1\\n\\\\]\\n\\nLet \\\\(e_{\\\\hat{x}_i}(x) = \\\\text{gt}(x_i, \\\\frac{1}{2})\\\\). Let:\\n\\n\\\\[\\ne_{\\\\hat{y}_i}(x) = \\\\text{or} (\\\\text{leq}(x_i, 0), \\\\text{geq}(x_i, 1))\\n\\\\]\\n\\nNote that \\\\(e_{\\\\hat{x}_i}(x'')\\\\) returns the truth value of \\\\(\\\\hat{x}_i\\\\) and \\\\(e_{\\\\hat{y}_i}(x'')\\\\) returns the truth value of \\\\(\\\\hat{y}_i\\\\) (as long as the input is within one of the ranges described in Equation (44)).\\n\\nInvalid Encodings\\n\\nAll the encodings other than the ones described in Equation (44) are not valid.\\n\\nWe define \\\\(\\\\text{inv}_F\\\\) as follows:\\n\\n\\\\[\\n\\\\text{inv}_F(x) = \\\\text{or} (\\\\text{out}(x_i), \\\\text{edge}(x_i))\\n\\\\]\\n\\nwhere \\\\(\\\\text{out}(x_i) = \\\\text{or} (\\\\text{leq}(x_i, 0), \\\\text{geq}(x_i, 1))\\\\) and \\\\(\\\\text{edge}(x_i) = \\\\text{or} (\\\\text{eq}(x_i, \\\\frac{1}{4}), \\\\text{eq}(x_i, \\\\frac{1}{2}), \\\\text{eq}(x_i, \\\\frac{3}{4}))\\\\).\\n\\nOn the other hand, we define \\\\(\\\\text{inv}_T\\\\) as follows:\\n\\n\\\\[\\n\\\\text{inv}_T(x) = \\\\text{or} (\\\\text{eq}(x_i, \\\\frac{1}{2}))\\n\\\\]\"}"}
{"id": "FDlfFbnI7AR", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We now prove that\\n\\nWe know that\\n\\nWe now prove that setting\\n\\nWe define\\n\\nAs a consequence,\\n\\nTherefore,\\n\\nProof. If\\n\\nLemma 5.\\n\\n\\\\( z \\\\) as follows. For every\\n\\nSince\\n\\nProof. If\\n\\nLemma 6.\\n\\nFor every\\n\\n\\\\( h \\\\) of\\n\\nDefinition of\\n\\nUnder review as a conference paper at ICLR 2023\\n\\nNote that:\\n\\n\\\\( h \\\\)\\n\\nand\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( i \\\\)\\n\\nas follows. For every\\n\\nSince\\n\\nProof. If\\n\\nLemma 6.\\n\\n\\\\( q \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( h \\\\)\\n\\n\\\\( \\\\forall \\\\)\\n\\n\\\\( x \\\\"}
{"id": "FDlfFbnI7AR", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"If $e(x) \\\\cdot (x^*) = 1$ and $e(y) \\\\cdot (x^*) = 1$, set $x''\\\\ast i$ equal to a value in $\\\\{3, 4, 1\\\\}$. By doing so, we have obtained a $x''\\\\ast$ such that $x''\\\\ast \\\\in B_{\\\\infty}(x^*, \\\\gamma)$ and $e(y) \\\\cdot (x''\\\\ast) = \\\\hat{y}^*$. Since:\\n\\n\u2022 $e(x) \\\\cdot (x''\\\\ast) = e(x) \\\\cdot (x^*)$ for all $x''\\\\ast$;\\n\u2022 $h(x''\\\\ast) = 0$ for all $x''\\\\ast$;\\n\u2022 $h(x''\\\\ast) = 1$ iff $R(e(x) \\\\cdot (x''\\\\ast), e(y) \\\\cdot (x''\\\\ast))$ is true;\\n\\n$R(e(x) \\\\cdot (x^*), \\\\hat{y}^*)$ is false for all choices of $\\\\hat{y}^*$. In other words, $\\\\hat{x}^*$ is a solution to Equation (30) and thus $z \\\\in co\\\\Pi_{23}SAT$. Since:\\n\\n\u2022 $q(z)$ can be computed in polynomial time;\\n\u2022 $z \\\\in co\\\\Pi_{23}SAT$ is equivalent to $q(z) \\\\in CCA_{\\\\infty}$;\\n\u2022 $q(z) \\\\in CCA_{\\\\infty}$ is equivalent to $z \\\\in co\\\\Pi_{23}SAT$;\\n\\nwe can conclude that $co\\\\Pi_{23}SAT \\\\leq CCA_{\\\\infty}$.\\n\\nF.3 Proof of Corollary 5.1\\n\\nThe proof of $CCA_p \\\\in \\\\Sigma_P$ is the same as the one for Theorem 5. For the hardness proof, we follow a more involved approach compared to those for Corollaries 1.1 and 3.1. First, let $\\\\epsilon_{\\\\rho p,n}$ be the value of epsilon such that $\\\\rho_{p,n} = \\\\frac{1}{2}$. In other words, $B_{\\\\rho}(x(s), \\\\epsilon_{\\\\rho p,n})$ is an $L_p$ ball that contains $[0, 1]^n$, while the intersection of the corresponding $L_p$ sphere and $[0, 1]^n$ is the set $\\\\{0, 1\\\\}^n$ (for $p < \\\\infty$).\\n\\nLet $inv''_T(x)$ be defined as follows:\\n\\n$inv''_T(x) = \\\\text{or}_i \\\\in [1, n] \\\\text{ or } eq(x_i, \\\\frac{1}{2}), leq(x_i, 0), geq(x_i, 1)$ (51)\\n\\nLet $inv''_F(x)$ be defined as follows:\\n\\n$inv''_F(x) = \\\\text{or}_i \\\\in [1, n] \\\\text{ or } eq(x_i, \\\\frac{1}{4}), eq(x_i, \\\\frac{3}{4})$ (52)\\n\\nWe define $h''$ as follows:\\n\\n$h''_1(x) = \\\\text{or}(inv''_T(x), \\\\text{and}(\\\\text{not}(inv''_F(x))), g(x))$ (53)\\n\\nwith $h''_0(x) = \\\\text{not}(h''_1(x))$.\\n\\nNote that:\\n\\n\u2022 If $x_i \\\\in (-\\\\infty, 0) \\\\cup \\\\{\\\\frac{1}{2}\\\\} \\\\cup [1, \\\\infty)$ for some $i$, then the top class is 1;\\n\u2022 Otherwise, if $x$ is not a valid encoding, the top class is 0;\\n\u2022 Otherwise, the top class is 1 if $R(e(x) \\\\cdot (x), e(y) \\\\cdot (x))$ is true and 0 otherwise.\\n\\nFinally, let $\\\\gamma' < \\\\frac{1}{2}$. Our query is thus:\\n\\n$q(z) = \\\\langle x(s), \\\\gamma', \\\\frac{1}{2}, C_u, h'' \\\\rangle$ (54)\"}"}
{"id": "FDlfFbnI7AR", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: $F_1$ scores in relation to $\\\\epsilon$ for CIFAR10 C for each considered percentile. For ease of visualization, we set the graph cutoff at $F_1 = 0.8$. We also mark 8/255 (a common choice for $\\\\epsilon$) with a dotted line.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 15: Performance of the balanced attack set on MNIST.\\n\\n| Architecture | Training Success Rate | Difference % Below 1/255 |\\n|--------------|-----------------------|---------------------------|\\n| **MNIST A**  | Standard              | 100.00%                   |\\n|              | Adversarial           | 100.00%                   |\\n|              | ReLU                  | 100.00%                   |\\n\\n| **MNIST B**  | Standard              | 100.00%                   |\\n|              | Adversarial           | 100.00%                   |\\n|              | ReLU                  | 100.00%                   |\\n\\n| **MNIST C**  | Standard              | 100.00%                   |\\n|              | Adversarial           | 100.00%                   |\\n|              | ReLU                  | 100.00%                   |\\n\\nTable 16: Performance of the strong attack set on CIFAR10.\\n\\n| Architecture | Training Success Rate | Difference % Below 1/255 |\\n|--------------|-----------------------|---------------------------|\\n| **CIFAR10 A**| Standard              | 100.00%                   |\\n|              | Adversarial           | 100.00%                   |\\n|              | ReLU                  | 100.00%                   |\\n\\n| **CIFAR10 B**| Standard              | 100.00%                   |\\n|              | Adversarial           | 100.00%                   |\\n|              | ReLU                  | 100.00%                   |\\n\\n| **CIFAR10 C**| Standard              | 100.00%                   |\\n|              | Adversarial           | 100.00%                   |\\n|              | ReLU                  | 100.00%                   |\\n\\nTable 17: Performance of the balanced attack set on CIFAR10.\\n\\n| Architecture | Training Success Rate | Difference % Below 1/255 |\\n|--------------|-----------------------|---------------------------|\\n| **CIFAR10 A**| Standard              | 100.00%                   |\\n|              | Adversarial           | 100.00%                   |\\n|              | ReLU                  | 100.00%                   |\\n\\n| **CIFAR10 B**| Standard              | 100.00%                   |\\n|              | Adversarial           | 100.00%                   |\\n|              | ReLU                  | 100.00%                   |\\n\\n| **CIFAR10 C**| Standard              | 100.00%                   |\\n|              | Adversarial           | 100.00%                   |\\n|              | ReLU                  | 100.00%                   |\\n\\nTable 18: Best pools of a given size by success rate and $R^2$ for MNIST strong.\\n\\n| n | Attacks | Success Rate | Difference % Below 1/255 | $R^2$ |\\n|---|---------|--------------|---------------------------|-------|\\n| 1 | PGD     | 100.00\u00b10.00% | 10.98\u00b14.41%               | 0.975\u00b10.010 |\\n| 2 | C&W, PGD| 100.00\u00b10.00% | 7.99\u00b13.31%                | 0.986\u00b10.005 |\\n| 3 | B&B, C&W, PGD | 100.00\u00b10.00% | 4.71\u00b11.97%               | 0.989\u00b10.004 |\\n| 4 | B&B, C&W, DF, PGD | 100.00\u00b10.00% | 4.36\u00b12.03%               | 0.991\u00b10.005 |\\n| 5 | No FGSM, Uniform | 100.00\u00b10.00% | 4.09\u00b12.02%               | 0.992\u00b10.005 |\\n| 6 | No Uniform | 100.00\u00b10.00% | 4.09\u00b12.02%               | 0.992\u00b10.005 |\\n| 7 | All     | 100.00\u00b10.00% | 4.09\u00b12.02%               | 0.992\u00b10.005 |\"}"}
{"id": "FDlfFbnI7AR", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 9: Decision boundary distances found by the attack pools compared to those found by MIPVerify on MNIST A. The black line represents the theoretical optimum. Note that no samples are below the black line.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 43, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 10: Decision boundary distances found by the attack pools compared to those found by MIPVerify on MNIST B. The black line represents the theoretical optimum. Note that no samples are below the black line.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 44, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 11: Decision boundary distances found by the attack pools compared to those found by MIPVerify on MNIST C. The black line represents the theoretical optimum. Note that no samples are below the black line.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 45, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 12: Decision boundary distances found by the attack pools compared to those found by MIPVerify on CIFAR10 A. The black line represents the theoretical optimum. Note that no samples are below the black line.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 46, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 13: Decision boundary distances found by the attack pools compared to those found by MIPVerify on CIFAR10 B. The black line represents the theoretical optimum. Note that no samples are below the black line.\"}"}
{"id": "FDlfFbnI7AR", "page_num": 47, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"### CIFAR10 C Standard Strong\\n\\n| True Distance | Estimated Distance |\\n|---------------|--------------------|\\n| 0 1 2 3       | \u00b710^-2             |\\n\\n### CIFAR10 C Standard Balanced\\n\\n| True Distance | Estimated Distance |\\n|---------------|--------------------|\\n| 0 2 4 6       | \u00b710^-2             |\\n\\n### CIFAR10 C Adversarial Strong\\n\\n| True Distance | Estimated Distance |\\n|---------------|--------------------|\\n| 0 2 4 6       | \u00b710^-2             |\\n\\n### CIFAR10 C Adversarial Balanced\\n\\n| True Distance | Estimated Distance |\\n|---------------|--------------------|\\n| 0 5 10 15     | \u00b710^-2             |\\n\\n### CIFAR10 C ReLU Strong\\n\\n| True Distance | Estimated Distance |\\n|---------------|--------------------|\\n| 0 5 10 15     | \u00b710^-2             |\\n\\n### CIFAR10 C ReLU Balanced\\n\\n| True Distance | Estimated Distance |\\n|---------------|--------------------|\\n| 0 5 10 15     | \u00b710^-2             |\\n\\nFigure 14: Decision boundary distances found by the attack pools compared to those found by MIPVerify on CIFAR10 C. The black line represents the theoretical optimum. Note that no samples are below the black line.\"}"}
