{"id": "SyoUVEyzJbE", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Weichao Qiu, Fangwei Zhong, Yi Zhang, Siyuan Qiao, Zihao Xiao, Tae Soo Kim, Yizhou Wang, and Alan Yuille. Unrealcv: Virtual worlds for computer vision. In ACM International Conference on Multimedia, pages 1221\u20131224, 2017.\"}"}
{"id": "SyoUVEyzJbE", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n   We introduce a novel environment as a new benchmark that is applicable to MSMT tasks and test multi-agent learning algorithms.\\n   (b) Did you describe the limitations of your work? [Yes]\\n   See Section 6.\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes]\\n   See Section 6.\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes]\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes]\\n   (b) Did you mention the license of the assets? [Yes]\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes]\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"}
{"id": "SyoUVEyzJbE", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We introduce the Multi-Agent Tracking Environment (MATE), a novel multi-agent environment simulates the target coverage control problems in the real world. MATE hosts an asymmetric cooperative-competitive game consisting of two groups of learning agents\u2014\u201ccameras\u201d and \u201ctargets\u201d\u2014with opposing interests. Specifically, \u201ccameras\u201d, a group of directional sensors, are mandated to actively control the directional perception area to maximize the coverage rate of targets. On the other side, \u201ctargets\u201d are mobile agents that aim to transport cargo between multiple randomly assigned warehouses while minimizing the exposure to the camera sensor networks. To showcase the practicality of MATE, we benchmark the multi-agent reinforcement learning (MARL) algorithms from different aspects, including cooperation, communication, scalability, robustness, and asymmetric self-play. We start by reporting results for cooperative tasks using MARL algorithms (MAPPO, IPPO, QMIX, MADDPG) and the results after augmenting with multi-agent communication protocols (TarMAC, I2C). We then evaluate the effectiveness of the popular self-play techniques (PSRO, fictitious self-play) in an asymmetric zero-sum competitive game. This process of co-evolution between cameras and targets helps to realize a less exploitable camera network. We also observe the emergence of different roles of the target agents while incorporating I2C into target-target communication. MATE is written purely in Python and integrated with OpenAI Gym API to enhance user-friendliness. Our project is released at https://github.com/UnrealTracking/mate.\\n\\nIntroduction\\n\\nThe target coverage problem studies the active control of the perception area of a group of agents to track the targets of interest, e.g., wireless sensor networks [1], surveillance camera networks [2, 3], and unmanned aerial vehicle (UAV) networks [4]. It has much real-life significance and received wide applications relating to social well-being, security, and entertainment. For example, smart camera networks can be used for anti-poaching [5, 6], anti-smuggling [7], border security [8] and\u2014for more recreational uses\u2014person-following [9, 10, 11] in filming and ball-tracking [12] in sports events, etc. However, it remains an open challenge to cooperatively control the cameras.\"}"}
{"id": "SyoUVEyzJbE", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Five snapshots of The Multi-Agent Tracking Environment at different scales. Note that here we abbreviate \\\"camera\\\" as \\\"C\\\", \\\"target\\\" as \\\"T\\\" and \\\"obstacle\\\" as \\\"O\\\".\\n\\nin distributed networks. A few notable issues hinder the progress: the quantities of the cameras and targets that vary in real-time, the increasingly diverse and unpredictable trajectories of targets, the partial observability of the cameras, and the limited bandwidth of the communication networks. These factors contribute to the difficulties in current research regarding multi-camera cooperation. Recent successes in multi-agent reinforcement learning (MARL) \\\\[13\\\\] have demonstrated the superior efficiency of multi-agent learning methods in tackling cooperative-competitive games at super-human levels, as shown in gaming AI \\\\[14, 15, 16\\\\], robotic manipulation \\\\[18, 19\\\\], autonomous driving \\\\[20, 21\\\\], population biology \\\\[22\\\\], and etc. Assuredly, the research on target coverage problems would benefit significantly from multi-agent reinforcement learning. Unfortunately, we notice that the popular MARL algorithms, e.g., MADDPG \\\\[23\\\\], QMIX \\\\[24\\\\], MAPPO \\\\[25\\\\], HAPPO \\\\[26, 27\\\\] perform poorly in the target coverage problem \\\\[28\\\\], though they achieved great success in other existing benchmarks \\\\[23, 29, 30\\\\]. These benchmarks are either based on video games or some simplified scenarios, neglecting which features the real-world multi-agent applications desperately demand, e.g., heterogeneous agents, asymmetric games, the variable population of agents, simulating partial observation, and peer-to-peer communication. We have yet to see an open-source and standardized environment that benchmarks the MARL algorithms under such practical settings in the context of the target coverage problem.\\n\\nMotivated by these findings, we build the Multi-Agent Tracking Environment (MATE) that advocates the proposal of a more practical multi-agent system. Our system has accounted for both aspects of fully-cooperative and fully-competitive games, the scalability and the robustness of agents, and the communication efficiency among agents. MATE is an open-source simulation that hosts an asymmetric two-team stochastic game between the \\\"cameras\\\" and the \\\"targets\\\". Inside MATE, the camera agents need to maximize the coverage rate on the \\\"targets\\\" while maintaining strong coordination within the team to minimize overlapping detection. On the other hand, the targets are tasked to maximize transport flow between randomly assigned warehouses while minimizing the time of being detected by the cameras. The game-theoretic theme in MATE stimulates the emergence of innovative strategies between two teams of asymmetric agents, thus facilitating the process of autocurriculum. As a design consideration to encourage multi-layer strategies and counterplay, MATE adds randomly spawned obstacles and transport tasks to offset the natural advantages of both types of agents. For example, cameras' mobility is restricted; therefore, targets may hide behind obstacles to temporarily avoid exposure. Still, the freight and bounties would drive the targets to stop hiding as soon as possible and carry on with their assigned tasks. Worth noting that MATE is not anchored to entirely service the target coverage problem for training a better camera network, but it also stands out as a multi-purpose benchmark to aid the advancement of MARL algorithms. As discussed in the related work section, MATE has many essential features demanded in algorithm-related research in MARL. Besides mixed-motive games, MATE supports fully cooperative and fully competitive game types, which found popularity in the current landscape of theoretical analysis on MARL algorithms \\\\[31, 32, 33, 34, 35\\\\]. MATE also provides Peer-to-Peer communication channels, a topic that recently gained much interest in developing means beyond information broadcast in multi-agent communication \\\\[36, 37\\\\]. The competitive game hosted in MATE involves two-team of heterogeneous agents. The target agents differ in carrying capacity and moving speed, marking further heterogeneity in learning the controls of target agents or learning counter strategies against various types of targets as the camera agents.\\n\\nWe showcased a series of experiments to confirm the feasibility of training for both teams inside MATE. We reported the performance of four MARL algorithms (MAPPO \\\\[25\\\\], IPPO \\\\[38\\\\], MADDPG \\\\[23\\\\], QMIX \\\\[24\\\\]) on different environment configurations. We also showed the performance of our agents in the cooperative game with multi-agent communication add-ons. We observed the emergence of different roles (e.g., \\\"distractors\\\" and \\\"running backs\\\") while training...\"}"}
{"id": "SyoUVEyzJbE", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison between relevant MAL environments (viewing with colors is recommended).\\n\\n| Environment      | Game Type       | Observations | Actions | Communication | Agent Type | Scalable |\\n|------------------|-----------------|--------------|---------|---------------|------------|----------|\\n| MPE (2017)       | Mixed-Motive    | Continuous & Discrete | Broadcast | Heterogeneous | Yes        |          |\\n| MAgent (2018)    | Mixed-Motive    | Discrete     | Discrete | No            | Homogeneous| Yes      |\\n| Pommerman (2018) | Fully-Competitive | Continuous | Discrete | Broadcast     | Homogeneous| No       |\\n| MARL\u00d6 (2018)     | Mixed-Motive    | Continuous + Pixels | Discrete | No            | Homogeneous| Yes      |\\n| Hanabi (2019)    | Fully-Cooperative | Discrete | Discrete | Broadcast     | Homogeneous| No       |\\n| SMAC (2019)      | Fully-Cooperative | Continuous | Discrete | No            | Heterogeneous| No       |\\n| Neural MMO (2019)| Mixed-Motive    | Discrete     | Multi-Discrete | No         | Homogeneous| Yes      |\\n| GFootball (2019) | Fully-Cooperative | Continuous | Discrete | No            | Homogeneous| No       |\\n| MAMuJoCo (2020)  | Fully-Cooperative | Continuous | Continuous | No           | Heterogeneous| No       |\\n| LBF (2020)       | Mixed-Motive    | Discrete     | Discrete | No            | Homogeneous| Yes      |\\n| RWARE (2020)     | Mixed-Motive    | Discrete     | Discrete | Broadcast     | Homogeneous| Yes      |\\n| DM Lab2D (2020)  | Mixed-Motive    | Discrete     | Discrete | No            | Homogeneous| No       |\\n| Flatland (2020)  | Fully-Cooperative | Continuous | Discrete | No            | Homogeneous| Yes      |\\n| SMART (2020)     | Mixed-Motive    | Continuous & Discrete | Continuous & Discrete | No | Heterogeneous| Yes |\\n| MATE (Ours)      | Fully-Coop. & Fully-Comp. | Continuous & Discrete | Peer-to-Peer | Heterogeneous | Yes |\\n\\nTarget agents with communications. Lastly, we employed the adversarial training algorithms (Policy Space Response Oracle (PSRO), fictitious self-play (FSP), and self-play) in our environment. The results suggest that training with such methods will decrease the exploitability of the trained policies and thus improve their robustness. Regarding the development aspects of MATE, it is extensively integrated with the OpenAI Gym API, which enables excellent compatibility with popular RL libraries such as RLlib, Tianshou, Stable-Baselines-3, and other Gym compatible frameworks. The lightweight of the MATE attributes to efficient computations on CPUs. This property facilitates the opportunities for mass-scale parallelisms using high-throughput architectures like Ape-X and IMPALA, etc. Also catering to various research needs, the setting of the environment can also be easily configured, including action space, observability, reward structure, population, and scene layouts. The development team will provide detailed documentation on various features and commit long-term support to this project.\\n\\nRelated Work\\n\\nTarget Coverage Problem. The target coverage problem is to find an optimal control strategy for sensors such that the time to monitor every interested target can be as long as possible. It is a long-standing problem in directional sensor networks, robotics, and computer vision. Most previous algorithms are heuristically designed for a specific setting or application, lacking a general solution for this problem. Recently, Xu et al. built a 2D environment, formulated the problem as a multi-agent cooperative game, and introduced a hierarchical multi-agent reinforcement learning approach to solve this game. However, compared with the real-world scenarios, the environment is over-simplified due to random-walking targets and a lack of obstacles. In MATE, we aim to build a more realistic simulator for benchmarking the off-the-shelf learning algorithms, e.g., account for occlusion caused by obstacles, the limited observing area of sensors, and controllable Field-of-View (FoV) of the Pan-Tilt-Zoom (PTZ) cameras. Besides, we reformulate the problem as a cooperative-competitive game and provide an interface to control the targets, i.e., the targets are controlled by adversaries to relentlessly challenge the camera policy with new strategies for the purpose of improving the robustness and generalizability of the trackers.\\n\\nMulti-Agent Learning Environments. Besides the context of MCMT, we have seen other multi-agent learning (MAL) environments that service different tasks. Table 1 summarizes the most relevant MAL environments based on our literature review. To our findings, MATE stands out as the environment that simultaneously offers fully-cooperative & fully-competitive game types, Peer-to-Peer communication support, and heterogeneous agents.\\n\\nSelf-Play and Population-Based Training Regime. MATE experimented with three training principles to promote camera-target competition in zero-sum games. Solving zero-sum games can be highly non-trivial due to the non-transitivity (e.g., Rock-Paper-Scissor) in the policy space. Conventional self-play makes the agent continuously play against the latest copy of itself. Since the agents in MATE are heterogeneous, we adopt the asymmetric version of the self-play training method. However, self-play may fail to converge due to the lack of policy diversity, thereby trapped by the non-transitivity. Fictitious Self-Play (FSP) is a population-based method that maintains a policy memory storing past versions of the policy and uniformly samples a policy from memory as the response against the opponent. Policy Space Response Oracle with Nash...\"}"}
{"id": "SyoUVEyzJbE", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Equilibrium solver (PSRO-Nash) is also a population-based method that computes a meta-strategy distribution. Instead of a uniform distribution, the distribution computed by PSRO-Nash resembles that of a mixed-strategy Nash Equilibrium. Recently, many efforts have been spent on extending PSRO methods to diverse PSRO methods \\\\[71\\\\], no-regret PSRO methods \\\\[70\\\\], and PSRO with meta-learning \\\\[72\\\\], \\\\[73\\\\]. In this paper, we conduct experiments to demonstrate the effectiveness of these training regimes for improving and evaluating the robustness of the tracking agents.\\n\\n3. MATE: the Multi-Agent Tracking Environment\\n\\nIn this section, we will introduce various details about the MATE environment. There are 4 kinds of entities in this 2D mini-world (shown in Fig. 1):\\n\\n- NC proactive cameras \\\\[\\\\mathcal{C} = \\\\{c_i\\\\}\\\\]\\n- NT mobile targets \\\\[\\\\mathcal{T} = \\\\{t_i\\\\}\\\\]\\n- NO static obstacles, and\\n- NW (= 4) warehouses storing cargoes.\\n\\nThe reward structure inside MATE resembles the \\\"min-max\\\" nature of a cooperative-competitive multi-agent game. Camera agents must maximize their coverage rate collaboratively while minimizing repeated detection on the same target. In the meantime, targets transport cargoes between warehouses as fast as possible while minimizing the surveillance from the cameras. The role of obstacles in the environment is to provide temporary shelter against camera surveillance, but at the same time can act as roadblocks on the path to the destination for the target agents. The warehouses are scattered at the four corners of the mini-world.\\n\\nPlenty of cargo needed to be delivered by the targets between the warehouses, in which the cargoes are priced based on the delivery duration.\\n\\n3.1 Entities and States\\n\\nWe define the state as the internal attributes of the entities, which may change continuously as the environment progresses. Every agent (or controllable entity) may obtain its own states (public + private) but can only observe the public states of other agents.\\n\\nCamera is an in-place, zoomable, directional sensor with a pie-shape field of view. The publicly accessible state of the camera \\\\[s_{pub} = [x, y, r, R_s, \\\\phi, \\\\theta]\\\\] contains the self-location data in the world coordinate system, the physical radius \\\\(r\\\\), the visible line of sight \\\\(R_s\\\\), the viewing direction angle \\\\(\\\\phi\\\\), and field-of-view angle \\\\(\\\\theta\\\\). In addition to these, the camera's privately accessible state \\\\[s_{pvt} = [s_{pub}, R_s, \\\\text{max}, \\\\Delta \\\\phi_{max}, \\\\Delta \\\\theta_{max}]\\\\] are constants indicating the maximum possible values for these parameters, with \\\\(\\\\Delta \\\\phi_{max}\\\\) being the camera's maximum rotation speed and \\\\(\\\\Delta \\\\theta_{max}\\\\) being the maximum zooming speed.\\n\\nTarget is a mobile vehicle equipped with an advanced omnidirectional sensor for which obstacles would not block the sensing field. The publicly accessible state \\\\[s_{pub} = [x, y, R_s, I]\\\\] consists of the self-location data, the sensible range \\\\(R_s\\\\), and an indicating variable \\\\(I_{[\\\\text{loaded}]}\\\\) that indicates whether the target is loaded with payloads. The privately accessible state \\\\[s_{pvt} = [s_{pub}, v_{max}, W(1), \\\\ldots, W(N_W), E(1), \\\\ldots, E(N_W)]\\\\] are the maximum movement speed \\\\(v_{max}\\\\), a one-hot-like vector \\\\(W\\\\) to indicate the payload destination, and a bit array \\\\(E\\\\) to \\\"memorize\\\" if the previously visited warehouse is empty. There are two kinds of vehicles for targets, one with high speed and small carrying capacity, and the other with low speed and large carrying capacity. The former is twice as fast as the latter but has a halved carrying capacity.\\n\\nThe Obstacle is a circular-shape static object that randomly spawns (controlled by a distribution) in the environment. Targets may use obstacles to stay hidden from camera surveillance. We added a transmittance attribute to the obstacles, so the cameras with a particular chance can detect the target hidden behind the obstacle at each timestep. This design feature prevents the target agents from over-reliance on this shortsighted strategy. The state of an obstacle includes the location and the radius, i.e., \\\\[s_o = s_{pub_o} = [x, y, r]\\\\].\\n\\n4\"}"}
{"id": "SyoUVEyzJbE", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Camera\\n\\n\\\\[ \\\\mathbf{R} \\\\]\\n\\n\\\\[ \\\\mathbf{r} \\\\]\\n\\n\\\\[ \\\\max \\\\theta \\\\]\\n\\nObstacle 1\\n\\nObstacle 2\\n\\nObstacle 3\\n\\nObstacle 4\\n\\nTarget 1\\n\\nTarget 2\\n\\nTarget 3\\n\\nTarget 4\\n\\nTarget 5\\n\\n(a) Camera Observation (within the green sector)\\n\\nCamera 1\\n\\nCamera 2\\n\\n... Camera 3\\n\\nTarget\\n\\nTarget\\n\\nTarget\\n\\nTarget\\n\\nTarget\\n\\nB. Target Observation (within the dotted circle)\\n\\nFigure 2: Schematic diagram of agent sight ranges. The agent (at the center, colored in purple) can obtain its own privately accessible state, and other agents' and obstacles' publicly accessible states within the sight range (colored in yellow and gray). The camera can perceive the target (colored in orange) behind an obstacle with the probability value of the obstacle's transmittance.\\n\\nCamera observations\\n\\nThe observation of a camera \\\\( c \\\\in \\\\mathcal{C} \\\\) can be divided into five parts, the preserved data \\\\( s_{\\\\text{psrv}} \\\\), its own privately accessible state \\\\( s_{pvt} \\\\), and the publicly accessible states of targets and obstacles and other cameras within its field of view. The preserved data \\\\( s_{\\\\text{psrv}} \\\\) contains global information about the environment, such as the number of each category of entities, the current agent ID in the team, etc. Fig. 2a shows an example of the camera's perception in the green-shaded area.\\n\\nTarget observations\\n\\nThe observation of the target \\\\( t \\\\in \\\\mathcal{T} \\\\) is also composed of multiple parts, including the preserved data \\\\( s_{\\\\text{psrv}} \\\\), its own privately accessible state \\\\( s_{pvt} \\\\); and the publicly accessible states of cameras, obstacles and other targets. Fig. 2b helps to demonstrate an example of the target's sight range.\\n\\nPlease refer to the supplementary for the mathematical formulation of the observations of both camera and target agents.\\n\\n3.3 Actions\\n\\nCamera is an in-place directional sensor with two types of continuous actions: rotation and zooming. These two action parameters can be adjusted simultaneously. Together they determine the shape of the camera's perception zone, though the area of perception remains unchanged. This design consideration balances the expected values of the number of perceivable targets under all possible action parameters.\\n\\nTarget is a mobile agent that can move freely inside this mini-world. Target's action space consists of the displacement vector \\\\( \\\\mathbf{v} = (\\\\Delta x, \\\\Delta y) \\\\) in Cartesian coordinates.\\n\\n3.4 Reward Structure and Types of Stochastic Games\\n\\nMATE can switch between three reward mechanisms corresponding to the typical three types of stochastic games. But first and foremost, we will motivate the use of Mean Coverage Rate as a reward function, which is an important metric used for evaluating MCMT tasks.\\n\\nMean Coverage Rate\\n\\nOne typical method in RL for evaluating different models' performances on an equal basis is using the mean reward of every episode. However, we argue that mean coverage rate, while strongly correlating to episode mean reward, is comparatively a more intuitive measure for evaluating the performance of camera agents on tracking tasks. The mean coverage rate is also a normalized measure that eases the problem of comparing results from different environment configurations. For an episode with length \\\\( L \\\\), it is:\\n\\n\\\\[\\n\\\\text{Mean Coverage Rate} = \\\\frac{1}{L} \\\\sum_{k=1}^{L} \\\\frac{\\\\text{Number of Detected Targets at step } k}{\\\\text{Total Number of Targets}}.\\n\\\\]\\n\\nIn an intra-team fully-cooperative game, all agents in the same team will receive team rewards based on the team performance and would not differ between individuals. MATE, by default, uses Mean...\"}"}
{"id": "SyoUVEyzJbE", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Coverage Rate as the team reward for the camera agents. For the target agents, we propose the transport reward to incentivize the transport of goods with the awareness of avoiding detection. Formally, the target team reward is $r(T) = F + B$ and empirically we keep $F$ and $B$ roughly equal. $F$ stands for \u201cfreight\u201d, a fixed-value sparse reward received upon every successful delivery of the assigned cargoes. $B$ is short for \u201cbounty\u201d on every cargo. The value of the bounty will depreciate per time-step and further decrease if the cameras have detected the target that carries this cargo.\\n\\nWhereas in the setting of inter-team fully-competitive game, we let the camera team receives the opposite value of the team reward of the targets, i.e., $r(C) = -r(T)$. The two teams are kept to play a zero-sum game in the environment. In addition to the built-in team rewards mentioned above, users can customize the reward functions with wrappers. Depending on the user configuration, the game setting may transform into a general mixed-motive game.\\n\\n### Core Features of MATE\\n\\n**Sample Efficient and Easy to Use**\\n\\nMATE is a multi-agent environment based on numerical simulation and implemented in pure Python with minimal dependencies. Users can install MATE with a single shell command. Without parallelization, a single-thread program can sample around 300 steps per second on a modern CPU in the default configuration (4 cameras, 8 targets, 9 obstacles). Besides, MATE ships with various custom wrappers and built-in rule-based agents, and the existing algorithms can run on MATE with few modifications. The source code is released under the MIT Open Source License with detailed documentation. The MATE environment is out-of-the-box compatible with OpenAI Gym API.\\n\\n```python\\nimport mate\\nenv = mate.make('MultiAgentTracking-v0')  # or gym.make\\nenv.reset()\\ndone = False\\nwhile not done:\\ncamera_joint_action, target_joint_action = env.action_space.sample()\\n((camera_joint_observation, target_joint_observation),\\n(camera_team_reward, target_team_reward),\\ndone,\\n(camera_infos, target_infos)) = env.step((camera_joint_action, target_joint_action))\\n```\\n\\n### Communicative Agents\\n\\nMATE implements an intra-team communication channel for each team that supports both broadcast and Peer-to-Peer communication. Unlike the widely used MPE environment, we explicitly isolate messages from agent observations so the user may customize the message format, such as vectors or texts. Communication facilitates strategic coordination among agents, preventing unnecessary exploration and repeated efforts, especially in a partially observable environment.\\n\\n![Diagram](image_url)\\n\\n**Figure 3:** Illumination of the multi-round communication mechanism in MATE.\"}"}
{"id": "SyoUVEyzJbE", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Overview of the baseline algorithms. The TarMAC and I2C algorithms are communication add-ons that can utilize with other multi-agent reinforcement learning algorithms.\\n\\n| Algorithm | Category | Centralized Training | On/Off-Policy | Action Space | Communication |\\n|-----------|----------|----------------------|---------------|--------------|---------------|\\n| QMIX      | Value-based | No                   | Off           | Discrete     | No            |\\n| MADDPG    | Policy-based | No                   | On            | Continuous   | No            |\\n| IPPO      | Policy-based | Yes                  | On            | Discrete / Continuous | No |\\n| MAPPO     | Policy-based | Yes                  | On            | Discrete / Continuous | Broadcast |\\n| TarMAC    | Communication | Yes                  | On / Off     | Discrete / Continuous | Broadcast |\\n| I2C       | Communication | Yes                  | On / Off     | Discrete / Continuous | Peer-to-Peer |\\n\\nUsers may add custom wrappers to simulate random signal noise, distance-based delays, restricted communication ranges, limited bandwidths, etc. As Fig. 3 illustrates, MATE supports multi-round communications, allowing agents to exchange several rounds of information within the same environment step. This mechanism can facilitate more multi-agent research regarding negotiations and conversations.\\n\\nHeterogeneous and Asymmetric\\nThe heterogeneity of the MATE environment exhibits two aspects: inter-team and intra-team. First, the agents in two opposite teams are completely distinct regarding their dynamics and tasks. Second, agents within the same team may have different abilities, e.g., the vehicles for targets with varying carrying capacities and movement speeds. These heterogeneities reflect a realistic theme and will allow agents to emerge with diversified strategies and individuality in a complex environment. The MATE environment hosts an asymmetric competitive game with two teams of heterogeneous agents. Asymmetry is not only reflected in the heterogeneity of agents but also in the variable quantities of players in the teams. Under different game setups, the equilibrium of the game change relative to the strength of both teams, and their corresponding strategy should also adapt accordingly.\\n\\nVariety, Flexibility, and Scalability\\nIn our default configuration, two groups of agents perform two tasks \u2013 the target coverage task for the camera agents and the transport task for the target agents. But with our user-friendly framework, researchers may extend this environment to suit more missions for the agents, such as deciding camera placement, trajectory prediction, resource collection, etc. The default setup of the environment reflects the most complex setting in which the following features are enabled: 1) mixed cooperation-competition, 2) continuous action, 3) partially observable, 4) communicative, and 5) team reward only. MATE is highly modularized so that the users can convert to different environments to suit their particular needs with our provided wrappers (presented in the supplementary). Users can train a target or camera network curriculum by dynamically adjusting the difficulty levels and transferring these policies across different settings. The number of entities in the environment is also configurable. MATE may support simultaneous interactions between two to hundreds of agents. As the quantity of agents varies, the complexity and difficulty of the environment also change accordingly, which allows the emergence of diverse strategies. Simply by varying the number of agents on both sides, the users can test the robustness of the newly-developed Multi-Agent Learning (MAL) algorithms.\\n\\nExperiments\\nIn this section, we will present the results for (1) collaborative game where training one team of agents (either cameras or targets) against rule-based opponents (2) additionally incorporates multi-agent communications into the collaboration games (3) competitive game where training two teams of agents using asymmetric self-play or Population Based Training (PBT).\\n\\nFor fair comparisons in the cooperative games, we ran each experiment between different algorithms for 10 million environment steps. The model performance was averaged across experiments from three random seeds to reflect statistical properties. We use and extend the RLlib to implement QMIX, MADDPG, IPPO, MAPPO, TarMAC, and I2C algorithms in all of our experiments. Table 2 lists the properties of the baseline algorithms.\"}"}
{"id": "SyoUVEyzJbE", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We conduct feasibility checks on the train-ability of camera agents using MARL algorithms on MATE and accordingly report the training performance of the camera's policies on the 4C vs. 8T setting. We presented a hierarchical RL (HRL) model with a selection-based low-level policy, which is controlled by a rule-based executor. The MARL algorithms only learn the high-level policy. Moreover, we added a 5-steps frame-skip to the high-level policy, in which the low-level policy would receive a signal from the high-level policy once in 5 timesteps. The results in Fig. 4a demonstrate the superior performance of the HRL methods and the steady convergence of the PPO-based methods. Fig. 4b shows the effect of adding multi-agent communication modules to various MARL algorithms on the Mean Coverage Rate. Enabling communication for the MAPPO algorithm with the hierarchical agent structure hardly shows improvement in convergence. We believe it is because of the strong inductive bias of the HRL method, given that the policies for the low-level executors are based on pre-set rules (recall that only the manager policy is trained).\\n\\nIn this section, we run multiple MARL algorithms to train the target agents competing against rule-based (greedy) cameras. As expressed in Fig. 5, target agents experience more conclusive performance gains by incorporating the multi-agent communication module compared to the camera agents shown in Fig. 4b. IPPO with the TarMAC protocol may achieve comparable performance to MAPPO+TarMAC, given that the critic model of the latter additionally has access to the global states. MAPPO+I2C attains the best convergence out of all methods implemented. Fig. 6 reflects the difficulty in normalized reward for two different settings. In 2C vs. 4T (0O), partial observability does not significantly hinder learning for MAPPO but causes an approximately 0.35 decrease in normalized episode reward for the IPPO. In 4C vs. 8T (0O), IPPO failed.\"}"}
{"id": "SyoUVEyzJbE", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: A comparison between two different settings shows the significance of the observability mode in learning meaningful policies for the target agents. In both modes of observability while MAPPO suffered an approximately 0.1 decrease in the reward when switched to partial observability.\\n\\n5.3 Zero-sum Fully-competitive Game\\n\\nIn Section 5.1 and 5.2, we present the results of training camera or target agents against fixed-policy opponents. However, as these models implicitly treat their opponents as integrated parts of the non-stationary environment, this would often result in over-fitting or failure to generalize against new opponents [39]. In realistic deployment, a stable and robust solution is often preferred over a better-performing but brittle solution. Therefore, to improve the robustness of the camera policy, we proposed to train camera and target agents in co-evolution with a zero-sum payoff structure. For the experiment, we trained the agents with three population-based methods: PSRO-Nash [39], Fictitious Self-Play [40], (asymmetric) self-play, and present the performance comparison in Fig. 7.\\n\\nNote that the exploitability of a policy or a population of policies is an intuitive measure of robustness. The exploitability, in the context of MATE, is formally defined as:\\n\\n$$\\\\text{exploitability}(\\\\Pi_C, \\\\Pi_T) = \\\\frac{1}{2} \\\\sum_{i \\\\in \\\\{C, T\\\\}} [U_i(BR(\\\\Pi_{-i}), \\\\Pi_{-i}) - U_i(\\\\Pi_i, \\\\Pi_{-i})], \\\\tag{2}$$\\n\\nwhere $C, T$ refer to \u201ccamera group\u201d and \u201ctarget group\u201d, $\\\\Pi_{-i} = \\\\{C, T\\\\} \\\\setminus \\\\{i\\\\}$, $BR$ stands for \u201cbest response\u201d, $U_i(\\\\cdot, \\\\cdot)$ ($i \\\\in \\\\{C, T\\\\}$) are the utility functions, and $\\\\Pi_i$ ($i \\\\in \\\\{C, T\\\\}$) are the policy populations accordingly. For fully-competitive settings, $U_C + U_T = 0$.\\n\\nThe exploitability can be interpreted as the average performance difference between the best-response (BR) and current policies. Low exploitability implies that both opposing groups have approximately converged to their best-response policies at the current iteration, indicating proximity to the Nash equilibrium. In Fig. 7, both sub-figures show that PSRO-Nash and self-play can converge to policy populations that are less exploitable than the populations trained against non-\"}"}
{"id": "SyoUVEyzJbE", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: The probability distribution of the meta-strategy for each PSRO iteration for the policy population of both camera and target teams. Both teams adaptively change their policy according to the adversaries. The agents are trained under the 4C vs. 8T (0O) configuration.\\n\\nSince Fictitious Self-Play uniformly samples policy from the policy distribution, the random policy has an equal probability of being sampled as the other policies in the distribution at every population iteration, which therefore explains the slow decreasing trend in exploitability. Fig. 8 visualizes the progression of distribution for the meta-strategy taken by the players (the camera team and the target team). A relatively scattered Nash distribution indicates the presence of multiple plausible strategies.\\n\\nConclusion\\n\\nMulti-Agent Tracking Environment (MATE) is a novel multi-agent simulation for benchmark multi-agent reinforcement learning algorithms in the target coverage problem. MATE hosts a zero-sum cooperative-competitive, asymmetric game between \u201ccameras\u201d and \u201ctargets\u201d, in which cameras gain team rewards to maximize the number of the covered targets. MATE incorporates the intra-team peer-to-peer (P2P) communication feature and trainable adversarial target agents. This environment is built purely in Python and integrated with OpenAI Gym API enabling great compatibility and extensibility to most distributive RL frameworks. The lightweight of this environment also ensures high sampling efficiency. MATE allows for flexible configurations for the simulation environments along with highly customized scenarios to fulfill specific research needs. We conduct a series of benchmarks to show the performance of target and camera agents trained by MARL methods and algorithms in various settings. We hope this work will serve as a useful guide for the community using MATE to conduct further research.\\n\\nLimitations.\\n\\nThe first-stage focus of MATE is to provide an all-in-one benchmark for testing various MARL algorithms and a new platform for studying distributed target coverage tasks with trainable adversaries. Admittedly, the focus of MATE is lesser on the aspects of visual perceptions, for example, evaluations in three-dimensional space. For these purposes, one of our future works will extend MATE into the high-quality 3D game engine, e.g., developing realistic environments on Unreal Engine 4 (UE4) with UnrealCV.\\n\\nFair Use of the Dataset.\\n\\nDespite MATE was not purposefully designed for scenarios that incur direct violation of privacy or harm the well-being of others, the theme of MATE brings up a valuable topic of discussion with regard to a recognized conflict between the advancement of AI technologies and the integrity of social well-being. Although the possibility of direct application of MATE to other exploitative systems remains slim, the transfer of knowledge between multi-agent tracking systems is theoretically plausible. While enjoying the benefits of training smarter camera systems using MATE, we do have genuine concerns about the negative societal impacts due to the misuse of tracking technologies in repressive surveillance. We hereby advocate for more responsible use of multi-agent tracking environments including MATE and we condemn the act of using MATE or other similar systems for malicious activities.\"}"}
{"id": "SyoUVEyzJbE", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nThis project was supported by MOST-2018AAA0102004, NSFC-62061136001, China National Post-doctoral Program for Innovative Talents (Grant No. BX2021008), and Qualcomm University Research Grant. We also thank Jing Xu, Yurong Chen, and Yuanfei Wang for their insightful discussions.\"}"}
{"id": "SyoUVEyzJbE", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Jennifer Yick, Biswanath Mukherjee, and Dipak Ghosal. Wireless sensor network survey. *Computer Networks*, 52(12):2292\u20132330, 2008.\\n\\n[2] Hamid Aghajan and Andrea Cavallaro. *Multi-camera networks: principles and applications*. Academic press, 2009.\\n\\n[3] Jing Li, Jing Xu, Fangwei Zhong, Xiangyu Kong, Yu Qiao, and Yizhou Wang. Pose-assisted multi-camera collaboration for active object tracking. *AAAI Conference on Artificial Intelligence*, 34(01):759\u2013766, Apr. 2020.\\n\\n[4] Samira Hayat, Ev\u00b8 sen Yanmaz, and Raheeb Muzaffar. Survey on unmanned aerial vehicle networks for civil applications: A communications viewpoint. *IEEE Communications Surveys & Tutorials*, 18(4):2624\u20132661, 2016.\\n\\n[5] Spynel-X Infrared Search & Track System - Wide area surveillance. https://hgh-infrared.com/spynel-x.\\n\\n[6] Andries M. Heyns. Optimisation of surveillance camera site locations and viewing angles using a novel multi-attribute, multi-objective genetic algorithm: A day/night anti-poaching application. *Computers, Environment and Urban Systems*, 88:101638, 2021.\\n\\n[7] Mark M. Pitt. Smuggling and price disparity. *Journal of International Economics*, 11(4):447\u2013458, 1981.\\n\\n[8] U S Patrol. Privacy impact assessment update for the border surveillance systems (BSS). DHS/CBP , PIA-022 (A), 2018.\\n\\n[9] Matthias M\u00fcller and Vladlen Koltun. Openbot: Turning smartphones into robots. In *International Conference on Robotics and Automation (ICRA)*.\\n\\n[10] Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong Zhang, and Yizhou Wang. End-to-end active object tracking via reinforcement learning. In *International Conference on Machine Learning*, pages 3286\u20133295, 2018.\\n\\n[11] Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong Zhang, and Yizhou Wang. End-to-end active object tracking and its real-world deployment via reinforcement learning. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 42(6):1317\u20131332, 2019.\\n\\n[12] Ball tracking - A new era of tactical game analysis. https://kinexon.com/technology/ball-tracking.\\n\\n[13] Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theoretical perspective. *arXiv preprint arXiv:2011.00583*, 2020.\\n\\n[14] Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula. In *International Conference on Learning Representations*, 2020.\\n\\n[15] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw D\u02db ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. *arXiv preprint arXiv:1912.06680*, 2019.\\n\\n[16] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grand-master level in starcraft ii using multi-agent reinforcement learning. *Nature*, 575(7782):350\u2013354, 2019.\\n\\n[17] P Peng, Q Yuan, Y Wen, Y Yang, Z Tang, H Long, and J Wang. Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games. *arXiv preprint arXiv:1703.10069*, 2017.\"}"}
{"id": "SyoUVEyzJbE", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yuanpei Chen, Tianhao Wu, Shengjie Wang, Xidong Feng, Jiechuan Jiang, Zongqing Lu, Stephen Marcus McAleer, Hao Dong, Song-Chun Zhu, and Yaodong Yang. Towards human-level bimanual dexterous manipulation with reinforcement learning. In Neural Information Processing Systems Datasets and Benchmarks Track, 2022.\\n\\nYiran Geng, Boshi An, Haoran Geng, Yuanpei Chen, Yaodong Yang, and Hao Dong. End-to-end affordance learning for robotic manipulation. arXiv preprint arXiv:2209.12941, 2022.\\n\\nMing Zhou, Jun Luo, Julian Villella, Yaodong Yang, David Rusu, Jiayu Miao, Weinan Zhang, Montgomery Alban, Iman Fadakar, Zheng Chen, Aurora Chongxi Huang, Ying Wen, Kimia Hassanzadeh, Daniel Graves, Dong Chen, Zhengbang Zhu, Nhat Nguyen, Mohamed Elsayed, Kun Shao, Sanjeevan Ahilan, Baokuan Zhang, Jiannan Wu, Zhengang Fu, Kasra Rezaee, Peyman Yadmellat, Mohsen Rohani, Nicolas Perez Nieves, Yihan Ni, Seyedershad Banijamali, Alexander Cowen Rivers, Zheng Tian, Daniel Palenicek, Haitham bou Ammar, Hongbo Zhang, Wulong Liu, Jianye Hao, and Jun Wang. Smarts: Scalable multi-agent reinforcement learning training school for autonomous driving. In Conference on Robot Learning (CoRL), 2020.\\n\\nMinne Li, Zhiwei Qin, Yan Jiao, Yaodong Yang, Jun Wang, Chenxi Wang, Guobin Wu, and Jieping Ye. Efficient ridesharing order dispatching with mean field multi-agent reinforcement learning. In The World Wide Web Conference, pages 983\u2013994, 2019.\\n\\nYaodong Yang, Lantao Yu, Yiwei Bai, Jun Wang, Weinan Zhang, Ying Wen, and Yong Yu. A study of ai population dynamics with million-agent reinforcement learning. arXiv preprint arXiv:1709.04511, 2017.\\n\\nRyan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pages 6379\u20136390, 2017.\\n\\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4295\u20134304. PMLR, 10\u201315 Jul 2018.\\n\\nChao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of PPO in cooperative multi-agent games. In Neural Information Processing Systems Datasets and Benchmarks Track, 2022.\\n\\nJakub Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. In International Conference on Learning Representations, 2021.\\n\\nJakub Grudzien Kuba, Xidong Feng, Shiyao Ding, Hao Dong, Jun Wang, and Yaodong Yang. Heterogeneous-agent mirror learning: A continuum of solutions to cooperative marl. arXiv preprint arXiv:2208.01682, 2022.\\n\\nJing Xu, Fangwei Zhong, and Yizhou Wang. Learning multi-agent coordination for enhancing target coverage in directional sensor networks. Advances in Neural Information Processing Systems, 33:10053\u201310064, 2020.\\n\\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.\\n\\nMing Zhou, Ziyu Wan, Hanjing Wang, Muning Wen, Runzhe Wu, Ying Wen, Yaodong Yang, Weinan Zhang, and Jun Wang. Malib: A parallel framework for population-based multi-agent reinforcement learning. arXiv preprint arXiv:2106.07551, 2021.\\n\\nJakub Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. In International Conference on Learning Representations, 2022.\"}"}
{"id": "SyoUVEyzJbE", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kaiqing Zhang, Zhuoran Yang, and Tamer Ba\u015far. Policy Optimization Provably Converges to Nash Equilibria in Zero-Sum Linear Quadratic Games. Red Hook, NY, USA, 2019.\\n\\nKyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. QTRAN: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5887\u20135896. PMLR, 09\u201315 Jun 2019.\\n\\nJulien Perolat, Bilal Piot, and Olivier Pietquin. Actor-critic fictitious play in simultaneous move multistage games. In International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pages 919\u2013928. PMLR, 09\u201311 Apr 2018.\\n\\nXiaotie Deng, Yuhao Li, David Henry Mguni, Jun Wang, and Yaodong Yang. On the complexity of computing markov perfect equilibrium in general-sum stochastic games. arXiv preprint arXiv:2109.01795, 2021.\\n\\nZiluo Ding, Tiejun Huang, and Zongqing Lu. Learning individually inferred communication for multi-agent cooperation. In Advances in Neural Information Processing Systems, volume 33, pages 22069\u201322079, 2020.\\n\\nYuanfei Wang, Fangwei Zhong, Jing Xu, and Yizhou Wang. Tom2c: Target-oriented multi-agent communication and cooperation with theory of mind. In International Conference on Learning Representations, 2022.\\n\\nChristian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip H. S. Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020.\\n\\nMarc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien P\u00e9rolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multi-agent reinforcement learning. Advances in neural information processing systems, 30, 2017.\\n\\nJohannes Heinrich, Marc Lanctot, and David Silver. Fictitious self-play in extensive-form games. In Francis R. Bach and David M. Blei, editors, International Conference on Machine Learning, volume 37 of JMLR Workshop and Conference Proceedings, pages 805\u2013813. JMLR.org, 2015.\\n\\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\\n\\nEric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gonzalez, Michael Jordan, and Ion Stoica. RLlib: Abstractions for distributed reinforcement learning. In International Conference on Machine Learning, volume 80, pages 3053\u20133062. PMLR, 10\u201315 Jul 2018.\\n\\nJiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su, and Jun Zhu. Tianshou: A highly modularized deep reinforcement learning library. Journal of Machine Learning Research, 23(267):1\u20136, 2022.\\n\\nAntonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1\u20138, 2021.\\n\\nDan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt, and David Silver. Distributed prioritized experience replay. In International Conference on Learning Representations, 2018.\\n\\nLasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures. In International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1407\u20131416. PMLR, 10\u201315 Jul 2018.\"}"}
{"id": "SyoUVEyzJbE", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"S Mini, Siba K Udgata, and Samrat L Sabat. Sensor deployment and scheduling for target coverage problem in wireless sensor networks. IEEE Sensors Journal, 14(3):636\u2013644, 2013.\\n\\nHuadong Ma and Yonghe Liu. Some problems of directional sensor networks. International Journal of Sensor Networks, 2(1-2):44\u201352, 2007.\\n\\nM Amac Guvensan and A Gokhan Yavuz. On coverage issues in directional sensor networks: A survey. Ad Hoc Networks, 9(7):1238\u20131255, 2011.\\n\\nDimitrios Zorbas, Luigi Di Puglia Pugliese, Tahiry Razafindralambo, and Francesca Guerriero. Optimal drone placement and cost-efficient target coverage. Journal of Network and Computer Applications, 75:16\u201331, 2016.\\n\\nAhmed Saeed, Ahmed Abdelkader, Mouhyemen Khan, Azin Neishaboori, Khaled A Harras, and Amr Mohamed. Argus: realistic target coverage by drones. In ACM/IEEE International Conference on Information Processing in Sensor Networks, pages 155\u2013166, 2017.\\n\\nMouhyemen Khan, Karel Heurtefeux, Amr Mohamed, Khaled A Harras, and Mohammd Mehedi Hassan. Mobile target coverage and tracking on drone-be-gone uav cyber-physical testbed. IEEE Systems Journal, 12(4):3485\u20133496, 2017.\\n\\nEva Tuba, Romana Capor-Hrosik, Adis Alihodzic, and Milan Tuba. Drone placement for optimal coverage by brain storm optimization algorithm. In International Conference on Hybrid Intelligent Systems, pages 167\u2013176. Springer, 2017.\\n\\nErgys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and a data set for multi-target, multi-camera tracking. In European Conference on Computer Vision, pages 17\u201335. Springer, 2016.\\n\\nLianmin Zheng, Jiacheng Yang, Han Cai, Ming Zhou, Weinan Zhang, Jun Wang, and Yong Yu. Magent: A many-agent reinforcement learning platform for artificial collective intelligence. In AAAI Conference on Artificial Intelligence, volume 32, 2018.\\n\\nCinjon Resnick, Wes Eldridge, David Ha, Denny Britz, Jakob Foerster, Julian Togelius, Kyunghyun Cho, and Joan Bruna. Pommerman: A multi-agent playground. arXiv preprint arXiv:1809.07124, 2018.\\n\\nDiego Perez-Liebana, Katja Hofmann, Sharada Prasanna Mohanty, Noburu Kuno, Andre Kramer, Sam Devlin, Raluca D. Gaina, and Daniel Ionita. The multi-agent reinforcement learning in malm\u00d6 (marl\u00d6) competition. arXiv preprint arXiv:1901.08129, 2019.\\n\\nNolan Bard, Jakob N. Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H. Francis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, Iain Dunning, Shibl Mourad, Hugo Larochelle, Marc G. Bellemare, and Michael Bowling. The hanabi challenge: A new frontier for ai research. Artificial Intelligence, 280, 2020.\\n\\nJoseph Suarez, Yilun Du, Clare Zhu, Igor Mordatch, and Phillip Isola. The neural mmo platform for massively multiagent research. In Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021.\\n\\nKarol Kurach, Anton Raichuk, Piotr Stanczyk, Michal Zajac, Olivier Bachem, Lasse Espeholt, Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, and Sylvain Gelly. Google research football: A novel reinforcement learning environment. In AAAI Conference on Artificial Intelligence, 2020.\\n\\nBei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin Boehmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy gradients. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 12208\u201312221, 2021.\\n\\nFilippos Christianos, Lukas Sch\u00e4fer, and Stefano Albrecht. Shared experience actor-critic for multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, volume 33, pages 10707\u201310717, 2020.\"}"}
{"id": "SyoUVEyzJbE", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Charles Beattie, Thomas K\u00f6ppe, Edgar A. Du\u00f1ez-Guzm\u00e1n, and Joel Z. Leibo. Deepmind lab2d. arXiv preprint arXiv:2011.07027, 2020.\\n\\nSharada Mohanty, Erik Nygren, Florian Laurent, Manuel Schneider, Christian Scheller, Nilabha Bhattacharya, Jeremy Watson, Adrian Egli, Christian Eichenberger, Christian Baumberger, Gereon Vienken, Irene Sturm, Guillaume Sartoretti, and Giacomo Spigler. Flatland-rl: Multi-agent reinforcement learning on trains. arXiv preprint arXiv:2012.05893, 2020.\\n\\nRicky Sanjaya, Jun Wang, and Yaodong Yang. Measuring the non-transitivity in chess. Algorithms, 15(5):152, 2022.\\n\\nFangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, and Yizhou Wang. AD-V AT: An asymmetric dueling mechanism for learning visual active tracking. In International Conference on Learning Representations, 2019.\\n\\nFangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, and Yizhou Wang. Ad-vat+: An asymmetric dueling mechanism for learning and understanding visual active tracking. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(5):1467\u20131482, 2021.\\n\\nFangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, and Yizhou Wang. Towards distraction-robust active visual tracking. In International Conference on Machine Learning, pages 12782\u201312792. PMLR, 2021.\\n\\nYaodong Yang, Jun Luo, Ying Wen, Oliver Slumbers, Daniel Graves, Haitham Bou Ammar, Jun Wang, and Matthew E Taylor. Diverse auto-curriculum is critical for successful real-world multiagent learning systems. In International Conference on Autonomous Agents and MultiAgent Systems, pages 51\u201356, 2021.\\n\\nLe Cong Dinh, Stephen Marcus McAleer, Zheng Tian, Nicolas Perez-Nieves, Oliver Slumbers, David Henry Mguni, Jun Wang, Haitham Bou Ammar, and Yaodong Yang. Online double oracle. Transactions on Machine Learning Research, 2022.\\n\\nNicolas Perez-Nieves, Yaodong Yang, Oliver Slumbers, David H Mguni, Ying Wen, and Jun Wang. Modelling behavioural diversity for learning in open-ended games. In International Conference on Machine Learning, pages 8514\u20138524. PMLR, 2021.\\n\\nXiangyu Liu, Hangtian Jia, Ying Wen, Yujing Hu, Yingfeng Chen, Changjie Fan, Zhipeng Hu, and Yaodong Yang. Towards unifying behavioral and response diversity for open-ended learning in zero-sum games. Advances in Neural Information Processing Systems, 34:941\u2013952, 2021.\\n\\nXidong Feng, Oliver Slumbers, Ziyu Wan, Bo Liu, Stephen McAleer, Ying Wen, Jun Wang, and Yaodong Yang. Neural auto-curricula in two-player zero-sum games. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nBo Liu, Xidong Feng, Haifeng Zhang, Jun Wang, and Yaodong Yang. Settling the bias and variance of meta-gradient estimation for meta-reinforcement learning. arXiv preprint arXiv:2112.15400, 2021.\\n\\nKris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, and Stephen Clark. Emergent communication through negotiation. In International Conference on Learning Representations, 2018.\\n\\nYoram Bachrach, Richard Everett, Edward Hughes, Angeliki Lazaridou, Joel Z Leibo, Marc Lanctot, Michael Johanson, Wojciech M Czarnecki, and Thore Graepel. Negotiating team formation using deep reinforcement learning. Artificial Intelligence, 288:103356, November 2020.\\n\\nDiane Bouchacourt and Marco Baroni. Miss tools and mr fruit: Emergent communication in agents learning about object affordances. arXiv preprint arXiv:1905.11871, 2019.\\n\\nAbhishek Das, Th\u00e9ophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and Joelle Pineau. Tarmac: Targeted multi-agent communication. In International Conference on Machine Learning, pages 1538\u20131546. PMLR, 2019.\"}"}
