{"id": "8eVgdwKs2N", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks\\n\\nZhaoyu Li, Jinpei Guo, Xujie Si\\n\\n1 University of Toronto, 2 Vector Institute, 3 Mila, 4 Shanghai Jiao Tong University\\n\\n{zhaoyu, six}@cs.toronto.edu, mike0728@sjtu.edu.cn\\n\\nAbstract\\n\\nGraph neural networks (GNNs) have recently emerged as a promising approach for solving the Boolean Satisfiability Problem (SAT), offering potential alternatives to traditional backtracking or local search SAT solvers. However, despite the growing volume of literature in this field, there remains a notable absence of a unified dataset and a fair benchmark to evaluate and compare existing approaches. To address this crucial gap, we present G4SATBench, the first benchmark study that establishes a comprehensive evaluation framework for GNN-based SAT solvers. In G4SATBench, we meticulously curate a large and diverse set of SAT datasets comprising 7 problems with 3 difficulty levels and benchmark a broad range of GNN models across various prediction tasks, training objectives, and inference algorithms. To explore the learning abilities and comprehend the strengths and limitations of GNN-based SAT solvers, we also compare their solving processes with the heuristics in search-based SAT solvers. Our empirical results provide valuable insights into the performance of GNN-based SAT solvers and further suggest that existing GNN models can effectively learn a solving strategy akin to greedy local search but struggle to learn backtracking search in the latent space.\\n\\n1 Introduction\\n\\nThe Boolean Satisfiability Problem (SAT) is a crucial problem at the nexus of computer science, logic, and operations research, which has garnered significant attention over the past five decades. To solve SAT instances efficiently, modern SAT solvers have been developed with backtracking (especially with conflict-driven clause learning, a.k.a. CDCL) or local search (LS) heuristics that effectively exploit the instance's structure and traverse its vast search space [4]. However, designing such heuristics remains a highly non-trivial and time-consuming task, with a lack of significant improvement in recent years. Conversely, the recent rapid advances in graph neural networks (GNNs) [23, 27, 41] have shown impressive performances in analyzing structured data, offering a promising opportunity to enhance or even replace modern SAT solvers. As such, there have been massive efforts to leverage GNNs to solve SAT over the last few years [16, 19]. Despite the recent progress, the question of how (well) GNNs can solve SAT remains unanswered. One of the main reasons for this is the variety of learning objectives and usage scenarios employed in existing work, making it difficult to evaluate different methods in a fair and comprehensive manner. For example, NeuroSAT [34] predicts satisfiability, QuerySAT [30] constructs a satisfying assignment, NeuroCore [33] classifies unsat-core variables, and NSNet [28] predicts marginal distributions of all satisfying solutions to solve the SAT problem. Moreover, most previous research has experimented on different datasets that vary in a range of settings (e.g., data distribution, instance size, and dataset size).\"}"}
{"id": "8eVgdwKs2N", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"which leads to a lack of unified and standardized datasets for training and evaluation. Additionally, some work [2, 35, 42] has noted the difficulty of re-implementing prior approaches as baselines, rendering it arduous to draw consistent conclusions about the performance of peer approaches. All of these issues impede the development of GNN-based solvers for SAT solving.\\n\\nTo systematically quantify the progress in this field and facilitate rapid, reproducible, and generalizable research, we propose G4SATBench, the first comprehensive benchmark study for SAT solving with GNNs. G4SATBench is characterized as follows:\\n\\n\u2022 First, we construct a large and diverse collection of SAT datasets that includes instances from distinct sources and difficulty levels. Specifically, our benchmark consists of 7 different datasets from 3 benchmark families, including random instances, pseudo-industrial instances, and combinatorial problems. It not only covers a wide range of prior datasets but also introduces 3 levels of difficulty for each dataset to enable fine-grained analyses.\\n\\n\u2022 Second, we re-implement various GNN-based SAT solvers with unified interfaces and configuration settings, establishing a general evaluation protocol for fair and comprehensive comparisons. Our framework allows for evaluating different GNN models in SAT solving with various prediction tasks, training objectives, and inference algorithms, encompassing the diverse learning frameworks employed in the existing literature.\\n\\n\u2022 Third, we present baseline results and conduct thorough analyses of GNN-based SAT solvers, providing a detailed reference of prior work and laying a solid foundation for future research. Our evaluations assess the performances of different choices of GNN models (e.g., graph constructions, message-passing schemes) with particular attention to some critical parameters (e.g., message-passing iterations), as well as their generalization ability across different distributions.\\n\\n\u2022 Lastly, we conduct a series of in-depth experiments to explore the learning abilities of GNN-based SAT solvers. Specifically, we compare the training and solving processes of GNNs with the heuristics employed in both CDCL and LS-based SAT solvers. Our experimental results reveal that GNNs tend to develop a solving heuristic similar to greedy local search to find a satisfying assignment but fail to effectively learn the backtracking heuristic in the latent space.\\n\\nWe believe that G4SATBench will enable the research community to make significant strides in understanding the capabilities and limitations of GNNs for solving SAT and facilitate further development in this area. Our codebase is available at https://github.com/zhaoyu-li/G4SATBench.\\n\\n2 Related Work\\n\\nSAT solving with GNNs. Existing GNN-based SAT solvers can be broadly categorized into two branches [16]: standalone neural solvers and neural-guided solvers. Standalone neural solvers utilize GNNs to solve SAT instances directly. For example, a stream of research [6, 34, 21, 7, 35] focuses on predicting the satisfiability of a given formula, while several alternative approaches [1, 2, 30, 26, 42] aim to construct a satisfying assignment. Neural-guided solvers, on the other hand, integrate GNNs with modern SAT solvers, trying to improve their search heuristics with the prediction of GNNs. These methods typically train GNN models using supervised learning on some tasks such as unsat-core variable prediction [33, 38], satisfying assignment prediction [44], glue variable prediction [17], and assignment marginal prediction [28], or through reinforcement learning [43, 24] by modeling the entire search procedure as a Markov decision process. Despite the rich literature on SAT solving with GNNs, there is no benchmark study to evaluate and compare the performance of these GNN models. We hope the proposed G4SATBench would address this gap.\\n\\nSAT datasets. Several established SAT benchmarks, including the prestigious SATLIB [20] and the SAT Competitions over the years, have provided a variety of practical instances to assess the performance of modern SAT solvers. Regrettably, these datasets are not particularly amenable for GNNs to learn from, given their relatively modest scale (less than 100 instances for a specific domain) or overly extensive instances (exceeding 10 million variables and clauses). To address this issue,\"}"}
{"id": "8eVgdwKs2N", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"researchers have turned to synthetic SAT instance generators [34, 25, 14, 37], which allow for the creation of a flexible number of instances with customizable settings. However, most of the existing datasets generated from these sources are limited to a few domains (less than 3 generators), small in size (less than 10k instances), or easy in difficulty (less than 40 variables within an instance), and there is no standardized dataset for evaluation. In G4SATBench, we include a variety of synthetic generators with carefully selected configurations, aiming to construct a broad collection of SAT datasets that are highly conducive for training and evaluating GNNs.\\n\\n3 Preliminaries\\n\\nThe SAT problem.\\nIn propositional logic, a Boolean formula is constructed from Boolean variables and logical operators such as conjunctions (\\\\( \\\\land \\\\)), disjunctions (\\\\( \\\\lor \\\\)), and negations (\\\\( \\\\neg \\\\)). It is typical to represent Boolean formulas in conjunctive normal form (CNF), expressed as a conjunction of clauses, where each clause is a disjunction of literals, which can be either a variable or its negation. Given a CNF formula, the SAT problem is to determine if there exists an assignment of boolean values to its variables such that the formula evaluates to true. If this is the case, the formula is called satisfiable; otherwise, it is unsatisfiable. For a satisfiable instance, one is expected to construct a satisfying assignment to prove its satisfiability. On the other hand, for an unsatisfiable formula, one can find a minimal subset of clauses whose conjunction is still unsatisfiable. Such a set of clauses is termed the unsat core, and variables in the unsat core are referred to as unsat-core variables.\\n\\nGraph representations of CNF formulas.\\nTraditionally, a CNF formula can be represented using 4 types of graphs [4]: Literal-Clause Graph (LCG), Variable-Clause Graph (VCG), Literal-Incidence Graph (LIG), and Variable-Incidence Graph (VIG). The LCG is a bipartite graph with literal and clause nodes connected by edges indicating the presence of a literal in a clause. The VCG is formed by merging the positive and negative literals of the same variables in LCG. The LIG, on the other hand, only consists of literal nodes, with edges indicating co-occurrence in a clause. Lastly, the VIG is derived from LIG using the same merging operation as VCG.\\n\\n4 G4SATBench: A Comprehensive Benchmark on GNNs for SAT Solving\\n\\nThe goal of G4SATBench is to establish a general framework that enables comprehensive comparisons and evaluations of various GNN-based SAT solvers. In this section, we will delve into the details of G4SATBench, including its datasets, GNN models, prediction tasks, as well as training and testing methodologies. The overview of the G4SATBench framework is shown in Figure 1.\\n\\n4.1 Datasets\\n\\nG4SATBench is built on a diverse set of synthetic CNF generators. It currently consists of 7 datasets sourced from 3 distinct domain areas: random problems, pseudo-industrial problems, and combinatorial problems. Specifically, we utilize the SR generator in NeuroSAT [34] and the 3-SAT generator in CNFGen [25] to produce random CNF formulas. For pseudo-industrial problems, we employ the Community Attachment (CA) model [14] and the Popularity-Similarity (PS) model [15],...\"}"}
{"id": "8eVgdwKs2N", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"which generate synthetic instances that exhibit similar statistical features, such as the community and locality, to those observed in real-world industrial SAT instances. For combinatorics, we resort to 3 synthetic generators in CNFGen [25] to create SAT instances derived from the translation of \\\\( k \\\\)-Clique, \\\\( k \\\\)-Dominating Set, and \\\\( k \\\\)-Vertex Cover problems.\\n\\nIn addition to the diversity of datasets, G4SATBench offers distinct difficulty levels for all datasets to enable fine-grained analyses. These levels include easy, medium, and hard, with the latter representing more complex problems with increased instance sizes. For example, the easy SR dataset contains instances with 10 to 40 variables, the medium SR dataset contains formulas with 40 to 200 variables, and the hard SR dataset consists of formulas with variables ranging from 200 to 400. For each easy and medium dataset, we generate 80k pairs of satisfiable and unsatisfiable instances for training, 10k pairs for validation, and 10k pairs for testing. For each hard dataset, we produce 10k testing pairs.\\n\\nIt is also worth noting that the parameters for our synthetic generators are meticulously selected to avoid generating trivial cases. For instance, we produce random 3-SAT formulas at the phase-transition region where the relationship between the number of clauses \\\\( m \\\\) and variables \\\\( n \\\\) is\\n\\n\\\\[\\nm = 4^{2/3}n + 58^{2/3}n - 2/3. \\\\tag{1}\\n\\\\]\\n\\nand utilize the vertex Erd\u0151s-R\u00e9nyi graph with an edge probability of \\\\( p = \\\\frac{v}{v^2 - 1} / (v^2) \\\\) to generate \\\\( k \\\\)-Clique problems, making the expected number of \\\\( k \\\\)-Cliques in a graph equals 1 [5]. To provide a detailed characterization of our generated datasets, we compute several statistics of the SAT instances across difficulty levels in G4SATBench. For more information about the generators we used and the dataset statistics, please refer to Appendix A.\\n\\n4.2 GNN Baselines\\n\\nGraph constructions. It is important to note that traditional graph representations of a CNF formula often lack the requisite details for optimally constructing GNNs. Specifically, the LIG and VIG exclude clause-specific information, while the LCG and VIG fail to differentiate between positive and negative literals of the same variable. To address these limitations, existing approaches typically build GNN models on the refined versions of the LCG and VIG encodings. In the LCG, a new type of edge is added between each literal and its negation, while the VCG is modified by using two types of edges to indicate the polarities of variables within a clause. These modified encodings are termed the LCG* and VCG* respectively, and an example of them is shown in Figure 2.\\n\\nMessage-passing schemes. G4SATBench enables performing various heterogeneous message-passing algorithms between neighboring nodes on the LCG* or VCG* encodings of a CNF formula. For the sake of illustration, we will take GNN models on the LCG* as an example. We first define a \\\\( d \\\\)-dimensional embedding for every literal node and clause node, denoted by \\\\( h_l \\\\) and \\\\( h_c \\\\) respectively. Initially, all these embeddings are assigned to two learnable vectors \\\\( h_0^l \\\\) and \\\\( h_0^c \\\\), depending on their node types. At the \\\\( k \\\\)-th iteration of message passing, these hidden representations are updated as:\\n\\n\\\\[\\n\\\\begin{align*}\\n    h_c^{(k)} &= \\\\text{UPD}_{AGG} \\\\left( \\\\bigcup_{l \\\\in N(c)} \\\\text{MLP}_{l} \\\\left( h_l^{(k-1)} \\\\right) \\\\right) \\\\\\\\\\n    h_c^{(k)} &= \\\\text{UPD}_{AGG} \\\\left( \\\\bigcup_{l \\\\in N(c)} \\\\text{MLP}_{l} \\\\left( h_c^{(k-1)} \\\\right) \\\\right) \\\\\\\\\\n    h_l^{(k)} &= \\\\text{UPD}_{AGG} \\\\left( \\\\bigcup_{c \\\\in N(l)} \\\\text{MLP}_{c} \\\\left( h_c^{(k-1)} \\\\right) \\\\right) \\\\\\\\\\n    h_l^{(k)} &= \\\\text{UPD}_{AGG} \\\\left( \\\\bigcup_{c \\\\in N(l)} \\\\text{MLP}_{c} \\\\left( h_l^{(k-1)} \\\\right) \\\\right)\\n\\\\end{align*}\\n\\\\]\\n\\nwhere \\\\( N(\\\\cdot) \\\\) denotes the set of neighbor nodes, MLP\\\\(_l\\\\) and MLP\\\\(_c\\\\) are two different multi-layer perceptrons (MLPs), UPD\\\\(_\\\\cdot\\\\) is the update function, and AGG\\\\(_\\\\cdot\\\\) is the aggregation function. Most GNN models on LCG* use Equation 1 with different choices of the update function and aggregation function. For instance, NeuroSAT employs LayerNormLSTM [3] as the update function and summation as the aggregation function. In G4SATBench, we provide a diverse range of GNN models, including NeuroSAT [34], Graph Convolutional Network (GCN) [23], Gated Graph Neural Network (GGNN) [27], and Graph Isomorphism Network (GIN) [41], on the both LCG* and VCG*.\\n\\nMore details of these GNN models are included in Appendix B.\"}"}
{"id": "8eVgdwKs2N", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3 Supported Tasks, Training and Testing Settings\\n\\nPrediction tasks. In G4SATBench, we support three essential prediction tasks for SAT solving: satisfiability prediction, satisfying assignment prediction, and unsat-core variable prediction. These tasks are widely used in both standalone neural solvers and neural-guided solvers. Technically, we model satisfiability prediction as a binary graph classification task, where 1/0 denotes the satisfiability/unsatisfiability of the given SAT instance $\\\\phi$. Here, we take GNN models on the LCG* as an example. After $T$ iterations of message passing, we obtain the graph embedding by applying mean pooling on all literal embeddings, and then predict the satisfiability using an MLP followed by the sigmoid function $\\\\sigma$:\\n\\n$$y_{\\\\phi} = \\\\sigma_{\\\\text{MLP}} \\\\{ h(T)|_l, l \\\\in \\\\phi \\\\}.$$ (2)\\n\\nFor satisfying assignment prediction and unsat-core variable prediction, we formulate them as binary node classification tasks, predicting the label for each variable in the given CNF formula $\\\\phi$. In the case of GNNs on the LCG*, we concatenate the embeddings of each pair of literals $h_l$ and $h_{\\\\neg l}$ to construct the variable embedding, and then readout using an MLP and the sigmoid function $\\\\sigma$:\\n\\n$$y_v = \\\\sigma_{\\\\text{MLP}} h(h(T)|_l, h(T)|_{\\\\neg l}),$$ (3)\\n\\nTraining objectives. To train GNN models on the aforementioned tasks, one common approach is to minimize the binary cross-entropy loss between the predictions and the ground truth labels. In addition to supervised learning, G4SATBench supports two unsupervised training paradigms for satisfying assignment prediction [1, 30]. The first approach aims to differentiate and maximize the satisfiability value of a CNF formula [1]. It replaces the $\\\\neg$ operator with the function $N(a) = 1 - a$ and uses smooth max and min functions to replace the $\\\\lor$ and $\\\\land$ operators. The smooth max and min functions are defined as follows:\\n\\n$$S_{\\\\text{max}}(x_1, x_2, \\\\ldots, x_d) = \\\\frac{\\\\prod_{i=1}^d x_i \\\\cdot e^{x_i/\\\\tau}}{\\\\prod_{i=1}^d e^{x_i/\\\\tau}}, S_{\\\\text{min}}(x_1, x_2, \\\\ldots, x_d) = \\\\frac{\\\\prod_{i=1}^d x_i \\\\cdot e^{-x_i/\\\\tau}}{\\\\prod_{i=1}^d e^{-x_i/\\\\tau}},$$ (4)\\n\\nwhere $\\\\tau \\\\geq 0$ is the temperature parameter. Given a predicted soft assignment $x = (x_1, x_2, \\\\ldots, x_n)$, we evaluate its satisfiability value $S(x)$ using the smoothed version of logical operators and minimize the following loss function:\\n\\n$$L_{\\\\phi}(x) = (1 - S(x)) \\\\kappa (1 - S(x)) + S(x) \\\\kappa.$$ (5)\\n\\nThe second unsupervised loss is defined as follows [30]:\\n\\n$$V_c(x) = 1 - \\\\sum_{i \\\\in c^+} (1 - x_i) \\\\sum_{i \\\\in c^-} x_i,$$\\n\\n$$L_{\\\\phi}(x) = -\\\\log V_c(x) = - \\\\sum_{c^+ \\\\subseteq \\\\phi} \\\\log (V_c(x)),$$ (6)\\n\\nwhere $c^+$ and $c^-$ are the sets of variables that occur in the clause $c$ in positive and negative form respectively. Note that these two losses reach the minimum only when the prediction $x$ is a satisfying assignment, thus minimizing such losses could help to construct a possible satisfying assignment.\\n\\nInference algorithms. In addition to using the standard readout process like training, G4SATBench offers two alternative inference algorithms for satisfying assignment prediction [34, 2]. The first method performs 2-clustering on the literal embeddings to obtain two centers $\\\\Delta_1$ and $\\\\Delta_2$ and then partitions the positive and negative literals of each variable into distinct groups based on the predicate $|| x_i - \\\\Delta_1 ||^2 + || \\\\neg x_i - \\\\Delta_2 ||^2 < || x_i - \\\\Delta_2 ||^2 + || \\\\neg x_i - \\\\Delta_1 ||^2$ [34]. This allows the construction of two possible assignments by mapping one group of literals to true. The second approach is to employ the readout function at each iteration of message passing, resulting in multiple assignment predictions for a given instance [2].\\n\\nEvaluation metrics. For satisfiability prediction and unsat-core variable prediction, we report the classification accuracy of each GNN model in G4SATBench. For satisfying assignment prediction, we report the solving accuracy of the predicted assignments. If multiple assignments are predicted for a SAT instance, the instance is considered solved if any of the predictions satisfy the formula.\"}"}
{"id": "8eVgdwKs2N", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"In this section, we present the benchmarking results of G4SATBench. To ensure a fair comparison, we conduct a grid search to tune the hyperparameters of each GNN baseline. The best checkpoint for each GNN model is selected based on its performance on the validation set. To mitigate the impact of randomness, we use 3 different random seeds to repeat the experiment in each setting and report the average performance. Each experiment is performed on a single RTX8000 GPU and 16 AMD EPYC 7502 CPU cores, and the total time cost is approximately 8,000 GPU hours. For detailed experimental setup and hyperparameters, please refer to Appendix C.1.\\n\\n### 5.1 Satisfiability Prediction\\n\\nEvaluation on the same distribution.\\n\\nTable 1 shows the benchmarking results of each GNN baseline when trained and evaluated on datasets possessing identical distributions. All GNN models exhibit strong performance across most easy and medium datasets, except for the medium SR dataset. This difficulty can be attributed to the inherent characteristic of this dataset, which includes satisfiable and unsatisfiable pairs of medium-sized instances distinguished by just a single differing literal. Such a subtle difference presents a substantial challenge for GNN models in satisfiability classification. Among all GNN models, the different graph constructions do not seem to have a significant impact on the results, and NeuroSAT (on LCG*) and GGNN (on VCG*) achieve the best overall performance.\\n\\n| Graph Method | Easy Datasets | Medium Datasets |\\n|--------------|---------------|-----------------|\\n|              | SR 3-SAT CA PS k-Clique k-Domset k-Vercov |\\n| LCG* NeuroSAT | 96.00 96.33 98.83 96.59 97.92 99.77 99.99 78.02 84.90 99.57 96.81 89.39 99.67 99.80 |\\n| GCN | 94.43 94.47 98.79 97.53 98.24 99.59 99.98 69.39 82.67 99.53 96.16 85.72 99.16 99.74 |\\n| GGNN | 96.36 95.70 98.81 97.47 98.80 99.77 99.97 71.44 83.45 99.50 96.21 81.20 99.69 99.83 |\\n| GIN | 95.78 95.37 98.14 96.98 97.60 99.71 99.97 70.54 82.80 99.49 95.80 83.87 99.61 99.62 |\\n| VCG* GCN | 93.19 94.92 97.82 95.79 98.72 99.54 99.99 66.35 83.75 99.49 95.48 82.99 99.42 99.89 |\\n| GGNN | 96.75 96.25 98.70 96.44 98.88 99.68 99.98 77.12 85.11 99.57 96.48 83.63 99.62 98.92 |\\n\\nEvaluation across different distributions.\\n\\nTo assess the generalization ability of GNN models, we evaluate the performance of NeuroSAT (on LCG*) and GGNN (on VCG*) across different datasets and difficulty levels. As shown in Figure 3 and Figure 4, NeuroSAT and GGNN struggle to generalize effectively to datasets distinct from their training data in most cases. However, when trained on the SR dataset, they exhibit better generalization performance across different datasets. Furthermore, while both GNN models demonstrate limited generalization to larger formulas beyond their training data, they perform relatively better on smaller instances. These observations suggest that the generalization performance of GNN models for satisfiability prediction is influenced by the distinct nature and complexity of its training data. Training on more challenging instances could potentially enhance their generalization ability.\\n\\n**Figure 3**: Results across different datasets. The x-axis denotes testing datasets and the y-axis denotes training datasets. Due to the limited space, Figure 4 exclusively displays the performance of NeuroSAT and GGNN on the SR and 3-SAT datasets. Comprehensive results on the other five datasets, as well as the experimental results on different massage passing iterations, are provided in Appendix C.2.\"}"}
{"id": "8eVgdwKs2N", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## 5.2 Satisfying Assignment Prediction\\n\\nEvaluation with different training losses. Table 2 presents the benchmarking results of each GNN baseline across three different training objectives. Interestingly, the unsupervised training methods outperform the supervised learning approach across the majority of datasets. We hypothesize that this is due to the presence of multiple satisfying assignments in most satisfiable instances. Supervised training tends to bias GNN models towards learning a specific satisfying solution, thereby neglecting the exploration of other feasible ones. This bias may compromise the models' ability to generalize effectively. Such limitations become increasingly apparent when the space of satisfying solutions is much larger, as seen in the medium CA and PS datasets. Additionally, it is noteworthy that employing \\\\( \\\\text{UNS}^1 \\\\) as the loss function can result in instability during the training of some GNN models, leading to a failure to converge in some cases. Conversely, using \\\\( \\\\text{UNS}^2 \\\\) loss demonstrates strong and stable performance across all datasets.\\n\\n### Table 2: Results on the datasets of the same distribution with different training losses.\\n\\n| Graph Method | SR      | 3-SAT   | CA      | PS      |\\n|--------------|---------|---------|---------|---------|\\n|              | SUP     | UNS     | SUP     | UNS     |\\n| LCG*NeuroSAT | 88.47   | 82.30   | 79.79   | 78.39   |\\n|              | 80.23   | 80.59   | 0.27    | 82.17   |\\n| GCN          | 83.74   | 73.09   | 77.02   | 70.34   |\\n|              | 74.79   | 75.31   | 0.17    | 75.30   |\\n| VCG*GCN      | 83.38   | 84.19   | 78.00   | 76.60   |\\n|              | 84.42   | 79.23   | 14.98   | 76.64   |\\n|              | 83.79   | 51.48   | 47.67   | 91.21   |\\n|              | 86.85   | 86.32   | 66.32   | 86.31   |\\n|              | 87.48   | 87.48   | 66.42   | 87.48   |\\n| GIN          | 84.61   | 89.56   | 83.27   | 79.23   |\\n|              | 87.65   | 81.72   | 17.81   | 83.28   |\\n|              | 86.03   | 48.92   | 48.92   | 86.03   |\\n|              | 41.61   | 0.00    | 35.45   | 70.83   |\\n|              | 41.61   | 0.00    | 35.45   | 70.83   |\\n\\nIn addition to evaluating the performance of GNN models under various training loss functions, we extend our analysis to explore how these models perform across different data distributions and under various inference algorithms. Furthermore, we assess the robustness of these GNN models when trained on noisy datasets that include unsatisfiable instances in an unsupervised fashion. For detailed results of these evaluations, please refer to Appendix C.3.\\n\\n## 5.3 Unsat-core Variable Prediction\\n\\nEvaluation on the same distribution.\\n\\nThe benchmarking results presented in Table 3 exhibit the superior performance of all GNN models on both easy and medium datasets, with NeuroSAT consistently achieving the best results across most datasets. It is important to note that the primary objective of predicting unsat-core variables is not to solve SAT problems directly but to provide valuable guidance for enhancing the backtracking search process. As such, even imperfect predictions \u2014 for instance, those with a classification accuracy of 90% \u2014 have been demonstrated to be sufficiently effective in improving the search heuristics employed by modern CDCL-based SAT solvers, as indicated by previous studies [33, 38].\\n\\nWe also conduct experiments to evaluate the generalization ability of GNN models on unsat-core variable prediction. Please see appendix C.4 for details.\"}"}
{"id": "8eVgdwKs2N", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Results on the datasets of the same distribution. Only unsatisfiable instances are evaluated.\\n\\n| Method | Graph | Easy Datasets | Medium Datasets |\\n|--------|-------|---------------|-----------------|\\n| SR     | 3-SAT | CA            | PS k            |\\n|        |       | k-Clique      | k-Domset        |\\n|        |       | k-Vercov      |                 |\\n|       | LCG*  | NeuroSAT      |                 |\\n|       | GCN   | 90.76         | 94.43           |\\n|       |       | 83.69         | 86.20           |\\n|       |       | 99.93         | 95.80           |\\n|       |       | 94.47         | 90.07           |\\n|       |       | 99.65         | 85.73           |\\n|       |       | 88.53         | 99.97           |\\n|       |       | 97.90         | 99.10           |\\n|       | VCG*  | GCN           |                 |\\n|       |       | 88.57         | 94.34           |\\n|       |       | 83.17         | 85.27           |\\n|       |       | 99.93         | 95.79           |\\n|       |       | 94.46         | 88.17           |\\n|       |       | 99.65         | 85.70           |\\n|       |       | 87.37         | 99.96           |\\n|       |       | 97.90         | 99.09           |\\n|       | GGNN  | 89.57         | 94.37           |\\n|       |       | 83.50         | 85.84           |\\n|       |       | 99.93         | 95.81           |\\n|       |       | 94.49         | 88.84           |\\n|       |       | 99.65         | 99.68           |\\n|       |       | 88.03         | 99.98           |\\n|       |       | 99.10         | 99.10           |\\n|       | GIN   | 89.50         | 94.35           |\\n|       |       | 83.23         | 85.69           |\\n|       |       | 99.93         | 95.79           |\\n|       |       | 94.47         | 89.51           |\\n|       |       | 99.65         | 85.72           |\\n|       |       | 88.13         | 99.96           |\\n|       |       | 97.89         | 99.10           |\\n\\n6 Advancing Evaluation on G4SATBench\\n\\nTo gain deeper insights into how GNNs tackle the SAT problem, we conduct comprehensive comparative analyses between GNN-based SAT solvers and the CDCL and LS heuristics in this section. Since these search heuristics aim to solve a SAT instance directly, our focus only lies on the tasks of (T1) satisfiability prediction and (T2) satisfying assignment prediction (with UNS as the training loss). We employ NeuroSAT (on LCG*) and GGNN (on VCG*) as our GNN models and experiment on the SR and 3-SAT datasets. Detailed experimental settings are included in Appendix D.\\n\\n6.1 Comparison with the CDCL Heuristic\\n\\nEvaluation on the clause-learning augmented instances. CDCL-based SAT solvers enhance backtracking search with conflict analysis and clause learning, enabling efficient exploration of the search space by iteratively adding \\\"learned clauses\\\" to avoid similar conflicts in future searches [36]. To assess whether GNN-based SAT solvers can learn and benefit from the backtracking search (with CDCL) heuristic, we augment the original formulas in the datasets with learned clauses and evaluate GNN models on these clause-learning augmented instances.\\n\\nTable 4 shows the testing results on augmented SAT datasets. Notably, training on the augmented instances leads to significant improvements in both satisfiability prediction and satisfying assignment prediction. These improvements can be attributed to the presence of \\\"learned clauses\\\" that effectively modify the graph structure of the original formulas, thereby facilitating GNNs to solve them with relative ease. However, despite the augmented instances being easily solvable using the backtracking search within a few search steps, GNN models fail to effectively handle these instances when trained on the original instances. These findings suggest that GNNs may not explicitly learn the backtracking search heuristic when trained for satisfiability prediction or satisfying assignment prediction.\\n\\nTable 4: Results on augmented datasets. Values inside/outside parentheses denote the results of models trained on augmented/original instances.\\n\\n| Method | Graph | Easy Datasets | Medium Datasets |\\n|--------|-------|---------------|-----------------|\\n|        |       | SR 3-SAT      | SR 3-SAT        |\\n|        | T1    | NeuroSAT      |                 |\\n|        |       | 100.00 (96.78)| 100.00 (96.06)  |\\n|        |       | 100.00 (84.57)| 96.78 (84.85)   |\\n|        | T2    | NeuroSAT      |                 |\\n|        |       | 85.05 (83.28) | 83.50 (81.04)   |\\n|        |       | 51.95 (45.51) | 39.00 (16.52)   |\\n|        | T1    | GGNN          |                 |\\n|        |       | 100.00 (97.66)| 100.00 (95.46)  |\\n|        |       | 100.00 (84.01)| 96.29 (85.80)   |\\n|        | T2    | GGNN          |                 |\\n|        |       | 85.35 (83.42) | 81.56 (79.99)   |\\n|        |       | 44.18 (40.09) | 34.67 (14.75)   |\\n\\nTable 5: Results using contrastive pretraining. Values in parentheses denote the difference between the results without pretraining.\\n\\n| Method | Graph | Easy Datasets | Medium Datasets |\\n|--------|-------|---------------|-----------------|\\n|        |       | SR 3-SAT      | SR 3-SAT        |\\n|        | T1    | NeuroSAT      |                 |\\n|        |       | 96.68 (+0.68) | 96.23 (-0.10)   |\\n|        |       | 78.31 (+0.29) | 85.02 (+0.12)   |\\n|        | T2    | NeuroSAT      |                 |\\n|        |       | 80.54 (+0.75) | 79.71 (-0.88)   |\\n|        |       | 36.42 (-0.83) | 41.23 (-0.38)   |\\n|        | T1    | GGNN          |                 |\\n|        |       | 96.46 (-0.29) | 96.45 (+0.20)   |\\n|        |       | 76.34 (-0.78) | 85.17 (+0.06)   |\\n|        | T2    | GGNN          |                 |\\n|        |       | 80.66 (-0.34) | 79.23 (-0.09)   |\\n|        |       | 33.44 (+0.07) | 36.39 (+0.44)   |\\n\\nEvaluation with contrastive pretraining. Observing that GNN models exhibit superior performance on clause-learning augmented SAT instances, there is potential to improve the performance of GNNs by learning a latent representation of the original formula similar to its augmented counterpart. Motivated by this, we also experiment with a contrastive learning approach (i.e., SimCLR [8]) to pretrain the representation of CNF formulas to be close to their augmented ones [11], trying to embed the CDCL heuristic in the latent space through representation learning.\\n\\nThe results of contrastive pretraining are presented in Table 5. In contrast to the findings in [11], our results show limited performance improvement through contrastive pretraining, indicating that GNN models still encounter difficulties in effectively learning the CDCL heuristic in the latent space. This observation aligns with the conclusions drawn in [9], which highlight that static GNNs may fail.\"}"}
{"id": "8eVgdwKs2N", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to exactly replicate the same search operations due to the dynamic changes in the graph structure introduced by the clause learning technique.\\n\\n### 6.2 Comparison with the LS Heuristic\\n\\nEvaluation with random initialization. LS-based SAT solvers typically begin by randomly initializing an assignment and then iteratively flip variables guided by specific heuristics until reaching a satisfying assignment. To compare the behaviors of GNNs with this solving procedure, we first conduct an evaluation of GNN models with randomized initial embeddings in both training and testing, emulating the initialization of LS SAT solvers.\\n\\n| Task | Method       | Easy Datasets | Medium Datasets |\\n|------|--------------|---------------|-----------------|\\n|      | SR 3-SAT     | T1 NeuroSAT   | 97.24 (+1.24)   |\\n|      | SR 3-SAT     | T1 NeuroSAT   | 96.44 (+0.11)   |\\n|      | medium SR dataset | T1 NeuroSAT | 77.29 (-0.91)   |\\n|      | medium 3-SAT dataset | T1 NeuroSAT | 84.85 (-0.05)   |\\n|      | medium SR dataset | T2 NeuroSAT | 79.09 (-0.70)   |\\n|      | medium 3-SAT dataset | T2 NeuroSAT | 80.79 (+0.20)   |\\n|      | medium SR dataset | T2 NeuroSAT | 37.27 (+0.02)   |\\n|      | medium 3-SAT dataset | T2 NeuroSAT | 40.75 (-0.86)   |\\n|      | medium SR dataset | T2 GGNN     | 80.10 (-0.90)   |\\n|      | medium 3-SAT dataset | T2 GGNN     | 79.83 (+0.51)   |\\n|      | medium SR dataset | T2 GGNN     | 32.85 (-0.52)   |\\n|      | medium 3-SAT dataset | T2 GGNN     | 36.59 (+0.64)   |\\n\\nThe results presented in Table 6 demonstrate that using random initialization has a limited impact on the overall performances of GNN-based SAT solvers. This suggests that GNN models do not aim to learn a fixed latent representation for each formula in SAT solving. Instead, they have developed a solving strategy that effectively exploits the inherent graph structure of each SAT instance.\\n\\n### Evaluation on the predicted assignments\\n\\nUnder random initialization, we further analyze the solving strategies of GNNs by evaluating their predicted assignments decoded from the literal embeddings of NeuroSAT at each iteration of message passing. For the task of satisfiability prediction, we employ the 2-clustering decoding algorithm to extract the predicted assignments from the literal embeddings of NeuroSAT at each iteration of message passing. For satisfying assignment prediction, we evaluate both NeuroSAT and GGNN using multiple-prediction decoding. Our evaluation focuses on three key aspects: (a) the number of distinct predicted assignments, (b) the number of flipped variables between two consecutive iterations, and (c) the number of unsatisfiable clauses associated with the predicted assignments.\\n\\nAs shown in Figure 5, all three GNN models initially generate a wide array of assignment predictions by flipping a considerable number of variables, resulting in a notable reduction in the number of unsatisfiable clauses. However, as the iterations progress, the number of flipped variables diminishes substantially, and most GNN models eventually converge towards predicting a specific assignment or making minimal changes to their predictions when there are no or very few unsatisfiable clauses remaining. This trend is reminiscent of the greedy solving strategy adopted by the LS solver GSAT [32], where changes are made to minimize the number of unsatisfied clauses in the new assignment. However, unlike GSAT's approach of flipping one variable at a time and incorporating random selection to break ties, GNN models simultaneously modify multiple variables and potentially converge to a particular unsatisfied assignment and find it challenging to deviate from such a prediction.\\n\\nIt is also noteworthy that despite being trained for satisfiability prediction, NeuroSAT* demonstrates similar behavior to the GNN models trained for assignment prediction. This observation indicates that GNNs also learn to search for a satisfying assignment implicitly in the latent space while performing satisfiability prediction.\"}"}
{"id": "8eVgdwKs2N", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations and future work. While G4SATBench represents a significant step in evaluating GNNs for SAT solving, there are still some limitations and potential future directions to consider.\\n\\nFirstly, G4SATBench primarily focuses on evaluating standalone neural SAT solvers, excluding the exploration of neural-guided SAT solvers that integrate GNNs with search-based SAT solvers. It also should be emphasized that the instances included in G4SATBench are relatively small compared to most practical instances found in real-world applications, where GNN models alone are not sufficient for solving such large-scale instances. Future research could explore techniques to effectively leverage GNNs in combination with modern SAT solvers to scale up to real-world instances. Secondly, G4SATBench benchmarks general GNN models on the LCG* and VCG* graph representations for SAT solving, but does not consider sophisticated GNN models designed for specific graph constructions in certain domains, such as Circuit SAT problems. Investigating domain-specific GNN models tailored to the characteristics of specific problems could lead to improved performance in specialized instances. Lastly, all existing GNN-based SAT solvers in the literature are static GNNs, which have limited learning ability to capture the CDCL heuristic. Exploring dynamic GNN models that can effectively learn the CDCL heuristic is also a potential direction for future research.\\n\\nConclusion. In this work, we present G4SATBench, a groundbreaking benchmark study that comprehensively evaluates GNN models in SAT solving. G4SATBench offers curated synthetic SAT datasets sourced from various domains and difficulty levels and benchmarks a wide range of GNN-based SAT solvers under diverse settings. Our empirical analysis yields valuable insights into the performances of GNN-based SAT solvers and further provides a deeper understanding of their capabilities and limitations. We hope the proposed G4SATBench will serve as a solid foundation for GNN-based SAT solving and inspire future research in this exciting field.\\n\\nReferences\\n\\n[1] Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve Circuit-SAT: An unsupervised differentiable approach. In *International Conference on Learning Representations (ICLR)*, 2019.\\n\\n[2] Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. PDP: A general neural framework for learning constraint satisfaction solvers. *arXiv preprint arXiv:1903.01969*, 2019.\\n\\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. *arXiv preprint arXiv:1607.06450*, 2016.\\n\\n[4] Armin Biere, Marijn Heule, and Hans van Maaren. *Handbook of Satisfiability*, volume 185. IOS press, 2009.\\n\\n[5] B\u00e9la Bollob\u00e1s and Paul Erd\u00f6s. Cliques in random graphs. In *Mathematical Proceedings of the Cambridge Philosophical Society*, 1976.\\n\\n[6] Benedikt B\u00fcnz and Matthew Lamm. Graph neural networks and boolean satisfiability. *arXiv preprint arXiv:1702.03592*, 2017.\\n\\n[7] Chris Cameron, Rex Chen, Jason Hartford, and Kevin Leyton-Brown. Predicting propositional satisfiability via end-to-end learning. In *AAAI Conference on Artificial Intelligence (AAAI)*, 2020.\\n\\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In *International Conference on Machine Learning (ICML)*, 2020.\\n\\n[9] Ziliang Chen and Zhanfu Yang. Graph neural reasoning may fail in certifying boolean unsatisfiability. *arXiv preprint arXiv:1909.11588*, 2019.\"}"}
{"id": "8eVgdwKs2N", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"James M. Crawford and Larry D. Auton. Experimental results on the crossover point in random 3-SAT. *Artificial Intelligence*, 1996.\\n\\nHaonan Duan, Pashootan Vaezipoor, Max B. Paulus, Yangjun Ruan, and Chris J. Maddison. Augment with care: Contrastive learning for combinatorial problems. In *International Conference on Machine Learning (ICML)*, 2022.\\n\\nMatthias Fey and Jan Eric Lenssen. Fast graph representation learning with *arXiv preprint arXiv:1903.02428*, 2019.\\n\\nABKFM Fleury and Maximilian Heisinger. Cadical, kissat, paracooba, plingeling and treengeling entering the sat competition 2020. *SAT COMPETITION*, 2020.\\n\\nJes\u00fas Gir\u00e1ldez-Cru and Jordi Levy. A modularity-based random SAT instances generator. In *International Joint Conference on Artificial Intelligence (IJCAI)*, 2015.\\n\\nJes\u00fas Gir\u00e1ldez-Cru and Jordi Levy. Locality in random SAT instances. In *International Joint Conference on Artificial Intelligence (IJCAI)*, 2017.\\n\\nWenxuan Guo, Junchi Yan, Hui-Ling Zhen, Xijun Li, Mingxuan Yuan, and Yaohui Jin. Machine learning methods in solving the boolean satisfiability problem. *arXiv preprint arXiv:2203.04755*, 2022.\\n\\nJesse Michael Han. Enhancing SAT solvers with glue variable predictions. *arXiv preprint arXiv:2007.02559*, 2020.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In *IEEE International Conference on Computer Vision (ICCV)*, 2015.\\n\\nSean B Holden et al. Machine learning for automated theorem proving: Learning to solve SAT and QSAT. *Foundations and Trends\u00ae in Machine Learning*, 14(6):807\u2013989, 2021.\\n\\nHolger H Hoos and Thomas St\u00fctzle. SATLIB: An online resource for research on SAT. *Workshop on Satisfiability (SAT)*, 2000.\\n\\nSebastian Jaszczur, Micha\u0142 \u0141uszczyk, and Henryk Michalewski. Neural heuristics for SAT solving. *arXiv preprint arXiv:2005.13406*, 2020.\\n\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In *International Conference on Learning Representations (ICLR)*, 2015.\\n\\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In *International Conference on Learning Representations (ICLR)*, 2017.\\n\\nVitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. Can q-learning with graph networks learn a generalizable branching heuristic for a SAT solver? In *Advances in Neural Information Processing Systems (NeurIPS)*, 2020.\\n\\nMassimo Lauria, Jan Elffers, Jakob Nordstr\u00f6m, and Marc Vinyals. Cnfgen: A generator of crafted benchmarks. In *Theory and Applications of Satisfiability Testing (SAT)*, 2017.\\n\\nMin Li, Zhengyuan Shi, Qiuxia Lai, Sadaf Khan, and Qiang Xu. Deepsat: An eda-driven learning framework for SAT. *arXiv preprint arXiv:2205.13745*, 2022.\\n\\nYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neural networks. In *International Conference on Learning Representations (ICLR)*, 2016.\\n\\nZhaoyu Li and Xujie Si. NSNet: A general neural probabilistic framework for satisfiability problems. In *Advances in Neural Information Processing Systems (NeurIPS)*, 2022.\"}"}
{"id": "8eVgdwKs2N", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[30] Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In International Conference on Machine Learning (ICML), 2010.\\n\\n[31] Emils Ozolins, Karlis Freivalds, Andis Draguns, Eliza Gaile, Ronalds Zakovskis, and Sergejs Kozlovics. Goal-aware neural SAT solver. In International Joint Conference on Neural Networks (IJCNN), 2022.\\n\\n[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), 2019.\\n\\n[33] Bart Selman, Hector J. Levesque, and David G. Mitchell. A new method for solving hard satisfiability problems. In National Conference on Artificial Intelligence (AAAI), 1992.\\n\\n[34] Daniel Selsam and Nikolaj S. Bj\u00f8rner. Guiding high-performance SAT solvers with unsat-core predictions. In Theory and Applications of Satisfiability Testing (SAT), 2019.\\n\\n[35] Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a SAT solver from single-bit supervision. In International Conference on Learning Representations (ICLR), 2019.\\n\\n[36] Zhengyuan Shi, Min Li, Sadaf Khan, Hui-Ling Zhen, Mingxuan Yuan, and Qiang Xu. Satformer: Transformers for SAT solving. arXiv preprint arXiv:2209.00953, 2022.\\n\\n[37] Jo\u00e3o P. Marques Silva and Karem A. Sakallah. GRASP: A search algorithm for propositional satisfiability. IEEE Transactions on Computers, 1999.\\n\\n[38] Volodymyr Skladanivskyy. Minimalistic round-reduced sha-1 pre-image attack. SAT RACE, 2019.\\n\\n[39] Wenxi Wang, Yang Hu, Mohit Tiwari, Sarfraz Khurshid, Kenneth McMillan, and Risto Miikkulainen. Neurocomb: Improving SAT solving with graph neural networks. arXiv preprint arXiv:2110.14053, 2021.\\n\\n[40] Nathan Wetzler, Marijn Heule, and Warren A. Hunt Jr. Drat-trim: Efficient checking and trimming using expressive clausal proofs. In Theory and Applications of Satisfiability Testing (SAT), 2014.\\n\\n[41] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations (ICLR), 2019.\\n\\n[42] Zhiyuan Yan, Min Li, Zhengyuan Shi, Wenjie Zhang, Yingcong Chen, and Hongce Zhang. Addressing variable dependency in gnn-based SAT solving. arXiv preprint arXiv:2304.08738, 2023.\\n\\n[43] Emre Yolcu and Barnab\u00e1s P\u00f3czos. Learning local search heuristics for boolean satisfiability. In Advances in Neural Information Processing Systems (NeurIPS), 2019.\\n\\n[44] Wenjie Zhang, Zeyu Sun, Qihao Zhu, Ge Li, Shaowei Cai, Yingfei Xiong, and Lu Zhang. Nlocalsat: Boosting local search with solution prediction. In International Joint Conference on Artificial Intelligence (IJCAI), 2020.\"}"}
{"id": "8eVgdwKs2N", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes] See Section 7.\\n   (c) Did you discuss any potential negative societal impacts of your work? [N/A]\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Section 1.\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 5, Appendix C.1, and Appendix D.\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A]\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 5.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes]\\n   (b) Did you mention the license of the assets? [N/A]\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"}
