{"id": "LJhfKeqZdu", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We introduce RL4CO, an extensive reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO employs state-of-the-art software libraries as well as best practices in implementation, such as modularity and configuration management, to be efficient and easily modifiable by researchers for adaptations of neural network architecture, environments, and RL algorithms. Contrary to the existing focus on specific tasks like the traveling salesman problem (TSP) for performance assessment, we underline the importance of scalability and generalization capabilities for diverse CO tasks. We also systematically benchmark zero-shot generalization, sample efficiency, and adaptability to changes in data distributions of various models. Our experiments show that some recent SOTA methods fall behind their predecessors when evaluated using these metrics, suggesting the necessity for a more balanced view of the performance of neural CO (NCO) solvers. We hope RL4CO will encourage the exploration of novel solutions to complex real-world tasks, allowing the NCO community to compare with existing methods through a standardized interface that decouples the science from software engineering. We make our library publicly available at https://github.com/kaist-silab/rl4co.\"}"}
{"id": "LJhfKeqZdu", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"lenging problems with minimal dependent (or even independent) of problem-specific knowledge.\\n\\nAmong CO tasks, the routing problems, such as Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP), serve as one of the central test suites for the capabilities of NCO due to the extensive NCO research on that types of problems and also, the applicability of at-hand comparison of highly dedicated heuristic solvers investigated over several decades of study by the OR community. Recent advances of NCO achieve comparable or superior performance to state-of-the-art solvers on these benchmarks, implying the potential of NCO to revolutionize the laborious manual design of CO solvers.\\n\\nHowever, despite the successes and popularity of RL for CO, the NCO community still lacks unified implementations of NCO solvers for easily benchmarking different NCO solvers. Similar to the other ML research, in NCO research, a unified open-source software would serve as a cornerstone for progress, bolstering reproducibility, and ensuring findings can be reliably validated by peers. This would provide a flexible and extensive RL for CO foundation and a unified library can thus bridge the gap between innovative ideas and practical applications, enabling convenient training and testing of different solvers under new settings, and decoupling science from engineering. In practice, this would also serve to expand the NCO area and make it accessible to researchers and practitioners.\\n\\nAnother problem that NCO research faces is the absence of standardized evaluation metrics that, especially account for the practical usage of CO solvers. Although most NCO solvers are customarily assessed based on their performance within training distributions, ideally, they should solve CO problems from out-of-training-distribution well. However, such out-of-distribution evaluation is overlooked in the literature. Furthermore, unlike the other ML research that already has shown the importance of the volume of training data, in NCO, the evaluation of the methods with the controls on the number of training samples is not usually discussed (e.g., state-of-the-art methods can underperform than the other methods). This also hinders the use of NCO in the real world, where the evaluation of solutions becomes expensive (e.g., evaluation of solutions involves the physical dispatching of goods in logistic systems or physical design problems).\\n\\nContributions. In this work, we introduce RL4CO, a new reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO is first and foremost a library of several environments, baselines and boilerplate from the literature implemented in a modular, flexible, and unified way with what we found are the best software practices and libraries, including TorchRL [47], PyTorch Lightning [18], TensorDict [46] and Hydra [74]. Through thoroughly tested unified implementations, we conduct several experiments to explore best practices in RL for CO and benchmark our baselines. We demonstrate that existing state-of-the-art methods may perform poorly on different evaluation metrics and sometimes even underperform their predecessors. We also introduce a new Pareto-optimal, simple-yet-effective sampling scheme based on greedy rollouts from random symmetric augmentations. Additionally, we incorporate real-world tasks, specifically hard-ware design, to highlight the importance of sample efficiency in scenarios where objective evaluation is black-box and expensive, further validating that the functionally decoupled implementation of RL4CO enhances accessibility for achieving better performance in a variety of tasks.\\n\\n2 Preliminaries\\nThe solution space of CO problems generally grows exponentially to their size. Such solution space hinders the learning of NCO solvers that generate the solution in a single shot. As a way to mitigate such difficulties, the constructive (e.g., [49; 70; 38; 40; 36]) methods generate solutions one step at a time in an autoregressive fashion, akin to language models [13; 68; 50]. In RL4CO we focus primarily on benchmarking autoregressive approaches for the above reasons.\"}"}
{"id": "LJhfKeqZdu", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Solving Combinatorial Optimization with Autoregressive Sequence Generation\\n\\nAutoregressive (or constructive) methods assume the autoregressive solution construction schemes, which decide the next \u201caction\u201d based on the current (partial) solution, and repeat this until the solver generates the complete solution (e.g., in TSP, the next action is deciding on a city to visit). Formally speaking,\\n\\n\\\\[ \\\\pi(a|x) \\\\equiv \\\\prod_{t=1}^{T-1} \\\\pi(a_t|a_{t-1}, \\\\ldots, a_1, x), \\\\]\\n\\nwhere \\\\( a = (a_1, \\\\ldots, a_T) \\\\), \\\\( T \\\\) is the solution construction steps, is a feasible (and potentially optimal) solution to CO problems, \\\\( x \\\\) is the problem description of CO, \\\\( \\\\pi \\\\) is a (stochastic) solver that maps \\\\( x \\\\) to a solution \\\\( a \\\\). For example, for a 2D TSP with \\\\( N \\\\) cities, \\\\( x = \\\\{(x_i, y_i)\\\\}_{i=1}^{N} \\\\), where \\\\((x_i, y_i)\\\\) is the coordinates of \\\\( i \\\\)th city \\\\( v_i \\\\), a solution \\\\( a = (v_1, v_2, \\\\ldots, v_N) \\\\).\\n\\nTraining NCO Solvers via Reinforcement Learning\\n\\nThe solver \\\\( \\\\pi_\\\\theta \\\\) parameterized with the parameters \\\\( \\\\theta \\\\) can be trained with supervised learning (SL) or RL schemes. In this work, we focus on RL-based solvers as they can be trained without relying on the optimal (or high-quality) solutions. Under the RL formalism, the training problem of NCOs becomes as follows:\\n\\n\\\\[ \\\\theta^* = \\\\arg\\\\max \\\\theta \\\\mathbb{E}_{x \\\\sim P(x)} \\\\mathbb{E}_{a \\\\sim \\\\pi_\\\\theta(a|x)} R(a, x), \\\\]\\n\\nwhere \\\\( P(x) \\\\) is problem distribution, \\\\( R(a, x) \\\\) is reward (i.e., the negative cost) of \\\\( a \\\\) given \\\\( x \\\\). To solve Eq. (2) via gradient-based optimization method, calculating the gradient of the objective function w.r.t. \\\\( \\\\theta \\\\) is required. However, due to the discrete nature of the CO, the computation of the gradient is not straightforward and often requires certain levels of approximation. Even though few researchers show breakthroughs for solving Eq. (2) with gradient-based optimization, they are restricted to some relatively simpler cases of CO problems [58; 60; 72]. Instead, it is common to rely on RL-formalism to solve Eq. (2). In theory, value-based methods [33] and policy gradient methods [38; 40; 36; 53], and also actor-critic methods [52; 75] are applicable to solve Eq. (2). However, in practice, it is shown that the policy gradient methods (e.g., REINFORCE [73] with proper baselines), generally outperform the value-based methods [38] in NCO.\\n\\nGeneral Structure of Autoregressive Policies\\n\\nThe autoregressive NCO solver (i.e., policy) encodes the given problem \\\\( x \\\\) and auto-regressively decodes the solution. This can be seen as a processing input problem with the encoder and planning (i.e., computing a complete solution) with the decoder. To maximize the solution-finding speed, a common design of the decoder is to fuse the RL environment (e.g., TSP solution construction schemes that update the partial solutions and constraints of CO as well) into the decoder. This aspect of NCO policy is distinctive from the other RL tasks, which maintains the environment separately from the policy. As a result, most competitive autoregressive NCO solver implementations show significant coupling with network architecture and targeting CO problems. This can hinder the reusability of NCO solver implementation to the new types of CO problems. Furthermore, this design choice introduces difficulties for the fairer comparison among the trained solvers, especially related to the effect of encoder/decoder architectures and training/evaluation data usage on the solver\u2019s solution qualities.\\n\\n3 RL4CO\\n\\nIn this paper, we present RL4CO, an extensive reinforcement learning (RL) for Combinatorial Optimization (CO) benchmark. RL4CO aims to provide a modular, flexible, and unified code base that addresses the challenges of autoregressive policy training/evaluation for NCO (discussed in Section 2) and performs extensive benchmarking capabilities on various settings.\\n\\n3.1 Unified and Modular Implementation\\n\\nAs shown in Fig. 3.1, RL4CO decouples the major components of the autoregressive NCO solvers and its training routine while prioritizing reusability. We consider the five major components, which are explained in the following paragraphs.\"}"}
{"id": "LJhfKeqZdu", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our initial investigation into various autoregressive NCO solvers, such as AM, POMO, Sym-NCO, across CO problems like Traveling TSP, Capacitated Vehicle Routing Problem (CVRP), Orienteering Problem (OP), Prize-collecting TSP (PCTSP), among others, has revealed a common structural pattern. The policy network $\\\\pi_\\\\theta$ follows an architecture that combines an encoder $f_\\\\theta$ and a decoder $g_\\\\theta$ as follows:\\n\\n$$\\\\pi_\\\\theta(a|x) \\\\equiv g_\\\\theta(f_\\\\theta(x)) \\\\tag{3}$$\\n\\nUpon analyzing encoder-decoder architectures, we have identified components that hinder the encapsulation of the policy from the environment. To achieve greater modularity, RL4CO modularizes such components in the form of embeddings:\\n\\n- $\\\\text{InitEmbedding}$\\n- $\\\\text{ContextEmbedding}$\\n- $\\\\text{DynamicEmbedding}$\\n\\nThe encoder's primary task is to encode input $x$ into a hidden embedding $h$. The structure of $f_\\\\theta$ comprises two trainable modules: the $\\\\text{InitEmbedding}$ and encoder blocks. The $\\\\text{InitEmbedding}$ module typically transforms problem features into the latent space and problem-specific compared to the encoder blocks, which often involve plain multi-head attention (MHA):\\n\\n$$h = f_\\\\theta(x) \\\\equiv \\\\text{EncoderBlocks}(\\\\text{InitEmbedding}(x)) \\\\tag{4}$$\\n\\nThe decoder autoregressively constructs the solution based on the encoder output $h$. Solution decoding involves iterative steps until a complete solution is constructed:\\n\\n$$q_t = \\\\text{ContextEmbedding}(h, a_{t-1:0})$$\\n\\n$$\\\\bar{q}_t = \\\\text{MHA}(q_t, W_{gk}h, W_{gv}h)$$\\n\\n$$\\\\pi(a_t) = \\\\text{MaskedSoftmax}(\\\\bar{q}_t \\\\cdot W_{vh}, M_t)$$\\n\\nwhere the $\\\\text{ContextEmbedding}$ is tailored to the specific problem environment, $q_t$ and $\\\\bar{q}_t$ represent the query and attended query (also referred to as glimpse in Mnih et al. [45]) at the $t$-th decoding step, $W_{gk}$, $W_{gv}$ and $W_{vh}$ are trainable linear projections computing keys and values from $h$, and $M_t$ denotes the action mask, which is provided by the environment to ensure solution feasibility. It is noteworthy that we also modularize the $\\\\text{DynamicEmbedding}$, which dynamically updates the keys and values of MHA and Softmax during decoding. This approach is often used in dynamic routing settings, such as split delivery VRP. For the details, please refer to Appendix A.4.\\n\\nFrom Eqs. (4) and (5), it is evident that creating embeddings demands problem-specific handling, often trigger coherence between the policy and CO problems. In RL4CO, we offer pre-coded environment embeddings investigated from NCO literature [35; 38; 41] and, more importantly, allow a drop-in replacement of pre-coded embedding modules to user-defined embedding modules to attain higher modularity. Furthermore, we accommodate various decoding schemes (which will be further discussed in \u00a7 4) proposed from milestone papers [38; 40; 36] into a unified decoder implementation so that those schemes can be applied to the different model, such as applying greedy multi-starts to the Attention Model from Kool et al. [38].\"}"}
{"id": "LJhfKeqZdu", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When implementing the environment, we focus on parallel execution of rollouts (i.e., problem-solving) while maintaining statelessness in updating every step of solution decoding. These features are essential for ensuring the reproducibility of NCO and supporting \u201clook-back\u201d decoding schemes such as Monte-Carlo Tree Search. Our environment designs and implementations are flexible enough to accommodate various types of NCO solvers that generate a single action at each decision-making step [3; 33; 52; 53; 75]. Additionally, our framework is extensible beyond routing problems. We investigate the use of RL4CO for electrical design automation in Appendix B.\\n\\nOur environment implementation is based on TorchRL [10], an open-source RL library for PyTorch [54], which aims at high modularity and good runtime performance, especially on GPUs. This design choice makes the Environment implementation standalone, even outside of RL4CO, and consistently empowered by a community-supporting library \u2013 TorchRL. Moreover, we employ TensorDicts [46] to move around data which allows for further flexibility.\\n\\n**RL Algorithm**\\nThis module defines the routine that takes the Policy, Environment, and problem instances and computes the gradients of the policy (and possibly the critic for actor-critic methods). We intentionally decouple the routines for gradient computations and parameter updates to support modern training practices, which will be explained in the next paragraph.\\n\\n**Trainer**\\nTraining a single NCO model is typically computationally demanding, especially since most CO problems are NP-hard. Therefore, implementing a modernized training routine becomes crucial. To this end, we implement the Trainer using Lightning [18], which seamlessly supports features of modern training pipelines, including logging, checkpoint management, automatic mixed-precision training, various hardware acceleration supports (e.g., CPU, GPU, TPU, and Apple Silicon), multi-GPU support, and even multi-machine expansion. We have found that using mixed-precision training significantly decreases training time without sacrificing NCO solver quality and enables us to leverage recent routines such as FlashAttention [16; 15].\\n\\n**Configuration Management**\\nOptionally, but usefully, we adopt Hydra [74], an open-source Python framework that enables hierarchical config management. It promotes modularity, scalability, and reproducibility, making it easier to manage complex configurations and experiments with different settings and maintain consistency across different environments.\\n\\n### 3.2 Availability and Future Support\\n\\nRL4CO can be installed through PyPI [1] and we adhere to continuous integration, deployment, and testing to ensure reproducibility and accessibility.\\n\\n```\\n$ pip install rl4co\\n```\\n\\n[Listing 1: Installation of RL4CO with PyPI]\\n\\nOur goal is to provide long-term support for RL4CO. It is actively maintained and will continue to update to accommodate new features and contributions from the community. Ultimately, our aim is to make RL4CO the to-go library in the RL for CO research area that provides encompassing, accessible, and extensive boilerplate code.\\n\\n### 4 Benchmark Experiments\\n\\nOur focus is to benchmark the NCO solvers under controlled settings, aiming to compare all benchmarked methods as closely as possible in terms of network architectures and the number of training samples consumed.\\n\\n---\\n\\n5 Listed at https://pypi.org/project/rl4co/\\n6 Documentation is also available on ReadTheDocs: https://rl4co.readthedocs.io/en/latest/\"}"}
{"id": "LJhfKeqZdu", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4.1: In-domain benchmark results. Gurobi \\\\[22\\\\] results are reproduced from \\\\[38\\\\]. As the non-learned heuristic baselines, we report the results of LKH3 \\\\[23\\\\] and algorithm-specific methods. For TSP, we used Concorde \\\\[48\\\\] as the classical method baseline. For CVRP, we used HGS \\\\[69\\\\] as the classical method baseline. The gaps are measured w.r.t. the best classical heuristic methods.\\n\\n| Method       | TSP (N = 20) | TSP (N = 50) | CVRP (N = 20) | CVRP (N = 50) |\\n|--------------|--------------|--------------|--------------|--------------|\\n|              | Cost \u2193       | Time         | Cost \u2193       | Time         |\\n| Gurobi       | 3.84         | 7s           | 5.70         | 2m           |\\n| Concorde     | 3.84         | 0.00%        | 5.70         | 0.00%        |\\n| HGS          | N/A          | 6.13         | 4h           | 10.37        |\\n| LKH3         | 3.84         | 0.00%        | 5.70         | 0.00%        |\\n| Greedy One Shot Evaluation | AM-critic 3.86 | 0.64% (<1s) | 5.83 | 2.22% (<1s) |\\n|              | AM 3.84      | 0.19%        | 5.78         | 1.41%        |\\n|              | POMO 3.84    | 0.18%        | 5.75         | 0.89%        |\\n|              | Sym-NCO 3.84 | 0.05%        | 5.72         | 0.47%        |\\n|              | AM-XL 3.84   | 0.07%        | 5.73         | 0.54%        |\\n| Sampling with width M = 1280 | AM-critic 3.84 | 0.15% | 20s | 0.72% | 40s | 2.08% | 24s | 3.07% | 1m24s |\\n|              | AM 3.84      | 0.04%        | 20s          | 0.40%        |\\n|              | POMO 3.84    | 0.02%        | 36s          | 0.18%        |\\n|              | Sym-NCO 3.84 | 0.01%        | 36s          | 0.14%        |\\n|              | AM-XL 3.84   | 0.02%        | 36s          | 0.17%        |\\n| Greedy Multistart (N) | AM-critic 3.85 | 0.36% (<1s) | 5.80 | 1.81% (<1s) |\\n|              | AM 3.84      | 0.12%        | 5.77         | 1.21%        |\\n|              | POMO 3.84    | 0.05%        | 5.71         | 0.29%        |\\n|              | Sym-NCO 3.84 | 0.01%        | 5.72         | 0.36%        |\\n|              | AM-XL 3.84   | 0.02%        | 5.72         | 0.42%        |\\n| Greedy with Augmentation (1280) | AM-critic 3.84 | 0.01%        | 20s | 0.18% | 40s | 1.35% | 24s | 2.49% | 1m24s |\\n|              | AM 3.84      | 0.00%        | 20s          | 0.07%        |\\n|              | POMO 3.84    | 0.00%        | 36s          | 0.18%        |\\n|              | Sym-NCO 3.84 | 0.00%        | 36s          | 0.01%        |\\n|              | AM-XL 3.84   | 0.00%        | 36s          | 0.02%        |\\n| Greedy Multistart with Augmentation (N \u00d7 16) | AM-critic 3.84 | 0.01% | 9s | 0.41% | 32s | 1.12% | 48s | 2.81% | 1m |\\n|              | AM 3.84      | 0.00%        | 9s           | 0.21%        |\\n|              | POMO 3.84    | 0.00%        | 13s          | 0.05%        |\\n|              | Sym-NCO 3.84 | 0.00%        | 13s          | 0.03%        |\\n|              | AM-XL 3.84   | 0.00%        | 13s          | 0.04%        |\\n\\nTL; DR\\nHere is a summary of the benchmark results.\\n\\n- AM \\\\[38\\\\], with minor encoder modifications and trained with a sufficient number of samples, can at times outperform or closely match state-of-the-art (SOTA) methods such as POMO and Sym-NCO for TSP and CVRP with 20 and 50 nodes. (See \u00a7 4.1)\\n- The choice of decoding schemes has a significant impact on the solution quality of NCO solvers. We introduce a simple-yet-effective decoding scheme based on greedy augmentations that significantly enhances the solution quality of the trained solver. (See \u00a7 4.1)\\n- We find that in-distribution performance trends do not always match with out-of-distribution ones when testing with different problem sizes. (See \u00a7 4.2)\\n- When the number of samples is limited, the ranking of baseline methods can significantly change. Actor-critic methods can be a good choice in data-constrained applications. (See \u00a7 4.3)\\n- We find that in-distribution results may not easily determine the downstream performance of pre-trained models when search methods are used, and models that perform worse in-distribution may perform better during adaptation. (See \u00a7 4.4)\\n\\nBenchmarked Solvers\\nWe evaluate the following NCO solvers:\"}"}
{"id": "LJhfKeqZdu", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4.1: Decoding schemes of the autoregressive NCO solvers evaluated in this paper.\\n\\n- AM [38] employs the multi-head attention (MHA) encoder and single-head attention decoder trained using REINFORCE and the rollout baseline.\\n- AM-Critic evaluates the baseline using the learned critic.\\n- POMO [40] is an extension of AM that employs the shared baseline instead of the rollout baseline.\\n- Sym-NCO [36] introduces a symmetric baseline to train the AM instead of the rollout baseline.\\n- AM-XL is AM that adopts POMO-style MHA encoder, using six MHA layers and InstanceNorm instead of BatchNorm. We train AM-XL on the same number of samples as POMO.\\n\\nFor all benchmarked solvers, we schedule the learning rate with MultiStepLinear, which seems to have a non-negligible effect on the performances of NCO solvers - for instance, compared to the original AM implementation and with the same hyperparameters, we can consistently improve performance, i.e., greedy one-shot evaluation on TSP50 from 580 to 578 and on CVRP50 from 1098 to 1095. In addition to the NCO solvers, we compare them to SOTA classical solvers that specialize in solving specific types of CO problems.\\n\\nDecoding Schemes\\n\\nThe solution quality of NCO solvers often shows large variations in performances to the different decoding schemes, even though using the same NCO solvers. Regarding that, we evaluate the trained solvers using five schemes:\\n\\n- Greedy: Selects the highest probabilities at each decoding step.\\n- Sampling: Concurrently samples N solutions using a trained stochastic policy.\\n- Multistart Greedy, inspired by POMO, decodes from the first given nodes and considers the best results from N cases starting at N different cities. For example, in TSP with N nodes, a single problem involves starting from N different cities.\\n- Augmentation: Selects the best greedy solutions from randomly augmented problems (e.g., random rotation and flipping) during evaluation.\\n- Multistart Greedy + Augmentation: Combines the Multistart Greedy with Augmentation.\\n\\nWe emphasize that our work introduces the new greedy Symmetric Augmentation during evaluation, a simple-yet-effective scheme. POMO utilized the 'x8 augmentation' through the dihedral group of order 8. However, we found that generalized symmetric augmentations - even without multistarts - as in Kim et al. [36] can perform better than other decoding schemes. For a visual explanation of the decoding scheme, please refer to Fig. 4.1.\\n\\n4.1 In-distribution Benchmark\\n\\nWe first measure the performances of NCO solvers on the datasets on which they are trained on. The results are summarized in Table 4.1. We first observe that, counter to the commonly known trends that AM < POMO < Sym-NCO, the trend can change to the selection of decoding schemes. Especially when the solver decodes the solutions with Augmentation or Greedy Multistart + Augmentation, the performance differences among the benchmarked solvers on TSP20/50, CVRP20/50 become insignificant. That implies we can improve the solution qualities by increasing the computational budget. These observations lead us to the requirements for an in-depth investigation of the sampling methods and their efficiency.\"}"}
{"id": "LJhfKeqZdu", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"More Sampling, which Decoding Scheme?\\n\\nBased on our previous findings, we anticipate that by investing more computational resources (i.e., increasing the number of samples), the trained NCO solver can discover improved solutions. In this investigation, we examine the performance gains achieved with varying numbers of samples on the TSP50 dataset. As shown in Fig. 4.2, all solvers demonstrate that the Augmentation decoding scheme achieves the Pareto front with limited samples and, notably, generally outperforms other decoding schemes. We observed a similar tendency in CVRP50 (see Fig. 4.3). Additional results on OP and PCTSP are available in Appendix E.\\n\\n4.2 Out-of-distribution Benchmark\\n\\nIn this section, we evaluate the out-of-distribution performance of the NCO solvers by measuring the optimality gap compared to the best-known tractable solver. The evaluation results are visualized in \u00a7 4.2. Contrary to the in-distribution results, we find that NCO solvers with sophisticated baselines (i.e., POMO and Sym-NCO) tend to exhibit worse generalization when the problem size changes, either for solving smaller or larger instances. This can be seen as an indication of \u201coverfitting\u201d to the training sizes. On the other hand, the variant of AM shows relatively better generalization results overall. We also evaluate the solvers in two canonical public benchmark instances (TSPLib and CVRPLib) in Appendix F, which exhibit both variations in the number of nodes as well as their distributions and find a similar trend.\\n\\n4.3 Sample Efficiency Benchmark\\n\\nWe evaluate the NCO solvers based on the number of training samples (i.e., the number of reward evaluations). As shown in Fig. 4.5, we found that actor-critic methods (e.g., AM trained with PPO detailed in Appendix D.7 or AM Critic) can exhibit efficacy in scenarios with limited training samples, as demonstrated by the TSP50/100 results in Fig. 4.5. This observation suggests that NCO solvers with control over the number of samples may exhibit a different trend from the commonly recognized trends. In the extension of this viewpoint, we conducted additional benchmarks in a different problem domain: electrical design automation (EDA) where reward evaluation is resource-intensive.\"}"}
{"id": "LJhfKeqZdu", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4.4: Out-of-distribution generalization results.\\n\\nFigure 4.5: Validation cost over the number of training samples (i.e., number of reward evaluations).\\n\\nThe search methods evaluated are:\\n\\n- Active Search (AS) from Bello et al. [6] finetunes a pre-trained model on the searched instances by adapting all the policy parameters.\\n- Efficient Active Search (EAS) from Hottung et al. [25] finetunes a subset of parameters (i.e., embeddings or new layers) and adds an imitation learning loss to improve convergence.\\n\\nThe benchmarking process is intensive, due to the necessity of electrical simulations. Therefore, sample efficiency becomes even more critical. For more details, please refer to Appendix B.\"}"}
{"id": "LJhfKeqZdu", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4.2: Search Methods Benchmark results of models pre-trained on 50 nodes. We apply the search methods with default parameters from the literature. OOM denotes \u201cOut of Memory\u201d, which occurred with AS on large-scale instances.\\n\\n| Type  | Metric | TSP 200 | CVRP 500 | CVRP 1000 |\\n|-------|--------|---------|----------|-----------|\\n| Classic | Cost   | 10.17   | 16.54    | 23.13     |\\n|        | Cost   | 10.72   | 16.54    | 23.13     |\\n|        |        | 27.95   | 63.45    | 120.47    |\\n| Zero-shot | Cost | 13.15   | 29.96    | 58.01     |\\n|        |        | 13.30   | 29.42    | 56.47     |\\n|        |        | 29.16   | 92.30    | 141.76    |\\n|        | Gap[%] | 29.30   | 81.14    | 150.80    |\\n|        |        | 24.07   | 77.87    | 144.14    |\\n|        |        | 4.33    | 45.47    | 17.67     |\\n|        |        | 17.17   | 36.83    | 58.29     |\\n| AS    | Cost   | 11.16   | 20.03    | OOM       |\\n|        | Gap[%] | 4.13    | 21.12    | OOM       |\\n|        |        | 11.21   | 35.48    | OOM       |\\n|        |        | 0.60    | 0.83     | OOM       |\\n|        |        | 2.00    | 4.79     | OOM       |\\n|        | Time[s] | 7504    | 10070    | OOM       |\\n| EAS   | Cost   | 11.10   | 20.94    | 35.36     |\\n|        |        | 11.65   | 22.80    | 38.77     |\\n|        |        | 28.10   | 64.74    | 125.54    |\\n|        | Gap[%] | 3.55    | 26.64    | 52.89     |\\n|        |        | 8.68    | 37.86    | 67.63     |\\n|        |        | 0.52    | 2.04     | 4.21      |\\n|        |        | 4.66    | 10.57    | 17.02     |\\n|        | Time[s] | 348     | 1562     | 13661     |\\n|        |        | 376     | 1589     | 14532     |\\n|        |        | 432     | 1972     | 20650     |\\n|        |        | 460     | 2051     | 17640     |\\n\\nResults\\n\\nWe extend RL4CO and apply AS and EAS to POMO and Sym-NCO pre-trained on TSP and CVRP with 50 nodes from \u00a7 4.1 to solve larger instances having \\\\( N \\\\in \\\\{200, 500, 1000\\\\} \\\\) nodes.\\n\\nAs shown in Table 4.2, solvers with search methods improve the solution quality. However, POMO generally shows better improvements over Sym-NCO. This may again imply the \u201coverfitting\u201d of sophisticated baselines that can perform better in-training but eventually worse in downstream tasks.\\n\\nDiscussion\\n\\n5.1 Future Directions in RL4CO\\n\\nThe utilization of symmetries in learning, such as by POMO and Sym-NCO, has its limitations in sample efficiency and generalizability, but recent studies like Kim et al. [34] offer promising results by exploring symmetries without reward simulation. There is also a trend toward few-shot learning, where models adapt rapidly to tasks and scales; yet, the transition from tasks like TSP to CVRP still requires investigation [43; 65]. Meanwhile, as AM\u2019s neural architecture poses scalability issues, leveraging architectures such as Hyena [59] that scale sub-quadratically might be key. Furthermore, the emergence of foundation models akin to LLMs, with a focus on encoding continuous features and applying environment-specific constraints, can reshape the landscape of NCO [68; 50]. Efficient finetuning methods could also be pivotal for optimizing performance under constraints [26; 67].\\n\\n5.2 Limitations\\n\\nWe identify some limitations with our current benchmark. In terms of benchmarking, we majorly focus on training the solvers on relatively smaller sizes, due to our limited computational budgets. Another limitation is the main focus on routing problems, even if RL4CO can be easily extended for handling different classes of CO problems, such as scheduling problems. Moreover, we did not benchmark shifts in data distributions for the time being (except for the real-world instances of TSPLib and CVRPLib), which could lead to new insights. In future works, we plan to implement new CO problems that stretch beyond the routing and tackle even larger instances, also owing to the capability of RL4CO library.\\n\\n5.3 Conclusion\\n\\nThis paper introduces RL4CO, a modular, flexible, and unified software library for Reinforcement Learning (RL) for Combinatorial Optimization (CO). Our benchmark library aims at filling the gap in a unified implementation for the NCO area by utilizing several best practices with the goal provide researchers and practitioners with a flexible starting point for NCO research. With RL4CO, we rigorously benchmarked various NCO solvers in the measures of in-distribution, out-of-distribution, sample-efficiency, and search methods performances. Our findings show that a comparison of NCO solvers across different metrics and tasks is fundamental, as state-of-the-art approaches may in fact perform worse than predecessors under these metrics. We hope that our benchmark library will inspire NCO researchers to explore new avenues and drive advancements in this field.\"}"}
{"id": "LJhfKeqZdu", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We want to express our gratitude towards all reviewers and people in the community who have contributed - and those who will - to RL4CO. A special thanks goes to the TorchRL team for helping us in solving issues and improving the library. This work was supported by a grant of the KAIST-KT joint research project through AI2XL Laboratory, Institute of Convergence Technology, funded by KT [Project No. G01210696, Development of Multi-Agent Reinforcement Learning Algorithm for Efficient Operation of Complex Distributed Systems].\\n\\nChecklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes] See ??\\n   (c) Did you discuss any potential negative societal impacts of your work? [N/A] Our work involves optimization problems, such as routing problems, with no clear negative societal impact.\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We focus on the reproducibility of the results. As a part of such efforts, we share all the details of code, data, and instructions for reproducing the results in a form of a configuration file in our code repository.\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] As similar to the previous question, we leave and share all training details as a configuration file.\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] We note that, as common practice in the field, we did not report multiple runs for the main tables as algorithms can take more than one day each to train. However, for experiments limited in the number of samples, such as for the sample efficiency experiments and the mDPP benchmarking, we reported multiple runs with different random seeds, where we demonstrated the robustness of different runs to random seeds.\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix D.1\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes] We based our implementation of baseline models on the original code - although with several modifications - and included proper citations and credits to the authors, as well as references to existing software packages.\\n   (b) Did you mention the license of the assets? [Yes] See Appendix A.2\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] Aside from the software library link, we included automatic download to the PDN data for the mDPP benchmarking with the link available in the library.\"}"}
{"id": "LJhfKeqZdu", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] Our library is based on local data generation. The data we use (PDN board, TSPLib, CVRPLib) is publicly available online and open source.\\n\\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] We do not include any offensive content; information is personally identifiable but thanks to the single-blind review process.\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\\n\\nReferences\\n\\n[1] Python package index - pypi. URL https://pypi.org/\\n\\n[2] S. Ahn, J. Kim, H. Lee, and J. Shin. Guiding deep molecular optimization with genetic exploitation. Advances in neural information processing systems, 33:12008\u201312021, 2020.\\n\\n[3] S. Ahn, Y. Seo, and J. Shin. Learning what to defer for maximum independent sets. In International Conference on Machine Learning, pages 134\u2013144. PMLR, 2020.\\n\\n[4] T. Barrett, W. Clements, J. Foerster, and A. Lvovsky. Exploratory combinatorial optimization with reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3243\u20133250, 2020.\\n\\n[5] T. D. Barrett, C. W. Parsons, and A. L. Laterre. Learning to solve combinatorial graph partitioning problems via efficient exploration. arXiv preprint arXiv:2205.14105, 2022.\\n\\n[6] I. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Bengio. Neural combinatorial optimization with reinforcement learning, 2017.\\n\\n[7] Y. Bengio, A. Lodi, and A. Prouvost. Machine learning for combinatorial optimization: a methodological tour d'horizon. European Journal of Operational Research, 290(2):405\u2013421, 2021.\\n\\n[8] E. Bisong and E. Bisong. Google colaboratory. Building machine learning and deep learning models on google cloud platform: a comprehensive guide for beginners, pages 59\u201364, 2019.\\n\\n[9] C. Bonnet, D. Luo, D. Byrne, S. Surana, V. Coyette, P. Duckworth, L. I. Midgley, T. Kalloniatis, S. Abramowitz, C. N. Waters, A. P. Smit, N. Grinsztajn, U. A. M. Sob, O. Mahjoub, E. Tegegn, M. A. Mimouni, R. Boige, R. de Kock, D. Furelos-Blanco, V. Le, A. Pretorius, and A. Laterre. Jumanji: a diverse suite of scalable reinforcement learning environments in jax, 2023. URL https://arxiv.org/abs/2306.09884.\\n\\n[10] A. Bou, M. Bettini, S. Dittert, V. Kumar, S. Sodhani, X. Yang, G. De Fabritiis, and V. Moens. TorchRL: A data-driven decision-making library for PyTorch. In arXiv, 2023. URL https://arxiv.org/abs/2306.00577.\\n\\n[11] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\\n\\n[12] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\\n\\n[13] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\"}"}
{"id": "LJhfKeqZdu", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"R. Cheng and J. Yan. On joint learning for solving placement and routing in chip design. Advances in Neural Information Processing Systems, 34:16508\u201316519, 2021.\\n\\nT. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023.\\n\\nT. Dao, D. Fu, S. Ermon, A. Rudra, and C. R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359, 2022.\\n\\nV. C. David Applegate, Robert Bixby and W. Cook. Concorde tsp solver, 2023. URL https://www.math.uwaterloo.ca/tsp/concorde/index.html.\\n\\nW. Falcon and The PyTorch Lightning team. PyTorch Lightning, 3 2019. URL https://github.com/Lightning-AI/lightning.\\n\\nM. Fischetti, J. J. S. Gonzalez, and P. Toth. Solving the orienteering problem through branch-and-cut. INFORMS Journal on Computing, 10(2):133\u2013148, 1998.\\n\\nZ.-H. Fu, K.-B. Qiu, and H. Zha. Generalize a small pre-trained model to arbitrarily large tsp instances. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 7474\u20137482, 2021.\\n\\nM. Gagrani, C. Rainone, Y. Yang, H. Teague, W. Jeon, R. Bondesan, H. van Hoof, C. Lott, W. Zeng, and P. Zappi. Neural topological ordering for computation graphs. Advances in Neural Information Processing Systems, 35:17327\u201317339, 2022.\\n\\nL. Gurobi Optimization. Gurobi optimizer reference manual, 2021. URL http://www.gurobi.com.\\n\\nK. Helsgaun. An extension of the lin-kernighan-helsgaun tsp solver for constrained traveling salesman and vehicle routing problems. Roskilde: Roskilde University, 12 2017. doi: 10.13140/RG.2.2.25569.40807.\\n\\nA. Hottung and K. Tierney. Neural large neighborhood search for the capacitated vehicle routing problem. CoRR, abs/1911.09539, 2019. URL http://arxiv.org/abs/1911.09539.\\n\\nA. Hottung, Y.-D. Kwon, and K. Tierney. Efficient active search for combinatorial optimization problems. arXiv preprint arXiv:2106.05126, 2021.\\n\\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\\n\\nS. Huang, R. F. J. Dossa, C. Ye, J. Braga, D. Chakraborty, K. Mehta, and J. G. Ara\u00fajo. Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. Journal of Machine Learning Research, 23(274):1\u201318, 2022. URL http://jmlr.org/papers/v23/21-1342.html.\\n\\nJ. Hwang, J. S. Pak, D. Yoon, H. Lee, J. Jeong, Y. Heo, and I. Kim. Enhancing on-die pdn for optimal use of package pdn with decoupling capacitor. In 2021 IEEE 71st Electronic Components and Technology Conference (ECTC), pages 1825\u20131830, 2021. doi: 10.1109/ECTC32696.2021.00288.\\n\\nL. Ivan. Capacitated vehicle routing problem library. http://vrp.atd-lab.inf.puc-rio.br/index.php/en/. 2014.\\n\\nY. Jin, Y. Ding, X. Pan, K. He, L. Zhao, T. Qin, L. Song, and J. Bian. Pointerformer: Deep reinforced multi-pointer transformer for the traveling salesman problem. Proceedings of the AAAI Conference on Artificial Intelligence, 37(7):8132\u20138140, Jun. 2023. doi: 10.1609/aaai.v37i7.25982. URL https://ojs.aaai.org/index.php/AAAI/article/view/25982.\\n\\nC. K. Joshi, Q. Cappart, L.-M. Rousseau, and T. Laurent. Learning tsp requires rethinking generalization. In 27th International Conference on Principles and Practice of Constraint Programming (CP 2021). Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik, 2021.\\n\\n\"}"}
{"id": "LJhfKeqZdu", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"J. Juang, L. Zhang, Z. Kiguradze, B. Pu, S. Jin, and C. Hwang. A modified genetic algorithm for the selection of decoupling capacitors in PDN design. In 2021 IEEE International Joint EMC/SI/PI and EMC Europe Symposium, pages 712\u2013717, 2021. doi: 10.1109/EMC/SI/PI/EMCEurope52599.2021.9559292.\\n\\nE. Khalil, H. Dai, Y. Zhang, B. Dilkina, and L. Song. Learning combinatorial optimization algorithms over graphs. Advances in neural information processing systems, 30, 2017.\\n\\nH. Kim, M. Kim, S. Ahn, and J. Park. Symmetric exploration in combinatorial optimization is free!, 2023.\\n\\nH. Kim, M. Kim, F. Berto, J. Kim, and J. Park. DevFormer: A symmetric transformer for context-aware device placement, 2023.\\n\\nM. Kim, J. Park, and J. Park. Sym-NCO: Leveraging symmetricity for neural combinatorial optimization. Advances in Neural Information Processing Systems, 2022.\\n\\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nW. Kool, H. Van Hoof, and M. Welling. Attention, learn to solve routing problems! International Conference on Learning Representations, 2019.\\n\\nW. Kool, H. van Hoof, J. A. S. Gromicho, and M. Welling. Deep policy dynamic programming for vehicle routing problems. CoRR, abs/2102.11756, 2021. URL https://arxiv.org/abs/2102.11756.\\n\\nY.-D. Kwon, J. Choo, B. Kim, I. Yoon, Y. Gwon, and S. Min. POMO: Policy optimization with multiple optima for reinforcement learning. Advances in Neural Information Processing Systems, 33:21188\u201321198, 2020.\\n\\nJ. Li, L. Xin, Z. Cao, A. Lim, W. Song, and J. Zhang. Heterogeneous attentions for solving pickup and delivery problem via deep reinforcement learning. IEEE Transactions on Intelligent Transportation Systems, 23(3):2306\u20132315, 2021.\\n\\nS. Li, Z. Yan, and C. Wu. Learning to delegate for large-scale vehicle routing. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nS. Manchanda, S. Michel, D. Drakulic, and J.-M. Andreoli. On the generalization of neural combinatorial optimization heuristics. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2022, Grenoble, France, September 19\u201323, 2022, Proceedings, Part V, pages 426\u2013442. Springer, 2023.\\n\\nN. Mazyavkina, S. Sviridov, S. Ivanov, and E. Burnaev. Reinforcement learning for combinatorial optimization: A survey. Computers & Operations Research, 134:105400, 2021.\\n\\nV. Mnih, N. Heess, A. Graves, et al. Recurrent models of visual attention. Advances in neural information processing systems, 27, 2014.\\n\\nV. Moens. TensorDict: your PyTorch universal data carrier, 2023. URL https://github.com/pytorch-labs/tensordict.\\n\\nV. Moens. TorchRL: an open-source Reinforcement Learning (RL) library for PyTorch, 2023. URL https://github.com/pytorch/rl.\\n\\nS. A. Mulder and D. C. Wunsch II. Million city traveling salesman problem solution by divide and conquer clustering with adaptive resonance neural networks. Neural Networks, 16(5-6):827\u2013832, 2003.\\n\\nM. Nazari, A. Oroojlooy, L. Snyder, and M. Tak\u00e1c. Reinforcement learning for solving the vehicle routing problem. Advances in neural information processing systems, 31, 2018.\\n\\nOpenAI. GPT-4 technical report, 2023.\\n\\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\\n\\nS. A. Mulder and D. C. Wunsch II. Million city traveling salesman problem solution by divide and conquer clustering with adaptive resonance neural networks. Neural Networks, 16(5-6):827\u2013832, 2003.\"}"}
{"id": "LJhfKeqZdu", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"J. Park, J. Chun, S. H. Kim, Y. Kim, and J. Park. Learning to schedule job-shop problems: representation and policy learning using graph neural network and reinforcement learning. International Journal of Production Research, 59(11):3360\u20133377, 2021.\\n\\nJ. Park, C. Kwon, and J. Park. Learn to solve the min-max multiple traveling salesmen problem with reinforcement learning. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, pages 878\u2013886, 2023.\\n\\nA. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in PyTorch. In NIPS-W, 2017.\\n\\nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\\n\\nY. Peng, B. Choi, and J. Xu. Graph learning for combinatorial optimization: a survey of state-of-the-art. Data Science and Engineering, 6(2):119\u2013141, 2021.\\n\\nL. Perron and V. Furnon. Or-tools. URL https://developers.google.com/optimization/.\\n\\nM. V. Pogan\u010di\u0107, A. Paulus, V. Musil, G. Martius, and M. Rolinek. Differentiation of blackbox combinatorial solvers. In International Conference on Learning Representations, 2019.\\n\\nM. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023.\\n\\nR. Qiu, Z. Sun, and Y. Yang. DIMES: A differentiable meta solver for combinatorial optimization problems. arXiv preprint arXiv:2210.04123, 2022.\\n\\nA. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and N. Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1\u20138, 2021. URL http://jmlr.org/papers/v22/20-1364.html.\\n\\nG. Reinelt. Tsplib\u2014a traveling salesman problem library. ORSA journal on computing, 3(4):376\u2013384, 1991.\\n\\nS. Ropke and D. Pisinger. An adaptive large neighborhood search heuristic for the pickup and delivery problem with time windows. Transportation science, 40(4):455\u2013472, 2006.\\n\\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\n\\nJ. Son, M. Kim, H. Kim, and J. Park. Meta-SAGE: Scale meta-learning scheduled adaptation with guided exploration for mitigating scale shift on combinatorial optimization, 2023.\\n\\nZ. Sun and Y. Yang. Difusco: Graph-based diffusion solvers for combinatorial optimization. arXiv preprint arXiv:2302.08224, 2023.\\n\\nR. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\\n\\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\\n\\nT. Vidal. Hybrid genetic search for the cvrp: Open-source implementation and swap* neighborhood. Computers & Operations Research, 140:105643, 2022.\\n\\nO. Vinyals, M. Fortunato, and N. Jaitly. Pointer networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28, pages 2692\u20132700. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/29921001f2f04bd3baee84a12e98098f-Paper.pdf.\\n\\nT. Vidal. Hybrid genetic search for the cvrp: Open-source implementation and swap* neighborhood. Computers & Operations Research, 140:105643, 2022.\"}"}
{"id": "LJhfKeqZdu", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C. P. Wan, T. Li, and J. M. Wang. Rlor: A flexible framework of deep reinforcement learning for operation research. arXiv preprint arXiv:2303.13117, 2023.\\n\\nR. Wang, L. Shen, Y. Chen, X. Yang, D. Tao, and J. Yan. Towards one-shot neural combinatorial solvers: Theoretical and empirical notes on the cardinality-constrained case. In The Eleventh International Conference on Learning Representations, 2022.\\n\\nR. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Reinforcement learning, pages 5\u201332, 1992.\\n\\nO. Yadan. Hydra - a framework for elegantly configuring complex applications. Github, 2019. URL https://github.com/facebookresearch/hydra.\\n\\nC. Zhang, W. Song, Z. Cao, J. Zhang, P. S. Tan, and X. Chi. Learning to dispatch for job shop scheduling via deep reinforcement learning. Advances in Neural Information Processing Systems, 33:1621\u20131632, 2020.\"}"}
